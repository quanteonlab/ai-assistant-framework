filename,title,text,len
01-Preface Or What Are You Getting Yourself Into Here.pdf,01-Preface Or What Are You Getting Yourself Into Here,"Preface Or: What Are You\nGetting \nYourself Into Here?\nHello adventurous reader, welcome to our book! At this point, I assume that\nyou’re either interested in learning more about the wonders of stream\nprocessing or hoping to spend a few hours reading about the glory of the\nmajestic brown trout. Either way, I salute you! That said, those of you in the\nlatter bucket who don’t also have an advanced understanding of computer\nscience should consider how prepared you are to deal with disappointment\nbefore forging ahead; \ncaveat piscator\n, and all that.\nTo set the tone for this book from the get go, I wanted to give you a heads up\nabout a couple of things. First, this book is a little strange in that we have\nmultiple authors, but we’re not pretending that we somehow all speak and\nwrite in the same voice like we’re weird identical triplets who happened to be\nborn to different sets of parents. Because as interesting as that sounds, the end\nresult would actually be less enjoyable to read. Instead, we’ve opted to each\nwrite in our own voices, and we’ve granted the book just enough self-\nawareness to be able to make reference to each of us where appropriate, but\nnot so much self-awareness that it resents us for making it only into a book\nand not something cooler like a robot dinosaur with a Scottish accent.\nAs far as voices go, there are three you’ll come across:\nTyler\nThat would be me. If you haven’t explicitly been told someone else is\nspeaking, you can assume that it’s me, because we added the other\nauthors somewhat late in the game, and I was basically like, “hells no”\nwhen I thought about going back and updating everything I’d already\nwritten. I’m the technical lead for the Data Processing Languages ands\nSystems\n group at Google, responsible for Google Cloud Dataflow,\nGoogle’s Apache Beam efforts, as well as Google-internal data\n1\n2\nprocessing systems such as Flume, MillWheel, and MapReduce. I’m also\na founding Apache Beam PMC member.\nFigure P-1. \nThe cover that could have been...",2044
02-Navigating This Book.pdf,02-Navigating This Book,"Slava\nSlava was a long-time member of the MillWheel team at Google, and\nlater an original member of the Windmill team that built MillWheel’s\nsuccessor, the heretofore unnamed system that powers the Streaming\nEngine in Google Cloud Dataflow. Slava is the foremost expert on\nwatermarks and time semantics in stream processing systems the world\nover, period. You might find it unsurprising then that he’s the author of\nChapter 3, \nWatermarks\n.\nReuven\nReuven is at the bottom of this list because he has more experience with\nstream processing than both Slava and me combined and would thus\ncrush us if he were placed any higher. Reuven has created or led the\ncreation of nearly all of the interesting systems-level magic in Google’s\ngeneral-purpose stream processing engines, including applying an untold\namount of attention to detail in providing high-throughput, low-latency,\nexactly-once semantics in a system that nevertheless utilizes fine-grained\ncheckpointing. You might find it unsurprising that he’s the author of\nChapter 5, \nExactly-Once and Side Effects\n. He also happens to be an\nApache Beam PMC member.\nNavigating This Book\nNow that you know who you’ll be hearing from, the next logical step would\nbe to find out what you’ll be hearing about, which brings us to the second\nthing I wanted to mention. There are conceptually two major parts to this\nbook, each with four chapters, and each followed up by a chapter that stands\nrelatively independently on its own.\nThe fun begins with \nPart I, \nThe Beam Model\n (Chapters \n1\n–\n4\n), which focuses\non the high-level batch plus streaming data processing model originally\ndeveloped for Google Cloud Dataflow, later donated to the Apache Software\nFoundation as Apache Beam, and also now seen in whole or in part across\nmost other systems in the industry. It’s composed of four chapters:\nChapter 1, \nStreaming 101\n, which covers the basics of stream\nprocessing, establishing some terminology, discussing the\ncapabilities of streaming systems, distinguishing between two\nimportant domains of time (processing time and event time), and\nfinally looking at some common data processing patterns.\nChapter 2, \nThe \nWhat\n, \nWhere\n, \nWhen\n, and \nHow\n \nof Data Processing\n,\nwhich covers in detail the core concepts of robust stream processing\nover out-of-order data, each analyzed within the context of a\nconcrete running example and with animated diagrams to highlight\nthe dimension of time.\nChapter 3, \nWatermarks\n (written by Slava), which provides a deep\nsurvey of temporal progress metrics, how they are created, and how\nthey propagate through pipelines. It ends by examining the details of\ntwo real-world watermark implementations.\nChapter 4, \nAdvanced Windowing\n, which picks up where \nChapter 2\nleft off, diving into some advanced windowing and triggering\nconcepts like processing-time windows, sessions, and continuation\ntriggers.\nBetween Parts \nI\n and \nII\n, providing an interlude as timely as the details\ncontained therein are important, stands \nChapter 5, \nExactly-Once and Side\nEffects\n (written by Reuven). In it, he enumerates the challenges of providing\nend-to-end exactly-once (or effectively-once) processing semantics and walks\nthrough the implementation details of three different approaches to exactly-\nonce processing: Apache Flink, Apache Spark, and Google Cloud Dataflow.\nNext begins \nPart II, \nStreams and Tables\n (Chapters \n6\n–\n9\n), which dives deeper\ninto the conceptual and investigates the lower-level “streams and tables” way\nof thinking about stream processing, recently popularized by some\nupstanding citizens in the Apache Kafka community but, of course, invented\ndecades ago by folks in the database community, because wasn’t everything?\nIt too is composed of four chapters:\nChapter 6, \nStreams and Tables\n, which introduces the basic idea of",3919
03-Figures.pdf,03-Figures,"streams and tables, analyzes the classic MapReduce approach\nthrough a streams-and-tables lens, and then constructs a theory of\nstreams and tables sufficiently general to encompass the full breadth\nof the Beam Model (and beyond).\nChapter 7, \nThe Practicalities of Persistent State\n, which considers the\nmotivations for persistent state in streaming pipelines, looks at two\ncommon types of implicit state, and then analyzes a practical use\ncase (advertising attribution) to inform the necessary characteristics\nof a general state management mechanism.\nChapter 8, \nStreaming SQL\n, which investigates the meaning of\nstreaming within the context of relational algebra and SQL, contrasts\nthe inherent stream and table biases within the Beam Model and\nclassic SQL as they exist today, and proposes a set of possible paths\nforward toward incorporating robust streaming semantics in SQL.\nChapter 9, \nStreaming Joins\n, which surveys a variety of different join\ntypes, analyzes their behavior within the context of streaming, and\nfinally looks in detail at a useful but ill-supported streaming join use\ncase: temporal validity windows.\nFinally, closing out the book is \nChapter 10, \nThe Evolution of Large-Scale\nData Processing\n, which strolls through a focused history of the MapReduce\nlineage of data processing systems, examining some of the important\ncontributions that have evolved streaming systems into what they are today.\nTakeaways\nAs a final bit of guidance, if you were to ask me to describe the things I most\nwant readers to take away from this book, I would say this:\nThe single most important thing you can learn from this book is the\ntheory of streams and tables and how they relate to one another.\nEverything else builds on top of that. No, we won’t get to this topic\nuntil \nChapter 6\n. That’s okay; it’s worth the wait, and you’ll be better\nprepared to appreciate its awesomeness by then.\nTime-varying relations are a revelation. They are stream processing\nincarnate: an embodiment of everything streaming systems are built\nto achieve and a powerful connection to the familiar tools we all\nknow and love from the world of batch. We won’t learn about them\nuntil \nChapter 8\n, but again, the journey there will help you appreciate\nthem all the more.\nA well-written distributed streaming engine is a magical thing. This\narguably goes for distributed systems in general, but as you learn\nmore about how these systems are built to provide the semantics\nthey do (in particular, the case studies from Chapters \n3\n and \n5\n), it\nbecomes all the more apparent just how much heavy lifting they’re\ndoing for you.\nLaTeX/Tikz is an amazing tool for making diagrams, animated or\notherwise. A horrible, crusty tool with sharp edges and tetanus, but\nan incredible tool nonetheless. I hope the clarity the animated\ndiagrams in this book bring to the complex topics we discuss will\ninspire more people to give LaTeX/Tikz a try (in \n“Figures”\n, we\nprovide for a link to the full source for the animations from this\nbook).\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file\nextensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to\nprogram elements such as variable or function names, databases, data\ntypes, environment variables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values\ndetermined by context.\nTIP\nThis element signifies a tip or suggestion.\nNOTE\nThis element signifies a general note.\nWARNING\nThis element indicates a warning or caution.\nOnline Resources\nThere are a handful of associated online resources to aid in your enjoyment of\nthis book.\nFigures\nAll the of the figures in this book are available in digital form on the book’s\nwebsite. This is particularly useful for the animated figures, only a few\nframes of which appear (comic-book style) in the non-Safari formats of the\nbook:",4192
04-Code Snippets.pdf,04-Code Snippets,"Online index: \nhttp://www.streamingbook.net/figures\nSpecific figures may be referenced at URLs of the form:\nhttp://www.streamingbook.net/fig/<FIGURE-NUMBER>\nFor example, for \nFigure 2-5\n: \nhttp://www.streamingbook.net/fig/2-5\nThe animated figures themselves are LaTeX/Tikz drawings, rendered first to\nPDF, then converted to animated GIFs via ImageMagick. For the more\nintrepid among you, full source code and instructions for rendering the\nanimations (from this book, the \n“Streaming 101”\n and \n“Streaming 102”\n blog\nposts, and the original \nDataflow Model paper\n) are available on GitHub at\nhttp://github.com/takidau/animations\n. Be warned that this is roughly 14,000\nlines of LaTeX/Tikz code that grew very organically, with no intent of ever\nbeing read and used by others. In other words, it’s a messy, intertwined web\nof archaic incantations; turn back now or abandon all hope ye who enter here,\nfor there be dragons.\nCode Snippets\nAlthough this book is largely conceptual, there are are number of code and\npsuedo-code snippets used throughout to help illustrate points. Code for the\nmore functional core Beam Model concepts from Chapters \n2\n and \n4\n, as well as\nthe more imperative state and timers concepts in \nChapter 7\n, is available\nonline at \nhttp://github.com/takidau/streamingbook\n. Since understanding\nsemantics is the main goal, the code is provided primarily as Beam\nPTransform\n/\nDoFn\n implementations and accompanying unit tests. There is\nalso a single standalone pipeline implementation to illustrate the delta\nbetween a unit test and a real pipeline. The code layout is as follows:\nsrc/main/java/net/streamingbook/BeamModel.java\nBeam \nPTransform\n implementations of Examples \n2-1\n through \n2-9\n and\nExample 4-3\n, each with an additional method returning the expected\noutput when executed over the example datasets from those chapters.\nsrc/test/java/net/streamingbook/BeamModelTest.java\nUnit tests verifying the example \nPTransforms\n in \nBeamModel.java\n via\ngenerated datasets matching those in the book.\nsrc/main/java/net/streamingbook/Example2_1.java\nStandalone version of the \nExample 2-1\n pipeline that can be run locally or\nusing a distributed Beam runner.\nsrc/main/java/net/streamingbook/inputs.csv\nSample input file for \nExample2_1.java\n containing the dataset from the\nbook.\nsrc/main/java/net/streamingbook/StateAndTimers.java\nBeam code implementing the conversion attribution example from\nChapter 7\n using Beam’s state and timers primitives.\nsrc/test/java/net/streamingbook/StateAndTimersTest.java\nUnit test verifying the conversion attribution \nDoFn\ns from\nStateAndTimers.java\n.\nsrc/main/java/net/streamingbook/ValidityWindows.java\nTemporal validity windows implementation.\nsrc/main/java/net/streamingbook/Utils.java\nShared utility methods.\nThis book is here to help you get your job done. In general, if example code\nis offered with this book, you may use it in your programs and\ndocumentation. You do not need to contact us for permission unless you’re\nreproducing a significant portion of the code. For example, writing a program\nthat uses several chunks of code from this book does not require permission.\nSelling or distributing a CD-ROM of examples from O’Reilly books does\nrequire permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation\ndoes require permission.",3543
05-OReilly Safari.pdf,05-OReilly Safari,,0
06-Acknowledgments.pdf,06-Acknowledgments,"We appreciate, but do not require, attribution. An attribution usually includes\nthe title, author, publisher, and ISBN. For example: “\nStreaming Systems\n by\nTyler Akidau, Slava Chernyak, and Reuven Lax (O’Reilly). Copyright 2018\nO’Reilly Media, Inc., 978-1-491-98387-4.”\nIf you feel your use of code examples falls outside fair use or the permission\ngiven above, feel free to contact us at \npermissions@oreilly.com\n.\nO’Reilly Safari\nSafari\n (formerly Safari Books Online) is a membership-based training and\nreference platform for enterprise, government, educators, and individuals.\nMembers have access to thousands of books, training videos, Learning Paths,\ninteractive tutorials, and curated playlists from over 250 publishers, including\nO’Reilly Media, Harvard Business Review, Prentice Hall Professional,\nAddison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press,\nAdobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan\nKaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning,\nNew Riders, McGraw-Hill, Jones & Bartlett, and Course Technology, among\nothers.\nFor more information, please visit \nhttp://www.oreilly.com/safari\n.\nHow to Contact Us\nPlease address comments and questions concerning this book to the\npublisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any\nadditional information. You can access this page at \nhttp://bit.ly/streaming-\nsystems\n.\nTo comment or ask technical questions about this book, send email to\nbookquestions@oreilly.com\n.\nFor more information about our books, courses, conferences, and news, see\nour website at \nhttp://www.oreilly.com\n.\nFind us on Facebook: \nhttp://facebook.com/oreilly\nFollow us on Twitter: \nhttp://twitter.com/oreillymedia\nWatch us on YouTube: \nhttp://www.youtube.com/oreillymedia\nAcknowledgments\nLast, but certainly not least: many people are awesome, and we would like to\nacknowledge a specific subset of them here for their help in creating this\ntome.\nThe content in this book distills the work of an untold number of extremely\nsmart individuals across Google, the industry, and academia at large. We owe\nthem all a sincere expression of gratitude and regret that we could not\npossibly list them all here, even if we tried, which we will not.\nAmong our colleagues at Google, much credit goes to everyone in the\nDataPLS team (and its various ancestor teams: Flume, MillWheel,\nMapReduce, et al.), who’ve helped bring so many of these ideas to life over\nthe years. In particular, we’d like to thank:\nPaul Nordstrom and the rest of the MillWheel team from the Golden\nAge of MillWheel: Alex Amato, Alex Balikov, Kaya Bekiroğlu,\nJosh Haberman, Tim Hollingsworth, Ilya Maykov, Sam McVeety,\nDaniel Mills, and Sam Whittle for envisioning and building such a\ncomprehensive, robust, and scalable set of low-level primitives on\ntop of which we were later able to construct the higher-level models\ndiscussed in this book. Without their vision and skill, the world of\nmassive-scale stream processing would look very different.\nCraig Chambers, Frances Perry, Robert Bradshaw, Ashish Raniwala,\nand the rest of the Flume team of yore for envisioning and creating\nthe expressive and powerful data processing foundation that we were\nlater able to unify with the world of streaming.\nSam McVeety for lead authoring the original MillWheel paper,\nwhich put our amazing little project on the map for the very first\ntime.\nGrzegorz Czajkowski for repeatedly supporting our evangelization\nefforts, even as competing deadlines and priorities loomed.\nLooking more broadly, a huge amount of credit is due to everyone in the\nApache Beam, Calcite, Kafka, Flink, Spark, and Storm communities. Each\nand every one of these projects has contributed materially to advancing the\nstate of the art in stream processing for the world at large over the past\ndecade. Thank you.\nTo shower gratitude a bit more specifically, we would also like to thank:\nMartin Kleppmann, for leading the charge in advocating for the\nstreams-and-tables way of thinking, and also for investing a huge\namount of time providing piles of insightful technical and editorial\ninput on the drafts of every chapter in this book. All this in addition\nto being an inspiration and all-around great guy.\nJulian Hyde, for his insightful vision and infectious passion for\nstreaming SQL.\nJay Kreps, for fighting the good fight against Lambda Architecture\ntyranny; it was your original \n“Questioning the Lambda Architecture”\npost that got Tyler pumped enough to go out and join the fray, as\nwell.\nStephan Ewen, Kostas Tzoumas, Fabian Hueske, Aljoscha Krettek,\nRobert Metzger, Kostas Kloudas, Jamie Grier, Max Michels, and the\nrest of the data Artisans extended family, past and present, for\nalways pushing the envelope of what’s possible in stream\nprocessing, and doing so in a consistently open and collaborative\nway. The world of streaming is a much better place thanks to all of\nyou.\nJesse Anderson, for his diligent reviews and for all the hugs. If you\nsee Jesse, give him a big hug for me.\nDanny Yuan, Sid Anand, Wes Reisz, and the amazing QCon\ndeveloper conference, for giving us our first opportunity to talk\npublicly within the industry about our work, at QCon San Francisco\n2014.\nBen Lorica at O’Reilly and the iconic Strata Data Conference, for\nbeing repeatedly supportive of our efforts to evangelize stream\nprocessing, be it online, in print, or in person.\nThe entire Apache Beam community, and in particular our fellow\ncommitters, for helping push forward the Beam vision: Ahmet Altay,\nAmit Sela, Aviem Zur, Ben Chambers, Griselda Cuevas, Chamikara\nJayalath, Davor Bonaci, Dan Halperin, Etienne Chauchot, Frances\nPerry, Ismaël Mejía, Jason Kuster, Jean-Baptiste Onofré, Jesse\nAnderson, Eugene Kirpichov, Josh Wills, Kenneth Knowles, Luke\nCwik, Jingsong Lee, Manu Zhang, Melissa Pashniak, Mingmin Xu,\nMax Michels, Pablo Estrada, Pei He, Robert Bradshaw, Stephan\nEwen, Stas Levin, Thomas Groh, Thomas Weise, and James Xu.\nNo acknowledgments section would be complete without a nod to the\notherwise faceless cohort of tireless reviewers whose insightful comments\nhelped turn garbage into awesomeness: Jesse Anderson, Grzegorz\nCzajkowski, Marián Dvorský, Stephan Ewen, Rafael J. Fernández-\nMoctezuma, Martin Kleppmann, Kenneth Knowles, Sam McVeety, Mosha\nPasumansky, Frances Perry, Jelena Pjesivac-Grbovic, Jeff Shute, and\nWilliam Vambenepe. You are the Mr. Fusion to our DeLorean Time\nMachine. That had a nicer ring to it in my head—see, this is what I’m talking\nabout.\nAnd of course, a big thanks to our authoring and production support team:\nMarie Beaugureau, our original editor, for all of her help and support\nin getting this project off the ground and her everlasting patience\nwith my persistent desire to subvert editorial norms. We miss you!\nJeff Bleiel, our editor 2.0, for taking over the reins and helping us\nland this monster of a project and his everlasting patience with our\ninability to meet even the most modest of deadlines. We made it!\nBob Russell, our copy editor, for reading our book more closely than\nanyone should ever have to. I tip my hat to your masterful command\nof grammar, punctuation, vocabulary, and Adobe Acrobat\nannotations.\nNick Adams, our intrepid production editor, for helping tame a mess\nof totally sketchy HTMLBook code into a print-worthy thing of\nbeauty and for not getting mad at me when I asked him to manually\nignore Bob’s many, many individual suggestions to switch our usage\nof the term “data” from plural to singular. You’ve managed to make\nthis book look even better than I’d hoped for, thank you.\nEllen Troutman-Zaig, our indexer, for somehow weaving a tangled\nweb of offhand references into a useful and comprehensive index. I\nstand in awe at your attention to detail.\nRebecca Panzer, our illustrator, for beautifying our static diagrams\nand for assuring Nick that I didn’t need to spend more weekends\nfiguring out how to refactor my animated LaTeX diagrams to have\nlarger fonts. Phew x2!\nKim Cofer, our proofreader, for pointing out how sloppy and\ninconsistent we were so others wouldn’t have to.\nTyler would like to thank:\nMy coauthors, Reuven Lax and Slava Chernyak, for bringing their\nideas and chapters to life in ways I never could have.\nGeorge Bradford Emerson II, for the Sean Connery inspiration.\nThat’s my favorite joke in the book and we haven’t even gotten to\nthe first chapter yet. It’s all downhill from here, folks.\nRob Schlender, for the amazing bottle of scotch he’s going to buy\nme shortly before robots take over the world. Here’s to going down\nin style!\nMy uncle, Randy Bowen, for making sure I discovered just how\nmuch I love computers and, in particular, that homemade POV-Ray\n2.x floppy disk that opened up a whole new world for me.\nMy parents, David and Marty Dauwalder, without whose dedication\nand unbelievable perseverance none of this would have ever been\npossible. You’re the best parents ever, for reals!\nDr. David L. Vlasuk, without whom I simply wouldn’t be here\ntoday. Thanks for everything, Dr. V.\nMy wonderful family, Shaina, Romi, and Ione Akidau for their\nunwavering support in completing this levianthantine effort, despite\nthe many nights and weekends we spent apart as a result. I love you\nalways.\nMy faithful writing partner, Kiyoshi: even though you only slept and\nbarked at postal carriers the entire time we worked on the book\ntogether, you did so flawlessly and seemingly without effort. You\nare a credit to your species.\nSlava would like to thank:\nJosh Haberman, Sam Whittle, and Daniel Mills for being\ncodesigners and cocreators of watermarks in MillWheel and\nsubsequently Streaming Dataflow as well as many other parts of\nthese systems. Systems as complex as these are never designed in a\nvacuum, and without all of the thoughts and hard work that each of\nyou put in, we would not be here today.\nStephan Ewen of data Artisans for helping shape my thoughts and\nunderstanding of the watermark implementation in Apache Flink.\nReuven would like to thank:\nPaul Nordstrom for his vision, Sam Whittle, Sam McVeety, Slava\nChernyak, Josh Haberman, Daniel Mills, Kaya Bekiroğlu, Alex\nBalikov, Tim Hollingsworth, Alex Amato, and Ilya Maykov for all\ntheir efforts in building the original MillWheel system and writing\nthe subsequent paper.\nStephan Ewen of data Artisans for his help reviewing the chapter on\nexactly-once semantics, and valuable feedback on the inner\nworkings of Apache Flink.\nLastly, we would all like to thank \nyou\n, glorious reader, for being willing to\nspend real money on this book to hear us prattle on about the cool stuff we\nget to build and play with. It’s been a joy writing it all down, and we’ve done\nour best to make sure you’ll get your money’s worth. If for some reason you\ndon’t like it...well hopefully you bought the print edition so you can at least\nthrow it across the room in disgust before you sell it at a used bookstore.\nWatch out for the cat.\n Which incidentally is what we requested our animal book cover be, but\nO’Reilly felt it wouldn’t translate well into line art. I respectfully disagree,\nbut a brown trout is a fair compromise.\n Or DataPLS, pronounced Datapals—get it?\n Or don’t. I actually don’t like cats.\n3\n1\n2\n3",11598
07-Terminology What Is Streaming.pdf,07-Terminology What Is Streaming,"Part I. \nThe Beam Model\nChapter 1. \nStreaming 101\nStreaming data processing is a big deal in big data these days, and for good\nreasons; among them \nare the following:\nBusinesses crave ever-more timely insights into their data, and\nswitching to streaming is a good way to achieve lower latency\nThe massive, unbounded datasets that are increasingly common in\nmodern business are more easily tamed using a system designed for\nsuch never-ending volumes of data.\nProcessing data as they arrive spreads workloads out more evenly\nover time, yielding more consistent and predictable consumption of\nresources.\nDespite this business-driven surge of interest in streaming, streaming systems\nlong remained relatively immature compared to their batch brethren. It’s only\nrecently that the tide has swung conclusively in the other direction. In my\nmore bumptious moments, I hope that might be in small part due to the solid\ndose of goading I originally served up in my \n“Streaming 101”\n and\n“Streaming 102”\n blog posts (on which the first few chapters of this book are\nrather obviously based). But in reality, there’s also just a lot of industry\ninterest in seeing streaming systems mature and a lot of smart and active\nfolks out there who enjoy building them.\nEven though the battle for general streaming advocacy has been, in my\nopinion, effectively won, I’m still going to present my original arguments\nfrom “Streaming 101” more or less unaltered. For one, they’re still very\napplicable today, even if much of industry has begun to heed the battle cry.\nAnd for two, there are a lot of folks out there who still haven’t gotten the\nmemo; this book is an extended attempt at getting these points across.\nTo begin, I cover some important background information that will help\nframe the rest of the topics I want to discuss. I do this in three specific\nsections:\nTerminology\nTo talk precisely about complex topics requires precise definitions of\nterms. For some terms that have overloaded interpretations in current use,\nI’ll try to nail down exactly what I mean when I say them.\nCapabilities\nI remark on the oft-perceived shortcomings of streaming systems. I also\npropose the frame of mind that I believe data processing system builders\nneed to adopt in order to address the needs of modern data consumers\ngoing forward.\nTime domains\nI introduce the two primary domains of time that are relevant in data\nprocessing, show how they relate, and point out some of the difficulties\nthese two domains impose.\nTerminology: What Is Streaming?\nBefore going any further, I’d like to get \none thing out of the way: what is\nstreaming? The term streaming is used today to mean a variety of different\nthings (and for simplicity I’ve been using it somewhat loosely up until now),\nwhich can lead to misunderstandings about what streaming really is or what\nstreaming systems are actually capable of. As a result, I would prefer to\ndefine the term somewhat precisely.\nThe crux of the problem is that many things that ought to be described by\nwhat\n they are (unbounded data processing, approximate results, etc.), have\ncome to be described colloquially by \nhow\n they historically have been\naccomplished (i.e., via streaming execution engines). This lack of precision\nin terminology clouds what streaming really means, and in some cases it\nburdens streaming systems themselves with the implication that their\ncapabilities are limited to characteristics historically described as\n“streaming,” such as approximate or speculative results.\nGiven that well-designed streaming systems are just as capable (technically\nmore so) of producing correct, consistent, repeatable results as any existing\nbatch engine, I prefer to isolate \nthe term “streaming” to a very specific\nmeaning:\nStreaming system\nA type of data processing engine that is designed with infinite datasets in\nmind.\nIf I want to talk about low-latency, approximate, or speculative results, I use\nthose specific words rather than imprecisely calling them “streaming.”\nPrecise terms are also useful when discussing the different types of data one\nmight encounter.\n From my perspective, there are two important (and\northogonal) dimensions \nthat define the shape of a given dataset: \ncardinality\nand \nconstitution\n.\nThe cardinality of a dataset dictates its size, with the most salient aspect of\ncardinality being whether a given dataset is finite or infinite. Here are the two\nterms I prefer to use for describing the coarse cardinality in a dataset:\nBounded data\nA type of dataset that\n is finite in size.\nUnbounded data\nA type of dataset that is infinite \nin size (at least theoretically).\nCardinality is important because the unbounded nature of infinite datasets\nimposes additional burdens on data processing frameworks that consume\nthem. More on this in the next section.\nThe constitution of a dataset, on the\n other hand, dictates\n its physical\nmanifestation. As a result, the constitution defines the ways one can interact\nwith the data in question. We won’t get around to deeply examining\nconstitutions until \nChapter 6\n, but to give you a brief sense of things, there are\ntwo primary constitutions of importance:\nTable\n1",5257
08-On the Greatly Exaggerated Limitations of Streaming.pdf,08-On the Greatly Exaggerated Limitations of Streaming,"A holistic view of a dataset at a specific point in time. SQL systems have\ntraditionally dealt in tables.\nStream\nAn element-by-element view of the evolution of a dataset over time. The\nMapReduce lineage\n of data processing systems have traditionally dealt in\nstreams.\nWe look quite deeply at the relationship between streams and tables in\nChapters \n6\n, \n8\n, and \n9\n, and in \nChapter 8\n we also learn about the unifying\nunderlying concept of \ntime-varying relations\n that ties them together. But until\nthen, we deal primarily in streams because that’s the constitution pipeline\ndevelopers directly interact with in most data processing systems today (both\nbatch and streaming). It’s also the constitution that most naturally embodies\nthe challenges that are unique to stream processing.\nOn the Greatly Exaggerated Limitations of\nStreaming\nOn that note, let’s next talk a bit about what streaming systems can and can’t\ndo, with an emphasis on can.\n One of the biggest things I want to get across in\nthis chapter is just how capable a well-designed streaming system can be.\nStreaming systems have historically been relegated to a somewhat niche\nmarket of providing low-latency, inaccurate, or speculative results, often in\nconjunction with a more capable batch system \nto provide eventually correct\nresults; in\n other words, the \nLambda Architecture\n.\nFor those of you not already familiar with the Lambda Architecture, the basic\nidea is that you run a streaming system alongside a batch system, both\nperforming essentially the same calculation. \nThe streaming system gives you\nlow-latency, inaccurate results (either because of the use of an approximation\nalgorithm, or because the streaming system itself does not provide\ncorrectness), and some time later a batch system rolls along and provides you\nwith correct output. Originally proposed by Twitter’s Nathan Marz (creator\nof \nStorm\n), it ended up being quite successful because it was, in fact, a\n2\nfantastic idea for the time; streaming engines were a bit of a letdown in the\ncorrectness department, and batch engines were as inherently unwieldy as\nyou’d expect, so Lambda gave you a way to have your proverbial cake and\neat it too. Unfortunately, maintaining a Lambda system is a hassle: you need\nto build, provision, and maintain two independent versions of your pipeline\nand then also somehow merge the results from the two pipelines at the end.\nAs someone who spent years working on a strongly consistent streaming\nengine, I also found the entire principle of the Lambda Architecture a bit\nunsavory. Unsurprisingly, I was a huge fan of Jay Kreps’ \n“Questioning the\nLambda Architecture”\n post when it came out. Here was one of the first highly\nvisible statements against the necessity of dual-mode execution.\n Delightful.\nKreps addressed the issue of repeatability in the context of using a replayable\nsystem like Kafka as the streaming interconnect, and went so far as to\npropose the Kappa Architecture, which basically means running a single\npipeline using a well-designed system that’s appropriately built for the job at\nhand. \nI’m not convinced that notion requires its own Greek letter name, but I\nfully support the idea in principle.\nQuite honestly,\n I’d take things a step further. I would argue that well-\ndesigned streaming systems actually provide a strict superset of batch\nfunctionality. Modulo perhaps an efficiency delta, there should be no need for\nbatch systems as they exist today. And kudos to the \nApache Flink\n folks for\ntaking this idea to heart and building a system that’s all-streaming-all-the-\ntime under the covers, even in “batch” mode; I love it.\nBATCH AND STREAMING EFFICIENCY DIFFERENCES\nOne which I propose is not an inherent limitation of streaming systems,\nbut simply a consequence of design choices made in most streaming\nsystems thus far. The efficiency delta between batch and streaming is\nlargely the result of the increased bundling and more efficient shuffle\ntransports found in batch systems. Modern batch systems go to great\nlengths to implement sophisticated optimizations that allow for\nremarkable levels of throughput using surprisingly modest compute\nresources. There’s no reason the types of clever insights that make batch\nsystems the efficiency heavyweights they are today couldn’t be\nincorporated into a system designed for unbounded data, providing users\nflexible choice between what we typically consider to be high-latency,\nhigher-efficiency “batch” processing and low-latency, lower-efficiency\n“streaming” processing. This is effectively what we’ve done at Google\nwith Cloud Dataflow by providing both batch and streaming runners\nunder the same unified model. In our case, we use separate runners\nbecause we happen to have two independently designed systems\noptimized for their specific use cases. Long term, from an engineering\nperspective, I’d love to see us merge the two into a single system that\nincorporates the best parts of both while still maintaining the flexibility of\nchoosing an appropriate efficiency level. But that’s not what we have\ntoday. And honestly, thanks to the unified Dataflow Model, it’s not even\nstrictly necessary; so it may well never happen.\nThe corollary of all this is that broad maturation of streaming systems\ncombined with robust frameworks for unbounded data processing will in time\nallow for the relegation of the Lambda Architecture to the antiquity of big\ndata history where it belongs. I believe the time has come to make this a\nreality. Because to do so—that is, to beat batch at its own game—you really\nonly need two things:\nCorrectness\nThis gets you parity with batch.\n At the core, correctness boils down to\nconsistent storage.\n Streaming systems need a method for checkpointing\npersistent state over time (something Kreps has talked about in his \n“Why\nlocal state is a fundamental primitive in stream processing”\n post), and it\nmust be well designed enough to remain consistent in light of machine\nfailures. When Spark Streaming first appeared in the public big data\nscene a few years ago, it was a beacon of consistency in an otherwise\ndark streaming world. Thankfully, things have improved substantially\nsince then, but it is remarkable how many streaming systems still try to\nget by without strong consistency.\nTo reiterate—because this point is important: strong consistency is\n3",6462
09-Event Time Versus Processing Time.pdf,09-Event Time Versus Processing Time,"required for exactly-once processing,\n which is required for correctness,\nwhich is a requirement for any system that’s going to have a chance at\nmeeting or exceeding the capabilities of batch systems.\n  Unless you just\ntruly don’t care about your results, I implore you to shun any streaming\nsystem that doesn’t provide strongly consistent state. Batch systems don’t\nrequire you to verify ahead of time if they are capable of producing\ncorrect answers; don’t waste your time on streaming systems that can’t\nmeet that same bar.\nIf you’re curious to learn more about what it takes to get strong\nconsistency in a streaming system,\n I recommend you check out the\nMillWheel\n, \nSpark Streaming\n, and \nFlink snapshotting\n papers.\n All three\nspend \na significant amount of time discussing consistency.\n Reuven will\ndive into consistency guarantees in \nChapter 5\n, and if you still find\nyourself craving more, there’s a large amount of quality information on\nthis topic in the literature and elsewhere.\nTools for reasoning about time\nThis gets you beyond batch.\n Good tools for reasoning \nabout time are\nessential for dealing with unbounded, unordered data of varying event-\ntime skew. An increasing number of modern datasets exhibit these\ncharacteristics, and existing batch systems (as well as many streaming\nsystems) lack the necessary tools to cope with the difficulties they impose\n(though this is now rapidly changing, even as I write this). We will spend\nthe bulk of this book explaining and focusing on various facets of this\npoint.\nTo begin with, we get a basic understanding of the important concept of\ntime domains, after which we take a deeper look at what I mean by\nunbounded, unordered data of varying event-time skew. We then spend\nthe rest of this chapter looking at common approaches to bounded and\nunbounded data processing, using both batch and streaming systems.\nEvent Time Versus Processing Time\n3\nTo speak cogently about unbounded data processing requires a clear\nunderstanding of the domains of time involved.\n Within any data processing\nsystem, there are typically two domains of time that we care about:\nEvent time\nThis is the time at which events actually occurred.\nProcessing time\nThis is the time at which events are observed in the system.\nNot all use \ncases care about event times (and if yours doesn’t, hooray! your\nlife is easier), but many do. Examples include characterizing user behavior\nover time, most billing applications, and many types of anomaly detection, to\nname a few.\nIn an ideal world, event time and processing time would always be equal,\nwith events being processed immediately as they occur. Reality is not so\nkind, however, and the skew between event time and processing time is not\nonly nonzero, but often a highly variable function of the characteristics of the\nunderlying input sources, execution engine, and hardware. Things that can\naffect the level of skew include the following:\nShared resource limitations, like network congestion, network\npartitions, or shared CPU in a nondedicated environment\nSoftware causes such as distributed system logic, contention, and so\non\nFeatures of the data themselves, like key distribution, variance in\nthroughput, or variance in disorder (i.e., a plane full of people taking\ntheir phones out of airplane mode after having used them offline for\nthe entire flight)\nAs a result, if you plot the progress of event time and processing time in any\nreal-world system, you typically end up with something that looks a bit like\nthe red line in \nFigure 1-1\n.\nFigure 1-1. \nTime-domain mapping. The x-axis represents event-time completeness in the\nsystem; that is, the time X in event time up to which all data with event times less than X\nhave been observed. The y-axis\n represents the progress of processing time; that is, normal\nclock time as observed by the data processing system as it executes.\nIn \nFigure 1-1\n, the black dashed line with slope of 1 represents the ideal,\nwhere processing time and event time are exactly equal; the red line\nrepresents reality. In this example, the system lags a bit at the beginning of\nprocessing time, veers closer toward the ideal in the middle, and then lags\n4\nagain a bit toward the end. At first glance, there are two types of skew visible\nin this diagram, each in different time domains:\nProcessing time\nThe vertical distance between the ideal and the red line is the lag in the\nprocessing-time domain. That distance tells you how much delay is\nobserved (in processing time) between when the events for a given time\noccurred and when they were processed. This is the perhaps the more\nnatural and intuitive of the two skews.\nEvent time\nThe horizontal distance between the ideal and the red line is the amount\nof event-time skew in the pipeline at that moment. It tells you how far\nbehind the ideal (in event time) the pipeline is currently.\nIn reality, processing-time lag and event-time skew at any given point in time\nare identical; they’re just two ways of looking at the same thing.\n The\nimportant takeaway regarding lag/skew is this: Because the overall mapping\nbetween event time and processing time is not static (i.e., the lag/skew can\nvary arbitrarily over time), this means that you cannot analyze your data\nsolely within the context of when they are observed by your pipeline if you\ncare about their event times (i.e., when the events actually occurred).\nUnfortunately, this is the way many systems designed for unbounded data\nhave historically operated. To cope with the infinite nature of unbounded\ndatasets, these systems typically provide some notion of windowing the\nincoming data.\n We discuss windowing in great depth a bit later, but it\nessentially means chopping up a dataset into finite pieces along temporal\nboundaries. If you care about correctness and are interested in analyzing your\ndata in the context of their event times, you cannot define those temporal\nboundaries using processing time (i.e., processing-time windowing), as many\nsystems do; with no consistent correlation between processing time and event\ntime, some of your event-time data are going to end up in the wrong\nprocessing-time windows (due to the inherent lag in distributed systems, the\nonline/offline nature of many types of input sources, etc.), throwing\ncorrectness out the window, as it were. We look at this problem in more\n5",6465
10-Unbounded Data Streaming.pdf,10-Unbounded Data Streaming,"detail in a number of examples in the sections that follow, as well as the\nremainder of the book.\nUnfortunately, the picture isn’t exactly rosy when windowing by event time,\neither. In the context of unbounded data, disorder and variable skew induce a\ncompleteness problem for event-time windows: lacking a predictable\nmapping between processing time and event time, how can you determine\nwhen you’ve observed all of the data for a given event time \nX\n? For many\nreal-world data sources, you simply can’t. But the vast majority of data\nprocessing systems in use today rely on some notion of completeness, which\nputs them at a severe disadvantage when applied to unbounded datasets.\nI propose that instead of attempting to groom unbounded data into finite\nbatches of information that eventually become complete, we should be\ndesigning tools that allow us to live in the world of uncertainty imposed by\nthese complex datasets. New data will arrive, old data might be retracted or\nupdated, and any system we build should be able to cope with these facts on\nits own, with notions of completeness being a convenient optimization for\nspecific and appropriate use cases rather than a semantic necessity across all\nof them.\nBefore getting into specifics about what such an approach might look like,\nlet’s finish up one more useful piece of background: common data processing\npatterns.\nData Processing Patterns\nAt this point, we have enough background established that we can begin\nlooking at the core types of usage patterns common across bounded and\nunbounded data processing today.\n We look at both types of processing and,\nwhere relevant, within the context of the two main types of engines we care\nabout (batch and streaming, where in this context, I’m essentially lumping\nmicrobatch in with streaming because the differences between the two aren’t\nterribly important at this level).\nBounded Data\nProcessing bounded data is conceptually quite straightforward, and likely\nfamiliar to everyone. \nIn \nFigure 1-2\n, we start out on the left with a dataset full\nof entropy.\n We run it through some data processing engine (typically batch,\nthough a well-designed streaming engine would work just as well), such as\nMapReduce\n, and on the right side end up with a new structured dataset with\ngreater inherent value.\nFigure 1-2. \nBounded data processing with a classic batch engine. A finite pool of\nunstructured data on the left is run through a data processing engine, resulting in\ncorresponding structured data on the right.\nThough there are of course infinite variations on what you can actually\ncalculate as part of this scheme, the overall model is quite simple. Much more\ninteresting is the task of processing an unbounded dataset. Let’s now look at\nthe various ways unbounded data are typically processed, beginning with the\napproaches used with traditional batch engines and then ending up with the\napproaches you can take with a system designed for unbounded data, such as\nmost streaming or microbatch engines.\nUnbounded Data: Batch\nBatch engines, though not explicitly designed with unbounded data in mind,\nhave nevertheless been used to process unbounded datasets since batch\nsystems were first conceived.\n As you might expect, such approaches revolve\naround slicing up the unbounded data into a collection of bounded datasets\nappropriate for batch \nprocessing\n.\nFixed windows\nThe most common way to process an unbounded dataset using repeated runs\nof a batch engine is by windowing \nthe input data into fixed-size \nwindows and\nthen processing each of those windows as a separate, bounded data source\n(sometimes also called \ntumbling windows\n), as in \nFigure 1-3\n. Particularly for\ninput sources like logs, for which events can be written into directory and file\nhierarchies whose names encode the window they correspond to, this sort of\nthing appears quite straightforward at first blush because you’ve essentially\nperformed the time-based shuffle to get data into the appropriate event-time\nwindows ahead of time.\nIn reality, however, most systems still have a completeness problem to deal\nwith (What if some of your events are delayed en route to the logs due to a\nnetwork partition? What if your events are collected globally and must be\ntransferred to a common location before processing? What if your events\ncome from mobile devices?), which means some sort of mitigation might be\nnecessary (e.g., delaying processing until you’re sure all events have been\ncollected or reprocessing the entire batch for a given window whenever data\narrive late).\nFigure 1-3. \nUnbounded data processing via ad hoc fixed windows with a classic batch\nengine. An unbounded dataset is collected up front into finite, fixed-size windows of\nbounded data that are then processed via successive runs a of classic batch engine.\nSessions\nThis approach breaks down even more when you try to use a batch engine to\nprocess unbounded data into more sophisticated windowing strategies, like\nsessions. \nSessions are typically\n defined as periods of activity (e.g., for a\nspecific user) terminated by a gap of inactivity. When calculating sessions\nusing a typical batch engine, you often end up with sessions that are split\nacross batches, as indicated by the red marks in \nFigure 1-4\n. We can reduce\nthe number of splits by increasing batch sizes, but at the cost of increased\nlatency. Another option is to add additional logic to stitch up sessions from\nprevious runs, but at the cost of further complexity.\nFigure 1-4. \nUnbounded data processing into sessions via ad hoc fixed windows with a\nclassic batch engine. An unbounded dataset is collected up front into finite, fixed-size\nwindows of bounded data that are then subdivided into dynamic session windows via\nsuccessive runs a of classic batch engine.\nEither way, using a classic batch engine to calculate sessions is less than\nideal. A nicer way would be to build up sessions in a streaming manner,\nwhich we look at later on.\nUnbounded Data: Streaming\nContrary to the ad hoc nature of most batch-based unbounded data processing\napproaches, streaming systems are built for \nunbounded data.\n As we talked\nabout earlier, for many real-world, distributed input sources, you not only\nfind yourself dealing with unbounded data, but also data such as the\nfollowing:\nHighly unordered with respect to event times, meaning that you need\nsome sort of time-based shuffle in your pipeline if you want to\nanalyze the data in the context in which they occurred.\nOf varying event-time skew, meaning that you can’t just assume\nyou’ll always see most of the data for a given event time \nX\n within\nsome constant epsilon of time \nY\n.\nThere are a handful of approaches that you can take when dealing with data\nthat have these characteristics. I generally categorize these approaches into\nfour groups: time-agnostic, approximation, windowing by processing time,\nand windowing by event time.\nLet’s now spend a little bit of time looking at each of these approaches.\nTime-agnostic\nTime-agnostic processing is used for cases in which time is essentially\nirrelevant; that is, all relevant logic is data driven.\n Because everything about\nsuch use cases is dictated by the arrival of more data, there’s really nothing\nspecial a streaming engine has to support other than basic data delivery. As a\nresult, essentially all streaming systems in existence support time-agnostic\nuse cases out of the box (modulo system-to-system variances in consistency\nguarantees, of course, if you care about correctness). Batch systems are also\nwell suited for time-agnostic processing of unbounded data sources by simply\nchopping the unbounded source into an arbitrary sequence of bounded\ndatasets and processing those datasets independently. We look at a couple of\nconcrete examples in this section, but given the straightforwardness of\nhandling time-agnostic processing (from a temporal perspective at least), we\nwon’t spend much more time on it beyond that.\nFiltering\nA very basic form of time-agnostic processing is filtering, an example of\nwhich is rendered in \nFigure 1-5\n. \nImagine that you’re processing web traffic\nlogs and you want to filter out all traffic that didn’t originate from a specific\ndomain. You would look at each record as it arrived, see if it belonged to the\ndomain of interest, and drop it if not. Because this sort of thing depends only\non a single element at any time, the fact that the data source is unbounded,\nunordered, and of varying event-time skew is irrelevant.\nFigure 1-5. \nFiltering unbounded data. A collection of data (flowing left to right) of varying\ntypes is filtered into a homogeneous collection containing a single type.\nInner joins\nAnother time-agnostic example is an inner join, diagrammed in \nFigure 1-6\n.\nWhen joining two unbounded data sources, if you care only about the results\nof a join when an element from both sources arrive, there’s no temporal\nelement to the logic.\n Upon seeing a value from one source, you can simply\nbuffer it up in persistent state; only after the second value from the other\nsource arrives do you need to emit the joined record. (In truth, you’d likely\nwant some sort of garbage collection policy for unemitted partial joins, which\nwould likely be time based. But for a use case with little or no uncompleted\njoins, such a thing might not be an issue.)\nFigure 1-6. \nPerforming an inner join on unbounded data. Joins are produced when\nmatching elements from both sources are observed.\nSwitching semantics to some sort of outer join introduces the data\ncompleteness problem we’ve talked about: after you’ve seen one side of the\njoin, how do you know whether the other side is ever going to arrive or not?\nTruth be told, you don’t, so you need to introduce some notion of a timeout,\nwhich introduces an element of time. That element of time is essentially a\nform of windowing, which we’ll look at more closely in a moment.\nApproximation algorithms\nThe second major \ncategory of approaches is approximation algorithms, \nsuch\nas \napproximate Top-N\n, \nstreaming k-means\n, and so on. They take an\nunbounded source of input and provide output data that, if you squint at them,\nlook more or less like what you were hoping to get, as in \nFigure 1-7\n. The\nupside of approximation algorithms is that, by design, they are low overhead\nand designed for unbounded data. The downsides are that a limited set of\nthem exist, the algorithms themselves are often complicated (which makes it\ndifficult to conjure up new ones), and their approximate nature limits their\nutility.\nFigure 1-7. \nComputing approximations on unbounded data. Data are run through a\ncomplex algorithm, yielding output data that look more or less like the desired result on the\nother side.\nIt’s worth noting that these algorithms typically do have some element of\ntime in their design (e.g., some sort of built-in decay). And because they\nprocess elements as they arrive, that time element is usually processing-time\nbased. This is particularly important for algorithms that provide some sort of\nprovable error bounds on their approximations. If those error bounds are\npredicated on data arriving in order, they mean essentially nothing when you\nfeed the algorithm unordered data with varying event-time skew. Something\nto keep in mind.\nApproximation algorithms themselves are a fascinating subject, but as they\nare essentially another example of time-agnostic processing (modulo the\ntemporal features of the algorithms themselves), they’re quite straightforward\nto use and thus not worth further attention, given our current focus.\nWindowing\nThe remaining two approaches for unbounded data processing are both\nvariations of windowing.\n Before diving into the differences between them, I\nshould make it clear exactly what I mean by windowing, insomuch as we\ntouched on it only briefly in the previous section. Windowing is simply the\nnotion of taking a data source (either unbounded or bounded), and chopping\nit up along temporal boundaries into finite chunks for processing. \nFigure 1-8\nshows three different windowing patterns.\nFigure 1-8. \nWindowing strategies. Each example is shown for three different keys,\nhighlighting the difference between aligned windows (which apply across all the data) and\nunaligned windows (which apply across a subset of the data).\nLet’s take a closer look at each strategy:\nFixed windows (aka tumbling windows)\nWe discussed fixed windows earlier. \nFixed windows \nslice time into\nsegments with a fixed-size temporal length. Typically (as shown in\nFigure 1-9\n), the segments for fixed windows are applied uniformly across\nthe entire dataset, which is an example of \naligned\n windows. \nIn some\ncases, it’s desirable to phase-shift the windows for different subsets of the\ndata (e.g., per key) to spread window completion load more evenly over\ntime, which instead is an example of \nunaligned\n windows because they\nvary across the data.\nSliding windows (aka hopping windows)\n6\nA generalization of fixed windows, sliding windows are defined by a\nfixed length and a fixed period.\n If the period is less than the length, the\nwindows overlap. If the period equals the length, you have fixed\nwindows. And if the period is greater than the length, you have a weird\nsort of sampling window that looks only at subsets of the data over time.\nAs with fixed windows, sliding windows are typically aligned, though\nthey can be unaligned as a performance optimization in certain use cases.\nNote that the sliding windows in \nFigure 1-8\n are drawn as they are to give\na sense of sliding motion; in reality, all five windows would apply across\nthe entire dataset.\nSessions\nAn example of dynamic windows, sessions are composed of sequences of\nevents terminated by a gap of inactivity greater than some timeout.\nSessions are commonly used for analyzing user behavior over time, by\ngrouping together a series of temporally related events (e.g., a sequence\nof videos viewed in one sitting). Sessions are interesting because their\nlengths cannot be defined a priori; they are dependent upon the actual\ndata involved. They’re also the canonical example of unaligned windows\nbecause sessions are practically never identical across different subsets of\ndata (e.g., different users).\nThe two domains of time we discussed earlier (processing time and event\ntime) are essentially the two we care about.\n Windowing makes sense in both\ndomains, so let’s look at each in detail and see how they differ. Because\nprocessing-time windowing has historically been more common, we’ll start\nthere.\nWindowing by processing time\nWhen windowing by processing time, the system essentially buffers up\nincoming data into windows until some amount of processing time has\npassed.\n For example, in the case of five-minute fixed windows, the system\nwould buffer data for five minutes of processing time, after which it would\ntreat all of the data it had observed in those five minutes as a window and\nsend them downstream for processing.\n7\nFigure 1-9. \nWindowing into fixed windows by processing time. Data are collected into\nwindows based on the order they arrive in the pipeline.\nThere are a few nice properties of processing-time windowing:\nIt’s simple. The implementation is extremely straightforward\nbecause you never worry about shuffling data within time. You just\nbuffer things as they arrive and send them downstream when the\nwindow closes.\nJudging window completeness is straightforward. Because the\nsystem has perfect knowledge of whether all inputs for a window\nhave been seen, it can make perfect decisions about whether a given\nwindow is complete. This means there is no need to be able to deal\nwith “late” data in any way when windowing by processing time.\nIf you’re wanting to infer information about the source \nas it is\nobserved\n, processing-time windowing is exactly what you want.\nMany monitoring scenarios fall into this category. Imagine tracking\nthe number of requests per second sent to a global-scale web service.\nCalculating a rate of these requests for the purpose of detecting\noutages is a perfect use of processing-time windowing.\nGood points aside, there is one very big downside to processing-time\nwindowing: \nif the data in question have event times associated with them,\nthose data must arrive in event-time order if the processing-time windows are\nto reflect the reality of when those events actually happened.\n Unfortunately,\nevent-time ordered data are uncommon in many real-world, distributed input\nsources.\nAs a simple example, imagine any mobile app that gathers usage statistics for\nlater processing. For cases in which a given mobile device goes offline for\nany amount of time (brief loss of connectivity, airplane mode while flying\nacross the country, etc.), the data recorded during that period won’t be\nuploaded until the device comes online again. This means that data might\narrive with an event-time skew of minutes, hours, days, weeks, or more. It’s\nessentially impossible to draw any sort of useful inferences from such a\ndataset when windowed by processing time.\nAs another example, many distributed input sources might \nseem\n to provide\nevent-time ordered (or very nearly so) data when the overall system is\nhealthy. Unfortunately, the fact that event-time skew is low for the input\nsource when healthy does not mean it will always stay that way. Consider a\nglobal service that processes data collected on multiple continents. If network\nissues across a bandwidth-constrained transcontinental line (which, sadly, are\nsurprisingly common) further decrease bandwidth and/or increase latency,\nsuddenly a portion of your input data might begin arriving with much greater\nskew than before. If you are windowing those data by processing time, your\nwindows are no longer representative of the data that actually occurred within\nthem; instead, they represent the windows of time as the events arrived at the\nprocessing pipeline, which is some arbitrary mix of old and current data.\nWhat we really want in both of those cases is to window data by their event\ntimes in a way that is robust to the order of arrival of events. What we really\nwant is event-time windowing.\nWindowing by event time\nEvent-time windowing is what you use when you need to observe a data\nsource in finite chunks that reflect the times at which those events actually\nhappened.\n It’s the gold standard of windowing. Prior to 2016, most data\nprocessing systems in use lacked native support for it (though any system\nwith a decent consistency model, like Hadoop or Spark Streaming 1.x, could\nact as a reasonable substrate for building such a windowing system). I’m\nhappy to say that the world of today looks very different, with multiple\nsystems, from Flink to Spark to Storm to Apex, natively supporting event-\ntime windowing of some sort.\nFigure 1-10\n shows an example of windowing an unbounded source into one-\nhour fixed windows.\nFigure 1-10. \nWindowing into fixed windows by event time. Data are collected into windows\nbased on the times at which they occurred. The black arrows call out example data that\narrived in processing-time windows that differed from the event-time windows to which\nthey belonged.\nThe black arrows in \nFigure 1-10\n call out two particularly interesting pieces of\ndata. Each arrived in processing-time windows that did not match the event-\ntime windows to which each bit of data belonged. As such, if these data had\nbeen windowed into processing-time windows for a use case that cared about\nevent times, the calculated results would have been incorrect. As you would\nexpect, event-time correctness is one nice thing about using event-time\nwindows.\nAnother nice thing about event-time windowing over an unbounded data\nsource is that you can create dynamically sized windows, such as sessions,\nwithout the arbitrary splits observed when generating sessions over fixed\nwindows (as we saw previously in the sessions example from \n“Unbounded\nData: Streaming”\n), as demonstrated in \nFigure 1-11\n.\nFigure 1-11. \nWindowing into session windows by event time. Data are collected into\nsession windows capturing bursts of activity based on the times that the corresponding\nevents occurred. The black arrows again call out the temporal shuffle necessary to put the\ndata into their correct event-time locations.\nOf course, powerful semantics rarely come for free, and event-time windows\nare no exception. Event-time windows have two notable drawbacks due to\nthe fact that windows \nmust often live longer (in processing time) than the\nactual length of the window itself:\nBuffering\nDue to extended window lifetimes, more buffering of data is required.\nThankfully,\n persistent storage is generally the cheapest of the resource\ntypes most data processing systems depend on (the others being primarily\nCPU, network bandwidth, and RAM). As such, this problem is typically\nmuch less of a concern than you might think when using any well-\ndesigned data processing system with strongly consistent persistent state\nand a decent in-memory caching layer. Also, many useful aggregations\ndo not require the entire input set to be buffered (e.g., sum or average),\nbut instead can be performed incrementally, with a much smaller,\nintermediate aggregate stored in persistent state.\nCompleteness\nGiven that we often have no good way of knowing when we’ve seen all\nof the data for a given window, how do we know when the results for the\nwindow are ready to materialize?\n In truth, we simply don’t. For many\ntypes of inputs, the system can give a reasonably accurate heuristic\nestimate of window completion via something like the watermarks found",21964
11-Summary.pdf,11-Summary,"in MillWheel, Cloud Dataflow, and Flink (which we talk about more in\nChapters \n3\n and \n4\n). But for cases in which absolute correctness is\nparamount (again, think billing), the only real option is to provide a way\nfor the pipeline builder to express when they want results for windows to\nbe materialized and how those results should be refined over time.\nDealing with window completeness (or lack thereof) is a fascinating topic\nbut one perhaps best explored in the context of concrete examples, which\nwe look at next.\nSummary\nWhew! That was a lot of information. If you’ve made it this far, you are to be\ncommended! But we are only just getting started. Before forging ahead to\nlooking in detail at the Beam Model approach, let’s briefly step back and\nrecap what we’ve learned so far. In this chapter, we’ve done the following:\nClarified terminology, focusing the definition of “streaming” to refer\nto systems built with unbounded data in mind, while using more\ndescriptive terms like approximate/speculative results for distinct\nconcepts often categorized under the “streaming” umbrella.\nAdditionally, we highlighted two important dimensions of large-\nscale datasets: cardinality (i.e., bounded versus unbounded) and\nencoding (i.e., table versus stream), the latter of which will consume\nmuch of the second half of the book.\nAssessed the relative capabilities of well-designed batch and\nstreaming systems, positing streaming is in fact a strict superset of\nbatch, and that notions like the Lambda Architecture, which \nare\npredicated on streaming being inferior to batch, are destined for\nretirement as streaming systems mature.\nProposed two high-level concepts necessary for streaming systems\nto both catch up to and ultimately surpass batch, those being\ncorrectness and tools for reasoning about time, respectively.\nEstablished the important differences between event time and\nprocessing time, characterized the difficulties those differences\nimpose when analyzing data in the context of when they occurred,\nand proposed a shift in approach away from notions of completeness\nand toward simply adapting to changes in data over time.\nLooked at the major data processing approaches in common use\ntoday for bounded and unbounded data, via both batch and streaming\nengines, roughly categorizing the unbounded approaches into: time-\nagnostic, approximation, windowing by processing time, and\nwindowing by event time.\nNext up, we dive into the details of the Beam Model, taking a conceptual\nlook at how we’ve broken up the notion of data processing across four related\naxes: what, where, when, and how. We also take a detailed look at processing\na simple, concrete example dataset across multiple scenarios, highlighting the\nplurality of use cases enabled by the Beam Model, with some concrete APIs\nto ground us in reality. These examples will help drive home the notions of\nevent time and processing time introduced in this chapter while additionally\nexploring new concepts such as watermarks.\n For completeness, it’s perhaps worth calling out that this definition includes\nboth true streaming as well as microbatch implementations. For those of you\nwho aren’t familiar with microbatch systems, they are streaming systems that\nuse repeated executions of a batch processing engine to process unbounded\ndata. Spark Streaming is the canonical example in the industry.\n Readers familiar with my original \n“Streaming 101”\n article might recall that\nI rather emphatically encouraged the abandonment of the term “stream” when\nreferring to datasets. That never caught on, which I initially thought was due\nto its catchiness and pervasive existing usage. In retrospect, however, I think\nI was simply wrong. There actually is great value in distinguishing between\nthe two different types of dataset constitutions: tables and streams. Indeed,\nmost of the second half of this book is dedicated to understanding the\nrelationship between those two.\n If you’re unfamiliar with what I mean when I say \nexactly-once\n, it’s referring\n1\n2\n3\nto a specific type of consistency guarantee that certain data processing\nframeworks provide. Consistency guarantees are typically bucketed into three\nmain classes: at-most-once processing, at-least-once processing, and exactly-\nonce processing. Note that the names in use here refer to the effective\nsemantics as observed within the outputs generated by the pipeline, not the\nactual number of times a pipeline might process (or attempt to process) any\ngiven record. For this reason, the term \neffectively-once\n is sometimes used\ninstead of exactly-once, since it’s more representative of the underlying\nnature of things. Reuven covers these concepts in much more detail in\nChapter 5\n.\n Since the original publication of “Streaming 101,” numerous individuals\nhave pointed out to me that it would have been more intuitive to place\nprocessing time on the x-axis and event time on the y-axis. I do agree that\nswapping the two axes would initially feel more natural, as event time seems\nlike the dependent variable to processing time’s independent variable.\nHowever, because both variables are monotonic and intimately related,\nthey’re effectively interdependent variables. So I think from a technical\nperspective you just have to pick an axis and stick with it. Math is confusing\n(especially outside of North America, where it suddenly becomes plural and\ngangs up on you).\n This result really shouldn’t be surprising (but was for me, hence why I’m\npointing it out), because we’re effectively creating a right triangle with the\nideal line when measuring the two types of skew/lag. Maths are cool.\n We look at aligned fixed windows in detail in \nChapter 2\n, and unaligned\nfixed windows in \nChapter 4\n.\n If you poke around enough in the academic literature or SQL-based\nstreaming systems, you’ll also come across a third windowing time domain:\ntuple-based windowing\n (i.e., windows whose sizes are counted in numbers of\nelements). However, tuple-based windowing is essentially a form of\nprocessing-time windowing in which elements are assigned monotonically\nincreasing timestamps as they arrive at the system. As such, we won’t discuss\ntuple-based windowing in detail any further.\n4\n5\n6\n7",6316
12-Roadmap.pdf,12-Roadmap,"Chapter 2. \nThe \nWhat\n, \nWhere\n,\nWhen\n, and \nHow\n \nof Data\nProcessing\nOkay party people, it’s time to get concrete!\nChapter 1\n focused on three main areas: \nterminology\n, defining precisely what I\nmean when\n I use overloaded terms like “streaming”; \nbatch versus streaming\n,\ncomparing the theoretical capabilities of the two types of systems, and\npostulating that only two things are necessary to take streaming systems\nbeyond their batch counterparts: correctness and tools for reasoning about\ntime; and \ndata processing patterns\n, looking at the conceptual approaches\ntaken with both batch and streaming systems when processing bounded and\nunbounded data.\nIn this chapter, we’re now going to focus further on the data processing\npatterns from \nChapter 1\n, but in more detail, and within the context of\nconcrete examples. By the time we’re finished, we’ll have covered what I\nconsider to be the core set of principles and concepts required for robust out-\nof-order data processing; these are the tools for reasoning about time that\ntruly get you beyond classic batch processing.\nTo give you a sense of what things look like in action, I use snippets of\nApache Beam\n code, coupled with time-lapse diagrams\n to provide a visual\nrepresentation of the concepts. \nApache Beam is a unified programming\nmodel and portability layer for batch and stream processing, with a set of\nconcrete SDKs in various languages (e.g., Java and Python). Pipelines written\nwith Apache Beam can then be portably run on any of the supported\nexecution engines (Apache Apex, Apache Flink, Apache Spark, Cloud\nDataflow, etc.).\nI use Apache Beam here for examples not because this is a Beam book (it’s\n1\nnot), but because it most completely embodies the concepts described in this\nbook.\n Back when \n“Streaming 102”\n was originally written (back when it was\nstill the Dataflow Model from Google Cloud Dataflow and not the Beam\nModel from Apache Beam), it was literally the only system in existence that\nprovided the amount of expressiveness necessary for all the examples we’ll\ncover here. \nA year and a half later, I’m happy to say much has changed, and\nmost of the major systems out there have moved or are moving toward\nsupporting a model that looks a lot like the one described in this book. So rest\nassured that the concepts we cover here, though informed through the Beam\nlens, as it were, will apply equally across most other systems you’ll come\nacross.\nRoadmap\nTo help set the stage for this chapter, I want to lay out the five main concepts\nthat will underpin all of the discussions therein, and really, for most of the\nrest of \nPart I\n. We’ve already covered two of them.\nIn \nChapter 1\n, I first established the critical distinction between event time (the\ntime that events happen) and processing time (the time they are observed\nduring processing). This provides the foundation for one of the main theses\nput forth in this book: if you care about both correctness and the context\nwithin which events actually occurred, you must analyze data relative to their\ninherent event times, not the processing time at which they are encountered\nduring the analysis itself.\nI then introduced the concept of \nwindowing\n (i.e., partitioning a dataset along\ntemporal boundaries), which is a common approach used to cope with the\nfact that unbounded data sources technically might never end. Some simpler\nexamples of windowing strategies are \nfixed\n and \nsliding\n windows, but more\nsophisticated types of windowing, such as \nsessions\n (in which the windows\nare defined by features of the data themselves; for example, capturing a\nsession of activity per user followed by a gap of inactivity) also see broad\nusage.\nIn addition to these two concepts, we’re now going to look closely at three\nmore:\nTriggers\nA trigger is a mechanism for declaring when the output for a window\nshould be materialized relative to some external signal. \nTriggers provide\nflexibility in choosing when outputs should be emitted. In some sense,\nyou can think of them as a flow control mechanism for dictating when\nresults should be materialized. Another way of looking at it is that\ntriggers are like the shutter-release on a camera, allowing you to declare\nwhen to take a snapshots in time of the results being computed.\nTriggers also make it possible to observe the output for a window\nmultiple times as it evolves. This in turn opens up the door to refining\nresults over time, which allows for providing speculative results as data\narrive, as well as dealing with changes in upstream data (revisions) over\ntime or data that arrive late (e.g., mobile scenarios, in which someone’s\nphone records various actions and their event times while the person is\noffline and then proceeds to upload those events for processing upon\nregaining connectivity).\nWatermarks\nA watermark is a notion of input completeness with \nrespect to event\ntimes. A watermark with\n value of time \nX\n makes the statement: “all input\ndata with event times less than \nX\n have been observed.” As such,\nwatermarks act as a metric of progress when observing an unbounded\ndata source with no known end. We touch upon the basics of watermarks\nin this chapter, and then Slava goes super deep on the subject in\nChapter 3\n.\nAccumulation\nAn accumulation mode specifies the relationship between multiple results\nthat are observed for the \nsame window.\n Those results might be\ncompletely disjointed; that is, representing independent deltas over time,\nor there might be overlap between them. Different accumulation modes\nhave different semantics and costs associated with them and thus find\napplicability across a variety of use cases.\nAlso, because I think it makes it easier to understand the relationships\nbetween all of these concepts, we revisit the old and explore the new within\nthe structure of answering four questions, all of which I propose are critical to\nevery unbounded data processing problem:\nWhat\n results are calculated? This question is answered by the types\nof transformations within the pipeline. This includes things like\ncomputing sums, building histograms, training machine learning\nmodels, and so on. It’s also essentially the question answered by\nclassic batch processing\nWhere\n in event time are results calculated? This question is\nanswered by the use of event-time windowing within the pipeline.\nThis includes the common examples of windowing from \nChapter 1\n(fixed, sliding, and sessions); use cases that seem to have no notion\nof windowing (e.g., time-agnostic processing; classic batch\nprocessing also generally falls into this category); and other, more\ncomplex types of windowing, such as time-limited auctions. Also\nnote that it can include processing-time windowing, as well, if you\nassign ingress times as event times for records as they arrive at the\nsystem.\nWhen\n in processing time are results materialized? This question is\nanswered by the use of triggers and (optionally) watermarks. There\nare infinite variations on this theme, but the most common patterns\nare those involving repeated updates (i.e., materialized view\nsemantics), those that utilize a watermark to provide a single output\nper window only after the corresponding input is believed to be\ncomplete (i.e., classic batch processing semantics applied on a per-\nwindow basis), or some combination of the two.\nHow\n do refinements of results relate? This question is answered by\nthe type of accumulation used: discarding (in which results are all\nindependent and distinct), accumulating (in which later results build\nupon prior ones), or accumulating and retracting (in which both the",7757
13-Batch Foundations What and Where.pdf,13-Batch Foundations What and Where,,0
14-What Transformations.pdf,14-What Transformations,"accumulating value plus a retraction for the previously triggered\nvalue(s) are emitted).\nWe look at each of these questions in much more detail throughout the rest of\nthe book. And, yes, I’m going to run this color scheme thing into the ground\nin an attempt to make it abundantly clear which concepts relate to which\nquestion in the \nWhat\n/\nWhere\n/\nWhen\n/\nHow\n idiom. You’re welcome <winky-\nsmiley/>.\nBatch Foundations: \nWhat\n and \nWhere\nOkay, let’s get this party started. First stop: batch processing.\nWhat\n: Transformations\nThe transformations applied in classic batch processing answer the question:\n“\nWhat\n results are calculated?” Even\n though\n you are likely already familiar\nwith classic batch processing, we’re going to start there anyway because it’s\nthe foundation on top of which we add all of the other concepts.\nIn the rest of this chapter (and indeed, through much of the book), we look at\na single example: computing keyed integer sums over a simple dataset\nconsisting of nine values. Let’s imagine that we’ve written a team-based\nmobile game and we want to build a pipeline that calculates team scores by\nsumming up the individual scores reported by users’ phones. If we were to\ncapture our nine example scores in a SQL table named “UserScores,” it might\nlook something like this:\n------------------------------------------------\n| Name  | Team  | Score | EventTime | ProcTime |\n------------------------------------------------\n| Julie | TeamX |     5 |  12:00:26 | 12:05:19 |\n| Frank | TeamX |     9 |  12:01:26 | 12:08:19 |\n| Ed    | TeamX |     7 |  12:02:26 | 12:05:39 |\n| Julie | TeamX |     8 |  12:03:06 | 12:07:06 |\n| Amy   | TeamX |     3 |  12:03:39 | 12:06:13 |\n2\n| Fred  | TeamX |     4 |  12:04:19 | 12:06:39 |\n| Naomi | TeamX |     3 |  12:06:39 | 12:07:19 |\n| Becky | TeamX |     8 |  12:07:26 | 12:08:39 |\n| Naomi | TeamX |     1 |  12:07:46 | 12:09:00 |\n------------------------------------------------\nNote that all the scores in this example are from users on the same team; this\nis to keep the example simple, given that we have a limited number of\ndimensions in our diagrams that follow. And because we’re grouping by\nteam, we really just care about the last three columns:\nScore\nThe individual user score associated with this event\nEventTime\nThe event time for the score; \nthat is, the time at which the score occurred\nProcTime\nThe processing for the score;\n that is, the time at which the score was\nobserved by the pipeline\nFor each example pipeline, we’ll look at a time-lapse diagram that highlights\nhow the data evolves over time. Those diagrams plot our nine scores in the\ntwo dimensions of time we care about: event time in the x-axis, and\nprocessing time in the y-axis. \nFigure 2-1\n illustrates what a static plot of the\ninput data looks like.\nFigure 2-1. \nNine input records, plotted in both event time and processing time\nSubsequent time-lapse diagrams are either animations (Safari) or a sequence\nof frames (print and all other digital formats), allowing you to see how the\ndata are processed over time (more on this shortly after we get to the first\ntime-lapse diagram).\nPreceding each example is a short snippet of Apache Beam Java SDK\npseudocode to make the definition of the pipeline more concrete.\n It is\npseudocode in the sense that I sometime bend the rules to make the examples\nclearer, elide details (like the use of concrete I/O sources), or simplify names\n(the trigger names in Beam Java 2.x and earlier are painfully verbose; I use\nsimpler names for clarity). Beyond minor things like those, it’s otherwise\nreal-world Beam code (and real code is available on \nGitHub\n for all examples\nin this chapter).\nIf you’re already familiar with something like Spark or Flink, you should\nhave a relatively easy time understanding what the Beam code is doing. But\nto give you a crash course in things, there are two basic primitives in Beam:\nPCollections\nThese represent datasets (possibly massive ones) across which parallel\ntransformations can be performed (hence the “P” at the beginning of the\nname).\nPTransforms\nThese are applied to \nPCollections\n to create new \nPCollections\n.\nPTransforms\n may perform element-wise transformations, they may\ngroup/aggregate multiple elements together, or they may be a composite\ncombination of other \nPTransforms\n, as depicted in \nFigure 2-2\n.\nFigure 2-2. \nTypes of transformations\nFor the purposes of our examples, we typically assume that we start out with\na pre-loaded \nPCollection<KV<Team, Integer>>\n named “input” (that is, a\nPCollection\n composed of key/value pairs of \nTeams\n and \nIntegers\n, where\nthe \nTeams\n are just something like \nStrings\n representing team names, and the\nIntegers\n are scores from any individual on the corresponding team). In a\nreal-world pipeline, we would’ve acquired input by reading in a\nPCollection<String>\n of raw data (e.g., log records) from an I/O source and\nthen transforming it into a \nPCollection<KV<Team, Integer>>\n by parsing\nthe log records into appropriate key/value pairs. For the sake of clarity in this\nfirst example, I include pseudocode for all of those steps, but in subsequent\nexamples, I elide the I/O and parsing.\nThus, for a pipeline that simply reads in data from an I/O source, parses\nteam/score pairs, and calculates per-team sums of scores, we’d have\nsomething like that shown in \nExample 2-1\n.\nExample 2-1. \nSummation pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals =\n  input.apply(\nSum.integersPerKey()\n);\nKey/value data are read from an I/O source, with a \nTeam\n (e.g., \nString\n of the\nteam name) as the key and an \nInteger\n (e.g., individual team member scores)\nas the value. The values for each key are then summed together to generate\nper-key sums (e.g., total team score) in the output collection.\nFor all the examples to come, after seeing a code snippet describing the\npipeline that we’re analyzing, we’ll then look at a time-lapse diagram\nshowing the execution of that pipeline over our concrete dataset for a single\nkey. In a real pipeline, you can imagine that similar operations would be\nhappening in parallel across multiple machines, but for the sake of our\nexamples, it will be clearer to keep things simple.\nAs noted previously, Safari editions present the complete execution as an\nanimated movie, whereas print and all other digital formats use a static\nsequence of key frames that provide a sense of how the pipeline progresses\nover time. In both cases, we also provide a URL to a fully animated version\non \nwww.streamingbook.net\n.\nEach diagram plots the inputs and outputs across two dimensions: event time\n(on the x-axis) and processing time (on the y-axis). Thus, real time as\nobserved by the pipeline progresses from bottom to top, as indicated by the\nthick horizontal black line that ascends in the processing-time axis as time\nprogresses. Inputs are circles, with the number inside the circle representing\nthe value of that specific record. They start out light gray, and darken as the\npipeline observes them.\nAs the pipeline observes values, it accumulates them in its intermediate state\nand eventually\n materializes the aggregate results as output. State and output\nare represented by rectangles (gray for state, blue for output), with the\naggregate value near the top, and with the area covered by the rectangle\nrepresenting the portions of event time and processing time accumulated into\nthe result. For the pipeline in \nExample 2-1\n, it would look something like that\nshown in \nFigure 2-3\n when executed on a classic batch engine.\nFigure 2-3. \nClassic batch processing\nBecause this is a batch pipeline, it accumulates state until it’s seen all of the\ninputs (represented by the dashed green line at the top), at which point it\nproduces its single output of 48. In this example, we’re calculating a sum\nover all of event time because we haven’t applied any specific windowing\ntransformations; hence the rectangles for state and output cover the entirety\nof the x-axis. If we want to process an unbounded data source, however,\nclassic batch processing won’t be sufficient; we can’t wait for the input to\nend, because it effectively never will. One of the concepts we want is\nwindowing, which we introduced in \nChapter 1\n. Thus, within the context of\nour second question—“\nWhere\n in event time are results calculated?”—we’ll\nnow briefly revisit windowing.\nWhere\n: Windowing\nAs discussed in \nChapter 1\n, windowing is \nthe process of slicing up a data\n00:00 / 00:00",8761
15-Going Streaming When and How.pdf,15-Going Streaming When and How,"source along temporal boundaries.\n Common windowing strategies include\nfixed windows, sliding windows, and sessions windows, as demonstrated in\nFigure 2-4\n.\nFigure 2-4. \nExample windowing strategies. Each example is shown for three different keys,\nhighlighting the difference between aligned windows (which apply across all the data) and\nunaligned windows (which apply across a subset of the data).\nTo get a better sense of what windowing looks like in practice, let’s take our\ninteger summation pipeline and window it into fixed, two-minute windows.\nWith Beam, the change is a simple addition of a \nWindow.into\n transform,\nwhich you can see highlighted in \nExample 2-2\n.\nExample 2-2. \nWindowed summation code\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n)\n  .apply(\nSum.integersPerKey()\n);\nRecall that Beam provides a unified model that works in both batch and\nstreaming because semantically batch is really just a subset of streaming. \nAs\nsuch, let’s first execute this pipeline on a batch engine; the mechanics are\nmore straightforward, and it will give us something to directly compare\nagainst when we switch to a streaming engine. \nFigure 2-5\n presents the result.",1255
16-When The Wonderful Thing About Triggers Is Triggers Are Wonderful Things.pdf,16-When The Wonderful Thing About Triggers Is Triggers Are Wonderful Things,"Figure 2-5. \nWindowed summation on a batch engine\nAs before, inputs are accumulated in state until they are entirely consumed,\nafter which output is produced. In this case, however, instead of one output,\nwe get four: a single output, for each of the four relevant two-minute event-\ntime windows.\nAt this point we’ve revisited the two main concepts that I introduced in\nChapter 1\n: the relationship between the event-time and processing-time\ndomains, and windowing. If we want to go any further, we’ll need to start\nadding the new concepts mentioned at the beginning of this section: triggers,\nwatermarks, and accumulation.\nGoing Streaming: \nWhen\n and \nHow\nWe just observed the execution of a windowed pipeline on a batch engine.\nBut, ideally, \nwe’d like to \nhave lower latency for our results, and we’d also\nlike to natively handle unbounded data sources. Switching to a streaming\nengine is a step in the right direction, but our previous strategy of waiting\nuntil our input has been consumed in its entirety to generate output is no\nlonger feasible. Enter triggers and watermarks.\nWhen\n: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nTriggers provide the answer to the question: “\nWhen\n in processing time are\nresults materialized?” Triggers \ndeclare \nwhen output for a window should\nhappen in processing time (though the triggers themselves might make those\ndecisions based on things that happen in other time domains, such as\n00:00 / 00:00\nwatermarks progressing in the event-time domain, as we’ll see in a few\nmoments). Each specific output for a window is referred to as a \npane\n of the\nwindow.\nThough it’s possible to imagine quite a breadth of possible triggering\nsemantics,\n conceptually there are only two generally useful types of triggers,\nand practical applications almost always boil down using either one or a\ncombination of both:\nRepeated update triggers\nThese periodically generate updated\n panes for a window\n as its contents\nevolve. These updates can be materialized with every new record, or they\ncan happen after some processing-time delay, such as once a minute. The\nchoice of period for a repeated update trigger is primarily an exercise in\nbalancing latency and cost.\nCompleteness triggers\nThese materialize a pane for a window only after the input for that\nwindow is believed to be complete to some threshold.\n This type of trigger\nis most analogous to what we’re familiar with in batch processing: only\nafter the input is complete do we provide a result. The difference in the\ntrigger-based approach is that the notion of completeness is scoped to the\ncontext of a single window, rather than always being bound to the\ncompleteness of the entire input.\nRepeated update triggers are the most common type of trigger encountered in\nstreaming systems. They are simple to implement and simple to understand,\nand they provide useful semantics for a specific type of use case: repeated\n(and eventually consistent) updates to a materialized dataset, analogous to the\nsemantics you get with materialized views in the database world.\nCompleteness triggers are less frequently encountered, but provide streaming\nsemantics that more closely align with those from the classic batch\nprocessing world. They also provide tools for reasoning about things like\nmissing data and late data, both of which we discuss shortly (and in the next\nchapter) as we explore the underlying primitive that drives completeness\n3\ntriggers: watermarks.\nBut first, let’s start simple and look at some basic repeated update triggers in\naction. To make the notion\n of triggers a bit more concrete, let’s go ahead and\nadd the most straightforward type of trigger to our example pipeline: a trigger\nthat fires with every new record, as shown in \nExample 2-3\n.\nExample 2-3. \nTriggering repeatedly with every record\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n                \n.triggering(Repeatedly(AfterCount(1)))\n);\n  .apply(\nSum.integersPerKey()\n);\nIf we were to run this new pipeline on a streaming engine, the results would\nlook something like that shown in \nFigure 2-6\n.\nFigure 2-6. \nPer-record triggering on a streaming engine\nYou can see how we now get multiple outputs (panes) for each window: once\nper corresponding input. This sort of triggering pattern works well when the\noutput stream is being written to some sort of table that you can simply poll\nfor results. Any time you look in the table, you’ll see the most up-to-date\nvalue for a given window, and those values will converge toward correctness\nover time.\nOne downside of per-record triggering is that it’s quite chatty. When\nprocessing large-scale data, aggregations like summation provide a nice\nopportunity to reduce the cardinality of the stream without losing\ninformation. \nThis is particularly noticeable for cases in which you have high-\nvolume keys; for our example, massive teams with lots of active players.\nImagine a massively multiplayer game in which players are split into one of\n00:00 / 00:00\ntwo factions, and you want to tally stats on a per-faction basis. It’s probably\nunnecessary to update your tallies with every new input record for every\nplayer in a given faction. Instead, you might be happy updating them after\nsome processing-time delay, say every second, or every minute. The nice side\neffect of using processing-time delays is that it has an equalizing effect across\nhigh-volume keys or windows: the resulting stream ends up being more\nuniform cardinality-wise.\nThere are two different approaches\n to processing-time \ndelays in triggers:\naligned delays\n (where the delay slices up processing time into fixed regions\nthat align across keys and windows) and \nunaligned delays\n (where the delay is\nrelative to the data observed within a given window). A pipeline with\nunaligned delays might look like \nExample 2-4\n, the results of which are shown\nin \nFigure 2-7\n.\nExample 2-4. \nTriggering on aligned two-minute processing-time boundaries\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(Repeatedly(\nAlignedDelay(TWO_MINUTES)\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 2-7. \nTwo-minute aligned delay triggers (i.e., microbatching)\nThis sort of aligned delay trigger is effectively what you get from a\nmicrobatch streaming system like Spark Streaming. The nice thing about it is\npredictability; you get regular updates across all modified windows at the\nsame time.\n That’s also the downside: all updates happen at once, which\nresults in bursty workloads that often require greater peak provisioning to\nproperly handle the load. The alternative is to use an unaligned delay. That\nwould look something \nExample 2-5\n in Beam. \nFigure 2-8\n presents the results.\n00:00 / 00:00\nExample 2-5. \nTriggering on unaligned two-minute processing-time\nboundaries\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(Repeatedly(\nUnalignedDelay\n(TWO_MINUTES))\n  .apply(\nSum.integersPerKey()\n);\nFigure 2-8. \nTwo-minute unaligned delay triggers\nContrasting the unaligned delays in \nFigure 2-8\n to the aligned delays in\nFigure 2-6\n, it’s easy to see how the unaligned delays spread the load out more\nevenly across time. The actual latencies involved for any given window differ\nbetween the two, sometimes more and sometimes less, but in the end the\naverage latency will remain essentially the same. From that perspective,\nunaligned delays are typically the better choice for large-scale processing\nbecause they result in a more even load distribution over time.\nRepeated update triggers are great for use cases in which we simply want\nperiodic updates to our results over time and are fine with those updates\nconverging toward correctness with no clear indication of when correctness is\nachieved. However, as we discussed in \nChapter 1\n, the vagaries of distributed\nsystems often lead to a varying level of skew between the time an event\nhappens and the time it’s actually observed by your pipeline, which means it\ncan be difficult to reason about when your output presents an accurate and\ncomplete view of your input data. For cases in which input completeness\nmatters, it’s important to have some way of reasoning about completeness\nrather than blindly trusting the results calculated by whichever subset of data\nhappen to have found their way to your pipeline. Enter watermarks.\n00:00 / 00:00",8682
17-When Watermarks.pdf,17-When Watermarks,"When\n: Watermarks\nWatermarks are a supporting aspect of the answer\n to the\n question: “\nWhen\n in\nprocessing time are results materialized?” Watermarks are temporal notions\nof input completeness in the event-time domain. Worded differently, they are\nthe way the system measures progress and completeness relative to the event\ntimes of the records being processed in a stream of events (either bounded or\nunbounded, though their usefulness is more apparent in the unbounded case).\nRecall this diagram from \nChapter 1\n, slightly modified in \nFigure 2-9\n, in which\nI described\n the skew between event time and \nprocessing time as an ever-\nchanging function of time for most real-world distributed data processing\nsystems.\nFigure 2-9. \nEvent-time progress, skew, and watermarks\nThat meandering red line that I claimed represented reality is essentially the\nwatermark; it captures the progress of event-time completeness as processing\ntime progresses.\n Conceptually, you can think of the watermark as a function,\nF\n(\nP\n) → \nE\n, which takes a point in \nprocessing time and returns a point in event\ntime.\n That point in event time, \nE\n, is the point up to which the system\nbelieves all inputs with event times less than \nE\n have been observed. In other\nwords, it’s an assertion that no more data with event times less than \nE\n will\n4\never be seen again. Depending upon the type of watermark, perfect or\nheuristic, that assertion can be a strict guarantee or an educated guess,\nrespectively:\nPerfect watermarks\nFor the case in which we have perfect knowledge of all of the input data,\nit’s possible to construct a perfect watermark.\n In such a case, there is no\nsuch thing as late data; all data are early or on time.\nHeuristic watermarks\nFor many distributed input sources, perfect knowledge of the input data is\nimpractical, in which case the next best option is to provide a heuristic\nwatermark.\n Heuristic watermarks use whatever information is available\nabout the inputs (partitions, ordering within partitions if any, growth rates\nof files, etc.) to provide an estimate of progress that is as accurate as\npossible. In many cases, such watermarks can be remarkably accurate in\ntheir predictions. Even so, the use of a heuristic watermark means that it\nmight sometimes be wrong, which will lead to late data. We show you\nabout ways to deal with late data soon.\nBecause they provide a notion of completeness relative to our inputs,\nwatermarks form the\n foundation for the second \ntype of trigger mentioned\npreviously: \ncompleteness triggers\n. Watermarks themselves are a fascinating\nand complex topic, as you’ll see when you get to Slava’s watermarks deep\ndive in \nChapter 3\n. But for now, let’s look at them in action by updating our\nexample pipeline to utilize a completeness trigger built upon watermarks, as\ndemonstrated in \nExample 2-6\n.\nExample 2-6. \nWatermark completeness trigger\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\nAfterWatermark()\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nNow, an interesting quality of watermarks is that they are a class of functions,\nmeaning there are multiple different functions \nF\n(\nP\n) → \nE\n that satisfy the\nproperties of a watermark, to varying degrees of success.\n As I noted earlier,\nfor situations in which you have perfect knowledge of your input data, it\nmight be possible to build a perfect watermark, which is the ideal situation.\nBut for cases in which you lack perfect knowledge of the inputs or for which\nit’s simply too computationally expensive to calculate the perfect watermark,\nyou might instead choose to utilize a heuristic for defining your watermark.\nThe point I want to make here is that the given watermark algorithm in use is\nindependent from the pipeline itself. We’re not going to discuss in detail what\nit means to implement a watermark here (Slava does that in \nChapter 3\n). For\nnow, to help drive home this idea that a given input set can have different\nwatermarks applied to it, let’s take a look at our pipeline in \nExample 2-6\nwhen \nexecuted\n on\n the same\n dataset but using two distinct watermark\nimplementations (\nFigure 2-10\n): on the left, a perfect watermark; on the right,\na heuristic watermark.\nIn both cases, windows are materialized as the watermark passes the end of\nthe window. The perfect watermark, as you might expect, perfectly captures\nthe event-time completeness of the pipeline as time progresses. In contrast,\nthe specific algorithm used for the heuristic watermark on the right fails to\ntake the value of 9 into account,\n which drastically changes the shape of the\nmaterialized outputs, both in terms of output latency and correctness (as seen\nby the incorrect answer of 5 that’s provided for the [12:00, 12:02) window).\nThe big difference between the watermark triggers from \nFigure 2-9\n and the\nrepeated update triggers we saw in Figures \n2-5\n through \n2-7\n is that the\nwatermarks give us a way to reason about the completeness of our input\n.\nUntil the system\n materializes an output for a given window, we know that the\nsystem does not yet believe the inputs to be complete. This is especially\nimportant for use cases in which you want to reason about a \nlack of data\n in\nthe input, or \nmissing data\n.\n5\n00:00 / 00:00\nFigure 2-10. \nWindowed summation on a streaming engine with perfect (left) and heuristic\n(right) watermarks\nA great example of a missing-data use case is outer joins. \nWithout a notion of\ncompleteness like watermarks, how do you know when to give up and emit a\npartial join rather than continue to wait for that join to complete? You don’t.\nAnd basing that decision on a processing-time delay, which is the common\napproach in streaming systems that lack true watermark support, is not a safe\nway to go, because of the variable nature of event-time skew we spoke about\nin \nChapter 1\n: as long as skew remains smaller than the chosen processing-\ntime delay, your missing-data results will be correct, but any time skew\ngrows beyond that delay, they will suddenly become \nin\ncorrect. From this\nperspective, event-time watermarks are a critical piece of the puzzle for many\nreal-world streaming use cases which must reason about a lack of data in the\ninput, such as outer joins, anomaly detection, and so on.\nNow, with that said, these watermark examples also highlight two\nshortcomings\n of watermarks (and any other notion of completeness),\nspecifically that they can be one of the following:\nToo slow\nWhen a watermark of any\n type is correctly delayed due to known\nunprocessed data (e.g., slowly growing input logs due to network\nbandwidth constraints), that translates directly into delays in output if\nadvancement of the watermark is the only thing you depend on for\nstimulating results.\nThis is most obvious in the left diagram of \nFigure 2-10\n, for which the late\narriving 9 holds back the watermark for all the subsequent windows, even\nthough the input data for those windows become complete earlier. This is\nparticularly apparent for the second window, [12:02, 12:04), for which it\ntakes nearly seven minutes from the time the first value in the window\noccurs until we see any results for the window whatsoever. The heuristic\nwatermark in this example doesn’t suffer the same issue quite so badly\n(five minutes until output), but don’t take that to mean heuristic\nwatermarks never suffer from watermark lag; that’s really just a\nconsequence of the record I chose to omit from the heuristic watermark in\nthis specific example.\nThe important point here is the following: Although watermarks provide\na very useful notion of completeness, depending upon completeness for\nproducing output is often not ideal from a latency perspective. Imagine a\ndashboard that contains valuable metrics, windowed by hour or day. It’s\nunlikely you’d want to wait a full hour or day to begin seeing results for\nthe current window; that’s one of the pain points of using classic batch\nsystems to power such systems. Instead, it would be much nicer to see the\nresults for those windows refine over time as the inputs evolve and\neventually become complete.\nToo fast\nWhen a heuristic watermark is incorrectly advanced earlier than it should\nbe, it’s possible for data with event times before the watermark to arrive\nsome time later, creating late data.\n This is what happened in the example\non the right: the watermark advanced past the end of the first window\nbefore all the input data for that window had been observed, resulting in\nan incorrect output value of 5 instead of 14. This shortcoming is strictly a\nproblem with heuristic watermarks; their heuristic nature implies they\nwill sometimes be wrong. As a result, relying on them alone for\ndetermining when to materialize output is insufficient if you care about\ncorrectness.\nIn \nChapter 1\n, I made some rather emphatic statements about notions of\ncompleteness being insufficient for most use cases requiring robust out-of-\norder processing of unbounded data streams. These two shortcomings—\nwatermarks being too slow or too fast—are the foundations for those\narguments. You simply cannot get both low latency and correctness out of a\nsystem that relies solely on notions of completeness.\n So, for cases for which\nyou do want the best of both worlds, what’s a person to do? Well, if repeated\nupdate triggers provide low-latency updates but no way to reason about\ncompleteness, and watermarks provide a notion of completeness but variable\nand possible high latency, why not combine their powers together?\n6",9759
18-When EarlyOn-TimeLate Triggers FTW.pdf,18-When EarlyOn-TimeLate Triggers FTW,"When\n: Early/On-Time/Late Triggers FTW!\nWe’ve now looked at the two main types of triggers: repeated update triggers\nand completeness/watermark triggers.\n In many case, neither of them alone is\nsufficient, but the combination of them together is. Beam recognizes this fact\nby providing an extension of the standard watermark trigger that also\nsupports repeated update triggering on either side of the watermark. This is\nknown as the early/on-time/late trigger because it partitions the panes that are\nmaterialized by the compound trigger into three categories:\nZero or more \nearly panes\n, which are the result of a repeated update\ntrigger that periodically fires up until the watermark passes the end\nof the window.\n The panes generated by these firings contain\nspeculative results, but allow us to observe the evolution of the\nwindow over time as new input data arrive. This compensates for the\nshortcoming of watermarks sometimes being \ntoo slow\n.\nA single \non-time pane\n, which is the result of the\ncompleteness/watermark trigger \nfiring \nafter the watermark passes the\nend of the window. This firing is special because it provides an\nassertion that the system now believes the input for this window to\nbe complete.\n This means that it is now safe to reason about \nmissing\ndata\n; for example, to emit a partial join when performing an outer\njoin.\nZero or more \nlate panes\n, which are the result of another (possibly\ndifferent) repeated \nupdate trigger that periodically fires any time late\ndata arrive after the watermark has passed the end of the window. In\nthe case of a perfect watermark, there will always be zero late panes.\nBut in the case of a heuristic watermark, any data the watermark\nfailed to properly account for will result in a late firing. This\ncompensates for the shortcoming of watermarks being \ntoo fast\n.\nLet’s see how this looks in action. We’ll update our pipeline to use a periodic\nprocessing-time trigger with an aligned delay of one minute for the early\nfirings, and a per-record trigger for the late firings. That way, the early firings\n7\nwill give us some amount of batching for high-volume windows (thanks to\nthe fact that the trigger will fire only once per minute, regardless of the\nthroughput into the window), but we won’t introduce unnecessary latency for\nthe late firings, which are hopefully somewhat rare if we’re using a\nreasonably accurate heuristic watermark. In Beam, that looks \nExample 2-7\n(\nFigure 2-11\n shows the results).\nExample 2-7. \nEarly, on-time, and late firings via the early/on-time/late API\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(AfterWatermark()\n        \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n        \n.withLateFirings(AfterCount(1))\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 2-11. \nWindowed summation on a streaming engine with early, on-time, and late\nfirings\nThis version has two clear improvements over \nFigure 2-9\n:\nFor the “watermarks too slow” case in the second window, [12:02,\n12:04): we now provide periodic early updates once per minute. The\ndifference is most stark in the perfect watermark case, for which\ntime-to-first-output is reduced from almost seven minutes down to\nthree and a half; but it’s also clearly improved in the heuristic case,\nas well. Both versions now provide steady refinements over time\n(panes with values 7, 10, then 18), with relatively minimal latency\nbetween the input becoming complete and materialization of the\nfinal output pane for the window.\nFor the “heuristic watermarks too fast” case in the first window,\n00:00 / 00:00",3711
19-When Allowed Lateness i.e. Garbage Collection.pdf,19-When Allowed Lateness i.e. Garbage Collection,"[12:00, 12:02): when the value of 9 shows up late, we immediately\nincorporate it into a new, corrected pane with value of 14.\nOne interesting side effect of these new triggers is that they effectively\nnormalize the output pattern between the perfect and \nheuristic watermark\nversions. \nWhereas the two versions in \nFigure 2-10\n were starkly different, the\ntwo versions here look quite similar. They also look much more similar to the\nvarious repeated update version from Figures \n2-6\n through \n2-8\n, with one\nimportant difference: thanks to the use of the watermark trigger, we can also\nreason about input completeness in the results we generate with the early/on-\ntime/late trigger. This allows us to better handle use cases that care about\nmissing data\n, like outer joins, anomaly detection, and so on.\nThe biggest remaining difference between the perfect and heuristic early/on-\ntime/late versions at this point is window lifetime bounds. In the perfect\nwatermark case, we know we’ll never see any more data for a window after\nthe watermark has passed the end of it, hence we can drop all of our state for\nthe window at that time. In the heuristic watermark case, we still need to hold\non to the state for a window for some amount of time to account for late data.\nBut as of yet, our system doesn’t have any good way of knowing just how\nlong state needs to be kept around for each window. That’s where \nallowed\nlateness\n comes in.\nWhen\n: Allowed Lateness (i.e., Garbage Collection)\nBefore moving on to our last question (“\nHow\n do refinements of results\nrelate?”), I’d like to touch\n on\n a practical\n necessity \nwithin long-lived, \nout-of-\norder stream processing systems: garbage collection. In the heuristic\nwatermarks example in \nFigure 2-11\n, the persistent state for each window\nlingers around for the entire lifetime of the example; this is necessary to\nallow us to appropriately deal with late data when/if they arrive.\n But while it\nwould be great to be able to keep around all of our persistent state until the\nend of time, in reality, when dealing with an unbounded data source, it’s\noften not practical to keep state (including metadata) for a given window\nindefinitely; we’ll eventually run out of disk space (or at the very least tire of\npaying for it, as the value for older data diminishes over time).\nAs a result, any real-world out-of-order processing system needs to provide\nsome way to bound the lifetimes of the windows it’s processing. A clean and\nconcise way of doing this is by defining a horizon on the allowed lateness\nwithin the system; that is, placing a bound on how late any given \nrecord\n may\nbe (relative to the watermark) for the system to bother processing it; any data\nthat arrives after this horizon are simply dropped. After you’ve bounded how\nlate individual data may be, you’ve also established precisely how long the\nstate for windows must be kept around: until the watermark exceeds the\nlateness horizon for the end of the window. But in addition, you’ve also given\nthe system the liberty to immediately drop any data later than the horizon as\nsoon as they’re observed, which means the system doesn’t waste resources\nprocessing data that no one cares about.\nMEASURING LATENESS\nIt might seem a little odd to be specifying a horizon for handling late data\nusing the very metric that resulted in the late data in the first place (i.e.,\nthe heuristic watermark). And in some sense it is. But of the options\navailable, it’s arguably the best. The only other practical option would be\nto specify the horizon in processing time (e.g., keep windows around for\n10 minutes of processing time after the watermark passes the end of the\nwindow), but using processing time would leave the garbage collection\npolicy vulnerable to issues within the pipeline itself (e.g., workers\ncrashing, causing the pipeline to stall for a few minutes), which could\nlead to windows that didn’t actually have a chance to handle late data that\nthey otherwise should have. By specifying the horizon in the event-time\ndomain, garbage collection is directly tied to the actual progress of the\npipeline, which decreases the likelihood that a window will miss its\nopportunity to handle late data appropriately.\nNote however, that not all watermarks are created equal. When we speak\nof watermarks in this book, we generally refer to \nlow\n watermarks, which\npessimistically attempt to capture the event time of the \noldest\nunprocessed record the system is aware of. The nice thing about dealing\nwith lateness via low watermarks is that they are resilient to changes in\nevent-time skew; no matter how large the skew in a pipeline may grow,\nthe low watermark will always track the oldest outstanding event known\nto the system, providing the best guarantee of correctness possible.\nIn contrast, some systems may use the term “watermark” to mean other\nthings. For example, \nwatermarks in Spark Structured Streaming\n are \nhigh\nwatermarks, which optimistically track the event time of the \nnewest\nrecord the system is aware of. When dealing with lateness, the system is\nfree to garbage collect any window older than the high watermark\nadjusted by some user-specified lateness threshold. In other words, the\nsystem allows you to specify the maximum amount of event-time skew\nyou expect to see in your pipeline, and then throws away any data outside\nof that skew window. This can work well if skew within your pipeline\nremains within some constant delta, but is more prone to incorrectly\ndiscarding data than low watermarking schemes.\nBecause the interaction between\n allowed lateness and the watermark is a little\nsubtle, it’s worth looking at an example. Let’s take the heuristic watermark\npipeline from \nExample 2-7\n/\nFigure 2-11\n and add in \nExample 2-8\n a lateness\nhorizon of one minute (note that this particular horizon has been chosen\nstrictly because it fits nicely into the diagram; for real-world use cases, a\nlarger horizon would likely be much more practical):\nExample 2-8. \nEarly/on-time/late firings with allowed lateness\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n               \n.withAllowedLateness(ONE_MINUTE)\n)\n .apply(\nSum.integersPerKey()\n);\nThe execution of this pipeline would look something like \nFigure 2-12\n, in\nwhich I’ve added the following features to highlight the effects of allowed\nlateness:\nThe thick black line denoting the current position in processing time\nis now annotated with ticks indicating the lateness horizon (in event\ntime) for all active windows.\nWhen the watermark passes the lateness horizon for a window, that\nwindow is closed, which means that all state for the window is\ndiscarded. I leave around a dotted rectangle showing the extent of\ntime (in both domains) that the window covered when it was closed,\nwith a little tail extending to the right to denote the lateness horizon\nfor the window (for contrasting against the watermark).\nFor this diagram only, I’ve added an additional late datum for the\nfirst window with value 6. The 6 is late, but still within the allowed\nlateness horizon and thus is incorporated into an updated result with\nvalue 11. The 9, however, arrives beyond the lateness horizon, so it\nis simply dropped.\nFigure 2-12. \nAllowed lateness with early/on-time/late firings\nTwo final side notes about lateness horizons:\nTo be absolutely clear, if you happen to be consuming data from\nsources for which perfect \nwatermarks are available, there’s no need\nto deal with late data, and an allowed lateness horizon of zero\nseconds will be optimal. This is what we saw in the perfect\nwatermark portion of \nFigure 2-10\n.\nOne noteworthy exception to the rule of needing to specify lateness\nhorizons, even when heuristic watermarks are in use, \nwould be\nsomething like computing global aggregates over all time for a\ntractably finite number of keys (e.g., computing the total number of\n00:00 / 00:00",8269
20-How Accumulation.pdf,20-How Accumulation,"visits to your site over all time, grouped by web browser family). In\nthis case, the number of active windows in the system is bounded by\nthe limited keyspace in use. As long as the number of keys remains\nmanageably low, there’s no need to worry about limiting the lifetime\nof windows via allowed lateness.\nPracticality sated, let’s\n move on\n to our\n fourth \nand final question.\nHow\n: Accumulation\nWhen triggers are used to produce multiple panes for a single window over\ntime, we find ourselves \nconfronted \nwith the last question: “\nHow\n do\nrefinements of results relate?” In the examples we’ve seen so far, each\nsuccessive pane is built upon the one immediately preceding it. However,\nthere are actually three\n different modes of accumulation:\nDiscarding\nEvery time a pane is materialized, any stored state is discarded.\n This\nmeans that each successive pane is independent from any that came\nbefore. Discarding mode is useful when the downstream consumer is\nperforming some sort of accumulation itself; for example, when sending\nintegers into a system that expects to receive deltas that it will sum\ntogether to produce a final count.\nAccumulating\nAs in Figures \n2-6\n through \n2-11\n, every time a pane is materialized, \nany\nstored state is retained, and future inputs are accumulated into the existing\nstate. This means that each successive pane builds upon the previous\npanes. Accumulating mode is useful when later results can simply\noverwrite previous results, such as when storing output in a key/value\nstore like HBase or Bigtable.\nAccumulating and retracting\nThis is like accumulating mode, but when producing a new pane, it also\nproduces independent retractions for the previous pane(s).\n Retractions\n8\n9\n(combined with the new accumulated result) are essentially an explicit\nway of saying “I previously told you the result was \nX\n, but I was wrong.\nGet rid of the \nX\n I told you last time, and replace it with \nY\n.” There are two\ncases for which retractions are particularly helpful:\nWhen consumers downstream are \nregrouping data by a different\ndimension\n, it’s entirely possible the new value may end up keyed\ndifferently from the previous value and thus end up in a different\ngroup. In that case, the new value can’t just overwrite the old value;\nyou instead need the retraction to remove the old value\nWhen \ndynamic windows\n (e.g., sessions, which we look at more\nclosely in a few moments) are \nin use, the new\n value might be\nreplacing more than one previous window, due to window merging.\nIn this case, it can be difficult to determine from the new window\nalone which old windows are being replaced. Having explicit\nretractions for the old windows makes the task straightforward. We\nsee an example of this in detail in \nChapter 8\n.\nThe different semantics for each group are somewhat clearer when seen side-\nby-side. Consider the two panes for the second window (the one with event-\ntime range [12:06, 12:08)) in \nFigure 2-11\n (the one with early/on-time/late\ntriggers). \nTable 2-1\n shows what the values for each pane would look like\nacross the three accumulation modes (with \naccumulating\n mode being the\nspecific mode used in \nFigure 2-11\n itself).\nTable 2-1. \nComparing accumulation modes using the second window\nfrom \nFigure 2-11\n \nDiscarding\nAccumulating\nAccumulating &\nRetracting\nPane 1: inputs=[3]\n3\n3\n3\nPane 2: inputs=[8, 1]\n9\n12\n12, –3\nValue of final normal\npane\n9\n12\n12\nSum of all panes\n12\n15\n12\nLet’s take a closer look at what’s happening:\nDiscarding\nEach pane incorporates only the values that arrived during that specific\npane. As such, the final value observed does not fully capture the total\nsum. However, if you were to sum all of the independent panes\nthemselves, you would arrive at a correct answer of 12. This is why\ndiscarding mode is useful when the downstream consumer itself is\nperforming some sort of aggregation on the materialized panes.\nAccumulating\nAs in \nFigure 2-11\n, each pane incorporates the values that arrived during\nthat specific pane, plus all of the values from previous panes. As such, the\nfinal value observed correctly captures the total sum of 12. If you were to\nsum up the individual panes themselves, however, you’d effectively be\ndouble-counting the inputs from pane 1, giving you an incorrect total sum\nof 15. This is why accumulating mode is most useful when you can\nsimply overwrite previous values with new values: the new value already\nincorporates all of the data seen thus far.\nAccumulating and retracting\nEach pane includes both a new accumulating mode value as well as a\nretraction of the previous pane’s value. As such, both the last value\nobserved (excluding retractions) as well as the total sum of all\nmaterialized panes (including retractions) provide you with the correct\nanswer of 12. This is why retractions are so powerful.\nExample 2-9\n demonstrates discarding mode in action, illustrating the\n changes\nwe would make to \nExample 2-7\n:\nExample 2-9. \nDiscarding mode version of early/on-time/late firings\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AtCount(1))\n)\n               \n.discardingFiredPanes()\n)\n  .apply(\nSum.integersPerKey()\n);\nRunning again on a streaming engine with a heuristic watermark would\nproduce output like that shown in \nFigure 2-13\n.\nFigure 2-13. \nDiscarding mode version of early/on-time/late firings on a streaming engine\nEven though the overall shape of the output is similar to the accumulating\nmode version from \nFigure 2-11\n, note how none of the panes in this discarding\nversion overlap. As a result, each output is independent from the others.\nIf we want to look at retractions in\n action, the change\n would be similar, as\nshown in \nExample 2-10\n. \n???\n depicts the results.\nExample 2-10. \nAccumulating and retracting mode version of early/on-\ntime/late firings\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AtCount(1))\n)\n               \n.accumulatingAndRetractingFiredPanes()\n)\n  .apply(\nSum.integersPerKey()\n);\n00:00 / 00:00",6585
21-Summary.pdf,21-Summary,"Accumulating and retracting mode version of early/late firings on a streaming\nengine\nBecause the panes for each window all overlap, it’s a little tricky to see the\nretractions clearly. \nThe retractions are indicated in red, which combines with\nthe overlapping blue panes to yield a slightly purplish color. I’ve also\nhorizontally shifted the values of the two outputs within a given pane slightly\n(and separated them with a comma) to make them easier to differentiate.\nFigure 2-14\n combines the final frames of Figures \n2-9\n, \n2-11\n (heuristic only),\nand \nside-by-side, providing a nice visual contrast of the three modes.\nFigure 2-14. \nSide-by-side comparison of accumulation modes\nAs you can imagine, the modes in the order presented (discarding,\naccumulating, accumulating and retracting) are each successively more\nexpensive in terms of storage and computation costs. To that end, choice of\naccumulation mode provides yet another dimension for making trade-offs\nalong the axes of correctness, latency, and cost.\nSummary\nWith this chapter complete, you now understand the basics of robust stream\nprocessing and are ready to go forth into the world and do amazing things. Of\n00:00 / 00:00\ncourse, there are eight more chapters anxiously waiting for your attention, so\nhopefully you won’t go forth like right now, this very minute. But regardless,\nlet’s recap what we’ve just covered, lest you forget any of it in your haste to\namble forward. First, the major concepts we touched upon:\nEvent time versus processing time\nThe all-important distinction between when events occurred and when\nthey are observed by your data processing system.\nWindowing\nThe commonly utilized approach to managing unbounded data by slicing\nit along temporal\n boundaries (in either processing time or event time,\nthough we narrow the definition of windowing \nin the Beam Model to\nmean only within event time).\nTriggers\nThe declarative mechanism for specifying precisely when materialization\nof output makes sense for your particular use case.\nWatermarks\nThe powerful notion of progress in event time that provides a means of\nreasoning about completeness (and thus missing data) in an out-of-order\nprocessing system operating on unbounded data.\nAccumulation\nThe relationship between refinements of results for a single window for\ncases in which it’s materialized multiple times as it evolves.\nSecond, the four questions we used to frame our exploration:\nWhat\n results are calculated? = transformations.\nWhere\n in event time are results calculated? = windowing.\nWhen\n in processing time are results materialized? = triggers plus\nwatermarks.\nHow\n do refinements of results relate? = accumulation.\nThird, to drive home the flexibility afforded by this model of stream\nprocessing (because in the end, that’s really what this is all about: balancing\ncompeting tensions like correctness, latency, and cost), a recap of the major\nvariations in output we were able to achieve over the same dataset with only a\nminimal amount of code change:\n \nInteger summation\nExample 2-1\n / \nFigure 2-3\nInteger summation\nFixed windows batch\nExample 2-2\n / \nFigure 2-5\nInteger summation\nFixed windows streaming\nRepeated per-record\ntrigger\nExample 2-3\n / \nFigure 2-6\n \nInteger summation\nFixed windows streaming\nRepeated aligned-delay\ntrigger\nExample 2-4\n / \nFigure 2-7\nInteger summation\nFixed windows streaming\nRepeated unaligned-delay\ntrigger\nExample 2-5\n / \nFigure 2-8\nInteger summation\nFixed windows streaming\nHeuristic watermark\ntrigger\nExample 2-6\n / \nFigure 2-10\n \nInteger summation\nFixed windows streaming\nEarly/on-time/late trigger\nInteger summation\nFixed windows streaming\nEarly/on-time/late trigger\nInteger summation\nFixed windows streaming\nEarly/on-time/late trigger\nDiscarding\nExample 2-9\n / \nFigure 2-13\nAccumulating\nExample 2-7\n / \nFigure 2-11\nAccumulating and\nRetracting\nExample 2-10\n / \n???\nAll that said, at this point, we’ve really looked at only one type of\nwindowing: fixed windowing in event time. As we know, there are a number\nof dimensions to windowing, and I’d like to touch upon at least two more of\nthose before we call it day with the Beam Model. First, however, we’re going\nto take a slight detour to dive deeper into the world of watermarks, as this\nknowledge will help frame future discussions (and be fascinating in and of\nitself).\n Enter Slava, stage right...\n If you’re fortunate enough to be reading the Safari version of the book, you\nhave full-blown time-lapse animations just like in \n“Streaming 102”\n. For print,\nKindle, and other ebook versions, there are static images with a link to\nanimated versions on the web.\n Bear with me here. Fine-grained emotional expressions via composite\npunctuation (i.e., emoticons) are strictly forbidden in O’Reilly publications <\nwinky-smiley/>.\n And indeed, we did just that with the original triggers feature in Beam. In\nretrospect, we went a bit overboard. Future iterations will be simpler and\neasier to use, and in this book I focus only on the pieces that are likely to\nremain in some form or another.\n More accurately, the input to the function is really the state at time \nP\n of\neverything upstream of the point in the pipeline where the watermark is being\nobserved: the input source, buffered data, data actively being processed, and\nso on; but conceptually it’s simpler to think of it as a mapping from\nprocessing time to event time.\n Note that I specifically chose to omit the value of 9 from the heuristic\nwatermark because it will help me to make some important points about late\ndata and watermark lag. In reality, a heuristic watermark might be just as\nlikely to omit some other value(s) instead, which in turn could have\n1\n2\n3\n4\n5\nsignificantly less drastic effect on the watermark. If winnowing late-arriving\ndata from the watermark is your goal (which is very valid in some cases, such\nas abuse detection, for which you just want to see a significant majority of the\ndata as quickly as possible), you don’t necessarily want a heuristic watermark\nrather than a perfect watermark. What you really want is a percentile\nwatermark, which explicitly drops some percentile of late-arriving data from\nits calculations. See \nChapter 3\n.\n Which isn’t to say there aren’t use cases that care primarily about\ncorrectness and not so much about latency; in those cases, using an accurate\nwatermark as the sole driver of output from a pipeline is a reasonable\napproach.\n And, as we know from before, this assertion is either guaranteed, in the case\nof a perfect watermark being used, or an educated guess, in the case of a\nheuristic watermark.\n You might note that there should logically be a fourth mode: discarding and\nretracting. That mode isn’t terribly useful in most cases, so I don’t discuss it\nfurther here.\n In retrospect, it probably would have been clearer to choose a different set\nof names that are more oriented toward the observed nature of data in the\nmaterialized stream (e.g., “output modes”) rather than names describing the\nstate management semantics that yield those data. Perhaps: discarding mode\n→ delta mode, accumulating mode → value mode, accumulating and\nretracting mode → value and retraction mode? However, the\ndiscarding/accumulating/accumulating and retracting names are enshrined in\nthe 1.x and 2.x lineages of the Beam Model, so I don’t want to introduce\npotential confusion in the book by deviating. Also, it’s very likely\naccumulating modes will blend into the background more with Beam 3.0 and\nthe introduction of \nsink triggers\n; more on this when we discuss SQL in\nChapter 8\n.\n6\n7\n8\n9",7752
22-3. Watermarks.pdf,22-3. Watermarks,,0
23-Definition.pdf,23-Definition,"Chapter 3. \nWatermarks\nSo far, we have been looking at stream processing from the perspective of the\npipeline author \nor data scientist. \nChapter 2\n introduced watermarks as part of\nthe answer to the fundamental questions of \nwhere\n in event-time processing is\ntaking place and \nwhen\n in processing time results are materialized. In this\nchapter, we approach the same questions, but instead from the perspective of\nthe underlying mechanics of the stream processing system. Looking at these\nmechanics will help us motivate, understand, and apply the concepts around\nwatermarks. We discuss how watermarks are created at the point of data\ningress, how they propagate through a data processing pipeline, and how they\naffect output timestamps. We also demonstrate how watermarks preserve the\nguarantees that are necessary for answering the questions of \nwhere\n in event-\ntime data are processed and \nwhen\n it is materialized, while dealing with\nunbounded data.\nDefinition\nConsider any pipeline that ingests data and outputs results continuously. \nWe\nwish to solve the general problem of when it is safe to call an event-time\nwindow closed, meaning that the window does not expect any more data. To\ndo so we would like to characterize the progress that the pipeline is making\nrelative to its unbounded input.\nOne naive approach for solving the event-time windowing problem would be\nto \nsimply\n base our event-time windows on the current processing time. As we\nsaw in \nChapter 1\n, we quickly run into trouble—data processing and transport\nis not instantaneous, so processing and event times are almost never equal.\nAny hiccup or spike in our pipeline might cause us to incorrectly assign\nmessages to windows. Ultimately, this strategy fails because we have no\nrobust way to make any guarantees about such windows.\nAnother intuitive, but ultimately incorrect, approach would be to consider the\nrate of messages processed by the pipeline. Although this is an interesting\nmetric, the rate may vary arbitrarily with changes in input, variability of\nexpected results, resources available for processing, and so on. Even more\nimportant, rate does not help answer the fundamental questions of\ncompleteness. Specifically, rate does not tell us when we have seen all of the\nmessages for a particular time interval. In a real-world system, there will be\nsituations in which messages are not making progress through the system.\nThis could be the result of transient errors (such as crashes, network failures,\nmachine downtime), or the result of persistent errors such as application-level\nfailures that require changes to the application logic or other manual\nintervention to resolve. Of course, if lots of failures are occurring, a rate-of-\nprocessing metric might be a good proxy for detecting this. However a rate\nmetric could never tell us that a single message is failing to make progress\nthrough our pipeline. Even a single such message, however, can arbitrarily\naffect the correctness of the output results.\nWe require a more robust measure of progress. To arrive there, we make one\nfundamental assumption about \nour streaming data: \neach message has an\nassociated logical event timestamp\n. This assumption is reasonable in the\ncontext of continuously arriving unbounded data because this implies the\ncontinuous generation of input data. In most cases, we can take the time of\nthe original event’s occurrence as its logical event timestamp. With all input\nmessages containing an event timestamp, we can then examine the\ndistribution of such timestamps in any pipeline. Such a pipeline might be\ndistributed to process in parallel over many agents and consuming input\nmessages with no guarantee of ordering between individual shards. Thus, the\nset of event timestamps for active in-flight messages in this pipeline will form\na distribution, as illustrated in \nFigure 3-1\n.\nMessages are ingested by the pipeline, processed, and eventually marked\ncompleted. Each message is either “in-flight,” meaning that it has been\nreceived but not yet completed, or “completed,” meaning that no more\nprocessing on behalf of this message is required. If we examine the\ndistribution of messages by event time, it will look\n something like \nFigure 3-1\n.\nAs time advances, more messages will be added to the “in-flight” distribution\non the right, and more of those messages from the “in-flight” part of the\ndistribution will be completed and moved into the “completed” distribution.\nFigure 3-1. \nDistribution of in-flight and completed message event times within a streaming\npipeline. New messages arrive as input and remain “in-flight” until processing for them\ncompletes. The leftmost edge of the “in-flight” distribution corresponds to the oldest\nunprocessed element at any given moment.\nThere is a key point on this distribution, located at the leftmost edge of the\n“in-flight” distribution, corresponding to the oldest event timestamp of any\nunprocessed message of our pipeline. We use this value to define the\nwatermark:\n00:00 / 00:00\n1",5109
24-Source Watermark Creation.pdf,24-Source Watermark Creation,"The watermark is a monotonically\n increasing timestamp of the oldest\nwork not yet completed.\nThere are two fundamental\n properties that are provided by this definition that\nmake it useful:\nCompleteness\nIf the watermark has advanced past some timestamp \nT\n, we are guaranteed\nby its monotonic property that no more processing will occur for on-time\n(nonlate data) events at or before \nT\n.\n Therefore, we can correctly emit any\naggregations at or before \nT\n. In other words, the watermark allows us to\nknow when it is correct to close a window.\nVisibility\nIf a message is stuck in our pipeline for any reason, the watermark cannot\nadvance. \nFurthermore, we will be able to find the source of the problem\nby examining the message that is preventing the watermark from\nadvancing.\nSource Watermark Creation\nWhere do these watermarks come from? To establish \na watermark for a data\nsource, we must assign a logical event timestamp to every message entering\nthe pipeline from that source. As \nChapter 2\n informs us, all watermark\ncreation falls\n into one \nof two broad categories: \nperfect\n or \nheuristic\n. To remind\nourselves about the difference between perfect and heuristic watermarks, let’s\nlook at \nFigure 3-2\n, which presents the windowed summation example from\nChapter 2\n.\n1\nFigure 3-2. \nWindowed summation with perfect (left) and heuristic (right) watermarks\n00:00 / 00:00",1422
25-Heuristic Watermark Creation.pdf,25-Heuristic Watermark Creation,"Notice that the distinguishing feature is that perfect watermarks ensure that\nthe watermark accounts for \nall\n data, whereas heuristic watermarks admit\nsome late-data elements.\nAfter the watermark is created as either perfect or heuristic, watermarks\nremain so throughout the rest of the pipeline. As to what makes watermark\ncreation perfect or heuristic, it depends a great deal on the nature of the\nsource that’s being consumed. To see why, let’s look at a few examples of\neach type of watermark creation.\nPerfect Watermark Creation\nPerfect watermark creation assigns timestamps to incoming messages in such\na way that the \nresulting\n watermark is a \nstrict guarantee\n that no data with\nevent times less than the watermark will ever be seen again from this source.\nPipelines using perfect watermark creation never have to deal with late data;\nthat is, data that arrive after the watermark has advanced past the event times\nof newly arriving messages. However, perfect watermark creation requires\nperfect knowledge of the input, and thus is impractical for many real-world\ndistributed input sources. Here are a couple of examples of use cases that can\ncreate perfect watermarks:\nIngress timestamping\nA source that assigns ingress times as the event times for data entering the\nsystem can create a perfect watermark.\n In this case, the source watermark\nsimply tracks the current processing time as observed by the pipeline.\nThis is essentially the method that nearly all streaming systems\nsupporting windowing prior to 2016 used.\nBecause event times are assigned from a single, monotonically increasing\nsource (actual processing time), the system thus has perfect knowledge\nabout which timestamps will come next in the stream of data. As a result,\nevent-time progress and windowing semantics become vastly easier to\nreason about. The downside, of course, is that the watermark has no\ncorrelation to the event times of the data themselves; those event times\nwere effectively discarded, and the watermark instead merely tracks the\nprogress of data relative to its arrival in the system.\nStatic sets of time-ordered logs\nA statically sized\n input source of time-ordered logs (e.g., an Apache\nKafka topic with a static set of partitions, where each \npartition of the\nsource contains monotonically increasing event times) would be\nrelatively straightforward source atop which to create a perfect\nwatermark. To do so, the source would simply track the minimum event\ntime of unprocessed data across the known and static set of source\npartitions (i.e., the minimum of the event times of the most recently read\nrecord in each of the partitions).\nSimilar to the aforementioned ingress timestamps, the system has perfect\nknowledge about which timestamps will come next, thanks to the fact that\nevent times across the static set of partitions are known to increase\nmonotonically. This is effectively a form of bounded out-of-order\nprocessing; the amount of disorder across the known set of partitions is\nbounded by the minimum observed event time among those partitions.\nTypically, the only way you can guarantee monotonically increasing\ntimestamps within partitions is if the timestamps within those partitions\nare assigned as data are written to it; for example, by web frontends\nlogging events directly into Kafka. Though still a limited use case, this is\ndefinitely a much more useful one than ingress timestamping upon arrival\nat the data processing system because the watermark tracks meaningful\nevent times of the underlying data.\nHeuristic Watermark Creation\nHeuristic watermark creation, on the other hand, creates a watermark that is\nmerely an \nestimate\n that no data with event times less than the watermark will\never be seen again.\n Pipelines using heuristic watermark creation might need\nto deal with some amount of \nlate data\n.\n Late data is any data that arrives after\nthe watermark has advanced past the event time of this data. Late data is only\npossible with heuristic watermark creation. If the heuristic is a reasonably\n2\ngood one, the amount of late data might be very small, and the watermark\nremains useful as a completion estimate. The system still needs to provide a\nway for the user to cope with late data if it’s to support use cases requiring\ncorrectness (e.g., things like billing).\nFor many real-world, distributed input sources, it’s computationally or\noperationally impractical to construct a perfect watermark, but still possible\nto build a highly accurate heuristic watermark by taking advantage of\nstructural features of the input data source. Following are two example for\nwhich heuristic watermarks (of varying quality) are possible:\nDynamic sets of time-ordered logs\nConsider a dynamic set of structured log files (each individual file\ncontaining records with monotonically increasing event times relative to\nother records in the same\n file but with\n no fixed relationship of event times\nbetween files), where the full set of expected log files (i.e., partitions, in\nKafka parlance) is not known at runtime. Such inputs are often found in\nglobal-scale services constructed and managed by a number of\nindependent teams. In such a use case, creating a perfect watermark over\nthe input is intractable, but creating an accurate heuristic watermark is\nquite possible.\nBy tracking the minimum event times of unprocessed data in the existing\nset of log files, monitoring growth rates, and utilizing external\ninformation like network topology and bandwidth availability, you can\ncreate a remarkably accurate watermark, even given the lack of perfect\nknowledge of all the inputs. This type of input source is one of the most\ncommon types of unbounded datasets found at Google, so we have\nextensive experience with creating and analyzing watermark quality for\nsuch scenarios and have seen them used to good effect across a number of\nuse cases.\nGoogle Cloud Pub/Sub\nCloud Pub/Sub is an interesting use case.\n Pub/Sub currently \nmakes no\nguarantees on in-order delivery; even if a single publisher publishes two\nmessages in order, there’s a chance (usually small) that they might be\ndelivered out of order (this is due to the dynamic nature of the underlying\narchitecture, which allows for transparent scaling up to very high levels\nof throughput with zero user intervention). As a result, there’s no way to\nguarantee a perfect watermark for Cloud Pub/Sub. The Cloud Dataflow\nteam has, however, built a reasonably accurate heuristic watermark by\ntaking advantage of what knowledge \nis\n available about the data in Cloud\nPub/Sub. The implementation of this heuristic is discussed at length as a\ncase study later in this chapter.\nConsider an example where users play a mobile game, and their scores are\nsent to our pipeline for processing: you can generally assume that for any\nsource utilizing mobile devices for input it will be generally impossible to\nprovide a perfect watermark. Due to the problem of devices that go offline for\nextended periods of time, there’s just no way to provide any sort of\nreasonable estimate of absolute completeness for such a data source. You\ncan, however, imagine building a watermark that accurately tracks input\ncompleteness for devices that are currently online, similar to the Google\nPub/Sub watermark described a moment ago. Users who are actively online\nare likely the most relevant subset of users from the perspective of providing\nlow-latency results anyway, so this often isn’t as much of a shortcoming as\nyou might initially think.\nWith heuristic watermark creation, broadly speaking, the more that is known\nabout the source, the better the heuristic, and the fewer late data items will be\nseen. There is no one-size-fits-all solution, given that the types of sources,\ndistributions of events, and usage patterns will vary greatly. But in either case\n(perfect or heuristic), after a watermark is created at the input source, the\nsystem can propagate the watermark through the pipeline perfectly. This\nmeans perfect watermarks will remain perfect downstream, and heuristic\nwatermarks will remain strictly as heuristic as they were when established.\nThis is the benefit of the watermark approach: you can reduce the complexity\nof tracking completeness in a pipeline entirely to the problem of creating a\nwatermark at the source.",8427
26-Watermark Propagation.pdf,26-Watermark Propagation,"Watermark Propagation\nSo far, we have considered only the watermark for the inputs within the\ncontext of a single operation or stage. \nHowever, most real-world pipelines\nconsist of multiple stages. Understanding how watermarks propagate across\nindependent stages is important in understanding how they affect the pipeline\nas a whole and the observed latency of its results.\nPIPELINE STAGES\nDifferent stages are typically necessary every time your pipeline groups\ndata together by some new dimension. For example, if you had a pipeline\nthat consumed raw data, computed some per-user aggregates, and then\nused those per-user aggregates to compute some per-team aggregates,\nyou’d likely end up with a three-stage pipeline:\nOne consuming the raw, ungrouped data\nOne grouping the data by user and computing per-user\naggregates\nOne grouping the data by team and computing per-team\naggregates\nWe learn more about the effects of grouping on pipeline shapes in\nChapter 6\n.\nWatermarks are created at input sources, as discussed in the preceding\nsection. They then conceptually flow through the system as data progress\nthrough it.\n You can track watermarks at varying levels of granularity. For\npipelines comprising multiple distinct stages, each stage likely tracks its own\nwatermark, whose value is a function of all the inputs and stages that come\nbefore it. Therefore, stages that come later in the pipeline will have\nwatermarks that are further in the past (because they’ve seen less of the\noverall input).\nWe can define watermarks at the boundaries of any single operation, or stage,\n3\nin the pipeline. This is useful not only in understanding the relative progress\nthat each stage in the pipeline is making, but for dispatching timely results\nindependently and as soon as possible for each individual stage. We give the\nfollowing definitions for the watermarks at the boundaries of stages:\nAn \ninput watermark\n, which captures the progress of everything\nupstream of that stage (i.e., how complete the input is for that stage).\nFor sources, the input watermark is a source-specific function\ncreating the watermark for the input data. For nonsource stages, the\ninput watermark is defined as the minimum of the output\nwatermarks of all shards/partitions/instances of all of its upstream\nsources and stages.\nAn \noutput watermark\n, which captures the progress \nof the stage itself,\nand is essentially defined as the minimum of the stage’s input\nwatermark and the event times of all nonlate data active messages\nwithin the stage. Exactly what “active” encompasses is somewhat\ndependent upon the operations a given stage actually performs, and\nthe implementation of the stream processing system. It typically\nincludes data buffered for aggregation but not yet materialized\ndownstream, pending output data in flight to downstream stages, and\nso on.\nOne nice feature of defining an input and output watermark for a specific\nstage is that we can use these to calculate the amount of event-time latency\nintroduced by a stage. Subtracting the value of a stage’s output watermark\nfrom the value of its input watermark gives the amount of event-time latency\nor \nlag\n introduced by the stage. This lag is the notion of how far delayed\nbehind real time the output of each stage will be. As an example, a stage\nperforming 10-second windowed aggregations will have a lag of 10 seconds\nor more, meaning that the output of the stage will be at least that much\ndelayed behind the input and real time. Definitions of input and output\nwatermarks provide a recursive relationship of watermarks throughout a\npipeline. Each subsequent stage in a pipeline delays the watermark as\nnecessary, based on event-time lag of the stage.\nProcessing within each stage is also not monolithic. We can segment the\nprocessing within one stage into a flow with several conceptual components,\neach of which contributes to the output watermark. As mentioned previously,\nthe exact nature of these components depends on the operations the stage\nperforms and the implementation of the system. Conceptually, each such\ncomponent serves as a buffer where active messages can reside until some\noperation has completed. For example, as data arrives, it is buffered for\nprocessing. Processing might then write the data to state for later delayed\naggregation. Delayed aggregation, when triggered, might write the results to\nan output buffer awaiting consumption from a downstream stage, as shown in\nFigure 3-3\n.\nFigure 3-3. \nExample system components of a streaming system stage, containing buffers of\nin-flight data. Each will have associated watermark tracking, and the overall output\nwatermark of the stage will be the minimum of the watermarks across all such buffers.\nWe can track each such buffer with its own watermark. The minimum of the\nwatermarks across the buffers of each stage forms the output watermark of\nthe stage.\n Thus the output watermark could be the minimum of the following:\nPer-source\n watermark—for each sending stage.\nPer-external input\n watermark—for sources external to the pipeline\nPer-state component\n watermark—for each type of state that can be\nwritten",5222
27-Understanding Watermark Propagation.pdf,27-Understanding Watermark Propagation,"Per-output buffer\n watermark—for each receiving stage\nMaking watermarks available at this level of granularity also provides better\nvisibility into the behavior of the system. The watermarks track locations of\nmessages across various buffers in the system, allowing for easier diagnosis\nof stuckness.\nUnderstanding Watermark Propagation\nTo get a better sense for the relationship between input and output\nwatermarks and how they affect watermark propagation, let’s look at an\nexample. Let’s consider gaming scores, but instead of computing sums of\nteam scores, we’re going to take a stab at measuring user engagement levels.\nWe’ll do this by first calculating per-user session lengths, under the\nassumption that the amount of time a user stays engaged with the game is a\nreasonable proxy for how much they’re enjoying it. After answering our four\nquestions once to calculate sessions lengths, we’ll then answer them a second\ntime to calculate average session lengths within fixed periods of time.\nTo make our example even more interesting, lets say that we are working\nwith two datasets, one for Mobile Scores and one for Console Scores. We\nwould like to perform identical score calculations via integer summation in\nparallel over these two independant datasets. One pipeline is calculating\nscores for users playing on mobile devices, whereas the other is for users\nplaying on home gaming consoles, perhaps due to different data collection\nstrategies employed for the different platforms. The important point is that\nthese two stages are performing the same operation but over different data,\nand thus with very different output watermarks.\nTo begin, \nlet’s take a look at \nExample 3-1\n to see what the abbreviated code\nfor what the first section of this pipeline might be like.\nExample 3-1. \nCalculating session lengths\nPCollection<Double> mobileSessions = IO.read(new MobileInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\nPCollection<Double> consoleSessions = IO.read(new ConsoleInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\nHere, we read in each of our inputs independently, and whereas previously\nwe were keying our collections by team, in this example we key by user.\nAfter that, for the first stage of each pipeline, we window into sessions and\nthen call a custom \nPTransform\n named \nCalculateWindowLength\n. This\nPTransform\n simply groups by key (i.e., \nUser\n) and then computes the per-\nuser session length by treating the size of the current window as the value for\nthat window. In this case, we’re fine with the default trigger (\nAtWatermark\n)\nand accumulation mode (\ndiscardingFiredPanes\n) settings, but I’ve listed\nthem explicitly for completeness. The output for each pipeline for two\nparticular users might look something like \nFigure 3-4\n.\nFigure 3-4. \nPer-user session lengths across two different input pipelines\nBecause we need to track data across multiple stages, we track everything\nrelated to Mobile Scores in red, everything related to Console Scores in blue,\nwhile the watermark and output for Average Session Lengths in \nFigure 3-5\nare yellow.\nWe have answered the four questions of \nwhat\n, \nwhere\n, \nwhen\n, and \nhow\n to\ncompute individual session lengths. Next we’ll answer them a second time to\ntransform those session lengths into global session-length averages within\n00:00 / 00:00\nfixed windows of time. This requires us to first flatten our two data sources\ninto one, and then re-window into fixed windows; we’ve already captured the\nimportant essence of the session in the session-length value we computed,\nand we now want to compute a global average of those sessions within\nconsistent windows of time over the course of the day. \nExample 3-2\n shows\nthe code for this.\nExample 3-2. \nCalculating session lengths\nPCollection<Double> mobileSessions = IO.read(new MobileInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\nPCollection<Double> consoleSessions = IO.read(new ConsoleInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\n  \nPCollection<Float> averageSessionLengths = PCollectionList\n  .of(mobileSessions).and(consoleSessions)\n  .apply(Flatten.pCollections())\n  .apply(\nWindow.into(FixedWindows.of(Duration.standardMinutes(2)))\n               \n.triggering(AtWatermark())\n  .apply(\nMean.globally()\n);\nIf we were to see this pipeline in action, it would look something like\nFigure 3-5\n. As before, the two input pipelines are computing individual\nsession lengths for mobile and console players. Those session lengths then\nfeed into the second stage of the pipeline, where global session-length\naverages are computed in fixed windows.\nFigure 3-5. \nAverage session lengths of mobile and console gaming sessions\nLet’s walk through some of this example, given that there’s a lot going on.\nThe two important points here are:\nThe \noutput watermark\n for each of the Mobile Sessions and Console\nSessions stages is at least as old as the corresponding input\nwatermark of each, and in reality a little bit older.\n This is because in\na real system computing answers takes time, and we don’t allow the\noutput watermark to advance until processing for a given input has\ncompleted.\nThe \ninput watermark\n for the Average Session Lengths stage is the\nminimum of the output watermarks \nfor the two stages directly\nupstream.\nThe result is that the downstream input watermark is an alias for the\nminimum composition of the upstream output watermarks. Note that this\nmatches the definitions for those two types of watermarks earlier in the\nchapter. Also notice how watermarks further downstream are further in the\npast, capturing the intuitive notion that upstream stages are going to be\nfurther ahead in time than the stages that follow them.\nOne observation worth making here is just how cleanly we were able to ask\nthe questions again in \nExample 3-1\n to substantially alter the results of the\npipeline. Whereas before we simply computed per-user session lengths, we\nnow compute two-minute global session-length averages. This provides a\nmuch more insightful look into the overall behaviors of the users playing our\ngames and gives you a tiny glimpse of the difference between simple data\ntransformations and real data science.\n00:00 / 00:00",7004
28-Watermark Propagation and Output Timestamps.pdf,28-Watermark Propagation and Output Timestamps,"Even better, now that we understand the basics of how this pipeline operates,\nwe can look more closely at one of the more subtle issues related to asking\nthe four questions over again: \noutput timestamps\n.\nWatermark Propagation and Output Timestamps\nIn \nFigure 3-5\n, I glossed over some of the details of output timestamps. \nBut if\nyou look closely at the second stage in the diagram, you can see that each of\nthe outputs from the first stage was assigned a timestamp that matched the\nend of its window. Although that’s a fairly natural choice for output\ntimestamps, it’s not the only valid choice. As you know from earlier in this\nchapter, watermarks are never allowed to move backward. Given that\nrestriction, you can infer that the range of valid timestamps for a given\nwindow begins with the timestamp of the earliest nonlate record in the\nwindow (because only nonlate records are guaranteed to hold a watermark\nup) and extends all the way to positive infinity. That’s quite a lot of options.\nIn practice, however, there tend to be only a few choices that make sense in\nmost circumstances:\nEnd of the window\nUsing the end of the window is the only safe choice if you want the\noutput timestamp to be representative of the window bounds. \nAs we’ll see\nin a moment, it also allows the smoothest watermark progression out of\nall of the options.\nTimestamp of first nonlate element\nUsing the timestamp of the first nonlate element is a good choice when\nyou want to keep your watermarks as conservative as possible. The trade-\noff, however, is that watermark progress will likely be more hindered, as\nwe’ll also see shortly.\nTimestamp of a specific element\nFor certain use cases, the timestamp of some other arbitrary (from the\nsystem’s perspective) element is the right choice. Imagine a use case in\nwhich you’re joining a stream of queries to a stream of clicks on results\n4\nfor that query. After performing the join, some systems will find the\ntimestamp of the query to be more useful; others will prefer the\ntimestamp of the click. Any such timestamp is valid from a watermark\ncorrectness perspective, as long as it corresponded to an element that did\nnot arrive late.\nHaving thought a bit about some alternate options for output timestamps,\nlet’s look at what effects the choice of output timestamp can have on the\noverall pipeline. To make the changes as dramatic as possible, in \nExample 3-\n3\n and \nFigure 3-6\n, we’ll switch to using the earliest timestamp possible for the\nwindow: the timestamp of the first nonlate element as the timestamp for the\nwindow. \nExample 3-3. \nAverage session lengths pipeline, that output timestamps for\nsession windows set at earliest element\nPCollection<Double> mobileSessions = IO.read(new MobileInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.withTimestampCombiner(EARLIEST)\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\nPCollection<Double> consoleSessions = IO.read(new ConsoleInputSource())\n  \n.apply(\nWindow.into(Sessions.withGapDuration(Duration.standardMinutes(1)))\n               \n.triggering(AtWatermark())\n               \n.withTimestampCombiner(EARLIEST)\n               \n.discardingFiredPanes()\n)\n  .apply(\nCalculateWindowLength()\n);\n  \nPCollection<Float> averageSessionLengths = PCollectionList\n  .of(mobileSessions).and(consoleSessions)\n  .apply(Flatten.pCollections())\n  .apply(\nWindow.into(FixedWindows.of(Duration.standardMinutes(2)))\n               \n.triggering(AtWatermark())\n  .apply(\nMean.globally()\n);\nFigure 3-6. \nAverage session lengths for sessions that are output at the timestamp of the\nearliest element\nTo help call out the effect of the output timestamp choice, look at the dashed\nlines in the first stages showing what the output watermark for each stage is\nbeing held to. The output watermark is delayed by our choice of timestamp,\nas compared to Figures \n3-7\n and \n3-8\n, in which the output timestamp was\nchosen to be the end of the window. You can see from this diagram that the\ninput watermark of the second stage is thus subsequently also delayed.\nFigure 3-7. \nComparison of watermarks and results with different choice of window outout\n00:00 / 00:00\ntimestamps. The watermarks in this figure correspond to output timestamps at the end of\nthe session windows (i.e., \nFigure 3-5\n).\nFigure 3-8. \nIn this figure, the watermarks are at the beginning of the session windows (i.e.,\nFigure 3-6\n). We can see that the watermark line in this figure is more delayed, and the\nresulting average session lengths are different.\nAs far as differences in this version compared to \nFigure 3-7\n, two are worth\nnoting:\nWatermark delay\nCompared to \nFigure 3-5\n, the watermark proceeds much more slowly in\nFigure 3-6\n. This is because the output watermark for the first stage is held\nback to the timestamp of the first element in every window until the input\nfor that window becomes complete. Only after a given window has been\nmaterialized is the output watermark (and thus the downstream input\nwatermark) allowed to advance.\nSemantic differences",5266
29-Percentile Watermarks.pdf,29-Percentile Watermarks,"Because the session timestamps are now assigned to match the earliest\nnonlate element in the session, the individual sessions often end up in\ndifferent fixed window buckets when we then calculate the session-length\naverages in the next stage. There’s nothing inherently right or wrong\nabout either of the two options we’ve seen so far; they’re just different.\nBut it’s important to understand that they \nwill\n be different as well as have\nan intuition for the way in which they’ll be different so that you can make\nthe correct choice for your specific use case when the time comes.\nThe Tricky Case of Overlapping Windows\nOne additional subtle but important issue regarding output timestamps is how\nto handle sliding windows.\n The naive approach of setting the output\ntimestamp to the earliest element can very easily lead to delays downstream\ndue to watermarks being (correctly) held back. To see why, consider an\nexample pipeline with two stages, each using the same type of sliding\nwindows. Suppose that each element ends up in three successive windows.\nAs the input watermark advances, the desired semantics for sliding windows\nin this case would be as follows:\nThe first window completes in the first stage and is emitted\ndownstream.\nThe first window then completes in the second stage and can also be\nemitted downstream.\nSome time later, the second window completes in the first stage…\nand so on.\nHowever, if output timestamps are chosen to be the timestamp of the first\nnonlate element in the pane, what actually happens is the following:\nThe first window completes in the first stage and is emitted\ndownstream.\nThe first window in the second stage remains unable to complete\nbecause its input watermark is being held up by the output\nwatermark of the second and third windows upstream. Those\nwatermarks are rightly being held back because the earliest element\ntimestamp is being used as the output timestamp for those windows.\nThe second window completes in the first stage and is emitted\ndownstream.\nThe first and second windows in the second stage remain unable to\ncomplete, held up by the third window upstream.\nThe third window completes in the first stage and is emitted\ndownstream.\nThe first, second, and third windows in the second stage are now all\nable to complete, finally emitting all three in one swoop.\nAlthough the results of this windowing are correct, this leads to the results\nbeing materialized in an unnecessarily delayed way. Because of this, Beam\nhas special logic for overlapping windows that ensures the output timestamp\nfor window \nN\n+1 is always greater than the end of window \nN\n.\nPercentile Watermarks\nSo far, we have concerned ourselves with watermarks as measured by the\nminimum event time of active messages in a stage.\n Tracking the minimum\nallows the system to know when all earlier timestamps have been accounted\nfor. On the other hand, we could consider the entire distribution of event\ntimestamps for active messages and make use of it to create finer-grained\ntriggering conditions.\nInstead of considering the minimum point of the distribution, we could take\nany percentile of the distribution and say that we are guaranteed to have\nprocessed this percentage of all events with earlier timestamps.\nWhat is the advantage of this scheme? If for the business logic “mostly”\ncorrect is sufficient, percentile watermarks provide a mechanism by which\n5\nthe watermark can advance more quickly and more smoothly than if we were\ntracking the minimum event time by discarding outliers in the long tail of the\ndistribution from the watermark. \nFigure 3-9\n shows a compact distribution of\nevent times where the 90\n percentile watermark is close to the 100\npercentile. \nFigure 3-10\n demonstrates a case where the outlier is further\nbehind, so the 90\n percentile watermark is significantly ahead of the 100\npercentile. By discarding the outlier data from the watermark, the percentile\nwatermark can still keep track of the bulk of the distribution without being\ndelayed by the outliers.\nFigure 3-9. \nNormal-looking watermark histogram\nFigure 3-10. \nWatermark histogram with outliers\nth\nth\nth\nth",4209
30-Processing-Time Watermarks.pdf,30-Processing-Time Watermarks,"Figure 3-11\n shows an example of percentile watermarks used to draw\nwindow boundaries for two-minute fixed windows. We can draw early\nboundaries based on the percentile of timestamps of arrived data as tracked\nby the percentile watermark.\nFigure 3-11. \nEffects of varying watermark percentiles. As the percentile increases, more\nevents are included in the window: however, the processing time delay to materialize the\nwindow also increases.\nFigure 3-11\n shows the 33\n percentile, 66\n percentile, and 100\n percentile\n(full) watermark, tracking the respective timestamp percentiles in the data\ndistribution. As expected, these allow boundaries to be drawn earlier than\ntracking the full 100\n percentile watermark. Notice that the 33\n and 66\npercentile watermarks each allow earlier triggering of windows but with the\ntrade-off of marking more data as late. For example, for the first window,\n[12:00, 12:02), a window closed based on the 33\n percentile watermark\nwould include only four events and materialize the result at 12:06 processing\ntime. If we use the 66\n percentile watermark, the same event-time window\nwould include seven events, and materialize at 12:07 processing time. Using\nthe 100\n percentile watermark includes all ten events and delays\nmaterializing the results until 12:08 processing time. Thus, percentile\nwatermarks provide a way to tune the trade-off between latency of\nmaterializing results and precision of the results.\nProcessing-Time Watermarks\nUntil now, we have been looking at watermarks as they relate to the data\nflowing through our system.\n We have seen how looking at the watermark can\nhelp us identify the overall delay between our oldest data and real time.\n00:00 / 00:00\nrd\nth\nth\nth\nrd\nth\nrd\nth\nth\nHowever, this is not enough to distinguish between old data and a delayed\nsystem. In other words, by only examining the event-time watermark as we\nhave defined it up until now, we cannot distinguish between a system that is\nprocessing data from an hour ago quickly and without delay, and a system\nthat is attempting to process real-time data and has been delayed for an hour\nwhile doing so.\nTo make this distinction, we need something more: processing-time\nwatermarks. We have already seen that there are two time domains in a\nstreaming system: processing time and event time. Until now, we have\ndefined the watermark entirely in the event-time domain, as a function of\ntimestamps of the data flowing through the system. This is an event-time\nwatermark. We will now apply the same model to the processing-time\ndomain to define a processing-time watermark.\nOur stream processing system is constantly performing operations such as\nshuffling messages between stages, reading or writing messages to persistent\nstate, or triggering delayed aggregations based on watermark progress. All of\nthese operations are performed in response to previous operations done at the\ncurrent or upstream stage of the pipeline. Thus, just as data elements “flow”\nthrough the system, a cascade of operations involved in processing these\nelements also “flows” through the system.\nWe define the processing-time watermark in the exact same way as we have\ndefined the event-time watermark, except instead of using the event-time\ntimestamp of oldest work not yet completed, we use the processing-time\ntimestamp of the oldest operation not yet completed. An example of delay to\nthe processing-time watermark could be a stuck message delivery from one\nstage to another, a stuck I/O call to read state or external data, or an exception\nwhile processing that prevents processing from completing.\nThe processing-time watermark, therefore, provides a notion of processing\ndelay separate from the data delay. To understand the value of this\ndistinction, consider the graph in \nFigure 3-12\n where we look at the event-time\nwatermark delay.\nWe see that the data delay is monotonically increasing, but there is not\nenough information to distinguish between the cases of a stuck system and\nstuck data. Only by looking at the processing-time watermark, shown in\nFigure 3-13\n, can we distinguish the cases.\nFigure 3-12. \nEvent-time watermark increasing. It is not possible to know from this\ninformation whether this is due to data buffering or system processing delay.\nFigure 3-13. \nProcessing-time watermark also increasing. This indicates that the system\nprocessing is delayed.\nIn the first case (\nFigure 3-12\n), when we examine the processing-time\nwatermark delay we see that it too is increasing. This tells us that an\noperation in our system is stuck, and the stuckness is also causing the data\ndelay to fall behind. Some real-world examples of situations in which this\nmight occur are when there is a network issue preventing message delivery\nbetween stages of a pipeline or if a failure has occurred and is being retried.\nIn general, a growing processing-time watermark indicates a problem that is\npreventing operations from completing that are necessary to the system’s\nfunction, and often involves user or administrator intervention to resolve.\nIn this second case, as seen in \nFigure 3-14\n, the processing-time watermark\ndelay is small. This tells us that there are no stuck operations. \nThe event-time\nwatermark delay is still increasing, which indicates that we have some\nbuffered state that we are waiting to drain.\n This is possible, for example, if\nwe are buffering some state while waiting for a window boundary to emit an\naggregation, and corresponds to a normal operation of the pipeline, as in\nFigure 3-15\n.\nFigure 3-14. \nEvent-time watermark delay increasing, processing-time watermark stable.\nThis is an indication that data are buffered in the system and waiting to be processed,\nrather than an indication that a system operation is preventing data processing from\ncompleting.\nFigure 3-15. \nWatermark delay for fixed windows. The event-time watermark delay\nincreases as elements are buffered for each window, and decreases as each window’s\naggregate is emitted via an on-time trigger, whereas the processing-time watermark simply\ntracks system-level delays (which remain relatively steady in a healthy pipeline).\nTherefore, the processing-time watermark is a useful tool in distinguishing\nsystem latency from data latency. In addition to visibility, we can use the\nprocessing-time watermark at the system-implementation level for tasks such\nas garbage collection of \ntemporary \nstate (Reuven talks more about an",6552
31-Case Studies.pdf,31-Case Studies,,0
32-Case Study Watermarks in Google Cloud Dataflow.pdf,32-Case Study Watermarks in Google Cloud Dataflow,"example of this in \nChapter 5\n).\nCase Studies\nNow that we’ve laid the groundwork for how watermarks ought to behave,\nit’s time to take a look at some real systems to understand how different\nmechanisms of the watermark are implemented.\n We hope that these shed\nsome light on the trade-offs that are possible between latency and correctness\nas well as scalability and availability for watermarks in real-world systems.\nCase Study: Watermarks in Google Cloud\nDataflow\nThere are many possible approaches to implementing watermarks in a stream\nprocessing system. \nHere, we present a quick survey of the implementation in\nGoogle Cloud Dataflow, a fully managed service for executing Apache Beam\npipelines. Dataflow includes SDKs for defining data processing workflows,\nand a Cloud Platform managed service to run those workflows on Google\nCloud Platform resources.\nDataflow stripes (shards) each of the data processing steps in its data\nprocessing graph across multiple physical workers by splitting the available\nkeyspace of each worker into key ranges and assigning each range to a\nworker. Whenever a \nGroupByKey\n operation with distinct keys is encountered,\ndata must be shuffled to corresponding keys.\nFigure 3-16\n depicts a logical representation of the processing graph with a\nGroupByKey\n.\nFigure 3-16. \nA GroupByKey step consumes data from another DoFn. This means that there\nis a data shuffle between the keys of the first step and the keys of the second step.\nWhereas the physical assignment of key ranges to workers might look\nFigure 3-17\n.\nFigure 3-17. \nKey ranges of both steps are assigned (striped) across the available workers.\nIn the watermark propagation section, we discussed that the watermark is\nmaintained for multiple subcomponents of each step. Dataflow keeps track of\nthe per-range watermarks of each of these components. Watermark\naggregation then involves computing the minimum of each watermark across\nall ranges, ensuring that the following guarantees are met:\nAll ranges must be reporting a watermark. If a watermark is not\npresent for a range, we cannot advance the watermark, because a\nrange not reporting must be treated as unknown.\nEnsure that the watermark is monotonically increasing. Because late\ndata is possible, we must not update the watermark if it would cause\nthe watermark to move backward.\nGoogle Cloud Dataflow performs aggregation via a centralized aggregator\nagent. We can shard this agent for efficiency. From a correctness standpoint,\nthe watermark aggregator serves as a “single source of truth” about the\nwatermark.\nEnsuring correctness in distributed watermark aggregation poses certain\nchallenges. It is paramount that watermarks are not advanced prematurely\nbecause advancing the watermark prematurely will turn on-time data into late\ndata. Specifically, as physical assignments are actuated to workers, the\nworkers maintain leases on the persistent state attached to the key ranges,\nensuring that only a single worker may mutate the persistent state for a key.\nTo guarantee watermark correctness, we must ensure that each watermark\nupdate from a worker process is admitted into the aggregate only if the\nworker process still maintains a lease on its persistent state; therefore, the\nwatermark update protocol must take state ownership lease validation into\naccount.\nCase Study: Watermarks in Apache Flink\nApache Flink is an open source stream processing framework for distributed,\nhigh-performing, always-available, and accurate data streaming applications.\nIt is possible to run Beam programs using a Flink runner. In doing so, Beam",3650
33-Case Study Watermarks in Apache Flink.pdf,33-Case Study Watermarks in Apache Flink,"relies on the implementation of stream processing concepts such as\nwatermarks within Flink. Unlike Google Cloud Dataflow, which implements\nwatermark aggregation via a centralized watermark aggregator agent, Flink\nperforms watermark tracking and aggregation in-band.\nTo understand how this works, let’s look at a Flink pipeline, as shown in\nFigure 3-18\n.\nFigure 3-18. \nA Flink pipeline with two sources and event-time watermarks propagating in-\nband\nIn this pipeline data is generated at two sources. These sources also both\ngenerate watermark “checkpoints” that are sent synchronously in-band with\nthe data stream. This means that when a watermark checkpoint from source A\nfor timestamp “53” is emitted, it guarantees that no nonlate data messages\nwill be emitted from source A with timestamp behind “53”. The downstream\n“keyBy” operators consume the input data and the watermark checkpoints.\nAs new watermark checkpoints are consumed, the downstream operators’\nview of the watermark is advanced, and a new watermark checkpoint for\ndownstream operators can be emitted.\nThis choice to send watermark checkpoints in-band with the data stream\ndiffers from the Cloud Dataflow approach that relies on central aggregation\nand leads to a few interesting trade-offs.\n6\nFollowing are some\n advantages of in-band watermarks:\nReduced watermark propagation latency, and very low-latency watermarks\nBecause it is not necessary to have watermark data traverse multiple hops\nand await central aggregation, it is possible to achieve very low latency\nmore easily with the in-band approach.\nNo single point of failure for watermark aggregation\nUnavailability in the central watermark aggregation agent will lead to a\ndelay in watermarks across the entire pipeline. With the in-band\napproach, unavailability of part of the pipeline cannot cause watermark\ndelay to the entire pipeline.\nInherent scalability\nAlthough Cloud Dataflow scales well in practice, more complexity is\nneeded to achieve scalability with a centralized watermark aggregation\nservice versus implicit scalability with in-band watermarks.\nHere are some advantages of out-of-band watermark aggregation:\nSingle source of “truth”\nFor debuggability, monitoring, \nand other applications such as throttling\ninputs based on pipeline progress, it is advantageous to have a service that\ncan vend the values of watermarks rather than having watermarks implicit\nin the streams, with each component of the system having its own partial\nview.\nSource watermark creation\nSome source watermarks require global information. \nFor example,\nsources might be temprarily idle, have low data rates, or require out-of-\nband information about the source or other system components to\ngenerate the watermarks. This is easier to achieve in a central service.\n For\nan example see the case study that follows on source watermarks for\nGoogle Cloud Pub/Sub.",2928
34-Case Study Source Watermarks for Google Cloud PubSub.pdf,34-Case Study Source Watermarks for Google Cloud PubSub,"Case Study: Source Watermarks for Google Cloud\nPub/Sub\nGoogle Cloud Pub/Sub is a fully managed real-time messaging service that\nallows you to send and receive messages between independent\n applications.\nHere, we discuss how to create a reasonable heuristic watermark for data sent\ninto a pipeline via Cloud Pub/Sub.\nFirst, we need to describe a little about how Pub/Sub works. Messages are\npublished on Pub/Sub \ntopics\n. A particular topic can be subscribed to by any\nnumber of Pub/Sub \nsubscriptions\n. The same messages are delivered on all\nsubscriptions subscribed to a given topic. The method of delivery is for\nclients to \npull\n messages off the subscription, and to ack the receipt of\nparticular messages via provided IDs. Clients do not get to choose which\nmessages are pulled, although Pub/Sub does attempt to provide oldest\nmessages first, with no hard guarantees around this.\nTo build a heuristic, we make some assumptions about the source that is\nsending data into Pub/Sub. Specifically, we assume that the timestamps of the\noriginal data are “well behaved”; in other words, we expect a bounded\namount of out-of-order timestamps on the source data, before it is sent to\nPub/Sub. Any data that are sent with timestamps outside the allowed out-of-\norder bounds will be considered late data. In our current implementation, this\nbound is at least 10 seconds, meaning reordering of timestamps up to 10\nseconds before sending to Pub/Sub will not create late data. We call this\nvalue the \nestimation band\n. Another way to look at this is that when the\npipepline is perfectly caught up with the input, the watermark will be 10\nseconds behind real time to allow for possible reorderings from the source. If\nthe pipeline is backlogged, all of the backlog (not just the 10-second band) is\nused for estimating the watermark.\nWhat are the challenges we face with Pub/Sub? Because Pub/Sub does not\nguarantee ordering, we must have some kind of additional metadata to know\nenough about the backlog. Luckily, Pub/Sub provides a measurement of\nbacklog in terms of the “oldest unacknowledged publish timestamp.” This is\nnot the same as the event timestamp of our message, because Pub/Sub is\nagnostic to the application-level metadata being sent through it; instead, this\nis the timestamp of when the message was ingested by Pub/Sub.\nThis measurement is not the same as an event-time watermark. It is in fact the\nprocessing-time watermark for Pub/Sub message delivery. The Pub/Sub\npublish timestamps are not equal to the event timestamps, and in the case that\nhistorical (past) data are being sent, it might be arbitrarily far away. The\nordering on these timestamps might also be different because, as mentioned\nearlier, we allow a limited amount of reordering.\nHowever, we can use this as a measure of backlog to learn enough\ninformation about the event timestamps present in the backlog so that we can\ncreate a reasonable watermark as follows.\nWe create two subscriptions to the topic containing\n the input messages: a\nbase subscription\n that the\n pipeline will actually use to read the data to be\nprocessed, and a \ntracking subscription\n, which is used for metadata only, to\nperform the watermark estimation.\nTaking a look at our base subscription in \nFigure 3-19\n, we see that messages\nmight arrive out of order. We label each message with its Pub/Sub publish\ntimestamp “pt” and its event-time timestamp “et.” Note that the two time\ndomains can be unrelated.\nFigure 3-19. \nProcessing-time and event-time timestamps of messages arriving on a\nPub/Sub subscription\nSome messages on the base subscription are unacknowledged forming a\nbacklog. This might be due to them not yet being delivered or they might\nhave been delivered but not yet processed. Remember also that pulls from\nthis subscription are distributed across multiple shards. Thus, it is not\npossible to say just by looking at the base subscription what our watermark\nshould be.\nThe tracking subscription, seen in \nFigure 3-20\n, is used to effectively inspect\nthe backlog of the base subscription and take the minimum of the event\ntimestamps in the backlog. By maintaining little or no backlog on the\ntracking subscription, we can inspect the messages ahead of the base\nsubsciption’s oldest unacknowledged message.\nFigure 3-20. \nAn additional “tracking” subscription receiving the same messages as the\n“base” subscription\nWe stay caught up on the tracking subscription by ensuring that pulling from\nthis subscription is computationally inexpensive. Conversely, if we fall\nsufficiently behind on the tracking subscription, we will stop advancing the\nwatermark. To do so, we ensure that at least one of the following conditions\nis met:\nThe tracking subscription is sufficiently ahead of the base\nsubscription. Sufficiently ahead means that the tracking subscription\nis ahead by at least the estimation band. This ensures that any\nbounded reorder within the estimation band is taken into account.\nThe tracking subscription is sufficiently close to real time. In other\nwords, there is no backlog on the tracking subscription.\nWe acknowledge the messages on the tracking subscription as soon as\npossible, after we have durably saved metadata about the publish and event\ntimestamps of the messages. We store this metadata in a sparse histogram\nformat to minimize the amount of space used and the size of the durable\nwrites.\nFinally, we ensure that we have enough data to make a reasonable watermark\nestimate. We take a band of event timestamps we’ve read from our tracking\nsubscription with publish timestamps newer than the oldest unacknowledged\nof the base subscription, or the width of the estimation band. This ensures\nthat we consider all event timestamps in the backlog, or if the backlog is\nsmall, the most recent estimation band, to make a watermark estimate.\nFinally, the watermark value is computed to be the minimum event time in\nthe band.\nThis method is correct in the sense that all timestamps within the reordering\nlimit of 10 seconds at the input will be accounted for by the watermark and\nnot appear as late data. However, it produces possibly an overly conservative\nwatermark, one that advances “too slowly” in the sense described in\nChapter 2\n. Because we consider all messages ahead of the base subscription’s\noldest unacknowledged message on the tracking subscription, we can include\nevent timestamps in the watermark estimate for messages that have already\nbeen acknowledged.\nAdditionally, there are a few heuristics to ensure progress. This method\nworks well in the case of dense, frequently arriving data. In the case of sparse\nor infrequent data, there might not be enough recent messages to build a\nreasonable estimate. In the case that we have not seen data on the\nsubscription in more than two minutes (and there’s no backlog), we advance\nthe watermark to near real time. This ensures that the watermark and the\npipeline continue to make progress even if no more messages are\nforthcoming.",7088
35-Summary.pdf,35-Summary,"All of the above ensures that as long as source data-event timestamp\nreordering is within the estimation band, there will be no additional late data.\n \nSummary\nAt this point, we have explored how we can use the event times of messages\nto give a robust definition of progress in a stream processing system. We saw\nhow this notion of progress can subsequently help us answer the question of\nwhere\n in event time processing is taking place and \nwhen\n in processing time\nresults are materialized. Specifically, we looked at how watermarks are\ncreated at the sources, the points of data ingestion into a pipeline, and then\npropagated throughout the pipeline to preserve the essential guarantees that\nallow the questions of \nwhere\n and \nwhen\n to be answered. We also looked at the\nimplications of changing the output window timestamps on watermarks.\nFinally, we explored some real-world system considerations when building\nwatermarks at scale.\nNow that we have a firm footing in how watermarks work under the covers,\nwe can take a dive into what they can do for us as we use windowing and\ntriggering to answer more complex queries in \nChapter 4\n.\n Note the additional mention of monotonicity; we have not yet discussed\nhow to achieve this. Indeed the discussion thus far makes no mention of\nmonotonicity. If we considered exclusively the oldest in-flight event time, the\nwatermark would not always be monotonic, as we have made no assumptions\nabout our input. We return to this discussion later on.\n To be precise, it’s not so much that the number of logs need be static as it is\nthat the number of logs at any given time be known a priori by the system. A\nmore sophisticated input source composed of a dynamically chosen number\nof inputs logs, such as \nPravega\n, could just as well be used for constructing a\nperfect watermark. It’s only when the number of logs that exist in the\ndynamic set at any given time is unknown (as in the example in the next\nsection) that one must fall back on a heuristic watermark.\n1\n2\n3\n Note that by saying “flow through the system,” I don’t necessarily imply\nthey flow along the same path as normal data. They might (as in Apache\nFlink), but they might also be transmitted out-of-band (as in\nMillWheel/Cloud Dataflow).\n The \nstart\n of the window is not a safe choice from a watermark correctness\nperspective because the first element in the window often comes \nafter\n the\nbeginning of the window itself, which means that the watermark is not\nguaranteed to have been held back as far as the start of the window.\n The percentile watermark triggering scheme described here is not currently\nimplemented by Beam; however, other systems such as MillWheel\nimplement this.\n For more information on Flink watermarks, see the \nFlink documentation on\nthe subject.\n3\n4\n5\n6",2852
36-4. Advanced Windowing.pdf,36-4. Advanced Windowing,,0
37-WhenWhere Processing-Time Windows.pdf,37-WhenWhere Processing-Time Windows,"Chapter 4. \nAdvanced Windowing\nHello again! I hope you enjoyed \nChapter 3\n as much as I did. Watermarks are\na fascinating topic, and Slava knows them better than anyone on the planet.\nNow that we have a deeper understanding of watermarks under our belts, I’d\nlike to dive into some more advanced topics related to the \nwhat\n, \nw\nhere\n, \nwhen\n,\nand \nhow\n questions. \nWe first look at \nprocessing-time windowing\n, which is an interesting \nmix of\nboth where and\n when, to understand better how it relates to event-time\nwindowing and get a sense for times when it’s actually the right approach to\ntake. We then dive into some more advanced event-time windowing\nconcepts, looking at \nsession windows\n in detail, and finally making a case for\nwhy generalized \ncustom windowing\n is a useful (and surprisingly\nstraightforward) concept by exploring three different types of custom\nwindows: \nunaligned\n fixed windows, \nper-key\n fixed windows, and \nbounded\nsessions windows.\nWhen\n/\nWhere\n: Processing-Time Windows\nProcessing-time windowing is important for two reasons:\nFor certain use cases, such as usage monitoring (e.g., web service\ntraffic QPS), for which you want to analyze an incoming stream of\ndata as it’s observed, processing-time windowing is absolutely the\nappropriate approach to take.\nFor use cases for which the time that events happened is important\n(e.g., analyzing user behavior trends, billing, scoring, etc.),\nprocessing-time windowing is absolutely the wrong approach to\ntake, and being able to recognize these cases is critical.\nAs such, it’s worth gaining a solid understanding of the differences between\nprocessing-time windowing and event-time windowing, particularly given the\nprevalence of processing-time windowing in many streaming systems today.\nWhen working within a model for which windowing\n as a first-class notion is\nstrictly event-time based, such as the one presented in this book, there are two\nmethods that you can use to achieve \nprocessing-time windowing:\nTriggers\nIgnore event\n time (i.e., use a global window spanning all of event time)\nand use triggers to provide snapshots of that window in the processing-\ntime axis.\nIngress time\nAssign ingress times as the event times for data as they arrive, and use\nnormal event-time windowing from there on. \nThis is essentially what\nsomething like Spark Streaming 1.x does.\nNote that the two methods are more or less equivalent, although they differ\nslightly in the case of multistage pipelines: in the triggers version, a\nmultistage pipeline will slice the processing-time “windows” independently\nat each stage, so, for example, data in window \nN\n for one stage might instead\nend up in window \nN\n–1 or \nN\n+1 in the following stage; in the ingress-time\nversion, after a datum is incorporated into window \nN\n, it will remain in\nwindow \nN\n for the duration of the pipeline due to synchronization of progress\nbetween stages via watermarks (in the Cloud Dataflow case), microbatch\nboundaries (in the Spark Streaming case), or whatever other coordinating\nfactor is involved at the engine level.\nAs I’ve noted to death, the big downside of processing-time windowing is\nthat the contents of the windows \nchange when the observation order of the\ninputs changes. To drive this point home in a more concrete manner, we’re\ngoing to look at these three use cases: \nevent-time\n windowing, \nprocessing-time\nwindowing via triggers, and \nprocessing-time\n windowing via ingress time.\nEach will be applied to two different input sets (so six variations total). The\ntwo inputs sets will be for the exact same events (i.e., same values, occurring\nat the same event times), but with different observation orders. \nThe first set",3786
38-Event-Time Windowing.pdf,38-Event-Time Windowing,"will be the observation order we’ve seen all along, colored white; the second\none will have all the values shifted in the processing-time axis as in \nFigure 4-\n1\n, colored purple. You can simply imagine that the purple example is another\nway reality could have happened if the winds had been blowing in from the\neast instead of the west (i.e., the underlying set of complex distributed\nsystems had played things out in a slightly different order).\nFigure 4-1. \nShifting input observation order in processing time, holding values, and event-\ntimes constant\nEvent-Time Windowing\nTo establish a baseline, let’s first compare fixed windowing in event time\nwith a heuristic watermark over these two observation orderings.\n We’ll reuse\nthe early/late code from \nExample 2-7\n/\nFigure 2-10\n to get the results shown in\nFigure 4-2\n. The lefthand side is essentially what we saw before; the righthand\nside is the results over the second observation order. The important thing to\nnote here is that even though the overall shape of the outputs differs (due to\nthe different orders of observation in processing time), \nthe final results for the\nfour windows remain the same\n: 14, 18, 3, and 12.\n00:00 / 00:00\nFigure 4-2. \nEvent-time windowing over two different processing-time orderings of the same\ninputs\nProcessing-Time Windowing via Triggers\nLet’s now compare this to the two processing-time methods just described.\nFirst, we’ll try the triggers method. \nThere are three aspects to making\n00:00 / 00:00",1529
39-Processing-Time Windowing via Ingress Time.pdf,39-Processing-Time Windowing via Ingress Time,"processing-time “windowing” work in this manner:\nWindowing\nWe use the global event-time window because we’re essentially\nemulating processing-time windows with event-time panes.\nTriggering\nWe trigger periodically in the processing-time domain based on the\ndesired size of the processing-time windows.\nAccumulation\nWe use discarding mode to keep \nthe panes independent from one another,\nthus letting each of them act like an independent processing-time\n“window.”\nThe corresponding code looks something like \nExample 4-1\n; note that global\nwindowing is the default in Beam, hence there is no specific override of the\nwindowing \nstrategy\n.\nExample 4-1. \nProcessing-time windowing via repeated, discarding panes of a\nglobal event-time window\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow\n.triggering(Repeatedly(AlignedDelay(ONE_MINUTE)))\n               \n.discardingFiredPanes()\n)\n  .apply(\nSum.integersPerKey()\n);\nWhen executed on a streaming runner against our two different orderings of\nthe input data, the results look like \nFigure 4-3\n. Here are some interesting\nnotes about this figure:\nBecause we’re emulating processing-time windows via event-time\npanes, the “windows” are delineated in the processing-time axis,\nwhich means their effective width is measured on the y-axis instead\nof the x-axis.\nBecause processing-time windowing is sensitive to the order that\ninput data are encountered, the results for each of the “windows”\ndiffers for each of the two observation orders, even though the\nevents themselves technically happened at the same times in each\nversion. On the left we get 12, 18, 18, whereas on the right we get 7,\n36, 5.\nFigure 4-3. \nProcessing-time “windowing” via triggers, over two different processing-time\norderings of the same inputs\nProcessing-Time Windowing via Ingress Time\nLastly, let’s look at\n processing-time windowing\n achieved by mapping\n the\nevent times of input data to be their ingress times.\n Code-wise, there are four\naspects worth mentioning here:\nTime-shifting\nWhen elements arrive, their event times need to be overwritten with the\ntime of ingress. We can do this in Beam by providing a new \nDoFn\n that\nsets the timestamp of the element to the current time via the\noutputWithTimestamp\n method.\nWindowing\nReturn to using standard event-time fixed windowing.\nTriggering\nBecause ingress time \naffords the ability to calculate a perfect watermark,\nwe can use the default trigger, which in this case implicitly fires exactly\nonce when the watermark passes the end of the window.\nAccumulation mode\nBecause we only ever have one output per window,\n the accumulation\n00:00 / 00:00\nmode is irrelevant.\nThe actual code might thus look something like that in \nExample 4-2\n.\nExample 4-2. \nProcessing-time windowing via repeated, discarding panes of a\nglobal event-time window\nPCollection<String> raw = IO.read()\n.apply(ParDo.of(\n  \nnew DoFn<String, String>() {\n    \npublic void processElement(ProcessContext c) {\n      \nc\n.outputWithTimestmap(new Instant())\n;\n    \n}\n  \n});\nPCollection<KV<Team, Integer>> input =\n  raw.apply(ParDo.of(new ParseFn());\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.info(FixedWindows.of(TWO_MINUTES)\n)\n  .apply(\nSum.integersPerKey()\n);\nExecution on a streaming engine would look like \nFigure 4-4\n. As data arrive,\ntheir event times are updated to match their ingress times (i.e., the processing\ntimes at arrival), resulting in a rightward horizontal shift onto the ideal\nwatermark line. Here are some interesting notes about this figure:\nAs with the other processing-time windowing example, we get\ndifferent results when the ordering of inputs changes, even though\nthe values and event times for the input stay constant.\nUnlike the other example, the windows are once again delineated in\nthe event-time domain (and thus along the x-axis). Despite this, they\naren’t bonafide event-time windows; we’ve simply mapped\nprocessing time onto the event-time domain, erasing the original\nrecord of occurrence for each input and replacing it with a new one\nthat instead represents the time the datum was first observed by the\npipeline.\nDespite this, thanks \nto the watermark, trigger firings still happen at\nexactly the same time as in the previous processing-time example.\nFurthermore, the output values produced are identical to that\nexample, as predicted: 12, 18, 18 on the left, and 7, 36, 5 on the",4510
40-Where Session Windows.pdf,40-Where Session Windows,"right.\nBecause perfect watermarks are possible when using ingress time,\nthe actual watermark matches the ideal watermark, ascending up and\nto the right with a slope of one.\nFigure 4-4. \nProcessing-time windowing via the use of ingress time, over two different\nprocessing-time orderings of the same inputs\nAlthough it’s interesting to see the different ways you can implement\nprocessing-time windowing, the big takeaway here is the one I’ve been\nharping on since the first chapter: event-time windowing is order-agnostic,\n at\nleast in the limit (actual panes along the way might differ until the input\nbecomes complete); processing-time windowing is not. \nIf you care about the\ntimes at which your events actually happened, you must use event-time\nwindowing or your results will be meaningless.\n I will get off my soapbox\nnow.\nWhere\n: Session Windows\nEnough with processing-time windowing.\n Let’s now \ngo back to tried-and-\ntrue event-time windowing, but now we’re going to look at one of my\nfavorite features: the dynamic, data-driven windows called \nsessions\n.\nSessions are a special type of window that captures a period of activity in the\ndata that is terminated by a gap of inactivity. They’re particularly useful in\ndata analysis because they can provide a view of the activities for a specific\nuser over a specific period of time during which they were engaged in some\nactivity. This allows for the correlation of activities within the session,\ndrawing inferences about levels of engagement based on the lengths of the\n00:00 / 00:00\nsessions, and so on.\nFrom a windowing perspective, sessions are particularly interesting in two\nways:\nThey are an \nexample of a \ndata-driven window\n: the location and sizes\nof the windows are a direct consequence of the input data\nthemselves, rather than being based on some predefined pattern\nwithin time, as are fixed and sliding windows.\nThey are also an example of an \nunaligned window\n; that is, a window\nthat does not apply uniformly across the data, but instead only to a\nspecific subset of the data (e.g., per user). This is in contrast to\naligned windows like fixed and sliding windows, which typically\napply uniformly across the data.\nFor some use cases, it’s possible to tag the data within a single session with a\ncommon identifier ahead of time (e.g., a video player that emits heartbeat\npings with quality-of-service information; for any given viewing, all of the\npings can be tagged ahead of time with a single session ID). In this case,\nsessions are much easier to construct because it’s basically just a form of\ngrouping by key.\nHowever, in the more general case (i.e., where the actual session itself is not\nknown ahead of time), the sessions must be constructed from the locations of\nthe data within time alone. When dealing with out-of-order data, this\nbecomes particularly tricky.\nFigure 4-5\n shows an example of this, with five independent records grouped\ntogether into session windows with a gap timeout of 60 minutes. Each record\nstarts out in a 60-minute window of its own (a proto-session). Merging\ntogether overlapping proto-sessions yields the two larger session windows\ncontaining three and two records, respectively.\nFigure 4-5. \nUnmerged proto-session windows, and the resultant merged sessions\nThey key insight in providing general session support is that a complete\nsession window is, by definition, a composition of a set of smaller,\noverlapping windows, each containing a single record, with each record in\nthe sequence separated from the next by a gap of inactivity no larger than a\npredefined timeout. Thus, even if we observe the data in the session out of\norder, we can build up the final session simply by merging together any\noverlapping windows for individual data as they arrive.\nTo look at this another way, consider the example we’ve been using so far. If\nwe specify a session timeout of one minute, we would expect to identify two\nsessions in the data, delineated in \nFigure 4-6\n by the dashed black lines. Each\nof those sessions captures a burst of activity from the user, with each event in\nthe session separate by less than one minute from at least one other event in\nthe session.\nFigure 4-6. \nSessions we want to compute\nTo see how the window merging works to build up these sessions over time\nas events are encountered, let’s look at it in action. We’ll take the early/late\ncode with retractions enabled from \nExample 2-10\n and update the windowing\nto build sessions with a one-minute gap duration timeout \ninstead. \nExample 4-\n3\n illustrates what this looks like.\nExample 4-3. \nEarly/on-time/late firings with session windows and retractions\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(\nSessions.withGapDuration(ONE_MINUTE)\n)\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n)\n  .apply(\nSum.integersPerKey()\n);\nExecuted on a streaming engine, you’d get something like that shown in\nFigure 4-7\n (note that I’ve left in the dashed black lines annotating the\nexpected final sessions for reference).\nFigure 4-7. \nEarly and late firings with session windows and retractions on a streaming\nengine\nThere’s quite a lot going on here, so I’ll walk you through some of it:\nWhen the first record with value 5 is encountered, it’s placed into a\nsingle proto-session window that begins at that record’s event time\nand spans the width of the session gap duration; for example, one\nminute beyond the point at which that datum occurred. Any\nwindows we encounter in the future that overlap this window should\nbe part of the same session and will be merged into it as such.\nThe second record to arrive is the 7, which similarly is placed into its\nown proto-session window, given that it doesn’t overlap with the\nwindow for the 5.\nIn the meantime, the watermark has passed the end of the first\nwindow, so the value of 5 is materialized as an on-time result just\nbefore 12:06. Shortly thereafter, the second window is also\nmaterialized as a speculative result with value 7, right as processing\ntime hits 12:06.\nWe next observe a pair of records 3 and 4, the proto-sessions for\nwhich overlap. As a result, they are merged together, and by the time\nthe early trigger for 12:07 fires, a single window with value 7 is\nemitted.\nWhen the 8 arrives shortly thereafter, it overlaps with both of the\nwindows with value 7. All three are thus merged together, forming a\nnew combined session with value 22. When the watermark then\npasses the end of this session, it materializes both the new session\n00:00 / 00:00",6773
41-Variations on Fixed Windows.pdf,41-Variations on Fixed Windows,"with value 22 as well as retractions for the two windows of value 7\nthat were previously emitted, but later incorporated into it.\nA similar dance occurs when the 9 arrives late, joining the proto-\nsession with value 5 and session with value 22 into a single larger\nsession of value 36. The 36 and the retractions for the 5 and 22\nwindows are all emitted immediately by the late data trigger.\nThis is some pretty powerful stuff. And what’s really awesome is how easy it\nis to describe something like this within a model that breaks apart the\ndimensions of stream processing into distinct, composable pieces. In the end,\nyou can focus more on the interesting business logic at hand, and less on the\nminutiae of shaping the data into some usable form.\nIf you don’t believe me, check out this \nblog post\n describing how to \nmanually\nbuild up sessions on Spark Streaming 1.x\n (note that this is not done to point\nfingers at them; the Spark folks had just done a good enough job with\neverything else that someone actually bothered to go to the trouble of\ndocumenting what it takes to build a specific variety of sessions support on\ntop of Spark 1.x; you can’t say the same for most other systems out there).\nIt’s quite involved, and they’re not even doing proper event-time sessions, or\nproviding speculative or late firings, or retractions.\nWhere\n: Custom Windowing\nUp until now, we’ve talked primarily about predefined types of windowing\nstrategies: fixed, sliding, and sessions.\n You can get a lot of mileage out of\nstandard types of windows, but there are plenty of real-world use cases for\nwhich being able to define a custom windowing strategy can really save the\nday (three of which we’re about to see now).\nMost systems today don’t support custom windowing to the degree that it’s\nsupported in Beam,\n so we focus on the Beam approach. In Beam, a custom\nwindowing strategy consists of two things:\nWindow assignment\n1\nThis places each\n element into an initial window.\n At the limit, this allows\nevery element to be placed within a unique window, which is very\npowerful.\n(Optional) window merging\nThis allows windows to merge at grouping times,\n which makes it possible\nfor windows to evolve over time, which we saw in action earlier with\nsession windows.\nTo give you a sense for how simple windowing strategies really are, and also\nhow useful custom windows support can be, we’re going to look in detail at\nthe stock implementations of fixed windows and sessions in Beam and then\nconsider a few real-world use cases that require custom variations on those\nthemes. In the process, we’ll see both how easy it is to create a custom\nwindowing strategy, and how limiting the lack of custom windowing support\ncan be when your use case doesn’t quite fit into the stock approaches.\nVariations on Fixed Windows\nTo begin, let’s look at the relatively simple strategy of fixed windows. \nThe\nstock fixed-windows implementation is as straightforward as you might\nimagine, and consists of the following logic:\nAssignment\nThe element is placed into the appropriate fixed-window \nbased on its\ntimestamp and the window’s size and offset parameters.\nMerging\nNone.\nAn abbreviated\n version of the code looks like \nExample 4-4\n.\nExample 4-4. \nAbbreviated FixedWindows implementation\npublic\n \nclass\n \nFixedWindows\n \nextends\n \nWindowFn\n<\nObject\n,\n \nIntervalWindow\n>\n \n{\n  \nprivate\n \nfinal\n \nDuration\n \nsize\n;\n  \nprivate\n \nfinal\n \nDuration\n \noffset\n;\n  \npublic\n \nCollection\n<\nIntervalWindow\n>\n \nassignWindow\n(\nAssignContext\n \nc\n)\n \n{\n    \nlong\n \nstart\n \n=\n \nc\n.\ntimestamp\n().\ngetMillis\n()\n \n-\n \nc\n.\ntimestamp\n()\n                   \n.\nplus\n(\nsize\n)\n                   \n.\nminus\n(\noffset\n)\n                   \n.\ngetMillis\n()\n \n%\n \nsize\n.\ngetMillis\n();\n    \nreturn\n \nArrays\n.\nasList\n(\nIntervalWindow\n(\nnew\n \nInstant\n(\nstart\n),\n \nsize\n));\n  \n}\n}\nKeep in mind that the point of showing you the code here isn’t so much to\nteach you how to write windowing strategies (although it’s nice to demystify\nthem and call out how simple they are). It’s really to help contrast the\ncomparative ease and difficulty of supporting some relatively basic use cases,\nboth with and without custom windowing, respectively. Let’s consider two\nsuch use cases that are variations on the fixed-windows theme now.\nUnaligned fixed windows\nOne characteristic of the default fixed-windows implementation that we\nalluded to previously is that windows are aligned across all of the data.\n In our\nrunning example, the window from noon to 1 PM for any given team aligns\nwith the corresponding windows for all other teams, which also extend from\nnoon to 1 PM. And in use cases for which you want to compare like windows\nacross another dimension, such as between teams, this alignment is very\nuseful. However, it comes at a somewhat subtle cost. All of the active\nwindows from noon to 1 PM become complete at around the same time,\nwhich means that once an hour the system is hit with a massive load of\nwindows to materialize.\nTo see what I mean, let’s look at a concrete example (\nExample 4-5\n). We’ll\nbegin with a score summation pipeline as we’ve used in most examples, with\nfixed two-minute windows, and a single watermark trigger.\nExample 4-5. \nWatermark completeness trigger (same as \nExample 2-6\n)\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(AfterWatermark())\n)\n  .apply(\nSum.integersPerKey()\n);\nBut in this instance, we’ll look at two different keys (see \nFigure 4-8\n) from the\nsame dataset in parallel. What we’ll see is that the outputs for those two keys\nare all aligned, on account of the windows being aligned across all of the\nkeys. As a result, we end up with \nN\n panes being materialized every time the\nwatermark passes the end of a window, where \nN\n is the number of keys with\nupdates in that window. In this example, where \nN\n is 2, that’s maybe not too\npainful. But when \nN\n starts to order in the thousands, millions, or more, that\nsynchronized burstiness can become problematic.\nFigure 4-8. \nAligned fixed windows\nIn circumstances for which comparing across windows is unnecessary, it’s\noften more desirable to spread window completion load out evenly across\ntime. This makes system load more predictable, which can reduce the\nprovisioning requirements for handling peak load. In most systems, however,\nunaligned fixed windows are only available if the system provides support for\nthem out of the box.\n But with custom-windowing support, it’s a relatively\ntrivial modification to the default fixed-windows implementation to provide\nunaligned fixed-windows support. What we want to do is continue\nguaranteeing that the windows for all elements being grouped together (i.e.,\nthe ones with the same key) have the same alignment, while relaxing the\nalignment restriction across different keys. The code changes to the default\nfixed-windowing strategy and looks something like \nExample 4-6\n.\nExample 4-6. \nAbbreviated UnalignedFixedWindows implementation\npublic\n \nclass\n \nUnaligned\nFixedWindows\n    \nextends\n \nWindowFn\n<\nKV\n<\nK\n,\n \nV\n>\n,\n \nIntervalWindow\n>\n \n{\n  \nprivate\n \nfinal\n \nDuration\n \nsize\n;\n  \nprivate\n \nfinal\n \nDuration\n \noffset\n;\n  \npublic\n \nCollection\n<\nIntervalWindow\n>\n \nassignWindow\n(\nAssignContext\n \nc\n)\n \n{\n    \nlong\n \nperKeyShift\n \n=\n \nhash\n(\nc\n.\nelement\n(\n)\n.\nkey\n(\n)\n)\n \n%\n \nsize\n;\n00:00 / 00:00\n2\n    \nlong\n \nstart\n \n=\n \nperKe\nyShift\n \n+\n \nc\n.\ntimestamp\n(\n)\n.\ngetMillis\n(\n)\n                   \n-\n \nc\n.\ntimestamp\n(\n)\n                      \n.\nplus\n(\nsize\n)\n                      \n.\nminus\n(\noffset\n)\n    \nreturn\n \nArrays\n.\nasList\n(\nIntervalWindow\n(\nnew\n \nInstant\n(\nstart\n)\n,\n \nsize\n)\n)\n;\n  \n}\n}\nWith this change, the windows for all elements \nwith the same key\n are\naligned,\n but the windows for elements \nwith different keys\n will (typically) be\nunaligned, thus spreading window completion load out at the cost of also\nmaking comparisons across keys somewhat less meaningful. We can switch\nour pipeline to use our new windowing strategy, illustrated in \nExample 4-7\n.\nExample 4-7. \nUnaligned fixed windows with a single watermark trigger\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(\nUnaligned\nFixedWindows.of(TWO_MINUTES))\n               \n.triggering(AfterWatermark())\n)\n  .apply(\nSum.integersPerKey()\n);\nAnd then you can see what this looks like in \nFigure 4-9\n by comparing\ndifferent fixed-window alignments across the same dataset as before (in this\ncase, I’ve chosen a maximal phase shift between the two alignments to most\nclearly call out the benefits, given that randomly chosen phases across a large\nnumber of keys will result in similar effects).\nFigure 4-9. \nUnaligned fixed windows\nNote how there are no instances where we emit multiple panes for multiple\nkeys simultaneously. Instead, the panes arrive individually at a much more\neven cadence. This is another example of being able to make trade-offs in one\ndimension (ability to compare across keys) in exchange for benefits in\n3\n00:00 / 00:00\nanother dimension (reduced peak resource provisioning requirements) when\nthe use case allows. Such flexibility is critical when you’re trying to process\nmassive quantities of data as efficiently as possible.\nLet’s now look at a second variation on fixed windows, one which is more\nintrinsically tied to the data being processed.\nPer-element/key fixed windows\nOur second example comes courtesy of one of the early \nadopters of Cloud\nDataflow.\n This company generates analytics data for its customers, but each\ncustomer is allowed to configure the window size over which it wants to\naggregate its metrics. In other words, each customer gets to define the\nspecific size of its fixed windows.\nSupporting a use case like this isn’t too difficult as long the number of\navailable window sizes is itself fixed. For example, you could imagine\noffering the option of choosing 30-minute, 60-minute, and 90-minute fixed\nwindows and then running a separate pipeline (or fork of the pipeline) for\neach of those options. Not ideal, but not too horrible. However, that rapidly\nbecomes intractable as the number of options increases, and in the limit of\nproviding support for truly arbitrary window sizes (which is what this\ncustomer’s use case required) is entirely impractical.\nFortunately, because each record the customer processes is already annotated\nwith metadata describing the desired size of window for aggregation,\nsupporting arbitrary, per-user fixed-window size was as simple as changing a\ncouple of lines from the stock fixed-windows implementation, as\ndemonstrated in \nExample 4-8\n.\nExample 4-8. \nModified (and abbreviated) FixedWindows implementation that\nsupports per-element window sizes\npublic class \nPerElement\nFixedWindows\n<T extends HasWindowSize%gt;\n    extends WindowFn<\nT\n, IntervalWindow> {\n  private final Duration offset;\n  public Collection<IntervalWindow> assignWindow(AssignContext c) {\n    \nlong perElementSize = c.element().getWindowSize();\n    long start = \nperKe\nyShift + \nc.timestamp().getMillis()\n                   - c.timestamp()\n                      .plus(size)\n                      .minus(offset)\n                      .getMillis() % size.getMillis();\n    return Arrays.asList(IntervalWindow(\n        new Instant(start), \nperElementSize\n));\n  }\n}\nWith this change, each element is assigned to a fixed window with the\nappropriate size, as dictated by metadata carried around in the element itself.\nChanging the pipeline code to use this new strategy is again trivial, as shown\nin \nExample 4-9\n.\nExample 4-9. \nPer-element fixed-window sizes with a single watermark\ntrigger\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(\nPerElement\nFixedWindows.of(TWO_MINUTES))\n               \n.triggering(AfterWatermark())\n)\n  .apply(\nSum.integersPerKey()\n);\nAnd then looking at an this pipeline in action (\nFigure 4-10\n), it’s easy to see\nthat the elements for Key A all have two minutes as their window size,\nwhereas the elements for Key B have one-minute window sizes.\nFigure 4-10. \nPer-key custom-sized fixed windows\nThis really isn’t something you would ever reasonably expect a system to\nprovide to you; the nature of where window size preferences are stored is too\nuse-case specific for it to make sense to try to build into a standard API.\nNevertheless, as exhibited by this customer’s needs, use cases like this do\nexist. That’s why the flexibility provided by custom windowing is so\npowerful.\n4\n00:00 / 00:00",12970
42-Variations on Session Windows.pdf,42-Variations on Session Windows,"Variations on Session Windows\nTo really drive home the usefulness of custom windowing, let’s look at one\nfinal example, which is a variation on sessions.\n Session windowing is\nunderstandably a bit more complex than fixed windows. Its implementation\nconsists of the following:\nAssignment\nEach element is\n initially placed into a proto-session window that begins at\nthe element’s timestamp and extends for the gap duration.\nMerging\nAt grouping time,\n all eligible windows are sorted, after which any\noverlapping windows are merged together.\nAn abbreviated version of the sessions code (hand merged together from a\nnumber of helper classes) looks something like that shown in \nExample 4-10\n.\nExample 4-10. \nAbbreviated Sessions implementation\npublic\n \nclass\n \nSessions\n \nextends\n \nWindowFn\n<\nObject\n,\n \nIntervalWindow\n>\n \n{\n  \nprivate\n \nfinal\n \nDuration\n \ngapDuration\n;\n  \npublic\n \nCollection\n<\nIntervalWindow\n>\n \nassignWindows\n(\nAssignContext\n \nc\n)\n \n{\n    \nreturn\n \nArrays\n.\nasList\n(\n      \nnew\n \nIntervalWindow\n(\nc\n.\ntimestamp\n(),\n \ngapDuration\n));\n  \n}\n  \npublic\n \nvoid\n \nmergeWindows\n(\nMergeContext\n \nc\n)\n \nthrows\n \nException\n \n{\n    \nList\n<\nIntervalWindow\n>\n \nsortedWindows\n \n=\n \nnew\n \nArrayList\n<>();\n    \nfor\n \n(\nIntervalWindow\n \nwindow\n \n:\n \nc\n.\nwindows\n())\n \n{\n      \nsortedWindows\n.\nadd\n(\nwindow\n);\n    \n}\n    \nCollections\n.\nsort\n(\nsortedWindows\n);\n    \nList\n<\nMergeCandidate\n>\n \nmerges\n \n=\n \nnew\n \nArrayList\n<>();\n    \nMergeCandidate\n \ncurrent\n \n=\n \nnew\n \nMergeCandidate\n();\n    \nfor\n \n(\nIntervalWindow\n \nwindow\n \n:\n \nsortedWindows\n)\n \n{\n      \nif\n \n(\ncurrent\n.\nintersects\n(\nwindow\n))\n \n{\n        \ncurrent\n.\nadd\n(\nwindow\n);\n      \n}\n \nelse\n \n{\n        \nmerges\n.\nadd\n(\ncurrent\n);\n        \ncurrent\n \n=\n \nnew\n \nMergeCandidate\n(\nwindow\n);\n      \n}\n    \n}\n    \nmerges\n.\nadd\n(\ncurrent\n);\n    \nfor\n \n(\nMergeCandidate\n \nmerge\n \n:\n \nmerges\n)\n \n{\n      \nmerge\n.\napply\n(\nc\n);\n    \n}\n  \n}\n}\nAs before, the point of seeing the code isn’t so much to teach you how\ncustom windowing functions are implemented, or even what the\nimplementation of sessions looks like; it’s really to show the ease with which\nyou can support new use via custom windowing.\nBounded sessions\nOne such custom use case I’ve come across multiple times is bounded\nsessions: sessions that are not allowed \nto grow beyond a certain size, either in\ntime, element count, or some other dimension. This can be for semantic\nreasons, or it can simply be an exercise in spam protection. However, given\nthe variations in types of limits (some use cases care about total session size\nin event time, some care about total element count, some care about element\ndensity, etc.), it’s difficult to provide a clean and concise API for bounded\nsessions. Much more practical is allowing users to implement their own\ncustom windowing logic, tailored to their specific use case. An example of\none such use case, in which session windows are time-limited, might look\nsomething like \nExample 4-11\n (eliding some of the builder boilerplate we’ll\nutilize here).\nExample 4-11. \nAbbreviated Sessions implementation\npublic\n \nclass\n \nBounded\nSessions\n \nextends\n \nWindowFn\n<\nObject\n,\n \nIntervalWindow\n>\n \n{\n  \nprivate\n \nfinal\n \nDuration\n \ngapDuration\n;\n  \nprivate\n \nfinal\n \nDuration\n \nmaxSize\n;\n  \npublic\n \nCollection\n<\nIntervalWindow\n>\n \nassignWindows\n(\nAssignContext\n \nc\n)\n \n{\n    \nreturn\n \nArrays\n.\nasList\n(\n      \nnew\n \nIntervalWindow\n(\nc\n.\ntimestamp\n(\n)\n,\n \ngapDuration\n)\n)\n;\n  \n}\n  \nprivate\n \nDuration\n \nwindowSize\n(\nIntervalWindow\n \nwindow\n)\n \n{\n    \nreturn\n \nwindow\n \n=\n=\n \nnull\n      \n?\n \nnew\n \nDuration\n(\n0\n)\n      \n:\n \nnew\n \nDuration\n(\nwindow\n.\nstart\n(\n)\n,\n \nwindow\n.\nend\n(\n)\n)\n;\n  \n}\n  \npublic\n \nstatic\n \nvoid\n \nmergeWindows\n(\n      \nWindowFn\n<\n?\n,\n \nIntervalWindow\n>\n.\nMergeContext\n \nc\n)\n \nthrows\n \nException\n \n{\n    \nList\n<\nIntervalWindow\n>\n \nsortedWindows\n \n=\n \nnew\n \nArrayList\n<\n>\n(\n)\n;\n    \nfor\n \n(\nIntervalWindow\n \nwindow\n \n:\n \nc\n.\nwindows\n(\n)\n)\n \n{\n      \nsortedWindows\n.\nadd\n(\nwindow\n)\n;\n    \n}\n    \nCollections\n.\nsort\n(\nsortedWindows\n)\n;\n    \nList\n<\nMergeCandidate\n>\n \nmerges\n \n=\n \nnew\n \nArrayList\n<\n>\n(\n)\n;\n    \nMergeCandidate\n \ncurrent\n \n=\n \nnew\n \nMergeCandidate\n(\n)\n;\n    \nfor\n \n(\nIntervalWindow\n \nwindow\n \n:\n \nsortedWindows\n)\n \n{\n      \nMergeCandidate\n \nnext\n \n=\n \nnew\n \nMergeCandidate\n(\nwindow\n)\n;\n      \nif\n \n(\ncurrent\n.\nintersects\n(\nwindow\n)\n)\n \n{\n        \ncurrent\n.\nadd\n(\nwindow\n)\n;\n        \nif\n \n(\nwindowSize\n(\ncurrent\n.\nunion\n)\n \n<\n=\n \n(\nmaxSize\n \n-\n \ngapDuration\n)\n)\n          \ncontinue\n;\n        \n// Current window exceeds bounds, so flush and move to next\n        \nnext\n \n=\n \nnew\n \nMergeCandidate\n(\n)\n;\n      \n}\n      \nmerges\n.\nadd\n(\ncurrent\n)\n;\n      \ncurrent\n \n=\n \nnext\n;\n    \n}\n    \nmerges\n.\nadd\n(\ncurrent\n)\n;\n    \nfor\n \n(\nMergeCandidate\n \nmerge\n \n:\n \nmerges\n)\n \n{\n      \nmerge\n.\napply\n(\nc\n)\n;\n    \n}\n  \n}\n}\nAs always, updating our pipeline (the early/on-time/late version of it, from\nExample 2-7\n, in this case) to use this custom windowing strategy is trivial,\n as\nyou can see in \nExample 4-12\n.\nExample 4-12. \nEarly, on-time, and late firings via the early/on-time/late API\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(\nBoundedSessions\n                       .withGapDuration(ONE_MINUTE)\n                       \n.withMaxSize(THREE_MINUTES)\n)\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n)\n  .apply(\nSum.integersPerKey()\n);\nAnd executed over our running example, it might then look something like\nFigure 4-11\n.\nFigure 4-11. \nPer-key custom-sized fixed windows\nNote how the large session with value 36 that spanned [12:00.26, 12:05.20),\nor nearly five minutes of time, in the unbounded sessions implementation\nfrom \nFigure 2-7\n now ends up broken apart into two shorter sessions of length\n2 minutes and 2 minutes 53 seconds.\nGiven how few systems provide custom windowing support today, it’s worth\npointing out how much more effort would be required to implement such a\nthing using a system that supported only an unbounded sessions\nimplementation. Your only real recourse would be to write code downstream\nof the session grouping logic that looked at the generated sessions and\nchopped them up if they exceed the length limit. This would require the\nability to decompose a session after the fact, which would obviate the\nbenefits of incremental aggregation (something we look at in more detail in\nChapter 7\n), increasing cost. It would also eliminate any spam protection\nbenefits one might hope to gain by limiting session lengths, because the\nsessions would first need to grow to their full sizes before being chopped or\ntruncated.\nOne Size Does Not Fit All\nWe’ve now looked at three real-world use cases, each of which was a \nsubtle\nvariation on the stock types of windowing \ntypically provided by data\n00:00 / 00:00",7583
43-One Size Does Not Fit All.pdf,43-One Size Does Not Fit All,,0
44-Summary.pdf,44-Summary,"processing systems: unaligned fixed windows, per-element fixed windows,\nand bounded sessions. In all three cases, we saw how simple it was to support\nthose use cases via custom windowing and how much more difficult (or\nexpensive) it would be to support those use cases without it. Though custom\nwindowing doesn’t see broad support across the industry as yet, it’s a feature\nthat provides much needed flexibility for balancing trade-offs when building\ndata processing pipelines that need to handle complex, real-world use cases\nover massive amounts of data as efficiently as\n possible. \nSummary\nAdvanced windowing is a complex and varied topic. In this chapter, we\ncovered three advanced concepts:\nProcessing-time windows\nWe saw how this relates to event-time windowing, calling out the places\nwhere it’s inherently useful and, most important, identifying those where\nit’s not by specifically highlighting the stability of results that event-time\nwindowing affords us.\nSession windows\nWe had our first introduction to the dynamic class of merging window\nstrategies and \nseeing just how much heavy lifting the system does for us\nin providing such a powerful construct that you can simply drop into\nplace.\nCustom windows\nHere, we looked at three real-world examples of custom windows that are\ndifficult or impossible to achieve in systems that provide only a static set\nof stock windowing strategies but relatively trivial to implement in a\nsystem with custom-windowing support:\nUnaligned fixed windows\n, which provide a more even distribution\nof outputs over time when using a watermark trigger in conjunction\nwith fixed windows.\nPer-element fixed windows\n, which provide the flexibility to\ndynamically choose the size of fixed windows per element (e.g., to\nprovide customizable per-user or per-ad-campaign window sizes),\nfor greater customization of the pipeline semantics to the use case at\nhand.\nBounded-session windows\n, which limit how large a given session\nmay grow; for example, to counteract spam attempts or to place a\nbound on the latency for completed sessions being materialized by\nthe pipeline.\nAfter deep diving through watermarks in \nChapter 3\n with Slava and taking a\nbroad survey of advanced windowing here, we’ve now gone well beyond the\nbasics of robust stream processing in multiple dimensions. With that, we\nconclude our focus on the Beam Model and thus \nPart I\n of the book.\nUp next is Reuven’s \nChapter 5\n on consistency guarantees, exactly-once\nprocessing, and side effects, after which we begin our journey into \nPart II,\nStreams and Tables\n with \nChapter 6\n.\n As far as I know, Apache Flink is the only other system to support custom\nwindowing to the extent that Beam does. And to be fair, its support extends\neven beyond that of Beam’s, thanks to the ability to provide a custom\nwindow evictor. Head asplode.\n And I’m not actually aware of any such systems at this time.\n This naturally implies the use of keyed data, but because windowing is\nintrinsically tied to grouping by key anyway, that restriction isn’t particularly\nburdensome.\n And it’s not critical that the element itself know the window size; you could\njust as easily look up and cache the appropriate window size for whatever the\ndesired dimension is; for example, per-user.\n1\n2\n3\n4",3345
45-5. Exactly-Once and Side Effects.pdf,45-5. Exactly-Once and Side Effects,,0
46-Why Exactly Once Matters.pdf,46-Why Exactly Once Matters,"Chapter 5. \nExactly-Once and\nSide Effects\nWe now shift from discussing programming models and APIs to the systems\nthat implement them.\n A model and API allows users to describe what they\nwant to compute. Actually running the computation accurately at scale\nrequires a system—usually a distributed system.\nIn this chapter, we focus on how an implementing system can correctly\nimplement the Beam Model to produce accurate results.\n Streaming systems\noften talk about \nexactly-once processing\n; that is, ensuring that every record is\nprocessed exactly one time. We will explain what we mean by this, and how\nit might be implemented.\nAs a motivating example, this chapter focuses on techniques used by \nGoogle\nCloud Dataflow to efficiently guarantee exactly-once processing of records.\nToward the end of the chapter, we also look at techniques used by some other\npopular streaming systems to guarantee exactly once.\nWhy Exactly Once Matters\nIt almost goes without saying that for many users, any risk of dropped\nrecords or data loss in their data processing pipelines is unacceptable. \nEven\nso, historically many general-purpose streaming systems made no guarantees\nabout record processing—all processing was “best effort” only. Other\nsystems provided at-least-once guarantees, ensuring that records were always\nprocessed at least once, but records might be duplicated (and thus result in\ninaccurate aggregations); in practice, many such at-least-once systems\nperformed aggregations in memory, and thus their aggregations could still be\nlost when machines crashed. These systems were used for low-latency,\nspeculative results but generally could guarantee nothing about the veracity\nof these results.\nAs \nChapter 1\n points out, this led to a strategy \nthat was coined the \nLambda\nArchitecture\n—run a streaming system to get fast, but inaccurate results.\nSometime later (often after end of day), a batch system runs to the correct\nanswer. This works only if the data stream is replayable; however, this was\ntrue for enough data sources that this strategy proved viable. Nonetheless,\nmany people who tried this experienced a number of issues with the Lambda\nArchitecture:\nInaccuracy\nUsers tend to underestimate the impact of failures.\n They often assume\nthat a small percentage of records will be lost or duplicated (often based\non experiments they ran), and are shocked on that one bad day when 10%\n(or more!) of records are lost or are duplicated. In a sense, such systems\nprovide only “half” a guarantee—and without a full one, anything is\npossible.\nInconsistency\nThe batch system used for the end-of-day calculation often has different\ndata semantics than the streaming system.\n Getting the two pipelines to\nproduce comparable results proved more difficult than initially thought.\nComplexity\nBy definition, Lambda requires you to write and maintain two different\ncodebases. \nYou also must run and maintain two complex distributed\nsystems, each with different failure modes. For anything but the simplest\nof pipelines, this quickly becomes overwhelming.\nUnpredictability\nIn many use cases, end users will see streaming \nresults that differ from\nthe daily results by an uncertain amount, which can change randomly. In\nthese cases, users will stop trusting the streaming data and wait for daily\nbatch results instead, thus destroying the value of getting low-latency\nresults in the first place.\nLatency",3469
47-Side Effects.pdf,47-Side Effects,"Some business\n use cases \nrequire\n low-latency correct results, which the\nLambda Architecture does not provide by design.\nFortunately, many Beam runners can do much better. In this chapter, we\nexplain how exactly-once stream processing helps users count on accurate\nresults and avoid the risk of data loss while relying on a single codebase and\nAPI. Because a variety of issues that can affect a pipeline’s output are often\nerroneously conflated with exactly-once guarantees, we first explain precisely\nwhich issues are in and out of scope when we refer to “exactly once” in the\ncontext of Beam and data processing.\nAccuracy Versus Completeness\nWhenever a Beam pipeline processes a record for a pipeline, we want to\nensure that the record is never dropped or duplicated.\n However, the nature of\nstreaming pipelines is such that records sometimes show up late, after\naggregates for their time windows have already been processed. The Beam\nSDK allows the user to configure how long the system should wait for late\ndata to arrive; any (and only) records arriving later than this deadline are\ndropped. This feature contributes to \ncompleteness\n, not to accuracy: all records\nthat showed up in time for processing are accurately processed exactly once,\nwhereas these late records are explicitly dropped.\nAlthough late records are usually discussed in the context of streaming\nsystems, it’s worth noting that batch pipelines have similar completeness\nissues. For example, a common batch paradigm is to run a job at 2 AM over\nall the previous day’s data. However, if some of yesterday’s data wasn’t\ncollected until after 2 AM, it won’t be processed by the batch job! Thus,\nbatch pipelines also provide accurate but not always complete results.\nSide Effects\nOne characteristic of Beam and Dataflow is that users inject\n custom code that\nis executed\n as part of their pipeline graph. Dataflow does \nnot\n guarantee that\nthis code is run only once per record,\n whether by the streaming or batch\n1",2025
48-Problem Definition.pdf,48-Problem Definition,"runner. It might run a given record through a user transform multiple times,\nor it might even run the same record simultaneously on multiple workers; this\nis necessary to guarantee at-least-once processing in the face of worker\nfailures. Only one of these invocations can “win” and produce output further\ndown the pipeline.\nAs a result, nonidempotent side effects are not guaranteed to execute exactly\nonce; if you write code that has side effects external to the pipeline, such as\ncontacting an outside service, these effects might be executed more than once\nfor a given record. This situation is usually unavoidable because there is no\nway to atomically commit Dataflow’s processing with the side effect on the\nexternal service. Pipelines do need to eventually send results to the outside\nworld, and such calls might not be idempotent. As you will see later in the\nchapter, often such sinks are able to add an extra stage to restructure the call\ninto an idempotent operation first.\nProblem Definition\nSo, we’ve given a couple of examples of what we’re \nnot\n talking about.\n What\ndo we mean then by exactly-once processing? To motivate this, let’s begin\nwith a simple streaming pipeline,\n shown in \nExample 5-1\n.\nExample 5-1. \nA simple streaming pipeline\nPipeline\n \np\n \n=\n \nPipeline\n.\ncreate\n(\noptions\n);\n// Calculate 1-minute counts of events per user.\nPCollection\n<..>\n \nperUserCounts\n \n=\n \n      \np\n.\napply\n(\nReadFromUnboundedSource\n.\nread\n())\n       \n.\napply\n(\nnew\n \nKeyByUser\n())\n       \n.\nWindow\n.<..>\ninto\n(\nFixedWindows\n.\nof\n(\nDuration\n.\nstandardMinutes\n(\n1\n)))\n       \n.\napply\n(\nCount\n.\nperKey\n());\n// Process these per-user counts, and write the output somewhere.\nperUserCounts\n.\napply\n(\nnew\n \nProcessPerUserCountsAndWriteToSink\n());\n// Add up all these per-user counts to get 1-minute counts of all events.\nperUserCounts\n.\napply\n(\nValues\n.<..>\ncreate\n())\n             \n.\napply\n(\nCount\n.\nglobally\n())\n             \n.\napply\n(\nnew\n \nProcessGlobalCountAndWriteToSink\n());\np\n.\nrun\n();\nThis pipeline computes two different windowed aggregations. The first\n2\ncounts how many events came from each individual user over the course of a\nminute, and the second counts how many total events came in each minute.\nBoth aggregations are written to unspecified streaming sinks.\nRemember that Dataflow executes pipelines on many different workers in\nparallel. After each \nGroupByKey\n (the \nCount\n operations use \nGroupByKey\nunder the covers), all records with the same key are processed on the same\nmachine following a process called \nshuffle\n. The Dataflow workers shuffle\ndata between themselves using Remote Procedure Calls (RPCs), ensuring\nthat records for a given key all end up on the same machine.\nFigure 5-1\n shows the shuffles that Dataflow creates for the pipeline in\nExample 5-1\n.\n The \nCount.perKey\n shuffles all the data for each user onto a\ngiven worker, whereas the \nCount.globally\n shuffles all these partial counts\nto a single worker to calculate the global sum.\nFigure 5-1. \nShuffles in a pipeline\nFor Dataflow to accurately process data, this shuffle process must ensure that\nevery record is shuffled exactly once. As you will see in a moment, the\ndistributed nature of shuffle makes this a challenging problem.\nThis pipeline also both reads and writes data from and to the outside world,\nso Dataflow must ensure that this interaction does not introduce any\ninaccuracies. Dataflow has always supported this task—what \nApache Spark\nand\n Apache Flink call \nend-to-end exactly once\n—for sources and sinks\nwhenever technically feasible.\nThe focus of this chapter will be on three things:\n3",3768
49-Bloom Filters.pdf,49-Bloom Filters,"Shuffle\nHow Dataflow guarantees \nthat every record is shuffled exactly once.\nSources\nHow Dataflow guarantees that every source record is processed exactly\nonce.\nSinks\nHow Dataflow guarantees that every\n sink produces \naccurate output.\nEnsuring Exactly Once in Shuffle\nAs just explained, Dataflow’s streaming shuffle uses RPCs. Now,\n any time\nyou have two machines communicating\n via RPC, you should think long and\nhard about data integrity. \nFirst of all, RPCs can fail for many reasons. The\nnetwork might be interrupted, the RPC might time out before completing, or\nthe receiving server might decide to fail the call. To guarantee that records\nare not lost in shuffle, Dataflow employs \nupstream backup\n. This simply\nmeans that the sender will retry RPCs until it receives positive\nacknowledgment of receipt. Dataflow also ensures that it will continue\nretrying these RPCs even if the sender crashes. \nThis guarantees that every\nrecord is delivered \nat least once\n.\nNow, the problem is that these retries might themselves create duplicates.\nMost RPC frameworks, including the one Dataflow uses, provide the sender\nwith a status indicating success or failure. In a distributed system, you need to\nbe aware that RPCs can sometimes succeed even when they have appeared to\nfail. There are many reasons for this: race conditions with the RPC timeout,\npositive acknowledgment from the server failing to transfer even though the\nRPC succeeded, and so on. The only status that a sender can really trust is a\nsuccessful one.\nAn RPC returning a failure status generally indicates that the call might or\nmight not have succeeded. Although specific error codes can communicate\nunambiguous failure, many common RPC failures, such as Deadline\n4\nExceeded, are ambiguous. In the case of streaming shuffle,\n retrying an RPC\nthat really succeeded means delivering a record twice! Dataflow needs some\nway of detecting and removing these duplicates.\nAt a high level, the algorithm for this task is quite \nsimple (see \nFigure 5-2\n):\nevery message sent is tagged with a unique identifier. Each receiver stores a\ncatalog of all identifiers that have already been seen and processed. Every\ntime a record is received, its identifier is looked up in this catalog. If it is\nfound, the record is dropped as a duplicate. Because Dataflow is built on top\nof a scalable key/value store, this store is used to hold the deduplication\ncatalog.\nFigure 5-2. \nDetecting duplicates in shuffle\nAddressing Determinism\nMaking this strategy work in the real world requires a lot of care, however.\nOne immediate wrinkle is\n that the Beam Model allows for user code to\nproduce nondeterministic output. This means that a \nParDo\n can execute twice\non the same input record (due to a retry), yet produce different output on each\nretry. The desired behavior is that only one of those outputs will commit into\nthe pipeline; however, the nondeterminism involved makes it difficult to\nguarantee that both outputs have the same deterministic ID. Even trickier, a\nParDo\n can output multiple records, so each of these retries might produce a\ndifferent number of outputs!\nSo, why don’t we simply require that all user processing be deterministic?\n4\nOur experience is that in practice, many pipelines require nondeterministic\ntransforms And all too often, pipeline authors do not realize that the code\nthey wrote is nondeterministic. For example, consider a transform that looks\nup supplemental data in Cloud Bigtable in order to enrich its input data. This\nis a nondeterministic task, as the external value might change in between\nretries of the transform. Any code that relies on current time is likewise not\ndeterministic. We have also seen transforms that need to rely on random\nnumber generators. And even if the user code is purely deterministic, any\nevent-time aggregation that allows for late data might have nondeterministic\ninputs.\nDataflow addresses this issue by using checkpointing to make\nnondeterministic processing effectively deterministic.\n Each output from a\ntransform is checkpointed, together with its unique ID, to stable storage\nbefore\n being delivered to the next stage.\n Any retries in the shuffle delivery\nsimply replay the output that has been checkpointed—the user’s\nnondeterministic code is not run again on retry. To put it another way, the\nuser’s code may be run multiple times but only one of those runs can “win.”\nFurthermore, Dataflow uses a consistent store that allows it to prevent\nduplicates from being written to stable storage.\nPerformance\nTo implement exactly-once shuffle delivery, a catalog of record IDs is stored\nin each receiver key. \nFor every record that arrives, Dataflow looks up the\ncatalog of IDs already seen to determine whether this record is a duplicate.\nEvery output from step to step is checkpointed to storage to ensure that the\ngenerated record IDs are stable.\nHowever, unless implemented carefully, this process would significantly\ndegrade pipeline performance for customers by creating a huge increase in\nreads and writes. Thus, for exactly-once processing to be viable for Dataflow\nusers, that I/O has to be reduced, in particular by preventing I/O on every\nrecord.\nDataflow achieves this goal via two key techniques: \ngraph optimization\n and\n5\nBloom \nfilters\n.\nGraph Optimization\nThe Dataflow service runs a series of optimizations on the pipeline graph\nbefore executing it. \nOne such optimization is \nfusion\n, in which the service\nfuses many logical steps into a single execution stage. \nFigure 5-3\n shows some\nsimple examples.\nFigure 5-3. \nExample optimizations: fusion\nAll fused steps are run as an in-process unit, so there’s no need to store\nexactly-once data for each of them. In many cases, fusion reduces the entire\ngraph down to a few physical steps, greatly reducing the amount of data\ntransfer needed (and saving on state usage, as well).\nDataflow also optimizes associative \nand commutative \nCombine\n operations\n(such as \nCount\n and \nSum\n) by performing partial combining  locally before\nsending the data to the main grouping operation, as illustrated in \nFigure 5-4\n.\nThis approach can greatly reduce the number of messages for delivery,\nconsequently also reducing the number of reads and writes.\nFigure 5-4. \nExample optimizations: combiner lifting\nBloom Filters\nThe aforementioned optimizations are general techniques that improve\nexactly-once performance as a byproduct. \nFor an optimization aimed strictly\nat improving exactly-once processing, we turn to \nBloom filters\n.\nIn a healthy pipeline, most arriving records will not be duplicates. We can use\nthat fact to greatly improve performance via Bloom filters, which are\ncompact data structures that allow for quick set-membership checks. Bloom\nfilters have a very interesting property: they can return false positives but\nnever false negatives. If the filter says “Yes, the element is in the set,” we\nknow that the element is \nprobably\n in the set (with a probability that can be\ncalculated). However, if the filter says an element is \nnot\n in the set, it\ndefinitely isn’t. This function is a perfect fit for the task at hand.\nThe implementation in Dataflow works like this: each worker keeps a Bloom\nfilter of every ID it has seen. Whenever a new record ID shows up, it looks it\nup in the filter. If the filter returns false, this record is not a duplicate and the\nworker can skip the more expensive lookup from stable storage. It needs to\ndo that second lookup only if the Bloom filter returns true, but as long as the\nfilter’s false-positive rate is low, that step is rarely needed.\nBloom filters tend to fill up over time, however, and as that happens, the\nfalse-positive rate increases. We also need to construct this Bloom filter anew\nany time a worker restarts by scanning the ID catalog stored in state.\nHelpfully, Dataflow attaches a system timestamp to each record.\n Thus,\ninstead of creating a single Bloom filter, the service creates a separate one for\nevery 10-minute range. When a record arrives, Dataflow queries the\nappropriate filter based on the system timestamp.\n This step prevents the\nBloom filters from saturating because filters are garbage-collected over time,\nand it also bounds the amount of data that needs to be scanned at startup.\nFigure 5-5\n illustrates this process: records arrive in the system and are\ndelegated to a Bloom filter based on their arrival time. None of the records\nhitting the first filter are duplicates, and all of their catalog lookups are\nfiltered. Record \nr1\n is delivered a second time, so a catalog lookup is needed\nto verify that it is indeed a duplicate; the same is true for records \nr4\n and \nr6\n.\nRecord \nr8\n is not a duplicate; however, due to a false positive in its Bloom\nfilter, a catalog lookup is generated (which will determine that \nr8\n is not a\nduplicate and should be processed).\nFigure 5-5. \nExactly-once Bloom filters\n6\n7\n8",9076
50-Example Sink Files.pdf,50-Example Sink Files,"Garbage Collection\nEvery Dataflow worker persistently stores a catalog of \nunique record IDs it\nhas seen.\n As Dataflow’s state and consistency model is per-key, in reality\neach key stores a catalog of records that have been delivered to that key. We\ncan’t store these identifiers forever, or all available storage will eventually fill\nup. To avoid that issue, you need garbage collection of acknowledged record\nIDs.\nOne strategy for accomplishing this goal would be for senders to tag each\nrecord with a strictly increasing sequence number in order to track the earliest\nsequence number still in flight (corresponding to an unacknowledged record\ndelivery). Any identifier in the catalog with an earlier sequence number could\nthen be garbage-collected because all earlier records have already been\nacknowledged.\nThere is a better alternative, however. As previously mentioned, Dataflow\nalready tags each record with a system timestamp that is used for bucketing\nexactly-once Bloom filters. \nConsequently, instead of using sequence numbers\nto garbage-collect the exactly-once catalog, Dataflow calculates a garbage-\ncollection watermark based on these system timestamps (this is the\nprocessing-time watermark discussed in \nChapter 3\n). A nice side benefit of\nthis approach is that because this watermark is based on the amount of\nphysical time spent waiting in a given stage (unlike the data watermark,\nwhich is based on custom event times), it provides intuition on what parts of\nthe pipeline are slow. This metadata is the basis for the System Lag metric\nshown in the Dataflow WebUI.\nWhat happens if a record arrives with an old timestamp and we’ve already\ngarbage-collected identifiers for this point in time? \nThis can happen due to an\neffect we call \nnetwork remnants\n, in which an old message becomes stuck for\nan indefinite period of time inside the network and then suddenly shows up.\nWell, the low watermark that triggers garbage collection won’t advance until\nrecord deliveries have been acknowledged, so we know that this record has\nalready been successfully processed. Such network remnants are clearly\nduplicates and\n are ignored.\nExactly Once in Sources\nBeam provides a source API for reading\n data into a Dataflow pipeline.\nDataflow might retry reads from a source if processing fails and needs to\nensure that every unique record produced by a source is processed exactly\nonce.\nFor most sources Dataflow handles this process transparently; such \nsources\nare \ndeterministic\n. For example, consider a source that reads data out of files.\nThe records in a file will always be in a deterministic order and at\ndeterministic byte locations, no matter how many times the file is read.\n The\nfilename and byte location uniquely identify each record, so the service can\nautomatically generate unique IDs for each record. \nAnother source that\nprovides similar determinism guarantees is Apache Kafka; each Kafka topic\nis divided into a static set of partitions, and records in a partition always have\na deterministic order. Such deterministic sources will work seamlessly in\nDataflow with no duplicates.\nHowever, not all sources are so simple. For example, one common source for\nDataflow pipelines is Google Cloud Pub/Sub. \nPub/Sub is a \nnondeterministic\nsource: multiple subscribers can pull from a Pub/Sub topic, but which\nsubscribers receive a given message is unpredictable. If processing fails\nPub/Sub will redeliver messages but the messages might be delivered to\ndifferent workers than those that processed them originally, and in a different\norder. This nondeterministic behavior means that Dataflow needs assistance\nfor detecting duplicates because there is no way for the service to\ndeterministically assign record IDs that will be stable upon retry. (We dive\ninto a more detailed case study of Pub/Sub later in this chapter.)\nBecause Dataflow cannot automatically assign record IDs, nondeterministic\nsources are \nrequired to inform the system what the record IDs should be.\nBeam’s Source API provides the \nUnboundedReader.getCurrentRecordId\nmethod. If a source provides unique IDs per record and notifies Dataflow that\nit requires deduplication,\n records with the same ID will be filtered out.\n9\n10\n11\n12\nExactly Once in Sinks\nAt some point, every pipeline needs to output data to the outside world, and a\nsink is simply a transform that does exactly that.\n Keep in mind that delivering\ndata externally is a side effect, and we have already mentioned that Dataflow\ndoes not guarantee exactly-once application of side effects. So, how can a\nsink guarantee that outputs are delivered exactly once?\nThe simplest answer is that a number of built-in sinks are provided as part of\nthe Beam SDK. These sinks are carefully designed to ensure that they do not\nproduce duplicates, even if executed multiple times. Whenever possible,\npipeline authors are encouraged to use one of these built-in sinks.\nHowever, sometimes the built-ins are insufficient and you need to write your\nown. The best approach is to ensure that your side-effect operation is\nidempotent and therefore robust in the face of replay.\n However, often some\ncomponent of a side-effect \nDoFn\n is nondeterministic and thus might change\non replay. For example, in a windowed aggregation, the set of records in the\nwindow can also be nondeterministic!\nSpecifically, the window might attempt to fire with elements \ne0\n, \ne1\n, \ne2\n, but\nthe worker crashes before committing the window processing (but not before\nthose elements are sent as a side effect). When the worker restarts, the\nwindow will fire again, but now a late element \ne3\n shows up. Because this\nelement shows up before the window is committed, it’s not counted as late\ndata, so the \nDoFn\n is called again with elements \ne0\n, \ne1\n, \ne2\n, \ne3\n. These are then\nsent to the side-effect operation. Idempotency does not help here, because\ndifferent logical record sets were sent each time.\nThere are other ways nondeterminism can be introduced. The standard way to\naddress this risk is to rely on the fact that Dataflow currently guarantees that\nonly one version of a \nDoFn\n’s output can make it past a shuffle boundary.\nA simple \nway\n of using this guarantee is via the built-in \nReshuffle\n transform.\nThe pattern presented in \nExample 5-2\n ensures that the side-effect operation\nalways receives a deterministic record to output.\nExample 5-2. \nReshuffle example\n13\nc\n.\napply\n(\nWindow\n.<..>\ninto\n(\nFixedWindows\n.\nof\n(\nDuration\n.\nstandardMinutes\n(\n1\n))))\n \n.\napply\n(\nGroupByKey\n.<..>.\ncreate\n())\n \n.\napply\n(\nnew\n \nPrepareOutputData\n())\n \n.\napply\n(\nReshuffle\n.<..>\nof\n())\n \n.\napply\n(\nWriteToSideEffect\n());\nThe preceding pipeline splits the sink into two steps: \nPrepareOutputData\nand \nWriteToSideEffect\n. \nPrepareOutputData\n outputs records\ncorresponding to idempotent writes. If we simply ran one after the other, the\nentire process might be replayed on failure, \nPrepareOutputData\n might\nproduce a different result, and both would be written as side effects. When\nwe add the \nReshuffle\n in between the two, Dataflow guarantees this can’t\nhappen.\nOf course, Dataflow might still run the \nWriteToSideEffect\n operation\nmultiple times. The side effects themselves still need to be idempotent, or the\nsink will receive duplicates. For example, an operation that sets or overwrites\na value in a data store is idempotent, and will generate correct output even if\nit’s run several times. An operation that appends to a list is not idempotent; if\nthe operation is run multiple times, the same value will be appended each\ntime.\nWhile \nReshuffle\n provides a simple way of achieving stable input to a \nDoFn\n,\na \nGroupByKey\n works just as well. However, there is currently a proposal that\nremoves the need to add a \nGroupByKey\n to achieve stable input into a \nDoFn\n.\nInstead, the user could \nannotate\n \nWriteToSideEffect\n with a special\nannotation, \n@RequiresStableInput\n, and the system would then ensure\nstable input to that transform.\nUse Cases\nTo illustrate, let’s examine some built-in sources and sinks to see how they\nimplement the\n aforementioned patterns.\nExample Source: Cloud Pub/Sub\nCloud Pub/Sub is a fully managed, scalable,\n reliable, and \nlow-latency system\nfor delivering messages from publishers to subscribers. Publishers publish\ndata on named topics, and subscribers create named subscriptions to pull data\nfrom these topics. Multiple subscriptions can be created for a single topic, in\nwhich case each subscription receives a full copy of all data published on the\ntopic from the time of the subscription’s creation. Pub/Sub guarantees that\nrecords will continue to be delivered until they are acknowledged; however, a\nrecord might be delivered multiple times.\nPub/Sub is intended for distributed use, so many publishing processes can\npublish to the same topic and many subscribing processes can pull from the\nsame subscription. After a record has been pulled, the subscriber must\nacknowledge it within a certain amount of time, or that pull expires and\nPub/Sub will redeliver that record to another of the subscribing processes.\nAlthough these characteristics make Pub/Sub highly scalable, they also make\nit a challenging source for a system like Dataflow. It’s impossible to know\nwhich record will be delivered to which worker, and in which order. What’s\nmore, in the case of failure, redelivery might send the records to different\nworkers in different orders!\nPub/Sub provides a stable message ID with each message, and this ID will be\nthe same upon redelivery. The Dataflow Pub/Sub source will default to using\nthis ID for removing duplicates from Pub/Sub. (The records are shuffled\nbased on a hash of the ID, so that repeated deliveries are always processed on\nthe same worker.) In some cases, however, this is not quite enough. The\nuser’s publishing process might retry publishes, and as a result introduce\nduplicates into Pub/Sub. From that service’s perspective these are unique\nrecords, so they will get unique record IDs. Dataflow’s Pub/Sub source\nallows the user to provide their own record IDs as a custom attribute. As long\nas the publisher sends the same ID when retrying, Dataflow will be able to\ndetect these duplicates.\nBeam (and therefore Dataflow) provides a reference source implementation\nfor Pub/Sub. However, keep in mind that this is \nnot\n what Dataflow uses but\nrather an implementation used only by non-Dataflow runners (such as\nApache Spark, Apache Flink, and the DirectRunner). For a variety of reasons,\nDataflow handles Pub/Sub internally and does not use the public Pub/Sub\nsource.\nExample Sink: Files\nThe streaming\n runner can \nuse Beam’s file\n sinks (\nTextIO\n, \nAvroIO\n, and any\nother sink that implements \nFileBasedSink\n) to continuously output records to\nfiles. \nExample 5-3\n provides an example use\n case.\nExample 5-3. \nWindowed file writes\nc\n.\napply\n(\nWindow\n.<..>\ninto\n(\nFixedWindows\n.\nof\n(\nDuration\n.\nstandardMinutes\n(\n1\n))))\n \n.\napply\n(\nTextIO\n.\nwriteStrings\n().\nto\n(\nnew\n \nMyNamePolicy\n()).\nwithWindowedWrites\n());\nThe snippet in \nExample 5-3\n writes 10 new files each minute, containing data\nfrom that window. \nMyNamePolicy\n is a user-written function that determines\noutput filenames based on the shard and the window. You can also use\ntriggers, in which case each trigger pane will be output as a new file.\nThis process is implemented using a variant on the pattern in \nExample 5-3\n.\nFiles are written out to temporary locations, and these temporary filenames\nare sent to a subsequent transform through a \nGroupByKey\n. After the\nGroupByKey\n is a finalize transform that atomically moves the temporary files\ninto their final location. The pseudocode in \nExample 5-4\n provides a sketch of\nhow a consistent streaming file sink is implemented in Beam. (For more\ndetails, see \nFileBasedSink\n and \nWriteFiles\n in the Beam codebase.)\nExample 5-4. \nFile sink\nc\n  \n// Tag each record with a random shard id.\n  \n.\napply\n(\n""AttachShard""\n,\n \nWithKeys\n.\nof\n(\nnew\n \nRandomShardingKey\n(\ngetNumShards\n())))\n  \n// Group all records with the same shard.\n  \n.\napply\n(\n""GroupByShard""\n,\n \nGroupByKey\n.<..>())\n  \n// For each window, write per-shard elements to a temporary file. This\n \nis the \n  \n// non-deterministic side effect. If this DoFn is executed multiple\n \ntimes, it will\n  \n// simply write multiple temporary files; only one of these will pass\n \non through \n  \n// to the Finalize stage.\n  \n.\napply\n(\n""WriteTempFile""\n,\n \nParDo\n.\nof\n(\nnew\n \nDoFn\n<..>\n \n{\n    \n@ProcessElement\n     \npublic\n \nvoid\n \nprocessElement\n(\nProcessContext\n \nc\n,\n \nBoundedWindow\n \nwindow\n)\n \n{\n       \n// Write the contents of c.element() to a temporary file.\n       \n// User-provided name policy used to generate a final filename.\n      \nc\n.\noutput\n(\nnew\n \nFileResult\n()).\n    \n}\n  \n}))\n  \n// Group the list of files onto a singleton key.\n  \n.\napply\n(\n""AttachSingletonKey""\n,\n \nWithKeys\n.<..>\nof\n((\nVoid\n)\nnull\n))\n  \n.\napply\n(\n""FinalizeGroupByKey""\n,\n \nGroupByKey\n.<..>\ncreate\n())\n  \n// Finalize the files by atomically renaming them. This operation is\n \nidempotent. \n  \n// Once this DoFn has executed once for a given FileResult, the\n \ntemporary file  \n  \n// is gone, so any further executions will have no effect. \n  \n.\napply\n(\n""Finalize""\n,\n \nParDo\n.\nof\n(\nnew\n \nDoFn\n<..>,\n \nVoid\n>\n \n{\n    \n@ProcessElement\n     \npublic\n \nvoid\n \nprocessElement\n(\nProcessContext\n \nc\n)\n  \n{\n       \nfor\n \n(\nFileResult\n \nresult\n \n:\n \nc\n.\nelement\n())\n \n{\n \n         \nrename\n(\nresult\n.\ngetTemporaryFileName\n(),\n \nresult\n.\ngetFinalFilename\n());\n       \n}\n}}));\nYou can see how the nonidempotent work is done in \nWriteTempFile\n. After\nthe \nGroupByKey\n completes, the \nFinalize\n step will always see the same\nbundles across retries. Because file rename is idempotent,\n this give us an\nexactly-once sink.\nExample Sink: Google BigQuery\nGoogle BigQuery is a fully managed, cloud-native data warehouse. \nBeam\nprovides a BigQuery sink, \nand BigQuery provides a streaming insert API that\nsupports extremely low-latency inserts. This streaming insert API allows\nallows you to tag inserts with a unique ID, and BigQuery will attempt to filter\n14\n15",14641
51-Other Systems.pdf,51-Other Systems,"duplicate inserts with the same ID.\n To use this capability, the BigQuery\nsink must generate statistically unique IDs for each record. It does this by\nusing the \njava.util.UUID\n package, which generates statistically unique\n128-bit IDs.\nGenerating a random universally unique identifier (UUID) is a\nnondeterministic operation, so we must add a \nReshuffle\n before we insert\ninto BigQuery. \nAfter we do this, any retries by Dataflow will always use the\nsame UUID that was shuffled. Duplicate attempts to insert into BigQuery\nwill always have the same insert ID, so BigQuery is able to filter them. The\npseudocode shown in \nExample 5-5\n illustrates how the BigQuery sink is\nimplemented.\nExample 5-5. \nBigQuery sink\n// Apply a unique identifier to each record\nc\n \n.\napply\n(\nnew\n \nDoFn\n<>\n \n{\n  \n@ProcessElement\n  \npublic\n \nvoid\n \nprocessElement\n(\nProcessContext\n \ncontext\n)\n \n{\n   \nString\n \nuniqueId\n \n=\n \nUUID\n.\nrandomUUID\n().\ntoString\n();\n   \ncontext\n.\noutput\n(\nKV\n.\nof\n(\nThreadLocalRandom\n.\ncurrent\n().\nnextInt\n(\n0\n,\n \n50\n),\n                                     \nnew\n \nRecordWithId\n(\ncontext\n.\nelement\n(),\n \nuniqueId\n)));\n \n}\n})\n// Reshuffle the data so that the applied identifiers are stable and will\n \nnot change.\n.\napply\n(\nReshuffle\n.<\nInteger\n,\n \nRecordWithId\n>\nof\n())\n// Stream records into BigQuery with unique ids for deduplication.\n.\napply\n(\nParDo\n.\nof\n(\nnew\n \nDoFn\n<..>\n \n{\n   \n@ProcessElement\n   \npublic\n \nvoid\n \nprocessElement\n(\nProcessContext\n \ncontext\n)\n \n{\n     \ninsertIntoBigQuery\n(\ncontext\n.\nelement\n().\nrecord\n(),\n \ncontext\n.\nelement\n.\nid\n());\n   \n}\n \n});\nAgain we split the sink into a nonidempotent step (generating a random\nnumber), followed by a step that is idempotent. \n15",1857
52-Apache Spark Streaming.pdf,52-Apache Spark Streaming,,0
53-Apache Flink.pdf,53-Apache Flink,"Other Systems\nNow that we have explained Dataflow’s exactly once in detail, let us contrast\nthis with some brief overviews of other popular streaming systems. \nEach\nimplements exactly-once guarantees in a different way and makes different\ntrade-offs as a result.\nApache Spark Streaming\nSpark Streaming uses a microbatch architecture for continuous data\nprocessing. Users logically deal with a stream object; \nhowever, under the\ncovers, Spark represents this \nstream as a continuous series of RDDs.\n Each\nRDD is processed as a batch, and Spark relies on the exactly-once nature of\nbatch processing to ensure correctness; as mentioned previously, techniques\nfor correct batch shuffles have been known for some time. This approach can\ncause increased latency to output—especially for deep pipelines and high\ninput volumes—and often careful tuning is required to achieve desired\nlatency.\nSpark does assume that operations are all idempotent and might replay the\nchain of operations up the current point in the graph. A checkpoint primitive\nis provided, however, that causes an RDD to be materialized, guaranteeing\nthat history prior to that RDD will not be replayed. This checkpoint feature is\nintended for performance reasons (e.g., to prevent replaying an expensive\noperation); however, you can also use it to implement nonidempotent side\neffects.\nApache Flink\nApache Flink also provides exactly-once processing for streaming pipelines\nbut does so in a manner different than either Dataflow or Spark. \nFlink\nstreaming pipelines periodically compute consistent snapshots, each\nrepresenting the consistent point-in-time state of an entire pipeline. \nFlink\nsnapshots are computed progressively, so there is no need to halt all\nprocessing while computing a snapshot. This allows records to continue\n16\nflowing through the system while taking a snapshot, alleviating some of the\nlatency issues with the Spark Streaming approach.\nFlink implements these snapshots by inserting special numbered snapshot\nmarkers into the data streams flowing from sources. As each operator\nreceives a snapshot marker, it executes a specific algorithm allowing it to\ncopy its state to an external location and propagate the snapshot marker to\ndownstream operators. After all operators have executed this snapshot\nalgorithm, a complete snapshot is made available. Any worker failures will\ncause the entire pipeline to roll back its state from the last complete snapshot.\nIn-flight messages do not need to be included in the snapshot. All message\ndelivery in Flink is done via an ordered TCP-based channel. Any connection\nfailures can be handled by resuming the connection from the last good\nsequence number;\n unlike Dataflow, Flink tasks are statically allocated to\nworkers, so it can assume that the connection will resume from the same\nsender and replay the same payloads.\nBecause Flink might roll back to the previous snapshot at any time, any state\nmodifications not yet in a snapshot must be considered tentative. A sink that\nsends data to the world outside the Flink pipeline must wait until a snapshot\nhas completed, and then send only the data that is included in that snapshot.\nFlink provides a \nnotifySnapshotComplete\n callback that allows sinks to\nknow when each snapshot is completed, and send the data onward. Even\nthough this does affect the output latency of Flink pipelines,\n this latency is\nintroduced only at sinks. In practice, this allows Flink to have lower end-to-\nend latency than Spark for deep pipelines because Spark introduces batch\nlatency at each stage in the pipeline.\nFlink’s distributed snapshots are an elegant way of dealing with consistency\nin a streaming pipeline; however, a number of assumptions are made about\nthe pipeline. Failures are assumed to be rare,\n as the impact of a failure\n(rolling back to the previous snapshot) is substantial. To maintain low-latency\noutput, it is also assumed that snapshots can complete quickly. It remains to\nbe seen whether this causes issues on very large clusters where the failure\nrate will likely increase, as will the time needed to complete a snapshot.\n17\n18\n19",4187
54-Summary.pdf,54-Summary,"Implementation is also simplified by assuming that tasks are statically\nallocated to workers (at least within a single snapshot epoch). This\nassumption allows Flink to provide a simple exactly-once transport between\nworkers because it knows that if a connection fails, the same data can be\npulled in order from the same worker. In contrast, tasks in Dataflow are\nconstantly load balanced between workers (and the set of workers is\nconstantly growing and shrinking), so Dataflow is unable to make this\nassumption. This forces Dataflow to implement a much more complex\ntransport layer in order to provide exactly-once processing.\nSummary\nIn summary, exactly-once data processing, which was once thought to be\nincompatible with low-latency results, is quite possible—Dataflow does it\nefficiently without sacrificing latency. This enables far richer uses for stream\nprocessing.\nAlthough this chapter has focused on Dataflow-specific techniques, other\nstreaming systems also provide exactly-once guarantees. Apache Spark\nStreaming runs streaming pipelines as a series of small batch jobs, relying on\nexactly-once guarantees in the Spark batch runner. \nApache Flink uses a\nvariation on Chandy Lamport distributed snapshots to get a running\nconsistent state and can use these snapshots to ensure exactly-once\nprocessing. We encourage you to learn about these other systems, as well, for\na broad understanding of how different stream-processing\n systems work! \n In fact, no system we are aware of that provides at-least once (or better) is\nable to guarantee this, including all other Beam runners.\n Dataflow also provides an accurate batch runner; however, in this context\nwe are focused on the streaming runner.\n The Dataflow optimizer groups many steps together and adds shuffles only\nwhere they are needed.\n Batch pipelines also need to guard against duplicates in shuffle. However\n1\n2\n3\n4\nthe problem is much easier to solve in batch, which is why historical batch\nsystems did do this and streaming systems did not. Streaming runtimes that\nuse a microbatch architecture, such as Spark Streaming, delegate duplicate\ndetection to a batch shuffler.\n A lot of care is taken to make sure this checkpointing is efficient; for\nexample, schema and access pattern optimizations that are intimately tied to\nthe characteristics of the underlying key/value store.\n This is not the custom user-supplied timestamp used for windowing. Rather\nthis is a deterministic processing-time timestamp that is assigned by the\nsending worker.\n Some care needs to be taken to ensure that this algorithm works. Each\nsender must guarantee that the system timestamps it generates are strictly\nincreasing, and this guarantee must be maintained across worker restarts.\n In theory, we could dispense with startup scans entirely by lazily building\nthe Bloom filter for a bucket only when a threshold number of records show\nup with timestamps in that bucket.\n At the time of this writing, a new, more-flexible API called \nSplittableDoFn\nis available for Apache Beam.\n We assume that nobody is maliciously modifying the bytes in the file\nwhile we are reading it.\n Again note that the \nSplittableDoFn API\n has different methods for this.\n Using the \nrequiresDedupping\n override.\n Note that these determinism boundaries might become more explicit in the\nBeam Model at some point. Other Beam runners vary in their ability to\nhandle nondeterministic user code.\n As long as you properly handle the failure when the source file no longer\nexists.\n Due to the global nature of the service, BigQuery does not guarantee that\nall duplicates are removed. Users can periodically run a query over their\ntables to remove any duplicates that were not caught by the streaming insert\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\nAPI. See the BigQuery documentation for more information.\n Resilient Distributed Datasets; Spark’s abstraction of a distributed dataset,\nsimilar to PCollection in Beam.\n These sequence numbers are per connection and are unrelated to the\nsnapshot epoch number.\n Only for nonidempotent sinks. Completely idempotent sinks do not need to\nwait for the snapshot to complete.\n Specifically, Flink assumes that the mean time to worker failure is less\nthan the time to snapshot; otherwise, the pipeline would be unable to make\nprogress.\n16\n17\n18\n19",4387
55-A Streams and Tables Analysis of MapReduce.pdf,55-A Streams and Tables Analysis of MapReduce,"Part II. \nStreams and Tables\nChapter 6. \nStreams and Tables\nYou have reached the part of the book where we talk about streams\n and\ntables.\n If you recall, back in \nChapter 1\n, we briefly discussed two important\nbut orthogonal \ndimensions of data: \ncardinality\n and \nconstitution\n.\n Until now,\nwe’ve focused strictly on the cardinality aspects (bounded versus unbounded)\nand otherwise ignored the constitution aspects (stream versus table). This has\nallowed us to learn about the challenges brought to the table by the\nintroduction of unbounded datasets, without worrying too much about the\nlower-level details that really drive the way things work. We’re now going to\nexpand our horizons and see what the added dimension of constitution brings\nto the mix.\nThough it’s a bit of a stretch, one way to think about this shift in approach is\nto compare the relationship of classical mechanics to quantum mechanics.\nYou know how in physics class they teach you a bunch of classical\nmechanics stuff like Newtonian theory and so on, and then after you think\nyou’ve more or less mastered that, they come along and tell you it was all\nbunk, and classical physics gives you only part of the picture, and there’s\nactually this other thing called quantum mechanics that really explains how\nthings work at a lower level, but it didn’t make sense to complicate matters\nup front by trying to teach you both at once, and...oh wait...we also haven’t\nfully reconciled everything between the two yet, so just squint at it and trust\nus that it all makes sense somehow? Well this is a lot like that, except your\nbrain will hurt less because physics is way harder than data processing, and\nyou won’t have to squint at anything and pretend it makes sense because it\nactually does come together beautifully in the end, which is really cool.\nSo, with the stage appropriately set, the point of this chapter is twofold:\nTo try to describe the relationship \nbetween the Beam Model (as\nwe’ve described it in the book up to this point) and \nthe theory of\n“streams and tables” (as popularized by \nMartin Kleppmann\n and \nJay\nKreps\n, among others, but essentially originating out of the database\nworld). It turns out that stream and table theory does an illuminating\njob of describing the low-level concepts that underlie the Beam\nModel. Additionally, a clear understanding of how they relate is\nparticularly informative when considering how robust stream\nprocessing concepts might be cleanly integrated into SQL\n(something we consider in \nChapter 8\n).\nTo bombard you with bad physics analogies for the sheer fun of it.\nWriting a book is a lot of work; you have to find little joys here and\nthere to keep you going.\nStream-and-Table Basics Or: a Special Theory of\nStream and Table Relativity\nThe basic idea of streams and tables derives from the database world. \nAnyone\nfamiliar with SQL is likely familiar with tables and their core properties,\nroughly summarized as: tables contain rows and columns of data, and each\nrow is uniquely identified by some sort of key, either explicit or implicit.\nIf you \nthink back to your database systems class in college,\n you’ll probably\nrecall the\n data structure underlying most databases is an \nappend-only log\n. As\ntransactions are applied to a table in the database, those transactions are\nrecorded in a log, the contents of which are then serially applied to the table\nto materialize those updates. In streams and tables nomenclature, that log is\neffectively the stream.\nFrom that perspective, we now understand how to create a table from a\nstream: the table is just the result of applying the transaction log of updates\nfound in the stream. But how to do we create a stream from a table? It’s\nessentially the inverse: a stream is a changelog for a table. \nThe motivating\nexample typically used for table-to-stream conversion is \nmaterialized views\n.\nMaterialized views in SQL let you specify a query on a table, which itself is\nthen manifested by the database system as another first-class table. This\nmaterialized view is essentially a cached version of that query, which the\n1\ndatabase system ensures is always up to date as the contents of the source\ntable evolve over time. Perhaps unsurprisingly, materialized views are\nimplemented via the changelog for the original table; any time the source\ntable changes, that change is logged. The database then evaluates that change\nwithin the context of the materialized view’s query and applies any resulting\nchange to the destination materialized view table.\nCombining these two points together and employing yet another questionable\nphysics analogy, we arrive at what one might call the Special Theory of\nStream and Table Relativity:\nStreams \n→\n tables\nThe aggregation of a stream of updates over time yields a table.\nTables \n→\n streams\nThe observation of changes to a table over time yields a stream.\nThis is a very powerful pair of concepts, and their careful application to the\nworld of stream processing is a big reason for the massive success of Apache\nKafka, the ecosystem that is built around these underlying principles.\nHowever, those statements themselves are not quite general enough to allow\nus to tie streams and tables to all of the concepts in the Beam Model. For that,\nwe must go a little bit deeper.\nToward a General Theory of Stream and Table\nRelativity\nIf we want to reconcile stream/table theory with everything we know of the\nBeam Model, we’ll need to tie up some loose ends,\n specifically:\nHow does batch processing fit into all of this?\nWhat is the relationship of streams to bounded and unbounded\ndatasets?\nHow do the four \nwhat\n, \nwhere\n, \nwhen\n, \nhow\n questions map onto a\nstreams/tables world?\nAs we attempt to do so, it will be helpful to have the right mindset about\nstreams and tables. In addition to understanding them in relation to each\nother, as captured by the previous definition, it can be illuminating to define\nthem independent of each other. Here’s a simple way of looking at it that will\nunderscore some of our future analyses:\nTables are data \nat rest\n.\nThis isn’t to say tables are static in any way; \nnearly all useful tables\nare continuously changing over time in some way. But at any given\ntime, a snapshot of the table provides some sort of picture of the\ndataset contained together as a whole.\n In that way, tables act as a\nconceptual resting place for data to accumulate and be observed over\ntime. Hence, data at rest.\nStreams are data \nin motion\n.\nWhereas tables capture a view of the dataset as a whole\n at a \nspecific\npoint in time\n, streams capture the evolution of that data \nover time\n.\nJulian Hyde is fond of saying streams are like the derivatives of\ntables, and tables the integrals of streams, which is a nice way of\nthinking about it for you math-minded individuals out there.\nRegardless, the important feature of streams is that they capture the\ninherent movement of data within a table as it changes. Hence, data\nin motion.\nThough tables and streams are intimately related, it’s important to keep in\nmind that they are very much \nnot\n the same thing, even if there are many cases\nin which one might be fully derived from the other. The differences are subtle\nbut important, as we’ll see.\nBatch Processing Versus Streams and Tables\nWith our \nproverbial knuckles now cracked, let’s start to tie up some loose\nends.\n To begin, we tackle the first one, regarding batch processing. At the\nend, we’ll discover that the resolution to the second issue, regarding the\n2\nrelationship of streams to bounded and unbounded data, will fall out naturally\nfrom the answer for the first. Score one for serendipity.\nA Streams and Tables Analysis of MapReduce\nTo keep our analysis relatively simple, but solidly \nconcrete, as it were,\n let’s\nlook at how a traditional \nMapReduce\n job fits into the streams/tables world.\nAs alluded to by its name, a MapReduce job superficially consists of two\nphases: Map and Reduce. For our purposes, though, it’s useful to look a little\ndeeper and treat it more like six:\nMapRead\nThis consumes the input data and preprocesses them a bit into a standard\nkey/value form for mapping.\nMap\nThis repeatedly (and/or in parallel) consumes a single key/value pair\nfrom the preprocessed input and outputs zero or more key/value pairs.\nMapWrite\nThis clusters together sets of Map-phase output values having identical\nkeys and writes those key/value-list groups to (temporary) persistent\nstorage. In this way, the MapWrite phase is essentially a group-by-key-\nand-checkpoint operation.\nReduceRead\nThis consumes the saved shuffle data and converts them into a standard\nkey/value-list form for reduction.\nReduce\nThis repeatedly (and/or in parallel) consumes a single key and its\nassociated value-list of records and outputs zero or more records, all of\nwhich may optionally remain associated with that same key.\nReduceWrite\n3\nThis writes the outputs from the Reduce phase to the output datastore.\nNote that the MapWrite and ReduceRead phases sometimes are referred to in\naggregate as the Shuffle phase, but for our purposes, it’s better to consider\nthem independently. It’s perhaps also worth noting that the functions served\nby the MapRead and ReduceWrite phases are more commonly referred to\nthese days as sources and sinks. Digressions aside, however, let’s now see\nhow this all relates to streams and tables.\nMap as streams/tables\nBecause we start and end with static\n datasets, it should be clear that we\nbegin with a table and end with a table. \nBut what do we have in between?\nNaively, one might assume that it’s tables all the way down; after all, batch\nprocessing is (conceptually) known to consume and produce tables. And if\nyou think of a batch processing job as a rough analog of executing a classic\nSQL query, that feels relatively natural. But let’s look a little more closely at\nwhat’s really happening, step by step.\nFirst up, MapRead consumes a table and produces \nsomething\n. That\nsomething is consumed next by the Map phase, so if we want to understand\nits nature, a good place to start would be with the Map phase API, which\nlooks something like this in Java:\nvoid\n \nmap\n(\nKI\n \nkey\n,\n \nVI\n \nvalue\n,\n \nEmit\n<\nKO\n,\n \nVO\n>\n \nemitter\n);\nThe map call will be repeatedly invoked for each key/value pair in the input\ntable. If you think this sounds suspiciously like the input table is being\nconsumed as a stream of records, you’d be right. We look more closely at\nhow the table is being converted into a stream later, but for now, suffice it to\nsay that the MapRead phase is iterating over the data at rest in the input table\nand putting them into motion in the form of a stream that is then consumed\nby the Map phase.\nNext up, the Map phase consumes that stream, and then does what? Because\nthe map operation is an element-wise transformation, it’s not doing anything\nthat will halt the moving elements and put them to rest. It might change the\n4\neffective cardinality of the stream by either filtering some elements out or\nexploding some elements into multiple elements, but those elements all\nremain independent from one another after the Map phase concludes. So, it\nseems safe to say that the Map phase both consumes a stream as well as\nproduces a stream.\nAfter the Map phase is done, we enter the MapWrite phase. As I noted\nearlier, the MapWrite groups records by key and then writes them in that\nformat to persistent storage. The \npersistent\n part of the write actually isn’t\nstrictly necessary at this point as long as there’s persistence \nsomewhere\n (i.e.,\nif the upstream inputs are saved and one can recompute the intermediate\nresults from them in cases of failure, similar to the approach Spark takes with\nResilient Distributed Datasets [RDDs]). What \nis\n important is that the records\nare grouped together into some kind of datastore, be it in memory, on disk, or\nwhat have you. This is important because, as a result of this grouping\noperation, records that were previously flying past one-by-one in the stream\nare now brought to rest in a location dictated by their key, thus allowing per-\nkey groups to accumulate as their like-keyed brethren and sistren arrive. Note\nhow similar this is to the definition of stream-to-table conversion provided\nearlier: \nthe aggregation of a stream of updates over time yields a table\n. The\nMapWrite phase, by virtue of grouping the stream of records by their keys,\nhas put those data to rest and thus converted the stream back into a table.\nCool!\nWe’re now halfway through the MapReduce, so, using \nFigure 6-1\n, let’s recap\nwhat we’ve seen so far.\nWe’ve gone from table to stream and back again across three operations.\nMapRead converted the table into a stream, which was then transformed into\na new stream by Map (via the user’s code), which was then converted back\ninto a table by MapWrite. We’re going to find that the next three operations\nin the MapReduce look very similar, so I’ll go through them more quickly,\nbut I still want to point out one important detail along the way.\n5\nFigure 6-1. \nMap phases in a MapReduce. Data in a table are converted to a stream and\nback again.\nReduce as streams/tables\nPicking up where we left\n off after the MapWrite phase, ReduceRead itself is\nrelatively uninteresting. It’s basically identical to MapRead, except that the\nvalues being read are singleton lists of values instead of singleton values,\nbecause the data stored by MapWrite were key/value-list pairs. But it’s still\njust iterating over a snapshot of a table to convert it into a stream. Nothing\nnew here.\nAnd even though it \nsounds\n like it might be interesting, Reduce in this context\nis really just a glorified Map phase that happens to receive a list of values for\neach key instead of a single value. So it’s still just mapping single\n(composite) records into zero or more new records. Nothing particularly new\nhere, either.\nReduceWrite is the one that’s a bit noteworthy. We know already that this\nphase must convert a stream to a table, given that Reduce produces a stream\nand the final output is a table. But how does that happen? If I told you it was\na direct result of key-grouping the outputs from the previous phase into\npersistent storage, just like we saw with MapWrite, you might believe me,\nuntil you remembered that I noted earlier that key-association was an\noptional\n feature of the Reduce phase. With that feature enabled, ReduceWrite\nis\n essentially identical to MapWrite.\n But if that feature is disabled and the\noutputs from Reduce have no associated keys, what exactly is happening to\nbring those data to rest?\nTo understand what’s going on, it’s useful to think again of the semantics of a\nSQL table. Though often recommended, it’s not strictly required for a SQL\ntable to have a primary key uniquely identifying each row. In the case of\nkeyless tables, each row that is inserted is considered to be a new,\nindependent row (even if the data therein are identical to one or more extant\nrows in the table), much as though there were an implicit\nAUTO_INCREMENT field being used as the key (which incidentally, is\nwhat’s effectively happening under the covers in most implementations, even\nthough the “key” in this case might just be some physical block location that\nis never exposed or expected to be used as a logical identifier). This implicit\nunique key assignment is precisely what’s happening in ReduceWrite with\n6\nunkeyed data. Conceptually, there’s still a group-by-key operation\nhappening; that’s what brings the data to rest. But lacking a user-supplied\nkey, the ReduceWrite is treating each record as though it has a new, never-\nbefore-seen key, and effectively grouping each record with itself, resulting\nagain in data at rest.\nTake a look at \nFigure 6-2\n, which shows the entire pipeline from the\nperspective of stream/tables. You can see that it’s a sequence of TABLE →\nSTREAM → STREAM → TABLE → STREAM → STREAM → TABLE.\nEven though we’re processing bounded data and even though we’re doing\nwhat we traditionally think of as batch processing, it’s really just streams and\ntables under the covers.\n7\nFigure 6-2. \nMap and Reduce phases in a MapReduce, viewed from the perspective of\nstreams and tables",16490
56-Reconciling with Batch Processing.pdf,56-Reconciling with Batch Processing,,0
57-What Transformations.pdf,57-What Transformations,"Reconciling with Batch Processing\nSo where\n does\n this leave us with respect to our first two questions?\n1\n. \nQ: \nHow does batch\n processing fit into \nstream/table theory?\nA:\n Quite nicely. The basic pattern is as follows:\na\n. \nTables are read in their entirety to become streams.\nb\n. \nStreams are processed into new streams until a grouping\noperation is hit.\nc\n. \nGrouping turns the stream into a table.\nd\n. \nSteps a through c repeat until you run out of stages in the\npipeline.\n2\n. \nQ:\n How do streams \nrelate to bounded/unbounded data?\nA:\n As we can see from the MapReduce example, streams are simply\nthe in-motion form of data, regardless of whether they’re bounded or\nunbounded.\nTaken from this perspective, it’s easy to see that stream/table theory isn’t\nremotely at odds with batch processing of bounded data. In fact, it only\nfurther supports the idea I’ve been harping on that batch and streaming really\naren’t that different: at the end of the of day, it’s streams and tables all the\nway down.\nWith that, we’re well on our way toward a general theory of streams and\ntables. But to wrap things up cleanly, we last need to revisit the four\nwhat\n/\nwhere\n/\nwhen\n/\nhow\n questions within the streams/tables context, to see how\nthey all relate.\nWhat\n, \nWhere\n, \nWhen\n, and \nHow\n in a Streams and\nTables World\nIn this section, we look at each of the four questions and \nsee how they relate\nto streams and tables. We’ll also answer any questions that may be lingering\nfrom the previous section, one big one being: if grouping is the thing that\nbrings data to rest, what precisely is the “ungrouping” inverse that puts them\nin motion? More on that later. But for now, on to transformations.\nWhat\n: Transformations\nIn \nChapter 3\n, we learned that transformations \ntell us \nwhat\n the pipeline is\ncomputing; that is, whether it’s building models, counting sums, filtering\nspam, and so on.\n We saw in the earlier MapReduce example\n that four of the\nsix stages answered \nwhat\n \nquestions\n:\nMap and Reduce both applied the pipeline author’s element-wise\ntransformation on each key/value or key/value-list pair in the input\nstream, respectively, yielding a new, transformed stream.\nMapWrite and ReduceWrite both grouped the outputs from the\nprevious stage according to the key assigned by that stage (possibly\nimplicitly, in the optional Reduce case), and in doing so transformed\nthe input stream into an output table.\nViewed in that light, you can see that there are essentially two types of \nwhat\ntransforms from the perspective of stream/table theory:\nNongrouping\nThese operations (as we saw in Map and Reduce) simply\n accept a stream\nof records and produce a new, transformed stream of records on the other\nside. Examples of nongrouping transformations are filters (e.g., removing\nspam messages), exploders (i.e., splitting apart a larger composite record\ninto its constituent parts), and mutators (e.g., divide by 100), and so on.\nGrouping\nThese \noperations (as we saw in MapWrite and ReduceWrite) accept a\nstream of records and group them together in some way, thereby\ntransforming the stream into a table. Examples of grouping\ntransformations are joins, aggregations, list/set accumulation, changelog\napplication, histogram creation, machine learning model training, and so\nforth.\nTo get a better sense for how all of this ties together, let’s look at an updated\nversion of \nFigure 2-2\n, where we first began to look at transformations. To\nsave you jumping back there to see what we were talking about, \nExample 6-1\ncontains the code snippet we were using.\nExample 6-1. \nSummation pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals =\n  input.apply(\nSum.integersPerKey()\n);\nThis pipeline is simply \nreading in input data, parsing individual team member\nscores, and then summing those scores per team. \nThe event-time/processing-\ntime visualization of it looks like the diagram presented in \nFigure 6-3\n.\nFigure 6-3. \nEvent-time/processing-time view of classic batch processing\nFigure 6-4\n depicts a more topological view of this pipeline over time,\nrendered from a streams-and-tables perspective.\n00:00 / 00:00\nFigure 6-4. \nStreams and tables view of classic batch processing\nIn the streams and tables version of this visualization, the passage of time is\nmanifested by scrolling the graph area downward in the processing-time\ndimension (y-axis) as time advances. The nice thing about rendering things\nthis way is that it very clearly calls out the difference between nongrouping\nand grouping operations. Unlike our previous diagrams, in which I elided all\ninitial transformations in the pipeline other than the \nSum.integersByKey\n,\nI’ve included the initial parsing operation here, as well, because the\nnongrouping aspect of the parsing operation provides a nice contrast to the\ngrouping aspect of the summation. Viewed in this light, it’s very easy to see\nthe difference between the two. The nongrouping operation does nothing to\nhalt the motion of the elements in the stream, and as a result yields another\nstream on the other side. In contrast, the grouping operation brings all the\nelements in the stream to rest as it adds them together into the final sum.\nBecause this example was running on a batch processing engine over\nbounded data, the final results are emitted only after the end of the input is\nreached. As we noted in \nChapter 2\n this example is sufficient for bounded\ndata, but is too limiting in the context of unbounded data because the input\nwill theoretically never end. But is it really insufficient?\nLooking at the new streams/tables portion of the diagram, if all we’re doing\n00:00 / 00:00\nis calculating sums as our final results (and not actually transforming those\nsums in any additional way further downstream within the pipeline), the table\nwe created with our grouping operation has our answer sitting right there,\nevolving over time as new data arrive. Why don’t we just read our results\nfrom there?\nThis is exactly the point being made by the folks championing stream\nprocessors as a database\n (primarily the Kafka and Flink crews): anywhere\nyou have a grouping operation in your pipeline, you’re creating a table that\nincludes what is effectively the output values of that portion of the stage. If\nthose output values happen to be the final thing your pipeline is calculating,\nyou don’t need to rematerialize them somewhere else if you can read them\ndirectly out of that table. Besides providing quick and easy access to results\nas they evolve over time, this approach saves on compute resources by not\nrequiring an additional sink stage in the pipeline to materialize the outputs,\nyields disk savings by eliminating redundant data storage, and obviates the\nneed for any engineering work building the aforementioned sink stages.\n The\nonly major caveat is that you need to take care to ensure that only the data\nprocessing pipeline has the ability to make modifications to the table. If the\nvalues in the table can change out from under the pipeline due to external\nmodification, all bets are off regarding consistency guarantees.\nA number of folks in the industry have been recommending this approach for\na while now, and it’s being put to great use in a variety of scenarios. We’ve\nseen MillWheel customers within Google do the same thing by serving data\ndirectly out of their Bigtable-based state tables, and we’re in the process of\nadding first-class support for accessing state from outside of your pipeline in\nthe C++–based Apache Beam equivalent we use internally at Google (Google\nFlume); hopefully those concepts will make their way to Apache Beam\nproper someday soon, as well.\nNow, reading from the state tables is great if the values therein are your final\nresults. But, if you have more processing to perform downstream in the\npipeline (e.g., imagine our pipeline was actually computing the top scoring\nteam), we still need some better way to cope with unbounded data, allowing\nus to transform the table back into a stream in a more incremental fashion.\n8\n9",8302
58-Where Windowing.pdf,58-Where Windowing,"For that, we’ll want to journey back through the remaining three questions,\nbeginning with windowing, expanding into triggering, and \nfinally tying it all\ntogether with accumulation.\nWhere\n: Windowing\nAs we know from \nChapter 3\n, windowing\n tells us \nwhere\n in event time\ngrouping occurs.\n Combined with our earlier experiences, we can thus also\ninfer it must play a role in stream-to-table conversion because grouping is\nwhat drives table creation. There are really two aspects of windowing that\ninteract with stream/table theory:\nWindow assignment\nThis effectively just means placing a record \ninto one or more windows.\nWindow merging\nThis is the logic that makes\n dynamic, data-driven types of windows, such\nas sessions, possible.\nThe effect of window assignment is quite straightforward. When a record is\nconceptually placed into a window, the definition of the window is\nessentially combined with the user-assigned key for that record to create an\nimplicit composite key used at grouping time.\n Simple.\nFor completeness, let’s take another look at the original windowing example\nfrom \nChapter 3\n, but from a streams and tables perspective. If you recall, the\ncode snippet looked something like \nExample 6-2\n (with parsing \nnot\n elided this\ntime).\nExample 6-2. \nSummation pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  \n.apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n)\n  .apply(\nSum.integersPerKey()\n);\nAnd the original visualization looked like that shown in \nFigure 6-5\n.\n10\nFigure 6-5. \nEvent-time/processing-time view of windowed summation on a batch engine\nAnd now, \nFigure 6-6\n shows the streams and tables version.\nFigure 6-6. \nStreams and tables view of windowed summation on a batch engine\nAs you might expect, this looks remarkably similar to \nFigure 6-4\n, but with\nfour groupings in the table (corresponding to the four windows occupied by\nthe data) instead of just one.\n But as before, we must wait until the end of our\nbounded input is reached before emitting results. We look at how to address\nthis for unbounded data in the next section, but first let’s touch briefly on\nmerging windows.\nWindow merging\nMoving on to merging, we’ll find that the effect of window merging is more\ncomplicated than window assignment, but still straightforward when you\nthink about the logical operations that would need to happen. \nWhen grouping\na stream into windows that can merge, that grouping operation has to take\ninto account all of the windows that could possibly merge together.\nTypically, this is limited to windows whose data all have the same key\n(because we’ve already established that windowing modifies grouping to not\nbe just by key, but also key and window). For this reason, the system doesn’t\nreally treat the key/window pair as a flat composite key, but rather as a\n00:00 / 00:00\n00:00 / 00:00\nhierarchical key, with the user-assigned key as the root, and the window a\nchild component of that root. When it comes time to actually group data\ntogether, the system first groups by the root of the hierarchy (the key\nassigned by the user). After the data have been grouped by key, the system\ncan then proceed with grouping by window within that key (using the child\ncomponents of the hierarchical composite keys). This act of grouping by\nwindow is where window merging happens.\nWhat’s interesting from a streams and tables perspective is how this window\nmerging changes the mutations that are ultimately applied to a table; that is,\nhow it modifies the changelog that dictates the contents of the table over\ntime. With nonmerging windows, each new element being grouped results in\na single mutation to the table (to add that element to the group for the\nelement’s key+window). With merging windows, the act of grouping a new\nelement can result in one or more existing windows being merged with the\nnew window. So, the merging operation must inspect all of the existing\nwindows for the current key, figure out which windows can merge with this\nnew window, and then atomically commit deletes for the old unmerged\nwindows in conjunction with an insert for the new merged window into the\ntable. This is why systems that support merging windows typically define the\nunit of atomicity/parallelization as key, rather than key+window. Otherwise,\nit would be impossible (or at least much more expensive) to provide the\nstrong consistency needed for correctness guarantees. When you begin to\nlook at it in this level of detail, you can see why it’s so nice to have the\nsystem taking care of the nasty business of dealing\n with window merges. For\nan even closer view of window merging semantics, I refer you to section\n2.2.2 of \n“The Dataflow Model”\n.\nAt the end of the day, windowing is really just a minor alteration to the\nsemantics of grouping, which means it’s a minor alteration to the semantics\nof stream-to-table conversion. For window assignment, it’s as simple as\nincorporating the window into an implicit composite key used at grouping\ntime. When window merging becomes involved, that composite key is treated\nmore like a hierarchical key, allowing the system to handle the nasty business\nof grouping by key, figuring out window merges within that key, and then",5417
59-When Triggers.pdf,59-When Triggers,"atomically applying all the necessary mutations to the corresponding table for\nus. Hooray for layers of abstraction!\nAll that said, we still haven’t actually addressed the problem of converting a\ntable to a stream in a more incremental fashion in the case of unbounded data.\nFor that, we need to revisit triggers.\nWhen\n: Triggers\nWe learned in \nChapter 3\n that we use triggers \nto dictate \nwhen\n the contents\n of a\nwindow will be materialized (with watermarks providing a useful signal of\ninput completeness for certain types of triggers). After data have been\ngrouped together into a window, we use triggers to dictate when that data\nshould be sent downstream. \nIn streams/tables terminology, we understand\nthat grouping means stream-to-table conversion. From there, it’s a relatively\nsmall leap to see that triggers are the complement to grouping; in other\nwords, that “ungrouping” operation we were grasping for earlier. Triggers are\nwhat drive table-to-stream conversion.\nIn streams/tables terminology, triggers are special procedures applied to a\ntable that allow for data within that table to be materialized in response to\nrelevant events. Stated that way, they actually sound suspiciously similar to\nclassic database triggers. And indeed, the choice of name here was no\ncoincidence; they are essentially the same thing. When you specify a trigger,\nyou are in effect writing code that then is evaluated for every row in the state\ntable as time progresses. When that trigger fires, it takes the corresponding\ndata that are currently at rest in the table and puts them into motion, yielding\na new stream.\nLet’s return to our examples. We’ll begin with the simple per-record trigger\nfrom \nChapter 2\n, which simply emits a new result every time a new record\narrives. The code and event-time/processing-time visualization for that\nexample is shown in \nExample 6-3\n. \nFigure 6-7\n presents the results.\nExample 6-3. \nTriggering repeatedly with every record\nPCollection<String>> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n); \nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(Repeatedly(AfterCount(1)))\n);\n  .apply(\nSum.integersPerKey()\n);\nFigure 6-7. \nStreams and tables view of windowed summation on a batch engine\nAs before, new results are materialized every time a new record is\nencountered. Rendered in a streams and tables type of view, this diagram\nwould look like \nFigure 6-8\n.\nFigure 6-8. \nStreams and tables view of windowed summation with per-record triggering on\na streaming engine\nAn interesting side effect of using per-record triggers is how it somewhat\nmasks the effect of data being brought to rest, given that they are then\nimmediately put back into motion again by the trigger. \nEven so, the aggregate\nartifact from the grouping remains at rest in the table, as the ungrouped\nstream of values flows away from it.\nTo get a better sense of the at-rest/in-motion relationship, let’s skip forward\nin our triggering examples to the basic watermark completeness streaming\nexample from \nChapter 2\n, which simply emitted results when complete (due to\nthe watermark passing the end of the window). The code and event-\ntime/processing-time visualization for that example are presented in\n00:00 / 00:00\n00:00 / 00:00\nExample 6-4\n (note that I’m only showing the heuristic watermark version\nhere, for brevity and ease of comparison) and \nFigure 6-9\n illustrates the\nresults.\nExample 6-4. \nWatermark completeness trigger\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(AfterWatermark())\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 6-9. \nEvent-time/processing-time view of windowed summation with a heuristic\nwatermark on a streaming engine\nThanks to the trigger specified in \nExample 6-4\n, which declares that windows\nshould be materialized when the watermark passes them, the system is able to\nemit results in a progressive fashion as the otherwise unbounded input to the\npipeline becomes more and more complete. Looking at the streams and tables\nversion in \nFigure 6-10\n, it looks as you might expect.\nFigure 6-10. \nStreams and tables view of windowed summation with a heuristic watermark\non a streaming engine\nIn this version, you\n can see very clearly the ungrouping effect triggers have\n00:00 / 00:00\n00:00 / 00:00\non the state table. As the watermark passes the end of each window, it pulls\nthe result for that window out of the table and sets it in motion downstream,\nseparate from all the other values in the table. We of course still have the late\ndata issue from before, which we can solve\n again with the more\ncomprehensive\n trigger shown in \nExample 6-5\n.\nExample 6-5. \nEarly, on-time, and late firings via the early/on-time/late API\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1))\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nThe event-time/processing-time diagram looks like \nFigure 6-11\n.\nFigure 6-11. \nEvent-time/processing-time view of windowed summation on a streaming\nengine with early/on-time/late trigger\nWhereas the streams and tables version looks like that shown in \nFigure 6-12\n.\nFigure 6-12. \nStreams and tables view of windowed summation on a streaming engine with\nearly/on-time/late trigger\n00:00 / 00:00\n00:00 / 00:00\nThis version makes even more clear the ungrouping effect triggers have,\nrendering an evolving view of the various independent pieces of the table into\na stream, as dictated by the triggers specified in \nExample 6-6\n.\nThe semantics of all the concrete triggers we’ve talked about so far (event-\ntime, processing-time, count, composites like early/on-time/late, etc.) are just\nas you would expect when viewed from the streams/tables perspective, so\nthey aren’t worth further discussion. However, we haven’t yet spent much\ntime talking about what triggers look like in a classic batch processing\nscenario. Now that we understand what the underlying streams/tables\ntopology of a batch pipeline looks like, this is worth touching upon briefly.\nAt the end of the day, there’s really only one type of trigger used in classic\nbatch scenarios: one that fires when the input is complete. For the initial\nMapRead stage of the MapReduce job we looked at earlier, that trigger would\nconceptually fire for all of the data in the input table as soon as the pipeline\nlaunched, given that the input for a batch job is assumed to be complete from\nthe get go.\n That input source table would thus be converted into a stream of\nindividual elements, after which the Map stage could begin processing them.\nFor table-to-stream conversions in the middle of the pipeline, such as the\nReduceRead stage in our example, the same type of trigger is used. In this\ncase, however, the trigger must actually wait for all of the data in the table to\nbe complete (i.e., what is more commonly referred to as all of the data being\nwritten to the shuffle), much as our example batch pipelines in Figures \n6-4\nand \n6-6\n waited for the end of the input before emitting their final results.\nGiven that classic batch processing effectively always makes use of the input-\ndata-complete trigger, you might ask what any custom triggers specified by\nthe author of the pipeline might mean in a batch scenario. The answer here\nreally is: it depends. There are two aspects worth discussing:\nTrigger guarantees (or lack thereof)\nMost existing batch processing systems have been designed with this\nlock-step read-process-group-write-repeat sequence in mind.\n In such\ncircumstances, it’s difficult to provide any sort of finer-grained trigger\nabilities, because the only place they would manifest any sort of change\n11\nwould be at the final shuffle stage of the pipeline. This doesn’t mean that\nthe triggers specified by the user aren’t honored, however; the semantics\nof triggers are such that it’s possible to resort to lower common\ndenominators when appropriate.\nFor example, an \nAfterWatermark\n trigger is meant to trigger \nafter\n the\nwatermark passes the end of a window. It makes no guarantees how \nfar\nbeyond the end of the window the watermark may be when it fires.\nSimilarly, an \nAfterCount(N)\n trigger only guarantees that \nat least N\nelements have been processed before triggering; \nN\n might very well be all\nof the elements in the input set.\nNote that this clever wording of trigger names wasn’t chosen simply to\naccommodate classic batch systems within the model; it’s a very\nnecessary part of the model itself, given the natural asynchronicity and\nnondeterminism of triggering. Even in a finely tuned, low-latency, true-\nstreaming system, it’s essentially impossible to guarantee that an\nAfterWatermark\n trigger will fire while the watermark is precisely \nat\n the\nend of any given window, except perhaps under the most extremely\nlimited circumstances (e.g., a single machine processing all of the data for\nthe pipeline with a relatively modest load). And even if you could\nguarantee it, what really would be the point? Triggers provide a means of\ncontrolling the flow of data from a table into a stream, nothing more.\nThe blending of batch and streaming\nGiven what we’ve learned in this writeup, it should be clear that the main\nsemantic \ndifference between batch and streaming systems is the ability to\ntrigger tables incrementally.\n But even that isn’t really a semantic\ndifference, but more of a latency/throughput trade-off (because batch\nsystems typically give you higher throughput at the cost of higher latency\nof results).\nThis goes back to something I said in \n“Batch and Streaming Efficiency\nDifferences”\n: there’s really not that much difference between batch and\nstreaming systems today except for an efficiency delta (in favor of batch)\nand a natural ability to deal with unbounded data (in favor of streaming).",10524
60-A Holistic View of Streams and Tables in the Beam Model.pdf,60-A Holistic View of Streams and Tables in the Beam Model,"I argued then that much of that efficiency delta comes from the\ncombination of larger bundle sizes (an explicit compromise of latency in\nfavor of throughput) and more efficient shuffle implementations (i.e.,\nstream → table → stream conversions). From that perspective, it should\nbe possible to provide a system that seamlessly integrates the best of both\nworlds: one which provides the ability to handle unbounded data\nnaturally but can also balance the tensions between latency, throughput,\nand cost across a broad spectrum of use cases by transparently tuning the\nbundle sizes, shuffle implementations, and other such implementation\ndetails under the covers.\nThis is precisely what Apache Beam already does at the API level.\n The\nargument being made here is that there’s room for unification at the\nexecution-engine level, as well. In a world like that, batch and streaming\nwill no longer be a thing, and we’ll be able to say goodbye to both batch\nand\n streaming as independent concepts once and for all. We’ll just have\ngeneral data processing systems that combine the best ideas from both\nbranches in the family tree to provide an optimal experience for the\nspecific use case at hand. Some day.\nAt this point, we can stick a fork in the trigger section. It’s done. We have\nonly one more brief stop on our way to having a holistic view of the\nrelationship between the Beam \nModel and \nstreams-and-tables theory:\naccumulation\n.\nHow\n: Accumulation\nIn \nChapter 2\n, we learned that the three accumulation modes (discarding,\naccumulating, accumulating and retracting\n) tell us how refinements of\nresults relate when a window is triggered multiple times over the course of its\nlife. \nFortunately, the relationship to streams and tables here is pretty\nstraightforward:\nDiscarding mode\n requires the system to either\n throw away the\nprevious value for the window when triggering or keep around a\ncopy of the previous value and compute the delta the next time the\n12\n13\n14\nwindow triggers.\n (This mode might have better been called Delta\nmode.)\nAccumulating mode\n requires no additional work; \nthe current value\nfor the window in the table at triggering time is what is emitted.\n(This mode might have better been called Value mode.)\nAccumulating and retracting mode\n requires keeping around copies\nof all previously triggered (but not yet retracted) values for the\nwindow.\n This list of previous values can grow quite large in the case\nof merging windows like sessions, but is vital to cleanly reverting\nthe effects of those previous trigger firings in cases where the new\nvalue cannot simply be used to overwrite a previous value. (This\nmode might have better been called Value and Retractions mode.)\nThe streams-and-tables visualizations of accumulation modes add little\nadditional insight into their semantics, so we won’t investigate them here.\nA Holistic View of Streams and Tables in the\nBeam Model\nHaving addressed the four questions, we can now take a holistic view of\nstreams and tables in a Beam Model\n pipeline. \nLet’s take our running example\n(the team scores calculation pipeline) and see what its structure looks like at\nthe streams-and-table level. The full code for the pipeline might look\nsomething like \nExample 6-6\n (repeating \nExample 6-4\n).\nExample 6-6. \nOur full score-parsing pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n)\n  .apply(\nSum.integersPerKey()\n);\n14\nBreaking that apart into stages separated by the intermediate \nPCollection\ntypes (where I’ve used more semantic “type” names like \nTeam\n and \nUser\nScore\n than real types for clarity of what is happening at each stage), you\nwould arrive at something like that depicted in \nFigure 6-13\n.\nFigure 6-13. \nLogical phases of a team score summation pipeline, with intermediate\nPCollection types\nWhen you actually run this pipeline, it first goes through an optimizer, whose\njob is to convert this logical execution plan into an optimized, physical\nexecution plan. Each execution engine is different, so actual physical\nexecution plans will vary between runners. But a believable strawperson plan\nmight look something like \nFigure 6-14\n.\nFigure 6-14. \nTheoretical physical phases of a team score summation pipeline, with\nintermediate PCollection types\nThere’s a lot going on here, so let’s walk through all of it. There are three\nmain differences between Figures \n6-13\n and \n6-14\n that we’ll be discussing:\nLogical versus physical operations\nAs part of building a physical execution plan, \nthe underlying engine must\nconvert the logical operations provided by the user into a sequence of\nprimitive operations supported by the engine. In some cases, those\nphysical equivalents look essentially the same (e.g., \nParse\n), and in others,\nthey’re very different.\nPhysical stages and fusion\nIt’s often inefficient to execute\n each logical phase as a fully independent\nphysical stage in the pipeline (with attendant serialization, network\ncommunication, and deserialization overhead between each). As a result,\nthe optimizer will typically try to fuse as many physical operations as\npossible into a single physical stage.\nKeys, values, windows, and partitioning\nTo make it more evident what each physical operation is doing, I’ve\nannotated the intermediate \nPCollection\ns with the type of key, value,\nwindow, and data partitioning in effect at each point.\nLet’s now walk through each logical operation in detail and see what it\ntranslated to in the physical plan and how they all relate\n to streams and tables:\nReadFromSource\nOther than being fused with the physical operation immediately following\nit (\nParse\n), not much interesting happens in translation for\nReadFromSource\n. As far as the characteristics of our data at this point,\nbecause the read is essentially consuming raw input bytes, we basically\nhave raw strings with no keys, no windows, and no (or random)\npartitioning. The original data source can be either a table (e.g., a\nCassandra table) or a stream (e.g., RabbitMQ) or something a little like\nboth (e.g., Kafka in log compaction mode). But regardless, the end result\nof reading from the input source is a stream.\nParse\nThe logical \nParse\n operation also translates in a relatively straightforward\nmanner to the physical version. \nParse\n takes the raw strings and extracts a\nkey (team ID) and value (user score) from them. It’s a nongrouping\noperation, and thus the stream it consumed remains a stream on the other\nside.\nWindow+Trigger\nThis logical operation is spread out across a number of distinct physical\noperations. The first is window assignment, in which each element is\nassigned to a set of windows. That happens immediately in the\nAssignWindows\n operation, which is a nongrouping operation that simply\nannotates each element in the stream with the window(s) it now belongs\nto, yielding another stream on the other side.\nThe second is window merging, which we learned earlier in the chapter\nhappens as part of the grouping operation. As such, it gets sunk down into\nthe \nGroupMergeAndCombine\n operation later in the pipeline. We discuss\nthat operation when we talk about the logical \nSum\n operation next.\nAnd finally, there’s triggering. Triggering happens after grouping and is\nthe way that we’ll convert the table created by grouping back into a\nstream. As such, it gets sunk into its own operation, which follows\nGroupMergeAndCombine\n.\nSum\nSummation is really a composite operation, consisting of a couple pieces:\npartitioning and aggregation. Partitioning is a nongrouping operation that\nredirects the elements in the stream in such a way that elements with the\nsame keys end up going to the same physical machine. Another word for\npartitioning is shuffling, though that term is a bit overloaded because\n“Shuffle” in the MapReduce sense is often used to mean both partitioning\nand\n grouping (\nand\n sorting, for that matter). Regardless, partitioning\nphysically alters the stream in way that makes it groupable but doesn’t do\nanything to actually bring the data to rest. As a result, it’s a nongrouping\noperation that yields another stream on the other side.\nAfter partitioning comes grouping. Grouping itself is a composite\noperation. First comes grouping by key (enabled by the previous\npartition-by-key operation). Next comes window merging and grouping\nby window, as we described earlier. And finally, because summation is\nimplemented as a \nCombineFn\n in Beam (essentially an incremental\naggregation operation), there’s combining, where individual elements are\nsummed together as they arrive. The specific details are not terribly\nimportant for our purposes here. What is important is the fact that, since\nthis is (obviously) a grouping operation, our chain of streams now comes\nto rest in a table containing the summed team totals as they evolve over\ntime.\nWriteToSink\nLastly, we have the write operation, which takes the stream yielded by\ntriggering (which was sunk below the \nGroupMergeAndCombine\n operation,\nas you might recall) and writes it out to our output data sink. That data\nitself can be either a table or stream. If it’s a table, \nWriteToSink\n will\nneed to perform some sort of grouping operation as part of writing the\ndata into the table. If it’s a stream, no grouping will be necessary (though\npartitioning might still be desired; for example, when writing into\nsomething like Kafka).\nThe big takeaway here is not so much the precise details of everything that’s\ngoing on in the physical plan, \nbut more the overall relationship of the\n Beam\nModel to the world of streams and tables. We saw three types of operations:\nnongrouping (e.g., \nParse\n), grouping (e.g., \nGroupMergeAndCombine\n), and\nungrouping (e.g., \nTrigger\n). The nongrouping operations always consumed\nstreams and produced streams on the other side. The grouping operations\nalways consumed streams and yielded tables. And the ungrouping operations\nconsumed tables and yielded streams. These insights, along with everything\nelse we’ve learned along the way, are enough for us to formulate a more\ngeneral theory about the relationship of the Beam Model to streams and",10645
61-A General Theory of Stream and Table Relativity.pdf,61-A General Theory of Stream and Table Relativity,"tables.\nA General Theory of Stream and Table Relativity\nHaving surveyed how stream processing,\n batch processing, the four\nwhat\n/\nwhere\n/\nwhen\n/\nhow\n questions, and the Beam Model as a whole relate to\nstream and table theory, let’s now attempt to articulate a more general\ndefinition of stream and table relativity.\nA general theory of stream and table relativity\n:\nData processing pipelines\n (both batch and streaming) consist of\ntables\n, \nstreams\n, and \noperations\n upon those tables and streams.\nTables\n are \ndata at rest\n, and act as a container for data to accumulate\nand be observed over time.\nStreams\n are \ndata in motion\n, and encode a discretized view of the\nevolution of a table over time.\nOperations\n act upon a stream or table\n and yield a new stream or\ntable. They are categorized as follows:\nstream → stream: Nongrouping (element-wise) operations\nApplying \nnongrouping\n operations to a stream alters the data\nin the stream while leaving them in motion, yielding a new\nstream with possibly different cardinality.\nstream → table: Grouping operations\nGrouping\n data within a stream brings those data to rest,\nyielding a \ntable\n that evolves over time.\nWindowing\n incorporates the dimension of event\ntime into such groupings.\nMerging windows\n dynamically combine over time,\nallowing them to reshape themselves in response to\nthe data observed and dictating that key remain the\nunit of atomicity/parallelization, with window\nbeing a child component of grouping within that\nkey.\ntable → stream: Ungrouping (triggering) operations\nTriggering\n data within a table ungroups them into motion,\nyielding a \nstream\n that captures a view of the table’s\nevolution over time.\nWatermarks\n provide a notion of input\ncompleteness relative to event time, which is a\nuseful reference point when triggering event-\ntimestamped data, particularly data grouped into\nevent-time windows from unbounded streams.\nThe \naccumulation mode\n for the trigger determines\nthe nature of the stream, dictating whether it\ncontains deltas or values, and whether retractions\nfor previous deltas/values are provided.\ntable → table: (none)\nThere are no operations that consume a table and yield a\ntable, because it’s not possible for data to go from rest and\nback to rest without being put into motion. As a result, all\nmodifications to a table are via conversion to a stream and\nback again.\nWhat I love about these rules is that they just make sense. They have a very\nnatural and intuitive feeling about them, and as a result they make it so much\neasier to understand how data flow (or don’t) through a sequence of\noperations. They codify the fact that data exist in one of two constitutions at\nany given time (streams or tables), and they provide simple rules for\nreasoning about the transitions between those states. They demystify\nwindowing by showing how it’s just a slight modification of a thing everyone",2972
62-Summary.pdf,62-Summary,"already innately understands: grouping. They highlight why grouping\noperations in general are always such a sticking point for streaming (because\nthey bring data in streams to rest as tables) but also make it very clear what\nsorts of operations are needed to get things unstuck (triggers; i.e., ungrouping\noperations). And they underscore just how unified batch and stream\nprocessing really are, at a conceptual level.\nWhen I set out to write this chapter, I wasn’t entirely sure what I was going to\nend up with, but the end result was much more satisfying than I’d imagined it\nmight be. In the chapters to come, we use this theory of stream and table\nrelativity again and again to help guide our analyses. And every time, its\napplication will bring clarity and insight that would otherwise have been\nmuch harder to gain. Streams and tables are the best.\nSummary\nIn this chapter, we first established the basics of stream and table theory. We\nfirst defined streams and tables relatively:\nstreams \n→\n tables\nThe aggregation of a stream of updates over time yields a table.\ntables \n→\n streams\nThe observation of changes to a table over time yields a stream.\nWe next defined them independently:\nTables are data \nat rest\n.\nStreams are data \nin motion\n.\nWe then assessed the classic MapReduce model of batch computation from a\nstreams and tables perspective and came to the conclusion that the following\nfour steps describe batch processing from that perspective:\n1\n. \nTables are read in their entirety to become streams.\n2\n. \nStreams are processed into new streams until a grouping operation is\nhit.\n3\n. \nGrouping turns the stream into a table.\n4\n. \nSteps 1 through 3 repeat until you run out of operations in the\npipeline.\nFrom this analysis, we were able to see that streams are just as much a part of\nbatch processing as they are stream processing, and also that the idea of data\nbeing a stream is an orthogonal one from whether the data in question are\nbounded or unbounded.\nNext, we spent a good deal of time considering the relationship between\nstreams and tables and the robust, out-of-order stream processing semantics\nafforded by the Beam Model, ultimately arriving at the general theory of\nstream and table relativity we enumerated in the previous section. In addition\nto the basic definitions of streams and tables, the key insight in that theory is\nthat there are four (really, just three) types of operations in a data\n processing\npipeline:\nstream \n→\n stream\nNongrouping (element-wise) operations\nstream \n→\n table\nGrouping operations\ntable \n→\n stream\nUngrouping (triggering) operations\ntable \n→\n table\n(nonexistent)\nBy classifying operations in this way, it becomes trivial to understand how\ndata flow through (and linger within) a given pipeline over time.\nFinally, and perhaps most important of all, we learned this: when you look at\nthings from the streams-and-tables point of view, it becomes abundantly clear\nhow batch and streaming really are just the same thing conceptually.\nBounded or unbounded, it doesn’t matter.\n It’s streams\n and tables from top to\nbottom.\n</bad-physics-jokes>\n If you didn’t go to college for computer science and you’ve made it this far\nin the book, you are likely either 1) my parents, 2) masochistic, or 3) very\nsmart (and for the record, I’m not implying these groups are necessarily\nmutually exclusive; figure that one out if you can, Mom and Dad! <winky-\nsmiley/>).\n And note that in some cases, the tables themselves can accept time as a\nquery parameter, allowing you to peer backward in time to snapshots of the\ntable as it existed in the past.\n Note that no guarantees are made about the keys of two successive records\nobserved by a single mapper, because no key-grouping has occurred yet. The\nexistence of the key here is really just to allow keyed datasets to be consumed\nin a natural way, and if there are no obvious keys for the input data, they’ll all\njust share what is effectively a global null key.\n Calling the inputs to a batch job “static” might be a bit strong. In reality, the\ndataset being consumed can be constantly changing as it’s processed; that is,\nif you’re reading directly from an HBase/Bigtable table within a timestamp\nrange in which the data aren’t guaranteed to be immutable. But in most cases,\nthe recommended approach is to ensure that you’re somehow processing a\nstatic snapshot of the input data, and any deviation from that assumption is at\nyour own peril.\n Note that grouping a stream by key is importantly distinct from simply\npartitioning\n that stream by key, which ensures that all records with the same\nkey end up being processed by the same machine but doesn’t do anything to\nput the records to rest. They instead remain in motion and thus continue on as\na stream. A grouping operation is more like a partition-by-key followed by a\nwrite to the appropriate group for that partition, which is what puts them to\n1\n2\n3\n4\n5\nrest and turns the stream into a table.\n One giant difference, from an implementation perspective at least, being\nthat ReduceWrite, knowing that keys have already been grouped together by\nMapWrite, and further knowing that Reduce is unable to alter keys for the\ncase in which its outputs remain keyed, can simply accumulate the outputs\ngenerated by reducing the values for a single key in order to group them\ntogether, which is much simpler than the full-blown shuffle implementation\nrequired for a MapWrite phase.\n Another way of looking at it is that there are two types of tables: updateable\nand appendable; this is the way the Flink folks have framed it for their Table\nAPI. But even though that’s a great intuitive way of capturing the observed\nsemantics of the two situations, I think it obscures the underlying nature of\nwhat’s actually happening that causes a stream to come to rest as a table; that\nis, grouping.\n Though as we can clearly see from this example, it’s not just a streaming\nthing; you can get the same effect with a batch system if its state tables are\nworld readable.\n This is particularly painful if a sink for your storage system of choice\ndoesn’t exist yet; building proper sinks that can uphold consistency\nguarantees is a surprisingly subtle and difficult task.\n This also means that if you place a value into multiple windows—for\nexample, sliding windows—the value must conceptually be duplicated into\nmultiple, independent records, one per window. Even so, it’s possible in\nsome cases for the underlying system to be smart about how it treats certain\ntypes of overlapping windows, thus optimize away the need for actually\nduplicating the value. Spark, for example, does this for sliding windows.\n Note that this high-level conceptual view of how things work in batch\npipelines belies the complexity of efficiently triggering an entire table of data\nat once, particularly when that table is sizeable enough to require a plurality\nof machines to process. The \nSplittableDoFn API\n recently added to Beam\nprovides some insight into the mechanics involved.\n6\n7\n8\n9\n10\n11\n12\n And yes, if you blend batch and streaming together you get Beam, which is\nwhere that name came from originally. For reals.\n This is why you should always use an Oxford comma.\n Note that in the case of merging windows, in addition to merging the\ncurrent values for the two windows to yield a merged current value, the\nprevious values for those two windows would need to be merged, as well, to\nallow for the later calculation of a merged delta come triggering time.\n12\n13\n14",7645
63-Motivation.pdf,63-Motivation,"Chapter 7. \nThe Practicalities of\nPersistent State\nWhy do people write books?\n When you factor out the joy of creativity, a\ncertain fondness for grammar and punctuation, and perhaps the occasional\ntouch of narcissism, you’re basically left with the desire to capture an\notherwise ephemeral idea so that it can be revisited in the future. At a very\nhigh level, I’ve just motivated and explained persistent state in data\nprocessing pipelines.\nPersistent state is, quite literally, the tables we just talked about in \nChapter 6\n,\nwith the additional requirement that the tables be robustly stored in a media\nrelatively immune to loss.\n Stored on local disk counts, as long as you don’t\nask your Site Reliability Engineers. Stored on a replicated set of disks is\nbetter. Stored on a replicated set of disks in distinct physical locations is\nbetter still. Stored in memory once definitely doesn’t count. Stored in\nreplicated memory across multiple machines with UPS power backup and\ngenerators onsite maybe does. You get the picture.\nIn this chapter, our objective is to do the following:\nMotivate the need for persistent state within pipelines\nLook at two forms of implicit state often found within pipelines\nConsider a real-world use case (advertising conversion attribution)\nthat lends itself poorly to implicit state, use that to motivate the\nsalient features of a general, explicit form of persistent state\nmanagement\nExplore a concrete manifestation of one such state API, as found in\nApache Beam",1526
64-Correctness and Efficiency.pdf,64-Correctness and Efficiency,"Motivation\nTo begin, let’s more precisely motivate persistent state. \nWe know from\nChapter 6\n that grouping is what gives us tables. And the core of what I\npostulated at the beginning of this chapter was correct: the point of persisting\nthese tables is to capture the otherwise ephemeral data contained therein. But\nwhy is that necessary?\nThe Inevitability of Failure\nThe answer to that question is most clearly seen in the case of processing\nunbounded input data, so we’ll start there.\n The main issue is that pipelines\nprocessing unbounded data are effectively intended to run forever. But\nrunning forever is a far more demanding Service-Level Objective than can be\nachieved by the environments in which these pipelines typically execute.\nLong-running pipelines will inevitably see interruptions thanks to machine\nfailures, planned maintenance, code changes, and the occasional\nmisconfigured command that takes down an entire cluster of production\npipelines. To ensure that they can resume where they left off when these\nkinds of things happen, long-running pipelines need some sort of durable\nrecollection of where they were before the interruption. That’s where\npersistent state comes in.\nLet’s expand on that idea a bit beyond unbounded data. Is this only relevant\nin the unbounded case? \nDo batch pipelines use persistent state, and why or\nwhy not? As with nearly every other batch-versus-streaming question we’ve\ncome across, the answer has less to do with the nature of batch and streaming\nsystems themselves (perhaps unsurprising given what we learned in\nChapter 6\n), and more to do with the types of datasets they historically have\nbeen used to process.\nBounded datasets by nature are finite in size.\n As a result, systems that process\nbounded data (historically batch systems) have been tailored to that use case.\nThey often assume that the input can be reprocessed in its entirety upon\nfailure. In other words, if some piece of the processing pipeline fails and if\nthe input data are still available, we can simply restart the appropriate piece\nof the processing pipeline and let it read the same input again.\n This is called\nreprocessing the input\n.\nThey might also assume failures are infrequent and thus optimize for the\ncommon case by persisting as little as possible, accepting the extra cost of\nrecomputation upon failure.\n For particularly expensive, multistage pipelines,\nthere might be some sort of per-stage global checkpointing that allows for\nmore efficiently resuming execution (typically as part of a shuffle), but it’s\nnot a strict requirement and might not be present in many systems.\nUnbounded datasets, on the other hand, must be assumed to have infinite\nsize. As a result, systems that process unbounded data (historically streaming\nsystems) have been built to match.\n They never assume that all of the data will\nbe available for reprocessing, only some known subset of it. To provide at-\nleast-once or exactly-once semantics, any data that are no longer available for\nreprocessing must be accounted for in durable checkpoints. And if at-most-\nonce is all you’re going for, you don’t need checkpointing.\nAt the end of the day, there’s nothing batch- or streaming-specific about\npersistent state. State can be useful in both circumstances. It just happens to\nbe critical when processing unbounded data, so you’ll find that streaming\nsystems typically provide more sophisticated support for persistent state.\nCorrectness and Efficiency\nGiven the inevitability of failures and the need to cope with them, persistent\nstate can be seen\n as providing two things:\nA \nbasis for correctness\n in light of ephemeral inputs. \nWhen\nprocessing bounded data, it’s often safe to assume inputs stay around\nforever;\n with unbounded data, this assumption typically falls short\nof reality. Persistent state allows you to keep around the intermediate\nbits of information necessary to allow processing to continue when\nthe inevitable happens, even after your input source has moved on\nand forgotten about records it gave you previously.\nA way to \nminimize work duplicated and data persisted\n as part of\n1\ncoping with failures. \nRegardless of whether your inputs are\nephemeral, when your pipeline experiences a machine failure, any\nwork on the failed machine that wasn’t checkpointed somewhere\nmust be redone. Depending upon the nature of the pipeline and its\ninputs, this can be costly in two dimensions: the amount of work\nperformed during reprocessing, and the amount of input data stored\nto support reprocessing.\nMinimizing duplicated work is relatively straightforward.\n By\ncheckpointing partial progress\n within a pipeline (both the\nintermediate results computed as well as the current location within\nthe input as of checkpointing time), it’s possible to greatly reduce\nthe amount of work repeated when failures occur because none of\nthe operations that came before the checkpoint need to be replayed\nfrom durable inputs. Most commonly, this involves data at rest (i.e.,\ntables), which is why we typically refer to persistent state in the\ncontext of tables and grouping. \nBut there are persistent forms of\nstreams (e.g., Kafka and its relatives) that serve this function, as\nwell.\nMinimizing the amount of data persisted is a larger discussion, one\nthat will consume a sizeable chunk of this chapter. For now, at least,\nsuffice it to say that, for many real-world use cases, rather than\nremembering all of the raw inputs within a checkpoint for any given\nstage in the pipeline, it’s often practical to instead remember some\npartial, intermediate form of the ongoing calculation that consumes\nless space than all of the original inputs (for example, when\ncomputing a mean, the total sum and the count of values seen are\nmuch more compact than the complete list of values contributing to\nthat sum and count). Not only can checkpointing these intermediate\ndata drastically reduce the amount of data that you need to remember\nat any given point in the pipeline, it also commensurately reduces\nthe amount of reprocessing needed for that specific stage to recover\nfrom a failure.\nFurthermore, by intelligently garbage-collecting those bits of",6266
65-Raw Grouping.pdf,65-Raw Grouping,"persistent state that are no longer needed (i.e., state for records\nwhich are known to have been processed completely by the pipeline\nalready), the amount of data stored in persistent state for a given\npipeline can be kept to a manageable size over time, even when the\ninputs are technically infinite. This is how pipelines processing\nunbounded data can continue to run effectively forever, while still\nproviding strong consistency guarantees but without a need for\ncomplete recall of the original inputs to the pipeline.\nAt the end of the day, persistent state is really just a means of providing\ncorrectness and efficient fault tolerance in data processing pipelines. The\namount of support needed in either of those dimensions depends greatly upon\nthe natures of the inputs to the pipeline and the operations being performed.\nUnbounded inputs tend to require more correctness support than bounded\ninputs. Computationally expensive operations tend to demand more\nefficiency support than computationally cheap operations.\nImplicit State\nLet’s now begin to talk about the practicalities of persistent state. \nIn most\ncases, this essentially boils down to finding the right balance between always\npersisting everything (good for consistency, bad for efficiency) and never\npersisting anything (bad for consistency, good for efficiency). We’ll begin at\nthe always-persisting-everything end of the spectrum, and work our way in\nthe other direction, looking at ways of trading off complexity of\nimplementation for efficiency without compromising consistency (because\ncompromising consistency by never persisting anything is the easy way out\nfor cases in which consistency doesn’t matter, and a nonoption, otherwise).\nAs before, we use the Apache Beam APIs to concretely ground our\ndiscussions, but the concepts we discuss are applicable across most systems\nin existence today.\nAlso, because there isn’t much you can do to reduce the size of raw inputs,\nshort of perhaps compressing the data, our discussion centers around the\nways data are persisted within the intermediate state tables created as part of\ngrouping operations within a pipeline. The inherent nature of grouping\nmultiple records together into some sort of composite will provide us with\nopportunities to eke out gains in efficiency at the cost of implementation\ncomplexity.\nRaw Grouping\nThe first step in our exploration, at the always-persisting-everything end of\nthe spectrum, is the most straightforward implementation of grouping within\na pipeline: raw grouping of the inputs.\n The grouping operation in this case is\ntypically akin to list appending: any time a new element arrives in the group,\nit’s appended to the list of elements seen for that group.\nIn Beam, this is exactly what you get when you apply a \nGroupByKey\ntransform to a \nPCollection\n. The stream representing that \nPCollection\n in\nmotion is grouped by key to yield a table at rest containing the records from\nthe stream,\n grouped together as lists of values with identical keys. This\nshows up in the \nPTransform\n signature for \nGroupByKey\n, which declares the\ninput as a \nPCollection\n of \nK\n/\nV\n pairs, and the output as a collection of\nK\n/\nIterable<V>\n pairs:\nclass\n \nGroupByKey\n<\nK\n,\n \nV\n>\n \nextends\n \nPTransform\n<\n    \nPCollection\n<\nKV\n<\nK\n,\n \nV\n>>,\n \nPCollection\n<\nKV\n<\nK\n,\n \nIterable\n<\nV\n>>>>>\nEvery time a trigger fires for a key+window in that table, it will emit a new\npane for that key+window, with the value being the \nIterable<V>\n we see in\nthe preceding signature.\nLet’s look at an example in action in \nExample 7-1\n. We’ll take the summation\npipeline from \nExample 6-5\n (the one with fixed windowing and early/on-\ntime/late triggers) and convert it to use raw grouping instead of incremental\ncombination (which we discuss a little later in this chapter). We do this by\nfirst applying a \nGroupByKey\n transformation to the parsed user/score\nkey/value pairs. The \nGroupByKey\n operation performs raw grouping, yielding\na \nPCollection\n with key/value pairs of users and \nIterable<Integer>\n2\ngroups of scores. We then sum up all of the \nInteger\ns in each iterable by\nusing a simple \nMapElements\n lambda that converts the \nIterable<Integer>\ninto an \nIntStream<Integer>\n and calls \nsum\n on it.\nExample 7-1. \nEarly, on-time, and late firings via the early/on-time/late API\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> \ngroupedScores\n = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n)\n  .apply(\nGroupBy.<String, Integer>create()\n);\nPCollection<KV<Team, Integer>> totals = input\n  \n.apply(\nMapElements.via((KV<String, Iterable<Integer>> kv) ->\n    \nStreamSupport.intStream(\n      \nkv.getValue().spliterator(), false).sum())\n);\nLooking at this pipeline in action, we would see something like that depicted\nin \nFigure 7-1\n.",5215
66-Incremental Combining.pdf,66-Incremental Combining,"Figure 7-1. \nSummation via raw grouping of inputs with windowing and early/on-time/late\ntriggering. The raw inputs are grouped together and stored in the table via the\nGroupByKey transformation. After being triggered, the MapElements lambda sums the raw\ninputs within a single pane together to yield per-team scores.\nComparing this to \nFigure 6-10\n (which was using incremental combining,\ndiscussed shortly), it’s clear to see this is a lot worse. \nFirst, we’re storing a lot\nmore data: instead of a single integer per window, we now store all the inputs\nfor that window. Second, if we have multiple trigger firings, we’re\nduplicating effort by re-summing inputs we already added together for\nprevious trigger firings. And finally, if the grouping operation is the point at\nwhich we checkpoint our state to persistent storage, upon machine failure we\nagain must recompute the sums for any retriggerings of the table. That’s a lot\nof duplicated data and computation. Far better would be to incrementally\ncompute and checkpoint the actual sums, which is an example of \nincremental\ncombining\n.\nIncremental Combining\n00:00 / 00:00\nThe first step in our journey of trading implementation complexity for\nefficiency is incremental combining.\n This concept is manifested in the \nBeam\nAPI\n via the \nCombineFn\n class. In a nutshell, incremental combining is a form\nof automatic state built upon a user-defined\n associative and commutative\ncombining operator (if you’re not sure what I mean by these two terms, I\ndefine them more precisely in a moment). Though not strictly necessary for\nthe discussion that follows, the important parts of the CombineFn API look\nlike \nExample 7-2\n.\nExample 7-2. \nAbbreviated CombineFn API from Apache Beam\nclass\n \nCombineFn\n<\nInputT\n,\n \nAccumT\n,\n \nOutputT\n>\n \n{\n    \n// Returns an accumulator representing the empty value.\n    \nAccumT\n \ncreateAccumulator\n();\n    \n// Adds the given input value into the given accumulator\n    \nAccumT\n \naddInput\n(\nAccumT\n \naccumulator\n,\n \nInputT\n \ninput\n);\n    \n// Merges the given accumulators into a new, combined accumulator\n    \nAccumT\n \nmergeAccumulators\n(\nIterable\n<\nAccumT\n>\n \naccumulators\n);\n    \n// Returns the output value for the given accumulator\n    \nOutputT\n \nextractOutput\n(\nAccumT\n \naccumulator\n);\n}\nA \nCombineFn\n accepts inputs of type \nInputT\n, which can be combined together\ninto partial aggregates called \naccumulators\n, of type \nAccumT\n.\n These\naccumulators themselves can also be combined together into new\naccumulators. And finally, an accumulator can be transformed into an output\nvalue of type \nOutputT\n. For something like an average, the inputs might be\nintegers, the accumulators pairs of integers (i.e., \nPair<sum of inputs,\ncount of inputs>\n), and the output a single floating-point value\nrepresenting the mean value of the combined inputs.\nBut what does all this structure buy us? Conceptually, the basic idea with\nincremental \ncombining is that many types of aggregations (sum, mean, etc.)\nexhibit the following properties:\nIncremental aggregations possess an \nintermediate form\n that captures\nthe \npartial progress\n of combining a set of \nN\n inputs \nmore compactly\nthan the full list of those inputs themselves (i.e., the \nAccumT\n type in\nCombineFn\n). As discussed earlier, for mean, this is a sum/count pair.\nBasic summation is even simpler, with a single number as its\naccumulator. A histogram would have a relatively complex\naccumulator composed of buckets, where each bucket contains a\ncount for the number of values seen within some specific range. In\nall three cases, however, the amount of space consumed by an\naccumulator that represents the aggregation of \nN\n elements remains\nsignificantly smaller than the amount of space consumed by the\noriginal \nN\n elements themselves, particularly as the size of \nN\n grows.\nIncremental aggregations are \nindifferent to ordering\n across two\ndimensions:\nIndividual elements\n, meaning:\nCOMBINE(a, b) == COMBINE(b, a)\nGroupings of elements\n, meaning:\nCOMBINE(COMBINE(a, b), c) == COMBINE(a,\nCOMBINE(b, c))\nThese properties are \nknown as \ncommutativity\n and \nassociativity\n,\nrespectively.\n In concert,\n they effectively mean that we are free to\ncombine elements and partial aggregates in any arbitrary order and\nwith any arbitrary subgrouping. This allows us to optimize the\naggregation in two ways:\nIncrementalization\nBecause the order of individual inputs\n doesn’t matter, \nwe don’t\nneed to buffer all of the inputs ahead of time and then process\nthem in some strict order (e.g., in order of event time; note,\nhowever, that this remains independent of \nshuffling\n elements by\nevent time into proper event-time windows before aggregating);\nwe can simply combine them one-by-one as they arrive. This not\n3\nonly greatly reduces the amount of data that must be buffered\n(thanks to the first property of our operation, which stated the\nintermediate form was a more compact representation of partial\naggregation than the raw inputs themselves), but also spreads the\ncomputation load more evenly over time (versus aggregating a\nburst of inputs all at once after the full input set has been\nbuffered).\nParallelization\nBecause the order in which\n partial subgroups\n of inputs are\ncombined doesn’t matter, we’re free to arbitrarily distribute the\ncomputation of those subgroups. More specifically, we’re free to\nspread the computation of those\n subgroups across a plurality of\nmachines. This optimization is at the heart of MapReduce’s\nCombiners\n (the genesis of Beam’s \nCombineFn\n).\nMapReduce’s Combiner optimization is essential to solving the\nhot-key problem, where some sort of grouping computation is\nperformed on an input stream that is too large to be reasonably\nprocessed by a single physical machine. A canonical example is\nbreaking down high-volume analytics data (e.g., web traffic to a\npopular website) across a relatively low number of dimensions\n(e.g., by web browser family: Chrome, Firefox, Safari, etc.). For\nwebsites with a particularly high volume of traffic, it’s often\nintractable to calculate stats for any single web browser family\non a single machine, even if that’s the only thing that machine is\ndedicated to doing; there’s simply too much traffic to keep up\nwith. But with an associative and commutative operation like\nsummation, it’s possible to spread the initial aggregation across\nmultiple machines, each of which computes a partial aggregate.\nThe set of partial aggregates generated by those machines\n(whose size is now many of orders magnitude smaller than the\noriginal inputs) might then be further combined together on a\nsingle machine to yield the final aggregate result.\nAs an aside, this ability to parallelize also yields one additional\nbenefit: the aggregation operation\n is naturally compatible with\nmerging windows. When two windows merge, their values must\nsomehow be merged, as well. \nWith raw grouping, this means\nmerging the two full lists of buffered values together, which has\na cost of O(N). But with a \nCombineFn\n, it’s a simple combination\nof two partial aggregates, typically an O(1) operation.\nFor the sake of \ncompleteness, \nconsider\n again \nExample 6-5\n, shown in\nExample 7-3\n, which implements a summation pipeline using incremental\ncombination.\nExample 7-3. \nGrouping and summation via incremental combination, as in\nExample 6-5\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\n                 \nAfterWatermark()\n                   \n.withEarlyFirings(AlignedDelay(ONE_MINUTE))\n                   \n.withLateFirings(AfterCount(1)))\n)\n  .apply(\nSum.integersPerKey()\n);\nWhen executed, we get what we saw \nFigure 6-10\n (shown here in \nFigure 7-2\n).\nCompared to \nFigure 7-1\n, this is clearly a big improvement, with much greater\nefficiency in terms of amount of data stored and amount of computation\nperformed.",8250
67-Generalized State.pdf,67-Generalized State,"Figure 7-2. \nGrouping and summation via incremental combination. In this version,\nincremental sums are computed and stored in the table rather than lists of inputs, which\nmust later be summed together independently.\nBy providing a more compact intermediate representation for a grouping\noperation, and by relaxing requirements on ordering (both at the element and\nsubgroup levels), Beam’s \nCombineFn\n trades off a certain amount of\nimplementation complexity in exchange for increases in efficiency. In doing\nso, it provides a clean solution for the hot-key problem and also plays nicely\nwith the concept of merging windows.\nOne shortcoming, however, is that your grouping operation must fit within a\nrelatively restricted structure. This is all well and good for sums, means, and\nso on, but there are plenty of real-world use cases in which a more general\napproach, one which allows precise control over trade-offs of complexity and\nefficiency, is needed. We’ll look next at what such a general approach entails.\nGeneralized State\n00:00 / 00:00\nThough both of the implicit approaches we’ve looked at so \nfar have their\nmerits, they each fall short in one \ndimension: flexibility.\n The raw grouping\nmethod requires you to always buffer up the raw inputs to the grouping\noperation before processing the group in whole, so there’s no way to partially\nprocess some of the data along the way; it’s all or nothing. The incremental\ncombining approach specifically allows for partial processing but with the\nrestriction that the processing in question be commutative and associative and\nhappen as records arrive one-by-one.\nIf we want to support a more generalized approach to streaming persistent\nstate, we need something more flexible.\n Specifically, we need flexibility in\nthree dimensions:\nFlexibility in data structures; that is, an ability to structure the data\nwe write and read in ways that are most appropriate and efficient for\nthe task at hand. Raw grouping essentially provides an appendable\nlist, and incremental combination essentially provides a single value\nthat is always written and read in its entirety. But there are myriad\nother ways in which we might want to structure our persistent data,\neach with different types of access patterns and associated costs:\nmaps, trees, graphs, sets, and so on. Supporting a variety of\npersistent data types is critical for efficiency.\nBeam supports flexibility in \ndata types by allowing a single \nDoFn\n to\ndeclare multiple state fields, each of a specific type. In this way,\nlogically independent pieces of state (e.g., visits and impressions)\ncan be stored separately, and semantically different types of state\n(e.g., maps and lists) can be accessed in ways that are natural given\ntheir types of access patterns.\nFlexibility in write and read granularity; that is, an ability to tailor\nthe amount and type of data written or read at any given time for\noptimal efficiency.\n What this boils down to is the ability to write and\nread precisely the necessary amount of data at any given point of\ntime: no more, and no less (and in parallel as much as possible).\nThis goes hand in hand with the previous point, given that dedicated\ndata types allow for focused types of access patterns (e.g., a set-\nmembership operation that can use something like a Bloom filter\nunder the covers to greatly minimize the amount of data read in\ncertain circumstances). But it goes beyond it, as well; for example,\nallowing multiple large reads to be dispatched in parallel (e.g., via\nfutures).\nIn Beam, flexibly granular writes and reads are enabled via datatype-\nspecific APIs that provide fine-grained access capabilities, combined\nwith an asynchronous I/O mechanism that allows for writes and\nreads to be batched together for efficiency.\nFlexibility in scheduling of processing; that is, an ability\n to bind the\ntime at which specific types of processing occur to the progress of\ntime in either of the two time domains we care about: event-time\ncompleteness and processing time. Triggers provide a restricted set\nof flexibility here, with completeness triggers providing a way to\nbind processing to the watermark passing the end of the window,\nand repeated update triggers providing a way to bind processing to\nperiodic progress in the processing-time domain. But for certain use\ncases (e.g., certain types of joins, for which you don’t necessarily\ncare about input completeness of the entire window, just input\ncompleteness up to the event-time of a specific record in the join),\ntriggers are insufficiently flexible. Hence, our need for a more\ngeneral solution.\nIn Beam, flexible scheduling of processing \nis provided via \ntimers\n.\n A\ntimer is a special type of state that binds a specific point in time in\neither supported time domain (event time or processing time) with a\nmethod to be called when that point in time is reached. In this way,\nspecific bits of processing can be delayed until a more appropriate\ntime in the future.\nThe common thread among these three characteristics is \nflexibility\n. A specific\nsubset of use cases are served very well by the relatively inflexible\napproaches of raw grouping or incremental combination. But when tackling\n4",5287
68-Case Study Conversion Attribution.pdf,68-Case Study Conversion Attribution,"anything outside their relatively narrow domain of expertise, those options\noften fall short. When that happens, you need the power and flexibility of a\nfully general-state API to let you tailor your utilization of persistent state\noptimally.\nTo think of it another way, raw grouping and incremental combination are\nrelatively high-level abstractions that enable the pithy expression of pipelines\nwith (in the case of combiners, at least) some good properties for automatic\noptimizations. But sometimes you need to go low level to get the behavior or\nperformance you need. That’s what generalized state lets you do.\nCase Study: Conversion Attribution\nTo see this in\n action, let’s now look at a use case that is poorly served by both\nraw grouping and \nincremental \ncombination: \nconversion attribution\n. \nThis is a\ntechnique that sees widespread use across the advertising world to provide\nconcrete feedback on the effectiveness of advertisements. Though relatively\neasy to understand, its somewhat diverse set of requirements doesn’t fit\nnicely into either of the two types of implicit state we’ve considered so far.\nImagine that you have an analytics pipeline that monitors traffic to a website\nin conjunction with advertisement impressions that directed traffic to that\nsite. The goal is to provide attribution of specific advertisements shown to a\nuser toward the achievement of some goal on the site itself (which often\nmight lie many steps beyond the initial advertisement landing page), such as\nsigning up for a mailing list or purchasing an item.\nFigure 7-3\n shows an example set of website visits, goals, and ad impressions,\nwith one attributed conversion highlighted in red. \nBuilding up conversion\nattributions over an unbounded, out-of-order stream of data requires keeping\ntrack of impressions, visits, and goals seen so far. That’s where persistent\nstate comes in.\nFigure 7-3. \nExample conversion attribution\nIn this diagram, a user’s traversal of various pages on a website is represented\nas a graph. Impressions are advertisements that were shown to the user and\nclicked, resulting in the user visiting a page on the site. Visits represent a\nsingle page viewed on the site. Goals are specific visited pages that have been\nidentified as a desired destination for users (e.g., completing a purchase, or\nsigning up for a mailing list). The goal of conversion attribution is to identify\nad impressions that resulted in the user achieving some goal on the site. In\nthis figure, there is one such conversion highlighted in red. Note that events\nmight arrive out of order, hence the event-time axis in the diagram and the\nwatermark reference point indicating the time up to which input is believed to\nbe correct.\nA lot goes into building a robust, large-scale attribution pipeline, but there are\na few aspects worth calling out explicitly. Any such pipeline we attempt to\nbuild must do the following:\nHandle out-of-order data\nBecause the website traffic and ad \nimpression data come from separate\nsystems, both of which are implemented as distributed collection services\nthemselves, the data might arrive wildly out of order. Thus, our pipeline\nmust be resilient to such disorder.\nHandle high volumes of data\nNot only \nmust we assume that this pipeline will be processing data for a\nlarge number of independent users, but depending upon the volume of a\ngiven ad campaign and the popularity of a given website, we might need\nto store a large amount of impression and/or traffic data as we attempt to\nbuild evidence of attribution. For example, it would not be unheard of to\nstore 90 days worth of visit, impression, and goal tree\n data per user to\nallow us to build up attributions that span multiple months’ worth of\nactivity.\nProtect against spam\nGiven that money is involved, correctness is paramount. \nNot only must\nwe ensure that visits and impressions are accounted for exactly once\n(something we’ll get more or less for free by simply using an execution\nengine that supports effectively-once processing), but we must also guard\nour advertisers against spam attacks\n that attempt to charge advertisers\nunfairly. For example, a single ad that is clicked multiple times in a row\nby the same user will arrive as multiple impressions, but as long as those\nclicks occur within a certain amount of time of one another (e.g., within\nthe same day), they must be attributed only once. In other words, even if\nthe system guarantees we’ll see every individual \nimpression\n once, we\nmust also perform some manual deduplication across impressions that are\ntechnically different events but which our business logic dictates we\ninterpret as duplicates.\nOptimize for performance\n5",4765
69-Conversion Attribution with Apache Beam.pdf,69-Conversion Attribution with Apache Beam,"Above all, because of the potential scale of this pipeline, we must always\nkeep an eye toward optimizing the performance of our pipeline.\n Persistent\nstate, because of the inherent costs of writing to persistent storage, can\noften be the performance bottleneck in such a pipeline. As such, the\nflexibility characteristics we discussed earlier will be critical in ensuring\nour design is as performant as \npossible\n.\nConversion Attribution with Apache Beam\nNow that we understand \nthe basic problem\n that we’re trying to solve and\nhave some \nof the important requirements squarely in mind,\n let’s use Beam’s\nState and Timers API to build a basic conversion attribution transformation.\nWe’ll write this just like we would any other \nDoFn\n in Beam, but we’ll make\nuse of state and timer extensions that allow us to write and read persistent\nstate and timer fields. Those of you that want to follow along in real code can\nfind the full implementation on \nGitHub\n.\nNote that, as with all grouping operations in Beam, usage of the State API is\nscoped to the current key and window, with window lifetimes dictated by the\nspecified allowed lateness parameter; in this example, we’ll be operating\nwithin a single global window. Parallelism is linearized per key, as with most\nDoFns\n. Also note that, for simplicity, we’ll be eliding the manual garbage\ncollection of visits and impressions falling outside of our 90-day horizon that\nwould be necessary to keep the persisted state from growing forever.\nTo begin, let’s define a few POJO classes for visits, impressions, a\nvisit/impression union (used for joining), and completed attributions, as\nshown in \nExample 7-4\n.\nExample 7-4. \nPOJO definitions of Visit, Impression, VisitOrImpression, and\nAttribution objects\n@DefaultCoder\n(\nAvroCoder\n.\nclass\n)\nclass\n \nVisit\n \n{\n    \n@Nullable\n \nprivate\n \nString\n \nurl\n;\n    \n@Nullable\n \nprivate\n \nInstant\n \ntimestamp\n;\n    \n// The referring URL. Recall that we’ve constrained the problem in\n \nthis\n    \n// example to assume every page on our website has exactly one\n \npossible\n    \n// referring URL, to allow us to solve the problem for simple trees\n    \n// rather than more general DAGs.\n    \n@Nullable\n \nprivate\n \nString\n \nreferer\n;\n    \n@Nullable\n \nprivate\n \nboolean\n \nisGoal\n;\n    \n@SuppressWarnings\n(\n""unused""\n)\n    \npublic\n \nVisit\n()\n \n{\n    \n}\n    \npublic\n \nVisit\n(\nString\n \nurl\n,\n \nInstant\n \ntimestamp\n,\n \nString\n \nreferer\n,\n                 \nboolean\n \nisGoal\n)\n \n{\n \nthis\n.\nurl\n \n=\n \nurl\n;\n \nthis\n.\ntimestamp\n \n=\n \ntimestamp\n;\n \nthis\n.\nreferer\n \n=\n \nreferer\n;\n \nthis\n.\nisGoal\n \n=\n \nisGoal\n;\n    \n}\n    \npublic\n \nString\n \nurl\n()\n \n{\n \nreturn\n \nurl\n;\n \n}\n    \npublic\n \nInstant\n \ntimestamp\n()\n \n{\n \nreturn\n \ntimestamp\n;\n \n}\n    \npublic\n \nString\n \nreferer\n()\n \n{\n \nreturn\n \nreferer\n;\n \n}\n    \npublic\n \nboolean\n \nisGoal\n()\n \n{\n \nreturn\n \nisGoal\n;\n \n}\n    \n@Override\n    \npublic\n \nString\n \ntoString\n()\n \n{\n        \nreturn\n \nString\n.\nformat\n(\n""{ %s %s from:%s%s }""\n,\n \nurl\n,\n \ntimestamp\n,\n \nreferer\n,\n                             \nisGoal\n \n?\n \n"" isGoal""\n \n:\n \n""""\n);\n    \n}\n}\n@DefaultCoder\n(\nAvroCoder\n.\nclass\n)\nclass\n \nImpression\n \n{\n    \n@Nullable\n \nprivate\n \nLong\n \nid\n;\n    \n@Nullable\n \nprivate\n \nString\n \nsourceUrl\n;\n    \n@Nullable\n \nprivate\n \nString\n \ntargetUrl\n;\n    \n@Nullable\n \nprivate\n \nInstant\n \ntimestamp\n;\n    \npublic\n \nstatic\n \nString\n \nsourceAndTarget\n(\nString\n \nsource\n,\n \nString\n \ntarget\n)\n \n{\n \n        \nreturn\n \nsource\n \n+\n \n"":""\n \n+\n \ntarget\n;\n    \n}\n    \n@SuppressWarnings\n(\n""unused""\n)\n    \npublic\n \nImpression\n()\n \n{\n    \n}\n    \npublic\n \nImpression\n(\nLong\n \nid\n,\n \nString\n \nsourceUrl\n,\n \nString\n \ntargetUrl\n,\n                      \nInstant\n \ntimestamp\n)\n \n{\n        \nthis\n.\nid\n \n=\n \nid\n;\n \nthis\n.\nsourceUrl\n \n=\n \nsourceUrl\n;\n \nthis\n.\ntargetUrl\n \n=\n \ntargetUrl\n;\n \nthis\n.\ntimestamp\n \n=\n \ntimestamp\n;\n    \n}\n    \npublic\n \nLong\n \nid\n()\n \n{\n \nreturn\n \nid\n;\n \n}\n    \npublic\n \nString\n \nsourceUrl\n()\n \n{\n \nreturn\n \nsourceUrl\n;\n \n}\n    \npublic\n \nString\n \ntargetUrl\n()\n \n{\n \nreturn\n \ntargetUrl\n;\n \n}\n    \npublic\n \nString\n \nsourceAndTarget\n()\n \n{\n        \nreturn\n \nsourceAndTarget\n(\nsourceUrl\n,\n \ntargetUrl\n);\n    \n}\n    \npublic\n \nInstant\n \ntimestamp\n()\n \n{\n \nreturn\n \ntimestamp\n;\n \n}\n    \n@Override\n    \npublic\n \nString\n \ntoString\n()\n \n{\n \nreturn\n \nString\n.\nformat\n(\n""{ %s source:%s target:%s %s }""\n,\n                             \nid\n,\n \nsourceUrl\n,\n \ntargetUrl\n,\n \ntimestamp\n);\n    \n}\n}\n@DefaultCoder\n(\nAvroCoder\n.\nclass\n)\nclass\n \nVisitOrImpression\n \n{\n    \n@Nullable\n \nprivate\n \nVisit\n \nvisit\n;\n    \n@Nullable\n \nprivate\n \nImpression\n \nimpression\n;\n    \n@SuppressWarnings\n(\n""unused""\n)\n    \npublic\n \nVisitOrImpression\n()\n \n{\n    \n}\n    \npublic\n \nVisitOrImpression\n(\nVisit\n \nvisit\n,\n \nImpression\n \nimpression\n)\n \n{\n \nthis\n.\nvisit\n \n=\n \nvisit\n;\n \nthis\n.\nimpression\n \n=\n \nimpression\n;\n    \n}\n    \npublic\n \nVisit\n \nvisit\n()\n \n{\n \nreturn\n \nvisit\n;\n \n}\n    \npublic\n \nImpression\n \nimpression\n()\n \n{\n \nreturn\n \nimpression\n;\n \n}\n}\n@DefaultCoder\n(\nAvroCoder\n.\nclass\n)\nclass\n \nAttribution\n \n{\n    \n@Nullable\n \nprivate\n \nImpression\n \nimpression\n;\n    \n@Nullable\n \nprivate\n \nList\n<\nVisit\n>\n \ntrail\n;\n    \n@Nullable\n \nprivate\n \nVisit\n \ngoal\n;\n    \n@SuppressWarnings\n(\n""unused""\n)\n    \npublic\n \nAttribution\n()\n \n{\n    \n}\n    \npublic\n \nAttribution\n(\nImpression\n \nimpression\n,\n \nList\n<\nVisit\n>\n \ntrail\n,\n \nVisit\n \ngoal\n)\n \n{\n \nthis\n.\nimpression\n \n=\n \nimpression\n;\n \nthis\n.\ntrail\n \n=\n \ntrail\n;\n \nthis\n.\ngoal\n \n=\n \ngoal\n;\n    \n}\n    \npublic\n \nImpression\n \nimpression\n()\n \n{\n \nreturn\n \nimpression\n;\n \n}\n    \npublic\n \nList\n<\nVisit\n>\n \ntrail\n()\n \n{\n \nreturn\n \ntrail\n;\n \n}\n    \npublic\n \nVisit\n \ngoal\n()\n \n{\n \nreturn\n \ngoal\n;\n \n}\n    \n@Override\n    \npublic\n \nString\n \ntoString\n()\n \n{\n \nStringBuilder\n \nbuilder\n \n=\n \nnew\n \nStringBuilder\n();\n \nbuilder\n.\nappend\n(\n""imp=""\n \n+\n \nimpression\n.\nid\n()\n \n+\n \n"" ""\n \n+\n \nimpression\n.\nsourceUrl\n());\n \nfor\n \n(\nVisit\n \nvisit\n \n:\n \ntrail\n)\n \n{\n     \nbuilder\n.\nappend\n(\n"" → ""\n \n+\n \nvisit\n.\nurl\n());\n \n}\n \nbuilder\n.\nappend\n(\n"" → ""\n \n+\n \ngoal\n.\nurl\n());\n \nreturn\n \nbuilder\n.\ntoString\n();\n    \n}\n}\nWe next define a Beam \nDoFn\n to consume a flattened collection of \nVisit\ns and\nImpression\ns, keyed by the user. In turn, it will yield a collection of\nAttribution\ns. Its signature looks like \nExample 7-5\n.\nExample 7-5. \nDoFn signature for our conversion attribution transformation\nclass\n \nAttributionFn\n \nextends\n \nDoFn\n<\nKV\n<\nString\n,\n \nVisitOrImpression\n>,\n \nAttribution\n>\nWithin that \nDoFn\n, we need to implement the following logic:\n1\n. \nStore all visits in a map keyed by their URL so that we can easily\nlook them up when tracing visit trails backward from a goal.\n2\n. \nStore all impressions in a map keyed by the URL they referred to, so\nwe can identify impressions that initiated a trail to a goal.\n3\n. \nAny time we see a visit that happens to be a goal, set an event-time\ntimer for the timestamp of the goal. Associated with this timer will\nbe a method that performs goal attribution for the pending goal. This\nwill ensure that attribution only happens once the input leading up to\nthe goal is complete.\n4\n. \nBecause Beam lacks support for a dynamic set of timers (currently\nall timers must be declared at pipeline definition time, though each\nindividual timer can be set and reset for different points in time at\nruntime), we also need to keep track of the timestamps for all of the\ngoals we still need to attribute. This will allow us to have a single\nattribution timer set for the minimum timestamp of all pending\ngoals. After we attribute the goal with the earliest timestamp, we set\nthe timer again with the timestamp of the next earliest goal.\nLet’s now walk through the implementation in pieces. First up, we need to\ndeclare specifications for all of our state and timer fields within the \nDoFn\n. For\nstate, the specification dictates the type of data structure for the field itself\n(e.g., map or list) as well as the type(s) of data contained therein, and their\nassociated coder(s); for timers, it dictates the associated time domain. Each\nspecification is then assigned a unique ID string (via the \n@StateID\n/\n@TimerId\nannotations), which will allow us to dynamically associate these\nspecifications with parameters and methods later on. For our use case, we’ll\ndefine (in \nExample 7-6\n) the following:\nTwo \nMapState\n specifications for visits and impressions\nA single \nSetState\n specification for goals\nA \nValueState\n specification for keeping track of the minimum\npending goal timestamp\nA \nTimer\n specification for our delayed attribution logic\nExample 7-6. \nState field specifications\nclass\n \nAttributionFn\n \nextends\n \nDoFn\n<\nKV\n<\nString\n,\n \nVisitOrImpression\n>,\n \nAttribution\n>\n \n{\n    \n@StateId\n(\n""visits""\n)\n    \nprivate\n \nfinal\n \nStateSpec\n<\nMapState\n<\nString\n,\n \nVisit\n>>\n \nvisitsSpec\n \n=\n \nStateSpecs\n.\nmap\n(\nStringUtf8Coder\n.\nof\n(),\n \nAvroCoder\n.\nof\n(\nVisit\n.\nclass\n));\n    \n// Impressions are keyed by both sourceUrl (i.e., the query) and\n \ntargetUrl\n    \n// (i.e., the click), since a single query can result in multiple\n \nimpressions.\n    \n// The source and target are encoded together into a single string by\n \nthe\n    \n// Impression.sourceAndTarget method.\n    \n@StateId\n(\n""impressions""\n)\n    \nprivate\n \nfinal\n \nStateSpec\n<\nMapState\n<\nString\n,\n \nImpression\n>>\n \nimpSpec\n \n=\n \nStateSpecs\n.\nmap\n(\nStringUtf8Coder\n.\nof\n(),\n \nAvroCoder\n.\nof\n(\nImpression\n.\nclass\n));\n    \n@StateId\n(\n""goals""\n)\n    \nprivate\n \nfinal\n \nStateSpec\n<\nSetState\n<\nVisit\n>>\n \ngoalsSpec\n \n=\n \nStateSpecs\n.\nset\n(\nAvroCoder\n.\nof\n(\nVisit\n.\nclass\n));\n    \n@StateId\n(\n""minGoal""\n)\n    \nprivate\n \nfinal\n \nStateSpec\n<\nValueState\n<\nInstant\n>>\n \nminGoalSpec\n \n=\n \nStateSpecs\n.\nvalue\n(\nInstantCoder\n.\nof\n());\n    \n@TimerId\n(\n""attribution""\n)\n    \nprivate\n \nfinal\n \nTimerSpec\n \ntimerSpec\n \n=\n \nTimerSpecs\n.\ntimer\n(\nTimeDomain\n.\nEVENT_TIME\n);\n...\n \ncontinued\n \nin\n \nExample\n \n7\n-\n7\n \nbelow\n \n...\nNext up, we implement our core \n@ProcessElement\n method. This is the\nprocessing logic that will run every time a new record arrives. As noted\nearlier, we need to record visits and impressions to persistent state as well as\nkeep track of goals and manage the timer that will bind our attribution logic\nto the progress of event-time completeness as tracked by the watermark.\nAccess to state and timers is provided via parameters passed to our\n@ProcessElement\n method, and the Beam runtime invokes our method with\nappropriate parameters indicated by \n@StateId\n and \n@TimerId\n annotations.\nThe logic itself is then relatively straightforward, as demonstrated in\nExample 7-7\n.\nExample 7-7. \n@ProcessElement implementation\n...\n \ncontinued\n \nfrom\n \nExample\n \n7\n-\n6\n \nabove\n \n...\n@ProcessElement\npublic\n \nvoid\n \nprocessElement\n(\n        \n@Element\n \nKV\n<\nString\n,\n \nVisitOrImpression\n>\n \nkv\n,\n \n@StateId\n(\n""visits""\n)\n \nMapState\n<\nString\n,\n \nVisit\n>\n \nvisitsState\n,\n \n@StateId\n(\n""impressions""\n)\n \nMapState\n<\nString\n,\n \nImpression\n>\n \nimpressionsState\n,\n \n@StateId\n(\n""goals""\n)\n \nSetState\n<\nVisit\n>\n \ngoalsState\n,\n \n@StateId\n(\n""minGoal""\n)\n \nValueState\n<\nInstant\n>\n \nminGoalState\n,\n \n@TimerId\n(\n""attribution""\n)\n \nTimer\n \nattributionTimer\n)\n \n{\n    \nVisit\n \nvisit\n \n=\n \nkv\n.\ngetValue\n().\nvisit\n();\n    \nImpression\n \nimpression\n \n=\n \nkv\n.\ngetValue\n().\nimpression\n();\n    \nif\n \n(\nvisit\n \n!=\n \nnull\n)\n \n{\n \nif\n \n(!\nvisit\n.\nisGoal\n())\n \n{\n     \nLOG\n.\ninfo\n(\n""Adding visit: {}""\n,\n \nvisit\n);\n     \nvisitsState\n.\nput\n(\nvisit\n.\nurl\n(),\n \nvisit\n);\n \n}\n \nelse\n \n{\n     \nLOG\n.\ninfo\n(\n""Adding goal (if absent): {}""\n,\n \nvisit\n);\n     \ngoalsState\n.\naddIfAbsent\n(\nvisit\n);\n     \nInstant\n \nminTimestamp\n \n=\n \nminGoalState\n.\nread\n();\n     \nif\n \n(\nminTimestamp\n \n==\n \nnull\n \n||\n \nvisit\n.\ntimestamp\n().\nisBefore\n(\nminTimestamp\n))\n \n{\n                \nLOG\n.\ninfo\n(\n""Setting timer from {} to {}""\n,\n                         \nUtils\n.\nformatTime\n(\nminTimestamp\n),\n                         \nUtils\n.\nformatTime\n(\nvisit\n.\ntimestamp\n()));\n                \nattributionTimer\n.\nset\n(\nvisit\n.\ntimestamp\n());\n  \nminGoalState\n.\nwrite\n(\nvisit\n.\ntimestamp\n());\n     \n}\n     \nLOG\n.\ninfo\n(\n""Done with goal""\n);\n \n}\n    \n}\n    \nif\n \n(\nimpression\n \n!=\n \nnull\n)\n \n{\n        \n// Dedup logical impression duplicates with the same source and\n \ntarget URL.\n \n// In this case, first one to arrive (in processing time) wins. A\n \nmore\n \n// robust approach might be to pick the first one in event time,\n \nbut that\n        \n// would require an extra read before commit, so the processing-\ntime\n        \n// approach may be slightly more performant.\n        \nLOG\n.\ninfo\n(\n""Adding impression (if absent): {} → {}""\n,\n                 \nimpression\n.\nsourceAndTarget\n(),\n \nimpression\n);\n \nimpressionsState\n.\nputIfAbsent\n(\nimpression\n.\nsourceAndTarget\n(),\n \nimpression\n);\n    \n}\n}\n...\n \ncontinued\n \nin\n \nExample\n \n7\n-\n8\n \nbelow\n \n...\nNote how this ties back to our three desired capabilities in a general state\nAPI:\nFlexibility in data structures\nWe have maps, a\n set, a value, and a timer. \nThey allow us to efficiently\nmanipulate our state in ways that are effective for our algorithm.\nFlexibility in write and read granularity\nOur \n@ProcessElement\n method is called for \nevery single visit and\nimpression we process. As such, we need it to be as efficient as possible.\nWe take advantage of the ability to make fine-grained, blind writes only\nto the specific fields we need. We also only ever read from state within\nour \n@ProcessElement\n method in the uncommon case of encountering a\nnew goal. And when we do, we read only a single integer value, without\ntouching the (potentially much larger) maps and list.\nFlexibility in scheduling of processing\nThanks to timers, we’re able to delay our complex goal \nattribution logic\n(defined next) until we’re confident we’ve received all the necessary\ninput data, minimizing duplicated work and maximizing efficiency.\nHaving defined the core processing logic, let’s now look at our final piece of\ncode, the goal attribution method. This method is annotated with an\n@TimerId\n annotation to identify it as the code to execute when the\ncorresponding attribution timer fires. The logic here is significantly more\ncomplicated than the \n@ProcessElement\n method:\n1\n. \nFirst, we need to load the entirety of our visit and impression maps,\nas well as our set of goals. We need the maps to piece our way\nbackward through the attribution trail we’ll be building, and we need\nthe goals to know which goals we’re attributing as a result of the\ncurrent timer firing, as well as the next pending goal we want to\nschedule for attribution in the future (if any).\n2\n. \nAfter we’ve loaded our state, we process goals for this timer one at a\ntime in a loop, repeatedly:\nChecking to see if any impressions referred the user to the\ncurrent visit in the trail (beginning with the goal). If so,\nwe’ve completed attribution of this goal and can break out\nof the loop and emit the attribution trail.\nChecking next to see if any visits were the referrer for the\ncurrent visit. If so, we’ve found a back pointer in our trail,\nso we traverse it and start the loop over.\nIf no matching impressions or visits are found, we have a\ngoal that was reached organically, with no associated\nimpression. In this case, we simply break out of the loop\nand move on to the next goal, if any.\n3\n. \nAfter we’ve exhausted our list of goals ready for attribution, we set a\ntimer for the next pending goal in the list (if any) and reset the\ncorresponding \nValueState\n tracking the minimum pending goal\ntimestamp.\nTo keep things concise, we first look at the core goal attribution logic, shown\nin \nExample 7-8\n, which roughly corresponds to point 2 in the preceding list.\nExample 7-8. \nGoal attribution logic\n...\n \ncontinued\n \nfrom\n \nExample\n \n7\n-\n7\n \nabove\n \n...\nprivate\n \nImpression\n \nattributeGoal\n(\nVisit\n \ngoal\n,\n     \nMap\n<\nString\n,\n \nVisit\n>\n \nvisits\n,\n     \nMap\n<\nString\n,\n \nImpression\n>\n \nimpressions\n,\n     \nList\n<\nVisit\n>\n \ntrail\n)\n \n{\n    \nImpression\n \nimpression\n \n=\n \nnull\n;\n    \nVisit\n \nvisit\n \n=\n \ngoal\n;\n    \nwhile\n \n(\ntrue\n)\n \n{\n        \nString\n \nsourceAndTarget\n \n=\n \nImpression\n.\nsourceAndTarget\n(\n            \nvisit\n.\nreferer\n(),\n \nvisit\n.\nurl\n());\n        \nLOG\n.\ninfo\n(\n""attributeGoal: visit={} sourceAndTarget={}""\n,\n                 \nvisit\n,\n \nsourceAndTarget\n);\n \nif\n \n(\nimpressions\n.\ncontainsKey\n(\nsourceAndTarget\n))\n \n{\n     \nLOG\n.\ninfo\n(\n""attributeGoal: impression={}""\n,\n \nimpression\n);\n     \n// Walked entire path back to impression. Return success.\n     \nreturn\n \nimpressions\n.\nget\n(\nsourceAndTarget\n);\n \n}\n \nelse\n \nif\n \n(\nvisits\n.\ncontainsKey\n(\nvisit\n.\nreferer\n()))\n \n{\n     \n// Found another visit in the path, continue searching.\n     \nvisit\n \n=\n \nvisits\n.\nget\n(\nvisit\n.\nreferer\n());\n     \ntrail\n.\nadd\n(\n0\n,\n \nvisit\n);\n \n}\n \nelse\n \n{\n     \nLOG\n.\ninfo\n(\n""attributeGoal: not found""\n);\n     \n// Referer not found, trail has gone cold. Return failure.\n     \nreturn\n \nnull\n;\n \n}\n    \n}\n}\n...\n \ncontinued\n \nin\n \nExample\n \n7\n-\n9\n \nbelow\n \n...\nThe rest of the code (eliding a few simple helper methods), which handles\ninitializing and fetching state, invoking the attribution logic, and handling\ncleanup to schedule any remaining pending goal attribution attempts, looks\nlike \nExample 7-9\n.\nExample 7-9. \nOverall @TimerId handling logic for goal attribution\n...\n \ncontinued\n \nfrom\n \nExample\n \n7\n-\n8\n \nabove\n \n...\n@OnTimer\n(\n""attribution""\n)\npublic\n \nvoid\n \nattributeGoal\n(\n        \n@Timestamp\n \nInstant\n \ntimestamp\n,\n \n@StateId\n(\n""visits""\n)\n \nMapState\n<\nString\n,\n \nVisit\n>\n \nvisitsState\n,\n \n@StateId\n(\n""impressions""\n)\n \nMapState\n<\nString\n,\n \nImpression\n>\n \nimpressionsState\n,\n \n@StateId\n(\n""goals""\n)\n \nSetState\n<\nVisit\n>\n \ngoalsState\n,\n \n@StateId\n(\n""minGoal""\n)\n \nValueState\n<\nInstant\n>\n \nminGoalState\n,\n \n@TimerId\n(\n""attribution""\n)\n \nTimer\n \nattributionTimer\n,\n \nOutputReceiver\n<\nAttribution\n>\n \noutput\n)\n \n{\n    \nLOG\n.\ninfo\n(\n""Processing timer: {}""\n,\n \nUtils\n.\nformatTime\n(\ntimestamp\n));\n    \n// Batch state reads together via futures.\n    \nReadableState\n<\nIterable\n<\nMap\n.\nEntry\n<\nString\n,\n \nVisit\n>\n \n>\n \n>\n \nvisitsFuture\n        \n=\n \nvisitsState\n.\nentries\n().\nreadLater\n();\n    \nReadableState\n<\nIterable\n<\nMap\n.\nEntry\n<\nString\n,\n \nImpression\n>\n \n>\n \n>\n \nimpressionsFuture\n        \n=\n \nimpressionsState\n.\nentries\n().\nreadLater\n();\n    \nReadableState\n<\nIterable\n<\nVisit\n>>\n \ngoalsFuture\n \n=\n \ngoalsState\n.\nreadLater\n();\n    \n// Accessed the fetched state.\n    \nMap\n<\nString\n,\n \nVisit\n>\n \nvisits\n \n=\n \nbuildMap\n(\nvisitsFuture\n.\nread\n());\n    \nMap\n<\nString\n,\n \nImpression\n>\n \nimpressions\n \n=\n \nbuildMap\n(\nimpressionsFuture\n.\nread\n());\n    \nIterable\n<\nVisit\n>\n \ngoals\n \n=\n \ngoalsFuture\n.\nread\n();\n    \n// Find the matching goal\n    \nVisit\n \ngoal\n \n=\n \nfindGoal\n(\ntimestamp\n,\n \ngoals\n);\n    \n// Attribute the goal\n    \nList\n<\nVisit\n>\n \ntrail\n \n=\n \nnew\n \nArrayList\n<>();\n    \nImpression\n \nimpression\n \n=\n \nattributeGoal\n(\ngoal\n,\n \nvisits\n,\n \nimpressions\n,\n \ntrail\n);\n    \nif\n \n(\nimpression\n \n!=\n \nnull\n)\n \n{\n \noutput\n.\noutput\n(\nnew\n \nAttribution\n(\nimpression\n,\n \ntrail\n,\n \ngoal\n));\n \nimpressions\n.\nremove\n(\nimpression\n.\nsourceAndTarget\n());\n    \n}\n    \ngoalsState\n.\nremove\n(\ngoal\n);\n    \n// Set the next timer, if any.\n    \nInstant\n \nminGoal\n \n=\n \nminTimestamp\n(\ngoals\n,\n \ngoal\n);\n    \nif\n \n(\nminGoal\n \n!=\n \nnull\n)\n \n{\n \nLOG\n.\ninfo\n(\n""Setting new timer at {}""\n,\n \nUtils\n.\nformatTime\n(\nminGoal\n));\n \nminGoalState\n.\nwrite\n(\nminGoal\n);\n \nattributionTimer\n.\nset\n(\nminGoal\n);\n    \n}\n \nelse\n \n{\n \nminGoalState\n.\nclear\n();\n    \n}\n}\nThis code block ties back to the three desired capabilities of a general state\nAPI in very similar ways as the \n@ProcessElement\n method, with one\nnoteworthy difference:\nFlexibility in write and read granularity\nWe were able to make a single, coarse-grained read up front to load all of\nthe data in the maps and set. This is typically much more efficient than\nloading each field separately, or even worse loading each field element by\nelement. It also shows the importance of being able to traverse the\nspectrum of access granularities, from fine-grained to coarse-grained.\nAnd that’s it! We’ve implemented a basic conversion attribution pipeline, in a\nway that’s efficient enough to be operated at respectable scales using a\nreasonable amount of resources. And importantly, it functions properly in the\nface of out-of-order data. If you look at the dataset used for the \nunit test\n in\nExample 7-10\n, you can see it presents a number of challenges, even at this\nsmall scale:\nTracking and attributing multiple distinct conversions across a\nshared set of URLs.\nData arriving out of order, and in particular, goals arriving (in\nprocessing time) before visits and impressions that lead to them, as\nwell as other goals which occurred earlier.\nSource URLs that generate multiple distinct impressions to different\ntarget URLs.\nPhysically distinct impressions (e.g., multiple clicks on the same\nadvertisement) that must be deduplicated to a single logical\nimpression.\nExample 7-10. \nExample dataset for validating conversion attribution logic\nprivate\n \nstatic\n \nTestStream\n<\nKV\n<\nString\n,\n \nVisitOrImpression\n>>\n \ncreateStream\n()\n \n{\n    \n// Impressions and visits, in event-time order, for two (logical)\n \nattributable\n    \n// impressions and one unattributable impression.\n    \nImpression\n \nsignupImpression\n \n=\n \nnew\n \nImpression\n(\n \n123L\n,\n \n""http://search.com?q=xyz""\n,\n \n""http://xyz.com/""\n,\n \nUtils\n.\nparseTime\n(\n""12:01:00""\n));\n    \nVisit\n \nsignupVisit\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/""\n,\n \nUtils\n.\nparseTime\n(\n""12:01:10""\n),\n \n""http://search.com?q=xyz""\n,\n \nfalse\n/*isGoal*/\n);\n    \nVisit\n \nsignupGoal\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/join-mailing-list""\n,\n \nUtils\n.\nparseTime\n(\n""12:01:30""\n),\n \n""http://xyz.com/""\n,\n \ntrue\n/*isGoal*/\n);\n    \nImpression\n \nshoppingImpression\n \n=\n \nnew\n \nImpression\n(\n \n456L\n,\n \n""http://search.com?q=thing""\n,\n \n""http://xyz.com/thing""\n,\n \nUtils\n.\nparseTime\n(\n""12:02:00""\n));\n    \nImpression\n \nshoppingImpressionDup\n \n=\n \nnew\n \nImpression\n(\n \n789L\n,\n \n""http://search.com?q=thing""\n,\n \n""http://xyz.com/thing""\n,\n \nUtils\n.\nparseTime\n(\n""12:02:10""\n));\n    \nVisit\n \nshoppingVisit1\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/thing""\n,\n \nUtils\n.\nparseTime\n(\n""12:02:30""\n),\n \n""http://search.com?q=thing""\n,\n \nfalse\n/*isGoal*/\n);\n    \nVisit\n \nshoppingVisit2\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/thing/add-to-cart""\n,\n \nUtils\n.\nparseTime\n(\n""12:03:00""\n),\n \n""http://xyz.com/thing""\n,\n \nfalse\n/*isGoal*/\n);\n    \nVisit\n \nshoppingVisit3\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/thing/purchase""\n,\n \nUtils\n.\nparseTime\n(\n""12:03:20""\n),\n \n""http://xyz.com/thing/add-to-cart""\n,\n \nfalse\n/*isGoal*/\n);\n    \nVisit\n \nshoppingGoal\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/thing/receipt""\n,\n \nUtils\n.\nparseTime\n(\n""12:03:45""\n),\n \n""http://xyz.com/thing/purchase""\n,\n \ntrue\n/*isGoal*/\n);\n    \nImpression\n \nunattributedImpression\n \n=\n \nnew\n \nImpression\n(\n \n000L\n,\n \n""http://search.com?q=thing""\n,\n \n""http://xyz.com/other-thing""\n,\n \nUtils\n.\nparseTime\n(\n""12:04:00""\n));\n    \nVisit\n \nunattributedVisit\n \n=\n \nnew\n \nVisit\n(\n \n""http://xyz.com/other-thing""\n,\n \nUtils\n.\nparseTime\n(\n""12:04:20""\n),\n \n""http://search.com?q=other thing""\n,\n \nfalse\n/*isGoal*/\n);\n    \n// Create a stream of visits and impressions, with data arriving out\n \nof order.\n    \nreturn\n \nTestStream\n.\ncreate\n(\n \nKvCoder\n.\nof\n(\nStringUtf8Coder\n.\nof\n(),\n \nAvroCoder\n.\nof\n(\nVisitOrImpression\n.\nclass\n)))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:00:00""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nshoppingVisit2\n,\n \nnull\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nshoppingGoal\n,\n \nnull\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nshoppingVisit3\n,\n \nnull\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nsignupGoal\n,\n \nnull\n))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:00:30""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nnull\n,\n \nsignupImpression\n))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:01:00""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nnull\n,\n \nshoppingImpression\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nsignupVisit\n,\n \nnull\n))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:01:30""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nnull\n,\n \nshoppingImpressionDup\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nshoppingVisit1\n,\n \nnull\n))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:03:45""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nnull\n,\n \nunattributedImpression\n))\n \n.\nadvanceWatermarkTo\n(\nUtils\n.\nparseTime\n(\n""12:04:00""\n))\n \n.\naddElements\n(\nvisitOrImpression\n(\nunattributedVisit\n,\n \nnull\n))\n \n.\nadvanceWatermarkToInfinity\n();\n}\nAnd remember, we’re working here on a relatively constrained version of\nconversion attribution. A full-blown impelementation would have additional\nchallenges to deal with (e.g., garbage collection, DAGs of visits instead of\ntrees). Regardless, this pipeline provides a nice contrast to the oftentimes\ninsufficiently flexible approaches provided by raw grouping an incremental\ncombination. By trading off some amount of implementation complexity, we\nwere able to find the necessary balance of efficiency, without compromising\non correctness.\n Additionally, this pipeline highlights the more imperative\napproach towards stream processing that state and timers afford (think C or\nJava), which is a nice complement to the more functional approach afforded\nby windowing and triggers (think Haskell).\nSummary",27947
70-Summary.pdf,70-Summary,"In this chapter, we’ve looked closely at why persistent state is important,\ncoming to the conclusion that it provides a basis for correctness and\nefficiency in long-lived pipelines. We then looked at the two most common\ntypes of implicit state encountered in data processing systems: raw grouping\nand incremental combination. \nWe learned that raw grouping is\nstraightforward but potentially inefficient and that incremental combination\ngreatly improves efficiency for operations that are commutative and\nassociative. Finally, we looked a relatively complex, but very practical use\ncase (and implementation via Apache Beam Java) grounded in real-world\nexperience, and used that to highlight the important characteristics needed\n in\na general state \nabstraction\n:\nFlexibility in data structures\n, allowing for the use of data types\ntailored to specific use cases at hand.\nFlexibility in write and read granularity\n, allowing the amount of\ndata written and read at any point to be tailored to the use case,\nminimizing or maximizing I/O as appropriate.\nFlexibility in scheduling of processing\n, allowing certain portions of\nprocessing to be delayed until a more appropriate point in time, such\nas when the input is believed to be complete up to a specific point in\nevent time.\n For some definition of “forever,” typically at least “until we successfully\ncomplete execution of our batch pipeline and no longer require the inputs.”\n Recall that Beam doesn’t currently expose these state tables directly; you\nmust trigger them back into a stream to observe their contents as a new\nPCollection.\n Or, as my colleague Kenn Knowles points out, if you take the definition as\nbeing commutativity across sets, the three-parameter version of\ncommutativity is actually sufficient to also imply associativity: \nCOMBINE(a,\nb, c) == COMBINE(a, c, b) == COMBINE(b, a, c) == COMBINE(b, c,\na) == COMBINE(c, a, b) == COMBINE(c, b, a)\n. Math is fun.\n1\n2\n3\n4\n And indeed, timers are the underlying feature used to implement most of the\ncompleteness and repeated updated triggers we discussed in \nChapter 2\n as well\nas garbage collection based on allowed lateness.\n Thanks to the nature of web browsing, the visit trails we’ll be analyzing are\ntrees of URLs linked by HTTP referrer fields. In reality, they would end up\nbeing directed graphs, but for the sake of simplicity, we’ll assume each page\non our website has incoming links from exactly one other referring page on\nthe site, thus yielding a simpler tree structure. Generalizing to graphs is a\nnatural extension of the tree-based implementation, and only further drives\nhome the points being made.\n4\n5",2688
71-8. Streaming SQL.pdf,71-8. Streaming SQL,,0
72-Time-Varying Relations.pdf,72-Time-Varying Relations,"Chapter 8. \nStreaming SQL\nLet’s talk SQL. In this chapter, we’re going to start\n somewhere in the middle\nwith the punchline, jump back in time a bit to establish additional context,\nand finally jump back to the future to wrap everything up with a nice bow.\nImagine Quentin Tarantino held a degree in computer science and was super\npumped to tell the world about the finer points of streaming SQL, and so he\noffered to ghostwrite this chapter with me; it’s sorta like that. Minus the\nviolence.\nWhat Is Streaming SQL?\nI would argue that the answer to this question has eluded our industry for\ndecades. In all fairness, \nthe database community has understood maybe 99%\nof the answer for quite a while now. But I have yet to see a truly cogent and\ncomprehensive definition of streaming SQL that encompasses the full breadth\nof robust streaming semantics. That’s what we’ll try to come up with here,\nalthough it would be hubris to assume we’re 100% of the way there now.\nMaybe 99.1%? Baby steps.\nRegardless, I want to point out up front that most of what we’ll discuss in this\nchapter is still purely hypothetical as of the time of writing. This chapter and\nthe one that follows (covering streaming joins) both describe an idealistic\nvision for what streaming SQL could be. Some pieces are already\nimplemented in systems like Apache Calcite, Apache Flink, and Apache\nBeam.\n Many others aren’t implemented anywhere. Along the way, I’ll try to\ncall out a few of the things that do exist in concrete form, but given what a\nmoving target that is, your best bet is to simply consult the documentation for\nyour specific system of interest.\nOn that note, it’s also worth highlighting that the vision for streaming SQL\npresented here is the result of a collaborative discussion between the Calcite,\nFlink, and \nBeam communities\n. Julian Hyde, the lead developer on Calcite,\nhas \nlong pitched\n his vision for what streaming SQL might look like. In 2016,\nmembers of the Flink community integrated Calcite SQL support into Flink\nitself, and began adding streaming-specific features such as windowing\nconstructs to the Calcite SQL dialect. Then, in 2017, all three communities\nbegan a \ndiscussion\n to try to come to agreement on what language extensions\nand semantics for robust stream processing in Calcite SQL should look like.\nThis chapter attempts to distill the ideas from that discussion down into a\nclear and cohesive narrative about integrating streaming concepts into SQL,\nregardless of whether it’s Calcite or some other dialect.\nRelational Algebra\nWhen talking about what streaming means for SQL, it’s important to keep in\nmind the theoretical foundation of SQL: relational algebra. \nRelational algebra\nis simply a mathematical way of describing relationships between data that\nconsist of named, typed tuples. At the heart of relational algebra is the\nrelation itself, which is a set of these tuples.\n In classic database terms, a\nrelation is something akin to a table, be it a physical database table, the result\nof a SQL query, a view (materialized or otherwise), and so on; it’s a set of\nrows containing named and typed columns of data.\nOne of\n the more critical \naspects of relational algebra is its closure property:\napplying any operator from the relational algebra to any valid relation\nalways yields another relation. In other words, relations are the common\ncurrency of relational algebra, and all operators consume them as input and\nproduce them as output.\nHistorically, many attempts to support streaming in SQL have fallen short of\nsatisfying the closure property. They treat streams separately from classic\nrelations, providing new operators to convert between the two, and restricting\nthe operations that can be applied to one or the other. This significantly raises\nthe bar of adoption for any such streaming SQL system: would-be users must\nlearn the new operators and understand the places where they’re applicable,\nwhere they aren’t, and similarly relearn the rules of applicability in this new\nworld for any old operators. What’s worse, most of these systems still fall\n1\nshort of providing the full suite of streaming semantics that we would want,\nsuch as support for robust out-of-order processing and strong temporal join\nsupport (the latter of which we cover in \nChapter 9\n). As a result, I would argue\nthat it’s basically impossible to name any existing streaming SQL\nimplementation that has achieved truly broad adoption. The additional\ncognitive overhead and restricted capabilities of such streaming SQL systems\nhave ensured that they remain a niche enterprise.\nTo change that, to truly bring streaming SQL to the forefront, what we need\nis a way for streaming to become a first-class citizen within the relational\nalgebra itself, such that the entire standard relational algebra can apply\nnaturally in both streaming and nonstreaming use cases. That isn’t to say that\nstreams and tables should be treated as exactly the same thing; they most\ndefinitely are not the same, and recognizing that fact lends clarity to\nunderstanding and power to navigating the stream/table relationship, as we’ll\nsee shortly. But the core algebra should apply cleanly and naturally to both\nworlds, with minimal extensions beyond the standard relational algebra only\nin the cases where absolutely necessary.\nTime-Varying Relations\nTo cut to the chase, the punchline I referred to at the beginning of the chapter\nis this: the key to naturally\n integrating streaming into SQL is to extend\nrelations,\n the core data objects of relational algebra, to represent a set of data\nover time\n rather than a set of data at a \nspecific point\n in time. More succinctly,\ninstead of \npoint-in-time\n relations, we need \ntime-varying relations\n.\nBut what are time-varying relations? Let’s first define them in terms of\nclassic relational algebra, after which we’ll also consider their relationship to\nstream and table theory.\nIn terms of relational algebra, a time-varying relation is really just the\nevolution of a classic relation over time. To understand what I mean by that,\nimagine a raw dataset consisting of user events. Over time, as users generate\nnew events, the dataset continues to grow and evolve. If you observe that set\nat a specific point in time, that’s a classic relation. But if you observe the\n2\nholistic evolution of the set \nover time\n, that’s a time-varying relation.\nPut differently, if classic relations are like two-dimensional tables consisting\nof named, typed columns in the x-axis and rows of records in the y-axis,\ntime-varying relations are like three-dimensional tables with x- and y-axes as\nbefore, but an additional z-axis capturing different versions of the two-\ndimensional table over time. As the relation changes, new snapshots of the\nrelation are added in the z dimension.\nLet’s look at an example. Imagine our raw dataset is users and scores; for\nexample, per-user scores from a mobile game as in most of the other\nexamples throughout the book. And suppose that our example dataset here\nultimately ends up looking like this when observed at a specific point in time,\nin this case 12:07:\n-------------------------\n| Name  | Score | Time  |\n-------------------------\n| Julie | 7     | 12:01 |\n| Frank | 3     | 12:03 |\n| Julie | 1     | 12:03 |\n| Julie | 4     | 12:07 |\n-------------------------\nIn other words, it recorded the arrivals of four scores over time: Julie’s score\nof 7 at 12:01, both Frank’s score of 3 and Julie’s second score of 1 at 12:03,\nand, finally, Julie’s third score of 4 at 12:07 (note that the \nTime\n column here\ncontains processing-time timestamps representing the \narrival time\n of the\nrecords within the system; we get into event-time timestamps a little later on).\nAssuming these were the only data to ever arrive for this relation, it would\nlook like the preceding table any time we observed it after 12:07. But if\ninstead we had observed the relation at 12:01, it would have looked like the\nfollowing, because only Julie’s first score would have arrived by that point:\n-------------------------\n| Name  | Score | Time  |\n-------------------------\n| Julie | 7     | 12:01 |\n-------------------------\nIf we had then observed it again at 12:03, Frank’s score and Julie’s second\nscore would have also arrived, so the relation would have evolved to look\nlike this:\n-------------------------\n| Name  | Score | Time  |\n-------------------------\n| Julie | 7     | 12:01 |\n| Frank | 3     | 12:03 |\n| Julie | 1     | 12:03 |\n-------------------------\nFrom this example we can begin to get a sense for what the \ntime-varying\nrelation for this dataset must look like: it would capture the entire evolution\nof the relation over time. Thus, if we observed the time-varying relation (or\nTVR) at or after 12:07, it would thus look like the following (note the use of\na hypothetical \nTVR\n keyword to signal that we want the query to return the full\ntime-varying relation, not the standard point-in-time snapshot of a classic\nrelation):\n---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Score | Time  | | | Name  | Score | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| |       |       |       | | |       |       |       | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Score | Time  | | | Name  | Score | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 7     | 12:01 | | | Julie | 7     | 12:01 | |\n| | Frank | 3     | 12:03 | | | Frank | 3     | 12:03 | |\n| | Julie | 1     | 12:03 | | | Julie | 1     | 12:03 | |\n| |       |       |       | | | Julie | 4     | 12:07 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nBecause the printed/digital page remains constrained to two dimensions, I’ve\ntaken the liberty of flattening the third dimension into a grid of two-\ndimensional relations. But you can see how the time-varying relation\nessentially consists of a sequence of classic relations (ordered left to right, top\nto bottom), each capturing the full state of the relation for a specific range of\ntime (all of which, by definition, are contiguous).\nWhat’s important about defining time-varying relations this way is that they\nreally are, for all intents and purposes, just a sequence of classic relations that\neach exist independently within their own disjointed (but adjacent) time\nranges, with each range capturing a period of time during which the relation\ndid not change. This is important, because it means that the application of a\nrelational operator to a time-varying relation is equivalent to individually\napplying that operator to each classic relation in the corresponding sequence.\nAnd taken one step further, the result of individually applying a relational\noperator \nto a sequence of relations, each associated with a time interval, will\nalways yield a corresponding sequence of relations with the same time\nintervals. In other words, the result is a corresponding time-varying relation.\nThis definition gives us two very important properties:\nThe \nfull set of operators\n from classic relational algebra \nremain valid\nwhen applied to time-varying relations, and furthermore continue to\nbehave exactly as you’d expect.\nThe \nclosure property\n of relational algebra \nremains intact\n when\napplied to time-varying relations.\nOr more succinctly, \nall the rules of classic relational algebra continue to\nhold when applied to time-varying relations\n. This is huge, because it means\nthat our substitution of time-varying relations for classic relations hasn’t\naltered the parameters of the game in any way. Everything continues to work\nthe way it did back in classic relational land, just on sequences of classic\nrelations instead of singletons. Going back to our examples, consider two\nmore time-varying relations over our raw dataset, both observed at some time\nafter 12:07. First a simple\n filtering relation using a \nWHERE\n clause:\n---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Score | Time  | | | Name  | Score | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Score | Time  | | | Name  | Score | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 7     | 12:01 | | | Julie | 7     | 12:01 | |\n| | Julie | 1     | 12:03 | | | Julie | 1     | 12:03 | |\n| |       |       |       | | | Julie | 4     | 12:07 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nAs you would expect, this relation looks a lot like the preceding one, but with\nFrank’s scores filtered out. Even though the time-varying relation captures\nthe added dimension of time necessary to record the evolution of this dataset\nover time, the query behaves exactly as you would expect, given your\nunderstanding of SQL.\nFor something a little more complex, \nlet’s consider a grouping relation in\nwhich we’re summing up all the per-user scores to generate a total overall\nscore for each user:",14284
73-Streams and Tables.pdf,73-Streams and Tables,"---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 8     | 12:03 | | | Julie | 12    | 12:07 | |\n| | Frank | 3     | 12:03 | | | Frank | 3     | 12:03 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nAgain, the time-varying version of this query behaves exactly as you would\nexpect, with each classic relation in the sequence simply containing the sum\nof the scores for each user. And indeed, no matter how complicated a query\nwe might choose, the results are always identical to applying that query\nindependently to the commensurate classic relations composing the input\ntime-varying relation. I cannot stress enough how important this is!\nAll right, that’s all well and good, but time-varying relations themselves are\nmore of a theoretical construct than a practical, physical manifestation of\ndata; it’s pretty easy to see how they could grow to be quite huge and\nunwieldy for large datasets that change frequently. To see how they actually\ntie into real-world stream processing, let’s now \nexplore the relationship\nbetween time-varying relations and stream and table theory.\nStreams and Tables\nFor this comparison, let’s consider again our\n grouped time-varying \nrelation\nthat we looked at earlier:\n---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 8     | 12:03 | | | Julie | 12    | 12:07 | |\n| | Frank | 3     | 12:03 | | | Frank | 3     | 12:03 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nWe understand that this sequence captures the full history of the relation over\ntime. Given our understanding of tables and streams from \nChapter 6\n, it’s not\ntoo difficult to understand how time-varying relations relate to stream and\ntable theory.\nTables are quite straightforward: because a time-varying relation is\nessentially a sequence of classic relations (each capturing a snapshot of the\nrelation at a specific point in time), and classic relations are analogous to\ntables, observing a time-varying relation as a table simply yields the point-in-\ntime relation snapshot for the time of observation.\nFor example, if we were to observe the previous grouped time-varying\nrelation as a table at 12:01, we’d get the following (note the use of another\nhypothetical keyword, \nTABLE\n, to explicitly\n call out our desire for the query to\nreturn a table):\n-------------------------\n| Name  | Total | Time  |\n-------------------------\n| Julie | 7     | 12:01 |\n-------------------------\nAnd observing at 12:07 would yield the expected:\n-------------------------\n| Name  | Total | Time  |\n-------------------------\n| Julie | 12    | 12:07 |\n| Frank | 3     | 12:03 |\n-------------------------\nWhat’s particularly interesting here is that there’s actually support for the\nidea of time-varying relations within SQL, even as it exists today. \nThe SQL\n2011 standard provides “temporal tables,” which store a versioned history of\nthe table over time (in essence, time-varying relations) as well an \nAS OF\nSYSTEM TIME\n construct that allows \nyou to explicitly \nquery and receive a\nsnapshot of the temporal table/time-varying relation at whatever point in time\nyou specified. For example, even if we performed our query at 12:07, we\ncould still see what the relation looked like back at 12:03:\n-------------------------\n| Name  | Total | Time  |\n-------------------------\n| Julie | 8     | 12:03 |\n| Frank | 3     | 12:03 |\n-------------------------\nSo there’s some amount of precedent for time-varying relations in SQL\nalready. But I digress. The main point here is that tables capture a snapshot of\nthe time-varying relation at a specific point in time.\n Most real-world table\nimplementations simply track real time as we observe it; others maintain\nsome additional historical information, which in the limit is equivalent to a\nfull-fidelity time-varying relation capturing the entire history of a relation\nover time.\nStreams are slightly different beasts.\n We learned in \nChapter 6\n that they too\ncapture the evolution of a table over time. But they do so somewhat\ndifferently than the time-varying relations we’ve looked at so far. Instead of\nholistically capturing snapshots of the entire relation each time it changes,\nthey capture the \nsequence of changes\n that result in those snapshots within a\ntime-varying relation. The subtle difference here becomes more evident with\nan example.\nAs a refresher, recall again our baseline example \nTVR\n query:\n---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 8     | 12:03 | | | Julie | 12    | 12:07 | |\n| | Frank | 3     | 12:03 | | | Frank | 3     | 12:03 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nLet’s now observe our time-varying relation as a stream as it exists at a few\ndistinct points in time. At each step of the way, we’ll compare the original\ntable rendering of the TVR at that point in time with the evolution of the\nstream up to that point. To see what stream renderings of our time-varying\nrelation look like, we’ll need to introduce two new hypothetical keywords:\nA \nSTREAM\n keyword, similar to the \nTABLE\n keyword\n I’ve already\nintroduced, that indicates we want our query to return an event-by-\nevent stream capturing the evolution of the time-varying relation\nover time. You can think of this as applying a per-record trigger to\nthe relation over time.\nA special \nSys.Undo\n column that can be referenced from a \nSTREAM\nquery, for the \nsake of identifying rows that are retractions. More on\nthis in a moment.\nThus, starting out from 12:01, we’d have the following:\n-------------------------                 -----------------------------\n---\n| Name  | Total | Time  |                 | Name  | Total | Time  |\n \nUndo |\n-------------------------                 -----------------------------\n---\n| Julie | 7     | 12:01 |                 | Julie | 7     | 12:01 |\n      \n|\n-------------------------                 ........ [12:01, 12:01]\n \n........\nThe table and stream renderings look almost identical at this point. Mod the\n3\nUndo\n column (discussed in more detail in the next example), there’s only one\ndifference: whereas the table version is complete as of 12:01 (signified by the\nfinal line of dashes closing off the bottom end of the relation), the stream\nversion remains incomplete, as signified by the final ellipsis-like line of\nperiods marking both the open tail of the relation (where additional data\nmight be forthcoming in the future) as well as the processing-time range of\ndata observed so far. And indeed, if executed on a real implementation, the\nSTREAM\n query would wait indefinitely for additional data to arrive. Thus, if\nwe waited until 12:03, three new rows would show up for the \nSTREAM\n query.\nCompare that to a fresh \nTABLE\n rendering of the TVR at 12:03:\n-------------------------                 -----------------------------\n---\n| Name  | Total | Time  |                 | Name  | Total | Time  |\n \nUndo |\n-------------------------                 -----------------------------\n---\n| Julie | 8     | 12:03 |                 | Julie | 7     | 12:01 |\n      \n|\n| Frank | 3     | 12:03 |                 | Frank | 3     | 12:03 |\n      \n|\n-------------------------                 | Julie | 7     | 12:03 |\n \nundo |\n                                          | Julie | 8     | 12:03 |\n      \n|\n                                          ........ [12:01, 12:03]\n \n........\nHere’s an interesting point worth addressing: why are there \nthree\n new rows in\nthe stream (Frank’s 3 and Julie’s undo-7 and 8) when our original dataset\ncontained only \ntwo\n rows (Frank’s 3 and Julie’s 1) for that time period? The\nanswer lies in the fact that here we are observing the stream of changes to an\naggregation\n of the original inputs; in particular, for the time period from\n12:01 to 12:03, the stream needs to capture two important pieces of\ninformation regarding the change in Julie’s aggregate score due to the arrival\nof the new 1 value:\nThe previously reported total of 7 was incorrect.\nThe new total is 8.\nThat’s what the special \nSys.Undo\n column allows \nus to do: distinguish\nbetween normal \nrows and rows that are a \nretraction\n of a previously reported\nvalue.\nA particularly nice feature of \nSTREAM\n queries is that you can begin to see how\nall of this \nrelates to the \nworld of classic Online Transaction Processing\n(OLTP) tables: the \nSTREAM\n rendering of this query is essentially capturing a\nsequence of \nINSERT\n and \nDELETE\n operations that you could use to materialize\nthis relation over time in an OLTP world (and really, when you think about it,\nOLTP tables themselves are essentially time-varying relations mutated over\ntime via a stream of \nINSERT\ns, \nUPDATE\ns, and \nDELETE\ns).\nNow, if we don’t care about the retractions in the stream, it’s also perfectly\nfine not to ask for them. In that case, our \nSTREAM\n query would look like this:\n------------------------- \n| Name  | Total | Time  |\n------------------------- \n| Julie | 7     | 12:01 | \n| Frank | 3     | 12:03 |\n| Julie | 8     | 12:03 |\n.... [12:01, 12:03] .....\n4\nBut there’s clearly value in understanding what the full stream looks like, so\nwe’ll go back to including the \nSys.Undo\n column for our final example.\nSpeaking of which, if we waited another four minutes until 12:07, we’d be\ngreeted by two additional rows in the \nSTREAM\n query, whereas the \nTABLE\n query\nwould continue to evolve as before:\n-------------------------                 -----------------------------\n---\n| Name  | Total | Time  |                 | Name  | Total | Time  |\n \nUndo |\n-------------------------                 -----------------------------\n---\n| Julie | 12    | 12:07 |                 | Julie | 7     | 12:01 |\n      \n|\n| Frank | 3     | 12:03 |                 | Frank | 3     | 12:03 |\n      \n|\n-------------------------                 | Julie | 7     | 12:03 |\n \nundo |\n                                          | Julie | 8     | 12:03 |\n      \n|\n                                          | Julie | 8     | 12:07 |\n \nundo |\n                                          | Julie | 12    | 12:07 |\n      \n|\n                                          ........ [12:01, 12:07]\n \n........\nAnd by this time, it’s quite clear that the \nSTREAM\n version of our time-varying\nrelation is a very different beast from the table version: the table captures a\nsnapshot of the entire relation \nat a specific point in time\n, whereas the stream\n5\ncaptures a view of the individual changes to the relation \nover time\n.\nInterestingly though, that means that the \nSTREAM\n rendering has more in\ncommon with our original, table-based TVR \nrendering\n:\n---------------------------------------------------------\n|       [-inf, 12:01)       |       [12:01, 12:03)      |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| |       |       |       | | | Julie | 7     | 12:01 | |\n| |       |       |       | | |       |       |       | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\n|       [12:03, 12:07)      |       [12:07, now)        |\n| ------------------------- | ------------------------- |\n| | Name  | Total | Time  | | | Name  | Total | Time  | |\n| ------------------------- | ------------------------- |\n| | Julie | 8     | 12:03 | | | Julie | 12    | 12:07 | |\n| | Frank | 3     | 12:03 | | | Frank | 3     | 12:03 | |\n| ------------------------- | ------------------------- |\n---------------------------------------------------------\nIndeed, it’s safe to\n say that the \nSTREAM\n query simply provides an alternate\nrendering of the entire history of data that exists in the corresponding \ntable-\nbased \nTVR\n query. The value of the \nSTREAM\n rendering is its conciseness: it\ncaptures only the delta of changes between each of the point-in-time relation\nsnapshots in the \nTVR\n. The value of the sequence-of-tables \nTVR\n rendering is the\nclarity it provides: it captures the evolution of the relation over time in a\nformat that highlights its natural relationship to classic relations, and in doing\nso provides for a simple and clear definition of relational semantics within the\ncontext of streaming as well as the additional dimension of time that\nstreaming brings.\nAnother important aspect of the similarities between the \nSTREAM\n and table-\nbased \nTVR\n renderings is the fact that they are essentially equivalent in the\n5\noverall data they encode. This gets to the core of the stream/table duality that\nits proponents have long preached: streams and tables\n are really just two\ndifferent sides of the same coin. Or to resurrect the bad physics analogy from\nChapter 6\n, streams and tables are to time-varying relations as waves and\nparticles are to light:\n a complete time-varying relation is both a table and a\nstream at the same time; tables and streams are simply different physical\nmanifestations of the same concept, depending upon the context.\nNow, it’s important to keep in mind that this stream/table duality is true only\nas long as both versions encode the same information; that is, when you have\nfull-fidelity tables or streams. In many cases, however, full fidelity is\nimpractical. As I alluded to earlier, encoding the full history of a time-varying\nrelation, no matter whether it’s in stream or table form, can be rather\nexpensive for a large data source. It’s quite common for stream and table\nmanifestations of a TVR to be lossy in some way. Tables typically encode\nonly the most recent version of a TVR; those that support temporal or\nversioned access often compress the encoded history to specific point-in-time\nsnapshots, and/or garbage-collect versions that are older than some threshold.\nSimilarly, streams typically encode only a limited duration of the evolution of\na TVR, often a relatively recent portion of that history. Persistent streams like\nKafka afford the ability to encode the entirety of a TVR, but again this is\nrelatively uncommon, with data older than some threshold typically thrown\naway via a garbage-collection process.\nThe main point here is that streams and tables are absolutely duals of one\nanother, each a valid way of encoding a time-varying relation. But in\npractice, it’s common for the physical stream/table manifestations of a TVR\nto be lossy in some way. These partial-fidelity streams and tables trade off a\ndecrease in total encoded information for some benefit, usually decreased\nresource costs. And these types of trade-offs are important because they’re\noften what allow us to build pipelines that operate over data sources of truly\nmassive scale. But they also complicate matters, and require a deeper\nunderstanding to use correctly. We discuss this topic in more detail later on\nwhen we get to SQL language extensions. But before we try to reason about\nSQL extensions, it will be useful to understand a little more concretely the\n6\n7",17585
74-Looking Backward Stream and Table Biases.pdf,74-Looking Backward Stream and Table Biases,,0
75-The Beam Model A Stream-Biased Approach.pdf,75-The Beam Model A Stream-Biased Approach,"biases present in both the SQL and non-SQL data processing approaches\ncommon today.\nLooking Backward: Stream and Table Biases\nIn many ways, the act of adding robust streaming \nsupport to SQL is largely\nan exercise in attempting \nto merge the \nwhere\n, \nwhen\n, and \nhow\n semantics of the\nBeam Model with the \nwhat\n semantics of the classic SQL model. But to do so\ncleanly, and in a way that remains true to the look and feel of classic SQL,\nrequires an understanding of how the two models relate to each other. Thus,\nmuch as we explored the relationship of the Beam Model to stream and table\ntheory in \nChapter 6\n, we’ll now explore the relationship of the Beam Model to\nthe classic SQL model, using stream and table theory as the underlying\nframework for our comparison. In doing so, we’ll uncover the inherent biases\npresent in each model, which will provide us some insights in how to best\nmarry the two in a clean, natural way.\nThe Beam Model: A Stream-Biased Approach\nLet’s begin with the Beam Model, building upon\n the discussion in \nChapter 6\n.\nTo begin, I want to discuss the inherent stream bias in the Beam Model as it\nexists today relative to streams and tables.\nIf you think back to Figures \n6-11\n and \n6-12\n, they showed two different views\nof the same score-summation pipeline that we’ve used as an example\nthroughout the book: in \nFigure 6-11\n a logical, Beam-Model view, and in\nFigure 6-12\n a physical, streams and tables–oriented view. Comparing the two\nhelped highlight the relationship of the Beam Model to streams and tables.\nBut by overlaying one on top of the other, as I’ve done in \nFigure 8-1\n, we can\nsee an additional interesting aspect of the relationship: the Beam Model’s\ninherent stream bias.\nFigure 8-1. \nStream bias in the Beam Model approach\nIn this figure, I’ve drawn dashed red lines connecting the transforms in the\nlogical view to their corresponding components in the physical view. The\nthing that stands out when observed this way is that all of the logical\ntransformations are connected by \nstreams\n, even the operations that involve\ngrouping (which we know from \nChapter 6\n results in a table being created\nsomewhere\n). In Beam parlance, these transformations are \nPTransforms\n, and\nthey are always applied to \nPCollections\n to yield new \nPCollections\n. The\nimportant takeaway here is that \nPCollections\n in Beam are \nalways\n streams.\nAs a result, the Beam Model is an inherently stream-biased approach to data\nprocessing: streams are the common currency in a Beam pipeline (even batch\npipelines), and tables are always treated specially, either abstracted behind\nsources and sinks at the edges of the pipeline or hidden away beneath a\ngrouping and triggering operation somewhere in the pipeline.\nBecause Beam operates in terms of streams, anywhere a table is involved\n(sources, sinks, and any intermediate groupings/ungroupings), some sort of\nconversion is necessary to keep the underlying table hidden. Those\nconversions in Beam look something like this:\nSources\n that \nconsume\n tables typically hardcode the manner in which\nthose tables are \ntriggered\n; there is no way for a user to specify\ncustom triggering of the table they want to consume. The source\nmay be written to trigger every new update to the table as a record, it\nmight batch groups of updates together, or it might provide a single,\nbounded snapshot of the data in the table at some point in time. It\nreally just depends on what’s practical for a given source, and what\nuse case the author of the source is trying to address.\nSinks\n that \nwrite\n tables typically hardcode the manner in which they\ngroup\n their input streams. Sometimes, this is done in a way that\ngives the user a certain amount of control; for example, by simply\ngrouping on a user-assigned key. In other cases, the grouping might\nbe implicitly defined; for example, by grouping on a random\nphysical partition number when writing input data with no natural\nkey to a sharded output source. As with sources, it really just\ndepends on what’s practical for the given sink and what use case the\nauthor of the sink is trying to address.\nFor \ngrouping/ungrouping operations\n, in contrast to sources and\nsinks, Beam provides users complete flexibility in how they group\ndata into tables and ungroup them back into streams.\n This is by\ndesign. Flexibility in grouping operations is necessary because the\nway data are grouped is a key ingredient of the algorithms that\ndefine a pipeline. \nAnd flexibility in ungrouping is important so that\nthe application can shape the generated streams in ways that are\nappropriate for the use case at hand.\nHowever, there’s a wrinkle here. Remember from \nFigure 8-1\n that the\nBeam Model is inherently biased toward streams. As result, although\nit’s possible to cleanly apply a grouping operation directly to a\nstream (this is Beam’s \nGroupByKey\n operation), the model never\nprovides first-class table objects to which a trigger can be directly\napplied. As a result, triggers must be applied somewhere else. There\nare basically two options\n here:\nPredeclaration of triggers\nThis is where triggers are specified\n at a point in the pipeline\nbefore\n the table to which they are actually applied. In this case,\nyou’re essentially prespecifying behavior you’d like to see later\non in the pipeline after a grouping operation is encountered.\nWhen declared this way, triggers are \nforward-propagating\n.\nPost-declaration of triggers\nThis is where triggers\n are specified at a point in the pipeline\nfollowing\n the table to which they are applied. In this case, you’re\nspecifying the behavior you’d like to see at the point where the\ntrigger is declared. When declared this way, triggers are\nbackward-propagating\n.\nBecause post-declaration of triggers allows you to specify the\nbehavior you want at the actual place you want to observe it, it’s\n8",5989
76-The SQL Model A Table-Biased Approach.pdf,76-The SQL Model A Table-Biased Approach,"much more intuitive. Unfortunately, Beam as it exists today (2.x and\nearlier) uses predeclaration of triggers (similar to how windowing is\nalso predeclared).\nEven though Beam provides a number of ways to cope with the fact that\ntables are hidden, we’re still left with the fact that tables must always be\ntriggered before they can be observed, even if the contents of that table are\nreally the final data that you want to consume. This is a shortcoming of the\nBeam Model as it exists today, one which could be addressed by moving\naway from a stream-centric model and toward one that treats both streams\nand tables as first-class entities.\nLet’s now look at the Beam Model’s conceptual converse: classic SQL.\nThe SQL Model: A Table-Biased Approach\nIn contrast to the Beam Model’s\n stream-biased approach, SQL has\nhistorically taken a table-biased\n approach: queries are applied to tables, and\nalways result in new tables. This is similar to the batch processing model we\nlooked at in \nChapter 6\n with MapReduce,\n but it will be useful to consider a\nconcrete example like the one we just looked at for the Beam Model.\nConsider the following denormalized SQL table:\nUserScores (user, team, score, timestamp)\nIt contains user scores, each annotated with the IDs of the corresponding user\nand their corresponding team. There is no primary key, so you can assume\nthat this is an append-only table, with each row being identified implicitly by\nits unique physical offset. If we want to compute team scores from this table,\nwe could use a query that looks something like this:\n    SELECT team, SUM(score) as total\n    FROM UserScores\n    GROUP BY team;\nWhen executed by a query engine, the optimizer will probably break this\n9\nquery down into roughly three steps:\n1\n. \nScanning the input table (i.e., triggering a snapshot of it)\n2\n. \nProjecting the fields in that table down to team and score\n3\n. \nGrouping rows by team and summing the scores\nIf we look at this using a diagram similar to \nFigure 8-1\n, it would look like\nFigure 8-2\n.\nThe \nSCAN\n operation takes the input table and triggers it into a bounded stream\nthat contains a snapshot of the contents of that table at query execution time.\nThat stream is consumed by the \nSELECT\n operation, which projects the four-\ncolumn input rows down to two-column output rows. Being a nongrouping\noperation, it yields another stream. Finally, that two-column stream of teams\nand user scores enters the \nGROUP BY\n and is grouped by team into a table, with\nscores for the same team \nSUM\n’d together, yielding our output table of teams\nand their corresponding team score totals.\nFigure 8-2. \nTable bias in a simple SQL query\nThis is a relatively simple example that naturally ends in a table, so it really\nisn’t sufficient to highlight the table-bias in classic SQL. But we can tease out\nsome more evidence by simply splitting the main pieces of this query\n(projection and grouping) into two separate queries:\n    SELECT team, score\n    INTO TeamAndScore\n    FROM UserScores;\n    SELECT team, SUM(score) as total\n    INTO TeamTotals\n    FROM TeamAndScore\n    GROUP BY team;\nIn these queries, we first project the \nUserScores\n table down to just the two\ncolumns we care about, storing the results in a temporary \nTeamAndScore\ntable. We then group that table by team, summing up the scores as we do so.\nAfter breaking things out into a pipeline of two queries, our diagram looks\nlike that shown in \nFigure 8-3\n.\nFigure 8-3. \nBreaking the query into two to reveal more evidence of table bias\nIf classic SQL exposed streams as first-class objects, you would expect the\nresult from the first query, \nTeamAndScore\n, to be a stream because the \nSELECT\noperation consumes a stream and produces a stream. But because SQL’s\ncommon currency is tables, it must first convert the projected stream into a\ntable. And because the user hasn’t specified any explicit key for grouping, it\nmust simply group keys by their identity (i.e., append semantics, typically\nimplemented by grouping by the physical storage offset for each row).\nBecause \nTeamAndScore\n is now a table, the second query must then prepend\nan additional \nSCAN\n operation to scan the table back into a stream to allow the\nGROUP BY\n to then group it back into a table again, this time with rows\ngrouped by team and with their individual scores summed together. Thus, we\nsee the two implicit conversions (from a stream and back again) that are\ninserted due to the explicit materialization of the intermediate table.\nThat said, tables in SQL are not \nalways\n explicit; implicit tables can exist, \nas\nwell. For example, if\n we \nwere to add a \nHAVING\n clause to \nthe end of the query\nwith the \nGROUP BY\n statement, to filter out teams with scores less than a\ncertain threshold, the diagram would change to look something like \nFigure 8-\n4\n.\nFigure 8-4. \nTable bias with a final HAVING clause\nWith the addition of the \nHAVING\n clause, what used to be the user-visible\nTeamTotals\n table is now an implicit, intermediate table. To filter the results\nof the table according to the rules in the \nHAVING\n clause, that table must be\ntriggered into a stream that can be filtered and then that stream must be\nimplicitly grouped back into a table to yield the new output table,\nLargeTeamTotals\n.\nThe important takeaway here is the clear table bias in classic SQL. Streams\nare always implicit, and thus for any materialized stream a conversion\nfrom/to a table is required. \nThe rules for such conversions can be categorized\nroughly as follows:\nInput tables (i.e., sources, in Beam Model terms)\nThese are always implicitly triggered in their entirety \nat a specific point in\ntime\n (generally query execution time) to\n yield a bounded stream\ncontaining a snapshot of the table at that time. This is identical to what\nyou get with classic batch processing, as well; for example, the\nMapReduce case we looked at in \nChapter 6\n.\nOutput tables (i.e., sinks, in Beam Model terms)\nThese tables are either direct manifestations of a table created by a final\ngrouping operation\n in the query, \nor are the result of an implicit grouping\n(by some unique identifier for the row) applied to a query’s terminal\nstream, for queries that do not end in a grouping operation (e.g., the\nprojection query in the previous examples, or a \nGROUP BY\n followed by a\nHAVING\n clause). As with inputs, this matches the behavior seen in classic\nbatch processing.\nGrouping/ungrouping operations\nUnlike Beam, these operations provide complete flexibility in one\ndimension only: grouping. Whereas classic SQL queries provide a full\nsuite of grouping operations (\nGROUP BY\n, \nJOIN\n, \nCUBE\n, etc.), they provide\nonly a single type of implicit ungrouping operation: trigger an\nintermediate table in its entirety after all of the upstream data contributing\n10\nto it have been incorporated (again, the exact same implicit trigger\nprovided in MapReduce as part of the shuffle operation). As a result, SQL\noffers great flexibility in shaping algorithms via grouping but essentially\nzero flexibility in shaping the implicit streams that exist under the covers\nduring query execution.\nMaterialized views\nGiven how analogous classic SQL queries are to classic batch processing, it\nmight be tempting to write off SQL’s inherent table bias as nothing more than\nan artifact of SQL not supporting stream processing in any way. \nBut to do so\nwould be to ignore the fact that databases have supported a specific type of\nstream processing for quite some time: \nmaterialized views\n. A materialized\nview is a view that is physically materialized as a table and kept up to date\nover\n time by the database as the source table(s) evolve. Note how this sounds\nremarkably similar to our definition of a time-varying relation. What’s\nfascinating about materialized views is that they add a very useful form of\nstream processing to SQL \nwithout\n significantly altering the way it operates,\nincluding its inherent table bias.\nFor example, let’s consider the queries we looked at in \nFigure 8-4\n. We can\nalter those queries to instead be \nCREATE MATERIALIZED VIEW\n statements:\n    CREATE MATERIALIZED VIEW TeamAndScoreView AS\n    SELECT team, score\n    FROM UserScores;\n    CREATE MATERIALIZED VIEW LargeTeamTotalsView AS\n    SELECT team, SUM(score) as total\n    FROM TeamAndScoreView\n    GROUP BY team\n    HAVING SUM(score) > 100;\nIn doing so, we transform them into continuous, standing queries that process\nthe updates to the \nUserScores\n table continuously, in a streaming manner.\nEven so, the resulting physical execution diagram for the views \nlooks almost\nexactly the same\n as it did for the one-off queries; nowhere are streams made\n11\ninto explicit first-class objects in order to support this idea of streaming\nmaterialized views. \nThe \nonly\n noteworthy change in the physical execution\nplan is the substitution of a different trigger: \nSCAN-AND-STREAM\n instead of\nSCAN\n, as illustrated in \nFigure 8-5\n.\nFigure 8-5. \nTable bias in materialized views",9209
77-Stream and Table Selection.pdf,77-Stream and Table Selection,"What is this \nSCAN-AND-STREAM\n trigger? \nSCAN-AND-STREAM\n starts out like a\nSCAN\n trigger, emitting the full contents of the table at a point in time into a\nstream. But instead of stopping there and declaring the stream to be done\n(i.e., bounded), it continues to also trigger all subsequent modifications to the\ninput table, yielding an unbounded stream that captures the evolution of the\ntable over time. In the general case, these modifications include not only\nINSERT\ns of new values, but also \nDELETE\ns of previous values and \nUPDATE\ns to\nexisting values (which, practically speaking, are treated as a simultaneous\nDELETE\n/\nINSERT\n pair, or \nundo\n/\nredo\n values as they are called in Flink).\nFurthermore, if we consider the table/stream conversion rules for materialized\nviews, the only real difference is the trigger used:\nInput tables\n are implicitly triggered via a \nSCAN-AND-STREAM\n trigger\ninstead of a \nSCAN\n trigger. Everything else is the same as classic batch\nqueries.\nOutput tables\n are treated the same as classi\nc batch queries.\nGrouping/ungrouping operations\n function the same as classic batch\nqueries, \nwith the only difference being the use of a \nSCAN-AND-\nSTREAM\n trigger instead of a \nSNAPSHOT\n trigger for implicit ungrouping\noperations.\nGiven this example, it’s clear to see that SQL’s inherent table bias is not just\nan artifact of SQL being limited to batch processing:\n materialized views\nlend SQL the ability to perform a specific type of stream processing without\nany significant changes in approach, including the inherent bias toward\ntables. Classic SQL is just a table-biased model, regardless \nof whether\n you’re\nusing it for batch or stream processing.\nLooking Forward: Toward Robust Streaming SQL\nWe’ve now looked at time-varying \nrelations, the ways in which tables and\nstreams provide different renderings of a time-varying relation, and what the\ninherent biases of the Beam and SQL models are with respect to stream and\n12\ntable theory. So where does all of this leave us? And perhaps more to the\npoint, what do we need to change or add within SQL to support robust stream\nprocessing? The surprising answer is: not much if we have good defaults.\nWe know that the key conceptual change is to replace classic, point-in-time\nrelations with time-varying relations. We saw earlier that this is a very\nseamless substitution, one which applies across the full breadth of relational\noperators already in existence, thanks to maintaining the critical closure\nproperty of relational algebra. But we also saw that dealing in time-varying\nrelations directly is often impractical; we need the ability to operate in terms\nof our two more-common physical manifestations: tables and streams. This is\nwhere some simple extensions with good defaults come in.\nWe also need some tools for robustly reasoning about time, specifically event\ntime. This is where things like timestamps, windowing, and triggering come\ninto play. But again, judicious choice of defaults will be important to\nminimize how often these extensions are necessary in practice.\nWhat’s great is that we don’t really need anything more than that. So let’s\nnow finally spend some time looking in detail at these two categories of\nextensions: \nstream/table selection\n and \ntemporal operators\n.\nStream and Table Selection\nAs we worked through time-varying relation examples, we already\nencountered the two key extensions related to\n stream and table selection.\nThey\n were \nthose \nTABLE\n and \nSTREAM\n keywords we placed after the \nSELECT\nkeyword to dictate our desired physical view of a given time-varying relation:\n                      \n \n-------------------------                 -------------------------\n| Name  | Total | Time  |                 | Name  | Total | Time  |\n-------------------------                 -------------------------\n| Julie | 12    | 12:07 |                 | Julie | 7     | 12:01 |\n| Frank | 3     | 12:03 |                 | Frank | 3     | 12:03 |\n-------------------------                 | Julie | 8     | 12:03 |\n                                          | Julie | 12    | 12:07 |\n                                          ..... [12:01, 12:07] ....\nThese extensions are \nrelatively straightforward and easy to use when\nnecessary. But the really important thing regarding stream and table selection\nis the choice of good defaults for times when they aren’t explicitly provided.\nSuch defaults should honor the classic, table-biased behavior of SQL that\neveryone is accustomed to, while also operating intuitively in a world that\nincludes streams. They should also be easy to remember. The goal here is to\nhelp maintain a natural feel to the system, while also greatly decreasing the\nfrequency with which we must use explicit extensions. A good choice of\ndefaults that satisfies all of these requirements is:\nIf \nall\n of the inputs are \ntables\n, the output is a \nTABLE\n.\nIf \nany\n of the inputs are \nstreams\n, the output is a \nSTREAM\n.\nWhat’s additionally important to call out here is that these physical\nrenderings of a time-varying relation are really only necessary when you\nwant to materialize the TVR in some way, either to view it directly or write it\nto some output table or stream. Given a SQL system that operates under the\ncovers in terms of full-fidelity time-varying relations, intermediate results\n(e.g., \nWITH AS\n or \nSELECT INTO\n statements) can remain as full-fidelity TVRs\nin whatever format the system naturally deals in, with no need to render them\ninto some other, more limited concrete manifestation.\nAnd that’s really it for stream and table selection. Beyond the ability to deal\nin streams and tables directly, we also need some better tools for reasoning\nabout time if we want to support robust, out-of-order stream processing\nwithin SQL. Let’s now look in more detail about what those entail.",5983
78-Temporal Operators.pdf,78-Temporal Operators,"Temporal Operators\nThe foundation of robust, out-of-order processing is the event-time\ntimestamp: that small piece of metadata that captures \nthe time \nat which an\nevent occurred rather than the time at which it is observed. In a SQL world,\nevent time is typically just another column of data for a given TVR, one\nwhich is natively present in the source data themselves.\n In that sense, this\nidea of materializing a record’s event time within the record itself is\nsomething SQL already handles naturally by putting a timestamp in a regular\ncolumn.\nBefore we go any further, let’s look at an example. To help tie all of this SQL\nstuff together with the concepts we’ve explored previously in the book, we\nresurrect our running example of summing up nine scores from various\nmembers of a team to arrive at that team’s total score. If you recall, those\nscores look like \nFigure 8-6\n when plotted on X = event-time/Y = processing-\ntime axes.\nFigure 8-6. \nData points in our running example\nIf we were to imagine these data as a classic SQL table, they might look\n13\nsomething like this, ordered by event time (left-to-right in \nFigure 8-6\n):\n------------------------------------------------\n| Name  | Team  | Score | EventTime | ProcTime |\n------------------------------------------------\n| Julie | TeamX |     5 |  12:00:26 | 12:05:19 |\n| Frank | TeamX |     9 |  12:01:26 | 12:08:19 |\n| Ed    | TeamX |     7 |  12:02:26 | 12:05:39 |\n| Julie | TeamX |     8 |  12:03:06 | 12:07:06 |\n| Amy   | TeamX |     3 |  12:03:39 | 12:06:13 |\n| Fred  | TeamX |     4 |  12:04:19 | 12:06:39 |\n| Naomi | TeamX |     3 |  12:06:39 | 12:07:19 |\n| Becky | TeamX |     8 |  12:07:26 | 12:08:39 |\n| Naomi | TeamX |     1 |  12:07:46 | 12:09:00 |\n------------------------------------------------\nIf you recall, we saw this table way back in \nChapter 2\n when I first introduced\nthis dataset. This rendering provides a little more detail on the data than\nwe’ve typically shown, explicitly highlighting the fact that the nine scores\nthemselves belong to seven different users, each a member of the same team.\nSQL provides a nice, concise way to see the data laid out fully before we\nbegin diving into examples.\nAnother nice\n thing \nabout this view of the data is that it fully captures the\nevent time and processing time for each record.\n You can imagine the event-\ntime column as being just another piece of the original data, and the\nprocessing-time column as being something supplied by the system (in this\ncase, using a hypothetical \nSys.MTime\n column that records the processing-\ntime modification timestamp of a given row; that is, the time at which that\nrow arrived in the source table), capturing the ingress time of the records\nthemselves into the system.\nThe fun thing about SQL is how easy it is to view your data in different ways.\nFor example, if we instead want to see the data in processing-time order\n(bottom-to-top in \nFigure 8-6\n), we could simply update the \nORDER BY\n clause:\n-----------------------------------------------\n| Name  | Team  | Score | EventTime | ProcTime |\n-----------------------------------------------\n| Julie | TeamX |     5 |  12:00:26 | 12:05:19 |\n| Ed    | TeamX |     7 |  12:02:26 | 12:05:39 |\n| Amy   | TeamX |     3 |  12:03:39 | 12:06:13 |\n| Fred  | TeamX |     4 |  12:04:19 | 12:06:39 |\n| Julie | TeamX |     8 |  12:03:06 | 12:07:06 |\n| Naomi | TeamX |     3 |  12:06:39 | 12:07:19 |\n| Frank | TeamX |     9 |  12:01:26 | 12:08:19 |\n| Becky | TeamX |     8 |  12:07:26 | 12:08:39 |\n| Naomi | TeamX |     1 |  12:07:46 | 12:09:00 |\n------------------------------------------------\nAs we learned earlier, these table renderings of the data are really a partial-\nfidelity view of the complete underlying TVR. If we were to instead query\nthe full table-oriented \nTVR\n (but only for the three most important columns, for\nthe sake of brevity), it would expand to something like this:\n-----------------------------------------------------------------------\n|         [-inf, 12:05:19)         |       [12:05:19, 12:05:39)       |\n  \n| -------------------------------- | -------------------------------- |\n  \n| | Score | EventTime | ProcTime | | | Score | EventTime | ProcTime | |\n| -------------------------------- | -------------------------------- |\n| -------------------------------- | |     5 |  12:00:26 | 12:05:19 | |\n|                                  | -------------------------------- |\n|                                  |                                  |\n-----------------------------------------------------------------------\n|       [12:05:39, 12:06:13)       |       [12:06:13, 12:06:39)       |\n  \n| -------------------------------- | -------------------------------- |\n  \n| | Score | EventTime | ProcTime | | | Score | EventTime | ProcTime | |\n| -------------------------------- | -------------------------------- |\n| |     5 |  12:00:26 | 12:05:19 | | |     5 |  12:00:26 | 12:05:19 | |\n| |     7 |  12:02:26 | 12:05:39 | | |     7 |  12:02:26 | 12:05:39 | |\n| -------------------------------- | |     3 |  12:03:39 | 12:06:13 | |\n|                                  | -------------------------------- |\n-----------------------------------------------------------------------\n|       [12:06:39, 12:07:06)       |       [12:07:06, 12:07:19)       |\n| -------------------------------- | -------------------------------- |\n| | Score | EventTime | ProcTime | | | Score | EventTime | ProcTime | |\n| -------------------------------- | -------------------------------- |\n| |     5 |  12:00:26 | 12:05:19 | | |     5 |  12:00:26 | 12:05:19 | |\n| |     7 |  12:02:26 | 12:05:39 | | |     7 |  12:02:26 | 12:05:39 | |\n| |     3 |  12:03:39 | 12:06:13 | | |     3 |  12:03:39 | 12:06:13 | |\n| |     4 |  12:04:19 | 12:06:39 | | |     4 |  12:04:19 | 12:06:39 | |\n| -------------------------------- | |     8 |  12:03:06 | 12:07:06 | |\n|                                  | -------------------------------- |\n-----------------------------------------------------------------------\n|       [12:07:19, 12:08:19)       |       [12:08:19, 12:08:39)       |\n  \n| -------------------------------- | -------------------------------- |\n  \n| | Score | EventTime | ProcTime | | | Score | EventTime | ProcTime | |\n| -------------------------------- | -------------------------------- |\n| |     5 |  12:00:26 | 12:05:19 | | |     5 |  12:00:26 | 12:05:19 | |\n| |     7 |  12:02:26 | 12:05:39 | | |     7 |  12:02:26 | 12:05:39 | |\n| |     3 |  12:03:39 | 12:06:13 | | |     3 |  12:03:39 | 12:06:13 | |\n| |     4 |  12:04:19 | 12:06:39 | | |     4 |  12:04:19 | 12:06:39 | |\n| |     8 |  12:03:06 | 12:07:06 | | |     8 |  12:03:06 | 12:07:06 | |\n| |     3 |  12:06:39 | 12:07:19 | | |     3 |  12:06:39 | 12:07:19 | |\n| -------------------------------- | |     9 |  12:01:26 | 12:08:19 | |\n|                                  | -------------------------------- |\n|                                  |                                  |\n-----------------------------------------------------------------------\n|       [12:08:39, 12:09:00)       |         [12:09:00, now)          |\n| -------------------------------- | -------------------------------- |\n| | Score | EventTime | ProcTime | | | Score | EventTime | ProcTime | |\n| -------------------------------- | -------------------------------- |\n| |     5 |  12:00:26 | 12:05:19 | | |     5 |  12:00:26 | 12:05:19 | |\n| |     7 |  12:02:26 | 12:05:39 | | |     7 |  12:02:26 | 12:05:39 | |\n| |     3 |  12:03:39 | 12:06:13 | | |     3 |  12:03:39 | 12:06:13 | |\n| |     4 |  12:04:19 | 12:06:39 | | |     4 |  12:04:19 | 12:06:39 | |\n| |     8 |  12:03:06 | 12:07:06 | | |     8 |  12:03:06 | 12:07:06 | |\n| |     3 |  12:06:39 | 12:07:19 | | |     3 |  12:06:39 | 12:07:19 | |\n| |     9 |  12:01:26 | 12:08:19 | | |     9 |  12:01:26 | 12:08:19 | |\n| |     8 |  12:07:26 | 12:08:39 | | |     8 |  12:07:26 | 12:08:39 | |\n| -------------------------------- | |     1 |  12:07:46 | 12:09:00 | |\n|                                  | -------------------------------- |\n-----------------------------------------------------------------------\nThat’s a lot of data. Alternatively, the \nSTREAM\n version would render much\nmore compactly in this instance; thanks to there being no explicit grouping in\nthe relation, it looks essentially identical to the point-in-time \nTABLE\n rendering\nearlier, with the addition of the trailing footer describing the range of\nprocessing time captured in the stream so far, plus the note that the system is\nstill waiting for more data in the stream (assuming we’re treating the stream\nas unbounded; we’ll see a bounded version of the stream shortly):\n--------------------------------\n| Score | EventTime | ProcTime |\n--------------------------------\n|     5 |  12:00:26 | 12:05:19 |\n|     7 |  12:02:26 | 12:05:39 |\n|     3 |  12:03:39 | 12:06:13 |\n|     4 |  12:04:19 | 12:06:39 |\n|     8 |  12:03:06 | 12:07:06 |\n|     3 |  12:06:39 | 12:07:19 |\n|     9 |  12:01:26 | 12:08:19 |\n|     8 |  12:07:26 | 12:08:39 |\n|     1 |  12:07:46 | 12:09:00 |\n........ [12:00, 12:10] ........\nBut this is all just looking at the raw input records without any sort of\ntransformations. Much more interesting is when we start altering the\nrelations. When we’ve explored this example in the past, we’ve always\nstarted with classic batch processing to sum up the scores over the entire\ndataset, so let’s do the same here. The first example pipeline (previously\nprovided as \nExample 6-1\n) looked like \nExample 8-1\n in Beam.\nExample 8-1. \nSummation pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals =\n  input.apply(\nSum.integersPerKey()\n);\nAnd rendered in the streams and tables view of the world, that pipeline’s\nexecution looked like \nFigure 8-7\n.\nFigure 8-7. \nStreams and tables view of classic batch processing\nGiven that we already have our\n data placed into an appropriate schema, we\nwon’t be doing any parsing in SQL; instead, we focus on everything in the\npipeline after the parse transformation. And because we’re going with the\nclassic batch model of retrieving a single answer only after all of the input\ndata have been processed, the \nTABLE\n and \nSTREAM\n views of the summation\nrelation would look essentially identical (recall that we’re dealing with\nbounded versions of our dataset for these initial, batch-style examples; as a\nresult, this \nSTREAM\n query actually terminates with a line of dashes and an\nEND-OF-STREAM\n marker):\n------------------------------------------\n| Total | MAX(EventTime) | MAX(ProcTime) |\n------------------------------------------\n|    48 |       12:07:46 |      12:09:00 |\n------------------------------------------\n00:00 / 00:00\n------------------------------------------\n| Total | MAX(EventTime) | MAX(ProcTime) |\n------------------------------------------\n|    48 |       12:07:46 |      12:09:00 |\n------ [12:00, 12:10] END-OF-STREAM ------\nMore interesting is when we start adding windowing into the mix. That will\ngive us a chance to begin looking more closely at the temporal operations that\nneed to be added to SQL to support robust stream processing.\nWhere\n: windowing\nAs we learned in \nChapter 6\n, windowing is\n a modification\n of grouping by key,\nin which the window becomes a secondary part of a hierarchical key. As with\nclassic programmatic batch processing, you can window data into more\nsimplistic windows quite easily within SQL as it exists now by simply\nincluding time as part of the \nGROUP BY\n parameter. Or, if the system in\nquestion provides it, you can use a built-in windowing operation. We look at\nSQL examples of both in a moment, but first, let’s revisit the programmatic\nversion from \nChapter 3\n. Thinking back to \nExample 6-2\n, the windowed Beam\npipeline looked like that shown in \nExample 8-2\n.\nExample 8-2. \nSummation pipeline\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  \n.apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n)\n  .apply(\nSum.integersPerKey()\n);\nAnd the execution of that pipeline (in streams and tables rendering from\nFigure 6-5\n), looked like the diagrams presented in \nFigure 8-8\n.\nFigure 8-8. \nStreams and tables view of windowed summation on a batch engine\nAs we saw before, the only material change from Figure \n8-7\n to \n8-8\n is that the\ntable created by the \nSUM\n operation is now partitioned into fixed, two-minute\nwindows of time, yielding four windowed answers at the end rather than the\nsingle global sum that we had previously.\nTo do the same thing in SQL, we have two options: implicitly window by\nincluding some unique feature of the window (e.g., the end timestamp) in the\nGROUP BY\n statement, or use a built-in windowing operation. Let’s look at\nboth.\nFirst, ad hoc windowing. In this case, we perform the math of calculating\nwindows ourselves in our SQL statement:\n------------------------------------------------\n00:00 / 00:00\n| Total | Window               | MAX(ProcTime) |\n------------------------------------------------\n| 14    | [12:00:00, 12:02:00) | 12:08:19      |\n| 18    | [12:02:00, 12:04:00) | 12:07:06      |\n| 4     | [12:04:00, 12:06:00) | 12:06:39      |\n| 12    | [12:06:00, 12:08:00) | 12:09:00      |\n------------------------------------------------\nWe can also achieve the same result using an explicit windowing statement\nsuch as those supported by Apache Calcite:\n------------------------------------------------\n| Total | Window               | MAX(ProcTime) |\n------------------------------------------------\n| 14    | [12:00:00, 12:02:00) | 12:08:19      |\n| 18    | [12:02:00, 12:04:00) | 12:07:06      |\n| 4     | [12:04:00, 12:06:00) | 12:06:39      |\n| 12    | [12:06:00, 12:08:00) | 12:09:00      |\n------------------------------------------------\nThis then begs the question: if we can implicitly window using existing SQL\nconstructs, why even bother supporting explicit windowing constructs? There\nare two reasons, only the first of which is apparent in this example (we’ll see\nthe other one in action later on in the chapter):\n1\n. \nWindowing takes care of the window-computation math for you. It’s\na lot easier to consistently get things right when you specify basic\nparameters like width and slide directly rather than computing the\nwindow math yourself.\n2\n. \nWindowing allows the concise expression of more complex,\ndynamic groupings such as sessions. Even though SQL is technically\nable to express the every-element-within-some-temporal-gap-of-\n14\nanother-element relationship that defines session windows, the\ncorresponding incantation is a tangled mess of analytic functions,\nself joins, and array unnesting that no mere mortal could be\nreasonably expected to conjure on their own.\nBoth are compelling arguments for providing first-class windowing\nconstructs in SQL, in addition to the ad hoc windowing capabilities that\nalready exist.\nAt this point, we’ve seen what windowing looks like from a classic\nbatch/classic relational perspective when consuming the data as a table. But if\nwe want to consume the data as a stream, we get back to that third question\nfrom the Beam Model: when in processing time do we materialize outputs?\nWhen\n: triggers\nThe answer to that question, as before, is triggers\n and watermarks.\n However,\nin the context of SQL, there’s a strong argument to be made for having a\ndifferent set of defaults than those we introduced with the Beam Model in\nChapter 3\n: rather than defaulting to using a single watermark trigger, a more\nSQL-ish default would be to take a cue from materialized views and trigger\non every element. In other words, any time a new input arrives, we produce a\ncorresponding new output.\nA SQL-ish default: per-record triggers\nThere are two compelling benefits to using trigger-every-record as the\ndefault:\nSimplicity\nThe semantics of per-record updates are easy to understand; materialized\nviews have operated this way for years.\nFidelity\nAs in change data capture systems, per-record triggering yields a full-\nfidelity stream rendering of a given time-varying relation; no information\nis lost as part of the conversion.\nThe downside is primarily cost: triggers are always applied after a grouping\noperation, and the nature of grouping often presents an opportunity to reduce\nthe cardinality of data flowing through the system, thus commensurately\nreducing the cost of further processing those aggregate results downstream.\nEven so, the benefits in clarity and simplicity for use cases where cost is not\nprohibitive arguably outweigh the cognitive complexity of defaulting to a\nnon-full-fidelity trigger up front.\nThus, for our first take at consuming aggregate team scores as a stream, let’s\nsee what things would look like using a per-record trigger. Beam itself\ndoesn’t have a precise per-record trigger, so, as demonstrated in \nExample 8-\n3\n, we instead use a repeated \nAfterCount(1)\n trigger, which will fire\nimmediately any time a new record arrives.\nExample 8-3. \nPer-record trigger\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(Repeatedly(AfterCount(1)))\n  .apply(\nSum.integersPerKey()\n);\nA streams and tables rendering of this pipeline would then look something\nlike that depicted in \nFigure 8-9\n.\nFigure 8-9. \nStreams and tables view of windowed summation on a streaming engine with\nper-record triggering\nAn interesting side effect of using per-record triggers is how it somewhat\nmasks the effect of data being brought to rest because they are then\nimmediately put back into motion again by the trigger. Even so, the aggregate\n00:00 / 00:00\nartifact from the grouping remains at rest in the table, as the ungrouped\nstream of values flows away from it.\nMoving back to SQL, we can see now what the effect of rendering the\ncorresponding time-value relation as a stream would be. It (unsurprisingly)\nlooks a lot like the stream of values in the animation in \nFigure 8-9\n:\n------------------------------------------------\n| Total | Window               | MAX(ProcTime) |\n------------------------------------------------\n| 5     | [12:00:00, 12:02:00) | 12:05:19      |\n| 7     | [12:02:00, 12:04:00) | 12:05:39      |\n| 10    | [12:02:00, 12:04:00) | 12:06:13      |\n| 4     | [12:04:00, 12:06:00) | 12:06:39      |\n| 18    | [12:02:00, 12:04:00) | 12:07:06      |\n| 3     | [12:06:00, 12:08:00) | 12:07:19      |\n| 14    | [12:00:00, 12:02:00) | 12:08:19      |\n| 11    | [12:06:00, 12:08:00) | 12:08:39      |\n| 12    | [12:06:00, 12:08:00) | 12:09:00      |\n................ [12:00, 12:10] ................\nBut even for this simple use case, it’s pretty chatty. If we’re building a\npipeline to process data for a large-scale mobile application, we might not\nwant to pay the cost of processing downstream updates for each and every\nupstream user score. This is where custom triggers come in.\nWatermark triggers\nIf we were to switch the\n Beam pipeline to use a watermark\n trigger, for\nexample, we could get exactly one output per window in the stream version\nof the TVR, as demonstrated in \nExample 8-4\n and shown in \nFigure 8-10\n.\nExample 8-4. \nWatermark trigger\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\nAfterWatermark()\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 8-10. \nWindowed summation with watermark triggering\nTo get the same effect in SQL, we’d need language support for specifying a\ncustom trigger. Something like an \nEMIT \n<when>\n statement, such as \nEMIT\nWHEN WATERMARK PAST \n<column>\n. This would signal to the system that the\ntable created by the aggregation should be triggered into a stream exactly\nonce per row, when the input watermark for the table exceeds the timestamp\nvalue in the specified column (which in this case happens to be the end of the\nwindow).\nLet’s look at this relation rendered as a stream. From the perspective of\nunderstanding when trigger firings occur, it’s also handy to stop relying on\nthe \nMTime\n values from the original inputs and instead capture the current\ntimestamp at which rows in the stream are emitted:\n-------------------------------------------\n| Total | Window               | EmitTime |\n-------------------------------------------\n| 5     | [12:00:00, 12:02:00) | 12:06:00 |\n00:00 / 00:00\n| 18    | [12:02:00, 12:04:00) | 12:07:30 |\n| 4     | [12:04:00, 12:06:00) | 12:07:41 |\n| 12    | [12:06:00, 12:08:00) | 12:09:22 |\n............. [12:00, 12:10] ..............\nThe main downside here is the late data problem due to the use of a heuristic\nwatermark, as we encountered in previous chapters. In light of late data, a\nnicer option might be to also immediately output \nan update any time a late\nrecord shows up, using a variation on the watermark trigger that supported\nrepeated late firings, as shown in \nExample 8-5\n and \nFigure 8-11\n.\nExample 8-5. \nWatermark trigger with late firings\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(\nAfterWatermark()\n                   \n.withLateFirings(AfterCount(1))\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 8-11. \nWindowed summation with on-time/late triggering\nWe can do the same thing in SQL by allowing the specification of two\ntriggers:\nA watermark trigger to give us an initial value: \nWHEN WATERMARK\nPAST \n<column>\n, with the end of the window used as the timestamp\n<column>\n.\nA repeated delay trigger for late data: \nAND THEN AFTER\n<duration>\n, with a \n<duration>\n of 0 to give us per-record\nsemantics.\n00:00 / 00:00\nNow that we’re getting multiple rows per window, it can also be useful to\nhave another two system columns available: the timing of each row/pane for\na given window relative \nto the\n watermark (\nSys.EmitTiming\n), and the index\nof the pane/row for a given window (\nSys.EmitIndex\n, to identify the\nsequence of revisions for a given row/window):\n-----------------------------------------------------------------------\n-----\n| Total | Window               | EmitTime | Sys.EmitTiming |\n \nSys.EmitIndex |\n-----------------------------------------------------------------------\n-----\n| 5     | [12:00:00, 12:02:00) | 12:06:00 | on-time        | 0\n             \n|\n| 18    | [12:02:00, 12:04:00) | 12:07:30 | on-time        | 0\n             \n|\n| 4     | [12:04:00, 12:06:00) | 12:07:41 | on-time        | 0\n             \n|\n| 14    | [12:00:00, 12:02:00) | 12:08:19 | late           | 1\n             \n|\n| 12    | [12:06:00, 12:08:00) | 12:09:22 | on-time        | 0\n             \n|\n.............................. [12:00, 12:10]\n \n..............................\nFor each pane, using this trigger, we’re able to get a single on-time answer\nthat is likely to be correct, thanks to our heuristic watermark. And for any\ndata that arrives late, we can get an updated version of the row amending our\nprevious results.\nRepeated delay triggers\nThe other main temporal trigger use case \nyou might want\n is repeated delayed\nupdates; that is, trigger a window one minute (in processing time) after any\nnew data for it arrive. Note that this is different than triggering on aligned\nboundaries, as you would get with a microbatch system. As \nExample 8-6\nshows, triggering via a delay relative to the most recent new record arriving\nfor the window/row helps spread triggering load out more evenly than a\nbursty, aligned trigger would. It also does not require any sort of watermark\nsupport. \nFigure 8-12\n presents the results.\nExample 8-6. \nRepeated triggering with one-minute delays\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(FixedWindows.of(TWO_MINUTES))\n               \n.triggering(Repeatedly(\nUnalignedDelay(ONE_MINUTE)\n)\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 8-12. \nWindowed summation with repeated one-minute-delay triggering\nThe effect of using such a trigger is very similar to the per-record triggering\nwe started out with but slightly less chatty thanks to the additional delay\nintroduced in triggering, which allows the system to elide some number of\nthe rows being produced. Tweaking the delay allows us to tune the volume of\ndata generated, and thus balance the tensions of cost and timeliness as\nappropriate for the use case.\nRendered as a SQL stream, it would look something like this:\n00:00 / 00:00\n-----------------------------------------------------------------------\n-----\n| Total | Window               | EmitTime | Sys.EmitTiming |\n \nSys.EmitIndex |\n-----------------------------------------------------------------------\n-----\n| 5     | [12:00:00, 12:02:00) | 12:06:19 | n/a            | 0\n             \n|\n| 10    | [12:02:00, 12:04:00) | 12:06:39 | n/a            | 0\n             \n|\n| 4     | [12:04:00, 12:06:00) | 12:07:39 | n/a            | 0\n             \n|\n| 18    | [12:02:00, 12:04:00) | 12:08:06 | n/a            | 1\n             \n|\n| 3     | [12:06:00, 12:08:00) | 12:08:19 | n/a            | 0\n             \n|\n| 14    | [12:00:00, 12:02:00) | 12:09:19 | n/a            | 1\n             \n|\n| 12    | [12:06:00, 12:08:00) | 12:09:22 | n/a            | 1\n             \n|\n.............................. [12:00, 12:10]\n \n..............................\nData-driven triggers\nBefore moving on to the final question in the \nBeam Model, it’s worth briefly\ndiscussing\n the idea of \ndata-driven triggers\n. Because of the dynamic way\ntypes are handled in SQL, it might seem like data-driven triggers would be a\nvery natural addition to the proposed \nEMIT \n<when>\n clause. For example,\nwhat if we want to trigger our summation any time the total score exceeds\n10? Wouldn’t something like \nEMIT WHEN Score > 10\n work very naturally?\nWell, yes and no. Yes, such a construct would fit very naturally. But when\nyou think about what would actually be happening with such a construct, you\nessentially would be triggering on every record, and then executing the \nScore\n> 10\n predicate to decide whether the triggered row should be propagated\ndownstream. As you might recall, this sounds a lot like what happens with a\nHAVING\n clause. And, indeed, you can get the exact same effect by simply\nprepending \nHAVING Score > 10\n to the end of the query. At which point, it\nbegs the question: is it worth adding explicit data-driven triggers? Probably\nnot. Even so, it’s still encouraging to see just how easy it is to get the desired\neffect of data-driven triggers using standard SQL and well-chosen defaults.\nHow\n: accumulation\nSo far in this section, we’ve been ignoring the \nSys.Undo\n column that I\nintroduced toward the beginning of this chapter.\n As a result, we’ve defaulted\nto using \naccumulating mode\n to answer the question of \nhow\n refinements for a\nwindow/row relate to one another.\n In other words, any time we observed\nmultiple revisions of an aggregate row, the later revisions built upon the\nprevious revisions, accumulating new inputs together with old ones. I opted\nfor this approach because it matches the approach used in an earlier chapter,\nand it’s a relatively straightforward translation from how things work in a\ntable world.\nThat said, accumulating mode has some major drawbacks. In fact, as we\ndiscussed in \nChapter 2\n, it’s plain broken for any query/pipeline with a\nsequence of two or more grouping operations due to over counting. The only\nsane way to allow for the consumption of multiple revisions of a row within a\nsystem that allows for queries containing more than one serial grouping\noperation is if it operates by default in \naccumulating and retracting\n mode.\nOtherwise, you run into issues where a given input record is included\nmultiple times in a single aggregation due to the blind incorporation of\nmultiple revisions for a single row.\nSo, when we come to the question of incorporating accumulation mode\nsemantics into a SQL world, the option that fits best with our goal of\nproviding an intuitive and natural experience is if the system uses retractions\nby default under the covers.\n As noted when \nI introduced the \nSys.Undo\ncolumn earlier, if you don’t care about the retractions (as in the examples in\n15\nthis section up until now), you don’t need to ask for them. But if you do ask\nfor them, they should be there.\nRetractions in a SQL world\nTo see what I mean, let’s look at another example. \nTo motivate the problem\nappropriately, let’s look at a use case that’s relatively impractical \nwithout\nretractions: building session windows and writing them incrementally to a\nkey/value store like HBase. In this case, we’ll be producing incremental\nsessions from our aggregation as they are built up. But in many cases, a given\nsession will simply be an evolution of one or more previous sessions. In that\ncase, you’d really like to delete the previous session(s) and replace it/them\nwith the new one. But how do you do that? The only way to tell whether a\ngiven session replaces another one is to compare them to see whether the new\none overlaps the old one. But that means duplicating some of the session-\nbuilding logic in a separate part of your pipeline. And, more important, it\nmeans that you no longer have idempotent output, and you’ll thus need to\njump through a bunch of extra hoops if you want to maintain end-to-end\nexactly-once semantics. Far better would be for the pipeline to simply tell\nyou which sessions were removed and which were added in their place. This\nis what retractions give you.\nTo see this in action (and in SQL), let’s modify our example pipeline to\ncompute session windows with a gap duration of one minute. For simplicity\nand clarity, we go back to using the default per-record trigger. Note that I’ve\nalso shifted a few of the data points within processing time for these session\nexamples to make the diagram cleaner; event-time timestamps remain the\nsame. The updated dataset looks like this (with shifted processing-time\ntimestamps highlighted in yellow):\n--------------------------------\n| Score | EventTime | ProcTime |\n--------------------------------\n|     5 |  12:00:26 | 12:05:19 |\n|     7 |  12:02:26 | 12:05:39 |\n|     3 |  12:03:39 | 12:06:13 |\n|     4 |  12:04:19 |\n 12:06:46 \n|  # Originally 12:06:39\n|     3 |  12:06:39 | 12:07:19 |\n|     8 |  12:03:06 |\n 12:07:33 \n|  # Originally 12:07:06\n|     8 |  12:07:26 |\n 12:08:13 \n|  # Originally 12:08:39\n|     9 |  12:01:26 | 12:08:19 |\n|     1 |  12:07:46 | 12:09:00 |\n........ [12:00, 12:10] ........\nTo begin with, let’s look at the pipeline without retractions. After it’s clear\nwhy that pipeline is problematic for the use case of writing incremental\nsessions to a key/value store, we’ll then look at the version with retractions.\nThe Beam code for the nonretracting pipeline would look something like\nExample 8-7\n. \nFigure 8-13\n shows the results.\nExample 8-7. \nSession windows with per-record triggering and accumulation\nbut no retractions\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(\nSessions.withGapDuration(ONE_MINUTE)\n)\n               \n.triggering(Repeatedly(AfterCount(1))\n               \n.accumulatingFiredPanes()\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 8-13. \nSession window summation with accumulation but no retractions\nAnd finally, rendered in SQL, the output stream would look like this:\n00:00 / 00:00\n-------------------------------------------\n| Total | Window               | EmitTime |\n-------------------------------------------\n| 5     | [12:00:26, 12:01:26) | 12:05:19 |\n| 7     | [12:02:26, 12:03:26) | 12:05:39 |\n| 3     | [12:03:39, 12:04:39) | 12:06:13 |\n| 7     | [12:03:39, 12:05:19) | 12:06:46 |\n| 3     | [12:06:39, 12:07:39) | 12:07:19 |\n| 22    | [12:02:26, 12:05:19) | 12:07:33 |\n| 11    | [12:06:39, 12:08:26) | 12:08:13 |\n| 36    | [12:00:26, 12:05:19) | 12:08:19 |\n| 12    | [12:06:39, 12:08:46) | 12:09:00 |\n............. [12:00, 12:10] ..............\nThe important thing to notice in here (in the animation as well as the SQL\nrendering) is what the stream of incremental sessions looks like. From our\nholistic viewpoint, it’s pretty easy to visually identify in the animation which\nlater sessions supersede those that came before. But imagine receiving\nelements in this stream one by one (as in the SQL listing) and needing to\nwrite them to HBase in a way that eventually results in the HBase table\ncontaining only the two final sessions (with values 36 and 12). How would\nyou do that? Well, you’d need to do a bunch of read-modify-write operations\nto read all of the existing sessions for a key, compare them with the new\nsession, determine which ones overlap, issue deletes for the obsolete sessions,\nand then finally issue a write for the new session—all at significant additional\ncost, and with a loss of idempotence, which would ultimately leave you\nunable to provide end-to-end, exactly-once semantics. It’s just not practical.\nContrast this then with the same pipeline, but with retractions enabled, as\ndemonstrated in \nExample 8-8\n and depicted in \nFigure 8-14\n.\nExample 8-8. \nSession windows with per-record triggering, accumulation, and\nretractions\nPCollection<String> raw = IO.read(...);\nPCollection<KV<Team, Integer>> input = raw.apply(\nnew ParseFn()\n);\nPCollection<KV<Team, Integer>> totals = input\n  .apply(\nWindow.into(Sessions.withGapDuration(ONE_MINUTE))\n               \n.triggering(Repeatedly(AfterCount(1))\n               \n.accumulating\nAndRetracting\nFiredPanes()\n)\n  .apply(\nSum.integersPerKey()\n);\nFigure 8-14. \nSession window summation with accumulation and retractions\nAnd, lastly, in SQL form. For the SQL version, we’re assuming that the\nsystem is using retractions under the covers by default, and individual\nretraction rows are then materialized in the stream any time we request the\nspecial \nSys.Undo\n column.\n As I described originally, the value of that\ncolumn is that it allows us to distinguish retraction rows (labeled \nundo\n in the\nSys.Undo\n column) from normal rows (unlabeled in the \nSys.Undo\n column\nhere for clearer contrast, though they could just as easily be labeled \nredo\n,\ninstead):\n--------------------------------------------------\n| Total | Window               | EmitTime | Undo |\n--------------------------------------------------\n| 5     | [12:00:26, 12:01:26) | 12:05:19 |      |\n| 7     | [12:02:26, 12:03:26) | 12:05:39 |      |\n| 3     | [12:03:39, 12:04:39) | 12:06:13 |      |\n| 3     | [12:03:39, 12:04:39) | 12:06:46 | undo |\n| 7     | [12:03:39, 12:05:19) | 12:06:46 |      |\n00:00 / 00:00\n16\n| 3     | [12:06:39, 12:07:39) | 12:07:19 |      |\n| 7     | [12:02:26, 12:03:26) | 12:07:33 | undo |\n| 7     | [12:03:39, 12:05:19) | 12:07:33 | undo |\n| 22    | [12:02:26, 12:05:19) | 12:07:33 |      |\n| 3     | [12:06:39, 12:07:39) | 12:08:13 | undo |\n| 11    | [12:06:39, 12:08:26) | 12:08:13 |      |\n| 5     | [12:00:26, 12:01:26) | 12:08:19 | undo |\n| 22    | [12:02:26, 12:05:19) | 12:08:19 | undo |\n| 36    | [12:00:26, 12:05:19) | 12:08:19 |      |\n| 11    | [12:06:39, 12:08:26) | 12:09:00 | undo |\n| 12    | [12:06:39, 12:08:46) | 12:09:00 |      |\n................. [12:00, 12:10] .................\nWith retractions included, the sessions stream no longer just includes new\nsessions, but also retractions for the old sessions that have been replaced.\nWith this stream, it’s trivial\n to properly build up the set of sessions in\nHBase over time: you simply write new sessions as they arrive (unlabeled\nredo\n rows) and delete old sessions as they’re retracted (\nundo\n rows). Much\nbetter!\nDiscarding mode, or lack thereof\nWith this example, we’ve shown\n how simply and\n naturally you can\nincorporate retractions into SQL to provide both \naccumulating mode\n and\naccumulating and retracting mode\n semantics. But what about \ndiscarding\nmode\n?\nFor specific use cases such as very simple pipelines that partially aggregate\nhigh-volume input data via a single grouping operation and then write them\ninto a storage system, which itself supports aggregation (e.g., a database-like\nsystem), discarding mode can be extremely valuable as a resource-saving\noption. But outside of those relatively narrow use cases, discarding mode is\nconfusing and error-prone. As such, it’s probably not worth incorporating\ndirectly into SQL. Systems that need it can provide it as an option outside of\nthe SQL language itself. Those that don’t can simply provide the more\nnatural default of \naccumulating and retracting mode\n, with the option to\nignore retractions when they aren’t needed.\n17",37832
79-Summary.pdf,79-Summary,"Summary\nThis has been a long journey but a fascinating one. We’ve covered a ton of\ninformation in this chapter, so let’s take a moment to reflect on it all.\nFirst, we reasoned that the key difference\n between streaming and\nnonstreaming data processing is the \nadded dimension of time\n. We observed\nthat relations (the foundational data object from relational algebra, which\nitself is the basis for SQL) themselves evolve over time, and from that\nderived the notion of a \nTVR\n, which captures the evolution of a relation as a\nsequence of classic snapshot relations over time. From that definition, we\nwere able to see\n that the \nclosure property\n of relational algebra \nremains intact\nin a world of TVRs, which means that the entire suite of relational operators\n(and thus SQL constructs) continues to function as one would expect as we\nmove from a world of point-in-time snapshot relations into a streaming-\ncompatible world of TVRs.\nSecond, we explored the biases inherent in both the Beam Model and the\nclassic SQL model as they exist today, coming to the conclusion that Beam\nhas a stream-oriented approach, whereas SQL takes a table-oriented\napproach.\nAnd finally, we looked at the hypothetical language extensions needed to add\nsupport for robust stream processing to SQL,\n as well as some carefully\nchosen defaults that can greatly decrease the need for those extensions to be\nused:\nTable/stream selection\nGiven that any time-varying relation can \nbe rendered in two different\nways (table or stream), we need the ability to choose which rendering we\nwant when materializing the results of a query. We introduced the \nTABLE\n,\nSTREAM\n, and \nTVR\n keywords to provide a nice explicit way to choose the\ndesired rendering.\nEven better is not needing to explicitly specify a choice, and that’s where\ngood defaults come in. If all the inputs are tables, a good default is for the\noutput to be a table, as well; this gives you the classic relational query\n18\nbehavior everyone is accustomed to. Conversely, if any of the inputs are\nstreams, a reasonable default is for the output to be a stream, as well.\nWindowing\nThough you can declare some types of\n simple windows declaratively\nusing existing SQL constructs, there is still value in having explicit\nwindowing operators:\nWindowing operators encapsulate the window-computation math.\nWindowing allows the concise expression of complex, dynamic\ngroupings like sessions.\nThus, the addition of simple windowing constructs for use in grouping\ncan help make queries less error prone while also providing capabilities\n(like sessions) that are impractical to express in declarative SQL as it\nexists today.\nWatermarks\nThis isn’t so much a SQL extension as it is a system-level feature. \nIf the\nsystem in question integrates watermarks under the covers, they can be\nused in conjunction with triggers to generate streams containing a single,\nauthoritative version of a row only after the input for that row is believed\nto be complete. This is critical for use cases in which it’s impractical to\npoll a materialized view table for results, and instead the output of the\npipeline must be consumed directly as a stream. Examples are\nnotifications and anomaly detection.\nTriggers\nTriggers define the shape of a stream as it is created from a TVR.\n If\nunspecified, the default should be per-record triggering, which provides\nstraightforward and natural semantics matching those of materialized\nviews. Beyond the default, there are essentially two main types of useful\ntriggers:\nWatermark triggers\n, for yielding a single output per\n window when\nthe inputs to that window are believed to be complete.\nRepeated delay triggers\n, for providing\n periodic updates.\nCombinations of those two can also be useful, especially in the case of\nheuristic watermarks, to provide the early/on-time/late pattern we saw\nearlier.\nSpecial system columns\nWhen consuming a TVR as a stream, there are some interesting metadata\nthat can be useful and which are most easily exposed as system-level\ncolumns. We looked at four:\nSys.MTime\nThe processing time at which a given \nrow was last modified in a\nTVR.\nSys.EmitTiming\nThe timing of the row \nemit relative to the watermark (early, on-time,\nlate).\nSys.EmitIndex\nThe zero-based \nindex of the emit version for this row.\nSys.Undo\nWhether the row is a \nnormal row or a retraction (\nundo\n). By default,\nthe system should operate with retractions under the covers, as is\nnecessary any time a series of more than one grouping operation\nmight exist. If the \nSys.Undo\n column is not projected when rendering\na TVR as a stream, only normal rows will be returned, providing a\nsimple way to toggle between \naccumulating\n and \naccumulating and\nretracting\n modes.\nStream processing with SQL doesn’t need to be difficult. In fact, stream\nprocessing in SQL is quite common already in the form of materialized\nviews. The important pieces really boil down to capturing the evolution of\n19\ndatasets/relations over time (via time-varying relations), providing the means\nof choosing between physical table or stream representations of those time-\nvarying relations, and providing the tools for reasoning about time\n(windowing, watermarks, and triggers) that we’ve been talking about\nthroughout this book. And, critically, you need good defaults to minimize\nhow often these extensions need to be used in practice.\n What I mean by “valid relation” here is simply a relation for which the\napplication of a given operator is well formed. For example, for the SQL\nquery \nSELECT x FROM y\n, a valid relation y would be any relation containing\nan attribute/column named x. Any relation not containing a such-named\nattribute would be invalid and, in the case of a real database system, would\nyield a query execution error.\n Much credit to Julian Hyde for this name and succinct rendering of the\nconcept.\n Note that the \nSys.Undo\n name used here is riffing off the concise \nundo/redo\nnomenclature from Apache Flink\n, which I think is a very clean way to\ncapture the ideas of retraction and nonretraction rows.\n Now, in this example, it’s not too difficult to figure out that the new value\nof 8 should replace the old value of 7, given that the mapping is 1:1. But\nwe’ll see a more complicated example later on when we talk about sessions\nthat is much more difficult to handle without having retractions as a guide.\n And indeed, this is a key point to remember. There are some systems that\nadvocate treating streams and tables as identical, claiming that we can simply\ntreat streams like never-ending tables. That statement is accurate inasmuch as\nthe true underlying primitive is the time-varying relation, and all relational\noperations may be applied equally to any time-varying relation, regardless of\nwhether the actual physical manifestation is a stream or a table. But that sort\nof approach conflates the two very different types of views that tables and\nstreams provide for a given time-varying relation. Pretending that two very\ndifferent things are the same might seem simple on the surface, but it’s not a\nroad toward understanding, clarity, and correctness.\n1\n2\n3\n4\n5\n6\n Here referring to tables in the sense of tables that can vary over time; that is,\nthe table-based TVRs we’ve been looking at.\n This one courtesy Julian Hyde.\n Though there are a number of efforts in flight across various projects that\nare trying to simplify the specification of triggering/ungrouping semantics.\nThe most compelling proposal, made independently within both the Flink and\nBeam communities, is that triggers should simply be specified at the outputs\nof a pipeline and automatically propagated up throughout the pipeline. In this\nway, one would describe only the desired shape of the streams that actually\ncreate materialized output; the shape of all other streams in the pipeline\nwould be implicitly derived from there.\n Though, of course, a single SQL query has vastly more expressive power\nthan a single MapReduce, given the far less-confining set of operations and\ncomposition options available.\n Note that we’re speaking conceptually here; there are of course a multitude\nof optimizations that can be applied in actual execution; for example, looking\nup specific rows via an index rather than scanning the entire table.\n It’s been brought to my attention multiple times that the “\nMATERIALIZED\n”\naspect of these queries is just an optimization: semantically speaking, these\nqueries could just as easily be replaced with generic \nCREATE VIEW\nstatements, in which case the database might instead simply rematerialize the\nentire view each time it is referenced. This is true. The reason I use the\nMATERIALIZED\n variant here is that the semantics of a materialized view are to\nincrementally update the view table in response to a stream of changes, which\nis indicative of the streaming nature behind them. That said, the fact that you\ncan instead provide a similar experience by re-executing a bounded query\neach time a view is accessed provides a nice link between streams and tables\nas well as a link between streaming systems and the way batch systems have\nbeen historically used for processing data that evolves over time. You can\neither incrementally process changes as they occur or you can reprocess the\nentire input dataset from time to time. Both are valid ways of processing an\nevolving table of data.\n6\n7\n8\n9\n10\n11\n12\n Though it’s probably fair to say that SQL’s table bias is likely an artifact of\nSQL’s \nroots\n in batch processing.\n For some use cases, capturing and using the current processing time for a\ngiven record as its event time going forward can be useful (for example,\nwhen logging events directly into a TVR, where the time of ingress is the\nnatural event time for that record).\n Maths are easy to get wrong.\n It’s sufficient for retractions to be used by default and not simply always\nbecause the system only needs the \noption\n to use retractions. There are\nspecific use cases; for example, queries with a single grouping operation\nwhose results are being written into an external storage system that supports\nper-key updates, where the system can detect retractions are not needed and\ndisable them as an optimization.\n Note that it’s a little odd for the simple addition of a new column in the\nSELECT\n statement to result in a new rows appearing in a query. A fine\nalternative approach would be to require \nSys.Undo\n rows to be filtered out via\na \nWHERE\n clause when not needed.\n Not that this triviality applies only in cases for which eventual consistency\nis sufficient. If you need to always have a globally coherent view of all\nsessions at any given time, you must 1) be sure to write/delete (via\ntombstones) each session at its emit time, and 2) only ever read from the\nHBase table at a timestamp that is less than the output watermark from your\npipeline (to synchronize reads against the multiple, independent\nwrites/deletes that happen when sessions merge). Or better yet, cut out the\nmiddle person and serve the sessions from your state tables directly.\n To be clear, they’re not all hypothetical. Calcite has support for the\nwindowing constructs described in this chapter.\n Note that the definition of “index” becomes complicated in the case of\nmerging windows like sessions. A reasonable approach is to take the\nmaximum of all of the previous sessions being merged together and\nincrement by one.\n12\n13\n14\n15\n16\n17\n18\n19",11637
80-9. Streaming Joins.pdf,80-9. Streaming Joins,,0
81-All Your Joins Are Belong to Streaming.pdf,81-All Your Joins Are Belong to Streaming,"Chapter 9. \nStreaming Joins\nWhen I first began learning about joins, it was an\n intimidating topic; \nLEFT\n,\nOUTER\n, \nSEMI\n, \nINNER\n, \nCROSS\n: the language of joins is expressive and\nexpansive. Add on top of that the dimension of time that streaming brings to\nthe table, and you’re left with what appears to be a challengingly complex\ntopic.\n The good news is that joins really aren’t the frightening beast with\nnasty, pointy teeth that they might initially appear to be. As is the case with\nso many other complex topics, after you understand the central ideas and\nthemes of joins, the broader landscape that’s built on top of these basics\nsuddenly becomes so much more accessible. So please join me now as we\nexplore the fascinating topic of...well, joins.\nAll Your Joins Are Belong to Streaming\nWhat does it mean to join two datasets? We understand intuitively that joins\nare just a specific type of grouping operation: by joining together\n data that\nshare some property (i.e., key), we collect together some number of\npreviously unrelated individual data elements into a \ngroup\n of related\nelements. And as we learned in \nChapter 6\n, grouping operations always\nconsume a stream and yield a table. Knowing these two things, it’s only a\nsmall leap to then arrive at the conclusion that forms the basis for this entire\nchapter: \nat their hearts, all joins are streaming joins\n.\nWhat’s great about this fact is that it actually makes the topic of streaming\njoins that much more tractable. All of the tools we’ve learned for reasoning\nabout time within the context of streaming grouping operations (windowing,\nwatermarks, triggers, etc.) continue to apply in the case of streaming joins.\nWhat’s perhaps intimidating is that adding streaming to the mix seems like it\ncould only serve to complicate things. But as you’ll see in the examples that\nfollow, there’s a certain elegant simplicity and consistency to modeling all\njoins as streaming joins. Instead of feeling like there are a confounding\nmultitude of different join approaches, it becomes clear that nearly all types\nof joins really boil down to minor variations on the same pattern. In the end,\nthat clarity of insight helps makes joins (streaming or otherwise) much less\nintimidating.\nTo give us something concrete to reason about, let’s consider a number of\ndifferent types of joins as they’re applied the following datasets, conveniently\nnamed \nLeft\n and \nRight\n to match the common nomenclature:\n12:10>\n SELECT \nTABLE\n * FROM Left;        \n12:10>\n SELECT \nTABLE\n \n* FROM Right;\n--------------------                    --------------------\n| Num | Id | Time  |                    | Num | Id | Time  |\n--------------------                    --------------------\n| 1   | L1 | 12:02 |                    | 2   | R2 | 12:01 |\n| 2   | L2 | 12:06 |                    | 3   | R3 | 12:04 |\n| 3   | L3 | 12:03 |                    | 4   | R4 | 12:05 |\n--------------------                    --------------------\nEach contains three columns:\nNum\nA single number.\nId\nA portmanteau of the first letter in the name of the corresponding table\n(“\nL\n” or “\nR\n”) and the \nNum\n, thus providing a way to uniquely identify the\nsource of a given cell in join results.\nTime\nThe arrival time of the given record in the system, which becomes\nimportant when considering streaming joins.\nTo keep things simple, note that our initial datasets will have strictly unique\njoin keys. When we get to \nSEMI\n joins, we’ll introduce some more\ncomplicated datasets to highlight join behavior in the presence of duplicate\nkeys.",3646
82-FULL OUTER.pdf,82-FULL OUTER,"We first look at \nunwindowed joins\n in a great deal of depth because\nwindowing often affects join semantics in only a minor way.\n After we\nexhaust our appetite for unwindowed joins, we then touch upon some of the\nmore interesting points of joins in a windowed context.\nUnwindowed Joins\nIt’s a popular myth that streaming joins over unbounded data always require\nwindowing.\n But by applying\n the concepts we learned in \nChapter 6\n, we can\nsee that’s simply not true. Joins (both windowed and unwindowed) are\nsimply another type of grouping operation, and grouping operations yield\ntables. Thus, if we want to consume the table created by an unwindowed join\n(or, equivalently,\n joins within a single global window covering all of time) as\na stream, we need only apply an ungrouping (or \ntrigger\n) operation that isn’t\nof the “wait until we’ve seen all the input” variety. Windowing the join into a\nnonglobal window and using a watermark trigger (i.e., a “wait until we’ve\nseen all the input in a finite temporal chunk of the stream” trigger) is indeed\none option, but so is triggering on every record (i.e., materialized view\nsemantics) or \nperiodically as processing time advances, regardless of whether\nthe join is windowed or not. Because it makes the examples easy to follow,\nwe assume the use of an implicit default per-record trigger in all of the\nfollowing unwindowed join examples that observe the join results as a\nstream.\nNow, onto joins themselves. ANSI SQL defines \nfive types of joins: \nFULL\nOUTER\n, \nLEFT OUTER\n, \nRIGHT OUTER\n, \nINNER\n, and \nCROSS\n. We look at the first\nfour in depth, and discuss the last only briefly in the next paragraph. We also\ntouch on two other interesting, but less-often encountered (and less well\nsupported, at least using standard syntax) variations: \nANTI\n and \nSEMI\n joins.\nOn the surface, it sounds like a lot of variations. But as you’ll see, there’s\nreally only one type of join at the core: the \nFULL OUTER\n join. A \nCROSS\n join is\njust a \nFULL OUTER\n join with a vacuously true join predicate; that is, it returns\nevery possible pairing of a row from the left table with a row from the right\ntable. All of the other join variations simply reduce down to some logical\n1\nsubset of the \nFULL OUTER\n join.\n As a result, after you understand the\ncommonality between all the different join types, it becomes a lot easier to\nkeep them all in your head. It also makes reasoning about them in the context\nof streaming all that much simpler.\nOne last note here before we get started: we’ll be primarily considering equi\njoins with at most 1:1 cardinality, by which\n I mean joins in which the join\npredicate is an equality statement and there is at most one matching row on\neach side of the join. This keeps the examples simple and concise. When we\nget to \nSEMI\n joins, we’ll expand our example to consider joins with arbitrary\nN:M cardinality, which will let us observe the behavior of more arbitrary\npredicate joins.\nFULL OUTER\nBecause they form the conceptual foundation for each of the other variations,\nwe first look at \nFULL OUTER\n joins.\n Outer joins embody a rather liberal and\noptimistic interpretation of the word “join”: the result of \nFULL OUTER\n–joining\ntwo datasets is essentially the full list of rows in both datasets,\n with rows in\nthe two datasets that share the same join key combined together, but\nunmatched rows for either side included unjoined.\nFor example, if we \nFULL OUTER\n–join our two example datasets into a new\nrelation containing only the joined IDs, the result would look something like\nthis:\n12:10>\n SELECT \nTABLE\n \n         Left.Id as L, \n         Right.Id as R,\n       FROM Left \nFULL OUTER\n JOIN Right\n       ON L.Num = R.Num;\n---------------\n| L    | R    |\n---------------\n| L1   | null | \n| L2   | R2   |\n| L3   | R3   |\n1\n2\n| null | R4   |\n---------------\nWe can see that the \nFULL OUTER\n join includes both rows that satisfied the\njoin predicate (e.g., “\nL2, R2\n” and “\nL3, R3\n”), but it also includes partial rows\nthat failed the predicate (e.g., “\nL1, null\n” and “\nnull, R4\n”, where the null is\nsignaling the unjoined portion of the data).\nOf course, that’s just a point-in-time snapshot of this \nFULL OUTER\n–join\nrelation, taken after all of the data have arrived in the system. We’re here to\nlearn about streaming joins, and streaming joins by definition involve the\nadded dimension of time. As we know from \nChapter 8\n, if we want to\nunderstand how a given dataset/relation changes over time, we want to speak\nin terms of time-varying relations (TVRs). \nSo to best understand how the join\nevolves over time, let’s look now at the full TVR for this join (with changes\nbetween each snapshot relation highlighted in yellow):\n12:10>\n SELECT \nTVR\n         Left.Id as L,\n         Right.Id as R,\n       FROM Left \nFULL OUTER\n JOIN Right\n       ON L.Num = R.Num;\n-----------------------------------------------------------------------\n--\n|  [-inf, 12:01)  |  [12:01, 12:02) |  [12:02, 12:03) |  [12:03, 12:04)\n \n|\n| --------------- | --------------- | --------------- | ---------------\n \n|\n| | L    | R    | | | L    | R    | | | L    | R    | | | L    | R    |\n \n|\n| --------------- | --------------- | --------------- | ---------------\n \n|\n| --------------- | \n| null | R2   |\n | \n| L1   | null |\n | | L1   | null |\n \n| \n|                 | --------------- | | null | R2   | | | null | R2   |\n \n|\n|                 |                 | --------------- | \n| L3   | null |\n \n|\n|                 |                 |                 | ---------------\n \n|\n-----------------------------------------------------------------------\n--\n|  [12:04, 12:05) |  [12:05, 12:06) |  [12:06, 12:07) |\n| --------------- | --------------- | --------------- |\n| | L    | R    | | | L    | R    | | | L    | R    | |\n| --------------- | --------------- | --------------- |\n| | L1   | null | | | L1   | null | | | L1   | null | |\n| | null | L2   | | | null | L2   | | |\n L2   \n| L2   | |\n| | L3   |\n L3   \n| | | L3   | L3   | | | L3   | L3   | |\n| --------------- | \n| null | L4   |\n | | null | L4   | |\n|                 | --------------- | --------------- |\n-------------------------------------------------------\nAnd, as you might then expect, the stream rendering of this TVR would\ncapture the specific deltas between each of those snapshots:\n12:00>\n SELECT \nSTREAM\n \n         Left.Id as L,\n         Right.Id as R, \n         CURRENT_TIMESTAMP as Time,\n         Sys.Undo as Undo\n       FROM Left \nFULL OUTER\n JOIN Right\n       ON L.Num = R.Num;\n------------------------------\n| L    | R    | Time  | Undo |\n------------------------------\n| null | R2   | 12:01 |      |\n| L1   | null | 12:02 |      |\n| L3   | null | 12:03 |      |\n| L3   | null | 12:04 | undo |\n| L3   | R3   | 12:04 |      |\n| null | R4   | 12:05 |      |\n| null | R2   | 12:06 | undo |\n| L2   | R2   | 12:06 |      |\n....... [12:00, 12:10] .......\nNote the inclusion of the \nTime\n and \nUndo\n columns, to highlight the times when\ngiven rows materialize in the stream, and also call out instances when an\nupdate to a given row first results in a retraction of the previous version of",7341
83-LEFT OUTER.pdf,83-LEFT OUTER,"that row. The undo/retraction rows are critical if this stream is to capture a\nfull-fidelity view of the TVR over time.\nSo, although each of these three renderings of the join (table, TVR, stream)\nare distinct from one another, it’s also pretty clear how they’re all just\ndifferent views on the same data: the table snapshot shows us the overall\ndataset as it exists after all the data have arrived, and the TVR and stream\nversions capture (in their own ways) the evolution of the entire relation over\nthe course of its existence.\nWith that basic familiarity of \nFULL OUTER\n joins in place, we now understand\nall of the core concepts of joins in a streaming context. No windowing\nneeded, no custom triggers, nothing particularly painful or unintuitive. Just a\nper-record evolution of the join over time, as you would expect. Even better,\nall of the other types of joins are just variations on this theme (conceptually,\nat least), essentially just an additional filtering operation performed on the\nper-record stream of the \nFULL OUTER\n join. Let’s now look at each of them in\nmore detail.\nLEFT OUTER\nLEFT OUTER\n joins are a just a \nFULL OUTER\n join \nwith any unjoined rows from\nthe right dataset removed. \nThis is most clearly seen by taking the original\nFULL OUTER\n join and graying out the rows that would be filtered. For a \nLEFT\nOUTER\n join, that would look like the following, where every row with an\nunjoined left side is filtered out of the original \nFULL OUTER\n join:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nLEFT OUTER\n JOIN Right          FROM Left\n \nLEFT OUTER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |                          \n| null | R2   | 12:01 |      |\n| L2   | R2   |                          | L1   | null | 12:02 |      |\n| L3   | R3   |                          | L3   | null | 12:03 |      |\n| null | R4   |\n                          | L3   | null | 12:04 | undo |\n---------------                          | L3   | R3   | 12:04 |      |\n                                         \n| null | R4   | 12:05 |      |\n                                         \n| null | R2   | 12:06 | undo |\n                                         | L2   | R2   | 12:06 |      |\n                                         ....... [12:00, 12:10] .......\nTo see what the table and stream would actually look like in practice, let’s\nlook at the same queries again, but this time with the grayed-out rows omitted\nentirely:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nLEFT OUTER\n JOIN Right          FROM Left\n \nLEFT OUTER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |                          | L1   | null | 12:02 |      |\n| L2   | R2   |                          | L3   | null | 12:03 |      |\n| L3   | R3   |                          | L3   | null | 12:04 | undo |\n---------------                          | L3   | R3   | 12:04 |      |\n                                         | L2   | R2   | 12:06 |      |",4101
84-INNER.pdf,84-INNER,"....... [12:00, 12:10] .......\nRIGHT OUTER\nRIGHT OUTER\n joins are the\n converse \nof a left join: all unjoined rows\n from the\nleft dataset in the full outer join are right out, *cough*, removed:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nRIGHT OUTER\n JOIN Right         FROM Left\n \nRIGHT OUTER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |\n                          | null | R2   | 12:01 |      |\n| L2   | R2   |                          \n| L1   | null | 12:02 |      |\n| L3   | R3   |                          \n| L3   | null | 12:03 |      |\n| null | R4   |                          \n| L3   | null | 12:04 | undo |\n---------------                          | L3   | R3   | 12:04 |      |\n                                         | null | R4   | 12:05 |      |\n                                         | null | R2   | 12:06 | undo |\n                                         | L2   | R2   | 12:06 |      |\n                                         ....... [12:00, 12:10] .......\nAnd here we see how the queries rendered as the actual \nRIGHT OUTER\n join\nwould appear:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nRIGHT OUTER\n JOIN Right         FROM Left\n \nRIGHT OUTER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L2   | R2   |                          | null | R2   | 12:01 |      |\n| L3   | R3   |                          | L3   | R3   | 12:04 |      |\n| null | R4   |                          | null | R4   | 12:05 |      |\n---------------                          | null | R2   | 12:06 | undo |\n                                         | L2   | R2   | 12:06 |      |\n                                         ....... [12:00, 12:10] .......\nINNER\nINNER\n joins are essentially the intersection of the \nLEFT OUTER\n and \nRIGHT\nOUTER\n joins.\n Or, to think of it subtractively, the rows removed from the\noriginal \nFULL OUTER\n join to create an \nINNER\n join are the union of the rows\nremoved from the \nLEFT OUTER\n and \nRIGHT OUTER\n joins. As a result, all rows\nthat remain unjoined on either side are absent from the \nINNER\n join:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nINNER\n JOIN Right               FROM Left\n \nINNER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |\n                          \n| null | R2   | 12:01 |      |\n| L2   | R2   |                          \n| L1   | null | 12:02 |      |\n| L3   | R3   |                          \n| L3   | null | 12:03 |      |\n| null | R4   |\n                          \n| L3   | null | 12:04 | undo |\n---------------                          | L3   | R3   | 12:04 |      |\n                                         \n| null | R4   | 12:05 |      |\n                                         \n| null | R2   | 12:06 | undo |\n                                         | L2   | R2   | 12:06 |      |\n                                         ....... [12:00, 12:10] .......\nAnd again, more succinctly rendered as the \nINNER\n join would look in reality:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nINNER\n JOIN Right               FROM Left\n \nINNER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L2   | R2   |                          | L3   | R3   | 12:04 |      |\n| L3   | R3   |                          | L2   | R2   | 12:06 |      |\n---------------                          ....... [12:00, 12:10] .......\nGiven this example, you might be inclined to think retractions never play a\npart in \nINNER\n join streams because they were all filtered out in this example.\nBut imagine if the value in the \nLeft\n table for the row with a \nNum\n of \n3\n were\nupdated from “\nL3\n” to “\nL3v2\n” at 12:07. In addition to resulting in a different\nvalue on the left side for our final \nTABLE\n query (again performed at 12:10,\nwhich is after the update to row \n3\n on the \nLeft\n arrived), it would also result in\na \nSTREAM\n that captures both the removal of the old value via a retraction and\nthe addition of the new value:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM LeftV2 \nINNER\n JOIN Right             FROM LeftV2\n \nINNER\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                           -----------------------------\n-\n| L    | R    |                           | L    | R    | Time  | Undo\n \n|\n---------------                           -----------------------------\n-\n| L2   | R2   |                           | L3   | R3   | 12:04 |\n      \n|\n| L3v2 | R3   |                           | L2   | R2   | 12:06 |\n      \n|\n---------------                           | L3   | R3   | 12:07 | undo\n \n| \n                                          | L3v2 | R3   | 12:07 |\n      \n|\n                                          ....... [12:00, 12:10]\n \n.......\nANTI\nANTI\n joins are the obverse of the \nINNER\n join: they \ncontain \nall of \nthe \nunjoined\nrows. Not all SQL systems support a clean \nANTI\n join syntax, but I’ll use the\nmost straightforward one here for clarity:\n                                         \n12:00>\n SELECT",7555
85-SEMI.pdf,85-SEMI,"STREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nANTI\n JOIN Right                FROM Left\n \nANTI\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n-\n| L    | R    |                          | L    |    R | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |                          | null | R2   | 12:01 |      |\n| L2   | R2   |\n                          | L1   | null | 12:02 |      |\n| L3   | R3   |\n                          | L3   | null | 12:03 |      |\n| null | R4   |                          | L3   | null | 12:04 | undo |\n---------------                          \n| L3   | R3   | 12:04 |      |\n                                         | null | R4   | 12:05 |      |\n                                         | null | R2   | 12:06 | undo |\n                                         \n| L2   | R2   | 12:06 |      |\n                                         ....... [12:00, 12:10] .......\nWhat’s slightly interesting about the stream rendering of the \nANTI\n join is that\nit ends up containing a bunch of false-starts and retractions for rows which\neventually do end up joining; in fact, the \nANTI\n join is as heavy on retractions\nas the \nINNER\n join is light. The more concise versions would look like this:\n                                         \n12:00>\n SELECT\n \nSTREAM\n Left.Id as L, \n12:10>\n SELECT \nTABLE\n                               Right.Id\n \nas R,\n         Left.Id as L,\n                            \nSys.EmitTime as Time, \n         Right.Id as R                            Sys.Undo\n \nas Undo \n       FROM Left \nANTI\n JOIN Right               FROM Left\n \nANTI\n JOIN Right\n       ON L.Num = R.Num;                        ON L.Num =\n \nR.Num;\n---------------                          ------------------------------\n| L    | R    |                          | L    | R    | Time  | Undo |\n---------------                          ------------------------------\n| L1   | null |                          | null | R2   | 12:01 |      |\n| null | R4   |                          | L1   | null | 12:02 |      |\n---------------                          | L3   | null | 12:03 |      |\n  \n                                         | L3   | null | 12:04 | undo |\n                                         | null | R4   | 12:05 |      |\n                                         | null | R2   | 12:06 | undo |\n                                         ....... [12:00, 12:10] .......\nSEMI\nWe now come to \nSEMI\n joins, and \nSEMI\n joins are kind of weird.\n At first\nglance, they basically look like inner joins with one side of the joined values\nbeing dropped. And, indeed, in cases for which the cardinality relationship of\n<side-being-kept>:<side-being-dropped>\n is N:M with M ≤ 1, this\nworks (note that we’ll be using kept=\nLeft\n, dropped=\nRight\n for all the\nexamples that follow). For example, on the \nLeft\n and \nRight\n datasets we’ve\nused so far (which had cardinalities of 0:1, 1:0, and 1:1 for the joined data),\nthe \nINNER\n and \nSEMI\n join variations look identical:\n12:10>\n SELECT \nTABLE\n            \n12:10>\n SELECT \nTABLE\n  Left.Id as L                   Left.Id as L\nFROM Left \nINNER\n JOIN           FROM Left \nSEMI\n JOIN\nRight ON L.Num = R.Num;        Right ON L.Num = R.Num;\n--------\n-------\n                --------\n-------\n| L    |\n R    |\n                | L    |\n R    |\n--------\n-------\n                --------\n-------\n| L1   | null |\n                \n| L1   | null |\n| L2   |\n R2   |\n                | L2   |\n R2   |\n| L3   |\n R3   |\n                | L3   |\n R3   |\n| null | R4   |\n                \n| null | R4   |\n--------\n-------\n                --------\n-------\nHowever, there’s an additional subtlety to \nSEMI\n joins in\n the case of N:M\ncardinality with M > 1: because the \nvalues\n on the M side are not being\nreturned, the \nSEMI\n join simply predicates the join condition on there being\nany\n matching row on the right, rather than repeatedly yielding a new result\nfor \nevery\n matching row.\nTo see this clearly, let’s switch to a slightly more complicated pair of input\nrelations that highlight the N:M join cardinality of the rows contained therein.\nIn these relations, the \nN_M\n column states what the cardinality relationship of\nrows is between the left and right sides, and the \nId\n column (as before)\nprovides an identifier that is unique for each row in each of the input\nrelations:\n12:15>\n SELECT \nTABLE\n * FROM LeftNM;    \n12:15>\n SELECT \nTABLE\n *\n \nFROM RightNM;\n---------------------                 ---------------------\n| N_M | Id  |  Time |                 | N_M | Id  |  Time |\n---------------------                 ---------------------\n| 1:0 | L2  | 12:07 |                 | 0:1 | R1  | 12:02 |\n| 1:1 | L3  | 12:01 |                 | 1:1 | R3  | 12:14 |\n| 1:2 | L4  | 12:05 |                 | 1:2 | R4A | 12:03 |\n| 2:1 | L5A | 12:09 |                 | 1:2 | R4B | 12:04 |\n| 2:1 | L5B | 12:08 |                 | 2:1 | R5  | 12:06 |\n| 2:2 | L6A | 12:12 |                 | 2:2 | R6A | 12:11 |\n| 2:2 | L6B | 12:10 |                 | 2:2 | R6B | 12:13 |\n---------------------                 ---------------------\nWith these inputs, the \nFULL OUTER\n join expands to look like these:\n                                       \n12:00>\n SELECT \nSTREAM\n                                                \nCOALESCE(LeftNM.N_M, \n12:15>\n SELECT \nTABLE\n                                      \nRightNM.N_M) as N_M, \n         COALESCE(LeftNM.N_M,                     LeftNM.Id\n \nas L,\n                  RightNM.N_M) as N_M,\n            \nRightNM.Id as R, \n         LeftNM.Id as L,\n                        \nSys.EmitTime as Time, \n         RightNM.Id as R,                         Sys.Undo\n \nas Undo\n       FROM LeftNM                            FROM LeftNM \n         \nFULL OUTER\n JOIN RightNM                \nFULL OUTER\n \nJOIN RightNM\n         ON LeftNM.N_M = RightNM.N_M;           ON\n \nLeftNM.N_M = RightNM.N_M;\n---------------------                  --------------------------------\n----\n| N_M | L    | R    |                  | N_M | L    | R    | Time  |\n \nUndo |\n---------------------                  --------------------------------\n----\n| 0:1 | null | R1   |                  | 1:1 | L3   | null | 12:01 |\n      \n|\n| 1:0 | L2   | null |                  | 0:1 | null | R1   | 12:02 |\n      \n|\n| 1:1 | L3   | R3   |                  | 1:2 | null | R4A  | 12:03 |\n      \n|\n| 1:2 | L4   | R4A  |                  | 1:2 | null | R4B  | 12:04 |\n      \n|\n| 1:2 | L4   | R4B  |                  | 1:2 | null | R4A  | 12:05 |\n \nundo |\n| 2:1 | L5A  | R5   |                  | 1:2 | null | R4B  | 12:05 |\n \nundo |\n| 2:1 | L5B  | R5   |                  | 1:2 | L4   | R4A  | 12:05 |\n      \n|\n| 2:2 | L6A  | R6A  |                  | 1:2 | L4   | R4B  | 12:05 |\n      \n|\n| 2:2 | L6A  | R6B  |                  | 2:1 | null | R5   | 12:06 |\n      \n|\n| 2:2 | L6B  | R6A  |                  | 1:0 | L2   | null | 12:07 |\n      \n|\n| 2:2 | L6B  | R6B  |                  | 2:1 | null | R5   | 12:08 |\n \nundo |\n---------------------                  | 2:1 | L5B  | R5   | 12:08 |\n      \n|\n                                       | 2:1 | L5A  | R5   | 12:09 |\n      \n|\n                                       | 2:2 | L6B  | null | 12:10 |\n      \n|\n                                       | 2:2 | L6B  | null | 12:11 |\n \nundo |\n                                       | 2:2 | L6B  | R6A  | 12:11 |\n      \n|\n                                       | 2:2 | L6A  | R6A  | 12:12 |\n      \n|\n                                       | 2:2 | L6A  | R6B  | 12:13 |\n      \n|\n                                       | 2:2 | L6B  | R6B  | 12:13 |\n      \n|\n                                       | 1:1 | L3   | null | 12:14 |\n \nundo |\n                                       | 1:1 | L3   | R3   | 12:14 |\n      \n|\n                                       .......... [12:00, 12:15]\n \n..........\nAs a side note, one additional benefit of these more complicated datasets is\nthat the multiplicative nature of joins when there are multiple rows on each\nside matching the same predicate begins to become more clear (e.g., the “2:2”\nrows, which expand from two rows in each the inputs to four rows in the\noutput; if the dataset had a set of “3:3” rows, they’d expand from three rows\nin each of the inputs to nine rows in the output, and so on).\nBut back to the subtleties of \nSEMI\n joins. With these datasets, it becomes much\nclearer what the difference between the filtered \nINNER\n join and the \nSEMI\n join\nis: the \nINNER\n join yields duplicate values for any of the rows where the N:M\ncardinality has M > 1, whereas the \nSEMI\n join doesn’t (note that I’ve\nhighlighted the duplicate rows in the \nINNER\n join version in red, and included\nin gray the portions of the full outer join that are omitted in the respective\nINNER\n and \nSEMI\n versions):\n12:15>\n SELECT \nTABLE\n                       \n12:15>\n SELECT\n \nTABLE\n         COALESCE(LeftNM.N_M,\n                      \nCOALESCE(LeftNM.N_M,\n                  RightNM.N_M) as N_M,\n                      \nRightNM.N_M) as N_M,\n         LeftNM.Id as L\n                            \nLeftNM.Id as L\n       FROM LeftNM \nINNER\n JOIN RightNM            FROM\n \nLeftNM \nSEMI\n JOIN RightNM\n       ON LeftNM.N_M = RightNM.N_M;              ON\n \nLeftNM.N_M = RightNM.N_M;\n--------------\n-------\n                     --------------\n-------\n| N_M | L    |\n R    |\n                     | N_M | L    |\n R    |\n--------------\n-------\n                     --------------\n-------\n| 0:1 | null | R1   |\n                     \n| 0:1 | null | R1   |\n| 1:0 | L2   | null |\n                     \n| 1:0 | L2   | null |\n| 1:1 | L3   |\n R3   |\n                     | 1:1 | L3   |\n R3   |\n| 1:2 | L4   |\n R5A  |\n                     | 1:2 | L4   |\n R5A  |\n|\n 1:2 \n|\n L4   \n|\n R5B  |\n                     \n| 1:2 | L4   | R5B  |\n| 2:1 | L5A  |\n R5   |\n                     | 2:1 | L5A  |\n R5   |\n| 2:1 | L5B  |\n R5   |\n                     | 2:1 | L5B  |\n R5   |\n| 2:2 | L6A  |\n R6A  |\n                     | 2:2 | L6A  |\n R6A  |\n|\n 2:2 \n|\n L6A  \n|\n R6B  |\n                     \n| 2:2 | L6A  | R6B  |\n| 2:2 | L6B  |\n R6A  |\n                     | 2:2 | L6B  |\n R6A  |\n|\n 2:2 \n|\n L6B  \n|\n R6B  |\n                     \n| 2:2 | L6B  | R6B  |\n--------------\n-------\n                     --------------\n-------\nOr, rendered more succinctly:\n12:15>\n SELECT \nTABLE\n                       \n12:15>\n SELECT\n \nTABLE\n         COALESCE(LeftNM.N_M,\n                      \nCOALESCE(LeftNM.N_M,\n                  RightNM.N_M) as N_M,\n                      \nRightNM.N_M) as N_M,\n         LeftNM.Id as L\n                            \nLeftNM.Id as L\n       FROM LeftNM \nINNER\n JOIN RightNM            FROM\n \nLeftNM \nSEMI\n JOIN RightNM\n       ON LeftNM.N_M = RightNM.N_M;              ON\n \nLeftNM.N_M = RightNM.N_M;\n-------------                             -------------\n| N_M | L   |                             | N_M | L   |\n-------------                             -------------\n| 1:1 | L3  |                             | 1:1 | L3  |\n| 1:2 | L4  |                             | 1:2 | L4  |\n|\n 1:2 \n|\n L4  \n|                             | 2:1 | L5A |\n| 2:1 | L5A |                             | 2:1 | L5B |\n| 2:1 | L5B |                             | 2:2 | L6A |\n| 2:2 | L6A |                             | 2:2 | L6B |\n|\n 2:2 \n|\n L6A \n|                             -------------\n| 2:2 | L6B |\n|\n 2:2 \n|\n L6B \n|\n-------------\nThe \nSTREAM\n renderings then provide a bit of context as to which rows are\nfiltered out—they are simply the later-arriving duplicate rows (from the\nperspective of the columns being projected):\n12:00>\n SELECT \nSTREAM\n                        \n12:00>\n SELECT\n \nSTREAM\n         COALESCE(LeftNM.N_M,\n                        \nCOALESCE(LeftNM.N_M,\n                  RightNM.N_M) as N_M,\n                        \nRightNM.N_M) as N_M,\n         LeftNM.Id as L\n                              \nLeftNM.Id as L\n         Sys.EmitTime as Time,\n                       \nSys.EmitTime as Time,\n         Sys.Undo as Undo,\n                           \nSys.Undo as Undo,\n       FROM LeftNM \nINNER\n JOIN RightNM              FROM\n \nLeftNM \nSEMI\n JOIN RightNM\n       ON LeftNM.N_M = RightNM.N_M;                ON\n \nLeftNM.N_M = RightNM.N_M;\n--------------\n-------\n---------------        --------------\n-------\n------\n---------\n| N_M | L    |\n R    |\n Time  | Undo |        | N_M | L    |\n R    |\n Time\n  \n| Undo |\n--------------\n-------\n---------------        --------------\n-------\n------\n---------\n| 1:1 | L3   | null | 12:01 |      |\n        \n| 1:1 | L3   | null | 12:01\n \n|      |\n| 0:1 | null | R1   | 12:02 |      |\n        \n| 0:1 | null | R1   | 12:02\n \n|      |\n| 1:2 | null | R4A  | 12:03 |      |\n        \n| 1:2 | null | R4A  | 12:03\n \n|      |\n| 1:2 | null | R4B  | 12:04 |      |\n        \n| 1:2 | null | R4B  | 12:04\n \n|      |\n| 1:2 | null | R4A  | 12:05 | undo |\n        \n| 1:2 | null | R4A  | 12:05\n \n| undo |\n| 1:2 | null | R4B  | 12:05 | undo |\n        \n| 1:2 | null | R4B  | 12:05\n \n| undo |\n| 1:2 | L4   |\n R4A  |\n 12:05 |      |        | 1:2 | L4   |\n R4A  |\n 12:05\n \n|      |\n|\n 1:2 \n|\n L4   \n|\n R4B  |\n 12:05 \n|\n      \n|        \n| 1:2 | L4   | R4B  | 12:05\n \n|      |\n| 2:1 | null | R5   | 12:06 |      |\n        \n| 2:1 | null | R5   | 12:06\n \n|      |\n| 1:0 | L2   | null | 12:07 |      |\n        \n| 1:0 | L2   | null | 12:07\n \n|      |\n| 2:1 | null | R5   | 12:08 | undo |\n        \n| 2:1 | null | R5   | 12:08\n \n| undo |\n| 2:1 | L5B  |\n R5   |\n 12:08 |      |        | 2:1 | L5B  |\n R5   |\n 12:08\n \n|      |\n| 2:1 | L5A  |\n R5   |\n 12:09 |      |        | 2:1 | L5A  |\n R5   |\n 12:09\n \n|      |\n| 2:2 | L6B  | null | 12:10 |      |\n        \n| 2:2 | L6B  | null | 12:10\n \n|      |\n| 2:2 | L6B  | null | 12:10 | undo |\n        \n| 2:2 | L6B  | null | 12:10\n \n| undo |\n| 2:2 | L6B  |\n R6A  |\n 12:11 |      |        | 2:2 | L6B  |\n R6A  |\n 12:11\n \n|      |\n| 2:2 | L6A  |\n R6A  |\n 12:12 |      |        | 2:2 | L6A  |\n R6A  |\n 12:12\n \n|      |\n|\n 2:2 \n|\n L6A  \n|\n R6B  |\n 12:13 \n|\n      \n|        \n| 2:2 | L6A  | R6B  | 12:13\n \n|      |\n|\n 2:2 \n|\n L6B  \n|\n R6B  |\n 12:13 \n|\n      \n|        \n| 2:2 | L6B  | R6B  | 12:13\n \n|      |\n| 1:1 | L3   | null | 12:14 | undo |\n        \n| 1:1 | L3   | null | 12:14\n \n| undo |\n| 1:1 | L3   |\n R3   |\n 12:14 |      |        | 1:1 | L3   |\n R3   |\n 12:14\n \n|      |\n.......... [12:00, 12:15] ..........        .......... [12:00, 12:15]\n \n..........\nAnd again, rendered succinctly:\n12:00>\n SELECT \nSTREAM\n                        \n12:00>\n SELECT\n \nSTREAM\n         COALESCE(LeftNM.N_M,\n                        \nCOALESCE(LeftNM.N_M,\n                  RightNM.N_M) as N_M,\n                        \nRightNM.N_M) as N_M,\n         LeftNM.Id as L\n                              \nLeftNM.Id as L\n         Sys.EmitTime as Time,\n                       \nSys.EmitTime as Time,\n         Sys.Undo as Undo,\n                           \nSys.Undo as Undo,\n       FROM LeftNM \nINNER\n JOIN RightNM              FROM\n \nLeftNM \nSEMI\n JOIN RightNM\n       ON LeftNM.N_M = RightNM.N_M;                ON\n \nLeftNM.N_M = RightNM.N_M;\n----------------------------                ---------------------------\n-\n| N_M | L   | Time  | Undo |                | N_M | L   | Time  | Undo\n \n|\n----------------------------                ---------------------------\n-\n| 1:2 | L4  | 12:05 |      |                | 1:2 | L4  | 12:05 |\n      \n|\n|\n 1:2 \n|\n L4  \n|\n 12:05 \n|\n      \n|                | 2:1 | L5B | 12:08 |\n      \n|\n| 2:1 | L5B | 12:08 |      |                | 2:1 | L5A | 12:09 |\n      \n|\n| 2:1 | L5A | 12:09 |      |                | 2:2 | L6B | 12:11 |\n      \n|\n| 2:2 | L6B | 12:11 |      |                | 2:2 | L6A | 12:12 |\n      \n|\n| 2:2 | L6A | 12:12 |      |                | 1:1 | L3  | 12:14 |\n      \n|\n|\n 2:2 \n|\n L6A \n|\n 12:13 \n|\n      \n|                ...... [12:00, 12:15]\n \n......\n|\n 2:2 \n|\n L6B \n|\n 12:13 \n|\n      \n|\n| 1:1 | L3  | 12:14 |      |\n...... [12:00, 12:15] ......",16932
86-Fixed Windows.pdf,86-Fixed Windows,"As we’ve seen over the course of a number of examples, there’s really\nnothing special about streaming joins. They function exactly as we might\nexpect given our knowledge of streams and tables, with join streams\ncapturing the history of the join over time as it evolves. This is in contrast to\njoin tables, which simply capture a snapshot of the entire join as it exists at a\nspecific point in time, as we’re perhaps more accustomed.\nBut, even more important, viewing joins through the lens of stream-table\ntheory has lent some additional clarity. The core underlying join primitive\n is\nthe \nFULL OUTER\n join, which is a stream → table grouping operation that\ncollects together all the joined and unjoined rows in a relation. All of the\nother variants we looked at in detail (\nLEFT OUTER\n, \nRIGHT OUTER\n, \nINNER\n,\nANTI\n, and \nSEMI\n) simply add an additional layer of filtering on\n the \njoined\nstream \nfollowing \nthe \nFULL OUTER\n join.\nWindowed Joins\nHaving looked at a variety of unwindowed joins, let’s next explore what\nwindowing adds to the mix.\n I would argue that there are two motivations for\nwindowing your joins:\nTo partition time in some meaningful way\nAn obvious case is fixed windows; for example,\n daily windows, for\nwhich events that occurred in the same day should be joined together for\nsome business reason (e.g., daily billing tallies). Another might be\nlimiting the range of time within a join for performance reasons.\nHowever, it turns out there are even more sophisticated (and useful) ways\nof partitioning time in joins, including one particularly interesting use\ncase that no streaming system I’m aware of today supports natively:\ntemporal validity joins\n. More on this in just a bit.\nTo provide a meaningful reference point for timing out a join\nThis is useful for a number of unbounded join situations,\n but it is perhaps\nmost obviously beneficial for use cases like outer joins, for which it is\n3\nunknown a priori if one side of the join will ever show up. For classic\nbatch processing (including standard interactive SQL queries), outer joins\nare timed out only when the bounded input dataset has been fully\nprocessed. But when processing unbounded data, we can’t wait for all\ndata to be processed. As we discussed in Chapters \n2\n and \n3\n, watermarks\nprovide a progress metric for gauging the completeness of an input source\nin event time. But to make use of that metric for timing out a join, we\nneed some reference point to compare against. Windowing a join\nprovides that reference by bounding the extent of the join to the end of\nthe window. After the watermark passes the end of the window, the\nsystem may consider the input for the window complete. At that point,\njust as in the bounded join case, it’s safe to time out any unjoined rows\nand materialize their partial results.\nThat said, as we saw earlier, windowing is absolutely not a requirement for\nstreaming joins. It makes a lot of sense in a many cases, but by no means is it\na necessity.\nIn practice, most of the use cases for windowed joins (e.g., daily windows)\nare relatively straightforward and easy to extrapolate from the concepts we’ve\nlearned up until now. To see why, we look briefly at what it means to apply\nfixed windows to some of the join examples we already encountered. After\nthat, we spend the rest of this chapter investigating the much more interesting\n(and mind-bending) topic of \ntemporal validity joins\n, looking first in detail at\nwhat I mean by temporal validity windows, and then moving on to looking at\nwhat joins mean within the context of such windows.\nFixed Windows\nWindowing a join adds the \ndimension of \ntime into the join criteria\nthemselves. In doing so,\n the window serves to scope the set of rows being\njoined to only those contained within the window’s time interval. This is\nperhaps more clearly seen with an example, so let’s take our original \nLeft\nand \nRight\n tables and window them into five-minute fixed windows:\n12:10>\n SELECT \nTABLE\n *,                     \n12:10>\n SELECT\n \nTABLE\n *,\n       TUMBLE(Time, INTERVAL '5' MINUTE)\n          \nTUMBLE(Time, INTERVAL '5' MINUTE)\n       as Window FROM Left;                       as Window\n \nFROM Right\n-------------------------------------      ----------------------------\n---------\n| Num | Id | Time  | Window         |      | Num | Id | Time  | Window\n         \n|\n-------------------------------------      ----------------------------\n---------\n| 1   | L1 | 12:02 | [12:00, 12:05) |      | 2   | R2 | 12:01 |\n [12:00,\n \n12:05) \n|\n| 2   | L2 | 12:06 |\n [12:05, 12:10) \n|      | 3   | R3 | 12:04 | [12:00,\n \n12:05) |\n| 3   | L3 | 12:03 | [12:00, 12:05) |      | 4   | R4 | 12:05 | [12:05,\n \n12:06) |\n-------------------------------------      ----------------------------\n---------\nIn our previous \nLeft\n and \nRight\n examples, the join criterion was simply\nLeft.Num = Right.Num\n. To turn this into a windowed join, we would\nexpand the join criteria to include window equality, as well: \nLeft.Num =\nRight.Num AND Left.Window = Right.Window\n. Knowing that, we can\nalready infer from the preceding windowed tables how our join is going to\nchange (highlighted for clarity): because the \nL2\n and \nR2\n rows do not fall\nwithin the same five-minute fixed window, they will not be joined together in\nthe windowed variant of our join.\nAnd indeed, if we compare the unwindowed and windowed variants side-by-\nside as tables, we can see this clearly (with the corresponding \nL2\n and \nR2\n rows\nhighlighted on each side of the join):\n                                 \n12:10>\n SELECT \nTABLE\n \n                                          Left.Id as L,\n                                          Right.Id as R,\n                                          COALESCE(\n                                            \nTUMBLE(Left.Time, INTERVAL '5' MINUTE),\n                                            \nTUMBLE(Right.Time, INTERVAL '5' MINUTE)\n12:10>\n SELECT \nTABLE\n                       ) AS Window\n         Left.Id as L,                  FROM Left\n         Right.Id as R,                   \nFULL OUTER\n JOIN\n \nRight \n       FROM Left                          ON L.Num = R.Num\n \nAND \n         \nFULL OUTER\n JOIN Right\n              \nTUMBLE(Left.Time, INTERVAL '5' MINUTE) =\n         ON L.Num = R.Num;\n                  \nTUMBLE(Right.Time, INTERVAL '5' MINUTE);\n---------------                  --------------------------------\n| L    | R    |                  | L    | R    | Window         |\n---------------                  --------------------------------\n| L1   | null |                  | L1   | null | [12:00, 12:05) |\n|\n L2   | R2   \n|                  |\n null | R2   | [12:00, 12:05) \n|\n| L3   | R3   |                  | L3   | R3   | [12:00, 12:05) |\n| null | R4   |                  |\n L2   | null | [12:05, 12:10) \n|\n---------------                  | null | R4   | [12:05, 12:10) |\n                                 --------------------------------\nThe difference is also readily apparent when comparing the unwindowed and\nwindowed joins as streams. As I’ve highlighted in the example that follows,\nthey differ primarily in their final rows. The unwindowed side completes the\njoin for \nNum = 2\n, yielding a retraction for the unjoined \nR2\n row in addition to\na new row for the completed \nL2, R2\n join. The windowed side, on the other\nhand, simply yields an unjoined \nL2\n row because \nL2\n and \nR2\n fall within\ndifferent five-minute windows:\n                                 \n12:10>\n SELECT \nSTREAM\n \n                                          Left.Id as L,\n                                          Right.Id as R,\n                                          Sys.EmitTime as\n \nTime,\n                                          COALESCE(\n                                            \nTUMBLE(Left.Time, INTERVAL '5' MINUTE),\n12:10>\n SELECT \nSTREAM\n                        \nTUMBLE(Right.Time, INTERVAL '5' MINUTE)\n         Left.Id as L,                    ) AS Window,\n         Right.Id as R,                 Sys.Undo as Undo\n         Sys.EmitTime as Time,          FROM Left\n         Sys.Undo as Undo                 \nFULL OUTER\n JOIN\n \nRight\n       FROM Left                          ON L.Num = R.Num\n \nAND\n         \nFULL OUTER\n JOIN Right\n              \nTUMBLE(Left.Time, INTERVAL '5' MINUTE) =\n         ON L.Num = R.Num;\n                  \nTUMBLE(Right.Time, INTERVAL '5' MINUTE);\n------------------------------   --------------------------------------\n---------\n| L    | R    | Time  | Undo |   | L    | R    | Time  | Window\n         \n| Undo |\n------------------------------   --------------------------------------\n---------\n| null | R2   | 12:01 |      |   | null | R2   | 12:01 | [12:00, 12:05)\n \n|      |\n| L1   | null | 12:02 |      |   | L1   | null | 12:02 | [12:00, 12:05)\n \n|      |\n| L3   | null | 12:03 |      |   | L3   | null | 12:03 | [12:00, 12:05)\n \n|      |\n| L3   | null | 12:04 | undo |   | L3   | null | 12:04 | [12:00, 12:05)\n \n| undo |\n| L3   | R3   | 12:04 |      |   | L3   | R3   | 12:04 | [12:00, 12:05)\n \n|      |\n| null | R4   | 12:05 |      |   | null | R4   | 12:05 | [12:05, 12:10)\n \n|      |\n|\n null | R2   | 12:06 | undo \n|   |\n L2   | null | 12:06 | [12:05, 12:10)\n \n|      \n|\n|\n L2   | R2   | 12:06 |      \n|   ............... [12:00, 12:10]\n \n................\n....... [12:00, 12:10] .......\nAnd with that, we now understand the effects of windowing on a \nFULL OUTER\njoin. By applying the rules we learned in the first half of the chapter, it’s then\neasy to derive the windowed variants of \nLEFT OUTER\n, \nRIGHT OUTER\n, \nINNER\n,\nANTI\n, and \nSEMI\n joins, as well. I will leave most of these derivations as an\nexercise for you to complete, but to give a single example, \nLEFT OUTER\n join,\nas we learned, is just the \nFULL OUTER\n join with null columns on the left side\nof the join removed (again, with \nL2\n and \nR2\n rows highlighted to compare the\ndifferences):\n                                 \n12:10>\n SELECT \nTABLE\n \n                                          Left.Id as L,\n                                          Right.Id as R,\n                                          COALESCE(\n                                            \nTUMBLE(Left.Time, INTERVAL '5' MINUTE),\n                                            \nTUMBLE(Right.Time, INTERVAL '5' MINUTE)\n12:10>\n SELECT \nTABLE\n                       ) AS Window\n         Left.Id as L,                  FROM Left\n         Right.Id as R,                   \nLEFT OUTER\n JOIN\n \nRight \n       FROM Left                          ON L.Num = R.Num\n \nAND \n         \nLEFT OUTER\n JOIN Right\n              \nTUMBLE(Left.Time, INTERVAL '5' MINUTE) =\n         ON L.Num = R.Num;\n                  \nTUMBLE(Right.Time, INTERVAL '5' MINUTE);\n---------------                  --------------------------------\n| L    | R    |                  | L    | R    | Window         |\n---------------                  --------------------------------\n| L1   | null |                  | L1   | null | [12:00, 12:05) |\n|\n L2   | R2   \n|                  |\n L2   | null | [12:05, 12:10) \n|\n| L3   | R3   |                  | L3   | R3   | [12:00, 12:05) |\n---------------                  --------------------------------\nBy scoping the region of time for the join into fixed five-minute intervals, we\nchopped our datasets into two distinct windows of time: \n[12:00, 12:05)\nand \n[12:05, 12:10)\n. The exact same join logic we observed earlier was\nthen applied within those regions, yielding a slightly different outcome for",11799
87-Temporal Validity.pdf,87-Temporal Validity,"the case in which the \nL2\n and \nR2\n rows fell into separate regions. And at a basic\nlevel, that’s really all there is to windowed joins.\nTemporal Validity\nHaving looked at the basics of windowed joins, \nwe now spend \nthe rest of the\nchapter looking at a somewhat more \nadvanced approach: temporal validity\nwindowing.\nTemporal validity windows\nTemporal validity windows apply in situations in which the rows in a relation\neffectively slice time into regions wherein a given \nvalue is valid. More\nconcretely, imagine a financial system for performing currency conversions.\nSuch a system might contain a time-varying relation that captured the current\nconversion rates for various types of currency. For example, there might be a\nrelation for converting from different currencies to Yen, like this:\n12:10>\n SELECT \nTABLE\n * FROM YenRates;\n--------------------------------------\n| Curr | Rate | EventTime | ProcTime |\n--------------------------------------\n| USD  | 102  | 12:00:00  | 12:04:13 |\n| Euro | 114  | 12:00:30  | 12:06:23 |\n| Yen  | 1    | 12:01:00  | 12:05:18 |\n| Euro | 116  | 12:03:00  | 12:09:07 |\n| Euro | 119  | 12:06:00  | 12:07:33 |\n--------------------------------------\nTo highlight what I mean by saying that temporal validity windows\n“effectively slice time into regions wherein a given value is valid,” consider\nonly the Euro-to-Yen conversion rates in that relation:\n12:10>\n SELECT \nTABLE\n * FROM YenRates WHERE Curr = ""Euro"";\n--------------------------------------\n| Curr | Rate | EventTime | ProcTime |\n--------------------------------------\n| Euro | 114  | 12:00:30  | 12:06:23 |\n4\n| Euro | 116  | 12:03:00  | 12:09:07 |\n| Euro | 119  | 12:06:00  | 12:07:33 |\n--------------------------------------\nFrom a database engineering perspective, we understand that these values\ndon’t mean that the rate for converting Euros to Yen is 114 ¥/€ at precisely\n12:00, 116 ¥/€ at 12:03, 119 ¥/€ at 12:06, and undefined at all other times.\nInstead, we know that the intent of this table is to capture the fact that the\nconversion rate for Euros to Yen is undefined until 12:00, 114 ¥/€ from 12:00\nto 12:03, 116 ¥/€ from 12:03 to 12:06, and 119 ¥/€ from then on. Or drawn\nout in a timeline:\n        Undefined              114 ¥/€                116 ¥/€\n              \n119 ¥/€\n|----[-inf, 12:00)----|----[12:00, 12:03)----|----[12:03, 12:06)----|--\n--[12:06, now)----→\nNow, if we knew all of the rates ahead of time, we could capture these\nregions explicitly in the row data themselves. But if we instead need to build\nup these regions incrementally, based only upon the start times at which a\ngiven rate becomes valid, we have a problem: the region for a given row will\nchange over time depending on the rows that come after it. \nThis is a problem\neven if the data arrive in order (because every time a new rate arrives, the\nprevious rate changes from being valid forever to being valid until the arrival\ntime of the new rate), but is further compounded if they can arrive \nout of\norder\n. For example, using the processing-time ordering in the preceding\nYenRates\n table, the sequence of timelines our table would effectively\nrepresent over time would be as follows:\nRange of processing time | Event-time validity timeline during that\n \nrange of processing-time\n=========================|=============================================\n=================================\n                         |\n                         |      Undefined\n        [-inf, 12:06:23) | |--[-inf, +inf)-----------------------------\n----------------------------→\n                         |\n                         |      Undefined          114 ¥/€\n    [12:06:23, 12:07:33) | |--[-inf, 12:00)--|--[12:00, +inf)----------\n----------------------------→\n                         |\n                         |      Undefined          114 ¥/€\n                              \n119 ¥/€\n    [12:07:33, 12:09:07) | |--[-inf, 12:00)--|--[12:00, 12:06)---------\n------------|--[12:06, +inf)→\n                         |\n                         |      Undefined          114 ¥/€\n            \n116 ¥/€           119 ¥/€\n         [12:09:07, now) | |--[-inf, 12:00)--|--[12:00, 12:03)--|--\n[12:03, 12:06)--|--[12:06, +inf)→\nOr, if we wanted to render this as a time-varying relation (with changes\nbetween each snapshot\n relation highlighted in yellow):\n12:10>\n SELECT \nTVR\n * FROM YenRatesWithRegion ORDER BY\n \nEventTime;\n-----------------------------------------------------------------------\n----------------------\n|              [-inf, 12:06:23)               |            [12:06:23,\n \n12:07:33)             |\n| ------------------------------------------- | -----------------------\n-------------------- |\n| | Curr | Rate |  Region        | ProcTime | | | Curr | Rate |  Region\n        \n| ProcTime | |\n| ------------------------------------------- | -----------------------\n-------------------- |\n| ------------------------------------------- | |\n Euro \n|\n 114  \n|\n [12:00,\n \n+inf)  \n|\n 12:06:23 \n| |\n|                                             | -----------------------\n-------------------- |\n-----------------------------------------------------------------------\n----------------------\n|            [12:07:33, 12:09:07)             |              [12:09:07,\n \n+inf)               |\n| ------------------------------------------- | -----------------------\n-------------------- |\n| | Curr | Rate |  Region        | ProcTime | | | Curr | Rate |  Region\n        \n| ProcTime | |\n| ------------------------------------------- | -----------------------\n-------------------- |\n| | Euro | 114  | [12:00, \n12:06\n) | 12:06:23 | | | Euro | 114  | [12:00,\n \n12:03\n) | 12:06:23 | |\n| |\n Euro \n|\n 119  \n|\n [12:06, +inf)  \n|\n 12:07:33 \n| | |\n Euro \n|\n 116  \n|\n [12:03,\n \n12:06) \n|\n 12:09:07 \n| |\n| ------------------------------------------- | | Euro | 119  | [12:06,\n \n+inf)  | 12:07:33 | |\n|                                             | -----------------------\n-------------------- |\n-----------------------------------------------------------------------\n----------------------\nWhat’s important to note here is that half of the changes involve updates to\nmultiple rows. That maybe doesn’t sound so bad, until you recall that the\ndifference between each of these snapshots is the arrival of exactly one new\nrow. In other words, the arrival of a single new input row results in\ntransactional modifications to multiple output rows. That sounds less good.\nOn the other hand, it also sounds a lot like the multirow transactions involved\nin building up session windows. And indeed, this is yet another example of\nwindowing providing benefits beyond simple partitioning of time: it also\naffords the ability to do so in ways that involve complex, multirow\ntransactions.\nTo see this in action, let’s look at an animation. If this were a Beam pipeline,\nit would probably look something like the following:\nPCollection<Currency, Decimal> yenRates = ...;\nPCollection<Decimal> validYenRates = yenRates\n    .apply(\nWindow.into(new ValidityWindows()\n)\n    .apply(\nGroupByKey.<Currency, Decimal>create()\n);\nRendered in a streams/tables animation, that pipeline would look like that\nshown in \nFigure 9-1\n.\nFigure 9-1. \nTemporal validity windowing over time\nThis animation highlights a critical aspect of temporal validity: shrinking\nwindows. Validity windows must be able to shrink over time, thereby\ndiminishing the reach of their validity and splitting any data contained therein\nacross the two new windows. See the \ncode snippets on GitHub\n for an\nexample partial implementation.\nIn SQL terms, the creation of these validity windows would look something\nlike the following (making using of a hypothetical \nVALIDITY_WINDOW\nconstruct), viewed as a table:\n12:10>\n SELECT \nTABLE\n \n         Curr,\n         MAX(Rate) as Rate,\n         VALIDITY_WINDOW(EventTime) as Window\n       FROM YenRates \n       GROUP BY\n         Curr,\n00:00 / 00:00\n5\n         VALIDITY_WINDOW(EventTime)\n       HAVING Curr = ""Euro"";\n--------------------------------\n| Curr | Rate | Window         |\n--------------------------------\n| Euro | 114  | [12:00, 12:03) |\n| Euro | 116  | [12:03, 12:06) |\n| Euro | 119  | [12:06, +inf)  |\n--------------------------------\nVALIDITY WINDOWS IN STANDARD SQL\nNote that it’s possible to describe validity windows in standard SQL\nusing a three-way self-join:\nSELECT\n  r1.Curr,\n  MAX(r1.Rate) AS Rate,\n  r1.EventTime AS WindowStart,\n  r2.EventTime AS WIndowEnd\nFROM YenRates r1\nLEFT JOIN YenRates r2\n  ON r1.Curr = r2.Curr\n     AND r1.EventTime < r2.EventTime\nLEFT JOIN YenRates r3\n  ON r1.Curr = r3.Curr\n     AND r1.EventTime < r3.EventTime \n     AND r3.EventTime < r2.EventTime\nWHERE r3.EventTime IS NULL\nGROUP BY r1.Curr, WindowStart, WindowEnd\nHAVING r1.Curr = 'Euro';\nThanks to Martin Kleppmann for pointing this out.\nOr, perhaps more interestingly, viewed as a stream:\n12:00>\n SELECT \nSTREAM\n         Curr,\n         MAX(Rate) as Rate,\n         VALIDITY_WINDOW(EventTime) as Window,\n         Sys.EmitTime as Time,\n         Sys.Undo as Undo,\n       FROM YenRates\n       GROUP BY\n         Curr,\n         VALIDITY_WINDOW(EventTime) \n       HAVING Curr = ""Euro"";\n--------------------------------------------------\n| Curr | Rate | Window         | Time     | Undo |\n--------------------------------------------------\n| Euro | 114  | [12:00, +inf)  | 12:06:23 |      |\n| Euro | 114  | [12:00, +inf)  | 12:07:33 | undo |\n| Euro | 114  | [12:00, 12:06) | 12:07:33 |      | \n| Euro | 119  | [12:06, +inf)  | 12:07:33 |      |\n| Euro | 114  | [12:00, 12:06) | 12:09:07 | undo | \n| Euro | 114  | [12:00, 12:03) | 12:09:07 |      |\n| Euro | 116  | [12:03, 12:06) | 12:09:07 |      |\n................. [12:00, 12:10] .................\nGreat, we have an understanding of how to use point-in-time values to\neffectively slice up time into ranges within which those values are valid. But\nthe real power of these temporal validity windows is when they are applied in\nthe context of joining them with other data. That’s where temporal validity\njoins come in.\nTemporal validity joins\nTo explore the semantics of\n temporal validity joins, suppose that our\nfinancial application contains another time-varying relation, one that tracks\ncurrency-conversion orders from various currencies to Yen:\n12:10>\n SELECT \nTABLE\n * FROM YenOrders;\n----------------------------------------\n| Curr | Amount | EventTime | ProcTime |\n----------------------------------------\n| Euro | 2      | 12:02:00  | 12:05:07 |\n| USD  | 1      | 12:03:00  | 12:03:44 |\n| Euro | 5      | 12:05:00  | 12:08:00 |\n| Yen  | 50     | 12:07:00  | 12:10:11 |\n| Euro | 3      | 12:08:00  | 12:09:33 |\n| USD  | 5      | 12:10:00  | 12:10:59 |\n----------------------------------------\nAnd for simplicity, as before, let’s focus on the Euro conversions:\n12:10>\n SELECT \nTABLE\n * FROM YenOrders WHERE Curr = ""Euro"";\n----------------------------------------\n| Curr | Amount | EventTime | ProcTime |\n----------------------------------------\n| Euro | 2      | 12:02:00  | 12:05:07 |\n| Euro | 5      | 12:05:00  | 12:08:00 |\n| Euro | 3      | 12:08:00  | 12:09:33 |\n----------------------------------------\nWe’d like to robustly join these orders to the \nYenRates\n relation, treating the\nrows in \nYenRates\n as defining validity windows. As such, we’ll actually want\nto join to the validity-windowed version of the \nYenRates\n relation we\nconstructed at the end of the last section:\n12:10>\n SELECT \nTABLE\n         Curr,\n         MAX(Rate) as Rate,\n         VALIDITY_WINDOW(EventTime) as Window\n       FROM YenRates\n       GROUP BY\n         Curr,\n         VALIDITY_WINDOW(EventTime)\n       HAVING Curr = ""Euro"";\n--------------------------------\n| Curr | Rate | Window         |\n--------------------------------\n| Euro | 114  | [12:00, 12:03) |\n| Euro | 116  | [12:03, 12:06) |\n| Euro | 119  | [12:06, +inf)  |\n--------------------------------\nFortunately, after we have our conversion rates placed into validity windows,\na windowed join between those rates and the \nYenOrders\n relation gives us\nexactly what we want:\n12:10>\n WITH ValidRates AS\n         (SELECT\n            Curr,\n            MAX(Rate) as Rate,\n            VALIDITY_WINDOW(EventTime) as Window\n          FROM YenRates\n          GROUP BY\n            Curr,\n            VALIDITY_WINDOW(EventTime))\n       SELECT \nTABLE\n         YenOrders.Amount as ""E"",\n         ValidRates.Rate as ""Y/E"", \n         YenOrders.Amount * ValidRates.Rate as ""Y"",\n         YenOrders.EventTime as Order, \n         ValidRates.Window as ""Rate Window""\n       FROM YenOrders FULL OUTER JOIN ValidRates \n         ON YenOrders.Curr = ValidRates.Curr\n           AND WINDOW_START(ValidRates.Window) <=\n \nYenOrders.EventTime\n           AND YenOrders.EventTime <\n \nWINDOW_END(ValidRates.Window)\n       HAVING Curr = ""Euro"";\n-------------------------------------------\n| E | Y/E | Y   | Order  | Rate Window    |\n-------------------------------------------\n| 2 | 114 | 228 | 12:02  | [12:00, 12:03) |\n| 5 | 116 | 580 | 12:05  | [12:03, 12:06) |\n| 3 | 119 | 357 | 12:08  | [12:06, +inf)  |\n-------------------------------------------\nThinking back to our original \nYenRates\n and \nYenOrders\n relations, this joined\nrelation indeed looks correct: each of the three conversions ended up with the\n(eventually) appropriate rate for the given window of event time within\nwhich their corresponding order fell. So we have a decent sense that this join\nis doing what we want in terms of providing us the eventual correctness we\nwant.\nThat said, this simple snapshot view of the relation, taken after all the values\nhave arrived and the dust has settled, belies the complexity of this join. To\nreally understand what’s going on here, we need to look at the full TVR.\nFirst, recall that the validity-windowed conversion rate relation was actually\nmuch more complex than the previous simple table snapshot view might lead\nyou to believe. For reference, here’s the \nSTREAM\n version of the validity\nwindows relation, which better highlights the evolution of those conversion\nrates over time:\n12:00>\n SELECT \nSTREAM\n         Curr,\n         MAX(Rate) as Rate,\n         VALIDITY(EventTime) as Window,\n         Sys.EmitTime as Time,\n         Sys.Undo as Undo,\n       FROM YenRates\n       GROUP BY\n         Curr,\n         VALIDITY(EventTime)\n       HAVING Curr = ""Euro"";\n--------------------------------------------------\n| Curr | Rate | Window         | Time     | Undo |\n--------------------------------------------------\n| Euro | 114  | [12:00, +inf)  | 12:06:23 |      |\n| Euro | 114  | [12:00, +inf)  | 12:07:33 | undo |\n| Euro | 114  | [12:00, 12:06) | 12:07:33 |      | \n| Euro | 119  | [12:06, +inf)  | 12:07:33 |      |\n| Euro | 114  | [12:00, 12:06) | 12:09:07 | undo | \n| Euro | 114  | [12:00, 12:03) | 12:09:07 |      |\n| Euro | 116  | [12:03, 12:06) | 12:09:07 |      |\n................. [12:00, 12:10] .................\nAs a result, if we look at the full TVR for our validity-windowed join, you\ncan see that the corresponding evolution of this join over time is much more\ncomplicated, due to the out-of-order arrival of values on both sides of the\njoin:\n12:10>\n WITH ValidRates AS\n         (SELECT\n            Curr,\n            MAX(Rate) as Rate,\n            VALIDITY_WINDOW(EventTime) as Window\n          FROM YenRates\n          GROUP BY\n            Curr,\n            VALIDITY_WINDOW(EventTime))\n       SELECT \nTVR\n         YenOrders.Amount as ""E"",\n         ValidRates.Rate as ""Y/E"", \n         YenOrders.Amount * ValidRates.Rate as ""Y"",\n         YenOrders.EventTime as Order,\n         ValidRates.Window as ""Rate Window""\n       FROM YenOrders FULL OUTER JOIN ValidRates \n         ON YenOrders.Curr = ValidRates.Curr\n           AND WINDOW_START(ValidRates.Window) <=\n \nYenOrders.EventTime\n           AND YenOrders.EventTime <\n \nWINDOW_END(ValidRates.Window)\n       HAVING Curr = ""Euro"";\n-----------------------------------------------------------------------\n--------------------\n|              [-inf, 12:05:07)              |            [12:05:07,\n \n12:06:23)            |\n| ------------------------------------------ | ------------------------\n------------------ |\n| | E | Y/E | Y   | Order | Rate Window    | | | E | Y/E | Y   | Order\n \n| Rate Window    | |\n| ------------------------------------------ | ------------------------\n------------------ |\n| ------------------------------------------ | |\n 2 \n|\n     \n|\n     \n|\n 12:02\n \n|\n                \n| |\n|                                            | ------------------------\n------------------ |\n-----------------------------------------------------------------------\n--------------------\n|            [12:06:23, 12:07:33)            |            [12:07:33,\n \n12:08:00)            |\n| ------------------------------------------ | ------------------------\n------------------ |\n| | E | Y/E | Y   | Order | Rate Window    | | | E | Y/E | Y   | Order\n \n| Rate Window    | |\n| ------------------------------------------ | ------------------------\n------------------ |\n| | 2 |\n 114 \n|\n 228 \n| 12:02 |\n [12:00, +inf)  \n| | | 2 | 114 | 228 | 12:02\n \n|\n [12:00, 12:06) \n| |\n| ------------------------------------------ | |\n   \n|\n 119 \n|\n     \n|\n       \n|\n [12:06, +inf)  \n| |\n|                                            | ------------------------\n------------------ |\n-----------------------------------------------------------------------\n--------------------\n|            [12:08:00, 12:09:07)            |            [12:09:07,\n \n12:09:33)            |\n| ------------------------------------------ | ------------------------\n------------------ |\n| | E | Y/E | Y   | Order | Rate Window    | | | E | Y/E | Y   | Order\n \n| Rate Window    | |\n| ------------------------------------------ | ------------------------\n------------------ |\n| | 2 | 114 | 228 | 12:02 | [12:00, 12:06) | | | 2 | 114 | 228 | 12:02\n \n|\n [12:00, 12:03) \n| |\n| |\n 5 \n|\n 114 \n|\n 570 \n|\n 12:05 \n|\n [12:03, 12:06) \n| | | 5 |\n 116 \n|\n 580 \n| 12:05\n \n|\n [12:03, 12:06) \n| |\n| |   | 119 |     |       | [12:06, +inf)  | | |   | 119 |     | 12:08\n \n| [12:06, +inf)  | |\n| ------------------------------------------ | ------------------------\n------------------ |\n-----------------------------------------------------------------------\n--------------------\n|               [12:09:33, now)              |\n| ------------------------------------------ |\n| | E | Y/E | Y   | Order | Rate Window    | |\n| ------------------------------------------ |\n| | 2 | 114 | 228 | 12:02 | [12:00, 12:03) | |\n| | 5 | 116 | 580 | 12:05 | [12:03, 12:06) | |\n| |\n 3 \n| 119 |\n 357 \n|\n 12:08 \n| [12:06, +inf)  | |\n| ------------------------------------------ |\n----------------------------------------------\nIn particular, the result for the 5 € order is originally quoted at 570 ¥ because\nthat order (which happened at 12:05) originally falls into the validity window\nfor the 114 ¥/€ rate. But when the 116 ¥/€ rate for event time 12:03 arrives\nout of order, the result for the 5 € order must be updated from 570 ¥ to 580 ¥.\nThis is also evident if you observe the results of the join as a stream (here\nI’ve highlighted the incorrect 570 ¥ in red, and the retraction for 570 ¥ and\nsubsequent corrected value of 580 ¥ in blue):\n12:00>\n WITH ValidRates AS\n         (SELECT\n            Curr,\n            MAX(Rate) as Rate,\n            VALIDITY_WINDOW(EventTime) as Window\n          FROM YenRates\n          GROUP BY\n            Curr,\n            VALIDITY_WINDOW(EventTime))\n       SELECT \nSTREAM\n         YenOrders.Amount as ""E"",\n         ValidRates.Rate as ""Y/E"", \n         YenOrders.Amount * ValidRates.Rate as ""Y"",\n         YenOrders.EventTime as Order,\n         ValidRates.Window as ""Rate Window"",\n         Sys.EmitTime as Time,\n         Sys.Undo as Undo\n       FROM YenOrders FULL OUTER JOIN ValidRates \n         ON YenOrders.Curr = ValidRates.Curr\n           AND WINDOW_START(ValidRates.Window) <=\n \nYenOrders.EventTime\n           AND YenOrders.EventTime <\n \nWINDOW_END(ValidRates.Window)\n       HAVING Curr = “Euro”;\n------------------------------------------------------------\n| E | Y/E | Y   | Order | Rate Window    | Time     | Undo | \n------------------------------------------------------------\n| 2 |     |     | 12:02 |                | 12:05:07 |      |\n| 2 |     |     | 12:02 |                | 12:06:23 | undo |\n| 2 | 114 | 228 | 12:02 | [12:00, +inf)  | 12:06:23 |      |\n| 2 | 114 | 228 | 12:02 | [12:00, +inf)  | 12:07:33 | undo |\n| 2 | 114 | 228 | 12:02 | [12:00, 12:06) | 12:07:33 |      |\n|   | 119 |     |       | [12:06, +inf)  | 12:07:33 |      |\n|\n 5 \n|\n 114 \n|\n 570 \n|\n 12:05 \n|\n [12:00, 12:06) \n|\n 12:08:00 \n|\n      \n|\n| 2 | 114 | 228 | 12:02 | [12:00, 12:06) | 12:09:07 | undo |\n|\n 5 \n|\n 114 \n|\n 570 \n|\n 12:05 \n|\n [12:00, 12:06) \n|\n 12:09:07 \n|\n undo \n|\n| 2 | 114 | 228 | 12:02 | [12:00, 12:03) | 12:09:07 |      |\n|\n 5 \n|\n 116 \n|\n 580 \n|\n 12:05 \n|\n [12:03, 12:06) \n|\n 12:09:07 \n|\n      \n|\n|   | 119 |     |       | [12:06, +inf)  | 12:09:33 | undo |\n| 3 | 119 | 357 | 12:08 | [12:06, +inf)  | 12:09:33 |      |\n...................... [12:00, 12:10] ......................\nIt’s worth calling out that this is a fairly messy stream due to the use of a\nFULL OUTER\n join.\n In reality, when consuming conversion orders as a stream,\nyou probably don’t care about unjoined rows; switching \nto an \nINNER\n join\nhelps eliminate those rows. You probably also don’t care about cases for\nwhich the rate window changes, but the actual conversion value isn’t\naffected. By removing the rate window from the stream, we can further\ndecrease its chattiness:\n12:00>\n WITH ValidRates AS\n         (SELECT\n            Curr,\n            MAX(Rate) as Rate,\n            VALIDITY_WINDOW(EventTime) as Window\n          FROM YenRates\n          GROUP BY\n            Curr,\n            VALIDITY_WINDOW(EventTime))\n       SELECT \nSTREAM\n         YenOrders.Amount as ""E"",\n         ValidRates.Rate as ""Y/E"", \n         YenOrders.Amount * ValidRates.Rate as ""Y"",\n         YenOrders.EventTime as Order,\n         \nValidRates.Window as ""Rate Window"",\n         Sys.EmitTime as Time,\n         Sys.Undo as Undo\n       FROM YenOrders \nINNER\n JOIN ValidRates \n         ON YenOrders.Curr = ValidRates.Curr\n           AND WINDOW_START(ValidRates.Window) <=\n \nYenOrders.EventTime\n           AND YenOrders.EventTime <\n \nWINDOW_END(ValidRates.Window)\n       HAVING Curr = ""Euro"";\n-------------------------------------------\n| E | Y/E | Y   | Order | Time     | Undo |\n-------------------------------------------\n| 2 | 114 | 228 | 12:02 | 12:06:23 |      |\n|\n 5 \n|\n 114 \n|\n 570 \n|\n 12:05 \n|\n 12:08:00 \n|\n      \n|\n|\n 5 \n|\n 114 \n|\n 570 \n|\n 12:05 \n|\n 12:09:07 \n|\n undo \n|\n|\n 5 \n|\n 116 \n|\n 580 \n|\n 12:05 \n|\n 12:09:07 \n|\n      \n|\n| 3 | 119 | 357 | 12:08 | 12:09:33 |      |\n............. [12:00, 12:10] ..............\nMuch nicer. We can now see that this query very succinctly does what we\noriginally set out to do: join two TVRs for currency conversion rates and\norders in a robust way that is tolerant of data arriving out of order. \nFigure 9-2\nvisualizes this query as an animated diagram. In it, you can also very clearly\nsee the way the overall structure of things change as they evolve over time.\nFigure 9-2. \nTemporal validity join, converting Euros to Yen with per-record triggering\nWatermarks and temporal validity joins\nWith this example, we’ve highlighted the first benefit of windowed joins\ncalled out at the\n beginning of\n this section: windowing a join allows you to\npartition that join within time for some practical business need. In this case,\nthe business need was slicing time into regions of validity for our currency\nconversion rates.\nBefore we call it a day, however, it turns out that this example also provides\nan opportunity to highlight the second point I called out: the fact that\nwindowing a join can provide a meaningful reference point for watermarks.\nTo see how that’s useful, imagine changing the previous query to replace the\nimplicit default per-record trigger with an explicit watermark trigger that\nwould fire only once when the watermark passed the end of the validity\nwindow in the join (assuming that we have a watermark available for both of\nour input TVRs that accurately tracks the completeness of those relations in\nevent time as well as an execution engine that knows how to take those\nwatermarks into consideration). Now, instead of our stream containing\nmultiple outputs and retractions for rates arriving out of order, we could\n00:00 / 00:00\ninstead end up with a stream containing a single, correct converted result per\norder, which is clearly even more ideal than before:\n12:00>\n WITH ValidRates AS\n         (SELECT\n            Curr,\n            MAX(Rate) as Rate,\n            VALIDITY_WINDOW(EventTime) as Window\n          FROM YenRates\n          GROUP BY\n            Curr,\n            VALIDITY_WINDOW(EventTime))\n       SELECT \nSTREAM\n         YenOrders.Amount as ""E"",\n         ValidRates.Rate as ""Y/E"", \n         YenOrders.Amount * ValidRates.Rate as ""Y"",\n         YenOrders.EventTime as Order,\n         Sys.EmitTime as Time,\n         Sys.Undo as Undo\n       FROM YenOrders INNER JOIN ValidRates \n         ON YenOrders.Curr = ValidRates.Curr\n           AND WINDOW_START(ValidRates.Window) <=\n \nYenOrders.EventTime\n           AND YenOrders.EventTime <\n \nWINDOW_END(ValidRates.Window)\n       HAVING Curr = ""Euro""\n       \nEMIT WHEN WATERMARK PAST\n \nWINDOW_END(ValidRates.Window)\n;\n-------------------------------------------\n| E | Y/E | Y   | Order | Time     | Undo |\n-------------------------------------------\n| 2 | 114 | 228 | 12:02 | 12:08:52 |      |\n| 5 | 116 | 580 | 12:05 | 12:10:04 |      |\n| 3 | 119 | 357 | 12:08 | 12:10:13 |      |\n............. [12:00, 12:11] ..............\nOr, rendered as an animation, which clearly shows how joined results are not\nemitted into the output stream until the watermark moves beyond them, as",26580
88-Summary.pdf,88-Summary,"demonstrated in \nFigure 9-3\n.\nFigure 9-3. \nTemporal validity join, converting Euros to Yen with watermark triggering\nEither way, it’s impressive\n to see how this query encapsulates such a\ncomplex set of interactions into a clean and concise rendering of the desired\nresults.\nSummary\nIn this chapter, we analyzed the world of joins (using the join vocabulary of\nSQL) within the context of stream processing. We began with unwindowed\njoins and saw how, conceptually, all joins are streaming joins as the core. We\nsaw how the foundation for essentially all of the other join variations is the\nFULL OUTER\n join, and discussed the specific alterations that occur as part of\nLEFT OUTER\n, \nRIGHT OUTER\n, \nINNER\n, \nANTI\n, \nSEMI\n, and even \nCROSS\n joins. In\naddition, we saw how all of those different join patterns interact in a world of\nTVRs and streams.\nWe next moved on to windowed joins, and learned that windowing a join is\ntypically motivated by one or both of the following benefits:\nThe ability to \npartition the join within time\n for some business need\nThe ability to \ntie results\n from the join \nto the progress of a watermark\nAnd, finally, we explored in depth one of the more interesting and useful\ntypes of windows with respect to joining: temporal validity windows. We\nsaw how temporal validity windows very naturally carve time into regions of\nvalidity for given values, based only on the specific points in time where\n00:00 / 00:00\nthose values change. We learned that joins within validity windows require a\nwindowing framework that supports windows that can split over time, which\nis something no existing streaming system today supports natively. And we\nsaw how concisely validity windows allowed us to solve the problem of\njoining TVRs for currency conversion rates and orders together in a robust,\nnatural way.\nJoins are often one of the more intimidating aspects of data processing,\nstreaming or otherwise. However, by understanding the theoretical\nfoundation of joins and how straightforwardly we can derive all the different\ntypes of joins from that basic foundation, joins become a much less\nfrightening beast, even with the additional dimension of time that streaming\nadds to the mix.\n From a conceptual perspective, at least. There are many different ways to\nimplement each of these types of joins, some of which are likely much more\nefficient than performing an actual \nFULL OUTER\n join and then filtering down\nits results, especially when the rest of the query and the distribution of the\ndata are taken into consideration.\n Again, ignoring what happens when there are duplicate join keys; more on\nthis when we get to \nSEMI\n joins.\n From a conceptual perspective, at least. There are, of course, many different\nways to implement each of these types of joins, some of which might be\nmuch more efficient than performing an actual \nFULL OUTER\n join and then\nfiltering down its results, depending on the rest of the query and the\ndistribution of the data.\n Note that the example data and the temporal join use case motivating it are\nlifted almost wholesale from Julian Hyde’s excellent \n“Streams, joins, and\ntemporal tables”\n document.\n It’s a partial implementation because it only works if the windows exist in\nisolation, as in \nFigure 9-1\n. As soon as you mix the windows with other data,\nsuch as the joining examples below, you would need some mechanism for\n1\n2\n3\n4\n5\nsplitting the data from the shrunken window into two separate windows,\nwhich Beam does not currently provide.",3588
89-MapReduce.pdf,89-MapReduce,"Chapter 10. \nThe Evolution of\nLarge-Scale \nData Processing\nYou have now arrived at the final chapter in the book, you stoic literate, you.\nYour journey will soon be complete!\nTo wrap things up, I’d like you to join me on a brief stroll through history,\nstarting back in the ancient days of large-scale data processing with\nMapReduce and touching upon some \nof the highlights over the ensuing\ndecade and a half that have brought streaming systems to the point they’re at\ntoday. It’s a relatively lightweight chapter in which I make a few\nobservations about important contributions from a number of well-known\nsystems (and a couple maybe not-so-well known), refer you to a bunch of\nsource material you can go read on your own should you want to learn more,\nall while attempting not to offend or inflame the folks responsible for systems\nwhose truly impactful contributions I’m going to either oversimplify or\nignore completely for the sake of space, focus, and a cohesive narrative.\nShould be a good time.\nOn that note, keep in mind as you read this chapter that we’re really just\ntalking about specific pieces of the MapReduce/Hadoop family tree of large-\nscale data processing here. I’m not covering the SQL arena in any way shape\nor form\n; we’re not talking HPC/supercomputers, and so on. So as broad and\nexpansive as the title of this chapter might sound, I’m really focusing on a\nspecific vertical swath of the grand universe of large-scale data processing.\nCaveat literatus, and all that.\nAlso note that I’m covering a \ndisproportionate amount of Google\ntechnologies here. You would be right in thinking that this might have\nsomething to do with the fact that I’ve worked at Google for more than a\ndecade. But there are two other reasons for it: 1) big data has always been\nimportant for Google, so there have been a number of worthwhile\ncontributions created there that merit discussing in detail, and 2) my\n1\nexperience has been that folks outside of Google generally seem to enjoy\nlearning more about the things we’ve done, because we as a company have\nhistorically been somewhat tight-lipped in that regard. So indulge me a bit\nwhile I prattle on excessively about the stuff we’ve been working on behind\nclosed doors.\nTo ground \nour travels in concrete chronology, we’ll be following the timeline\nin \nFigure 10-1\n, which shows rough dates of existence for the various systems\nI discuss.\nFigure 10-1. \nApproximate timeline of systems discussed in this chapter\nAt each stop, I give a brief history of the system as best I understand it and\nframe its contributions from the perspective of shaping streaming systems as\nwe know them today. At the end, we recap all of the contributions to see how\nthey’ve summed up to create the modern stream processing ecosystem of\ntoday.\nMapReduce\nWe begin the journey\n with MapReduce (\nFigure 10-2\n).\nFigure 10-2. \nTimeline: MapReduce\nI think it’s safe to say that large-scale data processing as we all know it today\ngot its start with MapReduce way back in 2003.\n At the time, engineers\nwithin Google were building all sorts of bespoke systems to tackle data\nprocessing challenges at the scale of the World Wide Web.\n As they did so,\nthey noticed three things:\nData processing is hard\nAs the data scientists and \nengineers among us well know, you can build a\ncareer out of just focusing on the best ways to extract useful insights from\nraw data.\nScalability is hard\nExtracting useful insights\n over massive-scale data is even more difficult\nyet.\nFault-tolerance is hard\nExtracting useful insights from \nmassive-scale data in a fault-tolerant,\ncorrect way on commodity hardware is brutal.\nAfter solving all three of these challenges in tandem across a number of use\ncases, they began to notice some similarities between the custom systems\nthey’d built. And they came to the conclusion that if they could build a\n2\n3\nframework that took care of the latter two issues (scalability and fault-\ntolerance), it would make focusing on the first issue a heck of a lot simpler.\nThus was born MapReduce.\nThe basic idea with MapReduce was \nto provide a simple data processing API\ncentered around two well-understand operations from the functional\nprogramming realm: map and reduce (\nFigure 10-3\n). Pipelines built with that\nAPI would then be executed on a distributed systems framework that took\ncare of all the nasty scalability and fault-tolerance stuff that quickens the\nhearts of hardcore distributed-systems engineers and crushes the souls of the\nrest of us mere mortals.\nFigure 10-3. \nVisualization of a MapReduce job\nWe already discussed the semantics of MapReduce in great detail back in\nChapter 6\n, so we won’t dwell on them here. Simply recall that we broke\nthings down into six discrete phases (MapRead, Map, MapWrite,\nReduceRead, Reduce, ReduceWrite) as part of our streams and tables\nanalysis, and we came to the conclusion in the end that there really\n wasn’t all\nthat much different between the overall Map and Reduce phases; at a high-\nlevel, they \nboth do the following:\nConvert a table to a stream\nApply a user transformation to that stream to yield another stream\n4\nGroup that stream into a table\nAfter it was placed into service within Google, MapReduce found such broad\napplication across a variety of tasks that the team decided it was worth\nsharing its ideas with the rest of the world. \nThe result was the \nMapReduce\npaper\n, published at OSDI 2004 (see \nFigure 10-4\n).\nFigure 10-4. \nThe \nMapReduce paper\n, published at OSDI 2004\nIn it, the team described in detail the history of the project, design of the API\nand implementation, and details about a number of different use cases to\nwhich MapReduce had been applied. Unfortunately, they provided no actual\nsource code, so the best that folks outside of Google at the time could do was\nsay, “Yes, that sounds very nice indeed,” and go back to building their\nbespoke systems.\nOver the course of the decade that followed, MapReduce continued to\nundergo heavy development within Google, with large amounts of time\ninvested in making the system scale to unprecedented levels. \nFor a more\ndetailed account of some of the highlights along that journey, I recommend\nthe post \n“History of massive-scale sorting experiments at Google”\n(\nFigure 10-5\n) written by our official MapReduce historian/scalability and\nperformance wizard, Marián \nDvorský\n.\nFigure 10-5. \nMarián Dvorský’s \n“History of massive-scale sorting experiments”\n blog post\nBut for our purposes here, suffice it to say that nothing else yet has touched\nthe magnitude of scale achieved by MapReduce, not even within Google.\nConsidering how long MapReduce has been around, that’s saying something;\n14 years is an eternity in our industry.",6848
90-Hadoop.pdf,90-Hadoop,"From a streaming systems perspective, the main takeaways I want to leave\nyou with for MapReduce are \nsimplicity\n and \nscalability\n. \nMapReduce took the\nfirst brave steps toward taming the unruly beast that is massive-scale data\nprocessing, exposing a simple and straightforward API for crafting powerful\ndata processing pipelines, its austerity belying the complex distributed\nsystems magic happening under the covers to allow those pipelines to run at\nscale on large clusters\n of commodity hardware. \nHadoop\nNext in our list is Hadoop (\nFigure 10-6\n). Fair warning: \nthis is one of\n those\ntimes where I will grossly oversimplify the impact of a system for the sake of\na focused narrative. The impact Hadoop has had on our industry and the\nworld at large cannot be overstated, and it extends well beyond the relatively\nspecific scope I discuss here.\nFigure 10-6. \nTimeline: Hadoop\nHadoop came about in 2005, when Doug Cutting and Mike Cafarella decided\nthat the ideas from the MapReduce paper were just the thing they needed as\nthey built a distributed version of their Nutch webcrawler. They had already\nbuilt their own version of Google’s distributed filesystem (originally called\nNDFS for Nutch Distributed File System, later \nrenamed to HDFS, or Hadoop\nDistributed File System), so it was a natural next step to add a MapReduce\nlayer on top after that paper was published. They called this layer Hadoop.\nThe key difference between Hadoop and MapReduce was that Cutting and\nCafarella made sure the source code for Hadoop was shared with the rest of\nthe world by open sourcing it (along with the source for HDFS) as part of\nwhat would eventually become the Apache Hadoop project. Yahoo’s hiring\nof Cutting to help transition the Yahoo webcrawler architecture onto Hadoop\ngave the project an additional boost of validity and engineering oomph, and\nfrom there, an entire ecosystem of open source data processing tools grew.\nAs with MapReduce, others have told the history of Hadoop in other fora far\nbetter than I can; one particularly good reference is Marko Bonaci’s \n“The\nhistory of Hadoop,”\n itself originally slated for inclusion in a print book\n(\nFigure 10-7\n).\nFigure 10-7. \nMarko Bonaci’s \n“The history of Hadoop”\nThe main point I want you to take away from this section is the massive",2344
91-Flume.pdf,91-Flume,"impact the \nopen source ecosystem\n that flowered around Hadoop had upon the\nindustry as a whole.\n By creating an open community in which engineers\ncould improve and extend the ideas from those early GFS and MapReduce\npapers, a thriving ecosystem was born, yielding dozens of useful tools like\nPig, Hive, HBase, Crunch, and on and on. That openness was key to\nincubating the diversity of ideas that exist now across our industry, and it’s\nwhy I’m pigeonholing Hadoop’s open source ecosystem as its single most\nimportant contribution to the world of streaming systems as we know them\ntoday.\nFlume\nWe now\n return to Google territory to talk about the official \nsuccessor to\nMapReduce within Google: Flume ([\nFigure 10-8\n] sometimes also called\nFlumeJava in reference to the original Java version of the system, \nand not to\nbe confused with Apache Flume, which is an entirely different beast that just\nso happens to share the same name).\nFigure 10-8. \nTimeline: Flume\nThe Flume project was founded by Craig Chambers when the Google Seattle\noffice opened in 2007. It was motivated by a desire to solve some of the\ninherent shortcomings of MapReduce, which had become apparent over the\nfirst few years of its success. Many of these shortcomings revolved around\nMapReduce’s rigid Map → Shuffle → Reduce structure; though\n refreshingly\nsimple, it carried with it some downsides:\nBecause many use cases cannot be served by the application of a\nsingle MapReduce, a number of bespoke \norchestration systems\nbegan popping up across Google for coordinating sequences of\nMapReduce jobs. These systems all served essentially the same\npurpose (gluing together multiple MapReduce jobs to create a\ncoherent pipeline solving a complex problem). However, having\nbeen developed independently, they were naturally incompatible and\na textbook example of unnecessary duplication of effort.\nWhat’s worse, there were numerous cases in which a clearly\n written\nsequence of MapReduce jobs would introduce \ninefficiencies\n thanks\nto the rigid structure of the API. For example, one team might write\na MapReduce that simply filtered out some number of elements; that\nis, a map-only job with an empty reducer. It might be followed up by\nanother team’s map-only job doing some element-wise enrichment\n(with yet another empty reducer). The output from the second job\nmight then finally be consumed by a final team’s MapReduce\nperforming some grouping aggregation over the data. This pipeline,\nconsisting of essentially a single chain of Map phases followed by a\nsingle Reduce phase, would require the orchestration of three\ncompletely independent jobs, each chained together by shuffle and\noutput phases materializing the data. But that’s assuming you\nwanted to keep the codebase logical and clean, which leads to the\nfinal downside…\nIn an effort to optimize away these inefficiencies in their\nMapReductions, engineers began introducing \nmanual optimizations\nthat would \nobfuscate\n the simple logic of the pipeline, increasing\nmaintenance and debugging costs.\nFlume addressed these issues\n by providing a composable, high-level API for\ndescribing data processing pipelines, essentially based around the same\nPCollection and PTransform concepts found in Beam, as illustrated in\nFigure 10-9\n.\nFigure 10-9. \nHigh-level pipelines in Flume (image credit: Frances Perry)\nThese pipelines, when launched, would be fed through an optimizer\n to\ngenerate a plan for an\n optimally efficient sequence of MapReduce jobs, the\nexecution of which was then orchestrated by the framework, which you can\nsee illustrated in \nFigure 10-10\n.\nFigure 10-10. \nOptimization from a logical pipeline to a physical execution plan\nPerhaps the most important example of an\n automatic optimization\n that Flume\n5\ncan perform is fusion (which Reuven discussed a bit back in \nChapter 5\n), in\nwhich two logically independent stages can be run in the same job either\nsequentially (consumer-producer fusion) or in parallel (sibling fusion), as\ndepicted in \nFigure 10-11\n.\nFigure 10-11. \nFusion optimizations combine successive or parallel operations together\ninto the same physical operation\nFusing two stages together eliminates serialization/deserialization and\nnetwork costs, which can be significant in pipelines processing large amounts\nof data.\nAnother type of\n automatic \noptimization is \ncombiner lifting\n (see \nFigure 10-\n12\n), the mechanics of which we already touched upon in \nChapter 7\n when we\ntalked about incremental combining. Combiner lifting is simply the automatic\napplication of multilevel combine logic that we discussed in that chapter: a\ncombining operation (e.g., summation) that logically happens after a\ngrouping operation is partially lifted into the stage preceding the group-by-\nkey (which by definition requires a trip across the network to shuffle the data)\nso that it can perform partial combining before the grouping happens. In\ncases of very hot keys, this can greatly reduce the amount of data shuffled\nover the network, and also spread the load of computing the final aggregate\nmore smoothly across multiple machines.\nFigure 10-12. \nCombiner lifting applies partial aggregation on the sender side of a group-\nby-key operation before completing aggregation on the consumer side\nAs a result of its cleaner API and automatic optimizations, Flume Java was\nan instant hit upon its introduction at Google in early 2009. Following on the\nheels of that success, the team published the\n paper titled \n“Flume Java: Easy,\nEfficient Data-Parallel Pipelines”\n (see \nFigure 10-13\n), itself an excellent\nresource for learning more about the system as it originally existed.\nFigure 10-13. \nFlumeJava paper\nFlume C++ followed not too much later in 2011, and in early 2012 Flume\nwas introduced into Noogler\n training provided to all new engineers at\nGoogle. That was the beginning of the end for MapReduce.\nSince then, Flume has been migrated to no longer use MapReduce as its\nexecution engine; instead,\n it uses a custom execution engine, called Dax,\nbuilt directly into the framework itself. By freeing Flume itself from the\nconfines of the previously underlying Map → Shuffle → Reduce structure of\nMapReduce, Dax enabled new optimizations, such as the dynamic work\nrebalancing feature described in Eugene Kirpichov and Malo Denielou’s \n“No\nshard left behind”\n blog post (\nFigure 10-14\n).\n6\nFigure 10-14. \n“No shard left behind”\n post\nThough discussed in that post in the context of Cloud Dataflow, dynamic\nwork rebalancing (or liquid sharding, as it’s colloquially known at Google)\nautomatically rebalances\n extra work from straggler shards to other idle\nworkers in the system as they complete their work early. \nBy dynamically\nrebalancing the work distribution over time, it’s possible to come much\ncloser to an optimal work distribution than even the best educated initial\nsplits could ever achieve. It also allows for adapting to variations across the\npool of workers, where a slow machine that might have otherwise held up the\ncompletion of a job is simply compensated for by moving most of its tasks to\nother workers. When liquid sharding was rolled out at Google, it recouped\nsignificant amounts of resources across the fleet.\nOne last point on Flume is that it was also later extended to support streaming\nsemantics.\n In addition to the batch Dax backend, Flume was extended to be\nable to execute pipelines on the MillWheel stream processing system\n(discussed in a moment). \nMost of the high-level streaming semantics",7647
92-Storm.pdf,92-Storm,"concepts we’ve discussed in this book were first incorporated into Flume\nbefore later finding their way into Cloud Dataflow and eventually Apache\nBeam.\nAll that said, the primary thing to take away from Flume in this section is the\nintroduction of a notion of \nhigh-level pipelines\n, which enabled the \nautomatic\noptimization\n of clearly written, logical pipelines. This enabled the creation of\nmuch larger and complex pipelines, without the need for manual\norchestration or optimization, and all while keeping\n the code for those\npipelines logical and clear.\nStorm\nNext up is Apache Storm (\nFigure 10-15\n), the \nfirst real streaming system we\ncover.\n Storm most certainly wasn’t the first streaming system in existence,\nbut I would argue it was the first streaming system \nto see truly broad\nadoption across the industry, and for that reason we give it a closer look here.\nFigure 10-15. \nTimeline: Storm\nFigure 10-16. \n“History of Apache Storm and lessons learned”\nStorm was the brainchild of Nathan Marz, who later chronicled the history of\nits creation \nin a blog \npost titled\n \n“History of Apache Storm and lessons\nlearned”\n (\nFigure 10-16\n). The TL;DR version of it is that Nathan’s team at the\nstartup employing him then, BackType, had been attempting to process the\nTwitter firehose using a custom system of queues and workers. He came to\nessentially the same realization that the MapReduce folks had nearly a decade\nearlier: the actual data processing portion of their code was only a tiny\namount of the system, and building those real-time data processing pipelines\nwould be a lot easier if there were a framework doing all the distributed\nsystem’s dirty work under the covers. Out of that was born Storm.\nThe interesting thing about Storm, in comparison to the rest of the systems\nwe’ve talked about so far, is that the team chose to loosen the strong\nconsistency guarantees found in all of the other systems we’ve talked about\nso far as a way of providing lower latency.\n By combining at-most once or at-\nleast once semantics with per-record processing and no integrated (i.e., no\nconsistent) notion of persistent state, \nStorm was able provide much lower\nlatency in providing results than systems that executed over batches of data\nand guaranteed exactly-once correctness. And for a certain type of use cases,\nthis was a very reasonable trade-off to make.\nUnfortunately, it quickly became clear that people really wanted to have their\ncake and eat it, too. They didn’t just want to get their answers quickly, they\nwanted to have both low-latency results \nand\n eventual correctness.\n But such a\nthing was impossible \nwith Storm alone.\n Enter the Lambda Architecture.\nGiven the limitations of Storm, shrewd engineers began running a weakly\nconsistent Storm streaming pipeline alongside a strongly consistent Hadoop\nbatch pipeline. The former produced low-latency, inexact results, whereas the\nlatter produced high-latency, exact results, both of which would then be\nsomehow merged together in the end to provide a single low-latency,\neventually consistent view of the outputs. We learned back in \nChapter 1\n that\nthe Lambda Architecture was Marz’s other brainchild, as detailed in his post\ntitled \n“How to beat the CAP theorem”\n (\nFigure 10-17\n).\n7\nFigure 10-17. \n“How to beat the CAP theorem”\nI’ve already spent a fair amount of time harping on the shortcomings of the\nLambda Architecture, so I won’t belabor those points here. But I will reiterate\nthis: the Lambda Architecture became quite popular, despite the costs and\nheadaches associated with it, simply because it met a critical need that a great\nmany businesses were otherwise having a difficult time fulfilling: that of\ngetting low-latency, but eventually correct results out of their data processing\npipelines.\nFrom the perspective of the evolution of streaming systems, I argue that\nStorm was responsible for first bringing low-latency data processing to the\nmasses. However, it did so at the cost of weak consistency, which in turn\nbrought about the rise of the Lambda Architecture, and the years\n of dual-\npipeline darkness that followed.\nFigure 10-18. \nHeron paper\nBut hyperbolic dramaticism aside, Storm was the system that gave the\nindustry its first taste of low-latency data processing, and the impact of that is\nreflected in the broad interest in and adoption of streaming systems today.\nBefore moving on, it’s also worth giving a shout out to Heron. In 2015,\nTwitter (the largest known user of Storm in the world,\n and the company that\noriginally fostered the Storm project) surprised the industry by announcing it\nwas abandoning the Storm execution engine in favor of a new system it had\ndeveloped in house, called Heron. Heron aimed to address a number of\nperformance and maintainability issues that had plagued Storm, while\nremaining API compatible, as detailed in the company’s paper titled \n“Twitter\nHeron: Stream Processing at Scale”\n (\nFigure 10-18\n). Heron itself was\nsubsequently \nopen sourced\n (with governance moved to its own independent\nfoundation, not an existing one like Apache). Given the continued\ndevelopment on Storm, there are now two competing variants of the Storm",5285
93-Spark.pdf,93-Spark,"lineage. Where things will end up is anyone’s guess, but it will \nbe exciting to\nwatch.\nSpark\nMoving on, we \nnow come to Apache Spark (\nFigure 10-19\n). This is another\nsection in which I’m going to greatly oversimplify the total impact that Spark\nhas had on the industry by focusing on a specific portion of its contributions:\nthose within the realm of stream processing. Apologies in advance.\nFigure 10-19. \nTimeline: Spark\nSpark got its start at the now famous AMPLab in UC Berkeley around 2009.\nThe thing that initially fueled Spark’s fame was its ability to oftentimes\nperform the bulk of a pipeline’s calculations entirely in memory, without\ntouching disk until the very end. Engineers achieved this\n via the Resilient\nDistributed Dataset (RDD) idea, which basically captured the full lineage of\ndata at any given point in the pipeline, allowing intermediate results to be\nrecalculated as needed on machine failure, under the assumptions that a) your\ninputs were always replayable, and b) your computations were deterministic.\nFor many use cases, these preconditions were true, or at least true enough\ngiven the massive gains in performance users were able to realize over\nstandard Hadoop jobs. From there, Spark gradually built up its eventual\nreputation as Hadoop’s de facto successor.\nA few years after Spark was created, Tathagata Das, then a graduate student\nin the AMPLab, came to the realization that: hey, we’ve got this fast batch\nprocessing engine, what if we just wired things up so we ran multiple batches\none after another, and used that to process streaming data? From that bit of\ninsight, Spark Streaming was born.\n \nWhat was really fantastic about Spark Streaming was this: thanks to the\nstrongly consistent batch engine powering things under the covers, the world\nnow had a stream processing engine that could provide correct results all by\nitself without needing the help of an additional batch job.\n In other words,\ngiven the right use case, you could ditch your Lambda Architecture system\nand just use Spark Streaming. All hail Spark Streaming!\nThe one major caveat here was the “right use case” part. The big downside to\nthe original version of Spark Streaming (the 1.x variants) was that it provided\nsupport for only a specific flavor of stream processing: processing-time\nwindowing. So any use case that cared about event time, needed to deal with\nlate data, and so on, couldn’t be handled out of the box without a bunch of\nextra code being written by the user to implement some form of event-time\nhandling on top of Spark’s processing-time windowing architecture. This\nmeant that Spark Streaming was best suited for in-order data or event-time-\nagnostic computations. And, as I’ve reiterated throughout this book, those\nconditions are not as prevalent as you would hope when dealing with the\nlarge-scale, user-centric datasets common today.\nAnother interesting controversy that surrounds Spark Streaming is the age-\nold “microbatch versus true streaming” debate.\n Because Spark Streaming is\nbuilt upon the idea of small, repeated runs of a batch processing engine,\ndetractors claim that Spark Streaming is not a true streaming engine in the\nsense that progress in the system is gated by the global barriers of each batch.\nThere’s some amount of truth there. Even though true streaming engines\nalmost always utilize some sort of batching or bundling for the sake of\nthroughput, they have the flexibility to do so at much finer-grained levels,\ndown to individual keys. The fact that microbatch architectures process\nbundles at a global level means that it’s virtually impossible to have both low\nper-key latency and high overall throughput, and there are a number of\nbenchmarks that have shown this to be more or less true. But at the same\ntime, latency on the order of minutes or multiple seconds is still quite good.\nAnd there are very few use cases that demand exact correctness and such\nstringent latency capabilities. So in some sense, Spark was absolutely right to\ntarget the audience it did originally; most people fall in that category. But that\nhasn’t stopped its competitors from slamming this as a massive disadvantage\nfor the platform. Personally, I see it as a minor complaint at best in most\ncases.\nShortcomings aside, Spark Streaming was a watershed moment for stream\nprocessing: the first publicly available, large-scale stream processing engine\nthat could also provide the correctness guarantees of a batch system. And of\ncourse, as previously noted, streaming is only a very small part of Spark’s\noverall success story, with important contributions made in the space of\niterative processing and machine learning, its native SQL integration, \nand the\naforementioned lightning-fast in-memory performance, to name a few.\nIf you’re curious to learn more about the details of the original Spark 1.x\narchitecture, I highly recommend Matei Zaharia’s dissertation on the subject,\n“An Architecture for Fast and General Data Processing on Large Clusters”\n(\nFigure 10-20\n). It’s 113 pages of Sparky goodness that’s well worth the\ninvestment.\nFigure 10-20. \nSpark dissertation\nAs of today, the 2.x variants of Spark are greatly expanding upon the\nsemantic capabilities of Spark Streaming, \nincorporating many parts of the\nmodel described in this book, while attempting to simplify some of the more\ncomplex pieces. And Spark is even pushing a new true streaming\narchitecture, to try to shut down the microbatch naysayer arguments. But\nwhen it first came on the scene, the important contribution that Spark brought\nto the table was the fact that it was the \nfirst publicly available stream\nprocessing engine with strong consistency semantics\n, albeit only in the case\nof in-order data or event-time-agnostic computation.",5849
94-MillWheel.pdf,94-MillWheel,"MillWheel\nNext we discuss MillWheel, a project that I first dabbled\n with in \nmy 20%\ntime after joining Google in 2008, later joining the team full time in 2010\n(\nFigure 10-21\n).\nFigure 10-21. \nTimeline: MillWheel\nMillWheel is Google’s original, general-purpose stream processing\narchitecture, and the project was founded by Paul Nordstrom around the time\nGoogle’s Seattle office opened. MillWheel’s success within Google has long\ncentered on an ability to provide low-latency, strongly consistent processing\nof unbounded, out-of-order data.\n Over the course of this book, we’ve looked\nat most of the bits and pieces that came together in MillWheel to make this\npossible:\nReuven discussed \nexactly-once guarantees\n in \nChapter 5\n. Exactly-\nonce guarantees\n are essential for correctness.\nIn \nChapter 7\n we\n looked at \npersistent state\n, the strongly consistent\nvariations of which provide the foundation for maintaining that\ncorrectness in long-running pipelines executing on unreliable\nhardware.\nSlava talked about \nwatermarks\n in \nChapter 3\n. Watermarks \nprovide a\nfoundation for reasoning about disorder in input data.\nAlso in \nChapter 7\n, we looked at \npersistent timers\n, which provide the\nnecessary link between watermarks and the pipeline’s business logic.\nIt’s perhaps somewhat surprising then to note that the MillWheel project was\nnot initially focused on correctness.\n Paul’s original vision more closely\ntargeted the niche that Storm later espoused: low-latency data processing with\nweak consistency. It was the initial MillWheel customers, one building\nsessions over search data and another performing anomaly detection on\nsearch queries (the Zeitgeist example from the MillWheel paper), who drove\nthe project in the direction of correctness. Both had a strong need for\nconsistent results: sessions were used to infer user behavior, and anomaly\ndetection was used to infer trends in search queries; the utility of both\ndecreased significantly if the data they provided were not reliable. As a result,\nMillWheel’s direction was steered toward one of strong consistency.\nSupport for out-of-order processing, which is the other core aspect of robust\nstreaming often attributed to MillWheel, was also motivated by customers.\nThe Zeitgeist pipeline, as a true streaming use case, wanted to\n generate an\noutput stream that identified anomalies in search query traffic, and only\nanomalies (i.e., it was not practical for consumers of its analyses to poll all\nthe keys in a materialized view output table waiting for an anomaly to be\nflagged; consumers needed a direct signal only when anomalies happened for\nspecific keys). For anomalous spikes (i.e., \nincreases\n in query traffic), this is\nrelatively straightforward: when the count for a given query exceeds the\nexpected value in your model for that query by some statistically significant\namount, you can signal an anomaly. But for anomalous dips (i.e., \ndecreases\nin query traffic), the problem is a bit trickier. It’s not enough to simply see\nthat the number of queries for a given search term has decreased, because for\nany period of time, the observed number always starts out at zero. What you\nreally need to do in these cases is wait until you have reason to believe that\nyou’ve seen a sufficiently representative portion of the input for a given time\nperiod, and only \nthen\n compare the count against your model.\nTRUE STREAMING\n“True streaming use case” bears a bit of explanation. One recent trend in\nstreaming systems is to try to simplify the programming models to make\nthem more accessible by limiting the types of use cases one can address.\nFor example, at the time of writing, both Spark’s Structured Streaming\nand Apache Kafka’s Kafka Streams systems limit themselves to what I\nrefer to in \nChapter 8\n as “materialized view semantics,” essentially\nrepeated updates to an eventually consistent output table. Materialized\nview semantics are great when you want to consume your output as a\nlookup table: any time you can just lookup a value in that table and be\nokay with the latest result as of query time, materialized views are a good\nfit. They are not, however, particularly well suited for use cases in which\nyou want to consume your output as a bonafide stream. I refer to these as\ntrue streaming use cases, with anomaly detection being one of the better\nexamples.\nAs we’ll discuss shortly, there are certain aspects of anomaly detection\nthat make it unsuitable for pure materialized view semantics (i.e., record-\nby-record processing only), specifically the fact that it relies on reasoning\nabout the completeness of the input data to accurately identify anomalies\nthat are the result of an absence of data (in addition to the fact that polling\nan output table to see if an anomaly signal has arrived is not an approach\nthat scales particularly well). True streaming use cases are thus the\nmotivation for features like watermarks (Preferably \nlow\n watermarks that\npessimistically track input completeness, as described in \nChapter 3\n, not\nhigh\n watermarks that track the event time of the newest record the system\nis aware of, as used by Spark Structured Streaming for garbage collecting\nwindows, since high watermarks are more prone to incorrectly throwing\naway data as event time skew varies within the pipeline) and triggers.\nSystems that omit these features do so for the sake of simplicity but at the\ncost of decreased ability. There can be great value in that, most certainly,\nbut don’t be fooled if you hear such systems claim these simplifications\nyield equivalent or even greater generality; you can’t address fewer use\ncases and be equally or more general.\nThe Zeitgeist pipeline first attempted to do this by inserting processing-time\ndelays before the analysis logic that looked for dips. This would work\nreasonably decently when data arrived in order, but the pipeline’s authors\ndiscovered that data could, at times, be greatly delayed and thus arrive wildly\nout of order. In these cases, the processing-time delays they were using\nweren’t sufficient, because the pipeline would erroneously report a flurry of\ndip anomalies that didn’t actually exist. What they really needed was a way to\nwait until the input became complete.\nWatermarks were thus born out of this need for reasoning about input\ncompleteness in out-of-order data.\n As Slava described in \nChapter 3\n, the basic\nidea was to track the known progress of the inputs being provided to the\nsystem, using as much or as little data available for the given type of data\nsource, to construct a progress metric that could be used to quantify input\ncompleteness. For simpler input sources like a statically partitioned Kafka\ntopic with each partition being written to in increasing event-time order (such\nas by web frontends logging events in real time), you can compute a perfect\nwatermark. For more complex input sources like a dynamic set of input logs,\na heuristic might be the best you can do. But either way, watermarks provide\na distinct advantage over the alternative of using processing time to reason\nabout event-time completeness, which experience has shown serves about as\nwell as a map of London while trying to navigate the streets of Cairo.\nSo thanks to the needs of its customers, MillWheel ended up as a system with\nthe right set of features for supporting robust stream processing on out-of-\norder data. As a result, the paper titled \n“MillWheel: Fault-Tolerant Stream\nProcessing at Internet Scale”\n (\nFigure 10-22\n) spends most of its time\ndiscussing the difficulties of providing correctness in a system like this, with\nconsistency guarantees and watermarks being the main areas of focus. It’s\nwell worth your time if you’re interested in the subject.\n8",7874
95-Kafka.pdf,95-Kafka,"Figure 10-22. \nMillWheel paper\nNot long after the MillWheel paper was published, MillWheel was integrated\nas an alternative, streaming backend for Flume, together often referred to as\nStreaming Flume.\n Within Google today, MillWheel is in the process of being\nreplaced by its successor,\n Windmill (the execution engine that also powers\nCloud Dataflow, discussed in a moment), a ground-up rewrite that\nincorporates all the best ideas from MillWheel, along with a few new ones\nlike better scheduling and dispatch, and a cleaner separation of user and\nsystem code.\nHowever, the big takeaway for MillWheel is that the four concepts listed\nearlier (exactly-once, persistent state, watermarks, persistent timers) together\nprovided the basis for a system that was finally able to deliver on the true\npromise of stream processing: robust, low-latency processing of out-of-order\ndata, even on unreliable commodity hardware.\nKafka\nWe now come to Kafka (\nFigure 10-23\n). \nKafka is unique among the systems\ndiscussed in this chapter\n in that it’s not a data processing framework,\n but\ninstead a transport layer. Make no mistake, however: Kafka has played one of\nthe most influential roles in advancing stream processing out of all the\nsystem’s we’re discussing here.\nFigure 10-23. \nTimeline: Kafka\nIf you’re not familiar with it, Kafka is essentially a persistent streaming\ntransport, implemented as a set of partitioned logs. It was developed\noriginally at LinkedIn by such industry luminaries as Neha Narkhede and Jay\nKreps, and its accolades include the\n following:\nProviding a clean model of persistence that packaged that warm\nfuzzy feeling of \ndurable\n, \nreplayable input sources\n from the batch\nworld in a streaming friendly interface.\nProviding an elastic \nisolation layer\n between producers and\nconsumers.\nEmbodying the relationship between \nstreams and tables\n that we\ndiscussed in \nChapter 6\n, revealing a foundational\n way of thinking\nabout data processing in general while also providing a conceptual\nlink to the rich and storied world of databases.\nAs of side of effect of all of the above, not only becoming the\n9\ncornerstone\n of a majority of stream processing installations across\nthe industry, but also fostering the stream-processing-as-databases\nand microservices movements.\nThey must get up very early in the morning.\nOf those accolades, there are two that stand out most to me. The first is the\napplication of durability and replayability to stream data.\n Prior to Kafka, most\nstream processing systems used some sort of ephemeral queuing system like\nRabbit MQ or even plain-old TCP sockets to send data around. Durability\nmight be provided to some degree via upstream backup in the producers (i.e.,\nthe ability for upstream producers of data to resend if the downstream\nworkers crashed), but oftentimes the upstream data was stored ephemerally,\nas well. And most approaches entirely ignored the idea of being able to\nreplay input data later in cases of backfills or for prototyping, development,\nand regression testing.\nKafka changed all that. By taking the battle-hardened concept of a durable\nlog from the database world and applying it to the realm of stream\nprocessing, Kafka gave us all back that sense of safety and security we’d lost\nwhen moving from the durable input sources common in the Hadoop/batch\nworld to the ephemeral sources prevalent at the time in the streaming world.\nWith durability and replayability, stream processing took yet another step\ntoward being a robust, reliable replacement for the ad hoc, continuous batch\nprocessing systems of yore that were still being applied to streaming use\ncases.\nAs a streaming system developer, one of the more interesting visible artifacts\nof the impact that Kafka’s durability and replayability features have had on\nthe industry is how many of the stream processing engines today have grown\nto fundamentally rely on that replayability to provide end-to-end exactly-once\nguarantees. \nReplayability is the foundation upon which end-to-end exactly-\nonce guarantees in Apex, Flink, Kafka Streams, Spark, and Storm are all\nbuilt. When executing in exactly-once mode, each of those systems\nassumes/requires that the input data source be able to rewind and replay all of\nthe data up until the most recent checkpoint. When used with an input source\nthat does not provide such ability (even if the source can guarantee reliable\ndelivery via upstream backup), end-to-end exactly-once semantics fall apart.\nThat sort of broad reliance on replayability (and the related aspect of\ndurability) is a huge testament to the amount of impact those features have\nhad across the industry.\nThe second noteworthy bullet from Kafka’s resume is the popularization of\nstream and table theory.\n We spent the entirety of \nChapter 6\n discussing\nstreams and tables as well as much of Chapters \n8\n and \n9\n. And for good reason.\nStreams and tables form the foundation of data processing, be it the\nMapReduce family tree of systems, the enormous legacy of SQL database\nsystems, or what have you. Not all data processing approaches need speak\ndirectly in terms of streams and tables but conceptually speaking, that’s how\nthey all operate. And as both users and developers of these systems, there’s\ngreat value in understanding the core underlying concepts that all of our\nsystems build upon. We all owe a collective thanks to the folks in the Kafka\ncommunity who helped shine a broader light on the streams-and-tables way\nof thinking.\nFigure 10-24. \nI \n❤\n Logs\nIf you’d like to learn more about Kafka and the foundations it’s built on, \nI \n❤\nLogs\n by Jay Kreps (O’Reilly; \nFigure 10-24\n) is an excellent resource.\nAdditionally, as cited originally in \nChapter 6\n, Kreps and Martin Kleppmann\nhave a pair of articles (\nFigure 10-25\n) that I highly recommend for reading up\non the origins of streams and table theory.\nKafka \nhas made huge contributions to the world of stream processing,\n10",6062
96-Cloud Dataflow.pdf,96-Cloud Dataflow,"arguably more than any other single system out there. In particular, the\napplication of durability and replayability to input and output streams played\na big part in helping move stream processing out of the niche realm of\napproximation tools and into the big leagues of general data processing.\nAdditionally, the theory of streams and tables, popularized\n by the Kafka\ncommunity, provides deep insight into the underlying mechanics of data\nprocessing in general. \nFigure 10-25. \nMartin’s post\n (left) and \nJay’s post\n (right)\nCloud Dataflow\nCloud Dataflow (\nFigure 10-26\n) is Google’s fully managed,\n cloud-based data\nprocessing service. Dataflow launched to\n the world in August 2015. It was\nbuilt with the intent to take the decade-plus of experiences that had gone into\nbuilding MapReduce, Flume, and MillWheel, and package them up into a\nserverless cloud experience.\nFigure 10-26. \nTimeline: Cloud Dataflow\nAlthough the serverless aspect of Cloud Dataflow is perhaps\n its most\ntechnically challenging and distinguishing factor from a systems perspective,\nthe primary contribution to streaming systems that I want to discuss here is its\nunified batch plus streaming programming model. \nThat’s all the\ntransformations, windowing, watermarks, triggers, and accumulation\ngoodness we’ve spent most of the book talking about. And all of them, of\ncourse, wrapped up the \nwhat\n/\nwhere\n/\nwhen\n/\nhow\n way of thinking about things.\nThe model first arrived back in Flume, as \nwe looked to incorporate the robust\nout-of-order processing support in MillWheel into the higher-level\nprogramming model Flume afforded. The combined batch and streaming\napproach available to Googlers internally with Flume was then the basis for\nthe fully unified model included in Dataflow.\nThe key insight in the unified model—the full extent of which none of us at\nthe time even truly appreciated—is that under the covers, batch and \nstreaming\nare really not that different: they’re both just minor variations on the streams\nand tables theme. As we learned in \nChapter 6\n, the main difference really boils\ndown to the ability to incrementally trigger tables into streams; everything\nelse is conceptually the same.\n By taking advantage of the underlying\ncommonalities of the two approaches, it was possible to provide a single,\nnearly seamless experience that applied to both worlds. This was a big step\nforward in making stream processing more accessible.\n11\nIn addition to taking advantage of the commonalities between batch and\nstreaming, we took a long, hard look at the variety of use cases we’d\nencountered over the years at Google and used those to inform the pieces that\nwent into the unified model.\n Key aspects we targeted included the following:\nUnaligned, event-time windows\n such as sessions, providing \nthe\nability to concisely express powerful analytic constructs and apply\nthem to out-of-order data.\nCustom windowing support\n, because one (or even three or four) sizes\nrarely fit all.\nFlexible triggering\n and \naccumulation modes\n, providing the ability to\nshape the way data flow through the pipeline to match the\ncorrectness, latency, and cost needs of the given use case.\nThe use of \nwatermarks\n for reasoning about \ninput completeness\n,\nwhich is critical for use cases\n like anomalous dip detection where the\nanalysis depends upon an absence of data.\nLogical abstraction\n of the underlying execution environment, be \nit\nbatch, microbatch, or streaming, providing flexibility of choice in\nexecution engine and avoiding system-level constructs (such as\nmicro-batch size) from creeping into the logical API.\nTaken together, these aspects provided the flexibility \nto balance the tensions\nbetween\n correctness, latency, \nand cost, allowing the model to\n be applied\nacross a wide breadth of use cases.\nFigure 10-27. \nDataflow Model paper\nGiven that you’ve just read an entire book covering the finer points of the\nDataflow/Beam Model, there’s little point in trying to retread any those\nconcepts here. However, if you’re looking for a slightly more academic take\non things as well as a nice overview of some of the motivating use cases\nalluded to earlier, you might find our 2015 \nDataflow Model paper\n worthwhile\n(\nFigure 10-27\n).\nThough there are many other compelling aspects to Cloud Dataflow, the\nimportant contribution from the perspective of this chapter is its \nunified batch\nplus streaming programming model\n. It brought the world a comprehensive\napproach to tackling unbounded, out-of-order datasets, and in a way that\nprovided the flexibility to make the trade-offs necessary to balance the",4707
97-Flink.pdf,97-Flink,"tensions between correctness, latency, and cost to match the requirements for\na given use case.\nFlink\nFlink (\nFigure 10-28\n) burst onto the scene in 2015, rapidly transforming\n itself\nfrom a system that almost no one had heard of into one of the powerhouses of\nthe streaming world, seemingly overnight.\nFigure 10-28. \nTimeline: Flink\nThere were two main reasons for Flink’s rise to prominence:\nIts \nrapid adoption of the Dataflow/Beam programming model\n, which\nput it in the position of being the most semantically capable fully\nopen source streaming system on the planet at the time.\nFollowed shortly \nthereafter \nby its \nhighly efficient snapshotting\nimplementation (derived from research in Chandy and Lamport’s\noriginal paper \n“Distributed Snapshots: Determining Global States of\nDistributed Systems”\n [\nFigure 10-29\n]), which gave it the strong\nconsistency guarantees needed for correctness.\nFigure 10-29. \nChandy-Lamport snapshots\nReuven covered Flink’s consistency mechanism briefly in \nChapter 5\n, but to\nreiterate, the basic idea is that periodic barriers are propagated along the\ncommunication paths between workers in the system.\n The barriers act as an\nalignment mechanism between the various distributed workers producing\ndata upstream from a consumer. When the consumer receives a given barrier\non all of its input channels (i.e., from all of its upstream producers), it\ncheckpoints its current progress for all active keys, at which point it is then\nsafe to acknowledge processing of all data that came before the barrier.\n By\ntuning how frequently barriers are sent through the system, it’s possible to\ntune the frequency of checkpointing and thus trade off increased latency (due\nto the need for side effects to be materialized only at checkpoint times) in\nexchange for higher throughput.\nFigure 10-30. \n“Extending the Yahoo! Streaming Benchmark”\nThe simple fact that Flink now had the capability to provide exactly-once\nsemantics along with native support for event-time processing was huge at\nthe time. But it wasn’t until Jamie Grier published his article titled\n“Extending the Yahoo! Streaming Benchmark”\n (\nFigure 10-30\n) that it became\nclear just how performant Flink was. In that article, Jamie described two\nimpressive\n achievements:\n1\n. \nBuilding a prototype Flink pipeline that achieved greater accuracy\nthan one of Twitter’s existing Storm pipelines (thanks to Flink’s\nexactly-once semantics) at 1% of the cost of the original.\n2\n. \nUpdating the \nYahoo! Streaming Benchmark\n to show Flink (with\nexactly-once) achieving 7.5 times the throughput of Storm (without\nexactly-once). Furthermore, Flink’s performance was shown to be\nlimited due to network saturation; removing the network bottleneck\nallowed Flink to achieve almost 40 times the throughput of Storm.\nSince then, numerous other projects (notably, Storm and Apex) have all\nadopted the same type of consistency\n mechanism.\nFigure 10-31. \n“Savepoints: Turning Back Time”\nWith the addition of a snapshotting mechanism, Flink gained the strong\nconsistency needed for end-to-end exactly-once.\n But to its credit, Flink went\none step further, and used the global nature of its snapshots to provide the\nability to restart an entire pipeline from any point in the past, a feature known\nas savepoints (described in the \n“Savepoints: Turning Back Time”\n post by\nFabian Hueske and Michael Winters [\nFigure 10-31\n]). The savepoints feature\ntook the warm fuzziness of durable replay that Kafka had applied to the\nstreaming transport layer and extended it to cover the breadth of an entire\npipeline. Graceful evolution of a long-running streaming pipeline over time\nremains an important open problem in the field, with lots of room for\nimprovement. But Flink’s savepoints feature stands as one of the first huge\nsteps in the right direction, and one that remains unique across the industry as\nof this writing.\nIf you’re\n interested in learning more about the system constructs underlying\nFlink’s snapshots and savepoints, the paper \n“State Management in Apache\nFlink”\n (\nFigure 10-32\n) discusses the implementation in good detail.\nFigure 10-32. \n“State Management in Apache Flink”\nBeyond savepoints, the Flink community has continued to innovate,",4323
98-Beam.pdf,98-Beam,"including bringing the first practical streaming SQL API to market for a\nlarge-scale, distributed stream processing engine, as we discussed in\nChapter 8\n.\nIn summary, Flink’s rapid rise to stream\n processing juggernaut can be\nattributed primarily to three characteristics of its approach: 1) incorporating\nthe \nbest existing ideas\n from across the industry (e.g., being the first open\nsource adopter of the Dataflow/Beam Model), 2) \nbringing its own\ninnovations\n to the table to push forward the state of the art (e.g., strong\nconsistency via snapshots and savepoints, streaming SQL), and 3) doing both\nof those things \nquickly\n and \nrepeatedly\n. Add in the fact that all of this is done\nin \nopen source\n, and you can see why Flink has consistently continued to raise\nthe bar for streaming processing across the industry.\nBeam\nThe last system we talk\n about is Apache\n Beam (\nFigure 10-33\n). Beam differs\nfrom most of the other systems in this chapter in that it’s primarily a\nprogramming model, API, and portability layer, not a full stack with an\nexecution engine underneath. But that’s exactly the point: just as SQL acts as\na lingua franca for declarative data processing, Beam aims to be the lingua\nfranca for programmatic data processing. Let’s explore how.\nFigure 10-33. \nTimeline: Beam\nConcretely, Beam is composed a number \nof components:\nA unified batch plus streaming \nprogramming model\n, inherited from\nCloud Dataflow where\n it originated, and the \nfiner points of which\nwe’ve spent the majority of this book discussing. The model is\nindependent of any language implementations or runtime systems.\nYou can think of this as Beam’s equivalent to SQL’s relational\nalgebra.\nA set of \nSDKs (software development kits)\n that implement that\nmodel, allowing pipelines to be expressed in terms of the model in\nidiomatic ways for a given language. Beam currently provides SDKs\nin Java, Python, and Go. You can think of these as Beam’s\nprogrammatic equivalents to the SQL language itself.\nA set of \nDSLs (domain specific languages)\n that build \nupon the\nSDKs, providing specialized interfaces that capture pieces of the\nmodel in unique ways.\n Whereas SDKs are required to surface all\naspects of the model, DSLs can expose only those pieces that make\nsense for the specific domain a DSL is targeting. Beam currently\nprovides a Scala DSL called Scio and an SQL DSL, both of which\nlayer on top of the existing Java SDK.\nA set of \nrunners\n that \ncan execute Beam pipelines. Runners take the\nlogical pipeline described in Beam SDK terms, and translate them as\nefficiently as possible into a physical pipeline that they can then\nexecute. Beam runners exist currently for Apex, Flink, Spark, and\nGoogle Cloud Dataflow. In SQL terms, you can think of these\nrunners as Beam’s equivalent to the various SQL database\nimplementations, such as Postgres, MySQL, Oracle, and so on.\nThe core vision for Beam is built around its value as a portability layer, and\none of the more compelling features in that realm is its planned support for\nfull cross-language portability.\n Though not yet fully complete \n(but \nlanding\nimminently\n), the plan is for Beam to provide sufficiently performant\nabstraction layers between SDKs and runners that will allow for a full cross-\nproduct of SDK × runner matchups. In such a world, a pipeline written in a\nJavaScript SDK could seamlessly execute on a runner written in Haskell,\neven if the Haskell runner itself had no native ability to execute JavaScript\ncode.\nAs an abstraction layer, the way that Beam positions itself relative to its\nrunners is critical to ensure that Beam actually brings value to the\ncommunity, rather than introducing just an unnecessary layer of abstraction.\nThe key point here is that Beam aims to never be just the intersection (lowest\ncommon denominator) or union (kitchen sink) of the features found in its\nrunners.\n Instead, it aims to include only the best ideas across the data\nprocessing community at large. This allows for innovation in two\ndimensions:\nInnovation in Beam\nFigure 10-34. \nPowerful and modular I/O\nBeam might include API support for runtime features that not all runners\ninitially support. This is okay. Over time, we expect many runners will\nincorporate such features into future versions; those that don’t will be a\nless-attractive runner choice for use cases that need such features.\nAn example here is Beam’s SplittableDoFn API for writing composable,\nscalable sources (described by Eugene Kirpichov in his post \n“Powerful\nand modular I/O connectors with Splittable DoFn in Apache Beam”\n[\nFigure 10-34\n]). It’s both unique and extremely powerful but also does\nnot yet see broad support across all runners for some of the more\ninnovative parts like dynamic work rebalancing. Given the value such\nfeatures bring, however, we expect that will change over time.\nInnovation in runners\nRunners might introduce runtime features \nfor which Beam does not\ninitially provide API support. This is okay. Over time, runtime features\nthat have proven their usefulness will have API support incorporated into\nBeam.\nAn example here is the state snapshotting mechanism in Flink, or\nsavepoints, which we discussed earlier. Flink is still the only publicly\navailable streaming system to support snapshots in this way, but there’s a\nproposal in Beam to provide an API around snapshots because we believe\ngraceful evolution of pipelines over time is an important feature that will\nbe valuable across the industry. If we were to magically push out such an\nAPI today, Flink would be the only runtime system to support it. But\nagain, that’s okay. The point here is that the industry as a whole will\nbegin to catch up over time as the value of these features becomes clear.\nAnd that’s better for everyone.\nBy encouraging innovation within both Beam itself as well as runners, we\nhope to push forward the capabilities of the entire industry at a greater pace\nover time, without accepting compromises along the way. And by delivering\non the promise of portability across runtime execution engines, we hope to\nestablish Beam as the common language for expressing programmatic data\nprocessing pipelines, similar to how SQL exists today as the common\ncurrency of declarative data processing. It’s an ambitious goal, and as of\nwriting, we’re still a ways off from seeing it fully realized, but we’ve also\n12",6487
99-Summary.pdf,99-Summary,"come a long way so far.\nSummary\nWe just took a whirlwind tour through a decade and a half of advances in\ndata processing technology, with a focus on the contributions that made\nstreaming systems what they are today. To summarize one last time, the main\ntakeaways for each system were:\nMapReduce—scalability and simplicity\nBy providing a simple set of abstractions for data \nprocessing on top of a\nrobust and scalable \nexecution engine, MapReduce allowed data engineers\nto focus on the business logic of their data processing needs rather than\nthe gnarly details of building distributed systems resilient to the failure\nmodes of commodity \nhardware\n.\nHadoop—open source ecosystem\nBy building an open source\n platform on the ideas of MapReduce, Hadoop\ncreated a thriving ecosystem that expanded well beyond the scope of its\nprogenitor and allowed a multitude of new ideas to flourish.\nFlume—pipelines, optimization\nBy coupling a high-level notion of logical pipeline\n operations with an\nintelligent optimizer, Flume made it possible to write clean and\nmaintainable pipelines whose capabilities extended beyond the Map →\nShuffle → Reduce confines of MapReduce, without sacrificing any of the\nperformance theretofore gained by contorting the logical pipeline via\nhand-tuned manual optimizations.\nStorm—low latency with weak consistency\nBy sacrificing correctness of \nresults in favor of decreased latency, Storm\nbrought stream processing to the masses and also ushered in the era of the\nLambda Architecture, where weakly consistent stream processing engines\nwere run alongside strongly consistent batch systems to realize the true\nbusiness goal of low-latency, eventually consistent results.\nSpark—strong consistency\nBy utilizing repeated runs of a strongly \nconsistent batch engine to provide\ncontinuous processing of unbounded datasets, Spark Streaming proved it\npossible to have both correctness and low-latency results, at least for in-\norder datasets.\nMillWheel—out-of-order processing\nBy coupling strong consistency\n and exactly-once processing with tools\nfor reasoning about time like watermarks and timers, MillWheel\nconquered the challenge of robust stream processing over out-of-order\ndata.\nKafka—durable streams, streams and tables\nBy applying the concept of a durable\n log to the problem of streaming\ntransports, Kafka brought back the warm, fuzzy feeling of replayability\nthat had been lost by ephemeral streaming transports like RabbitMQ and\nTCP sockets. And by popularizing the ideas of stream and table theory, it\nhelped shed light on the conceptual underpinnings of data processing in\ngeneral.\nCloud Dataflow—unified batch plus streaming\nBy melding the out-of-order stream\n processing concepts from MillWheel\nwith the logical, automatically optimizable pipelines of Flume, Cloud\nDataflow provided a unified model for batch plus streaming data\nprocessing that provided the flexibility to balance the tensions between\ncorrectness, latency, and cost to match any given use case.\nFlink—open source stream processing innovator\nBy rapidly bringing \nthe power of out-of-order processing to the world of\nopen source and combining it with innovations of their own like\ndistributed snapshots and its related savepoints features, Flink raised the\nbar for open source stream processing and helped lead the current charge\nof stream processing innovation across the industry.\nBeam—portability\nBy providing a robust abstraction layer that incorporates the best ideas\nfrom across the industry,\n Beam provides a portability layer positioned as\nthe programmatic equivalent to the declarative lingua franca provided by\nSQL, while also encouraging the adoption of innovative new ideas\nthroughout the industry.\nTo be certain, these 10 projects and the sampling of their achievements that\nI’ve highlighted here do not remotely encompass the full breadth of the\nhistory that has led the industry to where it exists today. But they stand out to\nme as important and noteworthy milestones along the way, which taken\ntogether paint an informative picture of the evolution of stream processing\nover the past decade and a half. We’ve come a long way since the early days\nof MapReduce, with a number of ups, downs, twists, and turns along the way.\nEven so, there remains a long road of open problems ahead of us in the realm\nof streaming systems. I’m excited to see what the future holds.\n Which means I’m skipping a ton of the academic literature around stream\nprocessing, because that’s where much of it started. If you’re really into\nhardcore academic papers on the topic, start from the references in \n“The\nDataflow Model” paper\n and work backward. You should be able to find your\nway pretty easily.\n Certainly, MapReduce itself was built upon many ideas that had been well\nknown before, as is even explicitly stated in the MapReduce paper. That\ndoesn’t change the fact that MapReduce was the system that tied those ideas\ntogether (along with some of its own) to create something practical that\nsolved an important and emerging problem better than anyone else before\never had, and in a way that inspired generations of data-processing systems\nthat followed.\n To be clear, Google was most certainly not the only company tackling data\nprocessing problems at this scale at the time. Google was just one among a\n1\n2\n3\nnumber of companies involved in that first generation of attempts at taming\nmassive-scale data processing.\n And to be clear, MapReduce actually built upon the Google File System,\nGFS, which itself solved the scalability and fault-tolerance issues for a\nspecific subset of the overall problem.\n Not unlike the query optimizers long used in the database world.\n Noogler == New + Googler == New hires at Google\n As an aside, I also highly recommend reading Martin Kleppmann’s \n“A\nCritique of the CAP Theorem”\n for very nice analysis of the shortcomings of\nthe CAP theorem itself, as well as a more principled alternative way of\nlooking at the same problem.\n For the record, written primarily by Sam McVeety with help from Reuven\nand bits of input from the rest of us on the author list; we shouldn’t have\nalphabetized that author list, because everyone always assumes I’m the\nprimary author on it, even though I wasn’t.\n Kafka Streams and now KSQL are of course changing that, but those are\nrelatively recent developments, and I’ll be focusing primarily on the Kafka of\nyore.\n While I recommend the book as the most comprehensive and cohesive\nresource, you can find much of the content from it scattered across O’Reilly’s\nwebsite if you just search around for Kreps’ articles. Sorry, Jay...\n As with many broad generalizations, this one is true in a specific context,\nbut belies the underlying complexity of reality. As I alluded to in \nChapter 1\n,\nbatch systems go to great lengths to optimize the cost and runtime of data\nprocessing pipelines over bounded datasets in ways that stream processing\nengines have yet to attempt to duplicate. To imply that modern batch and\nstreaming systems only differ in one small way is a sizeable\noversimplification in any realm beyond the purely conceptual.\n There’s an additional subtlety here that’s worth calling out: even as runners\nadopt new semantics and tick off feature checkboxes, it’s not the case that\n4\n5\n6\n7\n8\n9\n10\n11\n12\nyou can blindly choose any runner and have an identical experience. This is\nbecause the runners themselves can still vary greatly in their runtime and\noperational characteristics. Even for cases in which two given runners\nimplement the same set of semantic features within the Beam Model, the way\nthey go about executing those features at runtime is typically very different.\nAs a result, when building a Beam pipeline, it’s important to do your\nhomework regarding various runners, to ensure that you choose a runtime\nplatform that serves your use case best.",7981
100-Index.pdf,100-Index,"Index\nA\naccumulating and retracting mode\n, \nHow: Accumulation\n, \nHow: Accumulation\n,\nHow: accumulation\nearly/on-time/late triggers\n, \nHow: Accumulation\naccumulating mode (accumulation)\n, \nHow: Accumulation\n, \nHow:\nAccumulation\n, \nHow: accumulation\naccumulation\n, \nRoadmap\n, \nHow: Accumulation\n-\nHow: Accumulation\naccumulating and retracting mode\n, \nHow: Accumulation\naccumulating mode\n, \nHow: Accumulation\naccumulation mode in processing-time window via ingress time\n,\nProcessing-Time Windowing via Ingress Time\ndiscarding mode\n, \nHow: Accumulation\nin processsing-time windowing via triggers\n, \nProcessing-Time Windowing\nvia Triggers\nin streaming SQL\n, \nHow: accumulation\ndiscarding mode\n, \nDiscarding mode, or lack thereof\nretractions\n, \nRetractions in a SQL world\n-\nRetractions in a SQL world\nin streams and tables model\n, \nHow: Accumulation\nside-by-side comparison of modes\n, \nHow: Accumulation\naccumulators\n, \nIncremental Combining\naccuracy\nin lambda architecture processing\n, \nWhy Exactly Once Matters\nvs. completeness in exactly-once processing\n, \nAccuracy Versus\nCompleteness\n-\nProblem Definition\naggregations\ngrouping and summation via incremental combination\n, \nIncremental\nCombining\nincrementalization of\n, \nIncremental Combining\nparallelization of\n, \nIncremental Combining\nproperties of\n, \nIncremental Combining\naligned delays (processing time in triggers)\n, \nWhen: The Wonderful Thing\nAbout Triggers Is Triggers Are Wonderful Things!\nallowed lateness\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\n-\nWhen:\nAllowed Lateness (i.e., Garbage Collection)\nANTI joins\n, \nANTI\nApache Beam\n, \nBeam\n-\nSummary\nblending of batch and streaming\n, \nWhen: Triggers\ncode snippets for\n, \nThe What, Where, When, and How of Data Processing\nJava SDK pseudo-code\n, \nWhat: Transformations\nCombineFn API\n, \nIncremental Combining\ncomponents\n, \nBeam\nconversion attribution with\n, \nConversion Attribution with Apache Beam\n-\nConversion Attribution with Apache Beam\ninnovation in\n, \nBeam\nportability layer\n, \nBeam\nstreaming SQL in\n, \nWhat Is Streaming SQL?\nApache Calcite\n, \nWhat Is Streaming SQL?\nApache Flink\n, \nOn the Greatly Exaggerated Limitations of Streaming\n, \nWhat Is\nStreaming SQL?\n, \nFlink\n-\nFlink\n, \nSummary\nadoption of Dataflow/Beam programming model\n, \nFlink\nconsistency mechanism\n, \nFlink\nend-to-end exactly once\n, \nProblem Definition\nexactly-once processing in\n, \nApache Flink\nfactors in its rapid rise to stream procesing juggernaut\n, \nFlink\nhighly efficient snapshotting implementation\n, \nFlink\nimpressive performance of\n, \nFlink\nsavepoints feature\n, \nFlink\nsnapshotting paper\n, \nOn the Greatly Exaggerated Limitations of Streaming\nwatermarks in, case study\n, \nCase Study: Watermarks in Apache Flink\nApache Kafka\n, \nExactly Once in Sources\n, \nKafka\n-\nKafka\n, \nSummary\napplication of durability and replayability to stream data\n, \nKafka\ncapabilities of\n, \nKafka\nKafka's Streams API\n, \nKafka\npopularization of stream and table theory\n, \nKafka\nApache Spark\n, \nSpark\n-\nSpark\n, \nSummary\ncurrent developments in\n, \nSpark\nend-to-end exactly once\n, \nProblem Definition\nSpark Streaming\n, \nApache Spark Streaming\n, \nSpark\ningress times as event times\n, \nWhen/Where: Processing-Time Windows\nmanually building up sessions in\n, \nWhere: Session Windows\nsnapshotting paper\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\nApache Storm\n, \nStorm\n-\nStorm\n, \nSummary\nbringing low-latency data processing to the masses\n, \nStorm\nhistory of its creation\n, \nStorm\nappend-only logs\n, \nStream-and-Table Basics Or: a Special Theory of Stream\nand Table Relativity\napproximation algorithms\n, \nApproximation algorithms\nAS OF SYSTEM TIME construct (SQL)\n, \nStreams and Tables\nassignment (window)\n, \nWhere: Custom Windowing\nin fixed windows\n, \nVariations on Fixed Windows\nin session windows\n, \nVariations on Session Windows\nin streams and tables model\n, \nWhere: Windowing\nassociativity\n, \nIncremental Combining\nat least once guarantee\n, \nEnsuring Exactly Once in Shuffle\nB\nbase subscription (Google Cloud Pub/Sub case study)\n, \nCase Study: Source\nWatermarks for Google Cloud Pub/Sub\nbatch processing\nblending with streaming\n, \nWhen: Triggers\ncommonalities between batch and streaming\n, \nCloud Dataflow\nevent-time and processing-time view of\n, \nWhat: Transformations\npersistent state in\n, \nThe Inevitability of Failure\nstreams and tables view\n, \nTemporal Operators\nstreams and tables view of windowed summation on batch engine\n, \nWhere:\nWindowing\nunified batch plus streaming programming model\n, \nCloud Dataflow\n, \nBeam\nvs. streams and tables\n, \nBatch Processing Versus Streams and Tables\n-\nReconciling with Batch Processing\nreconciling the two\n, \nReconciling with Batch Processing\nbatch systems\n, \nOn the Greatly Exaggerated Limitations of Streaming\nbounded data processing with\n, \nBounded Data\nprocessing of unbounded data\n, \nUnbounded Data: Batch\nprocessing state and output in example mobile game with user scores\n,\nWhat: Transformations\nstreaming systems providing superset of\n, \nOn the Greatly Exaggerated\nLimitations of Streaming\nBeam Model\n, \nThe What, Where, When, and How of Data Processing\n,\nStreams and Tables\n(\nsee also\n Apache Beam)\ncorrect implementation to produce accurate results\n, \nExactly-Once and Side\nEffects\nholistic view of streams and tables in\n, \nA Holistic View of Streams and\nTables in the Beam Model\n-\nA Holistic View of Streams and Tables in the\nBeam Model\nrelationship to streams and tables model\n, \nA Holistic View of Streams and\nTables in the Beam Model\nwindowing in\n, \nSummary\nBloom filters\n, \nBloom Filters\nbounded data\n, \nTerminology: What Is Streaming?\nprocessing\n, \nBounded Data\nbounded datasets\nrecomputation on failure\n, \nThe Inevitability of Failure\nbounded sessions\n, \nBounded sessions\nbuffering\nin event-time windows\n, \nWindowing by event time\nC\nCalcite\n (\nsee\n Apache Calcite)\nCAP theorem\n, \nStorm\ncardinality\nin unwindowed joins\n, \nUnwindowed Joins\nSEMI join\n, \nSEMI\nof datasets\n, \nTerminology: What Is Streaming?\n, \nStreams and Tables\nreducing for a stream\n, \nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nChandy Lamport distributed snapshots\n, \nSummary\n, \nFlink\ncheckpointing\nin bounded datasets on batch processing pipelines\n, \nThe Inevitability of\nFailure\nin Flink\n, \nFlink\nin processing of unbounded datasets\n, \nThe Inevitability of Failure\npartial progress within a pipeline\n, \nCorrectness and Efficiency\npersistent state over time\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\nuse to make nondeterministic processing deterministic in Dataflow\n,\nAddressing Determinism\nclosure property (relational algebra)\n, \nRelational Algebra\n, \nSummary\nremaining intact when applied to time-varying relations\n, \nTime-Varying\nRelations\nCloud Dataflow\n, \nThe What, Where, When, and How of Data Processing\n,\nCloud Dataflow\n-\nCloud Dataflow\n, \nSummary\nbalancing correctness, latency, and cost\n, \nCloud Dataflow\nDataflow Model paper\n, \nCloud Dataflow\nexactly-once processing in\n, \nExactly-Once and Side Effects\nserverless aspect of\n, \nCloud Dataflow\nunified batch and streaming programming model\nkey aspects of\n, \nCloud Dataflow\nunified batch plus streaming programming model\n, \nCloud Dataflow\nwatermarks in, case study\n, \nCase Study: Watermarks in Google Cloud\nDataflow\n-\nCase Study: Watermarks in Google Cloud Dataflow\nCloud Pub/Sub\n, \nHeuristic Watermark Creation\nas example source\n, \nExample Source: Cloud Pub/Sub\nas nondeterministic source\n, \nExactly Once in Sources\nwatermarks for, case study\n, \nCase Study: Source Watermarks for Google\nCloud Pub/Sub\n-\nCase Study: Source Watermarks for Google Cloud\nPub/Sub\ncombination, incremental\n (\nsee\n incremental combination)\nCombineFn class (Beam)\n, \nIncremental Combining\ncombiner lifting optimization\n, \nGraph Optimization\n, \nFlume\ncommutativity\n, \nIncremental Combining\ncompleteness\naccuracy vs., in exactly-once processing\n, \nAccuracy Versus Completeness\nconcept provided by watermarks\n, \nDefinition\ndrawback of event-time windows in\n, \nWindowing by event time\nwatermarks for reasoning about input completeness\n, \nCloud Dataflow\nwatermarks giving notion of\n, \nWhen: Watermarks\ncompleteness triggers\n, \nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nwatermarks\n, \nWhen: Watermarks\ncomplexity in lambda architecture processing\n, \nWhy Exactly Once Matters\nconsistency\nconsistency mechanism in Flink\n, \nFlink\nstrong consistency for exactly-once processing\n, \nOn the Greatly\nExaggerated Limitations of Streaming\nconstitution of a dataset\n, \nTerminology: What Is Streaming?\n, \nStreams and\nTables\nconversion attribution\n, \nCase Study: Conversion Attribution\n-\nCase Study:\nConversion Attribution\nwith Apache Beam\n, \nConversion Attribution with Apache Beam\n-\nConversion Attribution with Apache Beam\ncorrectness\nApache Storm and\n, \nStorm\nbalancing with latency and cost in Cloud Dataflow\n, \nCloud Dataflow\nin batch and streaming systems\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\nMillWheel and\n, \nMillWheel\npersistent stte as basis for\n, \nCorrectness and Efficiency\nsupposed limitations of streaming systems\n, \nOn the Greatly Exaggerated\nLimitations of Streaming\ncustom windowing\n, \nWhere: Custom Windowing\n-\nOne Size Does Not Fit All\nbenefits of\n, \nOne Size Does Not Fit All\nvariations on fixed windows\n, \nVariations on Fixed Windows\n-\nPer-\nelement/key fixed windows\nper-element/key fixed windows\n, \nPer-element/key fixed windows\nunaligned fixed windows\n, \nUnaligned fixed windows\nvariations on session windows\n, \nVariations on Session Windows\n-\nBounded\nsessions\nbounded sessions\n, \nBounded sessions\nD\ndata processing\ndifficulty of\n, \nMapReduce\nwhat, where, when, and how of\n, \nThe What, Where, When, and How of\nData Processing\n-\nSummary\n, \nCloud Dataflow\nwhat, transformations\n, \nWhat: Transformations\n-\nWhat: Transformations\nwhen and how in streaming systems\n, \nGoing Streaming: When and How\n-\nHow: Accumulation\nwhere, windowing\n, \nWhere: Windowing\n-\nWhere: Windowing\ndata processing patterns\n, \nData Processing Patterns\n-\nWindowing by event time\nbounded data\n, \nBounded Data\nunbounded data, batch processing of\n, \nUnbounded Data: Batch\nunbounded data, streaming\n, \nUnbounded Data: Streaming\n-\nWindowing by\nevent time\ndata processing, large scale\n (\nsee\n large-scale data processing, evolution of)\ndata types, flexibility in\n, \nGeneralized State\n, \nConversion Attribution with\nApache Beam\ndata-driven triggers\n, \nData-driven triggers\ndata-driven windows, sessions as example\n, \nWhere: Session Windows\ndatabase systems\n, \nStream-and-Table Basics Or: a Special Theory of Stream\nand Table Relativity\nDataflow\n (\nsee\n Google Cloud Dataflow)\nDataflow Model\n, \nThe What, Where, When, and How of Data Processing\nDataflow Model paper\n, \nWindow merging\n, \nCloud Dataflow\ndatasets\ncardinality of\n, \nTerminology: What Is Streaming?\nconstitution of\n, \nTerminology: What Is Streaming?\ndeterminism\naddressing in exactly-once processing\n, \nAddressing Determinism\nin sources\n, \nExactly Once in Sources\nnondeterministic components in side effects\n, \nExactly Once in Sinks\ndiscarding mode (accumulation)\n, \nHow: Accumulation\n, \nHow: Accumulation\nearly/on-time/late triggers\n, \nHow: Accumulation\nin streaming SQL\n, \nDiscarding mode, or lack thereof\ndomain specific languages (DSLs)\n, \nBeam\nDSLs (domain specific languages)\n, \nBeam\nduplicated work, minimizing with persistent state\n, \nCorrectness and Efficiency\nduplicates, detecting in shuffle\n, \nEnsuring Exactly Once in Shuffle\ndynamic windows\n, \nHow: Accumulation\n(\nsee also\n sessions)\ndynamic work rebalancing (or liquid sharding)\n, \nFlume\nE\nearly/on-time/late triggers\n, \nWhen: Early/On-Time/Late Triggers FTW!\n-\nWhen: Early/On-Time/Late Triggers FTW!\naccumulating and retracting mode version\n, \nHow: Accumulation\ndiscarding mode version\n, \nHow: Accumulation\nearly panes\n, \nWhen: Early/On-Time/Late Triggers FTW!\nin bounded session window\n, \nBounded sessions\nin streams and tables model\n, \nWhen: Triggers\nlate panes\n, \nWhen: Early/On-Time/Late Triggers FTW!\non-time pane\n, \nWhen: Early/On-Time/Late Triggers FTW!\nwatermark trigger with late firing in streaming SQL\n, \nWatermark triggers\nwith allowed lateness\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\n-\nWhen: Allowed Lateness (i.e., Garbage Collection)\nwith session windows and retractions\n, \nWhere: Session Windows\nefficiency\ninefficiencies in MapReduce jobs\n, \nFlume\npersistent data, minimizing work duplicated and data persistend\n,\nCorrectness and Efficiency\nend-to-end exactly once\n, \nProblem Definition\n, \nKafka\nevent time\ndistribution of messages by\n, \nDefinition\nin SQL table UserScores (example)\n, \nWhat: Transformations\nin streaming SQL\n, \nTemporal Operators\nskew and watermarks\n, \nWhen: Watermarks\nview of batch processing\n, \nWhat: Transformations\nvs. processing time\n, \nEvent Time Versus Processing Time\nwatermarks\n, \nRoadmap\n, \nProcessing-Time Watermarks\nwindowing based on\n, \nWhen/Where: Processing-Time Windows\nwindowing by\n, \nWindowing by event time\ndrawbacks of\n, \nWindowing by event time\nevent-time windowing\nover two different processing-time orderings of same input\n, \nEvent-Time\nWindowing\nreasons for using in processing-time windowing\n, \nProcessing-Time\nWindowing via Ingress Time\nexactly-once processing\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\n, \nExactly-Once and Side Effects\n-\nSummary\n, \nMillWheel\naccuracy vs. completeness\n, \nAccuracy Versus Completeness\n, \nProblem\nDefinition\nproblem definition\n, \nProblem Definition\nside effects\n, \nSide Effects\ndeterminism and\n, \nAddressing Determinism\nend-to-end\n, \nKafka\nensuring exactly once in shuffles\n, \nEnsuring Exactly Once in Shuffle\nin Apache Flink\n, \nApache Flink\n, \nFlink\nin Apache Spark Streaming\n, \nOther Systems\nin conversion attribution pipeline\n, \nCase Study: Conversion Attribution\nperformance\n, \nPerformance\n-\nGarbage Collection\ngarbage collection\n, \nGarbage Collection\ngraph optimization\n, \nGraph Optimization\noptimization using Bloom filters\n, \nBloom Filters\nuse cases\nexample sink, files\n, \nExample Sink: Files\nexample sink, Google BigQuery\n, \nExample Sink: Google BigQuery\nexample source, Cloud Pub/Sub\n, \nUse Cases\nwhy exactly once matters\n, \nWhy Exactly Once Matters\nF\nfailures, inevitability of\n, \nThe Inevitability of Failure\nfault-tolerance in large-scale data processing\n, \nMapReduce\nfiles, using as sinks\n, \nExample Sink: Files\nfiltering\n, \nFiltering\nfiltering relation (WHERE clause), time-varying relation applied to\n, \nTime-\nVarying Relations\nfixed windows\n, \nWindowing\nunbounded data processing via in batch systems\n, \nFixed windows\nvariations on\n, \nVariations on Fixed Windows\n-\nPer-element/key fixed\nwindows\nper-element/key fixed windows\n, \nPer-element/key fixed windows\nunaligned fixed windows\n, \nUnaligned fixed windows\nwindowed joins in\n, \nFixed Windows\n-\nFixed Windows\nflexibility\nflexible triggering and accumulation modes\n, \nCloud Dataflow\nneeds in streaming persistent state\n, \nGeneralized State\nin conversion attribution using Apache Beam\n, \nConversion Attribution\nwith Apache Beam\nshortcomings of implicit approaches\n, \nGeneralized State\nFlume\n, \nFlume\n-\nFlume\n, \nSummary\ncombined batch and streaming approach in\n, \nCloud Dataflow\ncombiner lifting optimization\n, \nFlume\ndynamic work rebalancing (or liquid sharding)\n, \nFlume\nextension to support streaming semantics\n, \nFlume\nFlumeJava paper\n, \nFlume\nfusion optimizations\n, \nFlume\nhigh-level pipelines in\n, \nFlume\nmigration away from MapReduce to Dax execution engine\n, \nFlume\nMillWheel integration with\n, \nMillWheel\noptimization of MapReduce jobs\n, \nFlume\nFlumeJava\n, \nFlume\n(\nsee also\n Flume)\nFULL OUTER joins\n, \nFULL OUTER\n, \nUnwindowed Joins\n, \nTemporal validity\njoins\nfusion optimization on pipeline graph\n, \nGraph Optimization\n, \nFlume\nG\ngarbage collection\nbits of persistent state not needed\n, \nCorrectness and Efficiency\nin exactly-once processing\n, \nGarbage Collection\ngeneralized state\n, \nGeneralized State\n-\nConversion Attribution with Apache\nBeam\ncase study, conversion attribution\n, \nCase Study: Conversion Attribution\n-\nCase Study: Conversion Attribution\nflexibility in\n, \nSummary\nin conversion attribution\n, \nCase Study: Conversion Attribution\n-\nCase Study:\nConversion Attribution\nin conversion attribution using Apache Beam\n, \nConversion Attribution with\nApache Beam\n-\nConversion Attribution with Apache Beam\nGoogle BigQuery, use as a sink\n, \nExample Sink: Google BigQuery\nGoogle Cloud Dataflow\n (\nsee\n Cloud Dataflow; Dataflow Model)\nGoogle Cloud Pub/Sub\n (\nsee\n Cloud Pub/Sub)\nGoogle technologies in large-scale data processing\n, \nThe Evolution of Large-\nScale Data Processing\ngraph optimization in Dataflow\n, \nGraph Optimization\nGROUP BY statement with HAVING clause (SQL)\n, \nThe SQL Model: A\nTable-Biased Approach\ngrouping operations\ngrouping via incremental combination\n, \nIncremental Combining\ngrouping/ungrouping in Beam Model\n, \nThe Beam Model: A Stream-Biased\nApproach\ngrouping/ungrouping in SQL\n, \nThe SQL Model: A Table-Biased Approach\nin materialized views\n, \nMaterialized views\ngrouping/ungrouping in streams and tables\n, \nWhen: Triggers\nin Beam Model processing\n, \nA Holistic View of Streams and Tables in\nthe Beam Model\njoins as\n, \nAll Your Joins Are Belong to Streaming\nraw grouping of inputs\n, \nRaw Grouping\ngrouping relation, time-varying relation applied to\n, \nTime-Varying Relations\ngrouping transformations\n, \nWhat: Transformations\nH\nHadoop\n, \nHadoop\n, \nSummary\nSpark as successor to\n, \nSpark\nHAVING clause in GROUP BY statment (SQL)\n, \nThe SQL Model: A Table-\nBiased Approach\nHDFS (Hadoop Distributed File System)\n, \nHadoop\nHeron\n, \nStorm\nheuristic watermarks\n, \nWhen: Watermarks\n, \nSource Watermark Creation\nallowed lateness and\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\napplying to same dataset with a perfect watermark\n, \nWhen: Watermarks\ncreation of\n, \nHeuristic Watermark Creation\nfrom dynamic sets of time-ordered logs\n, \nHeuristic Watermark Creation\nfrom Google Cloud Pub/Sub\n, \nHeuristic Watermark Creation\nearly/on-time/late triggers and\n, \nWhen: Early/On-Time/Late Triggers FTW!\nhigh volumes of data, handling in conversion attribution pipeline\n, \nCase\nStudy: Conversion Attribution\nhopping windows\n (\nsee\n sliding windows)\nI\nimplicit state\n, \nImplicit State\n, \nIncremental Combining\nimplicit tables in SQL\n, \nThe SQL Model: A Table-Biased Approach\nin-band watermarks\n, \nCase Study: Watermarks in Apache Flink\ninaccuracy problems in lambda architecture\n, \nWhy Exactly Once Matters\ninconsistency in lambda architecture processing\n, \nWhy Exactly Once Matters\nincremental combination\n, \nIncremental Combining\n-\nIncremental Combining\n,\nSummary\nincrementalization of aggregations\n, \nIncremental Combining\ningress time\nprocessing-time windowing via\n, \nProcessing-Time Windowing via Ingress\nTime\n-\nProcessing-Time Windowing via Ingress Time\nuse in achieving processing-time windowing\n, \nWhen/Where: Processing-\nTime Windows\ningress timestamping, watermark creation by\n, \nPerfect Watermark Creation\ninner joins\n, \nInner joins\nINNER joins\n, \nINNER\n, \nTemporal validity joins\ninput completeness\n, \nCloud Dataflow\ninput tables (SQL)\n, \nThe SQL Model: A Table-Biased Approach\n, \nMaterialized\nviews\ninput watermarks\n, \nWatermark Propagation\nfor Average Session Lengths stage\n, \nUnderstanding Watermark Propagation\nJ\nJava pseudo-code in Apache Beam examples\n, \nWhat: Transformations\njoins\n, \nInner joins\n, \nStreaming Joins\n(\nsee also\n inner joins)\n(\nsee also\n streaming joins)\nall joins as streaming joins\n, \nAll Your Joins Are Belong to Streaming\nK\nkappa architecture\n, \nOn the Greatly Exaggerated Limitations of Streaming\nkeys, values, windows and partitioning in Beam Model\n, \nA Holistic View of\nStreams and Tables in the Beam Model\nL\nlambda architecture\n, \nOn the Greatly Exaggerated Limitations of Streaming\n,\nSummary\n, \nStorm\nissues with\n, \nWhy Exactly Once Matters\nusing Spark Streaming instead of\n, \nSpark\nlarge-scale data processing, evolution of\n, \nThe Evolution of Large-Scale Data\nProcessing\n-\nSummary\nApache Beam\n, \nBeam\n-\nBeam\nApache Kafka\n, \nKafka\n-\nKafka\nApache Storm\n, \nStorm\n-\nStorm\nCloud Dataflow\n, \nCloud Dataflow\n-\nCloud Dataflow\nFlume\n, \nFlume\n-\nFlume\nHadoop\n, \nHadoop\nMapReduce\n, \nMapReduce\n-\nMapReduce\nMillWheel\n, \nMillWheel\n-\nMillWheel\ntimeline of systems discussed\n, \nThe Evolution of Large-Scale Data\nProcessing\nlate data\nand heuristic watermarks\n, \nHeuristic Watermark Creation\nand perfect watermarks\n, \nPerfect Watermark Creation\nlate panes\n, \nWhen: Early/On-Time/Late Triggers FTW!\nlatency\nbalancing with correctness and cost in Cloud Dataflow\n, \nCloud Dataflow\nimprovements in Apache Storm\n, \nStorm\nin lambda architecture processing\n, \nWhy Exactly Once Matters\nlow-latency and eventually correct results with lambda architecture\n, \nStorm\nsystem vs. data, distinguishing with processing-time watermarks\n,\nProcessing-Time Watermarks\nlateness (allowed)\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\n-\nWhen: Allowed Lateness (i.e., Garbage Collection)\nLEFT OUTER joins\n, \nLEFT OUTER\nliquid sharding\n, \nFlume\nlogical abstraction of execution environment\n, \nCloud Dataflow\nlogical vs. physical operations in Beam Model\n, \nA Holistic View of Streams\nand Tables in the Beam Model\nand how they relate to streams and tables\n, \nA Holistic View of Streams and\nTables in the Beam Model\nlogs\ndymanic sets of time-ordered logs, watermark creation from\n, \nHeuristic\nWatermark Creation\nstatic sets of time-ordered logs, creation of watermarks\n, \nPerfect Watermark\nCreation\nM\nMapReduce\n, \nThe Evolution of Large-Scale Data Processing\n-\nMapReduce\n,\nSummary\nCombiners\n, \nIncremental Combining\nfunctionality of overall Map and Reduce phases\n, \nMapReduce\nhistory of massive-scale sorting experiments at Google\n, \nMapReduce\nMapReduce paper\n, \nMapReduce\nshortcomings of\n, \nFlume\nsimplicity and scalability\n, \nMapReduce\nstages answering what questions\n, \nWhat: Transformations\nstreams and tables analysis of\n, \nA Streams and Tables Analysis of\nMapReduce\n-\nReconciling with Batch Processing\nMap as streams/tables\n, \nMap as streams/tables\nReduce as streams/tables\n, \nReduce as streams/tables\nvisualization of a job\n, \nMapReduce\nmaterialized views\n, \nStream-and-Table Basics Or: a Special Theory of Stream\nand Table Relativity\n, \nMaterialized views\n-\nMaterialized views\n, \nUnwindowed\nJoins\nmerging windows\n, \nWhere: Custom Windowing\n, \nWhere: Windowing\nin parallelized aggregations\n, \nIncremental Combining\nin session windows\n, \nVariations on Session Windows\nin streams and tables model\n, \nWindow merging\nno merging in fixed windows\n, \nVariations on Fixed Windows\nmicrobatch vs. true streaming debate\n, \nSpark\nMillWheel\n, \nMillWheel\n-\nMillWheel\n, \nSummary\nfault-tolerant stream processing at internet scale\n, \nMillWheel\nFlume and\n, \nFlume\nlow-latency, strongly consistent processing of unbounded, out-of-order\ndata\n, \nMillWheel\nMillWheel paper\n, \nOn the Greatly Exaggerated Limitations of Streaming\nN\nnetwork remnants\n, \nGarbage Collection\nnongrouping operations\n, \nA General Theory of Stream and Table Relativity\n,\nSummary\nnongrouping transformations\n, \nWhat: Transformations\nO\nOLTP (Online Transaction Processing) tables\nSTREAM queries and\n, \nStreams and Tables\non-time pane\n, \nWhen: Early/On-Time/Late Triggers FTW!\nopen source ecosystem, Hadoop and\n, \nHadoop\noperators (relational algebra)\napplied to time-varying relations\n, \nTime-Varying Relations\napplying to valid relations\n, \nRelational Algebra\nout-of-band watermark aggregation\n, \nCase Study: Watermarks in Apache\nFlink\nout-of-order data\nhandling in conversion attribution pipeline\n, \nCase Study: Conversion\nAttribution\n-\nCase Study: Conversion Attribution\nin temporal validity windows\n, \nTemporal validity windows\nunbounded, processing in MillWheel\n, \nMillWheel\nouter joins\n, \nWhen: Watermarks\noutput tables (SQL)\n, \nThe SQL Model: A Table-Biased Approach\n,\nMaterialized views\noutput timestamps\nwatermark propagation and\n, \nWatermark Propagation and Output\nTimestamps\n-\nWatermark Propagation and Output Timestamps\nwatermarks and\nwith overlapping windows\n, \nThe Tricky Case of Overlapping Windows\noutput watermarks\n, \nWatermark Propagation\ncomponents of\n, \nWatermark Propagation\nfor Mobile Sessions and Console Sessions stages\n, \nUnderstanding\nWatermark Propagation\nP\nparallelization of aggregations\n, \nIncremental Combining\nper-element/key fixed windows\n, \nPer-element/key fixed windows\npercentile watermarks\n, \nPercentile Watermarks\n-\nPercentile Watermarks\nperfect watermarks\n, \nWhen: Watermarks\n, \nSource Watermark Creation\nallowed lateness and\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\napplying to same dataset with a heuristic watermark\n, \nWhen: Watermarks\ncreation of\n, \nPerfect Watermark Creation\nby ingress timestamping\n, \nPerfect Watermark Creation\nearly/on-time/late triggers and\n, \nWhen: Early/On-Time/Late Triggers FTW!\nperformance\nin exactly-once shuffle delivery\n, \nPerformance\n-\nGarbage Collection\ngarbage collection\n, \nGarbage Collection\ngraph optimization\n, \nGraph Optimization\noptimizing with Bloom filters\n, \nBloom Filters\noptimizing in conversion attribution pipeline\n, \nCase Study: Conversion\nAttribution\npersistent state\n, \nThe Practicalities of Persistent State\n-\nSummary\n, \nMillWheel\ngeneralized state\n, \nGeneralized State\n-\nConversion Attribution with Apache\nBeam\nin conversion attribution\n, \nCase Study: Conversion Attribution\n-\nCase\nStudy: Conversion Attribution\nin conversion attribution using Apache Beam\n, \nConversion Attribution\nwith Apache Beam\n-\nConversion Attribution with Apache Beam\nimplicit state\n, \nImplicit State\n-\nIncremental Combining\nin incremental combining\n, \nIncremental Combining\nraw grouping of inputs\n, \nRaw Grouping\nmotivation for\n, \nMotivation\n-\nCorrectness and Efficiency\ncorrectness and efficiency\n, \nCorrectness and Efficiency\ninevitability of failure\n, \nThe Inevitability of Failure\npersistent timers\n, \nMillWheel\nphysical stages and fusion in Beam Model\n, \nA Holistic View of Streams and\nTables in the Beam Model\npost-declaration of triggers\n, \nThe Beam Model: A Stream-Biased Approach\npredeclaration of triggers\n, \nThe Beam Model: A Stream-Biased Approach\nprocessing time\n, \nWhen: Watermarks\nconversion to event time in watermarks\n, \nWhen: Watermarks\ndelays in triggers\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\nevent time vs.\n, \nEvent Time Versus Processing Time\nin SQL table UserScores (example)\n, \nWhat: Transformations\nin streaming SQL\n, \nTemporal Operators\nshifting input observation order in\n, \nWhen/Where: Processing-Time\nWindows\nview of batch processing\n, \nWhat: Transformations\nwatermarks\n, \nProcessing-Time Watermarks\n-\nProcessing-Time Watermarks\nwindowing by\n, \nWindowing by processing time\nprocessing-time windowing\n, \nSummary\nevent-time windowing comparing two processing-time orderings of same\niput\n, \nEvent-Time Windowing\nvia ingress time\n, \nProcessing-Time Windowing via Ingress Time\n-\nProcessing-Time Windowing via Ingress Time\nvia triggers\n, \nProcessing-Time Windowing via Triggers\n-\nProcessing-Time\nWindowing via Ingress Time\nprocessing-time windows\n, \nAdvanced Windowing\n-\nProcessing-Time\nWindowing via Ingress Time\ndownside to\n, \nWhen/Where: Processing-Time Windows\nvia ingress time\n, \nWhen/Where: Processing-Time Windows\nvia triggers\n, \nWhen/Where: Processing-Time Windows\nPub/Sub\n (\nsee\n Google Cloud Pub/Sub)\nQ\nQuestioning the Lambda Architecture post\n, \nOn the Greatly Exaggerated\nLimitations of Streaming\nR\nraw grouping\n, \nRaw Grouping\n, \nSummary\nmerging windows\n, \nIncremental Combining\nrelational algebra\n, \nRelational Algebra\ndefining time-varying relations in terms of\n, \nTime-Varying Relations\n-\nTime-\nVarying Relations\nrelations (in databases)\n, \nRelational Algebra\nrepeated delay triggers\n, \nRepeated delay triggers\n, \nSummary\nrepeated update triggers\n, \nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nfiring with every new record\n, \nWhen: The Wonderful Thing About Triggers\nIs Triggers Are Wonderful Things!\nreprocessing the input\n, \nThe Inevitability of Failure\nReshuffle transform\n, \nExactly Once in Sinks\n, \nExample Sink: Google\nBigQuery\nresilient distributed datasets (RDDs)\n, \nApache Spark Streaming\n, \nSpark\nretractions (accumulating and retracting mode)\n, \nHow: Accumulation\nin session window\n, \nWhere: Session Windows\nin streaming SQL\n, \nRetractions in a SQL world\n-\nRetractions in a SQL world\nin Sys.Undo column (hypothetical) in streaming SQL\n, \nStreams and Tables\nRIGHT OUTER joins\n, \nRIGHT OUTER\nRPCs (remote procedure calls), use in shuffle and issues with RPCs\n, \nEnsuring\nExactly Once in Shuffle\nrunners in Apache Beam\n, \nBeam\ninnovation in\n, \nBeam\nS\nsavepoints\n, \nFlink\nscalability\nin large-scale data processing\n, \nMapReduce\nin MapReduce\n, \nMapReduce\n, \nSummary\nSCAN-AND-STREAM trigger\n, \nMaterialized views\nscheduling of processing, flexibility in\n, \nGeneralized State\n, \nConversion\nAttribution with Apache Beam\nSDKs (software development kits) in Apache Beam\n, \nBeam\nSELECT statement (SQL), STREAM and TABLE keywords after\n, \nStream\nand Table Selection\nSEMI joins\n, \nSEMI\n-\nUnwindowed Joins\nsession windows\n, \nWhere: Session Windows\n-\nWhere: Session Windows\n,\nSummary\nvariations on\n, \nVariations on Session Windows\n-\nBounded sessions\nbounded sessions\n, \nBounded sessions\nsessions\n, \nSessions\n, \nWindowing\ncalculating length per user across two input pipelines\n, \nUnderstanding\nWatermark Propagation\ninterest from windowing standpoint\n, \nWhere: Session Windows\nmanually building up on Spark Streaming 1.x (blog post)\n, \nWhere: Session\nWindows\nretractions and\n, \nHow: Accumulation\nshuffles in a pipeline\n, \nProblem Definition\n-\nProblem Definition\nensuring exactly once in\n, \nEnsuring Exactly Once in Shuffle\nReshuffle transform\n, \nExactly Once in Sinks\nside effects\nidempotent and robust in replay\n, \nExactly Once in Sinks\nin exactly-once processing\n, \nSide Effects\nsinks\n, \nThe SQL Model: A Table-Biased Approach\nexample sink, files\n, \nExample Sink: Files\nexample sink, Google BigQuery\n, \nExample Sink: Google BigQuery\nsliding windows\n, \nWindowing\nsnapshots\nFlink's highly efficient snapshotting implementation\n, \nFlink\n, \nFlink\nin Apache Flink\n, \nApache Flink\nsource watermarks, creation of\n, \nCase Study: Watermarks in Apache Flink\nsources\n, \nThe SQL Model: A Table-Biased Approach\nexactly-once processing in\n, \nExactly Once in Sources\nexample surce, Cloud Pub/Sub\n, \nExample Source: Cloud Pub/Sub\nspam attacks, protecting against in conversion attribution pipeline\n, \nCase\nStudy: Conversion Attribution\nSpark Streaming\n (\nsee\n Apache Spark Streaming)\nSpark Streaming paper\n, \nOn the Greatly Exaggerated Limitations of Streaming\nSQL\nSpark integration with\n, \nSpark\nsupport for time-varying relations in\n, \nStreams and Tables\ntable-biased approach\n, \nThe SQL Model: A Table-Biased Approach\n-\nMaterialized views\ntypes of joins defined in ANSI SQL\n, \nUnwindowed Joins\nState Management in Apache Flink\n, \nFlink\nStorm\n (\nsee\n Apache Storm)\nSTREAM keyword (hypothetical, in SQL)\n, \nStreams and Tables\n, \nStream and\nTable Selection\nSTREAM queries (hypothetical, in SQL)\nproviding alternate data history to table-based TVR query\n, \nStreams and\nTables\nrelation to OLTP tables\n, \nStreams and Tables\nSys.Undo column referenced from\n, \nStreams and Tables\nstreaming\n, \nStreaming 101\n-\nSummary\ncommonalities between batch and streaming\n, \nCloud Dataflow\ngreatly exaggerated limitations of\n, \nOn the Greatly Exaggerated Limitations\nof Streaming\nmicrobatch vs. true streaming debate\n, \nSpark\nsupport for stream processing in SQL materialized views\n, \nMaterialized\nviews\n-\nMaterialized views\nterminology\n, \nTerminology: What Is Streaming?\n-\nEvent Time Versus\nProcessing Time\nunified batch plus streaming programming model\n, \nCloud Dataflow\n, \nBeam\nZeitgeist pipeline, true streaming use case\n, \nMillWheel\nstreaming joins\n, \nStreaming Joins\n-\nSummary\nunwindowed joins\n, \nUnwindowed Joins\n-\nUnwindowed Joins\nFULL OUTER\n, \nFULL OUTER\nINNER\n, \nINNER\nLEFT OUTER\n, \nLEFT OUTER\nRIGHT OUTER\n, \nRIGHT OUTER\nSEMI\n, \nSEMI\n-\nUnwindowed Joins\nwindowed joins\n, \nWindowed Joins\n-\nWatermarks and temporal validity joins\nfixed windows\n, \nFixed Windows\n-\nFixed Windows\ntemporal validity\n, \nTemporal Validity\n-\nWatermarks and temporal validity\njoins\nstreaming SQL\n, \nStreaming SQL\n-\nSummary\ncomplete definition of\n, \nWhat Is Streaming SQL?\nlooking backward, stream and table biases\n, \nLooking Backward: Stream and\nTable Biases\n-\nMaterialized views\nlooking forward, toward robust streaming\n, \nLooking Forward: Toward\nRobust Streaming SQL\n-\nDiscarding mode, or lack thereof\nstream and table selection\n, \nStream and Table Selection\ntemporal operators\n, \nTemporal Operators\n-\nDiscarding mode, or lack\nthereof\nrelational algebra as theoretical foundation of SQL\n, \nRelational Algebra\ntemporal validity window in\n, \nTemporal validity windows\ntime-varying relations\n, \nTime-Varying Relations\n-\nTime-Varying Relations\nunwindowed joins\nANTI\n, \nANTI\nstreaming systems\n, \nTerminology: What Is Streaming?\nwhen and how of data processing\n, \nGoing Streaming: When and How\n-\nHow:\nAccumulation\nhow, accumulation\n, \nHow: Accumulation\n-\nHow: Accumulation\nwhen, allowed lateness\n, \nWhen: Allowed Lateness (i.e., Garbage\nCollection)\n-\nWhen: Allowed Lateness (i.e., Garbage Collection)\nwhen, early/on-time/late triggers\n, \nWhen: Early/On-Time/Late Triggers\nFTW!\n-\nWhen: Early/On-Time/Late Triggers FTW!\nwhen, triggers\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\n-\nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nwhen, watermarks\n, \nWhen: Watermarks\n-\nWhen: Watermarks\nstreams\n, \nTerminology: What Is Streaming?\nand tables\n, \nStreams and Tables\n-\nSummary\nas data in motion\n, \nToward a General Theory of Stream and Table\nRelativity\npersistent forms of\n, \nCorrectness and Efficiency\ntime-varying relations in\n, \nStreams and Tables\nstreams and tables\nbatch processing vs.\n, \nBatch Processing Versus Streams and Tables\n-\nReconciling with Batch Processing\nhow batch processing fits into stream/table theory\n, \nReconciling with\nBatch Processing\nhow streams relate to bounded/unbounded data\n, \nReconciling with Batch\nProcessing\nstreams and tables analysis of MapReduce\n, \nA Streams and Tables\nAnalysis of MapReduce\n-\nReconciling with Batch Processing\ncomparing classic SQL and Beam Model, looking backward\n, \nLooking\nBackward: Stream and Table Biases\n-\nMaterialized views\nstream-biased approch in Beam Model\n, \nThe Beam Model: A Stream-\nBiased Approach\n-\nThe Beam Model: A Stream-Biased Approach\ntable-biased approach in SQL\n, \nThe SQL Model: A Table-Biased\nApproach\n-\nMaterialized views\nconversions to and from in MapReduce\n, \nMapReduce\ngeneral theory of stream and table relativity\n, \nA General Theory of Stream\nand Table Relativity\n-\nA General Theory of Stream and Table Relativity\nholistic view of in Beam Model\n, \nA Holistic View of Streams and Tables in\nthe Beam Model\n-\nA Holistic View of Streams and Tables in the Beam\nModel\nKafka as embodiment of relationship between\n, \nKafka\npopularization of theory by Apache Kafka\n, \nKafka\nrelationship between Beam Model and\n, \nStreams and Tables\nspecial theory of stream and table relativity\n, \nStream-and-Table Basics Or: a\nSpecial Theory of Stream and Table Relativity\ntable/stream selection for TVRs in streaming SQL\n, \nSummary\ntime-varying relations in\n, \nStreams and Tables\n-\nStreams and Tables\ntoward a general theory of stream and table relativity\n, \nToward a General\nTheory of Stream and Table Relativity\nview of windowed summation\n, \nRaw Grouping\nwhat, where, when, and how of\n, \nWhat, Where, When, and How in a\nStreams and Tables World\nhow, accumulation\n, \nHow: Accumulation\nwhat, transformations\n, \nWhat: Transformations\n-\nWhat: Transformations\nwhen, triggers\n, \nWhen: Triggers\n-\nWhen: Triggers\nwhere, windowing\n, \nWhere: Windowing\n-\nWindow merging\nstrong consistency in a streaming system\n, \nOn the Greatly Exaggerated\nLimitations of Streaming\nsubscriptions in Google Cloud Pub/Sub case study\n, \nCase Study: Source\nWatermarks for Google Cloud Pub/Sub\nsummation via incremental combination\n, \nIncremental Combining\nSys.EmitIndex column (hypothetical, in SQL)\n, \nWatermark triggers\n, \nSummary\nSys.EmitTiming column (hypothetical, in SQL)\n, \nWatermark triggers\n,\nSummary\nSys.MTime column (hypothetical, in SQL)\n, \nTemporal Operators\n, \nSummary\nSys.Undo column (hypothetical, in SQL)\n, \nStreams and Tables\n, \nHow:\naccumulation\n, \nSummary\ndistinguishing between normal rows and rows retracting a previous value\n,\nStreams and Tables\nT\nTABLE keyword (hypothetical, in SQL)\n, \nStreams and Tables\n, \nStream and\nTable Selection\ntables\n, \nTerminology: What Is Streaming?\nas data at rest\n, \nToward a General Theory of Stream and Table Relativity\nconversion of streams from/to in SQL\n, \nThe SQL Model: A Table-Biased\nApproach\nexplicit and implicit in SQL\n, \nThe SQL Model: A Table-Biased Approach\npersistent state\n, \nThe Practicalities of Persistent State\ntable-based TVR vs. STREAM query TVR\n, \nStreams and Tables\ntime-varying relations in\n, \nStreams and Tables\ntables and streams\n (\nsee\n streams and tables)\ntemporal operators (in streaming SQL)\n, \nTemporal Operators\n-\nDiscarding\nmode, or lack thereof\ntriggers\n, \nWhen: triggers\n-\nData-driven triggers\nwindowing\n, \nWhere: windowing\n-\nWhere: windowing\ntemporal tables (SQL)\n, \nStreams and Tables\ntemporal validity\n, \nTemporal Validity\n-\nWatermarks and temporal validity joins\ntemporal validity joins\n, \nTemporal validity joins\n-\nWatermarks and temporal\nvalidity joins\nwatermarks and\n, \nWatermarks and temporal validity joins\ntemporal validity windows\n, \nTemporal validity windows\n-\nTemporal validity\nwindows\ntime\nevent time vs. processing time\n, \nEvent Time Versus Processing Time\npartitioning in windowed joins\n, \nWindowed Joins\ntime-agnostic processing of unbounded data\n, \nTime-agnostic\nfiltering\n, \nFiltering\ninner joins\n, \nInner joins\ntools for reasoning about\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\ntime-varying relations\n, \nTime-Varying Relations\n-\nTime-Varying Relations\n,\nStream and Table Selection\n, \nSummary\ndefining in terms of relational algebra\n, \nTime-Varying Relations\n-\nTime-\nVarying Relations\nfor FULL OUTER joins\n, \nFULL OUTER\nin temporal validity joins\n, \nTemporal validity joins\nin temporal validity windows\n, \nTemporal validity windows\nrelationship with stream and table theory\n, \nStreams and Tables\n-\nStreams and\nTables\ntimers\n, \nGeneralized State\n, \nMillWheel\ntimestamps\nevent\n, \nDefinition\nsystem timestamp for Bloom filters\n, \nBloom Filters\nwatermarks and\n, \nDefinition\ntiming out a join, providing reference point for\n, \nWindowed Joins\ntools for reasoning about time\n, \nOn the Greatly Exaggerated Limitations of\nStreaming\ntracking subscription (Google Cloud Pub/Sub case study)\n, \nCase Study:\nSource Watermarks for Google Cloud Pub/Sub\ntransformations\n, \nWhat: Transformations\n-\nWhat: Transformations\nin streams and tables model\n, \nWhat: Transformations\n-\nWhat:\nTransformations\ntriggers\n, \nRoadmap\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\n-\nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\ncompleteness\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\nwatermark completeness trigger\n, \nWhen: Watermarks\nearly/on-time/late\n, \nWhen: Early/On-Time/Late Triggers FTW!\n-\nWhen:\nEarly/On-Time/Late Triggers FTW!\nwith allowed lateness\n, \nWhen: Allowed Lateness (i.e., Garbage\nCollection)\n-\nWhen: Allowed Lateness (i.e., Garbage Collection)\nin processing-time window via ingress time\n, \nProcessing-Time Windowing\nvia Ingress Time\nin streaming SQL\n, \nWhen: triggers\n-\nData-driven triggers\n, \nSummary\ndata-driven triggers\n, \nData-driven triggers\nrepeated delay triggers\n, \nRepeated delay triggers\nwatermark triggers\n, \nWatermark triggers\n-\nWatermark triggers\nin streams and tables model\n, \nWhen: Triggers\n-\nWhen: Triggers\nblending of batch and streaming\n, \nWhen: Triggers\nearly/on-time/late firings\n, \nWhen: Triggers\nper-record triggering in streaming engine\n, \nWhen: Triggers\ntrigger guarantees (or lack of)\n, \nWhen: Triggers\nwindowed summation with heuristic watermark on streaming engine\n,\nWhen: Triggers\nin unwindowed joins\n, \nUnwindowed Joins\npredeclaration or post-declaration options, Beam Model and\n, \nThe Beam\nModel: A Stream-Biased Approach\nprocessing-time delays in\naligned delays\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\nunaligned delays\n, \nWhen: The Wonderful Thing About Triggers Is\nTriggers Are Wonderful Things!\nprocessing-time windowing via\n, \nProcessing-Time Windowing via\nTriggers\n-\nProcessing-Time Windowing via Ingress Time\nrepeated update\n, \nWhen: The Wonderful Thing About Triggers Is Triggers\nAre Wonderful Things!\nSCAN-AND-STREAM trigger in materialized views\n, \nMaterialized views\nuse in achieving processing-time windowing\n, \nWhen/Where: Processing-\nTime Windows\ntumbling windows\n (\nsee\n fixed windows)\nTwitter Heron\n, \nStorm\nU\nunaligned delays (processing time in triggers)\n, \nWhen: The Wonderful Thing\nAbout Triggers Is Triggers Are Wonderful Things!\nunaligned windows\n, \nWhere: Session Windows\nunaligned fixed windows\n, \nUnaligned fixed windows\nunbounded data\n, \nTerminology: What Is Streaming?\nprocessing by batch systems\n, \nUnbounded Data: Batch\nprocessing by streaming systems\n, \nUnbounded Data: Streaming\n-\nWindowing\nby event time\ntime-agnostic processing\n, \nTime-agnostic\nusing approximation algorithms\n, \nApproximation algorithms\nUnboundedReader.getCurrentRecordId method\n, \nExactly Once in Sources\nungrouping operations\n, \nThe Beam Model: A Stream-Biased Approach\n(\nsee also\n grouping operations)\nunified batch plus streaming programming model\nin Apache Beam\n, \nBeam\nunpredictability in lambda architecture processing\n, \nWhy Exactly Once\nMatters\nunwindowed joins\n, \nUnwindowed Joins\n-\nUnwindowed Joins\nANTI\n, \nANTI\nFULL OUTER\n, \nFULL OUTER\nINNER\n, \nINNER\nLEFT OUTER\n, \nLEFT OUTER\nRIGHT OUTER\n, \nRIGHT OUTER\nSEMI\n, \nSEMI\n-\nUnwindowed Joins\nV\nvisibility (watermarks)\n, \nDefinition\nW\nwatermark triggers\nin streaming SQL\n, \nWatermark triggers\n-\nWatermark triggers\n, \nSummary\nwatermarks\n, \nRoadmap\n, \nWhen: Watermarks\n-\nWhen: Watermarks\n, \nWatermarks\n-\nSummary\n, \nMillWheel\n-\nMillWheel\nabout\n, \nDefinition\nallowed lateness and\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\nand temporal validity joins\n, \nWatermarks and temporal validity joins\napplying perfect and heuristic watermarks to same dataset\n, \nWhen:\nWatermarks\nas class of functions\n, \nWhen: Watermarks\ncase study, watermarks for Google Cloud Pub/Sub\n, \nCase Study: Source\nWatermarks for Google Cloud Pub/Sub\n-\nCase Study: Source Watermarks\nfor Google Cloud Pub/Sub\ncase study, watermarks in Apache Flink\n, \nCase Study: Watermarks in\nApache Flink\ncase study, watermarks in Google Cloud Dataflow\n, \nCase Studies\n-\nCase\nStudy: Watermarks in Google Cloud Dataflow\nfor exactly-once garbage collection in Dataflow\n, \nGarbage Collection\nfunction, converting processing time to event time\n, \nWhen: Watermarks\nheuristic\n, \nWhen: Watermarks\nin processing-time windowing via ingress time\n, \nProcessing-Time\nWindowing via Ingress Time\nin streaming SQL\n, \nSummary\npercentile\n, \nPercentile Watermarks\n-\nPercentile Watermarks\nperfect\n, \nWhen: Watermarks\nprocessing-time\n, \nProcessing-Time Watermarks\n-\nProcessing-Time\nWatermarks\npropagation\n, \nWatermark Propagation\n-\nThe Tricky Case of Overlapping\nWindows\nand output timestamps\n, \nWatermark Propagation and Output\nTimestamps\n-\nWatermark Propagation and Output Timestamps\nwith overlapping windows\n, \nThe Tricky Case of Overlapping Windows\nsource watermark creation\n, \nSource Watermark Creation\n-\nHeuristic\nWatermark Creation\nheuristic watermarks\n, \nHeuristic Watermark Creation\nperfect watermarks\n, \nPerfect Watermark Creation\ntoo fast\n, \nWhen: Watermarks\ntoo slow\n, \nWhen: Watermarks\nuse in Cloud Dataflow\n, \nCloud Dataflow\nWindmill\n, \nMillWheel\nwindowing\n, \nEvent Time Versus Processing Time\n, \nWhere: Windowing\n-\nWhere: Windowing\n, \nSummary\n, \nAll Your Joins Are Belong to Streaming\n(\nsee also\n unwindowed joins)\naccumulation modes for a window\n, \nRoadmap\nby event time\n, \nWindowing by event time\nby processing time\n, \nWindowing by processing time\ncustom\n, \nWhere: Custom Windowing\n-\nOne Size Does Not Fit All\nbenefits of\n, \nOne Size Does Not Fit All\nelements of custom windowing strategy\n, \nWhere: Custom Windowing\nvariations on fixed windows\n, \nVariations on Fixed Windows\n-\nPer-\nelement/key fixed windows\nvariations on session windows\n, \nVariations on Session Windows\n-\nBounded sessions\ndifferent strategies for\n, \nWindowing\ndynamic windows and retractions\n, \nHow: Accumulation\nend of window and output timestamp\n, \nWatermark Propagation and Output\nTimestamps\nfixed windows\n, \nWindowing\nin Cloud Dataflow\n, \nCloud Dataflow\nin streaming SQL\n, \nWhere: windowing\n-\nWhere: windowing\n, \nSummary\nin streams and tables model\n, \nWhere: Windowing\n-\nWindow merging\nlifetime of windows\n, \nWhen: Allowed Lateness (i.e., Garbage Collection)\nnondeterministic records in windowed aggregation\n, \nExactly Once in Sinks\noverlapping windows and output timestamp\n, \nThe Tricky Case of\nOverlapping Windows\nsession windows\n, \nWhere: Session Windows\n-\nWhere: Session Windows\nsessions\n, \nWindowing\nsessions in unbounded data processing by batch systems\n, \nSessions\nsliding windows\n, \nWindowing\nsummation code example\n, \nWhere: Windowing\nsummation on a batch engine\n, \nWhere: Windowing\nsummation on streaming dataset with perfect and heuristic watermarks\n,\nWhen: Watermarks\ntriggers for output\n, \nRoadmap\nunbounded data processing via fixed windows in batch systems\n, \nFixed\nwindows\nwindowed file writes\n, \nExample Sink: Files\nwindowed joins\n, \nWindowed Joins\n-\nWatermarks and temporal validity joins\nfixed windows\n, \nFixed Windows\n-\nFixed Windows\ntemporal validity\n, \nTemporal Validity\n-\nWatermarks and temporal validity\njoins\nwrite and read granularity, flexibility in\n, \nGeneralized State\n, \nConversion\nAttribution with Apache Beam\nZ\nZeitgeist pipeline, true streaming use case\n, \nMillWheel\nAbout the Authors\nTyler Akidau\n is a senior staff software engineer at Google, where he is the\ntechnical lead for the Data Processing Languages & Systems group,\nresponsible for Google’s Apache Beam efforts, Google Cloud Dataflow, and\ninternal data processing tools like Google Flume, MapReduce, and\nMillWheel. Tyler is also a founding member of the Apache Beam PMC.\nThough deeply passionate and vocal about the capabilities and importance of\nstream processing, he is also a firm believer in batch and streaming as two\nsides of the same coin, with the real endgame for data processing systems\nbeing the seamless merging between the two. He is the author of the\n“Dataflow Model” paper\n and the \n“Streaming 101”\n and \n“Streaming 102”\narticles on the O’Reilly website. His preferred mode of transportation is by\ncargo bike, with his two young daughters in tow.\nSlava Chernyak\n is a senior software engineer at Google Seattle. Slava spent\nmore than six years working on Google’s internal massive-scale streaming\ndata processing systems and has since become involved with designing and\nbuilding Windmill, Google Cloud Dataflow’s next-generation streaming\nbackend, from the ground up. Slava is passionate about making massive-scale\nstream processing available and useful to a broader audience. When he is not\nworking on streaming systems, Slava is out enjoying the natural beauty of the\nPacific Northwest.\nReuven Lax\n is a senior staff software engineer at Google, Seattle, and has\nspent the past ten years helping to shape Google’s data processing and\nanalysis strategy. For much of that time he has focused on Google’s low-\nlatency, streaming data processing efforts, first as a long-time member and\nlead of the MillWheel team, and more recently founding and leading the team\nresponsible for Windmill, the next-generation stream processing engine\npowering Google Cloud Dataflow. He is also a Beam PMC member. He’s\nvery excited to bring Google’s data processing experience to the world at\nlarge and proud to have been a part of publishing both the \n“MillWheel” paper\nin 2013 and the \n“Dataflow Model” paper\n in 2015. When not at work, Reuven\nenjoys swing dancing, rock climbing, and exploring new parts of the world.\nColophon\nThe animal on the cover of \nStreaming Systems\n is a brown trout (\nSalmo trutta\n)\na species of medium-sized fish native to northern Europe but now distributed\nacross the globe. Brown trout generally weigh about 2 pounds and grow to a\nlength of 16–31 inches. They have an overall shiny brown color with many\ndark spots over their upper body.\nBrown trout feed mostly on aquatic invertebrates although larger members of\nthe species have been known to prey on other fish. During spawning, the\nfemale brown trout produces thousands of eggs. It takes 3–4 years for a\nbrown trout to reach \nmaturity\n.\nPopular with anglers, brown trout were introduced into lakes and rivers\nthroughout the world during the 19\n and early 20\n centuries. To this day,\nbrown trout are farmed commercially and stocked for recreational fishing.\nBrown trout are edible and can be prepared in several ways, including\ngrilling, frying, and smoking.\nThe animal on the improved cover in \nFigure P-1\n is a robotic tyrannosaurus\nrex imbued with the soul of Sean Connery. True to form, it always speaks\nwith a Scottish accent, even when playing the role of a Russian submarine\ncaptain.\nMany of the animals on O’Reilly covers are endangered; all of them are\nimportant to the world. To learn more about how you can help, go to\nanimals.oreilly.com\n.\nThe cover image is from Karen Montgomery. The cover fonts are URW\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the\nheading font is Adobe Myriad Condensed; and the code font is Dalton\nMaag’s Ubuntu Mono.\nth\nth",52477
