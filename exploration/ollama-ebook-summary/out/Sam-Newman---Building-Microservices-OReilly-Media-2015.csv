filename,title,text,len
01-Navigating This Book.pdf,01-Navigating This Book,"Preface\nMicroservices are an approach to distributed systems that promote the use of finely\ngrained services with their own lifecycles, which collaborate together. Because\nmicroservices are primarily modeled around business domains, they avoid the problems of\ntraditional tiered architectures. Microservices also integrate new technologies and\ntechniques that have emerged over the last decade, which helps them avoid the pitfalls of\nmany service-oriented architecture implementations.\nThis book is full of concrete examples of microservice use around the world, including in\norganizations like Netflix, Amazon, Gilt, and the REA group, who have all found that the\nincreased autonomy this architecture gives their teams is a huge advantage.\nWho Should Read This Book\nThe scope of this book is broad, as the implications of fine-grained microservice\narchitectures are also broad. As such, it should appeal to people interested in aspects of\ndesign, development, deployment, testing, and maintenance of systems. Those of you who\nhave already embarked on the journey toward finer-grained architectures, whether for a\ngreenfield application or as part of decomposing an existing, more monolithic system, will\nfind plenty of practical advice to help you. It will also help those of you who want to know\nwhat all the fuss is about, so that you can determine whether microservices are right for\nyou.\nWhy I Wrote This Book\nI started thinking about the topic of application architectures many years ago, when\nworking to help people deliver their software faster. I realized that while infrastructure\nautomation, testing, and continuous delivery techniques could help, if the fundamental\ndesign of the system doesn’t make it easy to make changes, then there are limits to what\ncan be accomplished.\nAt the same time, many organizations were experimenting with finer-grained architectures\nto accomplish similar goals, but also to achieve things like improved scaling, increasing\nautonomy of teams, or to more easily embrace new technologies. My own experiences, as\nwell as those of my colleagues at ThoughtWorks and elsewhere, reinforced the fact that\nusing larger numbers of services with their own independent lifecycles resulted in more\nheadaches that had to be dealt with. In many ways, this book was imagined as a one-stop\nshop that would help encompass the wide variety of topics that are necessary for\nunderstanding microservices — something that would have helped me greatly in the past!\nA Word on Microservices Today\nMicroservices is a fast-moving topic. Although the idea is not new (even if the term itself\nis), experiences from people all over the world, along with the emergence of new\ntechnologies, are having a profound effect on how they are used. Due to the fast pace of\nchange, I have tried to focus this book on ideas more than specific technologies, knowing\nthat implementation details always change faster than the thoughts behind them.\nNonetheless, I fully expect that in a few years from now we’ll have learned even more\nabout where microservices fit, and how to use them well.\nSo while I have done my best to distill out the essence of the topic in this book, if this\ntopic interests you, be prepared for many years of continuous learning to keep on top of\nthe state of the art!\nNavigating This Book\nThis book is primarily organized in a topic-based format. As such, you may want to jump\ninto the specific topics that interest you the most. While I have done my best to reference\nterms and ideas in the earlier chapters, I’d like to think that even people who consider\nthemselves fairly experienced \nwill find something of interest in all chapters here. I would\ncertainly suggest that you take a look at \nChapter 2\n, which touches on the breadth of the\ntopic as well as providing some framing for how I go about things in case if you want to\ndive deeper into some of the later topics.\nFor people new to the subject, I’ve structured the chapters in a way that I hope will make\nsense to read from beginning to end.\nHere is an overview of what we cover:\nChapter 1, Microservices\nWe’ll begin with an introduction to microservices, including the key benefits as well\nas some of the downsides.\nChapter 2, The Evolutionary Architect\nThis chapter discusses the difficulties we face in terms of making trade-offs as\narchitects, and covers specifically just how many things we need to think about with\nmicroservices.\nChapter 3, How to Model Services\nHere we’ll start to define the boundary of microservices, using techniques from\ndomain-driven design to help focus our thinking.\nChapter 4, Integration\nThis is where we start getting a bit deeper into specific technology implications, as\nwe discuss what sorts of service collaboration techniques will help us most. We’ll\nalso delve into the topic of user interfaces and integrating with legacy and\ncommercial off-the-shelf (COTS) products.\nChapter 5, Splitting the Monolith\nMany people get interested in microservices as an antidote to large, hard-to-change\nmonolithic systems, and this is exactly what we’ll cover in detail in this chapter.\nChapter 6, Deployment\nAlthough this book is primarily theoretical, few topics in the book have been as\nimpacted by recent changes in technology as deployment, which we’ll explore here.\nChapter 7, Testing\nThis chapter goes deep into the topic of testing, an area of particular concern when\nhandling the deployment of multiple discrete services. Of particular note will be the\nrole that consumer-driven contracts can play in helping us ensure the quality of our\nsoftware.\nChapter 8, Monitoring\nTesting our software before production doesn’t help if problems occur once we go\nlive, and this chapter explores how we can monitor our fine-grained systems and deal\nwith some of the emergent complexity of distributed systems.\nChapter 9, Security\nHere we’ll examine the security aspects of microservices and consider how to handle\nuser-to-service and service-to-service authentication and authorization. Security is a\nvery important topic in computing, one that is all too readily ignored. Although I am\nin no way a security expert, I hope that this chapter will at least help you consider\nsome of the aspects you need to be aware of when building systems, and\nmicroservice systems in particular.\nChapter 10, Conway’s Law and System Design\nThis chapter focuses on the interplay of organizational structure and architecture.\nMany organizations have realized that trouble will occur if you don’t keep the two in\nharmony. We’ll attempt to get to the bottom of this dilemma, and consider some\ndifferent ways to align system design with the structure of your teams.\nChapter 11, Microservices at Scale\nThis is where we start looking at doing all of this at scale, so that we can handle the\nincreased chance of failure that can happen with large numbers of services, as well as\nlarge volumes of traffic.\nChapter 12, Bringing It All Together\nThe final chapter attempts to distill down the core essence of what makes\nmicroservices different. It includes a list of seven microservices principles, as well as\na wrap-up of the key points of the book.",7231
02-Technology Heterogeneity.pdf,02-Technology Heterogeneity,"Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, databases, data types, environment variables,\nstatements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values determined\nby context.\nSafari® Books Online\nNOTE\nSafari Books Online\n is an on-demand digital library that delivers expert \ncontent\n in both\nbook and video form from the world’s leading authors in technology and business.\nTechnology professionals, software developers, web designers, and business and creative\nprofessionals use Safari Books Online as their primary resource for research, problem\nsolving, learning, and certification training.\nSafari Books Online offers a range of \nplans and pricing\n for \nenterprise\n, \ngovernment\n,\neducation\n, and individuals.\nMembers have access to thousands of books, training videos, and prepublication\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\nPeachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan\nKaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds \nmore\n. For more\ninformation about Safari Books Online, please visit us \nonline\n.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at \nhttp://bit.ly/building-microservices\n.\nTo comment or ask technical questions about this book, send email to\nbookquestions@oreilly.com\n.\nFor more information about our books, courses, conferences, and news, see our website at\nhttp://www.oreilly.com\n.\nFind us on Facebook: \nhttp://facebook.com/oreilly\nFollow us on Twitter: \nhttp://twitter.com/oreillymedia\nWatch us on YouTube: \nhttp://www.youtube.com/oreillymedia\nAcknowledgments\nThis book is dedicated to Lindy Stephens, without whom it wouldn’t exist. She\nencouraged me to start on this journey, supported me throughout the often stressful\nprocess of writing, and is the best partner I could have ever asked for. I would also like to\ndedicate this to my dad, Howard Newman, who has always been there for me. This is for\nboth of you.\nI would like to single out Ben Christensen, Vivek Subramaniam, and Martin Fowler for\nproviding detailed feedback throughout the writing process, helping shape what this book\nbecame. I’d also like to thank James Lewis, with whom I have consumed many beers\ndiscussing the ideas presented in this book. This book would be a shadow of itself without\ntheir help and guidance.\nIn addition, many others provided help and feedback on early versions of the book.\nSpecifically, I would like to thank (in no particular order) Kane Venables, Anand\nKrishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon\nEaves, Rolf Russell, Badrinath Janakiraman, Daniel Bryant, Ian Robinson, Jim Webber,\nStewart Gleadow, Evan Bottcher, Eric Sword, Olivia Leonard, and all my other colleagues\nat ThoughtWorks and across the industry who have helped me get this far.\nFinally, I would like to thank all the people at O’Reilly, including Mike Loukides for\ngetting me on board, my editor Brian MacDonald, Rachel Monaghan, Kristen Brown,\nBetsy Waliszewski, and all the other people who have helped in ways I may never know\nabout.\nChapter 1. \nMicroservices\nFor many years now, we have been finding better ways to build systems. We have been\nlearning from what has come before, adopting new technologies, and observing how a\nnew wave of technology companies operate in different ways to create IT systems that\nhelp make both their customers and their own developers happier.\nEric Evans’s book \nDomain-Driven Design\n (Addison-Wesley) helped us understand the\nimportance of representing the real world in our code, and showed us better ways to model\nour systems. The concept of continuous delivery showed how we can more effectively and\nefficiently get our software into production, instilling in us the idea that we should treat\nevery check-in as a release candidate. Our understanding of how the Web works has led us\nto develop better ways of having machines talk to other machines. Alistair Cockburn’s\nconcept of \nhexagonal architecture\n guided us away from layered architectures where\nbusiness logic could hide. Virtualization platforms allowed us to provision and resize our\nmachines at will, with infrastructure automation giving us a way to handle these machines\nat scale. Some large, successful organizations like Amazon and Google espoused the view\nof small teams owning the full lifecycle of their services. And, more recently, Netflix has\nshared with us ways of building antifragile systems at a scale that would have been hard to\ncomprehend just 10 years ago.\nDomain-driven design. Continuous delivery. On-demand virtualization. Infrastructure\nautomation. Small autonomous teams. Systems at scale. \nMicroservices have emerged from\nthis world. They weren’t invented or described before the fact; they emerged as a trend, or\na pattern, from real-world use. But they exist only because of all that has gone before.\nThroughout this book, I will pull strands out of this prior work to help paint a picture of\nhow to build, manage, and evolve microservices.\nMany organizations have found that by embracing fine-grained, microservice\narchitectures, they can deliver software faster and embrace newer technologies.\nMicroservices give us significantly more freedom to react and make different decisions,\nallowing us to respond faster to the inevitable change that impacts all of us.\nWhat Are Microservices?\nMicroservices are small, autonomous services that work together. Let’s break that\ndefinition down a bit and consider the characteristics that make microservices different.\nSmall, and Focused on Doing One Thing Well\nCodebases grow as we write code to add new features. Over time, it can be difficult to\nknow where a change needs to be made because the codebase is so large. Despite a drive\nfor clear, modular monolithic codebases, all too often these arbitrary in-process\nboundaries break down. Code related to similar functions starts to become spread all over,\nmaking fixing bugs or implementations more difficult.\nWithin a monolithic system, we fight against these forces by trying to ensure our code is\nmore cohesive, often by creating abstractions or modules. Cohesion — the drive to have\nrelated code grouped together — is an important concept when we think about\nmicroservices. This is reinforced by Robert C. Martin’s definition of the \nSingle\nResponsibility Principle\n, which states “Gather together those things that change for the\nsame reason, and separate those things that change for different reasons.”\nMicroservices take this same approach to independent services. We focus our service\nboundaries on business boundaries, making it obvious where code lives for a given piece\nof functionality. And by keeping this service focused on an explicit boundary, we avoid\nthe temptation for it to grow too large, with all the associated difficulties that this can\nintroduce.\nThe question I am often asked is \nhow small is small?\n Giving a number for lines of code is\nproblematic, as some languages are more expressive than others and can therefore do\nmore in fewer lines of code. We must also consider the fact that we could be pulling in\nmultiple dependencies, which themselves contain many lines of code. In addition, some\npart of your domain may be legitimately complex, requiring more code. Jon Eaves at\nRealEstate.com.au in Australia characterizes a microservice as something that could be\nrewritten in two weeks, a rule of thumb that makes sense for his particular context.\nAnother somewhat trite answer I can give is \nsmall enough and no smaller\n. When speaking\nat conferences, I nearly always ask the question \nwho has a system that is too big and that\nyou’d like to break down\n? Nearly everyone raises their hands. We seem to have a very\ngood sense of what is too big, and so it could be argued that once a piece of code no\nlonger \nfeels\n too big, it’s probably small enough.\nA strong factor in helping us answer \nhow small?\n is how well the service aligns to team\nstructures. If the codebase is too big to be managed by a small team, looking to break it\ndown is very sensible. We’ll talk more about organizational alignment later on.\nWhen it comes to how small is small enough, I like to think in these terms: the smaller the\nservice, the more you maximize the benefits and downsides of microservice architecture.\nAs you get smaller, the benefits around interdependence increase. But so too does some of\nthe complexity that emerges from having more and more moving parts, something that we\nwill explore throughout this book. As you get better at handling this complexity, you can\nstrive for smaller and smaller services.\nAutonomous\nOur microservice is a separate entity. It might be deployed as an isolated service on a\nplatform as a service (PAAS), or it might be its own operating system process. We try to\navoid packing multiple services onto the same machine, although the definition of\nmachine\n in today’s world is pretty hazy! As we’ll discuss later, although this isolation can\nadd some overhead, the resulting simplicity makes our distributed system much easier to\nreason about, and newer technologies are able to mitigate many of the challenges\nassociated with this form of deployment.\nAll communication between the services themselves are via network calls, to enforce\nseparation between the services and avoid the perils of tight coupling.\nThese services need to be able to change independently of each other, and be deployed by\nthemselves without requiring consumers to change. We need to think about what our\nservices should expose, and what they should allow to be hidden. If there is too much\nsharing, our consuming services become coupled to our internal representations. This\ndecreases our autonomy, as it requires additional coordination with consumers when\nmaking changes.\nOur service exposes an application programming interface (API), and collaborating\nservices communicate with us via those APIs. We also need to think about what\ntechnology is appropriate to ensure that this itself doesn’t couple consumers. This may\nmean picking technology-agnostic APIs to ensure that we don’t constrain technology\nchoices. We’ll come back time and again to the importance of good, decoupled APIs\nthroughout this book.\nWithout decoupling, everything breaks down for us. The golden rule: can you make a\nchange to a service and deploy it by itself without changing anything else? If the answer is\nno, then many of the advantages we discuss throughout this book will be hard for you to\nachieve.\nTo do decoupling well, you’ll need to model your services right and get the APIs right. I’ll\nbe talking about that a lot.\nKey Benefits\nThe benefits of microservices are many and varied. Many of these benefits can be laid at\nthe door of any distributed system. Microservices, however, tend to achieve these benefits\nto a greater degree primarily due to how far they take the concepts behind distributed\nsystems and service-oriented architecture.\nTechnology Heterogeneity\nWith a system composed of multiple, collaborating services, we can decide to use different\ntechnologies inside each one. This allows us to pick the right tool for each job, rather than\nhaving to select a more standardized, one-size-fits-all approach that often ends up being\nthe lowest common denominator.\nIf one part of our system needs to improve its performance, we might decide to use a\ndifferent technology stack that is better able to achieve the performance levels required.\nWe may also decide that how we store our data needs to change for different parts of our\nsystem. For example, for a social network, we might store our users’ interactions in a\ngraph-oriented database to reflect the highly interconnected nature of a social graph, but\nperhaps the posts the users make could be stored in a document-oriented data store, giving\nrise to a heterogeneous architecture like the one shown in \nFigure 1-1\n.\nFigure 1-1. \nMicroservices can allow you to more easily embrace different technologies\nWith microservices, we are also able to adopt technology more quickly, and understand\nhow new advancements may help us. One of the biggest barriers to trying out and\nadopting new technology is the risks associated with it. With a monolithic application, if I\nwant to try a new programming language, database, or framework, any change will impact\na large amount of my system. With a system consisting of multiple services, I have\nmultiple new places in which to try out a new piece of technology. I can pick a service that\nis perhaps lowest risk and use the technology there, knowing that I can limit any potential\nnegative impact. Many organizations find this ability to more quickly absorb new\ntechnologies to be a real advantage for them.\nEmbracing multiple technologies doesn’t come without an overhead, of course. Some\norganizations choose to place some constraints on language choices. Netflix and Twitter,\nfor example, mostly use the Java Virtual Machine (JVM) as a platform, as they have a\nvery good understanding of the reliability and performance of that system. They also\ndevelop libraries and tooling for the JVM that make operating at scale much easier, but\nmake it more difficult for non-Java-based services or clients. But neither Twitter nor\nNetflix use only one technology stack for all jobs, either. Another counterpoint to concerns\nabout mixing in different technologies is the size. If I really can rewrite my microservice\nin two weeks, you may well mitigate the risks of embracing new technology.\nAs you’ll find throughout this book, just like many things concerning microservices, it’s\nall about finding the right balance. We’ll discuss how to make technology choices in\nChapter 2\n, which focuses on evolutionary architecture; and in \nChapter 4\n, which deals with\nintegration, you’ll learn how to ensure that your services can evolve their technology\nindependently of each other without undue coupling.",15027
03-Modules.pdf,03-Modules,"Resilience\nA key concept in resilience engineering is the bulkhead. If one component of a system\nfails, but that failure doesn’t cascade, you can isolate the problem and the rest of the\nsystem can carry on working. Service boundaries become your obvious bulkheads. In a\nmonolithic service, if the service fails, everything stops working. With a monolithic\nsystem, we can run on multiple machines to reduce our chance of failure, but with\nmicroservices, we can build systems that handle the total failure of services and degrade\nfunctionality accordingly.\nWe do need to be careful, however. To ensure our microservice systems can properly\nembrace this improved resilience, we need to understand the new sources of failure that\ndistributed systems have to deal with. Networks can and will fail, as will machines. We\nneed to know how to handle this, and what impact (if any) it should have on the end user\nof our software.\nWe’ll talk more about better handling resilience, and how to handle failure modes, in\nChapter 11\n.\nScaling\nWith a large, monolithic service, we have to scale everything together. One small part of\nour overall system is constrained in performance, but if that behavior is locked up in a\ngiant monolithic application, we have to handle scaling everything as a piece. With\nsmaller services, we can just scale those services that need scaling, allowing us to run\nother parts of the system on smaller, less powerful hardware, like in \nFigure 1-2\n.\nFigure 1-2. \nYou can target scaling at just those microservices that need it\nGilt, an online fashion retailer, adopted microservices for this exact reason. Starting in\n2007 with a monolithic Rails application, by 2009 Gilt’s system was unable to cope with\nthe load being placed on it. By splitting out core parts of its system, Gilt was better able to\ndeal with its traffic spikes, and today has over 450 microservices, each one running on\nmultiple separate machines.\nWhen embracing on-demand provisioning systems like those provided by Amazon Web\nServices, we can even apply this scaling on demand for those pieces that need it. This\nallows us to control our costs more effectively. It’s not often that an architectural approach\ncan be so closely correlated to an almost immediate cost savings.\nEase of Deployment\nA one-line change to a million-line-long monolithic application requires the whole\napplication to be deployed in order to release the change. That could be a large-impact,\nhigh-risk deployment. In practice, large-impact, high-risk deployments end up happening\ninfrequently due to understandable fear. Unfortunately, this means that our changes build\nup and build up between releases, until the new version of our application hitting\nproduction has masses of changes. And the bigger the delta between releases, the higher\nthe risk that we’ll get something wrong!\nWith microservices, we can make a change to a single service and deploy it independently\nof the rest of the system. This allows us to get our code deployed faster. If a problem does\noccur, it can be isolated quickly to an individual service, making fast rollback easy to\nachieve. It also means we can get our new functionality out to customers faster. This is\none of the main reasons why organizations like Amazon and Netflix use these\narchitectures — to ensure they remove as many impediments as possible to getting\nsoftware out the door.\nThe technology in this space has changed greatly in the last couple of years, and we’ll be\nlooking more deeply into the topic of deployment in a microservice world in \nChapter 6\n.\nOrganizational Alignment\nMany of us have experienced the problems associated with large teams and large\ncodebases. These problems can be exacerbated when the team is distributed. We also\nknow that smaller teams working on smaller codebases tend to be more productive.\nMicroservices allow us to better align our architecture to our organization, helping us\nminimize the number of people working on any one codebase to hit the sweet spot of team\nsize and productivity. We can also shift ownership of services between teams to try to keep\npeople working on one service colocated. We will go into much more detail on this topic\nwhen we discuss Conway’s law in \nChapter 10\n.\nComposability\nOne of the key promises of distributed systems and service-oriented architectures is that\nwe open up opportunities for reuse of \nfunctionality. With microservices, we allow for our\nfunctionality to be consumed in different ways for different purposes. This can be\nespecially important when we think about how our consumers use our software. Gone is\nthe time when we could think narrowly about either our desktop website or mobile\napplication. Now we need to think of the myriad ways that we might want to weave\ntogether capabilities for the Web, native application, mobile web, tablet app, or wearable\ndevice. As organizations move away from thinking in terms of narrow channels to more\nholistic concepts of customer engagement, we need architectures that can keep up.\nWith microservices, think of us opening up seams in our system that are addressable by\noutside parties. As circumstances change, we can build things in different ways. With a\nmonolithic application, I often have one coarse-grained seam that can be used from the\noutside. If I want to break that up to get something more useful, I’ll need a hammer! In\nChapter 5\n, I’ll discuss ways for you to break apart existing monolithic systems, and\nhopefully change them into some reusable, re-composable microservices.\nOptimizing for Replaceability\nIf you work at a medium-size or bigger organization, chances are you are aware of some\nbig, nasty legacy system sitting in the corner. The one no one wants to touch. The one that\nis vital to how your company runs, but that happens to be written in some odd Fortran\nvariant and runs only on hardware that reached end of life 25 years ago. Why hasn’t it\nbeen replaced? You know why: it’s too big and risky a job.\nWith our individual services being small in size, the cost to replace them with a better\nimplementation, or even delete them altogether, is much easier to manage. How often have\nyou deleted more than a hundred lines of code in a single day and not worried too much\nabout it? With microservices often being of similar size, the barriers to rewriting or\nremoving services entirely are very low.\nTeams using microservice approaches are comfortable with completely rewriting services\nwhen required, and just killing a service when it is no longer needed. When a codebase is\njust a few hundred lines long, it is difficult for people to become emotionally attached to\nit, and the cost of replacing it is pretty small.\nWhat About Service-Oriented Architecture?\nService-oriented architecture (SOA) is a design approach where multiple services\ncollaborate to provide some end set of capabilities. A service here typically means a\ncompletely separate operating system process. Communication between these services\noccurs via calls across a network rather than method calls within a process boundary.\nSOA emerged as an approach to combat the challenges of the large monolithic\napplications. It is an approach that aims to promote the reusability of software; two or\nmore end-user applications, for example, could both use the same services. It aims to\nmake it easier to maintain or rewrite software, as theoretically we can replace one service\nwith another without anyone knowing, as long as the semantics of the service don’t\nchange too much.\nSOA at its heart is a very sensible idea. However, despite many efforts, there is a lack of\ngood consensus on how to do SOA \nwell\n. In my opinion, much of the industry has failed to\nlook holistically enough at the problem and present a compelling alternative to the\nnarrative set out by various vendors in this space.\nMany of the problems laid at the door of SOA are actually problems with things like\ncommunication protocols (e.g., SOAP), vendor middleware, a lack of guidance about\nservice granularity, or the wrong guidance on picking places to split your system. We’ll\ntackle each of these in turn throughout the rest of the book. A cynic might suggest that\nvendors co-opted (and in some cases drove) the SOA movement as a way to sell more\nproducts, and those selfsame products in the end undermined the goal of SOA.\nMuch of the conventional wisdom around SOA doesn’t help you understand how to split\nsomething big into something small. It doesn’t talk about how big is too big. It doesn’t talk\nenough about real-world, practical ways to ensure that services do not become overly\ncoupled. The number of things that go unsaid is where many of the pitfalls associated with\nSOA originate.\nThe microservice approach has emerged from real-world use, taking our better\nunderstanding of systems and architecture to do SOA well. So you should instead think of\nmicroservices as a specific approach for SOA in the same way that XP or Scrum are\nspecific approaches for Agile software development.\nOther Decompositional Techniques\nWhen you get down to it, many of the advantages of a microservice-based architecture\ncome from its granular nature and the fact that it gives you many more choices as to how\nto solve problems. But could similar decompositional techniques achieve the same\nbenefits?\nShared Libraries\nA very standard decompositional technique that is built into virtually any language is\nbreaking down a codebase into multiple libraries. These libraries may be provided by third\nparties, or created in your own organization.\nLibraries give you a way to share functionality between teams and services. I might create\na set of useful collection utilities, for example, or perhaps a statistics library that can be\nreused.\nTeams can organize themselves around these libraries, and the libraries themselves can be\nreused. But there are some drawbacks.\nFirst, you lose true technology heterogeneity. The library typically has to be in the same\nlanguage, or at the very least run on the same platform. Second, the ease with which you\ncan scale parts of your system independently from each other is curtailed. Next, unless\nyou’re using dynamically linked libraries, you cannot deploy a new library without\nredeploying the entire process, so your ability to deploy changes in isolation is reduced.\nAnd perhaps the kicker is that you lack the obvious seams around which to erect\narchitectural safety measures to ensure system resiliency.\nShared libraries do have their place. You’ll find yourself creating code for common tasks\nthat aren’t specific to your business domain that you want to reuse across the organization,\nwhich is an obvious candidate for becoming a reusable library. You do need to be careful,\nthough. Shared code used to communicate between services can become a point of\ncoupling, something we’ll discuss in \nChapter 4\n.\nServices can and should make heavy use of third-party libraries to reuse common code.\nBut they don’t get us all the way there.\nModules\nSome languages provide their own modular decomposition techniques that go beyond\nsimple libraries. They allow some lifecycle management of the modules, such that they\ncan be deployed into a running process, allowing you to make changes without taking the\nwhole process down.\nThe Open Source Gateway Initiative (OSGI) is worth calling out as one technology-\nspecific approach to modular decomposition. Java itself doesn’t have a true concept of\nmodules, and we’ll have to wait at least until Java 9 to see this added to the language.\nOSGI, which emerged as a framework to allow plug-ins to be installed in the Eclipse Java\nIDE, is now used as a way to retrofit a module concept in Java via a library.\nThe problem with OSGI is that it is trying to enforce things like module lifecycle\nmanagement without enough support in the language itself. This results in more work\nhaving to be done by module authors to deliver on proper module isolation. Within a\nprocess boundary, it is also much easier to fall into the trap of making modules overly\ncoupled to each other, causing all sorts of problems. My own experience with OSGI,\nwhich is matched by that of colleagues in the industry, is that even with good teams it is\neasy for OSGI to become a much bigger source of complexity than its benefits warrant.\nErlang follows a different approach, in which modules are baked into the language\nruntime. Thus, Erlang is a very mature approach to modular decomposition. Erlang\nmodules can be stopped, restarted, and upgraded without issue. Erlang even supports\nrunning more than one version of the module at a given time, allowing for more graceful\nmodule upgrading.\nThe capabilities of Erlang’s modules are impressive indeed, but even if we are lucky\nenough to use a platform with these capabilities, we still have the same shortcomings as\nwe do with normal shared libraries. We are strictly limited in our ability to use new\ntechnologies, limited in how we can scale independently, can drift toward integration\ntechniques that are overly coupling, and lack seams for architectural safety measures.\nThere is one final observation worth sharing. Technically, it should be possible to create\nwell-factored, independent modules within a single monolithic process. And yet we rarely\nsee this happen. The modules themselves soon become tightly coupled with the rest of the\ncode, surrendering one of their key benefits. Having a process boundary separation does\nenforce clean hygiene in this respect (or at least makes it harder to do the wrong thing!). I\nwouldn’t suggest that this should be the main driver for process separation, of course, but\nit is interesting that the promises of modular separation within process boundaries rarely\ndeliver in the real world.\nSo while modular decomposition within a process boundary may be something you want\nto do as well as decomposing your system into services, by itself it won’t help solve\neverything. If you are a pure Erlang shop, the quality of Erlang’s module implementation\nmay get you a very long way, but I suspect many of you are not in that situation. For the\nrest of us, we should see modules as offering the same sorts of benefits as shared libraries.",14350
04-Inaccurate Comparisons.pdf,04-Inaccurate Comparisons,"No Silver Bullet\nBefore we finish, I should call out that microservices are no free lunch or silver bullet, and\nmake for a bad choice as a golden hammer. They have all the associated complexities of\ndistributed systems, and while we have learned a lot about how to manage distributed\nsystems well (which we’ll discuss throughout the book) it is still hard. If you’re coming\nfrom a monolithic system point of view, you’ll have to get much better at handling\ndeployment, testing, and monitoring to unlock the benefits we’ve covered so far. You’ll\nalso need to think differently about how you scale your systems and ensure that they are\nresilient. Don’t also be surprised if things like distributed transactions or CAP theorem\nstart giving you headaches, either!\nEvery company, organization, and system is different. A number of factors will play into\nwhether or not microservices are right for you, and how aggressive you can be in adopting\nthem. Throughout each chapter in this book I’ll attempt to give you guidance highlighting\nthe potential pitfalls, which should help you chart a steady path.\nSummary\nHopefully by now you know what a microservice is, what makes it different from other\ncompositional techniques, and what some of the key advantages are. In each of the\nfollowing chapters we will go into more detail on how to achieve these benefits and how\nto avoid some of the common pitfalls.\nThere are a number of topics to cover, but we need to start somewhere. One of the main\nchallenges that microservices introduce is a shift in the role of those who often guide the\nevolution of our systems: the architects. We’ll look next at some different approaches to\nthis role that can ensure we get the most out of this new architecture.\nChapter 2. \nThe Evolutionary Architect\nAs we have seen so far, microservices give us a lot of choice, and accordingly a lot of\ndecisions to make. For example, how many different technologies should we use, should\nwe let different teams use different programming idioms, and should we split or merge a\nservice? How do we go about making these decisions? With the faster pace of change, and\nthe more fluid environment that these architectures allow, the role of the architect also has\nto change. In this chapter, I’ll take a fairly opinionated view of what the role of an\narchitect is, and hopefully launch one final assault on the ivory tower.\nInaccurate Comparisons\nYou keep using that word. I do not think it means what you think it means.\nInigo Montoya, from \nThe Princess Bride\nArchitects have an important job. They are in charge of making sure we have a joined-up\ntechnical vision, one that should help us deliver the system our customers need. In some\nplaces, they may only have to work with one team, in which case the role of the architect\nand technical lead is often the same. In others, they may be defining the vision for an\nentire program of work, coordinating with multiple teams across the world, or perhaps\neven an entire organization. At whatever level they operate, the role is a tricky one to pin\ndown, and despite it often being the obvious career progression for developers in\nenterprise organizations, it is also a role that gets more criticism than virtually any other.\nMore than any other role, architects can have a direct impact on the quality of the systems\nbuilt, on the working conditions of their colleagues, and on their organization’s ability to\nrespond to change, and yet we so frequently seem to get this role wrong. Why is that?\nOur industry is a young one. This is something we seem to forget, and yet we have only\nbeen creating programs that run on what we recognize as computers for around 70 years.\nTherefore, we are constantly looking to other professions in an attempt to explain what we\ndo. We aren’t medical doctors or engineers, but nor are we plumbers or electricians.\nInstead, we fall into some middle ground, which makes it hard for society to understand\nus, or for us to understand where we fit.\nSo we borrow from other professions. We call ourselves software “engineers,” or\n“architects.” But we aren’t, are we? Architects and engineers have a rigor and discipline\nwe could only dream of, and their importance in society is well understood. I remember\ntalking to a friend of mine, the day before he became a qualified architect. “Tomorrow,” he\nsaid, “if I give you advice down at the pub about how to build something and it’s wrong, I\nget held to account. I could get sued, as in the eyes of the law I am now a qualified\narchitect and I should be held responsible if I get it wrong.” The importance of these jobs\nto society means that there are required qualifications people have to meet. In the UK, for\nexample, a minimum of seven years study is required before you can be called an\narchitect. But these jobs are also based on a body of knowledge going back thousands of\nyears. And us? Not quite. Which is also why I view most forms of IT certification as\nworthless, as we know so little about what \ngood\n looks like.\nPart of us wants recognition, so we borrow names from other professions that already have\nthe recognition we as an industry crave. But this can be doubly harmful. First, it implies\nwe know what we are doing, when we plainly don’t. I wouldn’t say that buildings and\nbridges never fall down, but they fall down much less than the number of times our\nprograms will crash, making comparisons with engineers quite unfair. Second, the\nanalogies break down very quickly when given even a cursory glance. To turn things\naround, if bridge building were like programming, halfway through we’d find out that the\nfar bank was now 50 meters farther out, that it was actually mud rather than granite, and\nthat rather than building a footbridge we were instead building a road bridge. Our software\nisn’t constrained by the same physical rules that real architects or engineers have to deal\nwith, and what we create is designed to flex and adapt and evolve with user requirements.\nPerhaps the term \narchitect\n has done the most harm. The idea of someone who draws up\ndetailed plans for others to interpret, and expects this to be carried out. The balance of part\nartist, part engineer, overseeing the creation of what is normally a singular vision, with all\nother viewpoints being subservient, except for the occasional objection from the structural\nengineer regarding the laws of physics. In our industry, this view of the architect leads to\nsome terrible practices. Diagram after diagram, page after page of documentation, created\nwith a view to inform the construction of the perfect system, without taking into account\nthe fundamentally unknowable future. Utterly devoid of any understanding as to how hard\nit will be to implement, or whether or not it will actually work, let alone having any ability\nto change as we learn more.\nWhen we compare ourselves to engineers or architects, we are in danger of doing\neveryone a disservice. Unfortunately, we are stuck with the word \narchitect\n for now. So the\nbest we can do is to redefine what it means in our context.",7150
05-An Evolutionary Vision for the Architect.pdf,05-An Evolutionary Vision for the Architect,"An Evolutionary Vision for the Architect\nOur requirements shift more rapidly than they do for people who design and build\nbuildings — as do the tools and techniques at our disposal. The things we create are not\nfixed points in time. Once launched into production, our software will continue to evolve\nas the way it is used changes. For most things we create, we have to accept that once the\nsoftware gets into the hands of our customers we will have to react and adapt, rather than\nit being a never-changing artifact. Thus, our architects need to shift their thinking away\nfrom creating the perfect end product, and instead focus on helping create a framework in\nwhich the right systems can emerge, and continue to grow as we learn more.\nAlthough I have spent much of the chapter so far warning you off comparing ourselves too\nmuch to other professions, there is one analogy that I like when it comes to the role of the\nIT architect and that I think better encapsulates what we want this role to be. Erik\nDoernenburg first shared with me the idea that we should think of our role more as town\nplanners than architects for the built environment. The role of the town planner should be\nfamiliar to any of you who have played SimCity before. A town planner’s role is to look at\na multitude of sources of information, and then attempt to optimize the layout of a city to\nbest suit the needs of the citizens today, taking into account future use. The way he\ninfluences how the city evolves, though, is interesting. He does not say, “build this\nspecific building there”; instead, he \nzones a city\n. So as in SimCity, you might designate\npart of your city as an industrial zone, and another part as a residential zone. It is then up\nto other people to decide what exact buildings get created, but there are restrictions: if you\nwant to build a factory, it will need to be in an industrial zone. Rather than worrying too\nmuch about what happens in one zone, the town planner will instead spend far more time\nworking out how people and utilities move from one zone to another.\nMore than one person has likened a city to a living creature. The city changes over time. It\nshifts and evolves as its occupants use it in different ways, or as external forces shape it.\nThe town planner does his best to anticipate these changes, but accepts that trying to exert\ndirect control over all aspects of what happens is futile.\nThe comparison with software should be obvious. As our users use our software, we need\nto react and change. We cannot foresee everything that will happen, and so rather than\nplan for any eventuality, we should plan to allow for change by avoiding the urge to\noverspecify every last thing. Our city — the system — needs to be a good, happy place for\neveryone who uses it. One thing that people often forget is that our system doesn’t just\naccommodate users; it also accommodates developers and operations people who also\nhave to work there, and who have the job of making sure it can change as required. To\nborrow a term from Frank Buschmann, architects have a duty to ensure that the system is\nhabitable\n for developers too.\nA town planner, just like an architect, also needs to know when his plan isn’t being\nfollowed. As he is less prescriptive, the number of times he needs to get involved to\ncorrect direction should be minimal, but if someone decides to build a sewage plant in a\nresidential area, he needs to be able to shut it down.\nSo our architects as town planners need to set direction in broad strokes, and only get\ninvolved in being highly specific about implementation detail in limited cases. They need\nto ensure that the system is fit for purpose now, but also a platform for the future. And\nthey need to ensure that it is a system that makes users and developers equally happy. This\nsounds like a pretty tall order. Where do we start?",3906
06-Zoning.pdf,06-Zoning,"Zoning\nSo, to continue the metaphor of the architect as town planner for a moment, what are our\nzones? These are our service boundaries, or perhaps coarse-grained groups of services. As\narchitects, we need to worry much less about what happens \ninside\n the zone than what\nhappens \nbetween\n the zones. That means we need to spend time thinking about how our\nservices talk to each other, or ensuring that we can properly monitor the overall health of\nour system. How involved we get inside the zone will vary somewhat. Many organizations\nhave adopted microservices in order to maximize for autonomy of teams, something we’ll\nexpand on in \nChapter 10\n. If you are in such an organization, you will rely more on the\nteam to make the right local decision.\nBut between the zones, or the boxes on our traditional architecture diagram, we need to be\ncareful; getting things wrong here leads to all sorts of problems and can be very hard to\ncorrect.\nWithin each service, you may be OK with the team who owns that zone picking a different\ntechnology stack or data store. Other concerns may kick in here, of course. Your\ninclination to let teams pick the right tool for the job may be tempered by the fact that it\nbecomes harder to hire people or move them between teams if you have 10 different\ntechnology stacks to support. Similarly, if each team picks a completely different data\nstore, you may find yourself lacking enough experience to run any of them at scale.\nNetflix, for example, has mostly standardized on Cassandra as a data-store technology.\nAlthough it may not be the best fit for all of its cases, Netflix feels that the value gained by\nbuilding tooling and expertise around Cassandra is more important than having to support\nand operate at scale multiple other platforms that may be a better fit for certain tasks.\nNetflix is an extreme example, where scale is likely the strongest overriding factor, but\nyou get the idea.\nBetween services is where things can get messy, however. If one service decides to expose\nREST over HTTP, another makes use of protocol buffers, and a third uses Java RMI, then\nintegration can become a nightmare as consuming services have to understand and support\nmultiple styles of interchange. This is why I try to stick to the guideline that we should “be\nworried about what happens between the boxes, and be liberal in what happens inside.”\nTHE CODING ARCHITECT\nIf we are to ensure that the systems we create are habitable for our developers, then our architects need to\nunderstand the impact of their decisions. At the very least, this means spending time with the team, and ideally it\nshould mean that these developers actually spend time coding with the team too. For those of you who practice pair\nprogramming, it becomes a simple matter for an architect to join a team for a short period as one member of the\npair. Ideally, you should work on normal stories, to really understand what \nnormal\n work is like. I cannot emphasize\nhow important it is for the architect to actually sit with the team! This is significantly more effective than having a\ncall or just looking at her code.\nAs for how often you should do this, that depends greatly on the size of the team(s) you are working with. But the\nkey is that it should be a routine activity. If you are working with four teams, for example, spending half a day with\neach team every four weeks ensures you build an awareness and improved communications with the teams you are\nworking with.",3524
07-Tailored Service Template.pdf,07-Tailored Service Template,"A Principled Approach\nRules are for the obedience of fools and the guidance of wise men.\nGenerally attributed to Douglas Bader\nMaking decisions in system design is all about trade-offs, and microservice architectures\ngive us lots of trade-offs to make! When picking a datastore, do we pick a platform that\nwe have less experience with, but that gives us better scaling? Is it OK for us to have two\ndifferent technology stacks in our system? What about three? Some decisions can be made\ncompletely on the spot with information available to us, and these are the easiest to make.\nBut what about those decisions that might have to be made on incomplete information?\nFraming here can help, and a great way to help frame our decision making is to define a\nset of principles and practices that guide it, based on goals that we are trying to achieve.\nLet’s look at each in turn.\nStrategic Goals\nThe role of the architect is already daunting enough, so luckily we usually don’t have to\nalso define strategic goals! Strategic goals should speak to where your company is going,\nand how it sees itself as best making its customers happy. These will be high-level goals,\nand may not include technology at all. They could be defined at a company level or a\ndivision level. They might be things like “Expand into Southeast Asia to unlock new\nmarkets,” or “Let the customer achieve as much as possible using self-service.” The key is\nthat this is where your organization is headed, so you need to make sure the technology is\naligned to it.\nIf you’re the person defining the company’s technical vision, this may mean you’ll need to\nspend more time with the nontechnical parts of your organization (or \nthe business\n, as they\nare often called). What is the driving vision for the business? And how does it change?\nPrinciples\nPrinciples are rules you have made in order to align what you are doing to some larger\ngoal, and will sometimes change. For example, if one of your strategic goals as an\norganization is to decrease the time to market for new features, you may define a principle\nthat says that delivery teams have full control over the lifecycle of their software to ship\nwhenever they are ready, independently of any other team. If another goal is that your\norganization is moving to aggressively grow its offering in other countries, you may\ndecide to implement a principle that the entire system must be portable to allow for it to be\ndeployed locally in order to respect sovereignty of data.\nYou probably don’t want loads of these. Fewer than 10 is a good number — small enough\nthat people can remember them, or to fit on small posters. The more principles you have,\nthe greater the chance that they overlap or contradict each other.\nHeroku’s \n12 Factors\n are a set of design principles structured around the goal of helping\nyou create applications that work well on the Heroku platform. They also may well make\nsense in other contexts. Some of the principles are actually constraints based on behaviors\nyour application needs to exhibit in order to work on Heroku. A constraint is really\nsomething that is very hard (or virtually impossible) to change, whereas principles are\nthings we decide to choose. You may decide to explicitly call out those things that are\nprinciples versus those that are constraints, to help indicate those things you really can’t\nchange. Personally, I think there can be some value in keeping them in the same list to\nencourage challenging constraints every now and then and see if they really are\nimmovable!\nPractices\nOur practices are how we ensure our principles are being carried out. They are a set of\ndetailed, practical guidance for performing tasks. They will often be technology-specific,\nand should be low level enough that any developer can understand them. Practices could\ninclude coding guidelines, the fact that all log data needs to be captured centrally, or that\nHTTP/REST is the standard integration style. Due to their technical nature, practices will\noften change more often than principles.\nAs with principles, sometimes practices reflect constraints in your organization. For\nexample, if you support only CentOS, this will need to be reflected in your practices.\nPractices should underpin our principles. A principle stating that delivery teams control\nthe full lifecycle of their systems may mean you have a practice stating that all services\nare deployed into isolated AWS accounts, providing self-service management of the\nresources and isolation from other teams.\nCombining Principles and Practices\nOne person’s principles are another’s practices. You might decide to call the use of\nHTTP/REST a principle rather than a practice, for example. And that would be fine. The\nkey point is that there is value in having overarching ideas that guide how the system\nevolves, and in having enough detail so that people know how to implement those ideas.\nFor a small enough group, perhaps a single team, combining principles and practices\nmight be OK. However, for larger organizations, where the technology and working\npractices may differ, you may want a different set of practices in different places, as long\nas they both map to a common set of principles. A .NET team, for example, might have\none set of practices, and a Java team another, with a set of practices common to both. The\nprinciples, though, could be the same for both.\nA Real-World Example\nMy colleague Evan Bottcher developed the diagram shown in \nFigure 2-1\n in the course of\nworking with one of our clients. The figure shows the interplay of goals, principles, and\npractices in a very clear format. Over the course of a couple years, the practices on the far\nright will change fairly regularly, whereas the principles remain fairly static. A diagram\nsuch as this can be printed nicely on a single sheet of paper and shared, and each idea is\nsimple enough for the average developer to remember. There is, of course, more detail\nbehind each point here, but being able to articulate this in summary form is very useful.\nFigure 2-1. \nA real-world example of principles and practices\nIt makes sense to have documentation supporting some of these items. In the main,\nthough, I like the idea of having example code that you can look at, inspect, and run,\nwhich embodies these ideas. Even better, we can create tooling that does the right thing\nout of the box. We’ll discuss that in more depth momentarily.\nThe Required Standard\nWhen you’re working through your practices and thinking about the trade-offs you need to\nmake, one of the core balances to find is how much variability to allow in your system.\nOne of the key ways to identify what should be constant from service to service is to\ndefine what a well-behaved, good service looks like. What is a “good citizen” service in\nyour system? What capabilities does it need to have to ensure that your system is\nmanageable and that one bad service doesn’t bring down the whole system? And, as with\npeople, what a good citizen is in one context does not reflect what it looks like somewhere\nelse. Nonetheless, there are some common characteristics of well-behaved services that I\nthink are fairly important to observe. These are the few key areas where allowing too\nmuch divergence can result in a pretty torrid time. As Ben Christensen from Netflix puts\nit, when we think about the bigger picture, “it needs to be a cohesive system made of\nmany small parts with autonomous lifecycles but all coming together.” So we need to find\nthe balance between optimizing for autonomy of the individual microservice without\nlosing sight of the bigger picture. Defining clear attributes that each service should have is\none way of being clear as to where that balance sits.\nMonitoring\nIt is essential that we are able to draw up coherent, cross-service views of our system\nhealth. This has to be a system-wide view, not a service-specific view. As we’ll discuss in\nChapter 8\n, knowing the health of an individual service is useful, but often only when\nyou’re trying to diagnose a wider problem or understand a larger trend. To make this as\neasy as possible, I would suggest ensuring that all services emit health and general\nmonitoring-related metrics in the same way.\nYou might choose to adopt a push mechanism, where each service needs to push this data\ninto a central location. For your metrics this might be Graphite, and for your health it\nmight be Nagios. Or you might decide to use polling systems that scrape data from the\nnodes themselves. But whatever you pick, try to keep it standardized. Make the\ntechnology inside the box opaque, and don’t require that your monitoring systems change\nin order to support it. Logging falls into the same category here: we need it in one place.\nInterfaces\nPicking a small number of defined interface technologies helps integrate new consumers.\nHaving one standard is a good number. Two isn’t too bad, either. Having 20 different\nstyles of integration is bad. This isn’t just about picking the technology and the protocol. If\nyou pick HTTP/REST, for example, will you use verbs or nouns? How will you handle\npagination of resources? How will you handle versioning of end points?\nArchitectural Safety\nWe cannot afford for one badly behaved service to ruin the party for everyone. We have to\nensure that our services shield themselves accordingly from unhealthy, downstream calls.\nThe more services we have that do not properly handle the potential failure of downstream\ncalls, the more fragile our systems will be. This means you will probably want to mandate\nas a minimum that each downstream service gets its own connection pool, and you may\neven go as far as to say that each also uses a circuit breaker. This will get covered in more\ndepth when we discuss microservices at scale in \nChapter 11\n.\nPlaying by the rules is important when it comes to response codes, too. If your circuit\nbreakers rely on HTTP codes, and one service decides to send back 2XX codes for errors,\nor confuses 4XX codes with 5XX codes, then these safety measures can fall apart. Similar\nconcerns would apply even if you’re not using HTTP; knowing the difference between a\nrequest that was OK and processed correctly, a request that was bad and thus prevented\nthe service from doing anything with it, and a request that might be OK but we can’t tell\nbecause the server was down is key to ensuring we can fail fast and track down issues. If\nour services play fast and loose with these rules, we end up with a more vulnerable\nsystem.\nGovernance Through Code\nGetting together and agreeing on how things can be done is a good idea. But spending\ntime making sure people are following these guidelines is less fun, as is placing a burden\non developers to implement all these standard things you expect each service to do. I am a\ngreat believer in making it easy to do the right thing. Two techniques I have seen work\nwell here are using exemplars and providing service templates.\nExemplars\nWritten documentation is good, and useful. I clearly see the value; after all, I’ve written\nthis book. But developers also like code, and code they can run and explore. If you have a\nset of standards or best practices you would like to encourage, then having exemplars that\nyou can point people to is useful. The idea is that people can’t go far wrong just by\nimitating some of the better parts of your system.\nIdeally, these should be real-world services you have that get things right, rather than\nisolated services that are just implemented to be \nperfect examples\n. By ensuring your\nexemplars are actually being used, you ensure that all the principles you have actually\nmake sense.\nTailored Service Template\nWouldn’t it be great if you could make it really easy for all developers to follow most of\nthe guidelines you have with very little work? What if, out of the box, the developers had\nmost of the code in place to implement the core attributes that each service needs?\nDropwizard\n and \nKaryon\n are two open source, JVM-based microcontainers. They work in\nsimilar ways, pulling together a set of libraries to provide features like health checking,\nserving HTTP, or exposing metrics. So, out of the box, you have a service complete with\nan embedded servlet container that can be launched from the command line. This is a\ngreat way to get going, but why stop there? While you’re at it, why not take something\nlike a Dropwizard or Karyon, and add more features so that it becomes compliant for your\ncontext?\nFor example, you might want to mandate the use of circuit breakers. In that case, you\nmight integrate a circuit breaker library like \nHystrix\n. Or you might have a practice that all\nyour metrics need to be sent to a central Graphite server, so perhaps pull in an open source\nlibrary like Dropwizard’s \nMetrics\n and configure it so that, out of the box, response times\nand error rates are pushed automatically to a known location.\nBy tailoring such a service template for your own set of development practices, you ensure\nthat teams can get going faster, and also that developers have to go out of their way to\nmake their services badly behaved.\nOf course, if you embraced multiple disparate technology stacks, you’d need a matching\nservice template for each. This may be a way you subtly constrain language choices in\nyour teams, though. If the in-house service template supports only Java, then people may\nbe discouraged from picking alternative stacks if they have to do lots more work\nthemselves. Netflix, for example, is especially concerned with aspects like fault tolerance,\nto ensure that the outage of one part of its system cannot take everything down. To handle\nthis, a large amount of work has been done to ensure that there are client libraries on the\nJVM to provide teams with the tools they need to keep their services well behaved.\nAnyone introducing a new technology stack would mean having to reproduce all this\neffort. The main concern for Netflix is less about the duplicated effort, and more about the\nfact that it is so easy to get this wrong. The risk of a service getting newly implemented\nfault tolerance wrong is high if it could impact more of the system. Netflix mitigates this\nby using \nsidecar services\n, which communicate locally with a JVM that is using the\nappropriate libraries.\nYou do have to be careful that creating the service template doesn’t become the job of a\ncentral tools or architecture team who dictates how things should be done, albeit via code.\nDefining the practices you use should be a collective activity, so ideally your team(s)\nshould take joint responsibility for updating this template (an internal open source\napproach works well here).\nI have also seen many a team’s morale and productivity destroyed by having a mandated\nframework thrust upon them. In a drive to improve code reuse, more and more work is\nplaced into a centralized framework until it becomes an overwhelming monstrosity. If you\ndecide to use a tailored service template, think very carefully about what its job is. Ideally,\nits use should be purely optional, but if you are going to be more forceful in its adoption\nyou need to understand that ease of use for the developers has to be a prime guiding force.\nAlso be aware of the perils of shared code. In our desire to create reusable code, we can\nintroduce sources of coupling \nbetween services. At least one organization I spoke to is so\nworried about this that it actually copies its service template code manually into each\nservice. This means that an upgrade to the core service template takes longer to be applied\nacross its system, but this is less concerning to it than the danger of coupling. Other teams\nI have spoken to have simply treated the service template as a shared binary dependency,\nalthough they have to be very diligent in not letting the tendency for DRY (don’t repeat\nyourself) result in an overly coupled system! This is a nuanced topic, so we’ll explore it in\nmore detail in \nChapter 4\n.",16171
08-Governance and Leading from the Center.pdf,08-Governance and Leading from the Center,"Technical Debt\nWe are often put in situations where we cannot follow through to the letter on our\ntechnical vision. Often, we need to make a choice to cut a few corners to get some urgent\nfeatures out. This is just one more trade-off that we’ll find ourselves having to make. Our\ntechnical vision exists for a reason. If we deviate from this reason, it might have a short-\nterm benefit but a long-term cost. A concept that helps us understand this trade-off is\ntechnical debt. When we accrue technical debt, just like debt in the real world it has an\nongoing cost, and is something we want to pay down.\nSometimes technical debt isn’t just something we cause by taking shortcuts. What happens\nif our vision for the system changes, but not all of our system matches? In this situation,\ntoo, we have created new sources of technical debt.\nThe architect’s job is to look at the bigger picture, and understand this balance. Having\nsome view as to the level of debt, and where to get involved, is important. Depending on\nyour organization, you might be able to provide gentle guidance, but have the teams\nthemselves decide how to track and pay down the debt. For other organizations, you may\nneed to be more structured, perhaps maintaining a debt log that is reviewed regularly.\nException Handling\nSo our principles and practices guide how our systems should be built. But what happens\nwhen our system deviates from this? Sometimes we make a decision that is just an\nexception to the rule. In these cases, it might be worth capturing such a decision in a log\nsomewhere for future reference. If enough \nexceptions\n are found, it may eventually make\nsense to change the principle or practice to reflect a new understanding of the world. For\nexample, we might have a practice that states that we will always use MySQL for data\nstorage. But then we see compelling reasons to use Cassandra for highly scalable storage,\nat which point we change our practice to say, “Use MySQL for most storage requirements,\nunless you expect large growth in volumes, in which case use Cassandra.”\nIt’s probably worth reiterating, though, that every organization is different. I’ve worked\nwith some companies where the development teams have a high degree of trust and\nautonomy, and there the principles are lightweight (and the need for overt exception\nhandling is greatly reduced if not eliminated). In more structured organizations in which\ndevelopers have less freedom, tracking exceptions may be vital to ensure that the rules put\nin place properly reflect the challenges people are facing. With all that said, I am a fan of\nmicroservices as a way of optimizing for autonomy of teams, giving them as much\nfreedom as possible to solve the problem at hand. If you are working in an organization\nthat places lots of restrictions on how developers can do their work, then microservices\nmay not be for you.\nGovernance and Leading from the Center\nPart of what architects need to handle is governance. What do I mean by \ngovernance\n? It\nturns out the Control Objectives for Information and Related Technology (COBIT) has a\npretty good definition:\nGovernance ensures that enterprise objectives are achieved by evaluating stakeholder\nneeds, conditions and options; setting direction through prioritisation and decision\nmaking; and monitoring performance, compliance and progress against agreed-on\ndirection and objectives.\nCOBIT 5\nGovernance can apply to multiple things in the forum of IT. We want to focus on the\naspect of technical governance, something I feel is the job of the architect. If one of the\narchitect’s jobs is ensuring there is a technical vision, then governance is about ensuring\nwhat we are building matches this vision, and evolving the vision if needed.\nArchitects are responsible for a lot of things. They need to ensure there is a set of\nprinciples that can guide development, and that these principles match the organization’s\nstrategy. They need to make sure as well that these principles don’t require working\npractices that make developers miserable. They need to keep up to date with new\ntechnology, and know when to make the right trade-offs. This is an awful lot of\nresponsibility. All that, and they also need to carry people with them — that is, to ensure\nthat the colleagues they are working with understand the decisions being made and are\nbrought in to carry them out. Oh, and as we’ve already mentioned: they need to spend\nsome time with the teams to understand the impact of their decisions, and perhaps even\ncode too.\nA tall order? Absolutely. But I am firmly of the opinion that they shouldn’t do this alone.\nA properly functioning governance group can work together to share the work and shape\nthe vision.\nNormally, governance is a group activity. It could be an informal chat with a small enough\nteam, or a more structured regular meeting with formal group membership for a larger\nscope. This is where I think the principles we covered earlier should be discussed and\nchanged as required. This group needs to be led by a technologist, and to consist\npredominantly of people who are executing the work being governed. This group should\nalso be responsible for tracking and managing technical risks.\nA model I greatly favor is having the architect chair the group, but having the bulk of the\ngroup drawn from the technologists of each delivery team — the leads of each team at a\nminimum. The architect is responsible for making sure the group works, but the group as a\nwhole is responsible for governance. This shares the load, and ensures that there is a\nhigher level of buy-in. It also ensures that information flows freely from the teams into the\ngroup, and as a result, the decision making is much more sensible and informed.\nSometimes, the group may make decisions with which the architect disagrees. At this\npoint, what is the architect to do? Having been in this position before, I can tell you this is\none of the most challenging situations to face. Often, I take the approach that I should go\nwith the group decision. I take the view that I’ve done my best to convince people, but\nultimately I wasn’t convincing enough. The group is often much wiser than the individual,\nand I’ve been proven wrong more than once! And imagine how disempowering it can be\nfor a group to have been given space to come up with a decision, and then ultimately be\nignored. But sometimes I have overruled the group. But why, and when? How do you pick\nthe lines?\nThink about teaching children to ride a bike. You can’t ride it for them. You watch them\nwobble, but if you stepped in every time it looked like they might fall off, then they’d\nnever learn, and in any case they fall off far less than you think they will! But if you see\nthem about to veer into traffic, or into a nearby duck pond, then you have to step in.\nLikewise, as an architect, you need to have a firm grasp of when, figuratively, your team is\nsteering into a duck pond. You also need to be aware that even if you know you are right\nand overrule the team, this can undermine your position and also make the team feel that\nthey don’t have a say. Sometimes the right thing is to go along with a decision you don’t\nagree with. Knowing when to do this and when not to is tough, but is sometimes vital.",7354
09-Shared and Hidden Models.pdf,09-Shared and Hidden Models,"Building a Team\nBeing the main point person responsible for the technical vision of your system and\nensuring that you’re executing on this vision isn’t just about making technology decisions.\nIt’s the people you work with who will be doing the work. Much of the role of the\ntechnical leader is about helping grow them — to help them understand the vision\nthemselves — and also ensuring that they can be active participants in shaping and\nimplementing the vision too.\nHelping the people around you on their own career growth can take many forms, most of\nwhich are outside the scope of this book. There is one aspect, though, where a\nmicroservice architecture is especially relevant. With larger, monolithic systems, there are\nfewer opportunities for people to step up and \nown\n something. With microservices, on the\nother hand, we have multiple autonomous codebases that will have their own independent\nlifecycles. Helping people step up by having them take ownership of individual services\nbefore accepting more responsibility can be a great way to help them achieve their own\ncareer goals, and at the same time lightens the load on whoever is in charge!\nI am a strong believer that great software comes from great people. If you worry only\nabout the technology side of the equation, you’re missing way more than half of the\npicture.\nSummary\nTo summarize this chapter, here are what I see as the core responsibilities of the\nevolutionary architect:\nVision\nEnsure there is a clearly communicated technical vision for the system that will help\nyour system meet the requirements of your customers and organization\nEmpathy\nUnderstand the impact of your decisions on your customers and colleagues\nCollaboration\nEngage with as many of your peers and colleagues as possible to help define, refine,\nand execute the vision\nAdaptability\nMake sure that the technical vision changes as your customers or organization\nrequires it\nAutonomy\nFind the right balance between standardizing and enabling autonomy for your teams\nGovernance\nEnsure that the system being implemented fits the technical vision\nThe evolutionary architect is one who understands that pulling off this feat is a constant\nbalancing act. Forces are always pushing you one way or another, and understanding\nwhere to push back or where to go with the flow is often something that comes only with\nexperience. But the worst reaction to all these forces that push us toward change is to\nbecome more rigid or fixed in our thinking.\nWhile much of the advice in this chapter can apply to any systems architect, microservices\ngive us many more decisions to make. Therefore, being better able to balance all of these\ntrade-offs is essential.\nIn the next chapter, we’ll take some of our newfound awareness of the architect’s role with\nus as we start thinking about how to find the right boundaries for our \nmicroservices\n.\nChapter 3. \nHow to Model Services\nMy opponent’s reasoning reminds me of the heathen, who, being asked on what the\nworld stood, replied, “On a tortoise.” But on what does the tortoise stand? “On\nanother tortoise.”\nJoseph Barker (1854)\nSo you know what microservices are, and hopefully have a sense of their key benefits.\nYou’re probably eager now to go and start making them, right? But where to start? In this\nchapter, we’ll look at how to think about the boundaries of your microservices that will\nhopefully maximize the upsides and avoid some of the potential downsides. But first, we\nneed something to work with.\nIntroducing MusicCorp\nBooks about ideas work better with examples. Where possible, I’ll be sharing stories from\nreal-world situations, but I’ve found it’s also useful to have a fictional domain with which\nto work. Throughout the book, we’ll be returning to this domain, seeing how the concept\nof microservices works within this world.\nSo let’s turn our attention to the cutting-edge online retailer MusicCorp. MusicCorp was\nrecently a brick-and-mortar retailer, but after the bottom dropped out of the gramophone\nrecord business it focused more and more of its efforts online. The company has a website,\nbut feels that now is the time to double-down on the online world. After all, those iPods\nare just a passing fad (Zunes are way better, obviously) and music fans are quite happy to\nwait for CDs to arrive at their doorsteps. Quality over convenience, right? And while\nwe’re at it, what’s this Spotify thing people keep talking about — some sort of skin\ntreatment for teenagers?\nDespite being a little behind the curve, MusicCorp has grand ambitions. Luckily, it has\ndecided that its best chance of taking over the world is by making sure it can make\nchanges as easily as possible. Microservices for the win!\nWhat Makes a Good Service?\nBefore the team from MusicCorp tears off into the distance, creating service after service\nin an attempt to deliver eight-track tapes to all and sundry, let’s put the brakes on and talk\na bit about the most important underlying idea we need to keep in mind. What makes a\ngood service? If you’ve survived a failed SOA implementation, you may have some idea\nwhere I’m going next. But just in case you aren’t that (un)fortunate, I want you to focus on\ntwo key concepts: \nloose coupling\n and \nhigh cohesion\n. We’ll talk in detail throughout the\nbook about other ideas and practices, but they are all for naught if we get these two thing\nwrong.\nDespite the fact that these two terms are used a lot, especially in the context of object-\noriented systems, it is worth discussing what they mean in terms of microservices.\nLoose Coupling\nWhen services are loosely coupled, a change to one service should not require a change to\nanother. The whole point of a microservice is being able to make a change to one service\nand deploy it, without needing to change any other part of the system. This is really quite\nimportant.\nWhat sort of things cause tight coupling? A classic mistake is to pick an integration style\nthat tightly binds one service to another, causing changes inside the service to require a\nchange to consumers. We’ll discuss how to avoid this in more depth in \nChapter 4\n.\nA loosely coupled service knows as little as it needs to about the services with which it\ncollaborates. This also means we probably want to limit the number of different types of\ncalls from one service to another, because beyond the potential performance problem,\nchatty communication can lead to tight coupling.\nHigh Cohesion\nWe want related behavior to sit together, and unrelated behavior to sit elsewhere. Why?\nWell, if we want to change behavior, we want to be able to change it in one place, and\nrelease that change as soon as possible. If we have to change that behavior in lots of\ndifferent places, we’ll have to release lots of different services (perhaps at the same time)\nto deliver that change. Making changes in lots of different places is slower, and deploying\nlots of services at once is risky — both of which we want to avoid.\nSo we want to find boundaries within our problem domain that help ensure that related\nbehavior is in one place, and that communicate with other boundaries as loosely as\npossible.\nThe Bounded Context\nEric Evans’s book \nDomain-Driven Design\n (Addison-Wesley) focuses on how to create\nsystems that model real-world domains. The book is full of great ideas like using\nubiquitous language, repository abstractions, and the like, but there is one very important\nconcept Evans introduces that completely passed me by at first: \nbounded context\n. The idea\nis that any given domain consists of multiple bounded contexts, and residing within each\nare things (Eric uses the word \nmodel\n a lot, which is probably better than \nthings\n) that do not\nneed to be communicated outside as well as things that are shared externally with other\nbounded contexts. Each bounded context has an explicit interface, where it decides what\nmodels to share with other contexts.\nAnother definition of bounded contexts I like is “a specific responsibility enforced by\nexplicit boundaries.”\n1\n If you want information from a bounded context, or want to make\nrequests of functionality within a bounded context, you communicate with its explicit\nboundary using models. In his book, Evans uses the analogy of cells, where “[c]ells can\nexist because their membranes define what is in and out and determine what can pass.”\nLet’s return for a moment to the MusicCorp business. Our domain is the whole business in\nwhich we are operating. It covers everything from the warehouse to the reception desk,\nfrom finance to ordering. We may or may not model all of that in our software, but that is\nnonetheless the domain in which we are operating. Let’s think about parts of that domain\nthat look like the bounded contexts that Evans refers to. At MusicCorp, our warehouse is a\nhive of activity — managing orders being shipped out (and the odd return), taking delivery\nof new stock, having forklift truck races, and so on. Elsewhere, the finance department is\nperhaps less fun-loving, but still has a very important function inside our organization.\nThese employees manage payroll, keep the company accounts, and produce important\nreports. Lots of reports. They probably also have interesting desk toys.\nShared and Hidden Models\nFor MusicCorp, we can then consider the finance department and the warehouse to be two\nseparate bounded contexts. They both have an explicit interface to the outside world (in\nterms of inventory reports, pay slips, etc.), and they have details that only they need to\nknow about (forklift trucks, calculators).\nNow the finance department doesn’t need to know about the detailed inner workings of the\nwarehouse. It does need to know some things, though — for example it needs to know\nabout stock levels to keep the accounts up to date. \nFigure 3-1\n shows an example context\ndiagram. We see concepts that are internal to the warehouse, like Picker (people who pick\norders), shelves that represent stock locations, and so on. Likewise, the company’s general\nledger is integral to finance but is not shared externally here.\nFigure 3-1. \nA shared model between the finance department and the warehouse\nTo be able to work out the valuation of the company, though, the finance employees need\ninformation about the stock we hold. The stock item then becomes a shared model\nbetween the two contexts. However, note that we don’t need to blindly expose everything\nabout the stock item from the warehouse context. For example, although internally we\nkeep a record on a stock item as to where it should live within the warehouse, that doesn’t\nneed to be exposed in the shared model. So there is the internal-only representation, and\nthe external representation we expose. In many ways, this foreshadows the discussion\naround REST in \nChapter 4\n.\nSometimes we may encounter models with the same name that have very different\nmeanings in different contexts too. For example, we might have the concept of a return,\nwhich represents a customer sending something back. Within the context of the customer,\na return is all about printing a shipping label, dispatching a package, and waiting for a\nrefund. For the warehouse, this could represent a package that is about to arrive, and a\nstock item that needs to be restocked. It follows that within the warehouse we store\nadditional information associated with the return that relates to the tasks to be carried out;\nfor example, we may generate a restock request. The shared model of the return becomes\nassociated with different processes and supporting entities within each bounded context,\nbut that is very much an internal concern within the context itself.",11797
10-Turtles All the Way Down.pdf,10-Turtles All the Way Down,"Modules and Services\nBy thinking clearly about what models should be shared, and not sharing our internal\nrepresentations, we avoid one of the potential pitfalls that can result in tight coupling (the\nopposite of what we want). We have also identified a boundary within our domain where\nall like-minded business capabilities should live, giving us the high cohesion we want.\nThese bounded contexts, then, lend themselves extremely well to being compositional\nboundaries.\nAs we discussed in \nChapter 1\n, we have the option of using modules within a process\nboundary to keep related code together and attempt to reduce the coupling to other\nmodules in the system. When you’re starting out on a new codebase, this is probably a\ngood place to begin. So once you have found your bounded contexts in your domain,\nmake sure they are modeled within your codebase as modules, with shared and hidden\nmodels.\nThese modular boundaries then become excellent candidates for microservices. In general,\nmicroservices should cleanly align to bounded contexts. Once you become very proficient,\nyou may decide to skip the step of keeping the bounded context modeled as a module\nwithin a more monolithic system, and jump straight for a separate service. When starting\nout, however, keep a new system on the more monolithic side; getting service boundaries\nwrong can be costly, so waiting for things to stabilize as you get to grips with a new\ndomain is sensible. We’ll discuss this more in \nChapter 5\n, along with techniques to help\nbreak apart existing systems into microservices.\nSo, if our service boundaries align to the bounded contexts in our domain, and our\nmicroservices represent those bounded contexts, we are off to an excellent start in\nensuring that our microservices are loosely coupled and strongly cohesive.\nPremature Decomposition\nAt ThoughtWorks, we ourselves experienced the challenges of splitting out microservices\ntoo quickly. Aside from consulting, we also create a few products. One of them is SnapCI,\na hosted continuous integration and continuous delivery tool (we’ll discuss those concepts\nlater in \nChapter 6\n). The team had previously worked on another similar tool, Go-CD, a\nnow open source continuous delivery tool that can be deployed locally rather than being\nhosted in the cloud.\nAlthough there was some code reuse very early on between the SnapCI and Go-CD\nprojects, in the end SnapCI turned out to be a completely new codebase. Nonetheless, the\nprevious experience of the team in the domain of CD tooling emboldened them to move\nmore quickly in identifying boundaries, and building their system as a set of\nmicroservices.\nAfter a few months, though, it became clear that the use cases of SnapCI were subtly\ndifferent enough that the initial take on the service boundaries wasn’t quite right. This led\nto lots of changes being made across services, and an associated high cost of change.\nEventually the team merged the services back into one monolithic system, giving them\ntime to better understand where the boundaries should exist. A year later, the team was\nthen able to split the monolithic system apart into microservices, whose boundaries proved\nto be much more stable. This is far from the only example of this situation I have seen.\nPrematurely decomposing a system into microservices can be costly, especially if you are\nnew to the domain. In many ways, having an existing codebase you want to decompose\ninto microservices is much easier than trying to go to microservices from the beginning.\nBusiness Capabilities\nWhen you start to think about the bounded contexts that exist in your organization, you\nshould be thinking not in terms of data that is shared, but about the capabilities those\ncontexts provide the rest of the domain. The warehouse may provide the capability to get a\ncurrent stock list, for example, or the finance context may well expose the end-of-month\naccounts or let you set up payroll for a new recruit. These capabilities may require the\ninterchange of information — shared models — but I have seen too often that thinking\nabout \ndata\n leads to anemic, CRUD-based (create, read, update, delete) services. So ask\nfirst “What does this context do?”, and then “So what data does it need to do that?”\nWhen modeled as services, these capabilities become the key operations that will be\nexposed over the wire to other collaborators.\nTurtles All the Way Down\nAt the start, you will probably identify a number of coarse-grained bounded contexts. But\nthese bounded contexts can in turn contain further bounded contexts. For example, you\ncould decompose the warehouse into capabilities associated with order fulfillment,\ninventory management, or goods receiving. When considering the boundaries of your\nmicroservices, first think in terms of the larger, coarser-grained contexts, and then\nsubdivide along these nested contexts when you’re looking for the benefits of splitting out\nthese seams.\nI have seen these nested contexts remaining hidden to other, collaborating microservices\nto great effect. To the outside world, they are still making use of business capabilities in\nthe warehouse, but they are unaware that their requests are actually being mapped\ntransparently to two or more separate services, as you can see in \nFigure 3-2\n. \nSometimes,\nyou will decide it makes more sense for the higher-level bounded context to not be\nexplicitly modeled as a service boundary, as in \nFigure 3-3\n, so rather than a single\nwarehouse boundary, you might instead split out inventory, order fulfillment, and goods\nreceiving.\nFigure 3-2. \nMicroservices representing nested bounded contexts \nhidden\n inside the \nwarehouse\nFigure 3-3. \nThe bounded contexts inside the warehouse being \npopped up\n into their own top-level contexts\nIn general, there isn’t a hard-and-fast rule as to what approach makes the most sense.\nHowever, whether you choose the nested approach over the full separation approach\nshould be based on your organizational structure. If order fulfillment, inventory\nmanagement, and goods receiving are managed by different teams, they probably deserve\ntheir status as \ntop-level\n microservices. If, on the other hand, all of them are managed by\none team, then the nested model makes more sense. This is because of the interplay of\norganizational structures and software architecture, which we will discuss toward the end\nof the book in \nChapter 10\n.\nAnother reason to prefer the nested approach could be to chunk up your architecture to\nsimplify testing. For example, when testing services that consume the warehouse, I don’t\nhave to stub each service inside the warehouse context, just the more coarse-grained API.\nThis can also give you a unit of isolation when considering larger-scoped tests. I may, for\nexample, decide to have end-to-end tests where I launch all services inside the warehouse\ncontext, but for all other collaborators I might stub them out. We’ll explore more about\ntesting and isolation in \nChapter 7\n.",7081
11-The Technical Boundary.pdf,11-The Technical Boundary,"Communication in Terms of Business Concepts\nThe changes we implement to our system are often about changes the business wants to\nmake to how the system behaves. We are changing functionality — capabilities — that are\nexposed to our customers. If our systems are decomposed along the bounded contexts that\nrepresent our domain, the changes we want to make are more likely to be isolated to one,\nsingle microservice boundary. This reduces the number of places we need to make a\nchange, and allows us to deploy that change quickly.\nIt’s also important to think of the communication between these microservices in terms of\nthe same business concepts. The modeling of your software after your business domain\nshouldn’t stop at the idea of bounded contexts. The same terms and ideas that are shared\nbetween parts of your organization should be reflected in your interfaces. It can be useful\nto think of forms being sent between these microservices, much as forms are sent around\nan organization.\nThe Technical Boundary\nIt can be useful to look at what can go wrong when services are modeled incorrectly. A\nwhile back, a few colleagues and I were working with a client in California, helping the\ncompany adopt some cleaner code practices and move more toward automated testing.\nWe’d started with some of the low-hanging fruit, such as service decomposition, when we\nnoticed something much more worrying. I can’t go into too much detail as to what the\napplication did, but it was a public-facing application with a large, global customer base.\nThe team, and system, had grown. Originally one person’s vision, the system had taken on\nmore and more features, and more and more users. Eventually, the organization decided to\nincrease the capacity of the team by having a new group of developers based in Brazil take\non some of the work. The system got split up, with the front half of the application being\nessentially stateless, implementing the public-facing website, as shown in \nFigure 3-4\n. The\nback half of the system was simply a remote procedure call (RPC) interface over a data\nstore. Essentially, imagine you’d taken a repository layer in your codebase and made this a\nseparate service.\nFigure 3-4. \nA service boundary split across a technical seam\nChanges frequently had to be made to both services. Both services spoke in terms of low-\nlevel, RPC-style method calls, which were overly brittle (we’ll discuss this futher in\nChapter 4\n). The service interface was also very chatty too, resulting in performance issues.\nThis resulted in the need for elaborate RPC-batching mechanisms. I called this \nonion\narchitecture\n, as it had lots of layers and made me cry when we had to cut through it.\nNow on the face of it, the idea of splitting the previously monolithic system along\ngeographical/organizational lines makes perfect sense, as we’ll expand on in \nChapter 10\n.\nHere, however, rather than taking a vertical, business-focused slice through the stack, the\nteam picked what was previously an in-process API and made a horizontal slice.\nMaking decisions to model service boundaries along technical seams isn’t always wrong. I\nhave certainly seen this make lots of sense when an organization is looking to achieve\ncertain performance objectives, for example. However, it should be your secondary driver\nfor finding these seams, not your primary one.",3397
12-The Shared Database.pdf,12-The Shared Database,"Summary\nIn this chapter, you’ve learned a bit about what makes a good service, and how to find\nseams in our problem space that give us the dual benefits of both loose coupling and high\ncohesion. Bounded contexts are a vital tool in helping us find these seams, and by aligning\nour microservices to these boundaries we ensure that the resulting system has every\nchance of keeping those virtues intact. We’ve also got a hint about how we can subdivide\nour microservices further, something we’ll explore in more depth later. And we also\nintroduced MusicCorp, the example domain that we will use throughout this book.\nThe ideas presented in Eric Evans’s \nDomain-Driven Design\n are very useful to us in\nfinding sensible boundaries for our services, and I’ve just scratched the surface here. I\nrecommend Vaughn Vernon’s book \nImplementing Domain-Driven Design\n (Addison-\nWesley) to help you understand the practicalities of this approach.\nAlthough this chapter has been mostly high-level, we need to get much more technical in\nthe next. There are many pitfalls associated with implementing interfaces between\nservices that can lead to all sorts of trouble, and we will have to take a deep dive into this\ntopic if we are to keep our systems from becoming a giant, tangled mess.\n1\n \nhttp://bit.ly/bounded-context-explained\nChapter 4. \nIntegration\nGetting integration right is the single most important aspect of the technology associated\nwith microservices in my opinion. Do it well, and your microservices retain their\nautonomy, allowing you to change and release them independent of the whole. Get it\nwrong, and disaster awaits. Hopefully once you’ve read this chapter you’ll learn how to\navoid some of the biggest pitfalls that have plagued other attempts at SOA and could yet\nawait you in your journey to microservices.\nLooking for the Ideal Integration Technology\nThere is a bewildering array of options out there for how one microservice can talk to\nanother. But which is the right one: SOAP? XML-RPC? REST? Protocol buffers? We’ll\ndive into those in a moment, but before we do, let’s think about what we want out of\nwhatever technology we pick.\nAvoid Breaking Changes\nEvery now and then, we may make a change that requires our consumers to also change.\nWe’ll discuss how to handle this later, but we want to pick technology that ensures this\nhappens as rarely as possible. For example, if a microservice adds new fields to a piece of\ndata it sends out, existing consumers shouldn’t be impacted.\nKeep Your APIs Technology-Agnostic\nIf you have been in the IT industry for more than 15 minutes, you don’t need me to tell\nyou that we work in a space that is changing rapidly. The one certainty \nis\n change. New\ntools, frameworks, and languages are coming out all the time, implementing new ideas\nthat can help us work faster and more effectively. Right now, you might be a .NET shop.\nBut what about in a year from now, or five years from now? What if you want to\nexperiment with an alternative technology stack that might make you more productive?\nI am a big fan of keeping my options open, which is why I am such a fan of microservices.\nIt is also why I think it is very important to ensure that you keep the APIs used for\ncommunication between microservices technology-agnostic. This means avoiding\nintegration technology that dictates what technology stacks we can use to implement our\nmicroservices.\nMake Your Service Simple for Consumers\nWe want to make it easy for consumers to use our service. Having a beautifully factored\nmicroservice doesn’t count for much if the cost of using it as a consumer is sky high! So\nlet’s think about what makes it easy for consumers to use our wonderful new service.\nIdeally, we’d like to allow our clients full freedom in their technology choice, but on the\nother hand, providing a client library can ease adoption. Often, however, such libraries are\nincompatible with other things we want to achieve. For example, we might use client\nlibraries to make it easy for consumers, but this can come at the cost of increased\ncoupling.\nHide Internal Implementation Detail\nWe don’t want our consumers to be bound to our internal implementation. This leads to\nincreased coupling. This means that if we want to change something inside our\nmicroservice, we can break our consumers by requiring them to also change. That\nincreases the cost of change — the exact result we are trying to avoid. It also means we\nare less likely to want to make a change for fear of having to upgrade our consumers,\nwhich can lead to increased technical debt within the service. So any technology that\npushes us to expose internal representation detail should be avoided.\nInterfacing with Customers\nNow that we’ve got a few guidelines that can help us select a good technology to use for\nintegration between services, let’s look at some of the most common options out there and\ntry to work out which one works best for us. To help us think this through, let’s pick a\nreal-world example from MusicCorp.\nCustomer creation at first glance could be considered a simple set of CRUD operations,\nbut for most systems it is more complex than that. Enrolling a new customer may need to\nkick off additional processes, like setting up financial payments or sending out welcome\nemails. And when we change or delete a customer, other business processes might get\ntriggered as well.\nSo with that in mind, we should look at some different ways in which we might want to\nwork with customers in our MusicCorp system.\nThe Shared Database\nBy far the most common form of integration that I or any of my colleagues see in the\nindustry is database (DB) integration. In this world, if other services want information\nfrom a service, they reach into the database. And if they want to change it, they reach into\nthe database! This is really simple when you first think about it, and is probably the fastest\nform of integration to start with — which probably explains its popularity.\nFigure 4-1\n shows our registration UI, which creates customers by performing SQL\noperations directly on the database. It also shows our call center application that views and\nedits customer data by running SQL on the database. And the warehouse updates\ninformation about customer orders by querying the database. This is a common enough\npattern, but it’s one fraught with difficulties.\nFigure 4-1. \nUsing DB integration to access and change customer information\nFirst, we are allowing external parties to view and bind to internal implementation details.\nThe data structures I store in the DB are fair game to all; they are shared in their entirety\nwith all other parties with access to the database. If I decide to change my schema to better\nrepresent my data, or make my system easier to maintain, I can break my consumers. The\nDB is effectively a very large, shared API that is also quite brittle. If I want to change the\nlogic associated with, say, how the helpdesk manages customers and this requires a change\nto the database, I have to be extremely careful that I don’t break parts of the schema used\nby other services. This situation normally results in requiring a large amount of regression\ntesting.\nSecond, my consumers are tied to a specific technology choice. Perhaps right now it\nmakes sense to store customers in a relational database, so my consumers use an\nappropriate (potentially DB-specific) driver to talk to it. What if over time we realize we\nwould be better off storing data in a nonrelational database? Can it make that decision? So\nconsumers are intimately tied to the implementation of the customer service. As we\ndiscussed earlier, we really want to ensure that implementation detail is hidden from\nconsumers to allow our service a level of autonomy in terms of how it changes its\ninternals over time. Goodbye, loose coupling.\nFinally, let’s think about behavior for a moment. There is going to be logic associated with\nhow a customer is changed. Where is that logic? If consumers are directly manipulating\nthe DB, then they have to own the associated logic. The logic to perform the same sorts of\nmanipulation to a customer may now be spread among multiple consumers. If the\nwarehouse, registration UI, and call center UI all need to edit customer information, I need\nto fix a bug or change the behavior in three different places, and deploy those changes too.\nGoodbye, cohesion.\nRemember when we talked about the core principles behind good microservices? Strong\ncohesion and loose coupling — with database integration, we lose both things. Database\nintegration makes it easy for services to share data, but does nothing about \nsharing\nbehavior\n. Our internal representation is exposed over the wire to our consumers, and it can\nbe very difficult to avoid making breaking changes, which inevitably leads to a fear of any\nchange at all. Avoid at (nearly) all costs.\nFor the rest of the chapter, we’ll explore different styles of integration that involve\ncollaborating services, which themselves hide their own internal representations.",9155
13-Orchestration Versus Choreography.pdf,13-Orchestration Versus Choreography,"Synchronous Versus Asynchronous\nBefore we start diving into the specifics of different technology choices, we should discuss\none of the most important decisions we can make in terms of how services collaborate.\nShould communication be synchronous or asynchronous? This fundamental choice\ninevitably guides us toward certain implementation detail.\nWith synchronous communication, a call is made to a remote server, which blocks until\nthe operation completes. With asynchronous communication, the caller doesn’t wait for\nthe operation to complete before returning, and may not even care whether or not the\noperation completes at all.\nSynchronous communication can be easier to reason about. We know when things have\ncompleted successfully or not. Asynchronous communication can be very useful for long-\nrunning jobs, where keeping a connection open for a long period of time between the\nclient and server is impractical. It also works very well when you need low latency, where\nblocking a call while waiting for the result can slow things down. Due to the nature of\nmobile networks and devices, firing off requests and assuming things have worked (unless\ntold otherwise) can ensure that the UI remains responsive even if the network is highly\nlaggy. On the flipside, the technology to handle asynchronous communication can be a bit\nmore involved, as we’ll discuss shortly.\nThese two different modes of communication can enable two different idiomatic styles of\ncollaboration: \nrequest/response\n or \nevent-based\n. With request/response, a client initiates a\nrequest and waits for the response. This model clearly aligns well to synchronous\ncommunication, but can work for asynchronous communication too. I might kick off an\noperation and register a callback, asking the server to let me know when my operation has\ncompleted.\nWith an event-based collaboration, we invert things. Instead of a client initiating requests\nasking for things to be done, it instead says \nthis thing happened\n and expects other parties\nto know what to do. We never tell anyone else what to do. Event-based systems by their\nnature are asynchronous. The smarts are more evenly distributed — that is, the business\nlogic is not centralized into core brains, but instead pushed out more evenly to the various\ncollaborators. Event-based collaboration is also highly decoupled. The client that emits an\nevent doesn’t have any way of knowing who or what will react to it, which also means that\nyou can add new subscribers to these events without the client ever needing to know.\nSo are there any other drivers that might push us to pick one style over another? One\nimportant factor to consider is how well these styles are suited for solving an often-\ncomplex problem: how do we handle processes that span service boundaries and may be\nlong running?\nOrchestration Versus Choreography\nAs we start to model more and more complex logic, we have to deal with the problem of\nmanaging business processes that stretch across the boundary of individual services. And\nwith microservices, we’ll hit this limit sooner than usual. Let’s take an example from\nMusicCorp, and look at what happens when we create a customer:\n1\n. \nA new record is created in the loyalty points bank for the customer.\n2\n. \nOur postal system sends out a welcome pack.\n3\n. \nWe send a welcome email to the customer.\nThis is very easy to model conceptually as a flowchart, as we do in \nFigure 4-2\n.\nWhen it comes to actually implementing this flow, there are two styles of architecture we\ncould follow. With orchestration, we rely on a central brain to guide and drive the process,\nmuch like the conductor in an orchestra. With choreography, we inform each part of the\nsystem of its job, and let it work out the details, like dancers all finding their way and\nreacting to others around them in a ballet.\nFigure 4-2. \nThe process for creating a new customer\nLet’s think about what an orchestration solution would look like for this flow. Here,\nprobably the simplest thing to do would be to have our customer service act as the central\nbrain. On creation, it talks to the loyalty points bank, email service, and postal service as\nwe see in \nFigure 4-3\n, through a series of request/response calls. The customer service\nitself can then track where a customer is in this process. It can check to see if the\ncustomer’s account has been set up, or the email sent, or the post delivered. We get to take\nthe flowchart in \nFigure 4-2\n and model it directly into code. We could even use tooling that\nimplements this for us, perhaps using an appropriate rules engine. Commercial tools exist\nfor this very purpose in the form of business process modeling software. Assuming we use\nsynchronous request/response, we could even know if each stage has worked.\nFigure 4-3. \nHandling customer creation via orchestration\nThe downside to this orchestration approach is that the customer service can become too\nmuch of a central governing authority. It can become the hub in the middle of a web, and a\ncentral point where logic starts to live. I have seen this approach result in a small number\nof smart “god” services telling anemic CRUD-based services what to do.\nWith a choreographed approach, we could instead just have the customer service emit an\nevent in an asynchronous manner, saying \nCustomer created\n. The email service, postal\nservice, and loyalty points bank then just subscribe to these events and react accordingly,\nas in \nFigure 4-4\n. This approach is significantly more decoupled. If some other service\nneeded to reach to the creation of a customer, it just needs to subscribe to the events and\ndo its job when needed. The downside is that the explicit view of the business process we\nsee in \nFigure 4-2\n is now only implicitly reflected in our system.\nFigure 4-4. \nHandling customer creation via choreography\nThis means additional work is needed to ensure that you can monitor and track that the\nright things have happened. For example, would you know if the loyalty points bank had a\nbug and for some reason didn’t set up the correct account? One approach I like for dealing\nwith this is to build a monitoring system that explicitly matches the view of the business\nprocess in \nFigure 4-2\n, but then tracks what each of the services does as independent\nentities, letting you see odd exceptions mapped onto the more explicit process flow. The\nflowchart we saw earlier isn’t the driving force, but just one lens through which we can\nsee how the system is behaving.\nIn general, I have found that systems that tend more toward the choreographed approach\nare more loosely coupled, and are more flexible and amenable to change. You do need to\ndo extra work to monitor and track the processes across system boundaries, however. I\nhave found most heavily orchestrated implementations to be extremely brittle, with a\nhigher cost of change. With that in mind, I strongly prefer aiming for a choreographed\nsystem, where each service is smart enough to understand its role in the whole dance.\nThere are quite a few factors to unpack here. Synchronous calls are simpler, and we get to\nknow if things worked straightaway. If we like the semantics of request/response but are\ndealing with longer-lived processes, we could just initiate asynchronous requests and wait\nfor callbacks. On the other hand, asynchronous event collaboration helps us adopt a\nchoreographed approach, which can yield significantly more decoupled services —\nsomething we want to strive for to ensure our services are independently releasable.\nWe are, of course, free to mix and match. Some technologies will fit more naturally into\none style or another. We do, however, need to appreciate some of the different technical\nimplementation details that will further help us make the right call.\nTo start with, let’s look at two technologies that fit well when we are considering\nrequest/response: remote procedure call (RPC) and REpresentational State Transfer\n(REST).",8074
14-Brittleness.pdf,14-Brittleness,"Remote Procedure Calls\nRemote procedure call\n refers to the technique of making a local call and having it execute\non a remote service somewhere. There are a number of different types of RPC technology\nout there. Some of this technology relies on having an interface definition (SOAP, Thrift,\nprotocol buffers). The use of a separate interface definition can make it easier to generate\nclient and server stubs for different technology stacks, so, for example, I could have a Java\nserver exposing a SOAP interface, and a .NET client generated from the Web Service\nDefinition Language (WSDL) definition of the interface. Other technology, like Java RMI,\ncalls for a tighter coupling between the client and server, requiring that both use the same\nunderlying technology but avoid the need for a shared interface definition. All these\ntechnologies, however, have the same, core characteristic in that they make a local call\nlook like a remote call.\nMany of these technologies are binary in nature, like Java RMI, Thrift, or protocol buffers,\nwhile SOAP uses XML for its message formats. Some implementations are tied to a\nspecific networking protocol (like SOAP, which makes nominal use of HTTP), whereas\nothers might allow you to use different types of networking protocols, which themselves\ncan provide additional features. For example, TCP offers guarantees about delivery,\nwhereas UDP doesn’t but has a much lower overhead. This can allow you to use different\nnetworking technology for different use cases.\nThose RPC implementations that allow you to generate client and server stubs help you\nget started very, very fast. I can be sending content over a network boundary in no time at\nall. This is often one of the main selling points of RPC: its ease of use. The fact that I can\njust make a normal method call and theoretically ignore the rest is a huge boon.\nSome RPC implementations, though, do come with some downsides that can cause issues.\nThese issues aren’t always apparent initially, but nonetheless they can be severe enough to\noutweigh the benefits of being so easy to get up and running quickly.\nTechnology Coupling\nSome RPC mechanisms, like Java RMI, are heavily tied to a specific platform, which can\nlimit which technology can be used in the client and server. Thrift and protocol buffers\nhave an impressive amount of support for alternative languages, which can reduce this\ndownside somewhat, but be aware that sometimes RPC technology comes with\nrestrictions on interoperability.\nIn a way, this technology coupling can be a form of exposing internal technical\nimplementation details. For example, the use of RMI ties not only the client to the JVM,\nbut the server too.\nLocal Calls Are Not Like Remote Calls\nThe core idea of RPC is to hide the complexity of a remote call. Many implementations of\nRPC, though, hide too much. The drive in some forms of RPC to make remote method\ncalls look like local method calls hides the fact that these two things are very different. I\ncan make large numbers of local, in-process calls without worrying overly about the\nperformance. With RPC, though, the cost of marshalling and un-marshalling payloads can\nbe significant, not to mention the time taken to send things over the network. This means\nyou need to think differently about API design for remote interfaces versus local\ninterfaces. Just taking a local API and trying to make it a service boundary without any\nmore thought is likely to get you in trouble. In some of the worst examples, developers\nmay be using remote calls without knowing it if the abstraction is overly opaque.\nYou need to think about the network itself. Famously, the first of the fallacies of\ndistributed computing is \n“The network is reliable”\n. Networks aren’t reliable. They can and\nwill fail, even if your client and the server you are speaking to are fine. They can fail fast,\nthey can fail slow, and they can even malform your packets. You should assume that your\nnetworks are plagued with malevolent entities ready to unleash their ire on a whim.\nTherefore, the failure modes you can expect are different. A failure could be caused by the\nremote server returning an error, or by you making a bad call. Can you tell the difference,\nand if so, can you do anything about it? And what do you do when the remote server just\nstarts responding slowly? We’ll cover this topic when we talk about resiliency in\nChapter 11\n.\nBrittleness\nSome of the most popular implementations of RPC can lead to some nasty forms of\nbrittleness, Java’s RMI being a very good example. Let’s consider a very simple Java\ninterface that we have decided to make a remote API for our customer service. \nExample 4-\n1\n declares the methods we are going to expose remotely. Java RMI then generates the\nclient and server stubs for our method.\nExample 4-1. \nDefining a service endpoint using Java RMI\nimport\n \njava.rmi.Remote\n;\nimport\n \njava.rmi.RemoteException\n;\npublic\n \ninterface\n \nCustomerRemote\n \nextends\n \nRemote\n \n{\n  \npublic\n \nCustomer\n \nfindCustomer\n(\nString\n \nid\n)\n \nthrows\n \nRemoteException\n;\n  \npublic\n \nCustomer\n \ncreateCustomer\n(\nString\n \nfirstname\n,\n \nString\n \nsurname\n,\n \nString\n \nemailAddress\n)\n      \nthrows\n \nRemoteException\n;\n}\nIn this interface, \nfindCustomer\n takes the first name, surname, and email address. What\nhappens if we decide to allow the \nCustomer\n object to also be created with just an email\naddress? We could add a new method at this point pretty easily, like so:\n...\npublic\n \nCustomer\n \ncreateCustomer\n(\nString\n \nemailAddress\n)\n \nthrows\n \nRemoteException\n;\n...\nThe problem is that now we need to regenerate the client stubs too. Clients that want to\nconsume the new method need the new stubs, and depending on the nature of the changes\nto the specification, consumers that don’t need the new method may also need to have\ntheir stubs upgraded too. This is manageable, of course, but to a point. The reality is that\nchanges like this are fairly common. RPC endpoints often end up having a large number of\nmethods for different ways of creating or interacting with objects. This is due in part to the\nfact that we are still thinking of these remote calls as local ones.\nThere is another sort of brittleness, though. Let’s take a look at what our \nCustomer\n object\nlooks like:\npublic\n \nclass\n \nCustomer\n \nimplements\n \nSerializable\n \n{\n  \nprivate\n \nString\n \nfirstName\n;\n  \nprivate\n \nString\n \nsurname\n;\n  \nprivate\n \nString\n \nemailAddress\n;\n  \nprivate\n \nString\n \nage\n;\n}\nNow, what if it turns out that although we expose the \nage\n field in our \nCustomer\n objects,\nnone of our consumers ever use it? We decide we want to remove this field. But if the\nserver implementation removes \nage\n from its definition of this type, and we don’t do the\nsame to all the consumers, then even though they never used the field, the code associated\nwith deserializing the \nCustomer\n object on the consumer side will break. To roll out this\nchange, I would have to deploy both a new server and clients at the same time. This is a\nkey challenge with any RPC mechanism that promotes the use of binary stub generation:\nyou don’t get to separate client and server deployments. If you use this technology, lock-\nstep releases may be in your future.\nSimilar problems occur if I want to restructure the \nCustomer\n object even if I didn’t remove\nfields — for example, if I wanted to encapsulate \nfirstName\n and \nsurname\n into a new\nnaming\n type to make it easier to manage. I could, of course, fix this by passing around\ndictionary types as the parameters of my calls, but at that point, I lose many of the benefits\nof the generated stubs because I’ll still have to manually match and extract the fields I\nwant.\nIn practice, objects used as part of binary serialization across the wire can be thought of as\nexpand-only\n types. This brittleness results in the types being exposed over the wire and\nbecoming a mass of fields, some of which are no longer used but can’t be safely removed.",8178
15-Hypermedia As the Engine of Application State.pdf,15-Hypermedia As the Engine of Application State,"Is RPC Terrible?\nDespite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the\ncommon implementations that I have encountered can lead to the sorts of problems I have\noutlined here. Due to the challenges of using RMI, I would certainly give that technology\na wide berth. Many operations fall quite nicely into the RPC-based model, and more\nmodern mechanisms like protocol buffers or Thrift mitigate some of sins of the past by\navoiding the need for lock-step releases of client and server code.\nJust be aware of some of the potential pitfalls associated with RPC if you’re going to pick\nthis model. Don’t abstract your remote calls to the point where the network is completely\nhidden, and ensure that you can evolve the server interface without having to insist on\nlock-step upgrades for clients. Finding the right balance for your client code is important,\nfor example. Make sure your clients aren’t oblivious to the fact that a network call is going\nto be made. Client libraries are often used in the context of RPC, and if not structured right\nthey can be problematic. We’ll talk more about them shortly.\nCompared to database integration, RPC is certainly an improvement when we think about\noptions for request/response collaboration. But there’s another option to consider.\nREST\nREpresentational State Transfer (REST) is an architectural style inspired by the Web.\nThere are many principles and constraints behind the REST style, but we are going to\nfocus on those that really help us when we face integration challenges in a microservices\nworld, and when we’re looking for an alternative style to RPC for our service interfaces.\nMost important is the concept of resources. You can think of a resource as a thing that the\nservice itself knows about, like a \nCustomer\n. The server creates different representations of\nthis \nCustomer\n on request. How a resource is shown externally is \ncompletely\n decoupled\nfrom how it is stored internally. A client might ask for a JSON representation of a\nCustomer\n, for example, even if it is stored in a completely different format. Once a client\nhas a representation of this \nCustomer\n, it can then make requests to change it, and the\nserver may or may not comply with them.\nThere are many different styles of REST, and I touch only briefly on them here. I strongly\nrecommend you take a look at the \nRichardson Maturity Model\n, where the different styles\nof REST are compared.\nREST itself doesn’t really talk about underlying protocols, although it is most commonly\nused over HTTP. I have seen implementations of REST using very different protocols\nbefore, such as serial or USB, although this can require a lot of work. Some of the features\nthat HTTP gives us as part of the specification, such as verbs, make implementing REST\nover HTTP easier, whereas with other protocols you’ll have to handle these features\nyourself.\nREST and HTTP\nHTTP itself defines some useful capabilities that play very well with the REST style. For\nexample, the HTTP verbs (e.g., GET, POST, and PUT) already have well-understood\nmeanings in the HTTP specification as to how they should work with resources. The\nREST architectural style actually tells us that methods should behave the same way on all\nresources, and the HTTP specification happens to define a bunch of methods we can use.\nGET retrieves a resource in an idempotent way, and POST creates a new resource. This\nmeans we can avoid lots of different \ncreateCustomer\n or \neditCustomer\n methods. Instead,\nwe can simply POST a customer representation to request that the server create a new\nresource, and initiate a GET request to retrieve a representation of a resource.\nConceptually, there is one \nendpoint\n in the form of a \nCustomer\n resource in these cases, and\nthe operations we can carry out upon it are baked into the HTTP protocol.\nHTTP also brings a large ecosystem of supporting tools and technology. We get to use\nHTTP caching proxies like Varnish and load balancers like mod_proxy, and many\nmonitoring tools already have lots of support for HTTP out of the box. These building\nblocks allow us to handle large volumes of HTTP traffic and route them smartly, in a fairly\ntransparent way. We also get to use all the available security controls with HTTP to secure\nour communications. From basic auth to client certs, the HTTP ecosystem gives us lots of\ntools to make the security process easier, and we’ll explore that topic more in \nChapter 9\n.\nThat said, to get these benefits, you have to use HTTP well. Use it badly, and it can be as\ninsecure and hard to scale as any other technology out there. Use it right, though, and you\nget a lot of help.\nNote that HTTP can be used to implement RPC too. SOAP, for example, gets routed over\nHTTP, but unfortunately uses very little of the specification. Verbs are ignored, as are\nsimple things like HTTP error codes. All too often, it seems, the existing, well-understood\nstandards and technology are ignored in favor of new standards that can only be\nimplemented using brand-new technology — conveniently provided by the same\ncompanies that help design the new standards in the first place!\nHypermedia As the Engine of Application State\nAnother principle introduced in REST that can help us avoid the coupling between client\nand server is the concept of \nhypermedia as the engine of application state\n (often\nabbreviated as HATEOAS, and boy, did it need an abbreviation). This is fairly dense\nwording and a fairly interesting concept, so let’s break it down a bit.\nHypermedia is a concept whereby a piece of content contains links to various other pieces\nof content in a variety of formats (e.g., text, images, sounds). This should be pretty\nfamiliar to you, as it’s what the average web page does: you follow links, which are a form\nof hypermedia controls, to see related content. The idea behind HATEOAS is that clients\nshould perform interactions (potentially leading to state transitions) with the server via\nthese links to other resources. It doesn’t need to know where exactly customers live on the\nserver by knowing which URI to hit; instead, the client looks for and navigates links to\nfind what it needs.\nThis is a bit of an odd concept, so let’s first step back and consider how people interact\nwith a web page, which we’ve already established is rich with hypermedia controls.\nThink of the Amazon.com shopping site. The location of the shopping cart has changed\nover time. The graphic has changed. The link has changed. But as humans we are smart\nenough to still see a shopping cart, know what it is, and interact with it. We have an\nunderstanding of what a shopping cart means, even if the exact form and underlying\ncontrol used to represent it has changed. We know that if we want to view the cart, this is\nthe control we want to interact with. This is why web pages can change incrementally over\ntime. As long as these implicit contracts between the customer and the website are still\nmet, changes don’t need to be breaking changes.\nWith hypermedia controls, we are trying to achieve the same level of \nsmarts\n for our\nelectronic consumers. Let’s look at a hypermedia control that we might have for\nMusicCorp\n. We’ve accessed a resource representing a catalog entry for a given album in\nExample 4-2\n. Along with information about the album, we see a number of hypermedia\ncontrols.\nExample 4-2. \nHypermedia controls used on an album listing\n<album>\n  \n<name>\nGive Blood\n</name>\n  \n<link\n \nrel=\n""/artist""\n \nhref=\n""/artist/theBrakes""\n \n/>\n \n  \n<description>\n    Awesome, short, brutish, funny and loud. Must buy!\n  \n</description>\n  \n<link\n \nrel=\n""/instantpurchase""\n \nhref=\n""/instantPurchase/1234""\n \n/>\n \n</album>\nThis hypermedia control shows us where to find information about the artist.\nAnd if we want to purchase the album, we now know where to go.\nIn this document, we have two hypermedia controls. The client reading such a document\nneeds to know that a control with a relation of \nartist\n is where it needs to navigate to get\ninformation about the artist, and that \ninstantpurchase\n is part of the protocol used to\npurchase the album. The client has to understand the semantics of the API in much the\nsame way as a human being needs to understand that on a shopping website the cart is\nwhere the items to be purchased will be.\nAs a client, I don’t need to know which URI scheme to access to \nbuy\n the album, I just\nneed to access the resource, find the buy control, and navigate to that. The buy control\ncould change location, the URI could change, or the site could even send me to another\nservice altogether, and as a client I wouldn’t care. This gives us a huge amount of\ndecoupling between the client and server.\nWe are greatly abstracted from the underlying detail here. We could completely change the\nimplementation of how the control is presented as long as the client can still find a control\nthat matches its understanding of the protocol, in the same way that a shopping cart\ncontrol might go from being a simple link to a more complex JavaScript control. We are\nalso free to add new controls to the document, perhaps representing new state transitions\nthat we can perform on the resource in question. We would end up breaking our\nconsumers only if we fundamentally changed the semantics of one of the controls so it\nbehaved very differently, or if we removed a control altogether.\nUsing these controls to decouple the client and server yields significant benefits over time\nthat greatly offset the small increase in the time it takes to get these protocols up and\nrunning. By following the links, the client gets to progressively discover the API, which\ncan be a really handy capability when we are implementing new clients.\nOne of the downsides is that this navigation of controls can be quite chatty, as the client\nneeds to follow links to find the operation it wants to perform. Ultimately, this is a trade-\noff. I would suggest you start with having your clients navigate these controls first, then\noptimize later if necessary. Remember that we have a large amount of help out of the box\nby using HTTP, which we discussed earlier. The evils of premature optimization have\nbeen well documented before, so I don’t need to expand upon them here. Also note that a\nlot of these approaches were developed to create distributed hypertext systems, and not all\nof them fit! Sometimes you’ll find yourself just wanting good old-fashioned RPC.\nPersonally, I am a fan of using links to allow consumers to navigate API endpoints. The\nbenefits of progressive discovery of the API and reduced coupling can be significant.\nHowever, it is clear that not everyone is sold, as I don’t see it being used anywhere near as\nmuch as I would like. I think a large part of this is that there is some initial upfront work\nrequired, but the rewards often come later.",11022
16-Downsides to REST Over HTTP.pdf,16-Downsides to REST Over HTTP,"JSON, XML, or Something Else?\nThe use of standard textual formats gives clients a lot of flexibility as to how they\nconsume resources, and REST over HTTP lets us use a variety of formats. The examples I\nhave given so far used XML, but at this stage, JSON is a much more popular content type\nfor services that work over HTTP.\nThe fact that JSON is a much simpler format means that consumption is also easier. Some\nproponents also cite its relative compactness when compared to XML as another winning\nfactor, although this isn’t often a real-world issue.\nJSON does have some downsides, though. XML defines the \nlink\n control we used earlier\nas a hypermedia control. The JSON standard doesn’t define anything similar, so in-house\nstyles are frequently used to shoe-horn this concept in. The \nHypertext Application\nLanguage (HAL)\n attempts to fix this by defining some common standards for\nhyperlinking for JSON (and XML too, although arguably XML needs less help). If you\nfollow the HAL standard, you can use tools like the web-based HAL browser for\nexploring hypermedia controls, which can make the task of creating a client much easier.\nWe aren’t limited to these two formats, of course. We can send pretty much anything over\nHTTP if we want, even binary. I am seeing more and more people just using HTML as a\nformat instead of XML. For some interfaces, the HTML can do double duty as a UI and an\nAPI, although there are pitfalls to be avoided here, as the interactions of a human and a\ncomputer are quite different! But it is certainly an attractive idea. There are lots of HTML\nparsers out there, after all.\nPersonally, though, I am still a fan of XML. Some of the tool support is better. For\nexample, if I want to extract only certain parts of the payload (a technique we’ll discuss\nmore in \n“Versioning”\n) I can use XPATH, which is a well-understood standard with lots of\ntool support, or even CSS selectors, which many find even easier. With JSON, I have\nJSONPATH, but this is not widely supported. I find it odd that people pick JSON because\nit is nice and lightweight, then try and push concepts into it like hypermedia controls that\nalready exist in XML. I accept, though, that I am probably in the minority here and that\nJSON is the format of choice for most people!\nBeware Too Much Convenience\nAs REST has become more popular, so too have the frameworks that help us create\nRESTFul web services. However, some of these tools trade off too much in terms of short-\nterm gain for long-term pain; in trying to get you going fast, they can encourage some bad\nbehaviors. For example, some frameworks actually make it very easy to simply take\ndatabase representations of objects, deserialize them into in-process objects, and then\ndirectly expose these externally. I remember at a conference seeing this demonstrated\nusing Spring Boot and cited as a major advantage. The inherent coupling that this setup\npromotes will in most cases cause far more pain than the effort required to properly\ndecouple these concepts.\nThere is a more general problem at play here. How we decide to store our data, and how\nwe expose it to our consumers, can easily dominate our thinking. One pattern I saw used\neffectively by one of our teams was to delay the implementation of proper persistence for\nthe microservice, until the interface had stabilized enough. \nFor an interim period, entities\nwere just persisted in a file on local disk, which is obviously not a suitable long-term\nsolution. This ensured that how the consumers wanted to use the service drove the design\nand implementation decisions. The rationale given, which was borne out in the results,\nwas that it is too easy for the way we store domain entities in a backing store to overtly\ninfluence the models we send over the wire to collaborators. One of the downsides with\nthis approach is that we are deferring the work required to wire up our data store. I think\nfor new service boundaries, however, this is an acceptable trade-off.\nDownsides to REST Over HTTP\nIn terms of ease of consumption, you cannot easily generate a client stub for your REST\nover HTTP application protocol like you can with RPC. Sure, the fact that HTTP is being\nused means that you get to take advantage of all the excellent HTTP client libraries out\nthere, but if you want to implement and use hypermedia controls as a client you are pretty\nmuch on your own. Personally, I think client libraries could do much better at this than\nthey do, and they are certainly better now than in the past, but I have seen this apparent\nincreased complexity result in people backsliding into smuggling RPC over HTTP or\nbuilding shared client libraries. Shared code between client and server can be very\ndangerous, as we’ll discuss in \n“DRY and the Perils of Code Reuse in a Microservice\nWorld”\n.\nA more minor point is that some web server frameworks don’t actually support all the\nHTTP verbs well. That means that it might be easy for you to create a handler for GET or\nPOST requests, but you may have to jump through hoops to get PUT or DELETE requests\nto work. Proper REST frameworks like Jersey don’t have this problem, and you can\nnormally work around this, but if you are locked into certain framework choices this might\nlimit what style of REST you can use.\nPerformance may also be an issue. REST over HTTP payloads can actually be more\ncompact than SOAP because it supports alternative formats like JSON or even binary, but\nit will still be nowhere near as lean a binary protocol as Thrift might be. The overhead of\nHTTP for each request may also be a concern for low-latency requirements.\nHTTP, while it can be suited well to large volumes of traffic, isn’t great for low-latency\ncommunications when compared to alternative protocols that are built on top of\nTransmission Control Protocol (TCP) or other networking technology. Despite the name,\nWebSockets, for example, has very little to do with the Web. After the initial HTTP\nhandshake, it’s just a TCP connection between client and server, but it can be a much more\nefficient way for you to stream data for a browser. If this is something you’re interested in,\nnote that you aren’t really using much of HTTP, let alone anything to do with REST.\nFor server-to-server communications, if extremely low latency or small message size is\nimportant, HTTP communications in general may not be a good idea. You may need to\npick different underlying protocols, like User Datagram Protocol (UDP), to achieve the\nperformance you want, and many RPC frameworks will quite happily run on top of\nnetworking protocols other than TCP.\nConsumption of the payloads themselves requires more work than is provided by some\nRPC implementations that support advanced serialization and deserialization mechanisms.\nThese can become a coupling point in their own right between client and server, as\nimplementing tolerant readers is a nontrivial activity (we’ll discuss this shortly), but from\nthe point of view of getting up and running, they can be very attractive.\nDespite these disadvantages, REST over HTTP is a sensible default choice for service-to-\nservice interactions. If you want to know more, I recommend \nREST in Practice\n (O’Reilly),\nwhich covers the topic of REST over HTTP in depth.",7340
17-Technology Choices.pdf,17-Technology Choices,"Implementing Asynchronous Event-Based Collaboration\nWe’ve talked for a bit about some technologies that can help us implement\nrequest/response patterns. What about event-based, asynchronous communication?\nTechnology Choices\nThere are two main parts we need to consider here: a way for our microservices to emit\nevents, and a way for our consumers to find out those events have happened.\nTraditionally, message brokers like RabbitMQ try to handle both problems. Producers use\nan API to publish an event to the broker. The broker handles subscriptions, allowing\nconsumers to be informed when an event arrives. These brokers can even handle the state\nof consumers, for example by helping keep track of what messages they have seen before.\nThese systems are normally designed to be scalable and resilient, but that doesn’t come for\nfree. It can add complexity to the development process, because it is another system you\nmay need to run to develop and test your services. Additional machines and expertise may\nalso be required to keep this infrastructure up and running. But once it does, it can be an\nincredibly effective way to implement loosely coupled, event-driven architectures. In\ngeneral, I’m a fan.\nDo be wary, though, about the world of middleware, of which the message broker is just a\nsmall part. Queues in and of themselves are perfectly sensible, useful things. However,\nvendors tend to want to package lots of software with them, which can lead to more and\nmore smarts being pushed into the middleware, as evidenced by things like the Enterprise\nService Bus. Make sure you know what you’re getting: keep your middleware dumb, and\nkeep the smarts in the endpoints.\nAnother approach is to try to use HTTP as a way of propagating events. ATOM is a REST-\ncompliant specification that defines semantics (among other things) for publishing feeds\nof resources. Many client libraries exist that allow us to create and consume these feeds.\nSo our customer service could just publish an event to such a feed when our customer\nservice changes. Our consumers just poll the feed, looking for changes. On one hand, the\nfact that we can reuse the existing ATOM specification and any associated libraries is\nuseful, and we know that HTTP handles scale very well. However, HTTP is not good at\nlow latency (where some message brokers excel), and we still need to deal with the fact\nthat the consumers need to keep track of what messages they have seen and manage their\nown polling schedule.\nI have seen people spend an age implementing more and more of the behaviors that you\nget out of the box with an appropriate message broker to make ATOM work for some use\ncases. For example, the Competing Consumer pattern describes a method whereby you\nbring up multiple worker instances to compete for messages, which works well for scaling\nup the number of workers to handle a list of independent jobs. However, we want to avoid\nthe case where two or more workers see the same message, as we’ll end up doing the same\ntask more than we need to. With a message broker, a standard queue will handle this. With\nATOM, we now need to manage our own shared state among all the workers to try to\nreduce the chances of reproducing effort.\nIf you already have a good, resilient message broker available to you, consider using it to\nhandle publishing and subscribing to events. But if you don’t already have one, give\nATOM a look, but be aware of the sunk-cost fallacy. If you find yourself wanting more\nand more of the support that a message broker gives you, at a certain point you might want\nto change your approach.\nIn terms of what we actually send over these asynchronous protocols, the same\nconsiderations apply as with synchronous communication. If you are currently happy with\nencoding requests and responses using JSON, stick with it.",3866
18-Complexities of Asynchronous Architectures.pdf,18-Complexities of Asynchronous Architectures,"Complexities of Asynchronous Architectures\nSome of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead\nto significantly more decoupled, scalable systems. And they can. But these programming\nstyles do lead to an increase in complexity. This isn’t just the complexity required to\nmanage publishing and subscribing to messages as we just discussed, but also in the other\nproblems we might face. For example, when considering long-running async\nrequest/response, we have to think about what to do when the response comes back. Does\nit come back to the same node that initiated the request? If so, what if that node is down?\nIf not, do I need to store information somewhere so I can react accordingly? Short-lived\nasync can be easier to manage if you’ve got the right APIs, but even so, it is a different\nway of thinking for programmers who are accustomed to intra-process synchronous\nmessage calls.\nTime for a cautionary tale. Back in 2006, I was working on building a pricing system for a\nbank. We would look at market events, and work out which items in a portfolio needed to\nbe repriced. Once we determined the list of things to work through, we put these all onto a\nmessage queue. We were making use of a grid to create a pool of pricing workers,\nallowing us to scale up and down the pricing farm on request. These workers used the\ncompeting consumer pattern, each one gobbling messages as fast as possible until there\nwas nothing left to process.\nThe system was up and running, and we were feeling rather smug. One day, though, just\nafter we pushed a release out, we hit a nasty problem. Our workers kept dying. And dying.\nAnd dying.\nEventually, we tracked down the problem. A bug had crept in whereby a certain type of\npricing request would cause a worker to crash. We were using a transacted queue: as the\nworker died, its lock on the request timed out, and the pricing request was put back on the\nqueue — only for another worker to pick it up and die. This was a classic example of what\nMartin Fowler calls a \ncatastrophic failover\n.\nAside from the bug itself, we’d failed to specify a maximum retry limit for the job on the\nqueue. We fixed the bug itself, and also configured a maximum retry. But we also realized\nwe needed a way to view, and potentially replay, these bad messages. We ended up having\nto implement a message hospital (or dead letter queue), where messages got sent if they\nfailed. We also created a UI to view those messages and retry them if needed. These sorts\nof problems aren’t immediately obvious if you are only familiar with synchronous point-\nto-point communication.\nThe associated complexity with event-driven architectures and asynchronous\nprogramming in general leads me to believe that you should be cautious in how eagerly\nyou start adopting these ideas. Ensure you have good monitoring in place, and strongly\nconsider the use of correlation IDs, which allow you to trace requests across process\nboundaries, as we’ll cover in depth in \nChapter 8\n.\nI also strongly recommend \nEnterprise Integration Patterns\n (Addison-Wesley), which\ncontains a lot more detail on the different programming patterns that you may need to\nconsider in this space.",3253
19-Access by Reference.pdf,19-Access by Reference,"Services as State Machines\nWhether you choose to become a REST ninja, or stick with an RPC-based mechanism like\nSOAP, the core concept of the service as a state machine is powerful. We’ve spoken\nbefore (probably ad nauseum by this point) about our services being fashioned around\nbounded contexts. Our customer microservice \nowns\n all logic associated with behavior in\nthis context.\nWhen a consumer wants to change a customer, it sends an appropriate request to the\ncustomer service. The customer service, based on its logic, gets to decide if it accepts that\nrequest or not. Our customer service controls all lifecycle events associated with the\ncustomer itself. We want to avoid dumb, anemic services that are little more than CRUD\nwrappers. If the decision about what changes are allowed to be made to a customer leak\nout of the customer service itself, we are losing cohesion.\nHaving the lifecycle of key domain concepts explicitly modeled like this is pretty\npowerful. Not only do we have one place to deal with collisions of state (e.g., someone\ntrying to update a customer that has already been removed), but we also have a place to\nattach behavior based on those state changes.\nI still think that REST over HTTP makes for a much more sensible integration technology\nthan many others, but whatever you pick, keep this idea in mind.\nReactive Extensions\nReactive extensions\n, often shortened to Rx, are a mechanism to compose the results of\nmultiple calls together and run operations on them. The calls themselves could be\nblocking or nonblocking calls. At its heart, Rx inverts traditional flows. Rather than asking\nfor some data, then performing operations on it, you observe the outcome of an operation\n(or set of operations) and react when something changes. Some implementations of Rx\nallow you to perform functions on these observables, such as RxJava, which allows\ntraditional functions like \nmap\n or \nfilter\n to be used.\nThe various Rx implementations have found a very happy home in distributed systems.\nThey allow us to abstract out the details of how calls are made, and reason about things\nmore easily. I observe the result of a call to a downstream service. I don’t care if it was a\nblocking or nonblocking call, I just wait for the response and react. The beauty is that I\ncan compose multiple calls together, making handling concurrent calls to downstream\nservices much easier.\nAs you find yourself making more service calls, especailly when making multiple calls to\nperform a single operation, take a look at the reactive extensions for your chosen\ntechnology stack. You may be surprised how much simpler your life can become.\nDRY and the Perils of Code Reuse in a Microservice World\nOne of the acronyms we developers hear a lot is DRY: don’t repeat yourself. Though its\ndefinition is sometimes simplified as trying to avoid duplicating code, DRY more\naccurately means that we want to avoid duplicating our system \nbehavior and knowledge\n.\nThis is very sensible advice in general. Having lots of lines of code that do the same thing\nmakes your codebase larger than needed, and therefore harder to reason about. When you\nwant to change behavior, and that behavior is duplicated in many parts of your system, it\nis easy to forget everywhere you need to make a change, which can lead to bugs. So using\nDRY as a mantra, in general, makes sense.\nDRY is what leads us to create code that can be reused. We pull duplicated code into\nabstractions that we can then call from multiple places. Perhaps we go as far as making a\nshared library that we can use everywhere! This approach, however, can be deceptively\ndangerous in a microservice architecture.\nOne of the things we want to avoid at all costs is overly coupling a microservice and\nconsumers such that any small change to the microservice itself can cause unnecessary\nchanges to the consumer. Sometimes, however, the use of shared code can create this very\ncoupling. For example, at one client we had a library of common domain objects that\nrepresented the core entities in use in our system. This library was used by all the services\nwe had. But when a change was made to one of them, all services had to be updated. Our\nsystem communicated via message queues, which also had to be drained of their now\ninvalid\n contents, and woe betide you if you forgot.\nIf your use of shared code ever leaks outside your service boundary, you have introduced a\npotential form of coupling. Using common code like logging libraries is fine, as they are\ninternal concepts that are invisible to the outside world. RealEstate.com.au makes use of a\ntailored service template to help bootstrap new service creation. Rather than make this\ncode shared, the company copies it for every new service to ensure that coupling doesn’t\nleak in.\nMy general rule of thumb: don’t violate DRY within a microservice, but be relaxed about\nviolating DRY across all services. The evils of too much coupling between services are far\nworse than the problems caused by code duplication. There is one specific use case worth\nexploring further, though.\nClient Libraries\nI’ve spoken to more than one team who has insisted that creating client libraries for your\nservices is an essential part of creating services in the first place. The argument is that this\nmakes it easy to use your service, and avoids the duplication of code required to consume\nthe service itself.\nThe problem, of course, is that if the same people create both the server API and the client\nAPI, there is the danger that logic that should exist on the server starts leaking into the\nclient. I should know: I’ve done this myself. The more logic that creeps into the client\nlibrary, the more cohesion starts to break down, and you find yourself having to change\nmultiple clients to roll out fixes to your server. You also limit technology choices,\nespecially if you mandate that the client library has to be used.\nA model for client libraries I like is the one for Amazon Web Services (AWS). The\nunderlying SOAP or REST web service calls can be made directly, but everyone ends up\nusing just one of the various software development kits (SDKs) that exist, which provide\nabstractions over the underlying API. These SDKs, though, are written by the community\nor AWS people other than those who work on the API itself. This degree of separation\nseems to work, and avoids some of the pitfalls of client libraries. Part of the reason this\nworks so well is that the client is in charge of when the upgrade happens. If you go down\nthe path of client libraries yourself, make sure this is the case.\nNetflix in particular places special emphasis on the client library, but I worry that people\nview that purely through the lens of avoiding code duplication. In fact, the client libraries\nused by Netflix are as much (if not more) about ensuring reliability and scalability of their\nsystems. The Netflix client libraries handle service discovery, failure modes, logging, and\nother aspects that aren’t actually about the nature of the service itself. Without these\nshared clients, it would be hard to ensure that each piece of client/server communications\nbehaved well at the massive scale at which Netflix operates. Their use at Netflix has\ncertainly made it easy to get up and running and increased productivity while also\nensuring the system behaves well. However, according to at least one person at Netflix,\nover time this has led to a degree of coupling between client and server that has been\nproblematic.\nIf the client library approach is something you’re thinking about, it can be important to\nseparate out client code to handle the underlying transport protocol, which can deal with\nthings like service discovery and failure, from things related to the destination service\nitself. Decide whether or not you are going to insist on the client library being used, or if\nyou’ll allow people using different technology stacks to make calls to the underlying API.\nAnd finally, make sure that the clients are in charge of when to upgrade their client\nlibraries: we need to ensure we maintain the ability to release our services independently\nof each other!\nAccess by Reference\nOne consideration I want to touch on is how we pass around information about our\ndomain entities. We need to embrace the idea that a microservice will encompass the\nlifecycle of our core domain entities, like the \nCustomer\n. We’ve already talked about the\nimportance of the logic associated with changing this \nCustomer\n being held in the customer\nservice, and that if we want to change it we have to issue a request to the customer service.\nBut it also follows that we should consider the customer service as being the source of\ntruth for \nCustomer\ns.\nWhen we retrieve a given \nCustomer\n resource from the customer service, we get to see\nwhat that resource looked like when we made the request. It is possible that after we\nrequested that \nCustomer\n resource, something else has changed it. What we have in effect\nis a memory of what the \nCustomer\n resource once looked like. The longer we hold on to\nthis memory, the higher the chance that this memory will be false. Of course, if we avoid\nrequesting data more than we need to, our systems can become much more efficient.\nSometimes this memory is good enough. Other times you need to know if it has changed.\nSo whether you decide to pass around a memory of what an entity once looked like, make\nsure you also include a reference to the original resource so that the new state can be\nretrieved.\nLet’s consider the example where we ask the email service to send an email when an order\nhas been shipped. Now we could send in the request to the email service with the\ncustomer’s email address, name, and order details. However, if the email service is\nactually queuing up these requests, or pulling them from a queue, things could change in\nthe meantime. It might make more sense to just send a URI for the \nCustomer\n and \nOrder\nresources, and let the email server go look them up when it is time to send the email.\nA great counterpoint to this emerges when we consider event-based collaboration. With\nevents, we’re saying \nthis happened\n, but we need to know \nwhat\n happened. If we’re\nreceiving updates due to a \nCustomer\n resource changing, for example, it could be valuable\nto us to know what the \nCustomer\n looked like when the event occurred. As long as we also\nget a reference to the entity itself so we can look up its current state, then we can get the\nbest of both worlds.\nThere are other trade-offs to be made here, of course, when we’re accessing by reference.\nIf we always go to the customer service to look at the information associated with a given\nCustomer\n, the load on the customer service can be too great. If we provide additional\ninformation when the resource is retrieved, letting us know at what time the resource was\nin the given state and perhaps how long we can consider this information to be \nfresh\n, then\nwe can do a lot with caching to reduce load. HTTP gives us much of this support out of\nthe box with a wide variety of cache controls, some of which we’ll discuss in more detail\nin \nChapter 11\n.\nAnother problem is that some of our services might not need to know about the whole\nCustomer\n resource, and by insisting that they go look it up we are potentially increasing\ncoupling. It could be argued, for example, that our email service should be more dumb,\nand that we should just send it the email address and name of the customer. There isn’t a\nhard-and-fast rule here, but be very wary of passing around data in requests when you\ndon’t know its freshness.",11787
20-Defer It for as Long as Possible.pdf,20-Defer It for as Long as Possible,"Versioning\nIn every single talk I have ever done about microservices, I get asked \nhow do you do\nversioning?\n People have the legitimate concern that eventually they will have to make a\nchange to the interface of a service, and they want to understand how to manage that. Let’s\nbreak down the problem a bit and look at the various steps we can take to handle it.\nDefer It for as Long as Possible\nThe best way to reduce the impact of making breaking changes is to avoid making them in\nthe first place. You can achieve much of this by picking the right integration technology,\nas we’ve discussed throughout this chapter. Database integration is a great example of\ntechnology that can make it very hard to avoid breaking changes. REST, on the other\nhand, helps because changes to internal implementation detail are less likely to result in a\nchange to the service interface.\nAnother key to deferring a breaking change is to encourage good behavior in your clients,\nand avoid them binding too tightly to your services in the first place. Let’s consider our\nemail service, whose job it is to send out emails to our customers from time to time. It gets\nasked to send an order shipped email to customer with the ID 1234. It goes off and\nretrieves the customer with that ID, and gets back something like the response shown in\nExample 4-3\n.\nExample 4-3. \nSample response from the customer service\n<customer>\n  \n<firstname>\nSam\n</firstname>\n  \n<lastname>\nNewman\n</lastname>\n  \n<email>\nsam@magpiebrain.com\n</email>\n  \n<telephoneNumber>\n555-1234-5678\n</telephoneNumber>\n</customer>\nNow to send the email, we need only the \nfirstname\n, \nlastname\n, and \nemail\n fields. We\ndon’t need to know the \ntelephoneNumber\n. We want to simply pull out those fields we care\nabout, and ignore the rest. Some binding technology, especially that used by strongly\ntyped languages, can attempt to bind \nall\n fields whether the consumer wants them or not.\nWhat happens if we realize that no one is using the \ntelephoneNumber\n and we decide to\nremove it? This could cause consumers to break needlessly.\nLikewise, what if we wanted to restructure our \nCustomer\n object to support more details,\nperhaps adding some further structure as in \nExample 4-4\n? The data our email service\nwants is still there, and still with the same name, but if our code makes very explicit\nassumptions as to where the \nfirstname\n and \nlastname\n fields will be stored, then it could\nbreak again. In this instance, we could instead use XPath to pull out the fields we care\nabout, allowing us to be ambivalent about where the fields are, as long as we can find\nthem. This pattern — of implementing a reader able to ignore changes we don’t care about\n— is what Martin Fowler calls a \nTolerant Reader\n.\nExample 4-4. \nA restructured Customer resource: the data is all still there, but can our\nconsumers find it?\n<customer>\n  \n<naming>\n    \n<firstname>\nSam\n</firstname>\n    \n<lastname>\nNewman\n</lastname>\n    \n<nickname>\nMagpiebrain\n</nickname>\n    \n<fullname>\nSam ""Magpiebrain"" Newman\n</fullname>\n  \n</naming>\n  \n<email>\nsam@magpiebrain.com\n</email>\n</customer>\nThe example of a client trying to be as flexible as possible in consuming a service\ndemonstrates \nPostel’s Law\n (otherwise known as the \nrobustness principle\n), which states:\n“Be conservative in what you do, be liberal in what you accept from others.” The original\ncontext for this piece of wisdom was the interaction of devices over networks, where you\nshould expect all sorts of odd things to happen. In the context of our request/response\ninteraction, it can lead us to try our best to allow the service being consumed to change\nwithout requiring us to change.",3779
21-Coexist Different Endpoints.pdf,21-Coexist Different Endpoints,"Catch Breaking Changes Early\nIt’s crucial to make sure we pick up changes that will break consumers as soon as possible,\nbecause even if we choose the best possible technology, breaks can still happen. I am\nstrongly in favor of using consumer-driven contracts, which we’ll cover in \nChapter 7\n, to\nhelp spot these problems early on. If you’re supporting multiple different client libraries,\nrunning tests using each library you support against the latest service is another technique\nthat can help. Once you realize you are going to break a consumer, you have the choice to\neither try to avoid the break altogether or else embrace it and start having the right\nconversations with the people looking after the consuming \nservices\n.\nUse Semantic Versioning\nWouldn’t it be great if as a client you could look just at the version number of a service\nand know if you can integrate with it? \nSemantic versioning\n is a specification that allows\njust that. With semantic versioning, each version number is in the form\nMAJOR.MINOR.PATCH\n. When the \nMAJOR\n number increments, it means that backward\nincompatible changes have been made. When \nMINOR\n increments, new functionality has\nbeen added that should be backward compatible. Finally, a change to \nPATCH\n states that bug\nfixes have been made to existing functionality.\nTo see how useful semantic versioning can be, let’s look at a simple use case. Our\nhelpdesk application is built to work against version 1.2.0 of the customer service. If a\nnew feature is added, causing the customer service to change to 1.3.0, our helpdesk\napplication should see no change in behavior and shouldn’t be expected to make any\nchanges. We couldn’t guarantee that we could work against version 1.1.0 of the customer\nservice, though, as we may rely on functionality added in the 1.2.0 release. We could also\nexpect to have to make changes to our application if a new 2.0.0 release of the customer\nservice comes out.\nYou may decide to have a semantic version for the service, or even for an individual\nendpoint on a service if you are coexisting them as detailed in the next section.\nThis versioning scheme allows us to pack a lot of information and expectations into just\nthree fields. The full specification outlines in very simple terms the expectations clients\ncan have of changes to these numbers, and can simplify the process of communicating\nabout whether changes should impact consumers. Unfortunately, I haven’t see this\napproach used enough in the context of distributed systems.\nCoexist Different Endpoints\nIf we’ve done all we can to avoid introducing a breaking interface change, our next job is\nto limit the impact. The thing we want to avoid is forcing consumers to upgrade in lock-\nstep with us, as we always want to maintain the ability to release microservices\nindependently of each other. One approach I have used successfully to handle this is to\ncoexist both the old and new interfaces in the same running service. So if we want to\nrelease a breaking change, we deploy a new version of the service that exposes both the\nold and new versions of the endpoint.\nThis allows us to get the new microservice out as soon as possible, along with the new\ninterface, but give time for consumers to move over. Once all of the consumers are no\nlonger using the old endpoint, you can remove it along with any associated code, as shown\nin \nFigure 4-5\n.\nFigure 4-5. \nCoexisting different endpoint versions allows consumers to migrate gradually\nWhen I last used this approach, we had gotten ourselves into a bit of a mess with the\nnumber of consumers we had and the number of breaking changes we had made. This\nmeant that we were actually coexisting three different versions of the endpoint. This is not\nsomething I’d recommend! Keeping all the code around and the associated testing\nrequired to ensure they all worked was absolutely an additional burden. To make this more\nmanageable, we internally transformed all requests to the V1 endpoint to a V2 request,\nand then V2 requests to the V3 endpoint. This meant we could clearly delineate what code\nwas going to be retired when the old endpoint(s) died.\nThis is in effect an example of the expand and contract pattern, which allows us to phase\nbreaking changes in. We \nexpand\n the capabilities we offer, supporting both old and new\nways of doing something. Once the old consumers do things in the new way, we \ncontract\nour API, removing the old functionality.\nIf you are going to coexist endpoints, you need a way for callers to route their requests\naccordingly. For systems making use of HTTP, I have seen this done with both version\nnumbers in request headers and also in the URI itself — for example, \n/v1/customer/\n or\n/v2/customer/\n. I’m torn as to which approach makes the most sense. On the one hand, I\nlike URIs being opaque to discourage clients from hardcoding URI templates, but on the\nother hand, this approach does make things very obvious and can simplify request routing.\nFor RPC, things can be a little trickier. I have handled this with protocol buffers by putting\nmy methods in different namespaces — for example, \nv1.createCustomer\n and\nv2.createCustomer\n — but when you are trying to support different versions of the same\ntypes being sent over the network, this can become really painful.",5378
22-Use Multiple Concurrent Service Versions.pdf,22-Use Multiple Concurrent Service Versions,"Use Multiple Concurrent Service Versions\nAnother versioning solution often cited is to have different versions of the service live at\nonce, and for older consumers to route their traffic to the older version, with newer\nversions seeing the new one, as shown in \nFigure 4-6\n. This is the approach used sparingly\nby Netflix in situations where the cost of changing older consumers is too high, especially\nin rare cases where legacy devices are still tied to older versions of the API. Personally, I\nam not a fan of this idea, and understand why Netflix uses it rarely. First, if I need to fix an\ninternal bug in my service, I now have to fix and deploy two different sets of services.\nThis would probably mean I have to branch the codebase for my service, and this is\nalways problematic. Second, it means I need smarts to handle directing consumers to the\nright microservice. This behavior inevitably ends up sitting in middleware somewhere or a\nbunch of \nnginx\n scripts, making it harder to reason about the behavior of the system.\nFinally, consider any persistent state our service might manage. Customers created by\neither version of the service need to be stored and made visible to all services, no matter\nwhich version was used to create the data in the first place. This can be an additional\nsource of complexity.\nFigure 4-6. \nRunning multiple versions of the same service to support old endpoints\nCoexisting concurrent service versions for a short period of time can make perfect sense,\nespecially when you’re doing things like blue/green deployments or canary releases (we’ll\nbe discussing these patterns more in \nChapter 7\n). In these situations, we may be coexisting\nversions only for a few minutes or perhaps hours, and normally will have only two\ndifferent versions of the service present at the same time. The longer it takes for you to get\nconsumers upgraded to the newer version and released, the more you should look to\ncoexist different endpoints in the same microservice rather than coexist entirely different\nversions. I remain unconvinced that this work is worthwhile for the average project.",2142
23-UI Fragment Composition.pdf,23-UI Fragment Composition,"User Interfaces\nSo far, we haven’t really touched on the world of the user interface. A few of you out there\nmight just be providing a cold, hard, clinical API to your customers, but many of us find\nourselves wanting to create beautiful, functional user interfaces that will delight our\ncustomers. But we really do need to think about them in the context of integration. The\nuser interface, after all, is where we’ll be pulling all these microservices together into\nsomething that makes sense to our customers.\nIn the past, when I first started computing, we were mostly talking about big, fat clients\nthat ran on our desktops. I spent many hours with Motif and then Swing trying to make\nmy software as nice to use as possible. Often these systems were just for the creation and\nmanipulation of local files, but many of them had a server-side component. My first job at\nThoughtWorks involved creating a Swing-based electronic point-of-sale system that was\njust part of a large number of moving parts, most of which were on the server.\nThen came the Web. We started thinking of our UIs as being \nthin\n instead, with more logic\non the server side. In the beginning, our server-side programs rendered the entire page and\nsent it to the client browser, which did very little. Any interactions were handled on the\nserver side, via GETs and POSTs triggered by the user clicking on links or filling in forms.\nOver time, JavaScript became a more popular option to add dynamic behavior to the\nbrowser-based UI, and some applications could now be argued to be as \nfat\n as the old\ndesktop clients.\nToward Digital\nOver the last couple of years, organizations have started to move away from thinking that\nweb or mobile should be treated differently; they are instead thinking about digital more\nholistically. What is the best way for our customers to use the services we offer? And what\ndoes that do to our system architecture? The understanding that we cannot predict exactly\nhow a customer might end up interacting with our company has driven adoption of more\ngranular APIs, like those delivered by microservices. By combining the capabilities our\nservices expose in different ways, we can curate different experiences for our customers\non their desktop application, mobile device, wearable device, or even in physical form if\nthey visit our brick-and-mortar store.\nSo think of user interfaces as compositional layers — places where we weave together the\nvarious strands of the capabilities we offer. So with that in mind, how do we pull all these\nstrands together?\nConstraints\nConstraints are the different forms in which our users interact with our system. On a\ndesktop web application, for example, we consider constraints such as what browser\nvisitors are using, or their resolution. But mobile has brought a whole host of new\nconstraints. The way our mobile applications communicate with the server can have an\nimpact. It isn’t just about pure bandwidth concerns, where the limitations of mobile\nnetworks can play a part. Different sorts of interactions can drain battery life, leading to\nsome cross customers.\nThe nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile\nphone, I may want to design my interface to be used mostly one-handed, with most\noperations being controlled by a thumb. Elsewhere, I might allow people to interact with\nservices via SMS in places where bandwidth is at a premium — the use of SMS as an\ninterface is huge in the global south, for example.\nSo, although our core services — our core offering — might be the same, we need a way\nto adapt them for the different constraints that exist for each type of interface. When we\nlook at different styles of user interface composition, we need to ensure that they address\nthis challenge. Let’s look at a few models of user interfaces to see how this might be\nachieved.\nAPI Composition\nAssuming that our services already speak XML or JSON to each other via HTTP, an\nobvious option available to us is to have our user interface interact directly with these\nAPIs, as in \nFigure 4-7\n. A web-based UI could use JavaScript GET requests to retrieve\ndata, or POST requests to change it. Even for native mobile applications, initiating HTTP\ncommunications is fairly straightforward. The UI would then need to create the various\ncomponents that make up the interface, handling synchronization of state and the like with\nthe server. If we were using a binary protocol for service-to-service communication, this\nwould be more difficult for web-based clients, but could be fine for native mobile devices.\nThere are a couple of downsides with this approach. First, we have little ability to tailor\nthe responses for different sorts of devices. For example, when I retrieve a customer\nrecord, do I need to pull back all the same data for a mobile shop as I do for a helpdesk\napplication? One solution to this approach is to allow consumers to specify what fields to\npull back when they make a request, but this assumes that each service supports this form\nof interaction.\nAnother key question: who creates the user interface? The people who look after the\nservices are removed from how their services are surfaced to the users — for example, if\nanother team is creating the UI, we could be drifting back into the bad old days of layered\narchitecture where making even small changes requires change requests to multiple teams.\nFigure 4-7. \nUsing multiple APIs to present a user interface\nThis communication could also be fairly chatty. Opening lots of calls directly to services\ncan be quite intensive for mobile devices, and could be a very inefficient use of a\ncustomer’s mobile plan! Having an API gateway can help here, as you could expose calls\nthat aggregate multiple underlying calls, although that itself can have some downsides that\nwe’ll explore shortly.\nUI Fragment Composition\nRather than having our UI make API calls and map everything back to UI controls, we\ncould have our services provide parts of the UI directly, and then just pull these fragments\nin to create a UI, as in \nFigure 4-8\n. Imagine, for example, that the recommendation service\nprovides a recommendation widget that is combined with other controls or UI fragments\nto create an overall UI. It might get rendered as a box on a web page along with other\ncontent.\nA variation of this approach that can work well is to assemble a series of coarser-grained\nparts of a UI. So rather than creating small widgets, you are assembling entire panes of a\nthick client application, or perhaps a set of pages for a website.\nThese coarser-grained fragments are served up from server-side apps that are in turn\nmaking the appropriate API calls. This model works best when the fragments align well to\nteam ownership. For example, perhaps the team that looks after order management in the\nmusic shop serves up all the pages associated with order management.\nFigure 4-8. \nServices directly serving up UI components for assembly\nYou still need some sort of assembly layer to pull these parts together. This could be as\nsimple as some server-side templating, or, where each set of pages comes from a different\napp, perhaps you’ll need some smart URI routing.\nOne of the key advantages of this approach is that the same team that makes changes to\nthe services can also be in charge of making changes to those parts of the UI. It allows us\nto get changes out faster. But there are some problems with this approach.\nFirst, ensuring consistency of the user experience is something we need to address. Users\nwant to have a seamless experience, not to feel that different parts of the interface work in\ndifferent ways, or present a different design language. There are techniques to avoid this\nproblem, though, such as living style guides, where assets like HTML components, CSS,\nand images can be shared to help give some level of consistency.\nAnother probem is harder to deal with. What happens with native applications or thick\nclients? We can’t serve up UI components. We could use a hybrid approach and use native\napplications to serve up HTML components, but this approach has been shown time and\nagain to have downsides. So if you need a native experience, we will have to fall back to\nan approach where the frontend application makes API calls and handles the UI itself. But\neven if we consider web-only UIs, we still may want very different treatments for different\ntypes of devices. Building responsive components can help, of course.\nThere is one key problem with this approach that I’m not sure can be solved. Sometimes\nthe capabilities offered by a service do not fit neatly into a widget or a page. Sure, I might\nwant to surface recommendations in a box on a page on our website, but what if I want to\nweave in dynamic recommendations elsewhere? When I search, I want the type ahead to\nautomatically trigger fresh recommendations, for example. The more cross-cutting a form\nof interaction is, the less likely this model will fit and the more likely it is that we’ll fall\nback to just making API calls.",9176
24-Backends for Frontends.pdf,24-Backends for Frontends,"Backends for Frontends\nA common solution to the problem of chatty interfaces with backend services, or the need\nto vary content for different types of devices, is to have a server-side aggregation\nendpoint, or API gateway. This can marshal multiple backend calls, vary and aggregate\ncontent if needed for different devices, and serve it up, as we see in \nFigure 4-9\n. I’ve seen\nthis approach lead to disaster when these server-side endpoints become thick layers with\ntoo much behavior. They end up getting managed by separate teams, and being another\nplace where logic has to change whenever some functionality changes.\nFigure 4-9. \nUsing a single monolithic gateway to handle calls to/from UIs\nThe problem that can occur is that normally we’ll have one giant layer for all our services.\nThis leads to everything being thrown in together, and suddenly we start to lose isolation\nof our various user interfaces, limiting our ability to release them independently. A model\nI prefer and that I’ve seen work well is to restrict the use of these backends to one specific\nuser interface or application, as we see in \nFigure 4-10\n.\nFigure 4-10. \nUsing dedicated backends for frontends\nThis pattern is sometimes referred to as \nbackends for frontends (BFFs)\n. It allows the team\nfocusing on any given UI to also handle its own server-side components. You can see\nthese backends as parts of the user interface that happen to be embedded in the server.\nSome types of UI may need a minimal server-side footprint, while others may need a lot\nmore. If you need an API authentication and authorization layer, this can sit between our\nBFFs and our UIs. We’ll explore this more in \nChapter 9\n.\nThe danger with this approach is the same as with any aggregating layer; it can take on\nlogic it shouldn’t. The business logic for the various capabilities these backends use\nshould stay in the services themselves. These BFFs should only contain behavior specific\nto delivering a particular user experience.",2019
25-On Your Own Terms.pdf,25-On Your Own Terms,"A Hybrid Approach\nMany of the aforementioned options don’t need to be one-size-fits-all. I could see an\norganization adopting the approach of fragment-based assembly to create a website, but\nusing a backends-for-frontends approach when it comes to its mobile application. The key\npoint is that we need to retain cohesion of the underlying capabilities that we offer our\nusers. We need to ensure that logic associated with ordering music or changing customer\ndetails lives inside those services that handle those operations, and doesn’t get smeared all\nover our system. Avoiding the trap of putting too much behavior into any intermediate\nlayers is a tricky balancing act.\nIntegrating with Third-Party Software\nWe’ve looked at approaches to breaking apart existing systems that are under our control,\nbut what about when we can’t change the things we talk to? For many valid reasons, the\norganizations we work for buy commercial off-the-shelf software (COTS) or make use of\nsoftware as a service (SaaS) offerings over which we have little control. So how do we\nintegrate with them sensibly?\nIf you’re reading this book, you probably work at an organization that writes code. You\nmight write software for your own internal purposes or for an external client, or both.\nNonetheless, even if you are an organization with the ability to create a significant amount\nof custom software, you’ll still use software products provided by external parties, be they\ncommercial or open source. Why is this?\nFirst, your organization almost certainly has a greater demand for software than can be\nsatisfied internally. Think of all the products you use, from office productivity tools like\nExcel to operating systems to payroll systems. Creating all of those for your own use\nwould be a mammoth undertaking. Second, and most important, it wouldn’t be cost\neffective! The cost for you to build your own email system, for example, is likely to dwarf\nthe cost of using an existing combination of mail server and client, even if you go for\ncommercial options.\nMy clients often struggle with the question “Should I build, or should I buy?” In general,\nthe advice I and my colleagues give when having this conversation with the average\nenterprise organization boils down to “Build if it is unique to what you do, and can be\nconsidered a strategic asset; buy if your use of the tool isn’t that special.”\nFor example, the average organization would not consider its payroll system to be a\nstrategic asset. People on the whole get paid the same the world over. Likewise, most\norganizations tend to buy content management systems (CMSes) off the shelf, as their use\nof such a tool isn’t considered something that is key to their business. On the other hand, I\nwas involved early on in rebuilding the \nGuardian’s\n website, and there the decision was\nmade to build a bespoke content management system, as it was core to the newspaper’s\nbusiness.\nSo the notion that we will occasionally encounter commercial, third-party software is\nsensible, and to be welcomed. However, many of us end up cursing some of these\nsystems. Why is that?\nLack of Control\nOne challenge associated with integrating with and extending the capabilities of COTS\nproducts like CMS or SaaS tools is that typically many of the technical decisions have\nbeen made for you. How do you integrate with the tool? That’s a vendor decision. Which\nprogramming language can you use to extend the tool? Up to the vendor. Can you store\nthe configuration for the tool in version control, and rebuild from scratch, so as to enable\ncontinuous integration of customizations? It depends on choices the vendor makes.\nIf you are lucky, how easy — or hard — it is to work with the tool from a development\npoint of view has been considered as part of the tool selection process. But even then, you\nare effectively ceding some level of control to an outside party. The trick is to bring the\nintegration and customization work back on to your terms.\nCustomization\nMany tools that enterprise organizations purchase sell themselves on their ability to be\nheavily customized \njust for you\n. Beware! Often, due to the nature of the tool chain you\nhave access to, the cost of customization can be more expensive than building something\nbespoke from scratch! If you’ve decided to buy a product but the particular capabilities it\nprovides aren’t that special to you, it might make more sense to change how your\norganization works rather than embark on complex customization.\nContent management systems are a great example of this danger. I have worked with\nmultiple CMSes that by design do not support continuous integration, that have terrible\nAPIs, and for which even a minor-point upgrade in the underlying tool can break any\ncustomizations you have made.\nSalesforce is especially troublesome in this regard. For many years it has pushed its\nForce.com platform, which requires the use of a programming language, Apex, that exists\nonly within the Force.com ecosystem!\nIntegration Spaghetti\nAnother challenge is how you integrate with the tool. As we discussed earlier, thinking\ncarefully about how you integrate between services is important, and ideally you want to\nstandardize on a small number of types of integration. But if one product decides to use a\nproprietary binary protocol, another some flavor of SOAP, and another XML-RPC, what\nare you left with? Even worse are the tools that allow you to reach right inside their\nunderlying data stores, leading to all the same coupling issues we discussed earlier.",5603
26-Example The multirole CRM system.pdf,26-Example The multirole CRM system,"On Your Own Terms\nCOTS and SAAS products absolutely have their place, and it isn’t feasible (or sensible)\nfor most of us to build everything from scratch. So how do we resolve these challenges?\nThe key is to move things back on to your own terms.\nThe core idea here is to do any customizations on a platform you control, and to limit the\nnumber of different consumers of the tool itself. To explore this idea in detail, let’s look at\na couple of examples.\nExample: CMS as a service\nIn my experience, the CMS is one of the most commonly used product that needs to be\ncustomized or integrated with. The reason for this is that unless you want a basic static\nsite, the average enterprise organization wants to enrich the functionality of its website\nwith dynamic content like customer records or the latest product offerings. The source of\nthis dynamic content is typically other services inside the organization, which you may\nhave actually built yourself.\nThe temptation — and often the selling point of the CMS — is that you can customize the\nCMS to pull in all this special content and display it to the outside world. However, the\ndevelopment environment for the average CMS is \nterrible\n.\nLet’s look at what the average CMS specializes in, and what we probably bought it for:\ncontent creation and content management. Most CMSes are pretty bad even at doing page\nlayout, typically providing drag-and-drop tools that don’t cut the mustard. And even then,\nyou end up needing to have someone who understands HTML and CSS to fine-tune the\nCMS templates. They tend to be terrible platforms on which to build custom code.\nThe answer? Front the CMS with your own service that provides the website to the outside\nworld, as shown in \nFigure 4-11\n. Treat the CMS as a service whose role is to allow for the\ncreation and retrieval of content. In your own service, you write the code and integrate\nwith services how you want. You have control over scaling the website (many commercial\nCMSes provide their own proprietary add-ons to handle load), and you can pick the\ntemplating system that makes sense.\nMost CMSes also provide APIs to allow for content creation, so you also have the ability\nto front that with your own service façade. For some situations, we’ve even used such a\nfaçade to abstract out the APIs for retrieving content.\nFigure 4-11. \nHiding a CMS using your own service\nWe’ve used this pattern multiple times across ThoughtWorks in the last few years, and\nI’ve done this more than once myself. One notable example was a client that was looking\nto push out a new website for its products. Initially, it wanted to build the entire solution\non the CMS, but it had yet to pick one. We instead suggested this approach, and started\ndevelopment of the fronting website. While waiting for the CMS tool to be selected, we\nfaked\n it by having a web service that just surfaced static content. We ended up going live\nwith the site well before the CMS was selected by using our fake content service in\nproduction to surface content to the live site. Later on, we were able to just drop in the\neventually selected tool without any change to the fronting application.\nUsing this approach, we keep the scope of what the CMS does down to a minimum and\nmove customizations onto our own technology stack.\nExample: The multirole CRM system\nThe CRM — or Customer Relationship Management — tool is an often-encountered beast\nthat can instill fear in the heart of even the hardiest architect. This sector, as typified by\nvendors like Salesforce or SAP, is rife with examples of tools that try to do everything for\nyou. This can lead to the tool itself becoming a single point of failure, and a tangled knot\nof dependencies. Many implementations of CRM tools I have seen are among the best\nexamples of \nadhesive\n (as opposed to cohesive) services.\nThe scope of such a tool typically starts small, but over time it becomes an increasingly\nimportant part of how your organization works. The problem is that the direction and\nchoices made around this now-vital system are often made by the tool vendor itself, not by\nyou.\nI was involved recently in an exercise to try to wrest some control back. The organization\nI was working with realized that although it was using the CRM tool for a lot of things, it\nwasn’t getting the value of the increasing costs associated with the platform. At the same\ntime, multiple internal systems were using the less-than-ideal CRM APIs for integration.\nWe wanted to move the system architecture toward a place where we had services that\nmodeled our businesses domain, and also lay the groundwork for a potential migration.\nThe first thing we did was identify the core concepts to our domain that the CRM system\ncurrently owned. One of these was the concept of a \nproject\n — that is, something to which\na member of staff could be assigned. Multiple other systems needed project information.\nWhat we did was instead create a project service. This service exposed projects as\nRESTful resources, and the external systems could move their integration points over to\nthe new, easier-to-work-with service. Internally, the project service was just a façade,\nhiding the detail of the underlying integration. You can see this in \nFigure 4-12\n.\nFigure 4-12. \nUsing façade services to mask the underlying CRM\nThe work, which at the time of this writing was still under way, was to identify other\ndomain concepts that the CRM was handling, and create more façades for them. When the\ntime comes for migration away from the underlying CRM, we could then look at each\nfaçade in turn to decide if an internal software solution or something off the shelf could fit\nthe bill.",5752
27-Its All About Seams.pdf,27-Its All About Seams,"The Strangler Pattern\nWhen it comes to legacy or even COTS platforms that aren’t totally under our control, we\nalso have to deal with what happens when we want to remove them or at least move away\nfrom them. A useful pattern here is the \nStrangler Application Pattern\n. Much like with our\nexample of fronting the CMS system with our own code, with a strangler you capture and\nintercept calls to the old system. This allows you to decide if you route these calls to\nexisting, legacy code, or direct them to new code you may have written. This allows you\nto replace functionality over time without requiring a big bang rewrite.\nWhen it comes to microservices, rather than having a single monolithic application\nintercepting all calls to the existing legacy system, you may instead use a series of\nmicroservices to perform this interception. Capturing and redirecting the original calls can\nbecome more complex in this situation, and you may require the use of a proxy to do this\nfor you.\nSummary\nWe’ve looked at a number of different options around integration, and I’ve shared my\nthoughts on what choices are most likely to ensure our microservices remain as decoupled\nas possible from their other collaborators:\nAvoid database integration at all costs.\nUnderstand the trade-offs between REST and RPC, but strongly consider REST as a\ngood starting point for request/response integration.\nPrefer choreography over orchestration.\nAvoid breaking changes and the need to version by understanding Postel’s Law and\nusing tolerant readers.\nThink of user interfaces as compositional layers.\nWe covered quite a lot here, and weren’t able to go into depth on all of these topics.\nNonetheless, this should be a good foundation to get you going and point you in the right\ndirection if you want to learn more.\nWe also spent some time looking at how to work with systems that aren’t completely\nunder our control in the form of COTS products. It turns out that this description can just\nas easily apply to software we wrote!\nSome of the approaches outlined here apply equally well to \nlegacy\n software, but what if\nwe want to tackle the often-monumental task of bringing these older systems to heel and\ndecomposing them into more usable parts? We’ll discuss that in detail in the next chapter.\nChapter 5. \nSplitting the Monolith\nWe’ve discussed what a good service looks like, and why smaller servers may be better for\nus. We also previously discussed the importance of being able to evolve the design of our\nsystems. But how do we handle the fact that we may already have a large number of\ncodebases lying about that don’t follow these patterns? How do we go about decomposing\nthese monolithic applications without having to embark on a big-bang rewrite?\nThe monolith grows over time. It acquires new functionality and lines of code at an\nalarming rate. Before long it becomes a big, scary giant presence in our organization that\npeople are scared to touch or change. But all is not lost! With the right tools at our\ndisposal, we can slay this beast.",3082
28-Breaking Apart MusicCorp.pdf,28-Breaking Apart MusicCorp,"It’s All About Seams\nWe discussed in \nChapter 3\n that we want our services to be highly cohesive and loosely\ncoupled. The problem with the monolith is that all too often it is the opposite of both.\nRather than tend toward cohesion, and keep things together that tend to change together,\nwe acquire and stick together all sorts of unrelated code. Likewise, loose coupling doesn’t\nreally exist: if I want to make a change to a line of code, I may be able to do that easily\nenough, but I cannot deploy that change without potentially impacting much of the rest of\nthe monolith, and I’ll certainly have to redeploy the entire system.\nIn his book \nWorking Effectively with Legacy Code\n (Prentice-Hall), Michael Feathers\ndefines the concept of a \nseam\n — that is, a portion of the code that can be treated in\nisolation and worked on without impacting the rest of the codebase. We also want to\nidentify seams. But rather than finding them for the purpose of cleaning up our codebase,\nwe want to identify seams that can become service boundaries.\nSo what makes a good seam? Well, as we discussed previously, bounded contexts make\nexcellent seams, because by definition they represent cohesive and yet loosely coupled\nboundaries in an organization. So the first step is to start identifying these boundaries in\nour code.\nMost programming languages provide namespace concepts that allow us to group similar\ncode together. Java’s \npackage\n concept is a fairly weak example, but gives us much of what\nwe need. All other mainstream programming languages have similar concepts built in,\nwith JavaScript very arguably being an exception.\nBreaking Apart MusicCorp\nImagine we have a large backend monolithic service that represents a substantial amount\nof the behavior of MusicCorp’s online systems. To start, we should identify the high-level\nbounded contexts that we think exist in our organization, as we discussed in \nChapter 3\n.\nThen we want to try to understand what bounded contexts the monolith maps to. Let’s\nimagine that initially we identify four contexts we think our monolithic backend covers:\nCatalog\nEverything to do with metadata about the items we offer for sale\nFinance\nReporting for accounts, payments, refunds, etc.\nWarehouse\nDispatching and returning of customer orders, managing inventory levels, etc.\nRecommendation\nOur patent-pending, revolutionary recommendation system, which is highly complex\ncode written by a team with more PhDs than the average science lab\nThe first thing to do is to create packages representing these contexts, and then move the\nexisting code into them. With modern IDEs, code movement can be done automatically\nvia refactorings, and can be done incrementally while we are doing other things. You’ll\nstill need tests to catch any breakages made by moving code, however, especially if you’re\nusing a dynamically typed language where the IDEs have a harder time of performing\nrefactoring. Over time, we start to see what code fits well, and what code is \nleft over\n and\ndoesn’t really fit anywhere. This remaining code will often identify bounded contexts we\nmight have missed!\nDuring this process we can use code to analyze the dependencies between these packages\ntoo. Our code should represent our organization, so our packages representing the\nbounded contexts in our organization should interact in the same way the real-life\norganizational groups in our domain interact. For example, tools like Structure 101 allow\nus to see the dependencies between packages graphically. If we spot things that look\nwrong — for example, the warehouse package depends on code in the finance package\nwhen no such dependency exists in the real organization — then we can investigate this\nproblem and try to resolve it.\nThis process could take an afternoon on a small codebase, or several weeks or months\nwhen you’re dealing with millions of lines of code. You may not need to sort all code into\ndomain-oriented packages before splitting out your first service, and indeed it can be more\nvaluable to concentrate your effort in one place. There is no need for this to be a big-bang\napproach. It is something that can be done bit by bit, day by day, and we have a lot of tools\nat our disposal to track our progress.\nSo now that we have our codebase organized around these seams, what next?",4372
29-Getting to Grips with the Problem.pdf,29-Getting to Grips with the Problem,"The Reasons to Split the Monolith\nDeciding that you’d like a monolithic service or application to be smaller is a good start.\nBut I would strongly advise you to chip away at these systems. An incremental approach\nwill help you learn about microservices as you go, and will also limit the impact of getting\nsomething wrong (and you will get things wrong!). Think of our monolith as a block of\nmarble. We could blow the whole thing up, but that rarely ends well. It makes much more\nsense to just chip away at it incrementally.\nSo if we are going to break apart the monolith a piece at a time, where should we start?\nWe have our seams now, but which one should we pull out first? It’s best to think about\nwhere you are going to get the most benefit from some part of your codebase being\nseparated, rather than just splitting things for the sake of it. Let’s consider some drivers\nthat might help guide our chisel.\nPace of Change\nPerhaps we know that we have a load of changes coming up soon in how we manage\ninventory. If we split out the warehouse seam as a service now, we could change that\nservice faster, as it is a separate autonomous unit.\nTeam Structure\nMusicCorp’s delivery team is actually split across two geographical regions. One team is\nin London, the other in Hawaii (some people have it easy!). It would be great if we could\nsplit out the code that the Hawaii team works on the most, so it can take full ownership.\nWe’ll explore this idea further in \nChapter 10\n.\nSecurity\nMusicCorp has had a security audit, and has decided to tighten up its protection of\nsensitive information. Currently, all of this is handled by the finance-related code. If we\nsplit this service out, we can provide additional protections to this individual service in\nterms of monitoring, protection of data at transit, and protection of data at rest — ideas\nwe’ll look at in more detail in \nChapter 9\n.\nTechnology\nThe team looking after our recommendation system has been spiking out some new\nalgorithms using a logic programming library in the language Clojure. The team thinks\nthis could benefit our customers by improving what we offer them. If we could split out\nthe recommendation code into a separate service, it would be easy to consider building an\nalternative implementation that we could test against.\nTangled Dependencies\nThe other point to consider when you’ve identified a couple of seams to separate is how\nentangled that code is with the rest of the system. We want to pull out the seam that is least\ndepended on if we can. If you can view the various seams you have found as a directed\nacyclical graph of dependencies (something the package modeling tools I mentioned\nearlier are very good at), this can help you spot the seams that are likely going to be harder\nto disentangle.\nThis brings us to what is often the mother of all tangled dependencies: the database.\nThe Database\nWe’ve already discussed at length the challenges of using databases as a method of\nintegrating multiple services. As I made it pretty clear earlier, I am not a fan! This means\nwe need to find seams in our databases too so we can split them out cleanly. Databases,\nhowever, are tricky beasts.\nGetting to Grips with the Problem\nThe first step is to take a look at the code itself and see which parts of it read to and write\nfrom the database. \nA common practice is to have a repository layer, backed by some sort\nof framework like Hibernate, to bind your code to the database, making it easy to map\nobjects or data structures to and from the database. If you have been following along so\nfar, you’ll have grouped our code into packages representing our bounded contexts; we\nwant to do the same for our database access code. This may require splitting up the\nrepository layer into several parts, as shown in \nFigure 5-1\n.\nFigure 5-1. \nSplitting out our repository layers\nHaving the database mapping code colocated inside the code for a given context can help\nus understand what parts of the database are used by what bits of code. Hibernate, for\nexample, can make this very clear if you are using something like a mapping file per\nbounded context.\nThis doesn’t give us the whole story, however. For example, we may be able to tell that the\nfinance code uses the ledger table, and that the catalog code uses the line item table, but it\nmight not be clear that the database enforces a foreign key relationship from the ledger\ntable to the line item table. To see these database-level constraints, which may be a\nstumbling block, we need to use another tool to visualize the data. A great place to start is\nto use a tool like the freely available \nSchemaSpy\n, which can generate graphical\nrepresentations of the relationships between tables.\nAll this helps you understand the coupling between tables that may span what will\neventually become service boundaries. But how do you cut those ties? And what about\ncases where the same tables are used from multiple different bounded contexts? Handling\nproblems like these is not easy, and there are many answers, but it is doable.\nComing back to some concrete examples, let’s consider our music shop again. We have\nidentified four bounded contexts, and want to move forward with making them four\ndistinct, collaborating services. We’re going to look at a few concrete examples of\nproblems we might face, and their potential solutions. And while some of these examples\ntalk specifically about challenges encountered in standard relational databases, you will\nfind similar problems in other alternative NOSQL stores.",5617
30-Example Breaking Foreign Key Relationships.pdf,30-Example Breaking Foreign Key Relationships,"Example: Breaking Foreign Key Relationships\nIn this example, our catalog code uses a generic line item table to store information about\nan album. Our finance code uses a ledger table to track financial transactions. At the end\nof each month we need to generate reports for various people in the organization so they\ncan see how we’re doing. We want to make the reports nice and easy to read, so rather\nthan saying, “We sold 400 copies of SKU 12345 and made $1,300,” we’d like to add more\ninformation about what was sold (i.e., “We sold 400 copies of Bruce Springsteen’s\nGreatest Hits\n and made $1,300”). To do this, our reporting code in the finance package\nwill reach into the line item table to pull out the title for the SKU. It may also have a\nforeign key constraint from the ledger to the line item table, as we see in \nFigure 5-2\n.\nFigure 5-2. \nForeign key relationship\nSo how do we fix things here? Well, we need to make a change in two places. First, we\nneed to stop the finance code from reaching into the line item table, as this table really\nbelongs to the catalog code, and we don’t want database integration happening once\ncatalog and finance are services in their own rights. The quickest way to address this is\nrather than having the code in finance reach into the line item table, we’ll expose the data\nvia an API call in the catalog package that the finance code can call. This API call will be\nthe forerunner of a call we will make over the wire, as we see in \nFigure 5-3\n.\nFigure 5-3. \nPost removal of the foreign key relationship\nAt this point it becomes clear that we may well end up having to make two database calls\nto generate the report. This is correct. And the same thing will happen if these are two\nseparate services. Typically concerns around performance are now raised. I have a fairly\neasy answer to those: how fast does your system need to be? And how fast is it now? If\nyou can test its current performance and know what good performance looks like, then\nyou should feel confident in making a change. Sometimes making one thing slower in\nexchange for other things is the right thing to do, especially if \nslower\n is still perfectly\nacceptable.\nBut what about the foreign key relationship? Well, we lose this altogether. This becomes a\nconstraint we need to now manage in our resulting services rather than in the database\nlevel. This may mean that we need to implement our own consistency check across\nservices, or else trigger actions to clean up related data. Whether or not this is needed is\noften not a technologist’s choice to make. For example, if our order service contains a list\nof IDs for catalog items, what happens if a catalog item is removed and an order now\nrefers to an invalid catalog ID? Should we allow it? If we do, then how is this represented\nin the order when it is displayed? If we don’t, then how can we check that this isn’t\nviolated? These are questions you’ll need to get answered by the people who define how\nyour system should behave for its users.",3057
31-Example Shared Static Data.pdf,31-Example Shared Static Data,"Example: Shared Static Data\nI have seen perhaps as many country codes stored in databases (shown in \nFigure 5-4\n) as I\nhave written \nStringUtils\n classes for in-house Java projects. This seems to imply that we\nplan to change the countries our system supports way more frequently than we’ll deploy\nnew code, but whatever the real reason, these examples of shared static data being stored\nin databases come up a lot. So what do we do in our music shop if all our potential\nservices read from the same table like this?\nFigure 5-4. \nCountry codes in the database\nWell, we have a few options. One is to duplicate this table for each of our packages, with\nthe long-term view that it will be duplicated within each service also. This leads to a\npotential consistency challenge, of course: what happens if I update one table to reflect the\ncreation of Newmantopia off the east coast of Australia, but not another?\nA second option is to instead treat this shared, static data as code. Perhaps it could be in a\nproperty file deployed as part of the service, or perhaps just as an enumeration. The\nproblems around the consistency of data remain, although experience has shown that it is\nfar easier to push out changes to configuration files than alter live database tables. This is\noften a very sensible approach.\nA third option, which may well be extreme, is to push this static data into a service of its\nown right. In a couple of situations I have encountered, the volume, complexity, and rules\nassociated with the static reference data were sufficient that this approach was warranted,\nbut it’s probably overkill if we are just talking about country codes!\nPersonally, in most situations I’d try to push for keeping this data in configuration files or\ndirectly in code, as it is the simple option for most cases.",1832
32-Example Shared Data.pdf,32-Example Shared Data,"Example: Shared Data\nNow let’s dive into a more complex example, but one that can be a common problem\nwhen you’re trying to tease apart systems: shared mutable data. Our finance code tracks\npayments made by customers for their orders, and also tracks refunds given to them when\nthey return items. Meanwhile, the warehouse code updates records to show that orders for\ncustomers have been dispatched or received. All of this data is displayed in one\nconvenient place on the website so that customers can see what is going on with their\naccount. To keep things simple, we have stored all this information in a fairly generic\ncustomer record table, as shown in \nFigure 5-5\n.\nFigure 5-5. \nAccessing customer data: are we missing something?\nSo both the finance and the warehouse code are writing to, and probably occasionally\nreading from, the same table. How can we tease this apart? What we actually have here is\nsomething you’ll see often — a domain concept that isn’t modeled in the code, and is in\nfact implicitly modeled in the database. Here, the domain concept that is missing is that of\nCustomer\n.\nWe need to make the current abstract concept of the customer concrete. As a transient\nstep, we create a new package called \nCustomer\n. We can then use an API to expose\nCustomer\n code to other packages, such as finance or warehouse. Rolling this all the way\nforward, we may now end up with a distinct customer service (\nFigure 5-6\n).\nFigure 5-6. \nRecognizing the bounded context of the customer",1523
33-Transactional Boundaries.pdf,33-Transactional Boundaries,"Example: Shared Tables\nFigure 5-7\n shows our last example. Our catalog needs to store the name and price of the\nrecords we sell, and the warehouse needs to keep an electronic record of inventory. We\ndecide to keep these two things in the same place in a generic line item table. Before, with\nall the code merged in together, it wasn’t clear that we are actually \nconflating\n concerns,\nbut now we can see that in fact we have two separate concepts that could be stored\ndifferently.\nFigure 5-7. \nTables being shared between different contexts\nThe answer here is to split the table in two as we have in \nFigure 5-8\n, perhaps creating a\nstock list table for the warehouse, and a catalog entry table for the catalog details.\nFigure 5-8. \nPulling apart the shared table\nRefactoring Databases\nWhat we have covered in the preceding examples are a few database refactorings that can\nhelp you separate your schemas. For a more detailed discussion of the subject, you may\nwant to take a look at \nRefactoring Databases\n by Scott J. Ambler and Pramod J. Sadalage\n(Addison-Wesley).\nStaging the Break\nSo we’ve found seams in our application code, grouping it around bounded contexts.\nWe’ve used this to identify seams in the database, and we’ve done our best to split those\nout. What next? Do you do a big-bang release, going from one monolithic service with a\nsingle schema to two services, each with its own schema? I would actually recommend\nthat you split out the schema but keep the service together before splitting the application\ncode out into separate microservices, as shown in \nFigure 5-9\n.\nFigure 5-9. \nStaging a service separation\nWith a separate schema, we’ll be potentially increasing the number of database calls to\nperform a single action. Where before we might have been able to have all the data we\nwanted in a single \nSELECT\n statement, now we may need to pull the data back from two\nlocations and join in memory. Also, we end up breaking transactional integrity when we\nmove to two schemas, which could have significant impact on our applications; we’ll be\ndiscussing this next. By splitting the schemas out but keeping the application code\ntogether, we give ourselves the ability to revert our changes or continue to tweak things\nwithout impacting any consumers of our service. Once we are satisfied that the DB\nseparation makes sense, we can then think about splitting out the application code into two\nservices.\nTransactional Boundaries\nTransactions are useful things. They allow us to say \nthese events either all happen\ntogether, or none of them happen\n. They are very useful when we’re inserting data into a\ndatabase; they let us update multiple tables at once, knowing that if anything fails,\neverything gets rolled back, ensuring our data doesn’t get into an inconsistent state.\nSimply put, a transaction allows us to group together multiple different activities that take\nour system from one consistent state to another — everything works, or nothing changes.\nTransactions don’t just apply to databases, although we most often use them in that\ncontext. Message brokers, for example, have long allowed you to post and receive\nmessages within transactions too.\nWith a monolithic schema, all our create or updates will probably be done within a single\ntransactional boundary. When we split apart our databases, we lose the safety afforded to\nus by having a single transaction. Consider a simple example in the \nMusicCorp\n context.\nWhen creating an order, I want to update the order table stating that a customer order has\nbeen created, and also put an entry into a table for the warehouse team so it knows there is\nan order that needs to be picked for dispatch. We’ve gotten as far as grouping our\napplication code into separate packages, and have also separated the customer and\nwarehouse parts of the schema well enough that we are ready to put them into their own\nschemas prior to separating the application code.\nWithin a single transaction in our existing monolithic schema, creating the order and\ninserting the record for the warehouse team takes place within a single transaction, as\nshown in \nFigure 5-10\n.\nFigure 5-10. \nUpdating two tables in a single transaction\nBut if we have pulled apart the schema into two separate schemas, one for customer-\nrelated data including our order table, and another for the warehouse, we have lost this\ntransactional safety. The order placing process now spans two separate transactional\nboundaries, as we see in \nFigure 5-11\n. If our insert into the order table fails, we can clearly\nstop everything, leaving us in a consistent state. But what happens when the insert into the\norder table works, but the insert into the picking table fails?\nFigure 5-11. \nSpanning transactional boundaries for a single operation",4854
34-The Reporting Database.pdf,34-The Reporting Database,"Try Again Later\nThe fact that the order was captured and placed might be enough for us, and we may\ndecide to retry the insertion into the warehouse’s picking table at a later date. We could\nqueue up this part of the operation in a queue or logfile, and try again later. For some sorts\nof operations this makes sense, but we have to assume that a retry would fix it.\nIn many ways, this is another form of what is called \neventual consistency\n. Rather than\nusing a transactional boundary to ensure that the system is in a consistent state when the\ntransaction completes, instead we accept that the system will get itself into a consistent\nstate at some point in the future. This approach is especially useful with business\noperations that might be long-lived. We’ll discuss this idea in more depth in \nChapter 11\nwhen we cover scaling patterns.\nAbort the Entire Operation\nAnother option is to reject the entire operation. In this case, we have to put the system\nback into a consistent state. The picking table is easy, as that insert failed, but we have a\ncommitted transaction in the order table. We need to unwind this. What we have to do is\nissue a \ncompensating transaction\n, kicking off a new transaction to wind back what just\nhappened. For us, that could be something as simple as issuing a \nDELETE\n statement to\nremove the order from the database. Then we’d also need to report back via the UI that the\noperation failed. Our application could handle both aspects within a monolithic system,\nbut we’d have to consider what we could do when we split up the application code. Does\nthe logic to handle the compensating transaction live in the customer service, the order\nservice, or somewhere else?\nBut what happens if our compensating transaction fails? It’s certainly possible. Then we’d\nhave an order in the order table with no matching pick instruction. In this situation, you’d\neither need to retry the compensating transaction, or allow some backend process to clean\nup the inconsistency later on. This could be something as simple as a maintenance screen\nthat admin staff had access to, or an automated process.\nNow think about what happens if we have not one or two operations we want to be\nconsistent, but three, four, or five. Handling compensating transactions for each failure\nmode becomes quite challenging to comprehend, let alone implement.\nDistributed Transactions\nAn alternative to manually orchestrating compensating transactions is to use a \ndistributed\ntransaction\n. \nDistributed transactions try to span multiple transactions within them, using\nsome overall governing process called a \ntransaction manager\n to orchestrate the various\ntransactions being done by underlying systems. Just as with a normal transaction, a\ndistributed transaction tries to ensure that everything remains in a consistent state, only in\nthis case it tries to do so across multiple different systems running in different processes,\noften communicating across network boundaries.\nThe most common algorithm for handling distributed transactions — especially short-\nlived transactions, as in the case of handling our customer order — is to use a \ntwo-phase\ncommit\n. With a two-phase commit, first comes the voting phase. This is where each\nparticipant (also called a \ncohort\n in this context) in the distributed transaction tells the\ntransaction manager whether it thinks its local transaction can go ahead. If the transaction\nmanager gets a \nyes\n vote from all participants, then it tells them all to go ahead and perform\ntheir commits. A single \nno\n vote is enough for the transaction manager to send out a\nrollback to all parties.\nThis approach relies on all parties halting until the central coordinating process tells them\nto proceed. This means we are vulnerable to outages. If the transaction manager goes\ndown, the pending transactions never complete. If a cohort fails to respond during voting,\neverything blocks. And there is also the case of what happens if a commit fails after\nvoting. There is an assumption implicit in this algorithm that this cannot happen: if a\ncohort says \nyes\n during the voting period, then we have to assume it \nwill\n commit. Cohorts\nneed a way of making this commit work at some point. This means this algorithm isn’t\nfoolproof — rather, it just tries to catch most failure cases.\nThis coordination process also mean locks; that is, pending transactions can hold locks on\nresources. Locks on resources can lead to contention, making scaling systems much more\ndifficult, especially in the context of distributed systems.\nDistributed transactions have been implemented for specific technology stacks, such as\nJava’s Transaction API, allowing for disparate resources like a database and a message\nqueue to all participate in the same, overarching transaction. The various algorithms are\nhard to get right, so I’d suggest you avoid trying to create your own. Instead, do lots of\nresearch on this topic if this seems like the route you want to take, and see if you can use\nan existing implementation.\nSo What to Do?\nAll of these solutions add complexity. As you can see, distributed transactions are hard to\nget right and can actually inhibit scaling. Systems that eventually converge through\ncompensating retry logic can be harder to reason about, and may need other compensating\nbehavior to fix up inconsistencies in data.\nWhen you encounter business operations that currently occur within a single transaction,\nask yourself if they really need to. Can they happen in different, local transactions, and\nrely on the concept of eventual consistency? These systems are much easier to build and\nscale (we’ll discuss this more in \nChapter 11\n).\nIf you do encounter state that really, really wants to be kept consistent, do everything you\ncan to avoid splitting it up in the first place. Try \nreally\n hard. If you really need to go ahead\nwith the split, think about moving from a purely technical view of the process (e.g., a\ndatabase transaction) and actually create a concrete concept to represent the transaction\nitself. This gives you a handle, or a hook, on which to run other operations like\ncompensating transactions, and a way to monitor and manage these more complex\nconcepts in your system. For example, you might create the idea of an “in-process-order”\nthat gives you a natural place to focus all logic around processing the order end to end\n(and dealing with exceptions).\nReporting\nAs we’ve already seen, in splitting a service into smaller parts, we need to also potentially\nsplit up how and where data is stored. This creates a problem, however, when it comes to\none vital and common use case: reporting.\nA change in architecture as fundamental as moving to a microservices architecture will\ncause a lot of disruption, but it doesn’t mean we have to abandon everything we do. The\naudience of our reporting systems are users like any other, and we need to consider their\nneeds. It would be arrogant to fundamentally change our architecture and just ask them to\nadapt. While I’m not suggesting that the space of reporting isn’t ripe for disruption — it\ncertainly is — there is value in determining how to work with existing processes first.\nSometimes we have to pick our battles.\nThe Reporting Database\nReporting typically needs to group together data from across multiple parts of our\norganization in order to generate useful output. For example, we might want to enrich the\ndata from our general ledger with descriptions of what was sold, which we get from a\ncatalog. Or we might want to look at the shopping behavior of specific, high-value\ncustomers, which could require information from their purchase history and their customer\nprofile.\nIn a standard, monolithic service architecture, all our data is stored in one big database.\nThis means all the data is in one place, so reporting across all the information is actually\npretty easy, as we can simply join across the data via SQL queries or the like. Typically we\nwon’t run these reports on the main database for fear of the load generated by our queries\nimpacting the performance of the main system, so often these reporting systems hang on a\nread replica as shown in \nFigure 5-12\n.\nFigure 5-12. \nStandard read replication\nWith this approach we have one sizeable upside — that all the data is already in one place,\nso we can use fairly straightforward tools to query it. But there are also a couple of\ndownsides with this approach. First, the schema of the database is now effectively a shared\nAPI between the running monolithic services and any reporting system. So a change in\nschema has to be carefully managed. In reality, this is another impediment that reduces the\nchances of anyone wanting to take on the task of making and coordinating such a change.\nSecond, we have limited options as to how the database can be optimized for either use\ncase — backing the live system or the reporting system. Some databases let us make\noptimizations on read replicas to enable faster, more efficient reporting; for example,\nMySQL would allow us to run a different backend that doesn’t have the overhead of\nmanaging transactions. However, we cannot structure the data differently to make\nreporting faster if that change in data structure has a bad impact on the running system.\nWhat often happens is that the schema either ends up being great for one use case and\nlousy for the other, or else becomes the lowest common denominator, great for neither\npurpose.\nFinally, the database options available to us have exploded recently. While standard\nrelational databases expose SQL query interfaces that work with many reporting tools,\nthey aren’t always the best option for storing data for our running services. What if our\napplication data is better modeled as a graph, as in Neo4j? Or what if we’d rather use a\ndocument store like MongoDB? Likewise, what if we wanted to explore using a column-\noriented database like Cassandra for our reporting system, which makes it much easier to\nscale for larger volumes? Being constrained in having to have one database for both\npurposes results in us often not being able to make these choices and explore new options.\nSo it’s not perfect, but it works (mostly). Now if our information is stored in multiple\ndifferent systems, what do we do? Is there a way for us to bring all the data together to run\nour reports? And could we also potentially find a way to eliminate some of the downsides\nassociated with the standard reporting database model?\nIt turns out we have a number of viable alternatives to this approach. Which solution\nmakes the most sense to you will depend on a number of factors, but we’ll explore a few\ndifferent options that I have seen in practice.",10858
35-Data Retrieval via Service Calls.pdf,35-Data Retrieval via Service Calls,"Data Retrieval via Service Calls\nThere are many variants of this model, but they all rely on pulling the required data from\nthe source systems via API calls. For a very simple reporting system, like a dashboard that\nmight just want to show the number of orders placed in the last 15 minutes, this might be\nfine. To report across data from two or more systems, you need to make multiple calls to\nassemble this data.\nThis approach breaks down rapidly with use cases that require larger volumes of data,\nhowever. Imagine a use case where we want to report on customer purchasing behavior for\nour music shop over the last 24 months, looking at various trends in customer behavior\nand how this has impacted on revenue. We need to pull large volumes of data from at least\nthe customer and finance systems. Keeping a local copy of this data in the reporting\nsystem is dangerous, as we may not know if it has changed (even historic data may be\nchanged after the fact), so to generate an accurate report we need all of the finance and\ncustomer records for the last two years. With even modest numbers of customers, you can\nsee that this quickly will become a very slow operation.\nReporting systems also often rely on third-party tools that expect to retrieve data in a\ncertain way, and here providing a SQL interface is the fastest way to ensure your reporting\ntool chain is as easy to integrate with as possible. We could still use this approach to pull\ndata periodically into a SQL database, of course, but this still presents us with some\nchallenges.\nOne of the key challenges is that the APIs exposed by the various microservices may well\nnot be designed for reporting use cases. For example, a customer service may allow us to\nfind a customer by an ID, or search for a customer by various fields, but wouldn’t\nnecessarily expose an API to retrieve all customers. This could lead to many calls being\nmade to retrieve all the data — for example, having to iterate through a list of all the\ncustomers, making a separate call for each one. Not only could this be inefficient for the\nreporting system, it could generate load for the service in question too.\nWhile we could speed up some of the data retrieval by adding cache headers to the\nresources exposed by our service, and have this data cached in something like a reverse\nproxy, the nature of reporting is often that we access the \nlong tail\n of data. This means that\nwe may well request resources that no one else has requested before (or at least not for a\nsufficiently long time), resulting in a potentially expensive \ncache miss\n.\nYou could resolve this by exposing batch APIs to make reporting easier. For example, our\ncustomer service could allow you to pass a list of customer IDs to it to retrieve them in\nbatches, or may even expose an interface that lets you page through all the customers. A\nmore extreme version of this is to model the batch request as a resource in its own right.\nFor example, the customer service might expose something like a \nBatchCustomerExport\nresource endpoint. The calling system would POST a \nBatchRequest\n, perhaps passing in a\nlocation where a file can be placed with all the data. The customer service would return an\nHTTP 202 response code, indicating that the request was accepted but has not yet been\nprocessed. The calling system could then poll the resource waiting until it retrieves a 201\nCreated status, indicating that the request has been fulfilled, and then the calling system\ncould go and fetch the data. This would allow potentially large data files to be exported\nwithout the overhead of being sent over HTTP; instead, the system could simply save a\nCSV file to a shared location.\nI have seen the preceding approach used for batch insertion of data, where it worked well.\nI am less in favor of it for reporting systems, however, as I feel that there are other,\npotentially simpler solutions that can scale more effectively when you’re dealing with\ntraditional reporting needs.",4033
36-Data Pumps.pdf,36-Data Pumps,"Data Pumps\nRather than have the reporting system pull the data, we could instead have the data pushed\nto the reporting system. One of the downsides of retrieving the data by standard HTTP\ncalls is the overhead of HTTP when we’re making a large number of calls, together with\nthe overhead of having to create APIs that may exist only for reporting purposes. An\nalternative option is to have a standalone program that directly accesses the database of\nthe service that is the source of data, and pumps it into a reporting database, as shown in\nFigure 5-13\n.\nFigure 5-13. \nUsing a data pump to periodically push data to a central reporting \ndatabase\nAt this point you’ll be saying, “But Sam, you said having lots of programs integrating on\nthe same database is a bad idea!” At least I \nhope\n you’ll be saying that, given how firmly I\nmade the point earlier! This approach, if implemented properly, is a notable exception,\nwhere the downsides of the coupling are more than mitigated by making the reporting\neasier.\nTo start with, the data pump should be built and managed by the same team that manages\nthe service. This can be something as simple as a command-line program triggered via\nCron\n. This program needs to have intimate knowledge of both the internal database for the\nservice, and also the reporting schema. The pump’s job is to map one from the other. We\ntry to reduce the problems with coupling to the service’s schema by having the same team\nthat manages the service also manage the pump. I would suggest, in fact, that you version-\ncontrol these together, and have builds of the data pump created as an additional artifact as\npart of the build of the service itself, with the assumption that whenever you deploy one of\nthem, you deploy them both. As we explicitly state that we deploy these together, and\ndon’t open up access to the schema to anyone outside of the service team, many of the\ntraditional DB integration challenges are largely mitigated.\nThe coupling on the reporting schema itself remains, but we have to treat it as a published\nAPI that is hard to change. Some databases give us techniques where we could further\nmitigate this cost. \nFigure 5-14\n shows an example of this for relational databases, where we\ncould have one schema in the reporting database for each service, using things like\nmaterialized views to create the aggregated view. That way, we expose only the reporting\nschema for the customer data to the customer data pump. Whether this is something that\nyou can do in a performant manner, however, will depend on the capabilities of the\ndatabase you picked for reporting.\nFigure 5-14. \nUtilizing materialized views to form a single monolithic reporting schema\nHere, of course, the complexity of integration is pushed deeper into the schema, and will\nrely on capabilities in the database to make such a setup performant. While I think data\npumps in general are a sensible and workable suggestion, I am less convinced that the\ncomplexity of a segmented schema is worthwhile, especially given the challenges in\nmanaging change in the database.",3126
37-Mapping Continuous Integration to Microservices.pdf,37-Mapping Continuous Integration to Microservices,"Alternative Destinations\nOn one project I was involved with, we used a series of data pumps to populate JSON files\nin AWS S3, effectively using S3 to masquerade as a giant data mart! This approach\nworked very well until we needed to scale our solution, and at the time of writing we are\nlooking to change these pumps to instead populate a cube that can be integrated with\nstandard reporting tools like Excel and Tableau.\nEvent Data Pump\nIn \nChapter 4\n, we touched on the idea of microservices emitting events based on the state\nchange of entities that they manage. For example, our customer service may emit an event\nwhen a given customer is created, or updated, or deleted. For those microservices that\nexpose such event feeds, we have the option of writing our own event subscriber that\npumps data into the reporting database, as shown in \nFigure 5-15\n.\nFigure 5-15. \nAn event data pump using state change events to populate a reporting \ndatabase\nThe coupling on the underlying database of the source microservice is now avoided.\nInstead, we are just binding to the events emitted by the service, which are designed to be\nexposed to external consumers. Given that events are temporal in nature, it also makes it\neasier for us to be smarter in what data we sent to our central reporting store; we can send\ndata to the reporting system as we see an event, allowing data to flow faster to our\nreporting system, rather than relying on a regular schedule as with the data pump.\nAlso, if we store which events have already been processed, we can just process the new\nevents as they arrive, assuming the old events have already been mapped into the reporting\nsystem. This means our insertion will be more efficient, as we only need to send deltas.\nWe can do similar things with a data pump, but we have to manage this ourselves, whereas\nthe fundamentally temporal nature of the stream of events (\nx\n happens at timestamp \ny\n)\nhelps us greatly.\nAs our event data pump is less coupled to the internals of the service, it is also easier to\nconsider this being managed by a separate group from the team looking after the\nmicroservice itself. As long as the nature of our event stream doesn’t overly couple\nsubscribers to changes in the service, this event mapper can evolve independently of the\nservice it subscribes to.\nThe main downsides to this approach are that all the required information must be\nbroadcast as events, and it may not scale as well as a data pump for larger volumes of data\nthat has the benefit of operating directly at the database level. Nonetheless, the looser\ncoupling and fresher data available via such an approach makes it strongly worth\nconsidering if you are already exposing the appropriate events.\nBackup Data Pump\nThis option is based on an approach used at Netflix, which takes advantage of existing\nbackup solutions and also resolves some scale issues that Netflix has to deal with. In some\nways, you can consider this a special case of a data pump, but it seemed like such an\ninteresting solution that it deserves inclusion.\nNetflix has decided to standardize on Cassandra as the backing store for its services, of\nwhich there are many. Netflix has invested significant time in building tools to make\nCassandra easy to work with, much of which the company has shared with the rest of the\nworld via numerous open source projects. Obviously it is very important that the data\nNetflix stores is properly backed up. To back up Cassandra data, the standard approach is\nto make a copy of the data files that back it and store them somewhere safe. Netflix stores\nthese files, known as SSTables, in Amazon’s S3 object store, which provides significant\ndata durability guarantees.\nNetflix needs to report across all this data, but given the scale involved this is a nontrivial\nchallenge. Its approach is to use Hadoop that uses SSTable backup as the source of its\njobs. In the end, Netflix ended up implementing a pipeline capable of processing large\namounts of data using this approach, which it then open sourced as the \nAegisthus project\n.\nLike data pumps, though, with this pattern we still have a coupling to the destination\nreporting schema (or target system).\nIt is conceivable that using a similar approach — that is, using mappers that work off\nbackups — would work in other contexts as well. And if you’re already using Cassandra,\nNetflix has already done much of the work for you!\nToward Real Time\nMany of the patterns previously outlined are different ways of getting a lot of data from\nmany different places to one place. But does the idea that all our reporting will be done\nfrom one location really stack up anymore? We have dashboards, alerting, financial\nreports, user analytics — all of these use cases have different tolerances for accuracy and\ntimeliness, which may result in different technical options coming to bear. As I will detail\nin \nChapter 8\n, we are moving more and more toward generic eventing systems capable of\nrouting our data to multiple different places depending on need.\nCost of Change\nThere are many reasons why, throughout the book, I promote the need to make small,\nincremental changes, but one of the key drivers is to understand the impact of each\nalteration we make and change course if required. This allows us to better mitigate the\ncost of mistakes, but doesn’t remove the chance of mistakes entirely. We \ncan — and\n will\n— make mistakes, and we should embrace that. What we should also do, though, is\nunderstand how best to mitigate the costs of those mistakes.\nAs we have seen, the cost involved in moving code around within a codebase is pretty\nsmall. We have lots of tools that support us, and if we cause a problem, the fix is generally\nquick. Splitting apart a database, however, is much more work, and rolling back a database\nchange is just as complex. Likewise, untangling an overly coupled integration between\nservices, or having to completely rewrite an API that is used by multiple consumers, can\nbe a sizeable undertaking. The large cost of change means that these operations are\nincreasingly risky. How can we manage this risk? My approach is to try to make mistakes\nwhere the impact will be lowest.\nI tend to do much of my thinking in the place where the cost of change and the cost of\nmistakes is as low as it can be: the whiteboard. Sketch out your proposed design. See what\nhappens when you run use cases across what you think your service boundaries will be.\nFor our music shop, for example, imagine what happens when a customer searches for a\nrecord, registers with the website, or purchases an album. What calls get made? Do you\nstart seeing odd circular references? Do you see two services that are overly chatty, which\nmight indicate they should be one thing?\nA great technique here is to adapt an approach more typically taught for the design of\nobject-oriented systems: class-responsibility-collaboration (CRC) cards. With CRC cards,\nyou write on one index card the name of the class, what its responsibilities are, and who it\ncollaborates with. When working through a proposed design, for each service I list its\nresponsibilities in terms of the capabilities it provides, with the collaborators specified in\nthe diagram. As you work through more use cases, you start to get a sense as to whether\nall of this hangs together properly.\nUnderstanding Root Causes\nWe have discussed how to split apart larger services into smaller ones, but why did these\nservices grow so large in the first place? The first thing to understand is that growing a\nservice to the point that it needs to be split is completely OK. We \nwant\n the architecture of\nour system to change over time in an incremental fashion. The key is knowing it needs to\nbe split before the split becomes too expensive.\nBut in practice many of us will have seen services grow well beyond the point of sanity.\nDespite knowing that a smaller set of services would be easier to deal with than the huge\nmonstrosity we currently have, we still plow on with growing the beast. Why?\nPart of the problem is knowing where to start, and I’m hoping this chapter has helped. But\nanother challenge is the cost associated with splitting out services. Finding somewhere to\nrun the service, spinning up a new service stack, and so on, are nontrivial tasks. So how do\nwe address this? Well, if doing something is right but difficult, we should strive to make\nthings easier. Investment in libraries and lightweight service frameworks can reduce the\ncost associated with creating the new service. Giving people access to self-service\nprovision virtual machines or even making a platform as a service (PaaS) available will\nmake it easier to provision systems and test them. Throughout the rest of the book, we’ll\nbe discussing a number of ways to help you keep this cost down.\nSummary\nWe decompose our system by finding seams along which service boundaries can emerge,\nand this can be an incremental approach. By getting good at finding these seams and\nworking to reduce the cost of splitting out services in the first place, we can continue to\ngrow and evolve our systems to meet whatever requirements come down the road. As you\ncan see, some of this work can be painstaking. But the very fact that it can be done\nincrementally means there is no need to fear this work.\nSo now we can split our services out, but we’ve introduced some new problems too. We\nhave many more moving parts to get into production now! So next up we’ll dive into the\nworld of deployment.\nChapter 6. \nDeployment\nDeploying a monolithic application is a fairly straightforward process. Microservices, with\ntheir interdependence, are a different kettle of fish altogether. If you don’t approach\ndeployment right, it’s one of those areas where the complexity can make your life a\nmisery. In this chapter, we’re going to look at some techniques and technology that can\nhelp us when deploying microservices into fine-grained architectures.\nWe’re going to start off, though, by taking a look at continuous integration and continuous\ndelivery. These related but different concepts will help shape the other decisions we’ll\nmake when thinking about what to build, how to build it, and how to deploy it.\nA Brief Introduction to Continuous Integration\nContinuous integration (CI)\n has been around for a number of years at this point. It’s worth\nspending a bit of time going over the basics, however, as especially when we think about\nthe mapping between microservices, builds, and version control repositories, there are\nsome different options to consider.\nWith CI, the core goal is to keep everyone in sync with each other, which we achieve by\nmaking sure that newly checked-in code properly integrates with existing code. To do this,\na CI server detects that the code has been committed, checks it out, and carries out some\nverification like making sure the code compiles and that tests pass.\nAs part of this process, we often create artifact(s) that are used for further validation, such\nas deploying a running service to run tests against it. Ideally, we want to build these\nartifacts once and once only, and use them for all deployments of that version of the code.\nThis is in order to avoid doing the same thing over and over again, and so that we can\nconfirm that the artifact we deployed is the one we tested. To enable these artifacts to be\nreused, we place them in a repository of some sort, either provided by the CI tool itself or\non a separate system.\nWe’ll be looking at what sorts of artifacts we can use for microservices shortly, and we’ll\nlook in depth at testing in \nChapter 7\n.\nCI has a number of benefits. We get some level of fast feedback as to the quality of our\ncode. It allows us to automate the creation of our binary artifacts. All the code required to\nbuild the artifact is itself version controlled, so we can re-create the artifact if needed. We\nalso get some level of traceability from a deployed artifact back to the code, and\ndepending on the capabilities of the CI tool itself, can see what tests were run on the code\nand artifact too. It’s for these reasons that CI has been so successful.\nAre You Really Doing It?\nI suspect you are probably using continuous integration in your own organization. If not,\nyou should start. It is a key practice that allows us to make changes quickly and easily, and\nwithout which the journey into microservices will be painful. That said, I have worked\nwith many teams who, despite saying that they do CI, aren’t actually doing it at all. They\nconfuse the use of a CI tool with adopting the practice of CI. The tool is just something\nthat enables the approach.\nI really like Jez Humble’s three questions he asks people to test if they really understand\nwhat CI is about:\nDo you check in to mainline once per day?\nYou need to make sure your code integrates. If you don’t check your code together\nwith everyone else’s changes frequently, you end up making future integration harder.\nEven if you are using short-lived branches to manage changes, integrate as frequently\nas you can into a single mainline branch.\nDo you have a suite of tests to validate your changes?\nWithout tests, we just know that syntactically our integration has worked, but we\ndon’t know if we have broken the behavior of the system. CI without some\nverification that our code behaves as expected isn’t CI.\nWhen the build is broken, is it the #1 priority of the team to fix it?\nA passing green build means our changes have safely been integrated. A red build\nmeans the last change possibly did not integrate. You need to stop all further check-\nins that aren’t involved in fixing the builds to get it passing again. If you let more\nchanges pile up, the time it takes to fix the build will increase drastically. I’ve worked\nwith teams where the build has been broken for days, resulting in substantial efforts\nto eventually get a passing build.\nMapping Continuous Integration to Microservices\nWhen thinking about microservices and continuous integration, we need to think about\nhow our CI builds map to individual microservices. As I have said many times, we want to\nensure that we can make a change to a single service and deploy it independently of the\nrest. With this in mind, how should we map individual microservices to CI builds and\nsource code?\nIf we start with the simplest option, we could lump everything in together. We have a\nsingle, giant repository storing all our code, and have one single build, as we see in\nFigure 6-1\n. Any check-in to this source code repository will cause our build to trigger,\nwhere we will run all the verification steps associated with all our microservices, and\nproduce multiple artifacts, all tied back to the same build.\nFigure 6-1. \nUsing a single source code repository and CI build for all microservices\nThis seems much simpler on the surface than other approaches: fewer repositories to\nworry about, and a conceptually simpler build. From a developer point of view, things are\npretty straightforward too. I just check code in. If I have to work on multiple services at\nonce, I just have to worry about one commit.\nThis model can work perfectly well if you buy into the idea of lock-step releases, where\nyou don’t mind deploying multiple services at once. In general, this is absolutely a pattern\nto avoid, but very early on in a project, especially if only one team is working on\neverything, this might make sense for short periods of time.\nHowever, there are some significant downsides. If I make a one-line change to a single\nservice — for example, changing the behavior in the user service in \nFigure 6-1\n — \nall\n the\nother services get verified and built. This could take more time than needed — I’m\nwaiting for things that probably don’t need to be tested. This impacts our cycle time, the\nspeed at which we can move a single change from development to live. More troubling,\nthough, is knowing what artifacts should or shouldn’t be deployed. Do I now need to\ndeploy all the build services to push my small change into production? It can be hard to\ntell; trying to guess which services \nreally\n changed just by reading the commit messages is\ndifficult. Organizations using this approach often fall back to just deploying everything\ntogether, which we really want to avoid.\nFurthermore, if my one-line change to the user service breaks the build, no other changes\ncan be made to the other services until that break is fixed. And think about a scenario\nwhere you have multiple teams all sharing this giant build. Who is in charge?\nA variation of this approach is to have one single source tree with all of the code in it, with\nmultiple CI builds mapping to parts of this source tree, as we see in \nFigure 6-2\n. With well-\ndefined structure, you can easily map the builds to certain parts of the source tree. In\ngeneral, I am not a fan of this approach, as this model can be a mixed blessing. On the one\nhand, my check-in/check-out process can be simpler as I have only one repository to\nworry about. On the other hand, it becomes very easy to get into the habit of checking in\nsource code for multiple services at once, which can make it equally easy to slip into\nmaking changes that couple services together. I would greatly prefer this approach,\nhowever, over having a single build for multiple services.\nFigure 6-2. \nA single source repo with subdirectories mapped to independent builds\nSo is there another alternative? The approach I prefer is to have a single CI build per\nmicroservice, to allow us to quickly make and validate a change prior to deployment into\nproduction, as shown in \nFigure 6-3\n. Here each microservice has its own source code\nrepository, mapped to its own CI build. When making a change, I run only the build and\ntests I need to. I get a single artifact to deploy. Alignment to team ownership is more clear\ntoo. If you own the service, you own the repository and the build. Making changes across\nrepositories can be more difficult in this world, but I’d maintain this is easier to resolve\n(e.g., by using command-line scripts) than the downside of the monolithic source control\nand build process.\nFigure 6-3. \nUsing one source code repository and CI build per microservice\nThe tests for a given microservice should live in source control with the microservice’s\nsource code too, to ensure we always know what tests should be run against a given\nservice.\nSo, each microservice will live in its own source code repository, and its own CI build\nprocess. We’ll use the CI build process to create our deployable artifacts too in a fully\nautomated fashion. Now lets look beyond CI to see how continuous delivery fits in.",18888
38-Build Pipelines and Continuous Delivery.pdf,38-Build Pipelines and Continuous Delivery,"Build Pipelines and Continuous Delivery\nVery early on in using continuous integration, we realized the value in sometimes having\nmultiple stages inside a build. Tests are a very common case where this comes into play. I\nmay have a lot of fast, small-scoped tests, and a small number of large-scoped, slow tests.\nIf we run all the tests together, we may not be able to get fast feedback when our fast tests\nfail if we’re waiting for our long-scoped slow tests to finally finish. And if the fast tests\nfail, there probably isn’t much sense in running the slower tests anyway! A solution to this\nproblem is to have different stages in our build, creating what is known as a \nbuild pipeline\n.\nOne stage for the faster tests, one for the slower tests.\nThis build pipeline concept gives us a nice way of tracking the progress of our software as\nit clears each stage, helping give us insight into the quality of our software. We build our\nartifact, and that artifact is used throughout the pipeline. As our artifact moves through\nthese stages, we feel more and more confident that the software will work in production.\nContinuous delivery (CD)\n builds on this concept, and then some. As outlined in Jez\nHumble and Dave Farley’s book of the same name, continuous delivery is the approach\nwhereby we get constant feedback on the production readiness of each and every check-in,\nand furthermore treat each and every check-in as a release candidate.\nTo fully embrace this concept, we need to model all the processes involved in getting our\nsoftware from check-in to production, and know where any given version of the software\nis in terms of being cleared for release. In CD, we do this by extending the idea of the\nmultistage build pipeline to model each and every stage our software has to go through,\nboth manual and automated. In \nFigure 6-4\n, we see a sample pipeline that may be familiar.\nFigure 6-4. \nA standard release process modeled as a build pipeline\nHere we really want a tool that embraces CD as a first-class concept. I have seen many\npeople try to hack and extend CI tools to make them do CD, often resulting in complex\nsystems that are nowhere as easy to use as tools that build in CD from the beginning.\nTools that fully support CD allow you to define and visualize these pipelines, modeling\nthe entire path to production for your software. As a version of our code moves through\nthe pipeline, if it passes one of these automated verification steps it moves to the next\nstage. Other stages may be manual. For example, if we have a manual user acceptance\ntesting (UAT) process I should be able to use a CD tool to model it. I can see the next\navailable build ready to be deployed into our UAT environment, deploy it, and if it passes\nour manual checks, mark that stage as being successful so it can move to the next.\nBy modeling the entire path to production for our software, we greatly improve visibility\nof the quality of our software, and can also greatly reduce the time taken between releases,\nas we have one place to observe our build and release process, and an obvious focal point\nfor introducing improvements.\nIn a microservices world, where we want to ensure we can release our services\nindependently of each other, it follows that as with CI, we’ll want one pipeline per service.\nIn our pipelines, it is an artifact that we want to create and move through our path to\nproduction. As always, it turns out our artifacts can come in lots of sizes and shapes. We’ll\nlook at some of the most common options available to us in a moment.",3595
39-Custom Images.pdf,39-Custom Images,"And the Inevitable Exceptions\nAs with all good rules, there are exceptions we need to consider too. The “one\nmicroservice per build” approach is absolutely something you should aim for, but are there\ntimes when something else makes sense? When a team is starting out with a new project,\nespecially a greenfield one where they are working with a blank sheet of paper, it is quite\nlikely that there will be \na large amount of churn in terms of working out where the service\nboundaries lie. This is a good reason, in fact, for keeping your initial services on the larger\nside until your understanding of the domain stabilizes.\nDuring this time of churn, changes across service boundaries are more likely, and what is\nin or not in a given service is likely to change frequently. During this period, having all\nservices in a single build to reduce the cost of cross-service changes may make sense.\nIt does follow, though, that in this case you need to buy into releasing all the services as a\nbundle. It also absolutely needs to be a transitionary step. As service APIs stabilize, start\nmoving them out into their own builds. If after a few weeks (or a very small number of\nmonths) you are unable to get stability in service boundaries in order to properly separate\nthem, merge them back into a more monolithic service (albeit retaining modular\nseparation within the boundary) and give yourself time to get to grips with the domain.\nThis reflects the experiences of our own SnapCI team, as we discussed in \nChapter 3\n.\nPlatform-Specific Artifacts\nMost technology stacks have some sort of first-class artifact, along with tools to support\ncreating and installing them. Ruby has gems, Java has JAR files and WAR files, and\nPython has eggs. Developers with experience in one of these stacks will be well versed in\nworking with (and hopefully creating) these artifacts.\nFrom the point of view of a microservice, though, depending on your technology stack,\nthis artifact may not be enough by itself. While a Java JAR file can be made to be\nexecutable and run an embedded HTTP process, for things like Ruby and Python\napplications, you’ll expect to use a process manager running inside Apache or Nginx. So\nwe may need some way of installing and configuring other software that we need in order\nto deploy and launch our artifacts. This is where automated configuration management\ntools like Puppet and Chef can help.\nAnother downfall here is that these artifacts are specific to a certain technology stack,\nwhich may make deployment more difficult when we have a mix of technologies in play.\nThink of it from the point of view of someone trying to deploy multiple services together.\nThey could be a developer or tester wanting to test some functionality, or it could be\nsomeone managing a production deployment. Now imagine that those services use three\ncompletely different deployment mechanisms. Perhaps we have a Ruby Gem, a JAR file,\nand a nodeJS NPM package. Would they thank you?\nAutomation can go a long way toward hiding the differences in the deployment\nmechanisms of the underlying artifacts. Chef, Puppet, and Ansible all support multiple\ndifferent common technology-specific build artifacts too. But there are different types of\nartifacts that might be even easier to work with.\nOperating System Artifacts\nOne way to avoid the problems associated with technology-specific artifacts is to create\nartifacts that are native to the underlying operating system. For example, for a RedHat– or\nCentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or\nfor Windows, an MSI.\nThe advantage of using OS-specific artifacts is that from a deployment point of view we\ndon’t care what the underlying technology is. We just use the tools native to the OS to\ninstall the package. The OS tools can also help us uninstall and get information about the\npackages too, and may even provide package repositories that our CI tools can push to.\nMuch of the work done by the OS package manager can also offset work that you might\notherwise do in a tool like Puppet or Chef. On all Linux platforms I have used, for\nexample, you can define dependencies from your packages to other packages you rely on,\nand the OS tools will automatically install them for you too.\nThe downside can be the difficulty in creating the packages in the first place. For Linux,\nthe \nFPM package manager tool\n gives a nicer abstraction for creating Linux OS packages,\nand converting from a tarball-based deployment to an OS-based deployment can be fairly\nstraightforward. The Windows space is somewhat trickier. The native packaging system in\nthe form of MSI installers and the like leave a lot to be desired when compared to the\ncapabilities in the Linux space. The NuGet package system has started to help address\nthis, at least in terms of helping manage development libraries. More recently, Chocolatey\nNuGet has extended these ideas, providing a package manager for Windows designed for\ndeploying tools and services, which is much more like the package managers in the Linux\nspace. This is certainly a step in the right direction, although the fact that the idiomatic\nstyle in Windows is still \ndeploy something in IIS\n means that this approach may be\nunappealing for some Windows teams.\nAnother downside, of course, could be if you are deploying onto multiple different\noperating systems. The overhead of managing artifacts for different OSes could be pretty\nsteep. If you’re creating software for other people to install, you may not have a choice. If\nyou are installing software onto machines you control, however, I would suggest you look\nat unifying or at least reducing the number of different operating systems you use. It can\ngreatly reduce variations in behavior from one machine to the next, and simplify\ndeployment and maintenance tasks.\nIn general, those teams I’ve seen that have moved to OS-based package management have\nsimplified their deployment approach, and tend to avoid the trap of big, complex\ndeployment scripts. Especially if you’re on Linux, this can be a good way to simplify\ndeployment of microservices using disparate technology stacks.\nCustom Images\nOne of the challenges with automated configuration management systems like Puppet,\nChef, and Ansible can be the time taken to run the scripts on a machine. Let’s take a\nsimple example of a server being provisioned and configured to allow for the deployment\nof a Java application. \nLet’s assume I’m using AWS to provision the server, using the\nstandard Ubuntu image. The first thing I need to do is install the Oracle JVM to run my\nJava application. I’ve seen this simple process take around five minutes, with a couple of\nminutes taken up by the machine being provisioned, and a few more to install the JVM.\nThen we can think about actually putting our software on it.\nThis is actually a fairly trivial example. We will often want to install other common bits of\nsoftware. For example, we might want to use collectd for gathering OS stats, use logstash\nfor log aggregation, and perhaps install the appropriate bits of nagios for monitoring (we’ll\ntalk more about this software in \nChapter 8\n). Over time, more things might get added,\nleading to longer and longer amounts of time needed for provisioning of these\ndependencies.\nPuppet, Chef, Ansible, and their ilk can be smart and will avoid installing software that is\nalready present. This does not mean that running the scripts on existing machines will\nalways be fast, unfortunately, as running all the checks takes time. We also want to avoid\nkeeping our machines around for too long, as we don’t want to allow for too much\nconfiguration drift (which we’ll explore in more depth shortly). And if we’re using an on-\ndemand compute platform we might be constantly shutting down and spinning up new\ninstances on a daily basis (if not more frequently), so the declarative nature of these\nconfiguration management tools may be of limited use.\nOver time, watching the same tools get installed over and over again can become a real\ndrag. If you are trying to do this multiple times per day — perhaps as part of development\nor CI — this becomes a real problem in terms of providing fast feedback. It can also lead\nto increased downtime when deploying in production if your systems don’t allow for zero-\ndowntime deployment, as you’re waiting to install all the pre-requisites on your machines\neven before you get to installing your software. Models like blue/green deployment\n(which we’ll discuss in \nChapter 7\n) can help mitigate this, as they allow us to deploy a new\nversion of our service without taking the old one offline.\nOne approach to reducing this spin-up time is to create a virtual machine image that bakes\nin some of the common dependencies we use, as shown in \nFigure 6-5\n. All virtualization\nplatforms I’ve used allow you to build your own images, and the tools to do so are much\nmore advanced than they were even a few years ago. This shifts things somewhat. Now\nwe could bake the common tools into our own image. When we want to deploy our\nsoftware, we spin up an instance of this custom image, and all we have to do is install the\nlatest version of our service.\nFigure 6-5. \nCreating a custom VM image\nOf course, because you build the image only once, when you subsequently launch copies\nof this image you don’t need to spend time installing your dependencies, as they are\nalready there. This can result in a significant time savings. If your core dependencies don’t\nchange, new versions of your service can continue to use the same base image.\nThere are some drawbacks with this approach, though. Building images can take a long\ntime. This means that for developers you may want to support other ways of deploying\nservices to ensure they don’t have to wait half an hour just to create a binary deployment.\nSecond, some of the resulting images can be large. This could be a real problem if you’re\ncreating your own VMWare images, for example, as moving a 20GB image around a\nnetwork isn’t always a simple activity. We’ll be looking at container technology shortly,\nand specifically Docker, which can avoid some of these drawbacks.\nHistorically, one of the challenges is that the tool chain required to build such an image\nvaried from platform to platform. Building a VMWare image is different from building an\nAWS AMI, a Vagrant image, or a Rackspace image. This may not have been a problem if\nyou had the same platform everywhere, but not all organizations were this lucky. And\neven if they were, the tools in this space were often difficult to work with, and they didn’t\nplay nicely with other tools you might be using for machine configuration.\nPacker\n is a tool designed to make creation of images much easier. Using configuration\nscripts of your choice (Chef, Ansible, Puppet, and more are supported), it allows us to\ncreate images for different platforms from the same configuration. At the time of writing,\nit has support for VMWare, AWS, Rackspace Cloud, Digital Ocean, and Vagrant, and I’ve\nseen teams use it successfully for building Linux and Windows images. This means you\ncould create an image for deployment on your production AWS environment and a\nmatching Vagrant image for local development and test, all from the same configuration.",11430
40-Environments.pdf,40-Environments,"Images as Artifacts\nSo we can create virtual machine images that bake in dependencies to speed up feedback,\nbut why stop there? We could go further, bake our service into the image itself, and adopt\nthe model of our service artifact being an image. Now, when we launch our image, our\nservice is there ready to go. This really fast spin-up time is the reason that Netflix has\nadopted the model of baking its own services as AWS AMIs.\nJust as with OS-specific packages, these VM images become a nice way of abstracting out\nthe differences in the technology stacks used to create the services. Do we care if the\nservice running on the image is written in Ruby or Java, and uses a gem or JAR file? All\nwe care about is that it works. We can focus our efforts, then, on automating the creation\nand deployment of these images. This also becomes a really neat way to implement\nanother deployment concept, the \nimmutable server\n.\nImmutable Servers\nBy storing all our configuration in source control, we are trying to ensure that we can\nautomatically reproduce services and hopefully entire environments at will. But once we\nrun our deployment process, what happens if someone comes along, logs into the box, and\nchanges things independently of what is in source control? This problem is often called\nconfiguration drift\n — the code in source control no longer reflects the configuration of the\nrunning host.\nTo avoid this, we can ensure that no changes are ever made to a running server. Instead,\nany change, no matter how small, has to go through a build pipeline in order to create a\nnew machine. You can implement this pattern without using image-based deployments,\nbut it is also a logical extension of using images as artifacts. During our image creation,\nfor example, we could actually disable SSH, ensuring that no one could even log onto the\nbox to make a change!\nThe same caveats we discussed earlier about cycle time still apply, of course. And we also\nneed to ensure that any data we care about that is stored on the box is stored elsewhere.\nThese complexities aside, I’ve seen adopting this pattern lead to much more\nstraightforward deployments, and easier-to-reason-about environments. And as I’ve\nalready said, anything we can do to simplify things should be pursued!\nEnvironments\nAs our software moves through our CD pipeline stages, it will also be deployed into\ndifferent types of environments. If we think of the example build pipeline in \nFigure 6-4\n,\nwe probably have to consider at least four distinct environments: one environment where\nwe run our slow tests, another for UAT, another for performance, and a final one for\nproduction. Our microservice should be the same throughout, but the environment will be\ndifferent. At the very least, they’ll be separate, distinct collections of configuration and\nhosts. But often they can vary much more than that. For example, our production\nenvironment for our service might consist of multiple load-balanced hosts spread across\ntwo data centers, whereas our test environment might just have everything running on a\nsingle host. These differences in environments can introduce a few problems.\nI was bitten by this personally many years ago. We were deploying a Java web service into\na clustered WebLogic application container in production. This WebLogic cluster\nreplicated session state between multiple nodes, giving us some level of resilience if a\nsingle node failed. However, the WebLogic licenses were expensive, as were the machines\nour software was deployed onto. This meant that in our test environment, our software\nwas deployed on a single machine, in a nonclustered configuration.\nThis hurt us badly during one release. For WebLogic to be able to copy session state\nbetween nodes, the session data needs to be properly serializable. Unfortunately, one of\nour commits broke this, so when we deployed into production our session replication\nfailed. We ended up resolving this by pushing hard to replicate a clustered setup in our test\nenvironment.\nThe service we want to deploy is the same in all these different environments, but each of\nthe environments serves a different purpose. On my developer laptop I want to quickly\ndeploy the service, potentially against stubbed collaborators, to run tests or carry out some\nmanual validation of behavior, whereas when I deploy into a production environment I\nmay want to deploy multiple copies of my service in a load-balanced fashion, perhaps split\nacross one or more data centers for durability reasons.\nAs you move from your laptop to build server to UAT environment all the way to\nproduction, you’ll want to ensure that your environments are more and more production-\nlike to catch any problems associated with these environmental differences sooner. This\nwill be a constant balance. Sometimes the time and cost to reproduce production-like\nenvironments can be prohibitive, so you have to make compromises. Additionally,\nsometimes using a production-like environment can slow down feedback loops; waiting\nfor 25 machines to install your software in AWS might be much slower than simply\ndeploying your service into a local Vagrant instance, for example.\nThis balance, between production-like environments and fast feedback, won’t be static.\nKeep an eye on the bugs you find further downstream and your feedback times, and adjust\nthis balance as required.\nManaging environments for single-artfact monolithic systems can be challenging,\nespecially if you don’t have access to systems that are easily automatable. When you think\nabout multiple environments per microservice, this can be even more daunting. We’ll look\nshortly at some different deployment platforms that can make this much easier for us.",5791
41-Multiple Services Per Host.pdf,41-Multiple Services Per Host,"Service Configuration\nOur services need some configuration. Ideally, this should be a small amount, and limited\nto those features that change from one environment to another, such as \nwhat username and\npassword should I use to connect to my database?\n Configuration that changes from one\nenvironment to another should be kept to an absolute minimum. The more your\nconfiguration changes fundamental service behavior, and the more that configuration\nvaries from one environment to another, the more you will find problems only in certain\nenvironments, which is painful in the extreme.\nSo if we have some configuration for our service that does change from one environment\nto another, how should we handle this as part of our deployment process? One option is to\nbuild one artifact per environment, with configuration inside the artifact itself. Initially\nthis seems sensible. The configuration is built right in; just deploy it and everything should\nwork fine, right? This is problematic. Remember the concept of continuous delivery. We\nwant to create an artifact that represents our release candidate, and move it through our\npipeline, confirming that it is good enough to go into production. Let’s imagine I build a\nCustomer-Service-Test and Customer-Service-Prod artifacts. If my Customer-Service-Test\nartifact passes the tests, but it’s the Customer-Service-Prod artifact that I actually deploy,\ncan I be sure that I have verified the software that actually ends up in production?\nThere are other challenges as well. First, there is the additional time taken to build these\nartifacts. Next, the fact that you need to know at build time what environments exist. And\nhow do you handle sensitive configuration data? I don’t want information about\nproduction passwords checked in with my source code, but if it is needed at build time to\ncreate all those artifacts, this is often difficult to avoid.\nA better approach is to create one single artifact, and manage configuration separately.\nThis could be a properties file that exists for each environment, or different parameters\npassed in to an install process. Another popular option, especially when dealing with a\nlarger number of microservices, is to use a dedicated system for providing configuration,\nwhich we’ll explore more in \nChapter 11\n.\nService-to-Host Mapping\nOne of the questions that comes up quite early on in the discussion around microservices\nis “How many services per machine?” Before we go on, we should pick a better term than\nmachine\n, or even the more generic \nbox\n that I used earlier. In this era of virtualization, the\nmapping between a single host running an operating system and the underlying physical\ninfrastructure can vary to a great extent. Thus, I tend to talk \nabout \nhosts\n, using them as a\ngeneric unit of isolation — namely, an operating system onto which I can install and run\nmy services. If you are deploying directly on to physical machines, then one physical\nserver maps to one \nhost\n (which is perhaps not completely correct terminology in this\ncontext, but in the absence of better terms may have to suffice). If you’re using\nvirtualization, a single physical machine can map to multiple independent hosts, each of\nwhich could hold one or more services.\nSo when thinking of different deployment models, we’ll talk about hosts. So, then, how\nmany services per host should we have?\nI have a definite view as to which model is preferable, but there are a number of factors to\nconsider when working out which model will be right for you. It’s also important to\nunderstand that some choices we make in this regard will limit some of the deployment\noptions available to us.\nMultiple Services Per Host\nHaving multiple services per host, as shown in \nFigure 6-6\n, is attractive for a number of\nreasons. First, purely from a host management point of view, it is simpler. In a world\nwhere one team manages the infrastructure and another team manages the software, the\ninfrastructure team’s workload is often a function of the number of hosts it has to manage.\nIf more services are packed on to a single host, the host management workload doesn’t\nincrease as the number of services increases. Second is cost. Even if you have access to a\nvirtualization platform that allows you to provision and resize virtual hosts, the\nvirtualization can add an overhead that reduces the underlying resources available to your\nservices. In my opinion, both these problems can be addressed with new working practices\nand technology, and we’ll explore that shortly.\nThis model is also familiar to those who deploy into some form of an application\ncontainer. In some ways, the use of an application container is a special case of the\nmultiple-services-per-host model, so we’ll look into that separately. This model can also\nsimplify the life of the developer. Deploying multiple services to a single host in\nproduction is synonymous with deploying multiple services to a local dev workstation or\nlaptop. If we want to look at an alternative model, we want to find a way to keep this\nconceptually simple for developers.\nFigure 6-6. \nMultiple microservices per host\nThere are some challenges with this model, though. First, it can make monitoring more\ndifficult. For example, when tracking CPU, do I need to track the CPU of one service\nindependent of the others? Or do I care about the CPU of the box as a whole? Side effects\ncan also be hard to avoid. If one service is under significant load, it can end up reducing\nthe resources available to other parts of the system. Gilt, when scaling out the number of\nservices it ran, hit this problem. Initially it coexisted many services on a single box, but\nuneven load on one of the services would have an adverse impact on everything else\nrunning on that host. This makes impact analysis of host failures more complex as well —\ntaking a single host out of commission can have a large ripple effect.\nDeployment of services can be somewhat more complex too, as ensuring one deployment\ndoesn’t affect another leads to additional headaches. For example, if I use Puppet to\nprepare a host, but each service has different (and potentially contradictory) dependencies,\nhow can I make that work? In the worst-case scenario, I have seen people tie multiple\nservice deployments together, deploying multiple different services to a single host in one\nstep, to try to simplify the deployment of multiple services to one host. In my opinion, the\nsmall upside in improving simplicity is more than outweighed by the fact that we have\ngiven up one of the key benefits of microservices: striving for independent release of our\nsoftware. If you do adopt the multiple-services-per-host model, make sure you keep hold\nof the idea that each service should be deployed independently.\nThis model can also inhibit autonomy of teams. If services for different teams are installed\non the same host, who gets to configure the host for their services? In all likelihood, this\nends up getting handled by a centralized team, meaning it takes more coordination to get\nservices deployed.\nAnother issue is that this option can limit our deployment artifact options. Image-based\ndeployments are out, as are immutable servers unless you tie multiple different services\ntogether in a single artifact, which we really want to avoid.\nThe fact that we have multiple services on a single host means that efforts to target scaling\nto the service most in need of it can be complicated. Likewise, if one \nmicroservice\n handles\ndata and operations that are especially sensitive, we might want to set up the underlying\nhost differently, or perhaps even place the host itself in a separate network segment.\nHaving everything on one host means we might end up having to treat all services the\nsame way even if their needs are different.\nAs my colleague Neal Ford puts it, many of our working practices around deployment and\nhost management are an attempt to optimize for scarcity of resources. In the past, the only\noption if we wanted another host was to buy or rent another physical machine. This often\nhad a large lead time to it and resulted in a long-term financial commitment. It wasn’t\nuncommon for clients I have worked with to provision new servers only every two to three\nyears, and trying to get additional machines outside of these timelines was difficult. But\non-demand computing platforms have drastically reduced the costs of computing\nresources, and improvements in virtualization technology mean even for in-house hosted\ninfrastructure there is more flexibility.",8664
42-Application Containers.pdf,42-Application Containers,"Application Containers\nIf you’re familiar with deploying .NET applications behind IIS or Java applications into a\nservlet container, you will be well acquainted with the model where multiple distinct\nservices or applications sit inside a single application container, which in turn sits on a\nsingle host, as we see in \nFigure 6-7\n. The idea is that the application container your\nservices live in gives you benefits in terms of improved manageability, such as clustering\nsupport to handle grouping multiple instances together, monitoring tools, and the like.\nFigure 6-7. \nMultiple microservices per host\nThis setup can also yield benefits in terms of reducing overhead of language runtimes.\nConsider running five Java services in a single Java servlet container. I only have the\noverhead of one single JVM. Compare this with running five independent JVMs on the\nsame host when using embedded containers. That said, I still feel that these application\ncontainers have enough downsides that you should challenge yourself to see if they are\nreally required.\nFirst among the downsides is that they inevitably constrain technology choice. You have\nto buy into a technology stack. This can limit not only the technology choices for the\nimplementation of the service itself, but also the options you have in terms of automation\nand management of your systems. As we’ll discuss shortly, one of the ways we can\naddress the overhead of managing multiple hosts is around automation, and so\nconstraining our options for resolving this may well be doubly damaging.\nI would also question some of the value of the container features. Many of them tout the\nability to manage clusters to support shared in-memory session state, something we\nabsolutely want to avoid in any case due to the challenges this creates when scaling our\nservices. And the monitoring capabilities they provide won’t be sufficient when we\nconsider the sorts of joined-up monitoring we want to do in a microservices world, as\nwe’ll see in \nChapter 8\n. Many of them also have quite slow spin-up times, impacting\ndeveloper feedback cycles.\nThere are other sets of problems too. Attempting to do proper lifecycle management of\napplications on top of platforms like the JVM can be problematic, and more complex than\nsimply restarting a JVM. Analyzing resource use and threads is also much more complex,\nas you have multiple applications sharing the same process. And remember, even if you do\nget value from a technology-specific container, they aren’t free. Aside from the fact that\nmany of them are commercial and so have a cost implication, they add a resource\noverhead in and of themselves.\nUltimately, this approach is again an attempt to optimize for scarcity of resources that\nsimply may not hold up anymore. Whether you decide to have multiple services per host\nas a deployment model, I would strongly suggest looking at self-contained deployable\nmicroservices as artifacts. For .NET, this is possible with things like Nancy, and Java has\nsupported this model for years. For example, the venerable Jetty embedded container\nmakes for a very lightweight self-contained HTTP server, which is the core of the\nDropwizard stack. Google has been known to quite happily use embedded Jetty containers\nfor serving static content directly, so we know these things can operate at scale.",3379
43-Single Service Per Host.pdf,43-Single Service Per Host,"Single Service Per Host\nWith a single-service-per-host model shown in \nFigure 6-8\n, we avoid side effects of\nmultiple hosts living on a single host, making monitoring and remediation much simpler.\nWe have potentially reduced our single points of failure. An outage to one host should\nimpact only a single service, although that isn’t always clear when you’re using a\nvirtualized platform. We’ll cover designing for scale and failure more in \nChapter 11\n. We\nalso can more easily scale one service independent from others, and deal with security\nconcerns more easily by focusing our attention only on the service and host that requires\nit.\nFigure 6-8. \nA single microservice per host\nJust as important is that we have opened up the potential to use alternative deployment\ntechniques such as image-based deployments or the immutable server pattern, which we\ndiscussed earlier.\nWe’ve added a lot of complexity in adopting a microservice architecture. The last thing we\nwant to do is go looking for more sources of complexity. In my opinion, if you don’t have\na viable PaaS available, then this model does a very good job of reducing a system’s\noverall complexity. \nHaving a single-service-per-host model is significantly easier to\nreason about and can help reduce complexity. If you can’t embrace this model yet, I won’t\nsay microservices aren’t for you. But I would suggest that you look to move toward this\nmodel over time as a way of reducing the complexity that a microservice architecture can\nbring.\nHaving an increased number of hosts has potential downsides, though. We have more\nservers to manage, and there might also be a cost implication of running more distinct\nhosts. Despite these problems, this is still the model I prefer for microservice\narchitectures. And we’ll talk about a few things we can do to reduce the overhead of\nhandling large numbers of hosts shortly.",1908
44-Traditional Virtualization.pdf,44-Traditional Virtualization,"Platform as a Service\nWhen using a platform as a service (PaaS), you are working at a higher-level abstraction\nthan at a single host. Most of these platforms rely on taking a technology-specific artifact,\nsuch as a Java WAR file or Ruby gem, and automatically provisioning and running it for\nyou. Some of these platforms will transparently attempt to handle scaling the system up\nand down for you, although a more common (and in my experience less error-prone) way\nwill allow you some control over how many nodes your service might run on, but it\nhandles the rest.\nAt the time of writing, most of the best, most polished PaaS solutions are hosted. \nHeroku\ncomes to mind as being probably the gold class of PaaS. It doesn’t just handle running\nyour service, it also supports services like databases in a very simple fashion. Self-hosted\nsolutions do exist in this space, although they are more immature than the hosted\nsolutions.\nWhen PaaS solutions work well, they work very well indeed. However, when they don’t\nquite work for you, you often don’t have much control in terms of getting under the hood\nto fix things. This is part of the trade-off you make. I would say that in my experience the\nsmarter the PaaS solutions try to be, the more they go wrong. I’ve used more than one\nPaaS that attempts to autoscale based on application use, but does it badly. Invariably the\nheuristics that drive these smarts tend to be tailored for the average application rather than\nyour specific use case. The more nonstandard your application, the more likely it is that it\nmight not play nicely with a PaaS.\nAs the good PaaS solutions handle so much for you, they can be an excellent way of\nhandling the increased overhead we get with having many more moving parts. That said,\nI’m still not sure that we have all the models right in this space yet, and the limited self-\nhosted options mean that this approach might not work for you. In the coming decade\nthough I expect we’ll be targeting PaaS for deployment more than having to self-manage\nhosts and deployments of individual services.\nAutomation\nThe answer to so many problems we have raised so far comes down to automation. With a\nsmall number of machines, it is possible to manage everything manually. I used to do this.\nI remember running a small set of production machines, and I would collect logs, deploy\nsoftware, and check processes by manually logging in to the box. My productivity seemed\nto be constrained by the number of terminal windows I could have open at once — a\nsecond monitor was a huge step up. This breaks down really fast, though.\nOne of the pushbacks against the single-service-per-host setup is the perception that the\namount of overhead to manage these hosts will increase. This is certainly true if you are\ndoing everything manually. Double the servers, double the work! But if we automate\ncontrol of our hosts, and deployment of the services, then there is no reason why adding\nmore hosts should increase our workload in a linear fashion.\nBut even if we keep the number of hosts small, we still are going to have lots of services.\nThat means multiple deployments to handle, services to monitor, logs to collect.\nAutomation is essential.\nAutomation is also how we can make sure that our developers still remain productive.\nGiving them the ability to self-service-provision individual services or groups of services\nis key to making developers’ lives easier. Ideally, developers should have access to exactly\nthe same tool chain as is used for deployment of our production services so as to ensure\nthat we can spot problems early on. We’ll be looking at a lot of technology in this chapter\nthat embraces this view.\nPicking technology that enables automation is highly important. This starts with the tools\nused to manage hosts. Can you write a line of code to launch a virtual machine, or shut\none down? Can you deploy the software you have written automatically? Can you deploy\ndatabase changes without manual intervention? Embracing a culture of automation is key\nif you want to keep the complexities of microservice architectures in check.\nTwo Case Studies on the Power of Automation\nIt is probably helpful to give you a couple of concrete examples that explain the power of\ngood automation. One of our clients in Australia is RealEstate.com.au (REA). Among\nother things, the company provides real estate listings for retail and commercial customers\nin Australia and elsewhere in the Asia-Pacific region. Over a number of years, it has been\nmoving its platform toward a distributed, microservices design. When it started on this\njourney it had to spend a lot of time getting the tooling around the services just right —\nmaking it easy for developers to provision machines, to deploy their code, or monitor\nthem. This caused a front-loading of work to get things started.\nIn the first three months of this exercise, REA was able to move just two new\nmicroservices into production, with the development team taking full responsibility for the\nentire build, deployment, and support of the services. In the next three months, between\n10–15 services went live in a similar manner. By the end of the 18-month period, REA had\nover 60–70 services.\nThis sort of pattern is also borne out by the experiences of \nGilt\n, an online fashion retailer\nthat started in 2007. Gilt’s monolithic Rails application was starting to become difficult to\nscale, and the company decided in 2009 to start decomposing the system into\nmicroservices. Again automation, especially tooling to help developers, was given as a\nkey reason to drive Gilt’s explosion in the use of microservices. A year later, Gilt had\naround 10 microservices live; by 2012, over 100; and in 2014, over 450 microservices by\nGilt’s own count — in other words, around three services for every developer in Gilt.\nFrom Physical to Virtual\nOne of the key tools available to us in managing a large number of hosts is finding ways\nof chunking up existing physical machines into smaller parts. Traditional virtualization\nlike VMWare or that used by AWS has yielded huge benefits in reducing the overhead of\nhost management. However, there have been some new advances in this space that are\nwell worth exploring, as they can open up even more interesting possibilities for dealing\nwith our microservice architecture.\nTraditional Virtualization\nWhy is having lots of hosts expensive? Well, if you need a physical server per host, the\nanswer is fairly obvious. If this is the world you are operating in, then the multiple-\nservice-per-host model is probably right for you, although don’t be surprised if this\nbecomes an ever more challenging constraint. I suspect, however, that most of you are\nusing virtualization of some sort. Virtualization allows us to slice up a physical server into\nseparate hosts, each of which can run different things. So if we want one service per host,\ncan’t we just slice up our physical infrastructure into smaller and smaller pieces?\nWell, for some people, you can. However, slicing up the machine into ever increasing\nVMs isn’t free. Think of our physical machine as a sock drawer. If we put lots of wooden\ndividers into our drawer, can we store more socks or fewer? The answer is fewer: the\ndividers themselves take up room too! Our drawer might be easier to deal with and\norganize, and perhaps we could decide to put T-shirts in one of the spaces now rather than\njust socks, but more dividers means less overall space.\nIn the world of virtualization, we have a similar overhead as our sock drawer dividers. To\nunderstand where this overhead comes from, let’s look at how most virtualization is done.\nFigure 6-9\n shows a comparison of two types of virtualization. On the left, we see the\nvarious layers involved in what is called \ntype 2 virtualization\n, which is the sort\nimplemented by AWS, VMWare, VSphere, Xen, and KVM. (Type 1 virtualization refers\nto technology where the VMs run directly on hardware, not on top of another operating\nsystem.) On our physical infrastructure we have a host operating system. On this OS we\nrun something called a \nhypervisor\n, which has two key jobs. First, it maps resources like\nCPU and memory from the virtual host to the physical host. Second, it acts as a control\nlayer, allowing us to manipulate the virtual machines \nthemselves\n.\nFigure 6-9. \nA comparison of standard \nType 2\n virtualization, and lightweight containers\nInside the VMs, we get what looks like completely different hosts. They can run their own\noperating systems, with their own kernels. They can be considered almost hermetically\nsealed machines, kept isolated from the underlying physical host and the other virtual\nmachines by the hypervisor.\nThe problem is that the hypervisor here needs to set aside resources to do its job. This\ntakes away CPU, I/O, and memory that could be used elsewhere. The more hosts the\nhypervisor manages, the more resources it needs. At a certain point, this overhead\nbecomes a constraint in slicing up your physical infrastructure any further. In practice, this\nmeans that there are often diminishing returns in slicing up a physical box into smaller and\nsmaller parts, as proportionally more and more resources go into the overhead of the\nhypervisor.",9360
45-Linux Containers.pdf,45-Linux Containers,"Vagrant\nVagrant is a very useful deployment platform, which is normally used for dev and test\nrather than production. Vagrant provides you with a virtual cloud on your laptop.\nUnderneath, it uses a standard virtualization system (typically VirtualBox, although it can\nuse other platforms). It allows you to define a set of VMs in a text file, along with how the\nVMs are networked together and which images the VMs should be based on. This text file\ncan be checked in and shared between team members.\nThis makes it easier for you to create production-like environments on your local machine.\nYou can spin up multiple VMs at a time, shut individual ones to test failure modes, and\nhave the VMs mapped through to local directories so you can make changes and see them\nreflected immediately. Even for teams using on-demand cloud platforms like AWS, the\nfaster turnaround of using Vagrant can be a huge boon for development teams.\nOne of the downsides, though, is that running lots of VMs can tax the average\ndevelopment machine. If we have one service to one VM, you may not be able to bring up\nyour entire system on your local machine. This can result in the need to stub out some\ndependencies to make things manageable, which is one more thing you’ll have to handle\nto ensure that the development and test experience is a good one.\nLinux Containers\nFor Linux users, there is an alternative to virtualization. Rather than having a hypervisor\nto segment and control separate virtual hosts, Linux containers instead create a separate\nprocess space in which other processes live.\nOn Linux, process are run by a given user, and have certain capabilities based on how the\npermissions are set. Processes can spawn other processes. For example, if I launch a\nprocess in a terminal, that child process is generally considered a child of the terminal\nprocess. The Linux kernel’s job is maintaining this tree of processes.\nLinux containers extend this idea. Each container is effectively a subtree of the overall\nsystem process tree. These containers can have physical resources allocated to them,\nsomething the kernel handles for us. This general approach has been around in many\nforms, such as Solaris Zones and OpenVZ, but it is LXC that has become most popular.\nLXC is now available out of the box in any modern Linux kernel.\nIf we look at a stack diagram for a host running LXC in \nFigure 6-9\n, we see a few\ndifferences. First, we don’t need a hypervisor. Second, although each container can run its\nown operating system distribution, it has to share the same kernel (because the kernel is\nwhere the process tree lives). This means that our host operating system could run Ubuntu,\nand our containers CentOS, as long as they could both share the same kernel.\nWe don’t just benefit from the resources saved by not needing a hypervisor. We also gain\nin terms of feedback. Linux containers are \nmuch\n faster to provision than full-fat virtual\nmachines. It isn’t uncommon for a VM to take many minutes to start — but with Linux\ncontainers, startup can take a few seconds. You also have finer-grained control over the\ncontainers themselves in terms of assigning resources to them, which makes it much easier\nto tweak the settings to get the most out of the underlying \nhardware\n.\nDue to the lighter-weight nature of containers, we can have many more of them running\non the same hardware than would be possible with VMs. By deploying one service per\ncontainer, as in \nFigure 6-10\n, we get a degree of isolation from other containers (although\nthis isn’t perfect), and can do so much more cost effectively than would be possible if we\nwanted to run each service in its own VM.\nFigure 6-10. \nRunning services in separate containers\nContainers can be used well with full-fat virtualization too. I’ve seen more than one\nproject provision a large AWS EC2 instance and run LXC containers on it to get the best\nof both worlds: an on-demand ephemeral compute platform in the form of EC2, coupled\nwith highly flexible and fast containers running on top of it.\nLinux containers aren’t without some problems, however. Imagine I have lots of\nmicroservices running in their own containers on a host. How does the outside world see\nthem? You need some way to route the outside world through to the underlying containers,\nsomething many of the hypervisors do for you with normal virtualization. I’ve seen many\na person sink inordinate amounts of time into configuring port forwarding using IPTables\nto expose containers directly. Another point to bear in mind is that these containers cannot\nbe considered completely sealed from each other. There are many documented and known\nways in which a process from one container can bust out and interact with other containers\nor the underlying host. Some of these problems are by design and some are bugs that are\nbeing addressed, but either way if you don’t trust the code you are running, don’t expect\nthat you can run it in a container and be safe. If you need that sort of isolation, you’ll need\nto consider using virtual machines instead.",5126
46-Docker.pdf,46-Docker,"Docker\nDocker is a platform built on top of lightweight containers. It handles much of the work\naround handling containers for you. In Docker, you create and deploy \napps\n, which are\nsynonymous with images in the VM world, albeit for a container-based platform. Docker\nmanages the container provisioning, handles some of the networking problems for you,\nand even provides its own registry concept that allows you to store and version Docker\napplications.\nThe Docker app abstraction is a useful one for us, because just as with VM images the\nunderlying technology used to implement the service is hidden from us. We have our\nbuilds for our services create Docker applications, and store them in the Docker registry,\nand away we go.\nDocker can also alleviate some of the downsides of running lots of services locally for dev\nand test purposes. Rather than using Vagrant to host multiple independent VMs, each one\ncontaining its own service, we can host a single VM in Vagrant that runs a Docker\ninstance. We then use Vagrant to set up and tear down the Docker platform itself, and use\nDocker for fast provisioning of individual services.\nA number of different technologies are being developed to take advantage of Docker.\nCoreOS is a very interesting operating system designed with Docker in mind. It is a\nstripped-down Linux OS that provides only the essential services to allow Docker to run.\nThis means it consumes fewer resources than other operating systems, making it possible\nto dedicate even more resources of the underlying machine to our containers. Rather than\nusing a package manager like debs or RPMs, all software is installed as independent\nDocker apps, each running in its own container.\nDocker itself doesn’t solve all problems for us. Think of it as a simple PaaS that works on\na single machine. If you want tools to help you manage services across multiple Docker\ninstances across multiple machines, you’ll need to look at other software that adds these\ncapabilities. There is a key need for a scheduling layer that lets you request a container\nand then finds a Docker container that can run it for you. In this space, Google’s recently\nopen sourced Kubernetes and CoreOS’s cluster technology can help, and it seems every\nmonth there is a new entrant in this space. \nDeis\n is another interesting tool based on\nDocker, which is attempting to provide a Heroku-like PaaS on top of Docker.\nI talked earlier about PaaS solutions. My struggle with them has always been that they\noften get the abstraction level wrong, and that self-hosted solutions lag significantly\nbehind hosted solutions like Heroku. Docker gets much more of this right, and \nthe\nexplosion of interest in this space means I suspect it will become a much more viable\nplatform for all sorts of deployments over the next few years for all sorts of different use\ncases. In many ways, Docker with an appropriate scheduling layer sits between IaaS and\nPaaS solutions — the term \ncontainers as a service (CaaS)\n is already being used to\ndescribe it.\nDocker is being used in production by multiple companies. It provides many of the\nbenefits of lightweight containers in terms of efficiency and speed of provisioning,\ntogether with the tools to avoid many of the downsides. If you are interested in looking at\nalternative deployment platforms, I’d strongly suggest you give Docker a look.",3410
47-A Deployment Interface.pdf,47-A Deployment Interface,"A Deployment Interface\nWhatever underlying platform or artifacts you use, having a uniform interface to deploy a\ngiven service is vital. We’ll want to trigger deployment of a microservice on demand in a\nvariety of different situations, from deployments locally for dev and test to production\ndeployments. We’ll also want to keep our deployment mechanisms as similar as possible\nfrom dev to production, as the last thing we want is to find ourselves hitting problems in\nproduction because deployment uses a completely different process!\nAfter many years of working in this space, I am convinced that the most sensible way to\ntrigger any deployment is via a single, parameterizable command-line call. This can be\ntriggered by scripts, launched by your CI tool, or typed in by hand. I’ve built wrapper\nscripts in a variety of technology stacks to make this work, from Windows batch, to bash,\nto Python Fabric scripts, and more, but all of the command lines share the same basic\nformat.\nWe need to know what we are deploying, so we need to provide the name of a known\nentity, or in our case a microservice. We also need to know what version of the entity we\nwant. The answer to \nwhat version\n tends to be one of three possibilities. When you’re\nworking locally, it’ll be whatever version is on your local machine. When testing, you’ll\nwant the latest \ngreen\n build, which could just be the most recent blessed artifact in our\nartifact repository. Or when testing/diagnosing issues, we may want to deploy an exact\nbuild.\nThe third and final thing we’ll need to know is what environment we want the\nmicroservice deployed into. As we discussed earlier, our microservice’s topology may\ndiffer from one environment to the next, but that should be hidden from us here.\nSo, imagine we create a simple \ndeploy\n script that takes these three parameters. Say we’re\ndeveloping locally and want to deploy our catalog service into our local environment. I\nmight type:\n$ deploy artifact=catalog environment=local version=local\nOnce I’ve checked in, our CI build service picks up the change and creates a new build\nartifact, giving it the build number \nb456\n. As is standard in most CI tools, this value gets\npassed along the pipeline. When our test stage gets triggered, the CI stage will run:\n$ deploy artifact=catalog environment=ci version=b456\nMeanwhile, our QA wants to pull the latest version of the catalog service into an\nintegrated test environment to do some exploratory testing, and to help with a showcase.\nThat team runs:\n$ deploy artifact=catalog environment=integrated_qa version=latest\nThe tool I’ve used the most for this is Fabric, a Python library designed to map command-\nline calls to functions, along with good support for handling tasks like SSH into remote\nmachines. Pair it with an AWS client library like Boto, and you have everything you need\nto fully automate very large AWS environments. For Ruby, Capistrano is similar in some\nways to Fabric, and on Windows you could go a long way using PowerShell.",3058
48-Environment Definition.pdf,48-Environment Definition,"Environment Definition\nClearly, for this to work, we need to have some way of defining what our environments\nlook like, and what our service looks like in a given environment. You can think of an\nenvironment definition as a mapping from a microservice to compute, network, and\nstorage resources. I’ve done this with YAML files before, and used my scripts to pull this\ndata in. \nExample 6-1\n is a simplified version of some work I did a couple of years ago for a\nproject that used AWS.\nExample 6-1. \nAn example environment definition\ndevelopment\n:\n  \nnodes\n:\n  \n-\n \nami_id\n:\n \nami-e1e1234\n    \nsize\n:\n   \nt1.micro\n \n    \ncredentials_name\n:\n \neu-west-ssh\n \n    \nservices\n:\n \n[\ncatalog-service\n]\n    \nregion\n:\n \neu-west-1\nproduction\n:\n  \nnodes\n:\n  \n-\n \nami_id\n:\n \nami-e1e1234\n    \nsize\n:\n   \nm3.xlarge\n \n    \ncredentials_name\n:\n \nprod-credentials\n \n    \nservices\n:\n \n[\ncatalog-service\n]\n    \nnumber\n:\n \n5\n  \nWe varied the size of the instances we used to be more cost effective. You don’t need\na 16-core box with 64GB of RAM for exploratory testing!\nBeing able to specify different credentials for different environments is key.\nCredentials for sensitive environments were stored in different source code repos that\nonly select people would have access to.\nWe decided that by default if a service had more than one node configured, we would\nautomatically create a load balancer for it.\nI have removed some detail for the sake of brevity.\nThe \ncatalog-service\n information was stored elsewhere. It didn’t differ from one\nenvironment to the next, as you can see in \nExample 6-2\n.\nExample 6-2. \nAn example environment definition\ncatalog-service\n:\n  \npuppet_manifest\n \n:\n \ncatalog.pp\n \n  \nconnectivity\n:\n    \n-\n \nprotocol\n:\n \ntcp\n      \nports\n:\n \n[\n \n8080\n,\n \n8081\n \n]\n      \nallowed\n:\n \n[\n \nWORLD\n \n]\nThis was the name of the Puppet file to run — we happened to use Puppet solo in this\nsituation, but theoretically could have supported alternative configuration systems.\nObviously, a lot of the behavior here was convention based. For example, we decided to\nnormalize which ports services used wherever they ran, and automatically configured load\nbalancers if a service had more than one instance (something that AWS’s ELBs make\nfairly easy).\nBuilding a system like this required a significant amount of work. The effort is often front-\nloaded, but can be essential to manage the deployment complexity you have. I hope in the\nfuture you won’t have to do this yourself. Terraform is a very new tool from Hashicorp,\nwhich works in this space. I’d generally shy away from mentioning such a new tool in a\nbook that is more about ideas than technology, but it is attempting to create an open source\ntool along these lines. It’s early days yet, but already its capabilities seem really\ninteresting. With the ability to target deployments on a number of different platforms, in\nthe future it could be just the tool for the job.",3067
49-Types of Tests.pdf,49-Types of Tests,"Summary\nWe’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the\nability to release one service independently from another, and make sure that whatever\ntechnology you select supports this. I greatly prefer having a single repository per\nmicroservice, but am firmer still that you need one CI build per microservice if you want\nto deploy them separately.\nNext, if possible, move to a single-service per host/container. Look at alternative\ntechnologies like LXC or Docker to make managing the moving parts cheaper and easier,\nbut understand that whatever technology you adopt, a culture of automation is key to\nmanaging everything. Automate everything, and if the technology you have doesn’t allow\nthis, get some new technology! Being able to use a platform like AWS will give you huge\nbenefits when it comes to automation.\nMake sure you understand the impact your deployment choices have on developers, and\nmake sure they feel the love too. Creating tools that let you self-service-deploy any given\nservice into a number of different environments is really important, and will help\ndevelopers, testers, and operations people alike.\nFinally, if you want to go deeper into this topic, I thoroughly recommend you read Jez\nHumble and David Farley’s \nContinuous Delivery\n (Addison-Wesley), which goes into\nmuch more detail on subjects like pipeline design and artifact management.\nIn the next chapter, we’ll be going deeper into a topic we touched on briefly here. Namely,\nhow do we test our microservices to make sure they actually work?\nChapter 7. \nTesting\nThe world of automated testing has advanced significantly since I first started writing\ncode, and every month there seems to be some new tool or technique to make it even\nbetter. But challenges remain as how to effectively and efficiently test our functionality\nwhen it spans a distributed system. This chapter breaks down the problems associated with\ntesting finer-grained systems and presents some solutions to help you make sure you can\nrelease your new functionality with confidence.\nTesting covers a lot of ground. Even when we are \njust\n talking about automated tests, there\nare a large number to consider. With microservices, we have added another level of\ncomplexity. Understanding what different types of tests we can run is important to help us\nbalance the sometimes-opposing forces of getting our software into production as quickly\nas possible versus making sure our software is of sufficient quality.\nTypes of Tests\nAs a consultant, I like pulling out the odd quadrant as a way of categorizing the world, and\nI was starting to worry this book wouldn’t have one. Luckily, Brian Marick came up with\na fantastic categorization system for tests that fits right in. \nFigure 7-1\n shows a variation of\nMarick’s quadrant from Lisa Crispin and Janet Gregory’s book \nAgile Testing\n (Addison-\nWesley) that helps categorize the different types of tests.\nFigure 7-1. \nBrian Marick’s testing quadrant. Crispin, Lisa; Gregory, Janet, Agile Testing: A Practical Guide for Testers\nand Agile Teams, 1st Edition, © 2009. Adapted by permission of Pearson Education, Inc., Upper Saddle River, NJ.\nAt the bottom, we have tests that are \ntechnology-facing\n — that is, tests that aid the\ndevelopers in creating the system in the first place. Performance tests and small-scoped\nunit\n tests fall into this category — all typically automated. This is compared with the top\nhalf of the quadrant, where tests help the nontechnical stakeholders understand how your\nsystem works. These could be large-scoped, end-to-end tests, as shown in the top-left\nAcceptance Test square, or manual testing as typified by user testing done against a UAT\nsystem, as shown in the Exploratory Testing square.\nEach type of test shown in this quadrant has a place. Exactly how much of each test you\nwant to do will depend on the nature of your system, but the key point to understand is\nthat you have multiple choices in terms of how to test your system. The trend recently has\nbeen away from any large-scale manual testing, in favor of automating as much as\npossible, and I certainly agree with this approach. If you currently carry out large amounts\nof manual testing, I would suggest you address that before proceeding too far down the\npath of microservices, as you won’t get many of their benefits if you are unable to validate\nyour software quickly and efficiently.\nFor the purposes of this chapter, we will ignore manual testing. Although this sort of\ntesting can be very useful and certainly has its part to play, the differences with testing a\nmicroservice architecture mostly play out in the context of various types of automated\ntests, so that is where we will focus our time.\nBut when it comes to automated tests, how many of each test do we want? Another model\nwill come in very handy to help us answer this question, and understand what the different\ntrade-offs might be.",5011
50-Test Scope.pdf,50-Test Scope,"Test Scope\nIn his book \nSucceeding with Agile\n (Addison-Wesley), Mike Cohn outlines a model called\nthe Test Pyramid to help explain what types of automated tests you need. The pyramid\nhelps us think about the scopes the tests should cover, but also the proportions of different\ntypes of tests we should aim for. Cohn’s original model split automated tests into Unit,\nService, and UI, which you can see in \nFigure 7-2\n.\nFigure 7-2. \nMike Cohn’s Test Pyramid. Cohn, Mike, Succeeding with Agile: Software Development Using Scrum, 1st\nEdition, © 2010. Adapted by permission of Pearson Education, Inc., Upper Saddle River, NJ.\nThe problem with this model is that all these terms mean different things to different\npeople. “Service” is especially overloaded, and there are many definitions of a unit test out\nthere. Is a test a unit test if I only test one line of code? I’d say yes. Is it still a unit test if I\ntest multiple functions or classes? I’d say no, but many would disagree! I tend to stick\nwith the Unit and Service names despite their ambiguity, but much prefer calling \nUI\n tests\nend-to-end\n tests, which we’ll do from now on.\nGiven the confusion, it’s worth us looking at what these different layers mean.\nLet’s look at a worked example. In \nFigure 7-3\n, we have our helpdesk application and our\nmain website, both of which are interacting with our customer service to retrieve, review,\nand edit customer details. Our customer service in turn is talking to our loyalty points\nbank, where our customers accrue points by buying Justin Bieber CDs. Probably. This is\nobviously a sliver of our overall music shop system, but it is a good enough slice for us to\ndive into a few different scenarios we may want to test.\nFigure 7-3. \nPart of our music shop under test",1800
51-Those Tricky End-to-End Tests.pdf,51-Those Tricky End-to-End Tests,"Unit Tests\nThese are tests that typically test a single function or method call. The tests generated as a\nside effect of \ntest-driven design\n (TDD) will fall into this category, as do the sorts of tests\ngenerated by techniques such as property-based testing. We’re not launching services here,\nand are limiting the use of external files or network connections. In general, you want a\nlarge number of these sorts of tests. Done right, they are very, very fast, and on modern\nhardware you could expect to run many thousands of these in less than a minute.\nThese are tests that help us developers and so would be \ntechnology-facing\n, not \nbusiness-\nfacing\n, in Marick’s terminology. They are also where we hope to catch most of our bugs.\nSo, in our example, when we think about the customer service, unit tests would cover\nsmall parts of the code in isolation, as shown in \nFigure 7-4\n.\nFigure 7-4. \nScope of unit tests on our example system\nThe prime goal of these tests is to give us very fast feedback about whether our\nfunctionality is good. Tests can be important to support refactoring of code, allowing us to\nrestructure our code as we go, knowing that our small-scoped tests will catch us if we\nmake a mistake.\nService Tests\nService tests are designed to bypass the user interface and test services directly. In a\nmonolithic application, we might just be testing a collection of classes that provide a\nservice\n to the UI. For a system comprising a number of services, a service test would test\nan individual service’s capabilities.\nThe reason we want to test a single service by itself is to improve the isolation of the test\nto make finding and fixing problems faster. To achieve this isolation, we need to stub out\nall external collaborators so only the service itself is in scope, as \nFigure 7-5\n shows.\nFigure 7-5. \nScope of service tests on our example system\nSome of these tests could be as fast as small tests, but if you decide to test against a real\ndatabase, or go over networks to stubbed downstream collaborators, test times can\nincrease. They also cover more scope than a simple unit test, so that when they fail it can\nbe harder to detect what is broken than with a unit test. \nHowever, they have much fewer\nmoving parts and are therefore less brittle than larger-scoped tests.\nEnd-to-End Tests\nEnd-to-end tests are tests run against your entire system. Often they will be driving a GUI\nthrough a browser, but could easily be mimicking other sorts of user interaction, like\nuploading a file.\nThese tests cover a lot of production code, as we see in \nFigure 7-6\n . So when they pass,\nyou feel good: you have a high degree of confidence that the code being tested will work\nin production. But this increased scope comes with downsides, and as we’ll see shortly,\nthey can be very tricky to do well in a microservices context.\nFigure 7-6. \nScope of end-to-end tests on our example system\nTrade-Offs\nWhen you’re reading the pyramid, the key thing to take away is that as you go up the\npyramid, the test scope increases, as does our confidence that the functionality being\ntested works. On the other hand, the feedback cycle time increases as the tests take longer\nto run, and when a test fails it can be harder to determine which functionality has broken.\nAs you go down the pyramid, in general the tests become much faster, so we get much\nfaster feedback cycles. We find broken functionality faster, our continuous integration\nbuilds are faster, and we are less likely to move on to a new task before finding out we\nhave broken something. When those smaller-scoped tests fail, we also tend to know what\nbroke, often exactly what line of code. On the flipside, we don’t get a lot of confidence\nthat our system as a whole works if we’ve only tested one line of code!\nWhen broader-scoped tests like our service or end-to-end tests fail, we will try to write a\nfast unit test to catch that problem in the future. In that way, we are constantly trying to\nimprove our feedback cycles.\nVirtually every team I’ve worked on has used different names than the ones that Cohn\nuses in the pyramid. Whatever you call them, the key takeaway is that you will want tests\nof different scope for different purposes.\nHow Many?\nSo if these tests all have trade-offs, how many of each type do you want? A good rule of\nthumb is that you probably want an order of magnitude more tests as you descend the\npyramid, but the important thing is knowing that you do have different types of automated\ntests and understanding if your current balance gives you a problem!\nI worked on one monolithic system, for example, where we had 4,000 unit tests, 1,000\nservice tests, and 60 end-to-end tests. We decided that from a feedback point of view we\nhad way too many service and end-to-end tests (the latter of which were the worst\noffenders in impacting feedback loops), so we worked hard to replace the test coverage\nwith smaller-scoped tests.\nA common anti-pattern is what is often referred to as a \ntest snow cone\n, or inverted\npyramid. Here, there are little to no small-scoped tests, with all the coverage in large-\nscoped tests. These projects often have glacially slow test runs, and very long feedback\ncycles. If these tests are run as part of continuous integration, you won’t get many builds,\nand the nature of the build times means that the build can stay broken for a long period\nwhen something does break.\nImplementing Service Tests\nImplementing unit tests is a fairly simple affair in the grand scheme of things, and there is\nplenty of documentation out there explaining how to write them. The service and end-to-\nend tests are the ones that are more interesting.\nOur service tests want to test a slice of functionality across the whole service, but to\nisolate ourselves from other services we need to find some way to stub out all of our\ncollaborators. So, if we wanted to write a test like this for the customer service from\nFigure 7-3\n, we would deploy an instance of the customer service, and as discussed earlier\nwe would want to stub out any downstream services.\nOne of the first things our continuous integration build will do is create a binary artifact\nfor our service, so deploying that is pretty straightforward. But how do we handle faking\nthe downstream collaborators?\nOur service test suite needs to launch stub services for any downstream collaborators (or\nensure they are running), and configure the service under test to connect to the stub\nservices. We then need to configure the stubs to send responses back to mimic the real-\nworld services. For example, we might configure the stub for the loyalty points bank to\nreturn known points balances for certain customers.\nMocking or Stubbing\nWhen I talk about stubbing downstream collaborators, I mean that we create a stub service\nthat responds with canned responses to known requests from the service under test. For\nexample, I might tell my stub points bank that when asked for the balance of customer\n123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A\nvariation on this is to use a mock instead of a stub.\nWhen using a mock, I actually go further and make sure the call was made. If the expected\ncall is not made, the test fails. Implementing this approach requires more smarts in the\nfake collaborators that we create, and if overused can cause tests to become brittle. As\nnoted, however, a stub doesn’t care if it is called 0, 1, or many times.\nSometimes, though, mocks can be very useful to ensure that the expected side effects\nhappen. For example, I might want to check that when I create a customer, a new points\nbalance is set up for that customer. The balance between stubbing and mocking calls is a\ndelicate one, and is just as fraught in service tests as in unit tests. In general, though, I use\nstubs far more than mocks for service tests. For a more in-depth discussion of this trade-\noff, take a look at \nGrowing Object-Oriented Software, Guided by Tests\n, by Steve Freeman\nand Nat Pryce (Addison-Wesley).\nIn general, I rarely use mocks for this sort of testing. But having a tool that can do both is\nuseful.\nWhile I feel that stubs and mocks are actually fairly well differentiated, I know the\ndistinction can be confusing to some, especially when some people throw in other terms\nlike \nfakes\n, \nspies\n, and \ndummies\n. Martin Fowler calls all of these things, including stubs and\nmocks, \ntest doubles\n.\nA Smarter Stub Service\nNormally for stub services I’ve rolled them myself. I’ve used everything from Apache or\nNginx to embedded Jetty containers or even command-line-launched Python web servers\nused to launch stub servers for such test cases. I’ve probably reproduced the same work\ntime and time again in creating these stubs. My ThoughtWorks colleague Brandon Bryars\nhas potentially saved many of us a chunk of work with his stub/mock server called\nMountebank\n.\nYou can think of Mountebank as a small software appliance that is programmable via\nHTTP. The fact that it happens to be written in NodeJS is completely opaque to any\ncalling service. When it launches, you send it commands telling it what port to stub on,\nwhat protocol to handle (currently TCP, HTTP, and HTTPS are supported, with more\nplanned), and what responses it should send when requests are sent. It also supports\nsetting expectations if you want to use it as a mock. You can add or remove these stub\nendpoints at will, making it possible for a single Mountebank instance to stub more than\none downstream dependency.\nSo, if we want to run our service tests for just our customer service we can launch the\ncustomer service, and a Mountebank instance that acts as our loyalty points bank. And if\nthose tests pass, I can deploy the customer service straightaway! Or can I? What about the\nservices that call the customer service — the helpdesk and the web shop? Do we know if\nwe have made a change that may break them? Of course, we have forgotten the important\ntests at the top of the pyramid: the end-to-end tests.\nThose Tricky End-to-End Tests\nIn a microservice system, the capabilities we expose via our user interfaces are delivered\nby a number of services. The point of the end-to-end tests as outlined in Mike Cohn’s\npyramid is to drive functionality through these user interfaces against everything\nunderneath to give us an overview of a large amount of our system.\nSo, to implement an end-to-end test we need to deploy multiple services together, then run\na test against all of them. Obviously, this test has much more scope, resulting in more\nconfidence that our system works! On the other hand, these tests are liable to be slower\nand make it harder to diagnose failure. Let’s dig into them a bit more using our previous\nexample to see how these tests can fit in.\nImagine we want to push out a new version of the customer service. We want to deploy\nour changes into production as soon as possible, but are concerned that we may have\nintroduced a change that could break either the helpdesk or the web shop. No problem —\nlet’s deploy all of our services together, and run some tests against the helpdesk and web\nshop to see if we’ve introduced a bug. Now a naive approach would be to just add these\ntests onto the end of our customer service pipeline, as in \nFigure 7-7\n.\nFigure 7-7. \nAdding our end-to-end tests stage: the right approach?\nSo far, so good. But the first question we have to ask ourselves is which version of the\nother services should we use? Should we run our tests against the versions of helpdesk and\nweb shop that are in production? It’s a sensible assumption, but what if there is a new\nversion of either the helpdesk or web shop queued up to go live; what should we do then?\nAnother problem: if we have a set of customer service tests that deploy lots of services\nand run tests against them, what about the end-to-end tests that the other services run? If\nthey are testing the same thing, we may find ourselves covering lots of the same ground,\nand may duplicate a lot of the effort to deploy all those services in the first place.\nWe can deal with both of these problems elegantly by having multiple pipelines \nfan in\n to a\nsingle, end-to-end test stage. Here, whenever a new build of one of our services is\ntriggered, we run our end-to-end tests, an example of which we can see in \nFigure 7-8\n.\nSome CI tools with better build pipeline support will enable fan-in models like this out of\nthe box.\nFigure 7-8. \nA standard way to handle end-to-end tests across services\nSo any time any of our services changes, we run the tests local to that service. If those\ntests pass, we trigger our integration tests. Great, eh? Well, there are a few problems.",12873
52-Consumer-Driven Tests to the Rescue.pdf,52-Consumer-Driven Tests to the Rescue,"Downsides to End-to-End Testing\nThere are, unfortunately, many disadvantages to end-to-end testing.\nFlaky and Brittle Tests\nAs test scope increases, so too do the number of moving parts. These moving parts can\nintroduce test failures that do not show that the functionality under test is broken, but that\nsome other problem has occurred. As an example, if we have a test to verify that we can\nplace an order for a single CD, but we are running that test against four or five services, if\nany of them is down we could get a failure that has nothing to do with the nature of the\ntest itself. Likewise, a temporary network glitch could cause a test to fail without saying\nanything about the functionality under test.\nThe more moving parts, the more brittle our tests may be, and the less deterministic they\nare. If you have tests that \nsometimes\n fail, but everyone just re-runs them because they may\npass again later, then you have flaky tests. It isn’t only tests covering lots of different\nprocess that are the culprit here. Tests that cover functionality being exercised on multiple\nthreads are often problematic, where a failure could mean a race condition, a timeout, or\nthat the functionality is actually broken. Flaky tests are the enemy. When they fail, they\ndon’t tell us much. We re-run our CI builds in the hope that they will pass again later, only\nto see check-ins pile up, and suddenly we find ourselves with a load of broken\nfunctionality.\nWhen we detect flaky tests, it is essential that we do our best to remove them. Otherwise,\nwe start to lose faith in a test suite that “always fails like that.” A test suite with flaky tests\ncan become a victim of what Diane Vaughan calls the \nnormalization of deviance\n — the\nidea that over time we can become so accustomed to things being wrong that we start to\naccept them as being normal and not a problem.\n2\n This very human tendency means we\nneed to find and eliminate these tests as soon as we can before we start to assume that\nfailing tests are OK.\nIn \n“Eradicating Non-Determinism in Tests”\n, Martin Fowler advocates the approach that if\nyou have flaky tests, you should track them down and if you can’t immediately fix them,\nremove them from the suite so you can treat them. See if you can rewrite them to avoid\ntesting code running on multiple threads. See if you can make the underlying environment\nmore stable. Better yet, see if you can replace the flaky test with a smaller-scoped test that\nis less likely to exhibit problems. In some cases, changing the software under test to make\nit easier to test can also be the right way forward.\nWho Writes These Tests?\nWith the tests that run as part of the pipeline for a specific service, the sensible starting\npoint is that the team that owns that service should write those tests (we’ll talk more about\nservice ownership in \nChapter 10\n). But if we consider that we might have multiple teams\ninvolved, and the end-to-end-tests step is now effectively shared between the teams, who\nwrites and looks after these tests?\nI have seen a number of anti-patterns caused here. These tests become a free-for-all, with\nall teams granted access to add tests without any understanding of the health of the whole\nsuite. This can often result in an explosion of test cases, sometimes resulting in the test\nsnow cone we talked about earlier. I have seen situations where, because there was no real\nobvious ownership of these tests, their results get ignored. When they break, everyone\nassumes it is someone else’s problem, so they don’t care whether the tests are passing.\nSometimes organizations react by having a dedicated team write these tests. This can be\ndisastrous. The team developing the software becomes increasingly distant from the tests\nfor its code. Cycle times increase, as service owners end up waiting for the test team to\nwrite end-to-end tests for the functionality they just wrote. Because another team writes\nthese tests, the team that wrote the service is less involved with, and therefore less likely\nto know, how to run and fix these tests. Although it is unfortunately still a common\norganizational pattern, I see significant harm done whenever a team is distanced from\nwriting tests for the code it wrote in the first place.\nGetting this aspect right is really hard. We don’t want to duplicate effort, nor do we want\nto completely centralize this to the extent that the teams building services are too far\nremoved from things. The best balance I have found is to treat the end-to-end test suite as\na shared codebase, but with joint ownership. Teams are free to check in to this suite, but\nthe ownership of the health of the suite has to be shared between the teams developing the\nservices themselves. If you want to make extensive use of end-to-end tests with multiple\nteams I think this approach is essential, and yet I have seen it done very rarely, and never\nwithout issue.\nHow Long?\nThese end-to-end tests can take a while. I have seen them take up to a day to run, if not\nmore, and on one project I worked on, a full regression suite took six weeks! I rarely see\nteams actually curate their end-to-end test suites to reduce overlap in test coverage, or\nspend enough time in making them fast.\nThis slowness, combined with the fact that these tests can often be flaky, can be a major\nproblem. A test suite that takes all day and often has breakages that have nothing to do\nwith broken functionality are a disaster. Even if your functionality \nis\n broken, it could take\nyou many hours to find out — at which point many of us would already have moved on to\nother activities, and the context switch in shifting our brains back to fix the issue is\npainful.\nWe can ameliorate some of this by running tests in parallel — for example, making use of\ntools like Selenium Grid. However, this approach is not a substitute for actually\nunderstanding what needs to be tested and actively \nremoving\n tests that are no longer\nneeded.\nRemoving tests is sometimes a fraught exercise, and I suspect shares much in common\nwith people who want to remove certain airport security measures. No matter how\nineffective the security measures might be, any conversation about removing them is often\ncountered with knee-jerk reactions about not caring about people’s safety or wanting\nterrorists to win. It is hard to have a balanced conversation about the value something adds\nversus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get\nthanked if you remove a test? Maybe. But you’ll certainly get blamed if a test you\nremoved lets a bug through. When it comes to the larger-scoped test suites, however, this\nis exactly what we need to be able to do. If the same feature is covered in 20 different\ntests, perhaps we can get rid of half of them, as those 20 tests take 10 minutes to run!\nWhat this requires is a better understanding of risk, which something humans are\nfamously bad at. As a result, this intelligent curation and management of larger-scoped,\nhigh-burden tests happens incredibly infrequently. Wishing people did this more isn’t the\nsame thing as making it happen.\nThe Great Pile-up\nThe long feedback cycles associated with end-to-end tests aren’t just a problem when it\ncomes to developer productivity. With a long test suite, any breaks take a while to fix,\nwhich reduces the amount of time that the end-to-end tests can be expected to be passing.\nIf we deploy only software that has passed through all our tests successfully (which we\nshould!), this means fewer of our services get through to the point of being deployable\ninto production.\nThis can lead to a pile-up. While a broken integration test stage is being fixed, more\nchanges from upstream teams can pile in. Aside from the fact that this can make fixing the\nbuild harder, it means the scope of changes to be deployed increases. One way to resolve\nthis is to not let people check in if the end-to-end tests are failing, but given a long test\nsuite time this is often impractical. Try saying, “You 30 developers: no check-ins til we fix\nthis seven-hour-long build!”\nThe larger the scope of a deployment and the higher the risk of a release, the more likely\nwe are to break something. A key driver to ensuring we can release our software\nfrequently is based on the idea that we release small changes as soon as they are ready.\nThe Metaversion\nWith the end-to-end test step, it is easy to start thinking, \nSo, I know all these services at\nthese versions work together, so why not deploy them all together?\n This very quickly\nbecomes a conversation along the lines of, \nSo why not use a version number for the whole\nsystem\n? To quote \nBrandon Bryars\n, “Now you have 2.1.0 problems.”\nBy versioning together changes made to multiple services, we effectively embrace the idea\nthat changing and deploying multiple services at once is acceptable. It becomes the norm,\nit becomes OK. In doing so, we cede one of the main advantages of microservices: the\nability to deploy one service by itself, independently of other services.\nAll too often, the approach of accepting multiple services being deployed together drifts\ninto a situation where services become coupled. Before long, nicely separate services\nbecome increasingly tangled with others, and you never notice as you never try to deploy\nthem by themselves. You end up with a tangled mess where you have to orchestrate the\ndeployment of multiple services at once, and as we discussed previously, this sort of\ncoupling can leave us in a worse place than we would be with a single, monolithic\napplication.\nThis is bad.\nTest Journeys, Not Stories\nDespite the disadvantages just outlined, for many users end-to-end tests can still be\nmanageable with one or two services, and in these situations still make a lot of sense. But\nwhat happens with 3, 4, 10, or 20 services? Very quickly these test suites become hugely\nbloated, and in the worst case can result in Cartesian-like explosion in the scenarios under\ntest.\nThis situation worsens if we fall into the trap of adding a new end-to-end test for every\npiece of functionality we add. Show me a codebase where every new story results in a new\nend-to-end test, and I’ll show you a bloated test suite that has poor feedback cycles and\nhuge overlaps in test coverage.\nThe best way to counter this is to focus on a \nsmall\n number of core journeys to test for the\nwhole system. Any functionality not covered in these core journeys needs to be covered in\ntests that analyze services in isolation from each other. These journeys need to be mutually\nagreed upon, and jointly owned. For our music shop, we might focus on actions like\nordering a CD, returning a product, or perhaps creating a new customer — high-value\ninteractions and very few in number.\nBy focusing on a small number (and I mean \nsmall\n: very low double digits even for\ncomplex systems) of tests we can reduce the downsides of integration tests, but we cannot\navoid all of them. Is there a better way?\nConsumer-Driven Tests to the Rescue\nWhat is one of the key problems we are trying to address when we use the integration tests\noutlined previously? We are trying to ensure that when we deploy a new service to\nproduction, our changes won’t break consumers. One way we can do this without\nrequiring testing against the real consumer is by using a \nconsumer-driven contract (CDC)\n.\nWith CDCs, we are defining the expectations of a consumer on a service (or producer).\nThe expectations of the consumers are captured in code form as tests, which are then run\nagainst the producer. If done right, these CDCs should be run as part of the CI build of the\nproducer, ensuring that it never gets deployed if it breaks one of these contracts. Very\nimportantly from a test feedback point of view, these tests need to be run only against a\nsingle producer in isolation, so can be faster and more reliable than the end-to-end tests\nthey might replace.\nAs an example, let’s revisit our customer service scenario. The customer service has two\nseparate consumers: the helpdesk and web shop. Both these consuming services have\nexpectations for how the customer service will behave. In this example, you create two\nsets of tests: one for each consumer representing the helpdesk’s and web shop’s use of the\ncustomer service. A good practice here is to have someone from the producer and\nconsumer teams collaborate on creating the tests, so perhaps people from the web shop\nand helpdesk teams pair with people from the customer service team.\nBecause these CDCs are expectations on how the customer service should behave, they\ncan be run against the customer service by itself with any of its downstream dependencies\nstubbed out, as \nFigure 7-9\n shows. From a scope point of view, they sit at the same level in\nthe test pyramid as service tests, albeit with a very different focus, as shown in \nFigure 7-\n10\n. These tests are focused on how a consumer will use the service, and the trigger if they\nbreak is very different when compared with service tests. If one of these CDCs breaks\nduring a build of the customer service, it becomes obvious which consumer would be\nimpacted. At this point, you can either fix the problem or else start the discussion about\nintroducing a breaking change in the manner we discussed in \nChapter 4\n. So with CDCs,\nwe can identify a breaking change prior to our software going into production without\nhaving to use a potentially expensive end-to-end test.\nFigure 7-9. \nConsumer-driven testing in the context of our customer service example\nFigure 7-10. \nIntegrating consumer-driven tests into the test pyramid",13784
53-Pact.pdf,53-Pact,"Pact\nPact\n is a consumer-driven testing tool that was originally developed in-house at\nRealEstate.com.au, but is now open source, with Beth Skurrie driving most of the\ndevelopment. Originally just for Ruby, Pact now includes JVM and .NET ports.\nPact works in a very interesting way, as summarized in \nFigure 7-11\n. The consumer starts\nby defining the expectations of the producer using a Ruby DSL. Then, you launch a local\nmock server, and run this expectation against it to create the Pact specification file. The\nPact file is just a formal JSON specification; you could obviously handcode these, but\nusing the language API is much easier. This also gives you a running mock server that can\nbe used for further isolated tests of the consumer.\nFigure 7-11. \nAn overview of how Pact does consumer-driven testing\nOn the producer side, you then verify that this consumer specification is met by using the\nJSON Pact specification to drive calls against your API and verify responses. \nFor this to\nwork, the producer codebase needs access to the Pact file. As we discussed earlier in\nChapter 6\n, we expect both the consumer and producer to be in different builds. The use of\na language-agnostic JSON specification is an especially nice touch. It means that you can\ngenerate the consumer’s specification using a Ruby client, but use it to verify a Java\nproducer by using the JVM port of Pact.\nAs the JSON Pact specification is created by the consumer, this needs to become an\nartifact that the producer build has access to. You could store this in your CI/CD tool’s\nartifact repository, or else use the Pact Broker, which allows you to store multiple versions\nof your Pact specifications. This could let you run your consumer-driven contract tests\nagainst multiple different versions of the consumers, if you wanted to test against, say, the\nversion of the consumer in production and the version of the consumer that was most\nrecently built.\nConfusingly, there is a ThoughtWorks open source project called \nPacto\n, which is also a\nRuby tool used for consumer-driven testing. It has the ability to record interactions\nbetween client and server to generate the expectations. This makes writing consumer-\ndriven contracts for existing services fairly easy. With Pacto, once generated these\nexpectations are more or less static, whereas with Pact you regenerate the expectations in\nthe consumer with every build. The fact that you can define expectations for capabilities\nthe producer may not even have yet also better fits into a workflow where the producing\nservice is still being (or has yet to be) developed.",2642
54-Separating Deployment from Release.pdf,54-Separating Deployment from Release,"It’s About Conversations\nIn agile, stories are often referred to as a placeholder for a conversation. CDCs are just\nlike that. They become the codification of a set of discussions about what a service API\nshould look like, and when they break, they become a trigger point to have conversations\nabout how that API should evolve.\nIt is important to understand that CDCs require good communication and trust between the\nconsumer and producing service. If both parties are in the same team (or the same\nperson!), then this shouldn’t be hard. However, if you are consuming a service provided\nwith a third party, you may not have the frequency of communication, or trust, to make\nCDCs work. In these situations, you may have to make do with limited larger-scoped\nintegration tests just around the \nuntrusted\n component. Alternatively, if you are creating an\nAPI for thousands of potential consumers, such as with a publicly available web service\nAPI, you may have to play the role of the consumer yourself (or perhaps work with a\nsubset of your consumers) in defining these tests. Breaking huge numbers of external\nconsumers is a pretty bad idea, so if anything the importance of CDCs is increased!\nSo Should You Use End-to-End Tests?\nAs outlined in detail earlier in the chapter, end-to-end tests have a large number of\ndisadvantages that grow significantly as you add more moving parts under test. From\nspeaking to people who have been implementing microservices at scale for a while now, I\nhave learned that most of them over time remove the need entirely for end-to-end tests in\nfavor of tools like CDCs and improved monitoring. But they do not necessarily throw\nthose tests away. They end up using many of those end-to-end journey tests to monitor the\nproduction system using a technique called \nsemantic monitoring\n, which we will discuss\nmore in \nChapter 8\n.\nYou can view running end-to-end tests prior to production deployment as training wheels.\nWhile you are learning how CDCs work, and improving your production monitoring and\ndeployment techniques, these end-to-end tests may form a useful safety net, where you are\ntrading off cycle time for decreased risk. But as you improve those other areas, you can\nstart to reduce your reliance on end-to-end tests to the point where they are no longer\nneeded.\nSimilarly, you may work in an environment where the appetite to \nlearn in production\n is\nlow, and people would rather work as hard as they can to eliminate any defects before\nproduction, even if that means software takes longer to ship. As long as you understand\nthat you cannot be certain that you have eliminated all sources of defects, and \nthat you will\nstill need to have effective monitoring and remediation in place in production, this may be\na sensible decision.\nObviously you’ll have a better understanding of your own organization’s risk profile than\nme, but I would challenge you to think long and hard about how much end-to-end testing\nyou really need to do.\nTesting After Production\nMost testing is done before the system is in production. With our tests, we are defining a\nseries of models with which we hope to prove whether our system works and behaves as\nwe would like, both functionally and nonfunctionally. But if our models are not perfect,\nthen we will encounter problems when our systems are used in anger. Bugs slip into\nproduction, new failure modes are discovered, and our users use the system in ways we\ncould never expect.\nOne reaction to this is often to define more and more tests, and refine our models, to catch\nmore issues early and reduce the number of problems we encounter with our running\nproduction system. However, at a certain point we have to accept that we hit diminishing\nreturns with this approach. With testing prior to deployment, we cannot reduce the chance\nof failure to zero.\nSeparating Deployment from Release\nOne way in which we can catch more problems before they occur is to extend where we\nrun our tests beyond the traditional predeployment steps. Instead, if we can deploy our\nsoftware, and test it in situ prior to directing production loads against it, we can detect\nissues specific to a given environment. A common example of this is the \nsmoke test suite\n,\na collection of tests designed to be run against newly deployed software to confirm that\nthe deployment worked. These tests help you pick up any local environmental issues. If\nyou’re using a single command-line command to deploy any given microservice (and you\nshould), this command should run the smoke tests automatically.\nAnother example of this is what is called \nblue/green deployment\n. With blue/green, we\nhave two copies of our software deployed at a time, but only one version of it is receiving\nreal requests.\nLet’s consider a simple example, seen in \nFigure 7-12\n. In production, we have v123 of the\ncustomer service live. We want to deploy a new version, v456. We deploy this alongside\nv123, but do not direct any traffic to it. Instead, we perform some testing in situ against the\nnewly deployed version. Once the tests have worked, we direct the production load to the\nnew v456 version of the customer service. It is common to keep the old version around for\na short period of time, allowing for a fast fallback if you detect any errors.\nFigure 7-12. \nUsing blue/green deployments to separate deployment from release\nImplementing blue/green deployment requires a few things. First, you need to be able to\ndirect production traffic to different hosts (or collections of hosts). You could do this by\nchanging DNS entries, or updating load-balancing configuration. You also need to be able\nto provision enough hosts to have both versions of the microservice running at once. If\nyou’re using an elastic cloud provider, this could be straightforward. Using blue/green\ndeployments allows you to reduce the risk of deployment, as well as gives you the chance\nto revert should you encounter a problem. If you get good at this, the entire process can be\ncompletely automated, with either the full roll-out or revert happening without any human\nintervention.\nQuite aside from the benefit of allowing us to test our services in situ prior to sending\nthem production traffic, by keeping the old version running while we perform our release\nwe greatly reduce the downtime associated with releasing our software. Depending on\nwhat mechanism is used to implement the traffic redirection, the switchover between\nversions can be completely invisible to the customer, giving us zero-downtime\ndeployments.\nThere is another technique worth discussing briefly here too, which is sometimes confused\nwith blue/green deployments, as it can use some of the same technical implementations. It\nis known as \ncanary releasing\n.",6838
55-Performance Tests.pdf,55-Performance Tests,"Canary Releasing\nWith canary releasing, we are verifying our newly deployed software by directing\namounts of production traffic against the system to see if it performs as expected.\n“Performing as expected” can cover a number of things, both functional and\nnonfunctional. For example, we could check that a newly deployed service is responding\nto requests within 500ms, or that we see the same proportional error rates from the new\nand the old service. But you could go deeper than that. Imagine we’ve released a new\nversion of the recommendation service. We might run both of them side by side but see if\nthe recommendations generated by the new version of the service result in as many\nexpected sales, making sure that we haven’t released a suboptimal algorithm.\nIf the new release is bad, you get to revert quickly. If it is good, you can push increasing\namounts of traffic through the new version. Canary releasing differs from blue/green in\nthat you can expect versions to coexist for longer, and you’ll often vary the amounts of\ntraffic.\nNetflix uses this approach extensively. Prior to release, new service versions are deployed\nalongside a baseline cluster that represents the same version as production. Netflix then\nruns a subset of the production load over a number of hours against both the new version\nand the baseline, scoring both. If the canary passes, the company then proceeds to a full\nroll-out into production.\nWhen considering canary releasing, you need to decide if you are going to divert a portion\nof production requests to the canary or just copy production load. Some teams are able to\nshadow production traffic and direct it to their canary. In this way, the existing production\nand canary versions can see exactly the same requests, but only the results of the\nproduction requests are seen externally. This allows you to do a side-by-side comparison\nwhile eliminating the chance that a failure in the canary can be seen by a customer request.\nThe work to shadow production traffic can be complex, though, especially if the\nevents/requests being replayed aren’t idempotent.\nCanary releasing is a powerful technique, and can help you verify new versions of your\nsoftware with real traffic, while giving you tools to manage the risk of pushing out a bad\nrelease. It does require a more complex setup, however, than blue/green deployment, and a\nbit more thought. You could expect to coexist different versions of your services for longer\nthan with blue/green, so you may be tying up more hardware for longer than before. You’ll\nalso need more sophisticated traffic routing, as you may want to ramp up or down the\npercentages of the traffic to get more confidence that your release works. If you already\nhandle blue/green deployments, you may have some of the building blocks already.\nMean Time to Repair Over Mean Time Between Failures?\nSo by looking at techniques like blue/green deployment or canary releasing, we find a way\nto test closer to (or even in) production, and we also build tools to help us manage a failure\nif it occurs. Using these approaches is a tacit acknowledgment that we cannot spot and\ncatch all problems before we actually release our software.\nSometimes expending the same effort into getting better at remediation of a release can be\nsignificantly more beneficial than adding more automated functional tests. In the web\noperations world, this is often referred to as the trade-off between optimizing for \nmean\ntime between failures (MTBF)\n and \nmean time to repair (MTTR)\n.\nTechniques to reduce the time to recovery can be as simple as very fast rollbacks coupled\nwith good monitoring (which we’ll discuss in \nChapter 8\n), like blue/green deployments. If\nwe can spot a problem in production early, and roll back early, we reduce the impact to our\ncustomers. We can also use techniques like blue/green deployment, where we deploy a\nnew version of our software and test it in situ prior to directing our users to the new\nversion.\nFor different organizations, this trade-off between MTBF and MTTR will vary, and much\nof this lies with understanding the true impact of failure in a production environment.\nHowever, most organizations that I see spending time creating functional test suites often\nexpend little to no effort at all on better monitoring or recovering from failure. So while\nthey may reduce the number of defects that occur in the first place, they can’t eliminate all\nof them, and are unprepared for dealing with them if they pop up in production.\nTrade-offs other than MTBF and MTTR exist. For example, if you are trying to work out\nif anyone will actually use your software, it may make much more sense to get something\nout now, to prove the idea or the business model before building robust software. In an\nenvironment where this is the case, testing may be overkill, as the impact of not knowing\nif your idea works is much higher than having a defect in production. In these situations, it\ncan be quite sensible to avoid testing prior to production altogether.\nCross-Functional Testing\nThe bulk of this chapter has been focused on testing specific pieces of functionality, and\nhow this differs when you are testing a microservice-based system. However, there is\nanother category of testing that is important to discuss. \nNonfunctional requirements\n is an\numbrella term used to describe those characteristics your system exhibits that cannot\nsimply be implemented like a normal feature. They include aspects like the acceptable\nlatency of a web page, the number of users a system should support, how accessible your\nuser interface should be to people with disabilities, or how secure your customer data\nshould be.\nThe term \nnonfunctional\n never sat well with me. Some of the things that get covered by this\nterm seem very functional in nature! One of my colleagues, Sarah Taraporewalla, coined\nthe phrase \ncross-functional requirements (CFR)\n instead, which I greatly prefer. It speaks\nmore to the fact that these system behaviors really only emerge as the result of lots of\ncross-cutting work.\nMany, if not most, CFRs can really only be met in production. That said, we can define\ntest strategies to help us see if we are at least moving toward meeting these goals. These\nsorts of tests fall into the \nProperty Testing\n quadrant. A great example of this is the\nperformance test, which we’ll discuss in more depth shortly.\nFor some CFRs, you may want to track them at an individual service level. For example,\nyou may decide that the durability of service you require from your payment service is\nsignificantly higher, but you are happy with more downtime for your music\nrecommendation service, knowing that your core business can survive if you are unable to\nrecommend artists similar to Metallica for 10 minutes or so. These trade-offs will end up\nhaving a large impact on how you design and evolve your system, and once again the fine-\ngrained nature of a microservice-based system gives you many more chances to make\nthese trade-offs.\nTests around CFRs should follow the pyramid too. Some tests will have to be end-to-end,\nlike load tests, but others won’t. For example, once you’ve found a performance\nbottleneck in an end-to-end load test, write a smaller-scoped test to help you catch the\nproblem in the future. Other CFRs fit faster tests quite easily. I remember working on a\nproject where we had insisted on ensuring our HTML markup was using proper\naccessibility features to help people with disabilities use our website. Checking the\ngenerated markup to make sure that the appropriate controls were there could be done\nvery quickly without the need for any networking roundtrips.\nAll too often, considerations about CFRs come far too late. I strongly suggest looking at\nyour CFRs as early as possible, and reviewing them regularly.\nPerformance Tests\nPerformance tests are worth calling out explicitly as a way of ensuring that some of our\ncross-functional requirements can be met. When decomposing systems into smaller\nmicroservices, we increase the number of calls that will be made across network\nboundaries. Where previously an operation might have involved one database call, it may\nnow involve three or four calls across network boundaries to other services, with a\nmatching number of database calls. All of this can decrease the speed at which our\nsystems operate. Tracking down sources of latency is especially important. When you\nhave a call chain of multiple synchronous calls, if any part of the chain starts acting\nslowly, everything is affected, potentially leading to a significant impact. This makes\nhaving some way to performance test your applications even more important than it might\nbe with a more monolithic system. Often the reason this sort of testing gets delayed is\nbecause initially there isn’t enough of the system there to test. I understand this problem,\nbut all too often it leads to kicking the can down the road, with performance testing often\nonly being done just before you go live for the first time, if at all! Don’t fall into this trap.\nAs with functional tests, you may want a mix. You may decide that you want performance\ntests that isolate individual services, but start with tests that check core journeys in your\nsystem. You may be able to take end-to-end journey tests and simply run these at volume.\nTo generate worthwhile results, you’ll often need to run given scenarios with gradually\nincreasing numbers of simulated customers. This allows you to see how latency of calls\nvaries with increasing load. This means that performance tests can take a while to run. In\naddition, you’ll want the system to match production as closely as possible, to ensure that\nthe results you see will be indicative of the performance you can expect on the production\nsystems. This can mean that you’ll need to acquire a more production-like volume of data,\nand may need more machines to match the infrastructure — tasks that can be challenging.\nEven if you struggle to make the performance environment truly production-like, the tests\nmay still have value in tracking down bottlenecks. Just be aware that you may get false\nnegatives, or even worse, false positives.\nDue to the time it takes to run performance tests, it isn’t always feasible to run them on\nevery check-in. It is a common practice to run a subset every day, and a larger set every\nweek. Whatever approach you pick, make sure you run them as regularly as you can. The\nlonger you go without running performance tests, the harder it can be to track down the\nculprit. Performance problems are especially difficult to resolve, so if you can reduce the\nnumber of commits you need to look at in order to see a newly introduced problem, your\nlife will be much easier.\nAnd make sure you also look at the results! I’ve been very surprised by the number of\nteams I have encountered who have spent a lot of work implementing tests and running\nthem, and never check the numbers. Often this is because people don’t know what a \ngood\nresult looks like. You really need to have targets. This way, you can make the build go \nred\nor \ngreen\n based on the results, with a red (failing) build being a clear call to action.\nPerformance tesing needs to be done in concert with monitoring the real system\nperformance (which we’ll discuss more in \nChapter 8\n), and ideally should use the same\ntools in your performance test environment for visualizing system behavior as those you\nuse in production. This can make it much easier to compare like with like.",11658
56-Correlation IDs.pdf,56-Correlation IDs,"Summary\nBringing this all together, what I have outlined here is a holistic approach to testing that\nhopefully gives you some general guidance on how to proceed when testing your own\nsystems. To reiterate the basics:\nOptimize for fast feedback, and separate types of tests accordingly.\nAvoid the need for end-to-end tests wherever possible by using consumer-driven\ncontracts.\nUse consumer-driven contracts to provide focus points for conversations between\nteams.\nTry to understand the trade-off between putting more efforts into testing and\ndetecting issues faster in production (optimizing for MTBF versus MTTR).\nIf you are interested in reading more about testing, I recommend \nAgile Testing\n by Lisa\nCrispin and Janet Gregory (Addison-Wesley), which among other things covers the use of\nthe testing quadrant in more detail.\nThis chapter focused mostly on making sure our code works before it hits production, but\nwe also need to know how to make sure our code works once it’s deployed. In the next\nchapter, we’ll take a look at how to monitor our microservice-based \nsystems\n.\n2\n Diane Vaughan, \nThe Challenger Launch Decision: Risky Technology, Culture, and\nDeviance at NASA\n (Chicago: University of Chicago Press, 1996).\nChapter 8. \nMonitoring\nAs I’ve hopefully shown so far, breaking our system up into smaller, fine-grained\nmicroservices results in multiple benefits. It also, however, adds complexity when it\ncomes to monitoring the system in production. In this chapter, we’ll look at the challenges\nassociated with monitoring and identifying problems in our fine-grained systems, and I’ll\noutline some of the things you can do to have your cake and eat it too!\nPicture the scene. It’s a quiet Friday afternoon, and the team is looking forward to sloping\noff early to the pub as a way to start a weekend away from work. Then suddenly the\nemails arrive. The website is misbehaving! Twitter is ablaze with your company’s failings,\nyour boss is chewing your ear off, and the prospects of a quiet weekend vanish.\nWhat’s the first thing you need to know? What the hell has gone wrong?\nIn the world of the monolithic application, we at least have a very obvious place to start\nour investigations. Website slow? It’s the monolith. Website giving odd errors? It’s the\nmonolith. CPU at 100%? Monolith. Smell of burning? Well, you get the idea. Having a\nsingle point of failure also makes failure investigation somewhat simpler!\nNow let’s think about our own, microservice-based system. The capabilities we offer our\nusers are served from multiple small services, some of which communicate with yet more\nservices to accomplish their tasks. There are lots of advantages to such an approach\n(which is good, as otherwise this book would be a waste of time), but in the world of\nmonitoring, we have a more complex problem on our hands.\nWe now have multiple servers to monitor, multiple logfiles to sift through, and multiple\nplaces where network latency could cause problems. So how do we approach this? We\nneed to make sense of what otherwise might be a chaotic, tangled mess — the last thing\nany of us wants to deal with on a Friday afternoon (or at any time, come to that!).\nThe answer here is pretty straightforward: monitor the small things, and use aggregation to\nsee the bigger picture. To see how, we’ll start with the simplest system we can: a single\nnode.\nSingle Service, Single Server\nFigure 8-1\n presents a very simple setup: one host, running one service. Now we need to\nmonitor it in order to know when something goes wrong, so we can fix it. So what should\nwe look for?\nFigure 8-1. \nA single service on a single host\nFirst, we’ll want to monitor the host itself. CPU, memory — all of these things are useful.\nWe’ll want to know what they should be when things are healthy, so we can alert when\nthey go out of bounds. If we want to run our own monitoring software, we could use\nsomething like Nagios to do so, or else use a hosted service like New Relic.\nNext, we’ll want to have access to the logs from the server itself. If a user reports an error,\nthese logs should pick it up and hopefully tell us when and where the error is. At this\npoint, with our single host we can probably get by with just logging on to the host and\nusing command-line tools to scan the log. We may even get advanced and use \nlogrotate\nto move old logs out of the way and avoid them taking up all our disk space.\nFinally, we might want to monitor the application itself. At a bare minimum, monitoring\nthe response time of the service is a good idea. You’ll probably be able to do this by\nlooking at the logs coming either from a web server fronting your service, or perhaps from\nthe service itself. If we get very advanced, we might want to track the number of errors we\nare reporting.\nTime passes, loads increase, and we find ourselves needing to scale…\nSingle Service, Multiple Servers\nNow we have multiple copies of the service running on separate hosts, as shown in\nFigure 8-2\n, with requests to the different service instances distributed via a load balancer.\nThings start to get a bit trickier now. We still want to monitor all the same things as before,\nbut need to do so in such a way that we can isolate the problem. When the CPU is high, is\nit a problem we are seeing on all hosts, which would point to an issue with the service\nitself, or is it isolated to a single host, implying that the host itself has the problem —\nperhaps a rogue OS process?\nFigure 8-2. \nA single service distributed across multiple hosts\nSo at this point, we still want to track the host-level metrics, and alert on them. But now\nwe want to see what they are across all hosts, as well as individual hosts. In other words,\nwe want to aggregate them up, and still be able to drill down. Nagios lets us group our\nhosts like this — so far, so good. A similar approach will probably suffice for our\napplication.\nThen we have our logs. With our service running on more than one server, we’ll probably\nget tired of logging into each box to look at it. With just a few hosts, though, we can use\ntools like ssh-multiplexers, which allow us to run the same commands on multiple hosts.\nA big monitor and a \ngrep ""Error"" app.log\n later, and we can find our culprit.\nFor tasks like response time tracking, we can get some of the aggregation for free by\ntracking at the load balancer itself. But we need to track the load balancer as well, of\ncourse; if that misbehaves, we have a problem. At this point, we also probably care a lot\nmore about what a healthy service looks like, as we’ll configure our load balancer to\nremove unhealthy nodes from our application. Hopefully by the time we get here we have\nat least some idea of that…\nMultiple Services, Multiple Servers\nIn \nFigure 8-3\n, things get much more interesting. Multiple services are collaborating to\nprovide capabilities to our users, and those services are running on multiple hosts — be\nthey physical or virtual. How do you find the error you’re looking for in thousands of lines\nof logs on multiple hosts? How do you determine if one server is misbehaving, or if it is a\nsystematic issue? And how do you track back an error found deep down in a call chain\nbetween multiple hosts and work out what caused it?\nFigure 8-3. \nMultiple collaborating services distributed across multiple hosts\nThe answer is collection and central aggregation of as much as we can get our hands on,\nfrom logs to application metrics.\nLogs, Logs, and Yet More Logs…\nNow the number of hosts we are running on is becoming a challenge. SSH-multiplexing to\nretrieve logs probably isn’t going to cut it now, and there isn’t a screen big enough for you\nto have terminals open on every host. Instead, we’re looking to use specialized subsystems\nto grab our logs and make them available centrally. One example of this is \nlogstash\n, which\ncan parse multiple logfile formats and can send them to downstream systems for further\ninvestigation.\nKibana\n is an ElasticSearch-backed system for viewing logs, illustrated in \nFigure 8-4\n. You\ncan use a query syntax to search through logs, allowing you to do things like restrict time\nand date ranges or use regular expressions to find matching strings. Kibana can even\ngenerate graphs from the logs you send it, allowing you to see at a glance how many\nerrors have been generated over time, for example.\nFigure 8-4. \nUsing Kibana to view aggregated logs\nMetric Tracking Across Multiple Services\nAs with the challenge of looking at logs for different hosts, we need to look at better ways\nto gather and view our metrics. It can be hard to know what \ngood\n looks like when we’re\nlooking at metrics for a more complex system. Our website is seeing nearly 50 4XX HTTP\nerror codes per second. Is that bad? The CPU load on the catalog service has increased by\n20% since lunch; has something gone wrong? The secret to knowing when to panic and\nwhen to relax is to gather metrics about how your system behaves over a long-enough\nperiod of time that clear patterns emerge.\nIn a more complex environment, we’ll be provisioning new instances of our services\npretty frequently, so we want the system we pick to make it very easy to collect metrics\nfrom new hosts. We’ll want to be able to look at a metric aggregated for the whole system\n— for example, the avergage CPU load — but we’ll also want to aggregate that metric for\nall the instances of a given service, or even for a single instance of that service. That\nmeans we’ll need to be able to associate metadata with the metric to allow us to infer this\nstructure.\nGraphite is one such system that makes this very easy. It exposes a very simple API and\nallows you to send metrics in real time. It then allows you to query those metrics to\nproduce charts and other displays to see what is happening. The way it handles volume is\nalso interesting. Effectively, you configure it so that you reduce the resolution of older\nmetrics to ensure the volumes don’t get too large. So, for example, I might record the CPU\nfor my hosts once every 10 seconds for the last 10 minutes, then an aggregated sample\nevery minute for the last day, down to perhaps one sample every 30 minutes for the last\nseveral years. In this way, you can store information about how your system has behaved\nover a long period of time without needing huge amounts of storage.\nGraphite also enables you to aggregate across samples, or drill down to a single series, so\nyou can see the response time for your whole system, a group of services, or a single\ninstance. If Graphite doesn’t work for you for whatever reason, make sure you get similar\ncapabilities in any other tool you select. And certainly make sure you can get access to the\nraw data to provide your own reporting or dashboards if you need to.\nAnother key benefit of understanding your trends is when it comes to capacity planning.\nAre we reaching our limit? How long until we need more hosts? In the past when we\nbrought physical hosts, this was often an annual job. In the new age of on-demand\ncomputing provided by infrastructure as a service (IaaS) vendors, we can now scale up or\ndown in minutes, if not seconds. This means that if we understand our usage patterns, we\ncan make sure we have just enough infrastructure to serve our needs. The smarter we are\nin tracking our trends and knowing what to do with them, the more cost effective and\nresponsive our systems can be.\nService Metrics\nThe operating systems we run on generate a large number of metrics for us, as you’ll find\nthe moment you install collectd on a Linux box and point it at Graphite. Likewise,\nsupporting subsystems like Nginx or Varnish exposes useful information like response\ntimes or cache hit rates. But what about your own service?\nI would strongly suggest having your services expose basic metrics themselves. At a bare\nminimum, for a web service you should probably expose metrics like response times and\nerror rates — vital if your server isn’t fronted by a web server that is doing this for you.\nBut you should really go further. For example, our accounts service may want to expose\nthe number of times customers view their past orders, or your web shop might want to\ncapture how much money has been made during the last day.\nWhy do we care about this? Well, for a number of reasons. First, there is an old adage that\n80% of software features are never used. Now I can’t comment on how accurate that\nfigure is, but as someone who has been developing software for nearly 20 years, I \nknow\nthat I have spent a lot of time on features that never actually get used. Wouldn’t it be nice\nto know what they are?\nSecond, we are getting better than ever at reacting to how our users are using our system\nto work out how to improve it. Metrics that inform us of how our systems behave can only\nhelp us here. We push out a new version of the website, and find that the number of\nsearches by genre has gone up significantly on the catalog service. Is that a problem, or\nexpected?\nFinally, we can never know what data will be useful! More times than I can count I’ve\nwanted to capture data to help me understand something only after the chance to do so has\nlong passed. I tend to err toward exposing everything and relying on my metrics system to\nhandle this later.\nLibraries exist for a number of different platforms that allow our services to send metrics\nto standard systems. Codahale’s \nMetrics library\n is one such example library for the JVM.\nIt allows you to store metrics as counters, timers, or gauges; supports time-boxing metrics\n(so you can specify metrics like “number of orders in the last five minutes”); and also\ncomes out of the box with support for sending data to Graphite and other aggregating and\nreporting systems.\nSynthetic Monitoring\nWe can try to work out if a service is \nhealthy\n by, for example, deciding what a good CPU\nlevel is, or what makes for an acceptable response time. If our monitoring system detects\nthat the actual values fall outside this safe level, we can trigger an alert — something that\na tool like Nagios is more than capable of.\nHowever, in many ways, these values are one step removed from what we actually want to\ntrack — namely, \nis the system working?\n The more complex the interactions between the\nservices, the further removed we are from actually answering that question. So what if our\nmonitoring systems were programmed to act a bit like our users, and could report back if\nsomething goes wrong?\nI first did this back in 2005. I was part of a small ThoughtWorks team that was building a\nsystem for an investment bank. Throughout the trading day, lots of events came in\nrepresenting changes in the market. Our job was to react to these changes, and look at the\nimpact on the bank’s portfolio. We were working under some fairly tight deadlines, as the\ngoal was to have done all our calculations in less than 10 seconds after the event arrived.\nThe system itself consisted of around five discrete services, at least one of which was\nrunning on a computing grid that, among other things, was scavenging unused CPU cycles\non around 250 desktop hosts in the bank’s disaster recovery center.\nThe number of moving parts in the system meant a lot of noise was being generated from\nmany of the lower-level metrics we were gathering. We didn’t have the benefit of scaling\ngradually or having the system run for a few months to understand what \ngood\n looked like\nfor metrics like our CPU rate or even the latencies of some of the individual components.\nOur approach was to generate fake events to price part of the portfolio that was not\nbooked into the downstream systems. Every minute or so, we had Nagios run a command-\nline job that inserted a fake event into one of our queues. Our system picked it up and ran\nall the various calculations just like any other job, except the results appeared in the \njunk\nbook, which was used only for testing. If a re-pricing wasn’t seen within a given time,\nNagios reported this as an issue.\nThis fake event we created is an example of \nsynthetic transaction\n. We used this synthetic\ntransaction to ensure the system was behaving semantically, which is why this technique is\noften called \nsemantic monitoring\n.\nIn practice, I’ve found the use of synthetic transactions to perform semantic monitoring\nlike this to be a far better indicator of issues in systems than alerting on the lower-level\nmetrics. They don’t replace the need for the lower-level metrics, though — we’ll still want\nthat detail when we need to find out \nwhy\n our semantic monitoring is reporting a problem.\nImplementing Semantic Monitoring\nNow in the past, implementing semantic monitoring was a fairly daunting task. But the\nworld has moved on, and the means to do this is at our fingertips! You are running tests\nfor your systems, right? If not, go read \nChapter 7\n and come back. All done? Good!\nIf we look at the tests we have that test a given service end to end, or even our whole\nsystem end to end, we have much of what we need to implement semantic monitoring.\nOur system already exposes the hooks needed to launch the test and check the result. So\nwhy not just run a subset of these tests, on an ongoing basis, as a way of monitoring our\nsystem?\nThere are some things we need to do, of course. First, we need to be careful about the data\nrequirements of our tests. We may need to find a way for our tests to adapt to different live\ndata if this changes over time, or else set a different source of data. For example, we could\nhave a set of fake users we use in production with a known set of data.\nLikewise, we have to make sure we don’t accidentally trigger unforeseen side effects. A\nfriend told me a story about an ecommerce company that accidentally ran its tests against\nits production ordering systems. It didn’t realize its mistake until a large number of\nwashing machines arrived at the head office.\nCorrelation IDs\nWith a large number of services interacting to provide any given end-user capability, a\nsingle initiating call can end up generating multiple more downstream service calls. For\nexample, consider the example of a customer being registered. The customer fills in all her\ndetails in a form and clicks submit. Behind the scenes, we check validity of the credit card\ndetails with our payment service, talk to our postal service to send out a welcome pack in\nthe post, and send a welcome email using our email service. Now what happens if the call\nto the payment service ends up generating an odd error? We’ll talk at length about\nhandling the failure in \nChapter 11\n, but consider the difficulty of diagnosing what\nhappened.\nIf we look at the logs, the only service registering an error is our payment service. If we\nare lucky, we can work out what request caused the problem, and we may even be able to\nlook at the parameters of the call. \nNow consider that this is a simple example, and that one\ninitiating request could generate a chain of downstream calls and maybe events being fired\noff that are handled in an asynchronous manner. How can we reconstruct the flow of calls\nin order to reproduce and fix the problem? Often what we need is to see that error in the\nwider context of the initiating call; in other words, we’d like to trace the call chain\nupstream, just like we do with a stack trace.\nOne approach that can be useful here is to use correlation IDs. When the first call is made,\nyou generate a GUID for the call. This is then passed along to all subsequent calls, as seen\nin \nFigure 8-5\n, and can be put into your logs in a structured way, much as you’ll already do\nwith components like the log level or date. With the right log aggregation tooling, you’ll\nthen be able to trace that event all the way through your system:\n15-02-2014 16:01:01 Web-Frontend INFO [abc-123] Register\n15-02-2014 16:01:02 RegisterService INFO [abc-123] RegisterCustomer…\n15-02-2014 16:01:03 PostalSystem INFO [abc-123] SendWelcomePack…\n15-02-2014 16:01:03 EmailSystem INFO [abc-123] SendWelcomeEmail…\n15-02-2014 16:01:03 PaymentGateway ERROR [abc-123] ValidatePayment…\nFigure 8-5. \nUsing correlation IDs to track call chains across multiple services\nYou will, of course, need to ensure that each service knows to pass on the correlation ID.\nThis is where you need to standardize and be stronger in enforcing this across your\nsystem. But once you have done this, you can actually create tooling to track all sorts of\ninteractions. Such tooling can be useful in tracking down event storms, odd corner cases,\nor even identifying especially costly transactions, as you can picture the whole cascade of\ncalls.\nSoftware such as \nZipkin\n can also trace calls across multiple system boundaries. Based on\nthe ideas from Google’s own tracing system, Dapper, Zipkin can provide very detailed\ntracing of interservice calls, along with a UI to help present the data. Personally, I’ve\nfound the requirements of Zipkin to be somewhat heavyweight, requiring custom clients\nand supporting collection systems. Given that you’ll already want log aggregation for\nother purposes, it feels much simpler to instead make use of data you’re already collecting\nthan have to plumb in additional sources of data. That said, if you find that you need a\nmore advanced tool to track interservice calls like this, you might want to give them a\nlook.\nOne of the real problems with correlation IDs is that you often don’t know you need one\nuntil \nafter\n you already have a problem that could be diagnosed only if you had the ID at\nthe beginning! This is especially problematic, as retrofitting correlation IDs in is very\ndifficult; you need to handle them in a standardized way to be able to easily reconsititute\ncall chains. Although it might seem like additional work up front, I would strongly suggest\nyou consider putting them in as soon as you can, especially if your system will make use\nof event-drive architecture patterns, which can lead to some odd emergent behavior.\nNeeding to handle tasks like consistently passing through correlation IDs can be a strong\nargument for the use of thin shared client wrapper libraries. At a certain scale, it becomes\ndifficult to ensure that everyone is calling downstream services in the right way and\ncollecting the right sort of data. It only takes one service partway through the chain to\nforget to do this for you to lose critical information. If you do decide to create an in-house\nclient library to make things like this work out of the box, do make sure you keep it very\nthin and not tied to any particular producing service. For example, if you are using HTTP\nas the underlying protocol for communication, just wrap a standard HTTP client library,\nadding in code to make sure you propogate the correlation IDs in the headers.",23032
57-Summary.pdf,57-Summary,"The Cascade\nCascading failures can be especially perilous. Imagine a situation where the network\nconnection between our music shop website and the catalog service goes down. The\nservices themselves appear healthy, but they can’t talk to each other. If we just looked at\nthe health of the individual service, we wouldn’t know there is a problem. Using synthetic\nmonitoring — for example, to mimic a customer searching for a song — would pick up\nthe problem. But we’d also need to report on the fact that one service cannot see another\nin order to determine the cause of the problem.\nTherefore, monitoring the integration points between systems is key. Each service instance\nshould track and expose the health of its downstream dependencies, from the database to\nother collaborating services. You should also allow this information to be aggregated to\ngive you a rolled-up picture. You’ll want to see the response time of the downstream calls,\nand also detect if it is erroring.\nAs we’ll discuss more in \nChapter 11\n, you can use libraries to implement a circuit breaker\naround network calls to help you handle cascading failures in a more elegant fashion,\nallowing you to more gracefully degrade your system. Some of these libraries, such as\nHystrix for the JVM, also do a good job of providing these monitoring capabilities for\nyou.\nStandardization\nAs we’ve covered previously, one of the ongoing balancing acts you’ll need to pull off is\nwhere to allow for decisions to be made narrowly for a single service versus where you\nneed to standardize across your system. In my opinion, monitoring is one area where\nstandardization is incredibly important. With services collaborating in lots of different\nways to provide capabilities to users using multiple interfaces, you need to view the\nsystem in a holistic way.\nYou should try to write your logs out in a standard format. You definitely want to have all\nyour metrics in one place, and you may want to have a list of standard names for your\nmetrics too; it would be very annoying for one service to have a metric called\nResponseTime\n, and another to have one called \nRspTimeSecs\n, when they mean the same\nthing.\nAs always with standardization, tools can help. As I’ve said before, the key is making it\neasy to do the right thing — so why not provide preconfigured virtual machine images\nwith logstash and collectd ready to go, along with application libraries that let you talk to\nGraphite really easily?\nConsider the Audience\nAll this data we are gathering is for a purpose. More specifically, we are gathering all this\ndata for different people to help them do their jobs; this data becomes a call to action.\nSome of this data needs to trigger an immediate call to action for our support team — for\nexample, in the case of one of our synthetic monitoring tests failing. Other data, like the\nfact that our CPU load has increased by 2% over the last week, is potentially only of\ninterest when we’re doing capacity planning. Likewise, your boss is probably going to\nwant to know right away that revenue dipped 25% after the last release, but probably\ndoesn’t need to be woken up because searches for “Justin Bieber” have gone up 5% in the\nlast hour.\nWhat our people want to see and react to \nright now\n is different than what they need when\ndrilling down. So, for the type of person who will be looking at this data, consider the\nfollowing:\nWhat they need to know right now\nWhat they might want later\nHow they like to consume data\nAlert on the things they need to know right now. Create big visible displays with this\ninformation that sit in the corner of the room. Give them easy access to the data they need\nto know later. And spend time with them to know how they want to consume data. A\ndiscussion about all the nuances involved in the graphical display of quantitative\ninformation is certainly outside the scope of this book, but a great place to start is Stephen\nFew’s excellent book \nInformation Dashboard Design: Displaying Data for At-a-Glance\nMonitoring\n (Analytics Press).\nThe Future\nI have seen many organizations where metrics are siloed into different systems.\nApplication-level metrics, like the number of orders placed, end up in a proprietary\nanalytics system like Omniture, which is often available only to select parts of \nthe\nbusiness\n, or else ends up in the dreaded data warehouse, aka where data goes to die.\nReporting from such systems is often not available in real time, although that is starting to\nchange. Meanwhile, \nsystem\n metrics like response times, error rates, and CPU load are\nstored in systems that the operations teams can access. These systems typically allow for\nreal-time reporting, as normally the point of them is to provoke an immediate call to\naction.\nHistorically, the idea that we can find out about key business metrics a day or two later\nwas fine, as typically we were unable to react fast enough to this data to do anything about\nit anyway. Now, though, we operate in a world in which many of us can and do push out\nmultiple releases per day. Teams now measure themselves not in terms of how many\npoints\n they complete, but instead optimize for how long it takes for code to get from\nlaptop to live. In such an environment, we need all our metrics at our fingertips to take the\nright action. Ironically, the very systems that store business metrics are often not tuned for\nimmediate access to data, but our operational systems are.\nSo why handle operational and business metrics in the same way? Ultimately, both types\nof things break down to events that say \nsomething happened at X\n. So, if we can unify the\nsystems we use to gather, aggregate, and store these events, and make them available for\nreporting, we end up with a much simpler architecture.\nRiemann\n is an event server that allows for fairly advanced aggregation and routing of\nevents and can form part of such a solution. \nSuro\n is Netflix’s \ndata pipeline\n and operates in\na similar space. Suro is explicitly used to handle both metrics associated with user\nbehavior, and more operational data like application logs. This data can then be dispatched\nto a variety of systems, like Storm for real-time analysis, Hadoop for offline batch\nprocessing, or Kibana for log analysis.\nMany organizations are moving in a fundamentally different direction: away from having\nspecialized tool chains for different types of metrics and toward more generic event\nrouting systems capable of significant scale. These systems manage to provide much more\nflexibility, while at the same time actually simplifying our architecture.\nSummary\nSo, we’ve covered a lot here! I’ll attempt to summarize this chapter into some easy-to-\nfollow advice.\nFor each service:\nTrack inbound response time at a bare minimum. Once you’ve done that, follow with\nerror rates and then start working on application-level metrics.\nTrack the health of all downstream responses, at a bare minimum including the\nresponse time of downstream calls, and at best tracking error rates. Libraries like\nHystrix can help here.\nStandardize on how and where metrics are collected.\nLog into a standard location, in a standard format if possible. Aggregation is a pain if\nevery service uses a different layout!\nMonitor the underlying operating system so you can track down rogue processes and\ndo capacity planning.\nFor the system:\nAggregate host-level metrics like CPU together with application-level metrics.\nEnsure your metric storage tool allows for aggregation at a system or service level,\nand drill down to individual hosts.\nEnsure your metric storage tool allows you to maintain data long enough to\nunderstand trends in your system.\nHave a single, queryable tool for aggregating and storing logs.\nStrongly consider standardizing on the use of correlation IDs.\nUnderstand what requires a call to action, and structure alerting and dashboards\naccordingly.\nInvestigate the possibility of unifying how you aggregate all of your various metrics\nby seeing if a tool like Suro or Riemann makes sense for you.\nI’ve also attempted to outline the direction in which monitoring is moving: away from\nsystems specialized to do just one thing, and toward generic event processing systems that\nallow you to look at your system in a more holistic way. This is an exciting and emerging\nspace, and while a full investigation is outside the scope of this book, hopefully I’ve given\nyou enough to get started with. If you want to know more, I go into some of these ideas\nand more in my earlier publication \nLightweight Systems for Realtime Monitoring\n(O’Reilly).\nIn the next chapter, we’ll take a different holistic view of our systems to consider some of\nthe unique advantages — and challenges — that fine-grained architectures can provide in\nthe area of security.",8901
58-Single Sign-On Gateway.pdf,58-Single Sign-On Gateway,"Chapter 9. \nSecurity\nWe’ve become familiar with stories about security breaches of large-scale systems\nresulting in our data being exposed to all sorts of dodgy characters. But more recently,\nevents like the Edward Snowden revelations have made us even more aware of the value\nof data that companies hold about us, and the value of data that we hold for our customers\nin the systems we build. This chapter will give a brief overview of some aspects of\nsecurity you should consider when designing your systems. While not meant to be\nexhaustive, it will lay out some of the main options available to you and give you a\nstarting point for your own further research.\nWe need to think about what protection our data needs while in transit from one point to\nanother, and what protection it needs at rest. We need to think about the security of our\nunderlying operating systems, and our networks too. There is so much to think about, and\nso much we could do! So how much security do we need? How can we work out what is\nenough\n security?\nBut we also need to think of the human element. How do we know who a person is, and\nwhat he can do? And how does this relate to how our servers talk to each other? Let’s start\nthere.\nAuthentication and Authorization\nAuthentication and authorization are core concepts when it comes to people and things\nthat interact with our system. In the context of security, \nauthentication\n is the process by\nwhich we confirm that a party is who she says she is. For a human, you typically\nauthenticate a user by having her type in her username and password. We assume that only\nshe has access to this information, and therefore that the person entering this information\nmust be her. Other, more complex systems exist as well, of course. My phone now lets me\nuse my fingerprint to confirm that I am who I say I am. Generally, when we’re talking\nabstractly about who or what is being authenticated, we refer to that party as the \nprincipal\n.\nAuthorization\n is the mechanism by which we map from a principal to the action we are\nallowing her to do. Often, when a principal is authenticated, we will be given information\nabout her that will help us decide what we should let her do. We might, for example, be\ntold what department or office she works in — pieces of information that our systems can\nuse to decide what she can and cannot do.\nFor single, monolithic applications, it is common for the application itself to handle\nauthentication and authorization for you. Django, the Python web framework, comes out\nof the box with user management, for example. When it comes to distributed systems,\nthough, we need to think of more advanced schemes. We don’t want everyone to have to\nlog in separately for different systems, using a different username and password for each.\nThe aim is to have a single identity that we can authenticate once.\nCommon Single Sign-On Implementations\nA common approach to authentication and authorization is to use some sort of \nsingle sign-\non (SSO)\n solution. SAML, which is the reigning implementation in the enterprise space,\nand OpenID Connect both provide capabilities in this area. More or less they use the same\ncore concepts, although the terminology differs slightly. The terms used here are from\nSAML.\nWhen a principal tries to access a resource (like a web-based interface), she is directed to\nauthenticate with an \nidentity provider\n. This may ask her to provide a username and\npassword, or might use something more advanced like two-factor authentication. Once the\nidentity provider is satisfied that the principal has been authenticated, it gives information\nto the \nservice provider\n, allowing it to decide whether to grant her access to the resource.\nThis identity provider could be an externally hosted system, or something inside your own\norganization. Google, for example, provides an OpenID Connect identity provider. For\nenterprises, though, it is common to have your own identity provider, which may be linked\nto your company’s \ndirectory service\n. A directory service could be something like the\nLightweight Directory Access Protocol (LDAP) or Active Directory. These systems allow\nyou to store information about principals, such as what roles they play in the organization.\nOften, the directory service and the identity provider are one and the same, while\nsometimes they are separate but linked. Okta, for example, is a hosted SAML identity\nprovider that handles tasks like two-factor authentication, but can link to your company’s\ndirectory services as the source of truth.\nSAML is a SOAP-based standard, and is known for being fairly complex to work with\ndespite the libraries and tooling available to support it. OpenID Connect is a standard that\nhas emerged as a specific implementation of OAuth 2.0, based on the way Google and\nothers handle SSO. It uses simpler REST calls, and in my opinion is likely to make\ninroads into enterprises due to its improved ease of use. Its biggest stumbling block right\nnow is the lack of identity providers that support it. For a public-facing website, you might\nbe OK using Google as your provider, but for internal systems or systems where you want\nmore control over and visibility into how and where your data is installed, you’ll want\nyour own in-house identity provider. At the time of writing, OpenAM and Gluu are two of\nthe very few options available in this space, compared to a wealth of options for SAML\n(including Active Directory, which seems to be everywhere). Until and unless existing\nidentity providers start supporting OpenID Connect, its growth may be limited to those\nsituations where people are happy using a public identity provider.\nSo while I think OpenID Connect is the future, it’s quite possible it’ll take a while to reach\nwidespread adoption.\nSingle Sign-On Gateway\nWithin a microservice setup, each service could decide to handle the redirection to, and\nhandshaking with, the identity provider. Obviously, this could mean a lot of duplicated\nwork. A shared library could help, but we’d have to be careful to avoid the coupling that\ncan come from shared code. This also wouldn’t help if you had multiple different\ntechnology stacks.\nRather than having each service manage handshaking with your identity provider, you can\nuse a gateway to act as a proxy, sitting between your services and the outside world (as\nshown in \nFigure 9-1\n). The idea is that we can centralize the behavior for redirecting the\nuser and perform the handshake in only one place.\nFigure 9-1. \nUsing a gateway to handle SSO\nHowever, we still need to solve the problem of how the downstream service receives\ninformation about principals, such as their username or what roles they play. If you’re\nusing HTTP, it could populate headers with this information. Shibboleth is one tool that\ncan do this for you, and I’ve seen it used with Apache to great effect to handle integration\nwith SAML-based identity providers.\nAnother problem is that if we have decided to offload responsibility for authentication to a\ngateway, it can be harder to reason about how a microservice behaves when looking at it\nin isolation. Remember in \nChapter 7\n where we explored some of the challenges in\nreproducing production-like environments? If you go the gateway route, make sure your\ndevelopers can launch their services behind one without too much work.\nOne final problem with this approach is that it can lull you into a false sense of security. I\nlike the idea of defense in depth — from network perimeter, to subnet, to firewall, to\nmachine, to operating system, to the underlying hardware. You have the ability to\nimplement security measures at all of these points, some of which we’ll get into shortly. I\nhave seen some people put all their eggs in one basket, relying on the gateway to handle\nevery step for them. And we all know what happens when we have a single point of\nfailure…\nObviously you could use this gateway to do other things. If using a layer of Apache\ninstances running Shibboleth, for example, you could also decide to terminate HTTPS at\nthis level, run intrusion detection, and so on. Do be careful, though. Gateway layers tend\nto take on more and more functionality, which itself can end up being a giant coupling\npoint. And the more functionality something has, the greater the attack surface.",8429
59-API Keys.pdf,59-API Keys,"Fine-Grained Authorization\nA gateway may be able to provide fairly effective coarse-grained authentication. For\nexample, it could prevent access to any non-logged-in user to the helpdesk application.\nAssuming our gateway can extract attributes about the principal as a result of the\nauthentication, it may be able to make more nuanced decisions. For example, it is common\nto place people in groups, or assign them to roles. We can use this information to\nunderstand what they can do. So for the helpdesk application, we might allow access only\nto principals with a specific role (e.g., STAFF). Beyond allowing (or disallowing) access\nto specific resources or endpoints, though, we need to leave the rest to the microservice\nitself; it will need to make further decisions about what operations to allow.\nBack to our helpdesk application: do we allow any staff members to see any and all\ndetails? More likely, we’ll have different roles at work. For example, a principal in the\nCALL_CENTER group might be allowed to view any piece of information about a\ncustomer except his payment details. The principal might also be able to issue refunds, but\nthat amount might be capped. Someone who has the CALL_CENTER_TEAM_LEADER\nrole, however, might be able to issue larger refunds.\nThese decisions need to be local to the microservice in question. I have seen people use\nthe various attributes supplied by identity providers in horrible ways, using really fine-\ngrained roles like CALL_CENTER_50_DOLLAR_REFUND, where they end up putting\ninformation specific to one part of one of our system’s behavior into their directory\nservices. This is a nightmare to maintain and gives very little scope for our services to\nhave their own independent lifecycle, as suddenly a chunk of information about how a\nservice behaves lives elsewhere, perhaps in a system managed by a different part of the\norganization.\nInstead, favor coarse-grained roles, modeled around how your organization works. Going\nall the way back to the early chapters, remember that we are building software to match\nhow our organization works. So use your roles in this way too.\nService-to-Service Authentication and Authorization\nUp to this point we’ve been using the term \nprincipal\n to describe anything that can\nauthenticate and be authorized to do things, but our examples have actually been about\nhumans using computers. But what about programs, or other services, authenticating with\neach other?\nAllow Everything Inside the Perimeter\nOur first option could be to just assume that any calls to a service made from inside our\nperimeter are implicitly trusted.\nDepending on the sensitivity of the data, this might be fine. Some organizations attempt to\nensure security at the perimeter of their networks, and therefore assume they don’t need to\ndo anything else when two services are talking together. However, should an attacker\npenetrate your network, you will have little protection against a typical \nman-in-the-middle\nattack. If the attacker decides to intercept and read the data being sent, change the data\nwithout you knowing, or even in some circumstances pretend to be the thing you are\ntalking to, you may not know much about it.\nThis is by far the most common form of inside-perimeter trust I see in organizations. They\nmay decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying\nthat is a good thing! For most of the organizations I see using this model, I worry that the\nimplicit trust model is not a conscious decision, but more that people are unaware of the\nrisks in the first place.\nHTTP(S) Basic Authentication\nHTTP Basic Authentication allows for a client to send a username and password in a\nstandard HTTP header. The server can then check these details and confirm that the client\nis allowed to access the service. The advantage here is that this is an extremely well-\nunderstood and well-supported protocol. The problem is that doing this over HTTP is\nhighly problematic, as the username and password are not sent in a secure manner. Any\nintermediate party can look at the information in the header and see the data. Thus, HTTP\nBasic Authentication should normally be used over HTTPS.\nWhen using HTTPS, the client gains strong guarantees that the server it is talking to is\nwho the client thinks it is. It also gives us additional protection against people\neavesdropping on the traffic between the client and server or messing with the payload.\nThe server needs to manage its own SSL certificates, which can become problematic when\nit is managing multiple machines. Some organizations take on their own certificate issuing\nprocess, which is an additional administrative and operational burden. Tools around\nmanaging this in an automated fashion are nowhere near as mature as they could be, and it\nisn’t just the issuing process you have to handle. Self-signed certificates are not easily\nrevokable, and thus require a lot more thought around disaster scenarios. See if you can\ndodge all this work by avoiding self-signing altogether.\nAnother downside is that traffic sent via SSL cannot be cached by reverse proxies like\nVarnish or Squid. This means that if you need to cache traffic, it will have to be done\neither inside the server or inside the client. You can fix this by having a load balancer\nterminate the SSL traffic, and having the cache sit behind the load balancer.\nWe also have to think about what happens if we are using an existing SSO solution, like\nSAML, that already has access to usernames and passwords. Do we want our basic service\nauth to use the same set of credentials, allowing us one process for issuing and revoking\nthem? We could do this by having the service talk to the same directory service that backs\nour SSO solution. Alternatively, we could store the usernames and passwords ourselves\ninside the service, but then we run the risk of duplicating behavior.\nOne note: in this approach, all the server knows is that the client has the username and\npassword. We have no idea if this information is coming from a machine we expect; it\ncould be coming from anyone on our network.\nUse SAML or OpenID Connect\nIf you are already using SAML or OpenID Connect as your authentication and\nauthorization scheme, you could just use that for service-to-service interactions too. If\nyou’re using a gateway, you’ll need to route all in-network traffic via the gateway too, but\nif each service is handling the integration itself, this approach should just work out of the\nbox. The advantage here is that you’re making use of existing infrastructure, and get to\ncentralize all your service access controls in a central directory server. We’d still need to\nroute this over HTTPS if we wanted to avoid man-in-the-middle attacks.\nClients have a set of credentials they use to authenticate themselves with the identity\nprovider, and the service gets the information it needs to decide on any fine-grained\nauthentication.\nThis does mean you’ll need an account for your clients, sometimes referred to as a \nservice\naccount\n. Many organizations use this approach quite commonly. A word of warning,\nthough: if you are going to create service accounts, try to keep their use narrow. So\nconsider each microservice having its own set of credentials. This makes\nrevoking/changing access easier if the credentials become compromised, as you only need\nto revoke the set of credentials that have been affected.\nThere are a couple of other downsides, though. First, just as with Basic Auth, we need to\nsecurely store our credentials: where do the username and password live? The client will\nneed to find some secure way to store this data. The other problem is that some of the\ntechnology in this space to do the authentication is fairly tedious to code for. SAML, in\nparticular, makes implementing a client a painful affair. OpenID Connect has a simpler\nworkflow, but as we discussed earlier it isn’t that well supported yet.\nClient Certificates\nAnother approach to confirm the identity of a client is to make use of capabilities in\nTransport Layer Security (TLS), the successor to SSL, in the form of client certificates.\nHere, each client has an X.509 certificate installed that is used to establish a link between\nclient and server. The server can verify the authenticity of the client certificate, providing\nstrong guarantees that the client is valid.\nThe operational challenges here in certificate management are even more onerous than\nwith just using server-side certificates. It isn’t just some of the basic issues of creating and\nmanaging a greater number of certificates; rather, it’s that with all the complexities around\nthe certificates themselves, you can expect to spend a lot of time trying to diagnose why a\nservice won’t accept what you believe to be a completely valid client certificate. And then\nwe have to consider the difficulty of revoking and reissuing certificates should the worst\nhappen. \nUsing wildcard certificates can help, but won’t solve all problems. This additional\nburden means you’ll be looking to use this technique when you are especially concerned\nabout the sensitivity of the data being sent, or if you are sending data via networks you\ndon’t fully control. So you might decide to secure communication of very important data\nbetween parties that is sent over the Internet, for example.\nHMAC Over HTTP\nAs we discussed earlier, the use of Basic Authentication over plain HTTP is not terribly\nsensible if we are worried about the username and password being compromised. The\ntraditional alternative is route traffic HTTPS, but there are some downsides. Aside from\nmanaging the certificates, the overhead of HTTPS traffic can place additional strain on\nservers (although, to be honest, this has a lower impact than it did several years ago), and\nthe traffic cannot easily be cached.\nAn alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts\nof the OAuth specification, is to use a \nhash-based messaging code (HMAC)\n to sign the\nrequest.\nWith HMAC the body request along with a private key is hashed, and the resulting hash is\nsent along with the request. The server then uses its own copy of the private key and the\nrequest body to re-create the hash. If it matches, it allows the request. The nice thing here\nis that if a man in the middle messes with the request, then the hash won’t match and the\nserver knows the request has been tampered with. And the private key is never sent in the\nrequest, so it cannot be compromised in transit! The added benefit is that this traffic can\nthen more easily be cached, and the overhead of generating the hashes may well be lower\nthan handling HTTPS traffic (although your mileage may vary).\nThere are three downsides to this approach. First, both the client and server need a shared\nsecret that needs to be communicated somehow. How do they share it? It could be\nhardcoded at both ends, but then you have the problem of revoking access if the secret\nbecomes compromised. If you communicate this key over some alternative protocol, then\nyou need to make sure that that protocol is also very secure!\nSecond, this is a pattern, not a standard, and thus there are divergent ways of\nimplementing it. As a result, there is a dearth of good, open, and usable implementations\nof this approach. In general, if this approach interests you, then do some more reading to\nunderstand the different ways it is done. I’d go as far as to say just look at how Amazon\ndoes this for S3 and copy its approach, especially using a sensible hashing function with a\nsuitably long key like SHA-256. \nJSON web tokens (JWT)\n are also worth looking at, as\nthey implement a very similar approach and seem to be gaining traction. But be aware of\nthe difficulty of getting this stuff right. My colleague was working with a team that was\nimplementing its own JWT implementation, omitted a single Boolean check, and\ninvalidated its entire authentication code! Hopefully over time we’ll see more reusable\nlibrary implementations.\nFinally, understand that this approach ensures only that no third party has manipulated the\nrequest and that the private key itself remains private. The rest of the data in the request\nwill still be visible to parties snooping on the network.\nAPI Keys\nAll public APIs from services like Twitter, Google, Flickr, and AWS make use of API\nkeys. API keys allow a service to identify who is making a call, and place limits on what\nthey can do. Often the limits go beyond simply giving access to a resource, and can extend\nto actions like rate-limiting specific callers to protect quality of service for other people.\nWhen it comes to using API keys to handle your own microservice-to-microservice\napproach, the exact mechanics of how it works will depend on the technology you use.\nSome systems use a single API key that is shared, and use an approach similar to HMAC\nas just described. A more common approach is to use a public and private key pair.\nTypically, you’ll manage keys centrally, just as we would manage identities of people\ncentrally. The gateway model is very popular in this space.\nPart of their popularity stems from the fact that API keys are focused on ease of use for\nprograms. Compared to handling a SAML handshake, API key–based authentication is\nmuch simpler and more straightforward.\nThe exact capabilities of the systems vary, and you have multiple options in both the\ncommercial and open source space. Some of the products just handle the API key\nexchange and some basic key management. Other tools offer everything up to and\nincluding rate limiting, monetization, API catalogs, and discovery systems.\nSome API systems allow you to bridge API keys to existing directory services. This would\nallow you to issue API keys to principals (representing people or systems) in your\norganization, and control the lifecycle of those keys in the same way you’d manage their\nnormal credentials. This opens up the possibility of allowing access to your services in\ndifferent ways but keeping the same source of truth — for example, using SAML to\nauthenticate humans for SSO, and using API keys for service-to-service communication,\nas shown in \nFigure 9-2\n.\nFigure 9-2. \nUsing directory services to synchronize principal information between an SSO and an API gateway",14466
60-The Deputy Problem.pdf,60-The Deputy Problem,"The Deputy Problem\nHaving a principal authenticate with a given microserservice is simple enough. But what\nhappens if that service then needs to make additional calls to complete an operation? Take\na look at \nFigure 9-3\n, which illustrates MusicCorp’s online shopping site. Our online shop\nis a browser-based JavaScript UI. It makes calls to a server-side shop application, using\nthe backends-for-frontends pattern we described in \nChapter 4\n. Calls made between the\nbrowser and server calls can be authenticated using SAML or OpenID Connect or similar.\nSo far, so good.\nWhen I am logged in, I can click on a link to view details of an order. To display the\ninformation, we need to pull back the original order from the order service, but we also\nwant to look up shipping information for the order. So clicking the link to\n/orderStatus/12345\n causes the online shop to initiate a call from the online shop service to\nboth the order service and shipping service asking for those details. But should these\ndownstream services accept the calls from the online shop? We could adopt a stance of\nimplicit trust — that because the call came from within our perimeter, it is OK. We could\neven use certificates or API keys to confirm that yes, it really is the online shop asking for\nthis information. But is this enough?\nFigure 9-3. \nAn example where a confused deputy could come into play\nThere is a type of vulnerability called the \nconfused deputy problem\n, which in the context\nof service-to-service communication refers to a situation where a malicious party can trick\na deputy service into making calls to a downstream service on his behalf that he shouldn’t\nbe able to. For example, as a customer, when I log in to the online shopping system, I can\nsee my account details. What if I could trick the online shopping UI into making a request\nfor someone else’s details, maybe by making a call with my logged-in credentials?\nIn this example, what is to stop me from asking for orders that are not mine? Once logged\nin, I could start sending requests for other orders that aren’t mine to see if I could get\nuseful information. We could try to protect against this inside the online shop itself, by\nchecking who the order is for and rejecting it if someone’s asking for things he shouldn’t.\nIf we have lots of different applications that surface this information, though, we could\npotentially be duplicating this logic in lots of places.\nWe could route requests directly from the UI to the order service and allow it to validate\nthe request, but then we hit the various downsides we discussed in \nChapter 4\n.\nAlternatively, when the online shop sends the request to the order service, it could state\nnot just what order it wants, but also on whose behalf it is asking. Some authentication\nschemes allow us to pass in the original principal’s credentials downstream, although with\nSAML this is a bit of a nightmare, involving nested SAML assertions that are technically\nachievable — but so difficult that no one ever does this. This can become even more\ncomplex, of course. Imagine if the services the online shop talks to in turn make more\ndownstream calls. How far do we have to go in validating trust for all those deputies?\nThis problem, unfortunately, has no simple answer, because it isn’t a simple problem. Be\naware that it exists, though. Depending on the sensitivity of the operation in \nquestion\n, you\nmight have to choose between implicit trust, verifying the identity of the caller, or asking\nthe caller to provide the credentials of the original principal.",3616
61-A Worked Example.pdf,61-A Worked Example,"Securing Data at Rest\nData lying about is a liability, especially if it is sensitive. Hopefully we’ve done\neverything we can to ensure attackers cannot breach our network, and also that they\ncannot breach our applications or operating systems to get access to the underlying close\nup. However, we need to be prepared in case they do — defense in depth is key.\nMany of the high-profile security breaches involve data at rest being acquired by an\nattacker, and that data being readable by the attacker. This is either because the data was\nstored in an unencrypted form, or because the mechanism used to protect the data had a\nfundamental flaw.\nThe mechanisms by which secure information can be protected are many and varied, but\nwhichever approach you pick there are some general things to bear in mind.\nGo with the Well Known\nThe easiest way you can mess up data encryption is to try to implement your own\nencryption algorithms, or even try to implement someone else’s. Whatever programming\nlanguage you use, you’ll have access to reviewed, regularly patched implementations of\nwell-regarded encryption algorithms. Use those! And subscribe to the mailing\nlists/advisory lists for the technology you choose to make sure you are aware of\nvulnerabilities as they are found so you can keep them patched and up to date.\nFor encryption at rest, unless you have a very good reason for picking something else,\npick a well-known implementation of AES-128 or AES-256 for your platform.\n3\n Both the\nJava and .NET runtimes include implementations of AES that are highly likely to be well\ntested (and well patched), but separate libraries exist for most platforms too — for\nexample, the \nBouncy Castle libraries\n for Java and C#.\nFor passwords, you should consider using a technique called \nsalted password hashing\n.\nBadly implemented encryption could be worse than having none, as the false sense of\nsecurity (pardon the pun) can lead you to take your eye off the ball.\nIt’s All About the Keys\nAs has been covered so far, encryption relies on an algorithm taking the data to be\nencrypted and a key and then producing the encrypted data. So, where is your key stored?\nNow if I am encrypting my data because I am worried about someone stealing my whole\ndatabase, and I store the key I use in the same database, then I haven’t really achieved\nmuch! Therefore, we need to store the keys somewhere else. But where?\nOne solution is to use a separate security appliance to encrypt and decrypt data. Another is\nto use a separate key vault that your service can access when it needs a key. The lifecycle\nmanagement of the keys (and access to change them) can be a vital operation, and these\nsystems can handle this for you.\nSome databases even include built-in support for encryption, such as SQL Server’s\nTransparent Data Encryption, that aim to handle this in a transparent fashion. Even if your\ndatabase of choice does, research how the keys are handled and understand if the threat\nyou are protecting against is actually being mitigated.\nAgain, this stuff is complex. Avoid implementing your own, and do some good research!\nPick Your Targets\nAssuming everything should be encrypted can simplify things somewhat. There is no\nguesswork about what should or should not be protected. However, you’ll still need to\nthink about what data can be put into logfiles to help problem identification, and the\ncomputational overhead of encrypting everything can become pretty onerous, needing\nmore powerful hardware as a result. This is even more challenging when you’re applying\ndatabase migrations as part of refactoring schemas. Depending on the changes being\nmade, the data may need to be decrypted, migrated, and re-encrypted.\nBy subdividing your system into more fine-grained services, you might identify an entire\ndata store that can be encrypted wholesale, but even then it is unlikely. Limiting this\nencryption to a known set of tables is a sensible approach.\nDecrypt on Demand\nEncrypt data when you first see it. Only decrypt on demand, and ensure that data is never\nstored anywhere.\nEncrypt Backups\nBackups are good. We want to back up our important data, and almost by definition data\nwe are worried enough about that we want to encrypt it is important enough to back up!\nSo it may seem like an obvious point, but we need to make sure that \nour backups are also\nencrypted\n. This also means that we need to know which keys are needed to handle which\nversion of data, especially if the keys change. Having clear key management becomes\nfairly important.\nDefense in Depth\nAs I’ve mentioned earlier, I dislike putting all our eggs in one basket. It’s all about defence\nin depth. We’ve talked already about securing data in transit, and securing data at rest. But\nare there other protections we could put in place to help?\nFirewalls\nHaving one or more firewalls is a very sensible precaution to take. Some are very simple,\nable only to restrict access to certain types of traffic on certain ports. Others are more\nsophisticated. ModSecurity, for example, is a type of application firewall that can help\nthrottle connections from certain IP ranges and detect other sorts of malicious attacks.\nThere is value in having more than one firewall. For example, you may decide to use\nIPTables locally on a host to secure that host, setting up the allowable ingress and egress.\nThese rules could be tailored to the locally running services, with a firewall at the\nperimeter for controlling general access.\nLogging\nGood logging, and specifically the ability to aggregate logs from multiple systems, is not\nabout prevention, but can help with detecting and recovering from bad things happening.\nFor example, after applying security patches you can often see in logs if people have been\nexploiting certain vulnerabilities. Patching makes sure it won’t happen again, but if it\nalready \nhas\n happened, you may need to go into recovery mode. Having logs available\nallows you to see if something bad happened after the fact.\nNote, however, that we need to be careful about what information we store in our logs!\nSensitive information needs to be culled to ensure we aren’t leaking important data into\nour logs, which could end up being a great target for attackers.\nIntrusion Detection (and Prevention) System\nIntrusion detection systems (IDS)\n can monitor networks or hosts for suspicious behavior,\nreporting problems when it sees them. \nIntrusion prevention systems (IPS)\n, as well as\nmonitoring for suspicious activity, can step in to stop it from happening. \nUnlike a firewall,\nwhich is primarily looking outward to stop bad things from getting in, IDS and IPS are\nactively looking inside the perimeter for suspect behavior. When you’re starting from\nscratch, IDS may make most sense. These systems are heuristic-based (as are many\napplication firewalls), and it is possible that the generic starting set of rules will either be\ntoo lenient or not lenient enough for how your service behaves. Using a more passive IDS\nto alert you to problems is a good way to tune your rules before using it in a more active\ncapacity.\nNetwork Segregation\nWith a monolithic system, we have limits to how we can structure our networks to provide\nadditional protections. With microservices, though, you can put them into different\nnetwork segments to further control how services talk to each other. AWS, for example,\nprovides the ability to automatically provision a \nvirtual private cloud (VPC)\n, which allow\nhosts to live in separate subnets. You can then specify which VPCs can see each other by\ndefining peering rules, and even route traffic through gateways to proxy access, giving\nyou in effect multiple perimeters at which additional security measures can be put into\nplace.\nThis could allow you to segment networks based on team ownership, or perhaps by risk\nlevel.\nOperating System\nOur systems rely on a large amount of software that we didn’t write, and may have\nsecurity vulnerabilities that could expose our application, namely our operating systems\nand the other supporting tools we run on them. Here, basic advice can get you a long way.\nStart with only running services as OS users that have as few permissions as possible, to\nensure that if such an account is compromised it will do minimal damage.\nNext, patch your software. Regularly. This needs to be automated, and you need to know\nif your machines are out of sync with the latest patch levels. Tools like Microsoft’s SCCM\nor RedHat’s Spacewalk can be beneficial here, as they can help you see if machines are up\nto date with the latest patches and initiate updates if required. If you are using tools like\nAnsible, Puppet, or Chef, chances are you are already fairly happy with pushing out\nchanges automatically — these tools can get you a long way too, but won’t do everything\nfor you.\nThis really is basic stuff, but it is surprising how often I see critical software running on\nunpatched, old operating systems. You can have the most well-defined and protected\napplication-level security in the world, but if you have an old version of a web server\nrunning on your machine as root that has an unpatched buffer overflow vulnerability, then\nyour system could still be extremely vulnerable.\nAnother thing to look at if you are using Linux is the emergence of security modules for\nthe operating system itself. AppArmour, for example, allows you to define how your\napplication is expected to behave, with the kernel keeping an eye on it. If it starts doing\nsomething it shouldn’t, the kernel steps in. AppArmour has been around for a while, as\nhas SeLinux. Although technically either of them should work on any modern Linux\nsystem, in practice some distributions support one better than the other. AppArmour is\nused by default in Ubuntu and SuSE, for example, whereas SELinux has traditionally been\nwell supported by RedHat. A newer option is \nGrSSecurity\n, which aims to be simpler to\nuse than either AppArmour or GrSecurity while also trying to expand on their capabilities,\nbut it requires a custom kernel to work. I’d suggest taking a look at all three to see which\nfits your use cases best, but I like the idea of having another layer of protection and\nprevention at work.\nA Worked Example\nHaving a finer-grained system architecture gives us much more freedom in how we\nimplement our security. For those parts that deal with the most sensitive information or\nexpose the most valuable capabilities, we can adopt the strictest security provisions. But\nfor other parts of the system, we can afford to be much more lax in what we worry about.\nLet’s consider MusicCorp once again, and pull some of the preceding concepts together to\nsee where and how we might use some of these security techniques. We’re looking\nprimarily at the security concerns of data in transit and at rest. \nFigure 9-4\n shows a subset\nof the overall system that we’ll be analyzing, which currently shows a crushing lack of\nregard for security concerns. Everything is sent over plain old HTTP.\nFigure 9-4. \nA subset of MusicCorp’s unfortunately insecure architecture\nHere we have standard web browsers that are used by our customers to shop on the site.\nWe also introduce the concept of a third-party royalty gateway: we’ve started working\nwith a third-party company that will handle royalty payments for our new streaming\nservice. It contacts us occasionally to pull down records of what music has been streamed\nwhen — information we jealously protect as we are worried about competition from rival\ncompanies. Finally, we expose our catalog data to other third parties — for example,\nallowing the metadata about artist or song to be embedded in music review sites. Inside\nour network perimeter, we have some collaborating services, which are only ever used\ninternally.\nFor the browser, we’ll use a mix of standard HTTP traffic for nonsecure content, to allow\nfor it to be cached. For secure, logged-in pages, all secure content will be sent over\nHTTPS, giving our customers extra protection if they are doing things like running on\npublic WiFi networks.\nWhen it comes to the third-party royalty payment system, we are concerned not only about\nthe nature of the data we are exposing, but also about making sure the requests we’re\ngetting are legitimate. Here, we insist that our third party uses client certificates. All the\ndata is sent over a secure, cryptographic channel, increasing our ability to ensure we’re\nbeing asked for this data by the right person. We do, of course, have to think about what\nhappens when the data leaves our control. Will our partner care about the data as much as\nwe will?\nFor the feeds of catalog data, we want this information shared as widely as possible to\nallow people to easily buy music from us! However, we don’t want this abused, and we’d\nlike some idea of who is using our data. Here, API keys make perfect sense.\nInside the network perimeter, things are a bit more nuanced. How worried are we about\npeople compromising our internal networks? Ideally, we’d like to use HTTPS at a\nminimum, but managing it is somewhat painful. We decide instead to put the work\n(initially, at least) into hardening our network perimeter, including having a properly\nconfigured firewall and selecting an appropriate hardware or software security appliance\nto check for malicious traffic (e.g., port scanning or denial-of-service attacks).\nThat said, we are concerned about \nsome\n of our data and where it lives. We aren’t worried\nabout the catalog service; after all, we want that data shared and have provided an API for\nit! But we are very concerned about our customers’ data. Here, we decide to encrypt the\ndata held by the customer service, and decrypt data on read. If attackers do penetrate our\nnetwork, they could still run requests against the customer service’s API, but the current\nimplementation does not allow for the bulk retrieval of customer data. If it did, we would\nlikely consider the use of client certificates to protect this information. Even if attackers\ncompromise the machine the database is running on and manage to download the entire\ncontents, they would need access to the key used to encrypt and decrypt the data to make\nuse if it.\nFigure 9-5\n shows the final picture. As you can see, the choices we made about what\ntechnology to use were based on an understanding of the nature of the information being\nsecured. Your own architecture’s security concerns are likely to be very different, and so\nyou may end up with a different-looking solution.\nFigure 9-5. \nMusicCorp’s more secure system",14742
62-Adapting to Communication Pathways.pdf,62-Adapting to Communication Pathways,"Be Frugal\nAs disk space becomes cheaper and the capabilities of the databases improve, the ease\nwith which bulk amounts of information can be captured and stored is improving rapidly.\nThis data is valuable — not only to businesses themselves, which increasingly see data as\na valuable asset, but equally to the users who value their own privacy. The data that\npertains to an individual, or could be used to derive information about an individual, must\nbe the data we are most careful about.\nHowever, what if we made our lives a bit easier? Why not scrub as much information as\npossible that can be personally identifiable, and do it as soon as possible? When logging a\nrequest from a user, do we need to store the entire IP address forever, or could we replace\nthe last few digits with \nx\n? Do we need to store someone’s name, age, gender, and date of\nbirth in order to provide her with product offers, or is her age range and postcode enough\ninformation?\nThe advantages here are manifold. First, if you don’t store it, no one can steal it. Second, if\nyou don’t store it, no one (e.g., a governmental agency) can ask for it either!\nThe German phrase \nDatensparsamkeit\n represents this concept. Originating from German\nprivacy legislation, it encapsulates the concept of only storing as much information as is\nabsolutely required\n to fulfill business operations or satisfy local laws.\nThis is obviously in direct tension with the move toward storing more and more\ninformation, but it is a start to realize that this tension even exists!\nThe Human Element\nMuch of what we have covered here is the basics of how to implement technological\nsafeguards to protect your systems and data from malicious, external attackers. However,\nyou may also need processes and policies in place to deal with the human element in your\norganization. How do you revoke access to credentials when someone leaves the\norganization? How can you protect yourself against social engineering? As a good mental\nexercise, consider what damage a disgruntled ex-employee could do to your systems if she\nwanted to. Putting yourself in the mindset of a malicious party is often a good way to\nreason about the protections you may need, and few malicious parties have as much inside\ninformation as a recent employee!\nThe Golden Rule\nIf there is nothing else you take away from this chapter, let it be this: don’t write your own\ncrypto. Don’t invent your own security protocols. Unless you are a cryptographic expert\nwith years of experience, if you try inventing your own encoding or elaborate\ncryptographic protections, you will get it wrong. And even \nif\n you are a cryptographic\nexpert, you may still get it wrong.\nMany of the tools previously outlined, like AES, are industry-hardened technologies\nwhose underlying algorithms have been peer reviewed, and whose software\nimplementation has been rigorously tested and patched over many years. They are good\nenough! Reinventing the wheel in many cases is often just a waste of time, but when it\ncomes to security it can be outright dangerous.\nBaking Security In\nJust as with automated functional testing, we don’t want security to be left to a different\nset of people, nor do we want to leave everything to the last minute. Helping educate\ndevelopers about security concerns is key, as raising everyone’s general awareness of\nsecurity issues can help reduce them in the first place. Getting people familar with the\nOWASP Top Ten list and OWASP’s Security Testing Framework can be a great place to\nstart. Specialists absolutely have their place, though, and if you have access to them, use\nthem to help you.\nThere are automated tools that can probe our systems for vulnerabilities, such as by\nlooking for cross-site scripting attacks. The Zed Attack Proxy (aka ZAP) is a good\nexample. Informed by the work of OWASP, ZAP attempts to re-create malicious attacks\non your website. Other tools exist that use static analysis to look for common coding\nmistakes that can open up security holes, such as \nBrakeman\n for Ruby. Where these tools\ncan be easily integrated into normal CI builds, integrate them into your standard check-ins.\nOther sorts of automated tests are more involved. For example, using something like\nNessus to scan for vulnerabilities is a bit more involved and it may require a human to\ninterpret the results. That said, these tests are still automatable, and it may make sense to\nrun them with the same sort of cadence as load testing.\nMicrosoft’s \nSecurity Development Lifecycle\n also has some good models for how delivery\nteams can bake security in. Some aspects of it feel overly waterfall, but take a look and see\nwhat aspects can fit into your current workflow.\nExternal Verification\nWith security, I think there is great value in having an external assessment done. Exercises\nlike penetration testing, when done by an outside party, really do mimic real-world\nattempts. They also sidestep the issue that teams aren’t always able to see the mistakes\nthey have made themselves, as they are too close to the problem. If you’re a big enough\ncompany, you may have a dedicated infosec team that can help you. If not, find an\nexternal party who can. Reach out to them early, understand how they like to work, and\nfind out how much notice they need to do a test.\nYou’ll also need to consider how much verification you require before each release.\nGenerally, doing a full penetration test, for example, isn’t needed for small incremental\nreleases, but may be for larger changes. What you need depends on your own risk profile.\nSummary\nSo again we return to a core theme of the book — that having a system decomposed into\nfiner-grained services gives us many more options as to how to solve a problem. Not only\ncan having microservices potentially reduce the impact of any given breach, but it also\ngives us more ability to trade off the overhead of more complex and secure approaches\nwhere data is sensitive, and a lighter-weight approach when the risks are lower.\nOnce you understand the threat levels of different parts of your system, you should start to\nget a sense of when to consider security during transit, at rest, or not at all.\nFinally, understand the importance of defense in depth, make sure you patch your\noperating systems, and even if you consider yourself a rock star, don’t try to implement\nyour own cryptography!\nIf you want a general overview of security for browser-based applications, a great place to\nstart is the excellent \nOpen Web Application Security Project (OWASP) nonprofit\n, whose\nregularly updated \nTop 10 Security Risk\n document should be considered essential reading\nfor any developer. Finally, if you want a more general discussion of cryptography, check\nout the book \nCryptography Engineering\n by Niels Ferguson, Bruce Schneier, and\nTadayoshi Kohno (Wiley).\nGetting to grips with security is often about understanding people and how they work with\nour systems. One human-related aspect we haven’t yet discussed in terms of microservices\nis the interplay between organizational structures and the architectures themselves. But as\nwith security, we’ll see that ignoring the human element can be a grave mistake.\n3\n In general, key length increases the amount of work required to brute-force-break a key.\nTherefore you can assume the longer the key, the more secure your data. However, some\nminor concerns have been raised about the implementation of AES-256 for certain types\nof keys by respected security expert \nBruce Schneier\n. This is one of those areas where you\nneed to do more research on what the current advice is at the time of reading!\nChapter 10. \nConway’s Law and System\nDesign\nMuch of the book so far has focused on the technical challenges in moving toward a fine-\ngrained architecture. But there are other, organizational issues to consider as well. As\nwe’ll learn in this chapter, you ignore your company’s organization chart at your peril!\nOur industry is young, and seems to be constantly reinventing itself. And yet a few key\nlaws\n have stood the test of time. Moore’s law, for example, which states that the density of\ntransistors on integrated circuits doubles every two years, has proved to be uncannily\naccurate (although some people predict that this trend is already slowing). One law that I\nhave found to be almost universally true, and far more useful in my day-to-day work, is\nConway’s law.\nMelvin Conway’s paper \nHow Do Committees Invent\n, published in \nDatamation\n magazine\nin April 1968, observed that:\nAny organization that designs a system (defined more broadly here than just\ninformation systems) will inevitably produce a design whose structure is a copy of the\norganization’s communication structure.\nThis statement is often quoted, in various forms, as Conway’s law. Eric S. Raymond\nsummarized this phenomenon in \nThe New Hacker’s Dictionary\n (MIT Press) by stating “If\nyou have four groups working on a compiler, you’ll get a 4-pass compiler.”\nEvidence\nThe story goes that when Melvin Conway submitted his paper on this topic to the Harvard\nBusiness Review, they rejected it, claiming he hadn’t proved his thesis. I’ve seen this\ntheory borne out in so many different situations that I’ve accepted it as true. But you don’t\nhave to take my word for it: since Conway’s original submission, a lot of work has been\ndone in this area. A number of studies have been carried out to explore the interrelation of\norganizational structure and the systems they create.\nLoose and Tightly Coupled Organizations\nIn \nExploring the Duality Between Product and Organizational Architectures\n (Harvard\nBusiness School), the authors Alan MacCormack, John Rusnak, and Carliss Baldwin look\nat a number of different software systems, loosely categorized as being created either by\nloosely coupled organizations\n or \ntightly coupled organizations\n. For tightly coupled\norganizations, think commercial product firms that are typically colocated with strongly\naligned visions and goals, while loosely coupled organizations are well represented by\ndistributed open source communities.\nIn their study, in which they matched similar product pairs from each type of organization,\nthe authors found that the more loosely coupled organizations actually created more\nmodular, less coupled systems, whereas the more tightly focused organization’s software\nwas less modularized.\nWindows Vista\nMicrosoft carried out \nan empirical study\n where it looked at how its own organizational\nstructure impacted the software quality of a specific product, Windows Vista. Specifically,\nthe researchers looked at multiple factors to determine how error-prone a component in\nthe system would be.\n4\n After looking at multiple metrics, including commonly used\nsoftware quality metrics like code complexity, they found that the metrics associated with\norganizational structures proved to be the most statistically relevant measures.\nSo here we have another example of the organizational structure impacting the nature of\nthe system that organization creates.\nNetflix and Amazon\nProbably the two poster children for the idea that organizations and architecture should be\naligned are Amazon and Netflix. Early on, Amazon started to understand the benefits of\nteams owning the whole lifecycle of the systems they managed. It wanted teams to own\nand operate the systems they looked after, managing the entire lifecycle. But Amazon also\nknew that small teams can work faster than large teams. This led famously to its \ntwo-pizza\nteams\n, where no team should be so big that it could not be fed with two pizzas. This driver\nfor small teams owning the whole lifecycle of their services is a major reason why\nAmazon developed Amazon Web Services. It needed to create the tooling to allow its\nteams to be self-sufficient.\nNetflix learned from this example, and ensured that from the beginning it structured itself\naround small, independent teams, so that the services they created would also be\nindependent from each other. This ensured that the architecture of the system was\noptimized for speed of change. Effectively, Netflix designed the organizational structure\nfor the system architecture it wanted.\nWhat Can We Do with This?\nSo evidence, anecdotal and empirical, points to our organizational structure being a strong\ninfluence on the nature (and quality) of the systems we provide. So how does this\nunderstanding help us? Let’s look at a few different organizational situations and\nunderstand what impact each might have on our system design.\nAdapting to Communication Pathways\nLet’s first consider a simple, single team. It’s in charge of all aspects of the system design\nand implementation. It can have frequent, fine-grained communication. Imagine that this\nteam is in charge of a single service — say, our music shop’s catalog service. Now\nconsider the inside of a service: lots of fine-grained method or function calls. As we’ve\ndiscussed before, we aim to ensure our services are decomposed such that the pace of\nchange inside a service is much higher than the pace of change between services. This\nsingle team, with its ability for fine-grained communication, matches nicely with the\ncommunication pathways of the code within the service.\nThis single team finds it easy to communicate about proposed changes and refactorings,\nand typically has a good sense of ownership.\nNow let’s imagine a different scenario. Instead of a single, geolocated team owning our\ncatalog service, suppose that teams in the UK and India both are actively involved in\nchanging a service — effectively having joint ownership of the service. Geographical and\ntime zone boundaries here make fine-grained communication between those teams\ndifficult. Instead, they rely on more coarse-grained communication via video conferencing\nand email. How easy is it for a team member in the UK to make a simple refactoring with\nconfidence? The cost of communications in a geographically distributed team is higher,\nand therefore the cost of coordinating changes is higher.\nWhen the cost of coordinating change increases, one of two things happen. Either people\nfind ways to reduce the coordination/communication costs, or they stop making changes.\nThe latter is exactly how we end up with large, hard-to-maintain \ncodebases\n.\nI recall one client project I worked on where ownership of a single service was shared\nbetween two geographical locations. Eventually, each site started specializing what work it\nhandled. This allowed it to take ownership of part of the codebase, within which it could\nhave an easier cost of change. The teams then had more coarse-grained communication\nabout how the two parts interrelated; effectively, the communication pathways made\npossible within the organizational structure matched the coarse-grained API that formed\nthe boundary between the two halves of the \ncodebase\n.\nSo where does this leave us when considering evolving our own service design? Well, I\nwould suggest that geographical boundaries between people involved with the\ndevelopment of a system can be a great way to drive when services should be\ndecomposed, and that in general, you should look to assign ownership of a service to a\nsingle, colocated team who can keep the cost of change low.\nPerhaps your organization decides that it wants to increase the number of people working\non your project by opening up an office in another country. At this point, think actively\nabout what parts of your system can be moved over. Perhaps this is what drives your\ndecisions about what seams to split out next.\nIt is also worth noting at this point that, at least based on the observations of the authors of\nthe \nExploring the Duality Between Product and Organizational Architectures\n report\npreviously referenced, if the organization building the system is more loosely coupled\n(e.g., consisting of geographically distributed teams), the systems being built tend toward\nthe more modular, and therefore hopefully less coupled. The tendency of a single team\nthat owns many services to lean toward tighter integration is very hard to maintain in a\nmore distributed organization.",16329
63-Case Study RealEstate.com.au.pdf,63-Case Study RealEstate.com.au,"Service Ownership\nWhat do I mean by \nservice ownership\n? In general, it means that the team owning a service\nis responsible for making changes to that service. The team should feel free to restructure\nthe code however it wants, as long as that change doesn’t break consuming services. For\nmany teams, \nownership\n extends to all aspects of the service, from sourcing requirements\nto building, deploying, and maintaining the application. This model is especially prevalent\nwith microservices, where it is easier for a small team to own a small service. This\nincreased level of ownership leads to increased autonomy and speed of delivery. Having\none team responsible for deploying and maintaining the application means it has an\nincentive to create services that are \neasy\n to deploy; that is, concerns about “throwing\nsomething over the wall” dissipate when there is no one to throw it to!\nThis model is certainly one I favor. It pushes the decisions to the people best able to make\nthem, giving the team both increased power and autonomy, but also making it accountable\nfor its work. I’ve seen far too many developers hand their system over for testing or\ndeployment phases and think that their work is done at that point.\nDrivers for Shared Services\nI have seen many teams adopt a model of shared service ownership. I find this approach\nsuboptimal, for reasons already discussed. However, the drivers that cause people to pick\nshared services are important to understand, especially as we may be able to find some\ncompelling alternative models that can address people’s underlying concerns.\nToo Hard to Split\nObviously, one of the reasons you may find yourself with a single service owned by more\nthan one team is that the cost of splitting the service is too high, or perhaps your\norganization might not see the point of it. This is a common occurrence with large\nmonolithic systems. If this is the main challenge you face, then I hope some of the advice\ngiven in \nChapter 5\n will be of use. You could also consider merging teams together, to align\nmore closely with the architecture itself.\nFeature Teams\nThe idea of feature teams (aka feature-based teams) is that a small team drives the\ndevelopment of a set of features, implementing all functionality required even if it cuts\nacross component (or even service) boundaries. The goals of feature teams are sensible\nenough. This structure allows the team to retain a focus on the end result and ensures that\nthe work is joined up, avoiding some of the challenges of trying to coordinate changes\nacross multiple different teams.\nIn many situations, the feature team is a reaction to traditional IT organizations where\nteam structure is aligned around technical boundaries. For example, you might have a\nteam that is responsible for the UI, another that is responsible for the application logic, and\na third handling the database. In this environment, a feature team is a significant step up,\nas it works across all these layers to deliver the functionality.\nWith wholesale adoption of feature teams, all services can be considered shared. Everyone\ncan change every service, every piece of code. The role of the service custodians here\nbecomes much more complex, if the role exists at all. Unfortunately, I rarely see\nfunctioning custodians at all where this pattern is adopted, leading to the sorts of issues we\ndiscussed earlier.\nBut let’s again consider what microservices are: services modeled after a business domain,\nnot a technical one. And if our team that owns any given service is similarly aligned along\nthe business domain, it is much more likely that the team will be able to retain a customer\nfocus, and see more of the feature development through, because it has a holistic\nunderstanding and ownership of all the technology associated with a service.\nCross-cutting changes can occur, of course, but their likelihood is significantly reduced by\nour avoiding technology-oriented teams.\nDelivery Bottlenecks\nOne key reason people move toward shared services is to avoid delivery bottlenecks.\nWhat if there is a large backlog of changes that need to be made in a single service? Let’s\nimagine that we are rolling out the ability for a customer to see the genre of a track across\nour products, as well as adding a a brand new type of stock: virtual musical ringtones for\nthe mobile phone. The website team needs to make a change to surface the genre\ninformation, with the mobile app team working to allow users to browse, preview, and buy\nthe ringtones. Both changes need to be made to the catalog service, but unfortunately half\nthe team is out with the flu, and the other half is stuck diagnosing a production failure.\nWe have a couple of options that don’t involve shared services to avoid this situation. The\nfirst is to just wait. The website and mobile application teams move on to something else.\nDepending on how important the feature is, or how long the delay is likely to be, this may\nbe fine or it may be a major problem.\nYou could instead add people to the catalog team to help them move through their work\nfaster. The more standardized the technology stack and programming idioms in use across\nyour system, the easier it is for other people to make changes in your services. The\nflipside, of course, as we discussed earlier, is that standardization tends to reduce a team’s\nability to adopt the right solution for the job, and can lead to different sorts of\ninefficiencies. If the team is on the other side of the planet, this might be impossible,\nhowever.\nAnother option could be to split the catalog into a separate general music catalog and a\nringtone catalog. If the change being made to support ringtones is fairly small, and the\nlikelihood of this being an area in which we will develop heavily in the future is also quite\nlow, this may well be premature. On the other hand, if there are 10 weeks of ringtone-\nrelated features stacked up, splitting out the service could make sense, with the mobile\nteam taking ownership.\nThere is another model that could work well for us, though.\nInternal Open Source\nSo what if we’ve tried our hardest, but we just can’t find a way past having a few shared\nservices? At this point, properly embracing the internal open source model can make a lot\nof sense.\nWith normal open source, a small group of people are considered core committers. They\nare the custodians of the code. If you want a change to an open source project, you either\nask one of the committers to make the change for you, or else you make the change\nyourself and send them a pull request. The core committers are still in charge of the\ncodebase; they are the owners.\nInside the organization, this pattern can work well too. Perhaps the people who worked on\nthe service originally are no longer on a team together; perhaps they are now scattered\nacross the organization. Well, if they still have commit rights, you can find them and ask\nfor their help, perhaps pairing up with them, or if you have the right tooling you can send\nthem a pull request.\nRole of the Custodians\nWe still want our services to be sensible. We want the code to be of decent quality, and the\nservice itself to exhibit some sort of consistency in how it is put together. We also want to\nmake sure that changes being made now don’t make future planned changes much harder\nthan they need to be. This means that we need to adopt the same patterns used in normal\nopen source internally too, which means separating out a group of trusted committers (the\ncore team), and untrusted committers (people from outside the team submitting changes).\nThe core team needs to have some way of vetting and approving the changes. It needs to\nmake sure the changes are idiomatically consistent — that is, that they follow the general\ncoding guidelines of the rest of the codebase. The people doing the vetting are therefore\ngoing to have to spend time working with the submitters to make sure the change is of\nsufficient quality.\nGood gatekeepers put a lot of work into this, communicating clearly with the submitters\nand encouraging good behavior. Bad gatekeepers can use this as an excuse to exert power\nover others or have religious wars about arbitrary technical decisions. Having seen both\nsets of behavior, I can tell you one thing is clear: either way it takes time. When\nconsidering allowing untrusted committers to submit changes to your codebase, you have\nto decide if the overhead of being a gatekeeper is worth the trouble: could the core team be\ndoing better things with the time it spends vetting patches?\nMaturity\nThe less stable or mature a service is, the harder it will be to allow people outside the core\nteam to submit patches. Before the key spine of a service is in place, the team may not\nknow what \ngood\n looks like, and therefore may struggle to know what a good submission\nlooks like. During this stage, the service itself is undergoing a high degree of change.\nMost open source projects tend to not take submissions from a wider group of untrusted\ncommitters until the core of the first version is done. Following a similar model for your\nown organizations makes sense. If a service is pretty mature, and is rarely changed — for\nexample, our cart service — then perhaps that is the time to open it up for other\ncontributions.\nTooling\nTo best support an internal open source model, you’ll need some tooling in place. The use\nof a distributed version control tool with the ability for people to submit pull requests (or\nsomething similar) is important. Depending on the size of the organization, you may also\nneed tooling to allow for a discussion and evolution of patch requests; this may or may not\nmean a full-blown code review system, but the ability to comment inline on patches is\nvery useful. Finally, you’ll need to make it very easy for a committer to build and deploy\nyour software, and make it available for others. Typically this involves having well-\ndefined build and deployment pipelines and centralized artifact repositories.\nBounded Contexts and Team Structures\nAs mentioned before, we look to draw our service boundaries around bounded contexts. It\ntherefore follows that we would like our teams aligned along bounded contexts too. This\nhas multiple benefits. First, a team will find it easier to grasp domain concepts within a\nbounded context, as they are interrelated. Second, services within a bounded context are\nmore likely to be services that talk to each other, making system design and release\ncoordination easier. Finally, in terms of how the delivery team interacts with the business\nstakeholders, it becomes easier for the team to create good relationships with the one or\ntwo experts in that area.\nThe Orphaned Service?\nSo what about services that are no longer being actively maintained? As we move toward\nfiner-grained architectures, the services themselves become smaller. One of the goals of\nsmaller services, as we have discussed, is the fact that they are simpler. Simpler services\nwith less functionality may not need to change for a while. Consider the humble cart\nservice, which provides some fairly modest capabilities: Add to Cart, Remove from Cart,\nand so on. It is quite conceivable that this service may not have to change for months after\nfirst being written, even if active development is still going on. What happens here? Who\nowns this service?\nIf your team structures are aligned along the bounded contexts of your organization, then\neven services that are not changed frequently still have a de facto owner. Imagine a team\nthat is aligned with the consumer web sales context. It might handle the website, cart, and\nrecommendation services. Even if the cart service hasn’t been changed in months, it would\nnaturally fall to this team to make the change. One of the benefits of microservices, of\ncourse, is that if the team needs to change the service to add a new feature and not find it\nto its liking, rewriting it shouldn’t take too long at all.\nThat said, if you’ve adopted a truly polyglot approach, making use of multiple technology\nstacks, then the challenges of making changes to an orphaned service could be\ncompounded if your team doesn’t know the tech stack any longer.\nCase Study: RealEstate.com.au\nREA’s core business is real estate. But this encompasses multiple different facets, each of\nwhich operates as a single line of business (LOB). For example, one line of business deals\nwith residential property in Australia, another commercial, while another might relate to\none of REA’s overseas businesses. These lines of business have IT delivery teams (or\nsquads\n) associated with them; some may have only a single squad, while the biggest has\nfour. So for residential property, there are multiple teams involved with creating the\nwebsite and listing services to allow people to browse property. People rotate between\nthese teams every now and then, but \ntend to stay within that line of business for extended\nperiods, ensuring that the team members can build up a strong awareness of that part of\nthe domain. This in turn helps the communication between the various business\nstakeholders and the team delivering features for them.\nEach squad inside a line of business is expected to own the entire lifecycle of the services\nit creates, including building, testing and releasing, supporting, and even\ndecommissioning. A core delivery services team provides advice and guidance to these\nteams, as well as tooling to help it get the job done. A strong culture of automation is key,\nand REA makes heavy use of AWS as a key part of enabling the teams to be more\nautonomous. \nFigure 10-1\n illustrates how this all works.\nFigure 10-1. \nAn overview of Realestate.com.au’s organizational and team structure, and alignment with architecture\nIt isn’t just the delivery organization that is aligned to how the business operates. It\nextends to the architecture too. One example of this is integration methods. Within an\nLOB, all services are free to talk to each other in any way they see fit, as decided by the\nsquads who act as their custodians. But between LOBs, all communication is mandated to\nbe asynchronous batch, one of the few cast-iron rules of the very small architecture team.\nThis coarse-grained communication matches the coarse-grained communication that exists\nbetween the different parts of the business too. By insisting on it being batch, each LOB\nhas a lot of freedom in how it acts and manages itself. It could afford to take its services\ndown whenever it wanted, knowing that as long as it can satisfy the batch integration with\nother parts of the business and its own business stakeholders, no one would care.\nThis structure has allowed for significant autonomy of not only the teams but also the\ndifferent parts of the business. From a handful of services a few years ago, REA now has\nhundreds, with more services than people, and is growing at a rapid pace. The ability to\ndeliver change has helped the company achieve significant success in the local market to\nthe point where it is expanding overseas. And, most heartening of all, from talking to the\npeople there I get the impression that both the architecture and organizational structure as\nthey stand now are just the latest iteration rather than the destination. I daresay in another\nfive years REA will look very different again.\nThose organizations that are adaptive enough to change not only their system architecture\nbut also their organizational structure can yield huge benefits in terms of improved\nautonomy of teams and faster time to market for new features and functionality. REA is\njust one of a number of organizations that are realizing that system architecture doesn’t\nexist in a vacuum.",15905
64-How Much Is Too Much.pdf,64-How Much Is Too Much,"Conway’s Law in Reverse\nSo far, we’ve spoken about how the organization impacts the system design. But what\nabout the reverse? Namely, can a system design change the organization? While I haven’t\nbeen able to find the same quality of evidence to support the idea that Conway’s law\nworks in reverse, I’ve seen it anecdotally.\nProbably the best example was a client I worked with many years ago. Back in the days\nwhen the Web was fairly nascent, and the Internet was seen as something that arrived on\nan AOL floppy disk through the door, this company was a large print firm that had a\nsmall, modest website. It had a website because it was the thing to do, but in the grand\nscheme of things it was fairly unimportant to how the business operated. When the\noriginal system was created, a fairly arbitrary technical decision was made as to how the\nsystem would work.\nThe content for this system was sourced in multiple ways, but most of it came from third\nparties who were placing ads for viewing by the general public. There was an input system\nthat allowed content to be created by the paying third parties, a central system that took\nthat data and enriched it in various ways, and an output system that created the final\nwebsite that the general public could browse.\nWhether the original design decisions were right at the time is a conversation for\nhistorians, but many years on the company had changed quite a bit and I and many of my\ncolleagues were starting to wonder if the system design was fit for the company’s present\nstate. Its physical print business had diminished significantly, and the revenues and\ntherefore business operations of the organization were now dominated by its online\npresence.\nWhat we saw at that time was an organization tightly aligned to this three-part system.\nThree channels or divisions in the IT side of the business aligned with each of the input,\ncore, and output parts of the business. Within those channels, there were separate delivery\nteams. What I didn’t realize at the time was that these organizational structures didn’t\npredate the system design, but actually grew up around it. As the print side of the business\ndiminished, and the digital side of the business grew, the system design inadvertently lay\nthe path for how the organization grew.\nIn the end we realized that whatever the shortcomings of the system design were, we\nwould have to make changes to the organizational structure to make a shift. Many years\nlater, that process remains a work in progress!\nPeople\nNo matter how it looks at first, it’s always a people problem.\nGerry Weinberg, \nThe Second Law of Consulting\nWe have to accept that in a microservice environment, it is harder for a developer to just\nthink about writing code in his own little world. He has to be more aware of the\nimplications of things like calls across network boundaries, or the implications of failure.\nWe’ve also talked about the ability of microservices to make it easier to try out new\ntechnologies, from data stores to languages. But if you’re moving from a world where you\nhave a monolithic system, where the majority of your developers have just had to use one\nlanguage and remain completely oblivious to the operational concerns, then throwing\nthem into the world of microservices may be a rude awakening for them.\nLikewise, pushing power into development teams to increase autonomy can be fraught.\nPeople who have in the past thrown work over the wall to someone else are accustomed to\nhaving someone else to blame, and may not feel comfortable being fully accountable for\ntheir work. You may even find contractual barriers to having your developers carry\nsupport pagers for the systems they support!\nAlthough this book has mostly been about technology, people are not just a side issue to\nbe considered; they are the people who built what you have now, and will build what\nhappens next. Coming up with a vision for how things should be done without considering\nhow your current staff will feel about this or without considering what capabilities they\nhave is likely to lead to a bad place.\nEach organization has its own set of dynamics around this topic. Understand your staff’s\nappetite to change. Don’t push them too fast! Maybe you still have a separate team handle\nfrontline support or deployment for a short period of time, giving your developers time to\nadjust to other new practices. You may, however, have to accept that you need different\nsorts of people in your organization to make all this work. Whatever your approach,\nunderstand that you need to be clear in articulating the responsibilities of your people in a\nmicroservices world, and also be clear why those responsibilities are important to you.\nThis can help you see what your skill gaps might be, and think about how to close them.\nFor many people, this will be a pretty scary journey. Just remember that without people on\nboard, any change you might want to make could be doomed from the start.\nSummary\nConway’s law highlights the perils of trying to enforce a system design that doesn’t match\nthe organization. This leads us to trying to align service ownership to colocated teams,\nwhich themselves are aligned around the same bounded contexts of the organization.\nWhen the two are not in alignment, we get tension points as outlined throughout this\nchapter. By recognizing the link between the two, we’ll make sure the system we are\ntrying to build makes sense for the organization we’re building it for.\nSome of what we covered here touched on the challenges of working with organizations at\nscale. However, there are other technical considerations that we need to worry about when\nour systems start to grow beyond a few discrete services. We’ll address those next.\n4\n And we all know Windows Vista was quite error-prone!\nChapter 11. \nMicroservices at Scale\nWhen you’re dealing with nice, small, book-sized examples, everything seems simple. But\nthe real world is a more complex space. What happens when our microservice\narchitectures grow from simpler, more humble beginnings to something more complex?\nWhat happens when we have to handle failure of multiple separate services or manage\nhundreds of services? What are some of the coping patterns when you have more\nmicroservices than people? Let’s find out.\nFailure Is Everywhere\nWe understand that things can go wrong. Hard disks can fail. Our software can crash. And\nas anyone who has read the \nfallacies of distributed computing\n can tell you, we know that\nthe network is unreliable. We can do our best to try to limit the causes of failure, but at a\ncertain scale, failure becomes inevitable. Hard drives, for example, are more reliable now\nthan ever before, but they’ll break eventually. The more hard drives you have, the higher\nthe likelihood of failure for an individual unit; failure becomes a statistical certainty at\nscale.\nEven for those of us not thinking at extreme scale, if we can embrace the possibility of\nfailure we will be better off. For example, if we can handle the failure of a service\ngracefully, then it follows that we can also do in-place upgrades of a service, as a planned\noutage is much easier to deal with than an unplanned one.\nWe can also spend a bit less of our time trying to stop the inevitable, and a bit more of our\ntime dealing with it gracefully. I’m amazed at how many organizations put processes and\ncontrols in place to try to stop failure from occurring, but put little to no thought into\nactually making it easier to recover from failure in the first place.\nBaking in the assumption that everything can and will fail leads you to think differently\nabout how you solve problems.\nI saw one example of this thinking while spending some time on the Google campus many\nyears ago. In the reception area of one of the buildings in Mountain View \nwas an old rack\nof machines, there as a sort of exhibit. I noticed a couple of things. First, these servers\nweren’t in server enclosures, they were just bare motherboards slotted into the rack. The\nmain thing I noticed, though, was that the hard drives were attached by velcro. I asked one\nof the Googlers why that was. “Oh,” he said, “the hard drives fail so much we don’t want\nthem screwed in. We just rip them out, throw them in the bin, and velcro in a new one.”\nSo let me repeat: at scale, even if you buy the best kit, the most expensive hardware, you\ncannot avoid the fact that things can and will fail. Therefore, you need to assume failure\ncan happen. If you build this thinking into everything you do, and plan for failure, you can\nmake different trade-offs. If you know your system can handle the fact that a server can\nand will fail, why bother spending much on it at all? Why not use a bare motherboard with\ncheaper components (and some velcro) like Google did, rather than worrying too much\nabout the resiliency of a single node?\nHow Much Is Too Much?\nWe touched on the topic of cross-functional requirements in \nChapter 7\n. Understanding\ncross-functional requirements is all about considering aspects like durability of data,\navailability of services, throughput, and acceptable latency of services. Many of the\ntechniques covered in this chapter and elsewhere talk about approaches to implement\nthese requirements, but only you know exactly what the requirements themselves might\nbe.\nHaving an autoscaling system capable of reacting to increased load or failure of individual\nnodes might be fantastic, but could be overkill for a reporting system that only needs to\nrun twice a month, where being down for a day or two isn’t that big of a deal. Likewise,\nfiguring out how to do blue/green deployments to eliminate downtime of a service might\nmake sense for your online ecommerce system, but for your corporate intranet knowledge\nbase it’s probably a step too far.\nKnowing how much failure you can tolerate, or how fast your system needs to be, is\ndriven by the users of your system. That in turn will help you understand which\ntechniques will make the most sense for you. That said, your users won’t always be able to\narticulate what the exact requirements are. So you need to ask questions to help extract the\nright information, and help them understand the relative costs of providing different levels\nof service.\nAs I mentioned previously, these cross-functional requirements can vary from service to\nservice, but I would suggest defining some general cross-functionals and then overriding\nthem for particular use cases. When it comes to considering if and how to scale out your\nsystem to better handle load or failure, start by trying to understand the following\nrequirements:\nResponse time/latency\nHow long should various operations take? It can be useful here to measure this with\ndifferent numbers of users to understand how increasing load will impact the\nresponse time. Given the nature of networks, you’ll always have outliers, so setting\ntargets for a given percentile of the responses monitored can be useful. The target\nshould also include the number of concurrent connections/users you will expect your\nsoftware to handle. So you might say, “We expect the website to have a 90th-\npercentile response time of 2 seconds when handling 200 concurrent connections per\nsecond.”\nAvailability\nCan you expect a service to be down? Is this considered a 24/7 service? Some people\nlike to look at periods of acceptable downtime when measuring availability, but how\nuseful is this to someone calling your service? I should either be able to rely on your\nservice responding or not. Measuring periods of downtime is really more useful from\na historical reporting angle.\nDurability of data\nHow much data loss is acceptable? How long should data be kept for? This is highly\nlikely to change on a case-by-case basis. For example, you might choose to keep user\nsession logs for a year or less to save space, but your financial transaction records\nmight need to be kept for many years.\nOnce you have these requirements in place, you’ll want a way to systematically measure\nthem on an ongoing basis. You may decide to make use of performance tests, for example,\nto ensure your system meets acceptable performance targets, but you’ll want to make sure\nyou are monitoring these stats in production as well!",12387
65-Architectural Safety Measures.pdf,65-Architectural Safety Measures,"Degrading Functionality\nAn essential part of building a resilient system, especially when your functionality is\nspread over a number of different microservices that may be up or down, is the ability to\nsafely degrade functionality. Let’s imagine a standard web page on our ecommerce site. To\npull together the various parts of that website, we might need several microservices to play\na part. One microservice might display the details about the album being offered for sale.\nAnother might show the price and stock level. And we’ll probably be showing shopping\ncart contents too, which may be yet another microservice. Now if one of those services is\ndown, and that results in the whole web page being unavailable, then we have arguably\nmade a system that is less resilient than one that requires only one service to be available.\nWhat we need to do is understand the impact of each outage, and work out how to\nproperly degrade functionality. If the shopping cart service is unavailable, we’re probably\nin a lot of trouble, but we could still show the web page with the listing. Perhaps we just\nhide the shopping cart or replace it with an icon saying “Be Back Soon!”\nWith a single, monolithic application, we don’t have many decisions to make. System\nhealth is binary. But with a microservice architecture, we need to consider a much more\nnuanced situation. The right thing to do in any situation is often not a technical decision.\nWe might know what is technically possible when the shopping cart is down, but unless\nwe understand the business context we won’t understand what action we should be taking.\nFor example, perhaps we close the entire site, still allow people to browse the catalog of\nitems, or replace the part of the UI containing the cart control with a phone number for\nplacing an order. But for every customer-facing interface that uses multiple microservices,\nor every microservice that depends on multiple downstream collaborators, you need to ask\nyourself, “What happens if this is down?” and know what to do.\nBy thinking about the criticality of each of our capabilities in terms of our cross-functional\nrequirements, we’ll be much better positioned to know what we can do. Now let’s\nconsider some things we can do from a technical point of view to make sure that when\nfailure occurs we can handle it gracefully.\nArchitectural Safety Measures\nThere are a few patterns, which collectively I refer to as \narchitectural safety measures\n, that\nwe can make use of to ensure that if something does go wrong, it doesn’t cause nasty\nripple-out effects. These are points it is essential you understand, and should strongly\nconsider standardizing in your system to ensure that one bad citizen doesn’t bring the\nwhole world crashing down around your ears. In a moment, we’ll take a look at a few key\nsafety measures you should consider, but before we do, I’d like to share a brief story to\noutline the sort of thing that can go wrong.\nI was a technical lead on a project where we were building an online classified ads\nwebsite. The website itself handled fairly high volumes, and generated a good deal of\nincome for the business. Our core application handled some display of classified ads itself,\nand also proxied calls to other services that provided different types of products, as shown\nin \nFigure 11-1\n. This is actually an example of a \nstrangler application\n, where a new system\nintercepts calls made to legacy applications and gradually replaces them altogether. As\npart of this project, we were partway through retiring the older applications. We had just\nmoved over the highest volume and biggest earning product, but much of the rest of the\nads were still being served by a number of older applications. In terms of both the number\nof searches and the money made by these applications, there was a very long tail.\nFigure 11-1. \nA classified ads website \nstrangling\n older legacy applications\nOur system had been live for a while and was behaving very well, handling a not\ninsignificant load. At that time we must have been handling around 6,000–7,000 requests\nper second during peak, and although most of that was very heavily cached by reverse\nproxies sitting in front of our application servers, the searches for products (the most\nimportant aspect of the site) were mostly uncached and required a full server round-trip.\nOne morning, just before we hit our daily lunchtime peak, the system started behaving\nslowly, then gradually started failing. We had some level of monitoring on our new core\napplication, enough to tell us that each of our application nodes was hitting a 100% CPU\nspike, well above the normal levels even at peak. In a short period of time, the entire site\nwent down.\nWe managed to track down the culprit and bring the site back up. It turned out one of the\ndownstream ad systems, one of the oldest and least actively maintained, had started\nresponding very slowly. Responding very slowly is one of the worst failure modes you can\nexperience. If a system is just not there, you find out pretty quickly. When it’s just \nslow\n,\nyou end up waiting around for a while before giving up. But whatever the cause of the\nfailure, we had created a system that was vulnerable to a cascading failure. A downstream\nservice, over which we had little control, was able to take down our whole system.\nWhile one team looked at the problems with the downstream system, the rest of us started\nlooking at what had gone wrong in our application. We found a few problems. We were\nusing an HTTP connection pool to handle our downstream connections. The threads in the\npool itself had timeouts configured for how long they would wait when making the\ndownstream HTTP call, which is good. The problem was that the workers were all taking\na while to time out due to the slow downstream system. While they were waiting, more\nrequests went to the pool asking for worker threads. With no workers available, these\nrequests themselves hung. It turned out the connection pool library we were using did\nhave a timeout for waiting for workers, but this was \ndisabled by default\n! This led to a huge\nbuild-up of blocked threads. Our application normally had 40 concurrent connections at\nany given time. In the space of five minutes, this situation caused us to peak at around 800\nconnections, bringing the system down.\nWhat was worse was that the downstream service we were talking to represented\nfunctionality that less than 5% of our customer base used, and generated even less revenue\nthan that. When you get down to it, we discovered the hard way that systems that just act\nslow are \nmuch\n harder to deal with than systems that just fail fast. In a distributed system,\nlatency kills.\nEven if we’d had the timeouts on the pool set correctly, we were also sharing a single\nHTTP connection pool for all outbound requests. This meant that one slow service could\nexhaust the number of available workers all by itself, even if everything else was healthy.\nLastly, it was clear that the downstream service in question wasn’t healthy, but we kept\nsending traffic its way. In our situation, this meant we were actually making a bad\nsituation worse, as the downstream service had no chance to recover. We ended up\nimplementing three fixes to avoid this happening again: getting our \ntimeouts\n right,\nimplementing \nbulkheads\n to separate out different connection pools, and implementing a\ncircuit breaker\n to avoid sending calls to an unhealthy system in the first place.",7582
66-The Antifragile Organization.pdf,66-The Antifragile Organization,"The Antifragile Organization\nIn his book \nAntifragile\n (Random House), Nassim Taleb talks about things that actually\nbenefit from failure and disorder. Ariel Tseitlin used this concept to coin the concept of the\nantifragile organization\n in regards to how Netflix operates.\nThe scale at which Netflix operates is well known, as is the fact that Netflix is based\nentirely on the AWS infrastructure. These two factors mean that it has to embrace failure\nwell. Netflix goes beyond that by actually \ninciting\n failure to ensure that its systems are\ntolerant of it.\nSome organizations would be happy with \ngame days\n, where failure is simulated by\nsystems being switched off and having the various teams react. During my time at Google,\nthis was a fairly common occurrence for various systems, and I certainly think that many\norganizations could benefit from having these sorts of exercises regularly. Google goes\nbeyond simple tests to mimic server failure, and as part of its annual \nDiRT (Disaster\nRecovery Test) exercises\n it has simulated large-scale disasters such as earthquakes. Netflix\nalso takes a more aggressive approach, by writing programs that cause failure and running\nthem in production on a daily basis.\nThe most famous of these programs is the Chaos Monkey, which during certain hours of\nthe day will turn off random machines. Knowing that this can and will happen in\nproduction means that the developers who create the systems really have to be prepared\nfor it. The Chaos Monkey is just one part of Netflix’s Simian Army of failure bots. The\nChaos Gorilla is used to take out an entire availability center (the AWS equivalent of a\ndata center), whereas the Latency Monkey simulates slow network connectivity between\nmachines. Netflix has made these tools available under an \nopen source license\n. For many,\nthe ultimate test of whether your system really is robust might be unleashing your very\nown Simian Army on your production infrastructure.\nEmbracing and inciting failure through software, and building systems that can handle it,\nis only part of what Netflix does. It also understands the importance of learning from the\nfailure when it occurs, and adopting a blameless culture when mistakes do happen.\nDevelopers are further empowered to be part of this learning and evolving process, as each\ndeveloper is also responsible for managing his or her production services.\nBy causing failure to happen, and building for it, Netflix has ensured that the systems it\nhas scale better, and better support the needs of its customers.\nNot everyone needs to go to the sorts of extremes that Google or Netflix do, but it is\nimportant to understand the mindset shift that is required with distributed systems. Things\nwill fail. The fact that your system is now spread across multiple machines (which can and\nwill fail) across a network (which will be unreliable) can actually make your system more\nvulnerable, not less. So regardless of whether you’re trying to provide a service at the\nscale of Google or Netflix, preparing yourself for the sorts of failure that happen with\nmore distributed architectures is pretty important. So what do we need to do to handle\nfailure in our systems?",3244
67-Circuit Breakers.pdf,67-Circuit Breakers,"Timeouts\nTimeouts are something it is easy to overlook, but in a downstream system they are\nimportant to get right. How long can I wait before I can consider a downstream system to\nactually be down?\nWait too long to decide that a call has failed, and you can slow the whole system down.\nTime out too quickly, and you’ll consider a call that might have worked as failed. Have no\ntimeouts at all, and a downstream system being down could hang your whole system.\nPut timeouts on all out-of-process calls, and pick a default timeout for everything. Log\nwhen timeouts occur, look at what happens, and change them accordingly.\nCircuit Breakers\nIn your own home, circuit breakers exist to protect your electrical devices from spikes in\nthe power. If a spike occurs, the circuit breaker gets blown, protecting your expensive\nhome appliances. You can also manually disable a circuit breaker to cut the power to part\nof your home, allowing you to work safely on the electrics. Michael Nygard’s book\nRelease It!\n (Pragmatic Programmers) shows how the same idea can work wonders as a\nprotection mechanism for our software.\nConsider the story I shared just a moment ago. The downstream legacy ad application was\nresponding very slowly, before eventually returning an error. Even if we’d got the\ntimeouts right, we’d be waiting a long time before we got the error. And then we’d try it\nagain the next time a request came in, and wait. It’s bad enough that the downstream\nservice is malfunctioning, but it’s making us go slow too.\nWith a circuit breaker, after a certain number of requests to the downstream resource have\nfailed, the circuit breaker is blown. All further requests fail fast while the circuit breaker is\nin its blown state. After a certain period of time, the client sends a few requests through to\nsee if the downstream service has recovered, and if it gets enough healthy responses it\nresets the circuit breaker. You can see an overview of this process in \nFigure 11-2\n.\nHow you implement a circuit breaker depends on what a \nfailed\n request means, but when\nI’ve implemented them for HTTP connections I’ve taken failure to mean either a timeout\nor a 5XX HTTP return code. In this way, when a downstream resource is down, or timing\nout, or returning errors, after a certain threshold is reached we automatically stop sending\ntraffic and start failing fast. And we can automatically start again when things are healthy.\nGetting the settings right can be a little tricky. You don’t want to blow the circuit breaker\ntoo readily, nor do you want to take too long to blow it. Likewise, you really want to make\nsure that the downstream service is healthy again before sending traffic. As with timeouts,\nI’d pick some sensible defaults and stick with them everywhere, then change them for\nspecific cases.\nWhile the circuit breaker is blown, you have some options. One is to queue up the requests\nand retry them later on. For some use cases, this might be appropriate, especially if you’re\ncarrying out some work as part of a asynchronous job. If this call is being made as part of\na synchronous call chain, however, it is probably better to fail fast. This could mean\npropagating an error up the call chain, or a more subtle degrading of functionality.\nIf we have this mechanism in place (as with the circuit breakers in our home), we could\nuse them manually to make it safer to do our work. For example, if we wanted to take a\nmicroservice down as part of routine maintenance, we could manually blow all the circuit\nbreakers of the dependent systems so they fail fast while the microservice is offline. Once\nit’s back, we can reset the circuit breakers and everything should go back to normal.\nFigure 11-2. \nAn overview of circuit breakers",3787
68-Bulkheads.pdf,68-Bulkheads,"Bulkheads\nIn another pattern from \nRelease It!\n, Nygard introduces the concept of a \nbulkhead\n as a way\nto isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be\nsealed off to protect the rest of the ship. So if the ship springs a leak, you can close the\nbulkhead doors. You lose part of the ship, but the rest of it remains intact.\nIn software architecture terms, there are lots of different bulkheads we can consider.\nReturning to my own experience, we actually missed the chance to implement a bulkhead.\nWe should have used different connection pools for each downstream connection. That\nway, if one connection pool gets exhausted, the other connections aren’t impacted, as we\nsee in \nFigure 11-3\n. This would ensure that if a downstream service started behaving\nslowly in the future, only that one connection pool would be impacted, allowing other\ncalls to proceed as normal.\nFigure 11-3. \nUsing a connection pool per downstream service to provide \nbulkheads\nSeparation of concerns can also be a way to implement bulkheads. By teasing apart\nfunctionality into separate microservices, we reduce the chance of an outage in one area\naffecting another.\nLook at all the aspects of your system that can go wrong, both inside your microservices\nand between them. Do you have bulkheads in place? I’d suggest starting with separate\nconnection pools for each downstream connection at the very least. You may want to go\nfurther, however, and consider using circuit breakers too.\nWe can think of our circuit breakers as an automatic mechanism to seal a bulkhead, to not\nonly protect the consumer from the downstream problem, but also to potentially protect\nthe downstream service from more calls that may be having an adverse impact. Given the\nperils of cascading failure, I’d recommend mandating circuit breakers for all your\nsynchronous downstream calls. You don’t have to write your own, either. Netflix’s \nHystrix\nlibrary\n is a JVM circuit breaker abstraction that comes with some powerful monitoring,\nbut other implementations exist for different technology stacks, such as \nPolly \nfor .NET\n, or\nthe \ncircuit_breaker mixin for Ruby\n.\nIn many ways, bulkheads are the most important of these three patterns. Timeouts and\ncircuit breakers help you free up resources when they are becoming constrained, but\nbulkheads can ensure they don’t become constrained in the first place. Hystrix allows you,\nfor example, to implement bulkheads that actually reject requests in certain conditions to\nensure that resources don’t become even more saturated; this is known as \nload shedding\n.\nSometimes rejecting a request is the best way to stop an important system from becoming\noverwhelmed and being a bottleneck for multiple upstream services.",2811
69-Idempotency.pdf,69-Idempotency,"Isolation\nThe more one service depends on another being up, the more the health of one impacts the\nability of the other to do its job. If we can use integration techniques that allow a\ndownstream server to be offline, upstream services are less likely to be affected by\noutages, planned or unplanned.\nThere is another benefit to increasing isolation between services. When services are\nisolated from each other, much less coordination is needed between service owners. The\nless coordination needed between teams, the more autonomy those teams have, as they are\nable to operate and evolve their services more freely.\nIdempotency\nIn \nidempotent\n operations, the outcome doesn’t change after the first application, even if\nthe operation is subsequently applied multiple times. If operations are idempotent, we can\nrepeat the call multiple times without adverse impact. This is very useful when we want to\nreplay messages that we aren’t sure have been processed, a common way of recovering\nfrom error.\nLet’s consider a simple call to add some points as a result of one of our customers placing\nan order. We might make a call with the sort of payload shown in \nExample 11-1\n.\nExample 11-1. \nCrediting points to an account\n<credit>\n  \n<amount>\n100\n</amount>\n  \n<forAccount>\n1234\n</account>\n</credit>\nIf this call is received multiple times, we would add 100 points multiple times. As it\nstands, therefore, this call is not idempotent. With a bit more information, though, we\nallow the points bank to make this call idempotent, as shown in \nExample 11-2\n.\nExample 11-2. \nAdding more information to the points credit to make it idempotent\n<credit>\n  \n<amount>\n100\n</amount>\n  \n<forAccount>\n1234\n</account>\n  \n<reason>\n    \n<forPurchase>\n4567\n</forPurchase>\n  \n</reason>\n</credit>\nNow we know that this credit relates to a specific order, \n4567\n. Assuming that we could\nreceive only one credit for a given order, we could apply this credit again without\nincreasing the overall number of points.\nThis mechanism works just as well with event-based collaboration, and can be especially\nuseful if you have multiple instances of the same type of service subscribing to events.\nEven if we store which events have been processed, with some forms of asynchronous\nmessage delivery there may be small windows where two workers can see the same\nmessage. By processing the events in an idempotent manner, we ensure this won’t cause\nus any issues.\nSome people get quite caught up with this concept, and assume it means that subsequent\ncalls with the same parameters can’t have \nany\n impact, which then leaves us in an\ninteresting position. We really would still like to record the fact that a call was received in\nour logs, for example. We want to record the response time of the call and collect this data\nfor monitoring. The key point here is that it is the underlying business operation that we\nare considering idempotent, not the entire state of the system.\nSome of the HTTP verbs, such as GET and PUT, are defined in the HTTP specification to\nbe idempotent, but for that to be the case, they rely on your service handling these calls in\nan idempotent manner. If you start making these verbs nonidempotent, but callers think\nthey can safely execute them repeatedly, you may get yourself into a mess. Remember,\njust because you’re using HTTP as an underlying protocol doesn’t mean you get\neverything for free!",3472
70-Load Balancing.pdf,70-Load Balancing,"Scaling\nWe scale our systems in general for one of two reasons. First, to help deal with failure: if\nwe’re worried that something will fail, then having more of it will help, right? Second, we\nscale for performance, either in terms of handling more load, reducing latency, or both.\nLet’s look at some common scaling techniques we can use and think about how they apply\nto microservice architectures.\nGo Bigger\nSome operations can just benefit from more grunt. Getting a bigger box with faster CPU\nand better I/O can often improve latency and throughput, allowing you to process more\nwork in less time. However, this form of scaling, often called \nvertical scaling\n, can be\nexpensive — sometimes one big server can cost more than two smaller servers with the\nsame combined raw power, especially when you start getting to really big machines.\nSometimes our software itself cannot do much with the extra resources available to it.\nLarger machines often just give us more CPU cores, but not enough of our software is\nwritten to take advantage of them. The other problem is that this form of scaling may not\ndo much to improve our server’s resiliency if we only have one of them! Nonetheless, this\ncan be a good quick win, especially if you’re using a virtualization provider that lets you\nresize machines easily.\nSplitting Workloads\nAs outlined in \nChapter 6\n, having a single microservice per host is certainly preferable to a\nmultiservice-per-host model. Initially, however, many people decide to coexist multiple\nmicroservices on one box to keep costs down or to simplify host management (although\nthat is an arguable reason). As the microservices are independent processes that\ncommunicate over the network, it should be an easy task to then move them onto their\nown hosts to improve throughput and scaling. This can also increase the resiliency of the\nsystem, as a single host outage will impact a reduced number of microservices.\nOf course, we could also use the need for increased scale to split an existing microservice\ninto parts to better handle the load. As a simplistic example, let’s imagine that our\naccounts service provides the ability to create and manage individual customers’ financial\naccounts, but also exposes an API for running queries to generate reports. This query\ncapability places a significant load on the system. The query capacity is considered\nnoncritical, as it isn’t needed to keep orders flowing in during the day. The ability to\nmanage the financial records for our customers \nis\n critical, though, and we can’t afford for\nit to be down. By splitting these two capabilities into separate services, we reduce the load\non the critical accounts service, and introduce a new accounts reporting service that is\ndesigned not only with querying in mind (perhaps using some of the techniques we\noutlined in \nChapter 4\n, but also as a noncritical system doesn’t need to be deployed in as\nresilient a way as the core accounts service.\nSpreading Your Risk\nOne way to scale for resilience is to ensure that you don’t put all your eggs in one basket.\nA simplistic example of this is making sure that you don’t have multiple services on one\nhost, where an outage would impact multiple services. But let’s consider what \nhost\n means.\nIn most situations nowadays, a \nhost\n is actually a virtual concept. So what if I have all of\nmy services on different hosts, but all those hosts are actually virtual hosts, running on the\nsame physical box? If that box goes down, I could lose multiple services. Some\nvirtualization platforms enable you to ensure that your hosts are distributed across\nmultiple different physical boxes to reduce this chance.\nFor internal virtualization platforms, it is a common practice to have the virtual machine’s\nroot partition mapped to a single SAN (storage area network). If that SAN goes down, it\ncan take down all connected VMs. SANs are big, expensive, and designed not to fail. That\nsaid, I have had big expensive SANs fail on me at least twice in the last 10 years, and each\ntime the results were fairly serious.\nAnother common form of separation to reduce failure is to ensure that not all your services\nare running in a single rack in the data center, or that your services are distributed across\nmore than one data center. If you’re using an underlying service provider, it is important to\nknow if a service-level agreement (SLA) is offered and plan accordingly. If you need to\nensure your services are down for no more than four hours every quarter, but your hosting\nprovider can only guarantee a downtime of eight hours per quarter, you have to either\nchange the SLA, or come up with an alternative solution.\nAWS, for example, is split into regions, which you can think of as distinct clouds. Each\nregion is in turn split into two or more availability zones (AZs). AZs are AWS’s equivalent\nof a data center. It is essential to have services distributed across multiple availability\nzones, as AWS does not offer any guarantees about the availability of a single node, or\neven an entire availability zone. For its compute service, it offers only a 99.95% uptime\nover a given monthly period of the region as a whole, so you’ll want to distribute your\nworkloads across multiple availability zones inside a single region. For some people, this\nisn’t good enough, and instead they run their services across multiple regions too.\nIt should be noted, of course, that because providers give you an SLA \nguarantee\n, they will\ntend to limit their liability! If them missing their targets costs you customers and a large\namount of money, you might find yourself searching through contracts to see if you can\nclaw anything back from them. Therefore, I would strongly suggest you understand the\nimpact of a supplier failing in its obligations to you, and work out if you need to have a\nplan B (or C) in your pocket. More than one client I’ve worked with has had a disaster\nrecovery hosting platform with a different supplier, for example, to ensure they weren’t\ntoo vulnerable to the mistakes of one company.\nLoad Balancing\nWhen you need your service to be resilient, you want to avoid single points of failure. For\na typical microservice that exposes a synchronous HTTP endpoint, the easiest way to\nachieve this is to have multiple hosts running your microservice instance, sitting behind a\nload balancer, as shown in \nFigure 11-4\n. To consumers of the microservice, you don’t know\nif you are talking to one microservice instance or a hundred.\nFigure 11-4. \nAn example of a load balancing approach to scale the number of customer service instances\nLoad balancers come in all shapes and sizes, from big and expensive hardware appliances\nto software-based load balancers like mod_proxy. They all share some key capabilities.\nThey distribute calls sent to them to one or more instances based on some algorithm,\nremove instances when they are no longer healthy, and hopefully add them back in when\nthey are.\nSome load balancers provide useful features. A common one is \nSSL termination\n, where\ninbound HTTPS connections to the load balancer are transformed to HTTP connections\nonce they hit the instance itself. Historically, the overhead of managing SSL was\nsignificant enough that having a load balancer handle this process for you was fairly\nuseful. Nowadays, this is as much about simplifying the set-up of the individual hosts\nrunning the instance. The point of using HTTPS, though, is to ensure that the requests\naren’t vulnerable to a man-in-the-middle attack, as we discussed in \nChapter 9\n, so if we use\nSSL termination, we are potentially exposing ourselves somewhat. One mitigation is to\nhave all the instances of the microservice inside a single VLAN, as we see in \nFigure 11-5\n.\nA VLAN is a virtual local area network, that is isolated in such a way that requests from\noutside it can come only via a router, and in this case our router is also our SSL-\nterminating load balancer. The only communication to the microservice from outside the\nVLAN comes over HTTPS, but internally everything is HTTP.\nFigure 11-5. \nUsing HTTPS termination at the load balancer with a VLAN for improved security\nAWS provides HTTPS-terminating load balancers in the form of ELBs (elastic load\nbalancers) and you can use its security groups or virtual private clouds (VPCs) to\nimplement the VLAN. Otherwise, software like mod_proxy can play a similar role as a\nsoftware load balancer. Many organizations have hardware load balancers, which can be\ndifficult to automate. In such circumstances I have found myself advocating for software\nload balancers sitting \nbehind\n the hardware load balancers to allow teams the freedom to\nreconfigure these as required. You do want to watch for the fact that all too often the\nhardware load balancers themselves are single points of failure! Whatever approach you\ntake, when considering the configuration of a load balancer, treat it as you treat the\nconfiguration of your service: make sure it is stored in version control and can be applied\nautomatically.\nLoad balancers allow us to add more instances of our microservice in a way that is\ntransparent to any service consumers. This gives us an increased ability to handle load,\nand also reduce the impact of a single host failing. However, many, if not most, of your\nmicroservices will have some sort of persistent data store, probably a database sitting on a\ndifferent machine. If we have multiple microservice instances on different machines, but\nonly a single host running the database instance, our database is still a single source of\nfailure. We’ll talk about patterns to handle this shortly.",9758
71-Caching in HTTP.pdf,71-Caching in HTTP,"Worker-Based Systems\nLoad balancing isn’t the only way to have multiple instances of your service share load\nand reduce fragility. Depending on the nature of the operations, a worker-based system\ncould be just as effective. Here, a collection of instances all work on some shared backlog\nof work. This could be a number of Hadoop processes, or perhaps a number of listeners to\na shared queue of work. These types of operations are well suited to batch work or\nasynchronous jobs. Think of tasks like image thumbnail processing, sending email, or\ngenerating reports.\nThe model also works well for \npeaky\n load, where you can spin up additional instances on\ndemand to match the load coming in. As long as the work queue itself is resilient, this\nmodel can be used to scale both for improved throughput of work, but also for improved\nresiliency — the impact of a worker failing (or not being there) is easy to deal with. Work\nwill take longer, but nothing gets lost.\nI’ve seen this work well in organizations where there is lots of unused compute capacity at\ncertain times of day. For example, overnight you might not need as many machines to run\nyour ecommerce system, so you can temporarily use them to run workers for a reporting\njob instead.\nWith worker-based systems, although the workers themselves don’t need to be that\nreliable, the system that contains the work to be done does. You could handle this by\nrunning a persistent message broker, for example, or perhaps a system like Zookeeper. The\nbenefit here is that if we use existing software for this purpose, someone has done much of\nthe hard work for us. However, we still need to know how to set up and maintain these\nsystems in a resilient fashion.\nStarting Again\nThe architecture that gets you started may not be the architecture that keeps you going\nwhen your system has to handle very different volumes of load. As Jeff Dean said in his\npresentation “Challenges in Building Large-Scale Information Retrieval Systems”\n(WSDM 2009 conference), you should “design for ~10× growth, but plan to rewrite\nbefore ~100×.” At certain points, you need to do something pretty radical to support the\nnext level of growth.\nRecall the story of Gilt, which we touched on in \nChapter 6\n. A simple monolithic Rails\napplication did well for Gilt for two years. Its business became increasingly successful,\nwhich meant more customers and more load. At a certain tipping point, the company had\nto redesign the application to handle the load it was seeing.\nA redesign may mean splitting apart an existing monolith, as it did for Gilt. Or it might\nmean picking new data stores that can handle the load better, which we’ll look at in a\nmoment. It could also mean adopting new techniques, such as moving from synchronous\nrequest/response to event-based systems, adopting new deployment platforms, changing\nwhole technology stacks, or everything in between.\nThere is a danger that people will see the need to rearchitect when certain scaling\nthresholds are reached as a reason to build for massive scale from the beginning. This can\nbe disastrous. At the start of a new project, we often don’t know exactly what we want to\nbuild, nor do we know if it will be successful. We need to be able to rapidly experiment,\nand understand what capabilities we need to build. If we tried building for massive scale\nup front, we’d end up front-loading a huge amount of work to prepare for load that may\nnever come, while diverting effort away from more important activities, like\nunderstanding if anyone will want to actually use our product. Eric Ries tells the story of\nspending six months building a product that no one ever downloaded. He reflected that he\ncould have put up a link on a web page that 404’d when people clicked on it to see if there\nwas any demand, spent six months on the beach instead, and learned just as much!\nThe need to change our systems to deal with scale isn’t a sign of failure. It is a sign of\nsuccess.\nScaling Databases\nScaling stateless microservices is fairly straightforward. But what if we are storing data in\na database? We’ll need to know how to scale that too. Different types of databases provide\ndifferent forms of scaling, and understanding what form suits your use case best will\nensure you select the right database technology from the beginning.\nAvailability of Service Versus Durability of Data\nStraight off, it is important to separate the concept of availability of the service from the\ndurability of the data itself. You need to understand that these are two different things, and\nas such they will have different solutions.\nFor example, I could store a copy of all data written to my database in a resilient\nfilesystem. If the database goes down, my data isn’t lost, as I have a copy, but the database\nitself isn’t available, which may make my microservice unavailable too. A more common\nmodel would be using a standby. All data written to the primary database gets copied to\nthe standby replica database. If the primary goes down, my data is safe, but without a\nmechanism to either bring it back up or promote the replica to the primary, we don’t have\nan available database, even though our data is safe.\nScaling for Reads\nMany services are read-mostly. Think of a catalog service that stores information for the\nitems we have for sale. We add records for new items on a fairly irregular basis, and it\nwouldn’t at all be surprising if we get more than 100 reads of our catalog’s data for every\nwrite. Happily, scaling for reads is much easier than scaling for writes. Caching of data\ncan play a large part here, and we’ll discuss that in more depth shortly. Another model is\nto make use of \nread replicas\n.\nIn a relational database management system (RDBMS) like MySQL or Postgres, data can\nbe copied from a primary node to one or more replicas. This is often done to ensure that a\ncopy of our data is kept safe, but we can also use it to distribute our reads. A service could\ndirect all writes to the single primary node, but distribute reads to one or more read\nreplicas, as we see in \nFigure 11-6\n. The replication from the primary database to the\nreplicas happens at some point after the write. This means that with this technique reads\nmay sometimes see \nstale\n data until the replication has completed. Eventually the reads\nwill see the consistent data. Such a setup is called \neventually consistent\n, and if you can\nhandle the temporary inconsistency it is a fairly easy and common way to help scale\nsystems. We’ll look into this in more depth shortly when we look at the CAP theorem.\nFigure 11-6. \nUsing read replicas to scale reads\nYears ago, using read replicas to scale was all the rage, although nowadays I would\nsuggest you look to caching first, as it can deliver much more significant improvements in\nperformance, often with less work.\nScaling for Writes\nReads are comparatively easy to scale. What about writes? One approach is to use\nsharding\n. With sharding, you have multiple database nodes. You take a piece of data to be\nwritten, apply some hashing function to the key of the data, and based on the result of the\nfunction learn where to send the data. To pick a very simplistic (and actually bad)\nexample, imagine that customer records A–M go to one database instance, and N–Z\nanother. You can manage this yourself in your application, but some databases, like\nMongo, handle much of it for you.\nThe complexity with sharding for writes comes from handling queries. Looking up an\nindividual record is easy, as I can just apply the hashing function to find which instance\nthe data should be on, and then retrieve it from the correct shard. But what about queries\nthat span the data in multiple nodes — for example, finding all the customers who are over\n18? If you want to query all shards, you either need to query each individual shard and\njoin in memory, or have an alternative read store where both data sets are available. Often\nquerying across shards is handled by an asynchronous mechanism, using cached results.\nMongo uses map/reduce jobs, for example, to perform these queries.\nOne of the questions that emerges with sharded systems is, what happens if I want to add\nan extra database node? In the past, this would often require significant downtime —\nespecially for large clusters — as you might have to take the entire database down and\nrebalance the data. More recently, more systems support adding extra shards to a live\nsystem, where the rebalancing of data happens in the background; Cassandra, for example,\nhandles this very well. Adding shards to an existing cluster isn’t for the faint of heart,\nthough, so make sure you test this thoroughly.\nSharding for writes may scale for write volume, but may not improve resiliency. If\ncustomer records A–M always go to Instance X, and Instance X is unavailable, access to\nrecords A–M can be lost. Cassandra offers additional capabilities here, where we can\nensure that data is replicated to multiple nodes in a \nring\n (Cassandra’s term for a collection\nof Cassandra nodes).\nAs you may have inferred from this brief overview, scaling databases for writes are where\nthings get very tricky, and where the capabilities of the various databases really start to\nbecome differentiated. I often see people changing database technology when they start\nhitting limits on how easily they can scale their existing write volume. If this happens to\nyou, buying a bigger box is often the quickest way to solve the problem, but in the\nbackground you might want to look at systems like Cassandra, Mongo, or Riak to see if\ntheir alternative scaling models might offer you a better long-term solution.\nShared Database Infrastructure\nSome types of databases, such as the traditional RDBMS, separate the concept of the\ndatabase itself and the schema. This means one running database could host multiple,\nindependent schemas, one for each microservice. This can be very useful in terms of\nreducing the number of machines we need to run our system, but we are introducing a\nsignificant single point of failure. If this database infrastructure goes down, it can impact\nmultiple microservices at once, potentially resulting in a catastrophic outage. If you are\nrunning this sort of setup, make sure you consider the risks. And be very sure that the\ndatabase itself is as resilient as it can be.\nCQRS\nThe Command-Query Responsibility Segregation (CQRS) pattern refers to an alternate\nmodel for storing and querying information. With normal databases, we use one system\nfor performing modifications to data and querying the data. With CQRS, part of the\nsystem deals with commands, which capture requests to modify state, while another part\nof the system deals with queries.\nCommands come in requesting changes in state. These commands are validated, and if\nthey work, they will be applied to the model. Commands should contain information about\ntheir intent. They can be processed synchronously or asynchronously, allowing for\ndifferent models to handle scaling; we could, for example, just queue up inbound requests\nand process them later.\nThe key takeaway here is that the internal models used to handle commands and queries\nare themselves completely separate. For example, I might choose to handle and process\ncommands as events, perhaps just storing the list of commands in a data store (a process\nknown as \nevent sourcing\n). My query model could query an event store and create\nprojections from stored events to assemble the state of domain objects, or could just pick\nup a feed from the command part of the system to update a different type of store. In many\nways, we get the same benefits of read replicas that we discussed earlier, without the\nrequirement that the backing store for the replicas be the same as the data store used to\nhandle data modifications.\nThis form of separation allows for different types of scaling. The command and query\nparts of our system could live in different services, or on different hardware, and could\nmake use of radically different types of data store. This can unlock a large number of ways\nto handle scale. You could even support different types of read format by having multiple\nimplementations of the query piece, perhaps supporting a graph-based representation of\nyour data, or a key/value-based form of your data.\nBe warned, however: this sort of pattern is quite a shift away from a model where a single\ndata store handles all our CRUD operations. I’ve seen more than one experienced\ndevelopment team struggle to get this pattern right!\nCaching\nCaching is a commonly used performance optimization whereby the previous result of\nsome operation is stored, so that subsequent requests can use this stored value rather than\nspending time and resources recalculating the value. More often than not, caching is about\neliminating needless round-trips to databases or other services to serve results faster. Used\nwell, it can yield huge performance benefits. The reason that HTTP scales so well in\nhandling large numbers of requests is that the concept of caching is built in.\nEven with a simple monolithic web application, there are quite a few choices as to where\nand how to cache. With a microservice architecture, where each service is its own source\nof data and behavior, we have many more choices to make about where and how to cache.\nWith a distributed system, we typically think of caching either on the client side or on the\nserver side. But which is best?\nClient-Side, Proxy, and Server-Side Caching\nIn client-side caching, the client stores the cached result. The client gets to decide when\n(and if) it goes and retrieves a fresh copy. Ideally, the downstream service will provide\nhints to help the client understand what to do with the response, so it knows when and if to\nmake a new request. With proxy caching, a proxy is placed between the client and the\nserver. A great example of this is using a reverse proxy or content delivery network\n(CDN). With server-side caching, the server handles caching responsibility, perhaps\nmaking use of a system like Redis or Memcache, or even a simple in-memory cache.\nWhich one makes the most sense depends on what you are trying to optimize. Client-side\ncaching can help reduce network calls drastically, and can be one of the fastest ways of\nreducing load on a downstream service. In this case, the client is in charge of the caching\nbehavior, and if you want to make changes to how caching is done, rolling out changes to\na number of consumers could be difficult. Invalidation of stale data can also be trickier,\nalthough we’ll discuss some coping mechanisms for this in a moment.\nWith proxy caching, everything is opaque to both the client and server. This is often a very\nsimple way to add caching to an existing system. If the proxy is designed to cache generic\ntraffic, it can also cache more than one service; a common example is a reverse proxy like\nSquid or Varnish, which can cache any HTTP traffic. Having a proxy between the client\nand server does introduce additional network hops, although in my experience it is very\nrare that this causes problems, as the performance optimizations resulting from the\ncaching itself outweigh any additional network costs.\nWith server-side caching, everything is opaque to the clients; they don’t need to worry\nabout anything. With a cache near or inside a service boundary, it can be easier to reason\nabout things like invalidation of data, or track and optimize cache hits. In a situation where\nyou have multiple types of clients, a server-side cache could be the fastest way to improve\nperformance.\nFor every public-facing website I’ve worked on, we’ve ended up doing a mix of all three\napproaches. But for more than one distributed system, I’ve gotten away with no caching at\nall. But it all comes down to knowing what load you need to handle, how fresh your data\nneeds to be, and what your system can do right now. Knowing that you have a number of\ndifferent tools at your disposal is just the beginning.\nCaching in HTTP\nHTTP provides some really useful controls to help us cache either on the client side or\nserver side, which are worth understanding even if you aren’t using HTTP itself.\nFirst, with HTTP, we can use \ncache-control\n directives in our responses to clients. These\ntell clients if they should cache the resource at all, and if so how long they should cache it\nfor in seconds. We also have the option of setting an \nExpires\n header, where instead of\nsaying how long a piece of content can be cached for, we specify a time and date at which\na resource should be considered stale and fetched again. The nature of the resources you\nare sharing determines which one is most likely to fit. Standard static website content like\nCSS or images often fit well with a simple \ncache-control\n time to live (TTL). On the\nother hand, if you know in advance when a new version of a resource will be updated,\nsetting an \nExpires\n header will make more sense. All of this is very useful in stopping a\nclient from even needing to make a request to the server in the first place.\nAside from \ncache-control\n and \nExpires\n, we have another option in \nour arsenal of HTTP\ngoodies: Entity Tags, or ETags. An ETag is used to determine if the value of a resource has\nchanged. If I update a customer record, the URI to the resource is the same, but the value\nis different, so I would expect the ETag to change. This becomes powerful when we’re\nusing what is called a \nconditional GET\n. When making a GET request, we can specify\nadditional headers, telling the service to send us the resource only if some criteria are met.\nFor example, let’s imagine we fetch a customer record, and its ETag comes back as\no5t6fkd2sa\n. Later on, perhaps because a \ncache-control\n directive has told us the resource\nshould be considered stale, we want to make sure we get the latest version. When issuing\nthe subsequent GET request, we can pass in a \nIf-None-Match: o5t6fkd2sa\n. This tells the\nserver that we want the resource at the specified URI, unless it already matches this ETag\nvalue. If we already have the up-to-date version, the service sends us a \n304 Not Modified\nresponse, telling us we have the latest version. If there is a newer version available, we get\na \n200 OK\n with the changed resource, and a new ETag for the resource.\nThe fact that these controls are built into such a widely used specification means we get to\ntake advantage of a lot of preexisting software that handles the caching for us. Reverse\nproxies like Squid or Varnish can sit transparently on the network between client and\nserver, storing and expiring cached content as required. These systems are geared toward\nserving huge numbers of concurrent requests very fast, and are a standard way of scaling\npublic-facing websites. CDNs like AWS’s CloudFront or Akamai can ensure that requests\nare routed to caches near the calling client, making sure that traffic doesn’t go halfway\nround the world when it needs to. And more prosaically, HTTP client libraries and client\ncaches can handle a lot of this work for us.\nETags, \nExpires\n, and \ncache-control\n can overlap a bit, and if you aren’t careful you can\nend up giving conflicting information if you decide to use all of them! For a more in-depth\ndiscussion of the various merits, take a look at the book \nREST In Practice\n (O’Reilly) or\nread section 13 of the \nHTTP 1.1 specification\n, which describes how both clients and\nservers are supposed to implement these various controls.\nWhether you decide to use HTTP as an interservice protocol, caching at the client and\nreducing the need for round-trips to the client is well worth it. If you decide to pick a\ndifferent protocol, understand when and how you can provide hints to the client to help it\nunderstand how long it can cache for.",20060
72-Autoscaling.pdf,72-Autoscaling,"Caching for Writes\nAlthough you’ll find yourself using caching for reads more often, there are some use cases\nwhere caching for writes make sense. For example, if you make use of a write-behind\ncache, you can write to a local cache, and at some later point the data will be flushed to a\ndownstream source, probably the canonical source of data. This can be useful when you\nhave bursts of writes, or when there is a good chance that the same data will be written\nmultiple times. When used to buffer and potentially batch writes, write-behind caches can\nbe a useful further performance optimization.\nWith a write-behind cache, if the buffered writes are suitably persistent, even if the\ndownstream service is unavailable we could queue up the writes and send them through\nwhen it is available again.\nCaching for Resilience\nCaching can be used to implement resiliency in case of failure. With client-side caching, if\nthe downstream service is unavailable, the client could decide to simply use cached but\npotentially stale data. We could also use something like a reverse proxy to serve up stale\ndata. For some systems, being available even with stale data is better than not returning a\nresult at all, but that is a judgment call you’ll have to make. Obviously, if we don’t have\nthe requested data in the cache, then we can’t do much to help, but there are ways to\nmitigate this.\nA technique I saw used at the \nGuardian\n, and subsequently elsewhere, was to crawl the\nexisting \nlive\n site periodically to generate a static version of the website that could be\nserved in the event of an outage. Although this crawled version wasn’t as fresh as the\ncached content served from the live system, in a pinch it could ensure that a version of the\nsite would get displayed.\nHiding the Origin\nWith a normal cache, if a request results in a cache miss, the request goes on to the origin\nto fetch the fresh data with the caller blocking, waiting on the result. In the normal course\nof things, this is to be expected. But if we suffer a massive cache miss, perhaps because an\nentire machine (or group of machines) that provide our cache fail, a large number of\nrequests will hit the origin.\nFor those services that serve up highly cachable data, it is common for the origin itself to\nbe scaled to handle only a fraction of the total traffic, as most requests get served out of\nmemory by the caches that sit in front of the origin. If we suddenly get a thundering herd\ndue to an entire cache region vanishing, our origin could be pummelled out of existence.\nOne way to protect the origin in such a situation is never to allow requests to go to the\norigin in the first place. Instead, the origin itself populates the cache asynchronously when\nneeded, as shown in \nFigure 11-7\n. If a cache miss is caused, this triggers an event that the\norigin can pick up on, alerting it that it needs to repopulate the cache. So if an entire shard\nhas vanished, we can rebuild the cache in the background. We could decide to block the\noriginal request waiting for the region to be repopulated, but this could cause contention\non the cache itself, leading to further problems. It’s more likely if we are prioritizing\nkeeping the system stable that we would fail the original request, but it would fail fast.\nFigure 11-7. \nHiding the origin from the client and populating the cache asynchronously\nThis sort of approach may not make sense for some situations, but it can be a way to\nensure the system remains up when parts of it fail. By failing requests fast, and ensuring\nwe don’t take up resources or increase latency, we avoid a failure in our cache from\ncascading downstream and give ourselves a chance to recover.\nKeep It Simple\nBe careful about caching in too many places! The more caches between you and the\nsource of fresh data, the more stale the data can be, and the harder it can be to determine\nthe freshness of the data that a client eventually sees. This can be especially problematic\nwith a microservice architecture where you have multiple services involved in a call chain.\nAgain, the more caching you have, the harder it will be to assess the freshness of any piece\nof data. So if you think a cache is a good idea, keep it simple, stick to one, and think\ncarefully before adding more!\nCache Poisoning: A Cautionary Tale\nWith caching we often think that if we get it wrong the worst thing that can happen is we\nserve stale data for a bit. But what happens if you end up serving stale data forever?\nEarlier I mentioned the project I worked on where we were using a strangler application to\nhelp intercept calls to multiple legacy systems with a view of incrementally retiring them.\nOur application operated effectively as a proxy. Traffic to our application was routed\nthrough to the legacy application. On the way back, we did a few housekeeping things; for\nexample, we made sure that the results from the legacy application had proper HTTP\ncache headers applied.\nOne day, shortly after a normal routine release, something odd started happening. A bug\nhad been introduced whereby a small subset of pages were falling through a logic\ncondition in our cache header insertion code, resulting in us not changing the header at all.\nUnfortunately, this downstream application had also been changed sometime previously to\ninclude an \nExpires: Never\n HTTP header. This hadn’t had any effect earlier, as we were\noverriding this header. Now we weren’t.\nOur application made heavy use of Squid to cache HTTP traffic, and we noticed the\nproblem quite quickly as we were seeing more requests bypassing Squid itself to hit our\napplication servers. We fixed the cache header code and pushed out a release, and also\nmanually cleared the relevant region of the Squid cache. However, that wasn’t enough.\nAs I mentioned earlier, you can cache in multiple places. When it comes to serving up\ncontent to users of a public-facing web application, you could have multiple caches\nbetween you and your customer. Not only might you be fronting your website with\nsomething like a CDN, but some ISPs make use of caching. Can you control those caches?\nAnd even if you could, there is one cache that you have little control over: the cache in a\nuser’s browser.\nThose pages with \nExpires: Never\n stuck in the caches of many of our users, and would\nnever be invalidated until the cache became full or the user cleaned them out manually.\nClearly we couldn’t make either thing happen; our only option was to change the URLs of\nthese pages so they were refetched.\nCaching can be very powerful indeed, but you need to understand the full path of data that\nis cached from source to destination to really appreciate its complexities and what can go\nwrong.\nAutoscaling\nIf you are lucky enough to have fully automatable provisioning of virtual hosts, and can\nfully automate the deployment of your microservice instances, then you have the building\nblocks to allow you to automatically scale your microservices.\nFor example, you could also have the scaling triggered by well-known trends. You might\nknow that your system’s peak load is between 9 a.m. and 5 p.m., so you bring up\nadditional instances at 8:45 a.m., and turn them off at 5:15 p.m.. If you’re using something\nlike AWS (which has very good support for autoscaling built in), turning off instances you\ndon’t need any longer will help save money. You’ll need data to understand how your load\nchanges over time, from day to day, week to week. Some businesses have obvious\nseasonal cycles too, so you may need data going back a fair way to make proper judgment\ncalls.\nOn the other hand, you could be reactive, bringing up additional instances when you see\nan increase in load or an instance failure, and remove instances when you no longer\nneeded them. Knowing how fast you can scale up once you spot an upward trend is key. If\nyou know you’ll only get a couple of minutes’ notice about an increase in load, but scaling\nup will take you at least 10 minutes, you know you’ll need to keep extra capacity around\nto bridge this gap. Having a good suite of load tests is almost essential here. You can use\nthem to test your autoscaling rules. If you don’t have tests that can reproduce different\nloads that will trigger scaling, then you’re only going to find out in production if you got\nthe rules wrong. And the consequences of failure aren’t great!\nA news site is a great example of a type of business where you may want a mix of\npredictive and reactive scaling. On the last news site I worked on, we saw very clear daily\ntrends, with views climbing from the morning to lunchtime and then starting to decline.\nThis pattern was repeated day in, day out, with traffic less pronounced at the weekend.\nThat gave you a fairly clear trend that could drive proactive scaling up (and down) of\nresources. On the other hand, a big news story would cause an unexpected spike, requiring\nmore capacity at often short notice.\nI actually see autoscaling used much more for handling failure of instances than for\nreacting to load conditions. AWS lets you specify rules like “There should be at least 5\ninstances in this group,” so if one goes down a new one is automatically launched. I’ve\nseen this approach lead to a fun game of whack-a-mole when someone forgets to turn off\nthe rule and then tries to take down the instances for maintenance, only to see them keep\nspinning up!\nBoth reactive and predictive scaling are very useful, and can help you be much more cost\neffective if you’re using a platform that allows you to pay only for the computing\nresources you use. But they also require careful observation of the data available to you.\nI’d suggest using autoscaling for failure conditions first while you collect the data. Once\nyou want to start scaling for load, make sure you are very cautious about scaling down too\nquickly. In most situations, having more computing power at your hands than you need is\nmuch better than not having enough!",10069
73-CAP Theorem.pdf,73-CAP Theorem,"CAP Theorem\nWe’d like to have it all, but unfortunately we know we can’t. And when it comes to\ndistributed systems like those we build using microservice architectures, we even have a\nmathematical proof that tells us we can’t. You may well have heard about the CAP\ntheorem, especially in discussions about the merits of various different types of data\nstores. At its heart it tells us that in a distributed system, we have three things we can trade\noff against each other: \nconsistency\n, \navailability\n, and \npartition tolerance\n. Specifically, the\ntheorem tells us that we get to keep two in a failure mode.\nConsistency is the system characteristic by which I will get the same answer if I go to\nmultiple nodes. Availability means that every request receives a response. Partition\ntolerance is the system’s ability to handle the fact that communication between its parts is\nsometimes impossible.\nSince Eric Brewer published his original conjecture, the idea has gained a mathematical\nproof. I’m not going to dive into the math of the proof itself, as not only is this not that\nsort of book, but I can also guarantee that I would get it wrong. Instead, let’s use some\nworked examples that will help us understand that under it all, the CAP theorem is a\ndistillation of a very logical set of reasoning.\nWe’ve already talked about some simple database scaling techniques. Let’s use one of\nthese to probe the ideas behind the CAP theorem. Let’s imagine that our inventory service\nis deployed across two separate data centers, as shown in \nFigure 11-8\n. Backing our service\ninstance in each data center is a database, and these two databases talk to each other to try\nto synchronize data between them. Reads and writes are done via the local database node,\nand replication is used to synchronize the data between the nodes.\nNow let’s think about what happens when something fails. Imagine that something as\nsimple as the network link between the two data centers stops working. The\nsynchronization at this point fails. Writes made to the primary database in DC1 will not\npropagate to DC2, and vice versa. Most databases that support these setups also support\nsome sort of queuing technique to ensure that we can recover from this afterward, but\nwhat happens in the meantime?\nFigure 11-8. \nUsing multiprimary replication to share data between two database nodes",2394
74-DNS.pdf,74-DNS,"Sacrificing Consistency\nLet’s assume that we don’t shut the inventory service down entirely. \nIf I make a change\nnow to the data in DC1, the database in DC2 doesn’t see it. This means any requests made\nto our inventory node in DC2 see potentially stale data. In other words, our system is still\navailable\n in that both nodes are able to serve requests, and we have kept the system\nrunning despite the \npartition\n, but we have lost \nconsistency\n. This is often called a \nAP\nsystem. We don’t get to keep all three.\nDuring this partition, if we keep accepting writes then we accept the fact that at some\npoint in the future they have to be resynchronized. The longer the partition lasts, the more\ndifficult this resynchronization can become.\nThe reality is that even if we don’t have a network failure between our database nodes,\nreplication of data is not instantaneous. As touched on earlier, systems that are happy to\ncede consistency to keep partition tolerance and availability are said to be \neventually\nconsistent\n; that is, we expect at some point in the future that all nodes will see the updated\ndata, but it won’t happen at once so we have to live with the possibility that users see old\ndata.\nSacrificing Availability\nWhat happens if we need to keep consistency and want to drop something else instead?\nWell, to keep consistency, each database node needs to know the copy of the data it has is\nthe same as the other database node. Now in the partition, if the database nodes can’t talk\nto each other, they cannot coordinate to ensure consistency. We are unable to guarantee\nconsistency, so our only option is to refuse to respond to the request. In other words, we\nhave sacrificed availability. Our system is consistent and partition tolerant, or CP. In this\nmode our service would have to work out how to degrade functionality until the partition\nis healed and the database nodes can be resynchronized.\nConsistency across multiple nodes is really hard. There are few things (perhaps nothing)\nharder in distributed systems. Think about it for a moment. Imagine I want to read a\nrecord from the local database node. How do I know it is up to date? I have to go and ask\nthe other node. But I also have to ask that database node to not allow it to be updated\nwhile the read completes; in other words, I need to initiate a transactional read across\nmultiple database nodes to ensure consistency. But in general people don’t do\ntransactional reads, do they? Because transactional reads are slow. They require locks. A\nread can block an entire system up. All consistent systems require some level of locking to\ndo their job.\nAs we’ve already discussed, distributed systems have to expect failure. Consider our\ntransactional read across a set of consistent nodes. I ask a remote node to lock a given\nrecord while the read is initiated. I complete the read, and ask the remote node to release\nits lock, but now I can’t talk to it. What happens now? Locks are really hard to get right\neven in a single process system, and are significantly more difficult to implement well in a\ndistributed system.\nRemember when we talked about distributed transactions in \nChapter 5\n? The core reason\nthey are challenging is because of this problem with ensuring consistency across multiple\nnodes.\nGetting multinode consistency right is so hard that I would strongly, \nstrongly\n suggest that\nif you need it, don’t try to invent it yourself. Instead, pick a data store or lock service that\noffers these characteristics. Consul, for example, which we’ll discuss shortly, implements\na strongly consistent key/value store designed to share configuration between multiple\nnodes. Along with “Friends don’t let friends write their own crypto” should go “Friends\ndon’t let friends write their own distributed consistent data store.” If you think you need to\nwrite your own CP data store, read all the papers on the subject first, then get a PhD, and\nthen look forward to spending a few years getting it wrong. Meanwhile, I’ll be using\nsomething off the shelf that does it for me, or more likely trying \nreally hard\n to build\neventually consistent AP systems instead.\nSacrificing Partition Tolerance?\nWe get to pick two, right? So we’ve got our eventually consistent AP system. We have our\nconsistent, but hard to build and scale, CP system. Why not a CA system? Well, how can\nwe sacrifice partition tolerance? If our system has no partition tolerance, it can’t run over a\nnetwork. In other words, it needs to be a single process operating locally. CA systems\ndon’t exist in distributed systems.\nAP or CP?\nWhich is right, AP or CP? Well, the reality is \nit depends\n. As the people building the\nsystem, we know the trade-off exists. We know that AP systems scale more easily and are\nsimpler to build, and we know that a CP system will require more work due to the\nchallenges in supporting distributed consistency. But we may not understand the business\nimpact of this trade-off. For our inventory system, if a record is out of date by five\nminutes, is that OK? If the answer is yes, an AP system might be the answer. But what\nabout the balance held for a customer in a bank? Can that be out of date? Without\nknowing the context in which the operation is being used, we can’t know the right thing to\ndo. Knowing about the CAP theorem just helps you understand that this trade-off exists\nand what questions to ask.\nIt’s Not All or Nothing\nOur system as a whole doesn’t need to be either AP or CP. Our catalog could be AP, as we\ndon’t mind too much about a stale record. But we might decide that our inventory service\nneeds to be CP, as we don’t want to sell a customer something we don’t have and then\nhave to apologize later.\nBut individual services don’t even need to be CP or AP.\nLet’s think about our points balance service, where we store records of how many loyalty\npoints our customers have built up. We could decide that we don’t care if the balance we\nshow for a customer is stale, but that when it comes to updating a balance we need it to be\nconsistent to ensure that customers don’t use more points than they have available. Is this\nmicroservice CP, or AP, or is it both? Really, what we have done is push the trade-offs\naround the CAP theorem down to individual service capabilities.\nAnother complexity is that neither consistency nor availability is all or nothing. Many\nsystems allow us a far more nuanced trade-off. For example, with Cassandra I can make\ndifferent trade-offs for individual calls. So if I need strict consistency, I can perform a read\nthat blocks until all replicas have responded confirming the value is consistent, or until a\nspecific quorum of replicas have responded, or even just a single node. Obviously, if I\nblock waiting for all replicas to report back and one of them is unavailable, I’ll be\nblocking for a long time. But if I am satisfied with just a simple quorum of nodes\nreporting back, I can accept some lack of consistency to be less vulnerable to a single\nreplica being unavailable.\nYou’ll often see posts about people \nbeating\n the CAP theorem. They haven’t. What they\nhave done is create a system where some capabilities are CP, and some are AP. The\nmathematical proof behind the CAP theorem holds. Despite many attempts at school, I’ve\nlearned that you don’t beat math.\nAnd the Real World\nMuch of what we’ve talked about is the electronic world — bits and bytes stored in\nmemory. We talk about consistency in an almost child-like fashion; we imagine that within\nthe scope of the system we have created, we can stop the world and have it all make sense.\nAnd yet so much of what we build is just a reflection of the real world, and we don’t get to\ncontrol that, do we?\nLet’s revisit our inventory system. This maps to real-world, physical items. We keep a\ncount in our system of how many albums we have. At the start of the day we had 100\ncopies of \nGive Blood\n by The Brakes. We sold one. Now we have 99 copies. Easy, right?\nBy what happens if when the order was being sent out, someone knocks a copy of the\nalbum onto the floor and it gets stepped on and broken? What happens now? Our systems\nsay 99, but there are 98 copies on the shelf.\nWhat if we made our inventory system AP instead, and occasionally had to contact a user\nlater on and tell him that one of his items is actually out of stock? Would that be the worst\nthing in the world? It would certainly be much easier to build, scale, and ensure it is\ncorrect.\nWe have to recognize that no matter how consistent our systems might be in and of\nthemselves, they cannot know everything that happens, especially when we’re keeping\nrecords of the real world. This is one of the main reasons why AP systems end up being\nthe right call in many situations. Aside from the complexity of building CP systems, they\ncan’t fix all our problems anyway.\nService Discovery\nOnce you have more than a few microservices lying around, your attention inevitably\nturns to knowing where on earth everything is. Perhaps you want to know what is running\nin a given environment so you know what you should be monitoring. Maybe it’s as simple\nas knowing where your accounts service is so that those microservices that use it know\nwhere to find it. Or perhaps you just want to make it easy for developers in your\norganization to know what APIs are available so they don’t reinvent the wheel. Broadly\nspeaking, all of these use cases fall under the banner of \nservice discovery\n. And as always\nwith microservices, we have quite a few different options at our disposal for dealing with\nit.\nAll of the solutions we’ll look at handle things in two parts. First, they provide some\nmechanism for an instance to register itself and say, “I’m here!” Second, they provide a\nway to find the service once it’s registered. Service discovery gets more complicated,\nthough, when we are considering an environment where we are constantly destroying and\ndeploying new instances of services. Ideally, we’d want whatever solution we pick to cope\nwith this.\nLet’s look at some of the most common solutions to service delivery and consider our\noptions.\nDNS\nIt’s nice to start simple. DNS lets us associate a name with the IP address of one or more\nmachines. We could decide, for example, that our accounts service is always found at\naccounts.musiccorp.com\n. We would then have that entry point to the IP address of the host\nrunning that service, or perhaps have it resolve to a load balancer that is distributing load\nacross a number of instances. This means we’d have to handle updating these entries as\npart of deploying our service.\nWhen dealing with instances of a service in different environments, I have seen a\nconvention-based domain template work well. For example, we might have a template\ndefined as \n<servicename>-<environment>.musiccorp.com\n, giving us entries like\naccounts-uat.musiccorp.com\n or \naccounts-dev.musiccorp.com\n.\nA more advanced way of handling different environments is to have different domain\nname servers for different environments. So I could assume that \naccounts.musiccorp.com\nis where I always find the accounts service, but it could resolve to different hosts\ndepending on where I do the lookup. If you already have your environments sitting in\ndifferent network segments and are comfortable with managing your own DNS servers\nand entries, this could be quite a neat solution, but it is a lot of work if you aren’t getting\nother benefits from this setup.\nDNS has a host of advantages, the main one being it is such a well-understood and well-\nused standard that almost any technology stack will support it. Unfortunately, while a\nnumber of services exist for managing DNS inside an organization, few of them seem\ndesigned for an environment where we are dealing with highly disposable hosts, making\nupdating DNS entries somewhat painful. Amazon’s Route53 service does a pretty good\njob of this, but I haven’t seen a self-hosted option that is as good yet, although (as we’ll\ndiscuss shortly) Consul may help us here. Aside from the problems in updating DNS\nentries, the DNS specification itself can cause us some issues.\nDNS entries for domain names have a \ntime to live\n (TTL). This is how long a client can\nconsider the entry fresh. When we want to change the host to which the domain name\nrefers, we update that entry, but we have to assume that clients will be holding on to the\nold IP for \nat least\n as long as the TTL states. DNS entries can get cached in multiple places\n(even the JVM will cache DNS entries unless you tell it not to), and the more places they\nare cached in, the more stale the entry can be.\nOne way to work around this problem is to have the domain name entry for your service\npoint to a load balancer, which in turn points to the instances of your service, as shown in\nFigure 11-9\n. When you deploy a new instance, you can take the old one out of the load-\nbalancer entry and add the new one. Some people use DNS round-robining, where the\nDNS entries themselves refer to a group of machines. This technique is extremely\nproblematic, as the client is hidden from the underlying host, and therefore cannot easily\nstop routing traffic to one of the hosts should it become sick.\nFigure 11-9. \nUsing DNS to resolve to a load balancer to avoid stale DNS entries\nAs mentioned, DNS is well understood and widely supported. But it does have one or two\ndownsides. I would suggest investigating whether it is a good fit for you before picking\nsomething more complex. For a situation where you have only single nodes, having DNS\nrefer directly to hosts is probably fine. But for those situations where you need more than\none instance of a host, have DNS entries resolve to load balancers that can handle putting\nindividual hosts into and out of service as appropriate.",13967
75-Index.pdf,75-Index,"Dynamic Service Registries\nThe downsides of DNS as a way of finding nodes in a highly dynamic environment have\nled to a number of alternative systems, most of which involve the service registering itself\nwith some central registry, which in turn offers the ability to look up these services later\non. Often, these systems do more than just providing service registration and discovery,\nwhich may or may not be a good thing. This is a crowded field, so we’ll just look at a few\noptions to give you a sense of what is available.\nZookeeper\nZookeeper\n was originally developed as part of the Hadoop project. It is used for an almost\nbewildering array of use cases, including configuration management, synchronizing data\nbetween services, leader election, message queues, and (usefully for us) as a naming\nservice.\nLike many similar types of systems, Zookeeper relies on running a number of nodes in a\ncluster to provide various guarantees. This means you should expect to be running at least\nthree Zookeeper nodes. Most of the smarts in Zookeeper are around ensuring that data is\nreplicated safely between these nodes, and that things remain consistent when nodes fail.\nAt its heart, Zookeeper provides a hierarchical namespace for storing information. Clients\ncan insert new nodes in this hierarchy, change them, or query them. Furthermore, they can\nadd watches to nodes to be told when they change. This means we could store the\ninformation about where our services are located in this structure, and as a client be told\nwhen they change. Zookeeper is often used as a general configuration store, so you could\nalso store service-specific configuration in it, allowing you to do tasks like dynamically\nchanging log levels or turning off features of a running system. Personally, I tend to shy\naway from the use of systems like Zookeeper as a configuration source, as I think it can\nmake it harder to reason about the behavior of a given service.\nZookeeper itself is fairly generic in what it offers, which is why it is used for so many use\ncases. You can think of it just as a replicated tree of information that you can be alerted\nabout when it changes. This means that you’ll typically build things on top of it to suit\nyour particular use case. Luckily, client libraries exist for most languages out there.\nIn the grand scheme of things, Zookeeper could be considered \nold\n by now, and doesn’t\nprovide us that much functionality out of the box to help with service discovery compared\nto some of the newer alternatives. That said, it is certainly tried and tested, and widely\nused. The underlying algorithms Zookeeper implements are quite hard to get right. I know\none database vendor, for example, that was using Zookeeper just for leader election in\norder to ensure that a primary node got properly promoted during failure conditions. The\nclient felt that Zookeeper was too heavyweight and spent a long time ironing out bugs in\nits own implementation of the PAXOS algorithm to replace what Zookeeper did. People\noften say you shouldn’t write your own cryptography libraries. I’d extend that by saying\nyou shouldn’t write your own distributed coordination systems either. There is a lot to be\nsaid for using existing stuff that just works.\nConsul\nLike Zookeeper, \nConsul\n supports both configuration management and service discovery.\nBut it goes further than Zookeeper in providing more support for these key use cases. For\nexample, it exposes an HTTP interface for service discovery, and one of Consul’s killer\nfeatures is that it actually provides a DNS server out of the box; specifically, it can serve\nSRV records, which give you both an IP and port for a given name. This means if part of\nyour system uses DNS already and can support SRV records, you can just drop in Consul\nand start using it without any changes to your existing system.\nConsul also builds in other capabilities that you might find useful, such as the ability to\nperform health checks on nodes. This means that Consul could well overlap the\ncapabilities provided by other dedicated monitoring tools, although you would more likely\nuse Consul as a source of this information and then pull it into a more comprehensive\ndashboard or alerting system. Consul’s highly fault-tolerant design and focus on handling\nsystems that make heavy use of ephemeral nodes does make me wonder, though, if it may\nend up replacing systems like Nagios and Sensu for some use cases.\nConsul uses a RESTful HTTP interface for everything from registering a service, querying\nthe key/value store, or inserting health checks. This makes integration with different\ntechnology stacks very straightforward. One of the other things I really like about Consul\nis that the team behind it has split out the underlying cluster management piece. Serf,\nwhich Consul sits on top of, handles detection of nodes in a cluster, failure management,\nand alerting. Consul then adds service discovery and configuration management. This\nseparation of concerns appeals to me, which should be no surprise to you given the themes\nthat run through this book!\nConsul is very new, and given the complexity of the algorithms it uses, this would\nnormally make me hesitant in recommending it for such an important job. That said,\nHashicorp, the team behind it, certainly has a great track record in creating very useful\nopen source technology (in the form of both Packer and Vagrant), the project is being\nactively developed, and I’ve spoken to a few people who are happily using it in\nproduction. Given that, I think it is well worth a look.\nEureka\nNetflix’s open source \nEureka system\n bucks the trend of systems like Consul and\nZookeeper in that it doesn’t also try to be a general-purpose configuration store. It is\nactually very targeted in its use case.\nEureka also provides basic load-balancing capabilities in that it can support basic round-\nrobin lookup of service instances. It provides a REST-based endpoint so you can write\nyour own clients, or you can use its own Java client. The Java client provides additional\ncapabilities, such as health checking of instances. Obviously if you bypass Eureka’s own\nclient and go directly to the REST endpoint, you’re on your own there.\nBy having the clients deal with service discovery directly, we avoid the need for a separate\nprocess. However, you do require that every client implement service discovery. Netflix,\nwhich standardizes on the JVM, achieves this by having all clients use Eureka. If you’re in\na more polyglot environment, this may be more of a challenge.\nRolling Your Own\nOne approach I have used myself and seen done elsewhere is to roll your own system. On\none project we were making heavy use of AWS, which offers the ability to add tags to\ninstances. When launching service instances, I would apply tags to help define what the\ninstance was and what it was used for. These allowed for some rich metadata to be\nassociated with a given host, for example:\nservice = accounts\nenvironment = production\nversion = 154\nI could then use the AWS APIs to query all the instances associated with a given AWS\naccount to find machines I cared about. Here, AWS itself is handling the storing of the\nmetadata associated with each instance, and providing us with the ability to query it. I then\nbuilt command-line tools for interacting with these instances, and making dashboards for\nstatus monitoring becomes fairly easy, especially if you adopt the idea of having each\nservice instance exposing health check details.\nThe last time I did this we didn’t go as far as having services use the AWS APIs to find\ntheir service dependencies, but there is no reason why you couldn’t. Obviously, if you\nwant upstream services to be alerted when the location of a downstream service changes,\nyou’re on your own.\nDon’t Forget the Humans!\nThe systems we’ve looked at so far make it easy for a service instance to register itself and\nlook up other services it needs to talk to. But as humans we sometimes want this\ninformation too. Whatever system you pick, make sure you have tools available that let\nyou build reports and dashboards on top of these registries to create displays for humans,\nnot just for computers.\nDocumenting Services\nBy decomposing our systems into finer-grained microservices, we’re hoping to expose lots\nof seams in the form of APIs that people can use to do many, hopefully wonderful, things.\nIf you get your discovery right, we know where things are. But how do we know what\nthose things do, or how to use them? One option is obviously to have documentation about\nthe APIs. Of course, documentation can often be out of date. Ideally, we’d ensure that our\ndocumentation is always up to date with the microservice API, and make it easy to see this\ndocumentation when we know where a service endpoint is. Two different pieces of\ntechnology, Swagger and HAL, try to make this a reality, and both are worth looking at.\nSwagger\nSwagger lets you describe your API in order to generate a very nice web UI that allows\nyou to view the documentation and interact with the API via a web browser. The ability to\nexecute requests is very nice: you can define POST templates, for example, making it\nclear what sort of content the server expects.\nTo do all of this, Swagger needs the service to expose a sidecar file matching the Swagger\nformat. Swagger has a number of libraries for different languages that does this for you.\nFor example, for Java you can annotate methods that match your API calls, and the file\ngets generated for you.\nI like the end-user experience that Swagger gives you, but it does little for the incremental\nexploration concept at the heart of hypermedia. Still, it’s a pretty nice way to expose\ndocumentation about your services.\nHAL and the HAL Browser\nBy itself, the \nHypertext Application Language (HAL)\n is a standard that describes\nstandards for hypermedia controls that we expose. As we covered in \nChapter 4\n,\nhypermedia controls are the means by which we allow clients to progressively explore our\nAPIs to use our service’s capabilities in a less coupled fashion than other integration\ntechniques. If you decide to adopt HAL’s hypermedia standard, then not only can you\nmake use of a wide number of client libraries for consuming the API (at the time of\nwriting, the HAL wiki listed 50 supporting libraries for a number of different languages),\nbut you can also make use of the HAL browser, which gives you a way to explore the API\nvia a web browser.\nLike Swagger, this UI can be used not only to act as living documentation, but also to\nexecute calls against the service itself. Executing calls isn’t quite as slick, though.\nWhereas with Swagger you can define templates to do things like issue a POST request,\nwith HAL you’re more on your own. The flipside to this is that the inherent power of\nhypermedia controls lets you much more effectively explore the API exposed by the\nservice, as you can follow links around very easily. It turns out that web browsers are\npretty good at that sort of thing!\nUnlike with Swagger, all the information needed to drive this documentation and sandbox\nis embedded in the hypermedia controls. This is a double-edged sword. If you are already\nusing hypermedia controls, it takes little effort to expose a HAL browser and have clients\nexplore your API. However, if you aren’t using hypermedia, you either can’t use HAL or\nhave to retrofit your API to use hypermedia, which is likely to be an exercise that breaks\nexisting consumers.\nThe fact that HAL also describes a hypermedia standard with some supporting client\nlibraries is an added bonus, and I suspect is a big reason why I’ve seen more uptake of\nHAL as a way of documenting APIs than Swagger for those people already using\nhypermedia controls. If you’re using hypermedia, my recommendation is to go with HAL\nover Swagger. But if you’re using hypermedia and can’t justify the switch, I’d definitely\nsuggest giving Swagger a go.\nThe Self-Describing System\nDuring the early evolution of SOA, standards like Universal Description, Discovery, and\nIntegration (UDDI) emerged to help people make sense of what services were running.\nThese approaches were fairly heavyweight, which led to alternative techniques to try to\nmake sense of our systems. Martin Fowler discussed the concept of the \nhumane registry\n,\nwhere a much more lightweight approach is simply to have a place where humans can\nrecord information about the services in the organization in something as basic as a wiki.\nGetting a picture of our system and how it is behaving is important, especially when we’re\nat scale. We’ve covered a number of different techniques that will help us gain\nunderstanding directly from our system. By tracking the health of our downstream\nservices together with correlation IDs to help us see call chains, we can get real data in\nterms of how our services interrelate. Using service discovery systems like Consul, we can\nsee where our microservices are running. HAL lets us see what capabilities are being\nhosted on any given endpoint, while our health-check pages and monitoring systems let us\nknow the health of both the overall system and individual services.\nAll of this information is available programatically. All of this data allows us to make our\nhumane registry more powerful than a simple wiki page that will no doubt get out of date.\nInstead, we should use it to harness and display all the information our system will be\nemitting. By creating custom dashboards, we can pull together the vast array of\ninformation that is available to help us make sense of our ecosystem.\nBy all means, start with something as simple as a static web page or wiki that perhaps\nscrapes in a bit of data from the live system. But look to pull in more and more\ninformation over time. Making this information readily available is a key tool to managing\nthe emerging complexity that will come from running these systems at scale.\nSummary\nAs a design approach, microservices are still fairly young, so although we have some\nnotable experiences to draw upon, I’m sure the next few years will yield more useful\npatterns in handling them at scale. Nonetheless, I hope this chapter has outlined some\nsteps you can take on your journey to microservices at scale that will hold you in good\nstead.\nIn addition to what I have covered here, I recommend Michael Nygard’s excellent book\nRelease It!\n. In it he shares a collection of stories about system failure and some patterns to\nhelp deal with it well. The book is well worth a read (in fact, I would go so far as to say it\nshould be considered essential reading for anyone building systems at scale).\nWe’ve covered quite a lot of ground, and we’re nearing the end. In our next and final\nchapter, we will look to pull everything back together and summarize what we have\nlearned in the book overall.\nChapter 12. \nBringing It All Together\nWe’ve covered a fair amount in the previous chapters, from what microservices are to how\nto define their boundaries, and from integration technology to concerns about security and\nmonitoring. And we even found time to work out how the role of the architect fits in.\nThere is a lot to take in, as although microservices themselves may be small, the breadth\nand impact of their architecture are not. So here I’ll try to summarize some of the key\npoints covered throughout the book.\nPrinciples of Microservices\nWe discussed the role that principles can play in \nChapter 2\n. They are statements about how\nthings should be done, and why we think they should be done that way. They help us\nframe the various decisions we have to make when building our systems. You should\nabsolutely define your own principles, but I thought it worth spelling out what I see as\nbeing the key principles for microservice architectures, which you can see summarized in\nFigure 12-1\n. These are the principles that will help us create small autonomous services\nthat work well together. We’ve already covered everything here at least once so far, so\nnothing should be new, but there is value in distilling it down to its core essence.\nYou can choose to adopt these principles wholesale, or perhaps tweak them to make sense\nin your own organization. But note the value that comes from using them in combination:\nthe whole should be greater than the sum of the parts. So if you decide to drop one of\nthem, make sure you understand what you’ll be missing.\nFor each of these principles, I’ve tried to pull out some of the supporting practices that we\nhave covered in the book. As the saying goes, there is more than one way to skin a cat:\nyou might find your own practices to help deliver on these principles, but this should get\nyou started.\nFigure 12-1. \nPrinciples of microservices\nModel Around Business Concepts\nExperience has shown us that interfaces structured around business-bounded contexts are\nmore stable than those structured around technical concepts. By modeling the domain in\nwhich our system operates, not only do we attempt to form more stable interfaces, but we\nalso ensure that we are better able to reflect changes in business processes easily. Use\nbounded contexts\n to define potential domain boundaries.\nAdopt a Culture of Automation\nMicroservices add a lot of complexity, a key part of which comes from the sheer number\nof moving parts we have to deal with. Embracing a culture of automation is one key way\nto address this, and front-loading effort to create the tooling to support microservices can\nmake a lot of sense. \nAutomated testing\n is essential, as ensuring our services still work is a\nmore complex process than with monolithic systems. Having a uniform command-line call\nto \ndeploy the same way everywhere\n can help, and this can be a key part of adopting\ncontinuous delivery\n to give us fast feedback on the production quality of each check-in.\nConsider using \nenvironment definitions\n to help you specify the differences from one\nenvironment to another, without sacrificing the ability to use a uniform deployment\nmethod. Think about creating \ncustom images\n to speed up deployment, and embracing the\ncreation of fully automated \nimmutable servers\n to make it easier to reason about your\nsystems.\nHide Internal Implementation Details\nTo maximize the ability of one service to evolve independently of any others, it is vital\nthat we hide implementation details. Modeling \nbounded contexts\n can help, as this helps us\nfocus on those models that should be shared, and those that should be hidden. Services\nshould also \nhide their databases\n to avoid falling into one of the most common sorts of\ncoupling that can appear in traditional service-oriented architectures, and use \ndata pumps\nor \nevent data pumps\n to consolidate data across multiple services for reporting purposes.\nWhere possible, pick \ntechnology-agnostic APIs\n to give you freedom to use different\ntechnology stacks. \nConsider using \nREST\n, which formalizes the separation of internal and\nexternal implementation details, although even if using remote procedure calls (RPCs),\nyou can still embrace these ideas.\nDecentralize All the Things\nTo maximize the autonomy that microservices make possible, we need to constantly be\nlooking for the chance to delegate decision making and control to the teams that own the\nservices themselves. This process starts with embracing \nself-service\n wherever possible,\nallowing people to deploy software on demand, making development and testing as easy\nas possible, and avoiding the need for separate teams to perform these activities.\nEnsuring that \nteams own their services\n is an important step on this journey, making teams\nresponsible for the changes that are made, ideally even having them decide when to\nrelease those changes. Making use of \ninternal open source\n ensures that \npeople can make\nchanges on services owned by other teams, although remember that this requires work to\nimplement. \nAlign teams to the organization\n to ensure that Conway’s law works for you,\nand help your team become domain experts in the business-focused services they are\ncreating. Where some overarching guidance is needed, try to embrace a \nshared\ngovernance\n model where people from each team collectively share responsibility for\nevolving the technical vision of the system.\nThis principle can apply to architecture too. Avoid approaches like enterprise service bus\nor orchestration systems, which can lead to centralization of business logic and dumb\nservices. Instead, \nprefer choreography over orchestration\n and \ndumb middleware, with\nsmart endpoints\n to ensure that you keep associated logic and data within service\nboundaries, helping keep things cohesive.\nIndependently Deployable\nWe should always strive to ensure that our microservices can and are deployed by\nthemselves. Even when breaking changes are required, we should seek to \ncoexist\nversioned endpoints\n to allow our consumers to change over time. This allows us to\noptimize for speed of release of new features, as well as increasing the autonomy of the\nteams owning these microservices by ensuring that they don’t have to constantly\norchestrate their deployments. When using RPC-based integration, \navoid tightly bound\nclient/server stub generation\n such as that promoted by Java RMI.\nBy adopting a \none-service-per-host\n model, you reduce side effects that could cause\ndeploying one service to impact another unrelated service. Consider using \nblue/green\n or\ncanary\n release techniques to separate deployment from release, reducing the risk of a\nrelease going wrong. Use \nconsumer-driven contracts\n to catch breaking changes before\nthey happen.\nRemember that it should be the norm, not the exception, that you can make a change to a\nsingle service and release it into production, without having to deploy any other services\nin lock-step. Your \nconsumers should decide when they update themselves\n, and you need to\naccommodate this.\nIsolate Failure\nA microservice architecture can be more resilient than a monolithic system, but only if we\nunderstand and plan for failures in part of our system. If we don’t account for the fact that\na downstream call can and will fail, our systems might suffer catastrophic cascading\nfailure, and we could find ourselves with a system that is much more fragile than before.\nWhen using network calls, \ndon’t treat remote calls like local calls\n, as this will hide\ndifferent sorts of failure mode. So make sure if you’re using client libraries that the\nabstraction of the remote call doesn’t go too far.\nIf we hold the tenets of \nantifragility\n in mind, and expect failure will occur anywhere and\neverywhere, we are on the right track. Make sure your \ntimeouts\n are set appropriately.\nUnderstand when and how to use \nbulkheads\n and \ncircuit breakers\n to limit the fallout of a\nfailing component. Understand what the customer-facing impact will be if only one part of\nthe system is misbehaving. Know what the implications of a network partition might be,\nand whether sacrificing \navailability\n or \nconsistency\n in a given situation is the right call.\nHighly Observable\nWe cannot rely on observing the behavior of a single service instance or the status of a\nsingle machine to see if the system is functioning correctly. Instead, we need a joined-up\nview of what is happening. Use \nsemantic monitoring\n to see if your system is behaving\ncorrectly, by injecting \nsynthetic transactions\n into your system to simulate real-user\nbehavior. \nAggregate your logs\n, and \naggregate your stats\n, so that when you see a problem\nyou can drill down to the source. And when it comes to reproducing nasty issues or just\nseeing how your system is interacting in production, use \ncorrelation IDs\n to allow you to\ntrace calls through the system.\nWhen Shouldn’t You Use Microservices?\nI get asked this question a lot. My first piece of advice would be that the less well you\nunderstand a domain, the harder it will be for you to find proper bounded contexts for your\nservices. As we discussed previously, getting service boundaries wrong can result in\nhaving to make lots of changes in service-to-service collaboration — an expensive\noperation. So if you’re coming to a monolithic system for which you don’t understand the\ndomain, spend some time learning what the system does first, and then look to identify\nclean module boundaries prior to splitting out services.\nGreenfield development is also quite challenging. It isn’t just that the domain is also likely\nto be new; it’s that it is much easier to chunk up something you have than something you\ndon’t! So again, consider starting monolithic first and break things out when you’re stable.\nMany of the challenges you’re going to face with microservices get worse with scale. If\nyou mostly do things manually, you might be OK with 1 or 2 services, but 5 or 10?\nSticking with old monitoring practices where you just look at stats like CPU and memory\nlikewise might work OK for a few services, but the more service-to-service collaboration\nyou do, the more painful this will become. You’ll find yourself hitting these pain points as\nyou add more services, and I hope the advice in this book will help you see some of these\nproblems coming, and give you some concrete tips for how to deal with them. I spoke\nbefore about REA and Gilt taking a while to build the tooling and practices to manage\nmicroservices well, prior to being able to use them in any large quantity. These stories just\nreinforce to me the importance of starting gradually so you understand your organization’s\nappetite and ability to change, which will help you properly adopt microservices.\nParting Words\nMicroservice architectures give you more options, and more decisions to make. Making\ndecisions in this world is a far more common activity than in simpler, monolithic systems.\nYou won’t get all of these decisions right, I can guarantee that. So, knowing we are going\nto get some things wrong, what are our options? Well, I would suggest finding ways to\nmake each decision small in scope; that way, if you get it wrong, you only impact a small\npart of your system. Learn to embrace the concept of evolutionary architecture, where\nyour system bends and flexes and changes over time as you learn new things. Think not of\nbig-bang rewrites, but instead of a series of changes made to your system over time to\nkeep it supple.\nHopefully by now I’ve shared with you enough information and experiences to help you\ndecide if microservices are for you. If they are, I hope you think of this as a journey, not a\ndestination. Go incrementally. Break your system apart piece by piece, learning as you go.\nAnd get used to it: in many ways, the discipline to continually change and evolve our\nsystems is a far more important lesson to learn than any other I have shared with you\nthrough this book. Change is inevitable. Embrace it.\nIndex\nA\nacceptance testing\n, \nTypes of Tests\naccess by reference\n, \nAccess by Reference\naccountability\n, \nPeople\nadaptability\n, \nSummary\nAegisthus project\n, \nBackup Data Pump\naggregated logs\n, \nLogs, Logs, and Yet More Logs…\nantifragile systems\n, \nMicroservices\n, \nThe Antifragile Organization\n-\nIsolation\nbulkheads\n, \nBulkheads\ncircuit breakers\n, \nCircuit Breakers\nexamples of\n, \nThe Antifragile Organization\nincreased use of\n, \nMicroservices\nisolation\n, \nIsolation\nload shedding\n, \nBulkheads\ntimeouts\n, \nTimeouts\nAP system\ndefinition of term\n, \nSacrificing Consistency\nvs. CP system\n, \nAP or CP?\nAPI key-based authentication\n, \nAPI Keys\n, \nIt’s All About the Keys\napplication containers\n, \nApplication Containers\narchitects\n (\nsee\n systems architects)\narchitectural principles\ndevelopment of\n, \nPrinciples\nHeroku’s 12 factors\n, \nPrinciples\nkey microservices principles\n, \nBringing It All Together\nreal-world example\n, \nA Real-World Example\narchitectural safety\n, \nArchitectural Safety\n, \nArchitectural Safety Measures\nartifacts\nimages\n, \nImages as Artifacts\noperating system\n, \nOperating System Artifacts\nplatform-specific\n, \nPlatform-Specific Artifacts\nasynchronous collaboration\ncomplexities of\n, \nComplexities of Asynchronous Architectures\nimplementing\n, \nImplementing Asynchronous Event-Based Collaboration\nvs. synchronous\n, \nSynchronous Versus Asynchronous\nATOM specification\n, \nTechnology Choices\nauthentication/authorization\n, \nAuthentication and Authorization\n-\nThe Deputy\nProblem\ndefinition of terms\n, \nAuthentication and Authorization\nfine-grained\n, \nFine-Grained Authorization\nservice-to-service\n, \nService-to-Service Authentication and Authorization\nsingle sign-on (SSO)\n, \nCommon Single Sign-On Implementations\nsingle sign-on gateway\n, \nSingle Sign-On Gateway\nterminology\n, \nCommon Single Sign-On Implementations\nautomation\nbenefits for deployment\n, \nAutomation\ncase studies on\n, \nTwo Case Studies on the Power of Automation\nautonomy\nmicroservices and\n, \nAutonomous\nrole of systems architect in\n, \nSummary\nautoscaling\n, \nAutoscaling\navailability\nin CAP theorem\n, \nCAP Theorem\nkey microservices principle of\n, \nHow Much Is Too Much?\nsacrificing\n, \nSacrificing Availability\nB\nbackends for frontends (BFFs)\n, \nBackends for Frontends\nbackup data pumps\n, \nBackup Data Pump\nbackups, encryption of\n, \nEncrypt Backups\nblue/green deployment\n, \nSeparating Deployment from Release\nbottlenecks\n, \nDelivery Bottlenecks\nbounded contexts\nconcept of\n, \nThe Bounded Context\nmodules and services\n, \nModules and Services\nnested\n, \nTurtles All the Way Down\npremature decomposition\n, \nPremature Decomposition\nshared vs. hidden models\n, \nShared and Hidden Models\nsystem design and\n, \nBounded Contexts and Team Structures\nBrakeman\n, \nBaking Security In\nbreaking changes\navoiding\n, \nAvoid Breaking Changes\ndeferring\n, \nDefer It for as Long as Possible\nearly detection of\n, \nCatch Breaking Changes Early\nbrittle tests\n, \nFlaky and Brittle Tests\nbrittleness\n, \nBrittleness\nbuild pipelines\n, \nBuild Pipelines and Continuous Delivery\nbulkheads\n, \nBulkheads\nbundled service release\n, \nAnd the Inevitable Exceptions\nbusiness capabilities\n, \nBusiness Capabilities\nbusiness concepts\n, \nCommunication in Terms of Business Concepts\nbusiness-facing tests\n, \nTypes of Tests\nC\ncaching\nbenefits of\n, \nCaching\ncache failures\n, \nHiding the Origin\ncache poisoning\n, \nCache Poisoning: A Cautionary Tale\nclient-side\n, \nClient-Side, Proxy, and Server-Side Caching\nfor writes\n, \nCaching for Writes\nin HTTP\n, \nCaching in HTTP\nproxy\n, \nClient-Side, Proxy, and Server-Side Caching\nserver-side\n, \nClient-Side, Proxy, and Server-Side Caching\ncanary releasing\n, \nCanary Releasing\nCAP theorem\n, \nCAP Theorem\n-\nAnd the Real World\nAP/CP systems\n, \nIt’s Not All or Nothing\napplication of\n, \nAnd the Real World\nbasics of\n, \nCAP Theorem\nsacrificing availability\n, \nSacrificing Availability\nsacrificing consistency\n, \nSacrificing Consistency\nsacrificing partition tolerance\n, \nSacrificing Partition Tolerance?\ncascading failures\n, \nThe Cascade\ncertificate management\n, \nClient Certificates\nChaos Gorilla\n, \nThe Antifragile Organization\nChaos Monkey\n, \nThe Antifragile Organization\nchoreographed architecture\n, \nOrchestration Versus Choreography\ncircuit breakers\n, \nTailored Service Template\n, \nCircuit Breakers\ncircuit_breaker mixin for Ruby\n, \nBulkheads\nclass-responsibility-collaboration (CRC)\n, \nCost of Change\nclient certificates\n, \nClient Certificates\nclient libraries\n, \nClient Libraries\nclient-side caching\n, \nClient-Side, Proxy, and Server-Side Caching\ncode reuse\n, \nDRY and the Perils of Code Reuse in a Microservice World\ncoding architect\n, \nZoning\ncohesion\n, \nSmall, and Focused on Doing One Thing Well\n, \nHigh Cohesion\nCohn’s Test Pyramid\n, \nTest Scope\ncollaboration\n, \nSummary\nevent-based\n, \nSynchronous Versus Asynchronous\nrequest/response\n, \nSynchronous Versus Asynchronous\nCommand-Query Responsibility Segregation (CQRS)\n, \nCQRS\ncommits, two-phase\n, \nDistributed Transactions\ncommunication\nadapting to pathways\n, \nAdapting to Communication Pathways\nprotocols for (SOAP)\n, \nWhat About Service-Oriented Architecture?\nsynchronous vs. asynchronous\n, \nSynchronous Versus Asynchronous\ncompensating transactions\n, \nAbort the Entire Operation\ncomposability\n, \nComposability\nconfiguration drift\n, \nImmutable Servers\nconfiguration, service\n, \nService Configuration\nconfused deputy problem\n, \nThe Deputy Problem\nconsistency\nin CAP theorem\n, \nCAP Theorem\nsacrificing\n, \nSacrificing Consistency\nconstraints\n, \nConstraints\nConsul\n, \nConsul\nconsumer-driven contracts (CDCs)\n, \nConsumer-Driven Tests to the Rescue\n-\nIt’s\nAbout Conversations\ncontent delivery network (CDN)\n, \nClient-Side, Proxy, and Server-Side Caching\ncontent management systems (CMS)\n, \nExample: CMS as a service\ncontinuous delivery (CD)\n, \nMicroservices\n, \nBuild Pipelines and Continuous\nDelivery\ncontinuous integration (CI)\nbasics\n, \nA Brief Introduction to Continuous Integration\nchecklist for\n, \nAre You Really Doing It?\nmapping to microservices\n, \nMapping Continuous Integration to\nMicroservices\nConway’s law\nevidence of\n, \nEvidence\nin reverse\n, \nConway’s Law in Reverse\nstatement of\n, \nConway’s Law and System Design\nsummary of\n, \nSummary\ncoordination process\n, \nDistributed Transactions\ncore team\n, \nRole of the Custodians\nCoreOS\n, \nDocker\ncorrelation IDs\n, \nCorrelation IDs\nCP system\n, \nAP or CP?\ncross-functional requirements (CFR)\n, \nCross-Functional Testing\n, \nHow Much Is\nToo Much?\ncustodians\n, \nRole of the Custodians\ncustom images\n, \nCustom Images\nCustomer Relationship Management (CRM)\n, \nExample: The multirole CRM\nsystem\ncustomers, interfacing with\nenrolling new customers\n, \nInterfacing with Customers\n, \nOrchestration\nVersus Choreography\nshared databases\n, \nThe Shared Database\nD\ndata\nbatch insertion of\n, \nData Retrieval via Service Calls\ndurability of\n, \nHow Much Is Too Much?\nencryption of backup\n, \nEncrypt Backups\nretrieval via service calls\n, \nData Retrieval via Service Calls\nsecuring at rest\n, \nSecuring Data at Rest\n(\nsee also\n security)\nshared\n, \nExample: Shared Data\nshared static\n, \nExample: Shared Static Data\ndata encryption\n, \nGo with the Well Known\ndata pumps\nbackup\n, \nBackup Data Pump\ndata retrieval via\n, \nData Pumps\nevent\n, \nEvent Data Pump\nserial use of\n, \nAlternative Destinations\ndatabase decomposition\n, \nThe Database\n-\nUnderstanding Root Causes\nbreaking foreign key relationships\n, \nExample: Breaking Foreign Key\nRelationships\nincremental approach to\n, \nCost of Change\noverview of\n, \nSummary\nrefactoring databases\n, \nStaging the Break\nselecting separation points\n, \nGetting to Grips with the Problem\nselecting separation timing\n, \nUnderstanding Root Causes\nshared data\n, \nExample: Shared Data\nshared static data\n, \nExample: Shared Static Data\nshared tables\n, \nExample: Shared Tables\ntransactional boundaries\n, \nTransactional Boundaries\ndatabase integration\n, \nThe Shared Database\ndatabase scaling\nCommand-Query Responsibility Segregation (CQRS)\n, \nCQRS\nfor reads\n, \nScaling for Reads\nfor writes\n, \nScaling for Writes\nservice availability vs. data durability\n, \nAvailability of Service Versus\nDurability of Data\nshared infrastructure\n, \nShared Database Infrastructure\ndecision-making guidelines\n, \nA Principled Approach\n-\nA Real-World Example\ncustomized approach to\n, \nCombining Principles and Practices\npractices for\n, \nPractices\nprinciples for\n, \nPrinciples\nreal-world example\n, \nA Real-World Example\nstrategic goals\n, \nStrategic Goals\ndecompositional techniques\ndatabases\n (\nsee\n database decomposition)\nidentifying/packaging contexts\n, \nBreaking Apart MusicCorp\nmodules\n, \nModules\nseam concept\n, \nIt’s All About Seams\nselecting separation points\n, \nThe Reasons to Split the Monolith\nselecting separation timing\n, \nUnderstanding Root Causes\nshared libraries\n, \nShared Libraries\ndecoupling\n, \nAutonomous\n, \nOrchestration Versus Choreography\ndegrading functionality\n, \nDegrading Functionality\ndelivery bottlenecks\n, \nDelivery Bottlenecks\ndeployment\nartifacts, images as\n, \nImages as Artifacts\nartifacts, operating system\n, \nOperating System Artifacts\nartifacts, platform-specific\n, \nPlatform-Specific Artifacts\nautomation\n, \nAutomation\nblue/green deployment\n, \nSeparating Deployment from Release\nbuild pipeline\n, \nBuild Pipelines and Continuous Delivery\nbundled service release\n, \nAnd the Inevitable Exceptions\ncontinuous integration basics\n, \nA Brief Introduction to Continuous\nIntegration\ncontinuous integration checklist\n, \nAre You Really Doing It?\ncontinuous integration in microservices\n, \nMapping Continuous\nIntegration to Microservices\ncustom images\n, \nCustom Images\nenvironment definition\n, \nEnvironment Definition\nenvironments to consider\n, \nEnvironments\nimmutable servers\n, \nImmutable Servers\ninterfaces\n, \nA Deployment Interface\nmicroservices vs. monolithic systems\n, \nEase of Deployment\n, \nDeployment\noverview of\n, \nSummary\nseparating from release\n, \nSeparating Deployment from Release\nservice configuration\n, \nService Configuration\nvirtualization approach\n, \nFrom Physical to Virtual\nvirtualization, hypervisors\n, \nTraditional Virtualization\nvirtualization, traditional\n, \nTraditional Virtualization\nvirtualization, type 2\n, \nTraditional Virtualization\ndeputy problem\n, \nThe Deputy Problem\ndesign principles\n, \nPrinciples\n, \nBringing It All Together\n-\nHighly Observable\n(\nsee also\n architectural principles)\ndesign/delivery practices\ndevelopment of\n, \nPractices\nreal-world example\n, \nA Real-World Example\ndirectory service\n, \nCommon Single Sign-On Implementations\nDiRT (Disaster Recovery Test)\n, \nThe Antifragile Organization\ndistributed systems\nfallacies of\n, \nLocal Calls Are Not Like Remote Calls\n, \nFailure Is\nEverywhere\nkey promises of\n, \nComposability\ndistributed transactions\n, \nDistributed Transactions\nDNS service\n, \nDNS\nDocker\n, \nDocker\ndocumentation\nHAL (Hypertext Application Language)\n, \nHAL and the HAL Browser\nimportance of\n, \nDocumenting Services\nself-describing systems\n, \nHAL and the HAL Browser\nSwagger\n, \nSwagger\ndomain-driven design\n, \nMicroservices\nDropwizard\n, \nTailored Service Template\nDRY (Don’t Repeat Yourself)\n, \nDRY and the Perils of Code Reuse in a\nMicroservice World\ndummies\n, \nMocking or Stubbing\ndurability\n, \nHow Much Is Too Much?\ndynamic service registries\nbenefits of\n, \nDynamic Service Registries\nConsul\n, \nConsul\nEureka\n, \nEureka\nlaunching\n, \nRolling Your Own\nZookeeper\n, \nZookeeper\nE\nempathy\n, \nSummary\nencryption\n, \nGo with the Well Known\nend-to-end tests\nappropriate uses for\n, \nTest Journeys, Not Stories\n, \nSo Should You Use End-\nto-End Tests?\nCohn’s Test Pyramid\n, \nTest Scope\ncreation of\n, \nWho Writes These Tests?\ndrawbacks of\n, \nFlaky and Brittle Tests\nfeedback cycle\n, \nThe Great Pile-up\nimplementation of\n, \nThose Tricky End-to-End Tests\nmetaversion\n, \nThe Metaversion\nscope of\n, \nEnd-to-End Tests\ntiming of\n, \nHow Long?\nendpoints\ncoexisting different\n, \nCoexist Different Endpoints\nserver-side aggregation\n, \nBackends for Frontends\nenvironments\ndefinition during deployment\n, \nEnvironment Definition\ndeployment considerations\n, \nEnvironments\nmanaging\n, \nEnvironments\nErlang modules\n, \nModules\nEureka\n, \nEureka\nevent data pumps\n, \nEvent Data Pump\nevent sourcing\n, \nCQRS\nevent-based collaboration\n, \nSynchronous Versus Asynchronous\neventual consistency\n, \nTry Again Later\n, \nSacrificing Consistency\nevolutionary architects\n (\nsee\n systems architects)\nexception handling\n, \nException Handling\nexemplars\n, \nExemplars\nexploratory testing\n, \nTypes of Tests\nF\nfailure bots\n, \nThe Antifragile Organization\nfailures\ncascading\n, \nThe Cascade\n(\nsee also\n monitoring)\ndealing with\n, \nFailure Is Everywhere\nfakes\n, \nMocking or Stubbing\nfeature-based teams\n, \nFeature Teams\nfirewalls\n, \nFirewalls\nflaky tests\n, \nFlaky and Brittle Tests\nforeign key relationships, breaking\n, \nExample: Breaking Foreign Key\nRelationships\nFPM package manager tool\n, \nOperating System Artifacts\nfunctionality, degrading\n, \nDegrading Functionality\nG\ngame days\n, \nThe Antifragile Organization\ngatekeepers\n, \nRole of the Custodians\ngovernance\nconcept of\n, \nGovernance and Leading from the Center\nrole of systems architect in\n, \nSummary\ngranularity\n, \nWhat About Service-Oriented Architecture?\nGraphite\n, \nMetric Tracking Across Multiple Services\nH\nhabitable systems\n, \nAn Evolutionary Vision for the Architect\nHAL (Hypertext Application Language)\n, \nHAL and the HAL Browser\nhash-based messaging code (HMAC)\n, \nHMAC Over HTTP\nHATEOS principle\n, \nHypermedia As the Engine of Application State\nheterogeneity\nbenefits of\n, \nTechnology Heterogeneity\nshared libraries and\n, \nShared Libraries\nhexagonal architecture\n, \nMicroservices\nhidden models\n, \nShared and Hidden Models\nhigh cohesion\n, \nHigh Cohesion\nHMAC (hash-based messaging code)\n, \nHMAC Over HTTP\nHTTP (Hypertext Transfer Protocol)\ncaching in\n, \nCaching in HTTP\nHATEOS principle\n, \nHypermedia As the Engine of Application State\nHTTP over REST benefits\n, \nREST and HTTP\nHTTP over REST drawbacks\n, \nDownsides to REST Over HTTP\nHTTP termination\n, \nLoad Balancing\nHTTP(S) Basic Authentication\n, \nHTTP(S) Basic Authentication\nhumane registry\n, \nHAL and the HAL Browser\nhypermedia\n, \nHypermedia As the Engine of Application State\nhypervisors\n, \nTraditional Virtualization\nHystrix library\n, \nTailored Service Template\n, \nBulkheads\nI\nidempotent operations\n, \nIdempotency\nidentity provider\n, \nCommon Single Sign-On Implementations\nimages\nas artifacts\n, \nImages as Artifacts\ncustom\n, \nCustom Images\nimmutable servers\n, \nImmutable Servers\ninfrastructure automation\n, \nMicroservices\nintegration\naccess by reference\n, \nAccess by Reference\nasynchronous event-based collaboration\n, \nImplementing Asynchronous\nEvent-Based Collaboration\ncustomer interface\n, \nInterfacing with Customers\nDRY (Don’t Repeat Yourself)\n, \nDRY and the Perils of Code Reuse in a\nMicroservice World\ngoals for\n, \nLooking for the Ideal Integration Technology\nguidelines for\n, \nSummary\nimportance of\n, \nIntegration\norchestration vs. choreography\n, \nOrchestration Versus Choreography\nreactive extensions\n, \nReactive Extensions\nremote procedure calls\n, \nRemote Procedure Calls\nREST (Representational State Transfer)\n, \nREST\nservices as state machines\n, \nServices as State Machines\nshared databases\n, \nThe Shared Database\nsynchronous vs. asynchronous communication\n, \nSynchronous Versus\nAsynchronous\nthird-party software\n, \nIntegrating with Third-Party Software\nuser interfaces\n, \nUser Interfaces\nversioning\n, \nVersioning\ninterfaces\ncoexisting new and old\n, \nCoexist Different Endpoints\ndeployment\n, \nA Deployment Interface\nstandards establishment for\n, \nInterfaces\n(\nsee also\n user interfaces)\ninternal implementation detail\n, \nHide Internal Implementation Detail\ninternal open source model\n, \nInternal Open Source\nintrusion detection systems (IDS)\n, \nIntrusion Detection (and Prevention) System\nintrusion prevention systems (IPS)\n, \nIntrusion Detection (and Prevention) System\nisolation\n, \nIsolation\nIT architects\n (\nsee\n systems architects)\nJ\nJSON\n, \nJSON, XML, or Something Else?\nJSON web tokens (JWT)\n, \nHMAC Over HTTP\nK\nKaryon\n, \nTailored Service Template\nkey-based authentication\n, \nAPI Keys\nKibana\n, \nLogs, Logs, and Yet More Logs…\nL\nlatency\n, \nHow Much Is Too Much?\nLatency Monkey\n, \nThe Antifragile Organization\nlayered architectures\n, \nMicroservices\nlibraries\nclient\n, \nClient Libraries\nservice metrics\n, \nService Metrics\nshared\n, \nShared Libraries\nLinux containers\n, \nLinux Containers\nload balancing\n, \nLoad Balancing\nload shedding\n, \nBulkheads\nlocal calls\n, \nLocal Calls Are Not Like Remote Calls\nlogs\naggregated\n, \nLogs, Logs, and Yet More Logs…\n(\nsee also\n monitoring)\nsecurity issues\n, \nLogging\nstandardization of\n, \nStandardization\nlogstash\n, \nLogs, Logs, and Yet More Logs…\nloose coupling\n, \nLoose Coupling\n, \nOrchestration Versus Choreography\n, \nLoose and\nTightly Coupled Organizations\nM\nman-in-the-middle attacks\n, \nAllow Everything Inside the Perimeter\nMarick’s quadrant\n, \nTypes of Tests\nmaturity\n, \nMaturity\nmean time between failures (MTBF)\n, \nMean Time to Repair Over Mean Time\nBetween Failures?\nmean time to repair (MTTR)\n, \nMean Time to Repair Over Mean Time Between\nFailures?\nmessage brokers\n, \nTechnology Choices\nmetrics\nlibraries for\n, \nService Metrics\nservice metrics\n, \nService Metrics\ntracking across multiple services\n, \nMetric Tracking Across Multiple\nServices\nMetrics library\n, \nTailored Service Template\nmicroservices\nappropriate application of\n, \nWhen Shouldn’t You Use Microservices?\nautonomy and\n, \nAutonomous\n, \nBuilding a Team\nbenefits of\n, \nMicroservices\ncomposability of\n, \nComposability\ndefinition of term\n, \nWhat Are Microservices?\ndeployment ease of\n, \nEase of Deployment\ndrawbacks of\n, \nNo Silver Bullet\n, \nWhen Shouldn’t You Use Microservices?\nkey principles of\n, \nBringing It All Together\norganizational alignment and\n, \nOrganizational Alignment\norigins of\n, \nMicroservices\nreplaceability and\n, \nOptimizing for Replaceability\nresilience of\n, \nResilience\nscaling and\n, \nScaling\nsize and\n, \nSmall, and Focused on Doing One Thing Well\ntechnology heterogeneity of\n, \nTechnology Heterogeneity\nvs. modules\n, \nModules\nvs. service-oriented architecture\n, \nWhat About Service-Oriented\nArchitecture?\nvs. shared libraries\n, \nShared Libraries\nmicroservices at scale\nantifragile systems\n, \nThe Antifragile Organization\narchitectural safety measures\n, \nArchitectural Safety Measures\nautoscaling\n, \nAutoscaling\ncaching\n, \nCaching\ncaching for writes\n, \nCaching for Writes\nCAP theorem\n, \nCAP Theorem\ncross-functional requirements (CFR)\n, \nHow Much Is Too Much?\ndealing with failures\n, \nFailure Is Everywhere\ndegrading functionality\n, \nDegrading Functionality\ndocumenting services\n, \nDocumenting Services\ndynamic service registries\n, \nDynamic Service Registries\nidempotent operations\n, \nIdempotency\nscaling\n, \nScaling\nscaling databases\n, \nScaling Databases\nself-describing systems\n, \nHAL and the HAL Browser\nservice discovery\n, \nService Discovery\nmiddleware\n, \nTechnology Choices\nmocking vs. stubbing\n, \nMocking or Stubbing\nmodeling services\nbounded contexts\n, \nThe Bounded Context\nbusiness capabilities\n, \nBusiness Capabilities\nbusiness concepts\n, \nCommunication in Terms of Business Concepts\nkey concepts\n, \nWhat Makes a Good Service?\nmodules and services\n, \nModules and Services\nnested bounded contexts\n, \nTurtles All the Way Down\npremature decomposition\n, \nPremature Decomposition\nshared vs. hidden models\n, \nShared and Hidden Models\ntechnical boundaries\n, \nThe Technical Boundary\nmodular decomposition\n, \nModules\nmodules\n, \nModules and Services\nmonitoring\ncascading failures\n, \nThe Cascade\ncentral logging\n, \nLogs, Logs, and Yet More Logs…\ncomplexities of\n, \nMonitoring\ncorrelation IDs\n, \nCorrelation IDs\ndisplaying/sharing results\n, \nConsider the Audience\nmetric tracking across multiple services\n, \nMetric Tracking Across\nMultiple Services\nmultiple services/multiple servers\n, \nMultiple Services, Multiple Servers\noverview of\n, \nSummary\nreal-time reporting\n, \nThe Future\nsemantic\n, \nImplementing Semantic Monitoring\nservice metrics\n, \nService Metrics\nsingle service/multiple servers\n, \nSingle Service, Multiple Servers\nsingle service/single server\n, \nSingle Service, Single Server\nstandardization of\n, \nStandardization\nstandards establishment and\n, \nMonitoring\nsynthetic\n, \nSynthetic Monitoring\nmonolithic systems\ncodebases in\n, \nSmall, and Focused on Doing One Thing Well\nlack of cohesion/loose coupling in\n, \nIt’s All About Seams\nreporting databases in\n, \nThe Reporting Database\nvs. service-oriented architecture\n, \nWhat About Service-Oriented\nArchitecture?\nMoore’s law\n, \nConway’s Law and System Design\nMountebank\n, \nA Smarter Stub Service\nMTBF (mean time between failures)\n, \nMean Time to Repair Over Mean Time\nBetween Failures?\nMTTR (mean time to repair)\n, \nMean Time to Repair Over Mean Time Between\nFailures?\nN\nnested bounded contexts\n, \nTurtles All the Way Down\nnetwork segregation\n, \nNetwork Segregation\nnonfunctional requirements\n, \nCross-Functional Testing\nnormalization of deviance\n, \nFlaky and Brittle Tests\nO\non-demand provisioning systems\n, \nScaling\non-demand virtualization\n, \nMicroservices\nonion architecture\n, \nThe Technical Boundary\nOpen Web Application Security Project (OWASP)\n, \nBaking Security In\nOpenID Connect\n, \nCommon Single Sign-On Implementations\n, \nUse SAML or\nOpenID Connect\noperating system artifacts\n, \nOperating System Artifacts\noperating systems security\n, \nOperating System\norchestration architecture\n, \nOrchestration Versus Choreography\norganizational alignment\n, \nOrganizational Alignment\norganizational structure\nConway’s law and\n, \nConway’s Law and System Design\neffect on systems design\n, \nEvidence\nloose vs. tightly coupled\n, \nLoose and Tightly Coupled Organizations\norphaned services\n, \nThe Orphaned Service?\nOSGI (Open Source Gateway Initiative)\n, \nModules\nownership\nshared\n, \nDrivers for Shared Services\nsystem design and\n, \nService Ownership\nP\nPacker\n, \nCustom Images\nPact\n, \nPact\nPacto\n, \nPact\npartition tolerance\nin CAP theorem\n, \nCAP Theorem\nsacrificing\n, \nSacrificing Partition Tolerance?\npasswords\n, \nGo with the Well Known\nperformance tests\n, \nPerformance Tests\nplatform as a service (PaaS)\n, \nPlatform as a Service\nplatform-specific artifacts\n, \nPlatform-Specific Artifacts\nPolly for .NET\n, \nBulkheads\nPostel’s Law\n, \nDefer It for as Long as Possible\npredictive scaling\n, \nAutoscaling\nprincipal party\n, \nAuthentication and Authorization\nprivacy issues\n, \nBe Frugal\nproperty testing\n, \nTypes of Tests\nproxy caching\n, \nClient-Side, Proxy, and Server-Side Caching\nR\nRabbitMQ\n, \nTechnology Choices\nRDBMS (relational database management systems)\n, \nScaling for Reads\nreactive extensions (Rx)\n, \nReactive Extensions\nreactive scaling\n, \nAutoscaling\nread replicas\n, \nScaling for Reads\nredesign\n, \nStarting Again\nrefactoring databases\n, \nStaging the Break\nremote procedure calls\n, \nRemote Procedure Calls\n-\nIs RPC Terrible?\nbenefits and drawbacks of\n, \nIs RPC Terrible?\nbrittleness\n, \nBrittleness\ndefinition of term\n, \nRemote Procedure Calls\ntechnologies available\n, \nRemote Procedure Calls\ntechnology coupling\n, \nTechnology Coupling\nvs. local calls\n, \nLocal Calls Are Not Like Remote Calls\nreplaceability\n, \nOptimizing for Replaceability\nreporting databases\nbackup data pumps\n, \nBackup Data Pump\ndata pump guidelines\n, \nData Pumps\ndata retrieval via service calls\n, \nData Retrieval via Service Calls\nevent data pumps\n, \nEvent Data Pump\ngeneric eventing systems\n, \nToward Real Time\nmonolithic approach to\n, \nThe Reporting Database\nthird-party software\n, \nData Retrieval via Service Calls\nrequest/response collaboration\n, \nSynchronous Versus Asynchronous\nresilience engineering\n, \nResilience\n, \nCaching for Resilience\nresources\n, \nREST\nresponse time\n, \nHow Much Is Too Much?\nREST (Representational State Transfer)\n, \nREST\n-\nDownsides to REST Over\nHTTP\nATOM specification\n, \nTechnology Choices\nconcept of\n, \nREST\nframeworks for\n, \nBeware Too Much Convenience\nHTTP over REST benefits\n, \nREST and HTTP\nHTTP over REST drawbacks\n, \nDownsides to REST Over HTTP\ntextual formats\n, \nJSON, XML, or Something Else?\nreverse proxy\n, \nClient-Side, Proxy, and Server-Side Caching\nRiemann\n, \nThe Future\nrisk, spreading\n, \nSpreading Your Risk\nRobustness principle\n, \nDefer It for as Long as Possible\nRx (reactive extensions)\n, \nReactive Extensions\nS\nSAML\n, \nCommon Single Sign-On Implementations\n, \nUse SAML or OpenID\nConnect\nSAN (storage area networks)\n, \nSpreading Your Risk\nscaling\n, \nScaling\n-\nStarting Again\nautoscaling\n, \nAutoscaling\nbenefits of\n, \nScaling\ndatabases\n, \nScaling Databases\nload balancing\n, \nLoad Balancing\nreasons for\n, \nScaling\nsplitting workloads\n, \nSplitting Workloads\nspreading risk\n, \nSpreading Your Risk\nvertical\n, \nGo Bigger\nvs. redesign\n, \nStarting Again\nworker-based systems\n, \nWorker-Based Systems\nseams, concept of\n, \nIt’s All About Seams\nsecurity\n, \nSecurity\n-\nSummary\nattacks, defending from\n, \nDefense in Depth\nauthentication/authorization\n, \nAuthentication and Authorization\nbackups\n, \nBackup Data Pump\n, \nEncrypt Backups\neducation/awareness of\n, \nBaking Security In\nencryption\n, \nGo with the Well Known\nexample setup\n, \nA Worked Example\nexternal verification of\n, \nExternal Verification\nfirewalls\n, \nFirewalls\nhuman element of\n, \nThe Human Element\nimportance of\n, \nSecurity\nintrusion detection/prevention\n, \nIntrusion Detection (and Prevention)\nSystem\nkey storage\n, \nIt’s All About the Keys\nlogs\n, \nLogging\nman-in-the-middle attacks\n, \nAllow Everything Inside the Perimeter\noperating systems\n, \nOperating System\noverview of\n, \nSummary\npasswords\n, \nGo with the Well Known\nprivacy issues\n, \nBe Frugal\nsecuring data at rest\n, \nSecuring Data at Rest\nservice-to-service authentication/authorization\n, \nService-to-Service\nAuthentication and Authorization\ntool selection\n, \nThe Golden Rule\nvirtual private clouds\n, \nNetwork Segregation\nSecurity Development Lifecycle\n, \nBaking Security In\nself-describing systems\n, \nHAL and the HAL Browser\nsemantic monitoring\n, \nSo Should You Use End-to-End Tests?\n, \nSynthetic\nMonitoring\nsemantic versioning\n, \nUse Semantic Versioning\nserver-side caching\n, \nClient-Side, Proxy, and Server-Side Caching\nservice accounts\n, \nUse SAML or OpenID Connect\nservice boundaries\n, \nZoning\n(\nsee also\n modeling services)\nservice calls, data retrieval via\n, \nData Retrieval via Service Calls\nservice configuration\n, \nService Configuration\nservice discovery\n, \nService Discovery\nservice ownership\ncomprehensive approach\n, \nService Ownership\nshared\n, \nDrivers for Shared Services\nservice provider\n, \nCommon Single Sign-On Implementations\nservice separation, staging\n, \nStaging the Break\n(\nsee also\n database decomposition)\nservice templates\n, \nTailored Service Template\nservice tests\nCohn’s Test Pyramid\n, \nTest Scope\nimplementation of\n, \nImplementing Service Tests\nmocking vs. stubbing\n, \nMocking or Stubbing\nMountebank server for\n, \nA Smarter Stub Service\nscope of\n, \nService Tests\nservice-oriented architectures (SOA)\nconcept of\n, \nWhat About Service-Oriented Architecture?\ndrawbacks of\n, \nWhat About Service-Oriented Architecture?\nreuse of functionality in\n, \nComposability\nvs. microservices\n, \nWhat About Service-Oriented Architecture?\nservice-to-host mapping\n, \nService-to-Host Mapping\n-\nPlatform as a Service\napplication containers\n, \nApplication Containers\nmultiple services per host\n, \nMultiple Services Per Host\nplatform as a service (PaaS)\n, \nPlatform as a Service\nsingle service per host\n, \nSingle Service Per Host\nterminology\n, \nService-to-Host Mapping\nservice-to-service authentication/authorization\n, \nService-to-Service\nAuthentication and Authorization\n-\nThe Deputy Problem\nAPI keys\n, \nAPI Keys\nclient certificates\n, \nClient Certificates\nconfused deputy problem\n, \nThe Deputy Problem\nHMAC over HTTP\n, \nHMAC Over HTTP\nHTTP(S) basic\n, \nHTTP(S) Basic Authentication\nman-in-the-middle attacks\n, \nAllow Everything Inside the Perimeter\nSAML/OpenID Connect\n, \nUse SAML or OpenID Connect\nsharding\n, \nScaling for Writes\nshared code\n, \nDRY and the Perils of Code Reuse in a Microservice World\nshared data\n, \nExample: Shared Data\nshared libraries\n, \nShared Libraries\nshared models\n, \nShared and Hidden Models\nshared static data\n, \nExample: Shared Static Data\nshared tables\n, \nExample: Shared Tables\nsharing behavior\n, \nThe Shared Database\nsingle sign-on (SSO)\n, \nCommon Single Sign-On Implementations\n-\nSingle Sign-On\nGateway\nsmoke test suites\n, \nSeparating Deployment from Release\nspies\n, \nMocking or Stubbing\nSSH-multiplexing\n, \nSingle Service, Multiple Servers\nSSL certificates\n, \nHTTP(S) Basic Authentication\nSSL termination\n, \nLoad Balancing\nstandards enforcement\nexemplars\n, \nExemplars\ntailored service templates\n, \nTailored Service Template\nstandards establishment\n, \nThe Required Standard\n-\nArchitectural Safety\narchitectural safety\n, \nArchitectural Safety\nimportance of\n, \nThe Required Standard\ninterfaces\n, \nInterfaces\nmonitoring\n, \nMonitoring\nstatic data\n, \nExample: Shared Static Data\nStrangler Application Pattern\n, \nThe Strangler Pattern\n, \nArchitectural Safety\nMeasures\nstrategic goals\nreal-world example\n, \nA Real-World Example\nunderstanding\n, \nStrategic Goals\nstubbing vs. mocking\n, \nMocking or Stubbing\nSuro\n, \nThe Future\nSwagger\n, \nSwagger\nsynchronous communication\n, \nSynchronous Versus Asynchronous\nsynthetic monitoring\n, \nSynthetic Monitoring\nsystem design\naccountability and\n, \nPeople\nadapting to communication pathways\n, \nAdapting to Communication\nPathways\nbounded contexts\n, \nBounded Contexts and Team Structures\ncase study\n, \nCase Study: RealEstate.com.au\nConway’s law of\n, \nConway’s Law and System Design\ndelivery bottlenecks\n, \nDelivery Bottlenecks\neffect on organizational structure\n, \nConway’s Law in Reverse\nfeature teams\n, \nFeature Teams\ninternal open source model\n, \nInternal Open Source\norganizational structure and\n, \nEvidence\norphaned services\n, \nThe Orphaned Service?\noverview of\n, \nSummary\nrole of custodians\n, \nRole of the Custodians\nservice maturity\n, \nMaturity\nservice ownership\n, \nService Ownership\nshared service ownership\n, \nDrivers for Shared Services\ntooling\n, \nTooling\nsystems architects\nchallenges faced by\n, \nSummary\ndecision-making guidelines for\n, \nA Principled Approach\nexception handling\n, \nException Handling\ngovernance\n, \nGovernance and Leading from the Center\nresponsibilities of\n, \nInaccurate Comparisons\n, \nTechnical Debt\n, \nSummary\nrole of\n, \nAn Evolutionary Vision for the Architect\nservice boundaries and\n, \nZoning\nstandards enforcement by\n, \nGovernance Through Code\n-\nTailored Service\nTemplate\nstandards establishment by\n, \nThe Required Standard\nteam building by\n, \nBuilding a Team\nteam participation by\n, \nZoning\ntechnical debt and\n, \nTechnical Debt\nT\ntables, shared\n, \nExample: Shared Tables\ntailored service templates\n, \nTailored Service Template\nteam building\n, \nBuilding a Team\nteam structures\n, \nBounded Contexts and Team Structures\ntechnical boundaries\n, \nThe Technical Boundary\ntechnical debt\n, \nTechnical Debt\ntechnology heterogeneity\n, \nTechnology Heterogeneity\ntechnology-agnostic APIs\n, \nKeep Your APIs Technology-Agnostic\ntechnology-facing tests\n, \nTypes of Tests\ntemplates\n, \nTailored Service Template\ntest doubles\n, \nMocking or Stubbing\nTest Pyramid\n, \nTest Scope\n, \nConsumer-Driven Tests to the Rescue\ntest snow cone\n, \nHow Many?\ntest-driven design (TDD)\n, \nUnit Tests\ntesting\ncanary releasing\n, \nCanary Releasing\nconsiderations for\n, \nTrade-Offs\nconsumer-driven tests\n, \nConsumer-Driven Tests to the Rescue\ncross-functional\n, \nCross-Functional Testing\nend-to-end tests\n, \nThose Tricky End-to-End Tests\nMTTR over MTBF\n, \nMean Time to Repair Over Mean Time Between\nFailures?\noverview of\n, \nSummary\nperformance tests\n, \nPerformance Tests\npost-production\n, \nTesting After Production\nscope of\n, \nTest Scope\nselecting number of\n, \nHow Many?\nsemantic monitoring\n, \nSo Should You Use End-to-End Tests?\nseparating deployment from release\n, \nSeparating Deployment from\nRelease\nservice test implementation\n, \nImplementing Service Tests\ntypes of tests\n, \nTypes of Tests\nthird-party software\n, \nIntegrating with Third-Party Software\n-\nThe Strangler\nPattern\n, \nData Retrieval via Service Calls\nbuilding vs. buying\n, \nIntegrating with Third-Party Software\ncontent management systems (CMS)\n, \nExample: CMS as a service\nCustomer Relationship Management (CRM)\n, \nExample: The multirole\nCRM system\ncustomization of\n, \nCustomization\nintegration issues\n, \nIntegration Spaghetti\nlack of control over\n, \nLack of Control\nreporting databases\n, \nData Retrieval via Service Calls\nStrangler Application Pattern\n, \nThe Strangler Pattern\ntight coupling\n, \nLoose Coupling\n, \nLoose and Tightly Coupled Organizations\ntime to live (TTL)\n, \nDNS\ntimeouts\n, \nTimeouts\ntransaction managers\n, \nDistributed Transactions\ntransactional boundaries\n, \nTransactional Boundaries\n-\nSo What to Do?\ntransactions\ncompensating\n, \nAbort the Entire Operation\ndistributed\n, \nDistributed Transactions\nTransport Layer Security (TLS)\n, \nClient Certificates\ntwo-phase commits\n, \nDistributed Transactions\ntype 2 virtualization\n, \nTraditional Virtualization\nU\nUDDI (Universal Description, Discovery, and Integration)\n, \nHAL and the HAL\nBrowser\nunit tests\nCohn’s Test Pyramid\n, \nTest Scope\ngoals of\n, \nUnit Tests\nMarick’s quadrant\n, \nTypes of Tests\nscope of\n, \nUnit Tests\nuser interfaces\n, \nUser Interfaces\n-\nA Hybrid Approach\nAPI composition\n, \nAPI Composition\nAPI gateways\n, \nBackends for Frontends\nAPI granularity\n, \nToward Digital\nCohn’s Test Pyramid\n, \nTest Scope\n(\nsee also\n end-to-end tests)\nconstraints\n, \nConstraints\nevolution of\n, \nUser Interfaces\nfragment composition\n, \nUI Fragment Composition\nhybrid approaches\n, \nA Hybrid Approach\nV\nVagrant\n, \nVagrant\nversioning\n, \nVersioning\n-\nUse Multiple Concurrent Service Versions\ncatching breaking changes early\n, \nCatch Breaking Changes Early\ncoexisting different endpoints\n, \nCoexist Different Endpoints\ndeferring breaking changes\n, \nDefer It for as Long as Possible\nmultiple concurrent versions\n, \nUse Multiple Concurrent Service Versions\nsemantic\n, \nUse Semantic Versioning\nvertical scaling\n, \nGo Bigger\nvirtual private clouds (VPC)\n, \nNetwork Segregation\nvirtualization\nhypervisors\n, \nTraditional Virtualization\ntraditional\n, \nTraditional Virtualization\ntype 2\n, \nTraditional Virtualization\nvirtualization platforms\nDocker\n, \nDocker\nLinux containers\n, \nLinux Containers\non-demand\n, \nMicroservices\nstorage area networks in\n, \nSpreading Your Risk\nVagrant\n, \nVagrant\nvision\n, \nSummary\nW\nworker-based systems\n, \nWorker-Based Systems\nwrite-behind caches\n, \nCaching for Resilience\nX\nXML\n, \nJSON, XML, or Something Else?\nZ\nZed Attack Proxy (ZAP)\n, \nBaking Security In\nZipkin\n, \nCorrelation IDs\nZookeeper\n, \nZookeeper\nAbout the Author\nSam Newman\n is a technologist at ThoughtWorks, where he currently splits his time\nbetween helping clients and working as an architect for ThoughtWorks’ own internal\nsystems. He has worked with a variety of companies in multiple domains around the\nworld, often with one foot in the developer world, and another in the IT operations space.\nIf you asked him what he does, he’d say, “I work with people to build better software\nsystems.” He has written articles, presented at conferences, and sporadically commits to\nopen source projects.\nColophon\nThe animals on the cover of \nBuilding Microservices\n are honey bees (of the genus \nApis\n). Of\n20,000 known species of bees, only seven are considered honey bees. They are distinct\nbecause they produce and store honey, as well as building hives from wax. Beekeeping to\ncollect honey has been a human pursuit for thousands of years.\nHoney bees live in hives with thousands of individuals and have a very organized social\nstructure. There are three castes: queen, drone, and worker. Each hive has one queen, who\nremains fertile for 3–5 years after her mating flight, and lays up to 2,000 eggs per day.\nDrones are male bees who mate with the queen (and die in the act because of their barbed\nsex organs). Worker bees are sterile females who fill many roles during their lifetime, such\nas nursemaid, construction worker, grocer, guard, undertaker, and forager. Foraging\nworker bees communicate with one another by “dancing” in particular patterns to share\ninformation about nearby resources.\nAll three castes of honey bee are similar in appearance, with wings, six legs, and a body\nsegmented into a head, thorax, and abdomen. They have short fuzzy hairs in a striped\nyellow and black pattern. Their diet is made up exclusively of honey, which is created by a\nprocess of partially digesting and regurgitating sugar-rich flower nectar.\nBees are crucial to agriculture, as they pollinate crops and other flowering plants while\nthey collect pollen and nectar. On average, each hive of bees gathers 66 pounds of pollen a\nyear. In recent years, the decline of many bee species has been cause for concern and is\nknown as “colony collapse disorder.” It is still unclear what is causing this die-off: some\ntheories include parasites, insecticide use, or disease, but no effective preventative\nmeasures have been found to date.\nMany of the animals on O’Reilly covers are endangered; all of them are important to the\nworld. To learn more about how you can help, go to \nanimals.oreilly.com\n.\nThe cover image is from Johnson’s \nNatural History\n. The cover fonts are URW Typewriter\nand Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\nPreface\nWho Should Read This Book\nWhy I Wrote This Book\nA Word on Microservices Today\nNavigating This Book\nConventions Used in This Book\nSafari® Books Online\nHow to Contact Us\nAcknowledgments\n1. Microservices\nWhat Are Microservices?\nSmall, and Focused on Doing One Thing Well\nAutonomous\nKey Benefits\nTechnology Heterogeneity\nResilience\nScaling\nEase of Deployment\nOrganizational Alignment\nComposability\nOptimizing for Replaceability\nWhat About Service-Oriented Architecture?\nOther Decompositional Techniques\nShared Libraries\nModules\nNo Silver Bullet\nSummary\n2. The Evolutionary Architect\nInaccurate Comparisons\nAn Evolutionary Vision for the Architect\nZoning\nA Principled Approach\nStrategic Goals\nPrinciples\nPractices\nCombining Principles and Practices\nA Real-World Example\nThe Required Standard\nMonitoring\nInterfaces\nArchitectural Safety\nGovernance Through Code\nExemplars\nTailored Service Template\nTechnical Debt\nException Handling\nGovernance and Leading from the Center\nBuilding a Team\nSummary\n3. How to Model Services\nIntroducing MusicCorp\nWhat Makes a Good Service?\nLoose Coupling\nHigh Cohesion\nThe Bounded Context\nShared and Hidden Models\nModules and Services\nPremature Decomposition\nBusiness Capabilities\nTurtles All the Way Down\nCommunication in Terms of Business Concepts\nThe Technical Boundary\nSummary\n4. Integration\nLooking for the Ideal Integration Technology\nAvoid Breaking Changes\nKeep Your APIs Technology-Agnostic\nMake Your Service Simple for Consumers\nHide Internal Implementation Detail\nInterfacing with Customers\nThe Shared Database\nSynchronous Versus Asynchronous\nOrchestration Versus Choreography\nRemote Procedure Calls\nTechnology Coupling\nLocal Calls Are Not Like Remote Calls\nBrittleness\nIs RPC Terrible?\nREST\nREST and HTTP\nHypermedia As the Engine of Application State\nJSON, XML, or Something Else?\nBeware Too Much Convenience\nDownsides to REST Over HTTP\nImplementing Asynchronous Event-Based Collaboration\nTechnology Choices\nComplexities of Asynchronous Architectures\nServices as State Machines\nReactive Extensions\nDRY and the Perils of Code Reuse in a Microservice World\nClient Libraries\nAccess by Reference\nVersioning\nDefer It for as Long as Possible\nCatch Breaking Changes Early\nUse Semantic Versioning\nCoexist Different Endpoints\nUse Multiple Concurrent Service Versions\nUser Interfaces\nToward Digital\nConstraints\nAPI Composition\nUI Fragment Composition\nBackends for Frontends\nA Hybrid Approach\nIntegrating with Third-Party Software\nLack of Control\nCustomization\nIntegration Spaghetti\nOn Your Own Terms\nThe Strangler Pattern\nSummary\n5. Splitting the Monolith\nIt’s All About Seams\nBreaking Apart MusicCorp\nThe Reasons to Split the Monolith\nPace of Change\nTeam Structure\nSecurity\nTechnology\nTangled Dependencies\nThe Database\nGetting to Grips with the Problem\nExample: Breaking Foreign Key Relationships\nExample: Shared Static Data\nExample: Shared Data\nExample: Shared Tables\nRefactoring Databases\nStaging the Break\nTransactional Boundaries\nTry Again Later\nAbort the Entire Operation\nDistributed Transactions\nSo What to Do?\nReporting\nThe Reporting Database\nData Retrieval via Service Calls\nData Pumps\nAlternative Destinations\nEvent Data Pump\nBackup Data Pump\nToward Real Time\nCost of Change\nUnderstanding Root Causes\nSummary\n6. Deployment\nA Brief Introduction to Continuous Integration\nAre You Really Doing It?\nMapping Continuous Integration to Microservices\nBuild Pipelines and Continuous Delivery\nAnd the Inevitable Exceptions\nPlatform-Specific Artifacts\nOperating System Artifacts\nCustom Images\nImages as Artifacts\nImmutable Servers\nEnvironments\nService Configuration\nService-to-Host Mapping\nMultiple Services Per Host\nApplication Containers\nSingle Service Per Host\nPlatform as a Service\nAutomation\nTwo Case Studies on the Power of Automation\nFrom Physical to Virtual\nTraditional Virtualization\nVagrant\nLinux Containers\nDocker\nA Deployment Interface\nEnvironment Definition\nSummary\n7. Testing\nTypes of Tests\nTest Scope\nUnit Tests\nService Tests\nEnd-to-End Tests\nTrade-Offs\nHow Many?\nImplementing Service Tests\nMocking or Stubbing\nA Smarter Stub Service\nThose Tricky End-to-End Tests\nDownsides to End-to-End Testing\nFlaky and Brittle Tests\nWho Writes These Tests?\nHow Long?\nThe Great Pile-up\nThe Metaversion\nTest Journeys, Not Stories\nConsumer-Driven Tests to the Rescue\nPact\nIt’s About Conversations\nSo Should You Use End-to-End Tests?\nTesting After Production\nSeparating Deployment from Release\nCanary Releasing\nMean Time to Repair Over Mean Time Between Failures?\nCross-Functional Testing\nPerformance Tests\nSummary\n8. Monitoring\nSingle Service, Single Server\nSingle Service, Multiple Servers\nMultiple Services, Multiple Servers\nLogs, Logs, and Yet More Logs…\nMetric Tracking Across Multiple Services\nService Metrics\nSynthetic Monitoring\nImplementing Semantic Monitoring\nCorrelation IDs\nThe Cascade\nStandardization\nConsider the Audience\nThe Future\nSummary\n9. Security\nAuthentication and Authorization\nCommon Single Sign-On Implementations\nSingle Sign-On Gateway\nFine-Grained Authorization\nService-to-Service Authentication and Authorization\nAllow Everything Inside the Perimeter\nHTTP(S) Basic Authentication\nUse SAML or OpenID Connect\nClient Certificates\nHMAC Over HTTP\nAPI Keys\nThe Deputy Problem\nSecuring Data at Rest\nGo with the Well Known\nIt’s All About the Keys\nPick Your Targets\nDecrypt on Demand\nEncrypt Backups\nDefense in Depth\nFirewalls\nLogging\nIntrusion Detection (and Prevention) System\nNetwork Segregation\nOperating System\nA Worked Example\nBe Frugal\nThe Human Element\nThe Golden Rule\nBaking Security In\nExternal Verification\nSummary\n10. Conway’s Law and System Design\nEvidence\nLoose and Tightly Coupled Organizations\nWindows Vista\nNetflix and Amazon\nWhat Can We Do with This?\nAdapting to Communication Pathways\nService Ownership\nDrivers for Shared Services\nToo Hard to Split\nFeature Teams\nDelivery Bottlenecks\nInternal Open Source\nRole of the Custodians\nMaturity\nTooling\nBounded Contexts and Team Structures\nThe Orphaned Service?\nCase Study: RealEstate.com.au\nConway’s Law in Reverse\nPeople\nSummary\n11. Microservices at Scale\nFailure Is Everywhere\nHow Much Is Too Much?\nDegrading Functionality\nArchitectural Safety Measures\nThe Antifragile Organization\nTimeouts\nCircuit Breakers\nBulkheads\nIsolation\nIdempotency\nScaling\nGo Bigger\nSplitting Workloads\nSpreading Your Risk\nLoad Balancing\nWorker-Based Systems\nStarting Again\nScaling Databases\nAvailability of Service Versus Durability of Data\nScaling for Reads\nScaling for Writes\nShared Database Infrastructure\nCQRS\nCaching\nClient-Side, Proxy, and Server-Side Caching\nCaching in HTTP\nCaching for Writes\nCaching for Resilience\nHiding the Origin\nKeep It Simple\nCache Poisoning: A Cautionary Tale\nAutoscaling\nCAP Theorem\nSacrificing Consistency\nSacrificing Availability\nSacrificing Partition Tolerance?\nAP or CP?\nIt’s Not All or Nothing\nAnd the Real World\nService Discovery\nDNS\nDynamic Service Registries\nZookeeper\nConsul\nEureka\nRolling Your Own\nDon’t Forget the Humans!\nDocumenting Services\nSwagger\nHAL and the HAL Browser\nThe Self-Describing System\nSummary\n12. Bringing It All Together\nPrinciples of Microservices\nModel Around Business Concepts\nAdopt a Culture of Automation\nHide Internal Implementation Details\nDecentralize All the Things\nIndependently Deployable\nIsolate Failure\nHighly Observable\nWhen Shouldn’t You Use Microservices?\nParting Words\nIndex",74520
