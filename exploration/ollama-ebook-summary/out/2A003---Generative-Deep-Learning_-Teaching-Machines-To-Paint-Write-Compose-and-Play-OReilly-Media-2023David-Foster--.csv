filename,title,text,len
01-Cover.pdf,01-Cover,"Second \n Edition\nDavid Foster\nForeword by Karl FristonGenerative\nDeep Learning\nTeaching Machines to Paint, Write,  \nCompose, and PlaySECOND  \nEDITION\nAI / DEEP LEARNING “Generative Deep \nLearning  is an \naccessible introduction \nto the deep learning \ntoolkit for generative \nmodeling. If you are a \ncreative practitioner \nwho loves to tinker \nwith code and want to \napply deep learning \nto your work, then this \nis the book for you.”\n—David Ha\nHead of Strategy, Stability AI\n“An excellent book that \ndives right into all of \nthe major techniques \nbehind state-of-the-\nart generative deep \nlearning. An exciting \nexploration of one of \nthe most fascinating \ndomains in AI!” \n—François Chollet\nCreator of KerasGenerative Deep Learning\nTwitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia Generative AI is the hottest topic in tech. This practical book \nteaches machine learning engineers and data scientists how \nto use TensorFlow and Keras to create impressive generative \ndeep learning models from scratch, including variational \nautoencoders (VAEs), generative adversarial networks \n(GANs), Transformers, normalizing flows, energy-based \nmodels, and denoising diffusion models.\nThe book starts with the basics of deep learning and \nprogresses to cutting-edge architectures. Through tips  \nand tricks, you’ll understand how to make your models  \nlearn more efficiently and become more creative.\n• Discover how VAEs can change facial expressions in photos\n• Train GANs to generate images based on your own dataset\n• Build diffusion models to produce new varieties of flowers\n• Train your own GPT for text generation\n• Learn how large language models like ChatGPT are trained\n• Explore state-of-the-art architectures such as StyleGAN2 \nand ViT-VQGAN\n• Compose polyphonic music using Transformers  \nand MuseGAN\n• Understand how generative world models can solve \nreinforcement learning tasks\n• Dive into multimodal models such as DALL.E 2, Imagen,  \nand Stable Diffusion\nThis book also explores the future of generative AI and  \nhow individuals and companies can proactively begin \nto leverage this remarkable new technology to create \ncompetitive advantage.David Foster  is the cofounder of \nApplied Data Science Partners.\nUS $79.99  CAN $99.99\nISBN: 978-1-098-13418-1SECOND  \nEDITION\nPraise for Generative Deep Learning\nGenerative Deep Learning  is an accessible introduction to the deep learning toolkit for\ngenerative modeling. If you are a creative practitioner who loves to tinker with code and\nwant to apply deep learning to your work, then this is the book for you.\n—David Ha, Head of Strategy, Stability AI\nAn excellent book that dives right into all of the major techniques behind state-of-the-art\ngenerative deep learning. Y ou’ll find intuitive explanations and clever analogies—\nbacked by didactic, highly readable code examples. An exciting\nexploration of one of the most fascinating domains in AI!\n—François Chollet, Creator of Keras\nDavid Foster’s explanations of complex concepts are clear and concise,\nenriched with intuitive visuals, code examples, and exercises.\nAn excellent resource for students and practitioners!\n—Suzana Ilić, Principal Program Manager Responsible AI,\nMicrosoft  Azure OpenAI\nGenerative AI is the next revolutionary step in AI technology that will have a massive\nimpact on the world. This book provides a great introduction to this field\nand its incredible potential and potential risks.\n—Connor Leahy, CEO at Conjecture\nand Cofounder of EleutherAI\nPredicting the world means understanding the world—in all modalities. In that sense,\ngenerative AI is solving the very core of intelligence.\n—Jonas Andrulis, Founder & CEO Aleph Alpha\nGenerative AI is reshaping countless industries and powering a new generation of\ncreative tools. This book is the perfect way to get going with generative modeling and\nstart building with this revolutionary technology yourself.\n—Ed Newton-Rex, VP Audio at Stability AI and composer\nDavid taught me everything I know about machine learning and has a\nknack for explaining the underlying concepts. Generative Deep Learning  is my go-to\nresource for generative AI and lives on a shelf next to my work desk amongst\nmy small collection of favorite technical books.\n—Zack Thoutt,  CPO at AutoSalesVelocity\nGenerative AI is likely to have a profound impact on society. This book gives an\nintroduction to the field that is accessible without skimping on technical detail.\n—Raza Habib, Cofounder of Humanloop\nWhen people ask me how to get started with generative AI, I always recommend David’s\nbook. The second edition is awesome because it covers the strongest models,\nsuch as diffusion models and Transformers. Definitely a must-have for\nanyone interested in computational creativity!\n—Dr. Tristan Behrens, AI Expert and AI Music Artist in\nResidence at KI Salon Heilbronn\nDense in tech knowledge, this is my number one go-to literature when I have ideas\naround generative AI. It should be on every data scientist’s bookshelf.\n—Martin Musiol, Founder of generativeAI.net\nThe book covers the full taxonomy of generative models in excellent detail. One of the\nbest things I found about the book is that it covers the important theory behind the\nmodels as well as solidifying the reader’s understanding with practical examples.\nI must point out that the chapter on GANs is one of the best explanations I have read\nand provides intuitive means to fine-tune your models. The book covers a\nwide range of generative AI modalities including text, image, and music.\nA great resource for anyone getting started with GenAI.\n—Aishwarya Srinivasan, Data Scientist, Google Cloud\nDavid Foster\nForeword by Karl FristonGenerative Deep Learning\nTeaching Machines to Paint, Write,\nCompose, and PlaySECOND EDITION\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing",5988
02-Copyright.pdf,02-Copyright,"978-1-098-13418-1\n[LSI]Generative Deep Learning\nby David Foster\nCopyright © 2023 Applied Data Science Partners Ltd. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Nicole Butterfield\nDevelopment Editor:  Michele Cronin\nProduction Editor:  Christopher Faucher\nCopyeditor:  Charles Roumeliotis\nProofreader:  Rachel HeadIndexer:  Judith McConville\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nJuly 2019:  First Edition\nMay 2023:  Second Edition\nRevision History for the Second Edition\n2023-04-28: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098134181  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Generative Deep Learning , the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the author and do not represent the publisher’s views. While\nthe publisher and the author have used good faith efforts to ensure that the information and instructions\ncontained in this work are accurate, the publisher and the author disclaim all responsibility for errors or\nomissions, including without limitation responsibility for damages resulting from the use of or reliance\non this work. Use of the information and instructions contained in this work is at your own risk. If any\ncode samples or other technology this work contains or describes is subject to open source licenses or the\nintellectual property rights of others, it is your responsibility to ensure that your use thereof complies\nwith such licenses and/or rights.\nFor Alina, the loveliest noise vector of them all.",2076
03-Table of Contents.pdf,03-Table of Contents,"Table of Contents\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xvii\nPart I. Introduction to Generative Deep Learning\n1.Generative Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nWhat Is Generative Modeling?                                                                                       4\nGenerative Versus Discriminative Modeling                                                           5\nThe Rise of Generative Modeling                                                                              6\nGenerative Modeling and AI                                                                                      8\nOur First Generative Model                                                                                           9\nHello World!                                                                                                                  9\nThe Generative Modeling Framework                                                                    10\nRepresentation Learning                                                                                           12\nCore Probability Theory                                                                                               15\nGenerative Model Taxonomy                                                                                       18\nThe Generative Deep Learning Codebase                                                                  20\nCloning the Repository                                                                                             20\nUsing Docker                                                                                                              21\nRunning on a GPU                                                                                                     21\nSummary                                                                                                                         21\n2.Deep Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23\nData for Deep Learning                                                                                                24\nDeep Neural Networks                                                                                                  25\nvii\nWhat Is a Neural Network?                                                                                       25\nLearning High-Level Features                                                                                  26\nTensorFlow and Keras                                                                                               27\nMultilayer Perceptron (MLP)                                                                                       28\nPreparing the Data                                                                                                     28\nBuilding the Model                                                                                                    30\nCompiling the Model                                                                                                 35\nTraining the Model                                                                                                    37\nEvaluating the Model                                                                                                 38\nConvolutional Neural Network (CNN)                                                                      40\nConvolutional Layers                                                                                                 41\nBatch Normalization                                                                                                  46\nDropout                                                                                                                       49\nBuilding the CNN                                                                                                      51\nTraining and Evaluating the CNN                                                                           53\nSummary                                                                                                                         54\nPart II. Methods\n3.Variational Autoencoders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59\nIntroduction                                                                                                                   60\nAutoencoders                                                                                                                  61\nThe Fashion-MNIST Dataset                                                                                   62\nThe Autoencoder Architecture                                                                                63\nThe Encoder                                                                                                                64\nThe Decoder                                                                                                                65\nJoining the Encoder to the Decoder                                                                        67\nReconstructing Images                                                                                              69\nVisualizing the Latent Space                                                                                     70\nGenerating New Images                                                                                            71\nVariational Autoencoders                                                                                             74\nThe Encoder                                                                                                                75\nThe Loss Function                                                                                                      80\nTraining the Variational Autoencoder                                                                    82\nAnalysis of the Variational Autoencoder                                                                84\nExploring the Latent Space                                                                                           85\nThe CelebA Dataset                                                                                                   85\nTraining the Variational Autoencoder                                                                    87\nAnalysis of the Variational Autoencoder                                                                89\nGenerating New Faces                                                                                               90\nviii | Table of Contents\nLatent Space Arithmetic                                                                                            91\nMorphing Between Faces                                                                                          92\nSummary                                                                                                                         93\n4.Generative Adversarial Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\nIntroduction                                                                                                                   96\nDeep Convolutional GAN (DCGAN)                                                                        97\nThe Bricks Dataset                                                                                                     98\nThe Discriminator                                                                                                      99\nThe Generator                                                                                                          101\nTraining the DCGAN                                                                                              104\nAnalysis of the DCGAN                                                                                          109\nGAN Training: Tips and Tricks                                                                              110\nWasserstein GAN with Gradient Penalty (WGAN-GP)                                        113\nWasserstein Loss                                                                                                       114\nThe Lipschitz Constraint                                                                                        115\nEnforcing the Lipschitz Constraint                                                                       116\nThe Gradient Penalty Loss                                                                                      117\nTraining the WGAN-GP                                                                                         119\nAnalysis of the WGAN-GP                                                                                     121\nConditional GAN (CGAN)                                                                                        122\nCGAN Architecture                                                                                                 123\nTraining the CGAN                                                                                                 124\nAnalysis of the CGAN                                                                                             126\nSummary                                                                                                                       127\n5.Autoregressive Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\nIntroduction                                                                                                                 130\nLong Short-Term Memory Network (LSTM)                                                         131\nThe Recipes Dataset                                                                                                 132\nWorking with Text Data                                                                                          133\nTokenization                                                                                                             134\nCreating the Training Set                                                                                        137\nThe LSTM Architecture                                                                                          138\nThe Embedding Layer                                                                                             138\nThe LSTM Layer                                                                                                       140\nThe LSTM Cell                                                                                                         142\nTraining the LSTM                                                                                                   144\nAnalysis of the LSTM                                                                                              146\nRecurrent Neural Network (RNN) Extensions                                                       149\nStacked Recurrent Networks                                                                                  149\nTable of Contents | ix\nGated Recurrent Units                                                                                             151\nBidirectional Cells                                                                                                    153\nPixelCNN                                                                                                                      153\nMasked Convolutional Layers                                                                                154\nResidual Blocks                                                                                                         156\nTraining the PixelCNN                                                                                           158\nAnalysis of the PixelCNN                                                                                       159\nMixture Distributions                                                                                             162\nSummary                                                                                                                       164\n6.Normalizing Flow Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\nIntroduction                                                                                                                 168\nNormalizing Flows                                                                                                      169\nChange of Variables                                                                                                 170\nThe Jacobian Determinant                                                                                      172\nThe Change of Variables Equation                                                                        173\nRealNVP                                                                                                                        174\nThe Two Moons Dataset                                                                                         174\nCoupling Layers                                                                                                       175\nTraining the RealNVP Model                                                                                 181\nAnalysis of the RealNVP Model                                                                            184\nOther Normalizing Flow Models                                                                              186\nGLOW                                                                                                                       186\nFFJORD                                                                                                                     187\nSummary                                                                                                                       188\n7.Energy-Based Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  189\nIntroduction                                                                                                                 189\nEnergy-Based Models                                                                                                  191\nThe MNIST Dataset                                                                                                 192\nThe Energy Function                                                                                               193\nSampling Using Langevin Dynamics                                                                    194\nTraining with Contrastive Divergence                                                                  197\nAnalysis of the Energy-Based Model                                                                    201\nOther Energy-Based Models                                                                                  202\nSummary                                                                                                                       203\n8.Diffusion  Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  205\nIntroduction                                                                                                                 206\nDenoising Diffusion Models (DDM)                                                                        208\nThe Flowers Dataset                                                                                                208\nx | Table of Contents\nThe Forward Diffusion Process                                                                             209\nThe Reparameterization Trick                                                                               210\nDiffusion Schedules                                                                                                 211\nThe Reverse Diffusion Process                                                                               214\nThe U-Net Denoising Model                                                                                  217\nTraining the Diffusion Model                                                                                224\nSampling from the Denoising Diffusion Model                                                  225\nAnalysis of the Diffusion Model                                                                            228\nSummary                                                                                                                       231\nPart III. Applications\n9.Transformers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  235\nIntroduction                                                                                                                 236\nGPT                                                                                                                                236\nThe Wine Reviews Dataset                                                                                     237\nAttention                                                                                                                   238\nQueries, Keys, and Values                                                                                       239\nMultihead Attention                                                                                                241\nCausal Masking                                                                                                        242\nThe Transformer Block                                                                                           245\nPositional Encoding                                                                                                 248\nTraining GPT                                                                                                            250\nAnalysis of GPT                                                                                                        252\nOther Transformers                                                                                                     255\nT5                                                                                                                                256\nGPT-3 and GPT-4                                                                                                    259\nChatGPT                                                                                                                    260\nSummary                                                                                                                       264\n10. Advanced GANs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  267\nIntroduction                                                                                                                 268\nProGAN                                                                                                                        269\nProgressive Training                                                                                                269\nOutputs                                                                                                                      276\nStyleGAN                                                                                                                      277\nThe Mapping Network                                                                                            278\nThe Synthesis Network                                                                                            279\nOutputs from StyleGAN                                                                                         280\nStyleGAN2                                                                                                                    281\nTable of Contents | xi\nWeight Modulation and Demodulation                                                                282\nPath Length Regularization                                                                                    283\nNo Progressive Growing                                                                                         284\nOutputs from StyleGAN2                                                                                       286\nOther Important GANs                                                                                              286\nSelf-Attention GAN (SAGAN)                                                                               286\nBigGAN                                                                                                                     288\nVQ-GAN                                                                                                                   289\nViT VQ-GAN                                                                                                           292\nSummary                                                                                                                       294\n11. Music Generation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  297\nIntroduction                                                                                                                 298\nTransformers for Music Generation                                                                         299\nThe Bach Cello Suite Dataset                                                                                 300\nParsing MIDI Files                                                                                                   300\nTokenization                                                                                                             303\nCreating the Training Set                                                                                        304\nSine Position Encoding                                                                                           305\nMultiple Inputs and Outputs                                                                                  307\nAnalysis of the Music-Generating Transformer                                                  309\nTokenization of Polyphonic Music                                                                        313\nMuseGAN                                                                                                                     317\nThe Bach Chorale Dataset                                                                                      317\nThe MuseGAN Generator                                                                                      320\nThe MuseGAN Critic                                                                                              326\nAnalysis of the MuseGAN                                                                                      327\nSummary                                                                                                                       329\n12. World Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  331\nIntroduction                                                                                                                 331\nReinforcement Learning                                                                                             332\nThe CarRacing Environment                                                                                 334\nWorld Model Overview                                                                                              336\nArchitecture                                                                                                              336\nTraining                                                                                                                     338\nCollecting Random Rollout Data                                                                              339\nTraining the V AE                                                                                                         340\nThe V AE Architecture                                                                                             341\nExploring the V AE                                                                                                   343\nCollecting Data to Train the MDN-RNN                                                                 346\nxii | Table of Contents\nTraining the MDN-RNN                                                                                            346\nThe MDN-RNN Architecture                                                                                347\nSampling from the MDN-RNN                                                                             348\nTraining the Controller                                                                                               348\nThe Controller Architecture                                                                                   349\nCMA-ES                                                                                                                     349\nParallelizing CMA-ES                                                                                              351\nIn-Dream Training                                                                                                      353\nSummary                                                                                                                       356\n13. Multimodal Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  359\nIntroduction                                                                                                                 360\nDALL.E 2                                                                                                                       361\nArchitecture                                                                                                              362\nThe Text Encoder                                                                                                     362\nCLIP                                                                                                                           362\nThe Prior                                                                                                                   367\nThe Decoder                                                                                                             369\nExamples from DALL.E 2                                                                                       373\nImagen                                                                                                                           377\nArchitecture                                                                                                              377\nDrawBench                                                                                                               378\nExamples from Imagen                                                                                           379\nStable Diffusion                                                                                                            380\nArchitecture                                                                                                              380\nExamples from Stable Diffusion                                                                            381\nFlamingo                                                                                                                       381\nArchitecture                                                                                                              382\nThe Vision Encoder                                                                                                 382\nThe Perceiver Resampler                                                                                         383\nThe Language Model                                                                                               385\nExamples from Flamingo                                                                                        388\nSummary                                                                                                                       389\n14. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  391\nTimeline of Generative AI                                                                                          392\n2014–2017: The V AE and GAN Era                                                                      394\n2018–2019: The Transformer Era                                                                          394\n2020–2022: The Big Model Era                                                                              395\nThe Current State of Generative AI                                                                          396\nLarge Language Models                                                                                           396\nTable of Contents | xiii\nText-to-Code Models                                                                                               400\nText-to-Image Models                                                                                             402\nOther Applications                                                                                                   405\nThe Future of Generative AI                                                                                      407\nGenerative AI in Everyday Life                                                                              407\nGenerative AI in the Workplace                                                                            409\nGenerative AI in Education                                                                                    410\nGenerative AI Ethics and Challenges                                                                    411\nFinal Thoughts                                                                                                             413\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  417\nxiv | Table of Contents",31555
04-Foreword.pdf,04-Foreword,"Foreword\nThis book is becoming part of my life. On finding a copy in my living room I asked\nmy son, “When did you get this?” He replied, “When you gave it to me, ” bemused by\nmy senior moment. Going through various sections together, I came to regard Gener‐\native Deep Learning  as the Gray’s Anatomy  of Generative AI.\nThe author dissects the anatomy of generative AI with an incredible clarity and reas‐\nsuring authority. He offers a truly remarkable account of a fast-moving field, under‐\nwritten with pragmatic examples, engaging narratives, and references that are so\ncurrent, it reads like a living history.\nThroughout his deconstructions, the author maintains a sense of wonder and excite‐\nment about the potential of generative AI—especially evident in the book’s compel‐\nling dénouement. Having laid bare the technology, he reminds us that we are at the\ndawn of a new age of intelligence, an age in which generative AI holds a mirror up to\nour language, our art, our creativity; reflecting not just what we have created, but\nwhat we could create—what we can create—limited only by “your own imagination. ”\nThe central theme of generative models in artificial intelligence resonates deeply with\nme, because I see exactly the same themes emerging in the natural sciences; namely, a\nview of ourselves as generative models of our lived world. I suspect in the next edition\nof this book we will read about the confluence of artificial and natural intelligence.\nUntil that time, I will keep this edition next to my copy of Gray’s Anatomy , and other\ntreasures on my bookshelf.\n— Karl Friston, FRS\nProfessor of Neuroscience\nUniversity College London\nxv",1678
05-Prerequisites.pdf,05-Prerequisites,"Preface\nWhat I cannot create, I do not understand.\n—Richard Feynman\nGenerative AI is one of the most revolutionary technologies of our time, transform‐\ning the way we interact with machines. Its potential to revolutionize the way we live,\nwork, and play has been the subject of countless conversations, debates, and predic‐\ntions. But what if there was an even greater potential to this powerful technology?\nWhat if the possibilities of generative AI extend beyond our current imagination?\nThe future of generative AI may be more exciting than we ever thought possible…\nSince  our earliest days, we have sought opportunities to generate original and beauti‐\nful creations. For early humans, this took the form of cave paintings depicting wild\nanimals and abstract patterns, created with pigments placed carefully and methodi‐\ncally onto rock. The Romantic Era gave us the mastery of Tchaikovsky symphonies,\nwith their ability to inspire feelings of triumph and tragedy through sound waves,\nwoven together to form beautiful melodies and harmonies. And in recent times, we\nhave found ourselves rushing to bookshops at midnight to buy stories about a fic‐\ntional wizard, because the combination of letters creates a narrative that wills us to\nturn the page and find out what happens to our hero.\nIt is therefore not surprising that humanity has started to ask the ultimate question of\ncreativity: can we create something that is in itself creative?\nThis is the question that generative AI aims to answer. With recent advances in meth‐\nodology and technology, we are now able to build machines that can paint original\nartwork in a given style, write coherent blocks of text with long-term structure, com‐\npose music that is pleasant to listen to, and develop winning strategies for complex\ngames by generating imaginary future scenarios. This is just the start of a generative\nrevolution that will leave us with no choice but to find answers to some of the biggest\nquestions about the mechanics of creativity, and ultimately, what it means to be\nhuman.\nxvii\nIn short, there has never been a better time to learn about generative AI—so let’s get\nstarted!\nObjective and Approach\nThis  book assumes no prior knowledge of generative AI. We will build up all of the\nkey concepts from scratch in a way that is intuitive and easy to follow, so don’t worry\nif you have no experience with generative AI. Y ou have come to the right place!\nRather than only covering the techniques that are currently in vogue, this book serves\nas a complete guide to generative modeling that covers a broad range of model fami‐\nlies. There is no one technique that is objectively better  or worse  than any other—in\nfact, many state-of-the-art models now mix together ideas from across the broad\nspectrum of approaches to generative modeling. For this reason, it is important to\nkeep abreast of developments across all areas of generative AI, rather than focusing\non one particular kind of technique. One thing is certain: the field of generative AI is\nmoving fast, and you never know where the next groundbreaking idea will come\nfrom!\nWith this in mind, the approach I will take is to show you how to train your own\ngenerative models on your own data, rather than relying on pre-trained off-the-shelf\nmodels. While there are now many impressive open source generative models that\ncan be downloaded and run in a few lines of code, the aim of this book is to dig\ndeeper into their architecture and design from first principles, so that you gain a\ncomplete understanding of how they work and can code up examples of each techni‐\nque from scratch using Python and Keras.\nIn summary, this book can be thought of as a map of the current generative AI land‐\nscape that covers both theory and practical applications, including full working\nexamples of key models from the literature. We will walk through the code for each\nstep by step, with clear signposts that show how the code implements the theory\nunderpinning each technique. This book can be read cover to cover or used as a ref‐\nerence book that you can dip into. Above all, I hope you find it a useful and enjoyable\nread!\nThroughout the book, you will find short, allegorical stories that\nhelp explain the mechanics of some of the models we will be build‐\ning. I believe that one of the best ways to teach a new abstract\ntheory is to first convert it into something that isn’t quite so\nabstract, such as a story, before diving into the technical explana‐\ntion. The story and the model explanation are just the same\nmechanics explained in two different domains—you might there‐\nfore find it useful to refer back to the relevant story while learning\nabout the technical details of each model!\nxviii | Preface",4794
06-Roadmap.pdf,06-Roadmap,"Prerequisites\nThis book assumes that you have experience coding in Python. If you are not familiar\nwith Python, the best place to start is through LearnPython.org . There are many free\nresources online that will allow you to develop enough Python knowledge to work\nwith the examples in this book.\nAlso, since some of the models are described using mathematical notation, it will be\nuseful to have a solid understanding of linear algebra (for example, matrix multiplica‐\ntion) and general probability theory. A useful resource is Deisenroth et al. ’s book\nMathematics for Machine Learning  (Cambridge University Press), which is freely\navailable.\nThe book assumes no prior knowledge of generative modeling (we will examine the\nkey concepts in Chapter 1 ) or TensorFlow and Keras (these libraries will be intro‐\nduced in Chapter 2 ).\nRoadmap\nThis book is divided into three parts.\nPart I  is a general introduction to generative modeling and deep learning, where we\nexplore the core concepts that underpin all of the techniques in later parts of the\nbook:\n•In Chapter 1, “Generative Modeling” , we define generative modeling and con‐\nsider a toy example that we can use to understand some of the key concepts that\nare important to all generative models. We also lay out the taxonomy of genera‐\ntive model families that we will explore in Part II  of this book.\n•In Chapter 2, “Deep Learning” , we begin our exploration of deep learning and\nneural networks by building our first example of a multilayer perceptron (MLP)\nusing Keras. We then adapt this to include convolutional layers and other\nimprovements, to observe the difference in performance.\nPart II  walks through the six key techniques that we will be using to build generative\nmodels, with practical examples for each:\n•In Chapter 3, “Variational Autoencoders” , we consider the variational autoen‐\ncoder (V AE) and see how it can be used to generate images of faces and morph\nbetween faces in the model’s latent space.\n•In Chapter 4, “Generative Adversarial Networks” , we explore generative adversa‐\nrial networks (GANs) for image generation, including deep convolutional GANs,\nconditional GANs, and improvements such as the Wasserstein GAN that make\nthe training process more stable.\nPreface | xix\n•In Chapter 5, “ Autoregressive Models” , we turn our attention to autoregressive\nmodels, starting with an introduction to recurrent neural networks such as long\nshort-term memory networks (LSTMs) for text generation and PixelCNN for\nimage generation.\n•In Chapter 6, “Normalizing Flow Models” , we focus on normalizing flows,\nincluding an intuitive theoretical exploration of the technique and a practical\nexample of how to build a RealNVP model to generate images.\n•In Chapter 7, “Energy-Based Models” , we cover energy-based models, including\nimportant methods such as how to train using contrastive divergence and sample\nusing Langevin dynamics.\n•In Chapter 8, “Diffusion Models” , we dive into a practical guide to building diffu‐\nsion models, which drive many state-of-the-art image generation models such as\nDALL.E 2 and Stable Diffusion.\nFinally, in Part III  we build on these foundations to explore the inner workings of\nstate-of-the-art models for image generation, writing, composing music, and model-\nbased reinforcement learning:\n•In Chapter 9, “Transformers” , we explore the lineage and technical details of the\nStyleGAN models, as well as other state-of-the-art GANs for image generation\nsuch as VQ-GAN.\n•In Chapter 10, “ Advanced GANs” , we consider the Transformer architecture,\nincluding a practical walkthrough for building your own version of GPT for text\ngeneration.\n•In Chapter 11, “Music Generation” , we turn our attention to music generation,\nincluding a guide to working with music data and application of techniques such\nas Transformers and MuseGAN.\n•In Chapter 12, “World Models” , we see how generative models can be used in the\ncontext of reinforcement learning, with the application of world models and\nTransformer-based methods.\n•In Chapter 13, “Multimodal Models” , we explain the inner workings of four\nstate-of-the-art multimodal models that incorporate more than one type of data,\nincluding DALL.E 2, Imagen, and Stable Diffusion for text-to-image generation\nand Flamingo, a visual language model.\n•In Chapter 14, “Conclusion” , we recap the key milestones of generative AI to date\nand discuss the ways in which generative AI will revolutionize our daily lives in\nyears to come.\nxx | Preface",4558
07-Other Resources.pdf,07-Other Resources,"Changes in the Second Edition\nThank you to everyone who read the first edition of this book—I am really pleased\nthat so many of you have found it a useful resource and provided feedback on things\nthat you would like to see in the second edition. The field of generative deep learning\nhas progressed significantly since the first edition was published in 2019, so as well as\nrefreshing the existing content I have added several new chapters to bring the mate‐\nrial in line with the current state of the art.\nThe following is a summary of the main updates, in terms of the individual chapters\nand general book improvements:\n•Chapter 1  now includes a section on the different families of generative models\nand a taxonomy of how they are related.\n•Chapter 2  contains improved diagrams and more detailed explanations of key\nconcepts.\n•Chapter 3  is refreshed with a new worked example and accompanying\nexplanations.\n•Chapter 4  now includes an explanation of conditional GAN architectures.\n•Chapter 5  now includes a section on autoregressive models for images (e.g.,\nPixelCNN).\n•Chapter 6  is an entirely new chapter, describing the RealNVP model.\n•Chapter 7  is also a new chapter, focusing on techniques such as Langevin dynam‐\nics and contrastive divergence.\n•Chapter 8  is a newly written chapter on denoising the diffusion models that\npower many of today’s state-of-the-art applications.\n•Chapter 9  is an expansion of the material provided in the conclusion of the first\nedition, with deeper focus on architectures of the various StyleGAN models and\nnew material on VQ-GAN.\n•Chapter 10  is a new chapter that explores the Transformer architecture in detail.\n•Chapter 11  includes modern Transformer architectures, replacing the LSTM\nmodels from the first edition.\n•Chapter 12  includes updated diagrams and descriptions, with a section on how\nthis approach is informing state-of-the-art reinforcement learning today.\n•Chapter 13  is a new chapter that explains in detail how impressive models like\nDALL.E 2, Imagen, Stable Diffusion, and Flamingo work.\n•Chapter 14  is updated to reflect the outstanding progress in the field since the\nfirst edition and give a more complete and detailed view of where generative AI is\nheading in the future.\nPreface | xxi",2295
08-Using Code Examples.pdf,08-Using Code Examples,"•All comments given as feedback to the first edition and typos identified have\nbeen addressed (to the best of my knowledge!).\n•Chapter goals have been added at the start of each chapter, so that you can see the\nkey topics covered in the chapter before you start reading.\n•Some of the allegorical stories have been rewritten to be more concise and\nclear—I  am pleased that so many readers have said that the stories have helped\nthem to better understand the key concepts!\n•The headings and subheadings of each chapter have been aligned so that is it\nclear which parts of the chapter are focused on explanation and which are\nfocused on building your own models.\nOther Resources\nI highly recommend the following books as general introductions to machine learn‐\ning and deep learning:\n•Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts,\nTools, and Techniques to Build Intelligent Systems  by Aurélien Géron (O’Reilly)\n•Deep Learning with Python  by Francois Chollet (Manning)\nMost of the papers in this book are sourced through arXiv , a free repository of scien‐\ntific research papers. It is now common for authors to post papers to arXiv before\nthey are fully peer-reviewed. Reviewing the recent submissions is a great way to keep\non top of the most cutting-edge developments in the field.\nI also highly recommend the website Papers with Code , where you can find the latest\nstate-of-the-art results in a variety of machine learning tasks, alongside links to the\npapers and official GitHub repositories. It is an excellent resource for anyone wanting\nto quickly understand which techniques are currently achieving the highest scores in\na range of tasks and has certainly helped me to decide which techniques to include in\nthis book.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for commands and program listings, as well as within paragraphs to refer to\nprogram elements such as variable or function names.\nxxii | Preface\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element signifies a warning or caution.\nCodebase\nThe code examples in this book can be found in a GitHub repository . I have deliber‐\nately ensured that none of the models require prohibitively large amounts of compu‐\ntational resources to train, so that you can start training your own models without\nhaving to spend lots of time or money on expensive hardware. There is a comprehen‐\nsive guide in the repository on how to get started with Docker and set up cloud\nresources with GPUs on Google Cloud if required.\nThe following changes have been made to the codebase since the first edition:\n•All examples are now runnable from within a single notebook, instead of some\ncode being imported from modules across the codebase. This is so that you can\nrun each example cell by cell and delve into exactly how each model is built, piece\nby piece.\n•The sections of each notebook are now broadly aligned between examples.\n•Many of the examples in this book now utilize code snippets from the amazing\nopen source Keras repository —this is to avoid creating a completely detached\nopen source repository of Keras generative AI examples, when there already exist\nexcellent implementations available through the Keras website. I have added ref‐\nerences and links to the original authors of code that I have utilized from the\nKeras website throughout this book and in the repository.\nPreface | xxiii",3752
09-How to Contact Us.pdf,09-How to Contact Us,"•I have added new data sources and improved the data collection process from the\nfirst edition—now, there is a script that can be easily run to collect data from the\nrequired sources in order to train the examples in the book, using tools such as\nthe Kaggle API .\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition .\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com .\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. Y ou do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing examples from O’Reilly\nbooks does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require\npermission.\nWe appreciate, but do not require, attribution. An attribution usually includes the\ntitle, author, publisher, and ISBN. For example: “ Generative Deep Learning , 2nd edi‐\ntion, by David Foster (O’Reilly). Copyright 2023 Applied Data Science Partners Ltd.,\n978-1-098-13418-1. ”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com .\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com .\nxxiv | Preface",2275
10-Acknowledgments.pdf,10-Acknowledgments,"How to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/generative-dl .\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit https://oreilly.com .\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\nFollow us on Twitter: https://twitter.com/oreillymedia\nWatch us on Y ouTube: https://youtube.com/oreillymedia\nAcknowledgments\nThere are so many people I would like to thank for helping me write this book.\nFirst, I would like to thank everyone who has taken time to technically review the\nbook—in particular Vishwesh Ravi Shrimali, Lipi Deepaakshi Patnaik, Luba Elliot,\nand Lorna Barclay. Thanks also to Samir Bico for helping to review and test the code‐\nbase that accompanies this book. Y our input has been invaluable.\nAlso, a huge thanks to my colleagues at Applied Data Science Partners , Ross Witeszc‐\nzak, Amy Bull, Ali Parandeh, Zine Eddine, Joe Rowe, Gerta Salillari, Aleshia Parkes,\nEvelina Kireilyte, Riccardo Tolli, Mai Do, Khaleel Syed, and Will Holmes. Y our\npatience with me while I have taken time to finish the book is hugely appreciated, and\nI am greatly looking forward to all the machine learning projects we will complete\ntogether in the future! Particular thanks to Ross—had we not decided to start a busi‐\nness together, this book might never have taken shape, so thank you for believing in\nme as your business partner!\nI also want to thank anyone who has ever taught me anything mathematical—I was\nextremely fortunate to have fantastic math teachers at school, who developed my\ninterest in the subject and encouraged me to pursue it further at university. I would\nPreface | xxv\nlike to thank you for your commitment and for going out of your way to share your\nknowledge of the subject with me.\nA huge thank you goes to the staff at O’Reilly for guiding me through the process of\nwriting this book. A special thanks goes to Michele Cronin, who has been there at\neach step, providing useful feedback and sending me friendly reminders to keep com‐\npleting chapters! Also to Nicole Butterfield, Christopher Faucher, Charles Roumelio‐\ntis, and Suzanne Huston for getting the book into production, and Mike Loukides for\nfirst reaching out to ask if I’ d be interested in writing a book. Y ou have all been so\nsupportive of this project from the start, and I want to thank you for providing me\nwith a platform on which to write about something that I love.\nThroughout the writing process, my family has been a constant source of encourage‐\nment and support. A huge thank you goes to my mum, Gillian Foster, for checking\nevery single line of text for typos and for teaching me how to add up in the first place!\nY our attention to detail has been extremely helpful while proofreading this book, and\nI’m really grateful for all the opportunities that both you and dad have given me. My\ndad, Clive Foster, originally taught me how to program a computer—this book is full\nof practical examples, and that’s thanks to his early patience while I fumbled around\nin BASIC trying to make football games as a teenager. My brother, Rob Foster, is the\nmost modest genius you will ever find, particularly within linguistics—chatting with\nhim about AI and the future of text-based machine learning has been amazingly\nhelpful. Last, I would like to thank my Nana, who was always a constant source of\ninspiration and fun for all of us. Her love of literature was one of the reasons I first\ndecided that writing a book would be an exciting thing to do.\nI would also like to thank my wife, Lorna Barclay. As well as providing me with end‐\nless support and cups of tea throughout the writing process, you have rigorously\nchecked every word of this book in meticulous detail. I couldn’t have done it without\nyou. Thank you for always being there for me, and for making this journey so much\nmore enjoyable. I promise I won’t talk about generative AI at the dinner table for at\nleast a few days after the book is published.\nLastly, I would like to thank our beautiful baby daughter Alina for providing endless\nentertainment during the long nights of book-writing. Y our adorable giggles have\nbeen the perfect background music to my typing. Thanks for being my inspiration\nand for always keeping me on my toes. Y ou’re the real brains behind this operation.\nxxvi | Preface",4795
11-Part I. Introduction to Generative  Deep Learning.pdf,11-Part I. Introduction to Generative  Deep Learning,"PART I\nIntroduction to Generative\nDeep Learning\nPart I  is a general introduction to generative modeling and deep learning—the two\nfields that we need to understand in order to get started with generative deep\nlearning!\nIn Chapter 1  we will define generative modeling and consider a toy example that we\ncan use to understand some of the key concepts that are important to all generative\nmodels. We will also lay out the taxonomy of generative model families that we will\nexplore in Part II  of this book.\nChapter 2  provides a guide to the deep learning tools and techniques that we will\nneed to start building more complex generative models. In particular, we will build\nour first example of a deep neural network—a multilayer perceptron (MLP)—using\nKeras. We will then adapt this to include convolutional layers and other improve‐\nments, to observe the difference in performance.\nBy the end of Part I  you will have a good understanding of the core concepts that\nunderpin all of the techniques in later parts of the book.",1040
12-The Rise of Generative Modeling.pdf,12-The Rise of Generative Modeling,"CHAPTER 1\nGenerative Modeling\nChapter Goals\nIn this chapter you will:\n•Learn the key differences between generative and discriminative models.\n•Understand the desirable properties of a generative model through a simple\nexample.\n•Learn about the core probabilistic concepts that underpin generative models.\n•Explore the different families of generative models.\n•Clone the codebase that accompanies this book, so that you can get started build‐\ning generative models!\nThis chapter is a general introduction to the field of generative modeling.\nWe will start with a gentle theoretical introduction to generative modeling and see\nhow it is the natural counterpart to the more widely studied discriminative modeling.\nWe will then establish a framework that describes the desirable properties that a good\ngenerative model should have. We will also lay out the core probabilistic concepts\nthat are important to know, in order to fully appreciate how different approaches\ntackle the challenge of generative modeling.\nThis will lead us naturally to the penultimate section, which lays out the six broad\nfamilies of generative models that dominate the field today. The final section explains\nhow to get started with the codebase that accompanies this book.\n3\nWhat Is Generative Modeling?\nGenerative modeling can be broadly defined as follows:\nGenerative modeling is a branch of machine learning that involves training a model to\nproduce new data that is similar to a given dataset.\nWhat does this mean in practice? Suppose we have a dataset containing photos of\nhorses. We can train  a generative model on this dataset to capture the rules that gov‐\nern the complex relationships between pixels in images of horses. Then we can sam‐\nple from this model to create novel, realistic images of horses that did not exist in the\noriginal dataset. This process is illustrated in Figure 1-1 .\nFigure 1-1. A generative model trained to generate realistic photos of horses\nIn order to build a generative model, we require a dataset consisting of many exam‐\nples of the entity we are trying to generate. This is known as the training data , and\none such data point is called an observation .\nEach observation consists of many features . For an image generation problem, the\nfeatures are usually the individual pixel values; for a text generation problem, the fea‐\ntures could be individual words or groups of letters. It is our goal to build a model\nthat can generate new sets of features that look as if they have been created using the\nsame rules as the original data. Conceptually, for image generation this is an incredi‐\nbly difficult task, considering the vast number of ways that individual pixel values can\nbe assigned and the relatively tiny number of such arrangements that constitute an\nimage of the entity we are trying to generate.\nA generative model must also be probabilistic  rather than deterministic , because we\nwant to be able to sample many different variations of the output, rather than get the\nsame output every time. If our model is merely a fixed calculation, such as taking the\naverage value of each pixel in the training dataset, it is not generative. A generative\n4 | Chapter 1: Generative Modeling\nmodel must include a random component that influences the individual samples gen‐\nerated by the model.\nIn other words, we can imagine that there is some unknown probabilistic distribution\nthat explains why some images are likely to be found in the training dataset and other\nimages are not. It is our job to build a model that mimics this distribution as closely\nas possible and then sample from it to generate new, distinct observations that look as\nif they could have been included in the original training set.\nGenerative Versus Discriminative Modeling\nIn order to truly understand what generative modeling aims to achieve and why this\nis important, it is useful to compare it to its counterpart, discriminative modeling . If\nyou have studied machine learning, most problems you will have faced will have most\nlikely been discriminative in nature. To understand the difference, let’s look at an\nexample.\nSuppose we have a dataset of paintings, some painted by Van Gogh and some by\nother artists. With enough data, we could train a discriminative model to predict if a\ngiven painting was painted by Van Gogh. Our model would learn that certain colors,\nshapes, and textures are more likely to indicate that a painting is by the Dutch master,\nand for paintings with these features, the model would upweight its prediction\naccordingly. Figure 1-2  shows the discriminative modeling process—note how it dif‐\nfers from the generative modeling process shown in Figure 1-1 .\nFigure 1-2. A discriminative model trained to predict if a given image is painted by\nVan Gogh\nWhen performing discriminative modeling, each observation in the training data has\na label . For a binary classification problem such as our artist discriminator, Van Gogh\npaintings would be labeled 1 and non–Van Gogh paintings labeled 0. Our model then\nWhat Is Generative Modeling? | 5\nlearns how to discriminate between these two groups and outputs the probability that\na new observation has label 1—i.e., that it was painted by Van Gogh.\nIn contrast, generative modeling doesn’t require the dataset to be labeled because it\nconcerns itself with generating entirely new images, rather than trying to predict a\nlabel of a given image.\nLet’s define these types of modeling formally, using mathematical notation:\nDiscriminative modeling  estimates py.\nThat is, discriminative modeling aims to model the probability of a label y given some\nobservation .\nGenerative modeling  estimates p.\nThat is, generative modeling aims to model the probability of observing an observa‐\ntion . Sampling from this distribution allows us to generate new observations.\nConditional Generative Models\nNote that we can also build a generative model to model the condi‐\ntional probability py—the probability of seeing an observation\n with a specific label y.\nFor example, if our dataset contains different types of fruit, we\ncould tell our generative model to specifically generate an image of\nan apple.\nAn important point to note is that even if we were able to build a perfect discrimina‐\ntive model to identify Van Gogh paintings, it would still have no idea how to create a\npainting that looks like a Van Gogh. It can only output probabilities against existing\nimages, as this is what it has been trained to do. We would instead need to train a\ngenerative model and sample from this model to generate images that have a high\nchance of belonging to the original training dataset.\nThe Rise of Generative Modeling\nUntil recently, discriminative modeling has been the driving force behind most pro‐\ngress in machine learning. This is because for any discriminative problem, the corre‐\nsponding generative modeling problem is typically much more difficult to tackle. For\nexample, it is much easier to train a model to predict if a painting is by Van Gogh\nthan it is to train a model to generate a Van Gogh–style painting from scratch.\n6 | Chapter 1: Generative Modeling\nSimilarly , it is much easier to train a model to predict if a page of text was written by\nCharles Dickens than it is to build a model to generate a set of paragraphs in the style\nof Dickens. Until recently, most generative challenges were simply out of reach and\nmany doubted that they could ever be solved. Creativity was considered a purely\nhuman capability that couldn’t be rivaled by AI.\nHowever, as machine learning technologies have matured, this assumption has grad‐\nually weakened. In the last 10 years many of the most interesting advancements in the\nfield have come through novel applications of machine learning to generative model‐\ning tasks. For example, Figure 1-3  shows the striking progress that has already been\nmade in facial image generation since 2014.\nFigure 1-3. Face generation using generative modeling has improved significantly  over\nthe last decade (adapted from Brundage et al., 2018 )1\nAs well as being easier to tackle, discriminative modeling has historically been more\nreadily applicable to practical problems across industry than generative modeling.\nFor example, a doctor may benefit from a model that predicts if a given retinal image\nshows signs of glaucoma, but wouldn’t necessarily benefit from a model that can gen‐\nerate novel pictures of the back of an eye.\nHowever, this is also starting to change, with the proliferation of companies offering\ngenerative services that target specific business problems. For example, it is now pos‐\nsible to access APIs that generate original blog posts given a particular subject matter,\nproduce a variety of images of your product in any setting you desire, or write social\nmedia content and ad copy to match your brand and target message. There are also\nclear positive applications of generative AI for industries such as game design and\ncinematography, where models trained to output video and music are beginning to\nadd value.\nWhat Is Generative Modeling? | 7",9188
13-Our First Generative Model.pdf,13-Our First Generative Model,"Generative Modeling and AI\nAs well as the practical uses of generative modeling (many of which are yet to be dis‐\ncovered), there are three deeper reasons why generative modeling can be considered\nthe key to unlocking a far more sophisticated form of artificial intelligence that goes\nbeyond what discriminative modeling alone can achieve.\nFirstly, purely from a theoretical point of view, we shouldn’t limit our machine train‐\ning to simply categorizing data. For completeness, we should also be concerned with\ntraining models that capture a more complete understanding of the data distribution,\nbeyond any particular label. This is undoubtedly a more difficult problem to solve,\ndue to the high dimensionality of the space of feasible outputs and the relatively small\nnumber of creations that we would class as belonging to the dataset. However, as we\nshall see, many of the same techniques that have driven development in discrimina‐\ntive modeling, such as deep learning, can be utilized by generative models too.\nSecondly, as we shall see in Chapter 12 , generative modeling is now being used to\ndrive progress in other fields of AI, such as reinforcement learning (the study of\nteaching agents to optimize a goal in an environment through trial and error). Sup‐\npose we want to train a robot to walk across a given terrain. A traditional approach\nwould be to run many experiments where the agent tries out different strategies in the\nterrain, or a computer simulation of the terrain. Over time the agent would learn\nwhich strategies are more successful than others and therefore gradually improve. A\nchallenge with this approach is that it is fairly inflexible because it is trained to opti‐\nmize the policy for one particular task. An alternative approach that has recently\ngained traction is to instead train the agent to learn a world model  of the environment\nusing a generative model, independent of any particular task. The agent can quickly\nadapt to new tasks by testing strategies in its own world model, rather than in the real\nenvironment, which is often computationally more efficient and does not require\nretraining from scratch for each new task.\nFinally, if we are to truly say that we have built a machine that has acquired a form of\nintelligence that is comparable to a human’s, generative modeling must surely be part\nof the solution. One of the finest examples of a generative model in the natural world\nis the person reading this book. Take a moment to consider what an incredible gener‐\native model you are. Y ou can close your eyes and imagine what an elephant would\nlook like from any possible angle. Y ou can imagine a number of plausible different\nendings to your favorite TV show, and you can plan your week ahead by working\nthrough various futures in your mind’s eye and taking action accordingly. Current\nneuroscientific theory suggests that our perception of reality is not a highly complex\ndiscriminative model operating on our sensory input to produce predictions of what\nwe are experiencing, but is instead a generative model that is trained from birth to\nproduce simulations of our surroundings that accurately match the future. Some the‐\nories even suggest that the output from this generative model is what we directly\n8 | Chapter 1: Generative Modeling",3332
14-The Generative Modeling Framework.pdf,14-The Generative Modeling Framework,"perceive  as reality. Clearly, a deep understanding of how we can build machines to\nacquire this ability will be central to our continued understanding of the workings of\nthe brain and general artificial intelligence.\nOur First Generative Model\nWith this in mind, let’s begin our journey into the exciting world of generative model‐\ning. To begin with, we’ll look at a toy example of a generative model and introduce\nsome of the ideas that will help us to work through the more complex architectures\nthat we will encounter later in the book.\nHello World!\nLet’s start by playing a generative modeling game in just two dimensions. I have\nchosen a rule that has been used to generate the set of points  in Figure 1-4 . Let’s call\nthis rule pdata. Y our challenge is to choose a different point =x1,x2 in the space\nthat looks like it has been generated by the same rule.\nFigure 1-4. A set of points in two dimensions, generated by an unknown rule pdata\nWhere did you choose? Y ou probably used your knowledge of the existing data points\nto construct a mental model, pmodel, of whereabouts in the space the point is more\nlikely to be found. In this respect, pmodel is an estimate  of pdata. Perhaps you decided\nthat pmodel should look like Figure 1-5 —a rectangular box where points may be\nfound, and an area outside of the box where there is no chance of finding any points.\nOur First Generative Model | 9\nFigure 1-5. The orange box, pmodel, is an estimate of the true data-generating distribu‐\ntion, pdata\nTo generate a new observation, you can simply choose a point at random within the\nbox, or more formally, sample  from the distribution pmodel. Congratulations, you have\njust built your first generative model! Y ou have used the training data (the black\npoints) to construct a model (the orange region) that you can easily sample from to\ngenerate other points that appear to belong to the training set.\nLet’s now formalize this thinking into a framework that can help us understand what\ngenerative modeling is trying to achieve.\nThe Generative Modeling Framework\nWe can capture our motivations and goals for building a generative model in the fol‐\nlowing framework.\nThe Generative Modeling Framework\n•We have a dataset of observations .\n•We assume that the observations have been generated according to some\nunknown distribution, pdata.\n•We want to build a generative model pmodel that mimics pdata. If we achieve this\ngoal, we can sample from pmodel to generate observations that appear to have\nbeen drawn from pdata.\n•Therefore, the desirable properties of pmodel are:\n10 | Chapter 1: Generative Modeling\nAccuracy\nIf pmodel is high for a generated observation, it should look like it has been\ndrawn from pdata. If \(p_{model}\)  is low for a generated observation, it\nshould not look like it has been drawn from pdata.\nGeneration\nIt should be possible to easily sample a new observation from pmodel.\nRepresentation\nIt should be possible to understand how different high-level features in the\ndata are represented by pmodel.\nLet’s now reveal the true data-generating distribution, pdata, and see how the frame‐\nwork applies to this example. As we can see from Figure 1-6 , the data-generating rule\nis simply a uniform distribution over the land mass of the world, with no chance of\nfinding a point in the sea.\nFigure 1-6. The orange box, pmodel, is an estimate of the true data-generating distribu‐\ntion, pdata (the gray area)\nClearly, our model, pmodel, is an oversimplification of pdata. We can inspect points A,\nB, and C to understand the successes and failures of our model in terms of how accu‐\nrately it mimics pdata:\n•Point A is an observation that is generated by our model but does not appear to\nhave been generated by pdata as it’s in the middle of the sea.\nOur First Generative Model | 11",3863
15-Representation Learning.pdf,15-Representation Learning,"•Point B could never have been generated by pmodel as it sits outside the orange\nbox. Therefore, our model has some gaps in its ability to produce observations\nacross the entire range of potential possibilities.\n•Point C is an observation that could be generated by pmodel and also by pdata.\nDespite its shortcomings, the model is easy to sample from, because it is simply a uni‐\nform distribution over the orange box. We can easily choose a point at random from\ninside this box, in order to sample from it.\nAlso, we can certainly say that our model is a simple representation of the underlying\ncomplex distribution that captures some of the underlying high-level features. The\ntrue distribution is separated into areas with lots of land mass (continents) and those\nwith no land mass (the sea). This is a high-level feature that is also true of our model,\nexcept we have one large continent, rather than many.\nThis example has demonstrated the fundamental concepts behind generative model‐\ning. The problems we will be tackling in this book will be far more complex and high-\ndimensional, but the underlying framework through which we approach the problem\nwill be the same.\nRepresentation Learning\nIt is worth delving a little deeper into what we mean by learning a representation  of\nthe high-dimensional data, as it is a topic that will recur throughout this book.\nSuppose you wanted to describe your appearance to someone who was looking for\nyou in a crowd of people and didn’t know what you looked like. Y ou wouldn’t start by\nstating the color of pixel 1 of a photo of you, then pixel 2, then pixel 3, etc. Instead,\nyou would make the reasonable assumption that the other person has a general idea\nof what an average human looks like, then amend this baseline with features that\ndescribe groups of pixels, such as I have very blond hair  or I wear glasses . With no\nmore than 10 or so of these statements, the person would be able to map the descrip‐\ntion back into pixels to generate an image of you in their head. The image wouldn’t be\nperfect, but it would be a close enough likeness to your actual appearance for them to\nfind you among possibly hundreds of other people, even if they’ve never seen you\nbefore.\n12 | Chapter 1: Generative Modeling\nThis is the core idea behind representation learning . Instead of trying to model the\nhigh-dimensional sample space directly, we describe each observation in the training\nset using some lower-dimensional latent space  and then learn a mapping function\nthat can take a point in the latent space and map it to a point in the original domain.\nIn other words, each point in the latent space is a representation  of some high-\ndimensional observation.\nWhat does this mean in practice? Let’s suppose we have a training set consisting of\ngrayscale images of biscuit tins ( Figure 1-7 ).\nFigure 1-7. The biscuit tin dataset\nTo us, it is obvious that there are two features that can uniquely represent each of\nthese tins: the height and width of the tin. That is, we can convert each image of a tin\nto a point in a latent space of just two dimensions, even though the training set of\nimages is provided in high-dimensional pixel space. Notably, this means that we can\nalso produce images of tins that do not exist in the training set, by applying a suitable\nmapping function f to a new point in the latent space, as shown in Figure 1-8 .\nRealizing that the original dataset can be described by the simpler latent space is not\nso easy for a machine—it would first need to establish that height and width are the\ntwo latent space dimensions that best describe this dataset, then learn the mapping\nfunction f that can take a point in this space and map it to a grayscale biscuit tin\nimage. Machine learning (and specifically, deep learning) gives us the ability to train\nmachines that can find these complex relationships without human guidance.\nOur First Generative Model | 13\nFigure 1-8. The 2D latent space of biscuit tins and the function f that maps a point in\nthe latent space back to the original image domain\nOne of the benefits of training models that utilize a latent space is that we can per‐\nform operations that affect high-level properties of the image by manipulating its rep‐\nresentation vector within the more manageable latent space. For example, it is not\nobvious how to adjust the shading of every single pixel to make an image of a biscuit\ntin taller . However, in the latent space, it’s simply a case of increasing the height  latent\ndimension, then applying the mapping function to return to the image domain. We\nshall see an explicit example of this in the next chapter, applied not to biscuit tins but\nto faces.\nThe concept of encoding the training dataset into a latent space so that we can sample\nfrom it and decode the point back to the original domain is common to many genera‐\ntive modeling techniques, as we shall see in later chapters of this book. Mathemati‐\ncally speaking, encoder-decoder  techniques try to transform the highly nonlinear\nmanifold  on which the data lies (e.g., in pixel space) into a simpler latent space that\ncan be sampled from, so that it is likely that any point in the latent space is the repre‐\nsentation of a well-formed image, as shown in Figure 1-9 .\n14 | Chapter 1: Generative Modeling",5361
16-Core Probability Theory.pdf,16-Core Probability Theory,"Figure 1-9. The dog manifold in high-dimensional pixel space is mapped to a simpler\nlatent space that can be sampled from\nCore Probability Theory\nWe have already seen that generative modeling is closely connected to statistical mod‐\neling of probability distributions. Therefore, it now makes sense to introduce some\ncore probabilistic and statistical concepts that will be used throughout this book to\nexplain the theoretical background of each model.\nIf you have never studied probability or statistics, don’t worry. To build many of the\ndeep learning models that we shall see later in this book, it is not essential to have a\ndeep understanding of statistical theory. However, to gain a full appreciation of the\ntask that we are trying to tackle, it’s worth trying to build up a solid understanding of\nbasic probabilistic theory. This way, you will have the foundations in place to under‐\nstand the different families of generative models that will be introduced later in this\nchapter.\nCore Probability Theory | 15\nAs a first step, we shall define five key terms, linking each one back to our earlier\nexample of a generative model that models the world map in two dimensions:\nSample space\nThe sample space  is the complete set of all values an observation  can take.\nIn our previous example, the sample space consists of all\npoints of latitude and longitude =x1,x2 on the world map.\nFor example,  = (40.7306, –73.9352) is a point in the sample\nspace (New Y ork City) that belongs to the true data-generating\ndistribution.  = (11.3493, 142.1996) is a point in the sample\nspace that does not belong to the true data-generating distri‐\nbution (it’s in the sea).\nProbability density function\nA probability density function  (or simply density function ) is a function p that\nmaps a point  in the sample space to a number between 0 and 1. The integral of\nthe density function over all points in the sample space must equal 1, so that it is\na well-defined probability distribution.\nIn the world map example, the density function of our genera‐\ntive model is 0 outside of the orange box and constant inside\nof the box, so that the integral of the density function over the\nentire sample space equals 1.\nWhile there is only one true density function pdata that is assumed to have\ngenerated the observable dataset, there are infinitely many density functions\npmodel that we can use to estimate pdata.\nParametric modeling\nParametric modeling  is a technique that we can use to structure our approach to\nfinding a suitable pmodel. A parametric model  is a family of density functions\npθ that can be described using a finite number of parameters, θ.\nIf we assume a uniform distribution as our model family, then\nthe set all possible boxes we could draw on Figure 1-5  is an\nexample of a parametric model. In this case, there are four\nparameters: the coordinates of the bottom-left θ1,θ2 and\ntop-right θ3,θ4 corners of the box.\nThus, each density function pθ in this parametric model\n(i.e., each box) can be uniquely represented by four numbers,\nθ=θ1,θ2,θ3,θ4 .\n16 | Chapter 1: Generative Modeling\nLikelihood\nThe likelihood  ℒθ of a parameter set θ is a function that measures the plausi‐\nbility of θ, given some observed point . It is defined as follows:\nℒθ=pθ\nThat is, the likelihood of θ given some observed point  is defined to be the value\nof the density function parameterized by θ, at the point . If we have a whole\ndataset  of independent observations, then we can write:\nℒθ=∏\n ∈ pθ\nIn the world map example, an orange box that only covered\nthe left half of the map would have a likelihood of 0—it\ncouldn’t possibly have generated the dataset, as we have\nobserved points in the right half of the map. The orange box in\nFigure 1-5  has a positive likelihood, as the density function is\npositive for all data points under this model.\nSince the product of a large number of terms between 0 and 1 can be quite com‐\nputationally difficult to work with, we often use the log-likelihood  ℓ instead:\nℓθ=∑\n ∈ logpθ\nThere are statistical reasons why the likelihood is defined in this way, but we can\nalso see that this definition intuitively makes sense. The likelihood of a set of\nparameters θ is defined to be the probability of seeing the data if the true data-\ngenerating distribution was the model parameterized by θ.\nNote that the likelihood is a function of the parameters , not the\ndata. It should not be interpreted as the probability that a given\nparameter set is correct—in other words, it is not a probability\ndistribution over the parameter space (i.e., it doesn’t sum/inte‐\ngrate to 1, with respect to the parameters).\nIt makes intuitive sense that the focus of parametric modeling should be to find\nthe optimal value θ of the parameter set that maximizes the likelihood of observ‐\ning the dataset .\nCore Probability Theory | 17",4912
17-Generative Model Taxonomy.pdf,17-Generative Model Taxonomy,"Maximum likelihood estimation\nMaximum likelihood estimation  is the technique that allows us to estimate θ—the\nset of parameters θ of a density function pθ that is most likely to explain some\nobserved data . More formally:\nθ= arg max\nℓθ\nθ is also called the maximum likelihood estimate  (MLE).\nIn the world map example, the MLE is the smallest rectangle\nthat still contains all of the points in the training set.\nNeural networks typically minimize  a loss function, so we can equivalently talk\nabout finding the set of parameters that minimize the negative log-likelihood :\nθ= arg min\nθ− ℓθ= arg min\nθ− log pθ\nGenerative modeling can be thought of as a form of maximum likelihood estimation,\nwhere the parameters θ are the weights of the neural networks contained in the\nmodel. We are trying to find the values of these parameters that maximize the likeli‐\nhood of observing the given data (or equivalently, minimize the negative log-\nlikelihood).\nHowever, for high-dimensional problems, it is generally not possible to directly cal‐\nculate pθ—it is intractable . As we shall see in the next section, different families of\ngenerative models take different approaches to tackling this problem.\nGenerative Model Taxonomy\nWhile  all types of generative models ultimately aim to solve the same task, they all\ntake slightly different approaches to modeling the density function pθ. Broadly\nspeaking, there are three possible approaches:\n1.Explicitly model the density function, but constrain the model in some way, so\nthat the density function is tractable (i.e., it can be calculated).\n2.Explicitly model a tractable approximation of the density function.\n18 | Chapter 1: Generative Modeling\n3.Implicitly model the density function, through a stochastic process that directly\ngenerates data.\nThese are shown in Figure 1-10  as a taxonomy, alongside the six families of genera‐\ntive models that we will explore in Part II  of this book. Note that these families are\nnot mutually exclusive—there are many examples of models that are hybrids between\ntwo different kinds of approaches. Y ou should think of the families as different gen‐\neral approaches to generative modeling, rather than explicit model architectures.\nFigure 1-10. A taxonomy of generative modeling approaches\nThe first split that we can make is between models where the probability density\nfunction p is modeled explicitly  and those where it is modeled implicitly .\nImplicit density models  do not aim to estimate the probability density at all, but\ninstead focus solely on producing a stochastic process that directly generates data.\nThe best-known example of an implicit generative model is a generative adversarial\nnetwork . We can further split explicit density models  into those that directly optimize\nthe density function (tractable models) and those that only optimize an approxima‐\ntion of it.\nTractable models  place constraints on the model architecture, so that the density func‐\ntion has a form that makes it easy to calculate. For example, autoregressive models\nimpose an ordering on the input features, so that the output can be generated sequen‐\ntially—e.g., word by word, or pixel by pixel. Normalizing flow models  apply a series of\ntractable, invertible functions to a simple distribution, in order to generate more\ncomplex distributions.\nGenerative Model Taxonomy | 19",3402
18-The Generative Deep Learning Codebase.pdf,18-The Generative Deep Learning Codebase,,0
19-Using Docker.pdf,19-Using Docker,"Approximate density models  include variational autoencoders , which introduce a latent\nvariable and optimize an approximation of the joint density function. Energy-based\nmodels  also utilize approximate methods, but do so via Markov chain sampling,\nrather than variational methods. Diffusion  models  approximate  the density function\nby training a model to gradually denoise a given image that has been previously\ncorrupted.\nA common thread that runs through all of the generative model family types is deep\nlearning . Almost all sophisticated generative models have a deep neural network at\ntheir core, because they can be trained from scratch to learn the complex relation‐\nships that govern the structure of the data, rather than having to be hardcoded with\ninformation a priori. We’ll explore deep learning in Chapter 2 , with practical exam‐\nples of how to get started building your own deep neural networks.\nThe Generative Deep Learning Codebase\nThe final section of this chapter will get you set up to start building generative deep\nlearning models by introducing the codebase that accompanies this book.\nMany  of the examples in this book are adapted from the excellent\nopen source implementations that are available through the Keras\nwebsite . I highly recommend you check out this resource, as new\nmodels and examples are constantly being added.\nCloning the Repository\nTo get started, you’ll first need to clone the Git repository. Git is an open source ver‐\nsion control system and will allow you to copy the code locally so that you can run\nthe notebooks on your own machine, or in a cloud-based environment. Y ou may\nalready have this installed, but if not, follow the instructions relevant to your operat‐\ning system .\nTo clone the repository for this book, navigate to the folder where you would like to\nstore the files and type the following into your terminal:\ngit clone https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition.git\nY ou should now be able to see the files in a folder on your machine.\n20 | Chapter 1: Generative Modeling",2094
20-Running on a GPU.pdf,20-Running on a GPU,,0
21-Summary.pdf,21-Summary,"Using Docker\nThe codebase for this book is intended to be used with Docker , a free containerization\ntechnology that makes getting started with a new codebase extremely easy, regardless\nof your architecture or operating system. If you have never used Docker, don’t\nworry—there  is a description of how to get started in the README  file in the book\nrepository.\nRunning on a GPU\nIf you don’t have access to your own GPU, that’s also no problem! All of the examples\nin this book will train on a CPU, though this will take longer than if you use a GPU-\nenabled machine. There is also a section in the README  about setting up a Google\nCloud environment that gives you access to a GPU on a pay-as-you-go basis.\nSummary\nThis chapter introduced the field of generative modeling, an important branch of\nmachine learning that complements the more widely studied discriminative model‐\ning. We discussed how generative modeling is currently one of the most active and\nexciting areas of AI research, with many recent advances in both theory and\napplications.\nWe started with a simple toy example and saw how generative modeling ultimately\nfocuses on modeling the underlying distribution of the data. This presents many\ncomplex and interesting challenges, which we summarized into a framework for\nunderstanding the desirable properties of any generative model.\nWe then walked through the key probabilistic concepts that will help to fully under‐\nstand the theoretical foundations of each approach to generative modeling and laid\nout the six different families of generative models that we will explore in Part II  of\nthis book. We also saw how to get started with the Generative Deep Learning  code‐\nbase, by cloning the repository.\nIn Chapter 2 , we will begin our exploration of deep learning and see how to use Keras\nto build models that can perform discriminative modeling tasks. This will give us the\nnecessary foundation to tackle generative deep learning problems in later chapters.\nReferences\n1. Miles Brundage et al., “The Malicious Use of Artificial Intelligence: Forecasting,\nPrevention, and Mitigation, ” February 20, 2018, https://www.eff.org/files/2018/02/20/\nmalicious_ai_report_final.pdf .\nSummary | 21",2239
22-Deep Neural Networks.pdf,22-Deep Neural Networks,"CHAPTER 2\nDeep Learning\nChapter Goals\nIn this chapter you will:\n•Learn about the different types of unstructured data that can be modeled using\ndeep learning.\n•Define a deep neural network and understand how it can be used to model com‐\nplex datasets.\n•Build a multilayer perceptron to predict the content of an image.\n•Improve the performance of the model by using convolutional layers, dropout,\nand batch normalization layers.\nLet’s start with a basic definition of deep learning:\nDeep learning is a class of machine learning algorithms that uses multiple stacked layers\nof processing units  to learn high-level representations from unstructured  data.\nTo understand deep learning fully, we need to delve into this definition a bit further.\nFirst, we’ll take a look at the different types of unstructured data that deep learning\ncan be used to model, then we’ll dive into the mechanics of building multiple stacked\nlayers of processing units to solve classification tasks. This will provide the founda‐\ntion for future chapters where we focus on deep learning for generative tasks.\n23\nData for Deep Learning\nMany  types of machine learning algorithms require structured , tabular data as input,\narranged into columns of features that describe each observation. For example, a per‐\nson’s age, income, and number of website visits in the last month are all features that\ncould help to predict if the person will subscribe to a particular online service in\nthe coming month. We could use a structured table of these features to train a logistic\nregression, random forest, or XGBoost model to predict the binary response\nvariable—did  the person subscribe (1) or not (0)? Here, each individual feature con‐\ntains a nugget of information about the observation, and the model would learn how\nthese features interact to influence the response.\nUnstructured  data refers to any data that is not naturally arranged into columns of\nfeatures, such as images, audio, and text. There is of course spatial structure to an\nimage, temporal structure to a recording or passage of text, and both spatial and tem‐\nporal structure to video data, but since the data does not arrive in columns of fea‐\ntures, it is considered unstructured, as shown in Figure 2-1 .\nFigure 2-1. The difference  between structured and unstructured data\nWhen our data is unstructured, individual pixels, frequencies, or characters are\nalmost entirely uninformative. For example, knowing that pixel 234 of an image is a\nmuddy shade of brown doesn’t really help identify if the image is of a house or a dog,\nand knowing that character 24 of a sentence is an e doesn’t help predict if the text is\nabout football or politics.\nPixels or characters are really just the dimples of the canvas into which higher-level\ninformative features, such as an image of a chimney or the word striker , are embed‐\nded. If the chimney in the image were placed on the other side of the house, the\nimage would still contain a chimney, but this information would now be carried by\ncompletely different pixels. If the word striker  appeared slightly earlier or later in the\ntext, the text would still be about football, but different character positions would\nprovide this information. The granularity of the data combined with the high degree\n24 | Chapter 2: Deep Learning",3356
23-Multilayer Perceptron MLP.pdf,23-Multilayer Perceptron MLP,"of spatial dependence destroys the concept of the pixel or character as an informative\nfeature in its own right.\nFor this reason, if we train logistic regression, random forest, or XGBoost models on\nraw pixel values, the trained model will often perform poorly for all but the simplest\nof classification tasks. These models rely on the input features to be informative and\nnot spatially dependent. A deep learning model, on the other hand, can learn how to\nbuild high-level informative features by itself, directly from the unstructured data.\nDeep learning can be applied to structured data, but its real power, especially with\nregard to generative modeling, comes from its ability to work with unstructured data.\nMost often, we want to generate unstructured data such as new images or original\nstrings of text, which is why deep learning has had such a profound impact on the\nfield of generative modeling.\nDeep Neural Networks\nThe majority of deep learning systems are artificial  neural networks  (ANNs, or just\nneural networks  for short) with multiple stacked hidden layers. For this reason, deep\nlearning  has now almost become synonymous with deep neural networks . However,\nany system that employs many layers to learn high-level representations of the input\ndata is also a form of deep learning (e.g., deep belief networks).\nLet’s start by breaking down exactly what we mean by a neural network and then see\nhow they can be used to learn high-level features from unstructured data.\nWhat Is a Neural Network?\nA neural network consists of a series of stacked layers . Each layer contains units  that\nare connected to the previous layer’s units through a set of weights . As we shall see,\nthere are many different types of layers, but one of the most common is the  fully con‐\nnected  (or dense ) layer that connects all units in the layer directly to every unit in the\nprevious layer.\nNeural networks where all adjacent layers are fully connected are called multilayer\nperceptrons  (MLPs). This is the first type of neural network that we will study. An\nexample of an MLP is shown in Figure 2-2 .\nDeep Neural Networks | 25\nFigure 2-2. An example of a multilayer perceptron that predicts if a face is smiling\nThe input (e.g., an image) is transformed by each layer in turn, in what is known as a\nforward pass  through the network, until it reaches the output layer. Specifically, each\nunit applies a nonlinear transformation to a weighted sum of its inputs and passes the\noutput through to the subsequent layer. The final output layer is the culmination of\nthis process, where the single unit outputs a probability that the original input\nbelongs to a particular category (e.g., smiling ).\nThe magic of deep neural networks lies in finding the set of weights for each layer\nthat results in the most accurate predictions. The process of finding these weights is\nwhat we mean by training  the network.\nDuring  the training process, batches of images are passed through the network and\nthe predicted outputs are compared to the ground truth. For example, the network\nmight output a probability of 80% for an image of someone who really is smiling and\na probability of 23% for an image of someone who really isn’t smiling. A perfect pre‐\ndiction would output 100% and 0% for these examples, so there is a small amount of\nerror. The error in the prediction is then propagated backward through the network,\nadjusting each set of weights a small amount in the direction that improves the pre‐\ndiction most significantly. This process is appropriately called backpropagation . Grad‐\nually, each unit becomes skilled at identifying a particular feature that ultimately\nhelps the network to make better predictions.\nLearning High-Level Features\nThe critical property that makes neural networks so powerful is their ability to learn\nfeatures from the input data, without human guidance. In other words, we do not\nneed to do any feature engineering, which is why neural networks are so useful! We\n26 | Chapter 2: Deep Learning\ncan let the model decide how it wants to arrange its weights, guided only by its desire\nto minimize the error in its predictions.\nFor example, let’s walk through the network shown in Figure 2-2 , assuming it has\nalready been trained to accurately predict if a given input face is smiling:\n1.Unit A receives the value for an individual channel of an input pixel.\n2.Unit B combines its input values so that it fires strongest when a particular low-\nlevel feature such as an edge is present.\n3.Unit C combines the low-level features so that it fires strongest when a higher-\nlevel feature such as teeth  are seen in the image.\n4.Unit D combines the high-level features so that it fires strongest when the person\nin the original image is smiling.\nUnits in each subsequent layer are able to represent increasingly sophisticated aspects\nof the original input, by combining lower-level features from the previous layer.\nAmazingly, this arises naturally out of the training process—we do not need to tell\neach unit what to look for, or whether it should look for high-level features or low-\nlevel features.\nThe layers between the input and output layers are called hidden  layers. While our\nexample only has two hidden layers, deep neural networks can have many more.\nStacking large numbers of layers allows the neural network to learn progressively\nhigher-level features by gradually building up information from the lower-level fea‐\ntures in previous layers. For example, ResNet, 1 designed for image recognition, con‐\ntains 152 layers.\nNext, we’ll dive straight into the practical side of deep learning and get set up with\nTensorFlow and Keras so that you can start building your own deep neural networks.\nTensorFlow and Keras\nTensorFlow  is an open source Python library for machine learning, developed by\nGoogle. TensorFlow is one of the most utilized frameworks for building machine\nlearning solutions, with particular emphasis on the manipulation of tensors (hence\nthe name). It provides the low-level functionality required to train neural networks,\nsuch as computing the gradient of arbitrary differentiable expressions and efficiently\nexecuting tensor operations.\nKeras  is a high-level API for building neural networks, built on top of TensorFlow\n(Figure 2-3 ). It is extremely flexible and very user-friendly, making it an ideal choice\nfor getting started with deep learning. Moreover, Keras provides numerous useful\nbuilding blocks that can be plugged together to create highly complex deep learning\narchitectures through its functional API.\nDeep Neural Networks | 27",6683
24-Preparing the Data.pdf,24-Preparing the Data,"Figure 2-3. TensorFlow and Keras are excellent tools for building deep learning solutions\nIf you are just getting started with deep learning, I can highly recommend using\nTensorFlow and Keras. This setup will allow you to build any network that you can\nthink of in a production environment, while also giving you an easy-to-learn API that\nenables rapid development of new ideas and concepts. Let’s start  by seeing how easy it\nis to build a multilayer perceptron using Keras.\nMultilayer Perceptron (MLP)\nIn this section, we will train an MLP to classify a given image using supervised learn‐\ning. Supervised learning is a type of machine learning algorithm in which the com‐\nputer is trained on a labeled dataset. In other words, the dataset used for training\nincludes input data with corresponding output labels. The goal of the algorithm is to\nlearn a mapping between the input data and the output labels, so that it can make\npredictions on new, unseen data.\nThe MLP is a discriminative (rather than generative) model, but supervised learning\nwill still play a role in many types of generative models that we will explore in later\nchapters of this book, so it is a good place to start our journey.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/02_deeplearning/01_mlp/mlp.ipynb  in the book\nrepository.\nPreparing the Data\nFor this example we will be using the CIFAR-10  dataset, a collection of 60,000 32 ×\n32–pixel color images that comes bundled with Keras out of the box. Each image is\nclassified into exactly one of 10 classes, as shown in Figure 2-4 .\n28 | Chapter 2: Deep Learning\nFigure 2-4. Example images from the CIFAR-10 dataset (source: Krizhevsky, 2009 )2\nBy default, the image data consists of integers between 0 and 255 for each pixel chan‐\nnel. We first need to preprocess the images by scaling these values to lie between\n0 and 1, as neural networks work best when the absolute value of each input is less\nthan 1.\nWe also need to change the integer labeling of the images to one-hot encoded vectors,\nbecause the neural network output will be a probability that the image belongs to\neach class. If the class integer label of an image is i, then its one-hot encoding is a\nvector of length 10 (the number of classes) that has 0s in all but the ith element,\nwhich is 1. These steps are shown in Example 2-1 .\nExample 2-1. Preprocessing the CIFAR-10 dataset\nimport numpy as np\nfrom tensorflow.keras  import datasets , utils\n(x_train, y_train), (x_test, y_test) = datasets .cifar10.load_data () \nMultilayer Perceptron (MLP) | 29",2651
25-Building the Model.pdf,25-Building the Model,"NUM_CLASSES  = 10\nx_train = x_train.astype('float32' ) / 255.0 \nx_test = x_test.astype('float32' ) / 255.0\ny_train = utils.to_categorical (y_train, NUM_CLASSES ) \ny_test = utils.to_categorical (y_test, NUM_CLASSES )\nLoad the CIFAR-10 dataset. x_train  and x_test  are numpy  arrays of shape\n[50000, 32, 32, 3]  and [10000, 32, 32, 3] , respectively. y_train  and\ny_test  are numpy  arrays of shape [50000, 1]  and [10000, 1] , respectively, con‐\ntaining the integer labels in the range 0 to 9 for the class of each image.\nScale each image so that the pixel channel values lie between 0 and 1.\nOne-hot encode the labels—the new shapes of y_train  and y_test  are [50000,\n10] and [10000, 10] , respectively.\nWe can see that the training image data ( x_train ) is stored in a tensor  of shape\n[50000, 32, 32, 3] . There are no columns  or rows  in this dataset; instead, this is a\ntensor with four dimensions. A tensor is just a multidimensional array—it is the nat‐\nural extension of a matrix to more than two dimensions. The first dimension of this\ntensor references the index of the image in the dataset, the second and third relate to\nthe size of the image, and the last is the channel (i.e., red, green, or blue, since these\nare RGB images).\nFor example, Example 2-2  shows how we can find the channel value of a specific pixel\nin an image.\nExample 2-2. The green channel (1) value of the pixel in the (12,13) position of image 54\nx_train[54, 12, 13, 1]\n# 0.36862746\nBuilding the Model\nIn Keras you can either define the structure of a neural network as a Sequential\nmodel or using the functional API.\nA Sequential  model is useful for quickly defining a linear stack of layers (i.e., where\none layer follows on directly from the previous layer without any branching). We can\ndefine our MLP model using the Sequential  class as shown in Example 2-3 .\n30 | Chapter 2: Deep Learning\nExample 2-3. Building our MLP using a Sequential  model\nfrom tensorflow.keras  import layers, models\nmodel = models.Sequential ([\n    layers.Flatten(input_shape =(32, 32, 3)),\n    layers.Dense(200, activation  = 'relu'),\n    layers.Dense(150, activation  = 'relu'),\n    layers.Dense(10, activation  = 'softmax' ),\n])\nMany of the models in this book require that the output from a layer is passed to mul‐\ntiple subsequent layers, or conversely, that a layer receives input from multiple pre‐\nceding layers. For these models, the Sequential  class is not suitable and we would\nneed to use the functional API instead, which is a lot more flexible.\nI recommend that even if you are just starting out building linear\nmodels with Keras, you still use the functional API rather than\nSequential  models, since it will serve you better in the long run as\nyour neural networks become more architecturally complex. The\nfunctional API will give you complete freedom over the design of\nyour deep neural network.\nExample 2-4  shows the same MLP coded using the functional API. When using the\nfunctional API, we use the Model  class to define the overall input and output layers of\nthe model.\nExample 2-4. Building our MLP using the functional API\nfrom tensorflow.keras  import layers, models\ninput_layer  = layers.Input(shape=(32, 32, 3))\nx = layers.Flatten()(input_layer )\nx = layers.Dense(units=200, activation  = 'relu')(x)\nx = layers.Dense(units=150, activation  = 'relu')(x)\noutput_layer  = layers.Dense(units=10, activation  = 'softmax' )(x)\nmodel = models.Model(input_layer , output_layer )\nBoth methods give identical models—a diagram of the architecture is shown in\nFigure 2-5 .\nMultilayer Perceptron (MLP) | 31\nFigure 2-5. A diagram of the MLP architecture\nLet’s now look in more detail at the different layers and activation functions used\nwithin the MLP .\nLayers\nTo build our MLP , we used three different types of layers: Input , Flatten , and Dense .\nThe Input  layer is an entry point into the network. We tell the network the shape of\neach data element to expect as a tuple. Notice that we do not specify the batch size;\nthis isn’t necessary as we can pass any number of images into the Input  layer simulta‐\nneously. We do not need to explicitly state the batch size in the Input  layer definition.\nNext we flatten this input into a vector, using a Flatten  layer. This results in a vector\nof length 3,072 (= 32 × 32 × 3). The reason we do this is because the subsequent\nDense  layer requires that its input is flat, rather than a multidimensional array. As we\nshall see later, other layer types require multidimensional arrays as input, so you need\nto be aware of the required input and output shape of each layer type to understand\nwhen it is necessary to use Flatten .\nThe Dense  layer is one of the most fundamental building blocks of a neural network.\nIt contains a given number of units that are densely connected to the previous layer—\nthat is, every unit in the layer is connected to every unit in the previous layer, through\na single connection that carries a weight (which can be positive or negative). The out‐\nput from a given unit is the weighted sum of the inputs it receives from the previous\nlayer, which is then passed through a nonlinear activation function  before being sent\nto the following layer. The activation function is critical to ensure the neural network\nis able to learn complex functions and doesn’t just output a linear combination of its\ninputs.\n32 | Chapter 2: Deep Learning\nActivation functions\nThere are many kinds of activation function, but three of the most important are\nReLU, sigmoid, and softmax.\nThe ReLU  (rectified linear unit) activation function is defined to be 0 if the input is\nnegative and is otherwise equal to the input. The LeakyReLU  activation function is\nvery similar to ReLU, with one key difference: whereas the ReLU activation function\nreturns 0 for input values less than 0, the LeakyReLU function returns a small nega‐\ntive number proportional to the input. ReLU units can sometimes die if they always\noutput 0, because of a large bias toward negative values pre-activation. In this case,\nthe gradient is 0 and therefore no error is propagated back through this unit.\nLeakyReLU  activations fix this issue by always ensuring the gradient is nonzero.\nReLU-based functions are among the most reliable activations to use between the lay‐\ners of a deep network to encourage stable training.\nThe sigmoid  activation is useful if you wish the output from the layer to be scaled\nbetween 0 and 1—for example, for binary classification problems with one output\nunit or multilabel classification problems, where each observation can belong to more\nthan one class. Figure 2-6  shows ReLU, LeakyReLU, and sigmoid activation functions\nside by side for comparison.\nFigure 2-6. The ReLU, LeakyReLU, and sigmoid activation functions\nThe softmax  activation function is useful if you want the total sum of the output from\nthe layer to equal 1; for example, for multiclass classification problems where each\nobservation only belongs to exactly one class. It is defined as:\nyi=exi\n∑j= 1Jexj\nHere, J is the total number of units in the layer. In our neural network, we use a soft‐\nmax activation in the final layer to ensure that the output is a set of 10 probabilities\nMultilayer Perceptron (MLP) | 33\nthat sum to 1, which can be interpreted as the likelihood that the image belongs to\neach class.\nIn Keras, activation functions can be defined within a layer ( Example 2-5 ) or as a sep‐\narate layer ( Example 2-6 ).\nExample 2-5. A ReLU activation function defined  as part of a Dense  layer\nx = layers.Dense(units=200, activation  = 'relu')(x)\nExample 2-6. A ReLU activation function defined  as its own layer\nx = layers.Dense(units=200)(x)\nx = layers.Activation ('relu')(x)\nIn our example, we pass the input through two Dense  layers, the first with 200 units\nand the second with 150, both with ReLU activation functions.\nInspecting the model\nWe can use the model.summary()  method to inspect the shape of the network at each\nlayer, as shown in Table 2-1 .\nTable 2-1. Output from the model.summary()  method\nLayer (type) Output shape Param #\nInputLayer (None, 32, 32, 3) 0\nFlatten (None, 3072) 0\nDense (None, 200) 614,600\nDense (None, 150) 30,150\nDense (None, 10) 1,510\nTotal params 646,260\nTrainable params 646,260\nNon-trainable params 0\nNotice how the shape of our Input  layer matches the shape of x_train  and the shape\nof our Dense  output layer matches the shape of y_train . Keras uses None  as a marker\nfor the first dimension to show that it doesn’t yet know the number of observations\nthat will be passed into the network. In fact, it doesn’t need to; we could just as easily\npass 1 observation through the network at a time as 1,000. That’s because tensor oper‐\nations are conducted across all observations simultaneously using linear algebra—this\nis the part handled by TensorFlow. It is also the reason why you get a performance\nincrease when training deep neural networks on GPUs instead of CPUs: GPUs are\n34 | Chapter 2: Deep Learning",9127
26-Compiling the Model.pdf,26-Compiling the Model,"optimized for large tensor operations since these calculations are also necessary for\ncomplex graphics manipulation.\nThe summary  method also gives the number of parameters (weights) that will be\ntrained at each layer. If ever you find that your model is training too slowly, check the\nsummary to see if there are any layers that contain a huge number of weights. If so,\nyou should consider whether the number of units in the layer could be reduced to\nspeed up training.\nMake sure you understand how the number of parameters is calcu‐\nlated in each layer! It’s important to remember that by default, each\nunit within a given layer is also connected to one additional bias\nunit that always outputs 1. This ensures that the output from the\nunit can still be nonzero even when all inputs from the previous\nlayer are 0.\nTherefore, the number of parameters in the 200-unit Dense  layer is\n200 * (3,072 + 1) = 614,600.\nCompiling the Model\nIn this step, we  compile the model with an optimizer and a loss function, as shown in\nExample 2-7 .\nExample 2-7. Defining  the optimizer and the loss function\nfrom tensorflow.keras  import optimizers\nopt = optimizers .Adam(learning_rate =0.0005)\nmodel.compile(loss='categorical_crossentropy' , optimizer =opt,\n              metrics=['accuracy' ])\nLet’s now look in more detail at what we mean by loss functions and optimizers.\nLoss functions\nThe loss function  is used by the neural network to compare its predicted output to the\nground truth. It returns a single number for each observation; the greater this num‐\nber, the worse the network has performed for this observation.\nKeras provides many built-in loss functions to choose from, or you can create your\nown. Three of the most commonly used are mean squared error, categorical cross-\nentropy, and binary cross-entropy. It is important to understand when it is appropri‐\nate to use each.\nIf your neural network is designed to solve a regression problem (i.e., the output is\ncontinuous), then you might use the mean squared error  loss. This is the mean of the\nMultilayer Perceptron (MLP) | 35\nsquared difference between the ground truth yi and predicted value pi of each output\nunit, where the mean is taken over all n output units:\nMSE =1\nn∑\ni= 1n\nyi−pi2\nIf you are working on a classification problem where each observation only belongs\nto one class, then categorical cross-entropy  is the correct loss function. This is defined\nas follows:\n−∑\ni= 1n\nyilog pi\nFinally, if you are working on a binary classification problem with one output unit, or\na multilabel problem where each observation can belong to multiple classes simulta‐\nneously, you should use binary cross-entropy :\n−1\nn∑\ni= 1n\nyilog pi+1 −yilog 1 −pi\nOptimizers\nThe optimizer  is the algorithm that will be used to update the weights in the neural\nnetwork based on the gradient of the loss function. One of the most commonly used\nand stable optimizers is Adam  (Adaptive Moment Estimation). 3 In most cases, you\nshouldn’t need to tweak the default parameters of the Adam optimizer, except the\nlearning rate . The greater the learning rate, the larger the change in weights at each\ntraining step. While training is initially faster with a large learning rate, the downside\nis that it may result in less stable training and may not find the global minimum of\nthe loss function. This is a parameter that you may want to tune or adjust during\ntraining.\nAnother  common optimizer that you may come across is RMSProp  (Root Mean\nSquared Propagation). Again, you shouldn’t need to adjust the parameters of this\noptimizer too much, but it is worth reading the Keras documentation  to understand\nthe role of each parameter.\nWe pass both the loss function and the optimizer into the compile  method of the\nmodel, as well as a metrics  parameter where we can specify any additional metrics\nthat we would like to report on during training, such as accuracy.\n36 | Chapter 2: Deep Learning",4001
27-Evaluating the Model.pdf,27-Evaluating the Model,"Training the Model\nThus  far, we haven’t shown the model any data. We have just set up the architecture\nand compiled the model with a loss function and optimizer.\nTo train the model against the data, we simply call the fit method, as shown in\nExample 2-8 .\nExample 2-8. Calling the fit method to train the model\nmodel.fit(x_train \n          , y_train \n          , batch_size  = 32 \n          , epochs = 10 \n          , shuffle = True \n          )\nThe raw image data.\nThe one-hot encoded class labels.\nThe batch_size  determines how many observations will be passed to the net‐\nwork at each training step.\nThe epochs  determine how many times the network will be shown the full train‐\ning data.\nIf shuffle = True , the batches will be drawn randomly without replacement\nfrom the training data at each training step.\nThis will start training a deep neural network to predict the category of an image\nfrom the CIFAR-10 dataset. The training process works as follows.\nFirst, the weights of the network are initialized to small random values. Then the net‐\nwork performs a series of training steps. At each training step, one batch  of images is\npassed through the network and the errors are backpropagated to update the weights.\nThe batch_size  determines how many images are in each training step batch. The\nlarger the batch size, the more stable the gradient calculation, but the slower each\ntraining step.\nIt would be far too time-consuming and computationally intensive\nto use the entire dataset to calculate the gradient at each training\nstep, so generally a batch size between 32 and 256 is used. It is also\nnow recommended practice to increase the batch size as training\nprogresses. 4\nMultilayer Perceptron (MLP) | 37\nThis continues until all observations in the dataset have been seen once. This com‐\npletes the first  epoch . The data is then passed through the network again in batches as\npart of the second epoch. This process repeats until the specified number of epochs\nhave elapsed.\nDuring training, Keras outputs the progress of the procedure, as shown in Figure 2-7 .\nWe can see that the training dataset has been split into 1,563 batches (each containing\n32 images) and it has been shown to the network 10 times (i.e., over 10 epochs), at a\nrate of approximately 2 milliseconds per batch. The categorical cross-entropy loss has\nfallen from 1.8377 to 1.3696, resulting in an accuracy increase from 33.69% after the\nfirst epoch to 51.67% after the tenth epoch.\nFigure 2-7. The output from the fit method\nEvaluating the Model\nWe know the model achieves an accuracy of 51.9% on the training set, but how does\nit perform on data it has never seen?\nTo answer this question we can use the evaluate  method provided by Keras, as\nshown in Example 2-9 .\nExample 2-9. Evaluating the model performance on the test set\nmodel.evaluate (x_test, y_test)\n38 | Chapter 2: Deep Learning\nFigure 2-8  shows the output from this method.\nFigure 2-8. The output from the evaluate  method\nThe output is a list of the metrics we are monitoring: categorical cross-entropy and\naccuracy. We can see that model accuracy is still 49.0% even on images that it has\nnever seen before. Note that if the model were guessing randomly, it would achieve\napproximately 10% accuracy (because there are 10 classes), so 49.0% is a good result,\ngiven that we have used a very basic neural network.\nWe can view some of the predictions on the test set using the predict  method, as\nshown in Example 2-10 .\nExample 2-10. Viewing predictions on the test set using the predict  method\nCLASSES = np.array(['airplane' , 'automobile' , 'bird', 'cat', 'deer', 'dog'\n                   , 'frog', 'horse', 'ship', 'truck'])\npreds = model.predict(x_test) \npreds_single  = CLASSES[np.argmax(preds, axis = -1)] \nactual_single  = CLASSES[np.argmax(y_test, axis = -1)]\npreds  is an array of shape [10000, 10] —i.e., a vector of 10 class probabilities for\neach observation.\nWe convert this array of probabilities back into a single prediction using numpy ’s\nargmax  function. Here, axis = –1  tells the function to collapse the array over the\nlast dimension (the classes dimension), so that the shape of preds_single  is then\n[10000, 1] .\nWe can view some of the images alongside their labels and predictions with the code\nin Example 2-11 . As expected, around half are correct.\nExample 2-11. Displaying predictions of the MLP against the actual labels\nimport matplotlib.pyplot  as plt\nn_to_show  = 10\nindices = np.random.choice(range(len(x_test)), n_to_show )\nfig = plt.figure(figsize=(15, 3))\nfig.subplots_adjust (hspace=0.4, wspace=0.4)\nMultilayer Perceptron (MLP) | 39",4706
28-Convolutional Layers.pdf,28-Convolutional Layers,"for i, idx in enumerate (indices):\n    img = x_test[idx]\n    ax = fig.add_subplot (1, n_to_show , i+1)\n    ax.axis('off')\n    ax.text(0.5, -0.35, 'pred = '  + str(preds_single [idx]), fontsize =10\n       , ha='center' , transform =ax.transAxes )\n    ax.text(0.5, -0.7, 'act = '  + str(actual_single [idx]), fontsize =10\n        , ha='center' , transform =ax.transAxes )\n    ax.imshow(img)\nFigure 2-9  shows a randomly chosen selection of predictions made by the model,\nalongside the true labels.\nFigure 2-9. Some predictions made by the model, alongside the actual labels\nCongratulations! Y ou’ve just built a multilayer perceptron using Keras and used it to\nmake predictions on new data. Even though this is a supervised learning problem,\nwhen we come to building generative models in future chapters many of the core\nideas from this chapter (such as loss functions, activation functions, and understand‐\ning layer shapes) will still be extremely important. Next we’ll look at ways of improv‐\ning this model, by introducing a few new layer types.\nConvolutional Neural Network (CNN)\nOne  of the reasons our network isn’t yet performing as well as it might is because\nthere isn’t anything in the network that takes into account the spatial structure of the\ninput images. In fact, our first step is to flatten the image into a single vector, so that\nwe can pass it to the first Dense  layer!\nTo achieve this we need to use a convolutional layer .\n40 | Chapter 2: Deep Learning\nConvolutional Layers\nFirst, we need to understand what is meant by a convolution  in the context of deep\nlearning.\nFigure 2-10  shows two different 3 × 3 × 1 portions of a grayscale image being convo‐\nluted with a 3 × 3 × 1 filter  (or kernel ). The convolution is performed by multiplying\nthe filter pixelwise with the portion of the image, and summing the results. The out‐\nput is more positive when the portion of the image closely matches the filter and\nmore negative when the portion of the image is the inverse of the filter. The top\nexample resonates strongly with the filter, so it produces a large positive value. The\nbottom example does not resonate much with the filter, so it produces a value near\nzero.\nFigure 2-10. A 3 × 3 convolutional filter  applied to two portions of a grayscale image\nIf we move the filter across the entire image from left to right and top to bottom,\nrecording the convolutional output as we go, we obtain a new array that picks out a\nparticular feature of the input, depending on the values in the filter. For example,\nFigure 2-11  shows two different filters that highlight horizontal and vertical edges.\nRunning the Code for This Example\nY ou can see this convolutional process worked through manually\nin the Jupyter notebook located at notebooks/02_deeplearning/\n02_cnn/convolutions.ipynb  in the book repository.\nConvolutional Neural Network (CNN) | 41\nFigure 2-11. Two convolutional filters  applied to a grayscale image\nA convolutional layer is simply a collection of filters, where the values stored in the\nfilters are the weights that are learned by the neural network through training. Ini‐\ntially these are random, but gradually the filters adapt their weights to start picking\nout interesting features such as edges or particular color combinations.\nIn Keras, the Conv2D  layer applies convolutions to an input tensor with two spatial\ndimensions (such as an image). For example, the code shown in Example 2-12  builds\na convolutional layer with two filters, to match the example in Figure 2-11 .\nExample 2-12. A Conv2D  layer applied to grayscale input images\nfrom tensorflow.keras  import layers\ninput_layer  = layers.Input(shape=(64,64,1))\nconv_layer_1  = layers.Conv2D(\n    filters = 2\n    , kernel_size  = (3,3)\n    , strides = 1\n    , padding = ""same""\n    )(input_layer )\nNext, let’s look at two of the arguments to the Conv2D  layer in more detail— strides\nand padding .\n42 | Chapter 2: Deep Learning\nStride\nThe strides  parameter is the step size used by the layer to move the filters across the\ninput. Increasing the stride therefore reduces the size of the output tensor. For exam‐\nple, when strides = 2 , the height and width of the output tensor will be half the size\nof the input tensor. This is useful for reducing the spatial size of the tensor as it passes\nthrough the network, while increasing the number of channels.\nPadding\nThe padding = ""same""  input parameter pads the input data with zeros so that the\noutput size from the layer is exactly the same as the input size when strides = 1 .\nFigure 2-12  shows a 3 × 3 kernel being passed over a 5 × 5 input image, with padding\n= ""same""  and strides = 1 . The output size from this convolutional layer would also\nbe 5 × 5, as the padding allows the kernel to extend over the edge of the image, so that\nit fits five times in both directions. Without padding, the kernel could only fit three\ntimes along each direction, giving an output size of 3 × 3.\nFigure 2-12. A 3 × 3 × 1 kernel (gray) being passed over a 5 × 5 × 1 input image (blue),\nwith padding = ""same""  and strides = 1 , to generate the 5 × 5 × 1 output (green)\n(source: Dumoulin and Visin, 2018 )5\nSetting padding = ""same""  is a good way to ensure that you are able to easily keep\ntrack of the size of the tensor as it passes through many convolutional layers. The\nshape of the output from a convolutional layer with padding = ""same""  is:\ninput height\nstride,input width\nstride,f ilters\nStacking convolutional layers\nThe output of a Conv2D  layer is another four-dimensional tensor, now of shape\n(batch_size, height, width, filters) , so we can stack Conv2D  layers on top of\neach other to grow the depth of our neural network and make it more powerful. To\ndemonstrate this, let’s imagine we are applying Conv2D  layers to the CIFAR-10 dataset\nand wish to predict the label of a given image. Note that this time, instead of one\ninput channel (grayscale) we have three (red, green, and blue).\nConvolutional Neural Network (CNN) | 43\nExample 2-13  shows how to build a simple convolutional neural network that we\ncould train to succeed at this task.\nExample 2-13. Code to build a convolutional neural network model using Keras\nfrom tensorflow.keras  import layers, models\ninput_layer  = layers.Input(shape=(32,32,3))\nconv_layer_1  = layers.Conv2D(\n    filters = 10\n    , kernel_size  = (4,4)\n    , strides = 2\n    , padding = 'same'\n    )(input_layer )\nconv_layer_2  = layers.Conv2D(\n    filters = 20\n    , kernel_size  = (3,3)\n    , strides = 2\n    , padding = 'same'\n    )(conv_layer_1 )\nflatten_layer  = layers.Flatten()(conv_layer_2 )\noutput_layer  = layers.Dense(units=10, activation  = 'softmax' )(flatten_layer )\nmodel = models.Model(input_layer , output_layer )\nThis code corresponds to the diagram shown in Figure 2-13 .\nFigure 2-13. A diagram of a convolutional neural network\nNote that now that we are working with color images, each filter in the first convolu‐\ntional layer has a depth of 3 rather than 1 (i.e., each filter has shape 4 × 4 × 3, rather\nthan 4 × 4 × 1). This is to match the three channels (red, green, blue) of the input\n44 | Chapter 2: Deep Learning\nimage. The same idea applies to the filters in the second convolutional layer that have\na depth of 10, to match the 10 channels output by the first convolutional layer.\nIn general, the depth of the filters in a layer is always equal to the\nnumber of channels output by the preceding layer.\nInspecting the model\nIt’s really informative to look at how the shape of the tensor changes as data flows\nthrough from one convolutional layer to the next. We can use the  model.summary()\nmethod to inspect the shape of the tensor as it passes through the network\n(Table 2-2 ).\nTable 2-2. CNN model summary\nLayer (type) Output shape Param #\nInputLayer (None, 32, 32, 3) 0\nConv2D (None, 16, 16, 10) 490\nConv2D (None, 8, 8, 20) 1,820\nFlatten (None, 1280) 0\nDense (None, 10) 12,810\nTotal params 15,120\nTrainable params 15,120\nNon-trainable params 0\nLet’s walk through our network layer by layer, noting the shape of the tensor as we go:\n1.The input shape is (None, 32, 32, 3) —Keras uses None  to represent the fact\nthat we can pass any number of images through the network simultaneously.\nSince the network is just performing tensor algebra, we don’t need to pass images\nthrough the network individually, but instead can pass them through together as\na batch.\n2.The shape of each of the 10 filters in the first convolutional layer is 4 × 4 × 3. This\nis because we have chosen each filter to have a height and width of 4 ( ker\nnel_size = (4,4) ) and there are three channels in the preceding layer (red,\ngreen, and blue). Therefore, the number of parameters (or weights) in the layer is\n(4 × 4 × 3 + 1) × 10 = 490, where the + 1 is due to the inclusion of a bias term\nattached to each of the filters. The output from each filter will be the pixelwise\nmultiplication of the filter weights and the 4 × 4 × 3 section of the image it is\nConvolutional Neural Network (CNN) | 45",9140
29-Batch Normalization.pdf,29-Batch Normalization,"covering. As strides = 2  and padding = ""same"" , the width and height of the\noutput are both halved to 16, and since there are 10 filters the output of the first\nlayer is a batch of tensors each having shape [16, 16, 10] .\n3.In the second convolutional layer, we choose the filters to be 3 × 3 and they now\nhave depth 10, to match the number of channels in the previous layer. Since there\nare 20 filters in this layer, this gives a total number of parameters (weights) of (3\n× 3 × 10 + 1) × 20 = 1,820. Again, we use strides = 2 and  padding = ""same"" ,\nso the width and height both halve. This gives us an overall output shape of\n(None, 8, 8, 20) .\n4.We now flatten the tensor using the Keras Flatten  layer. This results in a set of 8\n× 8 × 20 = 1,280 units. Note that there are no parameters to learn in a Flatten\nlayer as the operation is just a restructuring of the tensor.\n5.We finally connect these units to a 10-unit Dense  layer with softmax activation,\nwhich represents the probability of each category in a 10-category classification\ntask. This creates an extra 1,280 × 10 = 12,810 parameters (weights) to learn.\nThis example demonstrates how we can chain convolutional layers together to create\na convolutional neural network. Before we see how this compares in accuracy to our\ndensely connected neural network, we’ll examine two more techniques that can also\nimprove performance: batch normalization and dropout.\nBatch Normalization\nOne  common problem when training a deep neural network is ensuring that the\nweights of the network remain within a reasonable range of values—if they start to\nbecome too large, this is a sign that your network is suffering from what is known as\nthe exploding gradient  problem. As errors are propagated backward through the\nnetwork , the calculation of the gradient in the earlier layers can sometimes grow\nexponentially large, causing wild fluctuations in the weight values.\nIf your loss function starts to return NaN, chances are that your\nweights have grown large enough to cause an overflow error.\nThis doesn’t necessarily happen immediately as you start training the network. Some‐\ntimes it can be happily training for hours when suddenly the loss function returns\nNaN and your network has exploded. This can be incredibly annoying. To prevent it\nfrom happening, you need to understand the root cause of the exploding gradient\nproblem.\n46 | Chapter 2: Deep Learning\nCovariate shift\nOne of the reasons for scaling input data to a neural network is to ensure a stable start\nto training over the first few iterations. Since the weights of the network are initially\nrandomized, unscaled input could potentially create huge activation values that\nimmediately lead to exploding gradients. For example, instead of passing pixel values\nfrom 0–255 into the input layer, we usually scale these values to between –1 and 1.\nBecause the input is scaled, it’s natural to expect the activations from all future layers\nto be relatively well scaled as well. Initially this may be true, but as the network trains\nand the weights move further away from their random initial values, this assumption\ncan start to break down. This phenomenon is known as covariate shift.\nCovariate Shift Analogy\nImagine you’re carrying a tall pile of books, and you get hit by a\ngust of wind. Y ou move the books in a direction opposite to the\nwind to compensate, but as you do so, some of the books shift, so\nthat the tower is slightly more unstable than before. Initially, this is\nOK, but with every gust the pile becomes more and more unstable,\nuntil eventually the books have shifted so much that the pile collap‐\nses. This is covariate shift.\nRelating this to neural networks, each layer is like a book in the\npile. To remain stable, when the network updates the weights, each\nlayer implicitly assumes that the distribution of its input from the\nlayer beneath is approximately consistent across iterations. How‐\never, since there is nothing to stop any of the activation distribu‐\ntions shifting significantly in a certain direction, this can\nsometimes lead to runaway weight values and an overall collapse of\nthe network.\nTraining using batch normalization\nBatch normalization  is a technique that drastically reduces this problem. The solution\nis surprisingly simple. During training, a batch normalization layer calculates the\nmean and standard deviation of each of its input channels across the batch and nor‐\nmalizes by subtracting the mean and dividing by the standard deviation. There are\nthen two learned parameters for each channel, the scale (gamma) and shift (beta).\nThe output is simply the normalized input, scaled by gamma and shifted by beta.\nFigure 2-14  shows the whole process.\nConvolutional Neural Network (CNN) | 47\nFigure 2-14. The batch normalization process (source: Ioffe  and Szegedy, 2015 )6\nWe can place batch normalization layers after dense or convolutional layers to nor‐\nmalize the output.\nReferring to our previous example, it’s a bit like connecting the lay‐\ners of books with small sets of adjustable springs that ensure there\naren’t any overall huge shifts in their positions over time.\nPrediction using batch normalization\nY ou might be wondering how this layer works at prediction time. When it comes to\nprediction, we may only want to predict a single observation, so there is no batch  over\nwhich to calculate the mean and standard deviation. To get around this problem, dur‐\ning training a batch normalization layer also calculates the moving average of the\nmean and standard deviation of each channel and stores this value as part of the layer\nto use at test time.\nHow  many parameters are contained within a batch normalization layer? For every\nchannel in the preceding layer, two weights need to be learned: the scale (gamma) and\nshift (beta). These are the trainable  parameters. The moving average and standard\ndeviation also need to be calculated for each channel, but since they are derived from\nthe data passing through the layer rather than trained through backpropagation, they\nare called nontrainable  parameters. In total, this gives four parameters for each chan‐\nnel in the preceding layer, where two are trainable and two are nontrainable.\n48 | Chapter 2: Deep Learning",6330
30-Dropout.pdf,30-Dropout,"In Keras, the BatchNormalization  layer implements the batch normalization\nfunctionality, as shown in Example 2-14 .\nExample 2-14. A BatchNormalization  layer in Keras\nfrom tensorflow.keras  import layers\nlayers.BatchNormalization (momentum  = 0.9)\nThe momentum  parameter is the weight given to the previous value when calculating\nthe moving average and moving standard deviation.\nDropout\nWhen  studying for an exam, it is common practice for students to use past papers\nand sample questions to improve their knowledge of the subject material. Some stu‐\ndents try to memorize the answers to these questions, but then come unstuck in the\nexam because they haven’t truly understood the subject matter. The best students use\nthe practice material to further their general understanding, so that they are still able\nto answer correctly when faced with new questions that they haven’t seen before.\nThe same principle holds for machine learning. Any successful machine learning\nalgorithm must ensure that it generalizes to unseen data, rather than simply remem‐\nbering  the training dataset. If an algorithm performs well on the training dataset, but\nnot the test dataset, we say that it is suffering from overfitting . To counteract this\nproblem, we use regularization  techniques, which ensure that the model is penalized\nif it starts to overfit.\nThere  are many ways to regularize a machine learning algorithm, but for deep learn‐\ning, one of the most common is by using dropout  layers. This idea was introduced by\nHinton et al. in 2012 7 and presented in a 2014 paper by Srivastava et al. 8\nDropout layers are very simple. During training, each dropout layer chooses a ran‐\ndom set of units from the preceding layer and sets their output to 0, as shown in\nFigure 2-15 .\nIncredibly, this simple addition drastically reduces overfitting by ensuring that the\nnetwork doesn’t become overdependent on certain units or groups of units that, in\neffect, just remember observations from the training set. If we use dropout layers, the\nnetwork cannot rely too much on any one unit and therefore knowledge is more\nevenly spread across the whole network.\nConvolutional Neural Network (CNN) | 49\nFigure 2-15. A dropout layer\nThis makes the model much better at generalizing to unseen data, because the net‐\nwork has been trained to produce accurate predictions even under unfamiliar condi‐\ntions, such as those caused by dropping random units. There are no weights to learn\nwithin a dropout layer, as the units to drop are decided stochastically. At prediction\ntime, the dropout layer doesn’t drop any units, so that the full network is used to\nmake predictions.\nDropout Analogy\nReturning to our analogy, it’s a bit like a math student practicing\npast papers with a random selection of key formulae missing from\ntheir formula book. This way, they learn how to answer questions\nthrough an understanding of the core principles, rather than\nalways looking up the formulae in the same places in the book.\nWhen it comes to test time, they will find it much easier to answer\nquestions that they have never seen before, due to their ability to\ngeneralize beyond the training material.\nThe Dropout  layer in Keras implements this functionality, with the rate  parameter\nspecifying the proportion of units to drop from the preceding layer, as shown in\nExample 2-15 .\nExample 2-15. A Dropout  layer in Keras\nfrom tensorflow.keras  import layers\nlayers.Dropout(rate = 0.25)\n50 | Chapter 2: Deep Learning",3533
31-Building the CNN.pdf,31-Building the CNN,"Dropout layers are used most commonly after dense layers since these are the most\nprone to overfitting due to the higher number of weights, though you can also use\nthem after convolutional layers.\nBatch normalization also has been shown to reduce overfitting, and\ntherefore many modern deep learning architectures don’t use drop‐\nout at all, relying solely on batch normalization for regularization.\nAs with most deep learning principles, there is no golden rule that\napplies in every situation—the only way to know for sure what’s\nbest is to test different architectures and see which performs best\non a holdout set of data.\nBuilding the CNN\nY ou’ve  now seen three new Keras layer types: Conv2D , BatchNormalization , and\nDropout . Let’s put these pieces together into a CNN model and see how it performs\non the CIFAR-10 dataset.\nRunning the Code for This Example\nY ou can run the following example in the Jupyter notebook in\nthe book repository called notebooks/02_deeplearning/02_cnn/\ncnn.ipynb .\nThe model architecture we shall test is shown in Example 2-16 .\nExample 2-16. Code to build a CNN model using Keras\nfrom tensorflow.keras  import layers, models\ninput_layer  = layers.Input((32,32,3))\nx = layers.Conv2D(filters = 32, kernel_size  = 3\n , strides = 1, padding = 'same')(input_layer )\nx = layers.BatchNormalization ()(x)\nx = layers.LeakyReLU ()(x)\nx = layers.Conv2D(filters = 32, kernel_size  = 3, strides = 2, padding = 'same')(x)\nx = layers.BatchNormalization ()(x)\nx = layers.LeakyReLU ()(x)\nx = layers.Conv2D(filters = 64, kernel_size  = 3, strides = 1, padding = 'same')(x)\nx = layers.BatchNormalization ()(x)\nx = layers.LeakyReLU ()(x)\nx = layers.Conv2D(filters = 64, kernel_size  = 3, strides = 2, padding = 'same')(x)\nx = layers.BatchNormalization ()(x)\nConvolutional Neural Network (CNN) | 51\nx = layers.LeakyReLU ()(x)\nx = layers.Flatten()(x)\nx = layers.Dense(128)(x)\nx = layers.BatchNormalization ()(x)\nx = layers.LeakyReLU ()(x)\nx = layers.Dropout(rate = 0.5)(x)\noutput_layer  = layers.Dense(10, activation  = 'softmax' )(x)\nmodel = models.Model(input_layer , output_layer )\nWe use four stacked Conv2D  layers, each followed by a BatchNormalization  and a\nLeakyReLU  layer. After flattening the resulting tensor, we pass the data through a\nDense  layer of size 128, again followed by a BatchNormalization  and a LeakyReLU\nlayer. This is immediately followed by a Dropout  layer for regularization, and the net‐\nwork is concluded with an output Dense  layer of size 10.\nThe order in which to use the batch normalization and activation\nlayers is a matter of preference. Usually batch normalization layers\nare placed before the activation, but some successful architectures\nuse these layers the other way around. If you do choose to use\nbatch normalization before activation, you can remember the order\nusing the acronym BAD  (batch normalization, activation, then\ndropout)!\nThe model summary is shown in Table 2-3 .\nTable 2-3. Model summary of the CNN for CIFAR-10\nLayer (type) Output shape Param #\nInputLayer (None, 32, 32, 3) 0\nConv2D (None, 32, 32, 32) 896\nBatchNormalization (None, 32, 32, 32) 128\nLeakyReLU (None, 32, 32, 32) 0\nConv2D (None, 16, 16, 32) 9,248\nBatchNormalization (None, 16, 16, 32) 128\nLeakyReLU (None, 16, 16, 32) 0\nConv2D (None, 16, 16, 64) 18,496\nBatchNormalization (None, 16, 16, 64) 256\nLeakyReLU (None, 16, 16, 64) 0\nConv2D (None, 8, 8, 64) 36,928\nBatchNormalization (None, 8, 8, 64) 256\n52 | Chapter 2: Deep Learning",3537
32-Summary.pdf,32-Summary,"Layer (type) Output shape Param #\nLeakyReLU (None, 8, 8, 64) 0\nFlatten (None, 4096) 0\nDense (None, 128) 524,416\nBatchNormalization (None, 128) 512\nLeakyReLU (None, 128) 0\nDropout (None, 128) 0\nDense (None, 10) 1290\nTotal params 592,554\nTrainable params 591,914\nNon-trainable params 640\nBefore moving on, make sure you are able to calculate the output\nshape and number of parameters for each layer by hand. It’s a good\nexercise to prove to yourself that you have fully understood how\neach layer is constructed and how it is connected to the preceding\nlayer! Don’t forget to include the bias weights that are included as\npart of the Conv2D  and Dense  layers.\nTraining and Evaluating the CNN\nWe compile and train the model in exactly the same way as before and call the\nevaluate  method to determine its accuracy on the holdout set ( Figure 2-16 ).\nFigure 2-16. CNN performance\nAs you can see, this model is now achieving 71.5% accuracy, up from 49.0% previ‐\nously. Much better! Figure 2-17  shows some predictions from our new convolutional\nmodel.\nThis improvement has been achieved simply by changing the architecture of the\nmodel to include convolutional, batch normalization, and dropout layers. Notice that\nthe number of parameters is actually fewer in our new model than the previous\nmodel, even though the number of layers is far greater. This demonstrates the impor‐\ntance of being experimental with your model design and being comfortable with how\nthe different layer types can be used to your advantage. When building generative\nConvolutional Neural Network (CNN) | 53\nmodels, it becomes even more important to understand the inner workings of your\nmodel since it is the middle layers of your network that capture the high-level fea‐\ntures that you are most interested in.\nFigure 2-17. CNN predictions\nSummary\nThis chapter introduced the core deep learning concepts that you will need to start\nbuilding deep generative models. We started by building a multilayer perceptron\n(MLP) using Keras and trained the model to predict the category of a given image\nfrom the CIFAR-10 dataset. Then, we improved upon this architecture by introduc‐\ning convolutional, batch normalization, and dropout layers to create a convolutional\nneural network (CNN).\nA really important point to take away from this chapter is that deep neural networks\nare completely flexible by design, and there really are no fixed rules when it comes to\nmodel architecture. There are guidelines and best practices, but you should feel free\nto experiment with layers and the order in which they appear. Don’t feel constrained\nto only use the architectures that you have read about in this book or elsewhere! Like\na child with a set of building blocks, the design of your neural network is only limited\nby your own imagination.\nIn the next chapter, we shall see how we can use these building blocks to design a net‐\nwork that can generate images.\nReferences\n1. Kaiming He et al., “Deep Residual Learning for Image Recognition, ” December 10,\n2015, https://arxiv.org/abs/1512.03385 .\n54 | Chapter 2: Deep Learning\n2. Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images, ” April 8,\n2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf .\n3. Diederik Kingma and Jimmy Ba, “ Adam: A Method for Stochastic Optimization, ”\nDecember 22, 2014, https://arxiv.org/abs/1412.6980v8 .\n4. Samuel L. Smith et al., “Don’t Decay the Learning Rate, Increase the Batch Size, ”\nNovember 1, 2017, https://arxiv.org/abs/1711.00489 .\n5. Vincent Dumoulin and Francesco Visin, “ A Guide to Convolution Arithmetic for\nDeep Learning, ” January 12, 2018, https://arxiv.org/abs/1603.07285 .\n6. Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Net‐\nwork Training by Reducing Internal Covariate Shift, ” February 11, 2015, https://\narxiv.org/abs/1502.03167 .\n7. Hinton et al., “Networks by Preventing Co-Adaptation of Feature Detectors, ” July 3,\n2012, https://arxiv.org/abs/1207.0580 .\n8. Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from\nOverfitting, ” Journal of Machine Learning Research  15 (2014): 1929–1958, http://\njmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf .\nSummary | 55",4295
33-Part II. Methods.pdf,33-Part II. Methods,"PART II\nMethods\nIn Part II  we will dive into the six families of generative models, including the theory\nbehind how they work and practical examples of how to build each type of model.\nIn Chapter 3  we shall take a look at our first generative deep learning model, the var‐\niational autoencoder . This technique will allow us to not only generate realistic faces,\nbut also alter existing images—for example, by adding a smile or changing the color\nof someone’s hair.\nChapter 4  explores one of the most successful generative modeling techniques of\nrecent years, the generative adversarial network . We shall see the ways that GAN train‐\ning has been fine-tuned and adapted to continually push the boundaries of what gen‐\nerative modeling is able to achieve.\nIn Chapter 5  we will delve into several examples of autoregressive models , including\nLSTMs and PixelCNN. This family of models treats the generation process as a\nsequence prediction problem—it underpins today’s state-of-the-art text generation\nmodels and can also be used for image generation.\nIn Chapter 6  we will cover the family of normalizing flow models , including RealNVP .\nThis model is based on a change of variables formula, which allows the transforma‐\ntion of a simple distribution, such as a Gaussian distribution, into a more complex\ndistribution in way that preserves tractability.\nChapter 7  introduces the family of energy-based models . These models train a scalar\nenergy function to score the validity of a given input. We will explore a technique for\ntraining energy-based models called contrastive divergence and a technique for sam‐\npling new observations called Langevin dynamics.\nFinally, in Chapter 8  we shall explore the family of diffusion  models . This technique is\nbased on the idea of iteratively adding noise to an image and then training a model to\nremove the noise, giving us the ability to transform pure noise into realistic samples.\nBy the end of Part II  you will have built practical examples of generative models from\neach of the six generative modeling families and be able to explain how each works\nfrom a theoretical perspective.",2164
34-The Decoder.pdf,34-The Decoder,"CHAPTER 3\nVariational Autoencoders\nChapter Goals\nIn this chapter you will:\n•Learn how the architectural design of autoencoders makes them perfectly suited\nto generative modeling.\n•Build and train an autoencoder from scratch using Keras.\n•Use autoencoders to generate new images, but understand the limitations of this\napproach.\n•Learn about the architecture of the variational autoencoder and how it solves\nmany of the problems associated with standard autoencoders.\n•Build a variational autoencoder from scratch using Keras.\n•Use variational autoencoders to generate new images.\n•Use variational autoencoders to manipulate generated images using latent space\narithmetic.\nIn 2013, Diederik P . Kingma and Max Welling published a paper that laid the founda‐\ntions for a type of neural network known as a variational autoencoder  (V AE). 1 This is\nnow one of the most fundamental and well-known deep learning architectures for\ngenerative modeling and an excellent place to start our journey into generative deep\nlearning.\nIn this chapter, we shall start by building a standard autoencoder and then see how\nwe can extend this framework to develop a variational autoencoder. Along the way,\nwe will pick apart both types of models, to understand how they work at a granular\nlevel. By the end of the chapter you should have a complete understanding of how to\n59\nbuild and manipulate autoencoder-based models and, in particular, how to build a\nvariational autoencoder from scratch to generate images based on your own dataset.\nIntroduction\nLet’s  start with a simple story that will help to explain the fundamental problem that\nan autoencoder is trying to solve.\nBrian, the Stitch, and the Wardrobe\nImagine that on the floor in front of you is a pile of all the clothing you own—trou‐\nsers, tops, shoes, and coats, all of different styles. Y our stylist, Brian, is becoming\nincreasingly frustrated with how long it takes him to find the items you require, so he\ndevises a clever plan.\nHe tells you to organize your clothes into a wardrobe that is infinitely high and wide\n(Figure 3-1 ). When you want to request a particular item, you simply need to tell\nBrian its location and he will sew the item from scratch using his trusty sewing\nmachine. It soon becomes obvious that you will need to place similar items near to\neach other, so that Brian can accurately re-create each item given only its location.\nFigure 3-1. A man standing in front of an infinite  2D wardrobe (created with\nMidjourney )\nAfter  several weeks of practice, you and Brian have adjusted to each other’s under‐\nstandings of the wardrobe layout. It is now possible for you to tell Brian the location\nof any item of clothing that you desire, and he can accurately sew it from scratch!\nThis gives you an idea—what would happen if you gave Brian a wardrobe location\nthat was empty? To your amazement, you find that Brian is able to generate entirely\n60 | Chapter 3: Variational Autoencoders\nnew items of clothing that haven’t existed before! The process isn’t perfect, but you\nnow have limitless options for generating new clothing, just by picking an empty\nlocation in the infinite wardrobe and letting Brian work his magic with the sewing\nmachine.\nLet’s now explore how this story relates to building autoencoders.\nAutoencoders\nA diagram of the process described by the story is shown in Figure 3-2 . Y ou play the\npart of the encoder , moving each item of clothing to a location in the wardrobe. This\nprocess is called encoding . Brian plays the part of the decoder , taking a location in the\nwardrobe and attempting to re-create the item. This process is called decoding .\nFigure 3-2. Items of clothing in the infinite  wardrobe—each black dot represents an item\nof clothing\nEach location in the wardrobe is represented by two numbers (i.e., a 2D vector). For\nexample, the trousers in Figure 3-2  are encoded to the point [6.3, –0.9]. This vector is\nalso known as an embedding  because the encoder attempts to embed as much infor‐\nmation into it as possible, so that the decoder can produce an accurate\nreconstruction.\nAutoencoders | 61\nAn autoencoder  is simply a neural network that is trained to perform the task of\nencoding and decoding an item, such that the output from this process is as close to\nthe original item as possible. Crucially, it can be used as a generative model, because\nwe can decode any point in the 2D space that we want (in particular, those that are\nnot embeddings of original items) to produce a novel item of clothing.\nLet’s now see how we can build an autoencoder using Keras and apply it to a real\ndataset!\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/03_vae/01_autoencoder/autoencoder.ipynb  in\nthe book repository.\nThe Fashion-MNIST Dataset\nFor this example, we’ll be using the Fashion-MNIST dataset —a collection of grayscale\nimages of clothing items, each of size 28 × 28 pixels. Some example images from the\ndataset are shown in Figure 3-3 .\nFigure 3-3. Examples of images from the Fashion-MNIST dataset\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown\nin Example 3-1 .\nExample 3-1. Loading the Fashion-MNIST dataset\nfrom tensorflow.keras  import datasets\n(x_train,y_train), (x_test,y_test) = datasets .fashion_mnist .load_data ()\nThese are 28 × 28 grayscale images (pixel values between 0 and 255) out of the box,\nwhich we need to preprocess to ensure that the pixel values are scaled between 0 and\n1. We will also pad each image to 32 × 32 for easier manipulation of the tensor shape\nas it passes through the network, as shown in Example 3-2 .\n62 | Chapter 3: Variational Autoencoders\nExample 3-2. Preprocessing the data\ndef preprocess (imgs):\n    imgs = imgs.astype(""float32"" ) / 255.0\n    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values =0.0)\n    imgs = np.expand_dims (imgs, -1)\n    return imgs\nx_train = preprocess (x_train)\nx_test = preprocess (x_test)\nNext, we need to understand the overall structure of an autoencoder, so that we can\ncode it up using TensorFlow and Keras.\nThe Autoencoder Architecture\nAn autoencoder  is a neural network made up of two parts:\n•An encoder  network that compresses high-dimensional input data such as an\nimage into a lower-dimensional embedding vector\n•A decoder  network that decompresses a given embedding vector back to the orig‐\ninal domain (e.g., back to an image)\nA diagram of the network architecture is shown in Figure 3-4 . An input image is\nencoded to a latent embedding vector z, which is then decoded back to the original\npixel space.\nFigure 3-4. Autoencoder architecture diagram\nThe autoencoder is trained to reconstruct an image, after it has passed through the\nencoder and back out through the decoder. This may seem strange at first—why\nwould you want to reconstruct a set of images that you already have available to you?\nHowever, as we shall see, it is the  embedding space (also called the  latent space ) that is\nthe interesting part of the autoencoder, as sampling from this space will allow us to\ngenerate new images.\nLet’s first define what we mean by an embedding. The embedding ( z) is a compres‐\nsion of the original image into a lower-dimensional latent space. The idea is that by\nchoosing any point in the latent space, we can generate novel images by passing this\npoint through the decoder, since the decoder has learned how to convert points in the\nlatent space into viable images.\nAutoencoders | 63\nIn our example, we will embed images into a two-dimensional latent space. This will\nhelp us to visualize the latent space, since we can easily plot points in 2D. In practice,\nthe latent space of an autoencoder will usually have more than two dimensions in\norder to have more freedom to capture greater nuance in the images.\nAutoencoders as Denoising Models\nAutoencoders  can be used to clean noisy images, since the encoder\nlearns that it is not useful to capture the position of the random\nnoise inside the latent space in order to reconstruct the original.\nFor tasks such as this, a 2D latent space is probably too small to\nencode sufficient relevant information from the input. However, as\nwe shall see, increasing the dimensionality of the latent space\nquickly leads to problems if we want to use the autoencoder as a\ngenerative model.\nLet’s now see how to build the encoder and decoder.\nThe Encoder\nIn an autoencoder, the encoder’s job is to take the input image and map it to an\nembedding vector in the latent space. The architecture of the encoder we will be\nbuilding is shown in Table 3-1 .\nTable 3-1. Model summary of the encoder\nLayer (type) Output shape Param #\nInputLayer (None, 32, 32, 1) 0\nConv2D (None, 16, 16, 32) 320\nConv2D (None, 8, 8, 64) 18,496\nConv2D (None, 4, 4, 128) 73,856\nFlatten (None, 2048) 0\nDense (None, 2) 4,098\nTotal params 96,770\nTrainable params 96,770\nNon-trainable params 0\nTo achieve this, we first create an Input  layer for the image and pass this through\nthree Conv2D  layers in sequence, each capturing increasingly high-level features. We\nuse a stride of 2 to halve the size of the output of each layer, while increasing the\nnumber of channels. The last convolutional layer is flattened and connected to a\nDense  layer of size 2, which represents our two-dimensional latent space.\n64 | Chapter 3: Variational Autoencoders\nExample 3-3  shows how to build this in Keras.\nExample 3-3. The encoder\nencoder_input  = layers.Input(\n    shape=(32, 32, 1), name = ""encoder_input""\n) \nx = layers.Conv2D(32, (3, 3), strides = 2, activation  = 'relu', padding=""same"")(\n    encoder_input\n) \nx = layers.Conv2D(64, (3, 3), strides = 2, activation  = 'relu', padding=""same"")(x)\nx = layers.Conv2D(128, (3, 3), strides = 2, activation  = 'relu', padding=""same"")(x)\nshape_before_flattening  = K.int_shape (x)[1:]\nx = layers.Flatten()(x) \nencoder_output  = layers.Dense(2, name=""encoder_output"" )(x) \nencoder = models.Model(encoder_input , encoder_output ) \nDefine the Input  layer of the encoder (the image).\nStack Conv2D  layers sequentially on top of each other.\nFlatten the last convolutional layer to a vector.\nConnect this vector to the 2D embeddings with a Dense  layer.\nThe Keras Model  that defines the encoder—a model that takes an input image\nand encodes it into a 2D embedding.\nI strongly encourage you to experiment with the number of convo‐\nlutional layers and filters to understand how the architecture\naffects the overall number of model parameters, model perfor‐\nmance, and model runtime.\nThe Decoder\nThe decoder is a mirror image of the encoder—instead of convolutional layers, we\nuse convolutional transpose  layers, as shown in Table 3-2 .\nTable 3-2. Model summary of the decoder\nLayer (type) Output shape Param #\nInputLayer (None, 2) 0\nDense (None, 2048) 6,144\nAutoencoders | 65\nLayer (type) Output shape Param #\nReshape (None, 4, 4, 128) 0\nConv2DTranspose (None, 8, 8, 128) 147,584\nConv2DTranspose (None, 16, 16, 64) 73,792\nConv2DTranspose (None, 32, 32, 32) 18,464\nConv2D (None, 32, 32, 1) 289\nTotal params 246,273\nTrainable params 246,273\nNon-trainable params 0\nConvolutional Transpose Layers\nStandard convolutional layers allow us to halve the size of an input tensor in both\ndimensions (height and width), by setting strides = 2 .\nThe convolutional transpose layer uses the same principle as a standard convolutional\nlayer (passing a filter across the image), but is different in that setting strides = 2\ndoubles  the size of the input tensor in both dimensions.\nIn a convolutional transpose layer, the strides  parameter determines the internal\nzero padding between pixels in the image, as shown in Figure 3-5 . Here, a 3 × 3 × 1\nfilter (gray) is being passed across a 3 × 3 × 1 image (blue) with strides = 2 , to pro‐\nduce a 6 × 6 × 1 output tensor (green).\nFigure 3-5. A convolutional transpose layer example (source: Dumoulin and Visin,\n2018 )2\nIn Keras, the Conv2DTranspose  layer allows us to perform convolutional transpose\noperations on tensors. By stacking these layers, we can gradually expand the size of\neach layer, using strides of 2, until we get back to the original image dimension of\n32 × 32.\n66 | Chapter 3: Variational Autoencoders",12400
35-Joining the Encoder to the Decoder.pdf,35-Joining the Encoder to the Decoder,"Example 3-4  shows how we build the decoder in Keras.\nExample 3-4. The decoder\ndecoder_input  = layers.Input(shape=(2,), name=""decoder_input"" ) \nx = layers.Dense(np.prod(shape_before_flattening ))(decoder_input ) \nx = layers.Reshape(shape_before_flattening )(x) \nx = layers.Conv2DTranspose (\n    128, (3, 3), strides=2, activation  = 'relu', padding=""same""\n)(x) \nx = layers.Conv2DTranspose (\n    64, (3, 3), strides=2, activation  = 'relu', padding=""same""\n)(x)\nx = layers.Conv2DTranspose (\n    32, (3, 3), strides=2, activation  = 'relu', padding=""same""\n)(x)\ndecoder_output  = layers.Conv2D(\n    1,\n    (3, 3),\n    strides = 1,\n    activation =""sigmoid"" ,\n    padding=""same"",\n    name=""decoder_output""\n)(x)\ndecoder = models.Model(decoder_input , decoder_output ) \nDefine the Input  layer of the decoder (the embedding).\nConnect the input to a Dense  layer.\nReshape  this vector into a tensor that can be fed as input into the first\nConv2DTranspose  layer.\nStack Conv2DTranspose  layers on top of each other.\nThe Keras Model  that defines the decoder—a model that takes an embedding in\nthe latent space and decodes it into the original image domain.\nJoining the Encoder to the Decoder\nTo train the encoder and decoder simultaneously, we need to define a model that will\nrepresent the flow of an image through the encoder and back out through the\ndecoder. Luckily, Keras makes it extremely easy to do this, as you can see in\nExample 3-5 . Notice the way in which we specify that the output from the autoen‐\ncoder is simply the output from the encoder after it has been passed through the\ndecoder.\nAutoencoders | 67\nExample 3-5. The full autoencoder\nautoencoder  = Model(encoder_input , decoder(encoder_output )) \nThe Keras Model  that defines the full autoencoder—a model that takes an image\nand passes it through the encoder and back out through the decoder to generate\na reconstruction of the original image.\nNow that we’ve defined our model, we just need to compile it with a loss function and\noptimizer, as shown in Example 3-6 . The loss function is usually chosen to be either\nthe root mean squared error (RMSE) or binary cross-entropy between the individual\npixels of the original image and the reconstruction.\nExample 3-6. Compiling the autoencoder\n# Compile the autoencoder\nautoencoder .compile(optimizer =""adam"", loss=""binary_crossentropy"" )\nChoosing the Loss Function\nOptimizing  for RMSE means that your generated output will be symmetrically dis‐\ntributed around the average pixel values (because an overestimation is penalized\nequivalently to an underestimation).\nOn the other hand, binary cross-entropy loss is asymmetrical—it penalizes errors\ntoward the extremes more heavily than errors toward the center. For example, if the\ntrue pixel value is high (say 0.7), then generating a pixel with value 0.8 is penalized\nmore heavily than generating a pixel with value 0.6. If the true pixel value is low (say\n0.3), then generating a pixel with value 0.2 is penalized more heavily than generating\na pixel with value 0.4.\nThis has the effect of binary cross-entropy loss producing slightly blurrier images\nthan RMSE loss (as it tends to push predictions toward 0.5), but sometimes this is\ndesirable as RMSE can lead to obviously pixelized edges.\nThere is no right or wrong choice—you should choose whichever works best for your\nuse case after experimentation.\nWe can now train the autoencoder by passing in the input images as both the input\nand output, as shown in Example 3-7 .\n68 | Chapter 3: Variational Autoencoders",3594
36-Generating New Images.pdf,36-Generating New Images,"Example 3-7. Training the autoencoder\nautoencoder .fit(\n    x_train,\n    x_train,\n    epochs=5,\n    batch_size =100,\n    shuffle=True,\n    validation_data =(x_test, x_test),\n)\nNow that our autoencoder is trained, the first thing we need to check is that it is able\nto accurately reconstruct the input images.\nReconstructing Images\nWe can test the ability to reconstruct images by passing images from the test set\nthrough the autoencoder and comparing the output to the original images. The code\nfor this is shown in Example 3-8 .\nExample 3-8. Reconstructing images using the autoencoder\nexample_images  = x_test[:5000]\npredictions  = autoencoder .predict(example_images )\nIn Figure 3-6  you can see some examples of original images (top row), the 2D vectors\nafter encoding, and the reconstructed items after decoding (bottom row).\nFigure 3-6. Examples of encoding and decoding items of clothing\nNotice how the reconstruction isn’t perfect—there are still some details of the original\nimages that aren’t captured by the decoding process, such as logos. This is because by\nreducing each image to just two numbers, we naturally lose some information.\nLet’s now investigate how the encoder is representing images in the latent space.\nAutoencoders | 69\nVisualizing the Latent Space\nWe can visualize how images are embedded into the latent space by passing the test\nset through the encoder and plotting the resulting embeddings, as shown in\nExample 3-9 .\nExample 3-9. Embedding images using the encoder\nembeddings  = encoder.predict(example_images )\nplt.figure(figsize=(8, 8))\nplt.scatter(embeddings [:, 0], embeddings [:, 1], c=""black"", alpha=0.5, s=3)\nplt.show()\nThe resulting plot is the scatter plot shown in Figure 3-2 —each black point represents\nan image that has been embedded into the latent space.\nIn order to better understand how this latent space is structured, we can make use of\nthe labels that come with the Fashion-MNIST dataset, describing the type of item in\neach image. There are 10 groups altogether, shown in Table 3-3 .\nTable 3-3. The Fashion-MNIST labels\nID Clothing label\n0 T-shirt/top\n1 Trouser\n2 Pullover\n3 Dress\n4 Coat\n5 Sandal\n6 Shirt\n7 Sneaker\n8 Bag\n9 Ankle boot\nWe can color each point based on the label of the corresponding image to produce\nthe plot in Figure 3-7 . Now the structure becomes very clear! Even though the cloth‐\ning labels were never shown to the model during training, the autoencoder has natu‐\nrally grouped items that look alike into the same parts of the latent space. For\nexample, the dark blue cloud of points in the bottom-right corner of the latent space\nare all different images of trousers and the red cloud of points toward the center are\nall ankle boots.\n70 | Chapter 3: Variational Autoencoders\nFigure 3-7. Plot of the latent space, colored by clothing label\nGenerating New Images\nWe can generate novel images by sampling some points in the latent space and using\nthe decoder to convert these back into pixel space, as shown in Example 3-10 .\nExample 3-10. Generating novel images using the decoder\nmins, maxs = np.min(embeddings , axis=0), np.max(embeddings , axis=0)\nsample = np.random.uniform(mins, maxs, size=(18, 2))\nreconstructions  = decoder.predict(sample)\nSome examples of generated images are shown in Figure 3-8 , alongside their embed‐\ndings in the latent space.\nAutoencoders | 71\nFigure 3-8. Generated items of clothing\nEach blue dot maps to one of the images shown on the right of the diagram, with the\nembedding vector shown underneath. Notice how some of the generated items are\nmore realistic than others. Why is this?\nTo answer this, let’s first make a few observations about the overall distribution of\npoints in the latent space, referring back to Figure 3-7 :\n•Some clothing items are represented over a very small area and others over a\nmuch larger area.\n•The distribution is not symmetrical about the point (0, 0), or bounded. For\nexample, there are far more points with positive y-axis values than negative, and\nsome points even extend to a y-axis value > 8.\n•There are large gaps between colors containing few points.\nThese observations actually make sampling from the latent space quite challenging. If\nwe overlay the latent space with images of decoded points on a grid, as shown in\nFigure 3-9 , we can begin to understand why the decoder may not always generate\nimages to a satisfactory standard.\n72 | Chapter 3: Variational Autoencoders\nFigure 3-9. A grid of decoded embeddings, overlaid with the embeddings from the origi‐\nnal images in the dataset, colored by item type\nFirstly, we can see that if we pick points uniformly in a bounded space that we define,\nwe’re more likely to sample something that decodes to look like a bag (ID 8) than an\nankle boot (ID 9) because the part of the latent space carved out for bags (orange) is\nlarger than the ankle boot area (red).\nSecondly, it is not obvious how we should go about choosing a random  point in the\nlatent space, since the distribution of these points is undefined. Technically, we would\nbe justified in choosing any point in the 2D plane! It’s not even guaranteed that points\nwill be centered around (0, 0). This makes sampling from our latent space\nproblematic.\nLastly, we can see holes in the latent space where none of the original images are\nencoded. For example, there are large white spaces at the edges of the domain—the\nautoencoder has no reason to ensure that points here are decoded to recognizable\nclothing items as very few images in the training set are encoded here.\nEven points that are central may not be decoded into well-formed images. This is\nbecause the autoencoder is not forced to ensure that the space is continuous. For\nexample, even though the point (–1, –1) might be decoded to give a satisfactory\nAutoencoders | 73",5887
37-The Encoder.pdf,37-The Encoder,"image of a sandal, there is no mechanism in place to ensure that the point (–1.1, –1.1)\nalso produces a satisfactory image of a sandal.\nIn two dimensions this issue is subtle; the autoencoder only has a small number of\ndimensions to work with, so naturally it has to squash clothing groups together,\nresulting in the space between clothing groups being relatively small. However, as we\nstart to use more dimensions in the latent space to generate more complex images\nsuch as faces, this problem becomes even more apparent. If we give the autoencoder\nfree rein over how it uses the latent space to encode images, there will be huge gaps\nbetween groups of similar points with no incentive for the spaces in between to gen‐\nerate well-formed images.\nIn order to solve these three problems, we need to convert our autoencoder into a\nvariational autoencoder .\nVariational Autoencoders\nTo explain, let’s revisit the infinite wardrobe and make a few changes…\nRevisiting the Infinite  Wardrobe\nSuppose now, instead of placing every item of clothing at a single point in the ward‐\nrobe, you decide to allocate a general area where the item is more likely to be found.\nY ou reason that this more relaxed approach to item location will help to solve the cur‐\nrent issue around local discontinuities in the wardrobe.\nAlso, in order to ensure you do not become too careless with the new placement sys‐\ntem, you agree with Brian that you will try to place the center of each item’s area as\nclose to the middle of the wardrobe as possible and that deviation of the item from\nthe center should be as close to one meter as possible (not smaller and not larger).\nThe further you stray from this rule, the more you have to pay Brian as your stylist.\nAfter several months of operating with these two simple changes, you step back and\nadmire the new wardrobe layout, alongside some examples of new clothing items that\nBrian has generated. Much better! There is plenty of diversity in the generated items,\nand this time there are no examples of poor-quality garments. It seems the two\nchanges have made all the difference!\nLet’s now try to understand what we need to do to our autoencoder model to convert\nit into a variational autoencoder and thus make it a more sophisticated generative\nmodel.\nThe two parts that we need to change are the encoder and the loss function.\n74 | Chapter 3: Variational Autoencoders\nThe Encoder\nIn an autoencoder, each image is mapped directly to one point in the latent space. In\na variational autoencoder, each image is instead mapped to a multivariate normal dis‐\ntribution around a point in the latent space, as shown in Figure 3-10 .\nFigure 3-10. The difference  between the encoders in an autoencoder and a variational\nautoencoder\nThe Multivariate Normal Distribution\nA normal distribution  (or Gaussian distribution ) μ,σ is a probability distribution\ncharacterized by a distinctive bell curve  shape, defined by two variables: the mean  (μ)\nand the variance  (σ2). The standard deviation  (\(\sigma\) ) is the square root of the\nvariance.\nThe probability density function of the normal distribution in one dimension is:\nfx∣μ,σ2=1\n2πσ2e−x−μ2\n2σ2\nFigure 3-11  shows several normal distributions in one dimension, for different values\nof the mean and variance. The red curve is the standard normal  (or unit normal )\n0, 1—the normal distribution with mean equal to 0 and variance equal to 1.\nWe can sample a point z from a normal distribution with mean μ and standard devia‐\ntion σ using the following equation:\nz=μ+σ\nVariational Autoencoders | 75\nwhere  is sampled from a standard normal distribution.\nFigure 3-11. The normal distribution in one dimension (source: Wikipedia )\nThe concept of a normal distribution extends to more than one dimension—the\nprobability density function for a multivariate normal distribution  (or multivariate\nGaussian distribution ) μ,Σ in k dimensions with mean vector μ and symmetric\ncovariance matrix Σ is as follows:\nfx1, ...,xk=exp −1\n2−μTΣ−1−μ\n2πkΣ\nIn this book, we will typically be using isotropic  multivariate normal distributions,\nwhere the covariance matrix is diagonal. This means that the distribution is inde‐\npendent in each dimension (i.e., we can sample a vector where each element is nor‐\nmally distributed with independent mean and variance). This is the case for the\nmultivariate normal distribution that we will use in our variational autoencoder.\nA multivariate standard normal distribution  0, is a multivariate distribution with\na zero-valued mean vector and identity covariance matrix.\nNormal Versus Gaussian\nIn this book, the terms normal  and Gaussian  are used inter‐\nchangeably and the isotropic and multivariate nature of the\ndistribution is usually implied. For example, “we sample from\na Gaussian distribution” can be interpreted to mean “we sam‐\nple from an isotropic, multivariate Gaussian distribution. ”\n76 | Chapter 3: Variational Autoencoders\nThe encoder only needs to map each input to a mean vector and a variance vector\nand does not need to worry about covariance between dimensions. Variational\nautoencoders assume that there is no correlation between dimensions in the latent\nspace.\nVariance values are always positive, so we actually choose to map to the logarithm  of\nthe variance, as this can take any real number in the range ( −∞, ∞). This way we can\nuse a neural network as the encoder to perform the mapping from the input image to\nthe mean and log variance vectors.\nTo summarize, the encoder will take each input image and encode it to two vectors\nthat together define a multivariate normal distribution in the latent space:\nz_mean\nThe mean point of the distribution\nz_log_var\nThe logarithm of the variance of each dimension\nWe can sample a point z from the distribution defined by these values using the fol‐\nlowing equation:\nz = z_mean + z_sigma * epsilon\nwhere:\nz_sigma = exp(z_log_var * 0.5)\nepsilon ~ N(0,I)\nThe derivation of the relationship between z_sigma  (σ) and\nz_log_var  (log σ2) is as follows:\nσ= exp log σ= exp 2 log σ/2= exp log σ2/2\nThe decoder of a variational autoencoder is identical to the decoder of a plain autoen‐\ncoder, giving the overall architecture shown in Figure 3-12 .\nFigure 3-12. VAE architecture diagram\nVariational Autoencoders | 77\nWhy does this small change to the encoder help?\nPreviously, we saw that there was no requirement for the latent space to be continu‐\nous—even if the point (–2, 2) decodes to a well-formed image of a sandal, there’s no\nrequirement for (–2.1, 2.1) to look similar. Now, since we are sampling a random\npoint from an area around z_mean , the decoder must ensure that all points in the\nsame neighborhood produce very similar images when decoded, so that the recon‐\nstruction loss remains small. This is a very nice property that ensures that even when\nwe choose a point in the latent space that has never been seen by the decoder, it is\nlikely to decode to an image that is well formed.\nBuilding the VAE encoder\nLet’s now see how we build this new version of the encoder in Keras.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/03_vae/02_vae_fashion/vae_fashion.ipynb  in\nthe book repository.\nThe code has been adapted from the excellent V AE tutorial  created\nby Francois Chollet, available on the Keras website.\nFirst, we need to create a new type of Sampling  layer that will allow us to sample from\nthe distribution defined by z_mean  and z_log_var , as shown in Example 3-11 .\nExample 3-11. The Sampling  layer\nclass Sampling (layers.Layer): \n    def call(self, inputs):\n        z_mean, z_log_var  = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = K.random_normal (shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var ) * epsilon \nWe create a new layer by subclassing the Keras base Layer  class (see the “Sub‐\nclassing the Layer Class” sidebar).\nWe use the reparameterization trick (see “The Reparameterization Trick” side‐\nbar) to build a sample from the normal distribution parameterized by z_mean\nand z_log_var .\n78 | Chapter 3: Variational Autoencoders\nSubclassing the Layer Class\nY ou can create new layers in Keras by subclassing the abstract Layer  class and defin‐\ning the call  method, which describes how a tensor is transformed by the layer.\nFor example, in the variational autoencoder, we can create a Sampling  layer that can\nhandle the sampling of z from a normal distribution with parameters defined by\nz_mean  and z_log_var .\nThis is useful when you want to apply a transformation to a tensor that isn’t already\nincluded as one of the out-of-the-box Keras layer types.\nThe Reparameterization Trick\nRather  than sample directly from a normal distribution with parameters z_mean  and\nz_log_var , we can sample epsilon  from a standard normal and then manually adjust\nthe sample to have the correct mean and variance.\nThis is known as the reparameterization trick , and it’s important as it means gradients\ncan backpropagate freely through the layer. By keeping all of the randomness of the\nlayer contained within the variable epsilon , the partial derivative of the layer output\nwith respect to its input can be shown to be deterministic (i.e., independent of the\nrandom epsilon ), which is essential for backpropagation through the layer to be\npossible.\nThe complete code for the encoder, including the new Sampling  layer, is shown in\nExample 3-12 .\nExample 3-12. The encoder\nencoder_input  = layers.Input(\n    shape=(32, 32, 1), name=""encoder_input""\n)\nx = layers.Conv2D(32, (3, 3), strides=2, activation =""relu"", padding=""same"")(\n    encoder_input\n)\nx = layers.Conv2D(64, (3, 3), strides=2, activation =""relu"", padding=""same"")(x)\nx = layers.Conv2D(128, (3, 3), strides=2, activation =""relu"", padding=""same"")(x)\nshape_before_flattening  = K.int_shape (x)[1:]\nx = layers.Flatten()(x)\nz_mean = layers.Dense(2, name=""z_mean"" )(x) \nz_log_var  = layers.Dense(2, name=""z_log_var"" )(x)\nz = Sampling ()([z_mean, z_log_var ]) \nVariational Autoencoders | 79",10233
38-The Loss Function.pdf,38-The Loss Function,"encoder = models.Model(encoder_input , [z_mean, z_log_var , z], name=""encoder"" ) \nInstead of connecting the Flatten  layer directly to the 2D latent space, we con‐\nnect it to layers z_mean  and z_log_var .\nThe Sampling  layer samples a point z in the latent space from the normal distri‐\nbution defined by the parameters z_mean  and z_log_var .\nThe Keras Model  that defines the encoder—a model that takes an input image\nand outputs z_mean , z_log_var , and a sampled point z from the normal distribu‐\ntion defined by these parameters.\nA summary of the encoder is shown in Table 3-4 .\nTable 3-4. Model summary of the VAE encoder\nLayer (type) Output shape Param # Connected to\nInputLayer (input) (None, 32, 32, 1) 0 []\nConv2D (conv2d_1) (None, 16, 16, 32) 320 [input]\nConv2D (conv2d_2) (None, 8, 8, 64) 18,496 [conv2d_1]\nConv2D (conv2d_3) (None, 4, 4, 128) 73,856 [conv2d_2]\nFlatten (flatten) (None, 2048) 0 [conv2d_3]\nDense (z_mean) (None, 2) 4,098 [flatten]\nDense (z_log_var) (None, 2) 4,098 [flatten]\nSampling (z) (None, 2) 0 [z_mean, z_log_var]\nTotal params 100,868\nTrainable params 100,868\nNon-trainable params 0\nThe only other part of the original autoencoder that we need to change is the loss\nfunction.\nThe Loss Function\nPreviously, our loss function only consisted of the reconstruction loss  between images\nand their attempted copies after being passed through the encoder and decoder. The\nreconstruction loss also appears in a variational autoencoder, but we now require one\nextra component: the Kullback–Leibler (KL) divergence  term.\n80 | Chapter 3: Variational Autoencoders\nKL divergence is a way of measuring how much one probability distribution differs\nfrom another. In a V AE, we want to measure how much our normal distribution with\nparameters z_mean  and z_log_var  differs from a standard normal distribution. In\nthis special case, it can be shown that the KL divergence has the following closed\nform:\nkl_loss = -0.5 * sum(1 + z_log_var - z_mean ^ 2 - exp(z_log_var))\nor in mathematical notation:\nDKLNμ,σ∥N0, 1 = −1\n2∑1 +logσ2−μ2−σ2\nThe sum is taken over all the dimensions in the latent space. kl_loss  is minimized to\n0 when z_mean = 0  and z_log_var = 0  for all dimensions. As these two terms start\nto differ from 0, kl_loss  increases.\nIn summary, the KL divergence term penalizes the network for encoding observa‐\ntions to z_mean  and z_log_var  variables that differ significantly from the parameters\nof a standard normal distribution, namely z_mean = 0  and z_log_var = 0 .\nWhy does this addition to the loss function help?\nFirstly, we now have a well-defined distribution that we can use for choosing points\nin the latent space—the standard normal distribution. Secondly, since this term tries\nto force all encoded distributions toward the standard normal distribution, there is\nless chance that large gaps will form between point clusters. Instead, the encoder will\ntry to use the space around the origin symmetrically and efficiently.\nIn the original V AE paper, the loss function for a V AE was simply the addition of the\nreconstruction loss and the KL divergence loss term. A variant on this (the β-V AE)\nincludes a factor that weights the KL divergence to ensure that it is well balanced with\nthe reconstruction loss. If we weight the reconstruction loss too heavily, the KL loss\nwill not have the desired regulatory effect and we will see the same problems that we\nexperienced with the plain autoencoder. If the KL divergence term is weighted too\nheavily, the KL divergence loss will dominate and the reconstructed images will be\npoor. This weighting term is one of the parameters to tune when you’re training your\nV AE.\nVariational Autoencoders | 81",3745
39-Training the Variational Autoencoder.pdf,39-Training the Variational Autoencoder,"Training the Variational Autoencoder\nExample 3-13  shows how we build the overall V AE model as a subclass of the abstract\nKeras Model  class. This allows us to include the calculation of the KL divergence term\nof the loss function in a custom train_step  method.\nExample 3-13. Training the VAE\nclass VAE(models.Model):\n    def __init__ (self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__ (**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker  = metrics.Mean(name=""total_loss"" )\n        self.reconstruction_loss_tracker  = metrics.Mean(\n            name=""reconstruction_loss""\n        )\n        self.kl_loss_tracker  = metrics.Mean(name=""kl_loss"" )\n    @property\n    def metrics(self):\n        return [\n            self.total_loss_tracker ,\n            self.reconstruction_loss_tracker ,\n            self.kl_loss_tracker ,\n        ]\n    def call(self, inputs): \n        z_mean, z_log_var , z = encoder(inputs)\n        reconstruction  = decoder(z)\n        return z_mean, z_log_var , reconstruction\n    def train_step (self, data): \n        with tf.GradientTape () as tape:\n            z_mean, z_log_var , reconstruction  = self(data)\n            reconstruction_loss  = tf.reduce_mean (\n                500\n                * losses.binary_crossentropy (\n                    data, reconstruction , axis=(1, 2, 3)\n                )\n            ) \n            kl_loss = tf.reduce_mean (\n                tf.reduce_sum (\n                    -0.5\n                    * (1 + z_log_var  - tf.square(z_mean) - tf.exp(z_log_var )),\n                    axis = 1,\n                )\n            )\n            total_loss  = reconstruction_loss  + kl_loss \n        grads = tape.gradient (total_loss , self.trainable_weights )\n82 | Chapter 3: Variational Autoencoders\n        self.optimizer .apply_gradients (zip(grads, self.trainable_weights ))\n        self.total_loss_tracker .update_state (total_loss )\n        self.reconstruction_loss_tracker .update_state (reconstruction_loss )\n        self.kl_loss_tracker .update_state (kl_loss)\n        return {m.name: m.result() for m in self.metrics}\nvae = VAE(encoder, decoder)\nvae.compile(optimizer =""adam"")\nvae.fit(\n    train,\n    epochs=5,\n    batch_size =100\n)\nThis function describes what we would like returned what we call the V AE on a\nparticular input image.\nThis function describes one training step of the V AE, including the calculation of\nthe loss function.\nA beta value of 500 is used in the reconstruction loss.\nThe total loss is the sum of the reconstruction loss and the KL divergence loss.\nGradient Tape\nTensorFlow’s Gradient Tape  is a mechanism that allows the compu‐\ntation of gradients of operations executed during a forward pass of\na model. To use it, you need to wrap the code that performs the\noperations you want to differentiate in a tf.GradientTape()  con‐\ntext. Once you have recorded the operations, you can compute the\ngradient of the loss function with respect to some variables by call‐\ning tape.gradient() . The gradients can then be used to update the\nvariables with the optimizer.\nThis mechanism is useful for calculating the gradient of custom\nloss functions (as we have done here) and also for creating custom\ntraining loops, as we shall see in Chapter 4 .\nVariational Autoencoders | 83",3395
40-Exploring the Latent Space.pdf,40-Exploring the Latent Space,"Analysis of the Variational Autoencoder\nNow  that we have trained our V AE, we can use the encoder to encode the images in\nthe test set and plot the z_mean  values in the latent space. We can also sample from a\nstandard normal distribution to generate points in the latent space and use the\ndecoder to decode these points back into pixel space to see how the V AE performs.\nFigure 3-13  shows the structure of the new latent space, alongside some sampled\npoints and their decoded images. We can immediately see several changes in how the\nlatent space is organized.\nFigure 3-13. The new latent space: the black dots show the z_mean  value of each encoded\nimage, while blue dots show some sampled points in the latent space (with their decoded\nimages on the right)\nFirstly, the KL divergence loss term ensures that the z_mean  and z_log_var  values of\nthe encoded images never stray too far from a standard normal distribution. Sec‐\nondly, there are not so many poorly formed images as the latent space is now much\nmore continuous, due to fact that the encoder is now stochastic, rather than\ndeterministic.\nFinally, by coloring points in the latent space by clothing type ( Figure 3-14 ), we can\nsee that there is no preferential treatment of any one type. The righthand plot shows\nthe space transformed into p-values—we can see that each color is approximately\nequally represented. Again, it’s important to remember that the labels were not used\nat all during training; the V AE has learned the various forms of clothing by itself in\norder to help minimize reconstruction loss.\n84 | Chapter 3: Variational Autoencoders",1639
41-The CelebA Dataset.pdf,41-The CelebA Dataset,"Figure 3-14. The latent space of the VAE colored by clothing type\nExploring the Latent Space\nSo far, all of our work on autoencoders and variational autoencoders has been limited\nto a latent space with two dimensions. This has helped us to visualize the inner work‐\nings of a V AE on the page and understand why the small tweaks that we made to the\narchitecture of the autoencoder helped transform it into a more powerful class of net‐\nwork that can be used for generative modeling.\nLet’s now turn our attention to a more complex dataset and see the amazing things\nthat variational autoencoders can achieve when we increase the dimensionality of the\nlatent space.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/03_vae/03_faces/vae_faces.ipynb  in the book\nrepository.\nThe CelebA Dataset\nWe shall be using the CelebFaces Attributes (CelebA) dataset  to train our next varia‐\ntional autoencoder. This is a collection of over 200,000 color images of celebrity faces,\neach annotated with various labels (e.g., wearing hat , smiling , etc.). A few examples\nare shown in Figure 3-15 .\nExploring the Latent Space | 85\nFigure 3-15. Some examples from the CelebA dataset (source: Liu et al., 2015 )3\nOf course, we don’t need the labels to train the V AE, but these will be useful later\nwhen we start exploring how these features are captured in the multidimensional\nlatent space. Once our V AE is trained, we can sample from the latent space to gener‐\nate new examples of celebrity faces.\nThe CelebA dataset is also available through  Kaggle, so you can download the dataset\nby running the Kaggle dataset downloader script in the book repository, as shown in\nExample 3-14 . This will save the images and accompanying metadata locally to\nthe /data  folder.\nExample 3-14. Downloading the CelebA dataset\nbash scripts/download_kaggle_data.sh  jessicali9530  celeba-dataset\nWe use the Keras function image_dataset_from_directory  to create a TensorFlow\nDataset pointed at the directory where the images are stored, as shown in\nExample 3-15 . This allows us to read batches of images into memory only when\nrequired (e.g., during training), so that we can work with large datasets and not worry\nabout having to fit the entire dataset into memory. It also resizes the images to 64 ×\n64, interpolating between pixel values.\n86 | Chapter 3: Variational Autoencoders",2454
42-Training the Variational Autoencoder.pdf,42-Training the Variational Autoencoder,"Example 3-15. Preprocessing the CelebA dataset\ntrain_data  = utils.image_dataset_from_directory (\n    ""/app/data/celeba-dataset/img_align_celeba/img_align_celeba"" ,\n    labels=None,\n    color_mode =""rgb"",\n    image_size =(64, 64),\n    batch_size =128,\n    shuffle=True,\n    seed=42,\n    interpolation =""bilinear"" ,\n)\nThe original data is scaled in the range [0, 255] to denote the pixel intensity, which we\nrescale to the range [0, 1] as shown in Example 3-16 .\nExample 3-16. Preprocessing the CelebA dataset\ndef preprocess (img):\n    img = tf.cast(img, ""float32"" ) / 255.0\n    return img\ntrain = train_data .map(lambda x: preprocess (x))\nTraining the Variational Autoencoder\nThe network architecture for the faces model is similar to the Fashion-MNIST exam‐\nple, with a few slight differences:\n•Our data now has three input channels (RGB) instead of one (grayscale). This\nmeans we need to change the number of channels in the final convolutional\ntranspose layer of the decoder to 3.\n•We shall be using a latent space with 200 dimensions instead of 2. Since faces are\nmuch more complex than the Fashion-MNIST images, we increase the dimen‐\nsionality of the latent space so that the network can encode a satisfactory amount\nof detail from the images.\n•There are batch normalization layers after each convolutional layer to stabilize\ntraining. Even though each batch takes a longer time to run, the number of\nbatches required to reach the same loss is greatly reduced.\n•We increase the β factor for the KL divergence to 2,000. This is a parameter that\nrequires tuning; for this dataset and architecture this value was found to generate\ngood results.\nExploring the Latent Space | 87\nThe full architectures of the encoder and decoder are shown in Tables 3-5 and 3-6,\nrespectively.\nTable 3-5. Model summary of the VAE faces encoder\nLayer (type) Output shape Param # Connected to\nInputLayer (input) (None, 32, 32, 3) 0 []\nConv2D (conv2d_1) (None, 16, 16, 128) 3,584 [input]\nBatchNormalization (bn_1) (None, 16, 16, 128) 512 [conv2d_1]\nLeakyReLU (lr_1) (None, 16, 16, 128) 0 [bn_1]\nConv2D (conv2d_2) (None, 8, 8, 128) 147,584 [lr_1]\nBatchNormalization (bn_2) (None, 8, 8, 128) 512 [conv2d_2]\nLeakyReLU (lr_2) (None, 8, 8, 128) 0 [bn_2]\nConv2D (conv2d_3) (None, 4, 4, 128) 147,584 [lr_2]\nBatchNormalization (bn_3) (None, 4, 4, 128) 512 [conv2d_3]\nLeakyReLU (lr_3) (None, 4, 4, 128) 0 [bn_3]\nConv2D (conv2d_4) (None, 2, 2, 128) 147,584 [lr_3]\nBatchNormalization (bn_4) (None, 2, 2, 128) 512 [conv2d_4]\nLeakyReLU (lr_4) (None, 2, 2, 128) 0 [bn_4]\nFlatten (flatten) (None, 512) 0 [lr_4]\nDense (z_mean) (None, 200) 102,600 [flatten]\nDense (z_log_var) (None, 200) 102,600 [flatten]\nSampling (z) (None, 200) 0 [z_mean, z_log_var]\nTotal params 653,584\nTrainable params 652,560\nNon-trainable params 1,024\nTable 3-6. Model summary of the VAE faces decoder\nLayer (type) Output shape Param #\nInputLayer (None, 200) 0\nDense (None, 512) 102,912\nBatchNormalization (None, 512) 2,048\nLeakyReLU (None, 512) 0\nReshape (None, 2, 2, 128) 0\nConv2DTranspose (None, 4, 4, 128) 147,584\nBatchNormalization (None, 4, 4, 128) 512\nLeakyReLU (None, 4, 4, 128) 0\nConv2DTranspose (None, 8, 8, 128) 147,584\n88 | Chapter 3: Variational Autoencoders",3278
43-Summary.pdf,43-Summary,"Layer (type) Output shape Param #\nBatchNormalization (None, 8, 8, 128) 512\nLeakyReLU (None, 8, 8, 128) 0\nConv2DTranspose (None, 16, 16, 128) 147,584\nBatchNormalization (None, 16, 16, 128) 512\nLeakyReLU (None, 16, 16, 128) 0\nConv2DTranspose (None, 32, 32, 128) 147,584\nBatchNormalization (None, 32, 32, 128) 512\nLeakyReLU (None, 32, 32, 128) 0\nConv2DTranspose (None, 32, 32, 3) 3,459\nTotal params 700,803\nTrainable params 698,755\nNon-trainable params 2,048\nAfter around five epochs of training, our V AE should be able to produce novel\nimages of celebrity faces!\nAnalysis of the Variational Autoencoder\nFirst, let’s take a look at a sample of reconstructed faces. The top row in Figure 3-16\nshows the original images and the bottom row shows the reconstructions once they\nhave passed through the encoder and decoder.\nFigure 3-16. Reconstructed faces, after  passing through the encoder and decoder\nWe can see that the V AE has successfully captured the key features of each face—the\nangle of the head, the hairstyle, the expression, etc. Some of the fine detail is missing,\nbut it is important to remember that the aim of building variational autoencoders\nisn’t to achieve perfect reconstruction loss. Our end goal is to sample from the latent\nspace in order to generate new faces.\nFor this to be possible we must check that the distribution of points in the latent\nspace approximately resembles a multivariate standard normal distribution. If we see\nany dimensions that are significantly different from a standard normal distribution,\nExploring the Latent Space | 89\nwe should probably reduce the reconstruction loss factor, since the KL divergence\nterm isn’t having enough effect.\nThe first 50 dimensions in our latent space are shown in Figure 3-17 . There aren’t any\ndistributions that stand out as being significantly different from the standard normal,\nso we can move on to generating some faces!\nFigure 3-17. Distributions of points for the first 50 dimensions in the latent space\nGenerating New Faces\nTo generate new faces, we can use the code in Example 3-17 .\nExample 3-17. Generating new faces from the latent space\ngrid_width , grid_height  = (10,3)\nz_sample  = np.random.normal(size=(grid_width  * grid_height , 200)) \nreconstructions  = decoder.predict(z_sample ) \nfig = plt.figure(figsize=(18, 5))\nfig.subplots_adjust (hspace=0.4, wspace=0.4)\nfor i in range(grid_width  * grid_height ):\n    ax = fig.add_subplot (grid_height , grid_width , i + 1)\n    ax.axis(""off"")\n    ax.imshow(reconstructions [i, :, :]) \nSample 30 points from a standard multivariate normal distribution with 200\ndimensions.\nDecode the sampled points.\nPlot the images!\n90 | Chapter 3: Variational Autoencoders\nThe output is shown in Figure 3-18 .\nFigure 3-18. New generated faces\nAmazingly, the V AE is able to take the set of points that we sampled from a standard\nnormal distribution and convert each into a convincing image of a person’s face. This\nis our first glimpse of the true power of generative models!\nNext, let’s see if we can start to use the latent space to perform some interesting oper‐\nations on generated images.\nLatent Space Arithmetic\nOne  benefit of mapping images into a lower-dimensional latent space is that we can\nperform arithmetic on vectors in this latent space that has a visual analogue when\ndecoded back into the original image domain.\nFor example, suppose we want to take an image of somebody who looks sad and give\nthem a smile. To do this we first need to find a vector in the latent space that points in\nthe direction of increasing smile. Adding this vector to the encoding of the original\nimage in the latent space will give us a new point which, when decoded, should give\nus a more smiley version of the original image.\nSo how can we find the smile  vector? Each image in the CelebA dataset is labeled with\nattributes, one of which is Smiling . If we take the average position of encoded images\nin the latent space with the attribute Smiling  and subtract the average position of\nencoded images that do not have the attribute Smiling , we will obtain the vector that\npoints in the direction of Smiling , which is exactly what we need.\nConceptually, we are performing the following vector arithmetic in the latent space,\nwhere alpha  is a factor that determines how much of the feature vector is added or\nsubtracted:\nz_new = z + alpha * feature_vector\nExploring the Latent Space | 91\nLet’s see this in action. Figure 3-19  shows several images that have been encoded into\nthe latent space. We then add or subtract multiples of a certain vector (e.g., Smiling ,\nBlack_Hair , Eyeglasses , Young , Male , Blond_Hair ) to obtain different versions of the\nimage, with only the relevant feature changed.\nFigure 3-19. Adding and subtracting features to and from faces\nIt is remarkable that even though we are moving the point a significantly large dis‐\ntance in the latent space, the core image remains approximately the same, except for\nthe one feature that we want to manipulate. This demonstrates the power of varia‐\ntional autoencoders for capturing and adjusting high-level features in images.\nMorphing Between Faces\nWe can use a similar idea to morph between two faces. Imagine two points in the\nlatent space, A and B, that represent two images. If you started at point A and walked\ntoward point B in a straight line, decoding each point on the line as you went, you\nwould see a gradual transition from the starting face to the end face.\nMathematically, we are traversing a straight line, which can be described by the fol‐\nlowing equation:\nz_new = z_A * (1- alpha) + z_B * alpha\nHere, alpha  is a number between 0 and 1 that determines how far along the line we\nare, away from point A.\n92 | Chapter 3: Variational Autoencoders\nFigure 3-20  shows this process in action. We take two images, encode them into the\nlatent space, and then decode points along the straight line between them at regular\nintervals.\nFigure 3-20. Morphing between two faces\nIt is worth noting the smoothness of the transition—even where there are multiple\nfeatures to change simultaneously (e.g., removal of glasses, hair color, gender), the\nV AE manages to achieve this fluidly, showing that the latent space of the V AE is truly\na continuous space that can be traversed and explored to generate a multitude of dif‐\nferent human faces.\nSummary\nIn this chapter we have seen how variational autoencoders are a powerful tool in the\ngenerative modeling toolbox. We started by exploring how plain autoencoders can be\nused to map high-dimensional images into a low-dimensional latent space, so that\nhigh-level features can be extracted from the individually uninformative pixels. How‐\never, we quickly found that there were some drawbacks to using plain autoencoders\nas a generative model—sampling from the learned latent space was problematic, for\nexample.\nVariational autoencoders solve these problems by introducing randomness into the\nmodel and constraining how points in the latent space are distributed. We saw that\nwith a few minor adjustments, we can transform our autoencoder into a variational\nautoencoder, thus giving it the power to be a true generative model.\nSummary | 93\nFinally, we applied our new technique to the problem of face generation and saw how\nwe can simply decode points from a standard normal distribution to generate new\nfaces. Moreover, by performing vector arithmetic within the latent space, we can\nachieve some amazing effects, such as face morphing and feature manipulation.\nIn the next chapter, we shall explore a different kind of model that remains a popular\nchoice for generative image modeling: the generative adversarial network.\nReferences\n1. Diederik P . Kingma and Max Welling, “ Auto-Encoding Variational Bayes, ” Decem‐\nber 20, 2013, https://arxiv.org/abs/1312.6114 .\n2. Vincent Dumoulin and Francesco Visin, “ A Guide to Convolution Arithmetic for\nDeep Learning, ” January 12, 2018, https://arxiv.org/abs/1603.07285 .\n3. Ziwei Liu et al., “Large-Scale CelebFaces Attributes (CelebA) Dataset, ” 2015, http://\nmmlab.ie.cuhk.edu.hk/projects/CelebA.html .\n94 | Chapter 3: Variational Autoencoders",8259
44-The Discriminator.pdf,44-The Discriminator,"CHAPTER 4\nGenerative Adversarial Networks\nChapter Goals\nIn this chapter you will:\n•Learn about the architectural design of a generative adversarial network (GAN).\n•Build and train a deep convolutional GAN (DCGAN) from scratch using Keras.\n•Use the DCGAN to generate new images.\n•Understand some of the common problems faced when training a DCGAN.\n•Learn how the Wasserstein GAN (WGAN) architecture addresses these\nproblems.\n•Understand additional enhancements that can be made to the WGAN, such as\nincorporating a gradient penalty (GP) term into the loss function.\n•Build a WGAN-GP from scratch using Keras.\n•Use the WGAN-GP to generate faces.\n•Learn how a conditional GAN (CGAN) gives you the ability to condition gener‐\nated output on a given label.\n•Build and train a CGAN in Keras and use it to manipulate a generated image.\nIn 2014, Ian Goodfellow et al. presented a paper entitled “Generative Adversarial\nNets” 1 at the Neural Information Processing Systems conference (NeurIPS) in Mon‐\ntreal. The introduction of generative adversarial networks (or GANs, as they are more\ncommonly known) is now regarded as a key turning point in the history of generative\nmodeling, as the core ideas presented in this paper have spawned some of the most\nsuccessful and impressive generative models ever created.\n95\nThis chapter will first lay out the theoretical underpinning of GANs, then we will see\nhow to build our own GAN using Keras.\nIntroduction\nLet’s start with a short story to illustrate some of the fundamental concepts used in\nthe GAN training process.\nBrickki Bricks and the Forgers\nIt’s your first day at your new job as head of quality control for Brickki, a company\nthat specializes in producing high-quality building blocks of all shapes and sizes\n(Figure 4-1 ).\nFigure 4-1. The production line of a company making bricks of many different  shapes\nand sizes (created with Midjourney )\nY ou are immediately alerted to a problem with some of the items coming off the pro‐\nduction line. A competitor has started to make counterfeit copies of Brickki bricks\nand has found a way to mix them into the bags received by your customers. Y ou\ndecide to become an expert at telling the difference between the counterfeit bricks\nand the real thing, so that you can intercept the forged bricks on the production line\nbefore they are given to customers. Over time, by listening to customer feedback, you\ngradually become more adept at spotting the fakes.\nThe forgers are not happy about this—they react to your improved detection abilities\nby making some changes to their forgery process so that now, the difference between\nthe real bricks and the fakes is even harder for you to spot.\n96 | Chapter 4: Generative Adversarial Networks\nNot one to give up, you retrain yourself to identify the more sophisticated fakes and\ntry to keep one step ahead of the forgers. This process continues, with the forgers iter‐\natively updating their brick creation technologies while you try to become increas‐\ningly more accomplished at intercepting their fakes.\nWith every week that passes, it becomes more and more difficult to tell the difference\nbetween the real Brickki bricks and those created by the forgers. It seems that this\nsimple game of cat and mouse is enough to drive significant improvement in both the\nquality of the forgery and the quality of the detection.\nThe story of Brickki bricks and the forgers describes the training process of a genera‐\ntive adversarial network.\nA GAN is a battle between two adversaries, the generator  and the  discriminator . The\ngenerator tries to convert random noise into observations that look as if they have\nbeen sampled from the original dataset, and the discriminator tries to predict whether\nan observation comes from the original dataset or is one of the generator’s forgeries.\nExamples of the inputs and outputs to the two networks are shown in Figure 4-2 .\nFigure 4-2. Inputs and outputs of the two networks in a GAN\nAt the start of the process, the generator outputs noisy images and the discriminator\npredicts randomly. The key to GANs lies in how we alternate the training of the two\nnetworks, so that as the generator becomes more adept at fooling the discriminator,\nthe discriminator must adapt in order to maintain its ability to correctly identify\nwhich observations are fake. This drives the generator to find new ways to fool the\ndiscriminator, and so the cycle continues.\nDeep Convolutional GAN (DCGAN)\nTo see this in action, let’s start building our first GAN in Keras, to generate pictures of\nbricks.\nWe will be closely following one of the first major papers on GANs, “Unsupervised\nRepresentation Learning with Deep Convolutional Generative Adversarial\nDeep Convolutional GAN (DCGAN) | 97\nNetworks. ” 2 In this 2015 paper, the authors show how to build a deep convolutional\nGAN to generate realistic images from a variety of datasets. They also introduce sev‐\neral changes that significantly improve the quality of the generated images.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/04_gan/01_dcgan/dcgan.ipynb  in the book\nrepository.\nThe Bricks Dataset\nFirst, you’ll need to download the training data. We’ll be using the Images of LEGO\nBricks dataset  that is available through Kaggle. This is a computer-rendered collec‐\ntion of 40,000 photographic images of 50 different toy bricks, taken from multiple\nangles. Some example images of Brickki products are shown in Figure 4-3 .\nFigure 4-3. Examples of images from the Bricks dataset\nY ou can download the dataset by running the Kaggle dataset downloader script in the\nbook repository, as shown in Example 4-1 . This will save the images and accompany‐\ning metadata locally to the /data  folder.\nExample 4-1. Downloading the Bricks dataset\nbash scripts/download_kaggle_data.sh  joosthazelzet  lego-brick-images\nWe use the Keras function image_dataset_from_directory  to create a TensorFlow\nDataset pointed at the directory where the images are stored, as shown in\nExample 4-2 . This allows us to read batches of images into memory only when\nrequired (e.g., during training), so that we can work with large datasets and not worry\nabout having to fit the entire dataset into memory. It also resizes the images to 64 ×\n64, interpolating between pixel values.\nExample 4-2. Creating a TensorFlow Dataset from image files in a directory\ntrain_data  = utils.image_dataset_from_directory (\n    ""/app/data/lego-brick-images/dataset/"" ,\n    labels=None,\n    color_mode =""grayscale"" ,\n    image_size =(64, 64),\n    batch_size =128,\n98 | Chapter 4: Generative Adversarial Networks\n    shuffle=True,\n    seed=42,\n    interpolation =""bilinear"" ,\n)\nThe original data is scaled in the range [0, 255] to denote the pixel intensity. When\ntraining GANs we rescale the data to the range [–1, 1] so that we can use the tanh\nactivation function on the final layer of the generator, which tends to provide stron‐\nger gradients than the sigmoid function ( Example 4-3 ).\nExample 4-3. Preprocessing the Bricks dataset\ndef preprocess (img):\n    img = (tf.cast(img, ""float32"" ) - 127.5) / 127.5\n    return img\ntrain = train_data .map(lambda x: preprocess (x))\nLet’s now take a look at how we build the discriminator.\nThe Discriminator\nThe goal of the discriminator is to predict if an image is real or fake. This is a super‐\nvised image classification problem, so we can use a similar architecture to those we\nworked with in Chapter 2 : stacked convolutional layers, with a single output node.\nThe full architecture of the discriminator we will be building is shown in Table 4-1 .\nTable 4-1. Model summary of the discriminator\nLayer (type) Output shape Param #\nInputLayer (None, 64, 64, 1) 0\nConv2D (None, 32, 32, 64) 1,024\nLeakyReLU (None, 32, 32, 64) 0\nDropout (None, 32, 32, 64) 0\nConv2D (None, 16, 16, 128) 131,072\nBatchNormalization (None, 16, 16, 128) 512\nLeakyReLU (None, 16, 16, 128) 0\nDropout (None, 16, 16, 128) 0\nConv2D (None, 8, 8, 256) 524,288\nBatchNormalization (None, 8, 8, 256) 1,024\nLeakyReLU (None, 8, 8, 256) 0\nDropout (None, 8, 8, 256) 0\nConv2D (None, 4, 4, 512) 2,097,152\nBatchNormalization (None, 4, 4, 512) 2,048\nDeep Convolutional GAN (DCGAN) | 99\nLayer (type) Output shape Param #\nLeakyReLU (None, 4, 4, 512) 0\nDropout (None, 4, 4, 512) 0\nConv2D (None, 1, 1, 1) 8,192\nFlatten (None, 1) 0\nTotal params 2,765,312\nTrainable params 2,763,520\nNon-trainable params 1,792\nThe Keras code to build the discriminator is provided in Example 4-4 .\nExample 4-4. The discriminator\ndiscriminator_input  = layers.Input(shape=(64, 64, 1)) \nx = layers.Conv2D(64, kernel_size =4, strides=2, padding=""same"", use_bias  = False)(\n    discriminator_input\n) \nx = layers.LeakyReLU (0.2)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Conv2D(\n    128, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum  = 0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Conv2D(\n    256, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum  = 0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Conv2D(\n    512, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum  = 0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Conv2D(\n    1,\n    kernel_size =4,\n    strides=1,\n    padding=""valid"",\n    use_bias  = False,\n    activation  = 'sigmoid'\n)(x)\ndiscriminator_output  = layers.Flatten()(x) \n100 | Chapter 4: Generative Adversarial Networks",9787
45-The Generator.pdf,45-The Generator,"discriminator  = models.Model(discriminator_input , discriminator_output ) \nDefine the Input  layer of the discriminator (the image).\nStack Conv2D  layers on top of each other, with BatchNormalization , LeakyReLU\nactivation, and Dropout  layers sandwiched in between.\nFlatten the last convolutional layer—by this point, the shape of the tensor is 1 × 1\n× 1, so there is no need for a final Dense  layer.\nThe Keras model that defines the discriminator—a model that takes an input\nimage and outputs a single number between 0 and 1.\nNotice how we use a stride of 2 in some of the Conv2D  layers to reduce the spatial\nshape of the tensor as it passes through the network (64 in the original image, then\n32, 16, 8, 4, and finally 1), while increasing the number of channels (1 in the grayscale\ninput image, then 64, 128, 256, and finally 512), before collapsing to a single\nprediction.\nWe use a sigmoid activation on the final Conv2D  layer to output a number between 0\nand 1.\nThe Generator\nNow  let’s build the generator. The input to the generator will be a vector drawn from\na multivariate standard normal distribution. The output is an image of the same size\nas an image in the original training data.\nThis description may remind you of the decoder in a variational autoencoder. In fact,\nthe generator of a GAN fulfills exactly the same purpose as the decoder of a V AE:\nconverting a vector in the latent space to an image. The concept of mapping from a\nlatent space back to the original domain is very common in generative modeling, as it\ngives us the ability to manipulate vectors in the latent space to change high-level fea‐\ntures of images in the original domain.\nThe architecture of the generator we will be building is shown in Table 4-2 .\nTable 4-2. Model summary of the generator\nLayer (type) Output shape Param #\nInputLayer (None, 100) 0\nReshape (None, 1, 1, 100) 0\nConv2DTranspose (None, 4, 4, 512) 819,200\nBatchNormalization (None, 4, 4, 512) 2,048\nDeep Convolutional GAN (DCGAN) | 101\nLayer (type) Output shape Param #\nReLU (None, 4, 4, 512) 0\nConv2DTranspose (None, 8, 8, 256) 2,097,152\nBatchNormalization (None, 8, 8, 256) 1,024\nReLU (None, 8, 8, 256) 0\nConv2DTranspose (None, 16, 16, 128) 524,288\nBatchNormalization (None, 16, 16, 128) 512\nReLU (None, 16, 16, 128) 0\nConv2DTranspose (None, 32, 32, 64) 131,072\nBatchNormalization (None, 32, 32, 64) 256\nReLU (None, 32, 32, 64) 0\nConv2DTranspose (None, 64, 64, 1) 1,024\nTotal params 3,576,576\nTrainable params 3,574,656\nNon-trainable params 1,920\nThe code for building the generator is given in Example 4-5 .\nExample 4-5. The generator\ngenerator_input  = layers.Input(shape=(100,)) \nx = layers.Reshape((1, 1, 100))(generator_input ) \nx = layers.Conv2DTranspose (\n    512, kernel_size =4, strides=1, padding=""valid"", use_bias  = False\n)(x) \nx = layers.BatchNormalization (momentum =0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Conv2DTranspose (\n    256, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum =0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Conv2DTranspose (\n    128, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum =0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\nx = layers.Conv2DTranspose (\n    64, kernel_size =4, strides=2, padding=""same"", use_bias  = False\n)(x)\nx = layers.BatchNormalization (momentum =0.9)(x)\nx = layers.LeakyReLU (0.2)(x)\ngenerator_output  = layers.Conv2DTranspose (\n    1,\n102 | Chapter 4: Generative Adversarial Networks\n    kernel_size =4,\n    strides=2,\n    padding=""same"",\n    use_bias  = False,\n    activation  = 'tanh'\n)(x) \ngenerator  = models.Model(generator_input , generator_output ) \nDefine the Input  layer of the generator—a vector of length 100.\nWe use a Reshape  layer to give a 1 × 1 × 100 tensor, so that we can start applying\nconvolutional transpose operations.\nWe pass this through four Conv2DTranspose  layers, with BatchNormalization\nand LeakyReLU  layers sandwiched in between.\nThe final Conv2DTranspose  layer uses a tanh activation function to transform the\noutput to the range [–1, 1], to match the original image domain.\nThe Keras model that defines the generator—a model that accepts a vector of\nlength 100 and outputs a tensor of shape [64, 64, 1] .\nNotice how we use a stride of 2 in some of the Conv2DTranspose  layers to increase the\nspatial shape of the tensor as it passes through the network (1 in the original vector,\nthen 4, 8, 16, 32, and finally 64), while decreasing the number of channels (512 then\n256, 128, 64, and finally 1 to match the grayscale output).\nUpsampling Versus Conv2DTranspose\nAn alternative to using Conv2DTranspose  layers is to instead use an UpSampling2D\nlayer followed by a normal Conv2D  layer with stride 1, as shown in Example 4-6 .\nExample 4-6. Upsampling example\nx = layers.UpSampling2D (size = 2)(x)\nx = layers.Conv2D(256, kernel_size =4, strides=1, padding=""same"")(x)\nThe UpSampling2D  layer simply repeats each row and column of its input in order to\ndouble the size. The Conv2D  layer with stride 1 then performs the convolution opera‐\ntion. It is a similar idea to convolutional transpose, but instead of filling the gaps\nbetween pixels with zeros, upsampling just repeats the existing pixel values.\nIt has been shown that the Conv2DTranspose  method can lead  to artifacts , or small\ncheckerboard patterns in the output image (see Figure 4-4 ) that spoil the quality of\nthe output. However, they are still used in many of the most impressive GANs in the\nDeep Convolutional GAN (DCGAN) | 103",5679
46-Training the DCGAN.pdf,46-Training the DCGAN,"literature and have proven to be a powerful tool in the deep learning practitioner’s\ntoolbox.\nFigure 4-4. Artifacts when using convolutional transpose layers (source: Odena et al.,\n2016 )3\nBoth of these methods— UpSampling2D  + Conv2D  and Conv2DTranspose —are accepta‐\nble ways to transform back to the original image domain. It really is a case of testing\nboth methods in your own problem setting and seeing which produces better results.\nTraining the DCGAN\nAs we have seen, the architectures of the generator and discriminator in a DCGAN\nare very simple and not so different from the V AE models that we looked at in Chap‐\nter 3 . The key to understanding GANs lies in understanding the training process for\nthe generator and discriminator.\nWe can train the discriminator by creating a training set where some of the images\nare real observations from the training set and some are fake outputs from the gener‐\nator. We then treat this as a supervised learning problem, where the labels are 1 for\nthe real images and 0 for the fake images, with binary cross-entropy as the loss\nfunction.\nHow should we train the generator? We need to find a way of scoring each generated\nimage so that it can optimize toward high-scoring images. Luckily, we have a discrim‐\ninator that does exactly that! We can generate a batch of images and pass these\nthrough the discriminator to get a score for each image. The loss function for the\ngenerator is then simply the binary cross-entropy between these probabilities and a\n104 | Chapter 4: Generative Adversarial Networks\nvector of ones, because we want to train the generator to produce images that the dis‐\ncriminator thinks are real.\nCrucially, we must alternate the training of these two networks, making sure that we\nonly update the weights of one network at a time. For example, during the generator\ntraining process, only the generator’s weights are updated. If we allowed the discrimi‐\nnator’s weights to change as well, the discriminator would just adjust so that it is more\nlikely to predict the generated images to be real, which is not the desired outcome.\nWe want generated images to be predicted close to 1 (real) because the generator is\nstrong, not because the discriminator is weak.\nA diagram of the training process for the discriminator and generator is shown in\nFigure 4-5 .\nFigure 4-5. Training the DCGAN—gray boxes indicate that the weights are frozen dur‐\ning training\nKeras provides us with the ability to create a custom train_step  function to imple‐\nment this logic. Example 4-7  shows the full DCGAN  model class.\nDeep Convolutional GAN (DCGAN) | 105\nExample 4-7. Compiling the DCGAN\nclass DCGAN(models.Model):\n    def __init__ (self, discriminator , generator , latent_dim ):\n        super(DCGAN, self).__init__ ()\n        self.discriminator  = discriminator\n        self.generator  = generator\n        self.latent_dim  = latent_dim\n    def compile(self, d_optimizer , g_optimizer ):\n        super(DCGAN, self).compile()\n        self.loss_fn = losses.BinaryCrossentropy () \n        self.d_optimizer  = d_optimizer\n        self.g_optimizer  = g_optimizer\n        self.d_loss_metric  = metrics.Mean(name=""d_loss"" )\n        self.g_loss_metric  = metrics.Mean(name=""g_loss"" )\n    @property\n    def metrics(self):\n        return [self.d_loss_metric , self.g_loss_metric ]\n    def train_step (self, real_images ):\n        batch_size  = tf.shape(real_images )[0]\n        random_latent_vectors  = tf.random.normal(\n            shape=(batch_size , self.latent_dim )\n        ) \n        with tf.GradientTape () as gen_tape , tf.GradientTape () as disc_tape :\n            generated_images  = self.generator (\n                random_latent_vectors , training  = True\n            ) \n            real_predictions  = self.discriminator (real_images , training  = True) \n            fake_predictions  = self.discriminator (\n                generated_images , training  = True\n            ) \n            real_labels  = tf.ones_like (real_predictions )\n            real_noisy_labels  = real_labels  + 0.1 * tf.random.uniform(\n                tf.shape(real_predictions )\n            )\n            fake_labels  = tf.zeros_like (fake_predictions )\n            fake_noisy_labels  = fake_labels  - 0.1 * tf.random.uniform(\n                tf.shape(fake_predictions )\n            )\n            d_real_loss  = self.loss_fn(real_noisy_labels , real_predictions )\n            d_fake_loss  = self.loss_fn(fake_noisy_labels , fake_predictions )\n            d_loss = (d_real_loss  + d_fake_loss ) / 2.0 \n            g_loss = self.loss_fn(real_labels , fake_predictions ) \n        gradients_of_discriminator  = disc_tape .gradient (\n106 | Chapter 4: Generative Adversarial Networks\n            d_loss, self.discriminator .trainable_variables\n        )\n        gradients_of_generator  = gen_tape .gradient (\n            g_loss, self.generator .trainable_variables\n        )\n        self.d_optimizer .apply_gradients (\n            zip(gradients_of_discriminator , discriminator .trainable_variables )\n        ) \n        self.g_optimizer .apply_gradients (\n            zip(gradients_of_generator , generator .trainable_variables )\n        )\n        self.d_loss_metric .update_state (d_loss)\n        self.g_loss_metric .update_state (g_loss)\n        return {m.name: m.result() for m in self.metrics}\ndcgan = DCGAN(\n    discriminator =discriminator , generator =generator , latent_dim =100\n)\ndcgan.compile(\n    d_optimizer =optimizers .Adam(\n        learning_rate =0.0002, beta_1 = 0.5, beta_2 = 0.999\n    ),\n    g_optimizer =optimizers .Adam(\n        learning_rate =0.0002, beta_1 = 0.5, beta_2 = 0.999\n    ),\n)\ndcgan.fit(train, epochs=300)\nThe loss function for the generator and discriminator is BinaryCrossentropy .\nTo train the network, first sample a batch of vectors from a multivariate standard\nnormal distribution.\nNext, pass these through the generator to produce a batch of generated images.\nNow ask the discriminator to predict the realness of the batch of real images…\n…and the batch of generated images.\nThe discriminator loss is the average binary cross-entropy across both the real\nimages (with label 1) and the fake images (with label 0).\nThe generator loss is the binary cross-entropy between the discriminator predic‐\ntions for the generated images and a label of 1.\nDeep Convolutional GAN (DCGAN) | 107\nUpdate the weights of the discriminator and generator separately.\nThe discriminator and generator are constantly fighting for dominance, which can\nmake the DCGAN training process unstable. Ideally, the training process will find an\nequilibrium that allows the generator to learn meaningful information from the dis‐\ncriminator and the quality of the images will start to improve. After enough epochs,\nthe discriminator tends to end up dominating, as shown in Figure 4-6 , but this may\nnot be a problem as the generator may have already learned to produce sufficiently\nhigh-quality images by this point.\nFigure 4-6. Loss and accuracy of the discriminator and generator during training\nAdding Noise to the Labels\nA useful trick when training GANs is to add a small amount of ran‐\ndom noise to the training labels. This helps to improve the stability\nof the training process and sharpen the generated images. This\nlabel smoothing  acts as way to tame the discriminator, so that it is\npresented with a more challenging task and doesn’t overpower the\ngenerator.\n108 | Chapter 4: Generative Adversarial Networks",7589
47-GAN Training Tips and Tricks.pdf,47-GAN Training Tips and Tricks,"Analysis of the DCGAN\nBy observing images produced by the generator at specific epochs during training\n(Figure 4-7 ), it is clear that the generator is becoming increasingly adept at producing\nimages that could have been drawn from the training set.\nFigure 4-7. Output from the generator at specific  epochs during training\nIt is somewhat miraculous that a neural network is able to convert random noise into\nsomething meaningful. It is worth remembering that we haven’t provided the model\nwith any additional features beyond the raw pixels, so it has to work out high-level\nconcepts such as how to draw shadows, cuboids, and circles entirely by itself.\nAnother requirement of a successful generative model is that it doesn’t only repro‐\nduce images from the training set. To test this, we can find the image from the\ntraining  set that is closest to a particular generated example. A good measure for dis‐\ntance is the L1 distance , defined as:\ndef compare_images(img1, img2):\n    return np.mean(np.abs(img1 - img2))\nFigure 4-8  shows the closest observations in the training set for a selection of gener‐\nated images. We can see that while there is some degree of similarity between the gen‐\nerated images and the training set, they are not identical. This shows that the\ngenerator has understood these high-level features and can generate examples that\nare distinct from those it has already seen.\nDeep Convolutional GAN (DCGAN) | 109\nFigure 4-8. Closest matches of generated images from the training set\nGAN Training: Tips and Tricks\nWhile GANs are a major breakthrough for generative modeling, they are also notori‐\nously difficult to train. We will explore some of the most common problems and\nchallenges encountered when training GANs in this section, alongside potential solu‐\ntions. In the next section, we will look at some more fundamental adjustments to the\nGAN framework that we can make to remedy many of these problems.\nDiscriminator overpowers the generator\nIf the discriminator becomes too strong, the signal from the loss function becomes\ntoo weak to drive any meaningful improvements in the generator. In the worst-case\nscenario, the discriminator perfectly learns to separate real images from fake images\nand the gradients vanish completely, leading to no training whatsoever, as can be seen\nin Figure 4-9 .\n110 | Chapter 4: Generative Adversarial Networks\nFigure 4-9. Example output when the discriminator overpowers the generator\nIf you find your discriminator loss function collapsing, you need to find ways to\nweaken the discriminator. Try the following suggestions:\n•Increase the rate  parameter of the Dropout  layers in the discriminator to\ndampen the amount of information that flows through the network.\n•Reduce the learning rate of the discriminator.\n•Reduce the number of convolutional filters in the discriminator.\n•Add noise to the labels when training the discriminator.\n•Flip the labels of some images at random when training the discriminator.\nGenerator overpowers the discriminator\nIf the discriminator is not powerful enough, the generator will find ways to easily\ntrick the discriminator with a small sample of nearly identical images. This is known\nas mode collapse .\nFor example, suppose we were to train the generator over several batches without\nupdating the discriminator in between. The generator would be inclined to find a sin‐\ngle observation (also known as a mode ) that always fools the discriminator and would\nstart to map every point in the latent input space to this image. Moreover, the gradi‐\nents of the loss function would collapse to near 0, so it wouldn’t be able to recover\nfrom this state.\nEven if we then tried to retrain the discriminator to stop it being fooled by this one\npoint, the generator would simply find another mode that fools the discriminator,\nsince it has already become numb to its input and therefore has no incentive to diver‐\nsify its output.\nDeep Convolutional GAN (DCGAN) | 111\nThe effect of mode collapse can be seen in Figure 4-10 .\nFigure 4-10. Example of mode collapse when the generator overpowers the discriminator\nIf you find that your generator is suffering from mode collapse, you can try strength‐\nening the discriminator using the opposite suggestions to those listed in the previous\nsection. Also, you can try reducing the learning rate of both networks and increasing\nthe batch size.\nUninformative loss\nSince  the deep learning model is compiled to minimize the loss function, it would be\nnatural to think that the smaller the loss function of the generator, the better the qual‐\nity of the images produced. However, since the generator is only graded against the\ncurrent discriminator and the discriminator is constantly improving, we cannot com‐\npare the loss function evaluated at different points in the training process. Indeed, in\nFigure 4-6 , the loss function of the generator actually increases over time, even\nthough the quality of the images is clearly improving. This lack of correlation\nbetween the generator loss and image quality sometimes makes GAN training diffi‐\ncult to monitor.\nHyperparameters\nAs we have seen, even with simple GANs, there are a large number of hyperparame‐\nters to tune. As well as the overall architecture of both the discriminator and the gen‐\nerator, there are the parameters that govern batch normalization, dropout, learning\nrate, activation layers, convolutional filters, kernel size, striding, batch size, and latent\nspace size to consider. GANs are highly sensitive to very slight changes in all of these\nparameters, and finding a set of parameters that works is often a case of educated trial\nand error, rather than following an established set of guidelines.\n112 | Chapter 4: Generative Adversarial Networks",5834
48-The Gradient Penalty Loss.pdf,48-The Gradient Penalty Loss,"This is why it is important to understand the inner workings of the GAN and know\nhow to interpret the loss function—so that you can identify sensible adjustments to\nthe hyperparameters that might improve the stability of the model.\nTackling GAN challenges\nIn recent years, several key advancements have drastically improved the overall sta‐\nbility of GAN models and diminished the likelihood of some of the problems listed\nearlier, such as mode collapse.\nIn the remainder of this chapter we shall examine the Wasserstein GAN with Gradi‐\nent Penalty (WGAN-GP), which makes several key adjustments to the GAN frame‐\nwork we have explored thus far to improve the stability and quality of the image\ngeneration process.\nWasserstein GAN with Gradient Penalty (WGAN-GP)\nIn this section we will build a WGAN-GP to generate faces from the CelebA dataset\nthat we utilized in Chapter 3 .\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/04_gan/02_wgan_gp/wgan_gp.ipynb  in the\nbook repository.\nThe code has been adapted from the excellent WGAN-GP tutorial\ncreated by Aakash Kumar Nain, available on the Keras website.\nThe Wasserstein GAN (WGAN), introduced in a 2017 paper by Arjovsky et al., 4 was\none of the first big steps toward stabilizing GAN training. With a few changes, the\nauthors were able to show how to train GANs that have the following two properties\n(quoted from the paper):\n•A meaningful loss metric that correlates with the generator’s convergence and\nsample quality\n•Improved stability of the optimization process\nSpecifically, the paper introduces the Wasserstein loss function  for both the discrimi‐\nnator and the generator. Using this loss function instead of binary cross-entropy\nresults in a more stable convergence of the GAN.\nIn this section we’ll define the Wasserstein loss function and then see what other\nchanges we need to make to the model architecture and training process to incorpo‐\nrate our new loss function.\nWasserstein GAN with Gradient Penalty (WGAN-GP) | 113\nY ou can find the full model class in the Jupyter notebook located at chapter05/wgan-\ngp/faces/train.ipynb  in the book repository.\nWasserstein Loss\nLet’s first remind ourselves of the definition of binary cross-entropy loss—the func‐\ntion that we are currently using to train the discriminator and generator of the GAN\n(Equation 4-1 ).\nEquation 4-1. Binary cross-entropy loss\n−1\nn∑\ni= 1n\nyilog pi+1 −yilog 1 −pi\nTo train the GAN discriminator D, we calculate the loss when comparing predictions\nfor real images pi=Dxi to the response yi= 1 and predictions for generated images\npi=DGzi to the response yi= 0. Therefore, for the GAN discriminator, minimiz‐\ning the loss function can be written as shown in Equation 4-2 .\nEquation 4-2. GAN discriminator loss minimization\nminD−x∼pXlogDx+z∼pZlog 1 −DGz\nTo train the GAN generator G, we calculate the loss when comparing predictions for\ngenerated images pi=DGzi to the response yi= 1. Therefore, for the GAN gener‐\nator, minimizing the loss function can be written as shown in Equation 4-3 .\nEquation 4-3. GAN generator loss minimization\nminG−z∼pZlogDGz\nNow let’s compare this to the Wasserstein loss function.\nFirst, the Wasserstein loss requires that we use yi= 1 and yi = –1 as labels, rather than\n1 and 0. We also remove the sigmoid activation from the final layer of the discrimina‐\ntor, so that predictions pi are no longer constrained to fall in the range [0, 1]\nbut instead can now be any number in the range ( −∞, ∞). For this reason, the\ndiscriminator  in a WGAN is usually referred to as a critic  that outputs a score  rather\nthan a probability.\nThe Wasserstein loss function is defined as follows:\n114 | Chapter 4: Generative Adversarial Networks\n−1\nn∑\ni= 1n\nyipi\nTo train the WGAN critic D, we calculate the loss when comparing predictions for\nreal images pi=Dxi to the response yi= 1 and predictions for generated images\npi=DGzi to the response yi = –1. Therefore, for the WGAN critic, minimizing\nthe loss function can be written as follows:\nminD−x∼pXDx−z∼pZDGz\nIn other words, the WGAN critic tries to maximize the difference between its predic‐\ntions for real images and generated images.\nTo train the WGAN generator, we calculate the loss when comparing predictions for\ngenerated images pi=DGzi to the response yi= 1. Therefore, for the WGAN gen‐\nerator, minimizing the loss function can be written as follows:\nminG−z∼pZDGz\nIn other words, the WGAN generator tries to produce images that are scored as\nhighly as possible by the critic (i.e., the critic is fooled into thinking they are real).\nThe Lipschitz Constraint\nIt may surprise you that we are now allowing the critic to output any number in the\nrange ( −∞, ∞), rather than applying a sigmoid function to restrict the output to the\nusual [0, 1] range. The Wasserstein loss can therefore be very large, which is unset‐\ntling—usually, large numbers in neural networks are to be avoided!\nIn fact, the authors of the WGAN paper show that for the Wasserstein loss function\nto work, we also need to place an additional constraint on the critic. Specifically, it is\nrequired that the critic is a 1-Lipschitz continuous function . Let’s pick this apart to\nunderstand what it means in more detail.\nThe critic is a function D that converts an image into a prediction. We say that this\nfunction is 1-Lipschitz if it satisfies the following inequality for any two input images,\nx1 and x2:\nDx1−Dx2\nx1−x2≤ 1\nWasserstein GAN with Gradient Penalty (WGAN-GP) | 115\nHere, x1−x2 is the average pixelwise absolute difference between two images and\nDx1−Dx2 is the absolute difference between the critic predictions. Essentially,\nwe require a limit on the rate at which the predictions of the critic can change\nbetween two images (i.e., the absolute value of the gradient must be at most 1 every‐\nwhere). We can see this applied to a Lipschitz continuous 1D function in Figure 4-11\n—at no point does the line enter the cone, wherever you place the cone on the line. In\nother words, there is a limit on the rate at which the line can rise or fall at any point.\nFigure 4-11. A Lipschitz continuous function (source: Wikipedia )\nFor those who want to delve deeper into the mathematical rationale\nbehind why the Wasserstein loss only works when this constraint is\nenforced, Jonathan Hui offers an excellent explanation .\nEnforcing the Lipschitz Constraint\nIn the original WGAN paper, the authors show how it is possible to enforce the Lip‐\nschitz constraint by clipping the weights of the critic to lie within a small range,\n[–0.01, 0.01], after each training batch.\nOne of the criticisms of this approach is that the capacity of the critic to learn is\ngreatly diminished, since we are clipping its weights. In fact, even in the original\nWGAN paper the authors write, “Weight clipping is a clearly terrible way to enforce a\nLipschitz constraint. ” A strong critic is pivotal to the success of a WGAN, since\nwithout accurate gradients, the generator cannot learn how to adapt its weights to\nproduce better samples.\nTherefore, other researchers have looked for alternative ways to enforce the Lipschitz\nconstraint and improve the capacity of the WGAN to learn complex features. One\nsuch method is the Wasserstein GAN with Gradient Penalty.\nIn the paper introducing this variant, 5 the authors show how the Lipschitz constraint\ncan be enforced directly by including a gradient penalty  term in the loss function for\n116 | Chapter 4: Generative Adversarial Networks\nthe critic that penalizes the model if the gradient norm deviates from 1. This results\nin a far more stable training process.\nIn the next section, we’ll see how to build this extra term into the loss function for our\ncritic.\nThe Gradient Penalty Loss\nFigure 4-12  is a diagram of the training process for the critic of a WGAN-GP . If we\ncompare this to the original discriminator training process from Figure 4-5 , we can\nsee that the key addition is the gradient penalty loss included as part of the overall\nloss function, alongside the Wasserstein loss from the real and fake images.\nFigure 4-12. The WGAN-GP critic training process\nThe gradient penalty loss measures the squared difference between the norm of the\ngradient of the predictions with respect to the input images and 1. The model will\nnaturally be inclined to find weights that ensure the gradient penalty term is mini‐\nmized, thereby encouraging the model to conform to the Lipschitz constraint.\nWasserstein GAN with Gradient Penalty (WGAN-GP) | 117\nIt is intractable to calculate this gradient everywhere during the training process, so\ninstead the WGAN-GP evaluates the gradient at only a handful of points. To ensure a\nbalanced mix, we use a set of interpolated images that lie at randomly chosen points\nalong lines connecting the batch of real images to the batch of fake images pairwise,\nas shown in Figure 4-13 .\nFigure 4-13. Interpolating between images\nIn Example 4-8 , we show how the gradient penalty is calculated in code.\nExample 4-8. The gradient penalty loss function\ndef gradient_penalty (self, batch_size , real_images , fake_images ):\n    alpha = tf.random.normal([batch_size , 1, 1, 1], 0.0, 1.0) \n    diff = fake_images  - real_images\n    interpolated  = real_images  + alpha * diff \n    with tf.GradientTape () as gp_tape:\n        gp_tape.watch(interpolated )\n        pred = self.critic(interpolated , training =True) \n    grads = gp_tape.gradient (pred, [interpolated ])[0] \n    norm = tf.sqrt(tf.reduce_sum (tf.square(grads), axis=[1, 2, 3])) \n    gp = tf.reduce_mean ((norm - 1.0) ** 2) \n    return gp\nEach image in the batch gets a random number, between 0 and 1, stored as the\nvector alpha .\nA set of interpolated images is calculated.\nThe critic is asked to score each of these interpolated images.\nThe gradient of the predictions is calculated with respect to the input images.\n118 | Chapter 4: Generative Adversarial Networks",10097
49-Training the WGAN-GP.pdf,49-Training the WGAN-GP,"The L2 norm of this vector is calculated.\nThe function returns the average squared distance between the L2 norm and 1.\nTraining the WGAN-GP\nA key benefit of using the Wasserstein loss function is that we no longer need to\nworry about balancing the training of the critic and the generator—in fact, when\nusing the Wasserstein loss, the critic must be trained to convergence before updating\nthe generator, to ensure that the gradients for the generator update are accurate. This\nis in contrast to a standard GAN, where it is important not to let the discriminator get\ntoo strong.\nTherefore, with Wasserstein GANs, we can simply train the critic several times\nbetween generator updates, to ensure it is close to convergence. A typical ratio used is\nthree to five critic updates per generator update.\nWe have now introduced both of the key concepts behind the WGAN-GP—the Was‐\nserstein loss and the gradient penalty term that is included in the critic loss function.\nThe training step of the WGAN model that incorporates all of these ideas is shown in\nExample 4-9 .\nExample 4-9. Training the WGAN-GP\ndef train_step (self, real_images ):\n    batch_size  = tf.shape(real_images )[0]\n    for i in range(3): \n        random_latent_vectors  = tf.random.normal(\n            shape=(batch_size , self.latent_dim )\n        )\n        with tf.GradientTape () as tape:\n            fake_images  = self.generator (\n                random_latent_vectors , training  = True\n            )\n            fake_predictions  = self.critic(fake_images , training  = True)\n            real_predictions  = self.critic(real_images , training  = True)\n            c_wass_loss  = tf.reduce_mean (fake_predictions ) - tf.reduce_mean (\n                real_predictions\n            ) \n            c_gp = self.gradient_penalty (\n                batch_size , real_images , fake_images\n            ) \n            c_loss = c_wass_loss  + c_gp * self.gp_weight  \n        c_gradient  = tape.gradient (c_loss, self.critic.trainable_variables )\nWasserstein GAN with Gradient Penalty (WGAN-GP) | 119\n        self.c_optimizer .apply_gradients (\n            zip(c_gradient , self.critic.trainable_variables )\n        ) \n    random_latent_vectors  = tf.random.normal(\n        shape=(batch_size , self.latent_dim )\n    )\n    with tf.GradientTape () as tape:\n        fake_images  = self.generator (random_latent_vectors , training =True)\n        fake_predictions  = self.critic(fake_images , training =True)\n        g_loss = -tf.reduce_mean (fake_predictions ) \n    gen_gradient  = tape.gradient (g_loss, self.generator .trainable_variables )\n    self.g_optimizer .apply_gradients (\n        zip(gen_gradient , self.generator .trainable_variables )\n    ) \n    self.c_loss_metric .update_state (c_loss)\n    self.c_wass_loss_metric .update_state (c_wass_loss )\n    self.c_gp_metric .update_state (c_gp)\n    self.g_loss_metric .update_state (g_loss)\n    return {m.name: m.result() for m in self.metrics}\nPerform three critic updates.\nCalculate the Wasserstein loss for the critic—the difference between the average\nprediction for the fake images and the real images.\nCalculate the gradient penalty term (see Example 4-8 ).\nThe critic loss function is a weighted sum of the Wasserstein loss and the gradi‐\nent penalty.\nUpdate the weights of the critic.\nCalculate the Wasserstein loss for the generator.\nUpdate the weights of the generator.\nBatch Normalization in a WGAN-GP\nOne  last consideration we should note before training a WGAN-\nGP is that batch normalization shouldn’t be used in the critic. This\nis because batch normalization creates correlation between images\nin the same batch, which makes the gradient penalty loss less effec‐\ntive. Experiments have shown that WGAN-GPs can still produce\nexcellent results even without batch normalization in the critic.\n120 | Chapter 4: Generative Adversarial Networks",3933
50-Training the CGAN.pdf,50-Training the CGAN,"We have now covered all of the key differences between a standard GAN and a\nWGAN-GP . To recap:\n•A WGAN-GP uses the Wasserstein loss.\n•The WGAN-GP is trained using labels of 1 for real and –1 for fake.\n•There is no sigmoid activation in the final layer of the critic.\n•Include a gradient penalty term in the loss function for the critic.\n•Train the critic multiple times for each update of the generator.\n•There are no batch normalization layers in the critic.\nAnalysis of the WGAN-GP\nLet’s take a look at some example outputs from the generator, after 25 epochs of train‐\ning ( Figure 4-14 ).\nFigure 4-14. WGAN-GP face examples\nThe model has learned the significant high-level attributes of a face, and there is no\nsign of mode collapse.\nWe can also see how the loss functions of the model evolve over time ( Figure 4-15 )—\nthe loss functions of both the critic and generator are highly stable and convergent.\nIf we compare the WGAN-GP output to the V AE output from the previous chapter,\nwe can see that the GAN images are generally sharper—especially the definition\nbetween the hair and the background. This is true in general; V AEs tend to produce\nsofter images that blur color boundaries, whereas GANs are known to produce\nsharper, more well-defined images.\nWasserstein GAN with Gradient Penalty (WGAN-GP) | 121\nFigure 4-15. WGAN-GP loss curves: the critic loss ( epoch_c_loss ) is broken down into\nthe Wasserstein loss ( epoch_c_wass ) and the gradient penalty loss ( epoch_c_gp )\nIt is also true that GANs are generally more difficult to train than V AEs and take\nlonger to reach a satisfactory quality. However, many state-of-the-art generative mod‐\nels today are GAN-based, as the rewards for training large-scale GANs on GPUs over\na longer period of time are significant.\nConditional GAN (CGAN)\nSo far in this chapter, we have built GANs that are able to generate realistic images\nfrom a given training set. However, we haven’t been able to control the type of image\nwe would like to generate—for example, a male or female face, or a large or small\nbrick. We can sample a random point from the latent space, but we do not have the\nability to easily understand what kind of image will be produced given the choice of\nlatent variable.\nIn the final part of this chapter we shall turn our attention to building a GAN where\nwe are able to control the output—a so called conditional GAN . This idea, first intro‐\nduced in “Conditional Generative Adversarial Nets” by Mirza and Osindero in 2014, 6\nis a relatively simple extension to the GAN architecture.\n122 | Chapter 4: Generative Adversarial Networks\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/04_gan/03_cgan/cgan.ipynb  in the book\nrepository.\nThe code has been adapted from the excellent CGAN tutorial  cre‐\nated by Sayak Paul, available on the Keras website.\nCGAN Architecture\nIn this example, we will condition our CGAN on the blond hair  attribute of the faces\ndataset. That is, we will be able to explicitly specify whether we want to generate an\nimage with blond hair or not. This label is provided as part of the CelebA dataset.\nThe high-level CGAN architecture is shown in Figure 4-16 .\nFigure 4-16. Inputs and outputs of the generator and critic in a CGAN\nThe key difference between a standard GAN and a CGAN is that in a CGAN we pass\nin extra information to the generator and critic relating to the label. In the generator,\nthis is simply appended to the latent space sample as a one-hot encoded vector. In the\ncritic, we add the label information as extra channels to the RGB image. We do this\nby repeating the one-hot encoded vector to fill the same shape as the input images.\nCGANs work because the critic now has access to extra information regarding the\ncontent of the image, so the generator must ensure that its output agrees with the\nprovided label, in order to keep fooling the critic. If the generator produced perfect\nConditional GAN (CGAN) | 123\nimages that disagreed with the image label the critic would be able to tell that they\nwere fake simply because the images and labels did not match.\nIn our example, our one-hot encoded label will have length 2,\nbecause there are two classes (Blonde and Not Blond). However,\nyou can have as many labels as you like—for example, you could\ntrain a CGAN on the Fashion-MNIST dataset to output one of the\n10 different fashion items, by incorporating a one-hot encoded\nlabel vector of length 10 into the input of the generator and 10\nadditional one-hot encoded label channels into the input of the\ncritic.\nThe only change we need to make to the architecture is to concatenate the label infor‐\nmation to the existing inputs of the generator and the critic, as shown in\nExample 4-10 .\nExample 4-10. Input layers in the CGAN\ncritic_input  = layers.Input(shape=(64, 64, 3)) \nlabel_input  = layers.Input(shape=(64, 64, 2))\nx = layers.Concatenate (axis = -1)([critic_input , label_input ])\n...\ngenerator_input  = layers.Input(shape=(32,)) \nlabel_input  = layers.Input(shape=(2,))\nx = layers.Concatenate (axis = -1)([generator_input , label_input ])\nx = layers.Reshape((1,1, 34))(x)\n...\nThe image channels and label channels are passed in separately to the critic and\nconcatenated.\nThe latent vector and the label classes are passed in separately to the generator\nand concatenated before being reshaped.\nTraining the CGAN\nWe must also make some changes to the train_step  of the CGAN to match the new\ninput formats of the generator and critic, as shown in Example 4-11 .\nExample 4-11. The train_step  of the CGAN\ndef train_step (self, data):\n    real_images , one_hot_labels  = data \n    image_one_hot_labels  = one_hot_labels [:, None, None, :] \n    image_one_hot_labels  = tf.repeat(\n124 | Chapter 4: Generative Adversarial Networks\n        image_one_hot_labels , repeats=64, axis = 1\n    )\n    image_one_hot_labels  = tf.repeat(\n        image_one_hot_labels , repeats=64, axis = 2\n    )\n    batch_size  = tf.shape(real_images )[0]\n    for i in range(self.critic_steps ):\n        random_latent_vectors  = tf.random.normal(\n            shape=(batch_size , self.latent_dim )\n        )\n        with tf.GradientTape () as tape:\n            fake_images  = self.generator (\n                [random_latent_vectors , one_hot_labels ], training  = True\n            ) \n            fake_predictions  = self.critic(\n                [fake_images , image_one_hot_labels ], training  = True\n            ) \n            real_predictions  = self.critic(\n                [real_images , image_one_hot_labels ], training  = True\n            )\n            c_wass_loss  = tf.reduce_mean (fake_predictions ) - tf.reduce_mean (\n                real_predictions\n            )\n            c_gp = self.gradient_penalty (\n                batch_size , real_images , fake_images , image_one_hot_labels\n            ) \n            c_loss = c_wass_loss  + c_gp * self.gp_weight\n        c_gradient  = tape.gradient (c_loss, self.critic.trainable_variables )\n        self.c_optimizer .apply_gradients (\n            zip(c_gradient , self.critic.trainable_variables )\n        )\n    random_latent_vectors  = tf.random.normal(\n        shape=(batch_size , self.latent_dim )\n    )\n    with tf.GradientTape () as tape:\n        fake_images  = self.generator (\n            [random_latent_vectors , one_hot_labels ], training =True\n        ) \n        fake_predictions  = self.critic(\n            [fake_images , image_one_hot_labels ], training =True\n        )\n        g_loss = -tf.reduce_mean (fake_predictions )\n    gen_gradient  = tape.gradient (g_loss, self.generator .trainable_variables )\nConditional GAN (CGAN) | 125",7806
51-Summary.pdf,51-Summary,"self.g_optimizer .apply_gradients (\n        zip(gen_gradient , self.generator .trainable_variables )\n    )\nThe images and labels are unpacked from the input data.\nThe one-hot encoded vectors are expanded to one-hot encoded images that have\nthe same spatial size as the input images (64 × 64).\nThe generator is now fed with a list of two inputs—the random latent vectors and\nthe one-hot encoded label vectors.\nThe critic is now fed with a list of two inputs—the fake/real images and the one-\nhot encoded label channels.\nThe gradient penalty function also requires the one-hot encoded label channels\nto be passed through as it uses the critic.\nThe changes made to the critic training step also apply to the generator training\nstep.\nAnalysis of the CGAN\nWe can control the CGAN output by passing a particular one-hot encoded label into\nthe input of the generator. For example, to generate a face with nonblond hair, we\npass in the vector [1, 0] . To generate a face with blond hair, we pass in the vector\n[0, 1] .\nThe output from the CGAN can be seen in Figure 4-17 . Here, we keep the random\nlatent vectors the same across the examples and change only the conditional label\nvector. It is clear that the CGAN has learned to use the label vector to control only the\nhair color attribute of the images. It is impressive that the rest of the image barely\nchanges—this is proof that GANs are able to organize points in the latent space in\nsuch a way that individual features can be decoupled from each other.\n126 | Chapter 4: Generative Adversarial Networks\nFigure 4-17. Output from the CGAN when the Blond and Not Blond vectors are\nappended to the latent sample\nIf labels are available for your dataset, it is generally a good idea to\ninclude them as input to your GAN even if you do not necessarily\nneed to condition the generated output on the label, as they tend to\nimprove the quality of images generated. Y ou can think of the\nlabels as just a highly informative extension to the pixel input.\nSummary\nIn this chapter we explored three different generative adversarial network (GAN)\nmodels: the deep convolutional GAN (DCGAN), the more sophisticated Wasserstein\nGAN with Gradient Penalty (WGAN-GP), and the conditional GAN (CGAN).\nAll GANs are characterized by a generator versus discriminator (or critic) architec‐\nture, with the discriminator trying to “spot the difference” between real and fake\nimages and the generator aiming to fool the discriminator. By balancing how these\ntwo adversaries are trained, the GAN generator can gradually learn how to produce\nsimilar observations to those in the training set.\nWe first saw how to train a DCGAN to generate images of toy bricks. It was able to\nlearn how to realistically represent 3D objects as images, including accurate represen‐\ntations of shadow, shape, and texture. We also explored the different ways in which\nGAN training can fail, including mode collapse and vanishing gradients.\nSummary | 127\nWe then explored how the Wasserstein loss function remedied many of these prob‐\nlems and made GAN training more predictable and reliable. The WGAN-GP places\nthe 1-Lipschitz requirement at the heart of the training process by including a term in\nthe loss function to pull the gradient norm toward 1.\nWe applied the WGAN-GP to the problem of face generation and saw how by simply\nchoosing points from a standard normal distribution, we can generate new faces. This\nsampling process is very similar to a V AE, though the faces produced by a GAN are\nquite different—often sharper, with greater distinction between different parts of the\nimage.\nFinally, we built a CGAN that allowed us to control the type of image that is gener‐\nated. This works by passing in the label as input to the critic and generator, thereby\ngiving the network the additional information it needs in order to condition the gen‐\nerated output on a given label.\nOverall, we have seen how the GAN framework is extremely flexible and able to be\nadapted to many interesting problem domains. In particular, GANs have driven sig‐\nnificant progress in the field of image generation with many interesting extensions to\nthe underlying framework, as we shall see in Chapter 10 .\nIn the next chapter, we will explore a different family of generative model that is ideal\nfor modeling sequential data—autoregressive models.\nReferences\n1. Ian J. Goodfellow et al., “Generative Adversarial Nets, ” June 10, 2014, https://\narxiv.org/abs/1406.2661\n2. Alec Radford et al., “Unsupervised Representation Learning with Deep Convolu‐\ntional Generative Adversarial Networks, ” January 7, 2016, https://arxiv.org/abs/\n1511.06434 .\n3. Augustus Odena et al., “Deconvolution and Checkerboard Artifacts, ” October 17,\n2016, https://distill.pub/2016/deconv-checkerboard .\n4. Martin Arjovsky et al., “Wasserstein GAN, ” January 26, 2017, https://arxiv.org/abs/\n1701.07875 .\n5. Ishaan Gulrajani et al., “Improved Training of Wasserstein GANs, ” March 31, 2017,\nhttps://arxiv.org/abs/1704.00028 .\n6. Mehdi Mirza and Simon Osindero, “Conditional Generative Adversarial Nets, ”\nNovember 6, 2014, https://arxiv.org/abs/1411.1784 .\n128 | Chapter 4: Generative Adversarial Networks",5246
52-Tokenization.pdf,52-Tokenization,"CHAPTER 5\nAutoregressive Models\nChapter Goals\nIn this chapter you will:\n•Learn why autoregressive models are well suited to generating sequential data\nsuch as text.\n•Learn how to process and tokenize text data.\n•Learn about the architectural design of recurrent neural networks (RNNs).\n•Build and train a long short-term memory network (LSTM) from scratch using\nKeras.\n•Use the LSTM to generate new text.\n•Learn about other variations of RNNs, including gated recurrent units (GRUs)\nand bidirectional cells.\n•Understand how image data can be treated as a sequence of pixels.\n•Learn about the architectural design of a PixelCNN.\n•Build a PixelCNN from scratch using Keras.\n•Use the PixelCNN to generate images.\nSo far, we have explored two different families of generative models that have both\ninvolved latent variables—variational autoencoders (V AEs) and generative adversarial\nnetworks (GANs). In both cases, a new variable is introduced with a distribution that\nis easy to sample from and the model learns how to decode  this variable back into the\noriginal domain.\n129\nWe will now turn our attention to  autoregressive models —a family of models that sim‐\nplify the generative modeling problem by treating it as a sequential process. Autore‐\ngressive models condition predictions on previous values in the sequence, rather than\non a latent random variable. Therefore, they attempt to explicitly model the data-\ngenerating distribution rather than an approximation of it (as in the case of V AEs).\nIn this chapter we shall explore two different autoregressive models: long short-term\nmemory networks and PixelCNN. We will apply the LSTM to text data and the Pix‐\nelCNN to image data. We will cover another highly successful autoregressive model,\nthe Transformer, in detail in Chapter 9 .\nIntroduction\nTo understand how an LSTM works, we will first pay a visit to a strange prison, where\nthe inmates have formed a literary society…\nThe Literary Society for Troublesome Miscreants\nEdward Sopp hated his job as a prison warden. He spent his days watching over the\nprisoners and had no time to follow his true passion of writing short stories. He was\nrunning low on inspiration and needed to find a way to generate new content.\nOne day, he came up with a brilliant idea that would allow him to produce new works\nof fiction in his style, while also keeping the inmates occupied—he would get the\ninmates to collectively write the stories for him! He branded the new society the Liter‐\nary Society for Troublesome Miscreants, or LSTM ( Figure 5-1 ).\nFigure 5-1. A large cell of prisoners reading books (created with Midjourney )\n130 | Chapter 5: Autoregressive Models\nThe prison is particularly strange because it only consists of one large cell, containing\n256 prisoners. Each prisoner has an opinion on how Edward’s current story should\ncontinue. Every day, Edward posts the latest word from his novel into the cell, and it\nis the job of the inmates to individually update their opinions on the current state of\nthe story, based on the new word and the opinions of the inmates from the previous\nday.\nEach prisoner uses a specific thought process to update their own opinion, which\ninvolves balancing information from the new incoming word and other prisoners’\nopinions with their own prior beliefs. First, they decide how much of yesterday’s\nopinion they wish to forget, taking into account the information from the new word\nand the opinions of other prisoners in the cell. They also use this information to form\nnew thoughts and decide to what extent they want to mix these into the old beliefs\nthat they have chosen to carry forward from the previous day. This then forms the\nprisoner’s new opinion for the day.\nHowever, the prisoners are secretive and don’t always tell their fellow inmates all of\ntheir opinions. They each also use the latest chosen word and the opinions of the\nother inmates to decide how much of their opinion they wish to disclose.\nWhen Edward wants the cell to generate the next word in the sequence, the prisoners\neach tell their disclosable opinions to the guard at the door, who combines this infor‐\nmation to ultimately decide on the next word to be appended to the end of the novel.\nThis new word is then fed back into the cell, and the process continues until the full\nstory is completed.\nTo train the inmates and the guard, Edward feeds short sequences of words that he\nhas written previously into the cell and monitors whether the inmates’ chosen next\nword is correct. He updates them on their accuracy, and gradually they begin to learn\nhow to write stories in his own unique style.\nAfter many iterations of this process, Edward finds that the system has become quite\naccomplished at generating realistic-looking text. Satisfied with the results, he pub‐\nlishes a collection of the generated tales in his new book, entitled E. Sopp’s Fables .\nThe story of Mr. Sopp and his crowdsourced fables is an analogy for one of the most\nnotorious autoregressive techniques for sequential data such as text: the long short-\nterm memory network.\nLong Short-Term Memory Network (LSTM)\nAn LSTM is a particular type of recurrent neural network (RNN). RNNs contain a\nrecurrent layer (or cell) that is able to handle sequential data by making its own out‐\nput at a particular timestep form part of the input to the next timestep.\nLong Short-Term Memory Network (LSTM) | 131\nWhen RNNs were first introduced, recurrent layers were very simple and consisted\nsolely of a tanh operator that ensured that the information passed between timesteps\nwas scaled between –1 and 1. However, this approach was shown to suffer from the\nvanishing gradient problem and didn’t scale well to long sequences of data.\nLSTM  cells were first introduced in 1997 in a paper by Sepp Hochreiter and Jürgen\nSchmidhuber. 1 In the paper, the authors describe how LSTMs do not suffer from the\nsame vanishing gradient problem experienced by vanilla RNNs and can be trained on\nsequences that are hundreds of timesteps long. Since then, the LSTM architecture has\nbeen adapted and improved, and variations such as gated recurrent units (discussed\nlater in this chapter) are now widely utilized and available as layers in Keras.\nLSTMs have been applied to a wide range of problems involving sequential data,\nincluding time series forecasting, sentiment analysis, and audio classification. In this\nchapter we will be using LSTMs to tackle the challenge of text generation.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/05_autoregressive/01_lstm/lstm.ipynb  in the\nbook repository.\nThe Recipes Dataset\nWe’ll  be using the Epicurious Recipes dataset  that is available through Kaggle. This is\na set of over 20,000 recipes, with accompanying metadata such as nutritional infor‐\nmation and ingredient lists.\nY ou can download the dataset by running the Kaggle dataset downloader script in the\nbook repository, as shown in Example 5-1 . This will save the recipes and accompany‐\ning metadata locally to the /data  folder.\nExample 5-1. Downloading the Epicurious Recipe dataset\nbash scripts/download_kaggle_data.sh  hugodarwood  epirecipes\nExample 5-2  shows how the data can be loaded and filtered so that only recipes with a\ntitle and a description remain. An example of a recipe text string is given in\nExample 5-3 .\nExample 5-2. Loading the data\nwith open('/app/data/epirecipes/full_format_recipes.json' ) as json_data :\n    recipe_data  = json.load(json_data )\nfiltered_data  = [\n132 | Chapter 5: Autoregressive Models\n    'Recipe for '  + x['title']+ ' | ' + ' '.join(x['directions' ])\n    for x in recipe_data\n    if 'title' in x\n    and x['title'] is not None\n    and 'directions'  in x\n    and x['directions' ] is not None\n]\nExample 5-3. A text string from the Recipes dataset\nRecipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough\nparsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems\nand simmer with broth and garlic in a small saucepan, covered, 5 minutes.\nMeanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute.\nStrain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve.\nSeason with salt and pepper. Set bowl in an ice bath and cool to room temperature,\nstirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top\nand chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar,\n1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery,\ncornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and\n1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato\nsalad, over ham.\nBefore taking a look at how to build an LSTM network in Keras, we must first take a\nquick detour to understand the structure of text data and how it is different from the\nimage data that we have seen so far in this book.\nWorking with Text Data\nThere are several key differences between text and image data that mean that many of\nthe methods that work well for image data are not so readily applicable to text data.\nIn particular:\n•Text data is composed of discrete chunks (either characters or words), whereas\npixels in an image are points in a continuous color spectrum. We can easily make\na green pixel more blue, but it is not obvious how we should go about making the\nword cat more like the word dog, for example. This means we can easily apply\nbackpropagation to image data, as we can calculate the gradient of our loss func‐\ntion with respect to individual pixels to establish the direction in which pixel col‐\nors should be changed to minimize the loss. With discrete text data, we can’t\nobviously apply backpropagation in the same way, so we need to find a way\naround this problem.\n•Text data has a time dimension but no spatial dimension, whereas image data has\ntwo spatial dimensions but no time dimension. The order of words is highly\nimportant in text data and words wouldn’t make sense in reverse, whereas images\ncan usually be flipped without affecting the content. Furthermore, there are often\nLong Short-Term Memory Network (LSTM) | 133\nlong-term sequential dependencies between words that need to be captured by\nthe model: for example, the answer to a question or carrying forward the context\nof a pronoun. With image data, all pixels can be processed simultaneously.\n•Text data is highly sensitive to small changes in the individual units (words or\ncharacters). Image data is generally less sensitive to changes in individual pixel\nunits—a picture of a house would still be recognizable as a house even if some\npixels were altered—but with text data, changing even a few words can drastically\nalter the meaning of the passage, or make it nonsensical. This makes it very diffi‐\ncult to train a model to generate coherent text, as every word is vital to the overall\nmeaning of the passage.\n•Text data has a rules-based grammatical structure, whereas image data doesn’t\nfollow set rules about how the pixel values should be assigned. For example, it\nwouldn’t make grammatical sense in any context to write “The cat sat on the hav‐\ning. ” There are also semantic rules that are extremely difficult to model; it\nwouldn’t make sense to say “I am in the beach, ” even though grammatically, there\nis nothing wrong with this statement.\nAdvances in Text-Based Generative Deep Learning\nUntil recently, most of the most sophisticated generative deep\nlearning models have focused on image data, because many of the\nchallenges presented in the preceding list were beyond the reach of\neven the most advanced techniques. However, in the last five years\nastonishing progress has been made in the field of text-based gen‐\nerative deep learning, thanks to the introduction of the Trans‐\nformer model architecture, which we will explore in Chapter 9 .\nWith these points in mind, let’s now take a look at the steps we need to take in order\nto get the text data into the right shape to train an LSTM network.\nTokenization\nThe first step is to clean up and tokenize the text. Tokenization  is the process of split‐\nting the text up into individual units, such as words or characters.\nHow you tokenize your text will depend on what you are trying to achieve with your\ntext generation model. There are pros and cons to using both word and character\ntokens, and your choice will affect how you need to clean the text prior to modeling\nand the output from your model.\n134 | Chapter 5: Autoregressive Models\nIf you use word tokens:\n•All text can be converted to lowercase, to ensure capitalized words at the start of\nsentences are tokenized the same way as the same words appearing in the middle\nof a sentence. In some cases, however, this may not be desirable; for example,\nsome proper nouns, such as names or places, may benefit from remaining capi‐\ntalized so that they are tokenized independently.\n•The text vocabulary  (the set of distinct words in the training set) may be very\nlarge, with some words appearing very sparsely or perhaps only once. It may be\nwise to replace sparse words with a token for unknown word , rather than includ‐\ning them as separate tokens, to reduce the number of weights the neural network\nneeds to learn.\n•Words  can be stemmed , meaning that they are reduced to their simplest form, so\nthat different tenses of a verb remained tokenized together. For example, browse ,\nbrowsing , browses , and browsed  would all be stemmed to brows .\n•Y ou will need to either tokenize the punctuation, or remove it altogether.\n•Using word tokenization means that the model will never be able to predict\nwords outside of the training vocabulary.\nIf you use character tokens:\n•The model may generate sequences of characters that form new words outside of\nthe training vocabulary—this may be desirable in some contexts, but not in\nothers.\n•Capital letters can either be converted to their lowercase counterparts, or remain\nas separate tokens.\n•The vocabulary is usually much smaller when using character tokenization. This\nis beneficial for model training speed as there are fewer weights to learn in the\nfinal output layer.\nFor this example, we’ll use lowercase word tokenization, without word stemming.\nWe’ll also tokenize punctuation marks, as we would like the model to predict when it\nshould end sentences or use commas, for example.\nThe code in Example 5-4  cleans and tokenizes the text.\nExample 5-4. Tokenization\ndef pad_punctuation (s):\n    s = re.sub(f""([{string.punctuation }])"", r' \1 ', s)\n    s = re.sub(' +', ' ', s)\n    return s\nLong Short-Term Memory Network (LSTM) | 135\ntext_data  = [pad_punctuation (x) for x in filtered_data ] \ntext_ds = tf.data.Dataset.from_tensor_slices (text_data ).batch(32).shuffle(1000) \nvectorize_layer  = layers.TextVectorization ( \n    standardize  = 'lower',\n    max_tokens  = 10000,\n    output_mode  = ""int"",\n    output_sequence_length  = 200 + 1,\n)\nvectorize_layer .adapt(text_ds) \nvocab = vectorize_layer .get_vocabulary () \nPad the punctuation marks, to treat them as separate words.\nConvert to a TensorFlow Dataset.\nCreate a Keras TextVectorization  layer to convert text to lowercase, give the\nmost prevalent 10,000 words a corresponding integer token, and trim or pad the\nsequence to 201 tokens long.\nApply the TextVectorization  layer to the training data.\nThe vocab  variable stores a list of the word tokens.\nAn example of a recipe after tokenization is shown in Example 5-5 . The sequence\nlength that we use to train the model is a parameter of the training process. In this\nexample we choose to use a sequence length of 200, so we pad or clip the recipe to\none more than this length, to allow us to create the target variable (more on this in\nthe next section). To achieve this desired length, the end of the vector is padded with\nzeros.\nStop Tokens\nThe 0 token is known as a the stop token , signifying that the text\nstring has come to an end.\nExample 5-5. The recipe from Example 5-3  tokenized\n[  26   16  557    1    8  298  335  189    4 1054  494   27  332  228\n  235  262    5  594   11  133   22  311    2  332   45  262    4  671\n    4   70    8  171    4   81    6    9   65   80    3  121    3   59\n   12    2  299    3   88  650   20   39    6    9   29   21    4   67\n  529   11  164    2  320  171  102    9  374   13  643  306   25   21\n    8  650    4   42    5  931    2   63    8   24    4   33    2  114\n   21    6  178  181 1245    4   60    5  140  112    3   48    2  117\n136 | Chapter 5: Autoregressive Models",16801
53-The LSTM Architecture.pdf,53-The LSTM Architecture,"557    8  285  235    4  200  292  980    2  107  650   28   72    4\n  108   10  114    3   57  204   11  172    2   73  110  482    3  298\n    3  190    3   11   23   32  142   24    3    4   11   23   32  142\n   33    6    9   30   21    2   42    6  353    3 3224    3    4  150\n    2  437  494    8 1281    3   37    3   11   23   15  142   33    3\n    4   11   23   32  142   24    6    9  291  188    5    9  412  572\n    2  230  494    3   46  335  189    3   20  557    2    0    0    0\n    0    0    0    0    0]\nIn Example 5-6 , we can see a subset of the list of tokens mapped to their respective\nindices. The layer reserves the 0 token for padding (i.e., it is the stop token) and the 1\ntoken for unknown words that fall outside the top 10,000 words (e.g., persillade). The\nother words are assigned tokens in order of frequency. The number of words to\ninclude in the vocabulary is also a parameter of the training process. The more words\nincluded, the fewer unknown  tokens you will see in the text; however, your model will\nneed to be larger to accommodate the larger vocabulary size.\nExample 5-6. The vocabulary of the TextVectorization  layer\n0:\n1: [UNK]\n2: .\n3: ,\n4: and\n5: to\n6: in\n7: the\n8: with\n9: a\nCreating the Training Set\nOur LSTM will be trained to predict the next word in a sequence, given a sequence of\nwords preceding this point. For example, we could feed the model the tokens for gril‐\nled chicken with boiled  and would expect the model to output a suitable next word\n(e.g., potatoes , rather than bananas ).\nWe can therefore simply shift the entire sequence by one token in order to create our\ntarget variable.\nThe dataset generation step can be achieved with the code in Example 5-7 .\nExample 5-7. Creating the training dataset\ndef prepare_inputs (text):\n    text = tf.expand_dims (text, -1)\n    tokenized_sentences  = vectorize_layer (text)\n    x = tokenized_sentences [:, :-1]\nLong Short-Term Memory Network (LSTM) | 137",1994
54-The Embedding Layer.pdf,54-The Embedding Layer,"y = tokenized_sentences [:, 1:]\n    return x, y\ntrain_ds  = text_ds.map(prepare_inputs ) \nCreate the training set consisting of recipe tokens (the input) and the same vector\nshifted by one token (the target).\nThe LSTM Architecture\nThe architecture of the overall LSTM model is shown in Table 5-1 . The input to the\nmodel is a sequence of integer tokens and the output is the probability of each word\nin the 10,000-word vocabulary appearing next in the sequence. To understand how\nthis works in detail, we need to introduce two new layer types, Embedding  and LSTM .\nTable 5-1. Model summary of the LSTM\nLayer (type) Output shape Param #\nInputLayer (None, None) 0\nEmbedding (None, None, 100) 1,000,000\nLSTM (None, None, 128) 117,248\nDense (None, None, 10000) 1,290,000\nTotal params 2,407,248\nTrainable params 2,407,248\nNon-trainable params 0\nThe Input Layer of the LSTM\nNotice that the Input  layer does not need us to specify the\nsequence length in advance. Both the batch size and the sequence\nlength are flexible (hence the (None, None)  shape). This is because\nall downstream layers are agnostic to the length of the sequence\nbeing passed through.\nThe Embedding Layer\nAn embedding layer  is essentially a lookup table that converts each integer token into\na vector of length embedding_size , as shown in Figure 5-2 . The lookup vectors are\nlearned by the model as weights . Therefore, the number of weights learned by this\nlayer is equal to the size of the vocabulary multiplied by the dimension of the embed‐\nding vector (i.e., 10,000 × 100 = 1,000,000).\n138 | Chapter 5: Autoregressive Models\nFigure 5-2. An embedding layer is a lookup table for each integer token\nWe embed each integer token into a continuous vector because it enables the model\nto learn a representation for each word that is able to be updated through backpropa‐\ngation. We could also just one-hot encode each input token, but using an embedding\nlayer is preferred because it makes the embedding itself trainable, thus giving the\nmodel more flexibility in deciding how to embed each token to improve its\nperformance.\nTherefore, the Input  layer passes a tensor of integer sequences of shape\n[batch_size, seq_length]  to the Embedding  layer, which outputs a tensor of shape\n[batch_size, seq_length, embedding_size] . This is then passed on to the LSTM\nlayer ( Figure 5-3 ).\nFigure 5-3. A single sequence as it flows  through an embedding layer\nLong Short-Term Memory Network (LSTM) | 139",2506
55-The LSTM Layer.pdf,55-The LSTM Layer,"The LSTM Layer\nTo understand the LSTM layer, we must first look at how a general recurrent layer\nworks.\nA recurrent layer has the special property of being able to process sequential input\ndata x1,⋯,xn. It consists of a cell that updates its hidden state , ht, as each element of\nthe sequence xt is passed through it, one timestep at a time.\nThe hidden state is a vector with length equal to the number of units  in the cell—it\ncan be thought of as the cell’s current understanding of the sequence. At timestep t,\nthe cell uses the previous value of the hidden state, ht− 1, together with the data from\nthe current timestep xt to produce an updated hidden state vector, ht. This recurrent\nprocess continues until the end of the sequence. Once the sequence is finished, the\nlayer outputs the final hidden state of the cell, hn, which is then passed on to the next\nlayer of the network. This process is shown in Figure 5-4 .\nFigure 5-4. A simple diagram of a recurrent layer\nTo explain this in more detail, let’s unroll the process so that we can see exactly how a\nsingle sequence is fed through the layer ( Figure 5-5 ).\nCell Weights\nIt’s important to remember that all of the cells in this diagram share\nthe same weights (as they are really the same cell). There is no dif‐\nference between this diagram and Figure 5-4 ; it’s just a different\nway of drawing the mechanics of a recurrent layer.\n140 | Chapter 5: Autoregressive Models\nFigure 5-5. How a single sequence flows  through a recurrent layer\nHere, we represent the recurrent process by drawing a copy of the cell at each time‐\nstep and show how the hidden state is constantly being updated as it flows through\nthe cells. We can clearly see how the previous hidden state is blended with the current\nsequential data point (i.e., the current embedded word vector) to produce the next\nhidden state. The output from the layer is the final hidden state of the cell, after each\nword in the input sequence has been processed.\nLong Short-Term Memory Network (LSTM) | 141",2047
56-The LSTM Cell.pdf,56-The LSTM Cell,"The fact that the output from the cell is called a hidden  state is an\nunfortunate naming convention—it’s not really hidden, and you\nshouldn’t think of it as such. Indeed, the last hidden state is the\noverall output from the layer, and we will be making use of the fact\nthat we can access the hidden state at each individual timestep later\nin this chapter.\nThe LSTM Cell\nNow that we have seen how a generic recurrent layer works, let’s take a look inside an\nindividual LSTM cell.\nThe job of the LSTM cell is to output a new hidden state, ht, given its previous hidden\nstate, ht− 1, and the current word embedding, xt. To recap, the length of ht is equal to\nthe number of units in the LSTM. This is a parameter that is set when you define the\nlayer and has nothing to do with the length of the sequence.\nMake sure you do not confuse the term cell with unit. There is one\ncell in an LSTM layer that is defined by the number of units it con‐\ntains, in the same way that the prisoner cell from our earlier story\ncontained many prisoners. We often draw a recurrent layer as a\nchain of cells unrolled, as it helps to visualize how the hidden state\nis updated at each timestep.\nAn LSTM cell maintains a cell state, Ct, which can be thought of as the cell’s internal\nbeliefs about the current status of the sequence. This is distinct from the hidden state,\nht, which is ultimately output by the cell after the final timestep. The cell state is the\nsame length as the hidden state (the number of units in the cell).\nLet’s look more closely at a single cell and how the hidden state is updated\n(Figure 5-6 ).\nThe hidden state is updated in six steps:\n1.The hidden state of the previous timestep, ht− 1, and the current word embed‐\nding, xt, are concatenated and passed through the forget  gate. This gate is simply a\ndense layer with weights matrix Wf, bias bf, and a sigmoid activation function.\nThe resulting vector, ft, has length equal to the number of units in the cell and\ncontains values between 0 and 1 that determine how much of the previous cell\nstate, Ct− 1, should be retained.\n142 | Chapter 5: Autoregressive Models\nFigure 5-6. An LSTM cell\n2.The concatenated vector is also passed through an input  gate that, like the forget\ngate, is a dense layer with weights matrix Wi, bias bi, and a sigmoid activation\nfunction. The output from this gate, it, has length equal to the number of units in\nthe cell and contains values between 0 and 1 that determine how much new\ninformation will be added to the previous cell state, Ct− 1.\n3.The concatenated vector is passed through a dense layer with weights matrix WC,\nbias bC, and a tanh activation function to generate a vector Ct that contains the\nnew information that the cell wants to consider keeping. It also has length equal\nto the number of units in the cell and contains values between –1 and 1.\n4.ft and Ct− 1 are multiplied element-wise and added to the element-wise multipli‐\ncation of it and Ct. This represents forgetting parts of the previous cell state and\nthen adding new relevant information to produce the updated cell state, Ct.\n5.The concatenated vector is passed through an output  gate: a dense layer with\nweights matrix Wo, bias bo, and a sigmoid activation. The resulting vector, ot, has\nlength equal to the number of units in the cell and stores values between 0 and 1\nthat determine how much of the updated cell state, Ct, to output from the cell.\nLong Short-Term Memory Network (LSTM) | 143",3511
57-Training the LSTM.pdf,57-Training the LSTM,"6.ot is multiplied element-wise with the updated cell state, Ct, after a tanh activa‐\ntion has been applied to produce the new hidden state, ht.\nThe Keras LSTM Layer\nAll of this complexity is wrapped up within the LSTM  layer type in\nKeras, so you don’t have to worry about implementing it yourself!\nTraining the LSTM\nThe code to build, compile, and train the LSTM is given in Example 5-8 .\nExample 5-8. Building, compiling, and training the LSTM\ninputs = layers.Input(shape=(None,), dtype=""int32"") \nx = layers.Embedding (10000, 100)(inputs) \nx = layers.LSTM(128, return_sequences =True)(x) \noutputs = layers.Dense(10000, activation  = 'softmax' )(x) \nlstm = models.Model(inputs, outputs) \nloss_fn = losses.SparseCategoricalCrossentropy ()\nlstm.compile(""adam"", loss_fn) \nlstm.fit(train_ds , epochs=25) \nThe Input  layer does not need us to specify the sequence length in advance (it\ncan be flexible), so we use None  as a placeholder.\nThe Embedding  layer requires two parameters, the size of the vocabulary (10,000\ntokens) and the dimensionality of the embedding vector (100).\nThe LSTM layers require us to specify the dimensionality of the hidden vector\n(128). We also choose to return the full sequence of hidden states, rather than just\nthe hidden state at the final timestep.\nThe Dense  layer transforms the hidden states at each timestep into a vector of\nprobabilities for the next token.\nThe overall Model  predicts the next token, given an input sequence of tokens. It\ndoes this for each token in the sequence.\nThe model is compiled with SparseCategoricalCrossentropy  loss—this is the\nsame as categorical cross-entropy, but is used when the labels are integers rather\nthan one-hot encoded vectors.\n144 | Chapter 5: Autoregressive Models\nThe model is fit to the training dataset.\nIn Figure 5-7  you can see the first few epochs of the LSTM training process—notice\nhow the example output becomes more comprehensible as the loss metric falls.\nFigure 5-8  shows the cross-entropy loss metric falling throughout the training\nprocess.\nFigure 5-7. The first few epochs of the LSTM training process\nFigure 5-8. The cross-entropy loss metric of the LSTM training process by epoch\nLong Short-Term Memory Network (LSTM) | 145",2261
58-Analysis of the LSTM.pdf,58-Analysis of the LSTM,"Analysis of the LSTM\nNow  that we have compiled and trained the LSTM, we can start to use it to generate\nlong strings of text by applying the following process:\n1.Feed the network with an existing sequence of words and ask it to predict the fol‐\nlowing word.\n2.Append this word to the existing sequence and repeat.\nThe network will output a set of probabilities for each word that we can sample from.\nTherefore, we can make the text generation stochastic, rather than deterministic.\nMoreover, we can introduce a  temperature  parameter to the sampling process to indi‐\ncate how deterministic we would like the process to be.\nThe Temperature Parameter\nA temperature close to 0 makes the sampling more deterministic\n(i.e., the word with the highest probability is very likely to be\nchosen), whereas a temperature of 1 means each word is chosen\nwith the probability output by the model.\nThis is achieved with the code in Example 5-9 , which creates a callback function that\ncan be used to generate text at the end of each training epoch.\nExample 5-9. The TextGenerator  callback function\nclass TextGenerator (callbacks .Callback ):\n    def __init__ (self, index_to_word , top_k=10):\n        self.index_to_word  = index_to_word\n        self.word_to_index  = {\n            word: index for index, word in enumerate (index_to_word )\n        } \n    def sample_from (self, probs, temperature ): \n        probs = probs ** (1 / temperature )\n        probs = probs / np.sum(probs)\n        return np.random.choice(len(probs), p=probs), probs\n    def generate (self, start_prompt , max_tokens , temperature ):\n        start_tokens  = [\n            self.word_to_index .get(x, 1) for x in start_prompt .split()\n        ] \n        sample_token  = None\n        info = []\n        while len(start_tokens ) < max_tokens  and sample_token  != 0: \n            x = np.array([start_tokens ])\n            y = self.model.predict(x) \n146 | Chapter 5: Autoregressive Models\n            sample_token , probs = self.sample_from (y[0][-1], temperature ) \n            info.append({'prompt' : start_prompt  , 'word_probs' : probs})\n            start_tokens .append(sample_token ) \n            start_prompt  = start_prompt  + ' ' + self.index_to_word [sample_token ]\n        print(f""\ngenerated text: \n{start_prompt }\n"")\n        return info\n    def on_epoch_end (self, epoch, logs=None):\n        self.generate (""recipe for"" , max_tokens  = 100, temperature  = 1.0)\nCreate an inverse vocabulary mapping (from word to token).\nThis function updates the probabilities with a temperature  scaling factor.\nThe start prompt is a string of words that you would like to give the model to\nstart the generation process (for example, recipe for ). The words are first con‐\nverted to a list of tokens.\nThe sequence is generated until it is max_tokens  long or a stop token (0) is\nproduced.\nThe model outputs the probabilities of each word being next in the sequence.\nThe probabilities are passed through the sampler to output the next word, para‐\nmeterized by temperature .\nWe append the new word to the prompt text, ready for the next iteration of the\ngenerative process.\nLet’s take a look at this in action, at two different temperature values ( Figure 5-9 ).\nFigure 5-9. Generated outputs at temperature = 1.0  and temperature = 0.2\nLong Short-Term Memory Network (LSTM) | 147\nThere are a few things to note about these two passages. First, both are stylistically\nsimilar to a recipe from the original training set. They both open with a recipe title\nand contain generally grammatically correct constructions. The difference is that the\ngenerated text with a temperature of 1.0 is more adventurous and therefore less accu‐\nrate than the example with a temperature of 0.2. Generating multiple samples with a\ntemperature of 1.0 will therefore lead to more variety, as the model is sampling from a\nprobability distribution with greater variance.\nTo demonstrate this, Figure 5-10  shows the top five tokens with the highest probabili‐\nties for a range of prompts, for both temperature values.\nFigure 5-10. Distribution of word probabilities following various sequences, for tempera‐\nture values of 1.0 and 0.2\nThe model is able to generate a suitable distribution for the next most likely word\nacross a range of contexts. For example, even though the model was never told about\nparts of speech such as nouns, verbs, or numbers, it is generally able to separate\nwords into these classes and use them in a way that is grammatically correct.\n148 | Chapter 5: Autoregressive Models",4606
59-Recurrent Neural Network RNN Extensions.pdf,59-Recurrent Neural Network RNN Extensions,,0
60-Stacked Recurrent Networks.pdf,60-Stacked Recurrent Networks,"Moreover, the model is able to select an appropriate verb to begin the recipe instruc‐\ntions, depending on the preceding title. For roasted vegetables, it selects preheat ,\nprepare , heat , put, or combine  as the most likely possibilities, whereas for ice cream\nit selects in, combine , stir , whisk , and mix. This shows that the model has some con‐\ntextual understanding of the differences between recipes depending on their\ningredients.\nNotice also how the probabilities for the temperature = 0.2  examples are much\nmore heavily weighted toward the first choice token. This is the reason why there is\ngenerally less variety in generations when the temperature is lower.\nWhile our basic LSTM model is doing a great job at generating realistic text, it is clear\nthat it still struggles to grasp some of the semantic meaning of the words that it is\ngenerating. It introduces ingredients that are not likely to work well together (for\nexample, sour Japanese potatoes, pecan crumbs, and sorbet)! In some cases, this may\nbe desirable—say, if we want our LSTM to generate interesting and unique patterns of\nwords—but in other cases, we will need our model to have a deeper understanding of\nthe ways in which words can be grouped together and a longer memory of ideas\nintroduced earlier in the text.\nIn the next section, we’ll explore some of the ways that we can improve our basic\nLSTM network. In Chapter 9 , we’ll take a look at a new kind of autoregressive model,\nthe Transformer, which takes language modeling to the next level.\nRecurrent Neural Network (RNN) Extensions\nThe model in the preceding section is a simple example of how an LSTM can be\ntrained to learn how to generate text in a given style. In this section we will explore\nseveral extensions to this idea.\nStacked Recurrent Networks\nThe network we just looked at contained a single LSTM layer, but we can also train\nnetworks with stacked LSTM layers, so that deeper features can be learned from the\ntext.\nTo achieve this, we simply introduce another LSTM layer after the first. The second\nLSTM layer can then use the hidden states from the first layer as its input data. This is\nshown in Figure 5-11 , and the overall model architecture is shown in Table 5-2 .\nRecurrent Neural Network (RNN) Extensions | 149\nFigure 5-11. Diagram of a multilayer RNN: gt denotes hidden states of the first layer and ht denotes hidden states of the second layer\n150 | Chapter 5: Autoregressive Models",2479
61-Gated Recurrent Units.pdf,61-Gated Recurrent Units,"Table 5-2. Model summary of the stacked LSTM\nLayer (type) Output shape Param #\nInputLayer (None, None) 0\nEmbedding (None, None, 100) 1,000,000\nLSTM (None, None, 128) 117,248\nLSTM (None, None, 128) 131,584\nDense (None, None, 10000) 1,290,000\nTotal params 2,538,832\nTrainable params 2,538,832\nNon-trainable params 0\nThe code to build the stacked LSTM is given in Example 5-10 .\nExample 5-10. Building a stacked LSTM\ntext_in = layers.Input(shape = (None,))\nembedding  = layers.Embedding (total_words , embedding_size )(text_in)\nx = layers.LSTM(n_units, return_sequences  = True)(x)\nx = layers.LSTM(n_units, return_sequences  = True)(x)\nprobabilites  = layers.Dense(total_words , activation  = 'softmax' )(x)\nmodel = models.Model(text_in, probabilites )\nGated Recurrent Units\nAnother  type of commonly used RNN layer is the gated recurrent unit  (GRU). 2 The\nkey differences from the LSTM unit are as follows:\n1.The forget  and input  gates are replaced by reset  and update  gates.\n2.There is no cell state  or output  gate, only a hidden state  that is output from the\ncell.\nThe hidden state is updated in four steps, as illustrated in Figure 5-12 .\nRecurrent Neural Network (RNN) Extensions | 151\nFigure 5-12. A single GRU cell\nThe process is as follows:\n1.The hidden state of the previous timestep, ht− 1, and the current word embed‐\nding, xt, are concatenated and used to create the reset  gate. This gate is a dense\nlayer, with weights matrix Wr and a sigmoid activation function. The resulting\nvector, rt, has length equal to the number of units in the cell and stores values\nbetween 0 and 1 that determine how much of the previous hidden state, ht− 1,\nshould be carried forward into the calculation for the new beliefs of the cell.\n2.The reset gate is applied to the hidden state, ht− 1, and concatenated with the cur‐\nrent word embedding, xt. This vector is then fed to a dense layer with weights\nmatrix W and a tanh activation function to generate a vector, ht, that stores the\nnew beliefs of the cell. It has length equal to the number of units in the cell and\nstores values between –1 and 1.\n3.The concatenation of the hidden state of the previous timestep, ht− 1, and the\ncurrent word embedding, xt, are also used to create the update  gate. This gate is a\ndense layer with weights matrix Wz and a sigmoid activation. The resulting vec‐\ntor, zt, has length equal to the number of units in the cell and stores values\n152 | Chapter 5: Autoregressive Models",2506
62-Bidirectional Cells.pdf,62-Bidirectional Cells,,0
63-Masked Convolutional Layers.pdf,63-Masked Convolutional Layers,"between 0 and 1, which are used to determine how much of the new beliefs, ht, to\nblend into the current hidden state, ht− 1.\n4.The new beliefs of the cell, ht, and the current hidden state, ht− 1, are blended in a\nproportion determined by the update gate, zt, to produce the updated hidden\nstate, ht, that is output from the cell.\nBidirectional Cells\nFor prediction problems where the entire text is available to the model at inference\ntime, there is no reason to process the sequence only in the forward direction—it\ncould just as well be processed backward. A Bidirectional  layer takes advantage of\nthis by storing two sets of hidden states: one that is produced as a result of the\nsequence being processed in the usual forward direction and another that is pro‐\nduced when the sequence is processed backward. This way, the layer can learn from\ninformation both preceding and succeeding the given timestep.\nIn Keras, this is implemented as a wrapper around a recurrent layer, as shown in\nExample 5-11 .\nExample 5-11. Building a bidirectional GRU layer\nlayer = layers.Bidirectional (layers.GRU(100))\nHidden State\nThe hidden states in the resulting layer are vectors of length equal\nto double the number of units in the wrapped cell (a concatenation\nof the forward and backward hidden states). Thus, in this example\nthe hidden states of the layer are vectors of length 200.\nSo far, we have only applied autoregressive models (LSTMs) to text data. In the next\nsection, we will see how autoregressive models can also be used to generate images.\nPixelCNN\nIn 2016, van den Oord  et al. 3 introduced a model that generates images pixel by pixel\nby predicting the likelihood of the next pixel based on the pixels before it. The model\nis called PixelCNN , and it can be trained to generate images autoregressively.\nThere are two new concepts that we need to introduce to understand the PixelCNN—\nmasked convolutional layers  and residual blocks .\nPixelCNN | 153\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/05_autoregressive/02_pixelcnn/pixelcnn.ipynb\nin the book repository.\nThe code has been adapted from the excellent PixelCNN tutorial\ncreated by ADMoreau, available on the Keras website.\nMasked Convolutional Layers\nAs we saw in Chapter 2 , a convolutional layer can be used to extract features from an\nimage by applying a series of filters. The output of the layer at a particular pixel is a\nweighted sum of the filter weights multiplied by the preceding layer values over a\nsmall square centered on the pixel. This method can detect edges and textures and, at\ndeeper layers, shapes and higher-level features.\nWhilst convolutional layers are extremely useful for feature detection, they cannot\ndirectly be used in an autoregressive sense, because there is no ordering placed on the\npixels. They rely on the fact that all pixels are treated equally—no pixel is treated as\nthe start  or end of the image. This is in contrast to the text data that we have already\nseen in this chapter, where there is a clear ordering to the tokens so recurrent models\nsuch as LSTMs can be readily applied.\nFor us to be able to apply convolutional layers to image generation in an autoregres‐\nsive sense, we must first place an ordering on the pixels and ensure that the filters are\nonly able to see pixels that precede the pixel in question. We can then generate images\none pixel at a time, by applying convolutional filters to the current image to predict\nthe value of the next pixel from all preceding pixels.\nWe first need to choose an ordering for the pixels—a sensible suggestion is to order\nthe pixels from top left to bottom right, moving first along the rows and then down\nthe columns.\nWe then mask the convolutional filters so that the output of the layer at each pixel is\nonly influenced by pixel values that precede the pixel in question. This is achieved by\nmultiplying a mask of ones and zeros with the filter weights matrix, so that the values\nof any pixels that are after the target pixel are zeroed.\nThere are actually two different kinds of masks in a PixelCNN, as shown in\nFigure 5-13 :\n•Type A, where the value of the central pixel is masked\n•Type B, where the value of the central pixel is not masked\n154 | Chapter 5: Autoregressive Models\nFigure 5-13. Left:  a convolutional filter  mask; right: a mask applied to a set of pixels to\npredict the distribution of the central pixel value (source: van den Oord et al., 2016 )\nThe initial masked convolutional layer (i.e., the one that is applied directly to the\ninput image) cannot use the central pixel, because this is precisely the pixel we want\nthe network to guess! However, subsequent layers can use the central pixel because\nthis will have been calculated only as a result of information from preceding pixels in\nthe original input image.\nWe can see in Example 5-12  how a MaskedConvLayer  can be built using Keras.\nExample 5-12. A MaskedConvLayer  in Keras\nclass MaskedConvLayer (layers.Layer):\n    def __init__ (self, mask_type , **kwargs):\n        super(MaskedConvLayer , self).__init__ ()\n        self.mask_type  = mask_type\n        self.conv = layers.Conv2D(**kwargs) \n    def build(self, input_shape ):\n        self.conv.build(input_shape )\n        kernel_shape  = self.conv.kernel.get_shape ()\n        self.mask = np.zeros(shape=kernel_shape ) \n        self.mask[: kernel_shape [0] // 2, ...] = 1.0 \n        self.mask[kernel_shape [0] // 2, : kernel_shape [1] // 2, ...] = 1.0 \n        if self.mask_type  == ""B"":\n            self.mask[kernel_shape [0] // 2, kernel_shape [1] // 2, ...] = 1.0 \n    def call(self, inputs):\n        self.conv.kernel.assign(self.conv.kernel * self.mask) \n        return self.conv(inputs)\nThe MaskedConvLayer  is based on the normal Conv2D  layer.\nPixelCNN | 155",5916
64-Residual Blocks.pdf,64-Residual Blocks,"The mask is initialized with all zeros.\nThe pixels in the preceding rows are unmasked with ones.\nThe pixels in the preceding columns that are in the same row are unmasked with\nones.\nIf the mask type is B, the central pixel is unmasked with a one.\nThe mask is multiplied with the filter weights.\nNote that this simplified example assumes a grayscale image (i.e., with one channel).\nIf we have color images, we’ll have three color channels that we can also place an\nordering on so that, for example, the red channel precedes the blue channel, which\nprecedes the green channel.\nResidual Blocks\nNow that we have seen how to mask the convolutional layer, we can start to build our\nPixelCNN. The core building block that we will use is the residual block.\nA residual block  is a set of layers where the output is added to the input before being\npassed on to the rest of the network. In other words, the input has a fast-track  route\nto the output, without having to go through the intermediate layers—this is called a\nskip connection . The rationale behind including a skip connection is that if the opti‐\nmal transformation is just to keep the input the same, this can be achieved by simply\nzeroing the weights of the intermediate layers. Without the skip connection, the net‐\nwork would have to find an identity mapping through the intermediate layers, which\nis much harder.\nA diagram of the residual block in our PixelCNN is shown in Figure 5-14 .\n156 | Chapter 5: Autoregressive Models\nFigure 5-14. A PixelCNN residual block (the numbers of filters  are next to the arrows\nand the filter  sizes are next to the layers)\nWe can build a ResidualBlock  using the code shown in Example 5-13 .\nExample 5-13. A ResidualBlock\nclass ResidualBlock (layers.Layer):\n    def __init__ (self, filters, **kwargs):\n        super(ResidualBlock , self).__init__ (**kwargs)\n        self.conv1 = layers.Conv2D(\n            filters=filters // 2, kernel_size =1, activation =""relu""\n        ) \n        self.pixel_conv  = MaskedConv2D (\n            mask_type =""B"",\n            filters=filters // 2,\n            kernel_size =3,\n            activation =""relu"",\n            padding=""same"",\n        ) \n        self.conv2 = layers.Conv2D(\n            filters=filters, kernel_size =1, activation =""relu""\n        ) \n    def call(self, inputs):\n        x = self.conv1(inputs)\n        x = self.pixel_conv (x)\n        x = self.conv2(x)\n        return layers.add([inputs, x]) \nThe initial Conv2D  layer halves the number of channels.\nPixelCNN | 157",2559
65-Analysis of the PixelCNN.pdf,65-Analysis of the PixelCNN,"The Type B MaskedConv2D  layer with kernel size of 3 only uses information from\nfive pixels—three pixels in the row above the focus pixel, one to the left, and the\nfocus pixel itself.\nThe final Conv2D  layer doubles the number of channels to again match the input\nshape.\nThe output from the convolutional layers is added to the input—this is the skip\nconnection.\nTraining the PixelCNN\nIn Example 5-14  we put together the whole PixelCNN network, approximately fol‐\nlowing the structure laid out in the original paper. In the original paper, the output\nlayer is a 256-filter Conv2D  layer, with softmax activation. In other words, the network\ntries to re-create its input by predicting the correct pixel values, a bit like an autoen‐\ncoder. The difference is that the PixelCNN is constrained so that no information from\nearlier pixels can flow through to influence the prediction for each pixel, due to the\nway that network is designed, using MaskedConv2D  layers.\nA challenge with this approach is that the network has no way to understand that a\npixel value of, say, 200 is very close to a pixel value of 201. It must learn every pixel\noutput value independently, which means training can be very slow, even for the sim‐\nplest datasets. Therefore, in our implementation, we instead simplify the input so that\neach pixel can take only one of four values. This way, we can use a 4-filter Conv2D\noutput layer instead of 256.\nExample 5-14. The PixelCNN architecture\ninputs = layers.Input(shape=(16, 16, 1)) \nx = MaskedConv2D (mask_type =""A""\n                   , filters=128\n                   , kernel_size =7\n                   , activation =""relu""\n                   , padding=""same"")(inputs)\nfor _ in range(5):\n    x = ResidualBlock (filters=128)(x) \nfor _ in range(2):\n    x = MaskedConv2D (\n        mask_type =""B"",\n        filters=128,\n        kernel_size =1,\n        strides=1,\n        activation =""relu"",\n158 | Chapter 5: Autoregressive Models\n        padding=""valid"",\n    )(x) \nout = layers.Conv2D(\n    filters=4, kernel_size =1, strides=1, activation =""softmax"" , padding=""valid""\n)(x) \npixel_cnn  = models.Model(inputs, out) \nadam = optimizers .Adam(learning_rate =0.0005)\npixel_cnn .compile(optimizer =adam, loss=""sparse_categorical_crossentropy"" )\npixel_cnn .fit(\n    input_data\n    , output_data\n    , batch_size =128\n    , epochs=150\n) \nThe model Input  is a grayscale image of size 16 × 16 × 1, with inputs scaled\nbetween 0 and 1.\nThe first Type A MaskedConv2D  layer with a kernel size of 7 uses information\nfrom 24 pixels—21 pixels in the three rows above the focus pixel and 3 to the left\n(the focus pixel itself is not used).\nFive ResidualBlock  layer groups are stacked sequentially.\nTwo Type B MaskedConv2D  layers with a kernel size of 1 act as Dense  layers across\nthe number of channels for each pixel.\nThe final Conv2D  layer reduces the number of channels to four—the number of\npixel levels for this example.\nThe Model  is built to accept an image and output an image of the same\ndimensions.\nFit the model— input_data  is scaled in the range [0, 1] (floats); output_data  is\nscaled in the range [0, 3] (integers).\nAnalysis of the PixelCNN\nWe can train our PixelCNN on images from the Fashion-MNIST dataset that we\nencountered in Chapter 3 . To generate new images, we need to ask the model to pre‐\ndict the next pixel given all preceding pixels, one pixel at a time. This is a very slow\nprocess compared to a model such as a variational autoencoder! For a 32 × 32\nPixelCNN | 159\ngrayscale  image, we need to make 1,024 predictions sequentially using the model,\ncompared to the single prediction that we need to make for a V AE. This is one of the\nmajor downsides to autoregressive models such as a PixelCNN—they are slow to\nsample from, because of the sequential nature of the sampling process.\nFor this reason, we use an image size of 16 × 16, rather than 32 × 32, to speed up the\ngeneration of new images. The generation callback class is shown in Example 5-15 .\nExample 5-15. Generating new images using the PixelCNN\nclass ImageGenerator (callbacks .Callback ):\n    def __init__ (self, num_img):\n        self.num_img = num_img\n    def sample_from (self, probs, temperature ):\n        probs = probs ** (1 / temperature )\n        probs = probs / np.sum(probs)\n        return np.random.choice(len(probs), p=probs)\n    def generate (self, temperature ):\n        generated_images  = np.zeros(\n            shape=(self.num_img,) + (pixel_cnn .input_shape )[1:]\n        ) \n        batch, rows, cols, channels  = generated_images .shape\n        for row in range(rows):\n            for col in range(cols):\n                for channel in range(channels ):\n                    probs = self.model.predict(generated_images )[\n                        :, row, col, :\n                    ] \n                    generated_images [:, row, col, channel] = [\n                        self.sample_from (x, temperature ) for x in probs\n                    ] \n                    generated_images [:, row, col, channel] /= 4 \n        return generated_images\n    def on_epoch_end (self, epoch, logs=None):\n        generated_images  = self.generate (temperature  = 1.0)\n        display(\n            generated_images ,\n            save_to = ""./output/generated_img_ %03d.png"" % (epoch)\n        s)\nimg_generator_callback  = ImageGenerator (num_img=10)\nStart with a batch of empty images (all zeros).\n160 | Chapter 5: Autoregressive Models\nLoop over the rows, columns, and channels of the current image, predicting the\ndistribution of the next pixel value.\nSample a pixel level from the predicted distribution (for our example, a level in\nthe range [0, 3]).\nConvert the pixel level to the range [0, 1] and overwrite the pixel value in the cur‐\nrent image, ready for the next iteration of the loop.\nIn Figure 5-15 , we can see several images from the original training set, alongside\nimages that have been generated by the PixelCNN.\nFigure 5-15. Example images from the training set and generated images created by the\nPixelCNN model\nThe model does a great job of re-creating the overall shape and style of the original\nimages! It is quite amazing that we can treat images as a series of tokens (pixel values)\nand apply autoregressive models such as a PixelCNN to produce realistic samples.\nAs mentioned previously, one of the downsides to autoregressive models is that they\nare slow to sample from, which is why a simple example of their application is presen‐\nted in this book. However, as we shall see in Chapter 10 , more complex forms of\nautoregressive model can be applied to images to produce state-of-the-art outputs. In\nPixelCNN | 161",6765
66-Mixture Distributions.pdf,66-Mixture Distributions,"such cases, the slow generation speed is a necessary price to pay in return for\nexceptional-quality outputs.\nSince the original paper was published, several improvements have been made to the\narchitecture and training process of the PixelCNN. The following section introduces\none of those changes—using mixture distributions—and demonstrates how to train a\nPixelCNN model with this improvement using a built-in TensorFlow function.\nMixture Distributions\nFor our previous example, we reduced the output of the PixelCNN to just 4 pixel lev‐\nels to ensure the network didn’t have to learn a distribution over 256 independent\npixel values, which would slow the training process. However, this is far from ideal—\nfor color images, we wouldn’t want our canvas to be restricted to only a handful of\npossible colors.\nTo get around this problem, we can make the output of the network a mixture distri‐\nbution , instead of a softmax over 256 discrete pixel values, following the ideas presen‐\nted by Salimans et al. 4 A mixture distribution is quite simply a mixture of two or\nmore other probability distributions. For example, we could have a mixture distribu‐\ntion of five logistic distributions, each with different parameters. The mixture distri‐\nbution also requires a discrete categorical distribution that denotes the probability of\nchoosing each of the distributions included in the mix. An example is shown in\nFigure 5-16 .\nFigure 5-16. A mixture distribution of three normal distributions with different  parame‐\nters—the categorical distribution over the three normal distributions is [0.5, 0.3,\n0.2]\nTo sample from a mixture distribution, we first sample from the categorical distribu‐\ntion to choose a particular subdistribution and then sample from this in the usual\nway. This way, we can create complex distributions with relatively few parameters.\n162 | Chapter 5: Autoregressive Models\nFor example, the mixture distribution in Figure 5-16  only requires eight parameters\n—two for the categorical distribution and a mean and variance for each of the three\nnormal distributions. This is compared to the 255 parameters that would define a cat‐\negorical distribution over the entire pixel range.\nConveniently, the TensorFlow Probability library provides a function that allows us to\ncreate a PixelCNN with mixture distribution output in a single line. Example 5-16\nillustrates how to build a PixelCNN using this function.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook in\nnotebooks/05_autoregressive/03_pixelcnn_md/pixelcnn_md.ipynb  in\nthe book repository.\nExample 5-16. Building a PixelCNN using the TensorFlow function\nimport tensorflow_probability  as tfp\ndist = tfp.distributions .PixelCNN (\n    image_shape =(32, 32, 1),\n    num_resnet =1,\n    num_hierarchies =2,\n    num_filters =32,\n    num_logistic_mix =5,\n    dropout_p =.3,\n) \nimage_input  = layers.Input(shape=(32, 32, 1)) \nlog_prob  = dist.log_prob (image_input )\nmodel = models.Model(inputs=image_input , outputs=log_prob ) \nmodel.add_loss (-tf.reduce_mean (log_prob )) \nDefine the PixelCNN as a distribution—i.e., the output layer is a mixture distri‐\nbution made up of five logistic distributions.\nThe input is a grayscale image of size 32 × 32 × 1.\nThe Model  takes a grayscale image as input and outputs the log-likelihood of the\nimage under the mixture distribution calculated by the PixelCNN.\nThe loss function is the mean negative log-likelihood over the batch of input\nimages.\nPixelCNN | 163",3575
67-Summary.pdf,67-Summary,"The model is trained in the same way as before, but this time accepting integer pixel\nvalues as input, in the range [0, 255]. Outputs can be generated from the distribution\nusing the sample  function, as shown in Example 5-17 .\nExample 5-17. Sampling from the PixelCNN mixture distribution\ndist.sample(10).numpy()\nExample generated images are shown in Figure 5-17 . The difference from our previ‐\nous examples is that now the full range of pixel values is being utilized.\nFigure 5-17. Outputs from the PixelCNN using a mixture distribution output\nSummary\nIn this chapter we have seen how autoregressive models such as recurrent neural net‐\nworks can be applied to generate text sequences that mimic a particular style of writ‐\ning, and also how a PixelCNN can generate images in a sequential fashion, one pixel\nat a time.\nWe explored two different types of recurrent layers—long short-term memory\n(LSTM) and gated recurrent unit (GRU)—and saw how these cells can be stacked or\nmade bidirectional to form more complex network architectures. We built an LSTM\nto generate realistic recipes using Keras and saw how to manipulate the temperature\nof the sampling process to increase or decrease the randomness of the output.\nWe also saw how images can be generated in an autoregressive manner, using a Pix‐\nelCNN. We built a PixelCNN from scratch using Keras, coding the masked convolu‐\ntional layers and residual blocks to allow information to flow through the network so\nthat only preceding pixels could be used to generate the current pixel. Finally, we dis‐\ncussed how the TensorFlow Probability library provides a standalone PixelCNN  func‐\ntion that implements a mixture distribution as the output layer, allowing us to further\nimprove the learning process.\n164 | Chapter 5: Autoregressive Models\nIn the next chapter we will explore another generative modeling family that explicitly\nmodels the data-generating distribution—normalizing flow models.\nReferences\n1. Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory, ” Neural\nComputation  9 (1997): 1735–1780, https://www.bioinf.jku.at/publications/older/\n2604.pdf .\n2. Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-\nDecoder for Statistical Machine Translation, ” June 3, 2014, https://arxiv.org/abs/\n1406.1078 .\n3. Aaron van den Oord et al., “Pixel Recurrent Neural Networks, ” August 19, 2016,\nhttps://arxiv.org/abs/1601.06759 .\n4. Tim Salimans et al., “PixelCNN++: Improving the PixelCNN with Discretized\nLogistic Mixture Likelihood and Other Modifications, ” January 19, 2017, http://\narxiv.org/abs/1701.05517 .\nSummary | 165",2657
68-Change of Variables.pdf,68-Change of Variables,"CHAPTER 6\nNormalizing Flow Models\nChapter Goals\nIn this chapter you will:\n•Learn how normalizing flow models utilize the change of variables equation.\n•See how the Jacobian determinant plays a vital role in our ability to compute an\nexplicit density function.\n•Understand how we can restrict the form of the Jacobian using coupling layers.\n•See how the neural network is designed to be invertible.\n•Build a RealNVP model—a particular example of a normalizing flow to generate\npoints in 2D.\n•Use the RealNVP model to generate new points that appear to have been drawn\nfrom the data distribution.\n•Learn about two key extensions of the RealNVP model, GLOW and FFJORD.\nSo far, we have discussed three families of generative models: variational autoencod‐\ners, generative adversarial networks, and autoregressive models. Each presents a dif‐\nferent way to address the challenge of modeling the distribution px, either by\nintroducing a latent variable that can be easily sampled (and transformed using the\ndecoder in V AEs or generator in GANs), or by tractably modeling the distribution as\na function of the values of preceding elements (autoregressive models).\nIn this chapter, we will cover a new family of generative models—normalizing flow\nmodels. As we shall see, normalizing flows share similarities with both autoregressive\nmodels and variational autoencoders. Like autoregressive models, normalizing flows\nare able to explicitly and tractably model the data-generating distribution px. Like\n167\nV AEs, normalizing flows attempt to map the data into a simpler distribution, such as\na Gaussian distribution. The key difference is that normalizing flows place a con‐\nstraint on the form of the mapping function, so that it is invertible and can therefore\nbe used to generate new data points.\nWe will dig into this definition in detail in the first section of this chapter before\nimplementing a normalizing flow model called RealNVP using Keras. We will also see\nhow normalizing flows can be extended to create more powerful models, such as\nGLOW and FFJORD.\nIntroduction\nWe will begin with a short story to illustrate the key concepts behind normalizing\nflows.\nJacob and the F.L.O.W. Machine\nUpon visiting a small village, you notice a mysterious-looking shop with a sign above\nthe door that says JACOB’S . Intrigued, you cautiously enter and ask the old man\nstanding behind the counter what he sells ( Figure 6-1 ).\nFigure 6-1. Inside a steampunk shop, with a large metallic bell (created with\nMidjourney )\nHe replies that he offers a service for digitizing paintings, with a difference. After a\nbrief moment rummaging around the back of the shop, he brings out a silver box,\nembossed with the letters F.L.O.W . He tells you that this stands for Finding Likenesses\nOf Watercolors, which approximately describes what the machine does. Y ou decide to\ngive the machine a try.\n168 | Chapter 6: Normalizing Flow Models\nY ou come back the next day and hand the shopkeeper a set of your favorite paintings,\nand he passes them through the machine. The F.L.O.W . machine begins to hum and\nwhistle and after a while outputs a set of numbers that appear randomly generated.\nThe shopkeeper hands you the list and begins to walk to the till to calculate how\nmuch you owe him for the digitization process and the F.L.O.W . box. Distinctly unim‐\npressed, you ask the shopkeeper what you should do with this long list of numbers,\nand how you can get your favorite paintings back.\nThe shopkeeper rolls his eyes, as if the answer should be obvious. He walks back to\nthe machine and passes in the long list of numbers, this time from the opposite side.\nY ou hear the machine whir again and wait, puzzled, until finally your original paint‐\nings drop out from where they entered.\nRelieved to finally have your paintings back, you decide that it might be best to just\nstore them in the attic instead. However, before you have a chance to leave, the shop‐\nkeeper ushers you across to a different corner of the shop, where a giant bell hangs\nfrom the rafters. He hits the bell curve with a huge stick, sending vibrations around\nthe store.\nInstantly, the F.L.O.W . machine under your arm begins to hiss and whirr in reverse, as\nif a new set of numbers had just been passed in. After a few moments, more beautiful\nwatercolor paintings begin to fall out of the F.L.O.W . machine, but they are not the\nsame as the ones you originally digitized. They resemble the style and form of your\noriginal set of paintings, but each one is completely unique!\nY ou ask the shopkeeper how this incredible device works. He explains that the magic\nlies in the fact that he has developed a special process that ensures the transformation\nis extremely fast and simple to calculate while still being sophisticated enough to con‐\nvert the vibrations produced by the bell into the complex patterns and shapes present\nin the paintings.\nRealizing the potential of this contraption, you hurriedly pay for the device and exit\nthe store, happy that you now have a way to generate new paintings in your favorite\nstyle, simply by visiting the shop, chiming the bell, and waiting for your F.L.O.W .\nmachine to work its magic!\nThe story of Jacob and the F.L.O.W . machine is a depiction of a normalizing flow\nmodel. Let’s now explore the theory of normalizing flows in more detail, before we\nimplement a practical example using Keras.\nNormalizing Flows\nThe motivation of normalizing flow models is similar to that of variational autoen‐\ncoders, which we explored in Chapter 3 . To recap, in a variational autoencoder, we\nlearn an encoder  mapping function between a complex distribution and a much sim‐\npler distribution that we can sample from. We then also learn a decoder  mapping\nNormalizing Flows | 169\nfunction from the simpler distribution to the complex distribution, so that we can\ngenerate a new data point by sampling a point z from the simpler distribution and\napplying the learned transformation. Probabilistically speaking, the decoder models\npxz but the encoder is only an approximation qzx of the true pzx—the\nencoder and decoder are two completely distinct neural networks.\nIn a normalizing flow model, the decoding function is designed to be the exact\ninverse of the encoding function and quick to calculate, giving normalizing flows the\nproperty of tractability. However, neural networks are not by default invertible func‐\ntions. This raises the question of how we can create an invertible process that con‐\nverts between a complex distribution (such as the data generation distribution of a set\nof watercolor paintings) and a much simpler distribution (such as a bell-shaped\nGaussian distribution) while still making use of the flexibility and power of deep\nlearning.\nTo answer this question, we first need to understand a technique known as change of\nvariables . For this section, we will work with a simple example in just two dimen‐\nsions, so that you can see exactly how normalizing flows work in fine detail. More\ncomplex examples are just extensions of the basic techniques presented here.\nChange of Variables\nSuppose  we have a probability distribution pXx defined over a rectangle X in two\ndimensions ( x=x1,x2), as shown in Figure 6-2 .\nFigure 6-2. A probability distribution pXx defined  over two dimensions, shown in 2D\n(left)  and 3D (right)\nThis function integrates to 1 over the domain of the distribution (i.e., x1in the range\n[1, 4] and x2 in the range [0, 2]), so it represents a well-defined probability distribu‐\ntion. We can write this as follows:\n170 | Chapter 6: Normalizing Flow Models\n∫02∫14\npXxdx1dx2= 1\nLet’s say that we want to shift and scale this distribution so that it is instead defined\nover a unit square Z. We can achieve this by defining a new variable z=z1,z2 and a\nfunction f that maps each point in X to exactly one point in Z as follows:\nz=fx\nz1=x1− 1\n3\nz2=x2\n2\nNote that this function is invertible . That is, there is a function g that maps every z\nback to its corresponding x. This is essential for a change of variables, as otherwise we\ncannot consistently map backward and forward between the two spaces. We can find\ng simply by rearranging the equations that define f, as shown in Figure 6-3 .\nFigure 6-3. Changing variables between X and Z\nWe now need to see how the change of variables from X to Z affects the probability\ndistribution pXx. We can do this by plugging the equations that define g into pXx\nto transform it into a function pZz that is defined in terms of z:\npZz=3z1+ 1 − 1 2z2\n9\n=2z1z2\n3\nNormalizing Flows | 171",8672
69-RealNVP.pdf,69-RealNVP,"However, if we now integrate pZz over the unit square, we can see that we have a\nproblem!\n∫01∫012z1z2\n3dz1dz2=1\n6\nThe transformed function pZz is now no longer a valid probability distribution,\nbecause it only integrates to 1/6. If we want to transform our complex probability dis‐\ntribution over the data into a simpler distribution that we can sample from, we must\nensure that it integrates to 1.\nThe missing factor of 6 is due to the fact that the domain of our transformed proba‐\nbility distribution is six times smaller than the original domain—the original rectan‐\ngle X had area 6, and this has been compressed into a unit square Z that only has area\n1. Therefore, we need to multiply the new probability distribution by a normalization\nfactor that is equal to the relative change in area (or volume in higher dimensions).\nLuckily, there is a way to calculate this volume change for a given transformation—it\nis the absolute value of the Jacobian determinant of the transformation. Let’s unpack\nthat!\nThe Jacobian Determinant\nThe Jacobian  of a function z=fx is the matrix of its first-order partial derivatives,\nas shown here:\nJ =∂z\n∂x=∂z1\n∂x1⋯∂z1\n∂xn\n⋱ ⋮\n∂zm\n∂x1⋯∂zm\n∂xn\nThe best way to explain this is with our example. If we take the partial derivative of z1\nwith respect to x1, we obtain 1\n3. If we take the partial derivative of z1 with respect to\nx2, we obtain 0. Similarly, if we take the partial derivative of z2 with respect to x1, we\nobtain 0. Lastly, if we take the partial derivative of z2 with respect to x2, we obtain 1\n2.\nTherefore, the Jacobian matrix for our function fx is as follows:\n172 | Chapter 6: Normalizing Flow Models\nJ=1\n30\n01\n2\nThe determinant  is only defined for square matrices and is equal to the signed volume\nof the parallelepiped created by applying the transformation represented by the\nmatrix to the unit (hyper)cube. In two dimensions, this is therefore just the signed\narea of the parallelogram created by applying the transformation represented by the\nmatrix to the unit square.\nThere is a general formula  for calculating the determinant of a matrix with n dimen‐\nsions, which runs in n3 time. For our example, we only need the formula for two\ndimensions, which is simply as follows:\ndeta b\nc d=ad−bc\nTherefore, for our example, the determinant of the Jacobian is 1\n3×1\n2− 0 × 0 =1\n6. This\nis the scaling factor of 1/6 that we need to ensure that the probability distribution\nafter transformation still integrates to 1!\nBy definition, the determinant is signed—that is, it can be negative.\nTherefore we need to take the absolute value of the Jacobian deter‐\nminant in order to obtain the relative change of volume.\nThe Change of Variables Equation\nWe can now write down a single equation that describes the process for changing\nvariables between X and Z. This is known as the change of variables equation  (Equa‐\ntion 6-1 ).\nEquation 6-1. The change of variables equation\npXx=pZzdet∂z\n∂x\nHow does this help us build a generative model? The key is understanding that if\npZz is a simple distribution from which we can easily sample (e.g., a Gaussian), then\nin theory, all we need to do is find an appropriate invertible function fx that can\nmap from the data X into Z and the corresponding inverse function gz that can be\nNormalizing Flows | 173",3362
70-Coupling Layers.pdf,70-Coupling Layers,"used to map a sampled z back to a point x in the original domain. We can use the\npreceding equation involving the Jacobian determinant to find an exact, tractable for‐\nmula for the data distribution px.\nHowever, there are two major issues when applying this in practice that we first need\nto address!\nFirstly, calculating the determinant of a high-dimensional matrix is computationally\nextremely expensive—specifically, it is n3. This is completely impractical to imple‐\nment in practice, as even small 32 × 32–pixel grayscale images have 1,024 dimensions.\nSecondly, it is not immediately obvious how we should go about calculating the inver‐\ntible function fx. We could use a neural network to find some function fx but we\ncannot necessarily invert this network—neural networks only work in one direction!\nTo solve these two problems, we need to use a special neural network architecture\nthat ensures that the change of variables function f is invertible and has a determi‐\nnant that is easy to calculate.\nWe shall see how to do this in the following section using a technique called real-\nvalued non-volume preserving (RealNVP) transformations .\nRealNVP\nRealNVP  was first introduced by Dinh et al. in 2017. 1 In this paper the authors show\nhow to construct a neural network that can transform a complex data distribution\ninto a simple Gaussian, while also possessing the desired properties of being inverti‐\nble and having a Jacobian that can be easily calculated.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/06_normflow/01_realnvp/realnvp.ipynb  in the\nbook repository.\nThe code has been adapted from the excellent RealNVP tutorial\ncreated by Mandolini Giorgio Maria et al. available on the Keras\nwebsite.\nThe Two Moons Dataset\nThe dataset we will use for this example is created by the make_moons  function from\nthe Python library sklearn . This creates a noisy dataset of points in 2D that resemble\ntwo crescents, as shown in Figure 6-4 .\n174 | Chapter 6: Normalizing Flow Models\nFigure 6-4. The two moons dataset in two dimensions\nThe code for creating this dataset is given in Example 6-1 .\nExample 6-1. Creating a moons dataset\ndata = datasets .make_moons (3000, noise=0.05)[0].astype(""float32"" ) \nnorm = layers.Normalization ()\nnorm.adapt(data)\nnormalized_data  = norm(data) \nMake a noisy, unnormalized moons dataset of 3,000 points.\nNormalize the dataset to have mean 0 and standard deviation 1.\nWe will build a RealNVP model that can generate points in 2D that follow a similar\ndistribution to the two moons dataset. Whilst this is a very simple example, it will\nhelp us understand how a normalizing flow model works in practice, in fine detail.\nFirst, however, we need to introduce a new type of layer, called a coupling layer.\nCoupling Layers\nA coupling layer  produces  a scale and translation factor for each element of its input.\nIn other words, it produces two tensors that are exactly the same size as the input,\none for the scale factor and one for the translation factor, as shown in Figure 6-5 .\nFigure 6-5. A coupling layer outputs two tensors that are the same shape as the input: a\nscaling factor (s) and a translation factor (t)\nRealNVP | 175\nTo build a custom Coupling  layer for our simple example, we can stack Dense  layers\nto create the scale output and a different set of Dense  layers to create the translation\noutput, as shown in Example 6-2 .\nFor images, Coupling  layer blocks use Conv2D  layers instead of\nDense  layers.\nExample 6-2. A Coupling  layer in Keras\ndef Coupling ():\n    input_layer  = layers.Input(shape=2) \n    s_layer_1  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(input_layer ) \n    s_layer_2  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(s_layer_1 )\n    s_layer_3  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(s_layer_2 )\n    s_layer_4  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(s_layer_3 )\n    s_layer_5  = layers.Dense(\n        2, activation =""tanh"", kernel_regularizer =regularizers .l2(0.01)\n    )(s_layer_4 ) \n    t_layer_1  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(input_layer ) \n    t_layer_2  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(t_layer_1 )\n    t_layer_3  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(t_layer_2 )\n    t_layer_4  = layers.Dense(\n        256, activation =""relu"", kernel_regularizer =regularizers .l2(0.01)\n    )(t_layer_3 )\n    t_layer_5  = layers.Dense(\n        2, activation =""linear"" , kernel_regularizer =regularizers .l2(0.01)\n    )(t_layer_4 ) \n    return models.Model(inputs=input_layer , outputs=[s_layer_5 , t_layer_5 ]) \nThe input to the Coupling  layer block in our example has two dimensions.\n176 | Chapter 6: Normalizing Flow Models\nThe scaling  stream is a stack of Dense  layers of size 256.\nThe final scaling layer is of size 2 and has tanh  activation.\nThe translation  stream is a stack of Dense  layers of size 256.\nThe final translation layer is of size 2 and has linear  activation.\nThe Coupling  layer is constructed as a Keras Model  with two outputs (the scaling\nand translation factors).\nNotice how the number of channels is temporarily increased to allow for a more\ncomplex representation to be learned, before being collapsed back down to the same\nnumber of channels as the input. In the original paper, the authors also use regulariz‐\ners on each layer to penalize large weights.\nPassing data through a coupling layer\nThe architecture of a coupling layer is not particularly interesting—what makes it\nunique is the way the input data is masked and transformed as it is fed through the\nlayer, as shown in Figure 6-6 .\nFigure 6-6. The process of transforming the input x through a coupling layer\nNotice how only the first d dimensions of the data are fed through to the first cou‐\npling layer—the remaining D−d dimensions are completely masked (i.e., set to\nzero). In our simple example with D= 2, choosing d= 1 means that instead of the\ncoupling layer seeing two values, x1,x2, the layer sees x1, 0.\nRealNVP | 177\nThe outputs from the layer are the scale and translation factors. These are again\nmasked, but this time with the inverse  mask to previously, so that only the second\nhalves are let through—i.e., in our example, we obtain 0,s2 and 0,t2. These are\nthen applied element-wise to the second half of the input x2 and the first half of the\ninput x1 is simply passed straight through, without being updated at all. In summary,\nfor a vector with dimension D where d<D, the update equations are as follows:\nz1:d=x1:d\nzd+ 1:D=xd+ 1:D⊙exp sx1:d+tx1:d\nY ou may be wondering why we go to the trouble of building a layer that masks so\nmuch information. The answer is clear if we investigate the structure of the Jacobian\nmatrix of this function:\n∂z\n∂x= 0\n∂zd+ 1:D\n∂x1:ddiag exp sx1:d\nThe top-left d×d submatrix is simply the identity matrix, because z1:d=x1:d. These\nelements are passed straight through without being updated. The top-right submatrix\nis therefore 0, because z1:d is not dependent on xd+ 1:D.\nThe bottom-left submatrix is complex, and we do not seek to simplify this. The\nbottom-right submatrix is simply a diagonal matrix, filled with the elements of\nexp sx1:d, because zd+ 1:D is linearly dependent on xd+ 1:D and the gradient is\ndependent only on the scaling factor (not on the translation factor). Figure 6-7  shows\na diagram of this matrix form, where only the nonzero elements are filled in with\ncolor.\nNotice how there are no nonzero elements above the diagonal—for this reason, this\nmatrix form is called lower triangular . Now we see the benefit of structuring the\nmatrix in this way—the determinant of a lower-triangular matrix is just equal to the\nproduct of the diagonal elements. In other words, the determinant is not dependent\non any of the complex derivatives in the bottom-left submatrix!\n178 | Chapter 6: Normalizing Flow Models\nFigure 6-7. The Jacobian matrix of the transformation—a lower triangular matrix, with\ndeterminant equal to the product of the elements along the diagonal\nTherefore, we can write the determinant of this matrix as follows:\ndetJ= exp ∑\njsx1:dj\nThis is easily computable, which was one of the two original goals of building a nor‐\nmalizing flow model.\nThe other goal was that the function must be easily invertible. We can see that this is\ntrue as we can write down the invertible function just by rearranging the forward\nequations, as follows:\nx1:d=z1:d\nxd+ 1:D=zd+ 1:D−tx1:d⊙exp −sx1:d\nRealNVP | 179\nThe equivalent diagram is shown in Figure 6-8 .\nFigure 6-8. The inverse function x = g(z)\nWe now have almost everything we need to build our RealNVP model. However,\nthere is one issue that still remains—how should we update the first d elements of the\ninput? Currently they are left completely unchanged by the model!\nStacking coupling layers\nTo resolve this problem, we can use a really simple trick. If we stack coupling layers\non top of each other but alternate the masking pattern, the layers that are left\nunchanged by one layer will be updated in the next. This architecture has the added\nbenefit of being able to learn more complex representations of the data, as it is a\ndeeper neural network.\nThe Jacobian of this composition of coupling layers will still be simple to compute,\nbecause linear algebra tells us that the determinant of a matrix product is the product\nof the determinants. Similarly, the inverse of the composition of two functions is just\nthe composition of the inverses, as shown in the following equations:\ndetA · B = det AdetB\nfb∘fa−1=fa−1∘fb−1\nTherefore, if we stack coupling layers, flipping the masking each time, we can build a\nneural network that is able to transform the whole input tensor, while retaining the\nessential properties of having a simple Jacobian determinant and being invertible.\nFigure 6-9  shows the overall structure.\n180 | Chapter 6: Normalizing Flow Models",10410
71-Training the RealNVP Model.pdf,71-Training the RealNVP Model,"Figure 6-9. Stacking coupling layers, alternating the masking with each layer\nTraining the RealNVP Model\nNow that we have built the RealNVP model, we can train it to learn the complex dis‐\ntribution of the two moons dataset. Remember, we want to minimize the negative\nlog-likelihood of the data under the model − log pXx. Using Equation 6-1 , we can\nwrite this as follows:\n− log pXx= − log pZz− log det∂z\n∂x\nWe choose the target output distribution pZz of the forward process f to be a stan‐\ndard Gaussian, because we can easily sample from this distribution. We can then\ntransform a point sampled from the Gaussian back into the original image domain by\napplying the inverse process g, as shown in Figure 6-10 .\nFigure 6-10. Transforming between the complex distribution pXx and a simple Gaus‐\nsian pZz in 1D (middle row) and 2D (bottom row)\nRealNVP | 181\nExample 6-3  shows how to build a RealNVP network, as a custom Keras Model .\nExample 6-3. Building the RealNVP model in Keras\nclass RealNVP(models.Model):\n    def __init__ (self, input_dim , coupling_layers , coupling_dim , regularization ):\n        super(RealNVP, self).__init__ ()\n        self.coupling_layers  = coupling_layers\n        self.distribution  = tfp.distributions .MultivariateNormalDiag (\n            loc=[0.0, 0.0], scale_diag =[1.0, 1.0]\n        ) \n        self.masks = np.array(\n            [[0, 1], [1, 0]] * (coupling_layers  // 2), dtype=""float32""\n        ) \n        self.loss_tracker  = metrics.Mean(name=""loss"")\n        self.layers_list  = [\n            Coupling (input_dim , coupling_dim , regularization )\n            for i in range(coupling_layers )\n        ] \n    @property\n    def metrics(self):\n        return [self.loss_tracker ]\n    def call(self, x, training =True):\n        log_det_inv  = 0\n        direction  = 1\n        if training :\n            direction  = -1\n        for i in range(self.coupling_layers )[::direction ]: \n            x_masked  = x * self.masks[i]\n            reversed_mask  = 1 - self.masks[i]\n            s, t = self.layers_list [i](x_masked )\n            s *= reversed_mask\n            t *= reversed_mask\n            gate = (direction  - 1) / 2\n            x = (\n                reversed_mask\n                * (x * tf.exp(direction  * s) + direction  * t * tf.exp(gate * s))\n                + x_masked\n            ) \n            log_det_inv  += gate * tf.reduce_sum (s, axis = 1) \n        return x, log_det_inv\n    def log_loss (self, x):\n        y, logdet = self(x)\n        log_likelihood  = self.distribution .log_prob (y) + logdet \n        return -tf.reduce_mean (log_likelihood )\n    def train_step (self, data):\n        with tf.GradientTape () as tape:\n182 | Chapter 6: Normalizing Flow Models\n            loss = self.log_loss (data)\n        g = tape.gradient (loss, self.trainable_variables )\n        self.optimizer .apply_gradients (zip(g, self.trainable_variables ))\n        self.loss_tracker .update_state (loss)\n        return {""loss"": self.loss_tracker .result()}\n    def test_step (self, data):\n        loss = self.log_loss (data)\n        self.loss_tracker .update_state (loss)\n        return {""loss"": self.loss_tracker .result()}\nmodel = RealNVP(\n    input_dim  = 2\n    , coupling_layers = 6\n    , coupling_dim  = 256\n    , regularization  = 0.01\n)\nmodel.compile(optimizer =optimizers .Adam(learning_rate =0.0001))\nmodel.fit(\n    normalized_data\n    , batch_size =256\n    , epochs=300\n)\nThe target distribution is a standard 2D Gaussian.\nHere, we create the alternating mask pattern.\nA list of Coupling  layers that define the RealNVP network.\nIn the main call  function of the network, we loop over the Coupling  layers. If\ntraining=True , then we move forward through the layers (i.e., from data to\nlatent space). If training=False , then we move backward through the layers (i.e.,\nfrom latent space to data).\nThis line describes both the forward and backward equations dependent on the\ndirection  (try plugging in direction = -1  and direction = 1  to prove this to\nyourself!).\nThe log determinant of the Jacobian, which we need to calculate the loss func‐\ntion, is simply the sum of the scaling factors.\nThe loss function is the negative sum of the log probability of the transformed\ndata, under our target Gaussian distribution and the log determinant of the\nJacobian.\nRealNVP | 183",4411
72-Analysis of the RealNVP Model.pdf,72-Analysis of the RealNVP Model,"Analysis of the RealNVP Model\nOnce  the model is trained, we can use it to transform the training set into the latent\nspace (using the forward direction, f) and, more importantly, to transform a sampled\npoint in the latent space into a point that looks like it could have been sampled from\nthe original data distribution (using the backward direction, g).\nFigure 6-11  shows the output from the network before any learning has taken place—\nthe forward and backward directions just pass information straight through with\nhardly any transformation.\nFigure 6-11. The RealNVP model inputs (left)  and outputs (right) before training, for the\nforward process (top) and the reverse process (bottom)\nAfter training ( Figure 6-12 ), the forward process is able to convert the points from\nthe training set into a distribution that resembles a Gaussian. Likewise, the backward\nprocess can take points sampled from a Gaussian distribution and map them back to\na distribution that resembles the original data.\n184 | Chapter 6: Normalizing Flow Models\nFigure 6-12. The RealNVP model inputs (left)  and outputs (right) after  training, for the\nforward process (top) and the reverse process (bottom)\nThe loss curve for the training process is shown in Figure 6-13 .\nFigure 6-13. The loss curve for the RealNVP training process\nThis completes our discussion of RealNVP , a specific case of a normalizing flow gen‐\nerative model. In the next section, we’ll cover some modern normalizing flow models\nthat extend the ideas introduced in the RealNVP paper.\nRealNVP | 185",1572
73-Other Normalizing Flow Models.pdf,73-Other Normalizing Flow Models,,0
74-Chapter 7. Energy-Based Models.pdf,74-Chapter 7. Energy-Based Models,"Other Normalizing Flow Models\nTwo other successful and important normalizing flow models are GLOW  and\nFFJORD . The following sections describe the key advancements they made.\nGLOW\nPresented  at NeurIPS 2018, GLOW was one of the first models to demonstrate the\nability of normalizing flows to generate high-quality samples and produce a meaning‐\nful latent space that can be traversed to manipulate samples. The key step was to\nreplace the reverse masking setup with invertible 1 × 1 convolutional layers. For\nexample, with RealNVP applied to images, the ordering of the channels is flipped\nafter each step, to ensure that the network gets the chance to transform all of the\ninput. In GLOW a 1 × 1 convolution is applied instead, which effectively acts as a\ngeneral method to produce any permutation of the channels that the model desires.\nThe authors show that even with this addition, the distribution as a whole remains\ntractable, with determinants and inverses that are easy to compute at scale.\nFigure 6-14. Random samples from the GLOW model (source: Kingma and Dhariwal,\n2018 )2\n186 | Chapter 6: Normalizing Flow Models\nFFJORD\nRealNVP and GLOW are discrete time normalizing flows—that is, they transform the\ninput through a discrete set of coupling layers. FFJORD (Free-Form Continuous\nDynamics for Scalable Reversible Generative Models), presented at ICLR 2019, shows\nhow it is possible to model the transformation as a continuous time process (i.e., by\ntaking the limit as the number of steps in the flow tends to infinity and the step size\ntends to zero). In this case, the dynamics are modeled using an ordinary differential\nequation (ODE) whose parameters are produced by a neural network ( fθ). A black-\nbox solver is used to solve the ODE at time t1—i.e., to find z1 given some initial point\nz0 sampled from a Gaussian at t0, as described by the following equations:\nz0∼pz0\n∂zt\n∂t=fθxt,t\nx=z1\nA diagram of the transformation process is shown in Figure 6-15 .\nFigure 6-15. FFJORD models the transformation between the data distribution and a\nstandard Gaussian via an ordinary differential  equation, parameterized by a neural\nnetwork (source: Will Grathwohl et al., 2018 )3\nOther Normalizing Flow Models | 187\nSummary\nIn this chapter we explored normalizing flow models such as RealNVP , GLOW , and\nFFJORD.\nA normalizing flow model is an invertible function defined by a neural network that\nallows us to directly model the data density via a change of variables. In the general\ncase, the change of variables equation requires us to calculate a highly complex Jaco‐\nbian determinant, which is impractical for all but the simplest of examples.\nTo sidestep this issue, the RealNVP model restricts the form of the neural network,\nsuch that it adheres to the two essential criteria: it is invertible and has a Jacobian\ndeterminant that is easy to compute.\nIt does this through stacking coupling layers, which produce scale and translation fac‐\ntors at each step. Importantly, the coupling layer masks the data as it flows through\nthe network, in a way that ensures that the Jacobian is lower triangular and therefore\nhas a simple-to-compute determinant. Full visibility of the input data is achieved\nthrough flipping the masks at each layer.\nBy design, the scale and translation operations can be easily inverted, so that once the\nmodel is trained it is possible to run data through the network in reverse. This means\nthat we can target the forward transformation process toward a standard Gaussian,\nwhich we can easily sample from. We can then run the sampled points backward\nthrough the network to generate new observations.\nThe RealNVP paper also shows how it is possible to apply this technique to images,\nby using convolutions inside the coupling layers, rather than densely connected lay‐\ners. The GLOW paper extended this idea to remove the necessity for any hardcoded\npermutation of the masks. The FFJORD model introduced the concept of continuous\ntime normalizing flows, by modeling the transformation process as an ODE defined\nby a neural network.\nOverall, we have seen how normalizing flows are a powerful generative modeling\nfamily that can produce high-quality samples, while maintaining the ability to tracta‐\nbly describe the data density function.\nReferences\n1. Laurent Dinh et al., “Density Estimation Using Real NVP , ” May 27, 2016, https://\narxiv.org/abs/1605.08803v3 .\n2. Diedrick P . Kingma and Prafulla Dhariwal, “Glow: Generative Flow with Invertible\n1x1 Convolutions, ” July 10, 2018, https://arxiv.org/abs/1807.03039 .\n3. Will Grathwohl et al., “FFJORD: Free-Form Continuous Dynamics for Scalable\nReversible Generative Models, ” October 22, 2018, https://arxiv.org/abs/1810.01367 .\n188 | Chapter 6: Normalizing Flow Models",4828
75-Introduction.pdf,75-Introduction,"CHAPTER 7\nEnergy-Based Models\nChapter Goals\nIn this chapter you will:\n•Understand how to formulate a deep energy-based model (EBM).\n•See how to sample from an EBM using Langevin dynamics.\n•Train your own EBM using contrastive divergence.\n•Analyze the EBM, including viewing snapshots of the Langevin dynamics sam‐\npling process.\n•Learn about other types of EBM, such as restricted Boltzmann machines.\nEnergy-based models are a broad class of generative model that borrow a key idea\nfrom modeling physical systems—namely, that the probability of an event can be\nexpressed using a Boltzmann distribution, a specific function that normalizes a real-\nvalued energy function between 0 and 1. This distribution was originally formulated\nin 1868 by Ludwig Boltzmann, who used it to describe gases in thermal equilibrium.\nIn this chapter, we will see how we can use this idea to train a generative model that\ncan be used to produce images of handwritten digits. We will explore several new\nconcepts, including contrastive divergence for training the EBM and Langevin\ndynamics for sampling.\nIntroduction\nWe will begin with a short story to illustrate the key concepts behind energy-based\nmodels.\n189\nThe Long-au-Vin Running Club\nDiane Mixx was head coach of the long-distance running team in the fictional French\ntown of Long-au-Vin. She was well known for her exceptional abilities as a trainer\nand had acquired a reputation for being able to turn even the most mediocre of ath‐\nletes into world-class runners ( Figure 7-1 ).\nFigure 7-1. A running coach training some elite athletes (created with Midjourney )\nHer methods were based around assessing the energy levels of each athlete. Over\nyears of working with athletes of all abilities, she had developed an incredibly accurate\nsense of just how much energy a particular athlete had left after a race, just by looking\nat them. The lower an athlete’s energy level, the better—elite athletes always gave\neverything they had during the race!\nTo keep her skills sharp, she regularly trained herself by measuring the contrast\nbetween her energy sensing abilities on known elite athletes and the best athletes\nfrom her club. She ensured that the divergence between her predictions for these two\ngroups was as large as possible, so that people would take her seriously if she said that\nshe had found a true elite athlete within her club.\nThe real magic was her ability to convert a mediocre runner into a top-class runner.\nThe process was simple—she measured the current energy level of the athlete and\nworked out the optimal set of adjustments the athlete needed to make to improve\ntheir performance next time. Then, after making these adjustments, she measured the\nathlete’s energy level again, looking for it to be slightly lower than before, explaining\nthe improved performance on the track. This process of assessing the optimal adjust‐\nments and taking a small step in the right direction would continue until eventually\nthe athlete was indistinguishable from a world-class runner.\n190 | Chapter 7: Energy-Based Models",3108
76-Sampling Using Langevin Dynamics.pdf,76-Sampling Using Langevin Dynamics,"After many years Diane retired from coaching and published a book on her methods\nfor generating elite athletes—a system she branded the “Long-au-Vin, Diane Mixx”\ntechnique.\nThe story of Diane Mixx and the Long-au-Vin running club captures the key ideas\nbehind energy-based modeling. Let’s now explore the theory in more detail, before\nwe implement a practical example using Keras.\nEnergy-Based Models\nEnergy-based models attempt to model the true data-generating distribution using a\nBoltzmann distribution  (Equation 7-1 ) where Ex is know as  the energy function  (or\nscore ) of an observation x.\nEquation 7-1. Boltzmann distribution\np=e−E\n∫∈ e−E\nIn practice, this amounts to training a neural network Ex to output low scores for\nlikely observations (so p is close to 1) and high scores for unlikely observations (so\np is close to 0).\nThere are two challenges with modeling the data in this way. Firstly, it is not clear\nhow we should use our model for sampling new observations—we can use it to gen‐\nerate a score given an observation, but how do we generate an observation that has a\nlow score (i.e., a plausible observation)?\nSecondly, the normalizing denominator of Equation 7-1  contains an integral that is\nintractable for all but the simplest of problems. If we cannot calculate this integral,\nthen we cannot use maximum likelihood estimation to train the model, as this\nrequires that p is a valid probability distribution.\nThe key idea behind an energy-based model is that we can use approximation tech‐\nniques to ensure we never need to calculate the intractable denominator. This is in\ncontrast to, say, a normalizing flow, where we go to great lengths to ensure that the\ntransformations that we apply to our standard Gaussian distribution do not change\nthe fact that the output is still a valid probability distribution.\nWe sidestep the tricky intractable denominator problem by using a technique called\ncontrastive divergence (for training) and a technique called Langevin dynamics\n(for sampling), following the ideas from Du and Mordatch’s 2019 paper  “Implicit\nEnergy-Based Models | 191\nGeneration  and Modeling with Energy-Based Models. ” 1 We shall explore these tech‐\nniques in detail while building our own EBM later in the chapter.\nFirst, let’s get set up with a dataset and design a simple neural network that will repre‐\nsent our real-valued energy function Ex.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/07_ebm/01_ebm/ebm.ipynb  in the book\nrepository.\nThe code is adapted from the excellent tutorial on deep energy-\nbased generative models  by Phillip Lippe.\nThe MNIST Dataset\nWe’ll  be using the standard MNIST dataset , consisting of grayscale images of hand‐\nwritten digits. Some example images from the dataset are shown in Figure 7-2 .\nFigure 7-2. Examples of images from the MNIST dataset\nThe dataset comes prepackaged with TensorFlow, so it can be downloaded as shown\nin Example 7-1 .\nExample 7-1. Loading the MNIST dataset\nfrom tensorflow.keras  import datasets\n(x_train, _), (x_test, _) = datasets .mnist.load_data ()\nAs usual, we’ll scale the pixel values to the range [-1, 1] and add some padding to\nmake the images 32 × 32 pixels in size. We also convert it to a TensorFlow Dataset, as\nshown in Example 7-2 .\nExample 7-2. Preprocessing the MNIST dataset\ndef preprocess (imgs):\n    imgs = (imgs.astype(""float32"" ) - 127.5) / 127.5\n    imgs = np.pad(imgs , ((0,0), (2,2), (2,2)), constant_values = -1.0)\n192 | Chapter 7: Energy-Based Models\n    imgs = np.expand_dims (imgs, -1)\n    return imgs\nx_train = preprocess (x_train)\nx_test = preprocess (x_test)\nx_train = tf.data.Dataset.from_tensor_slices (x_train).batch(128)\nx_test = tf.data.Dataset.from_tensor_slices (x_test).batch(128)\nNow that we have our dataset, we can build the neural network that will represent our\nenergy function Ex.\nThe Energy Function\nThe energy function Eθx is a neural network with parameters θ that can transform\nan input image x into a scalar value. Throughout this network, we make use of an\nactivation function called swish , as described in the following sidebar.\nSwish Activation\nSwish is an alternative to ReLU that was introduced by Google in 2017 2 and is defined\nas follows:\nswish x=x· sigmoid x=x\ne−x+ 1\nSwish is visually similar to ReLU, with the key difference being that it is smooth,\nwhich helps to alleviate the vanishing gradient problem. This is particularly impor‐\ntant for energy-based models. A plot of the swish function is shown in Figure 7-3 .\nFigure 7-3. The swish activation function\nEnergy-Based Models | 193\nThe network is a set of stacked Conv2D  layers that gradually reduce the size of the\nimage while increasing the number of channels. The final layer is a single fully con‐\nnected unit with linear activation, so the network can output values in the range ( −∞,\n∞). The code to build it is given in Example 7-3 .\nExample 7-3. Building the energy function Ex neural network\nebm_input  = layers.Input(shape=(32, 32, 1))\nx = layers.Conv2D(\n    16, kernel_size =5, strides=2, padding=""same"", activation  = activations .swish\n)(ebm_input ) \nx = layers.Conv2D(\n    32, kernel_size =3, strides=2, padding=""same"", activation  = activations .swish\n)(x)\nx = layers.Conv2D(\n    64, kernel_size =3, strides=2, padding=""same"", activation  = activations .swish\n)(x)\nx = layers.Conv2D(\n    64, kernel_size =3, strides=2, padding=""same"", activation  = activations .swish\n)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(64, activation  = activations .swish)(x)\nebm_output  = layers.Dense(1)(x) \nmodel = models.Model(ebm_input , ebm_output ) \nThe energy function is a set of stacked Conv2D  layers, with swish activation.\nThe final layer is a single fully connected unit, with a linear activation function.\nA Keras Model  that converts the input image into a scalar energy value.\nSampling Using Langevin Dynamics\nThe energy function only outputs a score for a given input—how can we use this\nfunction to generate new samples that have a low energy score?\nWe will use a technique called Langevin dynamics , which makes use of the fact that we\ncan compute the gradient of the energy function with respect to its input. If we start\nfrom a random point in the sample space and take small steps in the opposite direc‐\ntion of the calculated gradient, we will gradually reduce the energy function. If our\nneural network is trained correctly, then the random noise should transform into an\nimage that resembles an observation from the training set before our eyes!\n194 | Chapter 7: Energy-Based Models\nStochastic Gradient Langevin Dynamics\nImportantly, we must also add a small amount of random noise to\nthe input as we travel across the sample space; otherwise, there is a\nrisk of falling into local minima. The technique is therefore known\nas stochastic gradient Langevin dynamics. 3\nWe can visualize this gradient descent as shown in Figure 7-4 , for a two-dimensional\nspace with the energy function value on the third dimension. The path is a noisy\ndescent downhill, following the negative gradient of the energy function Ex with\nrespect to the input x. In the MNIST image dataset, we have 1,024 pixels so are navi‐\ngating a 1,024-dimensional space, but the same principles apply!\nFigure 7-4. Gradient descent using Langevin dynamics\nIt is worth noting the difference between this kind of gradient descent and the kind of\ngradient descent we normally use to train a neural network.\nWhen training a neural network, we calculate the gradient of the loss function  with\nrespect to the parameters  of the network (i.e., the weights) using backpropagation.\nThen we update the parameters a small amount in the direction of the negative gradi‐\nent, so that over many iterations, we gradually minimize the loss.\nWith Langevin dynamics, we keep the neural network weights fixed  and calculate the\ngradient of the output  with respect to the input . Then we update the input a small\namount in the direction of the negative gradient, so that over many iterations, we\ngradually minimize the output (the energy score).\nBoth processes utilize the same idea (gradient descent), but are applied to different\nfunctions and with respect to different entities.\nEnergy-Based Models | 195\nFormally, Langevin dynamics can be described by the following equation:\nxk=xk− 1−η∇xEθxk− 1+ω\nwhere ω∼  0,σ and x0∼  (–1,1). η is the step size hyperparameter that must be\ntuned—too large and the steps jump over minima, too small and the algorithm will\nbe too slow to converge.\nx0∼  (–1,1) is the uniform distribution on the range [–1, 1].\nWe can code up our Langevin sampling function as illustrated in Example 7-4 .\nExample 7-4. The Langevin sampling function\ndef generate_samples (model, inp_imgs , steps, step_size , noise):\n    imgs_per_step  = []\n    for _ in range(steps): \n        inp_imgs  += tf.random.normal(inp_imgs .shape, mean = 0, stddev = noise) \n        inp_imgs  = tf.clip_by_value (inp_imgs , -1.0, 1.0)\n        with tf.GradientTape () as tape:\n            tape.watch(inp_imgs )\n            out_score  = -model(inp_imgs ) \n        grads = tape.gradient (out_score , inp_imgs ) \n        grads = tf.clip_by_value (grads, -0.03, 0.03)\n        inp_imgs  += -step_size  * grads \n        inp_imgs  = tf.clip_by_value (inp_imgs , -1.0, 1.0)\n        return inp_imgs\nLoop over given number of steps.\nAdd a small amount of noise to the image.\nPass the image through the model to obtain the energy score.\nCalculate the gradient of the output with respect to the input.\nAdd a small amount of the gradient to the input image.\n196 | Chapter 7: Energy-Based Models",9800
77-Training with Contrastive Divergence.pdf,77-Training with Contrastive Divergence,"Training with Contrastive Divergence\nNow  that we know how to sample a novel low-energy point from the sample space,\nlet’s turn our attention to training the model.\nWe cannot apply maximum likelihood estimation, because the energy function does\nnot output a probability; it outputs a score that does not integrate to 1 across the sam‐\nple space. Instead, we will apply a technique first proposed in 2002 by Geoffrey Hin‐\nton, called contrastive divergence , for training unnormalized scoring models. 4\nThe value that we want to minimize (as always) is the negative log-likelihood of the\ndata:\nℒ= −x∼datalogpθ\nWhen pθ has the form of a Boltzmann distribution, with energy function Eθ, it\ncan be shown that the gradient of this value can be written as follows (Oliver Wood‐\nford’s “Notes on Contrastive Divergence” for the full derivation): 5\n∇θℒ=x∼data∇θEθ−x∼model∇θEθ\nThis intuitively makes a lot of sense—we want to train the model to output large neg‐\native energy scores for real observations and large positive energy scores for gener‐\nated fake observations so that the contrast between these two extremes is as large as\npossible.\nIn other words, we can calculate the difference between the energy scores of real and\nfake samples and use this as our loss function.\nTo calculate the energy scores of fake samples, we would need to be able to sample\nexactly from the distribution pθ, which isn’t possible due to the intractable denom‐\ninator. Instead, we can use our Langevin sampling procedure to generate a set of\nobservations with low energy scores. The process would need to run for infinitely\nmany steps to produce a perfect sample (which is obviously impractical), so instead\nwe run for some small number of steps, on the assumption that this is good enough\nto produce a meaningful loss function.\nWe also maintain a buffer of samples from previous iterations, so that we can use this\nas the starting point for the next batch, rather than pure random noise. The code to\nproduce the sampling buffer is shown in Example 7-5 .\nEnergy-Based Models | 197\nExample 7-5. The Buffer\nclass Buffer:\n    def __init__ (self, model):\n        super().__init__ ()\n        self.model = model\n        self.examples  = [\n            tf.random.uniform(shape = (1, 32, 32, 1)) * 2 - 1\n            for _ in range(128)\n        ] \n    def sample_new_exmps (self, steps, step_size , noise):\n        n_new = np.random.binomial (128, 0.05) \n        rand_imgs  = (\n            tf.random.uniform((n_new, 32, 32, 1)) * 2 - 1\n        )\n        old_imgs  = tf.concat(\n            random.choices(self.examples , k=128-n_new), axis=0\n        ) \n        inp_imgs  = tf.concat([rand_imgs , old_imgs ], axis=0)\n        inp_imgs  = generate_samples (\n            self.model, inp_imgs , steps=steps, step_size =step_size , noise = noise\n        ) \n        self.examples  = tf.split(inp_imgs , 128, axis = 0) + self.examples  \n        self.examples  = self.examples [:8192]\n        return inp_imgs\nThe sampling buffer is initialized with a batch of random noise.\nOn average, 5% of observations are generated from scratch (i.e., random noise)\neach time.\nThe rest are pulled at random from the existing buffer.\nThe observations are concatenated and run through the Langevin sampler.\nThe resulting sample is added to the buffer, which is trimmed to a max length of\n8,192 observations.\nFigure 7-5  shows one training step of contrastive divergence. The scores of real\nobservations are pushed down by the algorithm and the scores of fake observations\nare pulled up, without caring about normalizing these scores after each step.\n198 | Chapter 7: Energy-Based Models\nFigure 7-5. One step of contrastive divergence\nWe can code up the training step of the contrastive divergence algorithm within a\ncustom Keras model as shown in Example 7-6 .\nExample 7-6. EBM trained using contrastive divergence\nclass EBM(models.Model):\n    def __init__ (self):\n        super(EBM, self).__init__ ()\n        self.model = model\n        self.buffer = Buffer(self.model)\n        self.alpha = 0.1\n        self.loss_metric  = metrics.Mean(name=""loss"")\n        self.reg_loss_metric  = metrics.Mean(name=""reg"")\n        self.cdiv_loss_metric  = metrics.Mean(name=""cdiv"")\n        self.real_out_metric  = metrics.Mean(name=""real"")\n        self.fake_out_metric  = metrics.Mean(name=""fake"")\n    @property\n    def metrics(self):\n        return [\n            self.loss_metric ,\n            self.reg_loss_metric ,\n            self.cdiv_loss_metric ,\n            self.real_out_metric ,\n            self.fake_out_metric\n        ]\n    def train_step (self, real_imgs ):\n        real_imgs  += tf.random.normal(\n            shape=tf.shape(real_imgs ), mean = 0, stddev = 0.005\n        ) \n        real_imgs  = tf.clip_by_value (real_imgs , -1.0, 1.0)\n        fake_imgs  = self.buffer.sample_new_exmps (\n            steps=60, step_size =10, noise = 0.005\n        ) \n        inp_imgs  = tf.concat([real_imgs , fake_imgs ], axis=0)\n        with tf.GradientTape () as training_tape :\nEnergy-Based Models | 199\n            real_out , fake_out  = tf.split(self.model(inp_imgs ), 2, axis=0) \n            cdiv_loss  = tf.reduce_mean (fake_out , axis = 0) - tf.reduce_mean (\n                real_out , axis = 0\n            ) \n            reg_loss  = self.alpha * tf.reduce_mean (\n                real_out  ** 2 + fake_out  ** 2, axis = 0\n            ) \n            loss = reg_loss  + cdiv_loss\n        grads = training_tape .gradient (loss, self.model.trainable_variables ) \n        self.optimizer .apply_gradients (\n            zip(grads, self.model.trainable_variables )\n        )\n        self.loss_metric .update_state (loss)\n        self.reg_loss_metric .update_state (reg_loss )\n        self.cdiv_loss_metric .update_state (cdiv_loss )\n        self.real_out_metric .update_state (tf.reduce_mean (real_out , axis = 0))\n        self.fake_out_metric .update_state (tf.reduce_mean (fake_out , axis = 0))\n        return {m.name: m.result() for m in self.metrics}\n    def test_step (self, real_imgs ): \n        batch_size  = real_imgs .shape[0]\n        fake_imgs  = tf.random.uniform((batch_size , 32, 32, 1)) * 2 - 1\n        inp_imgs  = tf.concat([real_imgs , fake_imgs ], axis=0)\n        real_out , fake_out  = tf.split(self.model(inp_imgs ), 2, axis=0)\n        cdiv = tf.reduce_mean (fake_out , axis = 0) - tf.reduce_mean (\n            real_out , axis = 0\n        )\n        self.cdiv_loss_metric .update_state (cdiv)\n        self.real_out_metric .update_state (tf.reduce_mean (real_out , axis = 0))\n        self.fake_out_metric .update_state (tf.reduce_mean (fake_out , axis = 0))\n        return {m.name: m.result() for m in self.metrics[2:]}\nebm = EBM()\nebm.compile(optimizer =optimizers .Adam(learning_rate =0.0001), run_eagerly =True)\nebm.fit(x_train, epochs=60, validation_data  = x_test,)\nA small amount of random noise is added to the real images, to avoid the model\noverfitting to the training set.\nA set of fake images are sampled from the buffer.\nThe real and fake images are run through the model to produce real and fake\nscores.\nThe contrastive divergence loss is simply the difference between the scores of real\nand fake observations.\nA regularization loss is added to avoid the scores becoming too large.\n200 | Chapter 7: Energy-Based Models",7412
78-Summary.pdf,78-Summary,"Gradients of the loss function with respect to the weights of the network are cal‐\nculated for backpropagation.\nThe test_step  is used during validation and calculates the contrastive diver‐\ngence between the scores of a set of random noise and data from the training set.\nIt can be used as a measure for how well the model is training (see the following\nsection).\nAnalysis of the Energy-Based Model\nThe loss curves and supporting metrics from the training process are shown in\nFigure 7-6 .\nFigure 7-6. Loss curves and metrics for the training process of the EBM\nFirstly, notice that the loss calculated during the training step is approximately con‐\nstant and small across epochs. While the model is constantly improving, so is the\nquality of generated images in the buffer that it is required to compare against real\nimages from the training set, so we shouldn’t expect the training loss to fall\nsignificantly.\nTherefore, to judge model performance, we also set up a validation process that\ndoesn’t sample from the buffer, but instead scores a sample of random noise and\ncompares this against the scores of examples from the training set. If the model is\nimproving, we should see that the contrastive divergence falls over the epochs (i.e., it\nis getting better at distinguishing random noise from real images), as can be seen in\nFigure 7-6 .\nEnergy-Based Models | 201\nGenerating new samples from the EBM is simply a case of running the Langevin sam‐\npler for a large number of steps, from a standing start (random noise), as shown in\nExample 7-7 . The observation is forced downhill , following the gradients of the scor‐\ning function with respect to the input, so that out of the noise, a plausible observation\nappears.\nExample 7-7. Generating new observations using the EBM\nstart_imgs  = np.random.uniform(size = (10, 32, 32, 1)) * 2 - 1\ngen_img = generate_samples (\n    ebm.model,\n    start_imgs ,\n    steps=1000,\n    step_size =10,\n    noise = 0.005,\n    return_img_per_step =True,\n)\nSome examples of observations produced by the sampler after 50 epochs of training\nare shown in Figure 7-7 .\nFigure 7-7. Examples produced by the Langevin sampler using the EBM model to direct\nthe gradient descent\nWe can even show a replay of how a single observation is generated by taking snap‐\nshots of the current observations during the Langevin sampling process—this is\nshown in Figure 7-8 .\nFigure 7-8. Snapshots of an observation at different  steps of the Langevin sampling\nprocess\nOther Energy-Based Models\nIn the previous example we made use of a deep EBM trained using contrastive diver‐\ngence with a Langevin dynamics sampler. However, early EBM models did not make\nuse of Langevin sampling, but instead relied on other techniques and architectures.\n202 | Chapter 7: Energy-Based Models\nOne  of the earliest examples of an EBM was the Boltzmann machine .6 This is a fully\nconnected, undirected neural network, where binary units are either visible  (v) or hid‐\nden (h). The energy of a given configuration of the network is defined as follows:\nEθv,h= −1\n2vTLv+hTJh+vTWh\nwhere W,L,J are the weights matrices that are learned by the model. Training is\nachieved by contrastive divergence, but using Gibbs sampling to alternate between\nthe visible and hidden layers until an equilibrium is found. In practice this is very\nslow and not scalable to large numbers of hidden units.\nSee Jessica Stringham’s blog post “Gibbs Sampling in Python”  for an\nexcellent simple example of Gibbs sampling.\nAn extension to this model, the restricted Boltzmann machine  (RBM), removes the\nconnections between units of the same type, therefore creating a two-layer bipartite\ngraph. This allows RBMs to be stacked into deep belief networks  to model more com‐\nplex distributions. However, modeling high-dimensional data with RBMs remains\nimpractical, due to the fact that Gibbs sampling with long mixing times is still\nrequired.\nIt was only in the late 2000s that EBMs were shown to have potential for modeling\nmore high-dimensional datasets and a framework for building deep EBMs was estab‐\nlished. 7 Langevin dynamics became the preferred sampling method for EBMs, which\nlater evolved into a training technique known as score matching . This further devel‐\noped into a model class known as Denoising Diffusion  Probabilistic Models , which\npower state-of-the-art generative models such as DALL.E 2 and ImageGen. We will\nexplore diffusion models in more detail in Chapter 8 .\nSummary\nEnergy-based models are a class of generative model that make use of an energy scor‐\ning function—a neural network that is trained to output low scores for real observa‐\ntions and high scores for generated observations. Calculating the probability\ndistribution given by this score function would require normalizing by an intractable\ndenominator. EBMs avoid this problem by utilizing two tricks: contrastive divergence\nfor training the network and Langevin dynamics for sampling new observations.\nThe energy function is trained by minimizing the difference between the generated\nsample scores and the scores of the training data, a technique known as contrastive\nSummary | 203\ndivergence. This can be shown to be equivalent to minimizing the negative log-\nlikelihood, as required by maximum likelihood estimation, but does not require us to\ncalculate the intractable normalizing denominator. In practice, we approximate the\nsampling process for the fake samples to ensure the algorithm remains efficient.\nSampling of deep EBMs is achieved through Langevin dynamics, a technique that\nuses the gradient of the score with respect to the input image to gradually transform\nrandom noise into a plausible observation by updating the input in small steps, fol‐\nlowing the gradient downhill. This improves upon earlier methods such as Gibbs\nsampling, which is utilized by restricted Boltzmann machines.\nReferences\n1. Yilun Du and Igor Mordatch, “Implicit Generation and Modeling with Energy-\nBased Models, ” March 20, 2019, https://arxiv.org/abs/1903.08689 .\n2. Prajit Ramachandran et al., “Searching for Activation Functions, ” October 16, 2017,\nhttps://arxiv.org/abs/1710.05941v2 .\n3. Max Welling and Y ee Whye Teh, “Bayesian Learning via Stochastic Gradient Lan‐\ngevin Dynamics, ” 2011, https://www.stats.ox.ac.uk/~teh/research/compstats/\nWelTeh2011a.pdf\n4. Geoffrey E. Hinton, “Training Products of Experts by Minimizing Contrastive\nDivergence, ” 2002, https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf .\n5. Oliver Woodford, “Notes on Contrastive Divergence, ” 2006, https://\nwww.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf .\n6. David H. Ackley et al., “ A Learning Algorithm for Boltzmann Machines, ” 1985,\nCognitive Science  9(1), 147-165.\n7. Y ann Lecun et al., “ A Tutorial on Energy-Based Learning, ” 2006, https://\nwww.researchgate.net/publication/200744586_A_tutorial_on_energy-based_learning .\n204 | Chapter 7: Energy-Based Models",7010
79-Introduction.pdf,79-Introduction,"CHAPTER 8\nDiffusion  Models\nChapter Goals\nIn this chapter you will:\n•Learn the underlying principles and components that define a diffusion model.\n•See how the forward process is used to add noise to the training set of images.\n•Understand the reparameterization trick and why it is important.\n•Explore different forms of forward diffusion scheduling.\n•Understand the reverse diffusion process and how it relates to the forward nois‐\ning process.\n•Explore the architecture of the U-Net, which is used to parameterize the reverse\ndiffusion process.\n•Build your own denoising diffusion model (DDM) using Keras to generate\nimages of flowers.\n•Sample new images of flowers from your model.\n•Explore the effect of the number of diffusion steps on image quality and interpo‐\nlate between two images in the latent space.\nAlongside GANs, diffusion models  are one of the most influential and impactful gen‐\nerative modeling techniques for image generation to have been introduced over the\nlast decade. Across many benchmarks, diffusion models now outperform previously\nstate-of-the-art GANs and are quickly becoming the go-to choice for generative mod‐\neling practitioners, particularly for visual domains (e.g., OpenAI’s DALL.E 2 and\nGoogle’s ImageGen for text-to-image generation). Recently, there has been an\n205\nexplosion of diffusion models being applied across wide range of tasks, reminiscent of\nthe GAN proliferation that took place between 2017–2020.\nMany of the core ideas that underpin diffusion models share similarities with earlier\ntypes of generative models that we have already explored in this book (e.g., denoising\nautoencoders, energy-based models). Indeed, the name diffusion  takes inspiration\nfrom the well-studied property of thermodynamic diffusion: an important link was\nmade between this purely physical field and deep learning in 2015. 1\nImportant progress was also being made in the field of score-based generative mod‐\nels, 2,3 a branch of energy-based modeling that directly estimates the gradient of the\nlog distribution (also known as the score function) in order to train the model, as an\nalternative to using contrastive divergence. In particular, Y ang Song and Stefano\nErmon used multiple scales of noise perturbations applied to the raw data to ensure\nthe model—a noise conditional score network  (NCSN)—performs well on regions of\nlow data density.\nThe breakthrough diffusion model paper came in the summer of 2020. 4 Standing on\nthe shoulders of earlier works, the paper uncovers a deep connection between diffu‐\nsion models and score-based generative models, and the authors use this fact to train\na diffusion model that can rival GANs across several datasets, called the Denoising\nDiffusion  Probabilistic Model  (DDPM).\nThis chapter will walk through the theoretical requirements for understanding how a\ndenoising diffusion model works. Y ou will then learn how to build your own denois‐\ning diffusion model using Keras.\nIntroduction\nTo help explain the key ideas that underpin diffusion models, let’s begin with a short\nstory!\nDiffuseTV\nY ou are standing in an electronics store that sells television sets. However, this store is\nclearly very different from ones you have visited in the past. Instead of a wide variety\nof different brands, there are hundreds of identical copies of the same TV connected\ntogether in sequence, stretching into the back of the shop as far as you can see. What’s\nmore, the first few TV sets appear to be showing nothing but random static noise\n(Figure 8-1 ).\nThe shopkeeper comes over to ask if you need assistance. Confused, you ask her\nabout the odd setup. She explains that this is the new DiffuseTV model that is set to\nrevolutionize the entertainment industry and immediately starts telling you how it\nworks, while walking deeper into the shop, alongside the line of TVs.\n206 | Chapter 8: Diffusion  Models\nFigure 8-1. A long line of connected television sets stretching out along an aisle of a shop\n(created with Midjourney )\nShe explains that during the manufacturing process, the DiffuseTV is exposed to\nthousands of images of previous TV shows—but each of those images has been grad‐\nually corrupted with random static, until it is indistinguishable from pure random\nnoise. The TVs are then designed to undo  the random noise, in small steps, essentially\ntrying to predict what the images looked like before the noise was added. Y ou can see\nthat as you walk further into the shop the images on each television set are indeed\nslightly clearer than the last.\nY ou eventually reach the end of the long line of televisions, where you can see a per‐\nfect picture on the last set. While this is certainly clever technology, you are curious to\nunderstand how this is useful to the viewer. The shopkeeper continues with her\nexplanation.\nInstead of choosing a channel to watch, the viewer chooses a random initial configu‐\nration of static. Every configuration will lead to a different output image, and in some\nmodels can even be guided by a text prompt that you choose to input. Unlike a nor‐\nmal TV , with a limited range of channels to watch, the DiffuseTV gives the viewer\nunlimited choice and freedom to generate whatever they would like to appear on the\nscreen!\nY ou purchase a DiffuseTV right away and are relieved to hear that the long line of\nTVs in the shop is for demonstration purposes only, so you won’t have to also buy a\nwarehouse to store your new device!\nThe DiffuseTV story describes the general idea behind a diffusion model. Now let’s\ndive into the technicalities of how we build such a model using Keras.\nIntroduction | 207",5696
80-Denoising Diffusion Models DDM.pdf,80-Denoising Diffusion Models DDM,,0
81-Diffusion Schedules.pdf,81-Diffusion Schedules,"Denoising Diffusion  Models (DDM)\nThe core idea behind a denoising diffusion model is simple—we train a deep learning\nmodel to denoise an image over a series of very small steps. If we start from pure ran‐\ndom noise, in theory we should be able to keep applying the model until we obtain an\nimage that looks as if it were drawn from the training set. What’s amazing is that this\nsimple concept works so well in practice!\nLet’s first get set up with a dataset and then walk through the forward (noising) and\nbackward (denoising) diffusion processes.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/08_diffusion/01_ddm/ddm.ipynb  in the book\nrepository.\nThe code is adapted from the excellent tutorial on denoising diffu‐\nsion implicit models  created by András Béres available on the\nKeras website.\nThe Flowers Dataset\nWe’ll  be using the Oxford 102 Flower dataset  that is available through Kaggle. This is\na set of over 8,000 color images of a variety of flowers.\nY ou can download the dataset by running the Kaggle dataset downloader script in the\nbook repository, as shown in Example 8-1 . This will save the flower images to\nthe /data  folder.\nExample 8-1. Downloading the Oxford 102 Flower dataset\nbash scripts/download_kaggle_data.sh  nunenuh pytorch-challange-flower-dataset\nAs usual, we’ll load the images in using the Keras image_dataset_from_directory\nfunction, resize the images to 64 × 64 pixels, and scale the pixel values to the range [0,\n1]. We’ll also repeat the dataset five times to increase the epoch length and batch the\ndata into groups of 64 images, as shown in Example 8-2 .\nExample 8-2. Loading the Oxford 102 Flower dataset\ntrain_data  = utils.image_dataset_from_directory (\n    ""/app/data/pytorch-challange-flower-dataset/dataset"" ,\n    labels=None,\n    image_size =(64, 64),\n    batch_size =None,\n    shuffle=True,\n208 | Chapter 8: Diffusion  Models\n    seed=42,\n    interpolation =""bilinear"" ,\n) \ndef preprocess (img):\n    img = tf.cast(img, ""float32"" ) / 255.0\n    return img\ntrain = train_data .map(lambda x: preprocess (x)) \ntrain = train.repeat(5) \ntrain = train.batch(64, drop_remainder =True) \nLoad dataset (when required during training) using the Keras image_data\nset_from_directory  function.\nScale the pixel values to the range [0, 1].\nRepeat the dataset five times.\nBatch the dataset into groups of 64 images.\nExample images from the dataset are shown in Figure 8-2 .\nFigure 8-2. Example images from the Oxford 102 Flower dataset\nNow that we have our dataset we can explore how we should add noise to the images,\nusing a forward diffusion process.\nThe Forward Diffusion  Process\nSuppose  we have an image 0 that we want to corrupt gradually over a large number\nof steps (say, T= 1, 000 ), so that eventually it is indistinguishable from standard\nGaussian noise (i.e., T should have zero mean and unit variance). How should we go\nabout doing this?\nDenoising Diffusion  Models (DDM) | 209\nWe can define a function q that adds a small amount of Gaussian noise with variance\nβt to an image t− 1 to generate a new image t. If we keep applying this function, we\nwill generate a sequence of progressively noisier images ( 0, ...,T), as shown in\nFigure 8-3 .\nFigure 8-3. The forward diffusion  process q\nWe can write this update process mathematically as follows (here, t− 1 is a standard\nGaussian with zero mean and unit variance):\nt=1 −βtt− 1+βtt− 1\nNote that we also scale the input image t− 1, to ensure that the variance of the output\nimage t remains constant over time. This way, if we normalize our original image 0\nto have zero mean and unit variance, then T will approximate a standard Gaussian\ndistribution for large enough T, by induction, as follows.\nIf we assume that t− 1 has zero mean and unit variance then 1 −βtt− 1 will have\nvariance 1 −βt and βtt− 1 will have variance βt, using the rule that\nVar aX =a2Var X. Adding these together, we obtain a new distribution t with\nzero mean and variance 1 −βt+βt= 1, using the rule that\nVar X+Y=Var X+Var Y for independent X and Y. Therefore, if 0 is normal‐\nized to a zero mean and unit variance, then we guarantee that this is also true for all\nt, including the final image T, which will approximate a standard Gaussian distri‐\nbution. This is exactly what we need, as we want to be able to easily sample T and\nthen apply a reverse diffusion process through our trained neural network model!\nIn other words, our forward noising process q can also be written as follows:\nqtt− 1=t;1 −βtt− 1,βt\nThe Reparameterization Trick\nIt would also be useful to be able to jump straight from an image 0 to any noised\nversion of the image t without having to go through t applications of q. Luckily,\nthere is a reparameterization trick that we can use to do this.\n210 | Chapter 8: Diffusion  Models\nIf we define αt= 1 − βt and αt= ∏i= 1tαi, then we can write the following:\nt=αtt− 1+1 −αtt− 1\n=αtαt− 1t− 2+1 −αtαt− 1\n=⋯\n=αt0+1 −αt\nNote that the second line uses the fact that we can add two Gaussians to obtain a new\nGaussian. We therefore have a way to jump from the original image 0 to any step of\nthe forward diffusion process t. Moreover, we can define the diffusion schedule\nusing the αt values, instead of the original βt values, with the interpretation that αt is\nthe variance due to the signal (the original image, 0) and 1 −αt is the variance due to\nthe noise ( ).\nThe forward diffusion process q can therefore also be written as follows:\nqt0=t;αt0,1 −αt\nDiffusion  Schedules\nNotice  that we are also free to choose a different βt at each timestep—they don’t all\nhave be the same. How the βt (or αt) values change with t is called the diffusion\nschedule .\nIn the original paper (Ho et al., 2020), the authors chose a  linear diffusion  schedule  for\nβt—that is, βt increases linearly with t, from β1= 0.0001 to βT= 0.02. This ensures\nthat in the early stages of the noising process we take smaller noising steps than in the\nlater stages, when the image is already very noisy.\nWe can code up a linear diffusion schedule as shown in Example 8-3 .\nExample 8-3. The linear diffusion  schedule\ndef linear_diffusion_schedule (diffusion_times ):\n    min_rate  = 0.0001\n    max_rate  = 0.02\n    betas = min_rate  + tf.convert_to_tensor (diffusion_times ) * (max_rate  - min_rate )\n    alphas = 1 - betas\n    alpha_bars  = tf.math.cumprod(alphas)\n    signal_rates  = alpha_bars\n    noise_rates  = 1 - alpha_bars\n    return noise_rates , signal_rates\nDenoising Diffusion  Models (DDM) | 211\nT = 1000\ndiffusion_times  = [x/T for x in range(T)] \nlinear_noise_rates , linear_signal_rates  = linear_diffusion_schedule (\n    diffusion_times\n) \nThe diffusion times are equally spaced steps between 0 and 1.\nThe linear diffusion schedule is applied to the diffusion times to produce the\nnoise and signal rates.\nIn a later paper it was found that a cosine diffusion  schedule  outperformed the linear\nschedule from the original paper. 5 A cosine schedule defines the following values of\nαt:\nαt= cos2t\nT·π\n2\nThe updated equation is therefore as follows (using the trigonometric identity\ncos2x+ sin2x= 1):\nt= cost\nT·π\n20+ sint\nT·π\n2\nThis equation is a simplified version of the actual cosine diffusion schedule used in\nthe paper. The authors also add an offset term and scaling to prevent the noising steps\nfrom being too small at the beginning of the diffusion process. We can code up the\ncosine and offset cosine diffusion schedules as shown in Example 8-4 .\nExample 8-4. The cosine and offset  cosine diffusion  schedules\ndef cosine_diffusion_schedule (diffusion_times ): \n    signal_rates  = tf.cos(diffusion_times  * math.pi / 2)\n    noise_rates  = tf.sin(diffusion_times  * math.pi / 2)\n    return noise_rates , signal_rates\ndef offset_cosine_diffusion_schedule (diffusion_times ): \n    min_signal_rate  = 0.02\n    max_signal_rate  = 0.95\n    start_angle  = tf.acos(max_signal_rate )\n    end_angle  = tf.acos(min_signal_rate )\n    diffusion_angles  = start_angle  + diffusion_times  * (end_angle  - start_angle )\n    signal_rates  = tf.cos(diffusion_angles )\n    noise_rates  = tf.sin(diffusion_angles )\n212 | Chapter 8: Diffusion  Models\n    return noise_rates , signal_rates\nThe pure cosine diffusion schedule (without offset or rescaling).\nThe offset cosine diffusion schedule that we will be using, which adjusts the\nschedule to ensure the noising steps are not too small at the start of the noising\nprocess.\nWe can compute the αt values for each t to show how much signal ( αt) and noise\n(1 −αt) is let through at each stage of the process for the linear, cosine, and offset\ncosine diffusion schedules, as shown in Figure 8-4 .\nFigure 8-4. The signal and noise at each step of the noising process, for the linear, cosine,\nand offset  cosine diffusion  schedules\nNotice how the noise level ramps up more slowly in the cosine diffusion schedule. A\ncosine diffusion schedule adds noise to the image more gradually than a linear diffu‐\nsion schedule, which improves training efficiency and generation quality. This can\nalso be seen in images that have been corrupted by the linear and cosine schedules\n(Figure 8-5 ).\nFigure 8-5. An image being corrupted by the linear (top) and cosine (bottom) diffusion\nschedules, at equally spaced values of t from 0 to T (source: Ho et al., 2020 )\nDenoising Diffusion  Models (DDM) | 213",9540
82-The Reverse Diffusion Process.pdf,82-The Reverse Diffusion Process,"The Reverse Diffusion  Process\nNow  let’s look at the reverse diffusion process. To recap, we are looking to build a\nneural network pθt− 1t that can undo  the noising process—that is, approximate\nthe reverse distribution qt− 1t. If we can do this, we can sample random noise\nfrom 0, and then apply the reverse diffusion process multiple times in order to\ngenerate a novel image. This is visualized in Figure 8-6 .\nFigure 8-6. The reverse diffusion  process pθ.t− 1t tries to undo the noise produced\nby the forward diffusion  process\nThere are many similarities between the reverse diffusion process and the decoder of\na variational autoencoder. In both, we aim to transform random noise into meaning‐\nful output using a neural network. The difference between diffusion models and\nV AEs is that in a V AE the forward process (converting images to noise) is part of the\nmodel (i.e., it is learned), whereas in a diffusion model it is unparameterized.\nTherefore, it makes sense to apply the same loss function as in a variational autoen‐\ncoder. The original DDPM paper derives the exact form of this loss function and\nshows that it can be optimized by training a network θ to predict the noise  that has\nbeen added to a given image 0 at timestep t.\nIn other words, we sample an image 0 and transform it by t noising steps to get the\nimage t=αt0+1 −αt. We give this new image and the noising rate αt to the\nneural network and ask it to predict , taking a gradient step against the squared error\nbetween the prediction θt and the true .\nWe’ll take a look at the structure of the neural network in the next section. It is worth\nnoting here that the diffusion model actually maintains two copies of the network:\none that is actively trained used gradient descent and another (the EMA network)\nthat is an exponential moving average (EMA) of the weights of the actively trained\nnetwork over previous training steps. The EMA network is not as susceptible to\nshort-term fluctuations and spikes in the training process, making it more robust for\ngeneration than the actively trained network. We therefore use the EMA network\nwhenever we want to produce generated output from the network.\nThe training process for the model is shown in Figure 8-7 .\n214 | Chapter 8: Diffusion  Models\nFigure 8-7. The training process for a denoising diffusion  model (source: Ho et al., 2020 )\nIn Keras, we can code up this training step as illustrated in Example 8-5 .\nExample 8-5. The train_step  function of the Keras diffusion  model\nclass DiffusionModel (models.Model):\n    def __init__ (self):\n        super().__init__ ()\n        self.normalizer  = layers.Normalization ()\n        self.network = unet\n        self.ema_network  = models.clone_model (self.network)\n        self.diffusion_schedule  = cosine_diffusion_schedule\n    ...\n    def denoise(self, noisy_images , noise_rates , signal_rates , training ):\n        if training :\n            network = self.network\n        else:\n            network = self.ema_network\n        pred_noises  = network(\n            [noisy_images , noise_rates **2], training =training\n        )\n        pred_images  = (noisy_images  - noise_rates  * pred_noises ) / signal_rates\n        return pred_noises , pred_images\n    def train_step (self, images):\n        images = self.normalizer (images, training =True) \n        noises = tf.random.normal(shape=tf.shape(images)) \n        batch_size  = tf.shape(images)[0]\n        diffusion_times  = tf.random.uniform(\n            shape=(batch_size , 1, 1, 1), minval=0.0, maxval=1.0\n        ) \n        noise_rates , signal_rates  = self.cosine_diffusion_schedule (\n            diffusion_times\n        ) \n        noisy_images  = signal_rates  * images + noise_rates  * noises \nDenoising Diffusion  Models (DDM) | 215\n        with tf.GradientTape () as tape:\n            pred_noises , pred_images  = self.denoise(\n                noisy_images , noise_rates , signal_rates , training =True\n            ) \n            noise_loss  = self.loss(noises, pred_noises )  \n        gradients  = tape.gradient (noise_loss , self.network.trainable_weights )\n        self.optimizer .apply_gradients (\n            zip(gradients , self.network.trainable_weights )\n        ) \n        self.noise_loss_tracker .update_state (noise_loss )\n        for weight, ema_weight  in zip(\n            self.network.weights, self.ema_network .weights\n        ):\n            ema_weight .assign(0.999 * ema_weight  + (1 - 0.999) * weight) \n        return {m.name: m.result() for m in self.metrics}\n    ...\nWe first normalize the batch of images to have zero mean and unit variance.\nNext, we sample noise to match the shape of the input images.\nWe also sample random diffusion times…\n…and use these to generate the noise and signal rates according to the cosine dif‐\nfusion schedule.\nThen we apply the signal and noise weightings to the input images to generate\nthe noisy images.\nNext, we denoise the noisy images by asking the network to predict the noise and\nthen undoing the noising operation, using the provided noise_rates  and\nsignal_rates .\nWe can then calculate the loss (mean absolute error) between the predicted noise\nand the true noise…\n…and take a gradient step against this loss function.\nThe EMA network weights are updated to a weighted average of the existing\nEMA weights and the trained network weights after the gradient step.\n216 | Chapter 8: Diffusion  Models",5490
83-The U-Net Denoising Model.pdf,83-The U-Net Denoising Model,"The U-Net Denoising Model\nNow  that we have seen the kind of neural network that we need to build (one that\npredicts the noise added to a given image), we can look at the architecture that makes\nthis possible.\nThe authors of the DDPM paper used a type of architecture known as a U-Net . A dia‐\ngram of this network is shown in Figure 8-8 , explicitly showing the shape of the ten‐\nsor as it passes through the network.\nFigure 8-8. U-Net architecture diagram\nIn a similar manner to a variational autoencoder, a U-Net consists of two halves: the\ndownsampling half, where input images are compressed spatially but expanded\nchannel-wise, and the upsampling half, where representations are expanded spatially\nwhile the number of channels is reduced. However, unlike in a V AE, there are also\nDenoising Diffusion  Models (DDM) | 217\nskip connections  between equivalent spatially shaped layers in the upsampling and\ndownsampling parts of the network. A V AE is sequential; data flows through the net‐\nwork from input to output, one layer after another. A U-Net is different, because the\nskip connections allow information to shortcut parts of the network and flow\nthrough to later layers.\nA U-Net is particularly useful when we want the output to have the same shape as the\ninput. In our diffusion model example, we want to predict the noise added to an\nimage, which has exactly the same shape as the image itself, so a U-Net is the natural\nchoice for the network architecture.\nFirst let’s take a look at the code that builds this U-Net in Keras, shown in\nExample 8-6 .\nExample 8-6. A U-Net model in Keras\nnoisy_images  = layers.Input(shape=(64, 64, 3)) \nx = layers.Conv2D(32, kernel_size =1)(noisy_images ) \nnoise_variances  = layers.Input(shape=(1, 1, 1)) \nnoise_embedding  = layers.Lambda(sinusoidal_embedding )(noise_variances ) \nnoise_embedding  = layers.UpSampling2D (size=64, interpolation =""nearest"" )(\n    noise_embedding\n) \nx = layers.Concatenate ()([x, noise_embedding ]) \nskips = [] \nx = DownBlock (32, block_depth  = 2)([x, skips]) \nx = DownBlock (64, block_depth  = 2)([x, skips])\nx = DownBlock (96, block_depth  = 2)([x, skips])\nx = ResidualBlock (128)(x) \nx = ResidualBlock (128)(x)\nx = UpBlock(96, block_depth  = 2)([x, skips]) \nx = UpBlock(64, block_depth  = 2)([x, skips])\nx = UpBlock(32, block_depth  = 2)([x, skips])\nx = layers.Conv2D(3, kernel_size =1, kernel_initializer =""zeros"")(x) \nunet = models.Model([noisy_images , noise_variances ], x, name=""unet"") \nThe first input to the U-Net is the image that we wish to denoise.\nThis image is passed through a Conv2D  layer to increase the number of channels.\n218 | Chapter 8: Diffusion  Models\nThe second input to the U-Net is the noise variance (a scalar).\nThis is encoded using a sinusoidal embedding.\nThis embedding is copied across spatial dimensions to match the size of the input\nimage.\nThe two input streams are concatenated across channels.\nThe skips  list will hold the output from the DownBlock  layers that we wish to\nconnect to UpBlock  layers downstream.\nThe tensor is passed through a series of DownBlock  layers that reduce the size of\nthe image, while increasing the number of channels.\nThe tensor is then passed through two ResidualBlock  layers that hold the image\nsize and number of channels constant.\nNext, the tensor is passed through a series of UpBlock  layers that increase the size\nof the image, while decreasing the number of channels. The skip connections\nincorporate output from the earlier DownBlock  layers.\nThe final Conv2D  layer reduces the number of channels to three (RGB).\nThe U-Net is a Keras Model  that takes the noisy images and noise variances as\ninput and outputs a predicted noise map.\nTo understand the U-Net in detail, we need to explore four more concepts: the sinus‐\noidal embedding of the noise variance, the ResidualBlock , the DownBlock , and the\nUpBlock .\nSinusoidal embedding\nSinusoidal embedding  was first introduced in a paper by Vaswani et al. 6 We will be\nusing an adaptation of that original idea as utilized in Mildenhall et al. ’s paper titled\n“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ” 7\nThe idea is that we want to be able to convert a scalar value (the noise variance) into a\ndistinct higher-dimensional vector that is able to provide a more complex representa‐\ntion, for use downstream in the network. The original paper used this idea to encode\nthe discrete position of words in a sentence into vectors; the NeRF paper extends this\nidea to continuous values.\nDenoising Diffusion  Models (DDM) | 219\nSpecifically, a scalar value x is encoded as shown in the following equation:\nγx=sin 2πe0fx,⋯, sin 2πeL− 1 fx, cos 2πe0fx,⋯, cos 2πeL− 1 fx\nwhere we choose L= 16  to be half the size of our desired noise embedding length and\nf=ln1000\nL− 1 to be the maximum scaling factor for the frequencies.\nThis produces the embedding pattern shown in Figure 8-9 .\nFigure 8-9. The pattern of sinusoidal embeddings for noise variances from 0 to 1\nWe can code this sinusoidal embedding function as shown in Example 8-7 . This con‐\nverts a single noise variance scalar value into a vector of length 32.\nExample 8-7. The sinusoidal_embedding  function that encodes the noise variance\ndef sinusoidal_embedding (x):\n    frequencies  = tf.exp(\n        tf.linspace (\n            tf.math.log(1.0),\n            tf.math.log(1000.0),\n            16,\n        )\n    )\n    angular_speeds  = 2.0 * math.pi * frequencies\n    embeddings  = tf.concat(\n        [tf.sin(angular_speeds  * x), tf.cos(angular_speeds  * x)], axis=3\n    )\n    return embeddings\n220 | Chapter 8: Diffusion  Models\nResidualBlock\nBoth the DownBlock  and the UpBlock  contain ResidualBlock  layers, so let’s start with\nthese. We already explored residual blocks in Chapter 5 , when we built a PixelCNN,\nbut we will recap here for completeness.\nA residual block  is a group of layers that contains a skip connection that adds the\ninput to the output. Residual blocks help us to build deeper networks that can learn\nmore complex patterns without suffering as greatly from vanishing gradient and deg‐\nradation problems. The vanishing gradient problem is the assertion that as the net‐\nwork gets deeper, the gradient propagated through deeper layers is tiny and therefore\nlearning is very slow. The degradation problem is the fact that as neural networks\nbecome deeper, they are not necessarily as accurate as their shallower counterparts—\naccuracy seems to become saturated at a certain depth and then degrade rapidly.\nDegradation\nThe degradation problem is somewhat counterintuitive, but\nobserved in practice as the deeper layers must at least learn the\nidentity mapping, which is not trivial—especially considering other\nproblems deeper networks face, such as the vanishing gradient\nproblem.\nThe solution, first introduced in the ResNet paper by He et al. in 2015, 8 is very sim‐\nple. By including a skip connection highway  around the main weighted layers, the\nblock has the option to bypass the complex weight updates and simply pass through\nthe identity mapping. This allows the network to be trained to great depth without\nsacrificing gradient size or network accuracy.\nA diagram of a ResidualBlock  is shown in Figure 8-10 . Note that in some residual\nblocks, we also include an extra Conv2D  layer with kernel size 1 on the skip connec‐\ntion, to bring the number of channels in line with the rest of the block.\nFigure 8-10. The ResidualBlock  in the U-Net\nDenoising Diffusion  Models (DDM) | 221\nWe can code a ResidualBlock  in Keras as shown in Example 8-8 .\nExample 8-8. Code for the ResidualBlock  in the U-Net\ndef ResidualBlock (width):\n    def apply(x):\n        input_width  = x.shape[3]\n        if input_width  == width: \n            residual  = x\n        else:\n            residual  = layers.Conv2D(width, kernel_size =1)(x)\n        x = layers.BatchNormalization (center=False, scale=False)(x) \n        x = layers.Conv2D(\n            width, kernel_size =3, padding=""same"", activation =activations .swish\n        )(x) \n        x = layers.Conv2D(width, kernel_size =3, padding=""same"")(x)\n        x = layers.Add()([x, residual ]) \n        return x\n    return apply\nCheck if the number of channels in the input matches the number of channels\nthat we would like the block to output. If not, include an extra Conv2D  layer on\nthe skip connection to bring the number of channels in line with the rest of the\nblock.\nApply a BatchNormalization  layer.\nApply two Conv2D  layers.\nAdd the original block input to the output to provide the final output from the\nblock.\nDownBlocks and UpBlocks\nEach successive DownBlock  increases the number of channels via block_depth  (=2 in\nour example) ResidualBlock s, while also applying a final AveragePooling2D  layer in\norder to halve the size of the image. Each ResidualBlock  is added to a list for use\nlater by the UpBlock  layers as skip connections across the U-Net.\nAn UpBlock  first applies an UpSampling2D  layer that doubles the size of the image,\nthrough bilinear interpolation. Each successive UpBlock  decreases the number of\nchannels via block_depth  (=2) ResidualBlock s, while also concatenating the outputs\nfrom the DownBlock s through skip connections across the U-Net. A diagram of this\nprocess is shown in Figure 8-11 .\n222 | Chapter 8: Diffusion  Models\nFigure 8-11. The DownBlock  and corresponding UpBlock  in the U-Net\nWe can code the DownBlock  and UpBlock  using Keras as illustrated in Example 8-9 .\nExample 8-9. Code for the DownBlock  and UpBlock  in the U-Net model\ndef DownBlock (width, block_depth ):\n    def apply(x):\n        x, skips = x\n        for _ in range(block_depth ):\n            x = ResidualBlock (width)(x) \n            skips.append(x) \n        x = layers.AveragePooling2D (pool_size =2)(x) \n        return x\n    return apply\ndef UpBlock(width, block_depth ):\n    def apply(x):\n        x, skips = x\n        x = layers.UpSampling2D (size=2, interpolation =""bilinear"" )(x) \n        for _ in range(block_depth ):\n            x = layers.Concatenate ()([x, skips.pop()]) \n            x = ResidualBlock (width)(x) \n        return x\n    return apply\nDenoising Diffusion  Models (DDM) | 223",10351
84-Sampling from the Denoising Diffusion Model.pdf,84-Sampling from the Denoising Diffusion Model,"The DownBlock  increases the number of channels in the image using a Residual\nBlock  of a given width …\n…each of which are saved to a list ( skips ) for use later by the UpBlock s.\nA final AveragePooling2D  layer reduces the dimensionality of the image by half.\nThe UpBlock  begins with an UpSampling2D  layer that doubles the size of the\nimage.\nThe output from a DownBlock  layer is glued to the current output using a\nConcatenate  layer.\nA ResidualBlock  is used to reduce the number of channels in the image as it\npasses through the UpBlock .\nTraining the Diffusion  Model\nWe now have all the components in place to train our denoising diffusion model!\nExample 8-10  creates, compiles, and fits the diffusion model.\nExample 8-10. Code for training the DiffusionModel\nmodel = DiffusionModel () \nmodel.compile(\n    optimizer =optimizers .experimental .AdamW(learning_rate =1e-3, weight_decay =1e-4),\n    loss=losses.mean_absolute_error ,\n) \nmodel.normalizer .adapt(train) \nmodel.fit(\n    train,\n    epochs=50,\n) \nInstantiate the model.\nCompile the model, using the AdamW optimizer (similar to Adam but with\nweight decay, which helps stabilize the training process) and mean absolute error\nloss function.\nCalculate the normalization statistics using the training set.\nFit the model over 50 epochs.\n224 | Chapter 8: Diffusion  Models\nThe loss curve (noise mean absolute error [MAE]) is shown in Figure 8-12 .\nFigure 8-12. The noise mean absolute error loss curve, by epoch\nSampling from the Denoising Diffusion  Model\nIn order to sample images from our trained model, we need to apply the reverse dif‐\nfusion process—that is, we need to start with random noise and use the model to\ngradually undo the noise, until we are left with a recognizable picture of a flower.\nWe must bear in mind that our model is trained to predict the total amount of noise\nthat has been added to a given noisy image from the training set, not just the noise\nthat was added at the last timestep of the noising process. However, we do not want to\nundo the noise all in one go—predicting an image from pure random noise in one\nshot is clearly not going to work! We would rather mimic the forward process and\nundo the predicted noise gradually over many small steps, to allow the model to\nadjust to its own predictions.\nTo achieve this, we can jump from xt to xt− 1 in two steps—first by using our model’s\nnoise prediction to calculate an estimate for the original image x0 and then by reap‐\nplying the predicted noise to this image, but only over t− 1 timesteps, to produce\nxt− 1. This idea is shown in Figure 8-13 .\nDenoising Diffusion  Models (DDM) | 225\nFigure 8-13. One step of the sampling process for our diffusion  model\nIf we repeat this process over a number of steps, we’ll eventually get back to an esti‐\nmate for x0 that has been guided gradually over many small steps. In fact, we are free\nto choose the number of steps we take, and crucially, it doesn’t have to be the same as\nthe large number of steps in the training noising process (i.e., 1,000). It can be much\nsmaller—in this example we choose 20.\nThe following equation (Song et al., 2020) this process mathematically:\nt− 1=αt− 1t−1 −αtθtt\nαt\npredicted 0+1 −αt− 1−σt2·θtt\ndirection pointing to t+ σtt\nrandom noise\nLet’s break this down. The first term inside the brackets on the righthand side of the\nequation is the estimated image x0, calculated using the noise predicted by our net‐\nwork θt. We then scale this by the t− 1 signal rate αt− 1 and reapply the predicted\nnoise, but this time scaled by the t− 1 noise rate 1 −αt− 1−σt2. Additional Gaussian\nrandom noise σtt is also added, with the factors σt determining how random we\nwant our generation process to be.\nThe special case σt= 0 for all t corresponds to a type of model known as a Denoising\nDiffusion  Implicit Model  (DDIM), introduced by Song et al. in 2020. 9 With a DDIM,\nthe generation process is entirely deterministic—that is, the same random noise input\nwill always give the same output. This is desirable as then we have a well-defined\nmapping between samples from the latent space and the generated outputs in pixel\nspace.\nIn our example, we will implement a DDIM, thus making our generation process\ndeterministic. The code for the DDIM sampling process (reverse diffusion) is shown\nin Example 8-11 .\n226 | Chapter 8: Diffusion  Models\nExample 8-11. Sampling from the diffusion  model\nclass DiffusionModel (models.Model):\n...\n    def reverse_diffusion (self, initial_noise , diffusion_steps ):\n        num_images  = initial_noise .shape[0]\n        step_size  = 1.0 / diffusion_steps\n        current_images  = initial_noise\n        for step in range(diffusion_steps ): \n            diffusion_times  = tf.ones((num_images , 1, 1, 1)) - step * step_size  \n            noise_rates , signal_rates  = self.diffusion_schedule (diffusion_times ) \n            pred_noises , pred_images  = self.denoise(\n                current_images , noise_rates , signal_rates , training =False\n            ) \n            next_diffusion_times  = diffusion_times  - step_size  \n            next_noise_rates , next_signal_rates  = self.diffusion_schedule (\n                next_diffusion_times\n            ) \n            current_images  = (\n                next_signal_rates  * pred_images  + next_noise_rates  * pred_noises\n            ) \n        return pred_images  \nLook over a fixed number of steps (e.g., 20).\nThe diffusion times are all set to 1 (i.e., at the start of the reverse diffusion\nprocess).\nThe noise and signal rates are calculated according to the diffusion schedule.\nThe U-Net is used to predict the noise, allowing us to calculate the denoised\nimage estimate.\nThe diffusion times are reduced by one step.\nThe new noise and signal rates are calculated.\nThe t-1 images are calculated by reapplying the predicted noise to the predicted\nimage, according to the t-1 diffusion schedule rates.\nAfter 20 steps, the final 0 predicted images are returned.\nDenoising Diffusion  Models (DDM) | 227",6105
85-Analysis of the Diffusion Model.pdf,85-Analysis of the Diffusion Model,"Analysis of the Diffusion  Model\nWe’ll  now take a look at three different ways that we can use our trained model: for\ngeneration of new images, testing how the number of reverse diffusion steps affects\nquality, and interpolating between two images in the latent space.\nGenerating images\nIn order to produce samples from our trained model, we can simply run the reverse\ndiffusion process, ensuring that we denormalize the output at the end (i.e., take the\npixel values back into the range [0, 1]). We can achieve this using the code in\nExample 8-12  inside the DiffusionModel  class.\nExample 8-12. Generating images using the diffusion  model\nclass DiffusionModel (models.Model):\n...\n    def denormalize (self, images):\n        images = self.normalizer .mean + images * self.normalizer .variance **0.5 \n        return tf.clip_by_value (images, 0.0, 1.0)\n    def generate (self, num_images , diffusion_steps ):\n        initial_noise  = tf.random.normal(shape=(num_images , 64, 64, 3)) \n        generated_images  = self.reverse_diffusion (initial_noise , diffusion_steps ) \n        generated_images  = self.denormalize (generated_images ) \n        return generated_images\nGenerate some initial noise maps.\nApply the reverse diffusion process.\nThe images output by the network will have mean zero and unit variance, so we\nneed to denormalize by reapplying the mean and variance calculated from the\ntraining data.\n228 | Chapter 8: Diffusion  Models\nIn Figure 8-14  we can observe some samples from the diffusion model at different\nepochs of the training process.\nFigure 8-14. Samples from the diffusion  model at different  epochs of the training process\nAdjusting the number of diffusion  steps\nWe can also test to see how adjusting the number of diffusion steps in the reverse\nprocess affects image quality. Intuitively, the more steps taken by the process, the\nhigher the quality of the image generation.\nWe can see in Figure 8-15  that the quality of the generations does indeed improve\nwith the number of diffusion steps. With one giant leap from the initial sampled\nnoise, the model can only predict a hazy blob of color. With more steps, the model is\nable to refine and sharpen its generations. However, the time taken to generate the\nimages scales linearly with the number of diffusion steps, so there is a trade-off. There\nis minimal improvement between 20 and 100 diffusion steps, so we choose 20 as a\nreasonable compromise between quality and speed in this example.\nDenoising Diffusion  Models (DDM) | 229\nFigure 8-15. Image quality improves with the number of diffusion  steps\nInterpolating between images\nLastly, as we have seen previously with variational autoencoders, we can interpolate\nbetween points in the Gaussian latent space in order to smoothly transition between\nimages in pixel space. Here we choose to use a form of spherical interpolation that\nensures that the variance remains constant while blending the two Gaussian noise\nmaps together. Specifically, the initial noise map at each step is given by\nasinπ\n2t+bcosπ\n2t, where t ranges smoothly from 0 to 1 and a and b are the two\nrandomly sampled Gaussian noise tensors that we wish to interpolate between.\nThe resulting images are shown in Figure 8-16 .\n230 | Chapter 8: Diffusion  Models",3315
86-Summary.pdf,86-Summary,"Figure 8-16. Interpolating between images using the denoising diffusion  model\nSummary\nIn this chapter we have explored one of the most exciting and promising areas of gen‐\nerative modeling in recent times: diffusion models. In particular, we implemented the\nideas from a key paper on generative diffusion models (Ho et al., 2020) that intro‐\nduced the original Denoising Diffusion Probabilistic Model (DDPM). We then exten‐\nded this with the ideas from the Denoising Diffusion Implicit Model (DDIM) paper\nto make the generation process fully deterministic.\nWe have seen how diffusion models are formed of a forward diffusion process and a\nreverse diffusion process. The forward diffusion process adds noise to the training\ndata through a series of small steps, while the reverse diffusion process consists of a\nmodel that tries to predict the noise added.\nWe make use of a reparameterization trick in order to calculate the noised images at\nany step of the forward process without having to go through multiple noising steps.\nWe have seen how the chosen schedule of parameters used to add noise to the data\nplays an important part in the overall success of the model.\nThe reverse diffusion process is parameterized by a U-Net that tries to predict the\nnoise at each timestep, given the noised image and the noise rate at that step. A U-Net\nconsists of DownBlock s that increase the number of channels while reducing the size\nof the image and UpBlock s that decrease the number of channels while increasing the\nsize. The noise rate is encoded using sinusoidal embedding.\nSampling from the diffusion model is conducted over a series of steps. The U-Net is\nused to predict the noise added to a given noised image, which is then used to\nSummary | 231\ncalculate  an estimate for the original image. The predicted noise is then reapplied\nusing a smaller noise rate. This process is repeated over a series of steps (which may\nbe significantly smaller than the number of steps used during training), starting from\na random point sampled from a standard Gaussian noise distribution, to obtain the\nfinal generation.\nWe saw how increasing the number of diffusion steps in the reverse process improves\nthe image generation quality, at the expense of speed. We also performed latent space\narithmetic in order to interpolate between two images.\nReferences\n1. Jascha Sohl-Dickstein et al., “Deep Unsupervised Learning Using Nonequilibrium\nThermodynamics, ” March 12, 2015, https://arxiv.org/abs/1503.03585\n2. Y ang Song and Stefano Ermon, “Generative Modeling by Estimating Gradients of\nthe Data Distribution, ” July 12, 2019, https://arxiv.org/abs/1907.05600 .\n3. Y ang Song and Stefano Ermon, “Improved Techniques for Training Score-Based\nGenerative Models, ” June 16, 2020, https://arxiv.org/abs/2006.09011 .\n4. Jonathon Ho et al., “Denoising Diffusion Probabilistic Models, ” June 19, 2020,\nhttps://arxiv.org/abs/2006.11239 .\n5. Alex Nichol and Prafulla Dhariwal, “Improved Denoising Diffusion Probabilistic\nModels, ” February 18, 2021, https://arxiv.org/abs/2102.09672 .\n6. Ashish Vaswani et al., “ Attention Is All Y ou Need, ” June 12, 2017, https://\narxiv.org/abs/1706.03762 .\n7. Ben Mildenhall et al., “NeRF: Representing Scenes as Neural Radiance Fields for\nView Synthesis, ” March 1, 2020, https://arxiv.org/abs/2003.08934 .\n8. Kaiming He et al., “Deep Residual Learning for Image Recognition, ” December 10,\n2015, https://arxiv.org/abs/1512.03385 .\n9. Jiaming Song et al., “Denoising Diffusion Implicit Models, ” October 6, 2020,\nhttps://arxiv.org/abs/2010.02502\n232 | Chapter 8: Diffusion  Models",3646
87-Part III. Applications.pdf,87-Part III. Applications,"PART III\nApplications\nIn Part III , we will explore some of the key applications of the generative modeling\ntechniques that we have seen so far, across images, text, music, and games. We will\nalso see how these domains can be traversed using state-of-the-art multimodal\nmodels.\nIn Chapter 9  we shall turn our attention to Transformers, a start-of-the-art architec‐\nture that powers most modern-day text generation models. In particular, we shall\nexplore the inner workings of GPT and build our own version using Keras, and we’ll\nsee how it forms the foundation of tools such as ChatGPT.\nIn Chapter 10  we will look at some of the most important GAN architectures that\nhave influenced image generation, including ProGAN, StyleGAN, StyleGAN2,\nSAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN. We shall explore the key contribu‐\ntions of each and look to understand how the technique has evolved over time.\nChapter 11  looks at music generation, which presents additional challenges such as\nmodeling musical pitch and rhythm. We’ll see that many of the techniques that work\nfor text generation (such as Transformers) can also be applied in this domain, but\nwe’ll also explore a deep learning architecture known as MuseGAN that applies a\nGAN-based approach to generating music.\nChapter 12  shows how generative models can be used within other machine learning\ndomains, such as reinforcement learning. We will focus on the “World Models” paper,\nwhich shows how a generative model can be used as the environment in which the\nagent trains, allowing it to train within a hallucinated dream version of the environ‐\nment rather than the real thing.\nIn Chapter 13  we will explore state-of-the-art multimodal models that cross over\ndomains such as images and text. This includes text-to-image models such as\nDALL.E 2, Imagen, and Stable Diffusion, as well as visual language models such as\nFlamingo.\nFinally, Chapter 14  summarizes the generative AI journey so far, the current genera‐\ntive AI landscape, and where we may be heading in the future. We will explore how\ngenerative AI may change the way we live and work, as well as considering whether it\nhas the potential to unlock deeper forms of artificial intelligence in the years to come.",2252
88-Introduction.pdf,88-Introduction,"CHAPTER 9\nTransformers\nChapter Goals\nIn this chapter you will:\n•Learn about the origins of GPT, a powerful decoder Transformer model for text\ngeneration.\n•Learn conceptually how an attention mechanism mimics our way of attaching\nmore importance to some words in a sentence than others.\n•Delve into how the attention mechanism works from first principles, including\nhow queries, keys, and values are created and manipulated.\n•See the importance of causal masking for text generation tasks.\n•Understand how attention heads can be grouped into a multihead attention layer.\n•See how multihead attention layers form one part of a Transformer block that\nalso includes layer normalization and skip connections.\n•Create positional encodings that capture the position of each token as well as the\nword token embedding.\n•Build a GPT model in Keras to generate the text contained in wine reviews.\n•Analyze the output from the GPT model, including interrogating the attention\nscores to inspect where the model is looking.\n•Learn about the different types of Transformers, including examples of the types\nof tasks that can be tackled by each and descriptions of the most famous state-of-\nthe-art implementations.\n•Understand how encoder-decoder architectures work, like Google’s T5 model.\n•Explore the training process behind OpenAI’s ChatGPT.\n235",1358
89-Queries Keys and Values.pdf,89-Queries Keys and Values,"We saw in Chapter 5  how we can build generative models on text data using recurrent\nneural networks (RNNs), such as LSTMs and GRUs. These autoregressive models\nprocess sequential data one token at a time, constantly updating a hidden vector that\ncaptures the current latent representation of the input. The RNN can be designed to\npredict the next word in a sequence by applying a dense layer and softmax activation\nover the hidden vector. This was considered the most sophisticated way to genera‐\ntively produce text until 2017, when one paper changed the landscape of text genera‐\ntion forever.\nIntroduction\nThe Google Brain paper, confidently entitled “ Attention Is All Y ou Need, ” 1 is famous\nfor popularizing the concept of attention —a mechanism that now powers most state-\nof-the-art text generation models.\nThe authors show how it is possible to create powerful neural networks called Trans‐\nformers  for sequential modeling that do not require complex recurrent or convolu‐\ntional architectures but instead only rely on attention mechanisms. This approach\novercomes a key downside to the RNN approach, which is that it is challenging to\nparallelize, as it must process sequences one token as a time. Transformers are highly\nparalellizable, allowing them to be trained on massive datasets.\nIn this chapter, we are going to delve into how modern text generation models make\nuse of the Transformer architecture to reach state-of-the-art performance on text\ngeneration challenges. In particular, we will explore a type of autoregressive model\nknown as the generative pre-trained transformer  (GPT), which powers OpenAI’s\nGPT-4 model, widely considered to be the current state of the art for text generation.\nGPT\nOpenAI introduced GPT in June 2018, in the paper “Improving Language Under‐\nstanding by Generative Pre-Training, ” 2 almost exactly a year after the appearance of\nthe original Transformer paper.\nIn this paper, the authors show how a Transformer architecture can be trained on a\nhuge amount of text data to predict the next word in a sequence and then subse‐\nquently fine-tuned to specific downstream tasks.\nThe pre-training process of GPT involves training the model on a large corpus of text\ncalled BookCorpus (4.5 GB of text from 7,000 unpublished books of different genres).\nDuring pre-training, the model is trained to predict the next word in a sequence\ngiven the previous words. This process is known as language modeling  and is used to\nteach the model to understand the structure and patterns of natural language.\n236 | Chapter 9: Transformers\nAfter pre-training, the GPT model can be fine-tuned for a specific task by providing\nit with a smaller, task-specific dataset. Fine-tuning involves adjusting the parameters\nof the model to better fit the task at hand. For example, the model can be fine-tuned\nfor tasks such as classification, similarity scoring, or question answering.\nThe GPT architecture has since been improved and extended by OpenAI with the\nrelease of subsequent models such as GPT-2, GPT-3, GPT-3.5, and GPT-4. These\nmodels are trained on larger datasets and have larger capacities, so they can generate\nmore complex and coherent text. The GPT models have been widely adopted by\nresearchers and industry practitioners and have contributed to significant advance‐\nments in natural language processing tasks.\nIn this chapter, we will build our own variation of the original GPT model, trained on\nless data, but still utilizing the same components and underlying principles.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/09_transformer/01_gpt/gpt.ipynb  in the book\nrepository.\nThe code is adapted from the excellent GPT tutorial  created by\nApoorv Nandan available on the Keras website.\nThe Wine Reviews Dataset\nWe’ll be using the Wine Reviews dataset  that is available through  Kaggle. This is a set\nof over 130,000 reviews of wines, with accompanying metadata such as description\nand price.\nY ou can download the dataset by running the Kaggle dataset downloader script in the\nbook repository, as shown in Example 9-1 . This will save the wine reviews and\naccompanying metadata locally to the /data  folder.\nExample 9-1. Downloading the Wine Reviews dataset\nbash scripts/download_kaggle_data.sh  zynicide  wine-reviews\nThe data preparation steps are identical to the steps used in Chapter 5  for preparing\ndata for input into an LSTM, so we will not repeat them in detail here. The steps, as\nshown in Figure 9-1 , are as follows:\nGPT | 237\n1.Load the data and create a list of text string descriptions of each wine.\n2.Pad punctuation with spaces, so that each punctuation mark is treated as a sepa‐\nrate word.\n3.Pass the strings through a TextVectorization  layer that tokenizes the data and\npads/clips each string to a fixed length.\n4.Create a training set where the inputs are the tokenized text strings and the out‐\nputs to predict are the same strings shifted by one token.\nFigure 9-1. Data processing for the Transformer\nAttention\nThe first step to understanding how GPT works is to understand how the attention\nmechanism  works. This mechanism is what makes the Transformer architecture\nunique and distinct from recurrent approaches to language modeling. When we have\ndeveloped a solid understanding of attention, we will then see how it is used within\nTransformer architectures such as GPT.\nWhen you write, the choice that you make for the next word in the sentence is influ‐\nenced by other words that you have already written. For example, suppose you start a\nsentence as follows:\nThe pink elephant tried to get into the car but it was too\nClearly, the next word should be something synonymous with big. How do we know\nthis?\nCertain other words in the sentence are important for helping us to make our deci‐\nsion. For example, the fact that it is an elephant, rather than a sloth, means that we\nprefer big rather than slow. If it were a swimming pool, rather than a car, we might\nchoose scared  as a possible alternative to big. Lastly, the action of getting into  the car\nimplies that size is the problem—if the elephant was trying to squash  the car instead,\nwe might choose fast as the final word, with it now referring to the car.\n238 | Chapter 9: Transformers\nOther words in the sentence are not important at all. For example, the fact that the\nelephant is pink has no influence on our choice of final word. Equally, the minor\nwords in the sentence ( the, but, it, etc.) give the sentence grammatical form, but here\naren’t important to determine the required adjective.\nIn other words, we are paying attention  to certain words in the sentence and largely\nignoring others. Wouldn’t it be great if our model could do the same thing?\nAn attention mechanism (also know as an attention head ) in a Transformer is\ndesigned to do exactly this. It is able to decide where in the input it wants to pull\ninformation from, in order to efficiently extract useful information without being\nclouded by irrelevant details. This makes it highly adaptable to a range of circumstan‐\nces, as it can decide where it wants to look for information at inference time.\nIn contrast, a recurrent layer tries to build up a generic hidden state that captures an\noverall representation of the input at each timestep. A weakness of this approach is\nthat many of the words that have already been incorporated into the hidden vector\nwill not be directly relevant to the immediate task at hand (e.g., predicting the next\nword), as we have just seen. Attention heads do not suffer from this problem, because\nthey can pick and choose how to combine information from nearby words, depend‐\ning on the context.\nQueries, Keys, and Values\nSo how does an attention head decide where it wants to look for information? Before\nwe get into the details, let’s explore how it works at a high level, using our pink ele‐\nphant  example.\nImagine that we want to predict what follows the word too. To help with this task,\nother preceding words chime in with their opinions, but their contributions are\nweighted by how confident they are in their own expertise in predicting words that\nfollow too. For example, the word elephant  might confidently contribute that it is\nmore likely to be a word related to size or loudness, whereas the word was doesn’t\nhave much to offer to narrow down the possibilities.\nIn other words, we can think of an attention head as a kind of information retrieval\nsystem, where a query  (“What word follows too?”) is made into a key/value  store\n(other words in the sentence) and the resulting output is a sum of the values, weigh‐\nted by the resonance  between the query and each key.\nWe will now walk through the process in detail ( Figure 9-2 ), again with reference to\nour pink elephant  sentence.\nGPT | 239\nFigure 9-2. The mechanics of an attention head\nThe query  (Q) can be thought of as a representation of the current task at hand (e.g.,\n“What word follows too?”). In this example, it is derived from the embedding of the\nword too, by passing it through a weights matrix WQ to change the dimensionality of\nthe vector from de to dk.\nThe key vectors ( K) are representations of each word in the sentence—you can think\nof these as descriptions of the kinds of prediction tasks that each word can help with.\nThey are derived in a similar fashion to the query, by passing each embedding\nthrough a weights matrix WK to change the dimensionality of each vector from de to\ndk. Notice that the keys and the query are the same length ( dk).\n240 | Chapter 9: Transformers",9713
90-Causal Masking.pdf,90-Causal Masking,"Inside the attention head, each key is compared to the query using a dot product\nbetween each pair of vectors ( QKT). This is why the keys and the query have to be the\nsame length. The higher this number is for a particular key/query pair, the more the\nkey resonates with the query, so it is allowed to make more of a contribution to the\noutput of the attention head. The resulting vector is scaled by dk to keep the var‐\niance of the vector sum stable (approximately equal to 1), and a softmax is applied to\nensure the contributions sum to 1. This is a vector of attention weights .\nThe value  vectors ( V) are also representations of the words in the sentence—you can\nthink of these as the unweighted contributions of each word. They are derived by\npassing each embedding through a weights matrix WV to change the dimensionality\nof each vector from de to dv. Notice that the value vectors do not necessarily have to\nhave the same length as the keys and query (but often do, for simplicity).\nThe value vectors are multiplied by the attention weights to give the attention  for a\ngiven Q, K, and V, as shown in Equation 9-1 .\nEquation 9-1. Attention equation\nAttention Q,K,V=so f tmaxQKT\ndkV\nTo obtain the final output vector from the attention head, the attention is summed to\ngive a vector of length dv. This context vector  captures a blended opinion from words\nin the sentence on the task of predicting what word follows too.\nMultihead Attention\nThere’s  no reason to stop at just one attention head! In Keras, we can build a Multi\nHeadAttention  layer that concatenates the output from multiple attention heads,\nallowing each to learn a distinct attention mechanism so that the layer as a whole can\nlearn more complex relationships.\nThe concatenated outputs are passed through one final weights matrix WO to project\nthe vector into the desired output dimension, which in our case is the same as the\ninput dimension of the query ( de), so that the layers can be stacked sequentially on\ntop of each other.\nFigure 9-3  shows how the output from a MultiHeadAttention  layer is constructed. In\nKeras we can simply write the line shown in Example 9-2  to create such a layer.\nGPT | 241\nExample 9-2. Creating a MultiHeadAttention  layer in Keras\nlayers.MultiHeadAttention (\n    num_heads  = 4, \n    key_dim = 128, \n    value_dim  = 64, \n    output_shape  = 256 \n    )\nThis multihead attention layer has four heads.\nThe keys (and query) are vectors of length 128.\nThe values (and therefore also the output from each head) are vectors of length\n64.\nThe output vector has length 256.\nFigure 9-3. A multihead attention layer with four heads\nCausal Masking\nSo far, we have assumed that the query input to our attention head is a single vector.\nHowever, for efficiency during training, we would ideally like the attention layer to be\nable to operate on every word in the input at once, predicting for each what the sub‐\nsequent word will be. In other words, we want our GPT model to be able to handle a\ngroup of query vectors in parallel (i.e., a matrix).\nY ou might think that we can just batch the vectors together into a matrix and let lin‐\near algebra handle the rest. This is true, but we need one extra step—we need to apply\na mask to the query/key dot product, to avoid information from future words leaking\nthrough. This is known as causal masking  and is shown in Figure 9-4 .\n242 | Chapter 9: Transformers\nFigure 9-4. Matrix calculation of the attention scores for a batch of input queries, using a\ncausal attention mask to hide keys that are not available to the query (because they\ncome later in the sentence)\nWithout this mask, our GPT model would be able to perfectly guess the next word in\nthe sentence, because it would be using the key from the word itself as a feature! The\ncode for creating a causal mask is shown in Example 9-3 , and the resulting numpy\narray (transposed to match the diagram) is shown in Figure 9-5 .\nExample 9-3. The causal mask function\ndef causal_attention_mask (batch_size , n_dest, n_src, dtype):\n    i = tf.range(n_dest)[:, None]\n    j = tf.range(n_src)\n    m = i >= j - n_src + n_dest\n    mask = tf.cast(m, dtype)\n    mask = tf.reshape(mask, [1, n_dest, n_src])\nGPT | 243\n    mult = tf.concat(\n        [tf.expand_dims (batch_size , -1), tf.constant ([1, 1], dtype=tf.int32)], 0\n    )\n    return tf.tile(mask, mult)\nnp.transpose (causal_attention_mask (1, 10, 10, dtype = tf.int32)[0])\nFigure 9-5. The causal mask as a numpy  array—1 means unmasked and 0 means\nmasked\nCausal masking is only required in decoder Transformers  such as\nGPT, where the task is to sequentially generate tokens given previ‐\nous tokens. Masking out future tokens during training is therefore\nessential.\nOther  flavors of Transformer (e.g., encoder Transformers ) do not\nneed causal masking, because they are not trained to predict the\nnext token. For example Google’s BERT predicts masked words\nwithin a given sentence, so it can use context from both before and\nafter the word in question. 3\nWe will explore the different types of Transformers in more detail\nat the end of the chapter.\nThis concludes our explanation of the multihead attention mechanism that is present\nin all Transformers. It is remarkable that the learnable parameters of such an influen‐\ntial layer consist of nothing more than three densely connected weights matrices for\neach attention head ( WQ, WK, WV) and one further weights matrix to reshape the\noutput ( WO). There are no convolutions or recurrent mechanisms at all in a multi‐\nhead attention layer!\nNext, we shall take a step back and see how the multihead attention layer forms just\none part of a larger component known as a Transformer block .\n244 | Chapter 9: Transformers",5814
91-The Transformer Block.pdf,91-The Transformer Block,"The Transformer Block\nA Transformer block  is a single component within a Transformer that applies some\nskip connections, feed-forward (dense) layers, and normalization around the multi‐\nhead attention layer. A diagram of a Transformer block is shown in Figure 9-6 .\nFigure 9-6. A Transformer block\nFirstly, notice how the query is passed around the multihead attention layer to be\nadded to the output—this is a skip connection and is common in modern deep learn‐\ning architectures. It means we can build very deep neural networks that do not suffer\nas much from the vanishing gradient problem, because the skip connection provides\na gradient-free highway  that allows the network to transfer information forward\nuninterrupted.\nSecondly, layer normalization  is used in the Transformer block to provide stability to\nthe training process. We have already seen the batch normalization layer in action\nthroughout this book, where the output from each channel is normalized to have a\nGPT | 245\nmean of 0 and standard deviation of 1. The normalization statistics are calculated\nacross the batch and spatial dimensions.\nIn contrast, layer normalization in a Transformer block normalizes each position of\neach sequence in the batch by calculating the normalizing statistics across the chan‐\nnels. It is the complete opposite of batch normalization, in terms of how the normal‐\nization statistics are calculated. A diagram showing the difference between batch\nnormalization and layer normalization is shown in Figure 9-7 .\nFigure 9-7. Layer normalization versus batch normalization—the normalization statis‐\ntics are calculated across the blue cells (source: Sheng et al., 2020 )4\nLayer Normalization Versus Batch Normalization\nLayer normalization was used in the original GPT paper and is\ncommonly used for text-based tasks to avoid creating normaliza‐\ntion dependencies across sequences in the batch. However, recent\nwork such as Shen et al. s challenges this assumption, showing that\nwith some tweaks a form of batch normalization can still be used\nwithin Transformers, outperforming more traditional layer\nnormalization.\nLastly, a set of feed-forward (i.e., densely connected) layers is included in the Trans‐\nformer block, to allow the component to extract higher-level features as we go deeper\ninto the network.\n246 | Chapter 9: Transformers\nA Keras implementation of a Transformer block is shown in Example 9-4 .\nExample 9-4. A TransformerBlock  layer in Keras\nclass TransformerBlock (layers.Layer):\n    def __init__ (self, num_heads , key_dim, embed_dim , ff_dim, dropout_rate =0.1): \n        super(TransformerBlock , self).__init__ ()\n        self.num_heads  = num_heads\n        self.key_dim = key_dim\n        self.embed_dim  = embed_dim\n        self.ff_dim = ff_dim\n        self.dropout_rate  = dropout_rate\n        self.attn = layers.MultiHeadAttention (\n            num_heads , key_dim, output_shape  = embed_dim\n        )\n        self.dropout_1  = layers.Dropout(self.dropout_rate )\n        self.ln_1 = layers.LayerNormalization (epsilon=1e-6)\n        self.ffn_1 = layers.Dense(self.ff_dim, activation =""relu"")\n        self.ffn_2 = layers.Dense(self.embed_dim )\n        self.dropout_2  = layers.Dropout(self.dropout_rate )\n        self.ln_2 = layers.LayerNormalization (epsilon=1e-6)\n    def call(self, inputs):\n        input_shape  = tf.shape(inputs)\n        batch_size  = input_shape [0]\n        seq_len = input_shape [1]\n        causal_mask  = causal_attention_mask (\n            batch_size , seq_len, seq_len, tf.bool\n        ) \n        attention_output , attention_scores  = self.attn(\n            inputs,\n            inputs,\n            attention_mask =causal_mask ,\n            return_attention_scores =True\n        ) \n        attention_output  = self.dropout_1 (attention_output )\n        out1 = self.ln_1(inputs + attention_output ) \n        ffn_1 = self.ffn_1(out1) \n        ffn_2 = self.ffn_2(ffn_1)\n        ffn_output  = self.dropout_2 (ffn_2)\n        return (self.ln_2(out1 + ffn_output ), attention_scores ) \nThe sublayers that make up the TransformerBlock  layer are defined within the\ninitialization function.\nThe causal mask is created to hide future keys from the query.\nThe multihead attention layer is created, with the attention masks specified.\nGPT | 247",4359
92-Positional Encoding.pdf,92-Positional Encoding,"The first add and normalization  layer.\nThe feed-forward layers.\nThe second add and normalization  layer.\nPositional Encoding\nThere  is one final step to cover before we can put everything together to train our\nGPT model. Y ou may have noticed that in the multihead attention layer, there is\nnothing that cares about the ordering of the keys. The dot product between each key\nand the query is calculated in parallel, not sequentially, like in a recurrent neural net‐\nwork. This is a strength (because of the parallelization efficiency gains) but also a\nproblem, because we clearly need the attention layer to be able to predict different\noutputs for the following two sentences:\n•The dog looked at the boy and … (barked?)\n•The boy looked at the dog and … (smiled?)\nTo solve this problem, we use a technique called positional encoding  when creating the\ninputs to the initial Transformer block. Instead of only encoding each token using a\ntoken embedding , we also encode the position of the token, using a position embed‐\nding.\nThe token embedding  is created using a standard Embedding  layer to convert each\ntoken into a learned vector. We can create the positional embedding  in the same way,\nusing a standard Embedding  layer to convert each integer position into a learned\nvector.\nWhile GPT uses an Embedding  layer to embed the position, the\noriginal Transformer paper used trigonometric functions—we’ll\ncover this alternative in Chapter 11 , when we explore music\ngeneration.\n248 | Chapter 9: Transformers\nTo construct the joint token–position encoding, the token embedding is added to the\npositional embedding, as shown in Figure 9-8 . This way, the meaning and position of\neach word in the sequence are captured in a single vector.\nFigure 9-8. The token embeddings are added to the positional embeddings to give the\ntoken position encoding\nGPT | 249",1889
93-Training GPT.pdf,93-Training GPT,"The code that defines our TokenAndPositionEmbedding  layer is shown in\nExample 9-5 .\nExample 9-5. The TokenAndPositionEmbedding  layer\nclass TokenAndPositionEmbedding (layers.Layer):\n    def __init__ (self, maxlen, vocab_size , embed_dim ):\n        super(TokenAndPositionEmbedding , self).__init__ ()\n        self.maxlen = maxlen\n        self.vocab_size  =vocab_size\n        self.embed_dim  = embed_dim\n        self.token_emb  = layers.Embedding (\n            input_dim =vocab_size , output_dim =embed_dim\n        ) \n        self.pos_emb = layers.Embedding (input_dim =maxlen, output_dim =embed_dim ) \n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions  = tf.range(start=0, limit=maxlen, delta=1)\n        positions  = self.pos_emb(positions )\n        x = self.token_emb (x)\n        return x + positions  \nThe tokens are embedded using an Embedding  layer.\nThe positions of the tokens are also embedded using an Embedding  layer.\nThe output from the layer is the sum of the token and position embeddings.\nTraining GPT\nNow we are ready to build and train our GPT model! To put everything together, we\nneed to pass our input text through the token and position embedding layer, then\nthrough our Transformer block. The final output of the network is a simple Dense\nlayer with softmax activation over the number of words in the vocabulary.\nFor simplicity, we will use just one Transformer block, rather than\nthe 12 in the paper.\n250 | Chapter 9: Transformers\nThe overall architecture is shown in Figure 9-9  and the equivalent code is provided in\nExample 9-6 .\nFigure 9-9. The simplified  GPT model architecture\nExample 9-6. A GPT model in Keras\nMAX_LEN = 80\nVOCAB_SIZE  = 10000\nEMBEDDING_DIM  = 256\nN_HEADS = 2\nKEY_DIM = 256\nFEED_FORWARD_DIM  = 256\ninputs = layers.Input(shape=(None,), dtype=tf.int32) \nx = TokenAndPositionEmbedding (MAX_LEN, VOCAB_SIZE , EMBEDDING_DIM )(inputs) \nx, attention_scores  = TransformerBlock (\n    N_HEADS, KEY_DIM, EMBEDDING_DIM , FEED_FORWARD_DIM\n)(x) \noutputs = layers.Dense(VOCAB_SIZE , activation  = 'softmax' )(x) \ngpt = models.Model(inputs=inputs, outputs=[outputs, attention ]) \ngpt.compile(""adam"", loss=[losses.SparseCategoricalCrossentropy (), None]) \ngpt.fit(train_ds , epochs=5)\nThe input is padded (with zeros).\nThe text is encoded using a TokenAndPositionEmbedding  layer.\nGPT | 251",2395
94-Analysis of GPT.pdf,94-Analysis of GPT,"The encoding is passed through a TransformerBlock .\nThe transformed output is passed through a Dense  layer with softmax activation\nto predict a distribution over the subsequent word.\nThe Model  takes a sequence of word tokens as input and outputs the predicted\nsubsequent word distribution. The output from the Transformer block is also\nreturned so that we can inspect how the model is directing its attention.\nThe model is compiled with SparseCategoricalCrossentropy  loss over the pre‐\ndicted word distribution.\nAnalysis of GPT\nNow that we have compiled and trained our GPT model, we can start to use it to gen‐\nerate long strings of text. We can also interrogate the attention weights that are output\nfrom the TransformerBlock , to understand where the Transformer is looking for\ninformation at different points in the generation process.\nGenerating text\nWe can generate new text by applying the following process:\n1.Feed the network with an existing sequence of words and ask it to predict the fol‐\nlowing word.\n2.Append this word to the existing sequence and repeat.\nThe network will output a set of probabilities for each word that we can sample from,\nso we can make the text generation stochastic, rather than deterministic.\nWe will use the same TextGenerator  class introduced in Chapter 5  for LSTM text\ngeneration, including the temperature  parameter that specifies how deterministic we\nwould like the sampling process to be. Let’s take a look at this in action, at two differ‐\nent temperature values ( Figure 9-10 ).\n252 | Chapter 9: Transformers\nFigure 9-10. Generated outputs at temperature = 1.0  and temperature = 0.5 .\nThere are a few things to note about these two passages. First, both are stylistically\nsimilar to a wine review from the original training set. They both open with the\nregion and type of wine, and the wine type stays consistent throughout the passage\n(for example, it doesn’t switch color halfway through). As we saw in Chapter 5 , the\ngenerated text with temperature 1.0 is more adventurous and therefore less accurate\nthan the example with temperature 0.5. Generating multiple samples with tempera‐\nture 1.0 will therefore lead to more variety as the model is sampling from a probabil‐\nity distribution with greater variance.\nViewing the attention scores\nWe can also ask the model to tell us how much attention is being placed on each\nword, when deciding on the next word in the sentence. The TransformerBlock  out‐\nputs the attention weights for each head, which are a softmax distribution over the\npreceding words in the sentence.\nTo demonstrate this, Figure 9-11  shows the top five tokens with the highest probabili‐\nties for three different input prompts, as well as the average attention across both\nheads, against each preceding word. The preceding words are colored according to\ntheir attention score, averaged across the two attention heads. Darker blue indicates\nmore attention is being placed on the word.\nGPT | 253\nFigure 9-11. Distribution of word probabilities following various sequences\nIn the first example, the model attends closely to the country ( germany ) in order to\ndecide on the word that relates to the region. This makes sense! To pick a region, it\nneeds to take lots of information from the words that relate to the country, to ensure\nthey match. It doesn’t need to pay as much attention to the first two tokens ( wine\nreview ) because they don’t hold any useful information regarding the region.\nIn the second example, it needs to refer back to the grape ( riesling ), so it pays atten‐\ntion to the first time that it was mentioned. It can pull this information by directly\nattending to the word, no matter how far back it is in the sentence (within the upper\nlimit of 80 words). Notice that this is very different from a recurrent neural network,\nwhich relies on a hidden state to maintain all interesting information over the length\nof the sequence so that it can be drawn upon if required—a much less efficient\napproach.\nThe final sequence shows an example of how our GPT model can choose an appro‐\npriate adjective based on a combination of information. Here the attention is again on\nthe grape ( riesling ), but also on the fact that it contains residual sugar . As Riesling is\ntypically a sweet wine, and sugar is already mentioned, it makes sense that it should\nbe described as slightly sweet  rather than slightly earthy , for example.\n254 | Chapter 9: Transformers",4504
95-T5.pdf,95-T5,"It is incredibly informative to be able to interrogate the network in this way, to under‐\nstand exactly where it is pulling information from in order to make accurate deci‐\nsions about each subsequent word. I highly recommend playing around with the\ninput prompts to see if you can get the model to attend to words really far back in the\nsentence, to convince yourself of the power of attention-based models over more tra‐\nditional recurrent models!\nOther Transformers\nOur GPT model is a decoder Transformer —it generates a text string one token at a\ntime and uses causal masking to only attend to previous words in the input string.\nThere are also encoder Transformers , which do not use causal masking—instead, they\nattend to the entire input string in order to extract a meaningful contextual represen‐\ntation of the input. For other tasks, such as language translation, there are also\nencoder-decoder Transformers  that can translate from one text string to another; this\ntype of model contains both encoder Transformer blocks and decoder Transformer\nblocks.\nTable 9-1  summarizes the three types of Transformers, with the best examples of each\narchitecture and typical use cases.\nTable 9-1. The three Transformer architectures\nType Examples Use cases\nEncoder BERT (Google) Sentence classification,  named entity recognition, extractive question answering\nEncoder-decoder T5 (Google) Summarization, translation, question answering\nDecoder GPT-3 (OpenAI) Text generation\nA well-known example of an encoder Transformer is the Bidirectional Encoder Repre‐\nsentations from Transformers  (BERT) model, developed by Google (Devlin et al.,\n2018) that predicts missing words from a sentence, given context from both before\nand after the missing word in all layers.\nEncoder Transformers\nEncoder Transformers are typically used for tasks that require an\nunderstanding of the input as a whole, such as sentence classifica‐\ntion, named entity recognition, and extractive question answering.\nThey are not used for text generation tasks, so we will not explore\nthem in detail in this book—see Lewis Tunstall et al. ’s Natural Lan‐\nguage Processing with Transformers  (O’Reilly) for more\ninformation.\nOther Transformers | 255\nIn the following sections we will explore how encoder-decoder transformers work\nand discuss extensions of the original GPT model architecture released by OpenAI,\nincluding ChatGPT, which has been specifically designed for conversational\napplications.\nT5\nAn example of a modern Transformer that uses the encoder-decoder structure is the\nT5 model from Google. 5 This model reframes a range of tasks into a text-to-text\nframework, including translation, linguistic acceptability, sentence similarity, and\ndocument summarization, as shown in Figure 9-12 .\nFigure 9-12. Examples of how T5 reframes a range of tasks into a text-to-text frame‐\nwork, including translation, linguistic acceptability, sentence similarity, and document\nsummarization (source: Raffel  et al., 2019 )\nThe T5 model architecture closely matches the encoder-decoder architecture used in\nthe original Transformer paper, shown in Figure 9-13 . The key difference is that T5 is\ntrained on an enormous 750 GB corpus of text (the Colossal Clean Crawled Corpus,\nor C4), whereas the original Transformer paper was focused only on language trans‐\nlation, so it was trained on 1.4 GB of English–German sentence pairs.\n256 | Chapter 9: Transformers\nFigure 9-13. An encoder-decoder Transformer model: each gray box is a Transformer\nblock (source: Vaswani et al., 2017 )\nMuch of this diagram is already familiar to us—we can see the Transformer blocks\nbeing repeated and positional embedding being used to capture the ordering of the\ninput sequences. The two key differences between this model and the GPT model that\nwe built earlier in the chapter are as follows:\n•On the lefthand side, a set of encoder  Transformer blocks encode the sequence to\nbe translated. Notice that there is no causal masking on the attention layer. This is\nbecause we are not generating further text to extend the sequence to be trans‐\nlated; we just want to learn a good representation of the sequence as a whole that\ncan be fed to the decoder. Therefore, the attention layers in the encoder can be\ncompletely unmasked to capture all the cross-dependencies between words, no\nmatter the order.\nOther Transformers | 257\n•On the righthand side, a set of decoder  Transformer blocks generate the trans‐\nlated text. The initial attention layer is self-referential  (i.e., the key, value, and\nquery come from the same input) and causal masking is used to ensure informa‐\ntion from future tokens is not leaked to the current word to be predicted. How‐\never, we can then see that the subsequent attention layer pulls the key and value\nfrom the encoder, leaving only the query passed through from the decoder itself.\nThis  is called cross-referential  attention and means that the decoder can attend to\nthe encoder representation of the input sequence to be translated. This is how the\ndecoder knows what meaning the translation needs to convey!\nFigure 9-14  shows an example of cross-referential attention. Two attention heads of\nthe decoder layer are able to work together to provide the correct German translation\nfor the word the, when used in the context of the street . In German, there are three\ndefinite articles ( der, die, das ) depending on the gender of the noun, but the Trans‐\nformer knows to choose die because one attention head is able to attend to the word\nstreet  (a feminine word in German), while another attends to the word to translate\n(the).\nFigure 9-14. An example of how one attention head attends to the word “the” and\nanother attends to the word “street” in order to correctly translate the word “the” to the\nGerman word “die” as the feminine definite  article of “Straße”\n258 | Chapter 9: Transformers",5956
96-ChatGPT.pdf,96-ChatGPT,"This example is from the Tensor2Tensor GitHub repository , which\ncontains a Colab notebook that allows you to play around with a\ntrained encoder-decoder Transformer model and see how the\nattention mechanisms of the encoder and decoder impact the\ntranslation of a given sentence into German.\nGPT-3 and GPT-4\nSince  the original 2018 publication of GPT, OpenAI has released multiple updated\nversions that improve upon the original model, as shown in Table 9-2 .\nTable 9-2. The evolution of OpenAI’s GPT collection of models\nModel Date Layers Attention\nheadsWord\nembedding\nsizeContext\nwindow# parameters Training data\nGPT Jun\n201812 12 768 512 120,000,000 BookCorpus: 4.5 GB of text\nfrom unpublished books\nGPT-2 Feb\n201948 48 1,600 1,024 1,500,000,000 WebText: 40 GB of text from\noutbound Reddit links\nGPT-3 May\n202096 96 12,888 2,048 175,000,000,000 CommonCrawl, WebText,\nEnglish Wikipedia, book\ncorpora and others: 570 GB\nGPT-4 Mar\n2023- - - - - -\nThe model architecture of GPT-3 is fairly similar to the original GPT model, except it\nis much larger and trained on much more data. At the time of writing, GPT-4 is in\nlimited beta—OpenAI has not publicly released details of the model’s structure and\nsize, though we do know that it is able to accept images as input, so crosses over into\nbeing a multimodal model for the first time. The model weights of GPT-3 and GPT-4\nare not open source, though the models are available through a commercial tool and\nAPI.\nGPT-3 can also be fine-tuned to your own training data —this allows you to provide\nmultiple examples of how it should react to a given style of prompt by physically\nupdating the weights of the network. In many cases this may not be necessary, as\nGPT-3 can be told how to react to a given style of prompt simply by providing a few\nexamples in the prompt itself (this is known as few-shot learning ). The benefit of fine-\ntuning is that you do not need to provide these examples as part of every single input\nprompt, saving costs in the long run.\nOther Transformers | 259\nAn example of the output from GPT-3, given a system prompt sentence, is shown in\nFigure 9-15 .\nFigure 9-15. An example of how GPT-3 can extend a given system prompt\nLanguage models such as GPT benefit hugely from scaling—both in terms of number\nof model weights and dataset size. The ceiling of large language model capability has\nyet to be reached, with researchers continuing to push the boundaries of what is pos‐\nsible with increasingly larger models and datasets.\nChatGPT\nA few months before the beta release of GPT-4, OpenAI announced ChatGPT —a tool\nthat allows users to interact with their suite of large language models through a con‐\nversational interface. The original release in November 2022 was powered by\nGPT-3.5 , a version of the model that was more powerful that GPT-3 and was fine-\ntuned to conversational responses.\nExample dialogue is shown in Figure 9-16 . Notice how the agent is able to maintain\nstate between inputs, understanding that the attention  mentioned in the second ques‐\ntion refers to attention in the context of Transformers, rather than a person’s ability to\nfocus.\n260 | Chapter 9: Transformers\nFigure 9-16. An example of ChatGPT answering questions about Transformers\nOther Transformers | 261\nAt the time of writing, there is no official paper that describes how ChatGPT works in\ndetail, but from the official blog post  we know that it uses a technique called reinforce‐\nment learning from human feedback  (RLHF) to fine-tune the GPT-3.5 model. This\ntechnique was also used in the ChatGPT group’s earlier paper 6 that introduced the\nInstructGPT  model, a fine-tuned GPT-3 model that is specifically designed to more\naccurately follow written instructions.\nThe training process for ChatGPT is as follows:\n1.Supervised fine-tuning : Collect a demonstration dataset of conversational inputs\n(prompts) and desired outputs that have been written by humans. This is used to\nfine-tune the underlying language model (GPT-3.5) using supervised learning.\n2.Reward modeling : Present a human labeler with examples of prompts and several\nsampled model outputs and ask them to rank the outputs from best to worst.\nTrain a reward model that predicts the score given to each output, given the con‐\nversation history.\n3.Reinforcement learning : Treat the conversation as a reinforcement learning envi‐\nronment where the policy  is the underlying language model, initialized to the\nfine-tuned model from step 1. Given the current state  (the conversation history)\nthe policy outputs an action  (a sequence of tokens), which is scored by the\nreward model trained in step 2. A reinforcement learning algorithm—proximal\npolicy optimization (PPO)—can then be trained to maximize the reward, by\nadjusting the weights of the language model.\nReinforcement Learning\nFor an introduction to reinforcement learning see Chapter 12 ,\nwhere we explore how generative models can be used in a rein‐\nforcement learning setting.\nThe RLHF process is shown in Figure 9-17 .\n262 | Chapter 9: Transformers\nFigure 9-17. The reinforcement learning from human feedback fine-tuning  process used\nin ChatGPT (source: OpenAI )\nOther Transformers | 263",5264
97-Summary.pdf,97-Summary,"While  ChatGPT still has many limitations (such as sometimes “hallucinating” factu‐\nally incorrect information), it is a powerful example of how Transformers can be used\nto build generative models that can produce complex, long-ranging, and novel output\nthat is often indistinguishable from human-generated text. The progress made thus\nfar by models like ChatGPT serves as a testament to the potential of AI and its trans‐\nformative impact on the world.\nMoreover, it is evident that AI-driven communication and interaction will continue\nto rapidly evolve in the future. Projects like Visual ChatGPT 7 are now combining the\nlinguistic power of ChatGPT with visual foundation models such as Stable Diffusion,\nenabling users to interact with ChatGPT not only through text, but also images. The\nfusion of linguistic and visual capabilities in projects like Visual ChatGPT and GPT-4\nhave the potential to herald a new era in human–computer interaction.\nSummary\nIn this chapter, we explored the Transformer model architecture and built a version\nof GPT—a model for state-of-the-art text generation.\nGPT makes use of a mechanism known as attention, which removes the need for\nrecurrent layers (e.g., LSTMs). It works like an information retrieval system, utilizing\nqueries, keys, and values to decide how much information it wants to extract from\neach input token.\nAttention heads can be grouped together to form what is known as a multihead atten‐\ntion layer. These are then wrapped up inside a Transformer block, which includes\nlayer normalization and skip connections around the attention layer. Transformer\nblocks can be stacked to create very deep neural networks.\nCausal masking is used to ensure that GPT cannot leak information from down‐\nstream tokens into the current prediction. Also, a technique known as positional\nencoding is used to ensure that the ordering of the input sequence is not lost, but\ninstead is baked into the input alongside the traditional word embedding.\nWhen analyzing the output from GPT, we saw it was possible not only to generate\nnew text passages, but also to interrogate the attention layer of the network to under‐\nstand where in the sentence it is looking to gather information to improve its predic‐\ntion. GPT can access information at a distance without loss of signal, because the\nattention scores are calculated in parallel and do not rely on a hidden state that is car‐\nried through the network sequentially, as is the case with recurrent neural networks.\nWe saw how there are three families of Transformers (encoder, decoder, and encoder-\ndecoder) and the different tasks that can be accomplished with each. Finally, we\nexplored the structure and training process of other large language models such as\nGoogle’s T5 and OpenAI’s ChatGPT.\n264 | Chapter 9: Transformers\nReferences\n1. Ashish Vaswani et al., “ Attention Is All Y ou Need, ” June 12, 2017, https://\narxiv.org/abs/1706.03762 .\n2. Alec Radford et al., “Improving Language Understanding by Generative Pre-\nTraining, ” June 11, 2018, https://openai.com/research/language-unsupervised .\n3. Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding, ” October 11, 2018, https://arxiv.org/abs/1810.04805 .\n4. Sheng Shen et al., “PowerNorm: Rethinking Batch Normalization in Transformers, ”\nJune 28, 2020, https://arxiv.org/abs/2003.07845 .\n5. Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-\nto-Text Transformer, ” October 23, 2019, https://arxiv.org/abs/1910.10683 .\n6. Long Ouyang et al., “Training Language Models to Follow Instructions with\nHuman Feedback, ” March 4, 2022, https://arxiv.org/abs/2203.02155 .\n7. Chenfei Wu et al., “Visual ChatGPT: Talking, Drawing and Editing with Visual\nFoundation Models, ” March 8, 2023, https://arxiv.org/abs/2303.04671 .\nSummary | 265",3904
98-ProGAN.pdf,98-ProGAN,"CHAPTER 10\nAdvanced GANs\nChapter Goals\nIn this chapter you will:\n•See how a ProGAN model progressively trains a GAN to generate high-\nresolution images.\n•Understand how ProGAN was adapted to build StyleGAN, a high-performing\nGAN for image synthesis.\n•Explore how StyleGAN was adjusted to create StyleGAN2, a state-of-the-art\nmodel that improves further upon the original work.\n•Learn about the key contributions of these models, including progressive train‐\ning, adaptive instance normalization, weight modulation and demodulation, and\npath length regularization.\n•Walk through the architecture of the Self-Attention GAN (SAGAN), which\nincorporates the attention mechanism into the GAN framework.\n•See how BigGAN expands upon the ideas in the SAGAN paper to produce high-\nquality images.\n•Learn how VQ-GAN uses a codebook to encode images into a discrete sequence\nof tokens that can be modeled using a Transformer.\n•See how ViT VQ-GAN adapts the VQ-GAN architecture to use Transformers\ninstead of convolutional layers in the encoder and decoder.\n267\nChapter 4  introduced generative adversarial networks (GANs), a class of generative\nmodel that has produced state-of-the-art results across a wide variety of image gener‐\nation tasks. The flexibility in the model architecture and training process has led aca‐\ndemics and deep learning practitioners to find new ways to design and train GANs,\nleading to many different advanced flavors  of the architecture that we shall explore in\nthis chapter.\nIntroduction\nExplaining all GAN developments and their repercussions in detail could easily fill\nanother book. The GAN Zoo repository  on GitHub contains over 500 distinct exam‐\nples of GANs with linked papers, ranging from ABC-GAN to ZipNet-GAN!\nIn this chapter we will cover the main GANs that have been influential in the field,\nincluding a detailed explanation of the model architecture and training process for\neach.\nWe will first explore three important models from NVIDIA that have pushed the\nboundaries of image generation: ProGAN, StyleGAN, and StyleGAN2. We will ana‐\nlyze each of these models in enough detail to understand the fundamental concepts\nthat underpin the architectures and see how they have each built on ideas from earlier\npapers.\nWe will also explore two other important GAN architectures that incorporate atten‐\ntion: the Self-Attention GAN (SAGAN) and BigGAN, which built on many of the\nideas in the SAGAN paper. We have already seen the power of the attention mecha‐\nnism in the context of Transformers in Chapter 9 .\nLastly, we will cover VQ-GAN and ViT VQ-GAN, which incorporate a blend of ideas\nfrom variational autoencoders, Transformers, and GANs. VQ-GAN is a key compo‐\nnent of Google’s state-of-the-art text-to-image generation model Muse. 1 We will\nexplore so-called multimodal models in more detail in Chapter 13 .\nTraining Your Own Models\nFor conciseness I have chosen not to include code to directly build\nthese models in the code repository for this book, but instead will\npoint to publicly available implementations where possible, so that\nyou can train your own versions if you wish.\n268 | Chapter 10: Advanced GANs",3205
99-Progressive Training.pdf,99-Progressive Training,"ProGAN\nProGAN is a technique developed by NVIDIA Labs in 2017 2 to improve both the\nspeed and stability of GAN training. Instead of immediately training a GAN on full-\nresolution images, the ProGAN paper suggests first training the generator and dis‐\ncriminator on low-resolution images of, say, 4 × 4 pixels and then incrementally\nadding layers throughout the training process to increase the resolution.\nLet’s take a look at the concept of progressive training  in more detail.\nTraining Your Own ProGAN\nThere is an excellent tutorial by Bharath K on training your own\nProGAN using Keras available on the Paperspace blog . Bear in\nmind that training a ProGAN to achieve the results from the paper\nrequires a significant amount of computing power.\nProgressive Training\nAs always with GANs, we build two independent networks, the generator and dis‐\ncriminator, with a fight for dominance taking place during the training process.\nIn a normal GAN, the generator always outputs full-resolution images, even in the\nearly stages of training. It is reasonable to think that this strategy might not be opti‐\nmal—the generator might be slow to learn high-level structures in the early stages of\ntraining, because it is immediately operating over complex, high-resolution images.\nWouldn’t it be better to first train a lightweight GAN to output accurate low-\nresolution images and then see if we can build on this to gradually increase the reso‐\nlution?\nThis simple idea leads us to progressive training , one of the key contributions of the\nProGAN paper. The ProGAN is trained in stages, starting with a training set that has\nbeen condensed down to 4 × 4–pixel images using interpolation, as shown in\nFigure 10-1 .\nProGAN | 269\nFigure 10-1. Images in the dataset can be compressed to lower resolution using\ninterpolation\nWe can then initially train the generator to transform a latent input noise vector z\n(say, of length 512) into an image of shape 4 × 4 × 3. The matching discriminator will\nneed to transform an input image of size 4 × 4 × 3 into a single scalar prediction. The\nnetwork architectures for this first step are shown in Figure 10-2 .\nThe blue box in the generator represents the convolutional layer that converts the set\nof feature maps into an RGB image ( toRGB ), and the blue box in the discriminator\nrepresents the convolutional layer that converts the RGB images into a set of feature\nmaps (fromRGB ).\n270 | Chapter 10: Advanced GANs\nFigure 10-2. The generator and discriminator architectures for the first stage of the Pro‐\nGAN training process\nIn the paper, the authors train this pair of networks until the discriminator has seen\n800,000 real images. We now need to understand how the generator and discrimina‐\ntor are expanded to work with 8 × 8–pixel images.\nTo expand the generator and discriminator, we need to blend in additional layers.\nThis is managed in two phases, transition and stabilization, as shown in Figure 10-3 .\nProGAN | 271\nFigure 10-3. The ProGAN generator training process, expanding the network from 4 × 4\nimages to 8 × 8 (dotted lines represent the rest of the network, not shown)\nLet’s first look at the generator. During the  transition phase , new upsampling and con‐\nvolutional layers are appended to the existing network, with a residual connection set\nup to maintain the output from the existing trained toRGB  layer. Crucially, the new\nlayers are initially masked using a parameter α that is gradually increased from 0 to 1\nthroughout the transition phase to allow more of the new toRGB  output through and\nless of the existing toRGB  layer. This is to avoid a shock  to the network as the new lay‐\ners take over.\nEventually, there is no flow through the old toRGB  layer and the network enters the\nstabilization phase —a further period of training where the network can fine-tune the\noutput, without any flow through the old toRGB  layer.\n272 | Chapter 10: Advanced GANs\nThe discriminator uses a similar process, as shown in Figure 10-4 .\nFigure 10-4. The ProGAN discriminator training process, expanding the network from\n4 × 4 images to 8 × 8 (dotted lines represent the rest of the network, not shown)\nHere, we need to blend in additional downscaling and convolutional layers. Again,\nthe layers are injected into the network—this time at the start of the network, just\nafter the input image. The existing fromRGB  layer is connected via a residual connec‐\ntion and gradually phased out as the new layers take over during the transition phase.\nThe stabilization phase allows the discriminator to fine-tune using the new layers.\nProGAN | 273\nAll transition and stabilization phases last until the discriminator has been shown\n800,000 real images. Note that even through the network is trained progressively, no\nlayers are frozen . Throughout the training process, all layers remain fully trainable.\nThis process continues, growing the GAN from 4 × 4 images to 8 × 8, then 16 × 16,\n32 × 32, and so on, until it reaches full resolution (1,024 × 1,024), as shown in\nFigure 10-5 .\nFigure 10-5. The ProGAN training mechanism, and some example generated faces\n(source: Karras et al., 2017 )\nThe overall structure of the generator and discriminator after the full progressive\ntraining process is complete is shown in Figure 10-6 .\n274 | Chapter 10: Advanced GANs\nFigure 10-6. The ProGAN generator and discriminator used to generate 1,024 × 1,024–\npixel CelebA faces (source: Karras et al., 2018 )\nThe paper also makes several other important contributions, namely minibatch stan‐\ndard deviation, equalized learning rates, and pixelwise normalization, which are\ndescribed briefly in the following sections.\nMinibatch standard deviation\nThe minibatch standard deviation  layer is an extra layer in the discriminator that\nappends the standard deviation of the feature values, averaged across all pixels and\nacross the minibatch as an additional (constant) feature. This helps to ensure the gen‐\nerator creates more variety in its output—if variety is low across the minibatch, then\nthe standard deviation will be small, and the discriminator can use this feature to dis‐\ntinguish the fake batches from the real batches! Therefore, the generator is incentiv‐\nized to ensure it generates a similar amount of variety as is present in the real training\ndata.\nProGAN | 275",6397
100-No Progressive Growing.pdf,100-No Progressive Growing,"Equalized learning rates\nAll dense and convolutional layers in ProGAN use equalized learning rates . Usually,\nweights in a neural network are initialized using a method such as He initialization —a\nGaussian distribution where the standard deviation is scaled to be inversely propor‐\ntional to the square root of the number of inputs to the layer. This way, layers with a\ngreater number of inputs will be initialized with weights that have a smaller deviation\nfrom zero, which generally improves the stability of the training process.\nThe authors of the ProGAN paper found that this was causing problems when used\nin combination with modern optimizers such as Adam or RMSProp. These methods\nnormalize the gradient update for each weight, so that the size of the update is inde‐\npendent of the scale (magnitude) of the weight. However, this means that weights\nwith a larger dynamic range (i.e., layers with fewer inputs) will take comparatively\nlonger to adjust than weights with a smaller dynamic range (i.e., layers with more\ninputs). It was found that this causes an imbalance between the speed of training of\nthe different layers of the generator and discriminator in ProGAN, so they used\nequalized learning rates  to solve this problem.\nIn ProGAN, weights are initialized using a simple standard Gaussian, regardless of\nthe number of inputs to the layer. The normalization is applied dynamically, as part of\nthe call to the layer, rather than only at initialization. This way, the optimizer sees\neach weight as having approximately the same dynamic range, so it applies the same\nlearning rate. It is only when the layer is called that the weight is scaled by the factor\nfrom the He initializer.\nPixelwise normalization\nLastly,  in ProGAN pixelwise normalization  is used in the generator, rather than batch\nnormalization. This normalizes the feature vector in each pixel to a unit length and\nhelps to prevent the signal from spiraling out of control as it propagates through the\nnetwork. The pixelwise normalization layer has no trainable weights.\nOutputs\nIn addition to the CelebA dataset, ProGAN was also applied to images from the\nLarge-scale Scene Understanding (LSUN) dataset with excellent results, as shown in\nFigure 10-7 . This demonstrated the power of ProGAN over earlier GAN architectures\nand paved the way for future iterations such as StyleGAN and StyleGAN2, which we\nshall explore in the next sections.\n276 | Chapter 10: Advanced GANs\nFigure 10-7. Generated examples from a ProGAN trained progressively on the LSUN\ndataset at 256 × 256 resolution (source: Karras et al., 2017 )\nStyleGAN\nStyleGAN 3 is a GAN architecture from 2018 that builds on the earlier ideas in the\nProGAN paper. In fact, the discriminator is identical; only the generator is changed.\nOften when training GANs it is difficult to separate out vectors in the latent space\ncorresponding to high-level attributes—they are frequently entangled , meaning that\nadjusting an image in the latent space to give a face more freckles, for example, might\nalso inadvertently change the background color. While ProGAN generates fantasti‐\ncally realistic images, it is no exception to this general rule. We would ideally like to\nhave full control of the style of the image, and this requires a disentangled separation\nof features in the latent space.\nStyleGAN achieves this by explicitly injecting style vectors into the network at differ‐\nent points: some that control high-level features (e.g., face orientation) and some that\ncontrol low-level details (e.g., the way the hair falls across the forehead).\nThe overall architecture of the StyleGAN generator is shown in Figure 10-8 . Let’s\nwalk through this architecture step by step, starting with the mapping network.\nStyleGAN | 277\nFigure 10-8. The StyleGAN generator architecture (source: Karras et al., 2018 )\nTraining Your Own StyleGAN\nThere is an excellent tutorial by Soon-Y au Cheong on training your\nown StyleGAN using Keras available on the Keras website . Bear in\nmind that training a StyleGAN to achieve the results from the\npaper requires a significant amount of computing power.\nThe Mapping Network\nThe mapping network  f is a simple feed-forward network that converts the input\nnoise  ∈   into a different latent space  ∈  . This gives the generator the oppor‐\ntunity to disentangle the noisy input vector into distinct factors of variation, which\ncan be easily picked up by the downstream style-generating layers.\nThe point of doing this is to separate out the process of choosing a style for the image\n(the mapping network) from the generation of an image with a given style (the syn‐\nthesis network).\n278 | Chapter 10: Advanced GANs\nThe Synthesis Network\nThe synthesis network is the generator of the actual image with a given style, as pro‐\nvided by the mapping network. As can be seen from Figure 10-8 , the style vector  is\ninjected into the synthesis network at different points, each time via a differently\ndensely connected layer Ai, which generates two vectors: a bias vector b,i and a scal‐\ning vector s,i. These vectors define the specific style that should be injected at this\npoint in the network—that is, they tell the synthesis network how to adjust the feature\nmaps to move the generated image in the direction of the specified style.\nThis adjustment is achieved through adaptive instance normalization  (AdaIN) layers.\nAdaptive instance normalization\nAn AdaIN layer is a type of neural network layer that adjusts the mean and variance\nof each feature map i with a reference style bias b,i and scale s,i, respectively. 4 Both\nvectors are of length equal to the number of channels output from the preceding con‐\nvolutional layer in the synthesis network. The equation for adaptive instance normal‐\nization is as follows:\nAdaINi,=s,ii−μi\nσi+b,i\nThe adaptive instance normalization layers ensure that the style vectors that are injec‐\nted into each layer only affect features at that layer, by preventing any style informa‐\ntion from leaking through between layers. The authors show that this results in the\nlatent vectors  being significantly more disentangled than the original  vectors.\nSince the synthesis network is based on the ProGAN architecture, it is trained pro‐\ngressively. The style vectors at earlier layers in the synthesis network (when the reso‐\nlution of the image is lowest—4 × 4, 8 × 8) will affect coarser features than those later\nin the network (64 × 64 to 1,024 × 1,024–pixel resolution). This means that not only\ndo we have complete control over the generated image through the latent vector ,\nbut we can also switch the  vector at different points in the synthesis network to\nchange the style at a variety of levels of detail.\nStyle mixing\nThe authors use a trick known as style mixing  to ensure that the generator cannot uti‐\nlize correlations between adjacent styles during training (i.e., the styles injected at\neach layer are as disentangled as possible). Instead of sampling only a single latent\nvector , two are sampled 1,2, corresponding to two style vectors 1,2. Then,\nat each layer, either 1 or 2 is chosen at random, to break any possible correlation\nbetween the vectors.\nStyleGAN | 279\nStochastic variation\nThe synthesizer network adds noise (passed through a learned broadcasting layer B)\nafter each convolution to account for stochastic details such as the placement of indi‐\nvidual hairs, or the background behind the face. Again, the depth at which the noise\nis injected affects the coarseness of the impact on the image.\nThis also means that the initial input to the synthesis network can simply be a learned\nconstant, rather than additional noise. There is enough stochasticity already present\nin the style inputs and the noise inputs to generate sufficient variation in the images.\nOutputs from StyleGAN\nFigure 10-9  shows StyleGAN in action.\nFigure 10-9. Merging styles between two generated images at different  levels of detail\n(source: Karras et al., 2018 )\n280 | Chapter 10: Advanced GANs\nHere, two images, source A and source B, are generated from two different  vectors.\nTo generate a merged image, the source A  vector is passed through the synthesis\nnetwork but, at some point, switched for the source B  vector. If this switch happens\nearly on (4 × 4 or 8 × 8 resolution), coarse styles such as pose, face shape, and glasses\nfrom source B are carried across onto source A. However, if the switch happens later,\nonly fine-grained detail is carried across from source B, such as colors and micro‐\nstructure of the face, while the coarse features from source A are preserved.\nStyleGAN2\nThe final contribution in this chain of important GAN papers is StyleGAN2. 5 This\nbuilds further upon the StyleGAN architecture, with some key changes that improve\nthe quality of the generated output. In particular, StyleGAN2 generations do not suf‐\nfer as greatly from  artifacts —water droplet–like areas of the image that were found to\nbe caused by the adaptive instance normalization layers in StyleGAN, as shown in\nFigure 10-10 .\nFigure 10-10. An artifact in a StyleGAN-generated image of a face (source: Karras et al.,\n2019 )\nBoth the generator and the discriminator in StyleGAN2 are different from the Style‐\nGAN. In the next sections we will explore the key differences between the\narchitectures.\nStyleGAN2 | 281\nTraining Your Own StyleGAN2\nThe official code for training your own StyleGAN using Tensor‐\nFlow is available on GitHub . Bear in mind that training a Style‐\nGAN2 to achieve the results from the paper requires a significant\namount of computing power.\nWeight Modulation and Demodulation\nThe artifact problem is solved by removing the AdaIN layers in the generator and\nreplacing them with weight modulation and demodulation steps, as shown in\nFigure 10-11 .  represents the weights of the convolutional layer, which are directly\nupdated by the modulation and demodulation steps in StyleGAN2 at runtime. In\ncomparison, the AdaIN layers of StyleGAN operate on the image tensor as it flows\nthrough the network.\nThe AdaIN layer in StyleGAN is simply an instance normalization followed by style\nmodulation (scaling and bias). The idea in StyleGAN2 is to apply style modulation\nand normalization (demodulation) directly to the weights of the convolutional layers\nat runtime, rather than the output from the convolutional layers, as shown in\nFigure 10-11 . The authors show how this removes the artifact issue while retaining\ncontrol of the image style.\nFigure 10-11. A comparison between the StyleGAN and StyleGAN2 style blocks\n282 | Chapter 10: Advanced GANs\nIn StyleGAN2, each dense layer A outputs a single style vector si, where i indexes the\nnumber of input channels in the corresponding convolutional layer. This style vector\nis then applied to the weights of the convolutional layer as follows:\nwi,j,k′ =si·wi,j,k\nHere, j indexes the output channels of the layer and k indexes the spatial dimensions.\nThis is the modulation  step of the process.\nThen, we need to normalize the weights so that they again have a unit standard devia‐\ntion, to ensure stability in the training process. This is the demodulation  step:\nwi,j,k′′ =wi,j,k′\n∑i,kwi,j,k′2+ε\nwhere  is a small constant value that prevents division by zero.\nIn the paper, the authors show how this simple change is enough to prevent water-\ndroplet artifacts, while retaining control over the generated images via the style vec‐\ntors and ensuring the quality of the output remains high.\nPath Length Regularization\nAnother  change made to the StyleGAN architecture is the inclusion of an additional\npenalty term in the loss function— this is known as path length regularization .\nWe would like the latent space to be as smooth and uniform as possible, so that a\nfixed-size step in the latent space in any direction results in a fixed-magnitude change\nin the image.\nTo encourage this property, StyleGAN2 aims to minimize the following term, along‐\nside the usual Wasserstein loss with gradient penalty:\n,∥ ⊤ ∥2−a2\nHere,  is a set of style vectors created by the mapping network,  is a set of noisy\nimages drawn from 0,, and =∂g\n∂ is the Jacobian of the generator network\nwith respect to the style vectors.\nThe term ∥ ⊤ ∥2 measures the magnitude of the images  after transformation by\nthe gradients given in the Jacobian. We want this to be close to a constant a, which is\nStyleGAN2 | 283\ncalculated dynamically as the exponential moving average of ∥ ⊤ ∥2 as the training\nprogresses.\nThe authors find that this additional term makes exploring the latent space more reli‐\nable and consistent. Moreover, the regularization terms in the loss function are only\napplied once every 16 minibatches, for efficiency. This  technique, called lazy regulari‐\nzation , does not cause a measurable drop in performance.\nNo Progressive Growing\nAnother major update is in how StyleGAN2 is trained. Rather than adopting the\nusual progressive training mechanism, StyleGAN2 utilizes skip connections in the\ngenerator and residual connections in the discriminator to train the entire network as\none. It no longer requires different resolutions to be trained independently and blen‐\nded as part of the training process.\nFigure 10-12  shows the generator and discriminator blocks in StyleGAN2.\nFigure 10-12. The generator and discriminator blocks in StyleGAN2\n284 | Chapter 10: Advanced GANs\nThe crucial property that we would like to be able to preserve is that the StyleGAN2\nstarts by learning low-resolution features and gradually refines the output as training\nprogresses. The authors show that this property is indeed preserved using this archi‐\ntecture. Each network benefits from refining the convolutional weights in the lower-\nresolution layers in the earlier stages of training, with the skip and residual\nconnections used to pass the output through the higher-resolution layers mostly\nunaffected. As training progresses, the higher-resolution layers begin to dominate, as\nthe generator discovers more intricate ways to improve the realism of the images in\norder to fool the discriminator. This process is demonstrated in Figure 10-13 .\nFigure 10-13. The contribution of each resolution layer to the output of the generator, by\ntraining time (adapted from Karras et al., 2019 )\nStyleGAN2 | 285",14452
101-Outputs from StyleGAN2.pdf,101-Outputs from StyleGAN2,,0
102-Other Important GANs.pdf,102-Other Important GANs,,0
103-Self-Attention GAN SAGAN.pdf,103-Self-Attention GAN SAGAN,"Outputs from StyleGAN2\nSome  examples of StyleGAN2 output are shown in Figure 10-14 . To date, the Style‐\nGAN2 architecture (and scaled variations such as  StyleGAN-XL 6) remain state of the\nart for image generation on datasets such as Flickr-Faces-HQ (FFHQ) and CIFAR-10,\naccording to the benchmarking website Papers with Code .\nFigure 10-14. Uncurated StyleGAN2 output for the FFHQ face dataset and LSUN car\ndataset (source: Karras et al., 2019 )\nOther Important GANs\nIn this section, we will explore two more architectures that have also contributed sig‐\nnificantly to the development of GANs—SAGAN and BigGAN.\nSelf-Attention GAN (SAGAN)\nThe Self-Attention GAN (SAGAN) 7 is a key development for GANs as it shows how\nthe attention mechanism that powers sequential models such as the Transformer can\nalso be incorporated into GAN-based models for image generation. Figure 10-15\nshows the self-attention mechanism from the paper introducing this architecture.\n286 | Chapter 10: Advanced GANs\nFigure 10-15. The self-attention mechanism within the SAGAN model (source: Zhang et\nal., 2018 )\nThe problem with GAN-based models that do not incorporate attention is that con‐\nvolutional feature maps are only able to process information locally. Connecting pixel\ninformation from one side of an image to the other requires multiple convolutional\nlayers that reduce the size of the image, while increasing the number of channels. Pre‐\ncise positional information is reduced throughout this process in favor of capturing\nhigher-level features, making it computationally inefficient for the model to learn\nlong-range dependencies between distantly connected pixels. SAGAN solves this\nproblem by incorporating the attention mechanism that we explored earlier in this\nchapter into the GAN. The effect of this inclusion is shown in Figure 10-16 .\nFigure 10-16. A SAGAN-generated image of a bird (leftmost  cell) and the attention\nmaps of the final  attention-based generator layer for the pixels covered by the three col‐\nored dots (rightmost cells) (source: Zhang et al., 2018 )\nThe red dot is a pixel that is part of the bird’s body, and so attention naturally falls on\nthe surrounding body cells. The green dot is part of the background, and here the\nattention actually falls on the other side of the bird’s head, on other background pix‐\nels. The blue dot is part of the bird’s long tail and so attention falls on other tail pixels,\nsome of which are distant from the blue dot. It would be difficult to maintain this\nOther Important GANs | 287",2570
104-VQ-GAN.pdf,104-VQ-GAN,"long-range dependency for pixels without attention, especially for long, thin struc‐\ntures in the image (such as the tail in this case).\nTraining Your Own SAGAN\nThe official code for training your own SAGAN using TensorFlow\nis available on GitHub . Bear in mind that training a SAGAN to\nachieve the results from the paper requires a significant amount of\ncomputing power.\nBigGAN\nBigGAN, 8 developed  at DeepMind, extends the ideas from the SAGAN paper.\nFigure 10-17  shows some of the images generated by BigGAN, trained on the Image‐\nNet dataset at 128 × 128 resolution.\nFigure 10-17. Examples of images generated by BigGAN (source: Brock et al., 2018 )\nAs well as some incremental changes to the base SAGAN model, there are also several\ninnovations outlined in the paper that take the model to the next level of sophistica‐\ntion. One such innovation is the so-called truncation trick . This is where the latent\ndistribution used for sampling is different from the z∼  0, distribution used\nduring training. Specifically, the distribution used during sampling is a truncated nor‐\nmal distribution  (resampling values of z that have magnitude greater than a certain\nthreshold). The smaller the truncation threshold, the greater the believability of gen‐\nerated samples, at the expense of reduced variability. This concept is shown in\nFigure 10-18 .\n288 | Chapter 10: Advanced GANs\nFigure 10-18. The truncation trick: from left to right, the threshold is set to 2, 1, 0.5, and\n0.04 (source: Brock et al., 2018 )\nAlso, as the name suggests, BigGAN is an improvement over SAGAN in part simply\nby being bigger . BigGAN uses a batch size of 2,048—8 times larger than the batch size\nof 256 used in SAGAN—and a channel size that is increased by 50% in each layer.\nHowever, BigGAN additionally shows that SAGAN can be improved structurally by\nthe inclusion of a shared embedding, by orthogonal regularization, and by incorpo‐\nrating the latent vector z into each layer of the generator, rather than just the initial\nlayer.\nFor a full description of the innovations introduced by BigGAN, I recommend read‐\ning the original paper and accompanying presentation material .\nUsing BigGAN\nA tutorial for generating images using a pre-trained BigGAN is\navailable on the TensorFlow website .\nVQ-GAN\nAnother important type of GAN is the Vector Quantized GAN (VQ-GAN), intro‐\nduced in 2020. 9 This model architecture builds upon an idea introduced in the 2017\npaper “Neural Discrete Representation Learning” 10—namely, that the representations\nlearned by a V AE can be discrete, rather than continuous. This new type of model,\nthe Vector Quantized V AE (VQ-V AE), was shown to generate high-quality images\nwhile avoiding some of the issues often seen with traditional continuous latent space\nV AEs, such as posterior collapse  (where the learned latent space becomes uninforma‐\ntive due to an overly powerful decoder).\nThe first version of DALL.E, a text-to-image model released by\nOpenAI in 2021 (see Chapter 13 ), utilized a V AE with a discrete\nlatent space, similar to VQ-V AE.\nOther Important GANs | 289\nBy a discrete latent space , we mean a learned list of vectors (the codebook ), each asso‐\nciated with a corresponding index. The job of the encoder in a VQ-V AE is to collapse\nthe input image to a smaller grid of vectors that can then be compared to the code‐\nbook. The closest codebook vector to each grid square vector (by Euclidean distance)\nis then taken forward to be decoded by the decoder, as shown in Figure 10-19 . The\ncodebook is a list of learned vectors of length d (the embedding size) that matches the\nnumber of channels in the output of the encoder and input to the decoder. For exam‐\nple, e1 is a vector that can be interpreted as background .\nFigure 10-19. A diagram of a VQ-VAE\nThe codebook can be thought of as a set of learned discrete concepts that are shared\nby the encoder and decoder in order to describe the contents of a given image. The\nVQ-V AE must find a way to make this set of discrete concepts as informative as pos‐\nsible so that the encoder can accurately label  each grid square with a particular code\nvector that is meaningful to the decoder. The loss function for a VQ-V AE is therefore\nthe reconstruction loss added to two terms (alignment and commitment loss) that\nensure that the output vectors from the encoder are as close as possible to vectors in\nthe codebook. These terms replace the the KL divergence term between the encoded\ndistribution and the standard Gaussian prior in a typical V AE.\nHowever, this architecture poses a question—how do we sample novel code grids to\npass to the decoder to generate new images? Clearly, using a uniform prior (picking\neach code with equal probability for each grid square) will not work. For example in\nthe MNIST dataset, the top-left grid square is highly likely to be coded as background ,\nwhereas grid squares toward the center of the image are not as likely to be coded as\nsuch. To solve this problem, the authors used another model, an autoregressive\nPixelCNN  (see Chapter 5 ), to predict the next code vector in the grid, given previous\n290 | Chapter 10: Advanced GANs\ncode vectors. In other words, the prior is learned by the model, rather than static as in\nthe case of the vanilla V AE.\nTraining Your Own VQ-VAE\nThere is an excellent tutorial by Sayak Paul on training your own\nVQ-V AE using Keras available on the Keras website .\nThe VQ-GAN paper details several key changes to the VQ-V AE architecture, as\nshown in Figure 10-20 .\nFigure 10-20. A diagram of a VQ-GAN: the GAN discriminator helps to encourage the\nVAE to generate less blurry images through an additional adversarial loss term\nFirstly, as the name suggests, the authors include a GAN discriminator that tries to\ndistinguish between the output from the V AE decoder and real images, with an\naccompanying adversarial term in the loss function. GANs are known to produce\nsharper images than V AEs, so this addition improves the overall image quality. Notice\nthat despite the name, the V AE is still present in a VQ-GAN model—the GAN dis‐\ncriminator is an additional component rather than a replacement of the V AE. The\nidea of combining a V AE with a GAN discriminator (V AE-GAN) was first introduced\nby Larsen et al. in their 2015 paper. 11\nSecondly, the GAN discriminator predicts if small patches of the images are real or\nfake, rather than the entire image at once. This idea (PatchGAN ) was applied in the\nsuccessful  pix2pix  image-to-image model introduced in 2016 by Isola et al. 12 and was\nalso successfully applied as part of  CycleGAN ,13 another image-to-image style transfer\nOther Important GANs | 291",6755
105-ViT VQ-GAN.pdf,105-ViT VQ-GAN,"model. The PatchGAN discriminator outputs a prediction vector (a prediction for\neach patch), rather than a single prediction for the overall image. The benefit of using\na PatchGAN discriminator is that the loss function can then measure how good the\ndiscriminator is at distinguishing images based on their style , rather than their con‐\ntent. Since each individual element of the discriminator prediction is based on a small\nsquare of the image, it must use the style of the patch, rather than its content, to make\nits decision. This is useful as we know that V AEs produce images that are stylistically\nmore blurry than real images, so the PatchGAN discriminator can encourage the\nV AE decoder to generate sharper images than it would naturally produce.\nThirdly, rather than use a single MSE reconstruction loss that compares the input\nimage pixels with the output pixels from the V AE decoder, VQ-GAN uses a percep‐\ntual loss  term that calculates the difference between feature maps at intermediate lay‐\ners of the encoder and corresponding layers of the decoder. This idea is from the 2016\npaper by Hou et al., 14 where the authors show that this change to the loss function\nresults in more realistic image generations.\nLastly, instead of PixelCNN, a Transformer is used as the autoregressive part of the\nmodel, trained to generate sequences of codes. The Transformer is trained in a sepa‐\nrate phase, after the VQ-GAN has been fully trained. Rather than use all previous\ntokens in a fully autoregressive manner, the authors choose to only use tokens that\nfall within a sliding window around the token to be predicted. This ensures that the\nmodel scales to larger images, which require a larger latent grid size and therefore\nmore tokens to be generated by the Transformer.\nViT VQ-GAN\nOne  final extension to the VQ-GAN was made by Yu et al. in their 2021 paper enti‐\ntled “Vector-Quantized Image Modeling with Improved VQGAN. ” 15 Here, the\nauthors show how the convolutional encoder and decoder of the VQ-GAN can be\nreplaced with Transformers as shown in Figure 10-21 .\nFor the encoder, the authors use a Vision Transformer  (ViT). 16 A ViT is a neural net‐\nwork architecture that applies the Transformer model, originally designed for natural\nlanguage processing, to image data. Instead of using convolutional layers to extract\nfeatures from an image, a ViT divides the image into a sequence of patches, which are\ntokenized and then fed as input to an encoder Transformer.\nSpecifically, in the ViT VQ-GAN, the nonoverlapping input patches (each of size 8 ×\n8) are first flattened, then projected into a low-dimensional embedding space, where\npositional embeddings are added. This sequence is then fed to a standard encoder\nTransformer and the resulting embeddings are quantized according to a learned\ncodebook. These integer codes are then processed by a decoder Transformer model,\nwith the overall output being a sequence of patches that can be stitched back together\n292 | Chapter 10: Advanced GANs\nto form the original image. The overall encoder-decoder model is trained end-to-end\nas an autoencoder.\nFigure 10-21. A diagram of a ViT VQ-GAN: the GAN discriminator helps to encourage\nthe VAE to generate less blurry images through an additional adversarial loss term\n(source: Yu and Koh, 2022 )17\nAs with the original VQ-GAN model, the second phase of training involves using an\nautoregressive decoder Transformer to generate sequences of codes. Therefore in\ntotal, there are three Transformers in a ViT VQ-GAN, in addition to the GAN dis‐\ncriminator and learned codebook. Examples of images generated by the ViT VQ-\nGAN from the paper are shown in Figure 10-22 .\nOther Important GANs | 293",3749
106-Summary.pdf,106-Summary,"Figure 10-22. Example images generated by a ViT VQ-GAN trained on ImageNet\n(source: Yu et al., 2021 )\nSummary\nIn this chapter, we have taken a tour of some of the most important and influential\nGAN papers since 2017. In particular, we have explored ProGAN, StyleGAN, Style‐\nGAN2, SAGAN, BigGAN, VQ-GAN, and ViT VQ-GAN.\nWe started by exploring the concept of progressive training that was pioneered in the\n2017 ProGAN paper. Several key changes were introduced in the 2018 StyleGAN\npaper that gave greater control over the image output, such as the mapping network\nfor creating a specific style vector and synthesis network that allowed the style to be\ninjected at different resolutions. Finally, StyleGAN2 replaced the adaptive instance\nnormalization of StyleGAN with weight modulation and demodulation steps, along‐\nside additional enhancements such as path regularization. The paper also showed\nhow the desirable property of gradual resolution refinement could be retained\nwithout having to the train the network progressively.\nWe also saw how the concept of attention could be built into a GAN, with the intro‐\nduction of SAGAN in 2018. This allows the network to capture long-range depen‐\ndencies, such as similar background colors over opposite sides of an image, without\nrelying on deep convolutional maps to spread the information over the spatial\ndimensions of the image. BigGAN was an extension of this idea that made several key\nchanges and trained a larger network to improve the image quality further.\nIn the VQ-GAN paper, the authors show how several different types of generative\nmodels can be combined to great effect. Building on the original VQ-V AE paper that\nintroduced the concept of a V AE with a discrete latent space, VQ-GAN additionally\nincludes a discriminator that encourages the V AE to generate less blurry images\nthrough an additional adversarial loss term. An autoregressive Transformer is used to\nconstruct a novel sequence of code tokens that can be decoded by the V AE decoder to\nproduce novel images. The ViT VQ-GAN paper extends this idea even further, by\nreplacing the convolutional encoder and decoder of VQ-GAN with Transformers.\n294 | Chapter 10: Advanced GANs\nReferences\n1. Huiwen Chang et al., “Muse: Text-to-Image Generation via Masked Generative\nTransformers, ” January 2, 2023, https://arxiv.org/abs/2301.00704 .\n2. Tero Karras et al., “Progressive Growing of GANs for Improved Quality, Stability,\nand Variation, ” October 27, 2017, https://arxiv.org/abs/1710.10196 .\n3. Tero Karras et al., “ A Style-Based Generator Architecture for Generative Adversa‐\nrial Networks, ” December 12, 2018, https://arxiv.org/abs/1812.04948 .\n4. Xun Huang and Serge Belongie, “ Arbitrary Style Transfer in Real-Time with Adap‐\ntive Instance Normalization, ” March 20, 2017, https://arxiv.org/abs/1703.06868 .\n5. Tero Karras et al., “ Analyzing and Improving the Image Quality of StyleGAN, ”\nDecember 3, 2019, https://arxiv.org/abs/1912.04958 .\n6. Axel Sauer et al., “StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets, ” Feb‐\nruary 1, 2022, https://arxiv.org/abs/2202.00273v2 .\n7. Han Zhang et al., “Self-Attention Generative Adversarial Networks, ” May 21, 2018,\nhttps://arxiv.org/abs/1805.08318 .\n8. Andrew Brock et al., “Large Scale GAN Training for High Fidelity Natural Image\nSynthesis, ” September 28, 2018, https://arxiv.org/abs/1809.11096 .\n9. Patrick Esser et al., “Taming Transformers for High-Resolution Image Synthesis, ”\nDecember 17, 2020, https://arxiv.org/abs/2012.09841 .\n10. Aaron van den Oord et al., “Neural Discrete Representation Learning, ” November\n2, 2017, https://arxiv.org/abs/1711.00937v2 .\n11. Anders Boesen Lindbo Larsen et al., “ Autoencoding Beyond Pixels Using a\nLearned Similarity Metric, ” December 31, 2015, https://arxiv.org/abs/1512.09300 .\n12. Phillip Isola et al., “Image-to-Image Translation with Conditional Adversarial\nNetworks, ” November 21, 2016, https://arxiv.org/abs/1611.07004v3 .\n13. Jun-Y an Zhu et al., “Unpaired Image-to-Image Translation using Cycle-\nConsistent Adversarial Networks, ” March 30, 2017, https://arxiv.org/abs/1703.10593 .\n14. Xianxu Hou et al., “Deep Feature Consistent Variational Autoencoder, ” October 2,\n2016, https://arxiv.org/abs/1610.00291 .\n15. Jiahui Yu et al., “Vector-Quantized Image Modeling with Improved VQGAN, ”\nOctober 9, 2021, https://arxiv.org/abs/2110.04627 .\n16. Alexey Dosovitskiy et al., “ An Image Is Worth 16x16 Words: Transformers for\nImage Recognition at Scale, ” October 22, 2020, https://arxiv.org/abs/2010.11929v2 .\nSummary | 295\n17. Jiahui Yu and Jing Yu Koh, “Vector-Quantized Image Modeling with Improved\nVQGAN, ” May 18, 2022, https://ai.googleblog.com/2022/05/vector-quantized-image-\nmodeling-with.html .\n296 | Chapter 10: Advanced GANs",4836
107-The Bach Cello Suite Dataset.pdf,107-The Bach Cello Suite Dataset,"CHAPTER 11\nMusic Generation\nChapter Goals\nIn this chapter you will:\n•Understand how we can treat music generation as a sequence prediction prob‐\nlem, so we can apply autoregressive models such as Transformers.\n•See how to parse and tokenize MIDI files using the music21  package to create a\ntraining set.\n•Learn how to use sine positional encoding.\n•Train a music-generating Transformer, with multiple inputs and outputs to han‐\ndle note and duration.\n•Understand how to handle polyphonic music, including grid tokenization and\nevent-based tokenization.\n•Train a MuseGAN model to generate multitrack music.\n•Use the MuseGAN to adjust different properties of the generated bars.\nMusical composition is a complex and creative process that involves combining dif‐\nferent musical elements such as melody, harmony, rhythm, and timbre. While this is\ntraditionally seen as a uniquely human activity, recent advancements have made it\npossible to generate music that both is pleasing to the ear and has long-term\nstructure.\nOne of the most popular techniques for music generation is the Transformer, as\nmusic can be thought of as a sequence prediction problem. These models have been\nadapted to generate music by treating musical notes as a sequence of tokens, similar\n297\nto words in a sentence. The Transformer model learns to predict the next note in the\nsequence based on the previous notes, resulting in a generated piece of music.\nMuseGAN takes a totally different approach to generating music. Unlike Transform‐\ners, which generate music note by note, MuseGAN generates entire musical tracks at\nonce by treating music as an image , consisting of a pitch axis and a time axis. More‐\nover, MuseGAN separates out different musical components such as chords, style,\nmelody, and groove so that they can be controlled independently.\nIn this chapter we will learn how to process music data and apply both a Transformer\nand MuseGAN to generate music that is stylistically similar to a given training set.\nIntroduction\nFor a machine to compose music that is pleasing to our ear, it must master many of\nthe same technical challenges that we saw in Chapter 9  in relation to text. In particu‐\nlar, our model must be able to learn from and re-create the sequential structure of\nmusic and be able to choose from a discrete set of possibilities for subsequent notes.\nHowever, music generation presents additional challenges that are not present for text\ngeneration, namely pitch and rhythm. Music is often polyphonic—that is, there are\nseveral streams of notes played simultaneously on different instruments, which com‐\nbine to create harmonies that are either dissonant (clashing) or consonant (harmo‐\nnious). Text generation only requires us to handle a single stream of text, in contrast\nto the parallel streams of chords that are present in music.\nAlso, text generation can be handled one word at a time. Unlike text data, music is a\nmultipart, interwoven tapestry of sounds that are not necessarily delivered at the\nsame time—much of the interest that stems from listening to music is in the interplay\nbetween different rhythms across the ensemble. For example, a guitarist might play a\nflurry of quicker notes while the pianist holds a longer sustained chord. Therefore,\ngenerating music note by note is complex, because we often do not want all the\ninstruments to change notes simultaneously.\nWe will start this chapter by simplifying the problem to focus on music generation for\na single (monophonic) line of music. Many of the techniques from Chapter 9  for text\ngeneration can also be used for music generation, as the two tasks share many com‐\nmon themes. We will start by training a Transformer to generate music in the style of\nthe J.S. Bach cello suites and see how the attention mechanism allows the model to\nfocus on previous notes in order to determine the most natural subsequent note.\nWe’ll then tackle the task of polyphonic music generation and explore how we can\ndeploy an architecture based around GANs to create music for multiple voices.\n298 | Chapter 11: Music Generation\nTransformers for Music Generation\nThe model we will be building here is a decoder Transformer, taking inspiration from\nOpenAI’s MuseNet , which also utilizes a decoder Transformer (similar to GPT-3)\ntrained to predict the next note given a sequence of previous notes.\nIn music generation tasks, the length of the sequence N grows large as the music pro‐\ngresses, and this means that the N×N attention matrix for each head becomes\nexpensive to store and compute. We ideally do not want to clip the input sequence to\na short number of tokens, as we would like the model to construct the piece around a\nlong-term structure and repeat motifs and phrases from several minutes ago, as a\nhuman composer would.\nTo tackle this problem, MuseNet utilizes a form of Transformer known as a Sparse\nTransformer . Each output position in the attention matrix only computes weights for\na subset of input positions, thereby reducing the computational complexity and\nmemory required to train the model. MuseNet can therefore operate with full atten‐\ntion over 4,096 tokens and can learn long-term structure and melodic structure\nacross a range of styles. (See, for example, OpenAI’s Chopin  and Mozart  recordings\non SoundCloud.)\nTo see how the continuation of a musical phrase is often influenced by notes from\nseveral bars ago, take a look at the opening bars of the Prelude to Bach’s Cello Suite\nNo. 1 ( Figure 11-1 ).\nFigure 11-1. The opening of Bach’s Cello Suite No. 1 (Prelude)\nBars\nBars  (or measures ) are small units of music that contain a fixed,\nsmall number of beats and are marked out by vertical lines that\ncross the staff. If you can count 1, 2, 1, 2 along to a piece of music,\nthen there are two beats in each bar and you’re probably listening\nto a march. If you can count 1, 2, 3, 1, 2, 3, then there are three\nbeats to each bar and you may be listening to a waltz.\nTransformers for Music Generation | 299",6074
108-Parsing MIDI Files.pdf,108-Parsing MIDI Files,"What note do you think comes next? Even if you have no musical training you may\nstill be able to guess. If you said G (the same as the very first note of the piece), then\nyou’ d be correct. How did you know this? Y ou may have been able to see that every\nbar and half bar starts with the same note and used this information to inform your\ndecision. We want our model to be able to perform the same trick—in particular, we\nwant it to pay attention to a particular note from the previous half bar, when the pre‐\nvious low G was registered. An attention-based model such as a Transformer will be\nable to incorporate this long-term look-back without having to maintain a hidden\nstate across many bars, as is the case with a recurrent neural network.\nAnyone tackling the task of music generation must first have a basic understanding of\nmusical theory. In the next section we’ll go through the essential knowledge required\nto read music and how we can represent this numerically, in order to transform\nmusic into the input data required to train our Transformer.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/11_music/01_transformer/transformer.ipynb  in\nthe book repository.\nThe Bach Cello Suite Dataset\nThe raw dataset that we shall be using is a set of MIDI files for the Cello Suites by J.S.\nBach. Y ou can download the dataset by running the dataset downloader script in the\nbook repository, as shown in Example 11-1 . This will save the MIDI files locally to\nthe /data  folder.\nExample 11-1. Downloading the J.S. Bach Cello Suites dataset\nbash scripts/download_music_data.sh\nTo view and listen to the music generated by the model, you’ll need some software\nthat can produce musical notation. MuseScore  is a great tool for this purpose and can\nbe downloaded for free.\nParsing MIDI Files\nWe’ll be using the Python library music21  to load the MIDI files into Python for pro‐\ncessing. Example 11-2  shows how to load a MIDI file and visualize it ( Figure 11-2 ),\nboth as a score and as structured data.\n300 | Chapter 11: Music Generation\nFigure 11-2. Musical notation\nTransformers for Music Generation | 301\nExample 11-2. Importing a MIDI file\nimport music21\nfile = ""/app/data/bach-cello/cs1-2all.mid""\nexample_score  = music21.converter .parse(file).chordify ()\nOctaves\nThe number after each note name indicates the octave  that the note\nis in—since the note names (A to G) repeat, this is needed to\nuniquely identify the pitch of the note. For example, G2 is an octave\nbelow G3.\nNow it’s time to convert the scores into something that looks more like text! We start\nby looping over each score and extracting the note and duration of each element in\nthe piece into two separate text strings, with elements separated by spaces. We encode\nthe key and time signature of the piece as special symbols, with zero duration.\nMonophonic Versus Polyphonic Music\nIn this first example, we will treat the music as monophonic  (one\nsingle line), taking just the top note of any chords. Sometimes we\nmay wish to keep the parts separate to generate music that is poly‐\nphonic  in nature. This presents additional challenges that we shall\ntackle later on in this chapter.\nThe output from this process is shown in Figure 11-3 —compare this to Figure 11-2\nso that you can see how the raw music data has been transformed into the two strings.\nFigure 11-3. Samples of the notes text string and the duration text string, corresponding\nto Figure 11-2\n302 | Chapter 11: Music Generation",3591
109-Sine Position Encoding.pdf,109-Sine Position Encoding,"This looks a lot more like the text data that we have dealt with previously. The words\nare the note–duration combinations, and we should try to build a model that predicts\nthe next note and duration, given a sequence of previous notes and durations. A key\ndifference between music and text generation is that we need to build a model that\ncan handle the note and duration prediction simultaneously—i.e., there are two\nstreams of information that we need to handle, compared to the single streams of text\nthat we saw in Chapter 9 .\nTokenization\nTo create the dataset that will train the model, we first need to tokenize each note and\nduration, exactly as we did previously for each word in a text corpus. We can achieve\nthis by using a TextVectorization  layer, applied to the notes and durations sepa‐\nrately, as shown in Example 11-3 .\nExample 11-3. Tokenizing the notes and durations\ndef create_dataset (elements ):\n    ds = (\n        tf.data.Dataset.from_tensor_slices (elements )\n        .batch(BATCH_SIZE , drop_remainder  = True)\n        .shuffle(1000)\n    )\n    vectorize_layer  = layers.TextVectorization (\n        standardize  = None, output_mode =""int""\n    )\n    vectorize_layer .adapt(ds)\n    vocab = vectorize_layer .get_vocabulary ()\n    return ds, vectorize_layer , vocab\nnotes_seq_ds , notes_vectorize_layer , notes_vocab  = create_dataset (notes)\ndurations_seq_ds , durations_vectorize_layer , durations_vocab  = create_dataset (\n    durations\n)\nseq_ds = tf.data.Dataset.zip((notes_seq_ds , durations_seq_ds ))\nTransformers for Music Generation | 303\nThe full parsing and tokenization process is shown in Figure 11-4 .\nFigure 11-4. Parsing the MIDI files and tokenizing the notes and durations\nCreating the Training Set\nThe final step of preprocessing is to create the training set that we will feed to our\nTransformer.\nWe do this by splitting both the note and duration strings into chunks of 50 elements,\nusing a sliding window technique. The output is simply the input window shifted by\none note, so that the Transformer is trained to predict the note and duration of the\nelement one timestep into the future, given previous elements in the window. An\nexample of this (using a sliding window of only four elements for demonstration pur‐\nposes) is shown in Figure 11-5 .\n304 | Chapter 11: Music Generation\nFigure 11-5. The inputs and outputs for the musical Transformer model—in this exam‐\nple, a sliding window of width 4 is used to create input chunks, which are then shifted  by\none element to create the target output\nThe architecture we will be using for our Transformer is the same as we used for text\ngeneration in Chapter 9 , with a few key differences.\nSine Position Encoding\nFirstly, we will be introducing a different type of encoding for the token positions. In\nChapter 9  we used a simple Embedding  layer to encode the position of each token,\neffectively mapping each integer position to a distinct vector that was learned by the\nmodel. We therefore needed to define a maximum length ( N) that the sequence could\nbe and train on this length of sequence. The downside to this approach is that it is\nthen impossible to extrapolate to sequences that are longer than this maximum\nlength. Y ou would have to clip the input to the last N tokens, which isn’t ideal if you\nare trying to generate long-form content.\nTransformers for Music Generation | 305\nTo circumvent this problem, we can switch to using a different type of embedding\ncalled a sine position embedding . This is similar to the embedding that we used in\nChapter 8  to encode the noise variances of the diffusion model. Specifically, the fol‐\nlowing function is used to convert the position of the word ( pos) in the input\nsequence into a unique vector of length d:\nPEpos, 2i= sinpos\n10, 0002i/d\nPEpos, 2i+ 1= cospos\n10, 0002i+ 1/d\nFor small i, the wavelength of this function is short and therefore the function value\nchanges rapidly along the position axis. Larger values of i create a longer wavelength.\nEach position thus has its own unique encoding, which is a specific combination of\nthe different wavelengths.\nNotice that this embedding is defined for all possible position val‐\nues. It is a deterministic function (i.e., it isn’t learned by the model)\nthat uses trigonometric functions to define a unique encoding for\neach possible position.\nThe Keras NLP  module has a built-in layer that implements this embedding for us—\nwe can therefore define our TokenAndPositionEmbedding  layer as shown in\nExample 11-4 .\nExample 11-4. Tokenizing the notes and durations\nclass TokenAndPositionEmbedding (layers.Layer):\n    def __init__ (self, vocab_size , embed_dim ):\n        super(TokenAndPositionEmbedding , self).__init__ ()\n        self.vocab_size  = vocab_size\n        self.embed_dim  = embed_dim\n        self.token_emb  = layers.Embedding (input_dim =vocab_size , output_dim =embed_dim )\n        self.pos_emb = keras_nlp .layers.SinePositionEncoding ()\n    def call(self, x):\n        embedding  = self.token_emb (x)\n        positions  = self.pos_emb(embedding )\n        return embedding  + positions\nFigure 11-6  shows how the two embeddings (token and position) are added to pro‐\nduce the overall embedding for the sequence.\n306 | Chapter 11: Music Generation",5346
110-Multiple Inputs and Outputs.pdf,110-Multiple Inputs and Outputs,"Figure 11-6. The TokenAndPositionEmbedding  layer adds the token embeddings to the\nsinusoidal position embeddings to produce the overall embedding for the sequence\nMultiple Inputs and Outputs\nWe now have two input streams (notes and durations) and two output streams (pre‐\ndicted notes and durations). We therefore need to adapt the architecture of our\nTransformer to cater for this.\nThere are many ways of handling the dual stream of inputs. We could create tokens\nthat represent each note–duration pair and then treat the sequence as a single stream\nof tokens. However, this has the downside of not being able to represent note–dura‐\ntion pairs that have not been seen in the training set (for example, we may have seen a\nG#2 note and a 1/3 duration independently, but never together, so there would be no\ntoken for G#2:1/3 .\nInstead, we choose to embed the note and duration tokens separately and then use a\nconcatenation layer to create a single representation of the input that can be used by\nthe downstream Transformer block. Similarly, the output from the Transformer block\nis passed to two separate dense layers, which represent the predicted note and dura‐\ntion probabilities. The overall architecture is shown in Figure 11-7 . Layer output\nshapes are shown with batch size b and sequence length l.\nTransformers for Music Generation | 307\nFigure 11-7. The architecture of the music-generating Transformer\nAn alternative approach would be to interleave the note and duration tokens into a\nsingle stream of input and let the model learn that the output should be a single\nstream where the note and duration tokens alternate. This comes with the added\ncomplexity of ensuring that the output can still be parsed when the model has not yet\nlearned how to interleave the tokens correctly.\nThere is no right  or wrong  way to design your model—part of the\nfun is experimenting with different setups and seeing which works\nbest for you!\n308 | Chapter 11: Music Generation",2000
111-Analysis of the Music-Generating Transformer.pdf,111-Analysis of the Music-Generating Transformer,"Analysis of the Music-Generating Transformer\nWe’ll  start by generating some music from scratch, by seeding the network with a\nSTART  note token and 0.0 duration token (i.e., we are telling the model to assume it is\nstarting from the beginning of the piece). Then we can generate a musical passage\nusing the same iterative technique we used in Chapter 9  for generating text sequen‐\nces, as follows:\n1.Given the current sequence (of notes and durations), the model predicts two dis‐\ntributions, one for the next note and one for the next duration.\n2.We sample from both of these distributions, using a temperature  parameter to\ncontrol how much variation we would like in the sampling process.\n3.The chosen note and duration are appended to the respective input sequences.\n4.The process repeats with the new input sequences for as many elements as we\nwish to generate.\nFigure 11-8  shows examples of music generated from scratch by the model at various\nepochs of the training process. We use a temperature of 0.5 for the notes and\ndurations.\nFigure 11-8. Some examples of passages generated by the model when seeded only with a\nSTART  note token and 0.0 duration token\nTransformers for Music Generation | 309\nMost of our analysis in this section will focus on the note predictions, rather than\ndurations, as for Bach’s Cello Suites the harmonic intricacies are more difficult to cap‐\nture and therefore more worthy of investigation. However, you can also apply the\nsame analysis to the rhythmic predictions of the model, which may be particularly\nrelevant for other styles of music that you could use to train this model (such as a\ndrum track).\nThere are several points to note about the generated passages in Figure 11-8 . First, see\nhow the music is becoming more sophisticated as training progresses. To begin with,\nthe model plays it safe by sticking to the same group of notes and rhythms. By epoch\n10, the model has begun to generate small runs of notes, and by epoch 20 it is pro‐\nducing interesting rhythms and is firmly established in a set key (E ♭ major).\nSecond, we can analyze the distribution of notes over time by plotting the predicted\ndistribution at each timestep as a heatmap. Figure 11-9  shows this heatmap for the\nexample from epoch 20 in Figure 11-8 .\nFigure 11-9. The distribution of possible next notes over time (at epoch 20): the darker\nthe square, the more certain the model is that the next note is at this pitch\n310 | Chapter 11: Music Generation\nAn interesting point to note here is that the model has clearly learned which notes\nbelong to particular keys, as there are gaps in the distribution at notes that do not\nbelong to the key. For example, there is a gray gap along the row for note 54 (corre‐\nsponding to G ♭/F ♯). This note is highly unlikely to appear in a piece of music in the\nkey of E ♭ major. The model establishes the key early on in the generation process,\nand as the piece progresses, the model chooses notes that are more likely to feature in\nthat key by attending to the token that represents it.\nIt is also worth pointing out that the model has learned Bach’s characteristic style of\ndropping to a low note on the cello to end a phrase and bouncing back up again to\nstart the next. See how around note 20, the phrase ends on a low E ♭—it is common\nin the Bach Cello Suites to then return to a higher, more sonorous range of the instru‐\nment for the start of next phrase, which is exactly what the model predicts. There is a\nlarge gray gap between the low E ♭ (pitch number 39) and the next note, which is\npredicted to be around pitch number 50, rather than continuing to rumble around\nthe depths of the instrument.\nLastly, we should check to see if our attention mechanism is working as expected. The\nhorizontal axis in Figure 11-10  shows the generated sequence of notes; the vertical\naxis shows where the attention of the network was aimed when predicting each note\nalong the horizontal axis. The color of each square shows the maximum attention\nweight across all heads at each point in the generated sequence. The darker the\nsquare, the more attention is being applied to this position in the sequence. For sim‐\nplicity, we only show the notes in this diagram, but the durations of each note are also\nbeing attended to by the network.\nWe can see that for the initial key signature, time signature, and rest, the network\nchose to place almost all of its attention on the START  token. This makes sense, as\nthese artifacts always appear at the start of a piece of music—once the notes start\nflowing the START  token essentially stops being attended to.\nAs we move beyond the initial few notes, we can see that the network places most\nattention on approximately the last two to four notes and rarely places significant\nweight on notes more than four notes ago. Again, this makes sense; there is probably\nenough information contained in the previous four notes to understand how the\nphrase might continue . Additionally, some notes attend more strongly back to the key\nsignature of D minor—for example, the E3 (7th note of the piece) and B-2 (B ♭–14th\nnote of the piece). This is fascinating, because these are the exact notes that rely on\nthe key of D minor to relieve any ambiguity. The network must look back  at the key\nsignature in order to tell that there is a B ♭ in the key signature (rather than a B natu‐\nral) but there isn’t an E ♭ in the key signature (E natural must be used instead).\nTransformers for Music Generation | 311\nFigure 11-10. The color of each square in the matrix indicates the amount of attention\ngiven to each position on the vertical axis, at the point of predicting the note on the hori‐\nzontal axis\nThere are also examples of where the network has chosen to ignore a certain note or\nrest nearby, as it doesn’t add any additional information to its understanding of the\nphrase. For example, the penultimate note ( A2) is not particularly attentive to the B-2\nthree notes back, but is slightly more attentive to the A2 four notes back. It is more\ninteresting for the model to look at the A2 that falls on the beat, rather than the B-2 off\nthe beat, which is just a passing note.\nRemember we haven’t told the model anything about which notes are related or\nwhich notes belong to which key signatures—it has worked this out for itself just by\nstudying the music of J.S. Bach.\n312 | Chapter 11: Music Generation",6475
112-Tokenization of Polyphonic Music.pdf,112-Tokenization of Polyphonic Music,"Tokenization of Polyphonic Music\nThe Transformer we’ve been exploring in this section works well for single-line\n(monophonic) music, but could it be adapted to multiline (polyphonic) music?\nThe challenge lies in how to represent the different lines of music as a single sequence\nof tokens. In the previous section we decided to split the notes and durations of the\nnotes into two distinct inputs and outputs of the network, but we also saw that we\ncould have interleaved these tokens into a single stream. We can use the same idea to\nhandle polyphonic music. Two different approaches will be introduced here: grid\ntokenization  and event-based tokenization , as discussed in the 2018 paper “Music\nTransformer: Generating Music with Long-Term Structure. ” 1\nGrid tokenization\nConsider  the two bars of music from a J.S. Bach chorale in Figure 11-11 . There are\nfour distinct parts (soprano [S], alto [A], tenor [T], bass [B]), written on different\nstaffs.\nFigure 11-11. The first two bars of a J.S. Bach chorale\nWe can imagine drawing this music on a grid, where the y-axis represents the pitch of\nthe note and the x-axis represents the number of 16th-notes (semiquavers) that have\npassed since the start of the piece. If the grid square is filled, then there is a note\nTransformers for Music Generation | 313\nplaying  at that point in time. All four parts are drawn on the same grid. This grid is\nknown as a piano roll  because it resembles a physical roll of paper with holes punched\ninto it, which was used as a recording mechanism before digital systems were\ninvented.\nWe can serialize the grid into a stream of tokens by moving first through the four voi‐\nces, then along the timesteps in sequence. This produces a sequence of tokens\nS1,A1,T1,B1,S2,A2,T2,B2, ..., where the subscript denotes the timestep, as shown in\nFigure 11-12 .\nFigure 11-12. Creating the grid tokenization for the first two bars of the Bach chorale\n314 | Chapter 11: Music Generation\nWe would then train our Transformer on this sequence of tokens, to predict the next\ntoken given the previous tokens. We can decode the generated sequence back into a\ngrid structure by rolling the sequence back out over time in groups of four notes (one\nfor each voice). This technique works surprisingly well, despite the same note often\nbeing split across multiple tokens with tokens from other voices in between.\nHowever, there are some disadvantages. Firstly, notice that there is no way for the\nmodel to tell the difference between one long note and two shorter adjacent notes of\nthe same pitch. This is because the tokenization does not explicitly encode the dura‐\ntion of notes, only whether a note is present at each timestep.\nSecondly, this method requires the music to have a regular beat that is divisible into\nreasonably sized chunks. For example, using the current system, we cannot encode\ntriplets (a group of three notes played across a single beat). We could divide the music\ninto 12 steps per quarter-note (crotchet) instead of 4, that would triple the number of\ntokens required to represent the same passage of music, adding overhead on the\ntraining process and affecting the lookback capacity of the model.\nLastly, it is not obvious how we might add other components to the tokenization,\nsuch as dynamics (how loud or quiet the music is in each part) or tempo changes. We\nare locked into the two-dimensional grid structure of the piano roll, which provides a\nconvenient way to represent pitch and timing, but not necessarily an easy way to\nincorporate other components that make music interesting to listen to.\nEvent-based tokenization\nA more flexible approach is to use event-based tokenization. This can be thought of\nas a vocabulary that literally describes how the music is created as a sequence of\nevents, using a rich set of tokens.\nFor example in Figure 11-13 , we use three types of tokens:\n•NOTE_ON< pitch> (start playing a note of a given pitch)\n•NOTE_OFF< pitch> (stop playing a note of a given pitch)\n•TIME_SHIFT< step> (shift forward in time by a given step)\nThis vocabulary can be used to create a sequence that describes the construction of\nthe music as a set of instructions.\nTransformers for Music Generation | 315\nFigure 11-13. An event tokenization for the first bar of the Bach chorale\nWe could easily incorporate other types of tokens into this vocabulary, to represent\ndynamic and tempo changes for subsequent notes. This method also provides a way\nto generate triplets against a backdrop of quarter-notes, by separating the notes of the\ntriplets with TIME_SHIFT<0.33>  tokens. Overall, it is a more expressive framework for\ntokenization, though it is also potentially more complex for the Transformer to learn\ninherent patterns in the training set music, as it is by definition less structured than\nthe grid method.\nI encourage you to try implementing these polyphonic techniques\nand train a Transformer on the new tokenized dataset using all\nthe knowledge you have built up so far in this book. I would also\nrecommend checking our Dr. Tristan Behrens’s guide to music gen‐\neration research, available on GitHub , which provides a compre‐\nhensive overview of different papers on the topic of music\ngeneration using deep learning.\nIn the next section we will take a completely different approach to music generation,\nusing GANs.\n316 | Chapter 11: Music Generation",5440
113-MuseGAN.pdf,113-MuseGAN,,0
114-The Bach Chorale Dataset.pdf,114-The Bach Chorale Dataset,"MuseGAN\nY ou may have thought that the piano roll shown in Figure 11-12  looks a bit like a\npiece of modern art. This begs the question—could we in fact treat this piano roll as a\npicture  and utilize image generation methods instead of sequence generation\ntechniques?\nAs we shall see, the answer to this question is yes, we can treat music generation\ndirectly as an image generation problem. This means that instead of using Trans‐\nformers, we can apply the same convolutional-based techniques that work so well for\nimage generation problems—in particular, GANs.\nMuseGAN  was introduced in the 2017 paper “MuseGAN: Multi-Track Sequential\nGenerative Adversarial Networks for Symbolic Music Generation and Accompani‐\nment. ” 2 The authors show how it is possible to train a model to generate polyphonic,\nmultitrack, multibar music through a novel GAN framework. Moreover, they show\nhow, by dividing up the responsibilities of the noise vectors that feed the generator,\nthey are able to maintain fine-grained control over the high-level temporal and track-\nbased features of the music.\nLet’s start by introducing the the J.S. Bach chorale dataset.\nRunning the Code for This Example\nThe code for this example can be found in the Jupyter notebook\nlocated at notebooks/11_music/02_musegan/musegan.ipynb  in the\nbook repository.\nThe Bach Chorale Dataset\nTo begin this project, you’ll first need to download the MIDI files that we’ll be using\nto train the MuseGAN. We’ll use a dataset of 229 J.S. Bach chorales for four voices.\nY ou can download the dataset by running the Bach chorale dataset downloader script\nin the book repository, as shown in Example 11-5 . This will save the MIDI files locally\nto the /data  folder.\nExample 11-5. Downloading the Bach chorale dataset\nbash scripts/download_bach_chorale_data.sh\nMuseGAN | 317\nThe dataset consists of an array of four numbers for each timestep: the MIDI note\npitches of each of the four voices. A timestep in this dataset is equal to a 16th note (a\nsemiquaver). So, for example, in a single bar of 4 quarter (crotchet) beats, there are 16\ntimesteps. The dataset is automatically split into train , validation , and test sets. We\nwill be using the train  dataset to train the MuseGAN.\nTo start, we need to get the data into the correct shape to feed the GAN. In this exam‐\nple we’ll generate two bars of music, so we’ll extract only the first two bars of each\nchorale. Each bar consists of 16 timesteps and there are a potential 84 pitches across\nthe 4 voices.\nVoices will be referred to as tracks  from here on, to keep the termi‐\nnology in line with the original paper.\nTherefore, the transformed data will have the following shape:\n[BATCH_SIZE, N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS]\nwhere:\nBATCH_SIZE = 64\nN_BARS = 2\nN_STEPS_PER_BAR = 16\nN_PITCHES = 84\nN_TRACKS = 4\nTo get the data into this shape, we one-hot encode the pitch numbers into a vector of\nlength 84 and split each sequence of notes into two bars of 16 timesteps each. We are\nmaking the assumption here that each chorale in the dataset has four beats in each\nbar, which is reasonable, and even if this were not the case it would not adversely\naffect the training of the model.\nFigure 11-14  shows how two bars of raw data are converted into the transformed\npiano roll dataset that we will use to train the GAN.\n318 | Chapter 11: Music Generation\nFigure 11-14. Processing two bars of raw data into piano roll data that we can use to\ntrain the GAN\nMuseGAN | 319",3537
115-The MuseGAN Generator.pdf,115-The MuseGAN Generator,"The MuseGAN Generator\nLike  all GANs, MuseGAN consists of a generator and a critic. The generator tries to\nfool the critic with its musical creations, and the critic tries to prevent this from hap‐\npening by ensuring it is able to tell the difference between the generator’s forged Bach\nchorales and the real thing.\nWhere MuseGAN differs is in the fact that the generator doesn’t just accept a single\nnoise vector as input, but instead has four separate inputs, which correspond to four\ndifferent characteristics of the music: chords, style, melody, and groove. By manipu‐\nlating each of these inputs independently we can change high-level properties of the\ngenerated music.\nA high-level view of the generator is shown in Figure 11-15 .\nFigure 11-15. High-level diagram of the MuseGAN generator\nThe diagram shows how the chords and melody inputs are first passed through  a\ntemporal network  that outputs a tensor with one of the dimensions equal to the num‐\nber of bars to be generated. The style and groove inputs are not stretched temporally\nin this way, as they remain constant through the piece.\nThen, to generate a particular bar for a particular track, the relevant outputs from the\nchords, style, melody, and groove parts of the network are concatenated to form a\nlonger vector. This is then passed to a bar generator, which ultimately outputs the\nspecified bar for the specified track.\n320 | Chapter 11: Music Generation\nBy concatenating the generated bars for all tracks, we create a score that can be com‐\npared with real scores by the critic.\nLet’s first take a look at how to build a temporal network.\nThe temporal network\nThe job of a temporal network—a neural network consisting of convolutional trans‐\npose layers—is to transform a single input noise vector of length Z_DIM = 32  into a\ndifferent noise vector for every bar (also of length 32). The Keras code to build this is\nshown in Example 11-6 .\nExample 11-6. Building the temporal network\ndef conv_t(x, f, k, s, a, p, bn):\n    x = layers.Conv2DTranspose (\n                filters = f\n                , kernel_size  = k\n                , padding = p\n                , strides = s\n                , kernel_initializer  = initializer\n                )(x)\n    if bn:\n        x = layers.BatchNormalization (momentum  = 0.9)(x)\n    x = layers.Activation (a)(x)\n    return x\ndef TemporalNetwork ():\n    input_layer  = layers.Input(shape=(Z_DIM,), name='temporal_input' ) \n    x = layers.Reshape([1,1,Z_DIM])(input_layer ) \n    x = conv_t(\n        x, f=1024, k=(2,1), s=(1,1), a = 'relu', p = 'valid', bn = True\n    ) \n    x = conv_t(\n        x, f=Z_DIM, k=(N_BARS - 1,1), s=(1,1), a = 'relu', p = 'valid', bn = True\n    )\n    output_layer  = layers.Reshape([N_BARS, Z_DIM])(x) \n    return models.Model(input_layer , output_layer )\nThe input to the temporal network is a vector of length 32 ( Z_DIM ).\nWe reshape this vector to a 1 × 1 tensor with 32 channels, so that we can apply\nconvolutional 2D transpose operations to it.\nWe apply Conv2DTranspose  layers to expand the size of the tensor along one axis,\nso that it is the same length as N_BARS .\nMuseGAN | 321\nWe remove the unnecessary extra dimension with a Reshape  layer.\nThe reason we use convolutional operations rather than requiring two independent\nvectors into the network is because we would like the network to learn how one bar\nshould follow on from another in a consistent way. Using a neural network to expand\nthe input vector along the time axis means the model has a chance to learn how\nmusic flows across bars, rather than treating each bar as completely independent of\nthe last.\nChords, style, melody, and groove\nLet’s now take a closer look at the four different inputs that feed the generator:\nChords\nThe chords input is a single noise vector of length Z_DIM . This vector’s job is to\ncontrol the general progression of the music over time, shared across tracks, so\nwe use a TemporalNetwork  to transform this single vector into a different latent\nvector for every bar. Note that while we call this input chords, it really could con‐\ntrol anything about the music that changes per bar, such as general rhythmic\nstyle, without being specific to any particular track.\nStyle\nThe style input is also a vector of length Z_DIM . This is carried forward without\ntransformation, so it is the same across all bars and tracks. It can be thought of as\nthe vector that controls the overall style of the piece (i.e., it affects all bars and\ntracks consistently).\nMelody\nThe melody input is an array of shape [N_TRACKS, Z_DIM] —that is, we provide\nthe model with a random noise vector of length Z_DIM  for each track.\nEach of these vectors is passed through a track-specific TemporalNetwork , where\nthe weights are not shared between tracks. The output is a vector of length Z_DIM\nfor every bar of every track. The model can therefore use these input vectors to\nfine-tune the content of every single bar and track independently.\nGroove\nThe groove input is also an array of shape [N_TRACKS, Z_DIM] —a random noise\nvector of length Z_DIM  for each track. Unlike the melody input, these vectors are\nnot passed through the temporal network but instead are fed straight through,\njust like the style vector. Therefore, each groove vector will affect the overall\nproperties of a track, across all bars.\n322 | Chapter 11: Music Generation\nWe can summarize the responsibilities of each component of the MuseGAN genera‐\ntor as shown in Table 11-1 .\nTable 11-1. Components of the MuseGAN generator\nOutput differs  across bars? Output differs  across parts?\nStyle Ｘ Ｘ\nGroove Ｘ ✓\nChords ✓ Ｘ\nMelody ✓ ✓\nThe final piece of the MuseGAN generator is the bar generator —let’s see how we can\nuse this to glue together the outputs from the chord, style, melody, and groove\ncomponents.\nThe bar generator\nThe bar generator receives four latent vectors—one from each of the chord, style,\nmelody, and groove components. These are concatenated to produce a vector of\nlength 4 * Z_DIM  as input. The output is a piano roll representation of a single bar\nfor a single track—i.e., a tensor of shape [1, n_steps_per_bar, n_pitches, 1] .\nThe bar generator is just a neural network that uses convolutional transpose layers to\nexpand the time and pitch dimensions of the input vector. We create one bar genera‐\ntor for every track, and weights are not shared between tracks. The Keras code to\nbuild a BarGenerator  is given in Example 11-7 .\nExample 11-7. Building the BarGenerator\ndef BarGenerator ():\n    input_layer  = layers.Input(shape=(Z_DIM * 4,), name='bar_generator_input' ) \n    x = layers.Dense(1024)(input_layer ) \n    x = layers.BatchNormalization (momentum  = 0.9)(x)\n    x = layers.Activation ('relu')(x)\n    x = layers.Reshape([2,1,512])(x)\n    x = conv_t(x, f=512, k=(2,1), s=(2,1), a= 'relu',  p = 'same', bn = True) \n    x = conv_t(x, f=256, k=(2,1), s=(2,1), a= 'relu', p = 'same', bn = True)\n    x = conv_t(x, f=256, k=(2,1), s=(2,1), a= 'relu', p = 'same', bn = True)\n    x = conv_t(x, f=256, k=(1,7), s=(1,7), a= 'relu', p = 'same', bn = True) \n    x = conv_t(x, f=1, k=(1,12), s=(1,12), a= 'tanh', p = 'same', bn = False) \n    output_layer  = layers.Reshape([1, N_STEPS_PER_BAR  , N_PITCHES  ,1])(x) \n    return models.Model(input_layer , output_layer )\nMuseGAN | 323\nThe input to the bar generator is a vector of length 4 * Z_DIM .\nAfter passing it through a Dense  layer, we reshape the tensor to prepare it for the\nconvolutional transpose operations.\nFirst we expand the tensor along the timestep axis…\n…then along the pitch axis.\nThe final layer has a tanh activation applied, as we will be using a WGAN-GP\n(which requires tanh output activation) to train the network.\nThe tensor is reshaped to add two extra dimensions of size 1, to prepare it for\nconcatenation with other bars and tracks.\nPutting it all together\nUltimately, the MuseGAN generator takes the four input noise tensors (chords, style,\nmelody, and groove) and converts them into a multitrack, multibar score. The Keras\ncode to build the MuseGAN generator is provided in Example 11-8 .\nExample 11-8. Building the MuseGAN generator\ndef Generator ():\n    chords_input  = layers.Input(shape=(Z_DIM,), name='chords_input' ) \n    style_input  = layers.Input(shape=(Z_DIM,), name='style_input' )\n    melody_input  = layers.Input(shape=(N_TRACKS , Z_DIM), name='melody_input' )\n    groove_input  = layers.Input(shape=(N_TRACKS , Z_DIM), name='groove_input' )\n    chords_tempNetwork  = TemporalNetwork () \n    chords_over_time  = chords_tempNetwork (chords_input )\n    melody_over_time  = [None] * N_TRACKS\n    melody_tempNetwork  = [None] * N_TRACKS\n    for track in range(N_TRACKS ):\n        melody_tempNetwork [track] = TemporalNetwork () \n        melody_track  = layers.Lambda(lambda x, track = track: x[:,track,:])(\n            melody_input\n        )\n        melody_over_time [track] = melody_tempNetwork [track](melody_track )\n    barGen = [None] * N_TRACKS\n    for track in range(N_TRACKS ):\n        barGen[track] = BarGenerator () \n    bars_output  = [None] * N_BARS\n    c = [None] * N_BARS\n    for bar in range(N_BARS): \n324 | Chapter 11: Music Generation\n        track_output  = [None] * N_TRACKS\n        c[bar] = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(chords_over_time )\n        s = style_input\n        for track in range(N_TRACKS ):\n            m = layers.Lambda(lambda x, bar = bar: x[:,bar,:])(\n                melody_over_time [track]\n            )\n            g = layers.Lambda(lambda x, track = track: x[:,track,:])(\n                groove_input\n            )\n            z_input = layers.Concatenate (\n                axis = 1, name = 'total_input_bar_ {}_track_{}'.format(bar, track)\n            )([c[bar],s,m,g])\n            track_output [track] = barGen[track](z_input)\n        bars_output [bar] = layers.Concatenate (axis = -1)(track_output )\n    generator_output  = layers.Concatenate (axis = 1, name = 'concat_bars' )(\n        bars_output\n    ) \n    return models.Model(\n        [chords_input , style_input , melody_input , groove_input ], generator_output\n    ) \ngenerator  = Generator ()\nDefine the inputs to the generator.\nPass the chords input through the temporal network.\nPass the melody input through the temporal network.\nCreate an independent bar generator network for every track.\nLoop over the tracks and bars, creating a generated bar for each combination.\nConcatenate everything together to form a single output tensor.\nThe MuseGAN model takes four distinct noise tensors as input and outputs a\ngenerated multitrack, multibar score.\nMuseGAN | 325",10761
116-Analysis of the MuseGAN.pdf,116-Analysis of the MuseGAN,"The MuseGAN Critic\nIn comparison to the generator, the critic architecture is much more straightforward\n(as is often the case with GANs).\nThe critic tries to distinguish full multitrack, multibar scores created by the generator\nfrom real excerpts from the Bach chorales. It is a convolutional neural network, con‐\nsisting mostly of Conv3D  layers that collapse the score into a single output prediction.\nConv3D Layers\nSo far in this book, we have only worked with Conv2D  layers, appli‐\ncable to three-dimensional input images (width, height, channels).\nHere we have to use Conv3D  layers, which are analogous to Conv2D\nlayers but accept four-dimensional input tensors ( n_bars ,\nn_steps_per_bar , n_pitches , n_tracks ).\nWe do not use batch normalization layers in the critic as we will be using the WGAN-\nGP framework for training the GAN, which forbids this.\nThe Keras code to build the critic is given in Example 11-9 .\nExample 11-9. Building the MuseGAN critic\ndef conv(x, f, k, s, p):\n    x = layers.Conv3D(filters = f\n                , kernel_size  = k\n                , padding = p\n                , strides = s\n                , kernel_initializer  = initializer\n                )(x)\n    x = layers.LeakyReLU ()(x)\n    return x\ndef Critic():\n    critic_input  = layers.Input(\n        shape=(N_BARS, N_STEPS_PER_BAR , N_PITCHES , N_TRACKS ),\n        name='critic_input'\n    ) \n    x = critic_input\n    x = conv(x, f=128, k = (2,1,1), s = (1,1,1), p = 'valid') \n    x = conv(x, f=128, k = (N_BARS - 1,1,1), s = (1,1,1), p = 'valid')\n    x = conv(x, f=128, k = (1,1,12), s = (1,1,12), p = 'same') \n    x = conv(x, f=128, k = (1,1,7), s = (1,1,7), p = 'same')\n    x = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = 'same') \n    x = conv(x, f=128, k = (1,2,1), s = (1,2,1), p = 'same')\n326 | Chapter 11: Music Generation\n    x = conv(x, f=256, k = (1,4,1), s = (1,2,1), p = 'same')\n    x = conv(x, f=512, k = (1,3,1), s = (1,2,1), p = 'same')\n    x = layers.Flatten()(x)\n    x = layers.Dense(1024, kernel_initializer  = initializer )(x)\n    x = layers.LeakyReLU ()(x)\n    critic_output  = layers.Dense(\n        1, activation =None, kernel_initializer  = initializer\n    )(x) \n    return models.Model(critic_input , critic_output )\ncritic = Critic()\nThe input to the critic is an array of multitrack, multibar scores, each of shape\n[N_BARS, N_STEPS_PER_BAR, N_PITCHES, N_TRACKS] .\nFirst, we collapse the tensor along the bar axis. We apply Conv3D  layers through‐\nout the critic as we are working with 4D tensors.\nNext, we collapse the tensor along the pitch axis.\nFinally, we collapse the tensor along the timesteps axis.\nThe output is a Dense  layer with a single unit and no activation function, as\nrequired by the WGAN-GP framework.\nAnalysis of the MuseGAN\nWe can perform some experiments with our MuseGAN by generating a score, then\ntweaking some of the input noise parameters to see the effect on the output.\nThe output from the generator is an array of values in the range [–1, 1] (due to the\ntanh activation function of the final layer). To convert this to a single note for each\ntrack, we choose the note with the maximum value over all 84 pitches for each time‐\nstep. In the original MuseGAN paper the authors use a threshold of 0, as each track\ncan contain multiple notes; however, in this setting we can simply take the maximum\nto guarantee exactly one note per timestep per track, as is the case for the Bach\nchorales.\nFigure 11-16  shows a score that has been generated by the model from random nor‐\nmally distributed noise vectors (top left). We can find the closest score in the dataset\n(by Euclidean distance) and check that our generated score isn’t a copy of a piece of\nmusic that already exists in the dataset—the closest score is shown just below it, and\nwe can see that it does not resemble our generated score.\nMuseGAN | 327\nFigure 11-16. Example of a MuseGAN predicted score, showing the closest real score in\nthe training data and how the generated score is affected  by changing the input noise\nLet’s now play around with the input noise to tweak our generated score. First, we can\ntry changing the chord noise vector—the bottom-left score in Figure 11-16  shows the\nresult. We can see that every track has changed, as expected, and also that the two\nbars exhibit different properties. In the second bar, the baseline is more dynamic and\nthe top line is higher in pitch than in the first bar. This is because the latent vectors\nthat affect the two bars are different, as the input chord vector was passed through a\ntemporal network.\nWhen we change the style vector (top right), both bars change in a similar way. The\nwhole passage has changed style from the original generated score, in a consistent\nway (i.e., the same latent vector is being used to adjust all tracks and bars).\nWe can also alter tracks individually, through the melody and groove inputs. In the\ncenter-right score in Figure 11-16  we can see the effect of changing just the melody\nnoise input for the top line of music. All other parts remain unaffected, but the top-\nline notes change significantly. Also, we can see a rhythmic change between the two\n328 | Chapter 11: Music Generation",5270
117-Summary.pdf,117-Summary,"bars in the top line: the second bar is more dynamic, containing faster notes than the\nfirst bar.\nLastly, the bottom-right score in the diagram shows the predicted score when we alter\nthe groove input parameter for only the baseline. Again, all other parts remain unaf‐\nfected, but the baseline is different. Moreover, the overall pattern of the baseline\nremains similar between bars, as we would expect.\nThis shows how each of the input parameters can be used to directly influence high-\nlevel features of the generated musical sequence, in much the same way as we were\nable to adjust the latent vectors of V AEs and GANs in previous chapters to alter the\nappearance of a generated image. One drawback to the model is that the number of\nbars to generate must be specified up front. To tackle this, the authors show an exten‐\nsion to the model that allows previous bars to be fed in as input, allowing the model\nto generate long-form scores by continually feeding the most recent predicted bars\nback in as additional input.\nSummary\nIn this chapter we have explored two different kinds of models for music generation:\na Transformer and a MuseGAN.\nThe Transformer is similar in design to the networks we saw in Chapter 9  for text\ngeneration. Music and text generation share a lot of features in common, and often\nsimilar techniques can be used for both. We extended the Transformer architecture\nby incorporating two input and output streams, for note and duration. We saw how\nthe model was able to learn about concepts such as keys and scales, simply by learn‐\ning to accurately generate the music of Bach.\nWe also explored how we can adapt the tokenization process to handle polyphonic\n(multitrack) music generation. Grid tokenization serializes a piano roll representation\nof the score, allowing us to train a Transformer on a single stream of tokens that\ndescribe which note is present in each voice, at discrete, equally spaced timestep\nintervals. Event-based tokenization produces a recipe  that describes how to create the\nmultiple lines of music in a sequential fashion, through a single stream of instruc‐\ntions. Both methods have advantages and disadvantages—the success or failure of a\nTransformer-based approach to music generation is often heavily dependent on the\nchoice of tokenization method.\nWe also saw that generating music does not always require a sequential approach—\nMuseGAN uses convolutions to generate polyphonic musical scores with multiple\ntracks, by treating the score as an image where the tracks are individual channels of\nthe image. The novelty of MuseGAN lies in the way the four input noise vectors\n(chords, style, melody, and groove) are organized so that it is possible to maintain full\ncontrol over high-level features of the music. While the underlying harmonization is\nSummary | 329\nstill not as perfect or varied as Bach’s, it is a good attempt at what is an extremely diffi‐\ncult problem to master and highlights the power of GANs to tackle a wide variety of\nproblems.\nReferences\n1. Cheng-Zhi Anna Huang et al., “Music Transformer: Generating Music with Long-\nTerm Structure, ” September 12, 2018, https://arxiv.org/abs/1809.04281 .\n2. Hao-Wen Dong et al., “MuseGAN: Multi-Track Sequential Generative Adversarial\nNetworks for Symbolic Music Generation and Accompaniment, ” September 19, 2017,\nhttps://arxiv.org/abs/1709.06298 .\n330 | Chapter 11: Music Generation",3449
118-Chapter 12. World Models.pdf,118-Chapter 12. World Models,,0
119-Reinforcement Learning.pdf,119-Reinforcement Learning,"CHAPTER 12\nWorld Models\nChapter Goals\nIn this chapter you will:\n•Walk through the basics of reinforcement learning (RL).\n•Understand how generative modeling can be used within a world model  approach\nto RL.\n•See how to train a variational autoencoder (V AE) to capture environment obser‐\nvations in a low-dimensional latent space.\n•Walk through the training process of a mixture density network–recurrent neu‐\nral network (MDN-RNN) that predicts the latent variable.\n•Use the covariance matrix adaptation evolution strategy (CMA-ES) to train a\ncontroller that can take intelligent actions in the environment.\n•Understand how the trained MDN-RNN can itself be used as an environment,\nallowing the agent to train the controller within its own hallucinated dreams,\nrather than the real environment.\nThis chapter introduces one of the most interesting applications of generative models\nin recent years, namely their use within so-called world models.\nIntroduction\nIn March 2018, David Ha and Jürgen Schmidhuber published their “World Models”\npaper. 1 The paper showed how it is possible to train a model that can learn how to\nperform a particular task through experimentation within its own generated dream\nenvironment, rather than inside the real environment. It is an excellent example of\n331\nhow generative modeling can be used to solve practical problems, when applied\nalongside other machine learning techniques such as reinforcement learning.\nA key component of the architecture is a generative model that can construct a prob‐\nability distribution for the next possible state, given the current state and action. Hav‐\ning built up an understanding of the underlying physics of the environment through\nrandom movements, the model is then able to train itself from scratch on a new task,\nentirely within its own internal representation of the environment. This approach led\nto world-best scores for both of the tasks on which it was tested.\nIn this chapter we will explore the model from the paper in detail, with particular\nfocus on a task that requires the agent to learn how to drive a car around a virtual\nracetrack as fast as possible. While we will be using a 2D computer simulation as our\nenvironment, the same technique could also be applied to real-world scenarios where\ntesting strategies in the live environment is expensive or infeasible.\nIn this chapter we will reference the excellent TensorFlow imple‐\nmentation of the “World Models” paper available publicly on\nGitHub , which I encourage you to clone and run yourself!\nBefore we start exploring the model, we need to take a closer look at the concept of\nreinforcement learning.\nReinforcement Learning\nReinforcement learning can be defined as follows:\nReinforcement learning (RL) is a field of machine learning that aims to train an agent\nto perform optimally within a given environment, with respect to a particular goal.\nWhile both discriminative modeling and generative modeling aim to minimize a loss\nfunction over a dataset of observations, reinforcement learning aims to maximize the\nlong-term reward of an agent in a given environment. It is often described as one of\nthe three major branches of machine learning, alongside  supervised learning  (predict‐\ning using labeled data) and unsupervised learning  (learning structure from unlabeled\ndata).\nLet’s first introduce some key terminology related to reinforcement learning:\nEnvironment\nThe world in which the agent operates. It defines the set of rules that govern the\ngame state update process and reward allocation, given the agent’s previous\naction and current game state. For example, if we were teaching a reinforcement\nlearning algorithm to play chess, the environment would consist of the rules that\n332 | Chapter 12: World Models\ngovern how a given action (e.g., the pawn move e2e4 ) affects the next game state\n(the new positions of the pieces on the board) and would also specify how to\nassess if a given position is checkmate and allocate the winning player a reward of\n1 after the winning move.\nAgent\nThe entity that takes actions in the environment.\nGame state\nThe data that represents a particular situation that the agent may encounter (also\njust called a state ). For example, a particular chessboard configuration with\naccompanying game information such as which player will make the next move.\nAction\nA feasible move that an agent can make.\nReward\nThe value given back to the agent by the environment after an action has been\ntaken. The agent aims to maximize the long-term sum of its rewards. For exam‐\nple, in a game of chess, checkmating the opponent’s king has a reward of 1 and\nevery other move has a reward of 0. Other games have rewards constantly awar‐\nded throughout the episode (e.g., points in a game of Space Invaders ).\nEpisode\nOne run of an agent in the environment; this is also called a rollout .\nTimestep\nFor a discrete event environment, all states, actions, and rewards are subscripted\nto show their value at timestep t.\nThe relationship between these concepts is shown in Figure 12-1 .\nFigure 12-1. Reinforcement learning diagram\nThe environment is first initialized with a current game state, s0. At timestep t, the\nagent receives the current game state st and uses this to decide on its next best action\nat, which it then performs. Given this action, the environment then calculates the\nnext state st+ 1 and reward rt+ 1 and passes these back to the agent, for the cycle to\nReinforcement Learning | 333",5556
120-The CarRacing Environment.pdf,120-The CarRacing Environment,"begin again. The cycle continues until the end criterion of the episode is met (e.g., a\ngiven number of timesteps elapse or the agent wins/loses).\nHow can we design an agent to maximize the sum of rewards in a given environ‐\nment? We could build an agent that contains a set of rules for how to respond to any\ngiven game state. However, this quickly becomes infeasible as the environment\nbecomes more complex and doesn’t ever allow us to build an agent that has\nsuperhuman  ability in a particular task, as we are hardcoding the rules. Reinforce‐\nment learning involves creating an agent that can learn optimal strategies by itself in\ncomplex environments through repeated play.\nLet’s now take a look at the CarRacing  environment that simulates a car driving\naround a track.\nThe CarRacing Environment\nCarRacing  is an environment that is available through the Gymnasium  package.\nGymnasium is a Python library for developing reinforcement learning algorithms\nthat contains several classic reinforcement learning environments, such as CartPole\nand Pong , as well as environments that present more complex challenges, such as\ntraining an agent to walk on uneven terrain or win an Atari game.\nGymnasium\nGymnasium is a maintained fork of OpenAI’s Gym library—since\n2021, further development of Gym has shifted to Gymnasium. In\nthis book, we therefore refer to Gymnasium environments as Gym\nenvironments.\nAll of the environments provide a step method through which you can submit a given\naction; the environment will return the next state and the reward. By repeatedly\ncalling the step method with the actions chosen by the agent, you can play out an epi‐\nsode in the environment. There is also a reset  method for returning the environment\nto its initial state and a render  method that allows you to watch your agent perform in\na given environment. This is useful for debugging and finding areas where your agent\ncould improve.\n334 | Chapter 12: World Models\nLet’s see how the game state, action, reward, and episode are defined for the\nCarRacing  environment:\nGame state\nA 64 × 64–pixel RGB image depicting an overhead view of the track and car.\nAction\nA set of three values: the steering direction (–1 to 1), acceleration (0 to 1), and\nbraking (0 to 1). The agent must set all three values at each timestep.\nReward\nA negative penalty of –0.1 for each timestep taken and a positive reward of 1,000/\nN if a new track tile is visited, where N is the total number of tiles that make up\nthe track.\nEpisode\nThe episode ends when the car completes the track or drives off the edge of the\nenvironment, or when 3,000 timesteps have elapsed.\nThese concepts are shown on a graphical representation of a game state in\nFigure 12-2 .\nFigure 12-2. A graphical representation of one game state in the CarRacing  environment\nPerspective\nWe should imagine the agent floating above the track and control‐\nling the car from a bird’s-eye view, rather than viewing the track\nfrom the driver’s perspective.\nReinforcement Learning | 335",3063
121-World Model Overview.pdf,121-World Model Overview,,0
122-Architecture.pdf,122-Architecture,"World Model Overview\nWe’ll  now cover a high-level overview of the entire world model architecture and\ntraining process, before diving into each component in more detail.\nArchitecture\nThe solution consists of three distinct parts, as shown in Figure 12-3 , that are trained\nseparately:\nV\nA variational autoencoder (V AE)\nM\nA recurrent neural network with a mixture density network (MDN-RNN)\nC\nA controller\nFigure 12-3. World model architecture diagram\nThe VAE\nWhen  you make decisions while driving, you don’t actively analyze every single pixel\nin your view—instead, you condense the visual information into a smaller number of\nlatent entities, such as the straightness of the road, upcoming bends, and your posi‐\ntion relative to the road, to inform your next action.\nWe saw in Chapter 3  how a V AE can take a high-dimensional input image and con‐\ndense it into a latent random variable that approximately follows a standard Gaussian\n336 | Chapter 12: World Models\ndistribution, through minimization of the reconstruction error and KL divergence.\nThis ensures that the latent space is continuous and that we are able to easily sample\nfrom it to generate meaningful new observations.\nIn the car racing example, the V AE condenses the 64 × 64 × 3 (RGB) input image into\na 32-dimensional normally distributed random variable, parameterized by two vari‐\nables, mu and logvar . Here, logvar  is the logarithm of the variance of the distribu‐\ntion. We can sample from this distribution to produce a latent vector z that represents\nthe current state. This is passed on to the next part of the network, the MDN-RNN.\nThe MDN-RNN\nAs you drive, each subsequent observation isn’t a complete surprise to you. If the cur‐\nrent observation suggests a left turn in the road ahead and you turn the wheel to the\nleft, you expect the next observation to show that you are still in line with the road.\nIf you didn’t have this ability, your car would probably snake all over the road as you\nwouldn’t be able to see that a slight deviation from the center is going to be worse in\nthe next timestep unless you do something about it now.\nThis forward thinking is the job of the MDN-RNN, a network that tries to predict the\ndistribution of the next latent state based on the previous latent state and the previous\naction.\nSpecifically, the MDN-RNN is an LSTM layer with 256 hidden units followed by a\nmixture density network (MDN) output layer that allows for the fact that the next\nlatent state could actually be drawn from any one of several normal distributions.\nThe same technique was applied by one of the authors of the “World Models” paper,\nDavid Ha, to a handwriting generation  task, as shown in Figure 12-4 , to describe the\nfact that the next pen point could land in any one of the distinct red areas.\nFigure 12-4. MDN for handwriting generation\nIn the car racing example, we allow for each element of the next observed latent state\nto be drawn from any one of five normal distributions.\nWorld Model Overview | 337",3051
123-The VAE Architecture.pdf,123-The VAE Architecture,"The controller\nUntil this point, we haven’t mentioned anything about choosing an action. That\nresponsibility lies with the controller. The controller is a densely connected neural\nnetwork, where the input is a concatenation of z (the current latent state sampled\nfrom the distribution encoded by the V AE) and the hidden state of the RNN. The\nthree output neurons correspond to the three actions (turn, accelerate, brake) and are\nscaled to fall in the appropriate ranges.\nThe controller is trained using reinforcement learning as there is no training dataset\nthat will tell us that a certain action is good  and another is bad. Instead, the agent dis‐\ncovers this for itself through repeated experimentation.\nAs we shall see later in the chapter, the crux of the “World Models” paper is that it\ndemonstrates how this reinforcement learning can take place within the agent’s own\ngenerative model of the environment, rather than the Gym environment. In other\nwords, it takes place in the agent’s hallucinated  version of how the environment\nbehaves, rather than the real thing.\nTo understand the different roles of the three components and how they work\ntogether, we can imagine a dialogue between them:\nVAE  (looking at latest 64 × 64 × 3 observation): This looks like a straight road, with a\nslight left bend approaching, with the car facing in the direction of the road ( z).\nRNN : Based on that description ( z) and the fact that the controller chose to accelerate\nhard at the last timestep ( action ), I will update my hidden state ( h) so that the next\nobservation is predicted to still be a straight road, but with slightly more left turn in\nview.\nController : Based on the description from the V AE ( z) and the current hidden state\nfrom the RNN ( h), my neural network outputs [0.34, 0.8, 0]  as the next action.\nThe action from the controller is then passed to the environment, which returns an\nupdated observation, and the cycle begins again.\nTraining\nThe training process consists of five steps, run in sequence, which are outlined here:\n1.Collect random rollout data. Here, the agent does not care about the given task,\nbut instead simply explores the environment using random actions. Multiple epi‐\nsodes are simulated and the observed states, actions, and rewards at each time‐\nstep are stored. The idea is to build up a dataset of how the physics of the\nenvironment works, which the V AE can then learn from to capture the states\nefficiently as latent vectors. The MDN-RNN can then subsequently learn how the\nlatent vectors evolve over time.\n338 | Chapter 12: World Models\n2.Train the V AE. Using the randomly collected data, we train a V AE on the obser‐\nvation images.\n3.Collect data to train the MDN-RNN. Once we have a trained V AE, we use it to\nencode each of the collected observations into mu and logvar  vectors, which are\nsaved alongside the current action and reward.\n4.Train the MDN-RNN. We take batches of episodes and load the corresponding\nmu, logvar , action , and reward  variables at each timestep that were generated in\nstep 3. We then sample a z vector from the mu and logvar  vectors. Given the cur‐\nrent z vector, action , and reward , the MDN-RNN is then trained to predict the\nsubsequent z vector and reward .\n5.Train the controller. With a trained V AE and RNN, we can now train the control‐\nler to output an action given the current z and hidden state, h, of the RNN. The\ncontroller uses an evolutionary algorithm, CMA-ES, as its optimizer. The algo‐\nrithm rewards matrix weightings that generate actions that lead to overall high\nscores on the task, so that future generations are also likely to inherit this desired\nbehavior.\nLet’s now take look at each of these steps in more detail.\nCollecting Random Rollout Data\nThe first step is to collect rollout data from the environment, using an agent taking\nrandom actions. This may seem strange, given we ultimately want our agent to learn\nhow to take intelligent actions, but this step will provide the data that the agent will\nuse to learn how the world operates and how its actions (albeit random at first) influ‐\nence subsequent observations.\nWe can capture multiple episodes in parallel by spinning up multiple Python pro‐\ncesses, each running a separate instance of the environment. Each process will run on\na separate core, so if your machine has lots of cores you can collect data much faster\nthan if you only have a few cores.\nThe hyperparameters used by this step are as follows:\nparallel_processes\nThe number of parallel processes to run (e.g., 8 if your machine has ≥8 cores)\nmax_trials\nHow many episodes each process should run in total (e.g., 125, so 8 processes\nwould create 1,000 episodes overall)\nmax_frames\nThe maximum number of timesteps per episode (e.g., 300)\nCollecting Random Rollout Data | 339\nFigure 12-5  shows an excerpt from frames 40 to 59 of one episode, as the car\napproaches a corner, alongside the randomly chosen action and reward. Note how\nthe reward changes to 3.22 as the car rolls over new track tiles but is otherwise –0.1.\nFigure 12-5. Frames 40 to 59 of one episode\nTraining the VAE\nWe now build a generative model (a V AE) on this collected data. Remember, the aim\nof the V AE is to allow us to collapse one 64 × 64 × 3 image into a normally distributed\nrandom variable z, whose distribution is parameterized by two vectors, mu and\n340 | Chapter 12: World Models\nlogvar . Each of these vectors is of length 32. The hyperparameters of this step are as\nfollows:\nvae_batch_size\nThe batch size to use when training the V AE (how many observations per batch)\n(e.g., 100)\nz_size\nThe length of latent z vector (and therefore mu and logvar  variables) (e.g., 32)\nvae_num_epoch\nThe number of training epochs (e.g., 10)\nThe VAE Architecture\nAs we have seen previously, Keras allows us to not only define the V AE model that\nwill be trained end-to-end, but also additional submodels that define the encoder and\ndecoder of the trained network separately. These will be useful when we want to\nencode a specific image or decode a given z vector, for example. We’ll define the V AE\nmodel and three submodels, as follows:\nvae\nThis is the end-to-end V AE that is trained. It accepts a 64 × 64 × 3 image as input\nand outputs a reconstructed 64 × 64 × 3 image.\nencode_mu_logvar\nThis accepts a 64 × 64 × 3 image as input and outputs the mu and logvar  vectors\ncorresponding to this input. Running the same input image through this model\nmultiple times will produce the same mu and logvar  vectors each time.\nencode\nThis accepts a 64 × 64 × 3 image as input and outputs a sampled z vector. Run‐\nning the same input image through this model multiple times will produce a dif‐\nferent z vector each time, using the calculated mu and logvar  values to define the\nsampling distribution.\ndecode\nThis accepts a z vector as input and returns the reconstructed 64 × 64 × 3 image.\nA diagram of the model and submodels is shown in Figure 12-6 .\nTraining the VAE | 341\nFigure 12-6. The VAE architecture from the “World Models” paper\n342 | Chapter 12: World Models",7172
124-Exploring the VAE.pdf,124-Exploring the VAE,"Exploring the VAE\nWe’ll now take a look at the output from the V AE and each submodel and then see\nhow the V AE can be used to generate completely new track observations.\nThe VAE model\nIf we feed the V AE with an observation, it is able to accurately reconstruct the origi‐\nnal image, as shown in Figure 12-7 . This is useful to visually check that the V AE is\nworking correctly.\nFigure 12-7. The input and output from the VAE model\nThe encoder models\nIf we feed the encode_mu_logvar  model with an observation, the output is the gener‐\nated mu and logvar  vectors describing a multivariate normal distribution. The encode\nmodel goes one step further by sampling a particular z vector from this distribution.\nThe diagram showing the output from the two encoder models is shown in\nFigure 12-8 .\nFigure 12-8. The output from the encoder models\nTraining the VAE | 343\nThe latent variable z is sampled from the Gaussian defined by mu and logvar  by sam‐\npling from a standard Gaussian and then scaling and shifting the sampled vector\n(Example 12-1 ).\nExample 12-1. Sampling z from the multivariate normal distribution defined  by mu and\nlogvar\neps = tf.random_normal (shape=tf.shape(mu))\nsigma = tf.exp(logvar * 0.5)\nz = mu + eps * sigma\nThe decoder model\nThe decode  model accepts a z vector as input and reconstructs the original image. In\nFigure 12-9  we linearly interpolate two of the dimensions of z to show how each\ndimension appears to encode a particular aspect of the track—in this example z[4]\ncontrols the immediate left/right direction of the track nearest the car and z[7]  con‐\ntrols the sharpness of the approaching left turn.\nThis shows that the latent space that the V AE has learned is continuous and can be\nused to generate new track segments that have never before been observed by the\nagent.\n344 | Chapter 12: World Models\nFigure 12-9. A linear interpolation of two dimensions of z\nTraining the VAE | 345",1957
125-Collecting Data to Train the MDN-RNN.pdf,125-Collecting Data to Train the MDN-RNN,,0
126-Sampling from the MDN-RNN.pdf,126-Sampling from the MDN-RNN,"Collecting Data to Train the MDN-RNN\nNow  that we have a trained V AE, we can use this to generate training data for our\nMDN-RNN.\nIn this step, we pass all of the random rollout observations through the\nencode_mu_logvar  model and store the mu and logvar  vectors corresponding to each\nobservation. This encoded data, along with the already collected action , reward , and\ndone  variables, will be used to train the MDN-RNN. This process is shown in\nFigure 12-10 .\nFigure 12-10. Creating the MDN-RNN training dataset\nTraining the MDN-RNN\nWe can now train the MDN-RNN to predict the distribution of the next z vector and\nreward one timestep ahead into the future, given the current z vector, current action,\nand previous reward. We can then use the internal hidden state of the RNN (which\ncan be thought of as the model’s current understanding of the environment dynam‐\nics) as part of the input into the controller, which will ultimately decide on the best\nnext action to take.\nThe hyperparameters of this step of the process are as follows:\nrnn_batch_size\nThe batch size to use when training the MDN-RNN (how many sequences per\nbatch) (e.g., 100)\nrnn_num_steps\nThe total number of training iterations (e.g., 4000 )\n346 | Chapter 12: World Models\nThe MDN-RNN Architecture\nThe architecture of the MDN-RNN is shown in Figure 12-11 .\nFigure 12-11. The MDN-RNN architecture\nThe MDN-RNN consists of an LSTM layer (the RNN), followed by a densely connec‐\nted layer (the MDN) that transforms the hidden state of the LSTM into the parame‐\nters of a mixture distribution. Let’s walk through the network step by step.\nThe input to the LSTM layer is a vector of length 36—a concatenation of the encoded\nz vector (length 32) from the V AE, the current action (length 3), and the previous\nreward (length 1).\nThe output from the LSTM layer is a vector of length 256—one value for each LSTM\ncell in the layer. This is passed to the MDN, which is just a densely connected layer\nthat transforms the vector of length 256 into a vector of length 481.\nWhy 481? Figure 12-12  explains the composition of the output from the MDN-RNN.\nThe aim of a mixture density network is to model the fact that our next z could be\ndrawn from one of several possible distributions with a certain probability. In the car\nracing example, we choose five normal distributions. How many parameters do we\nneed to define these distributions? For each of the 5 mixtures, we need a mu and a\nlogvar  (to define the distribution) and a log-probability of this mixture being chosen\n(logpi ), for each of the 32 dimensions of z. This makes 5 × 3 × 32 = 480 parameters.\nThe one extra parameter is for the reward prediction.\nFigure 12-12. The output from the mixture density network\nTraining the MDN-RNN | 347",2801
127-The Controller Architecture.pdf,127-The Controller Architecture,"Sampling from the MDN-RNN\nWe can sample from the MDN output to generate a prediction for the next z and\nreward at the following timestep, through the following process:\n1.Split the 481-dimensional output vector into the 3 variables ( logpi , mu, logvar )\nand the reward value.\n2.Exponentiate and scale logpi  so that it can be interpreted as 32 probability distri‐\nbutions over the 5 mixture indices.\n3.For each of the 32 dimensions of z, sample from the distributions created from\nlogpi  (i.e., choose which of the 5 distributions should be used for each dimen‐\nsion of z).\n4.Fetch the corresponding values of mu and logvar  for this distribution.\n5.Sample a value for each dimension of z from the normal distribution parameter‐\nized by the chosen parameters of mu and logvar  for this dimension.\nThe loss function for the MDN-RNN is the sum of the z vector reconstruction loss\nand the reward loss. The z vector reconstruction loss is the negative log-likelihood of\nthe distribution predicted by the MDN-RNN, given the true value of z, and the\nreward loss is the mean squared error between the predicted reward and the true\nreward.\nTraining the Controller\nThe final step is to train the controller (the network that outputs the chosen action)\nusing an evolutionary algorithm called the covariance matrix adaptation evolution\nstrategy (CMA-ES).\nThe hyperparameters of this step of the process are as follows:\ncontroller_num_worker\nThe number of workers that will test solutions in parallel\ncontroller_num_worker_trial\nThe number of solutions that each worker will be given to test at each generation\ncontroller_num_episode\nThe number of episodes that each solution will be tested against to calculate the\naverage reward\ncontroller_eval_steps\nThe number of generations between evaluations of the current best parameter set\n348 | Chapter 12: World Models",1884
128-CMA-ES.pdf,128-CMA-ES,"The Controller Architecture\nThe architecture of the controller is very simple. It is a densely connected neural net‐\nwork with no hidden layers. It connects the input vector directly to the action vector.\nThe input vector is a concatenation of the current z vector (length 32) and the current\nhidden state of the LSTM (length 256), giving a vector of length 288. Since we are\nconnecting each input unit directly to the 3 output action units, the total number of\nweights to tune is 288 × 3 = 864, plus 3 bias weights, giving 867 in total.\nHow should we train this network? Notice that this is not a supervised learning prob‐\nlem—we are not trying to predict  the correct action. There is no training set of cor‐\nrect actions, as we do not know what the optimal action is for a given state of the\nenvironment. This is what distinguishes this as a reinforcement learning problem. We\nneed the agent to discover the optimal values for the weights itself by experimenting\nwithin the environment and updating its weights based on received feedback.\nEvolutionary strategies are a popular choice for solving reinforcement learning prob‐\nlems, due to their simplicity, efficiency, and scalability. We shall use one particular\nstrategy, known as CMA-ES.\nCMA-ES\nEvolutionary strategies generally adhere to the following process:\n1.Create a population  of agents and randomly initialize the parameters to be opti‐\nmized for each agent.\n2.Loop over the following:\na.Evaluate each agent in the environment, returning the average reward over\nmultiple episodes.\nb.Breed the agents with the best scores to create new members of the\npopulation.\nc.Add randomness to the parameters of the new members.\nd.Update the population pool by adding the newly created agents and removing\npoorly performing agents.\nThis is similar to the process through which animals evolve in nature—hence the\nname evolutionary  strategies. “Breeding” in this context simply means combining the\nexisting best-scoring agents such that the next generation are more likely to produce\nhigh-quality results, similar to their parents. As with all reinforcement learning solu‐\ntions, there is a balance to be found between greedily searching for locally optimal\nsolutions and exploring unknown areas of the parameter space for potentially better\nTraining the Controller | 349\nsolutions. This is why it is important to add randomness to the population, to ensure\nwe are not too narrow in our search field.\nCMA-ES is just one form of evolutionary strategy. In short, it works by maintaining a\nnormal distribution from which it can sample the parameters of new agents. At each\ngeneration, it updates the mean of the distribution to maximize the likelihood of\nsampling the high-scoring agents from the previous timestep. At the same time, it\nupdates the covariance matrix of the distribution to maximize the likelihood of sam‐\npling the high-scoring agents, given the previous mean. It can be thought of as a form\nof naturally arising gradient descent, but with the added benefit that it is derivative-\nfree, meaning that we do not need to calculate or estimate costly gradients.\nOne generation of the algorithm demonstrated on a toy example is shown in\nFigure 12-13 . Here we are trying to find the minimum point of a highly nonlinear\nfunction in two dimensions—the value of the function in the red/black areas of the\nimage is greater than the value of the function in the white/yellow parts of the image.\nFigure 12-13. One update step from the CMA-ES algorithm (source: Ha, 2017 )2\nThe steps are as follows:\n1.We start with a randomly generated 2D normal distribution and sample a popu‐\nlation of candidates, shown in blue in Figure 12-13 .\n2.We then calculate the value of the function for each candidate and isolate the best\n25%, shown in purple in Figure 12-13 —we’ll call this set of points P.\n3.We set the mean of the new normal distribution to be the mean of the points in P.\nThis can be thought of as the breeding stage, wherein we only use the best candi‐\ndates to generate a new mean for the distribution. We also set the covariance\nmatrix of the new normal distribution to be the covariance matrix of the points\nin P, but use the existing mean in the covariance calculation rather than the cur‐\nrent mean of the points in P. The larger the difference between the existing mean\nand the mean of the points in P, the wider the variance of the next normal distri‐\nbution. This has the effect of naturally creating momentum  in the search for the\noptimal parameters.\n350 | Chapter 12: World Models",4611
129-Parallelizing CMA-ES.pdf,129-Parallelizing CMA-ES,"4.We can then sample a new population of candidates from our new normal distri‐\nbution with an updated mean and covariance matrix.\nFigure 12-14  shows several generations of the process. See how the covariance widens\nas the mean moves in large steps toward the minimum, but narrows as the mean set‐\ntles into the true minimum.\nFigure 12-14. CMA-ES (source: Wikipedia )\nFor the car racing task, we do not have a well-defined function to maximize, but\ninstead an environment where the 867 parameters to be optimized determine how\nwell the agent scores. Initially, some sets of parameters will, by random chance, gen‐\nerate scores that are higher than others and the algorithm will gradually move the\nnormal distribution in the direction of those parameters that score highest in the\nenvironment.\nParallelizing CMA-ES\nOne of the great benefits of CMA-ES is that it can be easily parallelized. The most\ntime-consuming part of the algorithm is calculating the score for a given set of\nparameters, since it needs to simulate an agent with these parameters in the environ‐\nment. However, this process can be parallelized, since there are no dependencies\nbetween individual simulations. There is a orchestrator process that sends out param‐\neter sets to be tested to many node processes in parallel. The nodes return the results\nto the orchestrator, which accumulates the results and then passes the overall result of\nthe generation to the CMA-ES object. This object updates the mean and covariance\nTraining the Controller | 351\nmatrix of the normal distribution as per Figure 12-13  and provides the orchestrator\nwith a new population to test. The loop then starts again. Figure 12-15  explains this\nin a diagram.\nFigure 12-15. Parallelizing CMA-ES—here there is a population size of eight and four\nnodes (so t = 2, the number of trials that each node is responsible for)\nThe orchestrator asks the CMA-ES object ( es) for a set of parameters to trial.\nThe orchestrator divides the parameters into the number of nodes available.\nHere, each of the four node processes gets two parameter sets to trial.\nThe nodes run a worker process that loops over each set of parameters and runs\nseveral episodes for each. Here we run three episodes for each set of parameters.\nThe rewards from each episode are averaged to give a single score for each set of\nparameters.\nEach node returns its list of scores to the orchestrator.\n352 | Chapter 12: World Models",2471
130-In-Dream Training.pdf,130-In-Dream Training,"The orchestrator groups all the scores together and sends this list to the es object.\nThe es object uses this list of rewards to calculate the new normal distribution as\nper Figure 12-13 .\nAfter around 200 generations, the training process achieves an average reward score\nof around 840 for the car racing task, as shown in Figure 12-16 .\nFigure 12-16. Average episode reward of the controller training process, by generation\n(source: Zac Wellmer, “World Models” )\nIn-Dream Training\nSo far, the controller training  has been conducted using the Gym CarRacing  environ‐\nment to implement the step method that moves the simulation from one state to the\nnext. This function calculates the next state and reward, given the current state of the\nenvironment and chosen action.\nNotice how the step method performs a very similar function to the MDN-RNN in\nour model. Sampling from the MDN-RNN outputs a prediction for the next z and\nreward, given the current z and chosen action.\nIn fact, the MDN-RNN can be thought of as an environment in its own right, but\noperating in z-space rather than in the original image space. Incredibly, this means\nthat we can actually substitute the real environment with a copy of the MDN-RNN\nand train the controller entirely within an MDN-RNN-inspired dream  of how the\nenvironment should behave.\nIn other words, the MDN-RNN has learned enough about the general physics of\nthe real environment from the original random movement dataset that it can be used\nas a proxy for the real environment when training the controller. This is quite\nIn-Dream Training | 353\nremarkable—it means that the agent can train itself to learn a new task by thinking\nabout how it can maximize reward in its dream environment, without ever having to\ntest out strategies in the real world. It can then perform well at the task the first time,\nhaving never attempted the task in reality.\nA comparison of the architectures for training in the real environment and the dream\nenvironment follows: the real-world architecture is shown in Figure 12-17  and the in-\ndream training setup is illustrated in Figure 12-18 .\nFigure 12-17. Training the controller in the Gym environment\nNotice how in the dream architecture, the training of the controller is performed\nentirely in z-space without the need to ever decode the z vectors back into recogniza‐\nble track images. We can of course do so, in order to visually inspect the performance\nof the agent, but it is not required for training.\n354 | Chapter 12: World Models\nFigure 12-18. Training the controller in the MDN-RNN dream environment\nOne of the challenges of training agents entirely within the MDN-RNN dream envi‐\nronment is overfitting. This occurs when the agent finds a strategy that is rewarding\nin the dream environment but does not generalize well to the real environment, due\nto the MDN-RNN not fully capturing how the true environment behaves under cer‐\ntain conditions.\nThe authors of the original paper highlight this challenge and show how including a\ntemperature  parameter to control model uncertainty can help alleviate the problem.\nIncreasing this parameter magnifies the variance when sampling z through the\nMDN-RNN, leading to more volatile rollouts when training in the dream environ‐\nment. The controller receives higher rewards for safer strategies that encounter well-\nunderstood states and therefore tend to generalize better to the real environment.\nIncreased temperature, however, needs to be balanced against not making the envi‐\nronment so volatile that the controller cannot learn any strategy, as there is not\nenough consistency in how the dream environment evolves over time.\nIn the original paper, the authors show this technique successfully applied to a differ‐\nent environment: DoomTakeCover , based around the computer game Doom .\nIn-Dream Training | 355",3893
131-Summary.pdf,131-Summary,"Figure 12-19  shows how changing the temperature  parameter affects both the virtual\n(dream) score and the actual score in the real environment.\nFigure 12-19. Using temperature to control dream environment volatility (source: Ha\nand Schmidhuber, 2018 )\nThe optimal temperature setting of 1.15 achieves a score of 1,092 in the real environ‐\nment, surpassing the current Gym leader at the time of publication. This is an amaz‐\ning achievement—remember, the controller has never  attempted the task in the real\nenvironment. It has only ever taken random steps in the real environment (to train\nthe V AE and MDN-RNN dream  model) and then used the dream environment to\ntrain the controller.\nA key benefit of using generative world models as an approach to reinforcement\nlearning is that each generation of training in the dream environment is much faster\nthan training in the real environment. This is because the z and reward prediction by\nthe MDN-RNN is faster than the z and reward calculation by the Gym environment.\nSummary\nIn this chapter we have seen how a generative model (a V AE) can be utilized within a\nreinforcement learning setting to enable an agent to learn an effective strategy by test‐\ning policies within its own generated dreams, rather than within the real\nenvironment.\nThe V AE is trained to learn a latent representation of the environment, which is then\nused as input to a recurrent neural network that forecasts future trajectories within\nthe latent space. Amazingly, the agent can then use this generative model as a pseudo-\nenvironment to iteratively test policies, using an evolutionary methodology, that gen‐\neralize well to the real environment.\nFor further information on the model, there is an excellent interactive explanation\navailable online , written by the authors of the original paper.\n356 | Chapter 12: World Models\nReferences\n1. David Ha and Jürgen Schmidhuber, “World Models, ” March 27, 2018, https://\narxiv.org/abs/1803.10122 .\n2. David Ha, “ A Visual Guide to Evolution Strategies, ” October 29, 2017, https://\nblog.otoro.net/2017/10/29/visual-evolution-strategies .\nSummary | 357",2154
132-Architecture.pdf,132-Architecture,"CHAPTER 13\nMultimodal Models\nChapter Goals\nIn this chapter you will:\n•Learn what is meant by a multimodal model.\n•Explore the inner workings of DALL.E 2, a large-scale text-to-image model from\nOpenAI.\n•Understand how CLIP and diffusion models such as GLIDE play an integral role\nin the overall DALL.E 2 architecture.\n•Analyze the limitations of DALL.E 2, as highlighted by the authors of the paper.\n•Explore the architecture of Imagen, a large-scale text-to-image model from Goo‐\ngle Brain.\n•Learn about the latent diffusion process used by Stable Diffusion, an open source\ntext-to-image model.\n•Understand the similarities and differences between DALL.E 2, Imagen, and Sta‐\nble Diffusion.\n•Investigate DrawBench, a benchmarking suite for evaluating text-to-image\nmodels.\n•Learn the architectural design of Flamingo, a novel visual language model from\nDeepMind.\n•Unpick the different components of Flamingo and learn how they each contrib‐\nute to the model as a whole.\n•Explore some of the capabilities of Flamingo, including conversational\nprompting.\n359\nSo far, we have analyzed generative learning problems that focus solely on one modal‐\nity of data: either text, images, or music. We have seen how GANs and diffusion mod‐\nels can generate state-of-the-art images and how Transformers are pioneering the way\nfor both text and image generation. However, as humans, we have no difficulties\ncrossing modalities—for example, writing a description of what is happening in a\ngiven photograph, creating digital art to depict a fictional fantasy world in a book, or\nmatching a film score to the emotions of a given scene. Can we train machines to do\nthe same?\nIntroduction\nMultimodal learning  involves training generative models to convert between two or\nmore different kinds of data. Some of the most impressive generative models intro‐\nduced in the last two years have been multimodal in nature. In this chapter we will\nexplore how they work in detail and consider how the future of generative modeling\nwill be shaped by large multimodal models.\nWe’ll explore four different vision-language models: DALL.E 2 from OpenAI; Imagen\nfrom Google Brain; Stable Diffusion from Stability AI, CompVis, and Runway; and\nFlamingo from DeepMind.\nThe aim of this chapter is to concisely explain how each model\nworks, without going into the fine detail of every design decision.\nFor more information, refer to the individual papers for each\nmodel, which explain all of the design choices and architecture\ndecisions in detail.\nText-to-image generation focuses on producing state-of-the-art images from a given\ntext prompt. For example, given the input “ A head of broccoli made out of modeling\nclay, smiling in the sun, ” we would like the model to be able to output a image that\naccurately matches the text prompt, as shown in Figure 13-1 .\nThis is clearly a highly challenging problem. Text understanding and image genera‐\ntion are difficult to solve in their own right, as we have seen in previous chapters of\nthis book. Multimodal modeling such as this presents an additional challenge,\nbecause the model must also learn how to cross the bridge between the two domains\nand learn a shared representation that allows it to accurately convert from a block of\ntext to a high-fidelity image without loss of information.\n360 | Chapter 13: Multimodal Models\nFigure 13-1. An example of text-to-image generation by DALL.E 2\nMoreover, in order to be successful the model must be able to combine concepts and\nstyles that it may never have seen before. For example, there are no Michelangelo\nfrescos containing people wearing virtual reality headsets, but we would like our\nmodel to be able to create such an image if we ask it to. Equally, it would be desirable\nfor the model to accurately infer how objects in the generated image relate to each\nother, based on the text prompt. For example, a picture of “an astronaut riding a\ndoughnut through space” should look very different from one of “an astronaut eating\na doughnut in a crowded space. ” The model must learn how words are given meaning\nthrough context and how to convert explicit textual relationships between entities to\nimages that imply the same meaning.\nDALL.E 2\nThe first model we shall explore is DALL.E 2 , a model designed by OpenAI for text-\nto-image generation. The first version of  this model, DALL.E, 1 was released in Febru‐\nary 2021 and sparked a new wave of interest in generative multimodal models. In this\nsection, we shall investigate the workings of the second iteration of the model,\nDALL.E 2, 2 released just over a year later in April 2022.\nDALL.E 2 is an extremely impressive model that has furthered our understanding of\nAI’s ability to solve these types of multimodal problems. It not only has ramifications\nacademically, but also forces us to ask big questions relating to the role of AI in crea‐\ntive processes that previously were thought to be unique to humans. We will start by\nexploring how DALL.E 2 works, building on key foundational ideas that we have\nalready explored earlier in this book.\nDALL.E 2 | 361",5154
133-The Text Encoder.pdf,133-The Text Encoder,,0
134-CLIP.pdf,134-CLIP,"Architecture\nTo understand how DALL.E 2 works, we must first survey its overall architecture, as\nshown in Figure 13-2 .\nFigure 13-2. The DALL.E 2 architecture\nThere are three distinct parts to consider: the text encoder , the prior , and the decoder .\nText is first passed through the text encoder to produce a text embedding vector. This\nvector is then transformed by the prior to produce an image embedding vector.\nFinally, this is passed through the decoder, along with the original text, to produce the\ngenerated image. We will step through each component in turn, to get a complete pic‐\nture of how DALL.E 2 works in practice.\nThe Text Encoder\nThe aim of the text encoder is to convert the text prompt into an embedding vector\nthat represents the conceptual meaning of the text prompt within a latent space. As\nwe have seen in previous chapters, converting discrete text to a continuous latent\nspace vector is essential for all downstream tasks, because we can continue to manip‐\nulate the vector further depending on our particular goal.\nIn DALL.E 2, the authors do not train the text encoder from scratch, but instead\nmake use of an existing model called Contrastive Language–Image Pre-training\n(CLIP), also produced by OpenAI. Therefore, to understand the text encoder, we\nmust first understand how CLIP works.\nCLIP\nCLIP 3 was unveiled in a paper published by OpenAI in February 2021 (just a few days\nafter the first DALL.E paper) that described it as “a neural network that efficiently\nlearns visual concepts from natural language supervision. ”\nIt uses a technique called contrastive learning  to match images with text descriptions.\nThe model is trained on a dataset of 400 million text–image pairs scraped from the\ninternet—some example pairs are shown in Figure 13-3 . For comparison, there are 14\n362 | Chapter 13: Multimodal Models\nmillion hand-annotated images in ImageNet. Given an image and a list of possible\ntext descriptions, its task is to find the one that actually matches the image.\nFigure 13-3. Examples of text–image pairs\nThe key idea behind contrastive learning is simple. We train two neural networks: a\ntext encoder  that converts text to a text embedding and an image encoder  that converts\nan image to an image embedding. Then, given a batch of text–image pairs, we com‐\npare all text and image embedding combinations using cosine similarity  and train the\nnetworks to maximize the score between matching text–image pairs and minimize\nthe score between incorrect text–image pairs. This process is shown in Figure 13-4 .\nCLIP Is Not Generative\nNote that CLIP is not itself a generative model—it cannot produce\nimages or text. Is it closer to a discriminative model, because the\nfinal output is a prediction about which text description from a\ngiven set most closely matches a given image (or the other way\naround, which image most closely matches a given text\ndescription).\nDALL.E 2 | 363\nFigure 13-4. The CLIP training process\nBoth the text encoder and the image encoder are Transformers—the image encoder is\na Vision Transformer (ViT), introduced in “ViT VQ-GAN”  on page 292, which\napplies the same concept of attention to images. The authors tested other model\narchitectures, but found this combination to produce the best results.\nWhat makes CLIP especially interesting is the way it can be used  for zero-shot predic‐\ntion on tasks that it has never been exposed to. For example, suppose we want to use\nCLIP to predict the label of a given image in the ImageNet dataset. We can first con‐\nvert the ImageNet labels into sentences by using a template (e.g., “a photo of a\n<label>”), as shown in Figure 13-5 .\n364 | Chapter 13: Multimodal Models\nFigure 13-5. Converting labels in a new dataset to captions, in order to produce CLIP\ntext embeddings\nTo predict the label of a given image, we can pass it through the CLIP image encoder\nand calculate the cosine similarity between the image embedding and all possible text\nembeddings in order to find the label with the maximum score, as shown in\nFigure 13-6 .\nFigure 13-6. Using CLIP to predict the content of an image\nNotice that we do not need to retrain either of the CLIP neural networks for it to be\nreadily applicable to new tasks. It uses language as the common domain through\nwhich any set of labels can be expressed.\nDALL.E 2 | 365\nUsing this approach, it is possible to show that CLIP performs well across a wide\nrange of image dataset labeling challenges ( Figure 13-7 ). Other models that have been\ntrained on a specific dataset to predict a given set of labels often fail when applied to\ndifferent datasets with the same labels because they are highly optimized to the indi‐\nvidual datasets on which they were trained. CLIP is much more robust, as it has\nlearned a deep conceptual understanding of full text descriptions and images, rather\nthan just excelling at the narrow task of assigning a single label to a given image in a\ndataset.\nFigure 13-7. CLIP performs well on a wide range of image labeling datasets (source:\nRadford et al., 2021 )\nAs mentioned, CLIP is measured on its discriminative ability, so how does it help us\nto build generative models such as DALL.E 2?\n366 | Chapter 13: Multimodal Models",5284
135-The Prior.pdf,135-The Prior,"The answer is that we can take the trained text encoder and use it as one part of a\nlarger model such as DALL.E 2, with frozen weights. The trained encoder is simply a\ngeneralized model for converting text to a text embedding, which should be useful for\ndownstream tasks such as generating images. The text encoder is able to capture a\nrich conceptual understanding of the text, as it has been trained to be as similar as\npossible to its matching image embedding counterpart, which is produced only from\nthe paired image. It is therefore the first part of the bridge that we need to be able to\ncross over from the text domain to the image domain.\nThe Prior\nThe next stage of the process involves converting the text embedding into a CLIP\nimage embedding. The DALL.E 2 authors tried two different methods for training the\nprior model:\n•An autoregressive model\n•A diffusion model\nThey found that the diffusion approach outperformed the autoregressive model and\nwas more computationally efficient. In this section, we’ll look at both and see how\nthey differ.\nAutoregressive prior\nAn autoregressive model generates output sequentially, by placing an ordering on the\noutput tokens (e.g., words, pixels) and conditioning the next token on previous\ntokens. We have seen in previous chapters how this is used in recurrent neural net‐\nworks (e.g., LSTMs), Transformers, and PixelCNN.\nThe autoregressive prior of DALL.E 2 is an encoder-decoder Transformer. It is\ntrained to reproduce the CLIP image embedding given a CLIP text embedding, as\nshown in Figure 13-8 . Note that there are some additional components to the autore‐\ngressive model mentioned in the original paper that we omit here for conciseness.\nDALL.E 2 | 367\nFigure 13-8. A simplified  diagram of the autoregressive prior of DALL.E 2\nThe model is trained on the CLIP text–image pair dataset. Y ou can think of it as the\nsecond part of the bridge that we need in order to jump from the text domain to the\nimage domain: we are converting a vector from the text embedding latent space to\nthe image embedding latent space.\nThe input text embedding is processed by the encoder of the Transformer to produce\nanother representation that is fed to the decoder, alongside the current generated out‐\nput image embedding. The output is generated one element at a time, using teacher\nforcing to compare the predicted next element to the actual CLIP image embedding.\nThe sequential nature of the generation means that the autoregressive model is less\ncomputationally efficient than the other method tried by the authors, which we’ll look\nat next.\nDiffusion  prior\nAs we saw in Chapter 8 , diffusion models are fast becoming the go-to choice for gen‐\nerative modeling practitioners, alongside Transformers. In DALL.E 2 a decoder-only\nTransformer is used as the prior, trained using a diffusion process.\n368 | Chapter 13: Multimodal Models",2921
136-The Decoder.pdf,136-The Decoder,"The training and generation process is shown in Figure 13-9 . Again, this is a simpli‐\nfied version; the original paper contains full details of how the diffusion model is\nstructured.\nFigure 13-9. A simplified  diagram of the diffusion  prior training and generation process\nof DALL.E 2\nDuring training, each CLIP text and image embedding pair are first concatenated\ninto a single vector. Then, the image embedding is noised over 1,000 timesteps until it\nis indistinguishable from random noise. The diffusion prior is then trained to predict\nthe denoised image embedding at the previous timestep. The prior has access to the\ntext embedding throughout, so it is able to condition its predictions on this informa‐\ntion, gradually transforming the random noise into a predicted CLIP image embed‐\nding. The loss function is the average mean-squared error across denoising steps.\nTo generate new image embeddings, we sample a random vector, prepend the rele‐\nvant text embedding, and pass it through the trained diffusion prior multiple times.\nThe Decoder\nThe final part of DALL.E 2 is the decoder. This is the part of the model that generates\nthe final image conditioned on the text prompt and the predicted image embedding\noutput by the prior.\nThe architecture and training process of the decoder borrows from an earlier OpenAI\npaper, published in December 2021, which presented a generative model called Gui‐\nded Language to Image Diffusion for Generation and Editing (GLIDE). 4\nDALL.E 2 | 369\nGLIDE is able to generate realistic images from text prompts, in much the same way\nthat DALL.E 2 can. The difference is that GLIDE does not make use of CLIP embed‐\ndings, but instead works directly with the raw text prompt, training the entire model\nfrom scratch, as shown in Figure 13-10 .\nFigure 13-10. A comparison between DALL.E 2 and GLIDE—GLIDE trains the entire\ngenerative model from scratch, whereas DALL.E 2 makes use of CLIP embeddings to\ncarry information forward from the initial text prompt\nLet’s see how GLIDE works first.\nGLIDE\nGLIDE is trained as a diffusion model, with U-Net architecture for the denoiser and\nTransformer architecture for the text encoder. It learns to undo the noise added to an\nimage, guided by the text prompt. Finally, an Upsampler  is trained to scale the gener‐\nated image to 1,024 × 1,024 pixels.\nGLIDE trains the 3.5 billion (B) parameter model from scratch—2.3B parameters for\nthe visual part of the model (U-Net and Upsampler) and 1.2B for the Transformer. It\nis trained on 250 million text–image pairs.\n370 | Chapter 13: Multimodal Models\nThe diffusion process is shown in Figure 13-11 . A Transformer is used to create an\nembedding of the input text prompt, which is then used to guide the U-Net through‐\nout the denoising process. We explored the U-Net architecture in Chapter 8 ; it’s a per‐\nfect model choice when the overall size of the image should stay the same (e.g., for\nstyle transfer, denoising, etc.).\nFigure 13-11. The GLIDE diffusion  process\nDALL.E 2 | 371\nThe DALL.E 2 decoder still uses the U-Net denoiser and Transformer text encoder\narchitectures, but additionally has the predicted CLIP image embeddings to condi‐\ntion on. This is the key difference between GLIDE and DALL.E 2, as shown in\nFigure 13-12 .\nFigure 13-12. The DALL.E 2 decoder additionally conditions on the image embedding\nproduced by the prior\nAs with all diffusion models, to generate a new image, we simply sample some ran‐\ndom noise and run this through the U-Net denoiser multiple times, conditioned on\nthe Transformer text encoding and image embedding. The output is a 64 × 64–pixel\nimage.\nUpsampler\nThe final part of the decoder is the Upsampler (two separate diffusion models). The\nfirst diffusion model transforms the image from 64 × 64 to 256 × 256 pixels. The sec‐\nond transforms it again, from 256 × 256 to 1,024 × 1,024 pixels, as shown in\nFigure 13-13 .\nUpsampling is useful because it means we do not have to build large upstream models\nto handle high-dimensional images. We can work with small images until the final\n372 | Chapter 13: Multimodal Models",4152
137-Examples from DALL.E 2.pdf,137-Examples from DALL.E 2,"stages of the process, when we apply the Upsamplers. This saves on model parameters\nand ensures a more efficient upstream training process.\nFigure 13-13. The first Upsampler diffusion  model converts the image from 64 × 64 pix‐\nels to 256 × 256 pixels while the second converts from 256 × 256 pixels to 1,024 × 1,024\npixels\nThis concludes the DALL.E 2 model explanation! In summary, DALL.E 2 makes use\nof the pre-trained CLIP model to immediately produce a text embedding of the input\nprompt. Then it converts this into an image embedding using a diffusion model\ncalled the prior. Lastly, it implements a GLIDE-style diffusion model to generate the\noutput image, conditioned on the predicted image embedding and Transformer-\nencoded input prompt.\nExamples from DALL.E 2\nExamples  of more images generated by DALL.E 2 can be found on the official web‐\nsite. The way that the model is able to combine complex, disparate concepts in a real‐\nistic, believable way is astonishing and represents a significant leap forward for AI\nand generative modeling.\nIn the paper, the authors show how the model can be used for additional purposes\nother than text-to-image generation. One of these applications is creating variations\nof a given image, which we explore in the following section.\nDALL.E 2 | 373\nImage variations\nAs discussed previously, to generate images using the DALL.E 2 decoder we sample\nan image consisting of pure random noise and then gradually reduce the amount of\nnoise using the denoising diffusion model, conditioned on the provided image\nembedding. Selecting different initial random noise samples will result in different\nimages.\nIn order to generate variations of a given image, we therefore just need to establish its\nimage embedding to feed to the decoder. We can obtain this using the original CLIP\nimage encoder, which is explicitly designed to convert an image into its CLIP image\nembedding. This process is shown in Figure 13-14 .\nFigure 13-14. DALL.E 2 can be used for generating variations of a given image\nImportance of the prior\nAnother  avenue explored by the authors is establishing the importance of the prior.\nThe purpose of the prior is to provide the decoder with a useful representation of the\nimage to be generated, making use of the pre-trained CLIP model. However, it is fea‐\nsible that this step isn’t necessary—perhaps we could just pass the text embedding\ndirectly to the decoder instead of the image embedding, or ignore the CLIP embed‐\ndings completely and condition only on the text prompt. Would this impact the qual‐\nity of the generations?\nTo test this, the authors tried three different approaches:\n1.Feed the decoder only with the text prompt (and a zero vector for the image\nembedding).\n2.Feed the decoder with the text prompt and the text embedding (as if it were an\nimage embedding).\n3.Feed the decoder with the text prompt and the image embedding (i.e., the full\nmodel).\nExample results are shown in Figure 13-15 . We can see that when the decoder is\nstarved of image embedding information, it can only produce a rough approximation\n374 | Chapter 13: Multimodal Models\nof the text prompt, missing key information such as the calculator. Using the text\nembedding as if it were an image embedding performs slightly better, though it is not\nable to capture the relationship between the hedgehog and the calculator. Only the\nfull model with the prior produces an image that accurately reflects all of the infor‐\nmation contained within the prompt.\nFigure 13-15. The prior provides the model with additional context and helps the\ndecoder to produce more accurate generations (source: Ramesh et al., 2022 )\nLimitations\nIn the DALL.E 2 paper, the authors also highlight several known limitations of the\nmodel. Two of these (attribute binding and text generation) are shown in\nFigure 13-16 .\nDALL.E 2 | 375\nFigure 13-16. Two limitations of DALL.E 2 lie in its ability to bind attributes to objects\nand reproduce textual information—top prompt: “ A red cube on top of a blue cube”; bot‐\ntom prompt: “ A sign that says deep learning” (source: Ramesh et al., 2022 )\nAttribute binding  is the ability of a model to understand the relationship between\nwords in a given text prompt, and in particular how attributes relate to objects. For\nexample, the prompt “ A red cube on top of a blue cube” must appear visually distinct\nfrom “ A blue cube on top of a red cube. ” DALL.E struggles somewhat with this, com‐\npared to earlier models such as GLIDE, though the overall quality of generations is\nbetter and more diverse.\nAlso, DALL.E 2 is not able to accurately reproduce text—this is probably due to the\nfact that the CLIP embeddings do not capture spellings, but instead only contain a\nhigher-level representation of the text. These representations can be decoded into text\nwith partial success (e.g., individual letters are mostly correct), but not with enough\ncompositional understanding to form full words.\n376 | Chapter 13: Multimodal Models",5053
138-Imagen.pdf,138-Imagen,,0
139-Stable Diffusion.pdf,139-Stable Diffusion,"Imagen\nJust over a month after OpenAI released DALL.E 2, the Google Brain team released\ntheir own text-to-image model called Imagen. 5 Many of the core themes that we have\nalready explored in this chapter are also relevant to Imagen: for example, it uses a text\nencoder and a diffusion model decoder.\nIn the next section, we’ll explore the overall architecture of Imagen and compare it\nwith DALL.E 2.\nArchitecture\nAn overview of the Imagen architecture is shown in Figure 13-17 .\nFigure 13-17. The Imagen architecture (source: Saharia et al., 2022 )\nThe frozen text encoder is the pre-trained T5-XXL model, a large encoder-decoder\nTransformer. Unlike CLIP , this was trained only on text and not images, so it is not a\nmultimodal model. However, the authors found that it still functions extremely well\nas a text encoder for Imagen and that scaling this model has more impact on overall\nperformance than scaling the diffusion model decoder.\nImagen | 377\nLike DALL.E 2’s, Imagen’s the decoding diffusion model is based on a U-Net architec‐\nture, conditioned on text embeddings. There are several architectural improvements\nmade to the standard U-Net architecture, to produce what the authors call the Effi‐\ncient  U-Net . This model uses less memory, converges faster, and has better sample\nquality than previous U-Net models.\nThe Upsampler super-resolution models that take the generated image from 64 × 64\nto 1,024 × 1,024 pixels are also diffusion models that continue to use the text embed‐\ndings to guide the upsampling process.\nDrawBench\nAn additional contribution of the Imagen paper is DrawBench —a suite of 200 text\nprompts for text-to-image evaluation. The text prompts cover 11 categories, such as\nCounting  (ability to generate a specified number of objects), Description  (ability to\ngenerate complex and long text prompts describing objects), and Text (ability to gen‐\nerate quoted text). To compare two models, the DrawBench text prompts are passed\nthrough each model and the outputs given to a panel of human raters for evaluation\nacross two metrics:\nAlignment\nWhich image more accurately describes the caption?\nFidelity\nWhich image is more photorealistic (looks more real)?\nThe results from the DrawBench human evaluation are shown in Figure 13-18 .\nBoth DALL.E 2 and Imagen are remarkable models that have made significant contri‐\nbutions to the field of text-to-image generation. Whilst Imagen outperforms DALL.E\n2 on many of the DrawBench benchmarks, DALL.E 2 provides additional functionali‐\nties that are not present in Imagen. For example, because DALL.E 2 utilizes CLIP (a\nmultimodal text–image model), it is able to accept images as input to generate image\nembeddings. This means DALL.E 2 is able to provide image editing and image varia‐\ntion capabilities. This is not possible with Imagen; the text encoder is a pure text\nmodel, so there is no way to input an image.\n378 | Chapter 13: Multimodal Models\nFigure 13-18. Comparison of Imagen and DALL.E 2 on DrawBench across alignment\nand image fidelity  (source: Saharia et al., 2022 )\nExamples from Imagen\nExample Imagen generations are shown in Figure 13-19 .\nFigure 13-19. Example Imagen generations (source: Saharia et al., 2022 )\nImagen | 379",3271
140-Examples from Stable Diffusion.pdf,140-Examples from Stable Diffusion,"Stable Diffusion\nThe last text-to-image diffusion model that we shall explore is Stable Diffusion ,\nreleased in August 2022 by Stability AI , in collaboration with the Computer Vision\nand Learning research group at Ludwig Maximilian University of Munich  and Run‐\nway. It is different from DALL.E 2 and Imagen in that its code and model weights\nhave been released publicly, through Hugging Face . This means that anyone can\ninteract with the model on their own hardware, without having to use proprietary\nAPIs.\nArchitecture\nThe main architectural difference between Stable Diffusion and the text-to-image\nmodels discussed previously is that it uses  latent diffusion  as its underlying generative\nmodel. Latent diffusion models (LDMs) were introduced by Rombach et al. in\nDecember 2021, in the paper “High-Resolution Image Synthesis with Latent Diffu‐\nsion Models. ” 6 The key idea from the paper is to wrap the diffusion model within an\nautoencoder, so that the diffusion process operates on a latent space representation of\nthe image rather than the image itself, as shown in Figure 13-20 .\nFigure 13-20. The Stable Diffusion  architecture\nThis breakthrough means that the denoising U-Net model can be kept relatively\nlightweight, in comparison to U-Net models that operate on full images. The autoen‐\ncoder handles the heavy lifting of encoding the image detail into latent space and\ndecoding the latent space back to a high-resolution image, leaving the diffusion\n380 | Chapter 13: Multimodal Models",1524
141-Architecture.pdf,141-Architecture,"model to work purely in a latent, conceptual space. This gives a significant speed and\nperformance boost to the training process.\nThe denoising process can also optionally be guided by a text prompt that has been\npassed through a text encoder. The first version of Stable Diffusion utilized the pre-\ntrained CLIP model from OpenAI (the same as in DALL.E 2), but Stable Diffusion 2\nhas a custom trained CLIP model called OpenCLIP , which has been trained from\nscratch.\nExamples from Stable Diffusion\nFigure 13-21  shows  some example outputs from Stable Diffusion 2.1—you can try\nyour own prompts through the model hosted on Hugging Face .\nFigure 13-21. Example outputs from Stable Diffusion  2.1\nExploring the Latent Space\nIf you’ d like to explore the latent space of the Stable Diffusion\nmodel, I highly recommended the walkthrough  on the Keras web‐\nsite.\nFlamingo\nSo far we have looked at three different kinds of text-to-image models. In this section,\nwe’ll explore a multimodal model that generates text given a stream of text and visual\ndata. Flamingo, introduced in a paper by DeepMind in April 2022, 7 is a family of vis‐\nual language models (VLMs) that act as a bridge between pre-trained vision-only and\nlanguage-only models.\nIn this section, we’ll run through the architecture of Flamingo models and compare\nthem to the text-to-image models we have seen so far.\nFlamingo | 381",1411
142-The Perceiver Resampler.pdf,142-The Perceiver Resampler,"Architecture\nThe overall architecture of Flamingo is shown in Figure 13-22 . For conciseness, we\nshall explore the core components of this model—the Vision Encoder, the Perceiver\nResampler, and the Language Mode—in just enough detail to highlight the key ideas\nthat make Flamingo unique. I highly recommend reading the original research paper\nfor a thorough review of each part of the model.\nFigure 13-22. The Flamingo architecture (source: Alayrac et al., 2022 )\nThe Vision Encoder\nThe first difference between a Flamingo model and pure text-to-image models such\nas DALL.E 2 and Imagen is that Flamingo can accept a combination of text and visual\ndata interleaved. Here, visual data  includes videos as well as images.\n382 | Chapter 13: Multimodal Models\nThe job of the Vision Encoder is to convert the vision data within the input into\nembedding vectors (similar to the image encoder in CLIP). The Vision Encoder in\nFlamingo is a pre-trained Normalizer-Free ResNet (NFNet), as introduced by Brock\net al. in 2021 8—in particular, an NFNet-F6 (the NFNet models range from F0 to F6,\nincreasing in size and power). This is one key difference between the CLIP image\nencoder and the Flamingo Vision Encoder: the former uses a ViT architecture,\nwhereas the latter uses a ResNet architecture.\nThe Vision Encoder is trained on image-text pairs using the same contrastive objec‐\ntive as introduced in the CLIP paper. After training, the weights are frozen so that any\nfurther training of the Flamingo model does not affect the weights of the Vision\nEncoder.\nThe output from the Vision Encoder is a 2D grid of features that then gets flattened\nto a 1D vector before being passed to the Perceiver Resampler. Video is handled by\nsampling at 1 frame per second and passing each snapshot through the Vision\nEncoder independently to produce several feature grids; learned temporal encodings\nare then added in before flattening the features and concatenating the results into a\nsingle vector.\nThe Perceiver Resampler\nMemory requirements in a traditional encoder Transformer (e.g., BERT) scale quad‐\nratically with input sequence length, which is why input sequences are normally cap‐\nped at a set number of tokens (e.g., 512 in BERT). However, the output from the\nVision Encoder is a vector of variable length (due to the variable input image resolu‐\ntion and the variable number of video frames) and is therefore potentially very long.\nThe Perceiver architecture is specifically designed to efficiently handle long input\nsequences. Instead of performing self-attention on the full input sequence, it works\nwith a fixed-length latent vector and only uses the input sequence for cross-attention.\nSpecifically, in the Flamingo Perceiver Resampler, the key and value  are a concatena‐\ntion of the input sequence and latent vector and the query  is the latent vector alone. A\ndiagram of the Vision Encoder and Perceiver Resampler process for video data is\nshown in Figure 13-23 .\nFlamingo | 383\nFigure 13-23. The Perceiver Resampler applied to video input (source: Alayrac et al.,\n2022 )\n384 | Chapter 13: Multimodal Models",3149
143-The Language Model.pdf,143-The Language Model,"The output of the Perceiver Resampler is a fixed-length latent vector that gets passed\nto the Language Model.\nThe Language Model\nThe Language Model consists of several stacked blocks, in the style of a decoder\nTransformer, that output a predicted text continuation. In fact, the majority of the\nLanguage Model is from a pre-trained DeepMind model called Chinchilla . The Chin‐\nchilla paper, published in March 2022, 9 showcases a language model that is designed\nto be considerably smaller than its peers (e.g., 70B parameters for Chinchilla com‐\npared to 170B for GPT-3), while using significantly more tokens for training. The\nauthors show that the model outperforms larger models on a range of tasks, high‐\nlighting the importance of optimizing the trade-off between training a larger model\nand using a larger number of tokens during training.\nA key contribution of the Flamingo paper is to show how Chinchilla can be adapted\nto work with additional vision data ( X) that is interspersed with the language data ( Y).\nLet’s first explore how the language and vision input are combined to produce the\ninput to the Language Model ( Figure 13-24 ).\nFirst the text is processed by replacing vision data (e.g., images) with an <image>  tag\nand the text is divided into chunks  using the <EOC>  (end of chunk) tag. Each chunk\ncontains at most one image, which is always at the start of the chunk—i.e., the subse‐\nquent text is assumed to relate only to that image. The beginning of the sequence is\nalso marked with the <BOS>  (beginning of sentence) tag.\nNext, the sequence is tokenized and each token is given an index ( phi) corresponding\nto the preceding image index (or 0 if there is no preceding image in the chunk). This\nway, the text tokens ( Y) can be forced to only cross-attend to the image tokens ( X) that\ncorrespond to their particular chunk, through masking. For example, in Figure 13-24\nthe first chunk contains no images, so all image tokens from the Perceiver Resampler\nare masked. The second chunk contains image 1, so these tokens are allowed to inter‐\nact with the image tokens from image 1. Likewise, the final chunk contains image 2,\nso these tokens are allowed to interact with the image tokens from image 2.\nFlamingo | 385\nFigure 13-24. Masked cross-attention (XATTN), combining vision and text data—light\nblue entries are masked and dark blue entries are nonmasked (source: Alayrac et al.,\n2022 )\nWe can now see how this masked cross-attention component fits into the overall\narchitecture of the Language Model ( Figure 13-25 ).\nThe blue LM layer components are frozen layers from Chinchilla—these are not\nupdated during the training process. The purple GATED XATTN-DENSE  layers are\ntrained as part of Flamingo and include the masked cross-attention components that\nblend the language and vision information, as well as subsequent feed-forward\n(dense) layers.\nThe layer is gated  because it passes the output from the cross-attention and feed-\nforward components through two distinct tanh gates, which are both initialized to\nzero. Therefore, when the network is initialized, there is no contribution from the\nGATED XATTN-DENSE  layers—the language information is just passed straight through.\n386 | Chapter 13: Multimodal Models\nThe alpha  gating parameters are learned by the network, to gradually blend in infor‐\nmation from the vision data as training progresses.\nFigure 13-25. A Flamingo Language Model block, comprising a frozen language model\nlayer from Chinchilla and a GATED XATTN-DENSE  layer (source: Alayrac et al., 2022 )\nFlamingo | 387",3619
144-Summary.pdf,144-Summary,"Examples from Flamingo\nFlamingo  can be used for a variety of purposes, including image and video under‐\nstanding, conversational prompting, and visual dialogue. In Figure 13-26  we can see a\nfew examples of what Flamingo is capable of.\nFigure 13-26. Examples of inputs and outputs obtained from the 80B parameter Fla‐\nmingo model (source: Alayrac et al., 2022 )\nNotice how in each example, Flamingo is blending information from the text and the\nimages in true multimodal style. The first example uses images in place of words and\nis able to suggest an appropriate book to continue the prompt. The second example\n388 | Chapter 13: Multimodal Models\nshows frames from a video, and Flamingo correctly identifies the consequence of the\naction. The last three examples all demonstrate how Flamingo can be used interac‐\ntively, to provide additional information through dialogue or probe with further\nquestioning.\nIt is astonishing to see a machine being able to answer complex questions across such\na wide range of modalities and input tasks. In the paper, the authors quantify Flamin‐\ngo’s ability across a set of benchmark tasks and find that across many benchmarks,\nFlamingo is able to surpass the performance of models that have been tailored to\nspecifically tackle the one task in question. This highlights how large multimodal\nmodels can be rapidly adapted to a wide range of tasks and paves the way for the\ndevelopment of AI agents that aren’t just tied to a single task, but instead are truly\ngeneral agents that can be guided by the user at inference time.\nSummary\nIn this chapter we have explored four different state-of-the-art multimodal models:\nDALL.E 2, Imagen, Stable Diffusion, and Flamingo.\nDALL.E 2 is a large-scale text-to-image model from OpenAI that can generate realis‐\ntic images across a range of styles given a text prompt. It works by combining pre-\ntrained models (e.g., CLIP) with diffusion model architectures from previous works\n(GLIDE). It also has additional capabilities, such as being able to edit images through\ntext prompting and provide variations of a given image. While it does have some lim‐\nitations, such as inconsistent text rendering and attribute binding, DALL.E 2 is an\nincredibly powerful AI model that has helped to propel the field of generative model‐\ning into a new era.\nAnother model that has surpassed previous benchmarks is Imagen from Google\nBrain. This model shares many similarities with DALL.E 2, such as a text encoder and\na diffusion model decoder. One of the key differences between the two models is that\nthe Imagen text encoder is trained on pure text data, whereas the training process for\nthe DALL.E 2 text encoder involves image data (through the contrastive CLIP learn‐\ning objective). The authors show that this approach leads to state-of-the-art perfor‐\nmance across a range of tasks, through their DrawBench evaluation suite.\nStable Diffusion is an open source offering from Stability AI, CompVis, and Runway.\nIt is a text-to-image model whose model weights and code are freely available, so you\ncan run it on your own hardware. Stable Diffusion is particularly fast and lightweight\ndue to the use of a latent diffusion model that operates on the latent space of an\nautoencoder, rather than the images themselves.\nFinally, DeepMind’s Flamingo is a visual language model—that is, it accepts a stream\nof interleaved text and visual data (images and video) and is able to continue\nthe prompt with additional text, in the style of a decoder Transformer. The key\nSummary | 389\ncontribution is showing how the visual information can be fed to the Transformer via\na Visual Encoder and Perceiver Resampler that encode the visual input features into a\nsmall number of visual tokens. The Language Model itself is an extension of Deep‐\nMind’s earlier Chinchilla model, adapted to blend in visual information.\nAll four are remarkable examples of the power of multimodal models. In the future, it\nis highly likely that generative modeling will become more multimodal and AI mod‐\nels will be able to easily cross modalities and tasks through interactive language\nprompting.\nReferences\n1. Aditya Ramesh et al., “Zero-Shot Text-to-Image Generation, ” February 24, 2021,\nhttps://arxiv.org/abs/2102.12092 .\n2. Aditya Ramesh et al., “Hierarchical Text-Conditional Image Generation with CLIP\nLatents, ” April 13, 2022, https://arxiv.org/abs/2204.06125 .\n3. Alec Radford et al., “Learning Transferable Visual Models From Natural Language\nSupervision, ” February 26, 2021, https://arxiv.org/abs/2103.00020 .\n4. Alex Nichol et al., “GLIDE: Towards Photorealistic Image Generation and Editing\nwith Text-Guided Diffusion Models, ” December 20, 2021, https://arxiv.org/abs/\n2112.10741 .\n5. Chitwan Saharia et al., “Photorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding, ” May 23, 2022, https://arxiv.org/abs/2205.11487 .\n6. Robin Rombach et al., “High Resolution Image Synthesis with Latent Diffusion\nModels, ” December 20, 2021, https://arxiv.org/abs/2112.10752 .\n7. Jean-Baptiste Alayrac et al., “Flamingo: A Visual Language Model for Few-Shot\nLearning, ” April 29, 2022, https://arxiv.org/abs/2204.14198 .\n8. Andrew Brock et al., “High-Performance Large-Scale Image Recognition Without\nNormalization, ” February 11, 2021, https://arxiv.org/abs/2102.06171 .\n9. Jordan Hoffmann et al., “Training Compute-Optimal Large Language Models, ”\nMarch 29, 2022, https://arxiv.org/abs/2203.15556v1 .\n390 | Chapter 13: Multimodal Models",5568
145-Timeline of Generative AI.pdf,145-Timeline of Generative AI,"CHAPTER 14\nConclusion\nChapter Goals\nIn this chapter you will:\n•Review the history of generative AI from 2014 to the present day, including a\ntimeline of key models and developments.\n•Understand the current state of generative AI, including the broad themes that\nare dominating the landscape.\n•See my predictions for the future of generative AI and how it will impact every‐\nday life, the workplace, and education.\n•Learn about the important ethical and practical challenges faced by generative AI\ngoing forward.\n•Read my final thoughts on the deeper meaning of generative AI and how it has\nthe potential to revolutionize our quest for artificial general intelligence.\nIn May 2018, I began work on the first edition of this book. Five years later, I am\nmore excited than ever about the endless possibilities and potential impact of genera‐\ntive AI.\nIn this time we have seen incredible progress in this field, with seemingly limitless\npotential for real-world applications. I am filled with a sense of awe and wonder at\nwhat we have been able to achieve so far and eagerly anticipate witnessing the effect\nthat generative AI will have on the world in the coming years. Generative deep learn‐\ning has the power to shape the future in ways we can’t even begin to imagine.\n391\nWhat’s more, as I have been researching content for this book, it has become ever\nclearer to me that this field isn’t just about creating images, text, or music. I believe\nthat at the core of generative deep learning lies the secret of intelligence itself.\nThe first section of this chapter summarizes how we have reached this point in our\ngenerative AI journey. We will walk through a timeline of generative AI develop‐\nments since 2014 in chronological order, so that you can see where each technique\nfits into the history of generative AI to date. The second section explains where we\ncurrently stand in terms of state-of-the-art generative AI. We will discuss current\ntrends in the approach to generative deep learning and the current off-the-shelf mod‐\nels available to the general public. Next, we will explore the future of generative AI\nand the opportunities and challenges that lie ahead. We will consider what generative\nAI might look like five years in the future and its potential impact on society and\nbusiness, and address some of the main ethical and practical concerns.\nTimeline of Generative AI\nFigure 14-1  is a timeline of the key developments in generative modeling that we have\nexplored together in this book. The colors represent different model types.\nTo field of generative AI stands on the shoulders of earlier developments in deep\nlearning, such as backpropagation and convolutional neural networks, which\nunlocked the possibility for models to learn complex relationships across large data‐\nsets at scale. In this section, we will study the modern history of generative AI, from\n2014 onwards, that has moved at such breathtaking speed.\nTo help us understand how everything fits together, we can loosely break down this\nhistory into three main eras:\n1.2014–2017: The V AE and GAN era\n2.2018–2019: The Transformer era\n3.2020–2022: The Big Model era\n392 | Chapter 14: Conclusion\nFigure 14-1. A brief history of generative AI from 2014 to 2023 (note: some important\ndevelopments such as LSTMs and early energy-based models [e.g., Boltzmann machines]\nprecede this timeline)\nTimeline of Generative AI | 393",3453
146-20142017 The VAE and GAN Era.pdf,146-20142017 The VAE and GAN Era,,0
147-The Current State of Generative AI.pdf,147-The Current State of Generative AI,"2014–2017: The VAE and GAN Era\nThe invention of the V AE in December 2013 can perhaps be thought of as the spark\nthat lit the generative AI touchpaper. This paper showed how it was possible to gener‐\nate not only simple images such as MNIST digits but also more complex images such\nas faces in a latent space that could be smoothly traversed. It was followed in 2014 by\nthe introduction of the GAN, an entirely new adversarial framework for tackling gen‐\nerative modeling problems.\nThe following three years were dominated by progressively more impressive exten‐\nsions of the GAN portfolio. In addition to fundamental changes to the GAN model\narchitecture (DCGAN, 2015), loss function (Wasserstein GAN, 2017), and training\nprocess  (ProGAN, 2017), new domains were tackled using GANs, such as image-to-\nimage translation (pix2pix, 2016, and CycleGAN, 2017) and music generation (Muse‐\nGAN, 2017).\nDuring this era, important V AE improvements were also introduced, such as V AE-\nGAN (2015) and later VQ-V AE (2017), and applications to reinforcement learning\nwere seen in the “World Models” paper (2018).\nEstablished  autoregressive models such as LSTMs and GRUs remained the dominant\nforce in text generation over this time. The same autoregressive ideas were also being\nused to generate images, with PixelRNN (2016) and PixelCNN (2016) introduced as\nnew ways to think about image generation. Other approaches to image generation\nwere also being tested, such as the RealNVP model (2016) that paved the way for later\ntypes of normalizing flow models.\nIn June 2017, a groundbreaking paper entitled “ Attention Is All Y ou Need” was\npublished  that would usher in the next era of generative AI, focused around\nTransformers.\n2018–2019: The Transformer Era\nAt the heart of a Transformer is the attention mechanism that negates the need for\nthe recurrent layers present in older autoregressive models such as LSTMs. The\nTransformer quickly rose to prominence with the introduction of GPT (a decoder-\nonly Transformer) and BERT (an encoder-only Transformer) in 2018. The following\nyear saw progressively larger language models being built that excelled at a wide\nrange of tasks by treating them as pure text-to-text generation problems, with GPT-2\n(2018, 1.5B parameters) and T5 (2019, 11B parameters) being standout examples.\nTransformers were also starting to be successfully applied to music generation, with\nthe introduction of, for example, the Music Transformer (2018) and MuseNet (2019)\nmodels.\n394 | Chapter 14: Conclusion\nOver these two years, several impressive GANs were also released that cemented the\ntechnique’s place as the state-of-the-art approach for image generation. In particular,\nSAGAN (2018) and the  larger BigGAN (2018) incorporated the attention mechanism\ninto the GAN framework with incredible results, and StyleGAN (2018) and later\nStyleGAN2 (2019) showed how images could be generated with amazing fine-grained\ncontrol over the style and content of a particular image.\nAnother field of generative AI that was gathering momentum was score-based mod‐\nels (NCSN, 2019), which would eventually pave the way for the next seismic shift in\nthe generative AI landscape—diffusion models.\n2020–2022: The Big Model Era\nThis  era saw the introduction of several models that merged ideas across different\ngenerative modeling families and turbo-charged existing architectures. For example,\nthe VQ-GAN (2020) brought the GAN discriminator into the VQ-V AE architecture\nand the Vision Transformer (2020) showed how it was possible to train a Trans‐\nformer to operate over images. 2022 saw the release of StyleGAN-XL, a further\nupdate to the StyleGAN architecture that enables 1,024 × 1,024–pixel images to be\ngenerated.\nTwo  models were introduced in 2020 that would lay the foundations for all future\nlarge image generation models: DDPM and DDIM. Suddenly, diffusion models were\na rival for GANs in terms of image generation quality, as explicitly stated in the title\nof the 2021 paper “Diffusion Models Beat GANs on Image Synthesis. ” The image\nquality of diffusion models is unbelievably good and they only require a single U-Net\nnetwork to be trained, rather than the dual-network setup of a GAN, making the\ntraining process much more stable.\nAround the same time, GPT-3 (2020) was released—an enormous 175B parameter\nTransformer that can generate text on just about any topic in a way that seems almost\nimpossible to comprehend. The model was released through a web application and\nAPI, allowing companies to build products and services on top of it. ChatGPT  (2022)\nis a web application and API wrapper around the latest version of GPT from OpenAI\nthat allows users to have natural conversations with the AI about any topic.\nOver 2021 and 2022, a flurry of other large language models were released to rival\nGPT-3, including Megatron-Turing NLG (2021) by Microsoft and NVIDIA, Gopher\n(2021) and Chinchilla by DeepMind (2022), LaMDA (2022) and PaLM (2022) by\nGoogle, and Luminous (2022) by Aleph Alpha. Some open source models were also\nreleased, such as GPT-Neo (2021), GPT-J (2021), and GPT-NeoX (2022) by Eleu‐\ntherAI; the 66B parameter  OPT model (2022) by Meta; the fine-tuned Flan-T5  model\n(2022) by Google, BLOOM (2022) by Hugging Face; and others. Each of these models\nis a variation of a Transformer, trained on a huge corpus of data.\nTimeline of Generative AI | 395",5462
148-Large Language Models.pdf,148-Large Language Models,"The rapid rise of powerful Transformers for text generation and state-of-the-art diffu‐\nsion models for image generation has meant that much of the focus of the last two\nyears of generative AI development has been on multimodal models—that is, models\nthat operate over more than one domain (for example, text-to-image models).\nThis trend was established in 2021 when OpenAI released DALL.E, a text-to-image\nmodel based upon a discrete V AE (similar to VQ-V AE) and CLIP (a Transformer\nmodel that predicts image/text pairs). This was followed by GLIDE (2021) and\nDALL.E 2 (2022), which updated the generative part of the model to use a diffusion\nmodel rather than a discrete V AE, with truly impressive results. This era also saw the\nrelease of three text-to-image models from Google: Imagen  (2022, using Transformer\nand diffusion models), Parti (2022, using Transformers and a ViT-VQGAN model),\nand later MUSE (2023, using Transformers and VQ-GANs). DeepMind also released\nFlamingo  (2022), a visual language model that builds upon their large language\nmodel Chinchilla by allowing images to be used as part of the prompt data.\nAnother important diffusion advancement introduced in 2021 was latent diffusion,\nwhere a diffusion model is trained within the latent space of an autoencoder. This\ntechnique powers the Stable Diffusion model, released as a joint collaboration\nbetween Stability AI, CompVis, and Runway in 2022. Unlike with DALL.E 2, Imagen,\nand Flamingo, the code and model weights of Stable Diffusion are open source,\nmeaning anyone can run the model on their own hardware.\nThe Current State of Generative AI\nAs we come to end of our journey through the history of generative AI, it is impor‐\ntant to now reflect on where we stand in terms of current state-of-the-art applications\nand models. Let’s take a moment to assess our progress and key accomplishments in\nthe field to date.\nLarge Language Models\nGenerative AI for text is now almost entirely focused on building large language\nmodels (LLMs), whose sole purpose is to directly model language from a huge corpus\nof text—that is, they are trained to predict the next word, in the style of a decoder\nTransformer.\nThe large language model approach has been adopted so widely because of its flexibil‐\nity and ability to excel at a wide range of tasks. The same model can be used for ques‐\ntion answering, text summarization, content creation, and many other examples\nbecause ultimately each use case can be framed as a text-to-text problem, where the\nspecific task instructions (the prompt ) are given as part of the input to the model.\nLet’s take GPT-3  as an example. Figure 14-2  shows how the same model can be used\nfor text summarization and content creation.\n396 | Chapter 14: Conclusion\nFigure 14-2. Output from GPT-3—the non-highlighted text is the prompt and the green\nhighlighted text is the output from GPT-3\nNotice  how in both cases, the prompt contains the relevant instructions. The job of\nGPT-3 is just to continue the prompt, one token at a time. It doesn’t have a database\nof facts from which it can look up information, or snippets of text that it can copy\ninto its answers. It is only asked to predict what token is most likely to follow the\nexisting tokens and then append this prediction to the prompt to generate the next\ntoken, and so on.\nIncredibly, this simple design is enough for the language model to excel at a range of\ntasks, as shown in Figure 14-2 . Moreover, it gives the language model incredible flexi‐\nbility to generate realistic text as a response to any prompt—imagination is often the\nlimiting factor!\nFigure 14-3  shows how large language models have grown in size since the original\nGPT model was published in 2018. The number of parameters grew exponentially\nThe Current State of Generative AI | 397\nuntil late 2021, with Megatron-Turing NLG reaching 530B parameters. Recently,\nmore emphasis has been placed on building more efficient language models that use\nfewer parameters, as larger models are more costly and slower to serve in a produc‐\ntion environment.\nFigure 14-3. The size of large language models (orange) and multimodal models (pink)\nin number of parameters over time\nOpenAI’s  GPT collection (GPT-3, GPT-3.5, GPT-4, etc.) is still considered by many\nto be the most powerful state-of-the-art suite of language models available for per‐\nsonal and commercial use. They are each available through a web application  and\nAPI.\nAnother recent addition to the large language model family  is Large Language Model\nMeta AI  (LLaMA) from Meta, 1 a suite of models ranging from 7B to 65B parameters\nin size that are trained purely on publicly available datasets.\nA summary of some of the most powerful LLMs in existence today is shown in\nTable 14-1 . Some, like LLaMA, are families of models of different sizes—in this case,\nthe size of the largest model is shown here. Pre-trained weights are fully open source\nfor some of the models, meaning that they are free for anyone to use and build upon.\n398 | Chapter 14: Conclusion\nTable 14-1. Large language models\nModel Date Developer # parameters Open source\nGPT-3 May 2020 OpenAI 175,000,000,000 No\nGPT-Neo Mar 2021 EleutherAI 2,700,000,000 Yes\nGPT-J Jun 2021 EleutherAI 6,000,000,000 Yes\nMegatron-Turing NLG Oct 2021 Microsoft & NVIDIA 530,000,000,000 No\nGopher Dec 2021 DeepMind 280,000,000,000 No\nLaMDA Jan 2022 Google 137,000,000,000 No\nGPT-NeoX Feb 2022 EleutherAI 20,000,000,000 Yes\nChinchilla Mar 2022 DeepMind 70,000,000,000 No\nPaLM Apr 2022 Google 540,000,000,000 No\nLuminous Apr 2022 Aleph Alpha 70,000,000,000 No\nOPT May 2022 Meta 175,000,000,000 Yes (66B)\nBLOOM Jul 2022 Hugging Face collaboration 175,000,000,000 Yes\nFlan-T5 Oct 2022 Google 11,000,000,000 Yes\nGPT-3.5 Nov 2022 OpenAI Unknown No\nLLaMA Feb 2023 Meta 65,000,000,000 No\nGPT-4 Mar 2023 OpenAI Unknown No\nDespite the impressive applications of large language models, there remain significant\nchallenges to overcome. Most notably, they are prone to inventing facts and cannot\nreliably apply logical thought processes, as shown in Figure 14-4 .\nFigure 14-4. While large language models excel at some tasks, they are also prone to mis‐\ntakes related to factual or logical reasoning (GPT-3 output shown)\nThe Current State of Generative AI | 399",6367
149-Text-to-Code Models.pdf,149-Text-to-Code Models,"It is important to remember that LLMs are trained only to predict the next word.\nThey have no other connection to reality that would allow them to reliably identify\nfactual or logical fallacies. Therefore, we must be extremely cautious about how we\nuse these powerful text prediction models in production—they cannot yet be reliably\nutilized for anything that requires precise reasoning.\nText-to-Code Models\nAnother  application of large language models is code generation. In July 2021,\nOpenAI introduced a model called  Codex , a GPT language model that had been fine-\ntuned on code from GitHub. 2 The model was able to successfully write novel coded\nsolutions to a range of problems, prompted only with a comment on the problem to\nbe solved, or a function name. The technology today powers GitHub Copilot , an AI\npair programmer that can be used to suggest code in real time as you type. Copilot is\na paid subscription-based service, with a free trial period.\nFigure 14-5  shows two examples of autogenerated completions. The first example is a\nfunction that fetches tweets from a given user, using the Twitter API. Given the func‐\ntion name and parameter, Copilot is able to autocomplete the rest of the function def‐\ninition. The second example asks Copilot to parse a list of expenses, by additionally\nincluding a free text description in the docstring that explains the format of the input\nparameter and specific instructions related to the task. Copilot is able to autocom‐\nplete the entire function from the description alone.\nThis remarkable technology is already beginning to change how programmers\napproach a given task. A significant proportion of a programmer’s time is usually\nspent searching for examples of existing solutions, reading community Q&A forums\nsuch as Stack Overflow, and looking up syntax in package documentation. This\nmeans leaving the interactive development environment (IDE) through which you\nare coding, switching to a web browser, and copying and pasting code snippets from\nthe web to see if they solve your specific problem. Copilot removes the need to do this\nin many cases, because you can simply tab through potential solutions generated by\nthe AI from within the IDE, after writing a brief description of what you are looking\nto achieve.\n400 | Chapter 14: Conclusion\nFigure 14-5. Two examples of GitHub Copilot capabilities (source: GitHub Copilot)\nThe Current State of Generative AI | 401",2463
150-Text-to-Image Models.pdf,150-Text-to-Image Models,"Text-to-Image Models\nState-of-the-art image generation is currently dominated by large multimodal models\nthat convert a given text prompt into an image. Text-to-image models are highly use‐\nful as they allow users to easily manipulate generated images via natural language.\nThis is in contrast to models such as StyleGAN, which, while extremely impressive,\ndoes not have a text interface through which you can describe the image that you\nwant to be generated.\nThree important text-to-image generation models that are currently available for\ncommercial and personal use are DALL.E 2, Midjourney, and Stable Diffusion.\nDALL.E 2 by OpenAI is a pay-as-you-go service that is available through a web appli‐\ncation and API. Midjourney  provides a subscription-based text-to-image service\nthrough its Discord channel. Both DALL.E 2 and Midjourney offer free credits to\nthose joining the platform for early experimentation.\nMidjourney\nMidjourney is the service used to create the illustrations for the sto‐\nries in Part II  of this book!\nStable Diffusion is different because it is fully open source. The model weights and\ncode to train the model are available on GitHub , so anyone can run the model on\ntheir own hardware. The dataset used to train Stable Diffusion is also open source.\nThis dataset, called  LAION-5B , contains 5.85 billion image-text pairs and is currently\nthe largest openly accessible image-text dataset in the world.\nAn important corollary of this approach is that the baseline Stable Diffusion model\ncan be built upon and adapted to different use cases. An excellent demonstration of\nthis is ControlNet, a neural network structure that allows fine-grained control of the\noutput from Stable Diffusion by adding extra conditions. 3 For example, output\nimages can be conditioned on  a Canny edge map  of a given input image, as shown in\nFigure 14-6 .\n402 | Chapter 14: Conclusion\nFigure 14-6. Conditioning the output of Stable Diffusion  using a Canny edge map and\nControlNet (source: Lvmin Zhang, ControlNet )\nControlNet contains a trainable copy of the Stable Diffusion encoder, alongside a\nlocked copy of the full Stable Diffusion model. The job of this trainable encoder is to\nlearn how to handle the input condition (e.g., the Canny edge map), whilst the locked\ncopy retains the power of the original model. This way, Stable Diffusion can be fine-\ntuned using only a small number of image pairs. Zero convolutions are simply 1 × 1\nconvolutions where all weights and biases are zero, so that before training, Control‐\nNet does not have any effect.\nThe Current State of Generative AI | 403\nFigure 14-7. The ControlNet architecture, with the trainable copies of the Stable Diffu‐\nsion encoder blocks highlighted in blue (source: Lvmin Zhang, ControlNet )\nAnother advantage of Stable Diffusion is that it is able to run on a single modestly\nsized GPU with only 8 GB of VRAM, making it possible to run on edge devices,\nrather than through calls to a cloud service. As text-to-image services are included in\ndownstream products, the speed of generation is becoming increasingly more impor‐\ntant. This is one reason why the size of multimodal models is generally trending\ndownward (see Figure 14-3 ).\n404 | Chapter 14: Conclusion",3286
151-Other Applications.pdf,151-Other Applications,"Example outputs for all three models can be seen in Figure 14-8 . All of these models\nare exceptional and are able to capture the content and style of the given description.\nFigure 14-8. Outputs from Stable Diffusion  v2.1, Midjourney, and DALL.E 2 for the\nsame prompt\nA summary of some of the most powerful text-to-image models in existence today is\nshown in Table 14-2 .\nTable 14-2. Text-to-image models\nModel Date Developer # parameters Open source\nDALL.E 2 Apr 2022 OpenAI 3,500,000,000 No\nImagen May 2022 Google 4,600,000,000 No\nParti Jun 2022 Google 20,000,000,000 No\nStable Diffusion Aug 2022 Stability AI, CompVis, and Runway 890,000,000 Yes\nMUSE Jan 2023 Google 3,000,000,000 No\nPart of the skill of working with text-to-image models is creating a prompt that both\ndescribes the content of the image you want to generate and uses keywords that\nencourage the model to produce a particular style or type of image. For example,\nadjectives such as stunning  or award-winning  can often be used to improve the quality\nof the generation. However, it is not always the case that the same prompt will work\nwell across different models—it depends on the contents of the specific text-image\ndataset used to train the model. The art of uncovering prompts that work well for a\nparticular model is known as prompt engineering .\nOther Applications\nGenerative AI is rapidly finding applications across a variety of novel domains, from\nreinforcement learning to other kinds of text-to-X  multimodal models.\nThe Current State of Generative AI | 405\nFor example, in November 2022 Meta published a paper on  CICERO , an AI agent\ntrained to play the board game Diplomacy . In this game, players represent different\ncountries in Europe before World War I and must negotiate with and deceive each\nother in order to gain control of the continent. It is a highly complex game for an AI\nagent to master, not least because there is a communicative element where players\nmust discuss their plans with other players in order to gain allies, coordinate maneu‐\nvers, and suggest strategic goals. To achieve this, CICERO contains a language model\nthat is able to initiate dialogue and respond to messages from other players. Crucially,\nthe dialogue is consistent with the agent’s strategic plans, which are generated by\nanother part of the model to adapt to the constantly evolving scenario. This includes\nthe ability for the agent to bluff when conversing with other players—that is, convince\nanother player to co-operate with the agent’s plans, only to then enact an aggressive\nmaneuver against the player in a later turn. Remarkably, in an anonymous online\nDiplomacy  league featuring 40 games, CICERO’s score was more than double the\naverage of the human players and it ranked in the top 10% of participants who played\nmultiple games. This is an excellent example of how generative AI can be successfully\nblended with reinforcement learning.\nThe development of embodied large language models is an exciting area of research,\nfurther exemplified by Google’s PaLM-E . This model combines the powerful language\nmodel PaLM with a Vision Transformer to convert visual and sensor data into tokens\nthat can be interleaved with text instructions, allowing robots to execute tasks based\non text prompts and continuous feedback from other sensory modalities. The PaLM-\nE website showcases the model’s abilities, including controlling a robot to arrange\nblocks and fetch objects based on text descriptions.\nText-to-video  models  involve the creation of videos from text input. This field, which\nbuilds on the concept of text-to-image modeling, has the additional challenge of\nincorporating a time dimension. For example, in September 2022 Meta published\nMake-A-Video , a generative model that is able to create a short video given only a text\nprompt as input. The model is also able to add motion between two static images and\nproduce variations of a given input video. Interestingly, it is trained only on paired\ntext–image data and unsupervised video footage, rather than text–video pairs\ndirectly. The unsupervised video data is enough for the model to learn how the world\nmoves; it then uses the text–image pairs to learn how to map between text image\nmodalities, which are then animated. The Dreamix  model  is able to perform video\nediting, where an input video is transformed based on a given text prompt while\nretaining other stylistic attributes. For example, a video of a glass of milk being\npoured could be converted to a cup of coffee being poured, while retaining the cam‐\nera angle, background, and lighting elements of the original video.\n406 | Chapter 14: Conclusion",4717
152-The Future of Generative AI.pdf,152-The Future of Generative AI,,0
153-Generative AI in Everyday Life.pdf,153-Generative AI in Everyday Life,"Similarly,  text-to-3D  models extend traditional text-to-image approaches into a third\ndimension. In September 2022 Google published DreamFusion , a diffusion model\nthat generates 3D assets given an input text prompt. Crucially, the model does not\nrequire labeled 3D assets to train on. Instead, the authors use a pre-trained 2D text-\nto-image model (Imagen) as a prior and then train a 3D Neural Radiance Field\n(NeRF), such that it is able to produce good images when rendered from random\nangles. Another example is OpenAI’s Point-E , published in December 2022. Point-E is\na pure diffusion-based system that is able to generate a 3D point cloud from a given\ntext prompt. While the output produced is not as high quality as DreamFusion’s, the\nadvantage of this approach is that is much faster than NeRF-based methods—it can\nproduce output in just one to two minutes on a single GPU, rather than requiring\nmultiple GPU-hours.\nGiven the similarities between text and music, it is not surprising that there have also\nbeen attempts to create text-to-music  models. MusicLM , released by Google in Janu‐\nary 2023, is a language model that is able to convert a text description of a piece of\nmusic (e.g., “a calming violin melody backed by a distorted guitar riff ”) into audio\nspanning several minutes that accurately reflects the description. It builds upon the\nearlier work AudioLM  by adding the ability for the model to be guided by a text\nprompt; examples that you can listen to are available on the Google Research website.\nThe Future of Generative AI\nIn this final section, we will explore the potential impact that powerful generative AI\nsystems may have on the world we live in—across our everyday lives, in the work‐\nplace, and within the field of education. We will also lay down the key practical and\nethical challenges generative AI will face if it is to become a ubiquitous tool that\nmakes a significant net positive contribution to society.\nGenerative AI in Everyday Life\nThere is no doubt that in the future generative AI will play an increasingly important\nrole in people’s everyday lives—particularly large language models. With OpenAI’s\nChatGPT , it is already possible to generate a perfect cover letter for a job application,\na professional email response to a colleague, or a funny social media post on a given\ntopic using generative AI. This technology is truly interactive: it is able to include\nspecific details that you request, respond to feedback, and ask its own questions back\nif something isn’t clear. This style of personal assistant  AI should be the stuff of sci‐\nence fiction, but it isn’t—it’s here right now, for anyone who chooses to use it.\nWhat are the repercussions of this kind of application becoming mainstream? It is\nlikely that the most immediate effect will be an increase in the quality of written com‐\nmunication. Access to large language models with a user-friendly interface will enable\npeople to translate a sketch of an idea into coherent, high-quality paragraphs in\nThe Future of Generative AI | 407\nseconds . Email writing, social media posts, and even short-form instant messaging\nwill be transformed by this technology. It goes beyond removing the common barri‐\ners associated with spelling, grammar, and readability—it directly links our thought\nprocesses to usable output, often removing the need to engage with the process of\nconstructing sentences at all.\nProduction of well-formed text is only one use of large language models. People will\nstart using these models for idea generation, advice, and information retrieval. I\nbelieve we can see this as the fourth stage of our ability as a species to acquire, share,\nretrieve, and synthesize information. We started by acquiring information from those\naround us, or physically traveling to new locations to transfer knowledge. The inven‐\ntion of the printing press allowed the book to become the primary vessel through\nwhich ideas were shared. Finally, the birth of the internet allowed us to instantane‐\nously search for and retrieve information at the touch of a button. Generative AI\nunlocks a new era of information synthesis that I believe will replace many of the cur‐\nrent uses of today’s search engines.\nFor example, OpenAI’s GPT suite of models can provide bespoke holiday destination\nrecommendations, as shown in Figure 14-9 , or advice on how to respond to a diffi‐\ncult situation, or a detailed explanation of an obscure concept. Using this technology\nfeels more like asking a friend than typing a query into a search engine, and for that\nreason, people are flocking to it extremely quickly. ChatGPT is the fastest-growing\ntech platform ever; it acquired 1 million users within 5 days of its launch. For context,\nit took Instagram 2.5 months to reach the same number of users and Facebook 10\nmonths.\nFigure 14-9. Output from GPT-3, giving bespoke holiday recommendations\n408 | Chapter 14: Conclusion",4980
154-Generative AI Ethics and Challenges.pdf,154-Generative AI Ethics and Challenges,"Generative AI in the Workplace\nAs well as general use, generative AI will find applications in specific jobs where crea‐\ntivity is required. A nonexhaustive list of occupations that may benefit follows:\nAdvertising\nGenerative AI can be used to create personalized ad campaigns that target spe‐\ncific demographics based on their browsing and purchase history.\nMusic production\nGenerative AI can be used to compose and produce original music tracks, allow‐\ning for a limitless range of possibilities.\nArchitecture\nGenerative AI can be used to design buildings and structures, taking into account\nfactors such as style and constraints around layout.\nFashion design\nGenerative AI can be used to create unique and diverse clothing designs, taking\ninto account trends and wearer preferences.\nAutomotive design\nGenerative AI can be used to design and develop new vehicle models and auto‐\nmatically find interesting variations on a particular design.\nFilm and video production\nGenerative AI can be used to create special effects and animations, as well as to\ngenerate dialogue for entire scenes or storylines.\nPharmaceutical research\nGenerative AI can be used to generate new drug compounds, which can aid in\nthe development of new treatments.\nCreative writing\nGenerative AI can be used to generate written content, such as fiction stories,\npoetry, news articles, and more.\nGame design\nGenerative AI can be used to design and develop new game levels and content,\ncreating an infinite variety of gameplay experiences.\nDigital design\nGenerative AI can be used to create original digital art and animations, as well as\nto design and develop new user interfaces and web designs.\nAI is often said to pose an existential threat to jobs in fields such as these, but I do not\nbelieve that this is actually the case. For me, AI is just another tool in the toolbox of\nthese creative roles (albeit a very powerful one), rather than a replacement for the role\nThe Future of Generative AI | 409\nitself. Those who choose to embrace this new technology will find that they are able\nto explore new ideas much faster and iterate over concepts in a way that previously\nwas not possible.\nGenerative AI in Education\nOne  final area of everyday life that I believe will be significantly impacted is educa‐\ntion. Generative AI challenges the fundamental axioms of education in a way that we\nhaven’t seen since the dawn of the internet. The internet gave students the ability to\nretrieve information instantaneously and unambiguously, making exams that purely\ntested memorization and recall seem old-fashioned and irrelevant. This prompted a\nshift in approach, focused on testing students’ ability to synthesize ideas in a novel\nway instead of only testing factual knowledge.\nI believe that generative AI will cause another transformative shift in the field of edu‐\ncation, necessitating a reevaluation and adjustment of current teaching methods and\nassessment criteria. If every student now has access to an essay-writing machine in\ntheir pocket that can generate novel responses to questions, what is the purpose of\nessay-based coursework?\nMany would call for the use of such AI tools to be banned, in the same way that plagi‐\narism is banned. However, it’s not that simple, as detecting AI-generated text is much\nharder than detecting plagiarism and even harder to prove beyond doubt. Moreover,\nstudents could use AI tools to generate a skeleton draft for the essay and then add\nextra detail or update factually incorrect information as required. In this case, is it the\nstudent’s original work, or the AI’s?\nClearly, these are huge questions that need to be addressed in order for education and\ncertifications to maintain their integrity. In my opinion, there is no sense in resisting\nthe proliferation of AI tools within education—any such approach is doomed to fail,\nas they will become so widespread in everyday life that trying to restrict their use will\nbe futile. Instead, we need to find ways to embrace the technology and ask how we\ncan design open-AI  coursework, in the same way that we allow open-book  course‐\nwork, and encourage students to openly research material using the internet and AI\ntools.\nThe potential for generative AI to assist with the learning process itself is also\nimmense and deeply profound. An AI-powered tutor could help a student learn a\nnew topic (as shown in Figure 14-10 ), overcome a misunderstanding, or generate an\nentirely personalized study plan. The challenge of filtering truth from generated fic‐\ntion is no different from what we currently have with information available on the\ninternet and is a life skill that needs further attention across the curriculum.\n410 | Chapter 14: Conclusion\nFigure 14-10. Output from GPT-3—an example of how large language models can be\nused for learning\nGenerative AI can be an incredibly powerful tool to level the playing field between\nthose who have access to excellent teachers and the best learning materials and those\nwho do not. I am excited to see the progress in this space, as I believe it could unlock\nmassive amounts of potential across the globe.\nGenerative AI Ethics and Challenges\nDespite the incredible progress that has been made in the field of generative AI, there\nremain many challenges to overcome. Some of these challenges are practical and oth‐\ners ethical.\nFor example, a major criticism of large language models is that they are prone to gen‐\nerate misinformation when asked about a topic that is unfamiliar or contradictory, as\nshown in Figure 14-4 . The danger with this is that it is difficult to know if the infor‐\nmation that is contained within a generated response is truly  accurate. Even if you ask\nthe LLM to explain its reasoning or cite sources, it might make up references or spout\na series of statements that do not logically follow on from one another. This is not an\neasy problem to solve, as the LLM is nothing more than a set of weights that accu‐\nrately capture the most likely next word given a set of input tokens—it does not have\na bank of true information that it can use as a reference.\nA potential solution to this problem is to provide large language models with the abil‐\nity to call upon structured tools such as calculators, code compilers, and online infor‐\nmation sources for tasks that require precise execution or facts. For example,\nFigure 14-11  shows output from a model called Toolformer , published by Meta in\nFebruary 2023. 4\nThe Future of Generative AI | 411\nFigure 14-11. An example of how Toolformer is able to autonomously call different  APIs\nin order to obtain precise information where necessary (source: Schick et al., 2023 )\nToolformer is able to explicitly call APIs for information, as part of its generative\nresponse. For example, it might use the Wikipedia API to retrieve information about\na particular person, rather than relying on this information being embedded in its\nmodel weights. This approach is particularly useful for precise mathematical opera‐\ntions, where Toolformer can state which operations it would like to enter into the cal‐\nculator API instead of trying to generate the answer autoregressively in the useful\nfashion.\nAnother prominent ethical concern with generative AI centers on the fact that large\ncompanies have used huge amounts of data scraped from the web to train their mod‐\nels, when consent was not explicitly given by the original creators to do so. Often this\ndata is not even publicly released, so it is impossible to know if your data is being\nused to train large language models or multimodal text-to-image models. Clearly this\nis a valid concern, particularly for artists, who may argue that it is usage of their art‐\nwork for which they are not being paid any royalties or commission. Moreover, an\nartist’s name may be used as a prompt in order to generate more artwork that is simi‐\nlar in style to the originals, thereby degrading the uniqueness of the content and com‐\nmoditizing the style.\nA solution to this problem is being pioneered by Stability AI, whose multimodal\nmodel Stable Diffusion is trained on a subset of the open source LAION-5B dataset.\n412 | Chapter 14: Conclusion",8273
155-Final Thoughts.pdf,155-Final Thoughts,"They have also launched the website Have I Been Trained?  where anyone can search\nfor a particular image or text passage within the training dataset and opt out of future\ninclusion in the model training process. This puts control back in the hands of the\noriginal creators and ensures that there is transparency in the data that is being used\nto create powerful tools like this one. However, this practice is not commonplace, and\nmany commercially available generative AI models do not make their datasets or\nmodel weights open source or provide any option to opt out of the training process.\nIn conclusion, while generative AI is a powerful tool for communication, productiv‐\nity, and learning across everyday life, in the workplace, and in the field of education,\nthere are both advantages and disadvantages to its widespread use. It is important to\nbe aware of the potential risks of using the output from a generative AI model and to\nalways be sure to use it responsibly. Nevertheless, I remain optimistic about the future\nof generative AI and am eager to see how businesses and people adapt to this new and\nexciting technology.\nFinal Thoughts\nIn this book we have taken a journey through the last decade of generative modeling\nresearch, starting out with the basic ideas behind V AEs, GANs, autoregressive mod‐\nels, normalizing flow models, energy-based models, and diffusion models and build‐\ning upon these foundations to understand how state-of-the-art techniques such as\nVQ-GAN, Transformers, world models, and multimodal models are now pushing the\nboundaries of what generative models are capable of achieving, across a variety of\ntasks.\nI believe that in the future, generative modeling may be the key to a deeper form of\nartificial intelligence that transcends any one particular task and allows machines to\norganically formulate their own rewards, strategies, and perhaps awareness within\ntheir environment. My beliefs are closely aligned to the principle of active inference ,\noriginally pioneered by Karl Friston. The theory behind active inference could easily\nfill another entire book—and does, in Thomas Parr et al. ’s excellent Active Inference:\nThe Free Energy Principle in Mind, Brain, and Behavior  (MIT Press), which I highly\nrecommend—so I will only attempt a short explanation here.\nAs babies, we are constantly exploring our surroundings, building up a mental model\nof possible futures with no apparent aim other than to develop a deeper understand‐\ning of the world. There are no labels on the data that we receive—a seemingly ran‐\ndom stream of light and sound waves that bombard our senses from the moment we\nare born. Even when our someone points to an apple and says apple , there is no rea‐\nson for our young brains to associate the two inputs and learn that the way in which\nlight entered our eye at that particular moment is in some way related to the way the\nsound waves entered our ear. There is no training set of sounds and images, no train‐\nFinal Thoughts | 413\ning set of smells and tastes, and no training set of actions and rewards; there’s just an\nendless stream of extremely noisy data.\nAnd yet here you are now, reading this sentence, perhaps enjoying the taste of a cup\nof coffee in a noisy cafe. Y ou pay no attention to the background noise as you concen‐\ntrate on converting the absence of light on a tiny portion of your retina into a\nsequence of abstract concepts that convey almost no meaning individually but, when\ncombined, trigger a wave of parallel representations in your mind’s eye—images,\nemotions, ideas, beliefs, and potential actions all flood your consciousness, awaiting\nyour recognition. The same noisy stream of data that was essentially meaningless to\nyour infant brain is not so noisy anymore. Everything makes sense to you. Y ou see\nstructure everywhere. Y ou are never surprised by the physics of everyday life. The\nworld is the way that it is because your brain decided it should be that way. In this\nsense, your brain is an extremely sophisticated generative model, equipped with the\nability to attend to particular parts of the input data, form representations of concepts\nwithin a latent space of neural pathways, and process sequential data over time.\nActive inference is a framework that builds upon this idea to explain how the brain\nprocesses and integrates sensory information to make decisions and actions. It states\nthat an organism has a generative model of the world it inhabits, and uses this model\nto make predictions about future events. In order to reduce the surprise caused by\ndiscrepancies between the model and reality, the organism adjusts its actions and\nbeliefs accordingly. Friston’s key idea is that action and perception optimization can\nbe framed as two sides of the same coin, with both seeking to minimize a single quan‐\ntity known as free energy .\nAt the heart of this framework is a generative model of the environment (captured\nwithin the brain) that is constantly being compared to reality. Crucially, the brain is\nnot a passive observer of events. In humans, it is attached to a neck and a set of legs\nthat can put its core input sensors in a myriad of positions relative to the source of the\ninput data. Therefore, the generated sequence of possible futures is not only depen‐\ndent on its understanding of the physics of the environment, but also on its under‐\nstanding of itself  and how it acts. This feedback loop of action and perception is\nextremely interesting to me, and I believe we have only scratched the surface of what\nis possible with embodied generative models that are able to take actions within a\ngiven environment according to the principles of active inference.\nThis is the core idea that I believe will continue to propel generative modeling into\nthe spotlight in the next decade, as one of the keys to unlocking artificial general\nintelligence.\nWith that in mind, I encourage you to continue learning more about generative mod‐\nels from all the great material that is available online and in other books. Thank you\nfor taking the time to read to the end of this book—I hope you have enjoyed reading\nit as much as I have enjoyed generating it!\n414 | Chapter 14: Conclusion\nReferences\n1. Hugo Touvron et al., “LLaMA: Open and Efficient Foundation Language Models, ”\nFebruary 27, 2023, https://arxiv.org/abs/2302.13971 .\n2. Mark Chen et al., “Evaluating Large Language Models Trained on Code, ” July 7,\n2021, https://arxiv.org/abs/2107.03374 .\n3. Lvmin Zhang and Maneesh Agrawala, “ Adding Conditional Control to Text-to-\nImage Diffusion Models, ” February 10, 2023, https://arxiv.org/abs/2302.05543 .\n4. Timo Schick et al., “Toolformer: Language Models Can Teach Themselves to Use\nTools, ” February 9, 2023, https://arxiv.org/abs/2302.04761 .\nFinal Thoughts | 415",6877
156-Index.pdf,156-Index,"Index\nSymbols\n1-Lipschitz continuous function, 115\nA\naccuracy, determining, 99, 264, 399, 411\naction, in reinforcement learning, 333\nactivation functions, 32\nactive inference, 414\nAdam (Adaptive Moment Estimation) opti‐\nmizer, 36\nadaptive instance normalization (AdaIN), 279,\n282\nagent, in reinforcement learning, 333\nAI (artificial intelligence), 8, 413\nAI ethics, 411-413\napproximate density models, 20\nartifacts, 103, 281\nartificial intelligence (AI), 8, 413\nartificial neural networks (ANNs), 25\narXiv, xxii\n“ Attention Is All Y ou Need” (Vaswani), 236, 394\nattention mechanisms\nattention equation, 241\nattention head, 239\nattention scores, 253\nattention weights, 241\ngenerating polyphonic music, 313\npaper popularizing, 236\nself- versus cross-referential, 258\nunderstanding, 238\nattribute binding, 376\nattributes, entangled, 277\nAudioLM, 407“ Auto-Encoding Variational Bayes” (Kingma\nand Welling), 59\nautoencoders (see also variational autoencod‐\ners)\narchitecture of, 63\ndecoder architecture, 65-67\ndiagram of process, 61\nencoder architecture, 64\nFashion-MNIST dataset, 62\ngenerating new images, 71-74\njoining encoder to decoder, 67\nreconstructing images, 69\nuses for, 64\nvisualizing latent space, 70\nautoregressive models\nautoregressive prior of DALL.E 2, 367, 374\nbidirectional cells, 153\ndescription of, 130\ngated recurrent units (GRUs), 151-153\ngenerative model taxonomy, 19\nhistory of, 394\nhow LSTMs work, 130\nlong short-term memory (LSTM) networks,\n131-149\nmasked convolutional layers, 154-156\nstacked recurrent networks, 149-151\nB\nBach chorale dataset, 317\nbackpropagation, 26\nbatch normalization, 46-49, 51, 120\nbatches, 37\nBERT (Bidirectional Encoder Representations\nfrom Transformers), 255, 394\n417\nbidirectional cells, 153\nbig model era, 395\nBigGAN, 288, 395\nbinary cross-entropy loss, 36\nBLOOM, 395, 399\nBoltzmann distribution, 191\nBoltzmann machine, 203\nBookCorpus, 236\nBricks dataset, 98\nC\nCanny edge maps, 402\ncategorical cross-entropy loss, 36\ncausal masking, 242-244\nCelebFaces Attributes (CelebA) dataset, 85, 275\nCGAN (see conditional GAN)\nchallenges, of generative AI, 264, 399, 411-413\nchange of variables equation, 173\nchange of variables technique, 170-172\ncharacter tokens, 135\nChatGPT, 260-264, 395, 407\nChinchilla, 395\nCICERO, 406\nCIFAR-10 dataset, 28\nCLIP (Contrastive Language-Image Pre-\ntraining)\ndescription of, 362\nhistory of, 396\nkey concepts behind, 363\ntraining process, 363-367\nCMA-ES (covariance matrix adaptation evolu‐\ntion strategy), 348-353\nCNN (see convolutional neural networks)\ncode examples, obtaining and using, xxiii , 20\ncodebook, 290\nCodex, 400\ncomments and questions, xxv\ncompile method, 36\nconditional GAN (CGAN)\nanalysis of, 126\narchitecture of, 123\ntraining, 124\n“Conditional Generative Adversarial Nets”\n(Mirza and Osindero), 122\ncontext vector, 241\ncontrastive divergence, 191, 197-201\nContrastive Language-Image Pre-training (see\nCLIP)\ncontrastive learning, 362ControlNet, 402\nconvolutional neural networks (CNNs)\nbatch normalization, 46-49\nbenefits of, 40\nbuilding, 51-53\nconvolutional layers, 41-46\ndropout, 49-51\nmasked convolutional layers, 154-156\ntraining and evaluating, 53\nconvolutional transpose layers, 65\nCopilot, 400\ncosine diffusion schedule, 212\ncosine similarity, 363\ncoupling layers, 175-177, 180\ncovariate shift, 47\ncross-referential attention, 258\nCycleGAN, 291, 394\nD\nDALL.E, 289, 361, 376, 396\nDALL.E 2\narchitecture, 362\navailability of, 402\ndecoder, 369-373\nexamples generated by, 373-375, 405\nhistory of, 361, 396\nlimitations of, 375\ntext encoder, 362-367\ntraining the prior model, 367-369\nDCGAN (see deep convolutional GAN)\nDDIM (see Denoising Diffusion Implicit\nModel)\nDDM (see denoising diffusion models)\nDDPM (see Denoising Diffusion Probabilistic\nModel)\ndecoder Transformers, 244, 255\ndeep convolutional GAN (DCGAN)\nanalysis of, 109\ndataset used, 98\ndiscriminator in, 99-101\ngenerator in, 101-103\nhistory of, 394\npublished paper on, 97\ntraining, 104-109\ntraining tips and tricks, 110-113\ndeep learning\ndeep neural networks, 25-27, 54\ndefined, 23\nKeras and TensorFlow for, 27\n418 | Index\nmodel creation, 28-40\nmodel improvement, 40-54\nstructured versus unstructured data, 24\ndemodulation step, 283\nDenoising Diffusion Implicit Model (DDIM)\ndescription of, 226\nhistory of, 395\ndenoising diffusion models (DDMs)\nanalysis of, 228-230\ndataset used, 208-209\ndescription of, 208\ndiffusion schedules, 211\nforward diffusion process, 209\nreparameterization trick, 210\nreverse diffusion process, 214-216\nsampling from, 225-227\ntraining, 224-225\nU-Net denoising model, 217-224, 370\nDenoising Diffusion Probabilistic Model\n(DDPM)\ndevelopment of, 203\nhistory of, 395\ndenoising models, 64 (see also denoising diffu‐\nsion models)\ndense layers, 25\ndensity function, 16, 18\ndeterminants, 172\nDhariwal, Prafulla, 395\ndiffusion models (see also denoising diffusion\nmodels)\ndescription of, 205\ndiffusion prior of DALL.E 2, 368\ngenerative model taxonomy, 20\nhistory of, 395\nkey ideas underpinning, 206-207\nlatent diffusion, 396\n“Diffusion Models Beat GANs on Image Syn‐\nthesis ” (Dhariwal and Nichol), 395\ndiffusion schedules, 211\nDinh, Laurent, 174\nDiplomacy board game, 406\ndiscrete latent space, 290\ndiscriminative modeling, 5\ndiscriminators, 97, 99-101, 110\nDong, Hae-Wen, 317\nDrawBench, 378\nDreamFusion, 407\nDreamix, 406\ndropout layers, 49Du, Yilun, 191\nE\neducational applications, 410\nEMA (exponential moving average), 214\nembedding space, 63, 70, 85-93, 219\nembodied large language models, 406\nencoder Transformers, 244, 255\nencoder-decoder Transformers, 255\nencoders, 64\nenergy function/energy score, 191, 193\nenergy-based models (EBMs)\nanalysis of, 201-202\nBoltzmann distribution, 191\nBoltzmann machine, 203\ndataset used, 192\ndescription of, 189\nenergy function, 193\ngenerative model taxonomy, 20\nkey concepts behind, 189\nRBM (restricted Boltzmann machine), 203\nsampling using Langevin dynamics, 194-196\nscore-based generative models, 206\ntraining with contrastive divergence,\n197-201\nentangled attributes, 277\nenvironment, in reinforcement learning, 332\nepisode, in reinforcement learning, 333\nepochs, 38\nequalized learning rates, 276\nethical concerns, of generative AI, 411-413\nevaluate method, 38\nevent-based tokenization, 315\nevolutionary strategies, 349\nexplicit density models, 19\nexploding gradient problem, 46\nexponential moving average (EMA), 214\nF\nfacial image generation\ndataset used, 85\ngenerating new faces, 90\nlatent space arithmetic, 91\nmorphing between faces, 92\nprogress in, 7\nV AE analysis, 89\nV AE training, 87\nFashion-MNIST dataset, 62, 70\nfast-track route, 156\nIndex | 419\nfeature engineering, 26\nfeatures\ndescriptions of, 4\nlearning high-level, 26\nFFJORD (Free-Form Continuous Dynamics for\nScalable Reversible Generative Models), 187\nfilters, 41\nfit method, 37\nFlamingo\narchitecture, 382-387\nexamples generated by, 388-389\nhistory of, 396\nFlan-T5, 395\nFlowers dataset, 208\nforward diffusion process, 209\nforward pass, 26\nFree-Form Continuous Dynamics for Scalable\nReversible Generative Models (FFJORD),\n187\nfully connected layers, 25\nfunctional API (Keras), 30-35\nG\ngame state, in reinforcement learning, 333\nGAN (see generative adversarial networks)\ngated recurrent units (GRUs), 132, 151, 394\nGaussian distribution (normal distribution), 75\nGenAI (see generative AI)\n“Generative Adversarial Nets” (Goodfellow), 95\ngenerative adversarial networks (GANs)\nBigGAN, 288\nchallenges of, 110, 113\nconditional GAN (CGAN), 122-127\ndeep convolutional GANs (DCGANs),\n97-113\nfundamental training concepts, 96\ngenerative model taxonomy, 19\nhistory of, 95, 394\nProGAN, 269-276\nStyleGAN, 277-281\nStyleGAN2, 281-286\nWasserstein GAN with Gradient Penalty\n(WGAN-GP), 113-122\nversus WGAN-GPs, 121\ngenerative AI (GenAI)\ncurrent state of, 396-407\nethics and challenges related to, 264, 411\nfuture of, 407-411, 413\nhistory of, 392-396generative deep learning (see also models)\nadditional resources, xxii\ncore probability theory, 15-18\ncreating something that is creative, xvii\ngenerative modeling framework, 10\nintroduction to, 4-9\nlearning objectives and approach, xviii\nprerequisites to learning, xix\ngenerative modeling (see models)\ngenerative pre-trained transformer (see GPT)\ngenerators\nattention-based, 287\nbar generator, 323\nDCGAN generator, 101-103, 110\nin GANs, 97, 101-103\nMuseGAN generator, 320\nStyleGAN generator, 277\nGitHub Copilot, 400\nGLIDE (Guided Language-to-Image Diffusion\nfor Generation and Editing), 369-373, 396\nGLOW model, 186\nGoodfellow, Ian, 95\nGopher, 395\nGPT (generative pre-trained transformer)\nanalysis of, 252-255\napplications in everyday life, 407\nattention mechanism, 238\ncausal masking, 242-244\ndataset used, 237\ndescription of, 236\nevolution of, 259\nhistory of, 236, 394\nimprovements to, 237\nmultihead attention, 241\npositional encoding, 248-250\nqueries, keys, and values, 239-241\nTransformer block, 245-248\nGPT-2, 237, 394\nGPT-3\navailability of, 398\nbenefits of, 237\nevolution of, 259\nexample generated by, 260, 396, 411\nhistory of, 395\nGPT-3.5, 237, 262\nGPT-4, 237, 259\ngradient descent using Langevin dynamics, 195\ngradient penalty loss, 117\nGradient Tape, 83\n420 | Index\ngrid tokenization, 313-315\nGRU (see gated recurrent units)\nGuided Language-to-Image Diffusion for Gen‐\neration and Editing (GLIDE), 369-373\nH\nHa, David, 331, 337, 394\nhallucinations, 264\nHe initialization, 276\nhidden layers, 27\nhidden state, 140, 142\nHinton, Geoffrey, 49\nHochreiter, Sepp, 132\nHuang, Cheng-Zhi Anna, 313\nHui, Jonathan, 116\nhyperparameters, 112\nI\nimage generation (see also facial image genera‐\ntion; PixelCNN)\nbenefits of diffusion models for, 205\nBigGAN, 288\nCIFAR-10 dataset for, 28\nDDM analysis, 228-230\ngenerating new images, 71-74\ngenerative modeling process, 4\ngenerative versus discriminative modeling,\n5\nhistory of, 395\nProGAN, 269-276\nprogress in facial image generation, 7\nreconstructing images, 69\nrepresentation learning for, 13\nSelf-Attention GAN (SAGAN), 286\nStyleGAN2, 281-286\nvisualizing latent space, 70\nimage-to-image models, 291, 394\nImagen\narchitecture, 377\nDrawBench, 378\nexamples generated by, 379\nhistory of, 377, 396\noverview of, 405\nImages of LEGO Bricks dataset, 98\nimplicit density models, 19\n“Implicit Generation and Modeling with\nEnergy-Based Models” (Du and Mordatch),\n191“Improving Language Understanding by Gen‐\nerative Pre-Training” (Radford), 236\nin-dream training, 353-356\nInstructGPT model, 262\nisotropic multivariate normal distributions, 76\nJ\nJacobian determinant, 172\njoint token/position encoding, 249\nK\nKaggle, 86, 237\nKeras (see also models)\nautoencoder creation in, 65\nbenefits of, 27\nConv2DTranspose layer, 66\ncreating new layers in, 79\ndata loading, 28\ndataset creation, 98\ndecoder creation in, 67\ndocumentation, 36\nGAN discriminator creation in, 100\nmodel building, 30-35\nmodel compilation, 35\nmodel evaluation, 38-40\nmodel improvement, 40-54\nmodel training, 37\nMuseGAN generator in, 324\nresources, 20\nStyleGAN tutorial, 278\nV AE creation in, 78\nKeras layers\nActivation, 52\nBatch Normalization, 46\nBidirectional, 153\nConv2D, 42\nConv2DTranspose, 66, 103\nConv3D, 326\nDense, 32\nDropout, 49\nEmbedding, 138\nFlatten, 32\nGRU, 132\nInput, 32\nLeakyReLU, 52\nLSTM, 140\nMultiHeadAttention, 241\nUpSampling2D, 103\nKeras NLP module, 306\nIndex | 421\nkernels, 41\nkey vectors, 239\nKingma, Diederik, 59\nKullback–Leibler (KL) divergence, 80\nL\nlabel smoothing, 108\nLAION-5B dataset, 402\nLaMDA, 395\nLangevin dynamics, 191, 194-196\nlanguage modeling, 236\nLarge Language Model Meta AI (LLaMA), 398\nlarge language models (LLMs), 396-400\nLarge-scale Scene Understanding (LSUN) data‐\nset, 276\nlatent diffusion, 380, 396\nlatent space, 63, 70, 85-93\nlayer normalization, 245\nlayers, 25, 79 (see also Keras layers)\nlazy regularization, 284\nLeakyReLU, 33\nlearning rate, 36\nlikelihood, 17\nlinear diffusion schedule, 211\nLipschitz constraint, 115, 116\nLLaMA (Large Language Model Meta AI), 398\nLLMs (large language models), 396-400\nlogarithm of the variance, 77\nlong short-term memory networks (see LSTM\nnetworks)\nloss functions, 35, 68, 80, 195\nlower triangle matrix, 178\nLSTM (long short-term memory) networks\nembedding layer, 138\ngenerating datasets, 137\ngenerating new text, 146\nhistory of, 131, 394\nLSTM architecture, 138\nLSTM cell, 142-144\nLSTM layer, 140-142\npublished paper on, 132\ntokenizing the text, 134\nLSUN (Large-scale Scene Understanding) data‐\nset, 276\nM\nmachine learning\nbenefits of, 13\ndata for, 24-25dropout principle, 49\ngenerative modeling and, 4-7\nlibraries for, 27\nmajor branches of, 23, 28, 332\nresources, xxii\nMake-A-Video, 406\nmapping network f, 278\nmasked convolutional layers, 154-156\nmasking, causal, 242-244\nmatrix determinants, 172\nmaximum likelihood estimation, 18\nMDN (mixture density network), 337, 346\nmean squared error loss, 35\nMegatron-Turing NLG, 395, 398\nmetrics parameter, 36\nMIDI files, 300\nMidjourney, 60, 96, 402, 405\nMildenhall, Ben, 219\nminibatch standard deviation layer, 275\nMirza, Mehdi, 122\nmixture distributions, 162-164\nMLP (see multilayer perceptrons)\nMNIST dataset, 192\nmode collapse, 111\nmodel.summary() method, 34, 45\nmodels (see also generative deep learning;\nKeras)\ncore probability theory, 15-18\ndeep neural networks, 25-28\ngenerative model taxonomy, 18\ngenerative modeling, 4\ngenerative versus discriminative modeling,\n5\nhistory of, 395\nimproving, 40-54\nparametric modeling, 16\nprobabilistic versus deterministic, 4\nvariational autoencoders (V AEs), 59\nWorld Model architecture, 336-356\nmodulation step, 283\nMordatch\nIgor, 191\nmultihead attention, 241\nmultilayer perceptrons (MLPs)\ndata preparation, 28\nexample of, 25\nmodel building, 30-35\nsupervised learning and, 28\nmultilayer RNNs, 149\n422 | Index\nmultimodal models\nchallenges of text-to-image generation, 360\nDALL.E 2, 361-376\nFlamingo, 381-389\nhistory of, 396\nImagen, 377-380\nStable Diffusion, 380\nmultivariate normal distribution, 75\nMUSE, 396, 405\nMuseGAN, 317-329\nanalysis of, 327\ndataset used, 317\nMuseGAN critic, 326\nMuseGAN generator, 320\n“MuseGAN: Multi-Track Sequential Generative\nAdversarial Networks for Symbolic Music\nGeneration and Accompaniment” (Dong),\n317\nMuseNet, 394\nMuseScore, 300\nmusic generation\nanalysis of music generation Transformer,\n309-312\ndataset used, 300\ngenerating polyphonic music, 313\nimporting MIDI files, 300\ninputs and outputs, 307\nMuseGAN, 317-329\nmusic versus text generation, 298\nprerequisites to, 300\nsine position encoding, 305-306\ntokenization, 303\ntraining set for, 304\nTransformers applied to, 394\nMusic Transformer, 394\n“Music Transformer: Generating Music with\nLong-Term Structure” (Huang), 313\nmusic21 library, 300\nMusicLM, 407\nN\nNain, Aakash Kumar, 113\nnatural language processing (NLP), 255\nNCSN (Noise Conditional Score Network), 395\n“NeRF: Representing Scenes as Neural Radi‐\nance Fields for View Synthesis” (Milden‐\nhall), 219\n“Neural Discrete Representation Learning”\n(van den Oord), 289neural networks (see also convolutional neural\nnetworks; deep learning)\ndeep neural networks, 54\ndefined, 25-27\nloss functions and, 18\nrole in deep learning, 20\nusing Keras to build, 27, 30-35\nNichol, Alex, 395\nNLP (natural language processing), 255\nNoise Conditional Score Network (NCSN), 395\nnoise, adding to labels, 108\nnontrainable parameters, 48\nnormal distribution (Gaussian distribution), 75\nnormalizing flow models\nchange of variables equation, 173\nchange of variables technique, 170-172\ndescription of, 167\nFFJORD (Free-Form Continuous Dynamics\nfor Scalable Reversible Generative Mod‐\nels), 187\ngenerative model taxonomy, 19\nGLOW, 186\nJacobian determinant, 172\nkey concepts behind, 168\nmotivation of, 169\nRealNVP model, 174-185\nO\nobservations, 4\nOPT, 395, 399\noptimizers, 35\nOsindero, Simon, 122\noverfitting, 49\nOxford 102 Flower dataset, 208\nP\npadding, 43\nPaLM-E, 406\nPapers with Code, xxii\nparameters, trainable and nontrainable, 48\nparametric modeling, 16\nParti, 396, 405\nPatchGAN, 291\npath length regularization, 283\nperceptual loss term, 292\npersonal assistants, 407\npiano roll grid, 314\npix2pix, 291, 394\nPixelCNN\nIndex | 423\nanalysis of, 159-162\nhistory of, 153, 394\nmasked convolutional layers, 154-156\nmixture distributions, 162\nresidual blocks, 156-158\ntraining, 158\nPixelRNN, 394\npixelwise normalization, 276\npoetry, 397\nPoint-E, 407\npositional embedding, 248\npositional encoding, 248-250\nposterior collapse, 289\nprediction, using batch normalization, 48\nprobability density function, 16, 18, 75\nprobability distributions, 162\nprobability theory, 15-18\nProGAN\nconcept of progressive training, 269-276\ndescription of, 269\nhistory of, 394\noutputs, 276\nprogressive training, 269-276\nprompt engineering, 405\nprompts, 396\nQ\nquery, 239\nquestions and comments, xxv\nR\nRadford, Alec, 97, 236\nrandom (stochastic) elements, 4\nrandom noise, 108, 195\nRBM (restricted Boltzmann machine), 203\nRealNVP\nanalysis of, 184\ncoupling layers, 175-177\ndataset used, 174\ndescription of, 174\nhistory of, 394\npassing data through coupling layers,\n177-180\nstacking coupling layers, 180\ntraining, 181-183\nRecipes dataset, 132\nrecurrent neural networks (see RNNs)\nregularization techniques, 49\nreinforcement learning (RL)ChatGPT and, 262\ndefined, 332\nkey terminology, 332\nprocess of, 333-335\nReinforcement Learning from Human Feed‐\nback (RLHF), 262\nReLU (rectified linear unit), 33\nreparameterization trick, 79, 210\nrepresentation learning, 13-14\nresidual blocks, 156-158, 221-222\nrestricted Boltzmann machine (RBM), 203\nreverse diffusion process, 214-216\nreward modeling, 262\nreward, in reinforcement learning, 333\nRLHF (Reinforcement Learning from Human\nFeedback), 262\nRMSE (root mean squared error), 68\nRMSProp (Root Mean Squared Propagation)\noptimizer, 36\nRNNs (recurrent neural networks)\nbidirectional cells, 153\ngated recurrent units (GRUs), 151\nhistory of, 131\nLSTM (long short-term memory) networks,\n131-149\nMDN-RNN World Model architecture, 337\nmultilayer, 149\nstacked recurrent networks, 149\nroot mean squared error (RMSE), 68\nRoot Mean Squared Propagation (RMSProp)\noptimizer, 36\nS\nSAGAN (self-attention GAN), 286, 395\nsample space, 16\nscaling streams, 177\nSchmidhuber, Jurgen, 132, 331, 394\nscore matching technique, 203\nscore-based generative models, 206\nSelf-Attention GAN (SAGAN), 286, 395\nself-referential layers, 258\nSequential models (Keras), 30-35\nsigmoid activation, 33\nsine position embedding, 305\nsinusoidal embedding, 219\nskip connections, 156, 217, 245, 284\nsoftmax activation, 33\nSparse Transformers, 299\nstabilization phase, 272\n424 | Index\nStable Diffusion\nadvantages of, 402-405\narchitecture, 380\nexamples generated by, 381\nhistory of, 380, 396\nstacked recurrent networks, 149\nstandard deviation, 75\nstandard normal curves, 75\nstemming, 135\nstochastic (random) elements, 4\nstochastic gradient Langevin dynamics, 195\nstochastic variation, 280\nstrides parameter (Keras), 43\nstructured data, 24\nstyle mixing, 279\nStyleGAN, 277-281, 395\nStyleGAN-XL, 286, 395\nStyleGAN2, 281-286, 395\nsubclassing layers, 79\nsummary method, 35\nsupervised fine-tuning, 262\nsupervised learning, 28, 332\nswish activation, 193\nsynthesis network, 279\nT\nT5, 256-259, 394\ntape.gradient() method, 83\ntaxonomy, 18\ntemperature parameter, 146\ntemporal networks, 320\nTensorFlow, 27\ntext data generation (see also GPT)\nLSTM (long short-term memory) networks,\n131-149\nRNN (recurrent neural network) exten‐\nsions, 149-153\nshort story generation example, 130\ntext versus image data, 133\ntext versus music generation, 298\ntext-to-3D models, 407\ntext-to-code models, 400-400\ntext-to-image models, 360, 402-405\ntext-to-music models, 407\ntext-to-video models, 406\ntext-to-X multimodal models, 405\nthermodynamic diffusion, 206\ntimeline of AI, 393\ntimestep, in reinforcement learning, 333token embedding, 248\ntokenization\nevent-based, 315\ngrid, 313-315\nof notes for music generation, 303\nprocess of, 134-137\nToolformer, 411\ntractable models, 19\ntrainable parameters, 48\ntraining data, 4\ntraining process, 26\nTransformer block, 245-248\nTransformers (see also GPT; music generation)\narchitectures for, 255\nBERT (Bidirectional Encoder Representa‐\ntions from Transformers), 255\nChatGPT, 260-264\ndecoder versus encoder, 244\ndescription of, 236\nGPT-3 and GPT-4, 259\nhistory of, 394\nSparse Transformers, 299\nT5, 256-259\ntransition phase, 272\ntranslation streams, 177\ntruncated normal distribution, 288\ntruncation trick, 288\ntruth, filtering from generated fiction, 99, 264,\n399, 410\ntwo moons dataset, 174\nU\nU-Net denoising model, 217-224, 370\nuninformative loss, 112\nunit normal curves, 75\nunits, 25, 140\nunstructured data, 24\nunsupervised learning, 332\n“Unsupervised Representation Learning with\nDeep Convolutional Generative Adversarial\nNetwork” (Radford), 97\nupsampling, 103, 372\nV\nV AE (see variational autoencoders)\nV AE with a GAN discriminator (V AE-GAN),\n394\nvalue vectors, 241\nvan den Oord, Aaron, 153, 289\nIndex | 425\nvanishing gradient problem, 132\nvariance, 75\nvariational autoencoders (V AEs) (see also\nautoencoders)\nanalysis of, 84\nautoencoder architecture, 61-74\ndecoders, 77\nencoder adjustments, 75-78\nfacial image generation using, 85-93\ngenerative model taxonomy, 20\nhistory of, 394\nintroduction to, 60, 74\npublished paper on, 59\ntraining, 82\nV AE build in Keras, 78\nV AE loss function, 80\nV AE model summary, 80\nWorld Model architecture, 336\nWorld Model training, 340-344\nVaswani, Ashish, 219, 236, 394\nVector Quantized Generative Adversarial Net‐\nwork (VQ-GAN), 289-292, 395\nVector Quantized V AE (VQ-V AE), 394\n“Vector-quantized Image Modeling with\nImproved VQGAN” (Yu), 292\nVision Transformer (ViT), 292, 364, 395\nVisual ChatGPT, 264\nvocabulary, 135\nVQ-GAN ( Vector Quantized Generative\nAdversarial Network), 289-292, 395\nVQ-V AE (Vector Quantized V AE), 394\nW\nWasserstein GAN with Gradient Penalty\n(WGAN-GP)\nanalysis of, 121gradient penalty loss, 117\nLipschitz constraint, 115\nversus standard GANs, 121\ntraining, 119\ntutorial on, 113\nWasserstein loss, 114\nweight clipping, 116\nWasserstein GANs (WGANs)\nbenefits of, 113\nhistory of, 394\nweight clipping, 116\nweight modulation and demodulation, 282-283\nweights, 25\nWelling, Max, 59\nWGAN (see Wasserstein GANs)\nWine Reviews dataset, 237\nworkplace applications, 409\nWorld Models\narchitecture, 336-338\ncollecting MDN-RNN training data, 346\ncollecting random rollout data, 339\npublished paper on, 331, 337, 394\ntraining in-dream, 353-356\ntraining process, 338\ntraining the controller, 348-353\ntraining the MDN-RNN, 346-348\ntraining the V AE, 340-344\nWorld Model architecture, 336-356\nY\nYu, Jiahui, 292\nZ\nzero-shot prediction, 364\n426 | Index",23237
157-About the Author.pdf,157-About the Author,,0
158-Colophon.pdf,158-Colophon,"About the Author\nDavid Foster  is a data scientist, entrepreneur, and educator specializing in AI appli‐\ncations within creative domains. As cofounder of Applied Data Science Partners\n(ADSP), he inspires and empowers organizations to harness the transformative\npower of data and AI. He holds an MA in Mathematics from Trinity College, Cam‐\nbridge, an MSc in Operational Research from the University of Warwick, and is a fac‐\nulty member of the Machine Learning Institute, with a focus on the practical\napplications of AI and real-world problem solving. His research interests include\nenhancing the transparency and interpretability of AI algorithms, and he has pub‐\nlished literature on explainable machine learning within healthcare.\nColophon\nThe animal on the cover of Generative Deep Learning  is a painted parakeet ( Pyrrhura\npicta ). The Pyrrhura  genus falls under the family Psittacidae , one of three families of\nparrots. Within its subfamily Arinae  are several macaw and parakeet species of the\nWestern Hemisphere. The painted parakeet inhabits the coastal forests and moun‐\ntains of northeastern South America.\nBright green feathers cover most of a painted parakeet, but they are blue above the\nbeak, brown in the face, and reddish in the breast and tail. Most strikingly, the feath‐\ners on the painted parakeet’s neck look like scales; the brown center is outlined in off-\nwhite. This combination of colors camouflages the birds in the rainforest.\nPainted parakeets tend to feed in the forest canopy, where their green plumage masks\nthem best. They forage in flocks of 5 to 12 birds for a wide variety of fruits, seeds, and\nflowers. Occasionally, when feeding below the canopy, painted parakeets will eat algae\nfrom forest pools. They grow to about 9 inches in length and live for 13 to 15 years. A\nclutch of painted parakeet chicks—each of which are less than an inch wide at hatch‐\ning—is usually around five eggs.\nMany of the animals on O’Reilly’s covers are endangered; all of them are important to\nthe world.\nThe cover illustration is by Karen Montgomery, based on a black and white engraving\nfrom Shaw’s Zoology . The cover fonts are Gilroy Semibold and Guardian Sans. The\ntext font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\ncode font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant Answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",2612
