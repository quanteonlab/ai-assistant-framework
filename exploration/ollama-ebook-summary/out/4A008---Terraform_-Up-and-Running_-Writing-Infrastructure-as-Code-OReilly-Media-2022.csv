filename,title,text,len
01-Cover.pdf,01-Cover,"Yevgeniy BrikmanTerraform \nUp & Running\nWriting Infrastructure as Code\nThird\n Edition\nBrikman\nINFRASTRUCTURE AS CODE“This book teaches you \neverything you need to \nknow about Terraform \nto massively improve \ninfrastructure provisioning \nefficiency and enjoyability \nacross any platform.”\n—Mitchell Hashimoto\nCreator of Terraform and  \nCofounder of HashiCorp\n“If you are a DevOps \npractitioner and want \n  to get started with  \ninfrastructure as code, \nthis book is the perfect \nresource for you.”\n—Akash Mahajan\nCofounder and Director, AppseccoTerraform: Up and Running\nUS $59.99  CAN $74.99\nISBN: 978-1-098-11674-3Twitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia Terraform has become a key player in the DevOps world for \ndefining, launching, and managing infrastructure as code \n(IaC) across a variety of cloud and virtualization platforms, \nincluding AWS, Google Cloud, Azure, and more. This hands-on \nthird edition, expanded and thoroughly updated for version \n1.0 and beyond, shows you the fastest way to get up and \nrunning with Terraform.\nGruntwork cofounder Yevgeniy (Jim) Brikman takes you \nthrough code examples that demonstrate Terraform’s \nsimple, declarative programming language for deploying \nand managing infrastructure with a few commands. Veteran \nsysadmins, DevOps engineers, and novice developers will \nquickly go from Terraform basics to running a full stack that \ncan support a massive amount of traffic and a large team of \ndevelopers.\n• Compare Terraform with Chef, Puppet, Ansible, \nCloudFormation, and Pulumi\n• Deploy servers, load balancers, and databases\n• Create reusable infrastructure with Terraform modules\n• Test your Terraform modules with static analysis, unit \ntests, and integration tests\n• Configure CI/CD pipelines for both your apps and \ninfrastructure code\n• Use advanced Terraform syntax for loops, conditionals, \nand zero-downtime deployment\n• Get up to speed on Terraform 0.13 to 1.0 and beyond\n• Work with multiple clouds and providers (including \nKubernetes!)Yevgeniy (Jim) Brikman is the \ncofounder of Gruntwork, a company \nwith the mission of making it 10 times \neasier to build software. The author of \nHello, Startup: A Programmer’s Guide \nto Building Products, Technologies, and \nTeams  (O’Reilly), he previously served \nas a software engineer at LinkedIn, \nTripAdvisor, Cisco, and Thomson \nFinancial. For more information, check \nout ybrikman.com.\nYevgeniy BrikmanTerraform: Up & Running\nWriting Infrastructure as CodeTHIRD EDITION\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing",2664
02-Copyright.pdf,02-Copyright,"978-1-098-11674-3\n[LSI]Terraform: Up & Running\nby Y evgeniy Brikman\nCopyright © 2022 Y evgeniy Brikman. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  John Devins\nDevelopmental Editor:  Corbin Collins\nProduction Editor:  Kate Galloway\nCopyeditor:  Piper Editorial Consulting, LLC\nProofreader:  Kim CoferIndexer:  nSight, Inc.\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nMarch 2017:  First Edition\nSeptember 2019:  Second Edition\nSeptember 2022:  Third Edition\nRevision History for the Third Edition\n2022-09-19: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098116743  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Terraform: Up & Running , the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the author, and do not represent the publisher’s views.\nWhile the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTo Mom, Dad, Lyalya, and Molly",2076
03-Table of Contents.pdf,03-Table of Contents,"Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\n1.Why Terraform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhat Is DevOps?                                                                                                                1\nWhat Is Infrastructure as Code?                                                                                      3\nAd Hoc Scripts                                                                                                                4\nConfiguration Management Tools                                                                               5\nServer Templating Tools                                                                                                7\nOrchestration Tools                                                                                                     12\nProvisioning Tools                                                                                                       14\nWhat Are the Benefits of Infrastructure as Code?                                                      16\nHow Does Terraform Work?                                                                                          17\nHow Does Terraform Compare to Other IaC Tools?                                                  20\nConfiguration Management Versus Provisioning                                                   21\nMutable Infrastructure Versus Immutable Infrastructure                                     21\nProcedural Language Versus Declarative Language                                                22\nGeneral-Purpose Language Versus Domain-Specific Language                           25\nMaster Versus Masterless                                                                                            26\nAgent Versus Agentless                                                                                               28\nPaid Versus Free Offering                                                                                           30\nLarge Community Versus Small Community                                                          31\nMature Versus Cutting Edge                                                                                       33\nUse of Multiple Tools Together                                                                                  34\nConclusion                                                                                                                        36\n2.Getting Started with Terraform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39\nSetting Up Y our AWS Account                                                                                      40\nInstalling Terraform                                                                                                        43\nv\nDeploying a Single Server                                                                                               44\nDeploying a Single Web Server                                                                                      52\nDeploying a Configurable Web Server                                                                         60\nDeploying a Cluster of Web Servers                                                                              66\nDeploying a Load Balancer                                                                                             70\nCleanup                                                                                                                             79\nConclusion                                                                                                                        80\n3.How to Manage Terraform State. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\nWhat Is Terraform State?                                                                                                82\nShared Storage for State Files                                                                                         83\nLimitations with Terraform’s Backends                                                                        91\nState File Isolation                                                                                                            93\nIsolation via Workspaces                                                                                             94\nIsolation via File Layout                                                                                            100\nThe terraform_remote_state Data Source                                                                  105\nConclusion                                                                                                                      113\n4.How to Create Reusable Infrastructure with Terraform Modules. . . . . . . . . . . . . . . . . .  115\nModule Basics                                                                                                                 118\nModule Inputs                                                                                                                121\nModule Locals                                                                                                                125\nModule Outputs                                                                                                             127\nModule Gotchas                                                                                                             129\nFile Paths                                                                                                                     129\nInline Blocks                                                                                                               130\nModule Versioning                                                                                                        133\nConclusion                                                                                                                      139\n5.Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas. . . . . . . .  141\nLoops                                                                                                                               142\nLoops with the count Parameter                                                                              142\nLoops with for_each Expressions                                                                            149\nLoops with for Expressions                                                                                       156\nLoops with the for String Directive                                                                         158\nConditionals                                                                                                                   160\nConditionals with the count Parameter                                                                  160\nConditionals with for_each and for Expressions                                                  165\nConditionals with the if String Directive                                                                167\nZero-Downtime Deployment                                                                                      169\nTerraform Gotchas                                                                                                        179\ncount and for_each Have Limitations                                                                     179\nvi | Table of Contents\nZero-Downtime Deployment Has Limitations                                                      181\nValid Plans Can Fail                                                                                                   184\nRefactoring Can Be Tricky                                                                                        186\nConclusion                                                                                                                      189\n6.Managing Secrets with Terraform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\nSecret Management Basics                                                                                           192\nSecret Management Tools                                                                                             193\nThe Types of Secrets Y ou Store                                                                                 193\nThe Way Y ou Store Secrets                                                                                        194\nThe Interface Y ou Use to Access Secrets                                                                 195\nA Comparison of Secret Management Tools                                                          195\nSecret Management Tools with Terraform                                                                 196\nProviders                                                                                                                     196\nResources and Data Sources                                                                                     206\nState Files and Plan Files                                                                                           217\nConclusion                                                                                                                      219\n7.Working with Multiple Providers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\nWorking with One Provider                                                                                         221\nWhat Is a Provider?                                                                                                    222\nHow Do Y ou Install Providers?                                                                                223\nHow Do Y ou Use Providers?                                                                                    225\nWorking with Multiple Copies of the Same Provider                                              226\nWorking with Multiple AWS Regions                                                                     226\nWorking with Multiple AWS Accounts                                                                   238\nCreating Modules That Can Work with Multiple Providers                               245\nWorking with Multiple Different Providers                                                               248\nA Crash Course on Docker                                                                                       249\nA Crash Course on Kubernetes                                                                                252\nDeploying Docker Containers in AWS Using Elastic Kubernetes Service         264\nConclusion                                                                                                                      272\n8.Production-Grade Terraform Code. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  275\nWhy It Takes So Long to Build Production-Grade Infrastructure                         277\nThe Production-Grade Infrastructure Checklist                                                      279\nProduction-Grade Infrastructure Modules                                                                280\nSmall Modules                                                                                                            281\nComposable Modules                                                                                                285\nTestable Modules                                                                                                        291\nVersioned Modules                                                                                                    298\nBeyond Terraform Modules                                                                                     305\nTable of Contents | vii\nConclusion                                                                                                                      313\n9.How to Test Terraform Code. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  315\nManual Tests                                                                                                                   316\nManual Testing Basics                                                                                               317\nCleaning Up After Tests                                                                                            319\nAutomated Tests                                                                                                             320\nUnit Tests                                                                                                                     321\nIntegration Tests                                                                                                         348\nEnd-to-End Tests                                                                                                        362\nOther Testing Approaches                                                                                        364\nConclusion                                                                                                                      372\n10. How to Use Terraform as a Team. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  375\nAdopting IaC in Y our Team                                                                                         376\nConvince Y our Boss                                                                                                   376\nWork Incrementally                                                                                                   379\nGive Y our Team the Time to Learn                                                                          380\nA Workflow for Deploying Application Code                                                           382\nUse Version Control                                                                                                   382\nRun the Code Locally                                                                                                 383\nMake Code Changes                                                                                                  383\nSubmit Changes for Review                                                                                      384\nRun Automated Tests                                                                                                 385\nMerge and Release                                                                                                      386\nDeploy                                                                                                                          387\nA Workflow for Deploying Infrastructure Code                                                       390\nUse Version Control                                                                                                   391\nRun the Code Locally                                                                                                 395\nMake Code Changes                                                                                                  396\nSubmit Changes for Review                                                                                      397\nRun Automated Tests                                                                                                 399\nMerge and Release                                                                                                      400\nDeploy                                                                                                                          401\nPutting It All Together                                                                                                  412\nConclusion                                                                                                                      414\nAppendix. Recommended Reading. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  417\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  421\nviii | Table of Contents",16849
04-What You Will Find in This Book.pdf,04-What You Will Find in This Book,"Preface\nA long time ago, in a datacenter far, far away, an ancient group of powerful beings\nknown as “sysadmins” used to deploy infrastructure manually. Every server, every\ndatabase, every load balancer, and every bit of network configuration was created\nand managed by hand. It was a dark and fearful age: fear of downtime, fear of\naccidental misconfiguration, fear of slow and fragile deployments, and fear of what\nwould happen if the sysadmins fell to the dark side (i.e., took a vacation). The good\nnews is that thanks to the DevOps movement, there is now a better way to do things:\nTerraform .\nTerraform  is an open source tool created by HashiCorp that allows you to define\nyour infrastructure as code using a simple, declarative language and to deploy and\nmanage that infrastructure across a variety of public cloud providers (e.g., Amazon\nWeb Services [AWS], Microsoft Azure, Google Cloud Platform, DigitalOcean) and\nprivate cloud and virtualization platforms (e.g., OpenStack, VMware) using a few\ncommands. For example, instead of manually clicking around a web page or running\ndozens of commands, here is all the code it takes to configure a server on AWS:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nAnd to deploy it, you just run the following:\n$ terraform init\n$ terraform apply\nix\nThanks to its simplicity and power, Terraform has emerged as a key player in the\nDevOps world. It allows you to replace the tedious, fragile, and manual parts of infra‐\nstructure management with a solid, automated foundation upon which you can build\nall your other DevOps practices (e.g., automated testing, Continuous Integration,\nContinuous Delivery) and tooling (e.g., Docker, Chef, Puppet).\nThis book is the fastest way to get up and running with Terraform.\nY ou’ll go from deploying the most basic “Hello, World” Terraform example (in fact,\nyou just saw it!) all the way up to running a full tech stack (virtual servers, Kubernetes\nclusters, Docker containers, load balancers, databases) capable of supporting a large\namount of traffic and a large team of developers—all in the span of just a few chap‐\nters. This is a hands-on tutorial that not only teaches you DevOps and infrastructure\nas code (IaC) principles but also walks you through dozens of code examples that you\ncan try at home, so make sure you have your computer handy.\nBy the time you’re done, you’ll be ready to use Terraform in the real world.\nWho Should Read This Book\nThis book is for anyone responsible for the code after it has been written. That\nincludes sysadmins, operations engineers, release engineers, site reliability engineers,\nDevOps engineers, infrastructure developers, full-stack developers, engineering man‐\nagers, and CTOs. No matter what your title is, if you’re the one managing infra‐\nstructure, deploying code, configuring servers, scaling clusters, backing up data,\nmonitoring apps, and responding to alerts at 3 a.m., this book is for you.\nCollectively, all of these tasks are usually referred to as operations . In the past, it was\ncommon to find developers who knew how to write code but did not understand\noperations; likewise, it was common to find sysadmins who understood operations\nbut did not know how to write code. Y ou could get away with that divide in the past,\nbut in the modern world, as cloud computing and the DevOps movement become\nubiquitous, just about every developer will need to learn operational skills, and every\nsysadmin will need to learn coding skills.\nThis book does not assume that you’re already an expert coder or expert sysadmin—a\nbasic familiarity with programming, the command line, and server-based software\n(e.g., websites) should suffice. Everything else you need you’ll be able to pick up as\nyou go, so that by the end of the book, you will have a solid grasp of one of the most\ncritical aspects of modern development and operations: managing infrastructure as\ncode.\nIn fact, you’ll learn not only how to manage infrastructure as code using Terraform\nbut also how this fits into the overall DevOps world. Here are some of the questions\nyou’ll be able to answer by the end of the book:\nx | Preface\n•Why use IaC at all?•\n•What are the differences between configuration management, orchestration, pro‐•\nvisioning, and server templating?\n•When should you use Terraform, Chef, Ansible, Puppet, Pulumi, CloudForma‐•\ntion, Docker, Packer, or Kubernetes?\n•How does Terraform work, and how do you use it to manage your infrastructure?•\n•How do you create reusable Terraform modules?•\n•How do you securely manage secrets when working with Terraform?•\n•How do you use Terraform with multiple regions, accounts, and clouds?•\n•How do you write Terraform code that’s reliable enough for production usage?•\n•How do you test your Terraform code?•\n•How do you make Terraform a part of your automated deployment process?•\n•What are the best practices for using Terraform as a team?•\nThe only tools you need are a computer (Terraform runs on most operating systems),\nan internet connection, and the desire to learn.\nWhy I Wrote This Book\nTerraform is a powerful tool. It works with all popular cloud providers. It uses a clean,\nsimple language and has strong support for reuse, testing, and versioning. It’s open\nsource and has a friendly, active community. But there is one area where it’s lacking:\nmaturity.\nTerraform has become wildly popular, but it’s still a relatively new technology, and\ndespite its popularity, it’s still difficult to find books, blog posts, or experts to help you\nbecome proficient with the tool. The official Terraform documentation does a good\njob of introducing the basic syntax and features, but it includes little information\non idiomatic patterns, best practices, testing, reusability, or team workflows. It’s like\ntrying to become fluent in French by studying only the vocabulary but not any of the\ngrammar or idioms.\nThe reason I wrote this book is to help developers become fluent in Terraform. I’ve\nbeen using Terraform for six out of the seven years it has existed, mostly at my\ncompany, Gruntwork , where Terraform is one of the core tools we’ve used to create\na library of more than 300,000 lines of reusable, battle-tested infrastructure code\nthat’s used in production by hundreds of companies. Writing and maintaining this\nmuch infrastructure code over this many years and using it with so many different\ncompanies and use cases has taught us a lot of hard lessons. My goal is to share these\nPreface | xi\nlessons with you so that you can cut this lengthy process down and become fluent in a\nmatter of days.\nOf course, you can’t become fluent just by reading. To become fluent in French, you\nneed to spend time conversing with native French speakers, watching French TV\nshows, and listening to French music. To become fluent in Terraform, you need to\nwrite real Terraform code, use it to manage real software, and deploy that software on\nreal servers. Therefore, be ready to read, write, and execute a lot of code.\nWhat You Will Find in This Book\nHere’s an outline of what the book covers:\nChapter 1, “Why Terraform”\nHow DevOps is transforming the way we run software; an overview of\ninfrastructure-as-code tools, including configuration management, server tem‐\nplating, orchestration, and provisioning tools; the benefits of infrastructure as\ncode; a comparison of Terraform, Chef, Puppet, Ansible, Pulumi, OpenStack\nHeat, and CloudFormation; how to combine tools such as Terraform, Packer,\nDocker, Ansible, and Kubernetes.\nChapter 2, “Getting Started with Terraform”\nInstalling Terraform; an overview of Terraform syntax; an overview of the Terra‐\nform CLI tool; how to deploy a single server; how to deploy a web server; how to\ndeploy a cluster of web servers; how to deploy a load balancer; how to clean up\nresources you’ve created.\nChapter 3, “How to Manage Terraform State”\nWhat Terraform state is; how to store state so that multiple team members\ncan access it; how to lock state files to prevent race conditions; how to isolate\nstate files to limit the damage from errors; how to use Terraform workspaces; a\nbest-practices file and folder layout for Terraform projects; how to use read-only\nstate.\nChapter 4, “How to Create Reusable Infrastructure with Terraform Modules”\nWhat modules are; how to create a basic module; how to make a module config‐\nurable with inputs and outputs; local values; versioned modules; module gotchas;\nusing modules to define reusable, configurable pieces of infrastructure.\nChapter 5, “Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas”\nLoops with the count  parameter, for_each  and for expressions, and the for\nstring directive; conditionals with the count  parameter, for_each  and for\nexpressions, and the if string directive; built-in functions; zero-downtime\ndeployment; common Terraform gotchas and pitfalls, including count  and\nxii | Preface\nfor_each  limitations, zero-downtime deployment gotchas, how valid plans can\nfail, and how to refactor Terraform code safely.\nChapter 6, “Managing Secrets with Terraform”\nAn introduction to secrets management; an overview of the different types of\nsecrets, different ways to store secrets, and different ways to access secrets; a\ncomparison of common secret management tools such as HashiCorp Vault, AWS\nSecrets Manager, and Azure Key Vault; how to manage secrets when working\nwith providers, including authentication via environment variables, IAM roles,\nand OIDC; how to manage secrets when working with resources and data sour‐\nces, including how to use environment variables, encrypted files, and centralized\nsecret stores; how to securely handle state files and plan files.\nChapter 7, “Working with Multiple Providers”\nA closer look at how Terraform providers work, including how to install them,\nhow to control the version, and how to use them in your code; how to use\nmultiple copies of the same provider, including how to deploy to multiple AWS\nregions, how to deploy to multiple AWS accounts, and how to build reusable\nmodules that can use multiple providers; how to use multiple different providers\ntogether, including an example of using Terraform to run a Kubernetes cluster\n(EKS) in AWS and deploy Dockerized apps into the cluster.\nChapter 8, “Production-Grade Terraform Code”\nWhy DevOps projects always take longer than you expect; the production-grade\ninfrastructure checklist; how to build Terraform modules for production; small\nmodules; composable modules; testable modules; releasable modules; Terraform\nRegistry; variable validation; versioning Terraform, Terraform providers, Terra‐\nform modules, and Terragrunt; Terraform escape hatches.\nChapter 9, “How to Test Terraform Code”\nManual tests for Terraform code; sandbox environments and cleanup; automated\ntests for Terraform code; Terratest; unit tests; integration tests; end-to-end tests;\ndependency injection; running tests in parallel; test stages; retries; the test pyra‐\nmid; static analysis; plan testing; server testing.\nChapter 10, “How to Use Terraform as a Team”\nHow to adopt Terraform as a team; how to convince your boss; a workflow\nfor deploying application code; a workflow for deploying infrastructure code;\nversion control; the golden rule of Terraform; code reviews; coding guidelines;\nTerraform style; CI/CD for Terraform; the deployment process.\nFeel free to read the book from beginning to end or jump around to the chapters\nthat interest you the most. Note that the examples in each chapter reference and build\nupon the examples from the previous chapters, so if you skip around, use the open\nsource code examples (as described in “Open Source Code Examples”  on page xix)\nPreface | xiii",11931
05-Changes from the Second Edition to the Third Edition.pdf,05-Changes from the Second Edition to the Third Edition,"to get your bearings. At the end of the book, in the Appendix , you’ll find a list of\nrecommended reading where you can learn more about Terraform, operations, IaC,\nand DevOps.\nChanges from the Second Edition to the Third Edition\nThe first edition of this book came out in 2017, the second edition came out in 2019,\nand although it’s hard for me to believe it, I’m now working on the third edition in\n2022. Time flies. It’s remarkable how much has changed over the years!\nIf you’ve read the second edition of the book and want to know what’s new, or if\nyou’re just curious to see how Terraform has evolved between 2019 and 2022, here are\nsome of the highlights of what changed between the second and third editions:\nHundreds of pages of updated content\nThe third edition of the book is about a hundred pages longer than the second\nedition. I also estimate that roughly one-third to one-half of the pages originally\nin the second edition were updated as well. Why so much churn? Well, Terraform\nwent through six major releases since the second edition came out: 0.13, 0.14,\n0.15, 1.0, 1.1, and 1.2. Moreover, many Terraform providers went through major\nupgrades of their own, including the AWS Provider, which was at version 2\nwhen the second edition came out and is now at version 4. Plus, the Terraform\ncommunity has seen massive growth over the last few years, which has led to the\nemergence of many new best practices, tools, and modules. I’ve tried to capture\nas much of this change as I could in the third edition, adding two completely new\nchapters and making major updates to all the existing chapters, as described next.\nNew provider functionality\nTerraform has significantly improved how you work with providers. In the third\nedition, I’ve added an entirely new chapter, Chapter 7 , that describes how to\nwork with multiple providers: e.g., how to deploy into multiple regions, multiple\naccounts, and multiple clouds. Also, by popular demand, this chapter includes a\nbrand-new set of examples showing how to use Terraform, Kubernetes, Docker,\nAWS, and EKS to run containerized apps. Finally, I’ve also updated all the\nother chapters to highlight new provider features from the last several releases,\nincluding the required_providers  block introduced in Terraform 0.13, the lock\nfile introduced in Terraform 0.14, and the configuration_aliases  parameter\nintroduced in Terraform 0.15.\nBetter secrets management\nWhen using Terraform code, you often have to deal with many types of secrets:\ndatabase passwords, API keys, cloud provider credentials, TLS certificates, and so\non. In the third edition, I added an entirely new chapter, Chapter 6 , dedicated\nto this topic, including a comparison of common secret management tools, as\nxiv | Preface\nwell as lots of new example code that shows a variety of techniques for securely\nusing secrets with Terraform, including environment variables, encrypted files,\ncentralized secret stores, IAM roles, OIDC, and more.\nNew module functionality\nTerraform 0.13 added the ability to use count , for_each , and depends_on  on\nmodule  blocks, making modules considerably more powerful, flexible, and reusa‐\nble. Y ou can find examples of how to use these new features in Chapters 5 and 7.\nNew validation functionality\nIn Chapter 8 , I’ve added examples of how to use the validation  feature intro‐\nduced in Terraform 0.13 to perform basic checks on variables (such as enforcing\nminimum or maximum values) and the precondition  and postcondition  fea‐\ntures introduced in Terraform 1.2 to perform basic checks on resources and data\nsources, either before running apply  (such as enforcing that the AMI a user\nselected uses the x86_64 architecture) or after running apply  (such as checking\nthat the EBS volume you’re using was successfully encrypted). In Chapter 6 , I\nshow how to use the sensitive  parameter introduced in Terraform 0.14 and\n0.15, which ensures that secrets won’t be logged when you run plan  or apply .\nNew refactoring functionality\nTerraform 1.1 introduced the moved  block, which provides a much better way to\nhandle certain types of refactoring, such as renaming a resource. In the past, this\ntype of refactoring required users to manually run error-prone terraform state\nmv operations, whereas now, as you’ll see in a new example in Chapter 5 , this\nprocess can be fully automated, making upgrades safer and more compatible.\nMore testing options\nThe tools available for automated testing of Terraform code continue to improve.\nIn Chapter 9 , I’ve added example code and comparisons of static analysis tools\nfor Terraform, including tfsec , tflint , terrascan , and the validate  command;\nplan  testing tools for Terraform, including Terratest, OPA, and Sentinel; and\nserver testing tools, including inspec , serverspec , and goss . I also added a\ncomparison of all the testing approaches out there, so you can pick the best ones\nfor your use cases.\nImproved stability\nTerraform 1.0 was a big milestone for Terraform, not only signifying that the\ntool had reached a certain level of maturity but also coming with a number\nof compatibility promises. Namely, there is a promise that all the 1.x releases\nwill be backward compatible, so upgrading between v1.x releases should no\nlonger require changes to your code, workflows, or state files. Terraform state\nfiles are now cross-compatible with Terraform 0.14, 0.15, and all 1.x releases,\nand Terraform remote state data sources are cross-compatible with Terraform\nPreface | xv",5566
06-Changes from the First Edition to the Second Edition.pdf,06-Changes from the First Edition to the Second Edition,"1Per the HashiCorp S1 .\n2Check out the Terraform upgrade guides  for details.0.12.30, 0.13.6, 0.14.0, 0.15.0, and all 1.x releases. I’ve also updated Chapter 8\nwith examples of how to better manage versioning of Terraform (including\nusing tfenv ), Terragrunt (including using tgswitch ), and Terraform providers\n(including how to use the lock file).\nImproved maturity\nTerraform has been downloaded over 100 million times, has had over 1,500 open\nsource contributors, and is in use at ~79% of Fortune 500 companies,1 so it’s\nsafe to say that the ecosystem has grown and matured significantly over the last\nseveral years. There are now more developers, providers, reusable modules, tools,\nplugins, classes, books, and tutorials for Terraform than ever before. Moreover,\nHashiCorp, the company that created Terraform, had its IPO (initial public\noffering) in 2021, so Terraform is no longer backed by a small startup but by a\nlarge, stable, publicly traded company, for which Terraform is its biggest business\nline.\nMany other changes\nThere were many other changes along the way, including the launch of Terraform\nCloud (a web UI for using Terraform); the improved maturity of popular com‐\nmunity tools such as Terragrunt, Terratest, and tfenv; the addition of many new\nprovider features (including new ways to do zero-downtime deployment, such\nas instance refresh, which I’ve added to Chapter 5 ) and new functions (e.g., I\nadded examples of how to use the one function in Chapter 5  and the try function\nin Chapter 7 ); the deprecation of many old features (e.g., template_file  data\nsource, many aws_s3_bucket  parameters, list  and map, support for external\nreferences on destroy  provisioners); and much more.\nChanges from the First Edition to the Second Edition\nGoing back in time even further, the second edition of the book added roughly 150\npages of new content on top of the first edition. Here is a summary of those changes,\nwhich also covers how Terraform changed between 2017 and 2019:\nFour major Terraform releases\nTerraform was at version 0.8 when the first edition came out; between then and\nthe time of the second edition, Terraform had four major releases, all the way up\nto version 0.12. These releases introduced some amazing new functionality, as I’ll\ndescribe shortly, as well as a fair amount of upgrade work for users!2\nxvi | Preface\nAutomated testing improvements\nThe tooling and practices for writing automated tests for Terraform code evolved\nconsiderably between 2017 and 2019. In the second edition, I added Chapter 9 ,\na completely new chapter dedicated to testing, covering topics such as unit tests,\nintegration tests, end-to-end tests, dependency injection, test parallelism, static\nanalysis, and more.\nModule improvements\nThe tooling and practices for creating Terraform modules also evolved consider‐\nably. In the second edition, I added Chapter 8 , a new chapter that contains a\nguide to building reusable, battle-tested, production-grade Terraform modules—\nthe kind of modules you’ d bet your company on.\nWorkflow  improvements\nChapter 10  was completely rewritten in the second edition to reflect the changes\nin how teams integrate Terraform into their workflows, including a detailed\nguide on how to take application code and infrastructure code from development\nthrough testing and all the way to production.\nHCL2\nTerraform 0.12 overhauled the underlying language from HCL to HCL2. This\nincluded support for first-class expressions, rich type constraints, lazily evalu‐\nated conditional expressions, support for null , for_each  and for expressions,\ndynamic inline blocks, and more. All the code examples in the second edition of\nthe book were updated to use HCL2, and the new language features were covered\nextensively in Chapters 5 and 8.\nTerraform state revamp\nTerraform 0.9 introduced backends as a first-class way to store and share Terra‐\nform state, including built-in support for locking. Terraform 0.9 also introduced\nstate environments as a way to manage deployments across multiple environ‐\nments. In Terraform 0.10, state environments were replaced with Terraform\nworkspaces. I cover all of these topics in Chapter 3 .\nTerraform providers split\nIn Terraform 0.10, the core Terraform code was split up from the code for all\nthe providers (i.e., the code for AWS, GCP , Azure, etc.). This allowed providers\nto be developed in their own repositories, at their own cadence, with their\nown versioning. However, you now must run terraform init  to download the\nprovider code every time you start working with a new module, as discussed in\nChapters 2 and 9.\nPreface | xvii\n3Y ou can find the full list of Terraform providers in the Terraform Registry .Massive provider growth\nFrom 2016 to 2019, Terraform grew from a handful of major cloud providers (the\nusual suspects, such as AWS, GCP , and Azure) to more than one hundred official\nproviders and many more community providers.3 This means that you can now\nuse Terraform to not only manage many other types of clouds (e.g., there are\nnow providers for Alicloud, Oracle Cloud Infrastructure, VMware vSphere, and\nothers) but also to manage many other aspects of your world as code, including\nversion control systems with the GitHub, GitLab, and Bitbucket providers; data\nstores with the MySQL, PostgreSQL, and InfluxDB providers; monitoring and\nalerting systems with the Datadog, New Relic, and Grafana providers; platform\ntools with the Kubernetes, Helm, Heroku, Rundeck, and RightScale providers;\nand much more. Moreover, each provider has much better coverage these days:\nAWS now covers the majority of important AWS services and often adds support\nfor new services even before CloudFormation does!\nTerraform Registry\nHashiCorp launched the Terraform Registry  in 2017, a UI that made it easy to\nbrowse and consume open source, reusable Terraform modules contributed by\nthe community. In 2018, HashiCorp added the ability to run a Private Terraform\nRegistry within your own organization. Terraform 0.11 added first-class syntax\nsupport for consuming modules from a Terraform Registry. We look at the\nRegistry in Chapter 8 .\nBetter error handling\nTerraform 0.9 updated state error handling: if there was an error writing state\nto a remote backend, the state would be saved locally in an errored.tfstate  file.\nTerraform 0.12 completely overhauled error handling, by catching errors earlier,\nshowing clearer error messages, and including the filepath, line number, and a\ncode snippet in the error message.\nMany other changes\nThere were many other changes along the way, including the introduction of\nlocal values (see “Module Locals” on page 125), new “escape hatches” for having\nTerraform interact with the outside world via scripts (see “Beyond Terraform\nModules”  on page 305), running plan  as part of the apply  command, fixes\nfor the create_before_destroy  cycle issues, major improvements to the count\nparameter so that it can include references to data sources and resources, dozens\nof new built-in functions, an overhaul in provider  inheritance, and much more.\nxviii | Preface",7186
07-What You Wont Find in This Book.pdf,07-What You Wont Find in This Book,,0
08-OReilly Online Learning.pdf,08-OReilly Online Learning,"What You Won’t Find in This Book\nThis book is not meant to be an exhaustive reference manual for Terraform. I do\nnot cover all of the cloud providers, or all of the resources supported by each cloud\nprovider, or every available Terraform command. For these nitty-gritty details, I refer\nyou instead to the Terraform documentation .\nThe documentation contains many useful answers, but if you’re new to Terraform,\ninfrastructure as code, or operations, you won’t even know what questions to ask.\nTherefore, this book is focused on what the documentation does not cover: namely,\nhow to go beyond introductory examples and use Terraform in a real-world setting.\nMy goal is to get you up and running quickly by discussing why you might want to\nuse Terraform in the first place, how to fit it into your workflow, and what practices\nand patterns tend to work best.\nTo demonstrate these patterns, I’ve included a number of code examples. I’ve tried\nto make it as easy as possible for you to try these examples at home by minimizing\ndependencies on any third parties. This is why almost all the examples use just a\nsingle cloud provider, AWS, so that you need to sign up only for a single third-party\nservice (also, AWS offers a generous free tier, so running the example code shouldn’t\ncost you much). This is why the book and the example code do not cover or require\nHashiCorp’s paid services, Terraform Cloud or Terraform Enterprise. And this is why\nI’ve released all of the code examples as open source.\nOpen Source Code Examples\nY ou can find all of the code samples in the book at the following URL:\nhttps://github.com/brikis98/terraform-up-and-running-code\nY ou might want to check out this repo before you begin reading so you can follow\nalong with all the examples on your own computer:\ngit clone https://github.com/brikis98/terraform-up-and-running-code.git\nThe code examples in that repo are in the code  folder, and they are organized first\nby the tool or language (e.g., Terraform, Packer, OPA) and then by chapter. The one\nexception is the Go code used for automated tests in Chapter 9 , which lives in the\nterraform  folder to follow the examples , modules , and test folder layout recommended\nin that chapter. Table P-1  shows a few examples of where to find different types of\ncode examples in the code samples repo.\nPreface | xix\nTable P-1. Where to find different  types of code examples in the code samples repo\nType of code Chapter Folder to look at in the samples repo\nTerraform Chapter 2 code/terraform/02-intro-to-terraform-syntax\nTerraform Chapter 5 code/terraform/05-tips-and-tricks\nPacker Chapter 1 code/packer/01-why-terraform\nOPA Chapter 9 code/opa/09-testing-terraform-code\nGo Chapter 9 code/terraform/09-testing-terraform-code/test\nIt’s worth noting that most of the examples show you what the code looks like at the\nend of a chapter. If you want to maximize your learning, you’re better off writing the\ncode yourself, from scratch, and checking the “official” solutions only at the very end.\nY ou’ll begin writing code in Chapter 2 , where you’ll learn how to use Terraform to\ndeploy a basic cluster of web servers from scratch. After that, follow the instructions\nin each subsequent chapter on how to develop and improve this web server cluster\nexample. Make the changes as instructed, try to write all the code yourself, and use\nthe sample code in the GitHub repo only as a way to check your work or get yourself\nunstuck.\nA Note About Versions\nAll of the examples in this book were tested against Terraform 1.x and AWS Pro‐\nvider 4.x, which were the most recent major releases as of this writing. Because\nTerraform is a relatively new tool, it is possible that future releases will contain\nbackward-incompatible changes and that some of the best practices will change and\nevolve over time.\nI’ll try to release updates as often as I can, but the Terraform project moves fast, so\nyou’ll need to do some work to keep up with it on your own. For the latest news, blog\nposts, and talks on Terraform and DevOps, be sure to check out this book’s website\nand subscribe to the newsletter !\nUsing the Code Examples\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com .\nThis book is here to help you get your job done. In general, if example code is\noffered with this book, you may use it in your programs and documentation. Y ou\ndo not need to contact us for permission unless you’re reproducing a significant\nportion of the code. For example, writing a program that uses several chunks of code\nfrom this book does not require permission. Selling or distributing examples from\nxx | Preface\nO’Reilly books does require permission. Answering a question by citing this book\nand quoting example code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation does\nrequire permission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “ Terraform: Up and\nRunning , Third Edition by Y evgeniy Brikman (O’Reilly). Copyright 2022 Y evgeniy\nBrikman, 978-1-098-11674-3. ”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact O’Reilly Media at permissions@oreilly.com .\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nPreface | xxi",6072
09-How to Contact OReilly Media.pdf,09-How to Contact OReilly Media,,0
10-Acknowledgments.pdf,10-Acknowledgments,"O’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com .\nHow to Contact O’Reilly Media\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/terraform-UR3 .\nEmail bookquestions@oreilly.com  to comment or ask technical questions about this\nbook.\nFor news and information about our books and courses, visit https://oreilly.com .\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media .\nFollow us on Twitter: https://twitter.com/oreillymedia .\nWatch us on Y ouTube: https://youtube.com/oreillymedia .\nAcknowledgments\nJosh Padnick\nThis book would not have been possible without you. Y ou were the one who\nintroduced me to Terraform in the first place, taught me all the basics, and helped\nme figure out all the advanced parts. Thank you for supporting me while I took\nour collective learnings and turned them into a book. Thank you for being an\nxxii | Preface\nawesome cofounder and making it possible to run a startup while still living a fun\nlife. And thank you most of all for being a good friend and a good person.\nO’Reilly Media\nThank you for publishing another one of my books. Reading and writing have\nprofoundly transformed my life, and I’m proud to have your help in sharing\nsome of my writing with others. A special thanks to Brian Anderson, Virginia\nWilson, and Corbin Collins for all your help on the first, second, and third\neditions, respectively.\nGruntwork employees\nI can’t thank you all enough for (a) joining our tiny startup, (b) building amazing\nsoftware, (c) holding down the fort while I worked on the third edition of this\nbook, and (d) being amazing colleagues and friends.\nGruntwork customers\nThank you for taking a chance on a small, unknown company and volunteering\nto be guinea pigs for our Terraform experiments. Gruntwork’s mission is to make\nit 10 times easier to understand, develop, and deploy software. We haven’t always\nsucceeded at that mission (I’ve captured many of our mistakes in this book!), so\nI’m grateful for your patience and willingness to be part of our audacious attempt\nto improve the world of software.\nHashiCorp\nThank you for building an amazing collection of DevOps tools, including Terra‐\nform, Packer, Consul, and Vault. Y ou’ve improved the world of DevOps and, with\nit, the lives of millions of software developers.\nReviewers\nThank you to Kief Morris, Seth Vargo, Mattias Gees, Ricardo Ferreira, Akash\nMahajan, Moritz Heiber, Taylor Dolezal, and Anton Babenko for reading early\nversions of this book and providing lots of detailed, constructive feedback. Y our\nsuggestions have made this book significantly better.\nReaders of the first and second editions\nThose of you who bought the first and second editions of this book made the\nthird edition possible. Thank you. Y our feedback, questions, pull requests, and\nconstant prodding for updates motivated a whole bunch of new and updated\ncontent. I hope you find the updates useful, and I’m looking forward to the\ncontinued prodding.\nMom, Dad, Larisa, Molly\nI accidentally wrote another book. That probably means I didn’t spend as much\ntime with you as I wanted. Thank you for putting up with me anyway. I love you.\nPreface | xxiii",4076
11-Chapter 1. Why Terraform.pdf,11-Chapter 1. Why Terraform,,0
12-What Is DevOps.pdf,12-What Is DevOps,"CHAPTER 1\nWhy Terraform\nSoftware isn’t done when the code is working on your computer. It’s not done when\nthe tests pass. And it’s not done when someone gives you a “ship it” on a code review.\nSoftware isn’t done until you deliver  it to the user.\nSoftware  delivery  consists of all of the work you need to do to make the code available\nto a customer, such as running that code on production servers, making the code\nresilient to outages and traffic spikes, and protecting the code from attackers. Before\nyou dive into the details of Terraform, it’s worth taking a step back to see where\nTerraform fits into the bigger picture of software delivery.\nIn this chapter, you’ll dive into the following topics:\n•What is DevOps?•\n•What is infrastructure as code?•\n•What are the benefits of infrastructure as code?•\n•How does Terraform work?•\n•How does Terraform compare to other infrastructure-as-code tools?•\nWhat Is DevOps?\nIn the not-so-distant past, if you wanted to build a software company, you also\nneeded to manage a lot of hardware. Y ou would set up cabinets and racks, load them\nup with servers, hook up wiring, install cooling, build redundant power systems, and\nso on. It made sense to have one team, typically called Developers (“Devs”), dedicated\nto writing the software, and a separate team, typically called Operations (“Ops”),\ndedicated to managing this hardware.\n1\nThe typical Dev team would build an application and “toss it over the wall” to the\nOps team. It was then up to Ops to figure out how to deploy and run that application.\nMost of this was done manually. In part, that was unavoidable, because much of the\nwork had to do with physically hooking up hardware (e.g., racking servers, hooking\nup network cables). But even the work Ops did in software, such as installing the\napplication and its dependencies, was often done by manually executing commands\non a server.\nThis works well for a while, but as the company grows, you eventually run into\nproblems. It typically plays out like this: because releases are done manually, as the\nnumber of servers increases, releases become slow, painful, and unpredictable. The\nOps team occasionally makes mistakes, so you end up with snowflake  servers , wherein\neach one has a subtly different configuration from all the others (a problem known as\nconfiguration  drift). As a result, the number of bugs increases. Developers shrug and\nsay, “It works on my machine!” Outages and downtime become more frequent.\nThe Ops team, tired from their pagers going off at 3 a.m. after every release, reduce\nthe release cadence to once per week. Then to once per month. Then once every six\nmonths. Weeks before the biannual release, teams begin trying to merge all of their\nprojects together, leading to a huge mess of merge conflicts. No one can stabilize the\nrelease branch. Teams begin blaming one another. Silos form. The company grinds to\na halt.\nNowadays, a profound shift is taking place. Instead of managing their own datacen‐\nters, many companies are moving to the cloud, taking advantage of services such as\nAmazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).\nInstead of investing heavily in hardware, many Ops teams are spending all their\ntime working on software, using tools such as Chef, Puppet, Terraform, Docker,\nand Kubernetes. Instead of racking servers and plugging in network cables, many\nsysadmins are writing code.\nAs a result, both Dev and Ops spend most of their time working on software, and\nthe distinction between the two teams is blurring. It might still make sense to have a\nseparate Dev team responsible for the application code and an Ops team responsible\nfor the operational code, but it’s clear that Dev and Ops need to work more closely\ntogether. This is where the DevOps movement  comes from.\nDevOps isn’t the name of a team or a job title or a particular technology. Instead, it’s\na set of processes, ideas, and techniques. Everyone has a slightly different definition of\nDevOps, but for this book, I’m going to go with the following:\nThe goal of DevOps is to make software delivery vastly more efficient.\nInstead of multiday merge nightmares, you integrate code continuously and always\nkeep it in a deployable state. Instead of deploying code once per month, you can\ndeploy code dozens of times per day, or even after every single commit. And instead\n2 | Chapter 1: Why Terraform",4452
13-Configuration Management Tools.pdf,13-Configuration Management Tools,"1From The DevOps Handbook: How to Create World-Class Agility, Reliability, & Security in Technology Organi‐\nzations  (IT Revolution Press, 2016) by Gene Kim, Jez Humble, Patrick Debois, and John Willis.of constant outages and downtime, you build resilient, self-healing systems and use\nmonitoring and alerting to catch problems that can’t be resolved automatically.\nThe results from companies that have undergone DevOps transformations are\nastounding. For example, Nordstrom found that after applying DevOps practices\nto its organization, it was able to increase the number of features it delivered per\nmonth by 100%, reduce defects by 50%, reduce lead times  (the time from coming\nup with an idea to running code in production) by 60%, and reduce the number of\nproduction incidents by 60% to 90%. After HP’s LaserJet Firmware division began\nusing DevOps practices, the amount of time its developers spent on developing new\nfeatures went from 5% to 40%, and overall development costs were reduced by 40%.\nEtsy used DevOps practices to go from stressful, infrequent deployments that caused\nnumerous outages to deploying 25 to 50 times per day, with far fewer outages.1\nThere are four core values in the DevOps movement: culture, automation, measure‐\nment, and sharing (sometimes abbreviated as the acronym CAMS). This book is\nnot meant as a comprehensive overview of DevOps (check out Appendix  for recom‐\nmended reading), so I will just focus on one of these values: automation.\nThe goal is to automate as much of the software delivery process as possible. That\nmeans that you manage your infrastructure not by clicking around a web page or\nmanually executing shell commands, but through code. This is a concept that is\ntypically called infrastructure as code .\nWhat Is Infrastructure as Code?\nThe idea behind infrastructure as code (IaC) is that you write and execute code to\ndefine, deploy, update, and destroy your infrastructure. This represents an important\nshift in mindset in which you treat all aspects of operations as software—even those\naspects that represent hardware (e.g., setting up physical servers). In fact, a key\ninsight of DevOps is that you can manage almost everything  in code, including\nservers, databases, networks, logfiles, application configuration, documentation, auto‐\nmated tests, deployment processes, and so on.\nThere are five broad categories of IaC tools:\n•Ad hoc scripts•\n•Configuration management tools•\n•Server templating tools•\nWhat Is Infrastructure as Code? | 3\n•Orchestration tools•\n•Provisioning tools•\nLet’s look at these one at a time.\nAd Hoc Scripts\nThe most straightforward approach to automating anything is to write an ad hoc\nscript . Y ou take whatever task you were doing manually, break it down into discrete\nsteps, use your favorite scripting language (e.g., Bash, Ruby, Python) to define each of\nthose steps in code, and execute that script on your server, as shown in Figure 1-1 .\nFigure 1-1. The most straightforward way to automate things is to create an ad hoc script\nthat you run on your servers.\nFor example, here is a Bash script called setup-webserver.sh  that configures a web\nserver by installing dependencies, checking out some code from a Git repo, and firing\nup an Apache web server:\n# Update the apt-get cache\nsudo apt-get update\n# Install PHP and Apache\nsudo apt-get install -y php apache2\n# Copy the code from the repository\nsudo git clone https://github.com/brikis98/php-app.git /var/www/html/app\n# Start Apache\nsudo service apache2 start\n4 | Chapter 1: Why Terraform\nThe great thing about ad hoc scripts is that you can use popular, general-purpose\nprogramming languages, and you can write the code however you want. The terrible\nthing about ad hoc scripts is that you can use popular, general-purpose programming\nlanguages, and you can write the code however you want.\nWhereas tools that are purpose-built for IaC provide concise APIs for accomplishing\ncomplicated tasks, if you’re using a general-purpose programming language, you\nneed to write completely custom code for every task. Moreover, tools designed\nfor IaC usually enforce a particular structure for your code, whereas with a general-\npurpose programming language, each developer will use their own style and do\nsomething different. Neither of these problems is a big deal for an eight-line script\nthat installs Apache, but it gets messy if you try to use ad hoc scripts to manage\ndozens of servers, databases, load balancers, network configurations, and so on.\nIf you’ve ever had to maintain a large repository of Bash scripts, you know that it\nalmost always devolves into a mess of unmaintainable spaghetti code. Ad hoc scripts\nare great for small, one-off tasks, but if you’re going to be managing all of your\ninfrastructure as code, then you should use an IaC tool that is purpose-built for the\njob.\nConfiguration  Management Tools\nChef, Puppet, and Ansible are all configuration  management tools , which means that\nthey are designed to install and manage software on existing servers. For example,\nhere is an Ansible role  called web-server.yml  that configures the same Apache web\nserver as the setup-webserver.sh  script:\n- name: Update the apt-get cache\n  apt:\n    update_cache : yes\n- name: Install PHP\n  apt:\n    name: php\n- name: Install Apache\n  apt:\n    name: apache2\n- name: Copy the code from the repository\n  git: repo=https://github.com/brikis98/php-app.git dest=/var/www/html/app\n- name: Start Apache\n  service: name=apache2 state=started enabled=yes\nWhat Is Infrastructure as Code? | 5\nThe code looks similar to the Bash script, but using a tool like Ansible offers a\nnumber of advantages:\nCoding conventions\nAnsible enforces a consistent, predictable structure, including documentation,\nfile layout, clearly named parameters, secrets management, and so on. While\nevery developer organizes their ad hoc scripts in a different way, most configura‐\ntion management tools come with a set of conventions that makes it easier to\nnavigate the code.\nIdempotence\nWriting an ad hoc script that works once isn’t too difficult; writing an ad hoc\nscript that works correctly even if you run it over and over again is much harder.\nEvery time you go to create a folder in your script, you need to remember to\ncheck whether that folder already exists; every time you add a line of configura‐\ntion to a file, you need to check that line doesn’t already exist; every time you\nwant to run an app, you need to check that the app isn’t already running.\nCode that works correctly no matter how many times you run it is called idem‐\npotent code . To make the Bash script from the previous section idempotent,\nyou’ d need to add many lines of code, including lots of if-statements. Most\nAnsible functions, on the other hand, are idempotent by default. For example, the\nweb-server.yml  Ansible role will install Apache only if it isn’t installed already and\nwill try to start the Apache web server only if it isn’t running already.\nDistribution\nAd hoc scripts are designed to run on a single, local machine. Ansible and other\nconfiguration management tools are designed specifically for managing large\nnumbers of remote servers, as shown in Figure 1-2 .\nFigure 1-2. A configuration  management tool like Ansible can execute your code\nacross a large number of servers.\n6 | Chapter 1: Why Terraform",7445
14-Server Templating Tools.pdf,14-Server Templating Tools,"For example, to apply the web-server.yml  role to five servers, you first create a file\ncalled hosts  that contains the IP addresses of those servers:\n[webservers]\n11.11.11.11\n11.11.11.12\n11.11.11.13\n11.11.11.14\n11.11.11.15\nNext, you define the following Ansible playbook :\n- hosts: webservers\n  roles:\n  - webserver\nFinally, you execute the playbook as follows:\nansible-playbook playbook.yml\nThis instructs Ansible to configure all five servers in parallel. Alternatively, by\nsetting a parameter called serial  in the playbook, you can do a rolling deploy‐\nment , which updates the servers in batches. For example, setting serial  to 2\ndirects Ansible to update two of the servers at a time, until all five are done.\nDuplicating any of this logic in an ad hoc script would take dozens or even\nhundreds of lines of code.\nServer Templating Tools\nAn alternative to configuration management that has been growing in popularity\nrecently are server templating tools  such as Docker, Packer, and Vagrant. Instead of\nlaunching a bunch of servers and configuring them by running the same code on\neach one, the idea behind server templating tools is to create an image  of a server that\ncaptures a fully self-contained “snapshot” of the operating system (OS), the software,\nthe files, and all other relevant details. Y ou can then use some other IaC tool to install\nthat image on all of your servers, as shown in Figure 1-3 .\nWhat Is Infrastructure as Code? | 7\nFigure 1-3. You can use a server templating tool like Packer to create a self-contained\nimage of a server. You can then use other tools, such as Ansible, to install that image\nacross all of your servers.\nThere are two broad categories of tools for working with images ( Figure 1-4 ):\nVirtual machines\nA virtual machine  (VM)  emulates an entire computer system, including the\nhardware. Y ou run a hypervisor , such as VMware, VirtualBox, or Parallels, to vir‐\ntualize (i.e., simulate) the underlying CPU, memory, hard drive, and networking.\nThe benefit of this is that any VM image  that you run on top of the hypervisor\ncan see only the virtualized hardware, so it’s fully isolated from the host machine\nand any other VM images, and it will run exactly the same way in all environ‐\nments (e.g., your computer, a QA server, a production server). The drawback is\nthat virtualizing all this hardware and running a totally separate OS for each VM\nincurs a lot of overhead in terms of CPU usage, memory usage, and startup time.\nY ou can define VM images as code using tools such as Packer and Vagrant.\n8 | Chapter 1: Why Terraform\n2On most modern operating systems, code runs in one of two “spaces”: kernel space  or user space . Code run‐\nning in kernel space has direct, unrestricted access to all of the hardware. There are no security restrictions\n(i.e., you can execute any CPU instruction, access any part of the hard drive, write to any address in memory)\nor safety restrictions (e.g., a crash in kernel space will typically crash the entire computer), so kernel space\nis generally reserved for the lowest-level, most trusted functions of the OS (typically called the kernel ). Code\nrunning in user space does not have any direct access to the hardware and must use APIs exposed by the OS\nkernel instead. These APIs can enforce security restrictions (e.g., user permissions) and safety (e.g., a crash in\na user space app typically affects only that app), so just about all application code runs in user space.\n3As a general rule, containers provide isolation that’s good enough to run your own code, but if you need\nto run third-party code (e.g., you’re building your own cloud provider) that might actively be performing\nmalicious actions, you’ll want the increased isolation guarantees of a VM.Containers\nA container  emulates the user space of an OS.2 Y ou run a container engine , such as\nDocker, CoreOS rkt, or cri-o, to create isolated processes, memory, mount points,\nand networking.\nThe benefit of this is that any container you run on top of the container engine\ncan see only its own user space, so it’s isolated from the host machine and other\ncontainers and will run exactly the same way in all environments (your computer,\na QA server, a production server, etc.). The drawback is that all of the containers\nrunning on a single server share that server’s OS kernel and hardware, so it’s\nmuch more difficult to achieve the level of isolation and security you get with a\nVM.3 However, because the kernel and hardware are shared, your containers can\nboot up in milliseconds and have virtually no CPU or memory overhead. Y ou\ncan define container images as code using tools such as Docker and CoreOS rkt;\nyou’ll see an example of how to use Docker in Chapter 7 .\nWhat Is Infrastructure as Code? | 9\nFigure 1-4. The two main types of images: VMs, on the left, and containers, on the right.\nVMs virtualize the hardware, whereas containers virtualize only user space.\nFor example, here is a Packer template called web-server.json  that creates an Amazon\nMachine Image  (AMI), which is a VM image that you can run on AWS:\n{\n  ""builders"" : [{\n    ""ami_name"" : ""packer-example-"" ,\n    ""instance_type"" : ""t2.micro"" ,\n    ""region"" : ""us-east-2"" ,\n    ""type"": ""amazon-ebs"" ,\n    ""source_ami"" : ""ami-0fb653ca2d3203ac1"" ,\n    ""ssh_username"" : ""ubuntu""\n  }],\n  ""provisioners"" : [{\n    ""type"": ""shell"",\n    ""inline"" : [\n      ""sudo apt-get update"" ,\n      ""sudo apt-get install -y php apache2"" ,\n      ""sudo git clone https://github.com/brikis98/php-app.git /var/www/html/app""\n    ],\n    ""environment_vars"" : [\n      ""DEBIAN_FRONTEND=noninteractive""\n    ],\n    ""pause_before"" : ""60s""\n10 | Chapter 1: Why Terraform\n  }]\n}\nThis Packer template configures the same Apache web server that you saw in setup-\nwebserver.sh  using the same Bash code. The only difference between the code in the\nPacker template and the previous examples is that this Packer template does not start\nthe Apache web server (e.g., by calling sudo service apache2 start ). That’s because\nserver templates are typically used to install software in images, but it’s only when you\nrun the image—for example, by deploying it on a server—that you should actually\nrun that software.\nTo build an AMI from this template, run packer build webserver.json . After the\nbuild completes, you can install that AMI on all of your AWS servers and configure\neach server to run Apache when the server is booting (you’ll see an example of this in\nthe next section), and they will all run exactly the same way.\nNote that the different server templating tools have slightly different purposes. Packer\nis typically used to create images that you run directly on top of production servers,\nsuch as an AMI that you run in your production AWS account. Vagrant is typically\nused to create images that you run on your development computers, such as a\nVirtualBox image that you run on your Mac or Windows laptop. Docker  is typically\nused to create images of individual applications. Y ou can run the Docker images on\nproduction or development computers, as long as some other tool has configured\nthat computer with the Docker Engine. For example, a common pattern is to use\nPacker to create an AMI that has the Docker Engine installed, deploy that AMI\non a cluster of servers in your AWS account, and then deploy individual Docker\ncontainers across that cluster to run your applications.\nServer templating is a key component of the shift to immutable infrastructure . This\nidea is inspired by functional programming, where variables are immutable, so after\nyou’ve set a variable to a value, you can never change that variable again. If you need\nto update something, you create a new variable. Because variables never change, it’s a\nlot easier to reason about your code.\nThe idea behind immutable infrastructure is similar: once you’ve deployed a server,\nyou never make changes to it again. If you need to update something, such as\ndeploying a new version of your code, you create a new image from your server\ntemplate and you deploy it on a new server. Because servers never change, it’s a lot\neasier to reason about what’s deployed.\nWhat Is Infrastructure as Code? | 11",8291
15-Orchestration Tools.pdf,15-Orchestration Tools,"Orchestration Tools\nServer  templating tools are great for creating VMs and containers, but how do you\nactually manage them? For most real-world use cases, you’ll need a way to do the\nfollowing:\n•Deploy VMs and containers, making efficient use of your hardware.•\n•Roll out updates to an existing fleet of VMs and containers using strategies such•\nas rolling deployment, blue-green deployment, and canary deployment.\n•Monitor the health of your VMs and containers and automatically replace•\nunhealthy ones ( auto healing ).\n•Scale the number of VMs and containers up or down in response to load ( auto •\nscaling ).\n•Distribute traffic across your VMs and containers ( load balancing ). •\n•Allow your VMs and containers to find and talk to one another over the network•\n(service discovery ).\nHandling these tasks is the realm of orchestration tools  such as Kubernetes, Mara‐\nthon/Mesos, Amazon Elastic Container Service (Amazon ECS), Docker Swarm, and\nNomad. For example, Kubernetes allows you to define how to manage your Docker\ncontainers as code. Y ou first deploy a Kubernetes cluster , which is a group of servers\nthat Kubernetes will manage and use to run your Docker containers. Most major\ncloud providers have native support for deploying managed Kubernetes clusters, such\nas Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE), and\nAzure Kubernetes Service (AKS).\nOnce you have a working cluster, you can define how to run your Docker container\nas code in a YAML file:\napiVersion : apps/v1\n# Use a Deployment to deploy multiple replicas of your Docker\n# container(s) and to declaratively roll out updates to them\nkind: Deployment\n# Metadata about this Deployment, including its name\nmetadata :\n  name: example-app\n# The specification that configures this Deployment\nspec:\n  # This tells the Deployment how to find your container(s)\n  selector :\n    matchLabels :\n      app: example-app\n12 | Chapter 1: Why Terraform\n  # This tells the Deployment to run three replicas of your\n  # Docker container(s)\n  replicas : 3\n  # Specifies how to update the Deployment. Here, we\n  # configure a rolling update.\n  strategy :\n    rollingUpdate :\n      maxSurge : 3\n      maxUnavailable : 0\n    type: RollingUpdate\n  # This is the template for what container(s) to deploy\n  template :\n    # The metadata for these container(s), including labels\n    metadata :\n      labels:\n        app: example-app\n    # The specification for your container(s)\n    spec:\n      containers :\n        # Run Apache listening on port 80\n        - name: example-app\n          image: httpd:2.4.39\n          ports:\n            - containerPort : 80\nThis file instructs Kubernetes to create a Deployment , which is a declarative way to\ndefine the following:\n•One or more Docker containers to run together. This group of containers is•\ncalled a Pod. The Pod defined in the preceding code contains a single Docker\ncontainer that runs Apache.\n•The settings for each Docker container in the Pod. The Pod in the preceding code•\nconfigures Apache to listen on port 80.\n•How many copies (aka replicas ) of the Pod to run in your cluster. The preceding •\ncode configures three replicas. Kubernetes automatically figures out where in\nyour cluster to deploy each Pod, using a scheduling algorithm to pick the optimal\nservers in terms of high availability (e.g., try to run each Pod on a separate\nserver so a single server crash doesn’t take down your app), resources (e.g., pick\nservers that have available the ports, CPU, memory, and other resources required\nby your containers), performance (e.g., try to pick servers with the least load\nand fewest containers on them), and so on. Kubernetes also constantly monitors\nthe cluster to ensure that there are always three replicas running, automatically\nreplacing any Pods that crash or stop responding.\nWhat Is Infrastructure as Code? | 13",3944
16-Provisioning Tools.pdf,16-Provisioning Tools,"•How to deploy updates. When deploying a new version of the Docker container,•\nthe preceding code rolls out three new replicas, waits for them to be healthy, and\nthen undeploys the three old replicas.\nThat’s a lot of power in just a few lines of YAML! Y ou run kubectl apply -f\nexample-app.yml  to instruct Kubernetes to deploy your app. Y ou can then make\nchanges to the YAML file and run kubectl apply  again to roll out the updates. Y ou\ncan also manage both the Kubernetes cluster and the apps within it using Terraform;\nyou’ll see an example of this in Chapter 7 .\nProvisioning Tools\nWhereas  configuration management, server templating, and orchestration tools\ndefine the code that runs on each server, provisioning tools  such as Terraform, Cloud‐\nFormation, OpenStack Heat, and Pulumi are responsible for creating the servers\nthemselves. In fact, you can use provisioning tools to create not only servers but also\ndatabases, caches, load balancers, queues, monitoring, subnet configurations, firewall\nsettings, routing rules, Secure Sockets Layer (SSL) certificates, and almost every other\naspect of your infrastructure, as shown in Figure 1-5 .\nFor example, the following code deploys a web server using Terraform:\nresource  ""aws_instance"" ""app""  {\n  instance_type      = ""t2.micro""\n  availability_zone  = ""us-east-2a""\n  ami               = ""ami-0fb653ca2d3203ac1""\n  user_data  = <<-EOF\n              #!/bin/bash\n              sudo service apache2 start\n              EOF\n}\nDon’t worry if you’re not yet familiar with some of the syntax. For now, just focus on\ntwo parameters:\nami\nThis parameter specifies the ID of an AMI to deploy on the server. Y ou could\nset this parameter to the ID of an AMI built from the web-server.json  Packer\ntemplate in the previous section, which has PHP , Apache, and the application\nsource code.\nuser_data\nThis is a Bash script that executes when the web server is booting. The preceding\ncode uses this script to boot up Apache.\nIn other words, this code shows you provisioning and server templating working\ntogether, which is a common pattern in immutable infrastructure.\n14 | Chapter 1: Why Terraform\nFigure 1-5. You can use provisioning tools with your cloud provider to create servers,\ndatabases, load balancers, and all other parts of your infrastructure.\nWhat Is Infrastructure as Code? | 15",2379
17-How Does Terraform Work.pdf,17-How Does Terraform Work,"4This is where the term bus factor  comes from: your team’s bus factor is the number of people you can lose\n(e.g., because they got hit by a bus) before you can no longer operate your business. Y ou never want to have a\nbus factor of 1.What Are the Benefits  of Infrastructure as Code?\nNow that you’ve seen all the different flavors of IaC, a good question to ask is, why\nbother? Why learn a bunch of new languages and tools and encumber yourself with\nyet more code to manage?\nThe answer is that code is powerful. In exchange for the upfront investment of con‐\nverting your manual practices to code, you get dramatic improvements in your ability\nto deliver software. According to the 2016 State of DevOps Report , organizations that\nuse DevOps practices, such as IaC, deploy 200 times more frequently, recover from\nfailures 24 times faster, and have lead times that are 2,555  times lower.\nWhen your infrastructure is defined as code, you are able to use a wide variety\nof software engineering practices to dramatically improve your software delivery\nprocess, including the following:\nSelf-service\nMost teams that deploy code manually have a small number of sysadmins (often,\njust one) who are the only ones who know all the magic incantations to make the\ndeployment work and are the only ones with access to production. This becomes\na major bottleneck as the company grows. If your infrastructure is defined in\ncode, the entire deployment process can be automated, and developers can kick\noff their own deployments whenever necessary.\nSpeed and safety\nIf the deployment process is automated, it will be significantly faster, since a\ncomputer can carry out the deployment steps far faster than a person, and safer,\ngiven that an automated process will be more consistent, more repeatable, and\nnot prone to manual error.\nDocumentation\nIf the state of your infrastructure is locked away in a single sysadmin’s head, and\nthat sysadmin goes on vacation or leaves the company or gets hit by a bus,4 you\nmay suddenly realize you can no longer manage your own infrastructure. On\nthe other hand, if your infrastructure is defined as code, then the state of your\ninfrastructure is in source files that anyone can read. In other words, IaC acts as\ndocumentation, allowing everyone in the organization to understand how things\nwork, even if the sysadmin goes on vacation.\n16 | Chapter 1: Why Terraform\n5Check out the Gruntwork Infrastructure as Code Library  for an example.Version control\nY ou can store your IaC source files in version control, which means that the\nentire history of your infrastructure is now captured in the commit log. This\nbecomes a powerful tool for debugging issues, because any time a problem pops\nup, your first step will be to check the commit log and find out what changed\nin your infrastructure, and your second step might be to resolve the problem by\nsimply reverting back to a previous, known-good version of your IaC code.\nValidation\nIf the state of your infrastructure is defined in code, for every single change, you\ncan perform a code review, run a suite of automated tests, and pass the code\nthrough static analysis tools—all practices that are known to significantly reduce\nthe chance of defects.\nReuse\nY ou can package your infrastructure into reusable modules so that instead of\ndoing every deployment for every product in every environment from scratch,\nyou can build on top of known, documented, battle-tested pieces.5\nHappiness\nThere is one other very important, and often overlooked, reason for why you\nshould use IaC: happiness. Deploying code and managing infrastructure man‐\nually is repetitive and tedious. Developers and sysadmins resent this type of\nwork, since it involves no creativity, no challenge, and no recognition. Y ou could\ndeploy code perfectly for months, and no one will take notice—until that one\nday when you mess it up. That creates a stressful and unpleasant environment.\nIaC offers a better alternative that allows computers to do what they do best\n(automation) and developers to do what they do best (coding).\nNow that you have a sense of why IaC is important, the next question is whether\nTerraform is the best IaC tool for you. To answer that, I’m first going to go through\na very quick primer on how Terraform works, and then I’ll compare it to the other\npopular IaC options out there, such as Chef, Puppet, and Ansible.\nHow Does Terraform Work?\nHere is a high-level and somewhat simplified view of how Terraform works. Ter‐\nraform is an open source tool created by HashiCorp and written in the Go pro‐\ngramming language. The Go code compiles down into a single binary (or rather,\none binary for each of the supported operating systems) called, not surprisingly,\nterraform .\nHow Does Terraform Work? | 17\nY ou can use this binary to deploy infrastructure from your laptop or a build server or\njust about any other computer, and you don’t need to run any extra infrastructure to\nmake that happen. That’s because under the hood, the terraform  binary makes API\ncalls on your behalf to one or more providers , such as AWS, Azure, Google Cloud,\nDigitalOcean, OpenStack, and more. This means that Terraform gets to leverage the\ninfrastructure those providers are already running for their API servers, as well as the\nauthentication mechanisms you’re already using with those providers (e.g., the API\nkeys you already have for AWS).\nHow does Terraform know what API calls to make? The answer is that you  create\nTerraform configurations , which are text files that specify what infrastructure you\nwant to create. These configurations are the “code” in “infrastructure as code. ” Here’s\nan example Terraform configuration:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nresource  ""google_dns_record_set"" ""a""  {\n  name         = ""demo.google-example.com""\n  managed_zone  = ""example-zone""\n  type         = ""A""\n  ttl          = 300\n  rrdatas       = [aws_instance.example.public_ip ]\n}\nEven if you’ve never seen Terraform code before, you shouldn’t have too much trou‐\nble reading it. This snippet instructs Terraform to make API calls to AWS to deploy\na server, and then make API calls to Google Cloud to create a Domain Name System\n(DNS) entry pointing to the AWS server’s IP address. In just a single, simple syntax\n(which you’ll learn in Chapter 2 ), Terraform allows you to deploy interconnected\nresources across multiple cloud providers.\nY ou can define your entire infrastructure—servers, databases, load balancers, net‐\nwork topology, and so on—in Terraform configuration files and commit those files\nto version control. Y ou then run certain Terraform commands, such as terraform\napply , to deploy that infrastructure. The terraform  binary parses your code, trans‐\nlates it into a series of API calls to the cloud providers specified in the code, and\nmakes those API calls as efficiently as possible on your behalf, as shown in Figure 1-6 .\n18 | Chapter 1: Why Terraform\nFigure 1-6. Terraform is a binary that translates the contents of your configurations  into\nAPI calls to cloud providers.\nWhen someone on your team needs to make changes to the infrastructure, instead\nof updating the infrastructure manually and directly on the servers, they make their\nchanges in the Terraform configuration files, validate those changes through automa‐\nted tests and code reviews, commit the updated code to version control, and then run\nthe terraform apply  command to have Terraform make the necessary API calls to\ndeploy the changes.\nTransparent Portability Between Cloud Providers\nBecause Terraform supports many different cloud providers, a\ncommon question that arises is whether it supports transparent\nportability  between them. For example, if you used Terraform to\ndefine a bunch of servers, databases, load balancers, and other\ninfrastructure in AWS, could you instruct Terraform to deploy\nexactly the same infrastructure in another cloud provider, such as\nAzure or Google Cloud, in just a few commands?\nThis question turns out to be a bit of a red herring. The reality is\nthat you can’t deploy “exactly the same infrastructure” in a different\ncloud provider because the cloud providers don’t offer the same\ntypes of infrastructure! The servers, load balancers, and databases\noffered by AWS are very different from those in Azure and Google\nCloud in terms of features, configuration, management, security,\nscalability, availability, observability, and so on. There is no easy\nway to “transparently” paper over these differences, especially as\nfunctionality in one cloud provider often doesn’t exist at all in the\nothers. Terraform’s approach is to allow you to write code that\nis specific to each provider, taking advantage of that provider’s\nunique functionality, but to use the same language, toolset, and IaC\npractices under the hood for all providers.\nHow Does Terraform Work? | 19",9046
18-Configuration Management Versus Provisioning.pdf,18-Configuration Management Versus Provisioning,"6Docker, Packer, and Kubernetes are not part of the comparison, because they can be used with any of the\nconfiguration management or provisioning tools.How Does Terraform Compare to Other IaC Tools?\nInfrastructure  as code is wonderful, but the process of picking an IaC tool is not.\nMany of the IaC tools overlap in what they do. Many of them are open source. Many\nof them offer commercial support. Unless you’ve used each one yourself, it’s not clear\nwhat criteria you should use to pick one or the other.\nWhat makes this even more difficult is that most of the comparisons you find\nbetween these tools do little more than list the general properties of each one and\nmake it sound as if you could be equally successful with any of them. And although\nthat’s technically true, it’s not helpful. It’s a bit like telling a programming newbie\nthat you could be equally successful building a website with PHP , C, or assembly—a\nstatement that’s technically true but one that omits a huge amount of information that\nis essential for making a good decision.\nIn the following sections, I’m going to do a detailed comparison between the most\npopular configuration management and provisioning tools: Terraform, Chef, Puppet,\nAnsible, Pulumi, CloudFormation, and OpenStack Heat. My goal is to help you\ndecide whether Terraform is a good choice by explaining why my company, Grunt‐\nwork, picked Terraform as our IaC tool of choice and, in some sense, why I wrote this\nbook.6 As with all technology decisions, it’s a question of trade-offs and priorities, and\neven though your particular priorities might be different than mine, my hope is that\nsharing this thought process will help you to make your own decision.\nHere are the main trade-offs to consider:\n•Configuration management versus provisioning•\n•Mutable infrastructure versus immutable infrastructure•\n•Procedural language versus declarative language•\n•General-purpose language versus domain-specific language•\n•Master versus masterless•\n•Agent versus agentless•\n•Paid versus free offering•\n•Large community versus small community•\n•Mature versus cutting-edge•\n•Use of multiple tools together•\n20 | Chapter 1: Why Terraform",2202
19-Procedural Language Versus Declarative Language.pdf,19-Procedural Language Versus Declarative Language,"Configuration  Management Versus Provisioning\nAs you saw earlier, Chef, Puppet, and Ansible are all configuration management\ntools, whereas CloudFormation, Terraform, OpenStack Heat, and Pulumi are all\nprovisioning tools.\nAlthough the distinction is not entirely clear cut, given that configuration manage‐\nment tools can typically do some degree of provisioning (e.g., you can deploy a\nserver with Ansible) and that provisioning tools can typically do some degree of\nconfiguration (e.g., you can run configuration scripts on each server you provision\nwith Terraform), you typically want to pick the tool that’s the best fit for your use\ncase.\nIn particular, if you use server templating tools, the vast majority of your configura‐\ntion management needs are already taken care of. Once you have an image created\nfrom a Dockerfile  or Packer template, all that’s left to do is provision the infrastruc‐\nture for running those images. And when it comes to provisioning, a provisioning\ntool is going to be your best choice. In Chapter 7 , you’ll see an example of how to use\nTerraform and Docker together, which is a particularly popular combination these\ndays.\nThat said, if you’re not using server templating tools, a good alternative is to use a\nconfiguration management and provisioning tool together. For example, a popular\ncombination is to use Terraform to provision your servers and Ansible to configure\neach one.\nMutable Infrastructure Versus Immutable Infrastructure\nConfiguration  management tools such as Chef, Puppet, and Ansible typically default\nto a mutable infrastructure paradigm.\nFor example, if you instruct Chef to install a new version of OpenSSL, it will run the\nsoftware update on your existing servers, and the changes will happen in place. Over\ntime, as you apply more and more updates, each server builds up a unique history of\nchanges. As a result, each server becomes slightly different than all the others, leading\nto subtle configuration bugs that are difficult to diagnose and reproduce (this is the\nsame configuration drift problem that happens when you manage servers manually,\nalthough it’s much less problematic when using a configuration management tool).\nEven with automated tests, these bugs are difficult to catch; a configuration manage‐\nment change might work just fine on a test server, but that same change might behave\ndifferently on a production server because the production server has accumulated\nmonths of changes that aren’t reflected in the test environment.\nIf you’re using a provisioning tool such as Terraform to deploy machine images\ncreated by Docker or Packer, most “changes” are actually deployments of a completely\nnew server. For example, to deploy a new version of OpenSSL, you would use Packer\nHow Does Terraform Compare to Other IaC Tools? | 21\nto create a new image with the new version of OpenSSL, deploy that image across\na set of new servers, and then terminate the old servers. Because every deployment\nuses immutable images on fresh servers, this approach reduces the likelihood of\nconfiguration drift bugs, makes it easier to know exactly what software is running on\neach server, and allows you to easily deploy any previous version of the software (any\nprevious image) at any time. It also makes your automated testing more effective,\nbecause an immutable image that passes your tests in the test environment is likely to\nbehave exactly the same way in the production environment.\nOf course, it’s possible to force configuration management tools to do immutable\ndeployments, too, but it’s not the idiomatic approach for those tools, whereas it’s\na natural way to use provisioning tools. It’s also worth mentioning that the immut‐\nable approach has downsides of its own. For example, rebuilding an image from\na server template and redeploying all your servers for a trivial change can take a\nlong time. Moreover, immutability lasts only until you actually run the image. After\na server is up and running, it will begin making changes on the hard drive and\nexperiencing some degree of configuration drift (although this is mitigated if you\ndeploy frequently).\nProcedural Language Versus Declarative Language\nChef and Ansible encourage a procedural  style in which you write code that specifies,\nstep by step, how to achieve some desired end state.\nTerraform, CloudFormation, Puppet, OpenStack Heat, and Pulumi all encourage a\nmore declarative  style in which you write code that specifies your desired end state,\nand the IaC tool itself is responsible for figuring out how to achieve that state.\nTo demonstrate the difference, let’s go through an example. Imagine that you\nwant to deploy 10 servers ( EC2 Instances  in AWS lingo) to run an AMI with ID\nami-0fb653ca2d3203ac1  (Ubuntu 20.04). Here is a simplified example of an Ansible\ntemplate that does this using a procedural approach:\n- ec2:\n    count: 10\n    image: ami-0fb653ca2d3203ac1\n    instance_type : t2.micro\nAnd here is a simplified example of a Terraform configuration that does the same\nthing using a declarative approach:\nresource  ""aws_instance"" ""example""  {\n  count         = 10\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\n22 | Chapter 1: Why Terraform\nOn the surface, these two approaches might look similar, and when you initially\nexecute them with Ansible or Terraform, they will produce similar results. The\ninteresting thing is what happens when you want to make a change.\nFor example, imagine traffic has gone up, and you want to increase the number\nof servers to 15. With Ansible, the procedural code you wrote earlier is no longer\nuseful; if you just updated the number of servers to 15 and reran that code, it would\ndeploy 15 new servers, giving you 25 total! So instead, you need to be aware of what\nis already deployed and write a totally new procedural script to add the five new\nservers:\n- ec2:\n    count: 5\n    image: ami-0fb653ca2d3203ac1\n    instance_type : t2.micro\nWith declarative code, because all you do is declare the end state that you want and\nTerraform figures out how to get to that end state, Terraform will also be aware of any\nstate it created in the past. Therefore, to deploy five more servers, all you need to do is\ngo back to the same Terraform configuration and update the count from 10 to 15:\nresource  ""aws_instance"" ""example""  {\n  count         = 15\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nIf you applied this configuration, Terraform would realize it had already created 10\nservers and therefore all it needs to do is create five new servers. In fact, before\napplying this configuration, you can use Terraform’s plan  command to preview what\nchanges it would make:\n$ terraform plan\n# aws_instance.example[11] will be created\n+ resource ""aws_instance"" ""example"" {\n    + ami            = ""ami-0fb653ca2d3203ac1""\n    + instance_type  = ""t2.micro""\n    + (...)\n  }\n# aws_instance.example[12] will be created\n+ resource ""aws_instance"" ""example"" {\n    + ami            = ""ami-0fb653ca2d3203ac1""\n    + instance_type  = ""t2.micro""\n    + (...)\n  }\n# aws_instance.example[13] will be created\n+ resource ""aws_instance"" ""example"" {\n    + ami            = ""ami-0fb653ca2d3203ac1""\nHow Does Terraform Compare to Other IaC Tools? | 23\n    + instance_type  = ""t2.micro""\n    + (...)\n  }\n# aws_instance.example[14] will be created\n+ resource ""aws_instance"" ""example"" {\n    + ami            = ""ami-0fb653ca2d3203ac1""\n    + instance_type  = ""t2.micro""\n    + (...)\n  }\nPlan: 5 to add, 0 to change, 0 to destroy.\nNow what happens when you want to deploy a different version of the app, such\nas AMI ID ami-02bcbb802e03574ba ? With the procedural approach, both of your\nprevious Ansible templates are again not useful, so you need to write yet another\ntemplate to track down the 10 servers you deployed previously (or was it 15 now?)\nand carefully update each one to the new version. With the declarative approach of\nTerraform, you go back to the exact same configuration file again and simply change\nthe ami parameter to ami-02bcbb802e03574ba :\nresource  ""aws_instance"" ""example""  {\n  count         = 15\n  ami           = ""ami-02bcbb802e03574ba""\n  instance_type  = ""t2.micro""\n}\nObviously, these examples are simplified. Ansible does allow you to use tags to search\nfor existing EC2 Instances before deploying new ones (e.g., using the instance_tags\nand count_tag  parameters), but having to manually figure out this sort of logic for\nevery single resource you manage with Ansible, based on each resource’s past history,\ncan be surprisingly complicated: for example, you may have to manually configure\nyour code to look up existing Instances not only by tag but also by image version,\nAvailability Zone, and other parameters. This highlights two major problems with\nprocedural IaC tools:\nProcedural code does not fully capture the state of the infrastructure\nReading through the three preceding Ansible templates is not enough to know\nwhat’s deployed. Y ou’ d also need to know the order  in which those templates\nwere applied. Had you applied them in a different order, you might have ended\nup with different infrastructure, and that’s not something you can see in the\ncodebase itself. In other words, to reason about an Ansible or Chef codebase, you\nneed to know the full history of every change that has ever happened.\nProcedural code limits reusability\nThe reusability of procedural code is inherently limited because you must man‐\nually take into account the current state of the infrastructure. Because that state\nis constantly changing, code you used a week ago might no longer be usable\n24 | Chapter 1: Why Terraform",9795
20-Master Versus Masterless.pdf,20-Master Versus Masterless,"because it was designed to modify a state of your infrastructure that no longer\nexists. As a result, procedural codebases tend to grow large and complicated over\ntime.\nWith Terraform’s declarative approach, the code always represents the latest state of\nyour infrastructure. At a glance, you can determine what’s currently deployed and\nhow it’s configured, without having to worry about history or timing. This also makes\nit easy to create reusable code, since you don’t need to manually account for the\ncurrent state of the world. Instead, you just focus on describing your desired state,\nand Terraform figures out how to get from one state to the other automatically. As a\nresult, Terraform codebases tend to stay small and easy to understand.\nGeneral-Purpose Language Versus Domain-Specific  Language\nChef and Pulumi allow you to use a general-purpose programming language  (GPL)\nto manage infrastructure as code: Chef supports Ruby; Pulumi supports a wide\nvariety of GPLs, including JavaScript, TypeScript, Python, Go, C#, Java, and oth‐\ners. Terraform, Puppet, Ansible, CloudFormation, and OpenStack Heat each use a\ndomain-specific  language  (DSL) to manage infrastructure as code: Terraform uses\nHCL; Puppet uses Puppet Language; Ansible, CloudFormation, and OpenStack Heat\nuse YAML (CloudFormation also supports JSON).\nThe distinction between GPLs and DSLs is not entirely clear-cut—it’s more of a\nhelpful mental model than a clean, separate categorization—but the basic idea is\nthat DSLs are designed for use in one specific domain, whereas GPLs can be used\nacross a broad range of domains. For example, the HCL code you write for Terraform\nworks only with Terraform and is limited solely to the functionality supported by\nTerraform, such as deploying infrastructure. This is in contrast to using a GPL such\nas JavaScript with Pulumi, where the code you write can not only manage infrastruc‐\nture using Pulumi libraries but also perform almost any other programming task you\nwish, such as run a web app (in fact, Pulumi offers an Automation API you can use\nto embed Pulumi within your application code), perform complicated control logic\n(loops, conditionals, and abstraction are all easier to do in a GPL than a DSL), run\nvarious validations and tests, integrate with other tools and APIs, and so on.\nDSLs have several advantages over GPLs:\nEasier to learn\nSince DSLs, by design, deal with just one domain, they tend to be smaller and\nsimpler languages than GPLs and therefore are easier to learn than GPLs. Most\ndevelopers will be able to learn Terraform faster than, say, Java.\nClearer and more concise\nSince DSLs are designed for one specific purpose, with all the keywords in the\nlanguage built to do that one thing, code written in DSLs tends to be easier to\nHow Does Terraform Compare to Other IaC Tools? | 25\nunderstand and more concise than code written to do the exact same thing but\nwritten in a GPL. The code to deploy a single server in AWS is usually going to be\nshorter and easier to understand in Terraform than in Java.\nMore uniform\nMost DSLs are limited in what they allow you to do. This has some drawbacks,\nas I’ll mention shortly, but one of the advantages is that code written in DSLs\ntypically uses a uniform, predictable structure, so it’s easier to navigate and\nunderstand than code written in GPLs, where every developer might solve the\nsame problem in a completely different way. There’s really only one way to deploy\na server in AWS using Terraform; there are hundreds of ways to do the same\nthing with Java.\nGPLs also have several advantages over DSLs:\nPossibly no need to learn anything new\nSince GPLs are used in many domains, there’s a chance you might not have to\nlearn a new language at all. This is especially true of Pulumi, as it supports several\nof the most popular languages in the world, including JavaScript, Python, and\nJava. If you already know Java, you’ll be able to jump into Pulumi faster than if\nyou had to learn HCL to use Terraform.\nBigger ecosystem and more mature tooling\nSince GPLs are used in many domains, they have far bigger communities and\nmuch more mature tooling than a typical DSL. The number and quality of\nIntegrated Development Environments (IDEs), libraries, patterns, testing tools,\nand so on for Java vastly exceeds what’s available for Terraform.\nMore power\nGPLs, by design, can be used to do almost any programming task, so they offer\nmuch more power and functionality than DSLs. Certain tasks, such as control\nlogic (loops and conditionals), automated testing, code reuse, abstraction, and\nintegration with other tools, are far easier with Java than with Terraform.\nMaster Versus Masterless\nBy default, Chef and Puppet require that you run a master server  for storing the\nstate of your infrastructure and distributing updates. Every time you want to update\nsomething in your infrastructure, you use a client (e.g., a command-line tool) to issue\nnew commands to the master server, and the master server either pushes the updates\nout to all of the other servers or those servers pull the latest updates down from the\nmaster server on a regular basis.\n26 | Chapter 1: Why Terraform\nA master server offers a few advantages. First, it’s a single, central place where you\ncan see and manage the status of your infrastructure. Many configuration manage‐\nment tools even provide a web interface (e.g., the Chef Console, Puppet Enterprise\nConsole) for the master server to make it easier to see what’s going on. Second,\nsome master servers can run continuously in the background and enforce your\nconfiguration. That way, if someone makes a manual change on a server, the master\nserver can revert that change to prevent configuration drift.\nHowever, having to run a master server has some serious drawbacks:\nExtra infrastructure\nY ou need to deploy an extra server, or even a cluster of extra servers (for high\navailability and scalability), just to run the master.\nMaintenance\nY ou need to maintain, upgrade, back up, monitor, and scale the master server(s).\nSecurity\nY ou need to provide a way for the client to communicate to the master server(s)\nand a way for the master server(s) to communicate with all the other servers,\nwhich typically means opening extra ports and configuring extra authentication\nsystems, all of which increases your surface area to attackers.\nChef and Puppet do have varying levels of support for masterless modes where you\nrun just their agent software on each of your servers, typically on a periodic schedule\n(e.g., a cron job that runs every five minutes), and use that to pull down the latest\nupdates from version control (rather than from a master server). This significantly\nreduces the number of moving parts, but, as I discuss in the next section, this still\nleaves a number of unanswered questions, especially about how to provision the\nservers and install the agent software on them in the first place.\nAnsible, CloudFormation, Heat, Terraform, and Pulumi are all masterless by default.\nOr, to be more accurate, some of them rely on a master server, but it’s already part of\nthe infrastructure you’re using and not an extra piece that you need to manage. For\nexample, Terraform communicates with cloud providers using the cloud provider’s\nAPIs, so in some sense, the API servers are master servers, except that they don’t\nrequire any extra infrastructure or any extra authentication mechanisms (i.e., just use\nyour API keys). Ansible works by connecting directly to each server over SSH, so\nagain, you don’t need to run any extra infrastructure or manage extra authentication\nmechanisms (i.e., just use your SSH keys).\nHow Does Terraform Compare to Other IaC Tools? | 27",7788
21-Agent Versus Agentless.pdf,21-Agent Versus Agentless,"Agent Versus Agentless\nChef and Puppet require you to install agent software  (e.g., Chef Client, Puppet\nAgent) on each server that you want to configure. The agent typically runs in the\nbackground on each server and is responsible for installing the latest configuration\nmanagement updates.\nThis has a few drawbacks:\nBootstrapping\nHow do you provision your servers and install the agent software on them in\nthe first place? Some configuration management tools kick the can down the\nroad, assuming that some external process will take care of this for them (e.g.,\nyou first use Terraform to deploy a bunch of servers with an AMI that has the\nagent already installed); other configuration management tools have a special\nbootstrapping process in which you run one-off commands to provision the\nservers using the cloud provider APIs and install the agent software on those\nservers over SSH.\nMaintenance\nY ou need to update the agent software on a periodic basis, being careful to keep it\nsynchronized with the master server if there is one. Y ou also need to monitor the\nagent software and restart it if it crashes.\nSecurity\nIf the agent software pulls down configuration from a master server (or some\nother server if you’re not using a master), you need to open outbound ports on\nevery server. If the master server pushes configuration to the agent, you need\nto open inbound ports on every server. In either case, you must figure out how\nto authenticate the agent to the server to which it’s communicating. All of this\nincreases your surface area to attackers.\nOnce again, Chef and Puppet do have varying levels of support for agentless modes,\nbut these feel like they were tacked on as an afterthought and don’t support the full\nfeature set of the configuration management tool. That’s why in the wild, the default\nor idiomatic configuration for Chef and Puppet almost always includes an agent and\nusually a master, too, as shown in Figure 1-7 .\n28 | Chapter 1: Why Terraform\nFigure 1-7. The typical architecture for Chef and Puppet involves many moving parts.\nFor example, the default setup for Chef is to run the Chef client on your computer, which\ntalks to a Chef master server, which deploys changes by communicating with Chef clients\nrunning on all your other servers.\nAll of these extra moving parts introduce a large number of new failure modes into\nyour infrastructure. Each time you get a bug report at 3 a.m., you’ll need to figure out\nwhether it’s a bug in your application code, or your IaC code, or the configuration\nmanagement client, or the master server(s), or the way the client communicates with\nthe master server(s), or the way other servers communicate with the master server(s),\nor…\nAnsible, CloudFormation, Heat, Terraform, and Pulumi do not require you to install\nany extra agents. Or, to be more accurate, some of them require agents, but these\nare typically already installed as part of the infrastructure you’re using. For example,\nAWS, Azure, Google Cloud, and all of the other cloud providers take care of instal‐\nling, managing, and authenticating agent software on each of their physical servers.\nAs a user of Terraform, you don’t need to worry about any of that: you just issue\ncommands, and the cloud provider’s agents execute them for you on all of your\nservers, as shown in Figure 1-8 . With Ansible, your servers need to run the SSH\ndaemon, which is common to run on most servers anyway.\nHow Does Terraform Compare to Other IaC Tools? | 29",3525
22-Large Community Versus Small Community.pdf,22-Large Community Versus Small Community,"Figure 1-8. Terraform uses a masterless, agentless architecture. All you need to run is the\nTerraform client, and it takes care of the rest by using the APIs of cloud providers, such\nas AWS.\nPaid Versus Free Offering\nCloudFormation and OpenStack Heat are completely free: the resources you deploy\nwith those tools may cost money, but you don’t pay anything to use the tools them‐\nselves. Terraform, Chef, Puppet, Ansible, and Pulumi are all available in free versions\nand paid versions: for example, you can use the free and open source version of\nTerraform by itself, or you could choose to use it with HashiCorp’s paid product,\nTerraform Cloud. The price points, packaging, and trade-offs with the paid versions\nare beyond the scope of this book. The one question I want to focus on here is\nwhether the free version is so limited that you are effectively forced  to use the paid\noffering for real-world, production use cases.\nTo be clear, there’s nothing wrong with a company offering a paid service for one of\nthese tools; in fact, if you’re using these tools in production, I strongly recommend\nlooking into the paid services, as many of them are well worth the money. However,\nyou have to realize that those paid services aren’t under your control—they could go\nout of business, or get acquired (e.g., Chef, Puppet, and Ansible have all gone through\nacquisitions that had significant impacts on their paid product offerings), or change\ntheir pricing model (e.g., Pulumi changed its pricing in 2021, which benefited some\nusers but increased prices by ~10x for others), or change the product, or discontinue\nthe product entirely—so it’s important to know whether the IaC tool you picked\nwould still be usable if, for some reason, you couldn’t use one of these paid services.\n30 | Chapter 1: Why Terraform\n7The data on contributors and stars comes from the open source repositories (mostly GitHub) for each tool.\nBecause CloudFormation is closed source, this information is not available.In my experience, the free versions of Terraform, Chef, Puppet, and Ansible can all\nbe used successfully for production use cases; the paid services can make these tools\neven better, but if they weren’t available, you could still get by. Pulumi, on the other\nhand, is harder to use in production without the paid offering known as Pulumi\nService.\nA key part of managing infrastructure as code is managing state (you’ll learn about\nhow Terraform manages state in Chapter 3 ), and Pulumi, by default, uses Pulumi\nService as the backend for state storage. Y ou can switch to other supported backends\nfor state storage, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage,\nbut the Pulumi backend documentation  explains that only Pulumi Service supports\ntransactional checkpointing (for fault tolerance and recovery), concurrent state lock‐\ning (to prevent corrupting your infrastructure state in a team environment), and\nencrypted state in transit and at rest. In my opinion, without these features, it’s not\npractical to use Pulumi in any sort of production environment (i.e., with more than\none developer), so if you’re going to use Pulumi, you more or less have to pay for\nPulumi Service.\nLarge Community Versus Small Community\nWhenever you pick a technology, you are also picking a community. In many cases,\nthe ecosystem around the project can have a bigger impact on your experience than\nthe inherent quality of the technology itself. The community determines how many\npeople contribute to the project; how many plugins, integrations, and extensions are\navailable; how easy it is to find help online (e.g., blog posts, questions on Stack Over‐\nflow); and how easy it is to hire someone to help you (e.g., an employee, consultant,\nor support company).\nIt’s difficult to do an accurate comparison between communities, but you can spot\nsome trends by searching online. Table 1-1  shows a comparison of popular IaC tools,\nwith data I gathered in June 2022, including whether the IaC tool is open source or\nclosed source, what cloud providers it supports, the total number of contributors and\nstars on GitHub, how many open source libraries are available for the tool, and the\nnumber of questions listed for that tool on Stack Overflow.7\nHow Does Terraform Compare to Other IaC Tools? | 31\nTable 1-1. A comparison of IaC communities\nSource Cloud Contributors Stars Libraries Stack Overflow\nChef Open All 640 6,910 3,695a8,295\nPuppet Open All 571 6,581 6,871b3,996\nAnsible Open All 5,328 53,479 31,329c22,052\nPulumi Open All 1,402 12,723 15d327\nCloudFormation Closed AWS ? ? 369e7,252\nHeat Open All 395 379 0f103\nTerraform Open All 1,621 33,019 9,641g13,370\na This is the number of cookbooks in the Chef Supermarket .\nb This is the number of modules in Puppet Forge .\nc This is the number of reusable roles in Ansible Galaxy .\nd This is the number of packages in the Pulumi Registry .\ne This is the number of templates in AWS Quick Starts .\nf I could not find  any collections of community Heat templates.\ng This is the number of modules in the Terraform Registry .\nObviously, this is not a perfect apples-to-apples comparison. For example, some of\nthe tools have more than one repository: e.g., Terraform split the provider code (i.e.,\nthe code specific to AWS, Google Cloud, Azure, etc.) out into separate repos in 2017,\nso the preceding table significantly understates activity; some tools offer alternatives\nto Stack Overflow for questions; and so on.\nThat said, a few trends are obvious. First, all of the IaC tools in this comparison are\nopen source and work with many cloud providers, except for CloudFormation, which\nis closed source and works only with AWS. Second, Ansible and Terraform seem to\nbe the clear leads in terms of popularity.\nAnother interesting trend to note is how these numbers have changed since the first\nedition of the book. Table 1-2  shows the percentage change in each of the numbers\nfrom the values I gathered in the first edition back in September 2016. (Note: Pulumi\nis not included in this table, as it wasn’t part of this comparison in the first edition of\nthe book.)\n32 | Chapter 1: Why Terraform",6217
23-Use of Multiple Tools Together.pdf,23-Use of Multiple Tools Together,"Table 1-2. How the IaC communities have changed between September 2016 and June 2022\nSource Cloud Contributors Stars Libraries Stack Overflow\nChef Open All +34% +56% +21% +98%\nPuppet Open All +32% +58% +55% +51%\nAnsible Open All +258% +183% +289% +507%\nCloudFormation Closed AWS ? ? +54%a+1,083%\nHeat Open All +40% +34% 0 +98%\nTerraform Open All +148% +476% +24,003% +10,106%\na In earlier editions of the book, I used CloudFormation templates in the awslabs GitHub repo, but these seem to be gone now,\nso I used AWS Quick Starts in this edition, so the numbers aren’t directly comparable.\nAgain, the data here is not perfect, but it’s good enough to spot a clear trend: Terra‐\nform and Ansible are experiencing explosive growth. The increase in the number of\ncontributors, stars, open source libraries, and Stack Overflow posts is through the\nroof. Both of these tools have large, active communities today, and judging by these\ntrends, it’s likely that they will become even larger in the future.\nMature Versus Cutting Edge\nAnother key factor to consider when picking any technology is maturity. Is this a\ntechnology that has been around for years, where all the usage patterns, best practi‐\nces, problems, and failure modes are well understood? Or is this a new technology\nwhere you’ll have to learn all those hard lessons from scratch? Table 1-3  shows\nthe initial release dates, current version numbers (as of June 2022), and my own\nsubjective perception of the maturity of each of the IaC tools.\nTable 1-3. A comparison of IaC maturity as of June 2022\nInitial release Current version Perceived maturity\nChef 2009 17.10.3 High\nPuppet 2005 7.17.0 High\nAnsible 2012 5.9.0 Medium\nPulumi 2017 3.34.1 Low\nCloudFormation 2011 ??? Medium\nHeat 2012 18.0.0 Low\nTerraform 2014 1.2.3 Medium\nAgain, this is not an apples-to-apples comparison: age alone does not determine\nmaturity—neither does a high version number (different tools have different version‐\ning schemes). Still, some trends are clear. Pulumi is the youngest IaC tool in this\ncomparison and, arguably, the least mature: this becomes apparent when you search\nfor documentation, best practices, community modules, etc. Terraform is a bit more\nHow Does Terraform Compare to Other IaC Tools? | 33\nmature these days: the tooling has improved, the best practices are better understood,\nthere are far more learning resources available (including this book!), and now that it\nhas reached the 1.0.0 milestone, it is a considerably more stable and reliable tool than\nwhen the first and second editions of this book came out. Chef and Puppet are the\noldest and arguably most mature tools on this list.\nUse of Multiple Tools Together\nAlthough I’ve been comparing IaC tools this entire chapter, the reality is that you will\nlikely need to use multiple tools to build your infrastructure. Each of the tools you’ve\nseen has strengths and weaknesses, so it’s your job to pick the right tools for the job.\nThe following sections show three common combinations I’ve seen work well at a\nnumber of companies.\nProvisioning plus configuration  management\nExample: Terraform and Ansible. Y ou use Terraform to deploy all the underlying\ninfrastructure, including the network topology (i.e., virtual private clouds [VPCs],\nsubnets, route tables), data stores (e.g., MySQL, Redis), load balancers, and servers.\nY ou then use Ansible to deploy your apps on top of those servers, as depicted in\nFigure 1-9 .\nFigure 1-9. Terraform deploys the infrastructure, including servers, and Ansible deploys\napps onto those servers.\nThis is an easy approach to get started with, because there is no extra infrastructure\nto run (Terraform and Ansible are both client-only applications), and there are many\nways to get Ansible and Terraform to work together (e.g., Terraform adds special tags\nto your servers, and Ansible uses those tags to find the servers and configure them).\nThe major downside is that using Ansible typically means that you’re writing a lot of\nprocedural code, with mutable servers, so as your codebase, infrastructure, and team\ngrow, maintenance can become more difficult.\n34 | Chapter 1: Why Terraform\nProvisioning plus server templating\nExample: Terraform and Packer. Y ou use Packer to package your apps as VM images.\nY ou then use Terraform to deploy servers with these VM images and the rest of your\ninfrastructure, including the network topology (i.e., VPCs, subnets, route tables), data\nstores (e.g., MySQL, Redis), and load balancers, as illustrated in Figure 1-10 .\nFigure 1-10. Terraform deploys the infrastructure, including servers, and Packer creates\nthe VMs that run on those servers.\nThis is also an easy approach to get started with, because there is no extra infrastruc‐\nture to run (Terraform and Packer are both client-only applications), and you’ll get\nplenty of practice deploying VM images using Terraform later in this book. Moreover,\nthis is an immutable infrastructure approach, which will make maintenance easier.\nHowever, there are two major drawbacks. First, VMs can take a long time to build\nand deploy, which will slow down your iteration speed. Second, as you’ll see in\nlater chapters, the deployment strategies you can implement with Terraform are\nlimited (e.g., you can’t implement blue-green deployment natively in Terraform), so\nyou either end up writing lots of complicated deployment scripts or you turn to\norchestration tools, as described next.\nProvisioning plus server templating plus orchestration\nExample: Terraform, Packer, Docker, and Kubernetes. Y ou use Packer to create a VM\nimage that has Docker and Kubernetes agents installed. Y ou then use Terraform to\ndeploy a cluster of servers, each of which runs this VM image, and the rest of your\ninfrastructure, including the network topology (i.e., VPCs, subnets, route tables),\ndata stores (e.g., MySQL, Redis), and load balancers. Finally, when the cluster of\nservers boots up, it forms a Kubernetes cluster that you use to run and manage your\nDockerized applications, as shown in Figure 1-11 .\nHow Does Terraform Compare to Other IaC Tools? | 35",6167
24-Conclusion.pdf,24-Conclusion,"Figure 1-11. Terraform deploys the infrastructure, including servers; Packer creates the\nVMs that run on those servers; and Kubernetes manages those VMs as a cluster for\nrunning Docker containers.\nThe advantage of this approach is that Docker images build fairly quickly, you can\nrun and test them on your local computer, and you can take advantage of all of the\nbuilt-in functionality of Kubernetes, including various deployment strategies, auto\nhealing, auto scaling, and so on. The drawback is the added complexity, both in\nterms of extra infrastructure to run (Kubernetes clusters are difficult and expensive\nto deploy and operate, though most major cloud providers now provide managed\nKubernetes services, which can offload some of this work) and in terms of several\nextra layers of abstraction (Kubernetes, Docker, Packer) to learn, manage, and debug.\nY ou’ll see an example of this approach in Chapter 7 .\nConclusion\nPutting it all together, Table 1-4  shows how the most popular IaC tools stack up. Note\nthat this table shows the default  or most common  way the various IaC tools are used,\nthough as discussed earlier in this chapter, these IaC tools are flexible enough to be\nused in other configurations, too (e.g., you can use Chef without a master, you can\nuse Puppet to do immutable infrastructure, etc.).\nTable 1-4. A comparison of the most common ways to use the most popular IaC tools\nChef Puppet Ansible Pulumi CloudFormation Heat Terraform\nSource Open Open Open Open Closed Open Open\nCloud All All All All AWS All All\nType Config  mgmt Config  mgmt Config  mgmt Provisioning Provisioning Provisioning Provisioning\nInfra Mutable Mutable Mutable Immutable Immutable Immutable Immutable\nParadigm Procedural Declarative Procedural Declarative Declarative Declarative Declarative\nLanguage GPL DSL DSL GPL DSL DSL DSL\n36 | Chapter 1: Why Terraform\nChef Puppet Ansible Pulumi CloudFormation Heat Terraform\nMaster Yes Yes No No No No No\nAgent Yes Yes No No No No No\nPaid Service Optional Optional Optional Must-have N/A N/A Optional\nCommunity Large Large Huge Small Small Small Huge\nMaturity High High Medium Low Medium Low Medium\nAt Gruntwork, what we wanted was an open source, cloud-agnostic provisioning tool\nwith a large community, a mature codebase, and support for immutable infrastruc‐\nture, a declarative language, a masterless and agentless architecture, and an optional\npaid service. Table 1-4  shows that Terraform, although not perfect, comes the closest\nto meeting all of our criteria.\nDoes Terraform fit your criteria? If so, head over to Chapter 2  to learn how to use it.\nConclusion | 37",2654
25-Setting Up Your AWS Account.pdf,25-Setting Up Your AWS Account,"1Source: “Global Cloud Services Spend Hits Record US$49.4 Billion in Q3 2021” , Canalys, October 28, 2021.\n2If you find the AWS terminology confusing, be sure to check out Amazon Web Services in Plain English .CHAPTER 2\nGetting Started with Terraform\nIn this chapter, you’re going to learn the basics of how to use Terraform. It’s an easy\ntool to learn, so in the span of about 40 pages, you’ll go from running your first\nTerraform commands all the way up to using Terraform to deploy a cluster of servers\nwith a load balancer that distributes traffic across them. This infrastructure is a\ngood starting point for running scalable, highly available web services. In subsequent\nchapters, you’ll develop this example even further.\nTerraform can provision infrastructure across public cloud providers such as AWS,\nAzure, Google Cloud, and DigitalOcean, as well as private cloud and virtualization\nplatforms such as OpenStack and VMware. For just about all of the code examples in\nthis chapter and the rest of the book, you are going to use AWS. AWS is a good choice\nfor learning Terraform because of the following:\n•AWS is the most popular cloud infrastructure provider, by far. It has a 32% share•\nin the cloud infrastructure market, which is more than the next three biggest\ncompetitors (Microsoft, Google, and IBM) combined.1\n•AWS provides a huge range of reliable and scalable cloud-hosting services,•\nincluding Amazon Elastic Compute Cloud (Amazon EC2), which you can use\nto deploy virtual servers; Auto Scaling Groups (ASGs), which make it easier to\nmanage a cluster of virtual servers; and Elastic Load Balancers (ELBs), which you\ncan use to distribute traffic across the cluster of virtual servers.2\n39\n3Check out the AWS Free Tier documentation  for details.\n4For more details on AWS user management best practices, see the documentation .\n•AWS offers a Free Tier for the first year that should allow you to run all of these•\nexamples for free or a very low cost.3 If you already used your Free Tier credits,\nthe examples in this book should still cost you no more than a few dollars.\nIf you’ve never used AWS or Terraform before, don’t worry; this tutorial is designed\nfor novices to both technologies. I’ll walk you through the following steps:\n•Setting up your AWS account•\n•Installing Terraform•\n•Deploying a single server•\n•Deploying a single web server•\n•Deploying a configurable web server•\n•Deploying a cluster of web servers•\n•Deploying a load balancer•\n•Cleaning up•\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\nSetting Up Your AWS Account\nIf you don’t already have an AWS account, head over to https://aws.amazon.com  and\nsign up. When you first register for AWS, you initially sign in as  the root user . This\nuser account has access permissions to do absolutely anything in the account, so\nfrom a security perspective, it’s not a good idea to use the root user on a day-to-day\nbasis. In fact, the only thing you should use the root user for is to create other user\naccounts with more-limited permissions, and then switch to one of those accounts\nimmediately.4\nTo create a more-limited user account, you will need to use the Identity and Access\nManagement  (IAM) service. IAM is where you manage user accounts as well as the\npermissions for each user. To create a new IAM user , go to the IAM Console , click\nUsers, and then click the Add Users button. Enter a name for the user, and make\nsure “ Access key - Programmatic access” is selected, as shown in Figure 2-1  (note\n40 | Chapter 2: Getting Started with Terraform\n5Y ou can learn more about IAM Policies on the AWS website .\n6I’m assuming that you’re running the examples in this book in an AWS account dedicated solely to learning\nand testing so that the broad permissions of the AdministratorAccess  Managed Policy are not a big\nrisk. If you are running these examples in a more sensitive environment—which, for the record, I don’t\nrecommend!—and you’re comfortable with creating custom IAM Policies, you can find a more pared-down\nset of permissions in this book’s code examples repo .that AWS occasionally makes changes to its web console, so what you see may look\nslightly different than the screenshots in this book).\nFigure 2-1. Use the AWS Console to create a new IAM user.\nClick the Next button. AWS will ask you to add permissions to the user. By default,\nnew IAM users have no permissions whatsoever and cannot do anything in an AWS\naccount. To give your IAM user the ability to do something, you need to associate one\nor more IAM Policies with that user’s account. An IAM Policy  is a JSON document\nthat defines what a user is or isn’t allowed to do. Y ou can create your own IAM\nPolicies or use some of the predefined IAM Policies built into your AWS account,\nwhich are known as Managed Policies .5\nTo run the examples in this book, the easiest way to get started is to add the\nAdministratorAccess  Managed Policy to your IAM user (search for it, and click\nthe checkbox next to it), as shown in Figure 2-2 .6\nSetting Up Your AWS Account | 41\nFigure 2-2. Add the AdministratorAccess  Managed IAM Policy to your new IAM user.\nClick Next a couple more times and then the “Create user” button. AWS will show\nyou the security credentials for that user, which consist of an Access Key ID  and a\nSecret Access Key , as shown in Figure 2-3 . Y ou must save these immediately because\nthey will never be shown again, and you’ll need them later on in this tutorial.\nRemember that these credentials give access to your AWS account, so store them\nsomewhere secure (e.g., a password manager such as 1Password, LastPass, or macOS\nKeychain), and never share them with anyone.\nAfter you’ve saved your credentials, click the Close button. Y ou’re now ready to move\non to using Terraform.\nA Note on Default Virtual Private Clouds\nAll of the AWS examples in this book use the Default VPC  in\nyour AWS account. A VPC , or virtual private cloud, is an isolated\narea of your AWS account that has its own virtual network and IP\naddress space. Just about every AWS resource deploys into a VPC.\nIf you don’t explicitly specify a VPC, the resource will be deployed\ninto the Default VPC, which is part of every AWS account created\nafter 2013. If for some reason you deleted the Default VPC in\nyour account, either use a different region (each region has its own\nDefault VPC) or create a new Default VPC using the AWS Web\nConsole . Otherwise, you’ll need to update almost every example\nto include a vpc_id  or subnet_id  parameter pointing to a custom\nVPC.\n42 | Chapter 2: Getting Started with Terraform",6703
26-Deploying a Single Server.pdf,26-Deploying a Single Server,"Figure 2-3. Store your AWS credentials somewhere secure. Never share them with\nanyone. (Don’t worry, the ones in the screenshot are fake.)\nInstalling Terraform\nThe easiest way to install Terraform is to use your operating system’s package man‐\nager. For example, on macOS, if you are a Homebrew user, you can run the following:\n$ brew tap hashicorp/tap\n$ brew install hashicorp/tap/terraform\nOn Windows, if you’re a Chocolatey user, you can run the following:\n$ choco install terraform\nCheck the Terraform documentation  for installation instructions on other operating\nsystems, including the various flavors of Linux.\nAlternatively, you can install Terraform manually by going to the Terraform home\npage , clicking the download link, selecting the appropriate package for your operating\nsystem, downloading the ZIP archive, and unzipping it into the directory where\nyou want Terraform to be installed. The archive will extract a single binary called\nterraform , which you’ll want to add to your PATH  environment variable.\nTo check whether things are working, run the terraform  command, and you should\nsee the usage instructions:\n$ terraform\nUsage: terraform [global options] <subcommand> [args]\nThe available commands for execution are listed below.\nThe primary workflow commands are given first, followed by\nless common or more advanced commands.\nInstalling Terraform | 43\n7Y ou can also write Terraform code in pure JSON in files with the extension .tf.json . Y ou can learn more about\nTerraform’s HCL and JSON syntax in the Terraform documentation .\nMain commands:\n  init          Prepare your working directory for other commands\n  validate      Check whether the configuration is valid\n  plan          Show changes required by the current configuration\n  apply         Create or update infrastructure\n  destroy       Destroy previously-created infrastructure\n(...)\nFor Terraform to be able to make changes in your AWS account, you will need to\nset the AWS credentials for the IAM user you created earlier as the environment\nvariables AWS_ACCESS_KEY_ID  and AWS_SECRET_ACCESS_KEY . For example, here is how\nyou can do it in a Unix/Linux/macOS terminal:\n$ export AWS_ACCESS_KEY_ID=(your access key id)\n$ export AWS_SECRET_ACCESS_KEY=(your secret access key)\nAnd here is how you can do it in a Windows command terminal:\n$ set AWS_ACCESS_KEY_ID=(your access key id)\n$ set AWS_SECRET_ACCESS_KEY=(your secret access key)\nNote that these environment variables apply only to the current shell, so if you reboot\nyour computer or open a new terminal window, you’ll need to export these variables\nagain.\nOther AWS Authentication Options\nIn addition to environment variables, Terraform supports the same\nauthentication mechanisms as all AWS CLI and SDK tools. There‐\nfore, it’ll also be able to use credentials in $HOME/.aws/credentials ,\nwhich are automatically generated if you run aws configure , or\nIAM roles, which you can add to almost any resource in AWS.\nFor more info, see A Comprehensive Guide to Authenticating to\nAWS on the Command Line . Y ou’ll also see more information on\nauthenticating to Terraform providers in Chapter 6 .\nDeploying a Single Server\nTerraform code is written in the HashiCorp Configuration  Language  (HCL) in files\nwith the extension .tf.7 It is a declarative language, so your goal is to describe the\ninfrastructure you want, and Terraform will figure out how to create it. Terraform\ncan create infrastructure across a wide variety of platforms, or what it calls providers ,\nincluding AWS, Azure, Google Cloud, DigitalOcean, and many others.\n44 | Chapter 2: Getting Started with Terraform\n8Y ou can learn more about AWS regions and Availability Zones on the AWS website .Y ou can write Terraform code in just about any text editor. If you search around,\nyou can find Terraform syntax highlighting support for most editors (note that you\nmay have to search for the word HCL  instead of Terraform ), including vim, emacs,\nSublime Text, Atom, Visual Studio Code, and IntelliJ (the latter even has support for\nrefactoring, find usages, and go to declaration).\nThe first step to using Terraform is typically to configure the provider(s) you want\nto use. Create an empty folder and put a file in it called main.tf  that contains the\nfollowing contents:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nThis tells Terraform that you are going to be using AWS as your provider and that\nyou want to deploy your infrastructure into the us-east-2  region. AWS has datacen‐\nters all over the world, grouped into regions. An AWS region  is a separate geographic\narea, such as us-east-2  (Ohio), eu-west-1  (Ireland), and ap-southeast-2  (Sydney).\nWithin each region, there are multiple isolated datacenters known as Availability\nZones  (AZs), such as us-east-2a , us-east-2b , and so on.8 There are many other\nsettings you can configure on this provider, but for now, let’s keep it simple, and we’ll\ntake a deeper look at provider configuration in Chapter 7 .\nFor each type of provider, there are many different kinds of resources  that you can\ncreate, such as servers, databases, and load balancers. The general syntax for creating\na resource in Terraform is as follows:\nresource  ""<PROVIDER>_<TYPE>"" ""<NAME>""  {\n  [CONFIG ...]\n}\nwhere PROVIDER  is the name of a provider (e.g., aws), TYPE  is the type of resource to\ncreate in that provider (e.g., instance ), NAME  is an identifier you can use throughout\nthe Terraform code to refer to this resource (e.g., my_instance ), and CONFIG  consists\nof one or more arguments  that are specific to that resource.\nFor example, to deploy a single (virtual) server in AWS, known as an EC2 Instance ,\nuse the aws_instance  resource in main.tf  as follows:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nThe aws_instance  resource supports many different arguments, but for now, you\nonly need to set the two required ones:\nDeploying a Single Server | 45\n9Finding AMI IDs is surprisingly complicated, as documented in this Gruntwork blog post .\nami\nThe Amazon Machine Image (AMI) to run on the EC2 Instance. Y ou can find\nfree and paid AMIs in the AWS Marketplace  or create your own using tools\nsuch as Packer. The preceding code sets the ami parameter to the ID of an\nUbuntu 20.04 AMI in us-east-2 . This AMI is free to use. Please note that AMI\nIDs are different in every AWS region, so if you change the region  parameter\nto something other than us-east-2 , you’ll need to manually look up the corre‐\nsponding Ubuntu AMI ID for that region,9 and copy it into the ami parameter. In\nChapter 7 , you’ll see how to fetch the AMI ID completely automatically.\ninstance_type\nThe type of EC2 Instance to run. Each type of EC2 Instance provides a different\namount of CPU, memory, disk space, and networking capacity. The EC2 Instance\nTypes  page lists all the available options. The preceding example uses t2.micro ,\nwhich has one virtual CPU, 1 GB of memory, and is part of the AWS Free Tier.\nUse the Docs!\nTerraform  supports dozens of providers, each of which supports\ndozens of resources, and each resource has dozens of arguments.\nThere is no way to remember them all. When you’re writing Ter‐\nraform code, you should be regularly referring to the Terraform\ndocumentation to look up what resources are available and how\nto use each one. For example, here’s the documentation for the\naws_instance  resource . I’ve been using Terraform for years, and I\nstill refer to these docs multiple times per day!\nIn a terminal, go into the folder where you created main.tf  and run the terraform\ninit  command:\n$ terraform init\nInitializing the backend...\nInitializing provider plugins...\n- Reusing previous version of hashicorp/aws from the dependency lock file\n- Using hashicorp/aws v4.19.0 from the shared cache directory\nTerraform has been successfully initialized!\nThe terraform  binary contains the basic functionality for Terraform, but it does not\ncome with the code for any of the providers (e.g., the AWS Provider, Azure provider,\nGCP provider, etc.), so when you’re first starting to use Terraform, you need to\n46 | Chapter 2: Getting Started with Terraform\nrun terraform init  to tell Terraform to scan the code, figure out which providers\nyou’re using, and download the code for them. By default, the provider code will\nbe downloaded into a .terraform  folder, which is Terraform’s scratch directory (you\nmay want to add it to .gitignore ). Terraform will also record information about the\nprovider code it downloaded into a .terraform.lock.hcl  file (you’ll learn more about\nthis file in “Versioned Modules” on page 298). Y ou’ll see a few other uses for the init\ncommand and .terraform  folder in later chapters. For now, just be aware that you\nneed to run init  anytime you start with new Terraform code and that it’s safe to run\ninit  multiple times (the command is idempotent).\nNow that you have the provider code downloaded, run the terraform plan\ncommand:\n$ terraform plan\n(...)\nTerraform will perform the following actions:\n  # aws_instance.example will be created\n  + resource ""aws_instance"" ""example"" {\n      + ami                          = ""ami-0fb653ca2d3203ac1""\n      + arn                          = (known after apply)\n      + associate_public_ip_address  = (known after apply)\n      + availability_zone            = (known after apply)\n      + cpu_core_count               = (known after apply)\n      + cpu_threads_per_core         = (known after apply)\n      + get_password_data            = false\n      + host_id                      = (known after apply)\n      + id                           = (known after apply)\n      + instance_state               = (known after apply)\n      + instance_type                = ""t2.micro""\n      + ipv6_address_count           = (known after apply)\n      + ipv6_addresses               = (known after apply)\n      + key_name                     = (known after apply)\n      (...)\n  }\nPlan: 1 to add, 0 to change, 0 to destroy.\nThe plan  command lets you see what Terraform will do before actually making any\nchanges. This is a great way to sanity-check your code before unleashing it onto\nthe world. The output of the plan  command is similar to the output of the diff\ncommand that is part of Unix, Linux, and git: anything with a plus sign (+) will be\ncreated, anything with a minus sign (–) will be deleted, and anything with a tilde sign\n(~) will be modified in place. In the preceding output, you can see that Terraform is\nplanning on creating a single EC2 Instance and nothing else, which is exactly what\nyou want.\nDeploying a Single Server | 47\nTo actually create the Instance, run the terraform apply  command:\n$ terraform apply\n(...)\nTerraform will perform the following actions:\n  # aws_instance.example will be created\n  + resource ""aws_instance"" ""example"" {\n      + ami                          = ""ami-0fb653ca2d3203ac1""\n      + arn                          = (known after apply)\n      + associate_public_ip_address  = (known after apply)\n      + availability_zone            = (known after apply)\n      + cpu_core_count               = (known after apply)\n      + cpu_threads_per_core         = (known after apply)\n      + get_password_data            = false\n      + host_id                      = (known after apply)\n      + id                           = (known after apply)\n      + instance_state               = (known after apply)\n      + instance_type                = ""t2.micro""\n      + ipv6_address_count           = (known after apply)\n      + ipv6_addresses               = (known after apply)\n      + key_name                     = (known after apply)\n      (...)\n  }\nPlan: 1 to add, 0 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value:\nY ou’ll notice that the apply  command shows you the same plan  output and asks you\nto confirm whether you actually want to proceed with this plan. So, while plan  is\navailable as a separate command, it’s mainly useful for quick sanity checks and during\ncode reviews (a topic you’ll see more of in Chapter 10 ), and most of the time you’ll\nrun apply  directly and review the plan output it shows you.\nType yes and hit Enter to deploy the EC2 Instance:\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value: yes\naws_instance.example: Creating...\naws_instance.example: Still creating... [10s elapsed]\naws_instance.example: Still creating... [20s elapsed]\n48 | Chapter 2: Getting Started with Terraform\naws_instance.example: Still creating... [30s elapsed]\naws_instance.example: Creation complete after 38s [id=i-07e2a3e006d785906]\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nCongrats, you’ve just deployed an EC2 Instance in your AWS account using Terra‐\nform! To verify this, head over to the EC2 console , and you should see something\nsimilar to Figure 2-4 .\nFigure 2-4. The AWS Console shows the EC2 Instance you deployed.\nSure enough, the Instance is there, though admittedly, this isn’t the most exciting\nexample. Let’s make it a bit more interesting. First, notice that the EC2 Instance\ndoesn’t have a name. To add one, you can add tags  to the aws_instance  resource:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n  tags = {\n    Name  = ""terraform-example""\n  }\n}\nRun terraform apply  again to see what this would do:\n$ terraform apply\naws_instance.example: Refreshing state...\n(...)\nDeploying a Single Server | 49\nTerraform will perform the following actions:\n  # aws_instance.example will be updated in-place\n  ~ resource ""aws_instance"" ""example"" {\n        ami                          = ""ami-0fb653ca2d3203ac1""\n        availability_zone            = ""us-east-2b""\n        instance_state               = ""running""\n        (...)\n      + tags                         = {\n          + ""Name"" = ""terraform-example""\n        }\n        (...)\n    }\nPlan: 0 to add, 1 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value:\nTerraform keeps track of all the resources it already created for this set of configu‐\nration files, so it knows your EC2 Instance already exists (notice Terraform says\nRefreshing state…  when you run the apply  command), and it can show you a diff\nbetween what’s currently deployed and what’s in your Terraform code (this is one of\nthe advantages of using a declarative language over a procedural one, as discussed in\n“How Does Terraform Compare to Other IaC Tools?”  on page 20 ). The preceding diff\nshows that Terraform wants to create a single tag called Name , which is exactly what\nyou need, so type yes and hit Enter.\nWhen you refresh your EC2 console, you’ll see something similar to Figure 2-5 .\n50 | Chapter 2: Getting Started with Terraform\nFigure 2-5. The EC2 Instance now has a name tag.\nNow that you have some working Terraform code, you may want to store it in version\ncontrol. This allows you to share your code with other team members, track the\nhistory of all infrastructure changes, and use the commit log for debugging. For\nexample , here is how you can create a local Git repository and use it to store your\nTerraform configuration file and the lock file (you’ll learn all about the lock file in\nChapter 8 ; for now, all you need to know is it should be added to version control\nalong with your code):\ngit init\ngit add main.tf .terraform.lock.hcl\ngit commit -m ""Initial commit""\nY ou should also create a .gitignore  file with the following contents:\n.terraform\n*.tfstate\n*.tfstate.backup\nThe preceding .gitignore  file instructs Git to ignore the .terraform  folder, which Terra‐\nform uses as a temporary scratch directory, as well as *.tfstate  files, which Terraform\nuses to store state (in Chapter 3 , you’ll see why state files shouldn’t be checked in).\nY ou should commit the .gitignore  file, too:\ngit add .gitignore\ngit commit -m ""Add a .gitignore file""\nTo share this code with your teammates, you’ll want to create a shared Git repository\nthat you can all access. One way to do this is to use GitHub. Head over to GitHub ,\nDeploying a Single Server | 51",16644
27-Deploying a Single Web Server.pdf,27-Deploying a Single Web Server,"create an account if you don’t have one already, and create a new repository. Config‐\nure your local Git repository to use the new GitHub repository as a remote endpoint\nnamed origin , as follows:\ngit remote add origin git@github.com:<YOUR_USERNAME>/<YOUR_REPO_NAME>.git\nNow, whenever you want to share your commits with your teammates, you can push\nthem to origin :\ngit push origin main\nAnd whenever you want to see changes your teammates have made, you can pull\nthem from origin :\ngit pull origin main\nAs you go through the rest of this book, and as you use Terraform in general, make\nsure to regularly git commit  and git push  your changes. This way, you’ll not only\nbe able to collaborate with team members on this code, but all of your infrastructure\nchanges will also be captured in the commit log, which is very handy for debugging.\nY ou’ll learn more about using Terraform as a team in Chapter 10 .\nDeploying a Single Web Server\nThe next step is to run a web server on this Instance. The goal is to deploy the\nsimplest web architecture possible: a single web server that can respond to HTTP\nrequests, as shown in Figure 2-6 .\nFigure 2-6. Start with a simple architecture: a single web server running in AWS that\nresponds to HTTP requests.\n52 | Chapter 2: Getting Started with Terraform\n10Y ou can find a handy list of HTTP server one-liners on GitHub .\nIn a real-world use case, you’ d probably build the web server using a web framework\nlike Ruby on Rails or Django, but to keep this example simple, let’s run a dirt-simple\nweb server that always returns the text “Hello, World”:10\n#!/bin/bash\necho ""Hello, World""  > index.html\nnohup busybox httpd -f -p 8080 &\nThis is a Bash script that writes the text “Hello, World” into index.html  and runs a\ntool called busybox  (which is installed by default on Ubuntu) to fire up a web server\non port 8080 to serve that file. I wrapped the busybox  command with nohup  and an\nampersand ( &) so that the web server runs permanently in the background, whereas\nthe Bash script itself can exit.\nPort Numbers\nThe reason this example uses port 8080, rather than the default\nHTTP port 80, is that listening on any port less than 1024 requires\nroot user privileges. This is a security risk since any attacker who\nmanages to compromise your server would get root privileges, too.\nTherefore, it’s a best practice to run your web server with a non-\nroot user that has limited permissions. That means you have to\nlisten on higher-numbered ports, but as you’ll see later in this\nchapter, you can configure a load balancer to listen on port 80 and\nroute traffic to the high-numbered ports on your server(s).\nHow do you get the EC2 Instance to run this script? Normally, as discussed in “Server\nTemplating Tools” on page 7 , you would use a tool like Packer to create a custom AMI\nthat has the web server installed on it. Since the dummy web server in this example\nis just a one-liner that uses busybox , you can use a plain Ubuntu 20.04 AMI and run\nthe “Hello, World” script as part of the EC2 Instance’s User Data  configuration. When\nyou launch an EC2 Instance, you have the option of passing either a shell script or\ncloud-init directive to User Data, and the EC2 Instance will execute it during its very\nfirst boot. Y ou pass a shell script to User Data by setting the user_data  argument in\nyour Terraform code as follows:\nresource  ""aws_instance"" ""example""  {\n  ami                    = ""ami-0fb653ca2d3203ac1""\n  instance_type           = ""t2.micro""\n  user_data  = <<-EOF\n              #!/bin/bash\n              echo ""Hello, World""  > index.html\n              nohup busybox httpd -f -p 8080 &\nDeploying a Single Web Server | 53\n11To learn more about how CIDR works, see its Wikipedia page . For a handy calculator that converts between\nIP address ranges and CIDR notation, use either https://cidr.xyz/  in your browser or install the ipcalc\ncommand in your terminal.              EOF\n  user_data_replace_on_change  = true\n  tags = {\n    Name  = ""terraform-example""\n  }\n}\nTwo things to notice about the preceding code:\n•The <<-EOF  and EOF are Terraform’s heredoc  syntax, which allows you to create •\nmultiline strings without having to insert \n characters all over the place.\n•The user_data_replace_on_change  parameter  is set to true  so that when you •\nchange the user_data  parameter and run apply , Terraform will terminate the\noriginal instance and launch a totally new one. Terraform’s default behavior is to\nupdate the original instance in place, but since User Data runs only on the very\nfirst boot, and your original instance already went through that boot process, you\nneed to force the creation of a new instance to ensure your new User Data script\nactually gets executed.\nY ou need to do one more thing before this web server works. By default, AWS does\nnot allow any incoming or outgoing traffic from an EC2 Instance. To allow the EC2\nInstance to receive traffic on port 8080, you need to create a security group :\nresource  ""aws_security_group"" ""instance""  {\n  name = ""terraform-example-instance""\n  ingress {\n    from_port    = 8080\n    to_port      = 8080\n    protocol     = ""tcp""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n}\nThis code creates a new resource called aws_security_group  (notice how all\nresources for the AWS Provider begin with aws_ ) and specifies that this group allows\nincoming TCP requests on port 8080 from the CIDR block 0.0.0.0/0. CIDR blocks  are\na concise way to specify IP address ranges. For example, a CIDR block of 10.0.0.0/24\nrepresents all IP addresses between 10.0.0.0 and 10.0.0.255. The CIDR block 0.0.0.0/0\nis an IP address range that includes all possible IP addresses, so this security group\nallows incoming requests on port 8080 from any IP .11\n54 | Chapter 2: Getting Started with Terraform\nSimply creating a security group isn’t enough; you need to tell the EC2 Instance\nto actually use it by passing the ID of the security group into the vpc_security\n_group_ids  argument of the aws_instance  resource. To do that, you first need to\nlearn about Terraform expressions .\nAn expression in Terraform is anything that returns a value. Y ou’ve already seen the\nsimplest type of expressions, literals , such as strings (e.g., ""ami-0fb653ca2d3203ac1"" )\nand numbers (e.g., 5). Terraform supports many other types of expressions that you’ll\nsee throughout the book.\nOne particularly useful type of expression is a reference , which allows you to access\nvalues from other parts of your code. To access the ID of the security group resource,\nyou are going to need to use a resource attribute reference , which uses the following\nsyntax:\n<PROVIDER>_<TYPE> .<NAME>.<ATTRIBUTE>\nwhere PROVIDER  is the name of the provider (e.g., aws), TYPE  is the type of resource\n(e.g., security_group ), NAME  is the name of that resource (e.g., the security group is\nnamed ""instance"" ), and ATTRIBUTE  is either one of the arguments of that resource\n(e.g., name ) or one of the attributes exported  by the resource (you can find the list\nof available attributes in the documentation for each resource). The security group\nexports an attribute called id, so the expression to reference it will look like this:\naws_security_group.instance.id\nY ou can use this security group ID in the vpc_security_group_ids  argument of the\naws_instance  as follows:\nresource  ""aws_instance"" ""example""  {\n  ami                    = ""ami-0fb653ca2d3203ac1""\n  instance_type           = ""t2.micro""\n  vpc_security_group_ids  = [aws_security_group.instance.id ]\n  user_data  = <<-EOF\n              #!/bin/bash\n              echo ""Hello, World""  > index.html\n              nohup busybox httpd -f -p 8080 &\n              EOF\n  user_data_replace_on_change  = true\n  tags = {\n    Name  = ""terraform-example""\n  }\n}\nWhen you add a reference from one resource to another, you create  an implicit\ndependency . Terraform parses these dependencies, builds a dependency graph from\nthem, and uses that to automatically determine in which order it should create\nDeploying a Single Web Server | 55\n12Note that while the graph  command can be useful for visualizing the relationships between a small number of\nresources, with dozens or hundreds of resources, the graphs tend to become too large and messy to be useful.resources. For example, if you were deploying this code from scratch, Terraform\nwould know that it needs to create the security group before the EC2 Instance,\nbecause the EC2 Instance references the ID of the security group. Y ou can even get\nTerraform to show you the dependency graph by running the graph  command:\n$ terraform graph\ndigraph {\ncompound = ""true""\nnewrank = ""true""\nsubgraph ""root"" {\n""[root] aws_instance.example""\n  [label = ""aws_instance.example"", shape = ""box""]\n""[root] aws_security_group.instance""\n  [label = ""aws_security_group.instance"", shape = ""box""]\n""[root] provider.aws""\n  [label = ""provider.aws"", shape = ""diamond""]\n""[root] aws_instance.example"" ->\n  ""[root] aws_security_group.instance""\n""[root] aws_security_group.instance"" ->\n  ""[root] provider.aws""\n""[root] meta.count-boundary (EachMode fixup)"" ->\n  ""[root] aws_instance.example""\n""[root] provider.aws (close)"" ->\n  ""[root] aws_instance.example""\n""[root] root"" ->\n  ""[root] meta.count-boundary (EachMode fixup)""\n""[root] root"" ->\n  ""[root] provider.aws (close)""\n}\n}\nThe output is in a graph description language called DOT, which you can turn into an\nimage, similar to the dependency graph shown in Figure 2-7 , by using a desktop app\nsuch as Graphviz or web app like GraphvizOnline .12\n56 | Chapter 2: Getting Started with Terraform\nFigure 2-7. This is what the dependency graph for the EC2 Instance and its security\ngroup looks like when rendered with Graphviz.\nWhen Terraform walks your dependency tree, it creates as many resources in parallel\nas it can, which means that it can apply your changes fairly efficiently. That’s the\nbeauty of a declarative language: you just specify what you want, and Terraform\ndetermines the most efficient way to make it happen.\nIf you run the apply  command, you’ll see that Terraform wants to create a security\ngroup and replace the EC2 Instance with a new one that has the new user data:\n$ terraform apply\n(...)\nTerraform will perform the following actions:\n  # aws_instance.example must be replaced\n-/+ resource ""aws_instance"" ""example"" {\n        ami                          = ""ami-0fb653ca2d3203ac1""\n      ~ availability_zone            = ""us-east-2c"" -> (known after apply)\n      ~ instance_state               = ""running"" -> (known after apply)\n        instance_type                = ""t2.micro""\n        (...)\n      + user_data                    = ""c765373..."" # forces replacement\n      ~ volume_tags                  = {} -> (known after apply)\n      ~ vpc_security_group_ids       = [\n          - ""sg-871fa9ec"",\n        ] -> (known after apply)\n        (...)\n    }\n  # aws_security_group.instance will be created\nDeploying a Single Web Server | 57\n  + resource ""aws_security_group"" ""instance"" {\n      + arn                    = (known after apply)\n      + description            = ""Managed by Terraform""\n      + egress                 = (known after apply)\n      + id                     = (known after apply)\n      + ingress                = [\n          + {\n              + cidr_blocks      = [\n                  + ""0.0.0.0/0"",\n                ]\n              + description      = """"\n              + from_port        = 8080\n              + ipv6_cidr_blocks = []\n              + prefix_list_ids  = []\n              + protocol         = ""tcp""\n              + security_groups  = []\n              + self             = false\n              + to_port          = 8080\n            },\n        ]\n      + name                   = ""terraform-example-instance""\n      + owner_id               = (known after apply)\n      + revoke_rules_on_delete = false\n      + vpc_id                 = (known after apply)\n    }\nPlan: 2 to add, 0 to change, 1 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value:\nThe -/+ in the plan  output means “replace”; look for the text “forces replacement”\nin the plan output to figure out what is forcing Terraform to do a replacement.\nSince you set user_data_replace_on_change  to true  and changed the user_data\nparameter, this will force a replacement, which means that the original EC2 Instance\nwill be terminated and a completely new Instance will be created. This is an example\nof the immutable infrastructure paradigm discussed in “Server Templating Tools”  on\npage 7. It’s worth mentioning that while the web server is being replaced, any users\nof that web server would experience downtime; you’ll see how to do a zero-downtime\ndeployment with Terraform in Chapter 5 .\n58 | Chapter 2: Getting Started with Terraform\nSince the plan looks good, enter yes, and you’ll see your new EC2 Instance deploying,\nas shown in Figure 2-8 .\nFigure 2-8. The new EC2 Instance with the web server code replaces the old Instance.\nIf you click your new Instance, you can find its public IP address in the description\npanel at the bottom of the screen. Give the Instance a minute or two to boot up,\nand then use a web browser or a tool like curl  to make an HTTP request to this IP\naddress at port 8080:\n$ curl http://<EC2_INSTANCE_PUBLIC_IP>:8080\nHello, World\nY ay! Y ou now have a working web server running in AWS!\nNetwork Security\nTo keep all of the examples in this book simple, they deploy not only into your\nDefault VPC (as mentioned earlier) but also into the default subnets  of that VPC. A\nVPC is partitioned into one or more subnets, each with its own IP addresses. The\nsubnets in the Default VPC are all public subnets , which means they get IP addresses\nthat are accessible from the public internet. This is why you are able to test your EC2\nInstance from your home computer.\nRunning a server in a public subnet is fine for a quick experiment, but in real-world\nusage, it’s a security risk. Hackers all over the world are constantly  scanning IP\naddresses at random for any weakness. If your servers are exposed publicly, all it takes\nDeploying a Single Web Server | 59",14422
28-Deploying a Configurable Web Server.pdf,28-Deploying a Configurable Web Server,"13From The Pragmatic Programmer  by Andy Hunt and Dave Thomas (Addison-Wesley Professional).is accidentally leaving a single port unprotected or running out-of-date code with a\nknown vulnerability, and someone can break in.\nTherefore, for production systems, you should deploy all of your servers, and cer‐\ntainly all of your data stores, in private subnets , which have IP addresses that can be\naccessed only from within the VPC and not from the public internet. The only servers\nyou should run in public subnets are a small number of reverse proxies and load\nbalancers that you lock down as much as possible (you’ll see an example of how to\ndeploy a load balancer later in this chapter).\nDeploying a Configurable  Web Server\nY ou might have noticed that the web server code has the port 8080 duplicated in both\nthe security group and the User Data configuration. This violates the Don’t Repeat\nYourself (DRY)  principle: every piece of knowledge must have a single, unambiguous,\nauthoritative representation within a system.13 If you have the port number in two\nplaces, it’s easy to update it in one place but forget to make the same change in the\nother place.\nTo allow you to make your code more DRY and more configurable, Terraform allows\nyou to define input variables . Here’s the syntax for declaring a variable:\nvariable  ""NAME"" {\n  [CONFIG ...]\n}\nThe body of the variable declaration can contain the following optional parameters:\ndescription\nIt’s always a good idea to use this parameter to document how a variable is used.\nY our teammates will be able to see this description not only while reading the\ncode but also when running the plan  or apply  commands (you’ll see an example\nof this shortly).\ndefault\nThere are a number of ways to provide a value for the variable, including passing\nit in at the command line (using the -var  option), via a file (using the -var-\nfile  option), or via an environment variable (Terraform looks for environment\nvariables of the name TF_VAR_<variable_name> ). If no value is passed in, the\nvariable will fall back to this default value. If there is no default value, Terraform\nwill interactively prompt the user for one.\n60 | Chapter 2: Getting Started with Terraform\ntype\nThis  allows you to enforce type constraints  on the variables a user passes in.\nTerraform supports a number of type constraints, including string , number ,\nbool , list , map, set, object , tuple , and any. It’s always a good idea to define\na type constraint to catch simple errors. If you don’t specify a type, Terraform\nassumes the type is any.\nvalidation\nThis allows you to define custom validation rules for the input variable that go\nbeyond basic type checks, such as enforcing minimum or maximum values on a\nnumber. Y ou’ll see an example of validations in Chapter 8 .\nsensitive\nIf you set this parameter to true  on an input variable, Terraform will not log it\nwhen you run plan  or apply . Y ou should use this on any secrets you pass into\nyour Terraform code via variables: e.g., passwords, API keys, etc. I’ll talk more\nabout secrets in Chapter 6 .\nHere is an example of an input variable that checks to verify that the value you pass in\nis a number:\nvariable  ""number_example""  {\n  description  = ""An example of a number variable in Terraform""\n  type        = number\n  default      = 42\n}\nAnd here’s an example of a variable that checks whether the value is a list:\nvariable  ""list_example""  {\n  description  = ""An example of a list in Terraform""\n  type        = list\n  default      = [""a"", ""b"", ""c"" ]\n}\nY ou can combine type constraints, too. For example, here’s a list input variable that\nrequires all of the items in the list to be numbers:\nvariable  ""list_numeric_example""  {\n  description  = ""An example of a numeric list in Terraform""\n  type        = list(number)\n  default      = [1, 2, 3]\n}\nAnd here’s a map that requires all of the values to be strings:\nvariable  ""map_example""  {\n  description  = ""An example of a map in Terraform""\n  type        = map(string)\n  default  = {\nDeploying a Configurable  Web Server | 61\n    key1  = ""value1""\n    key2  = ""value2""\n    key3  = ""value3""\n  }\n}\nY ou can also create more complicated structural types  using the object  type\nconstraint:\nvariable  ""object_example""  {\n  description  = ""An example of a structural type in Terraform""\n  type        = object({\n    name     = string\n    age     = number\n    tags     = list(string)\n    enabled  = bool\n  })\n  default  = {\n    name     = ""value1""\n    age     = 42\n    tags     = [""a"", ""b"", ""c"" ]\n    enabled  = true\n  }\n}\nThe preceding example creates an input variable that will require the value to be an\nobject with the keys name  (which must be a string), age (which must be a number),\ntags  (which must be a list of strings), and enabled  (which must be a Boolean). If you\ntry to set this variable to a value that doesn’t match this type, Terraform immediately\ngives you a type error. The following example demonstrates trying to set enabled  to a\nstring instead of a Boolean:\nvariable  ""object_example_with_error""  {\n  description  = ""An example of a structural type in Terraform with an error""\n  type        = object({\n    name     = string\n    age     = number\n    tags     = list(string)\n    enabled  = bool\n  })\n  default  = {\n    name     = ""value1""\n    age     = 42\n    tags     = [""a"", ""b"", ""c"" ]\n    enabled  = ""invalid""\n  }\n}\nY ou get the following error:\n62 | Chapter 2: Getting Started with Terraform\n$ terraform apply\nError: Invalid default value for variable\n  on variables.tf line 78, in variable ""object_example_with_error"":\n  78:   default = {\n  79:     name    = ""value1""\n  80:     age     = 42\n  81:     tags    = [""a"", ""b"", ""c""]\n  82:     enabled = ""invalid""\n  83:   }\nThis default value is not compatible with the variable's type constraint: a\nbool is required.\nComing back to the web server example, what you need is a variable that stores the\nport number:\nvariable  ""server_port""  {\n  description  = ""The port the server will use for HTTP requests""\n  type        = number\n}\nNote that the server_port  input variable has no default , so if you run the\napply  command now, Terraform will interactively prompt you to enter a value for\nserver_port  and show you the description  of the variable:\n$ terraform apply\nvar.server_port\n  The port the server will use for HTTP requests\n  Enter a value:\nIf you don’t want to deal with an interactive prompt, you can provide a value for the\nvariable via the -var  command-line option:\n$ terraform plan -var ""server_port=8080""\nY ou could also set the variable via an environment variable named TF_VAR_<name> ,\nwhere <name>  is the name of the variable you’re trying to set:\n$ export TF_VAR_server_port=8080\n$ terraform plan\nAnd if you don’t want to deal with remembering extra command-line arguments\nevery time you run plan  or apply , you can specify a default  value:\nvariable  ""server_port""  {\n  description  = ""The port the server will use for HTTP requests""\n  type        = number\n  default      = 8080\n}\nDeploying a Configurable  Web Server | 63\nTo use the value from an input variable in your Terraform code, you can use a new\ntype of expression called a variable reference , which has the following syntax:\nvar.<VARIABLE_NAME>\nFor example, here is how you can set the from_port  and to_port  parameters of the\nsecurity group to the value of the server_port  variable:\nresource  ""aws_security_group"" ""instance""  {\n  name = ""terraform-example-instance""\n  ingress {\n    from_port    = var.server_port\n    to_port      = var.server_port\n    protocol     = ""tcp""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n}\nIt’s also a good idea to use the same variable when setting the port in the User Data\nscript. To use a reference inside of a string literal, you need to use a new type of\nexpression called an interpolation , which has the following syntax:\n""${...}""\nY ou can put any valid reference within the curly braces, and Terraform will convert it\nto a string. For example, here’s how you can use var.server_port  inside of the User\nData string:\n  user_data  = <<-EOF\n              #!/bin/bash\n              echo ""Hello, World""  > index.html\n              nohup busybox httpd -f -p ${var.server_port}  &\n              EOF\nIn addition to input variables, Terraform also allows you to  define output variables  by\nusing the following syntax:\noutput ""<NAME>""  {\n  value = <VALUE>\n  [CONFIG ...]\n}\nThe NAME  is the name of the output variable, and VALUE  can be any Terraform expres‐\nsion that you would like to output. The CONFIG  can contain the following optional\nparameters:\ndescription\nIt’s always a good idea to use this parameter to document what type of data is\ncontained in the output variable.\n64 | Chapter 2: Getting Started with Terraform\nsensitive\nSet this parameter to true  to instruct Terraform not to log this output at the end\nof plan  or apply . This is useful if the output variable contains secrets such as\npasswords or private keys. Note that if your output variable references an input\nvariable or resource attribute marked with sensitive = true , you are required\nto mark the output variable with sensitive = true  as well to indicate you are\nintentionally outputting a secret.\ndepends_on\nNormally, Terraform automatically figures out your dependency graph based\non the references within your code, but in rare situations, you have to give it\nextra hints. For example, perhaps you have an output variable that returns the\nIP address of a server, but that IP won’t be accessible until a security group\n(firewall) is properly configured for that server. In that case, you may explicitly\ntell Terraform there is a dependency between the IP address output variable and\nthe security group resource using depends_on .\nFor example, instead of having to manually poke around the EC2 console to find the\nIP address of your server, you can provide the IP address as an output variable:\noutput ""public_ip""  {\n  value       = aws_instance.example.public_ip\n  description  = ""The public IP address of the web server""\n}\nThis code uses an attribute reference again, this time referencing the public_ip\nattribute of the aws_instance  resource. If you run the apply  command again, Terra‐\nform will not apply any changes (because you haven’t changed any resources), but it\nwill show you the new output at the very end:\n$ terraform apply\n(...)\naws_security_group.instance: Refreshing state... [id=sg-078ccb4f9533d2c1a]\naws_instance.example: Refreshing state... [id=i-028cad2d4e6bddec6]\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\npublic_ip = ""54.174.13.5""\nAs you can see, output variables show up in the console after you run terraform\napply , which users of your Terraform code might find useful (e.g., you now know\nwhat IP to test after the web server is deployed). Y ou can also use the terraform\noutput  command to list all outputs without applying any changes:\nDeploying a Configurable  Web Server | 65",11153
29-Deploying a Cluster of Web Servers.pdf,29-Deploying a Cluster of Web Servers,"14For a deeper look at how to build highly available and scalable systems on AWS, see “ A Comprehensive Guide\nto Building a Scalable Web App on Amazon Web Services - Part 1”  by Josh Padnick.$ terraform output\npublic_ip = ""54.174.13.5""\nAnd you can run terraform output <OUTPUT_NAME>  to see the value of a specific\noutput called <OUTPUT_NAME> :\n$ terraform output public_ip\n""54.174.13.5""\nThis is particularly handy for scripting. For example, you could create a deployment\nscript that runs terraform apply  to deploy the web server, uses terraform output\npublic_ip  to grab its public IP , and runs curl  on the IP as a quick smoke test to\nvalidate that the deployment worked.\nInput and output variables are also essential ingredients in creating configurable and\nreusable infrastructure code, a topic you’ll see more of in Chapter 4 .\nDeploying a Cluster of Web Servers\nRunning a single server is a good start, but in the real world, a single server is a single\npoint of failure. If that server crashes, or if it becomes overloaded from too much\ntraffic, users will be unable to access your site. The solution is to run a cluster of\nservers, routing around servers that go down and adjusting the size of the cluster up\nor down based on traffic.14\nManaging such a cluster manually is a lot of work. Fortunately, you can let AWS take\ncare of it for you by using an Auto Scaling Group (ASG), as shown in Figure 2-9 . An\nASG takes care of a lot of tasks for you completely automatically, including launching\na cluster of EC2 Instances, monitoring the health of each Instance, replacing failed\nInstances, and adjusting the size of the cluster in response to load.\n66 | Chapter 2: Getting Started with Terraform\n15These days, you should actually be using a launch template  (and the aws_launch_template  resource) with\nASGs rather than a launch configuration. However, I’ve stuck with the launch configuration in the examples\nin this book as it is convenient for teaching some of the concepts in the zero-downtime deployment section of\nChapter 5 .\nFigure 2-9. Instead of a single web server, run a cluster of web servers using an Auto\nScaling Group.\nThe first step in creating an ASG is to create a launch configuration , which speci‐\nfies how to configure each EC2 Instance in the ASG.15 The aws_launch_configura\ntion  resource uses almost the same parameters as the aws_instance resource ,\nalthough it doesn’t support tags (you’ll handle these in the aws_autoscaling_group\nresource later) or the user_data_replace_on_change  parameter (ASGs launch new\ninstances by default, so you don’t need this parameter), and two of the parameters\nhave different names ( ami is now image_id , and vpc_security_group_ids  is now\nsecurity_groups ), so replace aws_instance  with aws_launch_configuration  as\nfollows:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = ""ami-0fb653ca2d3203ac1""\n  instance_type    = ""t2.micro""\n  security_groups  = [aws_security_group.instance.id ]\n  user_data  = <<-EOF\n              #!/bin/bash\n              echo ""Hello, World""  > index.html\n              nohup busybox httpd -f -p ${var.server_port}  &\n              EOF\n}\nDeploying a Cluster of Web Servers | 67\nNow you can create the ASG itself using the aws_autoscaling_group  resource:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  min_size  = 2\n  max_size  = 10\n  tag {\n    key                 = ""Name""\n    value                = ""terraform-asg-example""\n    propagate_at_launch  = true\n  }\n}\nThis ASG will run between 2 and 10 EC2 Instances (defaulting to 2 for the initial\nlaunch), each tagged with the name terraform-asg-example . Note that the ASG\nuses a reference to fill in the launch configuration name. This leads to a problem:\nlaunch configurations are immutable, so if you change any parameter of your launch\nconfiguration, Terraform will try to replace it. Normally, when replacing a resource,\nTerraform would delete the old resource first and then creates its replacement, but\nbecause your ASG now has a reference to the old resource, Terraform won’t be able to\ndelete it.\nTo solve this problem, you can use a lifecycle  setting. Every Terraform resource\nsupports  several lifecycle settings that configure how that resource is created, upda‐\nted, and/or deleted. A particularly useful lifecycle setting is create_before_destroy .\nIf you set create_before_destroy  to true , Terraform will invert the order in which\nit replaces resources, creating the replacement resource first (including updating\nany references that were pointing at the old resource to point to the replacement)\nand then deleting the old resource. Add the lifecycle  block to your aws_launch\n_configuration  as follows:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = ""ami-0fb653ca2d3203ac1""\n  instance_type    = ""t2.micro""\n  security_groups  = [aws_security_group.instance.id ]\n  user_data  = <<-EOF\n              #!/bin/bash\n              echo ""Hello, World""  > index.html\n              nohup busybox httpd -f -p ${var.server_port}  &\n              EOF\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n  }\n}\n68 | Chapter 2: Getting Started with Terraform\nThere’s also one other parameter that you need to add to your ASG to make it work:\nsubnet_ids . This parameter specifies to the ASG into which VPC subnets the EC2\nInstances should be deployed (see “Network Security” on page 59 for background\ninfo on subnets). Each subnet lives in an isolated AWS AZ (that is, isolated datacen‐\nter), so by deploying your Instances across multiple subnets, you ensure that your\nservice can keep running even if some of the datacenters have an outage. Y ou could\nhardcode the list of subnets, but that won’t be maintainable or portable, so a better\noption is to use data sources  to get the list of subnets in your AWS account.\nA data source represents a piece of read-only information that is fetched from the\nprovider (in this case, AWS) every time you run Terraform. Adding a data source\nto your Terraform configurations does not create anything new; it’s just a way to\nquery the provider’s APIs for data and to make that data available to the rest of\nyour Terraform code. Each Terraform provider exposes a variety of data sources. For\nexample, the AWS Provider includes data sources to look up VPC data, subnet data,\nAMI IDs, IP address ranges, the current user’s identity, and much more.\nThe syntax for using a data source is very similar to the syntax of a resource:\ndata ""<PROVIDER>_<TYPE>"" ""<NAME>""  {\n  [CONFIG ...]\n}\nHere, PROVIDER  is the name of a provider (e.g., aws), TYPE  is the type of data source\nyou want to use (e.g., vpc), NAME  is an identifier you can use throughout the Terra‐\nform code to refer to this data source, and CONFIG  consists of one or more arguments\nthat are specific to that data source. For example, here is how you can use the aws_vpc\ndata source to look up the data for your Default VPC (see “ A Note on Default Virtual\nPrivate Clouds” on page 42  for background information):\ndata ""aws_vpc"" ""default""  {\n  default  = true\n}\nNote that with data sources, the arguments you pass in are typically search filters that\nindicate to the data source what information you’re looking for. With the aws_vpc\ndata source, the only filter you need is default = true , which directs Terraform to\nlook up the Default VPC in your AWS account.\nTo get the data out of a data source, you use the following attribute reference syntax:\ndata.<PROVIDER>_<TYPE> .<NAME>.<ATTRIBUTE>\nFor example, to get the ID of the VPC from the aws_vpc  data source, you would use\nthe following:\ndata.aws_vpc.default.id\nDeploying a Cluster of Web Servers | 69",7915
30-Deploying a Load Balancer.pdf,30-Deploying a Load Balancer,"Y ou can combine this with another data source, aws_subnets , to look up the subnets\nwithin that VPC:\ndata ""aws_subnets"" ""default""  {\n  filter {\n    name    = ""vpc-id""\n    values  = [data.aws_vpc.default.id ]\n  }\n}\nFinally, you can pull the subnet IDs out of the aws_subnets  data source and tell your\nASG to use those subnets via the (somewhat oddly named) vpc_zone_identifier\nargument:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  min_size  = 2\n  max_size  = 10\n  tag {\n    key                 = ""Name""\n    value                = ""terraform-asg-example""\n    propagate_at_launch  = true\n  }\n}\nDeploying a Load Balancer\nAt this point, you can deploy your ASG, but you’ll have a small problem: you now\nhave multiple servers, each with its own IP address, but you typically want to give\nyour end users only a single IP to use. One way to solve this problem is to deploy a\nload balancer  to distribute traffic across your servers and to give all your users the IP\n(actually, the DNS name) of the load balancer. Creating a load balancer that is highly\navailable and scalable is a lot of work. Once again, you can let AWS take care of it\nfor you, this time by using Amazon’s Elastic Load Balancer  (ELB) service, as shown in\nFigure 2-10 .\n70 | Chapter 2: Getting Started with Terraform\nFigure 2-10. Use Amazon ELB to distribute traffic  across the Auto Scaling Group.\nAWS offers three types of load balancers:\nApplication Load Balancer (ALB)\nBest suited for load balancing of HTTP and HTTPS traffic. Operates at the\napplication layer (Layer 7) of the Open Systems Interconnection (OSI) model.\nNetwork Load Balancer (NLB)\nBest suited for load balancing of TCP , UDP , and TLS traffic. Can scale up and\ndown in response to load faster than the ALB (the NLB is designed to scale to\ntens of millions of requests per second). Operates at the transport layer (Layer 4)\nof the OSI model.\nClassic Load Balancer (CLB)\nThis is the “legacy” load balancer that predates both the ALB and NLB. It can\nhandle HTTP , HTTPS, TCP , and TLS traffic but with far fewer features than\neither the ALB or NLB. Operates at both the application layer (L7) and transport\nlayer (L4) of the OSI model.\nMost applications these days should use either the ALB or the NLB. Because the\nsimple web server example you’re working on is an HTTP app without any extreme\nperformance requirements, the ALB is going to be the best fit.\nThe ALB consists of several parts, as shown in Figure 2-11 :\nListener\nListens on a specific port (e.g., 80) and protocol (e.g., HTTP).\nDeploying a Load Balancer | 71\n16To keep these examples simple, the EC2 Instances and ALB are running in the same subnets. In production\nusage, you’ d most likely run them in different subnets, with the EC2 Instances in private subnets (so they\naren’t directly accessible from the public internet) and the ALBs in public subnets (so users can access them\ndirectly).Listener rule\nTakes requests that come into a listener and sends those that match specific\npaths (e.g., /foo  and /bar ) or hostnames (e.g., foo.example.com  and bar.exam\nple.com ) to specific target groups.\nTarget groups\nOne or more servers that receive requests from the load balancer. The target\ngroup also performs health checks on these servers and sends requests only to\nhealthy nodes.\nFigure 2-11. An ALB consists of listeners, listener rules, and target groups.\nThe first step is to create the ALB itself using the aws_lb  resource:\nresource  ""aws_lb"" ""example""  {\n  name               = ""terraform-asg-example""\n  load_balancer_type  = ""application""\n  subnets             = data.aws_subnets.default.ids\n}\nNote that the subnets  parameter configures the load balancer to use all the subnets\nin your Default VPC by using the aws_subnets  data source.16 AWS load balancers\ndon’t consist of a single server, but of multiple servers that can run in separate subnets\n(and, therefore, separate datacenters). AWS automatically scales the number of load\nbalancer servers up and down based on traffic and handles failover if one of those\nservers goes down, so you get scalability and high availability out of the box.\nThe next step is to define a listener for this ALB using the aws_lb_listener  resource:\nresource  ""aws_lb_listener"" ""http""  {\n  load_balancer_arn  = aws_lb.example.arn\n  port              = 80\n  protocol           = ""HTTP""\n72 | Chapter 2: Getting Started with Terraform\n  # By default, return a simple 404 page\n  default_action  {\n    type  = ""fixed-response""\n    fixed_response  {\n      content_type  = ""text/plain""\n      message_body  = ""404: page not found""\n      status_code   = 404\n    }\n  }\n}\nThis listener configures the ALB to listen on the default HTTP port, port 80, use\nHTTP as the protocol, and send a simple 404 page as the default response for\nrequests that don’t match any listener rules.\nNote that, by default, all AWS resources, including ALBs, don’t allow any incoming or\noutgoing traffic, so you need to create a new security group specifically for the ALB.\nThis security group should allow incoming requests on port 80 so that you can access\nthe load balancer over HTTP , and allow outgoing requests on all ports so that the\nload balancer can perform health checks:\nresource  ""aws_security_group"" ""alb""  {\n  name = ""terraform-example-alb""\n  # Allow inbound HTTP requests\n  ingress {\n    from_port    = 80\n    to_port      = 80\n    protocol     = ""tcp""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n  # Allow all outbound requests\n  egress {\n    from_port    = 0\n    to_port      = 0\n    protocol     = ""-1""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n}\nY ou’ll need to tell the aws_lb  resource to use this security group via the secu\nrity_groups  argument:\nresource  ""aws_lb"" ""example""  {\n  name               = ""terraform-asg-example""\n  load_balancer_type  = ""application""\n  subnets             = data.aws_subnets.default.ids\n  security_groups     = [aws_security_group.alb.id ]\n}\nDeploying a Load Balancer | 73\nNext, you need to create a target group for your ASG using the aws_lb_target_group\nresource:\nresource  ""aws_lb_target_group"" ""asg""  {\n  name     = ""terraform-asg-example""\n  port     = var.server_port\n  protocol  = ""HTTP""\n  vpc_id    = data.aws_vpc.default.id\n  health_check  {\n    path                 = ""/""\n    protocol             = ""HTTP""\n    matcher              = ""200""\n    interval             = 15\n    timeout              = 3\n    healthy_threshold    = 2\n    unhealthy_threshold  = 2\n  }\n}\nThis target group will health check your Instances by periodically sending an HTTP\nrequest to each Instance and will consider the Instance “healthy” only if the Instance\nreturns a response that matches the configured matcher  (e.g., you can configure a\nmatcher to look for a 200 OK response). If an Instance fails to respond, perhaps\nbecause that Instance has gone down or is overloaded, it will be marked as “unheal‐\nthy, ” and the target group will automatically stop sending traffic to it to minimize\ndisruption for your users.\nHow does the target group know which EC2 Instances to send requests to? Y ou could\nattach a static list of EC2 Instances to the target group using the aws_lb_tar\nget_group_attachment  resource, but with an ASG, Instances can launch or termi‐\nnate at any time, so a static list won’t work. Instead, you can take advantage of the\nfirst-class integration between the ASG and the ALB. Go back to the aws_autoscal\ning_group  resource, and set its target_group_arns  argument to point at your new\ntarget group:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns  = [aws_lb_target_group.asg.arn ]\n  health_check_type  = ""ELB""\n  min_size  = 2\n  max_size  = 10\n  tag {\n    key                 = ""Name""\n    value                = ""terraform-asg-example""\n    propagate_at_launch  = true\n74 | Chapter 2: Getting Started with Terraform\n  }\n}\nY ou should also update the health_check_type  to ""ELB"" . The default\nhealth_check_type  is ""EC2"" , which is a minimal health check that considers an\nInstance unhealthy only if the AWS hypervisor says the VM is completely down or\nunreachable. The ""ELB""  health check is more robust, because it instructs the ASG\nto use the target group’s health check to determine whether an Instance is healthy\nand to automatically replace Instances if the target group reports them as unhealthy.\nThat way, Instances will be replaced not only if they are completely down but also if,\nfor example, they’ve stopped serving requests because they ran out of memory or a\ncritical process crashed.\nFinally, it’s time to tie all these pieces together by creating listener rules using the\naws_lb_listener_rule  resource:\nresource  ""aws_lb_listener_rule"" ""asg""  {\n  listener_arn  = aws_lb_listener.http.arn\n  priority      = 100\n  condition  {\n    path_pattern  {\n      values  = [""*""]\n    }\n  }\n  action {\n    type              = ""forward""\n    target_group_arn  = aws_lb_target_group.asg.arn\n  }\n}\nThe preceding code adds a listener rule that sends requests that match any path to the\ntarget group that contains your ASG.\nThere’s one last thing to do before you deploy the load balancer—replace the old\npublic_ip  output of the single EC2 Instance you had before with an output that\nshows the DNS name of the ALB:\noutput ""alb_dns_name""  {\n  value       = aws_lb.example.dns_name\n  description  = ""The domain name of the load balancer""\n}\nRun terraform apply , and read through the plan output. Y ou should see that your\noriginal single EC2 Instance is being removed, and in its place, Terraform will create\na launch configuration, ASG, ALB, and a security group. If the plan looks good, type\nyes and hit Enter. When apply  completes, you should see the alb_dns_name  output:\nOutputs:\nalb_dns_name = ""terraform-asg-example-123.us-east-2.elb.amazonaws.com""\nDeploying a Load Balancer | 75\nCopy down this URL. It’ll take a couple minutes for the Instances to boot and show\nup as healthy in the ALB. In the meantime, you can inspect what you’ve deployed.\nOpen up the ASG section of the EC2 console , and you should see that the ASG has\nbeen created, as shown in Figure 2-12 .\nFigure 2-12. The AWS Console shows all the ASGs you’ve created.\nIf you switch over to the Instances tab, you’ll see the two EC2 Instances launching, as\nshown in Figure 2-13 .\n76 | Chapter 2: Getting Started with Terraform\nFigure 2-13. The EC2 Instances in the ASG are launching.\nIf you click the Load Balancers tab, you’ll see your ALB, as shown in Figure 2-14 .\nFigure 2-14. The AWS Console shows all the ALBs you’ve created.\nFinally, if you click the Target Groups tab, you can find your target group, as shown\nin Figure 2-15 .\nDeploying a Load Balancer | 77\nFigure 2-15. The AWS Console shows all the target groups you’ve created.\nIf you click your target group and find the Targets tab in the bottom half of the\nscreen, you can see your Instances registering with the target group and going\nthrough health checks. Wait for the Status indicator to indicate “healthy” for\nboth of them. This typically takes one to two minutes. When you see it, test the\nalb_dns_name  output you copied earlier:\n$ curl http://<alb_dns_name>\nHello, World\nSuccess! The ALB is routing traffic to your EC2 Instances. Each time you access\nthe URL, it’ll pick a different Instance to handle the request. Y ou now have a fully\nworking cluster of web servers!\nAt this point, you can see how your cluster responds to firing up new Instances or\nshutting down old ones. For example, go to the Instances tab and terminate one of\nthe Instances by selecting its checkbox, clicking the Actions button at the top, and\nthen setting the Instance State to Terminate. Continue to test the ALB URL, and you\nshould get a 200 OK for each request, even while terminating an Instance, because\nthe ALB will automatically detect that the Instance is down and stop routing to it.\nEven more interesting, a short time after the Instance shuts down, the ASG will detect\nthat fewer than two Instances are running and automatically launch a new one to\nreplace it (self-healing!). Y ou can also see how the ASG resizes itself by adding a\ndesired_capacity  parameter to your Terraform code and rerunning apply .\n78 | Chapter 2: Getting Started with Terraform",12642
31-Shared Storage for State Files.pdf,31-Shared Storage for State Files,"Cleanup\nWhen you’re done experimenting with Terraform, either at the end of this chapter,\nor at the end of future chapters, it’s a good idea to remove all of the resources you\ncreated so that AWS doesn’t charge you for them. Because Terraform keeps track of\nwhat resources you created, cleanup is simple. All you need to do is run the destroy\ncommand:\n$ terraform destroy\n(...)\nTerraform will perform the following actions:\n  # aws_autoscaling_group.example will be destroyed\n  - resource ""aws_autoscaling_group"" ""example"" {\n      (...)\n    }\n  # aws_launch_configuration.example will be destroyed\n  - resource ""aws_launch_configuration"" ""example"" {\n      (...)\n    }\n  # aws_lb.example will be destroyed\n  - resource ""aws_lb"" ""example"" {\n      (...)\n    }\n  (...)\nPlan: 0 to add, 0 to change, 8 to destroy.\nDo you really want to destroy all resources?\n  Terraform will destroy all your managed infrastructure, as shown above.\n  There is no undo. Only 'yes' will be accepted to confirm.\n  Enter a value:\nIt goes without saying that you should rarely, if ever, run destroy  in a production\nenvironment! There’s no “undo” for the destroy  command, so Terraform gives you\none final chance to review what you’re doing, showing you the list of all the resources\nyou’re about to delete, and prompting you to confirm the deletion. If everything looks\ngood, type yes and hit Enter; Terraform will build the dependency graph and delete\nall of the resources in the correct order, using as much parallelism as possible. In a\nminute or two, your AWS account should be clean again.\nNote that later in the book, you will continue to develop this example, so don’t\ndelete the Terraform code! However, feel free to run destroy  on the actual deployed\nCleanup | 79\nresources whenever you want. After all, the beauty of infrastructure as code is that\nall of the information about those resources is captured in code, so you can re-create\nall of them at any time with a single command: terraform apply . In fact, you might\nwant to commit your latest changes to Git so that you can keep track of the history of\nyour infrastructure.\nConclusion\nY ou now have a basic grasp of how to use Terraform. The declarative language makes\nit easy to describe exactly the infrastructure you want to create. The plan  command\nallows you to verify your changes and catch bugs before deploying them. Variables,\nreferences, and dependencies allow you to remove duplication from your code and\nmake it highly configurable.\nHowever, you’ve only scratched the surface. In Chapter 3 , you’ll learn how Terraform\nkeeps track of what infrastructure it has already created, and the profound impact\nthat has on how you should structure your Terraform code. In Chapter 4 , you’ll see\nhow to create reusable infrastructure with Terraform modules.\n80 | Chapter 2: Getting Started with Terraform\nCHAPTER 3\nHow to Manage Terraform State\nIn Chapter 2 , as you were using Terraform to create and update resources, you\nmight have noticed that every time you ran terraform plan  or terraform apply ,\nTerraform was able to find the resources it created previously and update them\naccordingly. But how did Terraform know which resources it was supposed to\nmanage? Y ou could have all sorts of infrastructure in your AWS account, deployed\nthrough a variety of mechanisms (some manually, some via Terraform, some via the\nCLI), so how does Terraform know which infrastructure it’s responsible for?\nIn this chapter, you’re going to see how Terraform tracks the state of your infrastruc‐\nture and the impact that has on file layout, isolation, and locking in a Terraform\nproject. Here are the key topics I’ll go over:\n•What is Terraform state?•\n•Shared storage for state files•\n•Limitations with Terraform’s backends•\n•State file isolation•\n—Isolation via workspaces—\n—Isolation via file layout—\n•The terraform_remote_state  data source •\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\n81\nWhat Is Terraform State?\nEvery  time you run Terraform, it records information about what infrastructure\nit created in a Terraform state file. By default, when you run Terraform in the\nfolder /foo/bar , Terraform creates the file /foo/bar/terraform.tfstate . This file contains\na custom JSON format that records a mapping from the Terraform resources in your\nconfiguration files to the representation of those resources in the real world. For\nexample, let’s say your Terraform configuration contained the following:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nAfter running terraform apply , here is a small snippet of the contents of the\nterraform.tfstate  file (truncated for readability):\n{\n  ""version"" : 4,\n  ""terraform_version"" : ""1.2.3"",\n  ""serial"" : 1,\n  ""lineage"" : ""86545604-7463-4aa5-e9e8-a2a221de98d2"" ,\n  ""outputs"" : {},\n  ""resources"" : [\n    {\n      ""mode"": ""managed"" ,\n      ""type"": ""aws_instance"" ,\n      ""name"": ""example"" ,\n      ""provider"" : ""provider[\""registry.terraform.io/hashicorp/aws\""]"" ,\n      ""instances"" : [\n        {\n          ""schema_version"" : 1,\n          ""attributes"" : {\n            ""ami"": ""ami-0fb653ca2d3203ac1"" ,\n            ""availability_zone"" : ""us-east-2b"" ,\n            ""id"": ""i-0bc4bbe5b84387543"" ,\n            ""instance_state"" : ""running"" ,\n            ""instance_type"" : ""t2.micro"" ,\n            ""(...)"": ""(truncated)""\n          }\n        }\n      ]\n    }\n  ]\n}\nUsing this JSON format, Terraform knows that a resource with type aws_instance\nand name example  corresponds to an EC2 Instance in your AWS account with ID\ni-0bc4bbe5b84387543 . Every time you run Terraform, it can fetch the latest status\nof this EC2 Instance from AWS and compare that to what’s in your Terraform\n82 | Chapter 3: How to Manage Terraform State\nconfigurations  to determine what changes need to be applied. In other words, the\noutput of the plan  command is a diff between the code on your computer and the\ninfrastructure deployed in the real world, as discovered via IDs in the state file.\nThe State File Is a Private API\nThe state file format is a private API that is meant only for internal\nuse within Terraform. Y ou should never edit the Terraform state\nfiles by hand or write code that reads them directly.\nIf for some reason you need to manipulate the state file—which\nshould be a relatively rare occurrence—use the terraform import\nor terraform state  commands (you’ll see examples of both in\nChapter 5 ).\nIf you’re using Terraform for a personal project, storing state in a single terra‐\nform.tfstate  file that lives locally on your computer works just fine. But if you want to\nuse Terraform as a team on a real product, you run into several problems:\nShared storage for state files\nTo be able to use Terraform to update your infrastructure, each of your team\nmembers needs access to the same Terraform state files. That means you need to\nstore those files in a shared location.\nLocking state files\nAs soon as data is shared, you run into a new problem: locking. Without locking,\nif two team members are running Terraform at the same time, you can run into\nrace conditions as multiple Terraform processes make concurrent updates to the\nstate files, leading to conflicts, data loss, and state file corruption.\nIsolating state files\nWhen making changes to your infrastructure, it’s a best practice to isolate differ‐\nent environments. For example, when making a change in a testing or staging\nenvironment, you want to be sure that there is no way you can accidentally break\nproduction. But how can you isolate your changes if all of your infrastructure is\ndefined in the same Terraform state file?\nIn the following sections, I’ll dive into each of these problems and show you how to\nsolve them.\nShared Storage for State Files\nThe most common technique for allowing multiple team members to access a com‐\nmon set of files is to put them in version control (e.g., Git). Although you should\ndefinitely  store your Terraform code in version control, storing Terraform state in\nversion control is a bad idea  for the following reasons:\nShared Storage for State Files | 83\nManual error\nIt’s too easy to forget to pull down the latest changes from version control\nbefore running Terraform or to push your latest changes to version control after\nrunning Terraform. It’s just a matter of time before someone on your team runs\nTerraform with out-of-date state files and, as a result, accidentally rolls back or\nduplicates previous deployments.\nLocking\nMost version control systems do not provide any form of locking that would\nprevent two team members from running terraform apply  on the same state\nfile at the same time.\nSecrets\nAll data in Terraform state files is stored in plain text. This is a problem because\ncertain Terraform resources need to store sensitive data. For example, if you\nuse the aws_db_instance  resource to create a database, Terraform will store the\nusername and password for the database in a state file in plain text, and you\nshouldn’t store plain text secrets in version control.\nInstead of using version control, the best way to manage shared storage for state\nfiles is to use Terraform’s built-in support for remote backends. A Terraform backend\ndetermines how Terraform loads and stores state. The default backend, which you’ve\nbeen using this entire time, is the local backend , which stores the state file on your\nlocal disk. Remote backends  allow you to store the state file in a remote, shared store.\nA number of remote backends are supported, including Amazon S3, Azure Storage,\nGoogle Cloud Storage, and HashiCorp’s Terraform Cloud and Terraform Enterprise.\nRemote backends solve the three issues just listed:\nManual error\nAfter you configure a remote backend, Terraform will automatically load the\nstate file from that backend every time you run plan  or apply , and it’ll automati‐\ncally store the state file in that backend after each apply , so there’s no chance of\nmanual error.\nLocking\nMost of the remote backends natively support locking. To run terraform apply ,\nTerraform will automatically acquire a lock; if someone else is already running\napply , they will already have the lock, and you will have to wait. Y ou can run\napply  with the -lock-timeout=<TIME>  parameter to instruct Terraform to wait\nup to TIME  for a lock to be released (e.g., -lock-timeout=10m  will wait for 10\nminutes).\n84 | Chapter 3: How to Manage Terraform State\n1Learn more about S3’s guarantees on the AWS website .\n2See pricing information for S3 on the AWS website .Secrets\nMost of the remote backends natively support encryption in transit and encryp‐\ntion at rest of the state file. Moreover, those backends usually expose ways\nto configure access permissions (e.g., using IAM policies with an Amazon S3\nbucket), so you can control who has access to your state files and the secrets they\nmight contain. It would be better still if Terraform natively supported encrypting\nsecrets within the state file, but these remote backends reduce most of the\nsecurity concerns, given that at least the state file isn’t stored in plain text on disk\nanywhere.\nIf you’re using Terraform with AWS, Amazon S3 (Simple Storage Service), which is\nAmazon’s managed file store, is typically your best bet as a remote backend for the\nfollowing reasons:\n•It’s a managed service, so you don’t need to deploy and manage extra infrastruc‐•\nture to use it.\n•It’s designed for 99.999999999% durability and 99.99% availability, which means•\nyou don’t need to worry too much about data loss or outages.1\n•It supports encryption, which reduces worries about storing sensitive data in•\nstate files. Y ou still have to be very careful who on your team can access the\nS3 bucket, but at least the data will be encrypted at rest (Amazon S3 supports\nserver-side encryption using AES-256) and in transit (Terraform uses TLS when\ntalking to Amazon S3).\n•It supports locking via DynamoDB. (More on this later.)•\n•It supports versioning , so every revision of your state file is stored, and you can •\nroll back to an older version if something goes wrong.\n•It’s inexpensive, with most Terraform usage easily fitting into the AWS Free Tier.2•\nTo enable remote state storage with Amazon S3, the first step is to create an S3\nbucket. Create a main.tf  file in a new folder (it should be a different folder from where\nyou store the configurations from Chapter 2 ), and at the top of the file, specify AWS\nas the provider:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nShared Storage for State Files | 85\nNext, create an S3 bucket by using the aws_s3_bucket  resource:\nresource  ""aws_s3_bucket"" ""terraform_state""  {\n  bucket  = ""terraform-up-and-running-state""\n  # Prevent accidental deletion of this S3 bucket\n  lifecycle  {\n    prevent_destroy  = true\n  }\n}\nThis code sets the following arguments:\nbucket\nThis is the name of the S3 bucket. Note that S3 bucket names must be globally\nunique among all AWS customers. Therefore, you will need to change the bucket\nparameter from ""terraform-up-and-running-state""  (which I already created)\nto your own name. Make sure to remember this name and take note of what\nAWS region you’re using because you’ll need both pieces of information again a\nlittle later on.\nprevent_destroy\nprevent_destroy  is the second lifecycle setting you’ve seen (the first was\ncreate_before_destroy  in Chapter 2 ). When you set prevent_destroy  to true\non a resource, any attempt to delete that resource (e.g., by running terraform\ndestroy ) will cause Terraform to exit with an error. This is a good way to prevent\naccidental deletion of an important resource, such as this S3 bucket, which will\nstore all of your Terraform state. Of course, if you really mean to delete it, you\ncan just comment that setting out.\nLet’s now add several extra layers of protection to this S3 bucket.\nFirst, use the aws_s3_bucket_versioning  resource to enable versioning on the S3\nbucket so that every update to a file in the bucket actually creates a new version of\nthat file. This allows you to see older versions of the file and revert to those older\nversions at any time, which can be a useful fallback mechanism if something goes\nwrong:\n# Enable versioning so you can see the full revision history of your\n# state files\nresource  ""aws_s3_bucket_versioning"" ""enabled""  {\n  bucket  = aws_s3_bucket.terraform_state.id\n  versioning_configuration  {\n    status  = ""Enabled""\n  }\n}\n86 | Chapter 3: How to Manage Terraform State\n3Pricing information for DynamoDB is available on the AWS website .Second, use the aws_s3_bucket_server_side_encryption_configuration  resource\nto turn server-side encryption on by default for all data written to this S3 bucket. This\nensures that your state files, and any secrets they might contain, are always encrypted\non disk when stored in S3:\n# Enable server-side encryption by default\nresource  ""aws_s3_bucket_server_side_encryption_configuration"" ""default""  {\n  bucket  = aws_s3_bucket.terraform_state.id\n  rule {\n    apply_server_side_encryption_by_default  {\n      sse_algorithm  = ""AES256""\n    }\n  }\n}\nThird, use the aws_s3_bucket_public_access_block  resource to block all public\naccess to the S3 bucket. S3 buckets are private by default, but as they are often used to\nserve static content—e.g., images, fonts, CSS, JS, HTML—it is possible, even easy, to\nmake the buckets public. Since your Terraform state files may contain sensitive data\nand secrets, it’s worth adding this extra layer of protection to ensure no one on your\nteam can ever accidentally make this S3 bucket public:\n# Explicitly block all public access to the S3 bucket\nresource  ""aws_s3_bucket_public_access_block"" ""public_access""  {\n  bucket                   = aws_s3_bucket.terraform_state.id\n  block_public_acls        = true\n  block_public_policy      = true\n  ignore_public_acls       = true\n  restrict_public_buckets  = true\n}\nNext, you need to create a DynamoDB table to use for locking. DynamoDB is Ama‐\nzon’s distributed key-value store. It supports strongly consistent reads and conditional\nwrites, which are all the ingredients you need for a distributed lock system. Moreover,\nit’s completely managed, so you don’t have any infrastructure to run yourself, and it’s\ninexpensive, with most Terraform usage easily fitting into the AWS Free Tier.3\nTo use DynamoDB for locking with Terraform, you must create a DynamoDB table\nthat has a primary key called LockID  (with this exact  spelling and capitalization). Y ou\ncan create such a table using the aws_dynamodb_table  resource:\nresource  ""aws_dynamodb_table"" ""terraform_locks""  {\n  name         = ""terraform-up-and-running-locks""\n  billing_mode  = ""PAY_PER_REQUEST""\n  hash_key      = ""LockID""\nShared Storage for State Files | 87\n  attribute  {\n    name  = ""LockID""\n    type  = ""S""\n  }\n}\nRun terraform init  to download the provider code, and then run terraform apply\nto deploy. After everything is deployed, you will have an S3 bucket and DynamoDB\ntable, but your Terraform state will still be stored locally. To configure Terraform to\nstore the state in your S3 bucket (with encryption and locking), you need to add a\nbackend  configuration to your Terraform code. This is configuration for Terraform\nitself, so it resides within a terraform  block and has the following syntax:\nterraform  {\n  backend ""<BACKEND_NAME>""  {\n    [CONFIG...]\n  }\n}\nwhere BACKEND_NAME  is the name of the backend you want to use (e.g., ""s3"" ) and\nCONFIG  consists of one or more arguments that are specific to that backend (e.g., the\nname of the S3 bucket to use). Here’s what the backend  configuration looks like for an\nS3 bucket:\nterraform  {\n  backend ""s3"" {\n    # Replace this with your bucket name!\n    bucket          = ""terraform-up-and-running-state""\n    key            = ""global/s3/terraform.tfstate""\n    region          = ""us-east-2""\n    # Replace this with your DynamoDB table name!\n    dynamodb_table  = ""terraform-up-and-running-locks""\n    encrypt         = true\n  }\n}\nLet’s go through these settings one at a time:\nbucket\nThe name of the S3 bucket to use. Make sure to replace this with the name of the\nS3 bucket you created earlier.\nkey\nThe filepath within the S3 bucket where the Terraform state file should be\nwritten. Y ou’ll see a little later on why the preceding example code sets this to\nglobal/s3/terraform.tfstate .\n88 | Chapter 3: How to Manage Terraform State\nregion\nThe AWS region where the S3 bucket lives. Make sure to replace this with the\nregion of the S3 bucket you created earlier.\ndynamodb_table\nThe DynamoDB table to use for locking. Make sure to replace this with the name\nof the DynamoDB table you created earlier.\nencrypt\nSetting  this to true  ensures that your Terraform state will be encrypted on disk\nwhen stored in S3. We already enabled default encryption in the S3 bucket itself,\nso this is here as a second layer to ensure that the data is always encrypted.\nTo instruct Terraform to store your state file in this S3 bucket, you’re going to use the\nterraform init  command again. This command not only can download provider\ncode, but also configure your Terraform backend (and you’ll see yet another use later\non, too). Moreover, the init  command is idempotent, so it’s safe to run it multiple\ntimes:\n$ terraform init\nInitializing the backend...\nAcquiring state lock. This may take a few moments...\nDo you want to copy existing state to the new backend?\n  Pre-existing state was found while migrating the previous ""local"" backend\n  to the newly configured ""s3"" backend. No existing state was found in the\n  newly configured ""s3"" backend. Do you want to copy this state to the new\n  ""s3"" backend? Enter ""yes"" to copy and ""no"" to start with an empty state.\n  Enter a value:\nTerraform will automatically detect that you already have a state file locally and\nprompt you to copy it to the new S3 backend. If you type yes, you should see the\nfollowing:\nSuccessfully configured the backend ""s3""! Terraform will automatically\nuse this backend unless the backend configuration changes.\nAfter running this command, your Terraform state will be stored in the S3 bucket.\nY ou can check this by heading over to the S3 Management Console  in your browser\nand clicking your bucket. Y ou should see something similar to Figure 3-1 .\nShared Storage for State Files | 89\nFigure 3-1. You can use the AWS Console to see how your state file is stored in an S3\nbucket.\nWith this backend enabled, Terraform will automatically pull the latest state from this\nS3 bucket before running a command and automatically push the latest state to the\nS3 bucket after running a command. To see this in action, add the following output\nvariables:\noutput ""s3_bucket_arn""  {\n  value       = aws_s3_bucket.terraform_state.arn\n  description  = ""The ARN of the S3 bucket""\n}\noutput ""dynamodb_table_name""  {\n  value       = aws_dynamodb_table.terraform_locks.name\n  description  = ""The name of the DynamoDB table""\n}\nThese variables will print out the Amazon Resource Name (ARN) of your S3 bucket\nand the name of your DynamoDB table. Run terraform apply  to see it:\n$ terraform apply\n(...)\nAcquiring state lock. This may take a few moments...\naws_dynamodb_table.terraform_locks: Refreshing state...\naws_s3_bucket.terraform_state: Refreshing state...\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nReleasing state lock. This may take a few moments...\nOutputs:\ndynamodb_table_name = ""terraform-up-and-running-locks""\ns3_bucket_arn = ""arn:aws:s3:::terraform-up-and-running-state""\n90 | Chapter 3: How to Manage Terraform State",21929
32-Limitations with Terraforms Backends.pdf,32-Limitations with Terraforms Backends,"Note how Terraform is now acquiring a lock before running apply  and releasing the\nlock after!\nNow, head over to the S3 console again, refresh the page, and click the gray Show\nbutton next to Versions. Y ou should now see several versions of your terraform.tfstate\nfile in the S3 bucket, as shown in Figure 3-2 .\nFigure 3-2. If you enable versioning for your S3 bucket, every change to the state file will\nbe stored as a separate version.\nThis means that Terraform is automatically pushing and pulling state data to and\nfrom S3, and S3 is storing every revision of the state file, which can be useful for\ndebugging and rolling back to older versions if something goes wrong.\nLimitations with Terraform’s Backends\nTerraform’s backends have a few limitations and gotchas that you need to be aware of.\nThe first limitation is the chicken-and-egg situation of using Terraform to create the\nS3 bucket where you want to store your Terraform state. To make this work, you had\nto use a two-step process:\n1.Write Terraform code to create the S3 bucket and DynamoDB table, and deploy1.\nthat code with a local backend.\n2.Go back to the Terraform code, add a remote backend  configuration to it to use 2.\nthe newly created S3 bucket and DynamoDB table, and run terraform init  to\ncopy your local state to S3.\nIf you ever wanted to delete the S3 bucket and DynamoDB table, you’ d have to do this\ntwo-step process in reverse:\n1.Go to the Terraform code, remove the backend  configuration, and rerun 1.\nterraform init  to copy the Terraform state back to your local disk.\nLimitations with Terraform’s Backends | 91\n2.Run terraform destroy  to delete the S3 bucket and DynamoDB table. 2.\nThis two-step process is a bit awkward, but the good news is that you can share a\nsingle S3 bucket and DynamoDB table across all of your Terraform code, so you’ll\nprobably only need to do it once (or once per AWS account if you have multiple\naccounts). After the S3 bucket exists, in the rest of your Terraform code, you can\nspecify the backend  configuration right from the start without any extra steps.\nThe second limitation is more painful: the backend  block in Terraform does not allow\nyou to use any variables or references. The following code will not work:\n# This will NOT work. Variables aren't allowed in a backend configuration.\nterraform  {\n  backend ""s3"" {\n    bucket          = var.bucket\n    region          = var.region\n    dynamodb_table  = var.dynamodb_table\n    key            = ""example/terraform.tfstate""\n    encrypt         = true\n  }\n}\nThis means that you need to manually copy and paste the S3 bucket name, region,\nDynamoDB table name, etc., into every one of your Terraform modules (you’ll learn\nall about Terraform modules in Chapters 4 and 8; for now, it’s enough to understand\nthat modules are a way to organize and reuse Terraform code and that real-world\nTerraform code typically consists of many small modules). Even worse, you must\nvery carefully not copy and paste the key value but ensure a unique key for every\nTerraform module you deploy so that you don’t accidentally overwrite the state of\nsome other module! Having to do lots of copy-and-pastes and lots of manual changes\nis error prone, especially if you need to deploy and manage many Terraform modules\nacross many environments.\nOne option for reducing copy-and-paste is to use partial configurations , where you\nomit certain parameters from the backend  configuration in your Terraform code and\ninstead pass those in via -backend-config  command-line arguments when calling\nterraform init . For example, you could extract the repeated backend  arguments,\nsuch as bucket  and region , into a separate file called backend.hcl :\n# backend.hcl\nbucket         = ""terraform-up-and-running-state""\nregion         = ""us-east-2""\ndynamodb_table  = ""terraform-up-and-running-locks""\nencrypt        = true\nOnly the key parameter remains in the Terraform code, since you still need to set a\ndifferent key value for each module:\n92 | Chapter 3: How to Manage Terraform State",4088
33-Isolation via Workspaces.pdf,33-Isolation via Workspaces,"4Here’s a colorful example of what happens when you don’t isolate Terraform state .# Partial configuration. The other settings (e.g., bucket, region) will be\n# passed in from a file via -backend-config arguments to 'terraform init'\nterraform  {\n  backend ""s3"" {\n    key = ""example/terraform.tfstate""\n  }\n}\nTo put all your partial configurations together, run terraform init  with the\n-backend-config  argument:\n$ terraform init -backend-config=backend.hcl\nTerraform merges the partial configuration in backend.hcl  with the partial configura‐\ntion in your Terraform code to produce the full configuration used by your module.\nY ou can use the same backend.hcl  file with all of your modules, which reduces\nduplication considerably; however, you’ll still need to manually set a unique key value\nin every module.\nAnother option for reducing copy-and-paste is to use Terragrunt , an open source tool\nthat tries to fill in a few gaps in Terraform. Terragrunt can help you keep your entire\nbackend  configuration DRY (Don’t Repeat Y ourself) by defining all the basic backend\nsettings (bucket name, region, DynamoDB table name) in one file and automatically\nsetting the key argument to the relative folder path of the module.\nY ou’ll see an example of how to use Terragrunt in Chapter 10 .\nState File Isolation\nWith a remote backend and locking, collaboration is no longer a problem. However,\nthere is still one more problem remaining: isolation. When you first start using\nTerraform, you might be tempted to define all of your infrastructure in a single\nTerraform file or a single set of Terraform files in one folder. The problem with this\napproach is that all of your Terraform state is now stored in a single file, too, and a\nmistake anywhere could break everything.\nFor example, while trying to deploy a new version of your app in staging, you might\nbreak the app in production. Or, worse yet, you might corrupt your entire state file,\neither because you didn’t use locking or due to a rare Terraform bug, and now all of\nyour infrastructure in all environments is broken.4\nThe whole point of having separate environments is that they are isolated from one\nanother, so if you are managing all the environments from a single set of Terraform\nconfigurations, you are breaking that isolation. Just as a ship has bulkheads that act\nas barriers to prevent a leak in one part of the ship from immediately flooding all the\nState File Isolation | 93\nothers, you should have “bulkheads” built into your Terraform design, as shown in\nFigure 3-3 .\nFigure 3-3. Create isolation (“bulkheads”) between your environments by defining  each\nenvironment in a separate Terraform configuration.\nAs Figure 3-3  illustrates, instead of defining all your environments in a single set of\nTerraform configurations (top), you want to define each environment in a separate\nset of configurations (bottom), so a problem in one environment is completely\nisolated from the others. There are two ways you could isolate state files:\nIsolation via workspaces\nUseful for quick, isolated tests on the same configuration\nIsolation via file layout\nUseful for production use cases for which you need strong separation between\nenvironments\nLet’s dive into each of these in the next two sections.\nIsolation via Workspaces\nTerraform workspaces  allow  you to store your Terraform state in multiple, separate,\nnamed workspaces. Terraform starts with a single workspace called “default, ” and if\nyou never explicitly specify a workspace, the default workspace is the one you’ll use\nthe entire time. To create a new workspace or switch between workspaces, you use\nthe terraform workspace  commands. Let’s experiment with workspaces on some\nTerraform code that deploys a single EC2 Instance:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n94 | Chapter 3: How to Manage Terraform State\n  instance_type  = ""t2.micro""\n}\nConfigure a backend for this Instance using the S3 bucket and DynamoDB table\nyou created earlier in the chapter but with the key set to workspaces-example/\nterraform.tfstate :\nterraform  {\n  backend ""s3"" {\n    # Replace this with your bucket name!\n    bucket          = ""terraform-up-and-running-state""\n    key            = ""workspaces-example/terraform.tfstate""\n    region          = ""us-east-2""\n    # Replace this with your DynamoDB table name!\n    dynamodb_table  = ""terraform-up-and-running-locks""\n    encrypt         = true\n  }\n}\nRun terraform init  and terraform apply  to deploy this code:\n$ terraform init\nInitializing the backend...\nSuccessfully configured the backend ""s3""! Terraform will automatically\nuse this backend unless the backend configuration changes.\nInitializing provider plugins...\n(...)\nTerraform has been successfully initialized!\n$ terraform apply\n(...)\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nThe state for this deployment is stored in the default workspace. Y ou can confirm\nthis by running the terraform workspace show  command, which will identify which\nworkspace you’re currently in:\n$ terraform workspace show\ndefault\nState File Isolation | 95\nThe default workspace stores your state in exactly the location you specify via the key\nconfiguration. As shown in Figure 3-4 , if you take a look in your S3 bucket, you’ll\nfind a terraform.tfstate  file in the workspaces-example  folder.\nFigure 3-4. When using the default workspace, the S3 bucket will have just a single\nfolder and state file in it.\nLet’s create a new workspace called “example1” using the terraform workspace new\ncommand:\n$ terraform workspace new example1\nCreated and switched to workspace ""example1""!\nYou're now on a new, empty workspace. Workspaces isolate their state,\nso if you run ""terraform plan"" Terraform will not see any existing state\nfor this configuration.\nNow, note what happens if you try to run terraform plan :\n$ terraform plan\nTerraform will perform the following actions:\n  # aws_instance.example will be created\n  + resource ""aws_instance"" ""example"" {\n      + ami                          = ""ami-0fb653ca2d3203ac1""\n      + instance_type                = ""t2.micro""\n      (...)\n    }\nPlan: 1 to add, 0 to change, 0 to destroy.\n96 | Chapter 3: How to Manage Terraform State\nTerraform wants to create a totally new EC2 Instance from scratch! That’s because\nthe state files in each workspace are isolated from one another, and because you’re\nnow in the example1 workspace, Terraform isn’t using the state file from the default\nworkspace and therefore doesn’t see the EC2 Instance was already created there.\nTry running terraform apply  to deploy this second EC2 Instance in the new\nworkspace:\n$ terraform apply\n(...)\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nRepeat the exercise one more time and create another workspace called “example2”:\n$ terraform workspace new example2\nCreated and switched to workspace ""example2""!\nYou're now on a new, empty workspace. Workspaces isolate their state,\nso if you run ""terraform plan"" Terraform will not see any existing state\nfor this configuration.\nRun terraform apply  again to deploy a third EC2 Instance:\n$ terraform apply\n(...)\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nY ou now have three workspaces available, which you can see by  using the terraform\nworkspace list  command:\n$ terraform workspace list\n  default\n  example1\n* example2\nAnd you can switch between them at any time using the terraform workspace\nselect  command:\n$ terraform workspace select example1\nSwitched to workspace ""example1"".\nTo understand how this works under the hood, take a look again in your S3 bucket;\nyou should now see a new folder called env:, as shown in Figure 3-5 .\nState File Isolation | 97\nFigure 3-5. When using custom workspaces, the S3 bucket will have multiple folders and\nstate files in it.\nInside the env: folder, you’ll find one folder for each of your workspaces, as shown in\nFigure 3-6 .\nFigure 3-6. Terraform creates one folder per workspace.\nInside each of those workspaces, Terraform uses the key you specified in your\nbackend  configuration, so you should find an example1/workspaces-example/terra‐\nform.tfstate  and an example2/workspaces-example/terraform.tfstate . In other words,\nswitching to a different workspace is equivalent to changing the path where your state\nfile is stored.\n98 | Chapter 3: How to Manage Terraform State\nThis is handy when you already have a Terraform module deployed and you want\nto do some experiments with it (e.g., try to refactor the code) but you don’t want\nyour experiments to affect the state of the already-deployed infrastructure. Terraform\nworkspaces allow you to run terraform workspace new  and deploy a new copy of\nthe exact same infrastructure, but storing the state in a separate file.\nIn fact, you can even change how that module behaves based on the workspace you’re\nin by reading the workspace name using the expression terraform.workspace . For\nexample, here’s how to set the Instance type to t2.medium  in the default workspace\nand t2.micro  in all other workspaces (e.g., to save money when experimenting):\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = terraform.workspace  == ""default"" ? ""t2.medium"" : ""t2.micro""\n}\nThe preceding code uses ternary syntax  to conditionally set instance_type  to either\nt2.medium  or t2.micro , depending on the value of terraform.workspace . Y ou’ll see\nthe full details of ternary syntax and conditional logic in Terraform in Chapter 5 .\nTerraform workspaces can be a great way to quickly spin up and tear down different\nversions of your code, but they have a few drawbacks:\n•The state files for all of your workspaces are stored in the same backend (e.g.,•\nthe same S3 bucket). That means you use the same authentication and access\ncontrols for all the workspaces, which is one major reason workspaces are an\nunsuitable mechanism for isolating environments (e.g., isolating staging from\nproduction).\n•Workspaces are not visible in the code or on the terminal unless you run terra •\nform workspace  commands. When browsing the code, a module that has been\ndeployed in one workspace looks exactly the same as a module deployed in 10\nworkspaces. This makes maintenance more difficult, because you don’t have a\ngood picture of your infrastructure.\n•Putting the two previous items together, the result is that workspaces can be•\nfairly error prone. The lack of visibility makes it easy to forget what workspace\nyou’re in and accidentally deploy changes in the wrong one (e.g., accidentally\nrunning terraform destroy  in a “production” workspace rather than a “staging”\nworkspace), and because you must use the same authentication mechanism for\nall workspaces, you have no other layers of defense to protect against such errors.\nState File Isolation | 99",10998
34-Isolation via File Layout.pdf,34-Isolation via File Layout,"5The workspaces documentation  makes this same exact point, but it’s buried among several paragraphs of text,\nand as workspaces used to be called “environments, ” I find many users are still confused about when and\nwhen not to use workspaces.Due to these drawbacks, workspaces are not a suitable mechanism for isolating one\nenvironment from another: e.g., isolating staging from production.5 To get proper\nisolation between environments, instead of workspaces, you’ll most likely want to use\nfile layout, which is the topic of the next section.\nBefore moving on, make sure to clean up the three EC2 Instances you just deployed\nby running terraform workspace select <name>  and terraform destroy  in each\nof the three workspaces.\nIsolation via File Layout\nTo achieve full isolation between environments, you need to do the following:\n•Put the Terraform configuration files for each environment into a separate folder.•\nFor example, all of the configurations for the staging environment can be in a\nfolder called stage  and all the configurations for the production environment can\nbe in a folder called prod .\n•Configure a different backend for each environment, using different authentica‐•\ntion mechanisms and access controls: e.g., each environment could live in a\nseparate AWS account with a separate S3 bucket as a backend.\nWith this approach, the use of separate folders makes it much clearer which envi‐\nronments you’re deploying to, and the use of separate state files, with separate\nauthentication mechanisms, makes it significantly less likely that a screw-up in one\nenvironment can have any impact on another.\nIn fact, you might want to take the isolation concept beyond environments and down\nto the “component” level, where a component is a coherent set of resources that\nyou typically deploy together. For example, after you’ve set up the basic network\ntopology for your infrastructure—in AWS lingo, your Virtual Private Cloud (VPC)\nand all the associated subnets, routing rules, VPNs, and network ACLs—you will\nprobably change it only once every few months, at most. On the other hand, you\nmight deploy a new version of a web server multiple times per day. If you manage\nthe infrastructure for both the VPC component and the web server component in\nthe same set of Terraform configurations, you are unnecessarily putting your entire\nnetwork topology at risk of breakage (e.g., from a simple typo in the code or someone\naccidentally running the wrong command) multiple times per day.\n100 | Chapter 3: How to Manage Terraform State\nTherefore, I recommend using separate Terraform folders (and therefore separate\nstate files) for each environment (staging, production, etc.) and for each component\n(VPC, services, databases) within that environment. To see what this looks like in\npractice, let’s go through the recommended file layout for Terraform projects.\nFigure 3-7  shows the file layout for my typical Terraform project.\nFigure 3-7. The typical file layout for a Terraform project uses separate folders for each\nenvironment and for each component within that environment.\nState File Isolation | 101\nAt the top level, there are separate folders for each “environment. ” The exact environ‐\nments differ for every project, but the typical ones are as follows:\nstage\nAn environment for pre-production workloads (i.e., testing)\nprod\nAn environment for production workloads (i.e., user-facing apps)\nmgmt\nAn environment for DevOps tooling (e.g., bastion host, CI server)\nglobal\nA place to put resources that are used across all environments (e.g., S3, IAM)\nWithin each environment, there are separate folders for each “component. ” The\ncomponents differ for every project, but here are the typical ones:\nvpc\nThe network topology for this environment.\nservices\nThe apps or microservices to run in this environment, such as a Ruby on Rails\nfrontend or a Scala backend. Each app could even live in its own folder to isolate\nit from all the other apps.\ndata-storage\nThe data stores to run in this environment, such as MySQL or Redis. Each data\nstore could even reside in its own folder to isolate it from all other data stores.\nWithin each component, there are the actual Terraform configuration files, which are\norganized according to the following naming conventions:\nvariables.tf\nInput variables\noutputs.tf\nOutput variables\nmain.tf\nResources and data sources\nWhen you run Terraform, it simply looks for files in the current directory with\nthe .tf extension, so you can use whatever filenames you want. However, although\nTerraform may not care about filenames, your teammates probably do. Using a\nconsistent, predictable naming convention makes your code easier to browse: e.g.,\nyou’ll always know where to look to find a variable, output, or resource.\n102 | Chapter 3: How to Manage Terraform State\nNote that the preceding convention is the minimum  convention you should follow,\nbecause in virtually all uses of Terraform, it’s useful to be able to jump to the input\nvariables, output variables, and resources very quickly, but you may want to go\nbeyond this convention. Here are just a few examples:\ndependencies.tf\nIt’s common to put all your data sources in a dependencies.tf  file to make it easier\nto see what external things the code depends on.\nproviders.tf\nY ou may want to put your provider  blocks into a providers.tf  file so you can see,\nat a glance, what providers the code talks to and what authentication you’ll have\nto provide.\nmain-xxx.tf\nIf the main.tf  file is getting really long because it contains a large number of\nresources, you could break it down into smaller files that group the resources\nin some logical way: e.g., main-iam.tf  could contain all the IAM resources, main-\ns3.tf could contain all the S3 resources, and so on. Using the main-  prefix makes\nit easier to scan the list of files in a folder when they are organized alphabetically,\nas all the resources will be grouped together. It’s also worth noting that if you find\nyourself managing a very large number of resources and struggling to break them\ndown across many files, that might be a sign that you should break your code\ninto smaller modules instead, which is a topic I’ll dive into in Chapter 4 .\nLet’s take the web server cluster code you wrote in Chapter 2 , plus the Amazon S3\nand DynamoDB code you wrote in this chapter, and rearrange it using the folder\nstructure in Figure 3-8 .\nFigure 3-8. Move the web server cluster code into a stage/services/webserver-cluster\nfolder to indicate that this is a testing or staging version of the web server.\nState File Isolation | 103\nThe S3 bucket you created in this chapter should be moved into the global/s3\nfolder. Move the output variables ( s3_bucket_arn  and dynamodb_table_name )\ninto outputs.tf . When moving the folder, make sure that you don’t miss the (hid‐\nden) .terraform  folder when copying files to the new location so you don’t need to\nreinitialize everything.\nThe web server cluster you created in Chapter 2  should be moved into stage/services/\nwebserver-cluster  (think of this as the “testing” or “staging” version of that web server\ncluster; you’ll add a “production” version in the next chapter). Again, make sure\nto copy over the .terraform  folder, move input variables into variables.tf , and move\noutput variables into outputs.tf .\nY ou should also update the web server cluster to use S3 as a backend . Y ou can\ncopy and paste the backend  config from global/s3/main.tf  more or less verbatim, but\nmake sure to change the key to the same folder path as the web server Terraform\ncode: stage/services/webserver-cluster/terraform.tfstate . This gives you a 1:1 mapping\nbetween the layout of your Terraform code in version control and your Terraform\nstate files in S3, so it’s obvious how the two are connected. The s3 module already sets\nthe key using this convention.\nThis file layout has a number of advantages:\nClear code / environment layout\nIt’s easy to browse the code and understand exactly what components are\ndeployed in each environment.\nIsolation\nThis layout provides a good amount of isolation between environments and\nbetween components within an environment, ensuring that if something goes\nwrong, the damage is contained as much as possible to just one small part of your\nentire infrastructure.\nIn some ways, these advantages are drawbacks, too:\nWorking with multiple folders\nSplitting components into separate folders prevents you from accidentally blow‐\ning up your entire infrastructure in one command, but it also prevents you from\ncreating your entire infrastructure in one command. If all of the components for\na single environment were defined in a single Terraform configuration, you could\nspin up an entire environment with a single call to terraform apply . But if all of\nthe components are in separate folders, then you need to run terraform apply\nseparately in each one.\nSolution : If you use Terragrunt, you can run commands across multiple folders\nconcurrently using the run-all  command .\n104 | Chapter 3: How to Manage Terraform State",9180
35-The terraform_remote_state Data Source.pdf,35-The terraform_remote_state Data Source,"Copy/paste\nThe file layout described in this section has a lot of duplication. For example, the\nsame frontend-app  and backend-app  live in both the stage  and prod  folders.\nSolution : Y ou won’t actually need to copy and paste all of that code! In Chapter 4 ,\nyou’ll see how to use Terraform modules to keep all of this code DRY .\nResource dependencies\nBreaking the code into multiple folders makes it more difficult to use resource\ndependencies. If your app code was defined in the same Terraform configura‐\ntion files as the database code, that app code could directly access attributes of\nthe database using an attribute reference (e.g., access the database address via\naws_db_instance.foo.address ). But if the app code and database code live in\ndifferent folders, as I’ve recommended, you can no longer do that.\nSolution : One option is to use dependency  blocks in Terragrunt, as you’ll see in\nChapter 10 . Another option is to use the terraform_remote_state  data source,\nas described in the next section.\nThe terraform_remote_state Data Source\nIn Chapter 2 , you used data sources to fetch read-only information from AWS,\nsuch as the aws_subnets  data source, which returns a list of subnets in your VPC.\nThere is another data source that is particularly useful when working with state:\nterraform_remote_state . Y ou can use this data source to fetch the Terraform state\nfile stored by another set of Terraform configurations.\nLet’s go through an example. Imagine that your web server cluster needs to commu‐\nnicate with a MySQL database. Running a database that is scalable, secure, durable,\nand highly available is a lot of work. Again, you can let AWS take care of it for\nyou, this time by using Amazon’s Relational Database Service  (RDS), as shown in\nFigure 3-9 . RDS supports a variety of databases, including MySQL, PostgreSQL, SQL\nServer, and Oracle.\nY ou might not want to define the MySQL database in the same set of configuration\nfiles as the web server cluster, because you’ll be deploying updates to the web server\ncluster far more frequently and don’t want to risk accidentally breaking the database\neach time you do so.\nThe terraform_remote_state Data Source | 105\nFigure 3-9. The web server cluster communicates with MySQL, which is deployed on top\nof Amazon RDS.\nTherefore, your first step should be to create a new folder at stage/data-stores/mysql\nand create the basic Terraform files ( main.tf , variables.tf , outputs.tf ) within it, as\nshown in Figure 3-10 .\nFigure 3-10. Create the database code in the stage/data-stores folder.\n106 | Chapter 3: How to Manage Terraform State\nNext, create the database resources in stage/data-stores/mysql/main.tf :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = ""example_database""\n  # How should we set the username and password?\n  username  = ""???""\n  password  = ""???""\n}\nAt the top of the file, you see the typical provider  block, but just below that is a\nnew resource: aws_db_instance . This resource creates a database in RDS with the\nfollowing settings:\n•MySQL as the database engine.•\n•10 GB of storage.•\n•A db.t2.micro  Instance, which has one virtual CPU, 1 GB of memory, and is •\npart of the AWS Free Tier.\n•The final snapshot is disabled, as this code is just for learning and testing (if•\nyou don’t disable the snapshot, or don’t provide a name for the snapshot via the\nfinal_snapshot_identifier  parameter, destroy  will fail).\nNote that two of the parameters that you must pass to the aws_db_instance  resource\nare the master username and master password. Because these are secrets, you should\nnot put them directly into your code in plain text! In Chapter 6 , I’ll discuss a variety\nof options for how to securely handle secrets with Terraform. For now, let’s use an\noption that avoids storing any secrets in plain text and is easy to use: you store\nyour secrets, such as database passwords, outside of Terraform (e.g., in a password\nmanager such as 1Password, LastPass, or macOS Keychain), and you pass those\nsecrets into Terraform via environment variables.\nTo do that, declare variables called db_username  and db_password  in stage/data-\nstores/mysql/variables.tf :\nvariable  ""db_username""  {\n  description  = ""The username for the database""\n  type        = string\n  sensitive    = true\n}\nThe terraform_remote_state Data Source | 107\nvariable  ""db_password""  {\n  description  = ""The password for the database""\n  type        = string\n  sensitive    = true\n}\nFirst, note that these variables are marked with sensitive = true  to indicate they\ncontain secrets. This ensures Terraform won’t log the values when you run plan  or\napply . Second, note that these variables do not have a default . This is intentional.\nY ou should not store your database credentials or any sensitive information in plain\ntext. Instead, you’ll set these variables using environment variables.\nBefore doing that, let’s finish the code. First, pass the two new input variables through\nto the aws_db_instance  resource:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = ""example_database""\n  username  = var.db_username\n  password  = var.db_password\n}\nNext, configure this module to store its state in the S3 bucket you created earlier at\nthe path stage/data-stores/mysql/terraform.tfstate :\nterraform  {\n  backend ""s3"" {\n    # Replace this with your bucket name!\n    bucket          = ""terraform-up-and-running-state""\n    key            = ""stage/data-stores/mysql/terraform.tfstate""\n    region          = ""us-east-2""\n    # Replace this with your DynamoDB table name!\n    dynamodb_table  = ""terraform-up-and-running-locks""\n    encrypt         = true\n  }\n}\nFinally, add two output variables in stage/data-stores/mysql/outputs.tf  to return the\ndatabase’s address and port:\noutput ""address""  {\n  value       = aws_db_instance.example.address\n  description  = ""Connect to the database at this endpoint""\n}\n108 | Chapter 3: How to Manage Terraform State\noutput ""port"" {\n  value       = aws_db_instance.example.port\n  description  = ""The port the database is listening on""\n}\nY ou’re now ready to pass in the database username and password using environment\nvariables. As a reminder, for each input variable foo defined in your Terraform\nconfigurations, you can provide Terraform the value of this variable using the envi‐\nronment variable TF_VAR_foo . For the db_username  and db_password  input variables,\nhere is how you can set the TF_VAR_db_username  and TF_VAR_db_password  environ‐\nment variables on Linux/Unix/macOS systems:\n$ export TF_VAR_db_username=""(YOUR_DB_USERNAME)""\n$ export TF_VAR_db_password=""(YOUR_DB_PASSWORD)""\nAnd here is how you do it on Windows systems:\n$ set TF_VAR_db_username=""(YOUR_DB_USERNAME)""\n$ set TF_VAR_db_password=""(YOUR_DB_PASSWORD)""\nRun terraform init  and terraform apply  to create the database. Note that Amazon\nRDS can take roughly 10 minutes to provision even a small database, so be patient.\nAfter apply  completes, you should see the outputs in the terminal:\n$ terraform apply\n(...)\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nOutputs:\naddress = ""terraform-up-and-running.cowu6mts6srx.us-east-2.rds.amazonaws.com""\nport = 3306\nThese outputs are now also stored in the Terraform state for the database, which is in\nyour S3 bucket at the path stage/data-stores/mysql/terraform.tfstate .\nIf you go back to your web server cluster code, you can get the web server to read\nthose outputs from the database’s state file by adding the terraform_remote_state\ndata source in stage/services/webserver-cluster/main.tf :\ndata ""terraform_remote_state"" ""db""  {\n  backend  = ""s3""\n  config  = {\n    bucket  = ""(YOUR_BUCKET_NAME)""\n    key    = ""stage/data-stores/mysql/terraform.tfstate""\n    region  = ""us-east-2""\n  }\n}\nThe terraform_remote_state Data Source | 109\nThis terraform_remote_state  data source configures the web server cluster code to\nread the state file from the same S3 bucket and folder where the database stores its\nstate, as shown in Figure 3-11 .\nFigure 3-11. The database writes its state to an S3 bucket (top), and the web server\ncluster reads that state from the same bucket (bottom).\nIt’s important to understand that, like all Terraform data sources, the data returned\nby terraform_remote_state  is read-only. Nothing you do in your web server cluster\nTerraform code can modify that state, so you can pull in the database’s state data with\nno risk of causing any problems in the database itself.\nAll of the database’s output variables are stored in the state file, and you can read\nthem from the terraform_remote_state  data source using an attribute reference of\nthe form:\ndata.terraform_remote_state .<NAME>.outputs.<ATTRIBUTE>\nFor example, here is how you can update the User Data of the web server cluster\nInstances to pull the database address and port out of the terraform_remote_state\ndata source and expose that information in the HTTP response:\n110 | Chapter 3: How to Manage Terraform State\n6Y ou can find documentation for the sprintf  syntax on the Go website .\n7The full list of built-in functions is available on the Terraform website .user_data  = <<EOF\n#!/bin/bash\necho ""Hello, World""  >> index.html\necho ""${data.terraform_remote_state.db.outputs.address}""  >> index.html\necho ""${data.terraform_remote_state.db.outputs.port}""  >> index.html\nnohup busybox httpd -f -p ${var.server_port}  &\nEOF\nAs the User Data script is growing longer, defining it inline is becoming messier and\nmessier. In general, embedding one programming language (Bash) inside another\n(Terraform) makes it more difficult to maintain each one, so let’s pause here for a\nmoment to externalize the Bash script. To do that, you can use the templatefile\nbuilt-in function.\nTerraform includes a number of built-in functions  that you can execute using an\nexpression of the form:\nfunction_name (...)\nFor example, consider the format  function:\nformat(<FMT>, <ARGS>, ...)\nThis function formats the arguments in ARGS  according to the sprintf  syntax in the\nstring FMT.6 A great way to experiment with built-in functions is to run the terraform\nconsole  command to get an interactive console where you can try out Terraform\nsyntax, query the state of your infrastructure, and see the results instantly:\n$ terraform console\n> format(""%.3f"", 3.14159265359)\n3.142\nNote that the Terraform console is read-only, so you don’t need to worry about\naccidentally changing infrastructure or state.\nThere are a number of other built-in functions that you can use to manipulate strings,\nnumbers, lists, and maps.7 One of them is the templatefile  function:\ntemplatefile (<PATH>, <VARS>)\nThis function reads the file at PATH , renders it as a template, and returns the result as\na string. When I say “renders it as a template, ” what I mean is that the file at PATH  can\nuse the string interpolation syntax in Terraform ( ${...} ), and Terraform will render\nthe contents of that file, filling variable references from VARS .\nThe terraform_remote_state Data Source | 111\nTo see this in action, put the contents of the User Data script into the file stage/\nservices/webserver-cluster/user-data.sh  as follows:\n#!/bin/bash\ncat > index.html <<EOF\n<h1>Hello, World</h1>\n<p>DB address: ${db_address}</p>\n<p>DB port: ${db_port}</p>\nEOF\nnohup busybox httpd -f -p ${server_port } &\nNote that this Bash script has a few changes from the original:\n•It looks up variables using Terraform’s standard interpolation syntax, except the•\nonly variables it has access to are those you pass in via the second parameter to\ntemplatefile  (as you’ll see shortly), so you don’t need any prefix to access them:\nfor example, you should use ${server_port}  and not ${var.server_port} .\n•The script now includes some HTML syntax (e.g., <h1> ) to make the output a bit •\nmore readable in a web browser.\nThe final step is to update the user_data  parameter of the aws_launch_configura\ntion  resource to call the templatefile  function and pass in the variables it needs as a\nmap:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = ""ami-0fb653ca2d3203ac1""\n  instance_type    = ""t2.micro""\n  security_groups  = [aws_security_group.instance.id ]\n  # Render the User Data script as a template\n  user_data  = templatefile (""user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = data.terraform_remote_state.db.outputs.address\n    db_port      = data.terraform_remote_state.db.outputs.port\n  })\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n  }\n}\nAh, that’s much cleaner than writing Bash scripts inline!\nIf you deploy this cluster using terraform apply , wait for the Instances to register\nin the ALB, and open the ALB URL in a web browser, you’ll see something similar to\nFigure 3-12 .\n112 | Chapter 3: How to Manage Terraform State",13484
36-Conclusion.pdf,36-Conclusion,"8For more information on software safety mechanisms, see Agility Requires Safety .Congrats, your web server cluster can now programmatically access the database\naddress and port via Terraform. If you were using a real web framework (e.g., Ruby\non Rails), you could set the address and port as environment variables or write them\nto a config file so that they could be used by your database library (e.g., ActiveRecord)\nto communicate with the database.\nFigure 3-12. The web server cluster can programmatically access the database address\nand port.\nConclusion\nThe reason you need to put so much thought into isolation, locking, and state is\nthat infrastructure as code (IaC) has different trade-offs than normal coding. When\nyou’re writing code for a typical app, most bugs are relatively minor and break only a\nsmall part of a single app. When you’re writing code that controls your infrastructure,\nbugs tend to be more severe, given that they can break all of your apps—and all of\nyour data stores, and your entire network topology, and just about everything else.\nTherefore, I recommend including more “safety mechanisms” when working on IaC\nthan with typical code.8\nA common concern of using the recommended file layout is that it leads to code\nduplication. If you want to run the web server cluster in both staging and production,\nhow do you avoid having to copy and paste a lot of code between stage/services/\nwebserver-cluster  and prod/services/webserver-cluster ? The answer is that you need to\nuse Terraform modules, which are the main topic of Chapter 4 .\nConclusion | 113",1603
37-Chapter 4. How to Create Reusable Infrastructure with Terraform Modules.pdf,37-Chapter 4. How to Create Reusable Infrastructure with Terraform Modules,"CHAPTER 4\nHow to Create Reusable Infrastructure\nwith Terraform Modules\nAt the end of Chapter 3 , you deployed the architecture shown in Figure 4-1 .\nFigure 4-1. The architecture you deployed in previous chapters included a load balancer,\nweb server cluster, and database.\nThis works great as a first environment, but you typically need at least two environ‐\nments: one for your team’s internal testing (“staging”) and one that real users can\naccess (“production”), as shown in Figure 4-2 . Ideally, the two environments are\nnearly identical, though you might run slightly fewer/smaller servers in staging to\nsave money.\n115\nFigure 4-2. The architecture you’ll deploy in this chapter will have two environments,\neach with its own load balancer, web server cluster, and database.\nHow do you add this production environment without having to copy and paste all of\nthe code from staging? For example, how do you avoid having to copy and paste all\nthe code in stage/services/webserver-cluster  into prod/services/webserver-cluster  and all\nthe code in stage/data-stores/mysql  into prod/data-stores/mysql ?\nIn a general-purpose programming language such as Ruby, if you had the same code\ncopied and pasted in several places, you could put that code inside of a function and\nreuse that function everywhere:\n116 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\n# Define the function in one place\ndef example_function ()\n  puts ""Hello, World""\nend\n# Use the function in multiple other places\nexample_function ()\nWith Terraform, you can put your code inside of a Terraform module  and reuse that\nmodule in multiple places throughout your code. Instead of having the same code\ncopied and pasted in the staging and production environments, you’ll be able to have\nboth environments reuse code from the same module, as shown in Figure 4-3 .\nFigure 4-3. Putting code into modules allows you to reuse that code from multiple\nenvironments.\nThis is a big deal. Modules are the key ingredient to writing reusable, maintainable,\nand testable Terraform code. Once you start using them, there’s no going back. Y ou’ll\nstart building everything as a module, creating a library of modules to share within\nyour company, using modules that you find online, and thinking of your entire\ninfrastructure as a collection of reusable modules.\nHow to Create Reusable Infrastructure with Terraform Modules | 117",2441
38-Module Basics.pdf,38-Module Basics,"In this chapter, I’ll show you how to create and use Terraform modules by covering\nthe following topics:\n•Module basics•\n•Module inputs•\n•Module locals•\n•Module outputs•\n•Module gotchas•\n•Module versioning•\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\nModule Basics\nA Terraform module is very simple: any set of Terraform configuration files in a\nfolder is a module. All of the configurations you’ve written so far have technically\nbeen modules, although not particularly interesting ones, since you deployed them\ndirectly: if you run apply  directly on a module, it’s referred to as a root module . To see\nwhat modules are really capable of, you need to create a reusable module , which is a\nmodule that is meant to be used within other modules.\nAs an example, let’s turn the code in stage/services/webserver-cluster , which includes\nan Auto Scaling Group (ASG), Application Load Balancer (ALB), security groups,\nand many other resources, into a reusable module.\nAs a first step, run terraform destroy  in the stage/services/webserver-cluster  to\nclean up any resources that you created earlier. Next, create a new top-level folder\ncalled modules , and move all of the files from stage/services/webserver-cluster  to mod‐\nules/services/webserver-cluster . Y ou should end up with a folder structure that looks\nsomething like Figure 4-4 .\nOpen up the main.tf  file in modules/services/webserver-cluster , and remove the pro\nvider  definition. Providers should be configured only in root modules and not\nin reusable modules (you’ll learn a lot more about working with providers in\nChapter 7 ).\n118 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\nFigure 4-4. Move your reusable web server cluster code into a modules/services/web\nserver-cluster folder.\nY ou can now make use of this module in the staging environment. Here’s the syntax\nfor using a module:\nmodule ""<NAME>""  {\n  source  = ""<SOURCE>""\n  [CONFIG ...]\n}\nwhere NAME  is an identifier you can use throughout the Terraform code to refer\nto this module (e.g., webserver_cluster ), SOURCE  is the path where the module\ncode can be found (e.g., modules/services/webserver-cluster ), and CONFIG  consists of\narguments that are specific to that module. For example, you can create a new file in\nstage/services/webserver-cluster/main.tf  and use the webserver-cluster  module in it\nas follows:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""webserver_cluster""  {\nModule Basics | 119\n  source  = ""../../../modules/services/webserver-cluster""\n}\nY ou can then reuse the exact same module in the production environment by creating\na new prod/services/webserver-cluster/main.tf  file with the following contents:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n}\nAnd there you have it: code reuse in multiple environments that involves minimal\nduplication. Note that whenever you add a module to your Terraform configurations\nor modify the source  parameter of a module, you need to run the init  command\nbefore you run plan  or apply :\n$ terraform init\nInitializing modules...\n- webserver_cluster in ../../../modules/services/webserver-cluster\nInitializing the backend...\nInitializing provider plugins...\nTerraform has been successfully initialized!\nNow you’ve seen all the tricks the init  command has up its sleeve: it installs pro‐\nviders, it configures your backends, and it downloads modules, all in one handy\ncommand.\nBefore you run the apply  command on this code, be aware that there is a problem\nwith the webserver-cluster  module: all of the names are hardcoded. That is, the\nname of the security groups, ALB, and other resources are all hardcoded, so if\nyou use this module more than once in the same AWS account, you’ll get name\nconflict errors. Even the details for how to read the database’s state are hardcoded\nbecause the main.tf  file you copied into modules/services/webserver-cluster  is using a\nterraform_remote_state  data source to figure out the database address and port,\nand that terraform_remote_state  is hardcoded to look at the staging environment.\nTo fix these issues, you need to add configurable inputs to the webserver-cluster\nmodule so that it can behave differently in different environments.\n120 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",4494
39-Module Inputs.pdf,39-Module Inputs,"Module Inputs\nTo make a function configurable in a general-purpose programming language such as\nRuby, you can add input parameters to that function:\n# A function with two input parameters\ndef example_function (param1, param2)\n  puts ""Hello, #{param1} #{param2}""\nend\n# Pass two input parameters to the function\nexample_function (""foo"", ""bar"")\nIn Terraform, modules can have input parameters, too. To define them, you use\na mechanism you’re already familiar with: input variables. Open up modules/serv‐\nices/webserver-cluster/variables.tf  and add three new input variables:\nvariable  ""cluster_name""  {\n  description  = ""The name to use for all the cluster resources""\n  type        = string\n}\nvariable  ""db_remote_state_bucket""  {\n  description  = ""The name of the S3 bucket for the database's remote state""\n  type        = string\n}\nvariable  ""db_remote_state_key""  {\n  description  = ""The path for the database's remote state in S3""\n  type        = string\n}\nNext, go through modules/services/webserver-cluster/main.tf , and use var.clus\nter_name  instead of the hardcoded names (e.g., instead of ""terraform-asg-\nexample"" ). For example, here is how you do it for the ALB security group:\nresource  ""aws_security_group"" ""alb""  {\n  name = ""${var.cluster_name}-alb""\n  ingress {\n    from_port    = 80\n    to_port      = 80\n    protocol     = ""tcp""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n  egress {\n    from_port    = 0\n    to_port      = 0\n    protocol     = ""-1""\n    cidr_blocks  = [""0.0.0.0/0"" ]\nModule Inputs | 121\n  }\n}\nNotice how the name  parameter is set to ""${var.cluster_name}-alb"" . Y ou’ll need to\nmake a similar change to the other aws_security_group  resource (e.g., give it the\nname ""${var.cluster_name}-instance"" ), the aws_alb  resource, and the tag section\nof the aws_autoscaling_group  resource.\nY ou should also update the terraform_remote_state  data source to use the\ndb_remote_state_bucket  and db_remote_state_key  as its bucket  and key param‐\neter, respectively, to ensure you’re reading the state file from the right environment:\ndata ""terraform_remote_state"" ""db""  {\n  backend  = ""s3""\n  config  = {\n    bucket  = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n    region  = ""us-east-2""\n  }\n}\nNow, in the staging environment, in stage/services/webserver-cluster/main.tf , you can\nset these new input variables accordingly:\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n}\nY ou should also set these variables in the production environment in prod/services/\nwebserver-cluster/main.tf  but to different values that correspond to that environment:\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-prod""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""prod/data-stores/mysql/terraform.tfstate""\n}\nThe production database doesn’t actually exist yet. As an exercise,\nI leave it up to you to add a production database similar to the\nstaging one.\n122 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\nAs you can see, you set input variables for a module by using the same syntax as\nsetting arguments for a resource. The input variables are the API of the module,\ncontrolling how it will behave in different environments.\nSo far, you’ve added input variables for the name and database remote state, but you\nmay want to make other parameters configurable in your module, too. For example,\nin staging, you might want to run a small web server cluster to save money, but\nin production, you might want to run a larger cluster to handle lots of traffic. To\ndo that, you can add three more input variables to modules/services/webserver-cluster/\nvariables.tf :\nvariable  ""instance_type""  {\n  description  = ""The type of EC2 Instances to run (e.g. t2.micro)""\n  type        = string\n}\nvariable  ""min_size""  {\n  description  = ""The minimum number of EC2 Instances in the ASG""\n  type        = number\n}\nvariable  ""max_size""  {\n  description  = ""The maximum number of EC2 Instances in the ASG""\n  type        = number\n}\nNext, update the launch configuration in modules/services/webserver-cluster/main.tf  to\nset its instance_type  parameter to the new var.instance_type  input variable:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = ""ami-0fb653ca2d3203ac1""\n  instance_type    = var.instance_type\n  security_groups  = [aws_security_group.instance.id ]\n  user_data  = templatefile (""user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = data.terraform_remote_state.db.outputs.address\n    db_port      = data.terraform_remote_state.db.outputs.port\n  })\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n  }\n}\nSimilarly, you should update the ASG definition in the same file to set its min_size\nand max_size  parameters to the new var.min_size  and var.max_size  input vari‐\nables, respectively:\nModule Inputs | 123\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  tag {\n    key                 = ""Name""\n    value                = var.cluster_name\n    propagate_at_launch  = true\n  }\n}\nNow, in the staging environment ( stage/services/webserver-cluster/main.tf ), you can\nkeep the cluster small and inexpensive by setting instance_type  to ""t2.micro""  and\nmin_size  and max_size  to 2:\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type  = ""t2.micro""\n  min_size       = 2\n  max_size       = 2\n}\nOn the other hand, in the production environment, you can use a larger\ninstance_type  with more CPU and memory, such as m4.large  (be aware that this\nInstance type is not part of the AWS Free Tier, so if you’re just using this for learning\nand don’t want to be charged, stick with ""t2.micro""  for the instance_type ), and you\ncan set max_size  to 10 to allow the cluster to shrink or grow depending on the load\n(don’t worry, the cluster will launch with two Instances initially):\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-prod""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""prod/data-stores/mysql/terraform.tfstate""\n  instance_type  = ""m4.large""\n  min_size       = 2\n  max_size       = 10\n}\n124 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",7190
40-Module Locals.pdf,40-Module Locals,"Module Locals\nUsing input variables to define your module’s inputs is great, but what if you need\na way to define a variable in your module to do some intermediary calculation,\nor just to keep your code DRY , but you don’t want to expose that variable as a con‐\nfigurable input? For example, the load balancer in the webserver-cluster  module\nin modules/services/webserver-cluster/main.tf  listens on port 80, the default port for\nHTTP . This port number is currently copied and pasted in multiple places, including\nthe load balancer listener:\nresource  ""aws_lb_listener"" ""http""  {\n  load_balancer_arn  = aws_lb.example.arn\n  port              = 80\n  protocol           = ""HTTP""\n  # By default, return a simple 404 page\n  default_action  {\n    type  = ""fixed-response""\n    fixed_response  {\n      content_type  = ""text/plain""\n      message_body  = ""404: page not found""\n      status_code   = 404\n    }\n  }\n}\nAnd the load balancer security group:\nresource  ""aws_security_group"" ""alb""  {\n  name = ""${var.cluster_name}-alb""\n  ingress {\n    from_port    = 80\n    to_port      = 80\n    protocol     = ""tcp""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n  egress {\n    from_port    = 0\n    to_port      = 0\n    protocol     = ""-1""\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n}\nThe values in the security group, including the “all IPs” CIDR block 0.0.0.0/0 , the\n“any port” value of 0, and the “any protocol” value of ""-1""  are also copied and pasted\nin several places throughout the module. Having these magical values hardcoded\nin multiple places makes the code more difficult to read and maintain. Y ou could\nModule Locals | 125\nextract values into input variables, but then users of your module will be able to\n(accidentally) override these values, which you might not want. Instead of using input\nvariables, you can define these as local values  in a locals  block:\nlocals {\n  http_port     = 80\n  any_port      = 0\n  any_protocol  = ""-1""\n  tcp_protocol  = ""tcp""\n  all_ips       = [""0.0.0.0/0"" ]\n}\nLocal values allow you to assign a name to any Terraform expression and to use that\nname throughout the module. These names are visible only within the module, so\nthey will have no impact on other modules, and you can’t override these values from\noutside of the module. To read the value of a local, you need to use a local reference ,\nwhich uses the following syntax:\nlocal.<NAME>\nUse this syntax to update the port  parameter of your load-balancer listener:\nresource  ""aws_lb_listener"" ""http""  {\n  load_balancer_arn  = aws_lb.example.arn\n  port              = local.http_port\n  protocol           = ""HTTP""\n  # By default, return a simple 404 page\n  default_action  {\n    type  = ""fixed-response""\n    fixed_response  {\n      content_type  = ""text/plain""\n      message_body  = ""404: page not found""\n      status_code   = 404\n    }\n  }\n}\nSimilarly, update virtually all the parameters in the security groups in the module,\nincluding the load-balancer security group:\nresource  ""aws_security_group"" ""alb""  {\n  name = ""${var.cluster_name}-alb""\n  ingress {\n    from_port    = local.http_port\n    to_port      = local.http_port\n    protocol     = local.tcp_protocol\n    cidr_blocks  = local.all_ips\n  }\n  egress {\n126 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",3353
41-Module Outputs.pdf,41-Module Outputs,"from_port    = local.any_port\n    to_port      = local.any_port\n    protocol     = local.any_protocol\n    cidr_blocks  = local.all_ips\n  }\n}\nLocals make your code easier to read and maintain, so use them often.\nModule Outputs\nA powerful feature of ASGs is that you can configure them to increase or decrease the\nnumber of servers you have running in response to load. One way to do this is to use\na scheduled action , which can change the size of the cluster at a scheduled time during\nthe day. For example, if traffic to your cluster is much higher during normal business\nhours, you can use a scheduled action to increase the number of servers at 9 a.m. and\ndecrease it at 5 p.m.\nIf you define the scheduled action in the webserver-cluster  module, it would apply\nto both staging and production. Because you don’t need to do this sort of scaling\nin your staging environment, for the time being, you can define the auto scaling\nschedule directly in the production configurations (in Chapter 5 , you’ll see how to\nconditionally define resources, which lets you move the scheduled action into the\nwebserver-cluster  module).\nTo define a scheduled action, add the following two aws_autoscaling_schedule\nresources to prod/services/webserver-cluster/main.tf :\nresource  ""aws_autoscaling_schedule"" ""scale_out_during_business_hours""  {\n  scheduled_action_name  = ""scale-out-during-business-hours""\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 10\n  recurrence             = ""0 9 * * *""\n}\nresource  ""aws_autoscaling_schedule"" ""scale_in_at_night""  {\n  scheduled_action_name  = ""scale-in-at-night""\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 2\n  recurrence             = ""0 17 * * *""\n}\nThis code uses one aws_autoscaling_schedule  resource to increase the number of\nservers to 10 during the morning hours (the recurrence  parameter uses cron syntax,\nso ""0 9 * * *""  means “9 a.m. every day”) and a second aws_autoscaling_schedule\nresource to decrease the number of servers at night ( ""0 17 * * *""  means “5\np.m. every day”). However, both usages of aws_autoscaling_schedule  are missing a\nModule Outputs | 127\nrequired parameter, autoscaling_group_name , which specifies the name of the ASG.\nThe ASG itself is defined within the webserver-cluster  module, so how do you\naccess its name? In a general-purpose programming language such as Ruby, functions\ncan return values:\n# A function that returns a value\ndef example_function (param1, param2)\n  return ""Hello, #{param1} #{param2}""\nend\n# Call the function and get the return value\nreturn_value  = example_function (""foo"", ""bar"")\nIn Terraform, a module can also return values. Again, you do this using a mechanism\nyou already know: output variables. Y ou can add the ASG name as an output variable\nin /modules/services/webserver-cluster/outputs.tf  as follows:\noutput ""asg_name""  {\n  value       = aws_autoscaling_group.example.name\n  description  = ""The name of the Auto Scaling Group""\n}\nY ou can access module output variables using the following syntax:\nmodule.<MODULE_NAME> .<OUTPUT_NAME>\nFor example:\nmodule.frontend .asg_name\nIn prod/services/webserver-cluster/main.tf , you can use this syntax to set the autoscal\ning_group_name  parameter in each of the aws_autoscaling_schedule  resources:\nresource  ""aws_autoscaling_schedule"" ""scale_out_during_business_hours""  {\n  scheduled_action_name  = ""scale-out-during-business-hours""\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 10\n  recurrence             = ""0 9 * * *""\n  autoscaling_group_name  = module.webserver_cluster .asg_name\n}\nresource  ""aws_autoscaling_schedule"" ""scale_in_at_night""  {\n  scheduled_action_name  = ""scale-in-at-night""\n  min_size               = 2\n  max_size               = 10\n  desired_capacity       = 2\n  recurrence             = ""0 17 * * *""\n  autoscaling_group_name  = module.webserver_cluster .asg_name\n}\n128 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",4105
42-Module Gotchas.pdf,42-Module Gotchas,,0
43-Inline Blocks.pdf,43-Inline Blocks,"Y ou might want to expose one other output in the webserver-cluster  module: the\nDNS name of the ALB, so you know what URL to test when the cluster is deployed.\nTo do that, you again add an output variable in /modules/services/webserver-cluster/\noutputs.tf :\noutput ""alb_dns_name""  {\n  value       = aws_lb.example.dns_name\n  description  = ""The domain name of the load balancer""\n}\nY ou can then “pass through” this output in stage/services/webserver-cluster/outputs.tf\nand prod/services/webserver-cluster/outputs.tf  as follows:\noutput ""alb_dns_name""  {\n  value       = module.webserver_cluster .alb_dns_name\n  description  = ""The domain name of the load balancer""\n}\nY our web server cluster is almost ready to deploy. The only thing left is to take a few\ngotchas into account.\nModule Gotchas\nWhen creating modules, watch out for these gotchas:\n•File paths•\n•Inline blocks•\nFile Paths\nIn Chapter 3 , you moved the User Data script for the web server cluster into an\nexternal file, user-data.sh , and used the templatefile  built-in function to read this\nfile from disk. The catch with the templatefile  function is that the filepath you use\nmust be a relative path (you don’t want to use absolute file paths, as your Terraform\ncode may run on many different computers, each with a different disk layout)—but\nwhat is it relative to?\nBy default, Terraform interprets the path relative to the current working directory.\nThat works if you’re using the templatefile  function in a Terraform configuration\nfile that’s in the same directory as where you’re running terraform apply  (that is,\nif you’re using the templatefile  function in the root module), but that won’t work\nwhen you’re using templatefile  in a module that’s defined in a separate folder (a\nreusable module).\nModule Gotchas | 129\nTo solve this issue, you can use an expression known as a path reference , which is of\nthe form path.<TYPE> . Terraform supports the following types of path references:\npath.module\nReturns the filesystem path of the module where the expression is defined.\npath.root\nReturns the filesystem path of the root module.\npath.cwd\nReturns the filesystem path of the current working directory. In normal use of\nTerraform, this is the same as path.root , but some advanced uses of Terraform\nrun it from a directory other than the root module directory, causing these paths\nto be different.\nFor the User Data script, you need a path relative to the module itself, so you\nshould use path.module  when calling the templatefile  function in modules/services/\nwebserver-cluster/main.tf :\n  user_data  = templatefile (""${path.module}/user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = data.terraform_remote_state.db.outputs.address\n    db_port      = data.terraform_remote_state.db.outputs.port\n  })\nInline Blocks\nThe configuration for some Terraform resources can be defined either as inline\nblocks or as separate resources. An inline block  is an argument you set within a\nresource of the format:\nresource  ""xxx"" ""yyy""  {\n  <NAME> {\n    [CONFIG...]\n  }\n}\nwhere NAME  is the name of the inline block (e.g., ingress ) and CONFIG  consists of\none or more arguments that are specific to that inline block (e.g., from_port  and\nto_port ). For example, with the aws_security_group_resource , you can define\ningress and egress rules using either inline blocks (e.g., ingress { … } ) or separate\naws_security_group_rule  resources.\nIf you try to use a mix of both  inline blocks and separate resources, due to how\nTerraform is designed, you will get errors where the configurations conflict and\noverwrite one another. Therefore, you must use one or the other. Here’s my advice:\nwhen creating a module, you should always prefer using separate resources.\n130 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\nThe advantage of using separate resources is that they can be added anywhere,\nwhereas an inline block can only be added within the module that creates a resource.\nSo using solely separate resources makes your module more flexible and configurable.\nFor example, in the webserver-cluster  module ( modules/services/webserver-cluster/\nmain.tf ), you used inline blocks to define ingress and egress rules:\nresource  ""aws_security_group"" ""alb""  {\n  name = ""${var.cluster_name}-alb""\n  ingress {\n    from_port    = local.http_port\n    to_port      = local.http_port\n    protocol     = local.tcp_protocol\n    cidr_blocks  = local.all_ips\n  }\n  egress {\n    from_port    = local.any_port\n    to_port      = local.any_port\n    protocol     = local.any_protocol\n    cidr_blocks  = local.all_ips\n  }\n}\nWith these inline blocks, a user of this module has no way to add additional ingress\nor egress rules from outside the module. To make your module more flexible, you\nshould change it to define the exact same ingress and egress rules by using separate\naws_security_group_rule  resources (make sure to do this for both security groups\nin the module):\nresource  ""aws_security_group"" ""alb""  {\n  name = ""${var.cluster_name}-alb""\n}\nresource  ""aws_security_group_rule"" ""allow_http_inbound""  {\n  type              = ""ingress""\n  security_group_id  = aws_security_group.alb.id\n  from_port    = local.http_port\n  to_port      = local.http_port\n  protocol     = local.tcp_protocol\n  cidr_blocks  = local.all_ips\n}\nresource  ""aws_security_group_rule"" ""allow_all_outbound""  {\n  type              = ""egress""\n  security_group_id  = aws_security_group.alb.id\n  from_port    = local.any_port\n  to_port      = local.any_port\n  protocol     = local.any_protocol\nModule Gotchas | 131\n  cidr_blocks  = local.all_ips\n}\nY ou should also export the ID of the aws_security_group  as an output variable in\nmodules/services/webserver-cluster/outputs.tf :\noutput ""alb_security_group_id""  {\n  value       = aws_security_group.alb.id\n  description  = ""The ID of the Security Group attached to the load balancer""\n}\nNow, if you needed to expose an extra port in just the staging environment (e.g.,\nfor testing), you can do this by adding an aws_security_group_rule  resource to\nstage/services/webserver-cluster/main.tf :\nmodule ""webserver_cluster""  {\n  source  = ""../../../modules/services/webserver-cluster""\n  # (parameters hidden for clarity)\n}\nresource  ""aws_security_group_rule"" ""allow_testing_inbound""  {\n  type              = ""ingress""\n  security_group_id  = module.webserver_cluster .alb_security_group_id\n  from_port    = 12345\n  to_port      = 12345\n  protocol     = ""tcp""\n  cidr_blocks  = [""0.0.0.0/0"" ]\n}\nHad you defined even a single ingress or egress rule as an inline block, this code\nwould not work. Note that this same type of problem affects a number of Terraform\nresources, such as the following:\n•aws_security_group  and aws_security_group_rule •\n•aws_route_table  and aws_route •\n•aws_network_acl  and aws_network_acl_rule •\nAt this point, you are finally ready to deploy your web server cluster in both staging\nand production. Run terraform apply  as usual, and enjoy using two separate copies\nof your infrastructure.\n132 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",7241
44-Module Versioning.pdf,44-Module Versioning,"1For the full details on source URLs, see the Terraform website .Network Isolation\nThe examples in this chapter create two environments that are isolated in your\nTerraform code, as well as isolated in terms of having separate load balancers, servers,\nand databases, but they are not isolated at the network level. To keep all the examples\nin this book simple, all of the resources deploy into the same VPC. This means that a\nserver in the staging environment can communicate with a server in the production\nenvironment, and vice versa.\nIn real-world usage, running both environments in one VPC opens you up to two\nrisks. First, a mistake in one environment could affect the other. For example, if you’re\nmaking changes in staging and accidentally mess up the configuration of the route\ntables, all the routing in production can be affected, too. Second, if an attacker gains\naccess to one environment, they also have access to the other. If you’re making rapid\nchanges in staging and accidentally leave a port exposed, any hacker that broke in\nwould have access to not only your staging data but also your production data.\nTherefore, outside of simple examples and experiments, you should run each envi‐\nronment in a separate VPC. In fact, to be extra sure, you might even run each\nenvironment in a totally separate AWS account.\nModule Versioning\nIf both your staging and production environment are pointing to the same module\nfolder, as soon as you make a change in that folder, it will affect both environments\non the very next deployment. This sort of coupling makes it more difficult to test a\nchange in staging without any chance of affecting production. A better approach is to\ncreate versioned modules  so that you can use one version in staging (e.g., v0.0.2) and a\ndifferent version in production (e.g., v0.0.1), as shown in Figure 4-5 .\nIn all of the module examples you’ve seen so far, whenever you used a module, you\nset the source  parameter of the module to a local filepath. In addition to file paths,\nTerraform supports other types of module sources, such as Git URLs, Mercurial\nURLs, and arbitrary HTTP URLs.1\nModule Versioning | 133\nFigure 4-5. By versioning your modules, you can use different  versions in different\nenvironments: e.g., v0.0.1 in prod and v0.0.2 in stage.\nThe easiest way to create a versioned module is to put the code for the module in\na separate Git repository and to set the source  parameter to that repository’s URL.\nThat means your Terraform code will be spread out across (at least) two repositories:\nmodules\nThis repo defines reusable modules. Think of each module as a “blueprint” that\ndefines a specific part of your infrastructure.\nlive\nThis repo defines the live infrastructure you’re running in each environment\n(stage, prod, mgmt, etc.). Think of this as the “houses” you built from the\n“blueprints” in the modules  repo.\nThe updated folder structure for your Terraform code now looks something like\nFigure 4-6 .\n134 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\nFigure 4-6. You should store reusable, versioned modules in one repo (modules) and the\nconfiguration  for your live environments in another repo (live).\nTo set up this folder structure, you’ll first need to move the stage , prod , and global\nfolders into a folder called live. Next, configure the live and modules  folders as\nseparate Git repositories. Here is an example of how to do that for the modules  folder:\n$ cd modules\n$ git init\n$ git add .\n$ git commit -m ""Initial commit of modules repo""\n$ git remote add origin ""(URL OF REMOTE GIT REPOSITORY)""\n$ git push origin main\nY ou can also add a tag to the modules  repo to use as a version number. If you’re using\nGitHub, you can use the GitHub UI to create a release , which will create a tag under\nthe hood.\nIf you’re not using GitHub, you can use the Git CLI:\n$ git tag -a ""v0.0.1"" -m ""First release of webserver-cluster module""\n$ git push --follow-tags\nNow you can use this versioned module in both staging and production by speci‐\nfying a Git URL in the source  parameter. Here is what that would look like in\nlive/stage/services/webserver-cluster/main.tf  if your modules  repo was in the GitHub\nrepo github.com/foo/modules  (note that the double-slash in the following Git URL is\nrequired):\nModule Versioning | 135\nmodule ""webserver_cluster""  {\n  source  = ""github.com/foo/modules//services/webserver-cluster?ref =v0.0.1""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type  = ""t2.micro""\n  min_size       = 2\n  max_size       = 2\n}\nIf you want to try out versioned modules without messing with Git repos, you can\nuse a module from the code examples GitHub repo  for this book (I had to break up\nthe URL to make it fit in the book, but it should all be on one line):\nsource = ""github.com/brikis98/terraform-up-and-running-code//\n  code/terraform/04-terraform-module/module-example/modules/\n  services/webserver-cluster?ref=v0.3.0""\nThe ref parameter allows you to specify a particular Git commit via its sha1 hash, a\nbranch name, or, as in this example, a specific Git tag. I generally recommend using\nGit tags as version numbers for modules. Branch names are not stable, as you always\nget the latest commit on a branch, which may change every time you run the init\ncommand, and the sha1 hashes are not very human friendly. Git tags are as stable as\na commit (in fact, a tag is just a pointer to a commit), but they allow you to use a\nfriendly, readable name.\nA particularly useful naming scheme for tags is semantic versioning . This is a version‐\ning scheme of the format MAJOR.MINOR.PATCH  (e.g., 1.0.4 ) with specific rules on\nwhen you should increment each part of the version number. In particular, you\nshould increment the following:\n•The MAJOR  version when you make incompatible API changes •\n•The MINOR  version when you add functionality in a backward-compatible •\nmanner\n•The PATCH  version when you make backward-compatible bug fixes •\nSemantic versioning gives you a way to communicate to users of your module what\nkinds of changes you’ve made and the implications of upgrading.\nBecause you’ve updated your Terraform code to use a versioned module URL, you\nneed to instruct Terraform to download the module code by rerunning terraform\ninit :\n$ terraform init\nInitializing modules...\nDownloading git@github.com:brikis98/terraform-up-and-running-code.git?ref=v0.3.0\n136 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules\n2See the GitHub documentation  for a nice guide on working with SSH keys.\nfor webserver_cluster...\n(...)\nThis time, you can see that Terraform downloads the module code from Git rather\nthan your local filesystem. After the module code has been downloaded, you can run\nthe apply  command as usual.\nPrivate Git Repos\nIf your Terraform module is in a private Git repository, to use that\nrepo as a module source , you need to give Terraform a way to\nauthenticate to that Git repository. I recommend using SSH auth so\nthat you don’t need to hardcode the credentials for your repo in the\ncode itself. With SSH authentication, each developer can create an\nSSH key, associate it with their Git user, add it to ssh-agent , and\nTerraform will automatically use that key for authentication if you\nuse an SSH source  URL.2\nThe source  URL should be of the form:\ngit@github.com:<OWNER>/<REPO>.git//<PATH>?ref=<VERSION>\nFor example:\ngit@github.com:acme/modules.git//example?ref=v0.1.2\nTo check that you’ve formatted the URL correctly, try to git clone\nthe base URL from your terminal:\n$ git clone git@github.com:acme/modules.git\nIf that command succeeds, Terraform should be able to use the\nprivate repo, too.\nNow that you’re using versioned modules, let’s walk through the process of making\nchanges. Let’s say you made some changes to the webserver-cluster  module, and\nyou want to test them out in staging. First, you’ d commit those changes to the\nmodules  repo:\n$ cd modules\n$ git add .\n$ git commit -m ""Made some changes to webserver-cluster""\n$ git push origin main\nNext, you would create a new tag in the modules  repo:\n$ git tag -a ""v0.0.2"" -m ""Second release of webserver-cluster""\n$ git push --follow-tags\nModule Versioning | 137\nAnd now you can update just the source URL used in the staging environment ( live/\nstage/services/webserver-cluster/main.tf ) to use this new version:\nmodule ""webserver_cluster""  {\n  source  = ""github.com/foo/modules//services/webserver-cluster?ref =v0.0.2""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type  = ""t2.micro""\n  min_size       = 2\n  max_size       = 2\n}\nIn production ( live/prod/services/webserver-cluster/main.tf ), you can happily continue\nto run v0.0.1 unchanged:\nmodule ""webserver_cluster""  {\n  source  = ""github.com/foo/modules//services/webserver-cluster?ref =v0.0.1""\n  cluster_name            = ""webservers-prod""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""prod/data-stores/mysql/terraform.tfstate""\n  instance_type  = ""m4.large""\n  min_size       = 2\n  max_size       = 10\n}\nAfter v0.0.2 has been thoroughly tested and proven in staging, you can then update\nproduction, too. But if there turns out to be a bug in v0.0.2, no big deal, because it has\nno effect on the real users of your production environment. Fix the bug, release a new\nversion, and repeat the entire process again until you have something stable enough\nfor production.\nDeveloping Modules\nVersioned modules are great when you’re deploying to a shared\nenvironment (e.g., staging or production), but when you’re just\ntesting on your own computer, you’ll want to use local file paths.\nThis allows you to iterate faster, because you’ll be able to make\na change in the module folders and rerun the plan  or apply  com‐\nmand in the live folders immediately, rather than having to commit\nyour code, publish a new version, and rerun init  each time.\nSince the goal of this book is to help you learn and experiment with\nTerraform as quickly as possible, the rest of the code examples will\nuse local file paths for modules.\n138 | Chapter 4: How to Create Reusable Infrastructure with Terraform Modules",10538
45-Conclusion.pdf,45-Conclusion,"Conclusion\nBy defining infrastructure as code in modules, you can apply a variety of software\nengineering best practices to your infrastructure. Y ou can validate each change to\na module through code reviews and automated tests, you can create semantically\nversioned releases of each module, and you can safely try out different versions of\na module in different environments and roll back to previous versions if you hit a\nproblem.\nAll of this can dramatically increase your ability to build infrastructure quickly and\nreliably because developers will be able to reuse entire pieces of proven, tested, and\ndocumented infrastructure. For example, you could create a canonical module that\ndefines how to deploy a single microservice—including how to run a cluster, how to\nscale the cluster in response to load, and how to distribute traffic requests across the\ncluster—and each team could use this module to manage their own microservices\nwith just a few lines of code.\nTo make such a module work for multiple teams, the Terraform code in that module\nmust be flexible and configurable. For example, one team might want to use your\nmodule to deploy a single Instance of their microservice with no load balancer,\nwhereas another might want a dozen Instances of their microservice with a load\nbalancer to distribute traffic between those Instances. How do you do conditional\nstatements in Terraform? Is there a way to do a for-loop? Is there a way to use Terra‐\nform to roll out changes to this microservice without downtime? These advanced\naspects of Terraform syntax are the topic of Chapter 5 .\nConclusion | 139",1632
46-Loops.pdf,46-Loops,"CHAPTER 5\nTerraform Tips and Tricks: Loops,\nIf-Statements, Deployment, and Gotchas\nTerraform is a declarative language. As discussed in Chapter 1 , IaC in a declarative\nlanguage tends to provide a more accurate view of what’s actually deployed than a\nprocedural language, so it’s easier to reason about and makes it easier to keep the\ncodebase small. However, certain types of tasks are more difficult in a declarative\nlanguage.\nFor example, because declarative languages typically don’t have for-loops, how do\nyou repeat a piece of logic—such as creating multiple similar resources—without\ncopy and paste? And if the declarative language doesn’t support if-statements, how\ncan you conditionally configure resources, such as creating a Terraform module\nthat can create certain resources for some users of that module but not for others?\nFinally, how do you express an inherently procedural idea, such as a zero-downtime\ndeployment, in a declarative language?\nFortunately, Terraform provides a few primitives—namely, the meta-parameter\ncount , for_each  and for expressions, a ternary operator, a lifecycle block called\ncreate_before_destroy , and a large number of functions—that allow you to do\ncertain types of loops, if-statements, and zero-downtime deployments. Here are the\ntopics I’ll cover in this chapter:\n•Loops•\n•Conditionals•\n•Zero-downtime deployment•\n•Terraform gotchas•\n141",1411
47-Loops with the count Parameter.pdf,47-Loops with the count Parameter,"Example Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\nLoops\nTerraform offers several different looping constructs, each intended to be used in a\nslightly different scenario:\n•count  parameter, to loop over resources and modules •\n•for_each  expressions, to loop over resources, inline blocks within a resource, •\nand modules\n•for expressions, to loop over lists and maps •\n•for string directive, to loop over lists and maps within a string •\nLet’s go through these one at a time.\nLoops with the count Parameter\nIn Chapter 2 , you created an AWS Identity and Access Management (IAM) user by\nclicking around the Console. Now that you have this user, you can create and manage\nall future IAM users with Terraform. Consider the following Terraform code, which\nshould live in live/global/iam/main.tf :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nresource  ""aws_iam_user"" ""example""  {\n  name = ""neo""\n}\nThis code uses the aws_iam_user  resource to create a single new IAM user. What if\nyou want to create three IAM users? In a general-purpose programming language,\nyou’ d probably use a for-loop:\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) {\n  resource  ""aws_iam_user"" ""example""  {\n    name  = ""neo""\n  }\n}\n142 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nTerraform does not have for-loops or other traditional procedural logic built into\nthe language, so this syntax will not work. However, every Terraform resource has a\nmeta-parameter you can use called count . count  is Terraform’s oldest, simplest, and\nmost limited iteration construct: all it does is define how many copies of the resource\nto create. Here’s how you use count  to create three IAM users:\nresource  ""aws_iam_user"" ""example""  {\n  count = 3\n  name  = ""neo""\n}\nOne problem with this code is that all three IAM users would have the same name,\nwhich would cause an error, since usernames must be unique. If you had access to\na standard for-loop, you might use the index in the for-loop, i, to give each user a\nunique name:\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) {\n  resource  ""aws_iam_user"" ""example""  {\n    name  = ""neo.${i}""\n  }\n}\nTo accomplish the same thing in Terraform, you can use count.index  to get the\nindex of each “iteration” in the “loop”:\nresource  ""aws_iam_user"" ""example""  {\n  count = 3\n  name  = ""neo.${count.index}""\n}\nIf you run the plan  command on the preceding code, you will see that Terraform\nwants to create three IAM users, each with a different name ( ""neo.0"" , ""neo.1"" ,\n""neo.2"" ):\nTerraform will perform the following actions:\n  # aws_iam_user.example[0] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""neo.0""\n      (...)\n    }\n  # aws_iam_user.example[1] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""neo.1""\n      (...)\n    }\n  # aws_iam_user.example[2] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""neo.2""\nLoops | 143\n      (...)\n    }\nPlan: 3 to add, 0 to change, 0 to destroy.\nOf course, a username like ""neo.0""  isn’t particularly usable. If you combine\ncount.index  with some built-in functions from Terraform, you can customize each\n“iteration” of the “loop” even more.\nFor example, you could define all of the IAM usernames you want in an input\nvariable in live/global/iam/variables.tf :\nvariable  ""user_names""  {\n  description  = ""Create IAM users with these names""\n  type        = list(string)\n  default      = [""neo"", ""trinity"", ""morpheus"" ]\n}\nIf you were using a general-purpose programming language with loops and arrays,\nyou would configure each IAM user to use a different name by looking up index i in\nthe array var.user_names :\n# This is just pseudo code. It won't actually work in Terraform.\nfor (i = 0; i < 3; i++) {\n  resource  ""aws_iam_user"" ""example""  {\n    name  = vars.user_names[i ]\n  }\n}\nIn Terraform, you can accomplish the same thing by using count  along with the\nfollowing:\nArray lookup syntax\nThe syntax for looking up members of an array in Terraform is similar to most\nother programming languages:\nARRAY[<INDEX>]\nFor example, here’s how you would look up the element at index 1 of\nvar.user_names :\nvar.user_names[1 ]\nThe length  function\nTerraform has a built-in function called length  that has the following syntax:\nlength(<ARRAY>)\nAs you can probably guess, the length  function returns the number of items in\nthe given ARRAY . It also works with strings and maps.\n144 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nPutting these together, you get the following:\nresource  ""aws_iam_user"" ""example""  {\n  count = length(var.user_names )\n  name  = var.user_names[count.index ]\n}\nNow when you run the plan  command, you’ll see that Terraform wants to create\nthree IAM users, each with a unique, readable name:\nTerraform will perform the following actions:\n  # aws_iam_user.example[0] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""neo""\n      (...)\n    }\n  # aws_iam_user.example[1] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""trinity""\n      (...)\n    }\n  # aws_iam_user.example[2] will be created\n  + resource ""aws_iam_user"" ""example"" {\n      + name          = ""morpheus""\n      (...)\n    }\nPlan: 3 to add, 0 to change, 0 to destroy.\nNote that after you’ve used count  on a resource, it becomes an array of resources\nrather than just one resource. Because aws_iam_user.example  is now an array of\nIAM users, instead of using the standard syntax to read an attribute from that\nresource ( <PROVIDER>_<TYPE>.<NAME>.<ATTRIBUTE> ), you must specify which IAM\nuser you’re interested in by specifying its index in the array using the same array\nlookup syntax:\n<PROVIDER>_<TYPE> .<NAME>[INDEX].ATTRIBUTE\nFor example, if you want to provide the Amazon Resource Name (ARN) of the first\nIAM user in the list as an output variable, you would need to do the following:\noutput ""first_arn""  {\n  value       = aws_iam_user.example[0].arn\n  description  = ""The ARN for the first user""\n}\nIf you want the ARNs of all of the IAM users, you need to use a splat expression , “*” ,\ninstead of the index:\nLoops | 145\noutput ""all_arns""  {\n  value       = aws_iam_user.example [*].arn\n  description  = ""The ARNs for all users""\n}\nWhen you run the apply  command, the first_arn  output will contain just the ARN\nfor neo, whereas the all_arns  output will contain the list of all ARNs:\n$ terraform apply\n(...)\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\nOutputs:\nfirst_arn = ""arn:aws:iam::123456789012:user/neo""\nall_arns = [\n  ""arn:aws:iam::123456789012:user/neo"",\n  ""arn:aws:iam::123456789012:user/trinity"",\n  ""arn:aws:iam::123456789012:user/morpheus"",\n]\nAs of Terraform 0.13, the count  parameter can also be used on modules. For example,\nimagine you had a module at modules/landing-zone/iam-user  that can create a single\nIAM user:\nresource  ""aws_iam_user"" ""example""  {\n  name = var.user_name\n}\nThe username is passed into this module as an input variable:\nvariable  ""user_name""  {\n  description  = ""The user name to use""\n  type        = string\n}\nAnd the module returns the ARN of the created IAM user as an output variable:\noutput ""user_arn""  {\n  value       = aws_iam_user.example.arn\n  description  = ""The ARN of the created IAM user""\n}\nY ou could use this module with a count  parameter to create three IAM users as\nfollows:\nmodule ""users"" {\n  source  = ""../../../modules/landing-zone/iam-user""\n  count     = length(var.user_names )\n  user_name  = var.user_names[count.index ]\n}\n146 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nThe preceding code uses count  to loop over this list of usernames:\nvariable  ""user_names""  {\n  description  = ""Create IAM users with these names""\n  type        = list(string)\n  default      = [""neo"", ""trinity"", ""morpheus"" ]\n}\nAnd it outputs the ARNs of the created IAM users as follows:\noutput ""user_arns""  {\n  value       = module.users[*].user_arn\n  description  = ""The ARNs of the created IAM users""\n}\nJust as adding count  to a resource turns it into an array of resources, adding count  to\na module turns it into an array of modules.\nIf you run apply  on this code, you’ll get the following output:\n$ terraform apply\n(...)\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\nOutputs:\nall_arns = [\n  ""arn:aws:iam::123456789012:user/neo"",\n  ""arn:aws:iam::123456789012:user/trinity"",\n  ""arn:aws:iam::123456789012:user/morpheus"",\n]\nSo, as you can see, count  works more or less identically with resources and with\nmodules.\nUnfortunately, count  has two limitations that significantly reduce its usefulness. First,\nalthough you can use count  to loop over an entire resource, you can’t use count\nwithin a resource to loop over inline blocks.\nFor example, consider how tags are set in the aws_autoscaling_group  resource:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  tag {\nLoops | 147\n    key                 = ""Name""\n    value                = var.cluster_name\n    propagate_at_launch  = true\n  }\n}\nEach tag requires you to create a new inline block with values for key, value , and\npropagate_at_launch . The preceding code hardcodes a single tag, but you might\nwant to allow users to pass in custom tags. Y ou might be tempted to try to use the\ncount  parameter to loop over these tags and generate dynamic inline tag blocks, but\nunfortunately, using count  within an inline block is not supported.\nThe second limitation with count  is what happens when you try to change its value.\nConsider the list of IAM users you created earlier:\nvariable  ""user_names""  {\n  description  = ""Create IAM users with these names""\n  type        = list(string)\n  default      = [""neo"", ""trinity"", ""morpheus"" ]\n}\nImagine that you removed ""trinity""  from this list. What happens when you run\nterraform plan ?\n$ terraform plan\n(...)\nTerraform will perform the following actions:\n  # aws_iam_user.example[1] will be updated in-place\n  ~ resource ""aws_iam_user"" ""example"" {\n        id            = ""trinity""\n      ~ name          = ""trinity"" -> ""morpheus""\n    }\n  # aws_iam_user.example[2] will be destroyed\n  - resource ""aws_iam_user"" ""example"" {\n      - id            = ""morpheus"" -> null\n      - name          = ""morpheus"" -> null\n    }\nPlan: 0 to add, 1 to change, 1 to destroy.\nWait a second, that’s probably not what you were expecting! Instead of just deleting\nthe ""trinity""  IAM user, the plan  output is indicating that Terraform wants to\nrename the ""trinity""  IAM user to ""morpheus""  and delete the original ""morpheus""\nuser. What’s going on?\nWhen you use the count  parameter on a resource, that resource becomes an array\nof resources. Unfortunately, the way Terraform identifies each resource within the\n148 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",11493
48-Loops with for_each Expressions.pdf,48-Loops with for_each Expressions,"array is by its position (index) in that array. That is, after running apply  the first time\nwith three usernames, Terraform’s internal representation of these IAM users looks\nsomething like this:\naws_iam_user.example[0]: neo\naws_iam_user.example[1]: trinity\naws_iam_user.example[2]: morpheus\nWhen you remove an item from the middle of an array, all the items after it shift\nback by one, so after running plan  with just two bucket names, Terraform’s internal\nrepresentation will look something like this:\naws_iam_user.example[0]: neo\naws_iam_user.example[1]: morpheus\nNotice how ""morpheus""  has moved from index 2 to index 1. Because it sees the index\nas a resource’s identity, to Terraform, this change roughly translates to “rename the\nbucket at index 1 to morpheus and delete the bucket at index 2. ” In other words,\nevery time you use count  to create a list of resources, if you remove an item from\nthe middle of the list, Terraform will delete every resource after that item and then\nre-create those resources again from scratch. Ouch. The end result, of course, is\nexactly what you requested (i.e., two IAM users named ""morpheus""  and ""neo"" ),\nbut deleting resources is probably not how you want to get there, as you may lose\navailability (you can’t use the IAM user during the apply ), and, even worse, you may\nlose data (if the resource you’re deleting is a database, you may lose all the data in it!).\nTo solve these two limitations, Terraform 0.12 introduced for_each  expressions.\nLoops with for_each Expressions\nThe for_each  expression allows you to loop over lists, sets, and maps to create (a)\nmultiple copies of an entire resource, (b) multiple copies of an inline block within\na resource, or (c) multiple copies of a module. Let’s first walk through how to use\nfor_each  to create multiple copies of a resource.\nThe syntax looks like this:\nresource  ""<PROVIDER>_<TYPE>"" ""<NAME>""  {\n  for_each  = <COLLECTION>\n  [CONFIG ...]\n}\nwhere COLLECTION  is a set or map to loop over (lists are not supported when using\nfor_each  on a resource) and CONFIG  consists of one or more arguments that are\nspecific to that resource. Within CONFIG , you can use each.key  and each.value  to\naccess the key and value of the current item in COLLECTION .\nLoops | 149\nFor example, here’s how you can create the same three IAM users using for_each  on\na resource:\nresource  ""aws_iam_user"" ""example""  {\n  for_each  = toset(var.user_names )\n  name     = each.value\n}\nNote the use of toset  to convert the var.user_names  list into a set. This is because\nfor_each  supports sets and maps only when used on a resource. When for_each\nloops over this set, it makes each username available in each.value . The username\nwill also be available in each.key , though you typically use each.key  only with maps\nof key-value pairs.\nOnce you’ve used for_each  on a resource, it becomes a map of resources, rather\nthan just one resource (or an array of resources as with count ). To see what that\nmeans, remove the original all_arns  and first_arn  output variables, and add a new\nall_users  output variable:\noutput ""all_users""  {\n  value = aws_iam_user.example\n}\nHere’s what happens when you run terraform apply :\n$ terraform apply\n(...)\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\nOutputs:\nall_users = {\n  ""morpheus"" = {\n    ""arn"" = ""arn:aws:iam::123456789012:user/morpheus""\n    ""force_destroy"" = false\n    ""id"" = ""morpheus""\n    ""name"" = ""morpheus""\n    ""path"" = ""/""\n    ""tags"" = {}\n  }\n  ""neo"" = {\n    ""arn"" = ""arn:aws:iam::123456789012:user/neo""\n    ""force_destroy"" = false\n    ""id"" = ""neo""\n    ""name"" = ""neo""\n    ""path"" = ""/""\n    ""tags"" = {}\n  }\n  ""trinity"" = {\n150 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n    ""arn"" = ""arn:aws:iam::123456789012:user/trinity""\n    ""force_destroy"" = false\n    ""id"" = ""trinity""\n    ""name"" = ""trinity""\n    ""path"" = ""/""\n    ""tags"" = {}\n  }\n}\nY ou can see that Terraform created three IAM users and that the all_users  output\nvariable contains a map where the keys are the keys in for_each  (in this case, the\nusernames) and the values are all the outputs for that resource. If you want to bring\nback the all_arns  output variable, you’ d need to do a little extra work to extract\nthose ARNs using the values  built-in function (which returns just the values from a\nmap) and a splat expression:\noutput ""all_arns""  {\n  value = values(aws_iam_user.example )[*].arn\n}\nThis gives you the expected output:\n$ terraform apply\n(...)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\nall_arns = [\n  ""arn:aws:iam::123456789012:user/morpheus"",\n  ""arn:aws:iam::123456789012:user/neo"",\n  ""arn:aws:iam::123456789012:user/trinity"",\n]\nThe fact that you now have a map of resources with for_each  rather than an array of\nresources as with count  is a big deal, because it allows you to remove items from the\nmiddle of a collection safely. For example, if you again remove ""trinity""  from the\nmiddle of the var.user_names  list and run terraform plan , here’s what you’ll see:\n$ terraform plan\nTerraform will perform the following actions:\n  # aws_iam_user.example[""trinity""] will be destroyed\n  - resource ""aws_iam_user"" ""example"" {\n      - arn           = ""arn:aws:iam::123456789012:user/trinity"" -> null\n      - name          = ""trinity"" -> null\n    }\nPlan: 0 to add, 0 to change, 1 to destroy.\nLoops | 151\nThat’s more like it! Y ou’re now deleting solely the exact resource you want, without\nshifting all of the other ones around. This is why you should almost always prefer to\nuse for_each  instead of count  to create multiple copies of a resource.\nfor_each  works with modules in a more or less identical fashion. Using the iam-user\nmodule from earlier, you can create three IAM users with it using for_each  as\nfollows:\nmodule ""users"" {\n  source  = ""../../../modules/landing-zone/iam-user""\n  for_each   = toset(var.user_names )\n  user_name  = each.value\n}\nAnd you can output the ARNs of those users as follows:\noutput ""user_arns""  {\n  value       = values(module.users)[*].user_arn\n  description  = ""The ARNs of the created IAM users""\n}\nWhen you run apply  on this code, you get the expected output:\n$ terraform apply\n(...)\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\nOutputs:\nall_arns = [\n  ""arn:aws:iam::123456789012:user/morpheus"",\n  ""arn:aws:iam::123456789012:user/neo"",\n  ""arn:aws:iam::123456789012:user/trinity"",\n]\nLet’s now turn our attention to another advantage of for_each : its ability to create\nmultiple inline blocks within a resource. For example, you can use for_each  to\ndynamically generate tag inline blocks for the ASG in the webserver-cluster  mod‐\nule. First, to allow users to specify custom tags, add a new map input variable called\ncustom_tags  in modules/services/webserver-cluster/variables.tf :\nvariable  ""custom_tags""  {\n  description  = ""Custom tags to set on the Instances in the ASG""\n  type        = map(string)\n  default      = {}\n}\nNext, set some custom tags in the production environment, in live/prod/services/\nwebserver-cluster/main.tf , as follows:\n152 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nmodule ""webserver_cluster""  {\n  source  = ""../../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-prod""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""prod/data-stores/mysql/terraform.tfstate""\n  instance_type         = ""m4.large""\n  min_size              = 2\n  max_size              = 10\n  custom_tags  = {\n    Owner      = ""team-foo""\n    ManagedBy  = ""terraform""\n  }\n}\nThe preceding code sets a couple of useful tags: the Owner  tag specifies which team\nowns this ASG, and the ManagedBy  tag specifies that this infrastructure is managed\nusing Terraform (indicating that this infrastructure shouldn’t be modified manually).\nNow that you’ve specified your tags, how do you actually set them on the\naws_autoscaling_group  resource? What you need is a for-loop over var.cus\ntom_tags , similar to the following pseudocode:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  tag {\n    key                 = ""Name""\n    value                = var.cluster_name\n    propagate_at_launch  = true\n  }\n  # This is just pseudo code. It won't actually work in Terraform.\n  for (tag in var.custom_tags ) {\n    tag {\n      key                  = tag.key\n      value                = tag.value\n      propagate_at_launch  = true\n    }\n  }\n}\nThe preceding pseudocode won’t work, but a for_each  expression will. The syntax\nfor using for_each  to dynamically generate inline blocks looks like this:\nLoops | 153\ndynamic ""<VAR_NAME>""  {\n  for_each  = <COLLECTION>\n  content {\n    [CONFIG...]\n  }\n}\nwhere VAR_NAME  is the name to use for the variable that will store the value of each\n“iteration, ” COLLECTION  is a list or map to iterate over, and the content  block is what\nto generate from each iteration. Y ou can use <VAR_NAME>.key  and <VAR_NAME>.value\nwithin the content  block to access the key and value, respectively, of the current item\nin the COLLECTION . Note that when you’re using for_each  with a list, the key will be\nthe index, and the value  will be the item in the list at that index, and when using\nfor_each  with a map, the key and value  will be one of the key-value pairs in the map.\nPutting this all together, here is how you can dynamically generate tag blocks using\nfor_each  in the aws_autoscaling_group  resource:\nresource  ""aws_autoscaling_group"" ""example""  {\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  tag {\n    key                 = ""Name""\n    value                = var.cluster_name\n    propagate_at_launch  = true\n  }\n  dynamic ""tag"" {\n    for_each  = var.custom_tags\n    content {\n      key                  = tag.key\n      value                = tag.value\n      propagate_at_launch  = true\n    }\n  }\n}\nIf you run terraform plan  now, you should see a plan that looks something like this:\n$ terraform plan\nTerraform will perform the following actions:\n154 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n  # aws_autoscaling_group.example will be updated in-place\n  ~ resource ""aws_autoscaling_group"" ""example"" {\n        (...)\n        tag {\n            key                 = ""Name""\n            propagate_at_launch = true\n            value               = ""webservers-prod""\n        }\n      + tag {\n          + key                 = ""Owner""\n          + propagate_at_launch = true\n          + value               = ""team-foo""\n        }\n      + tag {\n          + key                 = ""ManagedBy""\n          + propagate_at_launch = true\n          + value               = ""terraform""\n        }\n    }\nPlan: 0 to add, 1 to change, 0 to destroy.\nEnforcing Tagging Standards\nIt’s typically a good idea to come up with a tagging standard for your team and\ncreate Terraform modules that enforce this standard as code. One way to do this\nis to manually ensure that every resource in every module sets the proper tags,\nbut with many resources, this is tedious and error prone. If there are tags that you\nwant to apply to all of your AWS resources, a more reliable approach is to add the\ndefault_tags  block to the aws provider in every one of your modules:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  # Tags to apply to all AWS resources by default\n  default_tags  {\n    tags  = {\n      Owner      = ""team-foo""\n      ManagedBy  = ""Terraform""\n    }\n  }\n}\nThe preceding code will ensure that every single AWS resource you create in this\nmodule will include the Owner  and ManagedBy  tags (the only exceptions are resources\nthat don’t support tags and the aws_autoscaling_group  resource, which does support\ntags but doesn’t work with default_tags , which is why you had to do all that work\nin the previous section to set tags in the webserver-cluster  module). default_tags\ngives you a way to ensure all resources have a common baseline of tags while still\nLoops | 155",12704
49-Loops with for Expressions.pdf,49-Loops with for Expressions,"allowing you to override those tags on a resource-by-resource basis. In Chapter 9 ,\nyou’ll see how to define and enforce policies as code such as “all resources must have\na ManagedBy  tag” using tools such as OPA.\nLoops with for Expressions\nY ou’ve now seen how to use loops to create multiple copies of entire resources and\ninline blocks, but what if you need a loop to set a single variable or parameter?\nImagine that you wrote some Terraform code that took in a list of names:\nvariable  ""names"" {\n  description  = ""A list of names""\n  type        = list(string)\n  default      = [""neo"", ""trinity"", ""morpheus"" ]\n}\nHow could you convert all of these names to uppercase? In a general-purpose pro‐\ngramming language such as Python, you could write the following for-loop:\nnames = [""neo"", ""trinity"" , ""morpheus"" ]\nupper_case_names  = []\nfor name in names:\n    upper_case_names .append(name.upper())\nprint upper_case_names\n# Prints out: ['NEO', 'TRINITY', 'MORPHEUS']\nPython offers another way to write the exact same code in one line using a syntax\nknown as a list comprehension :\nnames = [""neo"", ""trinity"" , ""morpheus"" ]\nupper_case_names  = [name.upper() for name in names]\nprint upper_case_names\n# Prints out: ['NEO', 'TRINITY', 'MORPHEUS']\nPython also allows you to filter the resulting list by specifying a condition:\nnames = [""neo"", ""trinity"" , ""morpheus"" ]\nshort_upper_case_names  = [name.upper() for name in names if len(name) < 5]\nprint short_upper_case_names\n# Prints out: ['NEO']\nTerraform offers similar functionality in the form of a for expression (not to be\nconfused with the for_each  expression you saw in the previous section). The basic\nsyntax of a for expression is as follows:\n[for <ITEM> in <LIST> : <OUTPUT> ]\n156 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nwhere LIST  is a list to loop over, ITEM  is the local variable name to assign to each item\nin LIST , and OUTPUT  is an expression that transforms ITEM  in some way. For example,\nhere is the Terraform code to convert the list of names in var.names  to uppercase:\noutput ""upper_names""  {\n  value = [for name in var.names  : upper(name)]\n}\nIf you run terraform apply  on this code, you get the following output:\n$ terraform apply\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\nupper_names = [\n  ""NEO"",\n  ""TRINITY"",\n  ""MORPHEUS"",\n]\nJust as with Python’s list comprehensions, you can filter the resulting list by specifying\na condition:\noutput ""short_upper_names""  {\n  value = [for name in var.names  : upper(name) if length(name) < 5]\n}\nRunning terraform apply  on this code gives you this:\nshort_upper_names = [\n  ""NEO"",\n]\nTerraform’s for expression also allows you to loop over a map using the following\nsyntax:\n[for <KEY>, <VALUE> in <MAP> : <OUTPUT> ]\nHere, MAP is a map to loop over, KEY and VALUE  are the local variable names to assign\nto each key-value pair in MAP, and OUTPUT  is an expression that transforms KEY and\nVALUE  in some way. Here’s an example:\nvariable  ""hero_thousand_faces""  {\n  description  = ""map""\n  type        = map(string)\n  default      = {\n    neo      = ""hero""\n    trinity   = ""love interest""\n    morpheus  = ""mentor""\n  }\n}\nLoops | 157",3280
50-Loops with the for String Directive.pdf,50-Loops with the for String Directive,"output ""bios"" {\n  value = [for name, role in var.hero_thousand_faces  : ""${name} is the ${role}"" ]\n}\nWhen you run terraform apply  on this code, you get the following:\nbios = [\n  ""morpheus is the mentor"",\n  ""neo is the hero"",\n  ""trinity is the love interest"",\n]\nY ou can also use for expressions to output a map rather than a list using the\nfollowing syntax:\n# Loop over a list and output a map\n{for <ITEM> in <LIST> : <OUTPUT_KEY>  => <OUTPUT_VALUE> }\n# Loop over a map and output a map\n{for <KEY>, <VALUE> in <MAP> : <OUTPUT_KEY>  => <OUTPUT_VALUE> }\nThe only differences are that (a) you wrap the expression in curly braces rather than\nsquare brackets, and (b) rather than outputting a single value each iteration, you\noutput a key and value, separated by an arrow. For example, here is how you can\ntransform a map to make all the keys and values uppercase:\noutput ""upper_roles""  {\n  value = {for name, role in var.hero_thousand_faces : upper(name)  => upper(role)}\n}\nHere’s the output from running this code:\nupper_roles = {\n  ""MORPHEUS"" = ""MENTOR""\n  ""NEO"" = ""HERO""\n  ""TRINITY"" = ""LOVE INTEREST""\n}\nLoops with the for String Directive\nEarlier  in the book, you learned about string interpolations, which allow you to\nreference Terraform code within strings:\n""Hello, ${var.name}""\nString directives  allow you to use control statements (e.g., for-loops and if-statements)\nwithin strings using a syntax similar to string interpolations, but instead of a dollar\nsign and curly braces ( ${…} ), you use a percent sign and curly braces ( %{…} ).\nTerraform supports two types of string directives: for-loops and conditionals. In this\nsection, we’ll go over for-loops; we’ll come back to conditionals later in the chapter.\nThe for string directive uses the following syntax:\n158 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n%{ for <ITEM> in <COLLECTION>  }<BODY>%{ endfor }\nwhere COLLECTION  is a list or map to loop over, ITEM  is the local variable name to\nassign to each item in COLLECTION , and BODY  is what to render each iteration (which\ncan reference ITEM ). Here’s an example:\nvariable  ""names"" {\n  description  = ""Names to render""\n  type        = list(string)\n  default      = [""neo"", ""trinity"", ""morpheus"" ]\n}\noutput ""for_directive""  {\n  value = ""%{ for name in var.names }${name}, %{ endfor }""\n}\nWhen you run terraform apply , you get the following output:\n$ terraform apply\n(...)\nOutputs:\nfor_directive = ""neo, trinity, morpheus, ""\nThere’s also a version of the for string directive syntax that gives you the index in the\nfor-loop:\n%{ for <INDEX>, <ITEM> in <COLLECTION>  }<BODY>%{ endfor }\nHere’s an example using the index:\noutput ""for_directive_index""  {\n  value = ""%{ for i, name in var.names }(${i}) ${name}, %{ endfor }""\n}\nWhen you run terraform apply , you get the following output:\n$ terraform apply\n(...)\nOutputs:\nfor_directive_index = ""(0) neo, (1) trinity, (2) morpheus, ""\nNote how in both outputs there is an extra trailing comma and space. Y ou can fix\nthis using conditionals—specifically, the if string directive—as described in the next\nsection.\nLoops | 159",3193
51-Conditionals.pdf,51-Conditionals,,0
52-Conditionals with the count Parameter.pdf,52-Conditionals with the count Parameter,"Conditionals\nJust as Terraform offers several different ways to do loops, there are also several\ndifferent ways to do conditionals, each intended to be used in a slightly different\nscenario:\ncount  parameter\nUsed for conditional resources\nfor_each  and for expressions\nUsed for conditional resources and inline blocks within a resource\nif string directive\nUsed for conditionals within a string\nLet’s go through these, one at a time.\nConditionals with the count Parameter\nThe count  parameter you saw earlier lets you do a basic loop. If you’re clever, you\ncan use the same mechanism to do a basic conditional. Let’s begin by looking at\nif-statements in the next section and then move on to if-else-statements in the section\nthereafter.\nIf-statements with the count parameter\nIn Chapter 4 , you created a Terraform module that could be used as a “blueprint”\nfor deploying web server clusters. The module created an Auto Scaling Group\n(ASG), Application Load Balancer (ALB), security groups, and a number of other\nresources. One thing the module did not create was the scheduled action. Because\nyou want to scale the cluster out only in production, you defined the aws_autoscal\ning_schedule  resources directly in the production configurations under live/prod/\nservices/webserver-cluster/main.tf . Is there a way you could define the aws_autoscal\ning_schedule  resources in the webserver-cluster  module and conditionally create\nthem for some users of the module and not create them for others?\nLet’s give it a shot. The first step is to add a Boolean input variable in modules/serv‐\nices/webserver-cluster/variables.tf  that you can use to specify whether the module\nshould enable auto scaling:\nvariable  ""enable_autoscaling""  {\n  description  = ""If set to true, enable auto scaling""\n  type        = bool\n}\nNow, if you had a general-purpose programming language, you could use this input\nvariable in an if-statement:\n160 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n# This is just pseudo code. It won't actually work in Terraform.\nif var.enable_autoscaling  {\n  resource  ""aws_autoscaling_schedule"" ""scale_out_during_business_hours""  {\n    scheduled_action_name   = ""${var.cluster_name}-scale-out-during-business-hours""\n    min_size                = 2\n    max_size                = 10\n    desired_capacity        = 10\n    recurrence              = ""0 9 * * *""\n    autoscaling_group_name  = aws_autoscaling_group.example.name\n  }\n  resource  ""aws_autoscaling_schedule"" ""scale_in_at_night""  {\n    scheduled_action_name   = ""${var.cluster_name}-scale-in-at-night""\n    min_size                = 2\n    max_size                = 10\n    desired_capacity        = 2\n    recurrence              = ""0 17 * * *""\n    autoscaling_group_name  = aws_autoscaling_group.example.name\n  }\n}\nTerraform doesn’t support if-statements, so this code won’t work. However, you can\naccomplish the same thing by using the count  parameter and taking advantage of two\nproperties:\n•If you set count  to 1 on a resource, you get one copy of that resource; if you set •\ncount  to 0, that resource is not created at all.\n•Terraform supports conditional expressions  of the format <CONDITION> ? •\n<TRUE_VAL> : <FALSE_VAL> . This ternary syntax , which may be familiar to\nyou from other programming languages, will evaluate the Boolean logic in\nCONDITION , and if the result is true , it will return TRUE_VAL , and if the result\nis false , it’ll return FALSE_VAL .\nPutting these two ideas together, you can update the webserver-cluster  module as\nfollows:\nresource  ""aws_autoscaling_schedule"" ""scale_out_during_business_hours""  {\n  count = var.enable_autoscaling  ? 1 : 0\n  scheduled_action_name   = ""${var.cluster_name}-scale-out-during-business-hours""\n  min_size                = 2\n  max_size                = 10\n  desired_capacity        = 10\n  recurrence              = ""0 9 * * *""\n  autoscaling_group_name  = aws_autoscaling_group.example.name\n}\nresource  ""aws_autoscaling_schedule"" ""scale_in_at_night""  {\n  count = var.enable_autoscaling  ? 1 : 0\nConditionals | 161\n  scheduled_action_name   = ""${var.cluster_name}-scale-in-at-night""\n  min_size                = 2\n  max_size                = 10\n  desired_capacity        = 2\n  recurrence              = ""0 17 * * *""\n  autoscaling_group_name  = aws_autoscaling_group.example.name\n}\nIf var.enable_autoscaling  is true , the count  parameter for each of the\naws_autoscaling_schedule  resources will be set to 1, so one of each will be cre‐\nated. If var.enable_autoscaling  is false , the count  parameter for each of the\naws_autoscaling_schedule  resources will be set to 0, so neither one will be created.\nThis is exactly the conditional logic you want!\nY ou can now update the usage of this module in staging (in live/stage/services/\nwebserver-cluster/main.tf ) to disable auto scaling by setting enable_autoscaling  to\nfalse :\nmodule ""webserver_cluster""  {\n  source  = ""../../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type         = ""t2.micro""\n  min_size              = 2\n  max_size              = 2\n  enable_autoscaling    = false\n}\nSimilarly, you can update the usage of this module in production (in live/prod/serv‐\nices/webserver-cluster/main.tf ) to enable auto scaling by setting enable_autoscaling\nto true  (make sure to also remove the custom aws_autoscaling_schedule  resources\nthat were in the production environment from Chapter 4 ):\nmodule ""webserver_cluster""  {\n  source  = ""../../../../modules/services/webserver-cluster""\n  cluster_name            = ""webservers-prod""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""prod/data-stores/mysql/terraform.tfstate""\n  instance_type         = ""m4.large""\n  min_size              = 2\n  max_size              = 10\n  enable_autoscaling    = true\n  custom_tags  = {\n    Owner      = ""team-foo""\n    ManagedBy  = ""terraform""\n162 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n  }\n}\nIf-else-statements with the count parameter\nNow that you know how to do an if-statement, what about an if-else-statement?\nEarlier in this chapter, you created several IAM users with read-only access to EC2.\nImagine that you wanted to give one of these users, neo, access to CloudWatch as well\nbut allow the person applying the Terraform configurations to decide whether neo is\nassigned only read access or both read and write access. This is a slightly contrived\nexample, but a useful one to demonstrate a simple type of if-else-statement.\nHere is an IAM Policy that allows read-only access to CloudWatch:\nresource  ""aws_iam_policy"" ""cloudwatch_read_only""  {\n  name   = ""cloudwatch-read-only""\n  policy  = data.aws_iam_policy_document.cloudwatch_read_only.json\n}\ndata ""aws_iam_policy_document"" ""cloudwatch_read_only""  {\n  statement  {\n    effect     = ""Allow""\n    actions    = [\n      ""cloudwatch:Describe*"" ,\n      ""cloudwatch:Get*"" ,\n      ""cloudwatch:List*""\n    ]\n    resources  = [""*""]\n  }\n}\nAnd here is an IAM Policy that allows full (read and write) access to CloudWatch:\nresource  ""aws_iam_policy"" ""cloudwatch_full_access""  {\n  name   = ""cloudwatch-full-access""\n  policy  = data.aws_iam_policy_document.cloudwatch_full_access.json\n}\ndata ""aws_iam_policy_document"" ""cloudwatch_full_access""  {\n  statement  {\n    effect     = ""Allow""\n    actions    = [""cloudwatch:*"" ]\n    resources  = [""*""]\n  }\n}\nThe goal is to attach one of these IAM Policies to ""neo"" , based on the value of a new\ninput variable called give_neo_cloudwatch_full_access :\nvariable  ""give_neo_cloudwatch_full_access""  {\n  description  = ""If true, neo gets full access to CloudWatch""\nConditionals | 163\n  type        = bool\n}\nIf you were using a general-purpose programming language, you might write an\nif-else-statement that looks like this:\n# This is just pseudo code. It won't actually work in Terraform.\nif var.give_neo_cloudwatch_full_access  {\n  resource  ""aws_iam_user_policy_attachment"" ""neo_cloudwatch_full_access""  {\n    user        = aws_iam_user.example[0].name\n    policy_arn  = aws_iam_policy.cloudwatch_full_access.arn\n  }\n} else {\n  resource  ""aws_iam_user_policy_attachment"" ""neo_cloudwatch_read_only""  {\n    user        = aws_iam_user.example[0].name\n    policy_arn  = aws_iam_policy.cloudwatch_read_only.arn\n  }\n}\nTo do this in Terraform, you can use the count  parameter and a conditional expres‐\nsion on each of the resources:\nresource  ""aws_iam_user_policy_attachment"" ""neo_cloudwatch_full_access""  {\n  count = var.give_neo_cloudwatch_full_access  ? 1 : 0\n  user       = aws_iam_user.example[0].name\n  policy_arn  = aws_iam_policy.cloudwatch_full_access.arn\n}\nresource  ""aws_iam_user_policy_attachment"" ""neo_cloudwatch_read_only""  {\n  count = var.give_neo_cloudwatch_full_access  ? 0 : 1\n  user       = aws_iam_user.example[0].name\n  policy_arn  = aws_iam_policy.cloudwatch_read_only.arn\n}\nThis code contains two aws_iam_user_policy_attachment  resources. The first one,\nwhich attaches the CloudWatch full access permissions, has a conditional expression\nthat will evaluate to 1 if var.give_neo_cloudwatch_full_access  is true , and 0\notherwise (this is the if-clause). The second one, which attaches the CloudWatch\nread-only permissions, has a conditional expression that does the exact opposite,\nevaluating to 0 if var.give_neo_cloudwatch_full_access  is true , and 1 otherwise\n(this is the else-clause). And there you are—you now know how to do if-else-\nstatements!\nNow that you have the ability to create one resource or the other based on an if/else\ncondition, what do you do if you need to access an attribute on the resource that\nactually got created? For example, what if you wanted to add an output variable called\nneo_cloudwatch_policy_arn , which contains the ARN of the policy you actually\nattached?\n164 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",10237
53-Conditionals with for_each and for Expressions.pdf,53-Conditionals with for_each and for Expressions,"The simplest option is to use ternary syntax:\noutput ""neo_cloudwatch_policy_arn""  {\n  value = (\n    var.give_neo_cloudwatch_full_access\n    ? aws_iam_user_policy_attachment.neo_cloudwatch_full_access[0].policy_arn\n    : aws_iam_user_policy_attachment.neo_cloudwatch_read_only[0].policy_arn\n  )\n}\nThis will work fine for now, but this code is a bit brittle: if you ever change\nthe conditional in the count  parameter of the aws_iam_user_policy_attachment\nresources—perhaps in the future, it’ll depend on multiple variables and not solely on\nvar.give_neo_cloudwatch_full_access —there’s a risk that you’ll forget to update\nthe conditional in this output variable, and as a result, you’ll get a very confusing\nerror when trying to access an array element that might not exist.\nA safer approach is to take advantage of the concat  and one functions. The concat\nfunction takes two or more lists as inputs and combines them into a single list. The\none function takes a list as input and if the list has 0 elements, it returns null ; if the\nlist has 1 element, it returns that element; and if the list has more than 1 element,\nit shows an error. Putting these two together, and combining them with a splat\nexpression, you get the following:\noutput ""neo_cloudwatch_policy_arn""  {\n  value = one(concat(\n    aws_iam_user_policy_attachment.neo_cloudwatch_full_access [*].policy_arn ,\n    aws_iam_user_policy_attachment.neo_cloudwatch_read_only [*].policy_arn\n  ))\n}\nDepending on the outcome of the if/else conditional, either neo_cloud\nwatch_full_access  will be empty and neo_cloudwatch_read_only  will contain one\nelement or vice versa, so once you concatenate them together, you’ll have a list with\none element, and the one function will return that element. This will continue to\nwork correctly no matter how you change your if/else conditional.\nUsing count  and built-in functions to simulate if-else-statements is a bit of a hack,\nbut it’s one that works fairly well, and as you can see from the code, it allows you to\nconceal lots of complexity from your users so that they get to work with a clean and\nsimple API.\nConditionals with for_each and for Expressions\nNow that you understand how to do conditional logic with resources using the\ncount  parameter, you can probably guess that you can use a similar strategy to do\nconditional logic by using a for_each  expression.\nConditionals | 165\nIf you pass a for_each  expression an empty collection, the result will be zero copies\nof the resource, inline block, or module where you have the for_each ; if you pass it\na nonempty collection, it will create one or more copies of the resource, inline block,\nor module. The only question is, how do you conditionally decide if the collection\nshould be empty or not?\nThe answer is to combine the for_each  expression with the for expression.\nFor example, recall the way the webserver-cluster  module in modules/services/\nwebserver-cluster/main.tf  sets tags:\n  dynamic ""tag"" {\n    for_each  = var.custom_tags\n    content {\n      key                  = tag.key\n      value                = tag.value\n      propagate_at_launch  = true\n    }\n  }\nIf var.custom_tags  is empty, the for_each  expression will have nothing to loop\nover, so no tags will be set. In other words, you already have some conditional logic\nhere. But you can go even further, by combining the for_each  expression with a for\nexpression as follows:\n  dynamic ""tag"" {\n    for_each  = {\n      for key, value in var.custom_tags:\n      key  => upper(value)\n      if key ! = ""Name""\n    }\n    content {\n      key                  = tag.key\n      value                = tag.value\n      propagate_at_launch  = true\n    }\n  }\nThe nested for expression loops over var.custom_tags , converts each value to\nuppercase (perhaps for consistency), and uses a conditional in the for expression to\nfilter out any key set to Name  because the module already sets its own Name  tag. By\nfiltering values in the for expression, you can implement arbitrary conditional logic.\nNote that even though you should almost always prefer for_each  over count  for\ncreating multiple copies of a resource or module, when it comes to conditional logic,\nsetting count  to 0 or 1 tends to be simpler than setting for_each  to an empty or\nnonempty collection. Therefore, I typically recommend using count  to conditionally\n166 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",4512
54-Conditionals with the if String Directive.pdf,54-Conditionals with the if String Directive,"create resources and modules, and using for_each  for all other types of loops and\nconditionals.\nConditionals with the if String Directive\nLet’s now look at the if string directive, which has the following syntax:\n%{ if <CONDITION>  }<TRUEVAL>% { endif }\nwhere CONDITION  is any expression that evaluates to a boolean and TRUEVAL  is the\nexpression to render if CONDITION  evaluates to true.\nEarlier in the chapter, you used the for string directive to do loops within a string\nto output several comma-separated names. The problem was that there was an extra\ntrailing comma and space at the end of the string. Y ou can use the if string directive\nto fix this issue as follows:\noutput ""for_directive_index_if""  {\n  value = <<EOF\n%{ for i, name in var.names  }\n  ${name}% { if i < length(var.names ) - 1 }, %{ endif }\n%{ endfor }\nEOF\n}\nThere are a few changes here from the original version:\n•I put the code in a HEREDOC , which is a way to define multiline strings. This •\nallows me to spread the code out across several lines so it is more readable.\n•I used the if string directive to not output the comma and space for the last item •\nin the list.\nWhen you run terraform apply , you get the following output:\n$ terraform apply\n(...)\nOutputs:\nfor_directive_index_if = <<EOT\n  neo,\n  trinity,\n  morpheus\nEOT\nConditionals | 167\nWhoops. The trailing comma is gone, but we’ve introduced a bunch of extra white‐\nspace (spaces and newlines). Every whitespace you put in a HEREDOC ends up in\nthe final string. Y ou can fix this by adding strip markers  (~) to your string directives,\nwhich will eat up the extra whitespace before or after the strip marker:\noutput ""for_directive_index_if_strip""  {\n  value = <<EOF\n%{~ for i, name in var.names  ~}\n${name}% { if i < length(var.names ) - 1 }, %{ endif }\n%{~ endfor ~}\nEOF\n}\nLet’s give this version a try:\n$ terraform apply\n(...)\nOutputs:\nfor_directive_index_if_strip = ""neo, trinity, morpheus""\nOK, that’s a nice improvement: no extra whitespace or commas. Y ou can make\nthis output even prettier by adding an else  to the string directive, which uses the\nfollowing syntax:\n%{ if <CONDITION>  }<TRUEVAL>% { else }<FALSEVAL>% { endif }\nwhere FALSEVAL  is the expression to render if CONDITION  evaluates to false. Here’s an\nexample of how to use the else  clause to add a period at the end:\noutput ""for_directive_index_if_else_strip""  {\n  value = <<EOF\n%{~ for i, name in var.names  ~}\n${name}% { if i < length(var.names ) - 1 }, %{ else }.%{ endif }\n%{~ endfor ~}\nEOF\n}\nWhen you run terraform apply , you get the following output:\n$ terraform apply\n(...)\nOutputs:\nfor_directive_index_if_else_strip = ""neo, trinity, morpheus.""\n168 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",2822
55-Zero-Downtime Deployment.pdf,55-Zero-Downtime Deployment,"Zero-Downtime Deployment\nNow that your module has a clean and simple API for deploying a web server cluster,\nan important question to ask is, how do you update that cluster? That is, when you\nmake changes to your code, how do you deploy a new Amazon Machine Image\n(AMI) across the cluster? And how do you do it without causing downtime for your\nusers?\nThe first step is to expose the AMI as an input variable in modules/services/webserver-\ncluster/variables.tf . In real-world examples, this is all you would need because the\nactual web server code would be defined in the AMI. However, in the simplified\nexamples in this book, all of the web server code is actually in the User Data script,\nand the AMI is just a vanilla Ubuntu image. Switching to a different version of\nUbuntu won’t make for much of a demonstration, so in addition to the new AMI\ninput variable, you can also add an input variable to control the text the User Data\nscript returns from its one-liner HTTP server:\nvariable  ""ami"" {\n  description  = ""The AMI to run in the cluster""\n  type        = string\n  default      = ""ami-0fb653ca2d3203ac1""\n}\nvariable  ""server_text""  {\n  description  = ""The text the web server should return""\n  type        = string\n  default      = ""Hello, World""\n}\nNow you need to update the modules/services/webserver-cluster/user-data.sh  Bash\nscript to use this server_text  variable in the <h1>  tag it returns:\n#!/bin/bash\ncat > index.html <<EOF\n<h1>${server_text}</h1>\n<p>DB address: ${db_address}</p>\n<p>DB port: ${db_port}</p>\nEOF\nnohup busybox httpd -f -p ${server_port } &\nFinally, find the launch configuration in modules/services/webserver-cluster/main.tf ,\nupdate the image_id  parameter to use var.ami , and update the templatefile  call in\nthe user_data  parameter to pass in var.server_text :\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = var.ami\n  instance_type    = var.instance_type\n  security_groups  = [aws_security_group.instance.id ]\nZero-Downtime Deployment | 169\n  user_data        = templatefile (""${path.module}/user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = data.terraform_remote_state.db.outputs.address\n    db_port      = data.terraform_remote_state.db.outputs.port\n    server_text  = var.server_text\n  })\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n  }\n}\nNow, in the staging environment, in live/stage/services/webserver-cluster/main.tf , you\ncan set the new ami and server_text  parameters:\nmodule ""webserver_cluster""  {\n  source  = ""../../../../modules/services/webserver-cluster""\n  ami         = ""ami-0fb653ca2d3203ac1""\n  server_text  = ""New server text""\n  cluster_name            = ""webservers-stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n}\nThis code uses the same Ubuntu AMI, but changes the server_text  to a new value.\nIf you run the plan  command, you should see something like the following:\nTerraform will perform the following actions:\n  # module.webserver_cluster.aws_autoscaling_group.ex will be updated in-place\n  ~ resource ""aws_autoscaling_group"" ""example"" {\n        id                        = ""webservers-stage-terraform-20190516""\n      ~ launch_configuration      = ""terraform-20190516"" -> (known after apply)\n        (...)\n    }\n  # module.webserver_cluster.aws_launch_configuration.ex must be replaced\n+/- resource ""aws_launch_configuration"" ""example"" {\n      ~ id                          = ""terraform-20190516"" -> (known after apply)\n        image_id                    = ""ami-0fb653ca2d3203ac1""\n        instance_type               = ""t2.micro""\n      ~ name                        = ""terraform-20190516"" -> (known after apply)\n      ~ user_data                   = ""bd7c0a6"" -> ""4919a13"" # forces replacement\n        (...)\n170 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n1Credit for this technique goes to Paul Hinze .    }\nPlan: 1 to add, 1 to change, 1 to destroy.\nAs you can see, Terraform wants to make two changes: first, replace the old launch\nconfiguration with a new one that has the updated user_data ; and second, modify\nthe Auto Scaling Group in place to reference the new launch configuration. There\nis a problem here: merely referencing the new launch configuration will have no\neffect until the ASG launches new EC2 Instances. So how do you instruct the ASG to\ndeploy new Instances?\nOne option is to destroy the ASG (e.g., by running terraform destroy ) and then\nre-create it (e.g., by running terraform apply ). The problem is that after you delete\nthe old ASG, your users will experience downtime until the new ASG comes up.\nWhat you want to do instead is a zero-downtime deployment . The way to accomplish\nthat is to create the replacement ASG first and then destroy the original one. As it\nturns out, the create_before_destroy  lifecycle setting you first saw in Chapter 2\ndoes exactly this.\nHere’s how you can take advantage of this lifecycle setting to get a zero-downtime\ndeployment:1\n1.Configure the name  parameter of the ASG to depend directly on the name of the 1.\nlaunch configuration. Each time the launch configuration changes (which it will\nwhen you update the AMI or User Data), its name changes, and therefore the\nASG’s name will change, which forces Terraform to replace the ASG.\n2.Set the create_before_destroy  parameter of the ASG to true  so that each time 2.\nTerraform tries to replace it, it will create the replacement ASG before destroying\nthe original.\n3.Set the min_elb_capacity  parameter of the ASG to the min_size  of the cluster 3.\nso that Terraform will wait for at least that many servers from the new ASG to\npass health checks in the ALB before it will begin destroying the original ASG.\nHere is what the updated aws_autoscaling_group  resource should look like in mod‐\nules/services/webserver-cluster/main.tf :\nresource  ""aws_autoscaling_group"" ""example""  {\n  # Explicitly depend on the launch configuration's name so each time it's\n  # replaced, this ASG is also replaced\n  name = ""${var.cluster_name}-${aws_launch_configuration.example.name}""\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\nZero-Downtime Deployment | 171\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  # Wait for at least this many instances to pass health checks before\n  # considering the ASG deployment complete\n  min_elb_capacity  = var.min_size\n  # When replacing this ASG, create the replacement first, and only delete the\n  # original after\n  lifecycle  {\n    create_before_destroy  = true\n  }\n  tag {\n    key                 = ""Name""\n    value                = var.cluster_name\n    propagate_at_launch  = true\n  }\n  dynamic ""tag"" {\n    for_each  = {\n      for key, value in var.custom_tags:\n      key  => upper(value)\n      if key ! = ""Name""\n    }\n    content {\n      key                  = tag.key\n      value                = tag.value\n      propagate_at_launch  = true\n    }\n  }\n}\nIf you rerun the plan  command, you’ll now see something that looks like the\nfollowing:\nTerraform will perform the following actions:\n  # module.webserver_cluster.aws_autoscaling_group.example must be replaced\n+/- resource ""aws_autoscaling_group"" ""example"" {\n      ~ id     = ""example-2019"" -> (known after apply)\n      ~ name   = ""example-2019"" -> (known after apply) # forces replacement\n        (...)\n    }\n  # module.webserver_cluster.aws_launch_configuration.example must be replaced\n+/- resource ""aws_launch_configuration"" ""example"" {\n      ~ id              = ""terraform-2019"" -> (known after apply)\n        image_id        = ""ami-0fb653ca2d3203ac1""\n172 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n        instance_type   = ""t2.micro""\n      ~ name            = ""terraform-2019"" -> (known after apply)\n      ~ user_data       = ""bd7c0a"" -> ""4919a"" # forces replacement\n        (...)\n    }\n    (...)\nPlan: 2 to add, 2 to change, 2 to destroy.\nThe key thing to notice is that the aws_autoscaling_group  resource now says forces\nreplacement  next to its name parameter, which means that Terraform will replace it\nwith a new ASG running your new AMI or User Data. Run the apply  command to\nkick off the deployment, and while it runs, consider how the process works.\nY ou start with your original ASG running, say, v1 of your code ( Figure 5-1 ).\nFigure 5-1. Initially, you have the original ASG running v1 of your code.\nZero-Downtime Deployment | 173\nY ou make an update to some aspect of the launch configuration, such as switching\nto an AMI that contains v2 of your code, and run the apply  command. This forces\nTerraform to begin deploying a new ASG with v2 of your code ( Figure 5-2 ).\nFigure 5-2. Terraform begins deploying the new ASG with v2 of your code.\nAfter a minute or two, the servers in the new ASG have booted, connected to the\ndatabase, registered in the ALB, and started to pass health checks. At this point, both\nthe v1 and v2 versions of your app will be running simultaneously; and which one\nusers see depends on where the ALB happens to route them ( Figure 5-3 ).\n174 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nFigure 5-3. The servers in the new ASG boot up, connect to the DB, register in the ALB,\nand begin serving traffic.\nAfter min_elb_capacity  servers from the v2 ASG cluster have registered in the ALB,\nTerraform will begin to undeploy the old ASG, first by deregistering the servers in\nthat ASG from the ALB, and then by shutting them down ( Figure 5-4 ).\nZero-Downtime Deployment | 175\nFigure 5-4. The servers in the old ASG begin to shut down.\nAfter a minute or two, the old ASG will be gone, and you will be left with just v2 of\nyour app running in the new ASG ( Figure 5-5 ).\n176 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nFigure 5-5. Now, only the new ASG remains, which is running v2 of your code.\nDuring this entire process, there are always servers running and handling requests\nfrom the ALB, so there is no downtime. Open the ALB URL in your browser, and you\nshould see something like Figure 5-6 .\nZero-Downtime Deployment | 177\nFigure 5-6. The new code is now deployed.\nSuccess! The new server text has deployed. As a fun experiment, make another\nchange to the server_text  parameter—for example, update it to say “foo bar”—and\nrun the apply  command. In a separate terminal tab, if you’re on Linux/Unix/macOS,\nyou can use a Bash one-liner to run curl  in a loop, hitting your ALB once per second\nand allowing you to see the zero-downtime deployment in action:\n$ while true; do curl http://<load_balancer_url>; sleep 1; done\nFor the first minute or so, you should see the same response: New server text . Then,\nyou’ll begin seeing it alternate between New server text  and foo bar . This means\nthe new Instances have registered in the ALB and passed health checks. After another\nminute, the New server text  message will disappear, and you’ll see only foo bar ,\nwhich means the old ASG has been shut down. The output will look something like\nthis (for clarity, I’m listing only the contents of the <h1>  tags):\nNew server text\nNew server text\nNew server text\nNew server text\nNew server text\nNew server text\nfoo bar\nNew server text\nfoo bar\nNew server text\nfoo bar\nNew server text\nfoo bar\nNew server text\nfoo bar\nNew server text\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\nfoo bar\n178 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",12066
56-Terraform Gotchas.pdf,56-Terraform Gotchas,,0
57-count and for_each Have Limitations.pdf,57-count and for_each Have Limitations,"As an added bonus, if something went wrong during the deployment, Terraform\nwill automatically roll back. For example, if there were a bug in v2 of your app\nand it failed to boot, the Instances in the new ASG will not register with the ALB.\nTerraform will wait up to wait_for_capacity_timeout  (default is 10 minutes) for\nmin_elb_capacity  servers of the v2 ASG to register in the ALB, after which it consid‐\ners the deployment a failure, deletes the v2 ASG, and exits with an error (meanwhile,\nv1 of your app continues to run just fine in the original ASG).\nTerraform Gotchas\nAfter going through all these tips and tricks, it’s worth taking a step back and pointing\nout a few gotchas, including those related to the loop, if-statement, and deployment\ntechniques, as well as those related to more general problems that affect Terraform as\na whole:\n•count  and for_each  have limitations. •\n•Zero-downtime deployment has limitations.•\n•Valid plans can fail.•\n•Refactoring can be tricky.•\ncount and for_each Have Limitations\nIn the examples in this chapter, you made extensive use of the count  parameter\nand for_each  expressions in loops and if-statements. This works well, but there’s an\nimportant limitation that you need to be aware of: you cannot reference any resource\noutputs in count  or for_each .\nImagine that you want to deploy multiple EC2 Instances, and for some reason you\ndidn’t want to use an ASG. The code might look like this:\nresource  ""aws_instance"" ""example_1""  {\n  count         = 3\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nBecause count  is being set to a hardcoded value, this code will work without issues,\nand when you run apply , it will create three EC2 Instances. Now, what if you want to\ndeploy one EC2 Instance per Availability Zone (AZ) in the current AWS region? Y ou\ncould update your code to fetch the list of AZs using the aws_availability_zones\ndata source and use the count  parameter and array lookups to “loop” over each AZ\nand create an EC2 Instance in it:\nTerraform Gotchas | 179\nresource  ""aws_instance"" ""example_2""  {\n  count             = length(data.aws_availability_zones.all.names )\n  availability_zone  = data.aws_availability_zones.all.names[count.index ]\n  ami               = ""ami-0fb653ca2d3203ac1""\n  instance_type      = ""t2.micro""\n}\ndata ""aws_availability_zones"" ""all""  {}\nAgain, this code works just fine, since count  can reference data sources without\nproblems. However, what happens if the number of instances you need to create\ndepends on the output of some resource? The easiest way to experiment with this\nis to use the random_integer  resource, which, as you can probably guess from the\nname, returns a random integer:\nresource  ""random_integer"" ""num_instances""  {\n  min = 1\n  max = 3\n}\nThis code generates a random integer between 1 and 3. Let’s see what happens if\nyou try to use the result  output from this resource in the count  parameter of your\naws_instance  resource:\nresource  ""aws_instance"" ""example_3""  {\n  count         = random_integer.num_instances.result\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nIf you run terraform plan  on this code, you’ll get the following error:\nError: Invalid count argument\n  on main.tf line 30, in resource ""aws_instance"" ""example_3"":\n  30:   count         = random_integer.num_instances.result\nThe ""count"" value depends on resource attributes that cannot be determined\nuntil apply, so Terraform cannot predict how many instances will be created.\nTo work around this, use the -target argument to first apply only the\nresources that the count depends on.\nTerraform requires that it can compute count  and for_each  during the plan  phase,\nbefore  any resources are created or modified. This means that count  and for_each\ncan reference hardcoded values, variables, data sources, and even lists of resources\n(so long as the length of the list can be determined during plan ), but not computed\nresource outputs.\n180 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",4139
58-Zero-Downtime Deployment Has Limitations.pdf,58-Zero-Downtime Deployment Has Limitations,"Zero-Downtime Deployment Has Limitations\nThere are a couple of gotchas with using create_before_destroy  with an ASG to do\nzero-downtime deployment.\nThe first issue is that it doesn’t work with auto scaling policies. Or, to be more\naccurate, it resets your ASG size back to its min_size  after each deployment, which\ncan be a problem if you had used auto scaling policies to increase the number of\nrunning servers. For example, the webserver-cluster  module includes a couple\nof aws_autoscaling_schedule  resources that increase the number of servers in\nthe cluster from 2 to 10 at 9 a.m. If you ran a deployment at, say, 11 a.m., the\nreplacement ASG would boot up with only 2 servers, rather than 10, and it would\nstay that way until 9 a.m. the next day. There are several possible workarounds,\nsuch as tweaking the recurrence  parameter on the aws_autoscaling_schedule  or\nsetting the desired_capacity  parameter of the ASG to get its value from a custom\nscript that uses the AWS API to figure out how many instances were running before\ndeployment.\nHowever, the second, and bigger, issue is that, for important and complicated\ntasks like a zero-downtime deployment, you really want to use native, first-class\nsolutions, and not workarounds that require you to haphazardly glue together cre\nate_before_destroy , min_elb_capacity , custom scripts, etc. As it turns out, for\nAuto Scaling Groups, AWS now offers a native solution called instance refresh .\nGo back to your aws_autoscaling_group  resource and undo the zero-downtime\ndeployment changes:\n•Set name  back to var.cluster_name , instead of having it depend on the •\naws_launch_configuration  name.\n•Remove the create_before_destroy  and min_elb_capacity  settings. •\nAnd now, update the aws_autoscaling_group  resource to instead use an\ninstance_refresh  block as follows:\nresource  ""aws_autoscaling_group"" ""example""  {\n  name                 = var.cluster_name\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = data.aws_subnets.default.ids\n  target_group_arns     = [aws_lb_target_group.asg.arn ]\n  health_check_type     = ""ELB""\n  min_size  = var.min_size\n  max_size  = var.max_size\n  # Use instance refresh to roll out changes to the ASG\n  instance_refresh  {\n    strategy  = ""Rolling""\nTerraform Gotchas | 181\n    preferences  {\n      min_healthy_percentage  = 50\n    }\n  }\n}\nIf you deploy this ASG, and then later change some parameter (e.g., change\nserver_text ) and run plan , the diff will be back to just updating the\naws_launch_configuration :\nTerraform will perform the following actions:\n  # module.webserver_cluster.aws_autoscaling_group.ex will be updated in-place\n  ~ resource ""aws_autoscaling_group"" ""example"" {\n        id                        = ""webservers-stage-terraform-20190516""\n      ~ launch_configuration      = ""terraform-20190516"" -> (known after apply)\n        (...)\n    }\n  # module.webserver_cluster.aws_launch_configuration.ex must be replaced\n+/- resource ""aws_launch_configuration"" ""example"" {\n      ~ id                          = ""terraform-20190516"" -> (known after apply)\n        image_id                    = ""ami-0fb653ca2d3203ac1""\n        instance_type               = ""t2.micro""\n      ~ name                        = ""terraform-20190516"" -> (known after apply)\n      ~ user_data                   = ""bd7c0a6"" -> ""4919a13"" # forces replacement\n        (...)\n    }\nPlan: 1 to add, 1 to change, 1 to destroy.\nIf you run apply , it’ll complete very quickly, and at first, nothing new will be\ndeployed. However, in the background, because you modified the launch configura‐\ntion, AWS will kick off the instance refresh process, as shown in Figure 5-7 .\n182 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nFigure 5-7. An instance refresh is in progress.\nAWS will initially launch one new instance, wait for it to pass health checks, shut\ndown one of the older instances, and then repeat the process with the second\ninstance, until the instance refresh is completed, as shown in Figure 5-8 .\nFigure 5-8. An instance refresh is completed.\nThis process is entirely managed by AWS, is reasonably configurable, handles errors\npretty well, and requires no workarounds. The only drawback is the process can\nsometimes be slow (taking up to 20 minutes to replace just two servers), but\nother than that, it’s a much more robust solution to use for most zero-downtime\ndeployments.\nTerraform Gotchas | 183",4541
59-Valid Plans Can Fail.pdf,59-Valid Plans Can Fail,"In general, you should prefer to use first-class, native deployment options like\ninstance refresh whenever possible. Although such options weren’t always available\nin the earlier days of Terraform, these days, many resources support native deploy‐\nment options. For example, if you’re using Amazon Elastic Container Service (ECS)\nto deploy Docker containers, the aws_ecs_service  resource natively supports zero-\ndowntime deployments via the deployment_maximum_percent  and deployment_min\nimum_healthy_percent  parameters; if you’re using Kubernetes to deploy Docker\ncontainers, the kubernetes_deployment  resource natively supports zero-downtime\ndeployments by setting the strategy  parameter to RollingUpdate  and providing\nconfiguration via the rolling_update  block. Check the docs for the resources you’re\nusing, and make use of native functionality when you can!\nValid Plans Can Fail\nSometimes, you run the plan  command and it shows you a perfectly valid-looking\nplan, but when you run apply , you’ll get an error. For example, try to add an\naws_iam_user  resource with the exact same name you used for the IAM user you\ncreated manually in Chapter 2 :\nresource  ""aws_iam_user"" ""existing_user""  {\n  # Make sure to update this to your own user name!\n  name = ""yevgeniy.brikman""\n}\nIf you now run the plan  command, Terraform will show you a plan that looks\nreasonable:\nTerraform will perform the following actions:\n  # aws_iam_user.existing_user will be created\n  + resource ""aws_iam_user"" ""existing_user"" {\n      + arn           = (known after apply)\n      + force_destroy = false\n      + id            = (known after apply)\n      + name          = ""yevgeniy.brikman""\n      + path          = ""/""\n      + unique_id     = (known after apply)\n    }\nPlan: 1 to add, 0 to change, 0 to destroy.\nIf you run the apply  command, you’ll get the following error:\nError: Error creating IAM User yevgeniy.brikman: EntityAlreadyExists:\nUser with name yevgeniy.brikman already exists.\n  on main.tf line 10, in resource ""aws_iam_user"" ""existing_user"":\n  10: resource ""aws_iam_user"" ""existing_user"" {\n184 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\nThe problem, of course, is that an IAM user with that name already exists. This can\nhappen not just with IAM users but with almost any resource. Perhaps someone\ncreated that resource manually or via CLI commands, but either way, some identifier\nis the same, and that leads to a conflict. There are many variations on this error, and\nTerraform newbies are often caught off guard by them.\nThe key realization is that terraform plan  looks only at resources in its Terraform\nstate file. If you create resources out of band —such as by manually clicking around\nthe AWS Console—they will not be in Terraform’s state file, and, therefore, Terraform\nwill not take them into account when you run the plan  command. As a result, a\nvalid-looking plan will still fail.\nThere are two main lessons to take away from this:\nAfter  you start using Terraform, you should only use Terraform.\nWhen a part of your infrastructure is managed by Terraform, you should never\nmanually make changes to it. Otherwise, you not only set yourself up for weird\nTerraform errors, but you also void many of the benefits of using infrastructure\nas code in the first place, given that the code will no longer be an accurate\nrepresentation of your infrastructure.\nIf you have existing infrastructure, use the import  command.\nIf you created infrastructure before you started using Terraform, you can use\nthe terraform import  command to add that infrastructure to Terraform’s state\nfile so that Terraform is aware of and can manage that infrastructure. The\nimport  command takes two arguments. The first argument is the “address”\nof the resource in your Terraform configuration files. This makes use of the\nsame syntax as resource references, such as <PROVIDER>_<TYPE>.<NAME>  (e.g.,\naws_iam_user.existing_user ). The second argument is a resource-specific ID\nthat identifies the resource to import. For example, the ID for an aws_iam_user\nresource is the name of the user (e.g., yevgeniy.brikman), and the ID for an\naws_instance  is the EC2 Instance ID (e.g., i-190e22e5). The documentation at\nthe bottom of the page for each resource typically specifies how to import it.\nFor example, here is the import  command that you can use to sync the\naws_iam_user  you just added in your Terraform configurations with the IAM\nuser you created back in Chapter 2  (obviously, you should replace “yevgeniy.brik‐\nman” with your own username in this command):\n$ terraform import aws_iam_user.existing_user yevgeniy.brikman\nTerraform will use the AWS API to find your IAM user and create an associa‐\ntion in its state file between that user and the aws_iam_user.existing_user\nresource in your Terraform configurations. From then on, when you run the\nTerraform Gotchas | 185",4979
60-Refactoring Can Be Tricky.pdf,60-Refactoring Can Be Tricky,"plan  command , Terraform will know that an IAM user already exists and not try\nto create it again.\nNote that if you have a lot of existing resources that you want to import into\nTerraform, writing the Terraform code for them from scratch and importing\nthem one at a time can be painful, so you might want to look into tools such\nas terraformer  and terracognita , which can import both code and state from\nsupported cloud environments automatically.\nRefactoring Can Be Tricky\nA common programming practice is refactoring , in which you restructure the internal\ndetails of an existing piece of code without changing its external behavior. The\ngoal is to improve the readability, maintainability, and general hygiene of the code.\nRefactoring is an essential coding practice that you should do regularly. However,\nwhen it comes to Terraform, or any IaC tool, you have to be careful about what\ndefines the “external behavior” of a piece of code, or you will run into unexpected\nproblems.\nFor example, a common refactoring practice is to rename a variable or a function\nto give it a clearer name. Many IDEs even have built-in support for refactoring and\ncan automatically rename the variable or function for you, across the entire codebase.\nAlthough such a renaming is something you might do without thinking twice in a\ngeneral-purpose programming language, you need to be very careful about how you\ndo it in Terraform, or it could lead to an outage.\nFor example, the webserver-cluster  module has an input variable named\ncluster_name :\nvariable  ""cluster_name""  {\n  description  = ""The name to use for all the cluster resources""\n  type        = string\n}\nPerhaps you start using this module for deploying microservices, and, initially, you\nset your microservice’s name to foo. Later on, you decide that you want to rename\nthe service to bar. This might seem like a trivial change, but it can actually cause an\noutage!\nThat’s because the webserver-cluster  module uses the cluster_name  variable in a\nnumber of resources, including the name  parameters of two security groups and the\nALB:\nresource  ""aws_lb"" ""example""  {\n  name               = var.cluster_name\n  load_balancer_type  = ""application""\n  subnets             = data.aws_subnets.default.ids\n186 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas\n  security_groups     = [aws_security_group.alb.id ]\n}\nIf you change the name  parameter of certain resources, Terraform will delete the old\nversion of the resource and create a new version to replace it. If the resource you\nare deleting happens to be an ALB, there will be nothing to route traffic to your web\nserver cluster until the new ALB boots up. Similarly, if the resource you are deleting\nhappens to be a security group, your servers will reject all network traffic until the\nnew security group is created.\nAnother refactor that you might be tempted to do is to change a Terraform identifier.\nFor example, consider the aws_security_group  resource in the webserver-cluster\nmodule:\nresource  ""aws_security_group"" ""instance""  {\n  # (...)\n}\nThe identifier for this resource is called instance . Perhaps you were doing a refactor\nand you thought it would be clearer to change this name to cluster_instance :\nresource  ""aws_security_group"" ""cluster_instance""  {\n  # (...)\n}\nWhat’s the result? Yup, you guessed it: downtime.\nTerraform associates each resource identifier with an identifier from the cloud pro‐\nvider, such as associating an iam_user  resource with an AWS IAM User ID or an\naws_instance  resource with an AWS EC2 Instance ID. If you change the resource\nidentifier, such as changing the aws_security_group  identifier from instance  to\ncluster_instance , as far as Terraform knows, you deleted the old resource and have\nadded a completely new one. As a result, if you apply  these changes, Terraform will\ndelete the old security group and create a new one, and in the time period in between,\nyour servers will reject all network traffic. Y ou may run into similar problems if\nyou change the identifier associated with a module, split one module into multiple\nmodules, or add count  or for_each  to a resource or module that didn’t have it before.\nThere are four main lessons that you should take away from this discussion:\nAlways use the plan  command\nY ou can catch all of these gotchas by running the plan  command, carefully\nscanning the output, and noticing that Terraform plans to delete a resource that\nyou probably don’t want deleted.\nCreate before destroy\nIf you do want to replace a resource, think carefully about whether its replace‐\nment should be created before you delete the original. If so, you might be able\nTerraform Gotchas | 187\nto use create_before_destroy  to make that happen. Alternatively, you can also\naccomplish the same effect through two manual steps: first, add the new resource\nto your configurations and run the apply  command; second, remove the old\nresource from your configurations and run the apply  command again.\nRefactoring may require changing state\nIf you want to refactor your code without accidentally causing downtime, you’ll\nneed to update the Terraform state accordingly. However, you should never\nupdate Terraform state files by hand! Instead, you have two options: do it man‐\nually by running terraform state mv  commands, or do it automatically by\nadding a moved  block to your code.\nLet’s first look at the terraform state mv  command, which has the following\nsyntax:\nterraform state mv <ORIGINAL_REFERENCE> <NEW_REFERENCE>\nwhere ORIGINAL_REFERENCE  is the reference expression to the resource as it\nis now and NEW_REFERENCE  is the new location you want to move it to. For\nexample, if you’re renaming an aws_security_group  group from instance  to\ncluster_instance , you could run the following:\n$ terraform state mv \\n  aws_security_group.instance \\n  aws_security_group.cluster_instance\nThis instructs Terraform that the state that used to be associated with aws_secu\nrity_group.instance  should now be associated with aws_security_group.clus\nter_instance . If you rename an identifier and run this command, you’ll know\nyou did it right if the subsequent terraform plan  shows no changes.\nHaving to remember to run CLI commands manually is error prone, especially if\nyou refactored a module used by dozens of teams in your company, and each of\nthose teams needs to remember to run terraform state mv  to avoid downtime.\nFortunately, Terraform 1.1 has added a way to handle this automatically: moved\nblocks. Any time you refactor your code, you should add a moved  block to cap‐\nture how the state should be updated. For example, to capture that the aws_secu\nrity_group  resource was renamed from instance  to cluster_instance , you\nwould add the following moved  block:\nmoved {\n  from = aws_security_group.instance\n  to   = aws_security_group.cluster_instance\n}\nNow, whenever anyone runs apply  on this code, Terraform will automatically\ndetect if it needs to update the state file:\n188 | Chapter 5: Terraform Tips and Tricks: Loops, If-Statements, Deployment, and Gotchas",7195
61-Conclusion.pdf,61-Conclusion,"Terraform will perform the following actions:\n  # aws_security_group.instance has moved to\n  # aws_security_group.cluster_instance\n    resource ""aws_security_group"" ""cluster_instance"" {\n        name                   = ""moved-example-security-group""\n        tags                   = {}\n        # (8 unchanged attributes hidden)\n    }\nPlan: 0 to add, 0 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value:\nIf you enter yes, Terraform will update the state automatically, and as the plan\nshows no resources to add, change, or destroy, Terraform will make no other\nchanges—which is exactly what you want!\nSome parameters are immutable\nThe parameters of many resources are immutable, so if you change them, Ter‐\nraform will delete the old resource and create a new one to replace it. The\ndocumentation for each resource often specifies what happens if you change a\nparameter, so get used to checking the documentation. And, once again, make\nsure to always use the plan  command and consider whether you should use a\ncreate_before_destroy  strategy.\nConclusion\nAlthough Terraform is a declarative language, it includes a large number of tools,\nsuch as variables and modules, which you saw in Chapter 4 , and count , for_each ,\nfor, create_before_destroy , and built-in functions, which you saw in this chapter,\nthat give the language a surprising amount of flexibility and expressive power. There\nare many permutations of the if-statement tricks shown in this chapter, so spend\nsome time browsing the functions documentation , and let your inner hacker go wild.\nOK, maybe not too wild, as someone still needs to maintain your code, but just wild\nenough that you can create clean, beautiful APIs for your modules.\nLet’s now move on to Chapter 6 , where I’ll go over how create modules that are not\nonly clean and beautiful but also handle secrets and sensitive data in a safe and secure\nmanner.\nConclusion | 189",2063
62-Secret Management Tools.pdf,62-Secret Management Tools,"CHAPTER 6\nManaging Secrets with Terraform\nAt some point, you and your software will be entrusted with a variety of secrets,\nsuch as database passwords, API keys, TLS certificates, SSH keys, GPG keys, and so\non. This is all sensitive data that, if it were to get into the wrong hands, could do a\nlot of damage to your company and its customers. If you build software, it is your\nresponsibility to keep those secrets secure.\nFor example, consider the following Terraform code for deploying a database:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\n  # How to set these parameters securely?\n  username  = ""???""\n  password  = ""???""\n}\nThis code requires you to set two secrets, the username and password, which are the\ncredentials for the master user of the database. If the wrong person gets access to\nthem, it could be catastrophic, as these credentials give you superuser access to that\ndatabase and all the data within it. So, how do you keep these secrets secure?\nThis is part of the broader topic of secrets management , which is the focus of this\nchapter. This chapter will cover:\n•Secret management basics•\n•Secret management tools•\n191\n•Secret management tools with Terraform•\nSecret Management Basics\nThe first rule of secrets management is:\nDo not store secrets in plain text.\nThe second rule of secrets management is:\nDO NOT STORE SECRETS IN PLAIN TEXT.\nSeriously, don’t do it. For example, do not hardcode your database credentials directly\nin your Terraform code and check it into version control:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\n  # DO NOT DO THIS!!!\n  username  = ""admin""\n  password  = ""password""\n  # DO NOT DO THIS!!!\n}\nStoring secrets in plain text in version control is a bad idea . Here are just a few of the\nreasons why:\nAnyone who has access to the version control system has access to that secret.\nIn the preceding example, every single developer at your company who can\naccess that Terraform code will have access to the master credentials for your\ndatabase.\nEvery computer that has access to the version control system keeps a copy of that secret.\nEvery single computer that has ever checked out that repo may still have a\ncopy of that secret on its local hard drive. That includes the computer of every\ndeveloper on your team, every computer involved in CI (e.g., Jenkins, CircleCI,\nGitLab, etc.), every computer involved in version control (e.g., GitHub, GitLab,\nBitBucket), every computer involved in deployment (e.g., all your pre-prod and\nprod environments), every computer involved in backup (e.g., CrashPlan, Time\nMachine, etc.), and so on.\n192 | Chapter 6: Managing Secrets with Terraform",3120
63-The Interface You Use to Access Secrets.pdf,63-The Interface You Use to Access Secrets,"Every piece of software  you run has access to that secret.\nBecause the secrets are sitting in plain text on so many hard drives, every single\npiece of software running on any of those computers can potentially read that\nsecret.\nThere’s  no way to audit or revoke access to that secret.\nWhen secrets are sitting on hundreds of hard drives in plain text, you have no\nway to know who accessed them (there’s no audit log) and no easy way to revoke\naccess.\nIn short, if you store secrets in plain text, you are giving malicious actors (e.g.,\nhackers, competitors, disgruntled former employees) countless ways to access your\ncompany’s most sensitive data—e.g., by compromising the version control system,\nor by compromising any of the computers you use, or by compromising any piece\nof software on any of those computers—and you’ll have no idea if you were compro‐\nmised or have any easy way to fix things if you were.\nTherefore, it’s essential that you use a proper secret management tool  to store your\nsecrets.\nSecret Management Tools\nA comprehensive overview of all aspects of secret management is beyond the scope\nof this book, but to be able to use secret management tools with Terraform, it’s worth\nbriefly touching on the following topics:\n•The types of secrets you store•\n•The way you store secrets•\n•The interface you use to access secrets•\n•A comparison of secret management tools•\nThe Types of Secrets You Store\nThere are three primary types of secrets: personal secrets, customer secrets, and\ninfrastructure secrets.\nPersonal secrets\nBelong to an individual. Examples include the usernames and passwords for\nwebsites you visit, your SSH keys, and your Pretty Good Privacy (PGP) keys.\nCustomer secrets\nBelong to your customers. Note that if you run software for other employees\nof your company—e.g., you manage your company’s internal Active Directory\nserver—then those other employees are your customers. Examples include the\nSecret Management Tools | 193\nusernames and passwords that your customers use to log into your product,\npersonally identifiable info (PII) for your customers, and personal health infor‐\nmation (PHI) for your customers.\nInfrastructure secrets\nBelong to your infrastructure. Examples include database passwords, API keys,\nand TLS certificates.\nMost secret management tools are designed to store exactly one of these types of\nsecrets, and while you could try to force it to store the other types, that’s rarely a\ngood idea from a security or usability standpoint. For example, the way you store\npasswords that are infrastructure secrets is completely different from how you store\npasswords that are customer secrets: for the former, you’ d typically use an encryption\nalgorithm such as AES (Advanced Encryption Standard), perhaps with a nonce, as\nyou need to be able to decrypt the secrets and get back the original password; on the\nother hand, for the latter, you’ d typically use a hashing algorithm (e.g., bcrypt) with\na salt, as there should be no way to get back the original password. Using the wrong\napproach can be catastrophic, so use the right tool for the job!\nThe Way You Store Secrets\nThe two most common strategies for storing secrets are to use either a file-based\nsecret store or a centralized secret store.\nFile-based secret stores  store secrets in encrypted files, which are typically checked\ninto version control. To encrypt the files, you need an encryption key. This key is\nitself a secret! This creates a bit of a conundrum: How do you securely store that key?\nY ou can’t check the key into version control as plain text, as then there’s no point of\nencrypting anything with it. Y ou could encrypt the key with another key, but then all\nyou’ve done is kicked the can down the road, as you still have to figure out how to\nsecurely store that second key.\nThe most common solution to this conundrum is to store the key in a key manage‐\nment service  (KMS) provided by your cloud provider, such as AWS KMS, GCP KMS,\nor Azure Key Vault. This solves the kick-the-can-down-the-road problem by trusting\nthe cloud provider to securely store the secret and manage access to it. Another\noption is to use PGP keys. Each developer can have their own PGP key, which\nconsists  of a public key  and a private key . If you encrypt a secret with one or more\npublic keys, only developers with the corresponding private keys will be able to\ndecrypt those secrets. The private keys, in turn, are protected by a password that the\ndeveloper either memorizes or stores in a personal secrets manager.\nCentralized secret stores  are typically web services that you talk to over the network\nthat encrypt your secrets and store them in a data store such as MySQL, PostgreSQL,\nDynamoDB, etc. To encrypt these secrets, these centralized secret stores need an\n194 | Chapter 6: Managing Secrets with Terraform",4904
64-Secret Management Tools with Terraform.pdf,64-Secret Management Tools with Terraform,"encryption key. Typically, the encryption key is managed by the service itself, or the\nservice relies on a cloud provider’s KMS.\nThe Interface You Use to Access Secrets\nMost secret management tools can be accessed via an API, CLI, and/or UI.\nJust about all centralized secret stores expose an API that you can consume via\nnetwork requests: e.g., a REST API you access over HTTP . The API is convenient for\nwhen your code needs to programmatically read secrets. For example, when an app\nis booting up, it can make an API call to your centralized secret store to retrieve a\ndatabase password. Also, as you’ll see later in this chapter, you can write Terraform\ncode that, under the hood, uses a centralized secret store’s API to retrieve secrets.\nAll the file-based secret stores work via a command-line interface (CLI) . Many of the\ncentralized secret stores also provide CLI tools that, under the hood, make API calls\nto the service. CLI tools are a convenient way for developers to access secrets (e.g.,\nusing a few CLI commands to encrypt a file) and for scripting (e.g., writing a script to\nencrypt secrets).\nSome of the centralized secret stores also expose a user interface (UI)  via the web,\ndesktop, or mobile. This is potentially an even more convenient way for everyone on\nyour team to access secrets.\nA Comparison of Secret Management Tools\nTable 6-1  shows a comparison of popular secret management tools, broken down by\nthe three considerations defined in the previous sections.\nTable 6-1. A comparison of secret management tools\nTypes of secrets Secret storage Secret interface\nHashiCorp Vault InfrastructureaCentralized service UI, API, CLI\nAWS Secrets Manager Infrastructure Centralized service UI, API, CLI\nGoogle Secrets Manager Infrastructure Centralized service UI, API, CLI\nAzure Key Vault Infrastructure Centralized service UI, API, CLI\nConfidant Infrastructure Centralized service UI, API, CLI\nKeywhiz Infrastructure Centralized service API, CLI\nsops Infrastructure Files CLI\ngit-secret Infrastructure Files CLI\n1Password Personal Centralized service UI, API, CLI\nLastPass Personal Centralized service UI, API, CLI\nBitwarden Personal Centralized service UI, API, CLI\nKeePass Personal Files UI, CLI\nSecret Management Tools | 195",2283
65-Providers.pdf,65-Providers,"Types of secrets Secret storage Secret interface\nKeychain (macOS) Personal Files UI, CLI\nCredential Manager (Windows) Personal Files UI, CLI\npass Personal Files CLI\nActive Directory Customer Centralized service UI, API, CLI\nAuth0 Customer Centralized service UI, API, CLI\nOkta Customer Centralized service UI, API, CLI\nOneLogin Customer Centralized service UI, API, CLI\nPing Customer Centralized service UI, API, CLI\nAWS Cognito Customer Centralized service UI, API, CLI\na Vault supports multiple secret engines , most of which are designed for infrastructure secrets, but a few support customer\nsecrets as well.\nSince this is a book about Terraform, from here on out, I’ll mostly be focusing on\nsecret management tools designed for infrastructure secrets that are accessed through\nan API or the CLI (although I’ll mention personal secret management tools from\ntime to time too, as those often contain the secrets you need to authenticate to the\ninfrastructure secret tools).\nSecret Management Tools with Terraform\nLet’s now turn to how to use these secret management tools with Terraform, going\nthrough each of the three places where your Terraform code is likely to brush up\nagainst secrets:\n•Providers•\n•Resources and data sources•\n•State files and plan files•\nProviders\nTypically, your first exposure to secrets when working with Terraform is when you\nhave to authenticate to a provider. For example, if you want to run terraform apply\non code that uses the AWS Provider, you’ll need to first authenticate to AWS, and that\ntypically means using your access keys, which are secrets. How should you store those\nsecrets? And how should you make them available to Terraform?\nThere are many ways to answer these questions. One way you should not do it,\neven though it occasionally comes up in the Terraform documentation, is by putting\nsecrets directly into the code, in plain text:\n196 | Chapter 6: Managing Secrets with Terraform\n1Note that in most Linux/Unix/macOS shells, every command you type is stored on disk in some sort of\nhistory file (e.g., ~/.bash_history ). That’s why the export  commands shown here have a leading space: if you\nstart your command with a space, most shells will skip writing that command to the history file. Note that you\nmight need to set the HISTCONTROL  environment variable to “ignoreboth” to enable this if your shell doesn’t\nenable it by default.provider  ""aws"" {\n  region  = ""us-east-2""\n  # DO NOT DO THIS!!!\n  access_key  = ""(ACCESS_KEY)""\n  secret_key  = ""(SECRET_KEY)""\n  # DO NOT DO THIS!!!\n}\nStoring credentials this way, in plain text, is not secure, as discussed earlier in this\nchapter. Moreover, it’s also not practical, as this would hardcode you to using one set\nof credentials for all users of this module, whereas in most cases, you’ll need different\ncredentials on different computers (e.g., when different developers or your CI server\nruns apply ) and in different environments (dev, stage, prod).\nThere are several techniques that are far more secure for storing your credentials and\nmaking them accessible to Terraform providers. Let’s take a look at these techniques,\ngrouping them based on the user who is running Terraform:\nHuman users\nDevelopers running Terraform on their own computers.\nMachine users\nAutomated systems (e.g., a CI server) running Terraform with no humans\npresent.\nHuman users\nJust about all Terraform providers allow you to specify your credentials in some way\nother than putting them directly into the code. The most common option is to use\nenvironment variables. For example, here’s how you use environment variables to\nauthenticate to AWS:\n$  export AWS_ACCESS_KEY_ID=(YOUR_ACCESS_KEY_ID)\n$  export AWS_SECRET_ACCESS_KEY=(YOUR_SECRET_ACCESS_KEY)\nSetting your credentials as environment variables keeps plain-text secrets out of your\ncode, ensures that everyone running Terraform has to provide their own credentials,\nand ensures that credentials are only ever stored in memory, and not on disk.1\nOne important question you may ask is where to store the access key ID and secret\naccess key in the first place. They are too long and random to memorize, but if you\nSecret Management Tools with Terraform | 197\nstore them on your computer in plain text, then you’re still putting those secrets\nat risk. Since this section is focused on human users, the solution is to store your\naccess keys (and other secrets) in a secret manager designed for personal secrets. For\nexample, you could store your access keys in 1Password or LastPass and copy/paste\nthem into the export  commands in your terminal.\nIf you’re using these credentials frequently on the CLI, an even more convenient\noption is to use a secret manager that supports a CLI interface. For example, 1Pass‐\nword offers a CLI tool called op. On Mac and Linux, you can use op to authenticate to\n1Password on the CLI as follows:\n$ eval $(op signin my)\nOnce you’ve authenticated, assuming you had used the 1Password app to store your\naccess keys under the name “aws-dev” with fields “id” and “secret” , here’s how you can\nuse op to set those access keys as environment variables:\n$ export AWS_ACCESS_KEY_ID=$(op get item 'aws-dev' --fields 'id')\n$ export AWS_SECRET_ACCESS_KEY=$(op get item 'aws-dev' --fields 'secret')\nWhile tools like 1Password and op are great for general-purpose secrets management,\nfor certain providers, there are dedicated CLI tools to make this even easier. For\nexample, for authenticating to AWS, you can use the open source tool aws-vault . Y ou\ncan save your access keys using the aws-vault add  command under a profile  named\ndev as follows:\n$ aws-vault add dev\nEnter Access Key Id: (YOUR_ACCESS_KEY_ID)\nEnter Secret Key: (YOUR_SECRET_ACCESS_KEY)\nUnder the hood, aws-vault  will store these credentials securely in your operating\nsystem’s native password manager (e.g., Keychain on macOS, Credential Manager on\nWindows). Once you’ve stored these credentials, now you can authenticate to AWS\nfor any CLI command as follows:\n$ aws-vault exec <PROFILE> -- <COMMAND>\nwhere PROFILE  is the name of a profile you created earlier via the add command (e.g.,\ndev) and COMMAND  is the command to execute. For example, here’s how you can use\nthe dev credentials you saved earlier to run terraform apply :\n$ aws-vault exec dev -- terraform apply\nThe exec  command automatically uses AWS STS to fetch temporary credentials and\nexposes them as environment variables to the command you’re executing (in this\ncase, terraform apply ). This way, not only are your permanent credentials stored\nin a secure manner (in your operating system’s native password manager), but now,\nyou’re also only exposing temporary credentials to any process you run, so the risk\nof leaking credentials is minimized. aws-vault  also has native support for assuming\n198 | Chapter 6: Managing Secrets with Terraform\nIAM roles, using multifactor authentication (MFA), logging into accounts on the web\nconsole, and more.\nMachine users\nWhereas a human user can rely on a memorized password, what do you do in cases\nwhere there’s no human present? For example, if you’re setting up a continuous\nintegration / continuous delivery (CI/CD) pipeline to automatically run Terraform\ncode, how do you securely authenticate that pipeline? In this case, you are dealing\nwith authentication for a machine user . The question is, how do you get one machine\n(e.g., your CI server) to authenticate itself to another machine (e.g., AWS API servers)\nwithout storing any secrets in plain text?\nThe solution here heavily depends on the type of machines involved: that is, the\nmachine you’re authenticating from  and the machine you’re authenticating to. Let’s go\nthrough three examples:\n•CircleCI as a CI server, with stored secrets•\n•EC2 Instance running Jenkins as a CI server, with IAM roles•\n•GitHub Actions as a CI server, with OIDC•\nWarning: Simplified  Examples\nThis section contains examples that fully flush out how to han‐\ndle provider authentication in a CI/CD context, but all other\naspects of the CI/CD workflow are highly simplified. Y ou’ll see\nmore complete, end-to-end, production-ready CI/CD workflows in\nChapter 9 .\nCircleCI as a CI server, with stored secrets.    Let’s imagine that you want to use CircleCI,\na popular managed CI/CD platform, to run Terraform code. With CircleCI, you\nconfigure your build steps in a .circleci/config.yml  file, where you might define a job to\nrun terraform apply  that looks like this:\nversion: '2.1'\norbs:\n  # Install Terraform using a CircleCi Orb\n  terraform : circleci/terraform@1.1.0\njobs:\n  # Define a job to run 'terraform apply'\n  terraform_apply :\n    executor : terraform/default\n    steps:\n      - checkout          # git clone the code\n      - terraform/init    # Run 'terraform init'\nSecret Management Tools with Terraform | 199\n      - terraform/apply   # Run 'terraform apply'\nworkflows :\n  # Create a workflow to run the 'terraform apply' job defined above\n  deploy:\n    jobs:\n      - terraform_apply\n    # Only run this workflow on commits to the main branch\n    filters:\n      branches :\n        only:\n          - main\nWith a tool like CircleCI, the way to authenticate to a provider is to create a machine\nuser in that provider (that is, a user solely used for automation, and not by any\nhuman), store the credentials for that machine user in CircleCI in what’s called a\nCircleCI Context , and when your build runs, CircleCI will expose the credentials\nin that Context to your workflows as environment variables. For example, if your\nTerraform code needs to authenticate to AWS, you would create a new IAM user in\nAWS, give that IAM user the permissions it needs to deploy your Terraform changes,\nand manually copy that IAM user’s access keys into a CircleCI Context, as shown in\nFigure 6-1 .\nFigure 6-1. A CircleCI Context with AWS credentials.\nFinally, you update the workflows  in your .circleci/config.yml  file to use your CircleCI\nContext via the context  parameter:\n200 | Chapter 6: Managing Secrets with Terraform\nworkflows :\n  # Create a workflow to run the 'terraform apply' job defined above\n  deploy:\n    jobs:\n      - terraform_apply\n    # Only run this workflow on commits to the main branch\n    filters:\n      branches :\n        only:\n          - main\n    # Expose secrets in the CircleCI context as environment variables\n    context:\n      - example-context\nWhen your build runs, CircleCI will automatically expose the secrets in that\nContext as environment variables—in this case, as the environment variables\nAWS_ACCESS_KEY_ID  and AWS_SECRET_ACCESS_KEY —and terraform apply  will auto‐\nmatically use those environment variables to authenticate to your provider.\nThe biggest drawbacks to this approach are that (a) you have to manually manage\ncredentials, and (b) as a result, you have to use permanent credentials, which once\nsaved in CircleCI, rarely (if ever) change. The next two examples show alternative\napproaches.\nEC2 Instance running Jenkins as a CI server, with IAM roles.    If you’re using an EC2 Instance\nto run Terraform code—e.g., you’re running Jenkins on an EC2 Instance as a CI\nserver—the solution I recommend for machine user authentication is to give that\nEC2 Instance an IAM role. An IAM role  is similar to an IAM user, in that it’s an entity\nin AWS that can be granted IAM permissions. However, unlike IAM users, IAM\nroles are not associated with any one person and do not have permanent credentials\n(password or access keys). Instead, a role can be assumed  by other IAM entities: for\nexample, an IAM user might assume a role to temporarily get access to different\npermissions than they normally have; many AWS services, such as EC2 Instances, can\nassume IAM roles to grant those services permissions in your AWS account.\nFor example, here’s code you’ve seen many times to deploy an EC2 Instance:\nresource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nTo create an IAM role, you must first define an assume role policy , which is an IAM\nPolicy that defines who is allowed to assume the IAM role. Y ou could write the\nIAM Policy in raw JSON, but Terraform has a convenient aws_iam_policy_document\ndata source that can create the JSON for you. Here’s how you can use an aws_iam_pol\nicy_document  to define an assume role policy that allows the EC2 service to assume\nan IAM role:\nSecret Management Tools with Terraform | 201\ndata ""aws_iam_policy_document"" ""assume_role""  {\n  statement  {\n    effect   = ""Allow""\n    actions  = [""sts:AssumeRole"" ]\n    principals  {\n      type         = ""Service""\n      identifiers  = [""ec2.amazonaws.com"" ]\n    }\n  }\n}\nNow, you can use the aws_iam_role  resource to create an IAM role and pass it the\nJSON from your aws_iam_policy_document  to use as the assume role policy:\nresource  ""aws_iam_role"" ""instance""  {\n  name_prefix         = var.name\n  assume_role_policy  = data.aws_iam_policy_document.assume_role.json\n}\nY ou now have an IAM role, but by default, IAM roles don’t give you any permissions.\nSo, the next step is to attach one or more IAM policies to the IAM role that specify\nwhat you can actually do with the role once you’ve assumed it. Let’s imagine that\nyou’re using Jenkins to run Terraform code that deploys EC2 Instances. Y ou can use\nthe aws_iam_policy_document  data source to define an IAM Policy that gives admin\npermissions over EC2 Instances as follows:\ndata ""aws_iam_policy_document"" ""ec2_admin_permissions""  {\n  statement  {\n    effect     = ""Allow""\n    actions    = [""ec2:*""]\n    resources  = [""*""]\n  }\n}\nAnd you can attach this policy to your IAM role using the aws_iam_role_policy\nresource:\nresource  ""aws_iam_role_policy"" ""example""  {\n  role   = aws_iam_role.instance.id\n  policy  = data.aws_iam_policy_document.ec2_admin_permissions.json\n}\nThe final step is to allow your EC2 Instance to automatically assume that IAM role by\ncreating an instance profile :\nresource  ""aws_iam_instance_profile"" ""instance""  {\n  role = aws_iam_role.instance.name\n}\nAnd then tell your EC2 Instance to use that instance profile via the\niam_instance_profile  parameter:\n202 | Chapter 6: Managing Secrets with Terraform\n2By default, the instance metadata endpoint is open to all OS users running on your EC2 Instances. I\nrecommend locking this endpoint down so that only specific OS users can access it: e.g., if you’re running an\napp on the EC2 Instance as user app, you could use iptables  or nftables  to only allow app to access the\ninstance metadata endpoint. That way, if an attacker finds some vulnerability and is able to execute code on\nyour instance, they will only be able to access the IAM role permissions if they are able to authenticate as user\napp (rather than as any user). Better still, if you only need the IAM role permissions during boot (e.g., to read\na database password), you could disable the instance metadata endpoint entirely after boot, so an attacker who\ngets access later can’t use the endpoint at all.resource  ""aws_instance"" ""example""  {\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n  # Attach the instance profile\n  iam_instance_profile  = aws_iam_instance_profile.instance.name\n}\nUnder the hood, AWS runs an instance metadata endpoint  on every EC2 Instance\nat http://169.254.169.254 . This is an endpoint that can only be reached by processes\nrunning on the instance itself, and those processes can use that endpoint to fetch\nmetadata about the instance. For example, if you SSH to an EC2 Instance, you can\nquery this endpoint using curl :\n$ ssh ubuntu@<IP_OF_INSTANCE>\nWelcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.11.0-1022-aws x86_64)\n(...)\n$ curl http://169.254.169.254/latest/meta-data/\nami-id\nami-launch-index\nami-manifest-path\nblock-device-mapping/\nevents/\nhibernation/\nhostname\nidentity-credentials/\n(...)\nIf the instance has an IAM role attached (via an instance profile), that metadata will\ninclude AWS credentials that can be used to authenticate to AWS and assume that\nIAM role. Any tool that uses the AWS SDK, such as Terraform, knows how to use\nthese instance metadata endpoint credentials automatically, so as soon as you run\nterraform apply  on the EC2 Instance with this IAM role, your Terraform code will\nauthenticate as this IAM role, which will thereby grant your code the EC2 admin\npermissions it needs to run successfully.2\nFor any automated process running in AWS, such as a CI server, IAM roles provide\na way to authenticate (a) without having to manage credentials manually, and (b) the\ncredentials AWS provides via the instance metadata endpoint are always temporary,\nSecret Management Tools with Terraform | 203\n3At the time this book was written, OIDC support between GitHub Actions and AWS was fairly new and the\ndetails subject to change. Make sure to check the latest GitHub OIDC documentation  for the latest updates.and rotated automatically. These are two big advantages over the manually managed,\npermanent credentials with a tool like CircleCI that runs outside of your AWS\naccount. However, as you’ll see in the next example, in some cases, it’s possible to\nhave these same advantages for external tools, too.\nGitHub Actions as a CI server, with OIDC.    GitHub Actions is another popular managed\nCI/CD platform you might want to use to run Terraform. In the past, GitHub Actions\nrequired you to manually copy credentials around, just like CircleCI. However, as\nof 2021, GitHub Actions offers a better alternative: Open ID Connect (OIDC) . Using\nOIDC, you can establish a trusted link between the CI system and your cloud\nprovider (GitHub Actions supports AWS, Azure, and Google Cloud) so that your CI\nsystem can authenticate to those providers without having to manage any credentials\nmanually.\nY ou define GitHub Actions workflows in YAML files in a .github/workflows  folder,\nsuch as the terraform.yml  file shown here:\nname: Terraform Apply\n# Only run this workflow on commits to the main branch\non:\n  push:\n    branches :\n      - 'main'\njobs:\n  TerraformApply :\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      # Run Terraform using HashiCorp's setup-terraform Action\n      - uses: hashicorp/setup-terraform@v1\n          with:\n            terraform_version : 1.1.0\n            terraform_wrapper : false\n        run: |\n          terraform init\n          terraform apply -auto-approve\nIf your Terraform code talks to a provider such as AWS, you need to provide a way\nfor this workflow to authenticate to that provider. To do this using OIDC,3 the first\nstep is to create an IAM OIDC identity provider  in your AWS account, using the\naws_iam_openid_connect_provider  resource, and to configure it to trust the GitHub\nActions thumbprint, fetched via the tls_certificate  data source:\n204 | Chapter 6: Managing Secrets with Terraform\n# Create an IAM OIDC identity provider that trusts GitHub\nresource  ""aws_iam_openid_connect_provider"" ""github_actions""  {\n  url             = ""https://token.actions.githubusercontent.com""\n  client_id_list   = [""sts.amazonaws.com"" ]\n  thumbprint_list  = [\n    data.tls_certificate.github.certificates[0].sha1_fingerprint\n  ]\n}\n# Fetch GitHub's OIDC thumbprint\ndata ""tls_certificate"" ""github""  {\n  url = ""https://token.actions.githubusercontent.com""\n}\nNow, you can create IAM roles exactly as in the previous section—e.g., an IAM role\nwith EC2 admin permissions attached—except the assume role policy for those IAM\nroles will look different:\ndata ""aws_iam_policy_document"" ""assume_role_policy""  {\n  statement  {\n    actions  = [""sts:AssumeRoleWithWebIdentity"" ]\n    effect   = ""Allow""\n    principals  {\n      identifiers  = [aws_iam_openid_connect_provider.github_actions.arn ]\n      type         = ""Federated""\n    }\n    condition  {\n      test      = ""StringEquals""\n      variable  = ""token.actions.githubusercontent.com:sub""\n      # The repos and branches defined in var.allowed_repos_branches\n      # will be able to assume this IAM role\n      values  = [\n        for a in var.allowed_repos_branches  :\n        ""repo:${a[""org""]}/${a[""repo""]}:ref:refs/heads/${a[""branch""]}""\n      ]\n    }\n  }\n}\nThis policy allows the IAM OIDC identity provider to assume the IAM role via\nfederated authentication. Note the condition  block, which ensures that only the\nspecific GitHub repos and branches you specify via the allowed_repos_branches\ninput variable can assume this IAM role:\nvariable  ""allowed_repos_branches""  {\n  description  = ""GitHub repos/branches allowed to assume the IAM role.""\n  type = list(object({\n    org    = string\n    repo    = string\n    branch  = string\nSecret Management Tools with Terraform | 205",20961
66-Resources and Data Sources.pdf,66-Resources and Data Sources,"}))\n  # Example:\n  # allowed_repos_branches = [\n  #   {\n  #     org    = ""brikis98""\n  #     repo   = ""terraform-up-and-running-code""\n  #     branch = ""main""\n  #   }\n  # ]\n}\nThis is important to ensure you don’t accidentally allow all GitHub repos to authenti‐\ncate to your AWS account! Y ou can now configure your builds in GitHub Actions\nto assume this IAM role. First, at the top of your workflow, give your build the\nid-token: write  permission:\npermissions :\n  id-token : write\nNext, add a build step just before running Terraform to authenticate to AWS using\nthe configure-aws-credentials  action:\n      # Authenticate to AWS using OIDC\n      - uses: aws-actions/configure-aws-credentials@v1\n        with:\n          # Specify the IAM role to assume here\n          role-to-assume : arn:aws:iam::123456789012:role/example-role\n          aws-region : us-east-2\n      # Run Terraform using HashiCorp's setup-terraform Action\n      - uses: hashicorp/setup-terraform@v1\n          with:\n            terraform_version : 1.1.0\n            terraform_wrapper : false\n        run: |\n          terraform init\n          terraform apply -auto-approve\nNow, when you run this build in one of the repos and branches in the\nallowed_repos_branches  variable, GitHub will be able to assume your IAM role\nautomatically, using temporary credentials, and Terraform will authenticate to AWS\nusing that IAM role, all without having to manage any credentials manually.\nResources and Data Sources\nThe next place you’ll run into secrets with your Terraform code is with resources\nand data sources. For example, you saw earlier in the chapter the example of passing\ndatabase credentials to the aws_db_instance  resource:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n206 | Chapter 6: Managing Secrets with Terraform\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\n  # DO NOT DO THIS!!!\n  username  = ""admin""\n  password  = ""password""\n  # DO NOT DO THIS!!!\n}\nI’ve said it multiple times in this chapter already, but it’s such an important point that\nit’s worth repeating again: storing those credentials in the code, as plain text, is a bad\nidea. So, what’s a better way to do it?\nThere are three main techniques you can use:\n•Environment variables•\n•Encrypted files•\n•Secret stores•\nEnvironment variables\nThis first technique, which you saw back in Chapter 3 , as well as earlier in this\nchapter when talking about providers, keeps plain-text secrets out of your code by\ntaking advantage of Terraform’s native support for reading environment variables.\nTo use this technique, declare variables for the secrets you wish to pass in:\nvariable  ""db_username""  {\n  description  = ""The username for the database""\n  type        = string\n  sensitive    = true\n}\nvariable  ""db_password""  {\n  description  = ""The password for the database""\n  type        = string\n  sensitive    = true\n}\nJust as in Chapter 3 , these variables are marked with sensitive = true  to indicate\nthey contain secrets (so Terraform won’t log the values when you run plan  or apply ),\nand these variables do not have a default  (so as not to store secrets in plain text).\nNext, pass the variables to the Terraform resources that need those secrets:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\nSecret Management Tools with Terraform | 207\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\n  # Pass the secrets to the resource\n  username  = var.db_username\n  password  = var.db_password\n}\nNow you can pass in a value for each variable foo by setting the environment variable\nTF_VAR_foo :\n$  export TF_VAR_db_username=(DB_USERNAME)\n$  export TF_VAR_db_password=(DB_PASSWORD)\nPassing in secrets via environment variables helps you avoid storing secrets in plain\ntext in your code, but it doesn’t answer an important question: How do you store\nthe secrets securely? One nice thing about using environment variables is that they\nwork with almost any type of secrets management solution. For example, one option\nis to store the secrets in a personal secrets manager (e.g., 1Password) and manually\nset those secrets as environment variables in your terminal. Another option is to store\nthe secrets in a centralized secret store (e.g., HashiCorp Vault) and write a script\nthat uses that secret store’s API or CLI to read those secrets out and set them as\nenvironment variables.\nUsing environment variables has the following advantages:\n•Keep plain-text secrets out of your code and version control system.•\n•Storing secrets is easy, as you can use just about any other secret management•\nsolution. That is, if your company already has a way to manage secrets, you can\ntypically find a way to make it work with environment variables.\n•Retrieving secrets is easy, as reading environment variables is straightforward in•\nevery language.\n•Integrating with automated tests is easy, as you can easily set the environment•\nvariables to mock values.\n•Using environment variables doesn’t cost any money, unlike some of the other•\nsecret management solutions discussed later.\nUsing environment variables has the following drawbacks:\n•Not everything is defined in the Terraform code itself. This makes understanding•\nand maintaining the code harder. Everyone using your code has to know to take\nextra steps to either manually set these environment variables or run a wrapper\nscript.\n208 | Chapter 6: Managing Secrets with Terraform\n•Standardizing secret management practices is harder. Since all the management•\nof secrets happens outside of Terraform, the code doesn’t enforce any security\nproperties, and it’s possible someone is still managing the secrets in an insecure\nway (e.g., storing them in plain text).\n•Since the secrets are not versioned, packaged, and tested with your code, config‐•\nuration errors are more likely, such as adding a new secret in one environment\n(e.g., staging) but forgetting to add it in another environment (e.g., production).\nEncrypted files\nThe second technique relies on encrypting the secrets, storing the ciphertext in a file,\nand checking that file into version control.\nTo encrypt some data, such as some secrets in a file, you need an encryption key. As\nmentioned earlier in this chapter, this encryption key is itself a secret, so you need a\nsecure way to store it. The typical solution is to either use your cloud provider’s KMS\n(e.g., AWS KMS, Google KMS, Azure Key Vault) or to use the PGP keys of one or\nmore developers on your team.\nLet’s look at an example that uses AWS KMS. First, you’ll need to create a KMS\nCustomer Managed Key  (CMK), which is an encryption key that AWS manages for\nyou. To create a CMK, you first have to define a key policy , which is an IAM Policy\nthat defines who can use that CMK. To keep this example simple, let’s create a key\npolicy that gives the current user admin permissions over the CMK. Y ou can fetch the\ncurrent user’s information—their username, ARN, etc.—using the aws_caller_iden\ntity  data source:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\ndata ""aws_caller_identity"" ""self""  {}\nAnd now you can use the aws_caller_identity  data source’s outputs inside an\naws_iam_policy_document  data source to create a key policy that gives the current\nuser admin permissions over the CMK:\ndata ""aws_iam_policy_document"" ""cmk_admin_policy""  {\n  statement  {\n    effect     = ""Allow""\n    resources  = [""*""]\n    actions    = [""kms:*""]\n    principals  {\n      type         = ""AWS""\n      identifiers  = [data.aws_caller_identity.self.arn ]\n    }\n  }\n}\nSecret Management Tools with Terraform | 209\nNext, you can create the CMK using the aws_kms_key  resource:\nresource  ""aws_kms_key"" ""cmk""  {\n  policy  = data.aws_iam_policy_document.cmk_admin_policy.json\n}\nNote that, by default, KMS CMKs are only identified by a long numeric identifier\n(e.g., b7670b0e-ed67-28e4-9b15-0d61e1485be3 ), so it’s a good practice to also create\na human-friendly alias  for your CMK using the aws_kms_alias  resource:\nresource  ""aws_kms_alias"" ""cmk""  {\n  name          = ""alias/kms-cmk-example""\n  target_key_id  = aws_kms_key.cmk.id\n}\nThe preceding alias will allow you to refer to your CMK as alias/kms-cmk-example\nwhen using the AWS API and CLI, rather than a long identifier such as b7670b0e-\ned67-28e4-9b15-0d61e1485be3 . Once you’ve created the CMK, you can start using\nit to encrypt and decrypt data. Note that, by design, you’ll never be able to see (and,\ntherefore, to accidentally leak) the underlying encryption key. Only AWS has access\nto that encryption key, but you can make use of it by using the AWS API and CLI, as\ndescribed next.\nFirst, create a file called db-creds.yml  with some secrets in it, such as the database\ncredentials:\nusername : admin\npassword : password\nNote: do not check this file into version control, as you haven’t encrypted it yet! To\nencrypt this data, you can use the aws kms encrypt  command and write the resulting\nciphertext to a new file. Here’s a small Bash script (for Linux/Unix/macOS) called\nencrypt.sh  that performs these steps using the AWS CLI:\nCMK_ID=""$1""\nAWS_REGION =""$2""\nINPUT_FILE =""$3""\nOUTPUT_FILE =""$4""\necho ""Encrypting contents of $INPUT_FILE  using CMK $CMK_ID...""\nciphertext =$(aws kms encrypt \\n  --key-id ""$CMK_ID"" \\n  --region ""$AWS_REGION "" \\n  --plaintext ""fileb:// $INPUT_FILE "" \\n  --output text \\n  --query CiphertextBlob )\necho ""Writing result to $OUTPUT_FILE ...""\necho ""$ciphertext "" > ""$OUTPUT_FILE ""\necho ""Done!""\n210 | Chapter 6: Managing Secrets with Terraform\nHere’s how you can use encrypt.sh  to encrypt the db-creds.yml  file with the KMS\nCMK you created earlier and store the resulting ciphertext in a new file called\ndb-creds.yml.encrypted :\n$ ./encrypt.sh \\n  alias/kms-cmk-example \\n  us-east-2 \\n  db-creds.yml \\n  db-creds.yml.encrypted\nEncrypting contents of db-creds.yml using CMK alias/kms-cmk-example...\nWriting result to db-creds.yml.encrypted...\nDone!\nY ou can now delete db-creds.yml  (the plain-text file) and safely check db-\ncreds.yml.encrypted  (the encrypted file) into version control. At this point, you have\nan encrypted file with some secrets inside of it, but how do you make use of that file\nin your Terraform code?\nThe first step is to decrypt the secrets in this file using the aws_kms_secrets  data\nsource:\ndata ""aws_kms_secrets"" ""creds""  {\n  secret {\n    name     = ""db""\n    payload  = file(""${path.module}/db-creds.yml.encrypted"" )\n  }\n}\nThe preceding code reads db-creds.yml.encrypted  from disk using the file  helper\nfunction and, assuming you have permissions to access the corresponding key in\nKMS, decrypts the contents. That gives you back the contents of the original db-\ncreds.yml  file, so the next step is to parse the YAML as follows:\nlocals {\n  db_creds  = yamldecode (data.aws_kms_secrets.creds.plaintext [""db""])\n}\nThis code pulls out the database secrets from the aws_kms_secrets  data source,\nparses the YAML, and stores the results in a local variable called db_creds . Finally,\nyou can read the username and password from db_creds  and pass those credentials\nto the aws_db_instance  resource:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\nSecret Management Tools with Terraform | 211\n  # Pass the secrets to the resource\n  username  = local.db_creds.username\n  password  = local.db_creds.password\n}\nSo now you have a way to store secrets in an encrypted file, which are safe to check\ninto version control, and you have a way to read those secrets back out of the file in\nyour Terraform code automatically.\nOne thing to note with this approach is that working with encrypted files is awkward.\nTo make a change, you have to locally decrypt the file with a long aws kms decrypt\ncommand, make some edits, re-encrypt the file with another long aws kms encrypt\ncommand, and the whole time, be extremely careful to not accidentally check the\nplain-text data into version control or leave it sitting behind forever on your com‐\nputer. This is a tedious and error-prone process.\nOne way to make this less awkward is to use an open source tool called sops . When\nyou run sops <FILE> , sops will automatically decrypt FILE  and open your default\ntext editor with the plain-text contents. When you’re done editing and exit the text\neditor, sops will automatically encrypt the contents. This way, the encryption and\ndecryption are mostly transparent, with no need to run long aws kms  commands\nand less chance of accidentally checking plain-text secrets into version control. As\nof 2022, sops can work with files encrypted via AWS KMS, GCP KMS, Azure\nKey Vault, or PGP keys. Note that Terraform doesn’t yet have native support for\ndecrypting files that were encrypted by sops, so you’ll either need to use a third-party\nprovider such as carlpett/sops  or, if you’re a Terragrunt user, you can use the built-in\nsops_decrypt_file  function .\nUsing encrypted files has the following advantages:\n•Keep plain-text secrets out of your code and version control system.•\n•Y our secrets are stored in an encrypted format in version control, so they are•\nversioned, packaged, and tested with the rest of your code. This helps reduce\nconfiguration errors, such as adding a new secret in one environment (e.g.,\nstaging) but forgetting to add it in another environment (e.g., production).\n•Retrieving secrets is easy, assuming the encryption format you’re using is natively•\nsupported by Terraform or a third-party plugin.\n•It works with a variety of different encryption options: AWS KMS, GCP KMS,•\nPGP , etc.\n•Everything is defined in the code. There are no extra manual steps or wrapper•\nscripts required (although sops integration does require a third-party plugin).\n212 | Chapter 6: Managing Secrets with Terraform\nUsing encrypted files has the following drawbacks:\n•Storing secrets is harder. Y ou either have to run lots of commands (e.g., aws kms •\nencrypt ) or use an external tool such as sops. There’s a learning curve to using\nthese tools correctly and securely.\n•Integrating with automated tests is harder, as you will need to do extra•\nwork to make encryption keys and encrypted test data available for your test\nenvironments.\n•The secrets are now encrypted, but as they are still stored in version control,•\nrotating and revoking secrets is hard. If anyone ever compromises the encryption\nkey, they can go back and decrypt all the secrets that were ever encrypted with it.\n•The ability to audit who accessed secrets is minimal. If you’re using a cloud key•\nmanagement service (e.g., AWS KMS), it will likely maintain an audit log of who\nused an encryption key, but you won’t be able to tell what the key was actually\nused for (i.e., what secrets were accessed).\n•Most managed key services cost a small amount of money. For example, each•\nkey you store in AWS KMS costs $1/month, plus $0.03 per 10,000 API calls,\nwhere each decryption and encryption operation requires one API call. A typical\nusage pattern, where you have a small number of keys in KMS and your apps use\nthose keys to decrypt secrets during boot, usually costs $1–$10/month. For larger\ndeployments, where you have dozens of apps and hundreds of secrets, the price is\ntypically in the $10–$50/month range.\n•Standardizing secret management practices is harder. Different developers or•\nteams may use different ways to store encryption keys or manage encrypted files,\nand mistakes are relatively common, such as not using encryption correctly or\naccidentally checking in a plain-text file into version control.\nSecret stores\nThe third technique relies on storing your secrets in a centralized secret store.\nSome of the more popular secret stores are AWS Secrets Manager, Google Secret\nManager, Azure Key Vault, and HashiCorp Vault. Let’s look at an example using AWS\nSecrets Manager. The first step is to store your database credentials in AWS Secrets\nManager, which you can do using the AWS Web Console, as shown in Figure 6-2 .\nSecret Management Tools with Terraform | 213\nFigure 6-2. Store secrets in JSON format in AWS Secrets Manager.\nNote that the secrets in Figure 6-2  are in a JSON format, which is the recommended\nformat for storing data in AWS Secrets Manager.\nGo to the next step, and make sure to give the secret a unique name, such as\ndb-creds , as shown in Figure 6-3 .\n214 | Chapter 6: Managing Secrets with Terraform\nFigure 6-3. Give the secret a unique name in AWS Secrets Manager.\nClick Next and Store to save the secret. Now, in your Terraform code, you can use the\naws_secretsmanager_secret_version  data source to read the db-creds  secret:\ndata ""aws_secretsmanager_secret_version"" ""creds""  {\n  secret_id  = ""db-creds""\n}\nSince the secret is stored as JSON, you can use the jsondecode  function to parse the\nJSON into the local variable db_creds :\nlocals {\n  db_creds  = jsondecode (\n    data.aws_secretsmanager_secret_version.creds.secret_string\n  )\n}\nAnd now you can read the database credentials from db_creds  and pass them into\nthe aws_db_instance  resource:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\nSecret Management Tools with Terraform | 215\n  db_name              = var.db_name\n  # Pass the secrets to the resource\n  username  = local.db_creds.username\n  password  = local.db_creds.password\n}\nUsing secret stores has the following advantages:\n•Keep plain-text secrets out of your code and version control system.•\n•Everything is defined in the code itself. There are no extra manual steps or•\nwrapper scripts required.\n•Storing secrets is easy, as you typically can use a web UI.•\n•Secret stores typically support rotating and revoking secrets, which is useful in•\ncase a secret gets compromised. Y ou can even enable rotation on a scheduled\nbasis (e.g., every 30 days) as a preventative measure.\n•Secret stores typically support detailed audit logs that show you exactly who•\naccessed what data.\n•Secret stores make it easier to standardize all your secret practices, as they•\nenforce specific types of encryption, storage, access patterns, etc.\nUsing secret stores has the following drawbacks:\n•Since the secrets are not versioned, packaged, and tested with your code, config‐•\nuration errors are more likely, such as adding a new secret in one environment\n(e.g., staging) but forgetting to add it in another environment (e.g., production).\n•Most managed secret stores cost money. For example, AWS Secrets Manager•\ncharges $0.40 per month for each secret you store, plus $0.05 for every 10,000\nAPI calls you make to store or retrieve data. A typical usage pattern, where you\nhave several dozen secrets stored across several environments and a handful of\napps that read those secrets during boot, usually costs around $10–$25/month.\nWith larger deployments, where you have dozens of apps reading hundreds of\nsecrets, the price can go up to hundreds of dollars per month.\n•If you’re using a self-managed secret store such as HashiCorp Vault, then you’re•\nboth spending money to run the store (e.g., paying AWS for 3–5 EC2 Instances to\nrun Vault in a highly available mode) and spending time and money to have your\ndevelopers deploy, configure, manage, update, and monitor the store. Developer\ntime is very expensive, so depending on how much time they have to spend on\nsetting up and managing the secret store, this could cost you thousands of dollars\nper month.\n216 | Chapter 6: Managing Secrets with Terraform",20292
67-State Files and Plan Files.pdf,67-State Files and Plan Files,"•Retrieving secrets is harder, especially in automated environments (e.g., an app•\nbooting up and trying to read a database password), as you have to solve how to\ndo secure authentication between multiple machines.\n•Integrating with automated tests is harder, as much of the code you’re testing•\nnow depends on a running, external system that either needs to be mocked out\nor have test data stored in it.\nState Files and Plan Files\nThere are two more places where you’ll come across secrets when using Terraform:\n•State files•\n•Plan files•\nState files\nHopefully, this chapter has convinced you to not store your secrets in plain text and\nprovided you with some better alternatives. However, something that catches many\nTerraform users off guard is that, no matter which technique you use, any secrets you\npass into your Terraform resources and data sources will end up in plain text in your\nTerraform state file!\nFor example, no matter where you read the database credentials from—environment\nvariables, encrypted files, a centralized secret store—if you pass those credentials to a\nresource such as aws_db_instance :\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  db_name              = var.db_name\n  # Pass the secrets to the resource\n  username  = local.db_creds.username\n  password  = local.db_creds.password\n}\nthen Terraform will store those credentials in your terraform.tfstate  file, in plain text.\nThis has been an open issue  since 2014, with no clear plans for a first-class solution.\nThere are some workarounds out there that can scrub secrets from your state files,\nbut these are brittle and likely to break with each new Terraform release, so I don’t\nrecommend them.\nSecret Management Tools with Terraform | 217\nFor the time being, no matter which of the techniques discussed you end up using to\nmanage secrets, you must do the following:\nStore Terraform state in a backend that supports encryption\nInstead of storing your state in a local terraform.tfstate  file and checking it into\nversion control, you should use one of the backends Terraform supports that\nnatively supports encryption, such as S3, GCS, and Azure Blob Storage. These\nbackends will encrypt your state files, both in transit (e.g., via TLS) and on disk\n(e.g., via AES-256).\nStrictly control who can access your Terraform backend\nSince Terraform state files may contain secrets, you’ll want to control who has\naccess to your backend with at least  as much care as you control access to the\nsecrets themselves. For example, if you’re using S3 as a backend, you’ll want to\nconfigure an IAM Policy that solely grants access to the S3 bucket for production\nto a small handful of trusted devs, or perhaps solely just the CI server you use to\ndeploy to prod.\nPlan files\nY ou’ve seen the terraform plan  command many times. One feature you may not\nhave seen yet is that you can store the output of the plan command (the “diff ”) in a\nfile:\n$ terraform plan -out=example.plan\nThe preceding command stores the plan in a file called example.plan . Y ou can then\nrun the apply  command on this saved plan file to ensure that Terraform applies\nexactly  the changes you saw originally:\n$ terraform apply example.plan\nThis is a handy feature of Terraform, but an important caveat applies: just as with\nTerraform state, any secrets you pass into your Terraform resources and data sources\nwill end up in plain text in your Terraform plan files!  For example, if you ran plan\non the aws_db_instance  code, and saved a plan file, the plan file would contain the\ndatabase username and password, in plain text.\nTherefore, if you’re going to use plan files, you must do the following:\nEncrypt your Terraform plan files\nIf you’re going to save your plan files, you’ll need to find a way to encrypt\nthose files, both in transit (e.g., via TLS) and on disk (e.g., via AES-256). For\nexample, you could store plan files in an S3 bucket, which supports both types of\nencryption.\n218 | Chapter 6: Managing Secrets with Terraform",4231
68-Conclusion.pdf,68-Conclusion,"Strictly control who can access your plan files\nSince Terraform plan files may contain secrets, you’ll want to control who has\naccess to them with at least  as much care as you control access to the secrets\nthemselves. For example, if you’re using S3 to store your plan files, you’ll want to\nconfigure an IAM Policy that solely grants access to the S3 bucket for production\nto a small handful of trusted devs, or perhaps solely just the CI server you use to\ndeploy to prod.\nConclusion\nHere are your key takeaways from this chapter. First, if you remember nothing else\nfrom this chapter, please remember this: you should not store secrets in plain text.\nSecond, to pass secrets to providers, human users can use personal secrets managers\nand set environment variables, and machine users can use stored credentials, IAM\nroles, or OIDC. See Table 6-2  for the trade-offs between the machine user options.\nTable 6-2. A comparison of methods for machine users (e.g., a CI server) to pass secrets to\nTerraform providers\nStored credentials IAM roles OIDC\nExample CircleCI Jenkins on an EC2 Instance GitHub Actions\nAvoid manually managing credentials x ✓ ✓\nAvoid using permanent credentials x ✓ ✓\nWorks inside of cloud provider x ✓ x\nWorks outside of cloud provider ✓ x ✓\nWidely supported as of 2022 ✓ ✓ x\nThird, to pass secrets to resources and data sources, use environment variables,\nencrypted files, or centralized secret stores. See Table 6-3  for the trade-offs between\nthese different options.\nTable 6-3. A comparison of methods for passing secrets to Terraform resources and data\nsources\nEnvironment variables Encrypted files Centralized secret stores\nKeeps plain-text secrets out of code ✓ ✓ ✓\nAll secrets management defined  as code x ✓ ✓\nAudit log for access to encryption keys x ✓ ✓\nAudit log for access to individual secrets x x ✓\nRotating or revoking secrets is easy x x ✓\nStandardizing secrets management is easy x x ✓\nSecrets are versioned with the code x ✓ x\nConclusion | 219\nEnvironment variables Encrypted files Centralized secret stores\nStoring secrets is easy ✓ x ✓\nRetrieving secrets is easy ✓ ✓ x\nIntegrating with automated testing is easy ✓ x x\nCost 0 $ $$$\nAnd finally, fourth, no matter how you pass secrets to resources and data stores,\nremember that Terraform will store those secrets in your state files and plan files, in\nplain text, so make sure to always encrypt those files (in transit and at rest) and to\nstrictly control access to them.\nNow that you understand how to manage secrets when working with Terraform,\nincluding how to securely pass secrets to Terraform providers, let’s move on to\nChapter 7 , where you’ll learn how to use Terraform in cases where you have multiple\nproviders (e.g., multiple regions, multiple accounts, multiple clouds).\n220 | Chapter 6: Managing Secrets with Terraform",2873
69-Chapter 7. Working with Multiple Providers.pdf,69-Chapter 7. Working with Multiple Providers,,0
70-How Do You Install Providers.pdf,70-How Do You Install Providers,"CHAPTER 7\nWorking with Multiple Providers\nSo far, almost every single example in this book has included just a single provider\nblock:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nThis provider  block configures your code to deploy to a single AWS region in a\nsingle AWS account. This raises a few questions:\n•What if you need to deploy to multiple AWS regions?•\n•What if you need to deploy to multiple AWS accounts?•\n•What if you need to deploy to other clouds, such as Azure or GCP?•\nTo answer these questions, this chapter takes a deeper look at Terraform providers:\n•Working with one provider•\n•Working with multiple copies of the same provider•\n•Working with multiple different providers•\nWorking with One Provider\nSo far, you’ve been using providers somewhat “magically. ” That works well enough\nfor simple examples with one basic provider, but if you want to work with multiple\nregions, accounts, clouds, etc., you’ll need to go deeper. Let’s start by taking a closer\nlook at a single provider to better understand how it works:\n221\n•What is a provider?•\n•How do you install providers?•\n•How do you use providers?•\nWhat Is a Provider?\nWhen I first introduced providers in Chapter 2 , I described them as the platforms\nTerraform works with: e.g., AWS, Azure, Google Cloud, DigitalOcean, etc. So how\ndoes Terraform interact with these platforms?\nUnder the hood, Terraform consists of two parts:\nCore\nThis is the terraform  binary, and it provides all the basic functionality in Terra‐\nform that is used by all platforms, such as a command-line interface (i.e., plan ,\napply , etc.), a parser and interpreter for Terraform code (HCL), the ability to\nbuild a dependency graph from resources and data sources, logic to read and\nwrite state files, and so on. Under the hood, the code is written in Go and lives in\nan open source GitHub repo  owned and maintained by HashiCorp.\nProviders\nTerraform  providers are plugins  for the Terraform core. Each plugin is written\nin Go to implement a specific interface, and the Terraform core knows how to\ninstall and execute the plugin. Each of these plugins is designed to work with\nsome platform in the outside world, such as AWS, Azure, or Google Cloud. The\nTerraform core communicates with plugins via remote procedure calls  (RPCs),\nand those plugins, in turn, communicate with their corresponding platforms via\nthe network (e.g., via HTTP calls), as shown in Figure 7-1 .\nThe code for each plugin typically lives in its own repo. For example, all the AWS\nfunctionality you’ve been using in the book so far comes from a plugin called the\nTerraform AWS Provider (or just AWS Provider for short) that lives in its own\nrepo . Although HashiCorp created most of the initial providers, and still helps to\nmaintain many of them, these days, much of the work for each provider is done\nby the company that owns the underlying platform: e.g., AWS employees work\non the AWS Provider, Microsoft employees work on the Azure provider, Google\nemployees work on the Google Cloud provider, and so on.\n222 | Chapter 7: Working with Multiple Providers\n1In fact, you could even skip the provider  block and just add any resource or data source from an official\nprovider and Terraform will figure out which provider to use based on the prefix: for example, if you add the\naws_instance  resource, Terraform will know to use the AWS Provider based on the aws_  prefix.\nFigure 7-1. The interaction between the Terraform core, providers, and the outside\nworld.\nEach provider claims a specific prefix and exposes one or more resources\nand data sources whose names include that prefix: e.g., all the resources and\ndata sources from the AWS Provider use the aws_  prefix (e.g., aws_instance ,\naws_autoscaling_group , aws_ami ), all the resources and data sources from the\nAzure provider use the azurerm_  prefix (e.g., azurerm_virtual_machine , azur\nerm_virtual_machine_scale_set , azurerm_image ), and so on.\nHow Do You Install Providers?\nFor official Terraform providers, such as the ones for AWS, Azure, and Google Cloud,\nit’s enough to just add a provider  block to your code:1\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nAs soon as you run terraform init , Terraform automatically downloads the code\nfor the provider:\n$ terraform init\nInitializing provider plugins...\n- Finding hashicorp/aws versions matching ""4.19.0""...\n- Installing hashicorp/aws v4.19.0...\n- Installed hashicorp/aws v4.19.0 (signed by HashiCorp)\nWorking with One Provider | 223\nThis is a bit magical, isn’t it? How does Terraform know what provider you want?\nOr which version you want? Or where to download it from? Although it’s OK to rely\non this sort of magic for learning and experimenting, when writing production code,\nyou’ll probably want a bit more control over how Terraform installs providers. Do\nthis by adding a required_providers  block, which has the following syntax:\nterraform  {\n  required_providers  {\n    <LOCAL_NAME>  = {\n      source   = ""<URL>""\n      version  = ""<VERSION>""\n    }\n  }\n}\nwhere:\nLOCAL_NAME\nThis is the local name  to use for the provider in this module. Y ou must give each\nprovider a unique name, and you use that name in the provider  block configura‐\ntion. In almost all cases, you’ll use the preferred local name  of that provider: e.g.,\nfor the AWS Provider, the preferred local name is aws, which is why you write\nthe provider block as provider ""aws"" { … } . However, in rare cases, you may\nend up with two providers that have the same preferred local name—e.g., two\nproviders that both deal with HTTP requests and have a preferred local name of\nhttp —so you can use this local name to disambiguate between them.\nURL\nThis is the URL from where Terraform should download the provider, in the for‐\nmat [<HOSTNAME>/]<NAMESPACE>/<TYPE> , where HOSTNAME  is the hostname of a\nTerraform Registry that distributes the provider, NAMESPACE  is the organizational\nnamespace (typically, a company name), and TYPE  is the name of the platform\nthis provider manages (typically, TYPE  is the preferred local name). For example,\nthe full URL for the AWS Provider, which is hosted in the public Terraform Reg‐\nistry , is registry.terraform.io/hashicorp/aws . However, note that HOSTNAME\nis optional, and if you omit it, Terraform will by default download the provider\nfrom the public Terraform Registry, so the shorter and more common way to\nspecify the exact same AWS Provider URL is hashicorp/aws . Y ou typically only\ninclude HOSTNAME  for custom providers that you’re downloading from private\nTerraform Registries (e.g., a private Registry you’re running in Terraform Cloud\nor Terraform Enterprise).\n224 | Chapter 7: Working with Multiple Providers",6786
71-Working with Multiple Copies of the Same Provider.pdf,71-Working with Multiple Copies of the Same Provider,"VERSION\nThis is a version constraint. For example, you could set it to a specific version,\nsuch as 4.19.0 , or to a version range, such as > 4.0, < 4.3 . Y ou’ll learn more\nabout how to handle versioning in Chapter 8 .\nFor example, to install version 4.x of the AWS Provider, you can use the following\ncode:\nterraform  {\n  required_providers  {\n    aws = {\n      source   = ""hashicorp/aws""\n      version  = ""~> 4.0""\n    }\n  }\n}\nSo now you can finally understand the magical provider installation behavior you\nsaw earlier. If you add a new provider  block named foo to your code, and you don’t\nspecify a required_providers  block, when you run terraform init , Terraform will\nautomatically do the following:\n•Try to download provider foo with the assumption that the HOSTNAME  is the •\npublic Terraform Registry and that the NAMESPACE  is hashicorp , so the download\nURL is registry.terraform.io/hashicorp/foo .\n•If that’s a valid URL, install the latest version of the foo provider available at that •\nURL.\nIf you want to install any provider not in the hashicorp  namespace (e.g., if you want\nto use providers from Datadog, Cloudflare, or Confluent, or a custom provider you\nbuilt yourself), or you want to control the version of the provider you use, you will\nneed to include a required_providers  block.\nAlways Include required_providers\nAs you’ll learn in Chapter 8 , it’s important to control the version\nof the provider you use, so I recommend always  including a\nrequired_providers  block in your code.\nHow Do You Use Providers?\nWith this new knowledge about providers, let’s revisit how to use them. The first step\nis to add a required_providers  block to your code to specify which provider you\nwant to use:\nterraform  {\n  required_providers  {\nWorking with One Provider | 225",1822
72-Working with Multiple AWS Regions.pdf,72-Working with Multiple AWS Regions,"aws = {\n      source   = ""hashicorp/aws""\n      version  = ""~> 4.0""\n    }\n  }\n}\nNext, you add a provider  block to configure that provider:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nSo far, you’ve only been configuring the region  to use in the AWS Provider, but there\nare many other settings you can configure. Always check your provider’s documenta‐\ntion for the details: typically, this documentation lives in the same Registry you use to\ndownload the provider (the one in the source  URL). For example, the documentation\nfor the AWS Provider  is in the public Terraform Registry. This documentation will\ntypically explain how to configure the provider to work with different users, roles,\nregions, accounts, and so on.\nOnce you’ve configured a provider, all the resources and data sources from that\nprovider (all the ones with the same prefix) that you put into your code will automat‐\nically use that configuration. So, for example, when you set the region in the aws\nprovider to us-east-2 , all the aws_  resources to your code will automatically deploy\ninto us-east-2 .\nBut what if you want some of those resources to deploy into us-east-2  and some\ninto a different region, such as us-west-1 ? Or what if you want to deploy some\nresources to a completely different AWS account? To do that, you’ll have to learn how\nto configure multiple copies of the same provider, as discussed in the next section.\nWorking with Multiple Copies of the Same Provider\nTo understand how to work with multiple copies of the same provider, let’s look at a\nfew of the common cases where this comes up:\n•Working with multiple AWS regions•\n•Working with multiple AWS accounts•\n•Creating modules that can work with multiple providers•\nWorking with Multiple AWS Regions\nMost cloud providers allow you to deploy into datacenters (“regions”) all over the\nworld, but when you configure a Terraform provider, you typically configure it to\n226 | Chapter 7: Working with Multiple Providers\ndeploy into just one of those regions. For example, so far you’ve been deploying into\njust a single AWS region, us-east-2 :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nWhat if you wanted to deploy into multiple regions? For example, how could you\ndeploy some resources into us-east-2  and other resources into us-west-1 ? Y ou\nmight be tempted to solve this by defining two provider  configurations, one for each\nregion:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nprovider  ""aws"" {\n  region  = ""us-west-1""\n}\nBut now there’s a new problem: How do you specify which of these provider  config‐\nurations each of your resources, data sources, and modules should use? Let’s look at\ndata sources first. Imagine you had two copies of the aws_region  data source, which\nreturns the current AWS region:\ndata ""aws_region"" ""region_1""  {\n}\ndata ""aws_region"" ""region_2""  {\n}\nHow do you get the region_1  data source to use the us-east-2  provider and the\nregion_2  data source to use the us-west-1  provider? The solution is to add an alias\nto each provider:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""region_1""\n}\nprovider  ""aws"" {\n  region  = ""us-west-1""\n  alias  = ""region_2""\n}\nAn alias  is a custom name for the provider , which you can explicitly pass to individ‐\nual resources, data sources, and modules to get them to use the configuration in that\nparticular provider. To tell those aws_region  data sources to use a specific provider,\nyou set the provider  parameter as follows:\nWorking with Multiple Copies of the Same Provider | 227\ndata ""aws_region"" ""region_1""  {\n  provider  = aws.region_1\n}\ndata ""aws_region"" ""region_2""  {\n  provider  = aws.region_2\n}\nAdd some output variables so you can check that this is working:\noutput ""region_1""  {\n  value       = data.aws_region.region_1.name\n  description  = ""The name of the first region""\n}\noutput ""region_2""  {\n  value       = data.aws_region.region_2.name\n  description  = ""The name of the second region""\n}\nAnd run apply :\n$ terraform apply\n(...)\nOutputs:\nregion_1 = ""us-east-2""\nregion_2 = ""us-west-1""\nAnd there you go: each of the aws_region  data sources is now using a different\nprovider and, therefore, running against a different AWS region. The same technique\nof setting the provider  parameter works with resources too. For example, here’s how\nyou can deploy two EC2 Instances in different regions:\nresource  ""aws_instance"" ""region_1""  {\n  provider  = aws.region_1\n  # Note different AMI IDs!!\n  ami           = ""ami-0fb653ca2d3203ac1""\n  instance_type  = ""t2.micro""\n}\nresource  ""aws_instance"" ""region_2""  {\n  provider  = aws.region_2\n  # Note different AMI IDs!!\n  ami           = ""ami-01f87c43e618bf8f0""\n  instance_type  = ""t2.micro""\n}\n228 | Chapter 7: Working with Multiple Providers\nNotice how each aws_instance  resource sets the provider  parameter to ensure it\ndeploys into the proper region. Also, note that the ami parameter has to be different\non the two aws_instance  resources: that’s because AMI IDs are unique to each AWS\nregion, so the ID for Ubuntu 20.04 in us-east-2  is different than for Ubuntu 20.04\nin us-west-1 . Having to look up and manage these AMI IDs manually is tedious and\nerror prone. Fortunately, there’s a better alternative: use the aws_ami  data source that,\ngiven a set of filters, can find AMI IDs for you automatically. Here’s how you can use\nthis data source twice, once in each region, to look up Ubuntu 20.04 AMI IDs:\ndata ""aws_ami"" ""ubuntu_region_1""  {\n  provider  = aws.region_1\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\ndata ""aws_ami"" ""ubuntu_region_2""  {\n  provider  = aws.region_2\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\nNotice how each data source sets the provider  parameter to ensure it’s looking up the\nAMI ID in the proper region. Go back to the aws_instance  code and update the ami\nparameter to use the output of these data sources instead of the hardcoded values:\nresource  ""aws_instance"" ""region_1""  {\n  provider  = aws.region_1\n  ami           = data.aws_ami.ubuntu_region_1.id\n  instance_type  = ""t2.micro""\n}\nresource  ""aws_instance"" ""region_2""  {\n  provider  = aws.region_2\n  ami           = data.aws_ami.ubuntu_region_2.id\n  instance_type  = ""t2.micro""\n}\nWorking with Multiple Copies of the Same Provider | 229\nMuch better. Now, no matter what region you deploy into, you’ll automatically get the\nproper AMI ID for Ubuntu. To check that these EC2 Instances are really deploying\ninto different regions, add output variables that show you which availability zone\n(each of which is in one region) each instance was actually deployed into:\noutput ""instance_region_1_az""  {\n  value       = aws_instance.region_1.availability_zone\n  description  = ""The AZ where the instance in the first region deployed""\n}\noutput ""instance_region_2_az""  {\n  value       = aws_instance.region_2.availability_zone\n  description  = ""The AZ where the instance in the second region deployed""\n}\nAnd now run apply :\n$ terraform apply\n(...)\nOutputs:\ninstance_region_1_az = ""us-east-2a""\ninstance_region_2_az = ""us-west-1b""\nOK, so now you know how to deploy data sources and resources into different\nregions. What about modules? For example, in Chapter 3 , you used Amazon RDS\nto deploy a single instance of a MySQL database in the staging environment ( stage/\ndata-stores/mysql ):\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  engine               = ""mysql""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  username  = var.db_username\n  password  = var.db_password\n}\nThis is fine in staging, but in production, a single database is a single point of\nfailure. Fortunately, Amazon RDS natively supports replication , where your data is\nautomatically copied from a primary database to a secondary database—a read-only\nreplica —which is useful for scalability and as a standby in case the primary goes\ndown. Y ou can even run the replica in a totally different AWS region, so if one region\n230 | Chapter 7: Working with Multiple Providers\ngoes down (e.g., there’s a major outage in us-east-2 ), you can switch to the other\nregion (e.g., us-west-1 ).\nLet’s turn that MySQL code in the staging environment into a reusable mysql  module\nthat supports replication. First, copy all the contents of stage/data-stores/mysql , which\nshould include main.tf , variables.tf , and outputs.tf , into a new modules/data-stores/\nmysql  folder. Next, open modules/data-stores/mysql/variables.tf  and expose two new\nvariables:\nvariable  ""backup_retention_period""  {\n  description  = ""Days to retain backups. Must be > 0 to enable replication.""\n  type        = number\n  default      = null\n}\nvariable  ""replicate_source_db""  {\n  description  = ""If specified, replicate the RDS database at the given ARN.""\n  type        = string\n  default      = null\n}\nAs you’ll see shortly, you’ll set the backup_retention_period  variable on the primary\ndatabase to enable replication, and you’ll set the replicate_source_db  variable on\nthe secondary database to turn it into a replica. Open up modules/data-stores/mysql/\nmain.tf , and update the aws_db_instance  resource as follows:\n1.Pass the backup_retention_period  and replicate_source_db  variables into 1.\nparameters of the same name in the aws_db_instance  resource.\n2.If a database instance is a replica, AWS does not allow you to set the engine , 2.\ndb_name , username , or password  parameters, as those are all inherited from\nthe primary. So you must add some conditional logic to the aws_db_instance\nresource to not set those parameters when the replicate_source_db  variable is\nset.\nHere’s what the resource should look like after the changes:\nresource  ""aws_db_instance"" ""example""  {\n  identifier_prefix    = ""terraform-up-and-running""\n  allocated_storage    = 10\n  instance_class       = ""db.t2.micro""\n  skip_final_snapshot  = true\n  # Enable backups\n  backup_retention_period  = var.backup_retention_period\n  # If specified, this DB will be a replica\n  replicate_source_db  = var.replicate_source_db\nWorking with Multiple Copies of the Same Provider | 231\n  # Only set these params if replicate_source_db is not set\n  engine    = var.replicate_source_db  == null ? ""mysql"" : null\n  db_name   = var.replicate_source_db  == null ? var.db_name  : null\n  username  = var.replicate_source_db  == null ? var.db_username  : null\n  password  = var.replicate_source_db  == null ? var.db_password  : null\n}\nNote that for replicas, this implies that the db_name , db_username , and db_password\ninput variables in this module should be optional, so it’s a good idea to go back to\nmodules/data-stores/mysql/variables.tf  and set the default  for those variables to null :\nvariable  ""db_name""  {\n  description  = ""Name for the DB.""\n  type        = string\n  default      = null\n}\nvariable  ""db_username""  {\n  description  = ""Username for the DB.""\n  type        = string\n  sensitive    = true\n  default      = null\n}\nvariable  ""db_password""  {\n  description  = ""Password for the DB.""\n  type        = string\n  sensitive    = true\n  default      = null\n}\nTo use the replicate_source_db  variable, you’ll need set it to the ARN of another\nRDS database, so you should also update modules/data-stores/mysql/outputs.tf  to add\nthe database ARN as an output variable:\noutput ""arn"" {\n  value       = aws_db_instance.example.arn\n  description  = ""The ARN of the database""\n}\nOne more thing: you should add a required_providers  block to this module to\nspecify that this module expects to use the AWS Provider, and to specify which\nversion of the provider the module expects.\nterraform  {\n  required_providers  {\n    aws = {\n      source   = ""hashicorp/aws""\n      version  = ""~> 4.0""\n    }\n  }\n}\n232 | Chapter 7: Working with Multiple Providers\nY ou’ll see in a moment why this is important when working with multiple regions,\ntoo!\nOK, you can now use this mysql  module to deploy a MySQL primary and a MySQL\nreplica in the production environment. First, create live/prod/data-stores/mysql/vari‐\nables.tf  to expose input variables for the database username and password (so you can\npass these secrets in as environment variables, as discussed in Chapter 6 ):\nvariable  ""db_username""  {\n  description  = ""The username for the database""\n  type        = string\n  sensitive    = true\n}\nvariable  ""db_password""  {\n  description  = ""The password for the database""\n  type        = string\n  sensitive    = true\n}\nNext, create live/prod/data-stores/mysql/main.tf , and use the mysql  module to config‐\nure the primary as follows:\nmodule ""mysql_primary""  {\n  source  = ""../../../../modules/data-stores/mysql""\n  db_name      = ""prod_db""\n  db_username  = var.db_username\n  db_password  = var.db_password\n  # Must be enabled to support replication\n  backup_retention_period  = 1\n}\nNow, add a second usage of the mysql  module to create a replica:\nmodule ""mysql_replica""  {\n  source  = ""../../../../modules/data-stores/mysql""\n  # Make this a replica of the primary\n  replicate_source_db  = module.mysql_primary .arn\n}\nNice and short! All you’re doing is passing the ARN of the primary database into the\nreplicate_source_db  parameter, which should spin up an RDS database as a replica.\nThere’s just one problem: How do you tell the code to deploy the primary and replica\ninto different regions? To do so, create two provider  blocks, each with its own alias:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""primary""\n}\nWorking with Multiple Copies of the Same Provider | 233\nprovider  ""aws"" {\n  region  = ""us-west-1""\n  alias  = ""replica""\n}\nTo tell a module which providers to use, you set the providers  parameter. Here’s\nhow you configure the MySQL primary to use the primary  provider (the one in\nus-east-2 ):\nmodule ""mysql_primary""  {\n  source  = ""../../../../modules/data-stores/mysql""\n  providers  = {\n    aws = aws.primary\n  }\n  db_name      = ""prod_db""\n  db_username  = var.db_username\n  db_password  = var.db_password\n  # Must be enabled to support replication\n  backup_retention_period  = 1\n}\nAnd here is how you configure the MySQL replica to use the replica  provider (the\none in us-west-1 ):\nmodule ""mysql_replica""  {\n  source  = ""../../../../modules/data-stores/mysql""\n  providers  = {\n    aws = aws.replica\n  }\n  # Make this a replica of the primary\n  replicate_source_db  = module.mysql_primary .arn\n}\nNotice that with modules, the providers  (plural) parameter is a map, whereas with\nresources and data sources, the provider  (singular) parameter is a single value.\nThat’s because each resource and data source deploys into exactly one provider, but a\nmodule may contain multiple data sources and resources and use multiple providers\n(you’ll see an example of multiple providers in a module later). In the providers\nmap you pass to a module, the key must match the local name of the provider in the\nrequired_providers  map within the module (in this case, both are set to aws). This\nis yet another reason defining required_providers  explicitly is a good idea in just\nabout every module.\n234 | Chapter 7: Working with Multiple Providers\nAlright, the last step is to create live/prod/data-stores/mysql/outputs.tf  with the follow‐\ning output variables:\noutput ""primary_address""  {\n  value       = module.mysql_primary .address\n  description  = ""Connect to the primary database at this endpoint""\n}\noutput ""primary_port""  {\n  value       = module.mysql_primary .port\n  description  = ""The port the primary database is listening on""\n}\noutput ""primary_arn""  {\n  value       = module.mysql_primary .arn\n  description  = ""The ARN of the primary database""\n}\noutput ""replica_address""  {\n  value       = module.mysql_replica .address\n  description  = ""Connect to the replica database at this endpoint""\n}\noutput ""replica_port""  {\n  value       = module.mysql_replica .port\n  description  = ""The port the replica database is listening on""\n}\noutput ""replica_arn""  {\n  value       = module.mysql_replica .arn\n  description  = ""The ARN of the replica database""\n}\nAnd now you’re finally ready to deploy! Note that running apply  to spin up a\nprimary and replica can take a long time, some 20–30 minutes, so be patient:\n$ terraform apply\n(...)\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\nOutputs:\nprimary_address = ""terraform-up-and-running.cmyd6qwb.us-east-2.rds.amazonaws.com""\nprimary_arn     = ""arn:aws:rds:us-east-2:111111111111:db:terraform-up-and-running""\nprimary_port    = 3306\nreplica_address = ""terraform-up-and-running.drctpdoe.us-west-1.rds.amazonaws.com""\nreplica_arn     = ""arn:aws:rds:us-west-1:111111111111:db:terraform-up-and-running""\nreplica_port    = 3306\nWorking with Multiple Copies of the Same Provider | 235\nAnd there you have it, cross-region replication! Y ou can log into the RDS Console  to\nconfirm replication is working. As shown in Figure 7-2 , you should see a primary in\nus-east-2  and a replica in us-west-1 .\nFigure 7-2. The RDS console shows a primary database in us-east-2  and a replica in\nus-west-1 .\nAs an exercise for the reader, I leave it up to you to update the staging environment\n(stage/data-stores/mysql ) to use your mysql  module ( modules/data-stores/mysql ) as\nwell, but to configure it without  replication, as you don’t usually need that level of\navailability in pre-production environments.\nAs you can see in these examples, by using multiple providers with aliases, deploying\nresources across multiple regions with Terraform is pretty easy. However, I want to\ngive two warnings before moving on:\nWarning 1: Multiregion is hard\nTo run infrastructure in multiple regions around the world, especially in “active-\nactive” mode, where more than one region is actively responding to user requests\nat the same time (as opposed to one region being a standby), there are many\nhard problems to solve, such as dealing with latency between regions, deciding\nbetween one writer (which means you have lower availability and higher latency)\nor multiple writers (which means you have either eventual consistency or shard‐\ning), figuring out how to generate unique IDs (the standard auto increment ID in\nmost databases no longer suffices), working to meet local data regulations, and so\non. These challenges are all beyond the scope of the book, but I figured I’ d at least\n236 | Chapter 7: Working with Multiple Providers\nmention them to make it clear that multiregion deployments in the real world are\nnot just a matter of tossing a few provider aliases into your Terraform code!\nWarning 2: Use aliases sparingly\nAlthough it’s easy to use aliases with Terraform, I would caution against using\nthem too often, especially  when setting up multiregion infrastructure. One of the\nmain reasons to set up multiregion infrastructure is so you can be resilient to\nthe outage of one region: e.g., if us-east-2  goes down, your infrastructure in\nus-west-1  can keep running. But if you use a single Terraform module that uses\naliases to deploy into both regions, then when one of those regions is down, the\nmodule will not be able to connect to that region, and any attempt to run plan  or\napply  will fail. So right when you need to roll out changes—when there’s a major\noutage—your Terraform code will stop working.\nMore generally, as discussed in Chapter 3 , you should keep environments com‐\npletely isolated: so instead of managing multiple regions in one module with\naliases, you manage each region in separate modules. That way, you minimize\nthe blast radius, both from your own mistakes (e.g., if you accidentally break\nsomething in one region, it’s less likely to affect the other) and from problems in\nthe world itself (e.g., an outage in one region is less likely to affect the other).\nSo when does it make sense to use aliases? Typically, aliases are a good fit when the\ninfrastructure you’re deploying across several aliased providers is truly coupled and\nyou want to always deploy it together. For example, if you wanted to use Amazon\nCloudFront as a CDN (Content Distribution Network), and to provision a TLS\ncertificate for it using AWS Certification Manager (ACM), then AWS requires the\ncertificate to be created in the us-east-1  region, no matter what other regions you\nhappen to be using for CloudFront itself. In that case, your code may have two\nprovider  blocks, one for the primary region you want to use for CloudFront and\none with an alias  hardcoded specifically to us-east-1  for configuring the TLS\ncertificate. Another use case for aliases is if you’re deploying resources designed for\nuse across many regions: for example, AWS recommends deploying GuardDuty, an\nautomated threat detection service, in every single region you’re using in your AWS\naccount. In this case, it may make sense to have a module with a provider  block and\ncustom alias  for each AWS region.\nBeyond a few corner cases like this, using aliases to handle multiple regions is rela‐\ntively rare. A more common use case for aliases is when you have multiple providers\nthat need to authenticate in different ways, such as each one authenticating to a\ndifferent AWS account.\nWorking with Multiple Copies of the Same Provider | 237",21802
73-Working with Multiple AWS Accounts.pdf,73-Working with Multiple AWS Accounts,"Working with Multiple AWS Accounts\nSo far, throughout this book, you’ve likely been using a single AWS account for all\nof your infrastructure. For production code, it’s more common to use multiple AWS\naccounts: e.g., you put your staging environment in a stage account, your production\nenvironment in a prod account, and so on. This concept applies to other clouds\ntoo, such as Azure and Google Cloud. Note that I’ll be using the term account  in\nthis book, even though some clouds use slightly different terminology for the same\nconcept (e.g., Google Cloud calls them projects  instead of accounts).\nThe main reasons for using multiple accounts are as follows:\nIsolation (aka compartmentalization)\nY ou use separate accounts to isolate different environments from each other and\nto limit the “blast radius” when things go wrong. For example, putting your\nstaging and production environments in separate accounts ensures that if an\nattacker manages to break into staging, they still have no access whatsoever to\nproduction. Likewise, this isolation ensures that a developer making changes in\nstaging is less likely to accidentally break something in production.\nAuthentication and authorization\nIf everything is in one account, it’s tricky to grant access to some things (e.g.,\nthe staging environment) but not accidentally grant access to other things (e.g.,\nthe production environment). Using multiple accounts makes it easier to have\nfine-grained control, as any permissions you grant in one account have no effect\non any other account.\nThe authentication requirements of multiple accounts also help reduce the\nchance of mistakes. With everything in a single account, it’s too easy to make\nthe mistake where you think you’re making a change in, say, your staging envi‐\nronment, but you’re actually making the change in production (which can be a\ndisaster if the change you’re making is, for example, to drop all database tables).\nWith multiple accounts, this is less likely, as authenticating to each account\nrequires a separate set of steps.\nNote that having multiple accounts does not imply that developers have multiple\nseparate user profiles (e.g., a separate IAM user in each AWS account). In fact,\nthat would be an antipattern, as that would require managing multiple sets of\ncredentials, permissions, etc. Instead, you can configure just about all the major\nclouds so that each developer has exactly one user profile, which they can use to\nauthenticate to any account they have access to. The cross-account authentication\nmechanism varies depending on the cloud you’re using: e.g., in AWS, you can\nauthenticate across AWS accounts by assuming IAM roles, as you’ll see shortly.\n238 | Chapter 7: Working with Multiple Providers\nAuditing and reporting\nA properly configured account structure will allow you to maintain an audit trail\nof all the changes happening in all your environments, check if you’re adhering to\ncompliance requirements, and detect anomalies. Moreover, you’ll be able to have\nconsolidated billing, with all the charges for all of your accounts in one place,\nincluding cost breakdowns by account, service, tag, etc. This is especially useful\nin large organizations, as it allows finance to track and budget spending by team\nsimply by looking at which account the charges are coming from.\nLet’s go through a multi-account example with AWS. First, you’ll want to create a new\nAWS account to use for testing. Since you already have one AWS account, to create\nnew child accounts , you can use AWS Organizations, which ensures that the billing\nfrom all the child accounts rolls up into the parent account (sometimes called the root\naccount ) and gives you a dashboard you can use to manage all the child accounts.\nHead over to the AWS Organizations Console , and click the “ Add an AWS account”\nbutton, as shown in Figure 7-3 .\nFigure 7-3. Use AWS Organizations to create a new AWS account.\nOn the next page, fill in the following info, as shown in Figure 7-4 :\nWorking with Multiple Copies of the Same Provider | 239\nAWS account name\nThe name to use for the account. For example, if this account was going to be\nused for your staging environment, you might name it “staging. ”\nEmail address of the account’s owner\nThe email address to use for the root user of the AWS account. Note that every\nAWS account must use a different email address for the root user, so you can’t\nreuse the email address you used to create your first (root) AWS account (see\n“How to Get Multiple Aliases from One Email Address”  on page 241 for a\nworkaround). So what about the root user’s password? By default, AWS does not\nconfigure a password for the root user of a new child account (you’ll see shortly\nan alternative way to authenticate to the child account). If you ever do want to log\nin as this root user, after you create the child account, you’ll need to go through\nthe password reset flow with the email address you’re specifying here.\nIAM role name\nWhen AWS Organizations creates a child AWS account, it automatically creates\nan IAM role within that child AWS account that has admin permissions and\ncan be assumed from the parent account. This is convenient, as it allows you to\nauthenticate to the child AWS account without having to create any IAM users or\nIAM roles yourself. I recommend leaving this IAM role name at the default value\nof OrganizationAccountAccessRole .\nFigure 7-4. Fill in the details for the new AWS account.\n240 | Chapter 7: Working with Multiple Providers\nHow to Get Multiple Aliases from One Email Address\nIf you use Gmail, you can get multiple email aliases out of a single address by taking\nadvantage of the fact that Gmail ignores everything after a plus sign in an email\naddress. For example, if your Gmail address is example@gmail.com , you can send\nemail to example+foo@gmail.com  and example+any-text-you-want@gmail.com , and all\nof those emails will go to example@gmail.com . This also works if your company uses\nGmail via Google Workspace, even with a custom domain: e.g., example+dev@com‐\npany.com  and example+stage@company.com  will all go to example@company.com .\nThis is useful if you’re creating a dozen child AWS accounts, as instead of having\nto create a dozen totally separate email addresses, you could use example+dev@com‐\npany.com  for your dev account, example+stage@company.com  for your stage account,\nand so on; AWS will see each of those email addresses as a different, unique address,\nbut under the hood, all the emails will go to the same account.\nClick the Create AWS Account button, wait a few minutes for AWS to create the\naccount, and then jot down the 12-digit ID of the AWS account that gets created. For\nthe rest of this chapter, let’s assume the following:\n•Parent AWS account ID: 111111111111 •\n•Child AWS account ID: 222222222222 •\nY ou can authenticate to your new child account from the AWS Console by clicking\nyour username and selecting “Switch role” , as shown in Figure 7-5 .\nFigure 7-5. Select the “Switch role” button.\nWorking with Multiple Copies of the Same Provider | 241\nNext, enter the details for the IAM role you want to assume, as shown in Figure 7-6 :\nAccount\nThe 12-digit ID of the AWS account to switch to. Y ou’ll want to enter the ID of\nyour new child account.\nRole\nThe name of the IAM role to assume in that AWS account. Enter the name you\nused for the IAM role when creating the new child account, which is Organiza\ntionAccountAccessRole  by default.\nDisplay name\nAWS will create a shortcut in the nav to allow you to switch to this account in the\nfuture with a single click. This is the name to show in this shortcut. It only affects\nyour IAM user in this browser.\nFigure 7-6. Enter the details for the role to switch to.\nClick Switch Role and, voilà, AWS should log you into the web console of the new\nAWS account!\nLet’s now write an example Terraform module in examples/multi-account-root  that\ncan authenticate to multiple AWS accounts. Just as with the multiregion AWS exam‐\nple, you will need to add two provider  blocks in main.tf , each with a different alias.\nFirst, the provider  block for the parent AWS account:\n242 | Chapter 7: Working with Multiple Providers\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""parent""\n}\nNext, the provider  block for the child AWS account:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""child""\n}\nTo be able to authenticate to the child AWS account, you’ll assume an IAM role. In\nthe web console, you did this by clicking the Switch Role button; in your Terraform\ncode, you do this by adding an assume_role  block to the child provider  block:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""child""\n  assume_role  {\n    role_arn  = ""arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME>""\n  }\n}\nIn the role_arn  parameter, you’ll need to replace ACCOUNT_ID  with your child\naccount ID and ROLE_NAME  with the name of the IAM role in that account, just as\nyou did when switching roles in the web console. Here’s what it looks like with the\naccount ID 222222222222  and role name OrganizationAccountAccessRole  plugged\nin:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""child""\n  assume_role  {\n    role_arn  = ""arn:aws:iam::222222222222:role/OrganizationAccountAccessRole""\n  }\n}\nNow, to check this is actually working, add two aws_caller_identity  data sources,\nand configure each one to use a different provider:\ndata ""aws_caller_identity"" ""parent""  {\n  provider  = aws.parent\n}\ndata ""aws_caller_identity"" ""child""  {\n  provider  = aws.child\n}\nWorking with Multiple Copies of the Same Provider | 243\nFinally, add output variables in outputs.tf  to print out the account IDs:\noutput ""parent_account_id""  {\n  value       = data.aws_caller_identity.parent.account_id\n  description  = ""The ID of the parent AWS account""\n}\noutput ""child_account_id""  {\n  value       = data.aws_caller_identity.child.account_id\n  description  = ""The ID of the child AWS account""\n}\nRun apply , and you should see the different IDs for each account:\n$ terraform apply\n(...)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\nparent_account_id = ""111111111111""\nchild_account_id = ""222222222222""\nAnd there you have it: by using provider aliases and assume_role  blocks, you now\nknow how to write Terraform code that can operate across multiple AWS accounts.\nAs with the multiregion section, a few warnings:\nWarning 1: Cross-account IAM roles are double opt-in\nIn order for an IAM role to allow access from one AWS account to another—e.g.,\nto allow an IAM role in account 222222222222  to be assumed from account\n111111111111 —you need to grant permissions in both  AWS accounts:\n•First, in the AWS account where the IAM role lives (e.g., the child account•\n222222222222 ), you must configure its assume role policy to trust the other\nAWS account (e.g., the parent account 111111111111 ). This happened magi‐\ncally for you with the OrganizationAccountAccessRole  IAM role because\nAWS Organizations automatically configures the assume role policy of this\nIAM role to trust the parent account. However, for any custom IAM roles\nyou create, you need to remember to explicitly grant the sts:AssumeRole\npermission yourself.\n•Second, in the AWS account from which you assume the role (e.g., the•\nparent account 111111111111 ), you must also grant your user permissions\nto assume that IAM role. Again, this happened for you magically because,\nin Chapter 2 , you gave your IAM user AdministratorAccess , which gives\nyou permissions to do just about everything in the parent AWS account,\nincluding assuming IAM roles. In most real-world use cases, your user won’t\n244 | Chapter 7: Working with Multiple Providers",11841
74-Creating Modules That Can Work with Multiple Providers.pdf,74-Creating Modules That Can Work with Multiple Providers,"be (shouldn’t be!) an admin, so you’ll need to explicitly grant your user\nsts:AssumeRole  permissions on the IAM role(s) you want to be able to\nassume.\nWarning 2: Use aliases sparingly\nI said this in the multiregion example, but it bears repeating: although it’s easy\nto use aliases with Terraform, I would caution against using them too often,\nincluding with multi-account code. Typically, you use multiple accounts to create\nseparation between them, so if something goes wrong in one account, it doesn’t\naffect the other. Modules that deploy across multiple accounts go against this\nprinciple. Only do it when you intentionally  want to have resources in multiple\naccounts coupled and deployed together.\nCreating Modules That Can Work with Multiple Providers\nWhen working with Terraform modules, you typically work with two types of\nmodules:\nReusable modules\nThese are low-level modules that are not meant to be deployed directly but\ninstead are to be combined with other modules, resources, and data sources.\nRoot modules\nThese are high-level modules that combine multiple reusable modules into a\nsingle unit that is meant to be deployed directly by running apply  (in fact, the\ndefinition of a root module is it’s the one on which you run apply ).\nThe multiprovider examples you’ve seen so far have put all the provider  blocks into\nthe root module. What do you do if you want to create a reusable module that works\nwith multiple providers? For example, what if you wanted to turn the multi-account\ncode from the previous section into a reusable module? As a first step, you might\nput all that code, unchanged, into the modules/multi-account  folder. Then, you could\ncreate a new example to test it with in the examples/multi-account-module  folder, with\na main.tf  that looks like this:\nmodule ""multi_account_example""  {\n  source  = ""../../modules/multi-account""\n}\nIf you run apply  on this code, it’ll work, but there is a problem: all of the provider\nconfiguration is now hidden in the module itself (in modules/multi-account ). Defin‐\ning provider  blocks within reusable modules is an antipattern for several reasons:\nConfiguration  problems\nIf you have provider  blocks defined in your reusable module, then that module\ncontrols all the configuration for that provider . For example, the IAM role ARN\nWorking with Multiple Copies of the Same Provider | 245\nand regions to use are currently hardcoded in the modules/multi-account  module.\nY ou could, of course, expose input variables to allow users to set the regions\nand IAM role ARNs, but that’s only the tip of the iceberg. If you browse the\nAWS Provider documentation, you’ll find that there are roughly 50 different\nconfiguration options you can pass into it! Many of these parameters are going\nto be important for users of your module, as they control how to authenticate to\nAWS, what region to use, what account (or IAM role) to use, what endpoints to\nuse when talking to AWS, what tags to apply or ignore, and much more. Having\nto expose 50 extra variables in a module will make that module very cumbersome\nto maintain and use.\nDuplication problems\nEven if you expose those 50 settings in your module, or whatever subset you\nbelieve is important, you’re creating code duplication for users of your module.\nThat’s because it’s common to combine multiple modules together, and if you\nhave to pass in some subset of 50 settings into each of those modules in order to\nget them to all authenticate correctly, you’re going to have to copy and paste a lot\nof parameters, which is tedious and error prone.\nPerformance problems\nEvery time you include a provider  block in your code, Terraform spins up a new\nprocess to run that provider, and communicates with that process via RPC. If you\nhave a handful of provider  blocks, this works just fine, but as you scale up, it\nmay cause performance problems. Here’s a real-world example: a few years ago,\nI created reusable modules for CloudTrail, AWS Config, GuardDuty, IAM Access\nAnalyzer, and Macie. Each of these AWS services is supposed to be deployed into\nevery region in your AWS account, and as AWS had ~25 regions, I included 25\nprovider  blocks in each of these modules. I then created a single root module to\ndeploy all of these as a “baseline” in my AWS accounts: if you do the math, that’s\n5 modules with 25 provider  blocks each, or 125 provider  blocks total. When I\nran apply , Terraform would fire up 125 processes, each making hundreds of API\nand RPC calls. With thousands of concurrent network requests, my CPU would\nstart thrashing, and a single plan  could take 20 minutes. Worse yet, this would\nsometimes overload the network stack, leading to intermittent failures in API\ncalls, and apply  would fail with sporadic errors.\nTherefore, as a best practice, you should not define any provider  blocks in your\nreusable modules and instead allow your users to create the provider  blocks they\nneed solely in their root modules. But then, how do you build a module that can work\nwith multiple providers? If the module has no provider  blocks in it, how do you\ndefine provider aliases that you can reference in your resources and data sources?\n246 | Chapter 7: Working with Multiple Providers\nThe solution is to use configuration  aliases . These are very similar to the provider\naliases you’ve seen already, except they aren’t defined in a provider  block. Instead,\nyou define them in a required_providers  block.\nOpen up modules/multi-account/main.tf , remove the nested provider  blocks, and\nreplace them with a required_providers  block with configuration aliases as follows:\nterraform  {\n  required_providers  {\n    aws = {\n      source                 = ""hashicorp/aws""\n      version                = ""~> 4.0""\n      configuration_aliases  = [aws.parent , aws.child ]\n    }\n  }\n}\nJust as with normal provider aliases, you can pass configuration aliases into resources\nand data sources using the provider  parameter:\ndata ""aws_caller_identity"" ""parent""  {\n  provider  = aws.parent\n}\ndata ""aws_caller_identity"" ""child""  {\n  provider  = aws.child\n}\nThe key difference from normal provider aliases is that configuration aliases don’t\ncreate any providers themselves; instead, they force users of your module to explicitly\npass in a provider for each of your configuration aliases using a providers  map.\nOpen up examples/multi-account-module/main.tf , and define the provider  blocks as\nbefore:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""parent""\n}\nprovider  ""aws"" {\n  region  = ""us-east-2""\n  alias  = ""child""\n  assume_role  {\n    role_arn  = ""arn:aws:iam::222222222222:role/OrganizationAccountAccessRole""\n  }\n}\nAnd now you can pass them into the modules/multi-account  module as follows:\nmodule ""multi_account_example""  {\n  source  = ""../../modules/multi-account""\nWorking with Multiple Copies of the Same Provider | 247",6967
75-A Crash Course on Docker.pdf,75-A Crash Course on Docker,"2See “Multi-Cloud is the Worst Practice” .  providers  = {\n    aws.parent  = aws.parent\n    aws.child   = aws.child\n  }\n}\nThe keys in the providers  map must match the names of the configuration aliases\nwithin the module; if any of the names from configuration aliases are missing in\nthe providers  map, Terraform will show an error. This way, when you’re building a\nreusable module, you can define what providers that module needs, and Terraform\nwill ensure users pass those providers in; and when you’re building a root module,\nyou can define your provider  blocks just once and pass around references to them to\nthe reusable modules you depend on.\nWorking with Multiple Different  Providers\nY ou’ve now seen how to work with multiple providers when all of them are the same\ntype of provider: e.g., multiple copies of the aws provider. This section talks about\nhow to work with multiple different providers.\nReaders of the first two editions of this book often asked for examples of using\nmultiple clouds together ( multicloud ), but I couldn’t find much useful to share. In\npart, this is because using multiple clouds is usually a bad practice,2 but even if you’re\nforced to do it (most large companies are multicloud, whether they want to be or\nnot), it’s rare to manage multiple clouds in a single module for the same reason\nit’s rare to manage multiple regions or accounts in a single module. If you’re using\nmultiple clouds, you’re far better off managing each one in a separate module.\nMoreover, translating every single AWS example in the book into the equivalent\nsolutions for other clouds (Azure and Google Cloud) is impractical: the book would\nend up way too long, and while you would learn more about each cloud, you wouldn’t\nlearn any new Terraform concepts along the way, which is the real goal of the book.\nIf you do want to see examples of what the Terraform code for similar infrastructure\nlooks like across different clouds, have a look at the examples  folder in the Terratest\nrepo . As you’ll see in Chapter 9 , Terratest provides a set of tools for writing automa‐\nted tests for different types of infrastructure code and different types of clouds, so\nin the examples  folder you’ll find Terraform code for similar infrastructure in AWS,\nGoogle Cloud, and Azure, including individual servers, groups of servers, databases,\nand more. Y ou’ll also find automated tests for all those examples in the test folder.\nIn this book, instead of an unrealistic multicloud example, I decided to instead show\nyou how to use multiple providers together in a slightly more realistic scenario (one\n248 | Chapter 7: Working with Multiple Providers\nthat was also requested by many readers of the first two editions): namely, how\nto use the AWS Provider with the Kubernetes provider to deploy Dockerized apps.\nKubernetes is, in many ways, a cloud of its own—it can run applications, networks,\ndata stores, load balancers, secret stores, and much more—so, in a sense, this is both\na multiprovider and multicloud example. And because Kubernetes is a cloud, that\nmeans there is a lot to learn, so I’m going to have to build up to it one step at a time,\nstarting with mini crash courses on Docker and Kubernetes, before finally moving on\nto the full multiprovider example that uses both AWS and Kubernetes:\n•A crash course on Docker•\n•A crash course on Kubernetes•\n•Deploying Docker containers in AWS using Elastic Kubernetes Service (EKS)•\nA Crash Course on Docker\nAs you may remember from Chapter 1 , Docker images are like self-contained “snap‐\nshots” of the operating system (OS), the software, the files, and all other relevant\ndetails. Let’s now see Docker in action.\nFirst, if you don’t have Docker installed already, follow the instructions on the Docker\nwebsite  to install Docker Desktop for your operating system. Once it’s installed, you\nshould have the docker  command available on your command line. Y ou can use the\ndocker run  command to run Docker images locally:\n$ docker run <IMAGE> [COMMAND]\nwhere IMAGE  is the Docker image to run and COMMAND  is an optional command\nto execute. For example, here’s how you can run a Bash shell in an Ubuntu 20.04\nDocker image (note that the following command includes the -it flag so you get an\ninteractive shell where you can type):\n$ docker run -it ubuntu:20.04 bash\nUnable to find image 'ubuntu:20.04' locally\n20.04: Pulling from library/ubuntu\nDigest: sha256:669e010b58baf5beb2836b253c1fd5768333f0d1dbcb834f7c07a4dc93f474be\nStatus: Downloaded newer image for ubuntu:20.04\nroot@d96ad3779966:/#\nAnd voilà, you’re now in Ubuntu! If you’ve never used Docker before, this can\nseem fairly magical. Try running some commands. For example, you can look at the\ncontents of /etc/os-release  to verify you really are in Ubuntu:\nroot@d96ad3779966:/# cat /etc/os-release\nNAME=""Ubuntu""\nVERSION=""20.04.3 LTS (Focal Fossa)""\nWorking with Multiple Different  Providers | 249\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=""Ubuntu 20.04.3 LTS""\nVERSION_ID=""20.04""\nVERSION_CODENAME=focal\nHow did this happen? Well, first, Docker searches your local filesystem for the\nubuntu:20.04  image. If you don’t have that image downloaded already, Docker\ndownloads it automatically from Docker Hub, which is a Docker Registry  that con‐\ntains shared Docker images. The ubuntu:20.04  image happens to be a public Docker\nimage—an official one maintained by the Docker team—so you’re able to download\nit without any authentication. However, it’s also possible to create private Docker\nimages that only certain authenticated users can use.\nOnce the image is downloaded, Docker runs the image, executing the bash  com‐\nmand, which starts an interactive Bash prompt, where you can type. Try running the\nls command to see the list of files:\nroot@d96ad3779966:/# ls -al\ntotal 56\ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 .\ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 ..\nlrwxrwxrwx   1 root root    7 Jan 13 16:59 bin -> usr/bin\ndrwxr-xr-x   2 root root 4096 Apr 15  2020 boot\ndrwxr-xr-x   5 root root  360 Feb 22 14:22 dev\ndrwxr-xr-x   1 root root 4096 Feb 22 14:22 etc\ndrwxr-xr-x   2 root root 4096 Apr 15  2020 home\nlrwxrwxrwx   1 root root    7 Jan 13 16:59 lib -> usr/lib\ndrwxr-xr-x   2 root root 4096 Jan 13 16:59 media\n(...)\nY ou might notice that’s not your filesystem. That’s because Docker images run in\ncontainers that are isolated at the userspace level: when you’re in a container, you can\nonly see the filesystem, memory, networking, etc., in that container. Any data in other\ncontainers, or on the underlying host operating system, is not accessible to you, and\nany data in your container is not visible to those other containers or the underlying\nhost operating system. This is one of the things that makes Docker useful for running\napplications: the image format is self-contained, so Docker images run the same way\nno matter where you run them, and no matter what else is running there.\nTo see this in action, write some text to a test.txt  file as follows:\nroot@d96ad3779966:/# echo ""Hello, World!"" > test.txt\n250 | Chapter 7: Working with Multiple Providers\nNext, exit the container by hitting Ctrl-D on Windows and Linux or Cmd-D on\nmacOS, and you should be back in your original command prompt on your underly‐\ning host OS. If you try to look for the test.txt  file you just wrote, you’ll see that it\ndoesn’t exist: the container’s filesystem is totally isolated from your host OS.\nNow, try running the same Docker image again:\n$ docker run -it ubuntu:20.04 bash\nroot@3e0081565a5d:/#\nNotice that this time, since the ubuntu:20.04  image is already downloaded, the\ncontainer starts almost instantly. This is another reason Docker is useful for running\napplications: unlike virtual machines, containers are lightweight, boot up quickly, and\nincur little CPU or memory overhead.\nY ou may also notice that the second time you fired up the container, the command\nprompt looked different. That’s because you’re now in a totally new container; any\ndata you wrote in the previous one is no longer accessible to you. Run ls -al  and\nyou’ll see that the test.txt  file does not exist. Containers are isolated not only from the\nhost OS but also from each other.\nHit Ctrl-D or Cmd-D again to exit the container, and back on your host OS, run the\ndocker ps -a  command:\n$ docker ps -a\nCONTAINER ID   IMAGE            COMMAND    CREATED          STATUS\n3e0081565a5d   ubuntu:20.04     ""bash""     5 min ago    Exited (0) 16 sec ago\nd96ad3779966   ubuntu:20.04     ""bash""     14 min ago   Exited (0) 5 min ago\nThis will show you all the containers on your system, including the stopped ones (the\nones you exited). Y ou can start a stopped container again by using the docker start\n<ID>  command, setting ID to an ID from the CONTAINER ID  column of the docker\nps output. For example, here is how you can start the first container up again (and\nattach an interactive prompt to it via the -ia flags):\n$ docker start -ia d96ad3779966\nroot@d96ad3779966:/#\nY ou can confirm this is really the first container by outputting the contents of test.txt :\nroot@d96ad3779966:/# cat test.txt\nHello, World!\nLet’s now see how a container can be used to run a web app. Hit Ctrl-D or Cmd-D\nagain to exit the container, and back on your host OS, run a new container:\n$ docker run training/webapp\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nThe training/webapp image  contains a simple Python “Hello, World” web app for\ntesting. When you run the image, it fires up the web app, listening on port 5000 by\nWorking with Multiple Different  Providers | 251",9711
76-A Crash Course on Kubernetes.pdf,76-A Crash Course on Kubernetes,"default. However, if you open a new terminal on your host operating system and try\nto access the web app, it won’t work:\n$ curl localhost:5000\ncurl: (7) Failed to connect to localhost port 5000: Connection refused\nWhat’s the problem? Actually, it’s not a problem but a feature! Docker containers are\nisolated from the host operating system and other containers, not only at the filesys‐\ntem level but also in terms of networking. So while the container really is listening on\nport 5000, that is only on a port inside  the container, which isn’t accessible on the host\nOS. If you want to expose a port from the container on the host OS, you have to do it\nvia the -p flag.\nFirst, hit Ctrl-C to shut down the training/webapp  container: note that it’s C this\ntime, not D, and it’s Ctrl regardless of OS, as you’re shutting down a process, rather\nthan exiting an interactive prompt. Now rerun the container but this time with the -p\nflag as follows:\n$ docker run -p 5000:5000 training/webapp\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\nAdding -p 5000:5000  to the command tells Docker to expose port 5000 inside the\ncontainer on port 5000 of the host OS. In another terminal on your host OS, you\nshould now be able to see the web app working:\n$ curl localhost:5000\nHello world!\nCleaning Up Containers\nEvery time you run docker run  and exit, you are leaving behind\ncontainers, which take up disk space. Y ou may wish to clean\nthem up with the docker rm <CONTAINER_ID>  command, where\nCONTAINER_ID  is the ID of the container from the docker ps  out‐\nput. Alternatively, you could include the --rm  flag in your docker\nrun command to have Docker automatically clean up when you\nexit the container.\nA Crash Course on Kubernetes\nKubernetes is an orchestration tool for Docker, which means it’s a platform for run‐\nning and managing Docker containers on your servers, including scheduling (picking\nwhich servers should run a given container workload), auto healing (automatically\nredeploying containers that failed), auto scaling (scaling the number of containers up\nand down in response to load), load balancing (distributing traffic across containers),\nand much more.\nUnder the hood, Kubernetes consists of two main pieces:\n252 | Chapter 7: Working with Multiple Providers\nControl plane\nThe control plane is responsible for managing the Kubernetes cluster. It is\nthe “brains” of the operation, responsible for storing the state of the cluster,\nmonitoring containers, and coordinating actions across the cluster. It also runs\nthe API server, which provides an API you can use from command-line tools\n(e.g., kubectl ), web UIs (e.g., the Kubernetes Dashboard), and IaC tools (e.g.,\nTerraform) to control what’s happening in the cluster.\nWorker nodes\nThe worker nodes are the servers used to actually run your containers. The\nworker nodes are entirely managed by the control plane, which tells each worker\nnode what containers it should run.\nKubernetes is open source, and one of its strengths is that you can run it anywhere:\nin any public cloud (e.g., AWS, Azure, Google Cloud), in your own datacenter, and\neven on your own developer workstation. A little later in this chapter, I’ll show you\nhow you can run Kubernetes in the cloud (in AWS), but for now, let’s start small and\nrun it locally. This is easy to do if you installed a relatively recent version of Docker\nDesktop, as it has the ability to fire up a Kubernetes cluster locally with just a few\nclicks.\nIf you open Docker Desktop’s preferences on your computer, you should see Kuber‐\nnetes in the nav, as shown in Figure 7-7 .\nFigure 7-7. Enable Kubernetes on Docker Desktop.\nWorking with Multiple Different  Providers | 253\nIf it’s not enabled already, check the Enable Kubernetes checkbox, click Apply &\nRestart, and wait a few minutes for that to complete. In the meantime, follow the\ninstructions on the Kubernetes website  to install kubectl , which is the command-line\ntool for interacting with Kubernetes.\nTo use kubectl , you must first update its configuration file, which lives in\n$HOME/.kube/config  (that is, the .kube  folder of your home directory), to tell it\nwhat Kubernetes cluster to connect to. Conveniently, when you enable Kubernetes in\nDocker Desktop, it updates this config file for you, adding a docker-desktop  entry to\nit, so all you need to do is tell kubectl  to use this configuration as follows:\n$ kubectl config use-context docker-desktop\nSwitched to context ""docker-desktop"".\nNow you can check if your Kubernetes cluster is working with the get nodes\ncommand:\n$ kubectl get nodes\nNAME             STATUS   ROLES                  AGE   VERSION\ndocker-desktop   Ready    control-plane,master   95m   v1.22.5\nThe get nodes  command shows you information about all the nodes in your cluster.\nSince you’re running Kubernetes locally, your computer is the only node, and it’s\nrunning both the control plane and acting as a worker node. Y ou’re now ready to run\nsome Docker containers!\nTo deploy something in Kubernetes, you create Kubernetes objects , which are persis‐\ntent entities you write to the Kubernetes cluster (via the API server) that record your\nintent: e.g., your intent to have specific Docker images running. The cluster runs a\nreconciliation loop , which continuously checks the objects you stored in it and works\nto make the state of the cluster match your intent.\nThere are many different types of Kubernetes objects available. For the examples in\nthis book, let’s use the following two objects:\nKubernetes Deployment\nA Kubernetes Deployment  is a declarative way to manage an application in Kuber‐\nnetes. Y ou declare what Docker images to run, how many copies of them to\nrun (called replicas ), a variety of settings for those images (e.g., CPU, memory,\nport numbers, environment variables), and the strategy to roll out updates to\nthose images, and the Kubernetes Deployment will then work to ensure that\nthe requirements you declared are always met. For example, if you specified\nyou wanted three replicas, but one of the worker nodes went down so only two\nreplicas are left, the Deployment will automatically spin up a third replica on one\nof the other worker nodes.\n254 | Chapter 7: Working with Multiple Providers\nKubernetes Service\nA Kubernetes Service  is a way to expose a web app running in Kubernetes as a\nnetworked service. For example, you can use a Kubernetes Service to configure\na load balancer that exposes a public endpoint and distributes traffic from that\nendpoint across the replicas in a Kubernetes Deployment.\nThe idiomatic way to interact with Kubernetes is to create YAML files describing\nwhat you want—e.g., one YAML file that defines the Kubernetes Deployment and\nanother one that defines the Kubernetes Service—and to use the kubectl apply\ncommand to submit those objects to the cluster. However, using raw YAML has\ndrawbacks, such as a lack of support for code reuse (e.g., variables, modules), abstrac‐\ntion (e.g., loops, if-statements), clear standards on how to store and manage the\nYAML files (e.g., to track changes to the cluster over time), and so on. Therefore,\nmany Kubernetes users turn to alternatives, such as Helm or Terraform. Since this\nis a book on Terraform, I’m going to show you how to create a Terraform module\ncalled k8s-app  (K8S is an acronym for Kubernetes in the same way that I18N is\nan acronym for internationalization) that deploys an app in Kubernetes using a\nKubernetes Deployment and Kubernetes Service.\nCreate a new module in the modules/services/k8s-app  folder. Within that folder, create\na variables.tf  file that defines the module’s API via the following input variables:\nvariable  ""name"" {\n  description  = ""The name to use for all resources created by this module""\n  type        = string\n}\nvariable  ""image"" {\n  description  = ""The Docker image to run""\n  type        = string\n}\nvariable  ""container_port""  {\n  description  = ""The port the Docker image listens on""\n  type        = number\n}\nvariable  ""replicas""  {\n  description  = ""How many replicas to run""\n  type        = number\n}\nvariable  ""environment_variables""  {\n  description  = ""Environment variables to set for the app""\n  type        = map(string)\n  default      = {}\n}\nWorking with Multiple Different  Providers | 255\nThis should give you just about all the inputs you need for creating the Kuber‐\nnetes Deployment and Service. Next, add a main.tf  file, and at the top, add the\nrequired_providers  block to it with the Kubernetes provider:\nterraform  {\n  required_version  = "">= 1.0.0, < 2.0.0""\n  required_providers  {\n    kubernetes  = {\n      source   = ""hashicorp/kubernetes""\n      version  = ""~> 2.0""\n    }\n  }\n}\nHey, a new provider, neat! OK, let’s make use of that provider to create a Kubernetes\nDeployment by using the kubernetes_deployment  resource:\nresource  ""kubernetes_deployment"" ""app""  {\n}\nThere are quite a few settings to configure within the kubernetes_deployment\nresource, so let’s go through them one at a time. First, you need to configure the\nmetadata  block:\nresource  ""kubernetes_deployment"" ""app""  {\n  metadata  {\n    name  = var.name\n  }\n}\nEvery Kubernetes object includes metadata that can be used to identify and target that\nobject in API calls. In the preceding code, I’m setting the Deployment name to the\nname  input variable.\nThe rest of the configuration for the kubernetes_deployment  resource goes into the\nspec  block:\nresource  ""kubernetes_deployment"" ""app""  {\n  metadata  {\n    name  = var.name\n  }\n  spec {\n  }\n}\nThe first item to put into the spec  block is to specify the number of replicas to create:\n  spec {\n    replicas  = var.replicas\n  }\n256 | Chapter 7: Working with Multiple Providers\nNext, define the template  block:\n  spec {\n    replicas  = var.replicas\n    template  {\n    }\n  }\nIn Kubernetes, instead of deploying one container at a time, you deploy Pods , which\nare groups of containers that are meant to be deployed together. For example, you\ncould have a Pod with one container to run a web app (e.g., the Python app you\nsaw earlier) and another container that gathers metrics on the web app and sends\nthem to a central service (e.g., Datadog). The template  block is where you define the\nPod Template , which specifies what container(s) to run, the ports to use, environment\nvariables to set, and so on.\nOne important ingredient in the Pod Template will be the labels to apply to the Pod.\nY ou’ll need to reuse these labels in several places—e.g., the Kubernetes Service uses\nlabels to identify the Pods that need to be load balanced—so let’s define those labels in\na local variable called pod_labels :\nlocals {\n  pod_labels  = {\n    app = var.name\n  }\n}\nAnd now use pod_labels  in the metadata  block of the Pod Template:\n  spec {\n    replicas  = var.replicas\n    template  {\n      metadata  {\n        labels  = local.pod_labels\n      }\n    }\n  }\nNext, add a spec  block inside of template :\n  spec {\n    replicas  = var.replicas\n    template  {\n      metadata  {\n        labels  = local.pod_labels\n      }\n      spec {\n        container  {\nWorking with Multiple Different  Providers | 257\n          name   = var.name\n          image  = var.image\n          port {\n            container_port  = var.container_port\n          }\n          dynamic ""env"" {\n            for_each  = var.environment_variables\n            content {\n              name   = env.key\n              value  = env.value\n            }\n          }\n        }\n      }\n    }\n  }\nThere’s a lot here, so let’s go through it one piece at a time:\ncontainer\nInside the spec  block, you can define one or more container  blocks to specify\nwhich Docker containers to run in this Pod. To keep this example simple, there’s\njust one container  block in the Pod. The rest of these items are all within this\ncontainer  block.\nname\nThe name to use for the container. I’ve set this to the name  input variable.\nimage\nThe Docker image to run in the container. I’ve set this to the image  input\nvariable.\nport\nThe ports to expose in the container. To keep the code simple, I’m assuming\nthe container only needs to listen on one port, set to the container_port  input\nvariable.\nenv\nThe environment variables to expose to the container. I’m using a dynamic  block\nwith for_each  (two concepts you may remember from Chapter 5 ) to set this to\nthe variables in the environment_variables  input variable.\nOK, that wraps up the Pod Template. There’s just one thing left to add to the\nkubernetes_deployment  resource—a selector  block:\n  spec {\n    replicas  = var.replicas\n258 | Chapter 7: Working with Multiple Providers\n    template  {\n      metadata  {\n        labels  = local.pod_labels\n      }\n      spec {\n        container  {\n          name   = var.name\n          image  = var.image\n          port {\n            container_port  = var.container_port\n          }\n          dynamic ""env"" {\n            for_each  = var.environment_variables\n            content {\n              name   = env.key\n              value  = env.value\n            }\n          }\n        }\n      }\n    }\n    selector  {\n      match_labels  = local.pod_labels\n    }\n  }\nThe selector  block tells the Kubernetes Deployment what to target. By setting it\nto pod_labels , you are telling it to manage deployments for the Pod Template you\njust defined. Why doesn’t the Deployment just assume that the Pod Template defined\nwithin that Deployment is the one you want to target? Well, Kubernetes tries to be an\nextremely flexible and decoupled system: e.g., it’s possible to define a Deployment for\nPods that are defined separately, so you always need to specify a selector  to tell the\nDeployment what to target.\nThat wraps up the kubernetes_deployment  resource. The next step is to use the\nkubernetes_service  resource to create a Kubernetes Service:\nresource  ""kubernetes_service"" ""app""  {\n  metadata  {\n    name  = var.name\n  }\n  spec {\n    type  = ""LoadBalancer""\n    port {\n      port         = 80\nWorking with Multiple Different  Providers | 259\n      target_port  = var.container_port\n      protocol     = ""TCP""\n    }\n    selector  = local.pod_labels\n  }\n}\nLet’s go through these parameters:\nmetadata\nJust as with the Deployment object, the Service object uses metadata to identify\nand target that object in API calls. In the preceding code, I’ve set the Service\nname to the name  input variable.\ntype\nI’ve configured this Service as type LoadBalancer , which, depending on how\nyour Kubernetes cluster is configured, will deploy a different type of load bal‐\nancer: e.g., in AWS, with EKS, you might get an Elastic Load Balancer, whereas in\nGoogle Cloud, with GKE, you might get a Cloud Load Balancer.\nport\nI’m configuring the load balancer to route traffic on port 80 (the default port for\nHTTP) to the port the container is listening on.\nselector\nJust as with the Deployment object, the Service object uses a selector to specify\nwhat that Service should be targeting. By setting the selector to pod_labels , the\nService and the Deployment will both operate on the same Pods.\nThe final step is to expose the Service endpoint (the load balancer hostname) as an\noutput variable in outputs.tf :\nlocals {\n  status  = kubernetes_service.app.status\n}\noutput ""service_endpoint""  {\n  value = try(\n    ""http://${local.status[0][""load_balancer""][0][""ingress""][0][""hostname""]}"" ,\n    ""(error parsing hostname from status)""\n  )\n  description  = ""The K8S Service endpoint""\n}\nThis convoluted code needs a bit of explanation. The kubernetes_service  resource\nhas an output attribute called status  that returns the latest status of the Service.\nI’ve stored this attribute in a local variable called status . For a Service of type\nLoadBalancer, status  will contain a complicated object that looks something like\nthis:\n260 | Chapter 7: Working with Multiple Providers\n[\n  {\n    load_balancer  = [\n      {\n        ingress  = [\n          {\n            hostname  = ""<HOSTNAME>""\n          }\n        ]\n      }\n    ]\n  }\n]\nBuried within this deeply nested object is the hostname for the load balancer that you\nwant. This is why the service_endpoint  output variable needs to use a complicated\nsequence of array lookups (e.g., [0]) and map lookups (e.g., [""load_balancer""] )\nto extract the hostname. But what happens if the status  attribute returned by the\nkubernetes_service  resource happens to look a little different? In that case, any of\nthose array and map lookups could fail, leading to a confusing error.\nTo handle this error gracefully, I’ve wrapped the entire expression in a function called\ntry. The try function has the following syntax:\ntry(ARG1, ARG2, ..., ARGN)\nThis function evaluates all the arguments you pass to it and returns the first argument\nthat doesn’t produce any errors. Therefore, the service_endpoint  output variable\nwill either end up with a hostname in it (the first argument) or, if reading the\nhostname caused an error, the variable will instead say “error parsing hostname from\nstatus” (the second argument).\nOK, that wraps up the k8s-app  module. To use it, add a new example in examples/\nkubernetes-local , and create a main.tf  file in it with the following contents:\nmodule ""simple_webapp""  {\n  source  = ""../../modules/services/k8s-app""\n  name           = ""simple-webapp""\n  image          = ""training/webapp""\n  replicas        = 2\n  container_port  = 5000\n}\nThis configures the module to deploy the training/webapp  Docker image you ran\nearlier, with two replicas listening on port 5000, and to name all the Kubernetes\nobjects (based on their metadata ) “simple-webapp” . To have this module deploy into\nyour local Kubernetes cluster, add the following provider  block:\nprovider  ""kubernetes""  {\n  config_path     = ""~/.kube/config""\nWorking with Multiple Different  Providers | 261\n  config_context  = ""docker-desktop""\n}\nThis code tells the Kubernetes provider to authenticate to your local Kubernetes clus‐\nter by using the docker-desktop  context from your kubectl  config. Run terraform\napply  to see how it works:\n$ terraform apply\n(...)\nApply complete! Resources: 2 added, 0 changed, 0 destroyed.\nOutputs:\nservice_endpoint = ""http://localhost""\nGive the app a few seconds to boot and then try out that service_endpoint :\n$ curl http://localhost\nHello world!\nSuccess!\nThat  said, this looks nearly identical to the output of the docker run  command, so\nwas all that extra work worth it? Well, let’s look under the hood to see what’s going\non. Y ou can use kubectl  to explore your cluster. First, run the get deployments\ncommand:\n$ kubectl get deployments\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\nsimple-webapp   2/2     2            2           3m21s\nY ou can see your Kubernetes Deployment, named simple-webapp , as that was the\nname in the metadata  block. This Deployment is reporting that 2/2 Pods (the two\nreplicas) are ready. To see those Pods, run the get pods  command:\n$ kubectl get pods\nNAME                            READY   STATUS    RESTARTS   AGE\nsimple-webapp-d45b496fd-7d447   1/1     Running   0          2m36s\nsimple-webapp-d45b496fd-vl6j7   1/1     Running   0          2m36s\nSo that’s one difference from docker run  already: there are multiple containers run‐\nning here, not just one. Moreover, those containers are being actively monitored and\nmanaged. For example, if one crashed, a replacement will be deployed automatically.\nY ou can see this in action by running the docker ps  command:\n$ docker ps\nCONTAINER ID   IMAGE             COMMAND           CREATED          STATUS\nb60f5147954a   training/webapp   ""python app.py""   3 seconds ago    Up 2 seconds\nc350ec648185   training/webapp   ""python app.py""   12 minutes ago   Up 12 minutes\n262 | Chapter 7: Working with Multiple Providers\nGrab the CONTAINER ID  of one of those containers, and use the docker kill  com‐\nmand to shut it down:\n$ docker kill b60f5147954a\nIf you run docker ps  again very quickly, you’ll see just one container left running:\n$ docker ps\nCONTAINER ID   IMAGE             COMMAND           CREATED          STATUS\nc350ec648185   training/webapp   ""python app.py""   12 minutes ago   Up 12 minutes\nBut just a few seconds later, the Kubernetes Deployment will have detected that\nthere is only one replica instead of the requested two, and it’ll launch a replacement\ncontainer automatically:\n$ docker ps\nCONTAINER ID   IMAGE             COMMAND           CREATED          STATUS\n56a216b8a829   training/webapp   ""python app.py""   1 second ago     Up 5 seconds\nc350ec648185   training/webapp   ""python app.py""   12 minutes ago   Up 12 minutes\nSo Kubernetes is ensuring that you always have the expected number of replicas\nrunning. Moreover, it is also running a load balancer to distribute traffic across those\nreplicas, which you can see by running the kubectl get services  command:\n$ kubectl get services\nNAME            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP      10.96.0.1      <none>        443/TCP        4h26m\nsimple-webapp   LoadBalancer   10.110.25.79   localhost     80:30234/TCP   4m58s\nThe first service in the list is Kubernetes itself, which you can ignore. The second is\nthe Service you created, also with the name simple-webapp  (based on the metadata\nblock). This service runs a load balancer for your app: you can see the IP it’s accessi‐\nble at (localhost ) and the port it’s listening on (80).\nKubernetes Deployments also provide automatic rollout of updates. A fun trick with\nthe training/webapp  Docker image is that if you set the environment variable PRO\nVIDER  to some value, it’ll use that value instead of the word world  in the text “Hello,\nworld!” Update examples/kubernetes-local/main.tf  to set this environment variable as\nfollows:\nmodule ""simple_webapp""  {\n  source  = ""../../modules/services/k8s-app""\n  name           = ""simple-webapp""\n  image          = ""training/webapp""\n  replicas        = 2\n  container_port  = 5000\n  environment_variables  = {\n    PROVIDER  = ""Terraform""\n  }\n}\nWorking with Multiple Different  Providers | 263",22428
77-Deploying Docker Containers in AWS Using Elastic Kubernetes Service.pdf,77-Deploying Docker Containers in AWS Using Elastic Kubernetes Service,"Run apply  one more time:\n$ terraform apply\n(...)\nApply complete! Resources: 0 added, 1 changed, 0 destroyed.\nOutputs:\nservice_endpoint = ""http://localhost""\nAfter a few seconds, try the endpoint again:\n$ curl http://localhost\nHello Terraform!\nAnd there you go, the Deployment has rolled out your change automatically: under\nthe hood, Deployments do a rolling deployment by default, similar to what you saw\nwith Auto Scaling Groups (note that you can change deployment settings by adding a\nstrategy  block to the kubernetes_deployment  resource).\nDeploying Docker Containers in AWS Using Elastic Kubernetes Service\nKubernetes has one more trick up its sleeve: it’s fairly portable. That is, you can\nreuse both the Docker images and the Kubernetes configurations in a totally different\ncluster and get similar results. To see this in action, let’s now deploy a Kubernetes\ncluster in AWS.\nSetting up and managing a secure, highly available, scalable Kubernetes cluster in the\ncloud from scratch is complicated. Fortunately, most cloud providers offer managed\nKubernetes services, where they run the control plane and worker nodes for you: e.g.,\nElastic Kubernetes Service (EKS) in AWS, Azure Kubernetes Service (AKS) in Azure,\nand Google Kubernetes Engine (GKE) in Google Cloud. I’m going to show you how\nto deploy a very basic EKS cluster in AWS.\nCreate a new module in modules/services/eks-cluster , and define the API for the\nmodule in a variables.tf  file with the following input variables:\nvariable  ""name"" {\n  description  = ""The name to use for the EKS cluster""\n  type        = string\n}\nvariable  ""min_size""  {\n  description  = ""Minimum number of nodes to have in the EKS cluster""\n  type        = number\n}\nvariable  ""max_size""  {\n  description  = ""Maximum number of nodes to have in the EKS cluster""\n264 | Chapter 7: Working with Multiple Providers\n  type        = number\n}\nvariable  ""desired_size""  {\n  description  = ""Desired number of nodes to have in the EKS cluster""\n  type        = number\n}\nvariable  ""instance_types""  {\n  description  = ""The types of EC2 instances to run in the node group""\n  type        = list(string)\n}\nThis code exposes input variables to set the EKS cluster’s name, size, and the types\nof instances to use for the worker nodes. Next, in main.tf , create an IAM role for the\ncontrol plane:\n# Create an IAM role for the control plane\nresource  ""aws_iam_role"" ""cluster""  {\n  name               = ""${var.name}-cluster-role""\n  assume_role_policy  = data.aws_iam_policy_document.cluster_assume_role.json\n}\n# Allow EKS to assume the IAM role\ndata ""aws_iam_policy_document"" ""cluster_assume_role""  {\n  statement  {\n    effect   = ""Allow""\n    actions  = [""sts:AssumeRole"" ]\n    principals  {\n      type         = ""Service""\n      identifiers  = [""eks.amazonaws.com"" ]\n    }\n  }\n}\n# Attach the permissions the IAM role needs\nresource  ""aws_iam_role_policy_attachment"" ""AmazonEKSClusterPolicy""  {\n  policy_arn  = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""\n  role       = aws_iam_role.cluster.name\n}\nThis IAM role can be assumed by the EKS service, and it has a Managed IAM Policy\nattached that gives the control plane the permissions it needs. Now, add the aws_vpc\nand aws_subnets  data sources to fetch information about the Default VPC and its\nsubnets:\n# Since this code is only for learning, use the Default VPC and subnets.\n# For real-world use cases, you should use a custom VPC and private subnets.\ndata ""aws_vpc"" ""default""  {\n  default  = true\n}\nWorking with Multiple Different  Providers | 265\n3For a comparison of the different types of EKS worker nodes, see the Gruntwork blog .data ""aws_subnets"" ""default""  {\n  filter {\n    name    = ""vpc-id""\n    values  = [data.aws_vpc.default.id ]\n  }\n}\nNow you can create the control plane for the EKS cluster by using the aws_eks_clus\nter resource:\nresource  ""aws_eks_cluster"" ""cluster""  {\n  name     = var.name\n  role_arn  = aws_iam_role.cluster.arn\n  version   = ""1.21""\n  vpc_config  {\n    subnet_ids  = data.aws_subnets.default.ids\n  }\n  # Ensure that IAM Role permissions are created before and deleted after\n  # the EKS Cluster. Otherwise, EKS will not be able to properly delete\n  # EKS managed EC2 infrastructure such as Security Groups.\n  depends_on  = [\n    aws_iam_role_policy_attachment.AmazonEKSClusterPolicy\n  ]\n}\nThe preceding code configures the control plane to use the IAM role you just created,\nand to deploy into the Default VPC and subnets.\nNext up are the worker nodes. EKS supports several different types of worker nodes:\nself-managed EC2 Instances (e.g., in an ASG that you create), AWS-managed EC2\nInstances (known as a managed node group ), and Fargate (serverless).3 The simplest\noption to use for the examples in this chapter will be the managed node groups.\nTo deploy a managed node group, you first need to create another IAM role:\n# Create an IAM role for the node group\nresource  ""aws_iam_role"" ""node_group""  {\n  name               = ""${var.name}-node-group""\n  assume_role_policy  = data.aws_iam_policy_document.node_assume_role.json\n}\n# Allow EC2 instances to assume the IAM role\ndata ""aws_iam_policy_document"" ""node_assume_role""  {\n  statement  {\n    effect   = ""Allow""\n    actions  = [""sts:AssumeRole"" ]\n266 | Chapter 7: Working with Multiple Providers\n    principals  {\n      type         = ""Service""\n      identifiers  = [""ec2.amazonaws.com"" ]\n    }\n  }\n}\n# Attach the permissions the node group needs\nresource  ""aws_iam_role_policy_attachment"" ""AmazonEKSWorkerNodePolicy""  {\n  policy_arn  = ""arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy""\n  role       = aws_iam_role.node_group.name\n}\nresource  ""aws_iam_role_policy_attachment"" ""AmazonEC2ContainerRegistryReadOnly""  {\n  policy_arn  = ""arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly""\n  role       = aws_iam_role.node_group.name\n}\nresource  ""aws_iam_role_policy_attachment"" ""AmazonEKS_CNI_Policy""  {\n  policy_arn  = ""arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy""\n  role       = aws_iam_role.node_group.name\n}\nThis IAM role can be assumed by the EC2 service (which makes sense, as managed\nnode groups use EC2 Instances under the hood), and it has several Managed IAM\nPolicies attached that give the managed node group the permissions it needs. Now\nyou can use the aws_eks_node_group  resource to create the managed node group\nitself:\nresource  ""aws_eks_node_group"" ""nodes""  {\n  cluster_name     = aws_eks_cluster.cluster.name\n  node_group_name  = var.name\n  node_role_arn    = aws_iam_role.node_group.arn\n  subnet_ids       = data.aws_subnets.default.ids\n  instance_types   = var.instance_types\n  scaling_config  {\n    min_size      = var.min_size\n    max_size      = var.max_size\n    desired_size  = var.desired_size\n  }\n  # Ensure that IAM Role permissions are created before and deleted after\n  # the EKS Node Group. Otherwise, EKS will not be able to properly\n  # delete EC2 Instances and Elastic Network Interfaces.\n  depends_on  = [\n    aws_iam_role_policy_attachment.AmazonEKSWorkerNodePolicy ,\n    aws_iam_role_policy_attachment.AmazonEC2ContainerRegistryReadOnly ,\n    aws_iam_role_policy_attachment.AmazonEKS_CNI_Policy ,\n  ]\n}\nWorking with Multiple Different  Providers | 267\nThis code configures the managed node group to use the control plane and IAM\nrole you just created, to deploy into the Default VPC, and to use the name, size, and\ninstance type parameters passed in as input variables.\nIn outputs.tf , add the following output variables:\noutput ""cluster_name""  {\n  value       = aws_eks_cluster.cluster.name\n  description  = ""Name of the EKS cluster""\n}\noutput ""cluster_arn""  {\n  value       = aws_eks_cluster.cluster.arn\n  description  = ""ARN of the EKS cluster""\n}\noutput ""cluster_endpoint""  {\n  value       = aws_eks_cluster.cluster.endpoint\n  description  = ""Endpoint of the EKS cluster""\n}\noutput ""cluster_certificate_authority""  {\n  value       = aws_eks_cluster.cluster.certificate_authority\n  description  = ""Certificate authority of the EKS cluster""\n}\nOK, the eks-cluster  module is now ready to roll. Let’s use it and the k8s-app\nmodule from earlier to deploy an EKS cluster and to deploy the training/webapp\nDocker image into that cluster. Create examples/kubernetes-eks/main.tf , and configure\nthe eks-cluster  module as follows:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""eks_cluster""  {\n  source  = ""../../modules/services/eks-cluster""\n  name         = ""example-eks-cluster""\n  min_size      = 1\n  max_size      = 2\n  desired_size  = 1\n  # Due to the way EKS works with ENIs, t3.small is the smallest\n  # instance type that can be used for worker nodes. If you try\n  # something smaller like t2.micro, which only has 4 ENIs,\n  # they'll all be used up by system services (e.g., kube-proxy)\n  # and you won't be able to deploy your own Pods.\n  instance_types  = [""t3.small"" ]\n}\nNext, configure the k8s-app  module as follows:\n268 | Chapter 7: Working with Multiple Providers\nprovider  ""kubernetes""  {\n  host = module.eks_cluster .cluster_endpoint\n  cluster_ca_certificate  = base64decode(\n    module.eks_cluster .cluster_certificate_authority [0].data\n  )\n  token = data.aws_eks_cluster_auth.cluster.token\n}\ndata ""aws_eks_cluster_auth"" ""cluster""  {\n  name = module.eks_cluster .cluster_name\n}\nmodule ""simple_webapp""  {\n  source  = ""../../modules/services/k8s-app""\n  name           = ""simple-webapp""\n  image          = ""training/webapp""\n  replicas        = 2\n  container_port  = 5000\n  environment_variables  = {\n    PROVIDER  = ""Terraform""\n  }\n  # Only deploy the app after the cluster has been deployed\n  depends_on  = [module.eks_cluster ]\n}\nThe preceding code configures the Kubernetes provider to authenticate to the EKS\ncluster, rather than your local Kubernetes cluster (from Docker Desktop). It then\nuses the k8s-app  module to deploy the training/webapp  Docker image exactly the\nsame way as you did when deploying it to Docker Desktop; the only difference is the\naddition of the depends_on  parameter to ensure that Terraform only tries to deploy\nthe Docker image after the EKS cluster has been deployed.\nNext, pass through the service endpoint as an output variable:\noutput ""service_endpoint""  {\n  value       = module.simple_webapp .service_endpoint\n  description  = ""The K8S Service endpoint""\n}\nOK, now you’re ready to deploy! Run terraform apply  as usual (note that EKS\nclusters can take 10–20 minutes to deploy, so be patient):\n$ terraform apply\n(...)\nApply complete! Resources: 10 added, 0 changed, 0 destroyed.\nOutputs:\nWorking with Multiple Different  Providers | 269\nservice_endpoint = ""http://774696355.us-east-2.elb.amazonaws.com""\nWait a little while for the web app to spin up and pass health checks, and then test out\nthe service_endpoint :\n$ curl http://774696355.us-east-2.elb.amazonaws.com\nHello Terraform!\nAnd there you have it! The same Docker image and Kubernetes code is now running\nin an EKS cluster in AWS, just the way it ran on your local computer. All the\nsame features work here too. For example, try updating environment_variables  to a\ndifferent PROVIDER  value, such as “Readers”:\nmodule ""simple_webapp""  {\n  source  = ""../../modules/services/k8s-app""\n  name           = ""simple-webapp""\n  image          = ""training/webapp""\n  replicas        = 2\n  container_port  = 5000\n  environment_variables  = {\n    PROVIDER  = ""Readers""\n  }\n  # Only deploy the app after the cluster has been deployed\n  depends_on  = [module.eks_cluster ]\n}\nRerun apply , and just a few seconds later, the Kubernetes Deployment will have\ndeployed the changes:\n$ curl http://774696355.us-east-2.elb.amazonaws.com\nHello Readers!\nThis is one of the advantages of using Docker: changes can be deployed very quickly.\nY ou can use kubectl  again to see what’s happening in your cluster. To authenticate\nkubectl  to the EKS cluster, you can use the aws eks update-kubeconfig  command\nto automatically update your $HOME/.kube/config  file:\n$ aws eks update-kubeconfig --region <REGION> --name <EKS_CLUSTER_NAME>\nwhere REGION  is the AWS region and EKS_CLUSTER_NAME  is the name of your EKS\ncluster. In the Terraform module, you deployed to the us-east-2  region and named\nthe cluster kubernetes-example , so the command will look like this:\n$ aws eks update-kubeconfig --region us-east-2 --name kubernetes-example\nNow, just as before, you can use the get nodes  command to inspect the worker\nnodes in your cluster, but this time, add the -o wide  flag to get a bit more info:\n270 | Chapter 7: Working with Multiple Providers\n$ kubectl get nodes\nNAME                             STATUS   AGE   EXTERNAL-IP    OS-IMAGE\nxxx.us-east-2.compute.internal   Ready    22m   3.134.78.187   Amazon Linux 2\nThe preceding snippet is highly truncated to fit into the book, but in the real output,\nyou should be able to see the one worker node, its internal and external IP , version\ninformation, OS information, and much more.\nY ou can use the get deployments  command to inspect your Deployments:\n$ kubectl get deployments\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\nsimple-webapp   2/2     2            2           19m\nNext, run get pods  to see the Pods:\n$ kubectl get pods\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\nsimple-webapp   2/2     2            2           19m\nAnd finally, run get services  to see the Services:\n$ kubectl get services\nNAME            TYPE           EXTERNAL-IP                              PORT(S)\nkubernetes      ClusterIP      <none>                                   443/TCP\nsimple-webapp   LoadBalancer   774696355.us-east-2.elb.amazonaws.com    80/TCP\nY ou should be able to see your load balancer and the URL you used to test it.\nSo there you have it: two different providers, both working in the same cloud, helping\nyou to deploy containerized workloads.\nThat said, just as in previous sections, I want to leave you with a few warnings:\nWarning 1: These  Kubernetes examples are very simplified!\nKubernetes is complicated, and it’s rapidly evolving and changing; trying to\nexplain all the details can easily fill a book all by itself. Since this is a book about\nTerraform, and not Kubernetes, my goal with the Kubernetes examples in this\nchapter was to keep them as simple and minimal as possible. Therefore, while\nI hope the code examples you’ve seen have been useful from a learning and\nexperimentation perspective, if you are going to use Kubernetes for real-world,\nproduction use cases, you’ll need to change many aspects of this code, such as\nconfiguring a number of additional services and settings in the eks-cluster\nmodule (e.g., ingress controllers, secret envelope encryption, security groups,\nOIDC authentication, Role-Based Access Control (RBAC) mapping, VPC CNI,\nkube-proxy, CoreDNS), exposing many other settings in the k8s-app  module\n(e.g., secrets management, volumes, liveness probes, readiness probes, labels,\nannotations, multiple ports, multiple containers), and using a custom VPC with\nWorking with Multiple Different  Providers | 271",15236
78-Conclusion.pdf,78-Conclusion,"4Alternatively, you can use off-the-shelf production-grade Kubernetes modules, such as the ones in the\nGruntwork Infrastructure as Code Library .private subnets for your EKS cluster instead of the Default VPC and public\nsubnets.4\nWarning 2: Use multiple providers sparingly\nAlthough you certainly can use multiple providers in a single module, I don’t\nrecommend doing it too often, for similar reasons to why I don’t recommend\nusing provider aliases too often: in most cases, you want each provider to be\nisolated in its own module so that you can manage it separately and limit the\nblast radius from mistakes or attackers.\nMoreover, Terraform doesn’t have great support for dependency ordering\nbetween providers. For example, in the Kubernetes example, you had a single\nmodule that deployed both the EKS cluster, using the AWS Provider, and a\nKubernetes app into that cluster, using the Kubernetes provider. As it turns\nout, the Kubernetes provider documentation  explicitly recommends against  this\npattern:\nWhen using interpolation to pass credentials to the Kubernetes provider from\nother resources, these resources SHOULD NOT be created in the same Terraform\nmodule where Kubernetes provider resources are also used. This will lead to\nintermittent and unpredictable errors which are hard to debug and diagnose. The\nroot issue lies with the order in which Terraform itself evaluates the provider\nblocks vs. actual resources.\nThe example code in this book is able to work around these issues by depending\non the aws_eks_cluster_auth  data source, but that’s a bit of a hack. Therefore,\nin production code, I always recommend deploying the EKS cluster in one\nmodule and deploying Kubernetes apps in separate modules, after the cluster has\nbeen deployed.\nConclusion\nAt this point, you hopefully understand how to work with multiple providers in\nTerraform code, and you can answer the three questions from the beginning of this\nchapter:\nWhat if you need to deploy to multiple AWS regions?\nUse multiple provider  blocks, each configured with a different region  and alias\nparameter.\n272 | Chapter 7: Working with Multiple Providers\nWhat if you need to deploy to multiple AWS accounts?\nUse multiple provider  blocks, each configured with a different assume_role\nblock and an alias  parameter.\nWhat if you need to deploy to other clouds, such as Azure or GCP or Kubernetes?\nUse multiple provider  blocks, each configured for its respective cloud.\nHowever, you’ve also seen that using multiple providers in one module is typically an\nantipattern. So the real answer to these questions, especially in real-world, production\nuse cases, is to use each provider in a separate module to keep different regions,\naccounts, and clouds isolated from one another, and to limit your blast radius.\nLet’s now move on to Chapter 8 , where I’ll go over several other patterns for how to\nbuild Terraform modules for real-world, production use cases—the kind of modules\nyou could bet your company on.\nConclusion | 273",3042
79-Chapter 8. Production-Grade Terraform Code.pdf,79-Chapter 8. Production-Grade Terraform Code,"CHAPTER 8\nProduction-Grade Terraform Code\nBuilding production-grade infrastructure is difficult. And stressful. And time con‐\nsuming. By production-grade infrastructure , I mean the kind of infrastructure you’ d\nbet your company on. Y ou’re betting that your infrastructure won’t fall over if traffic\ngoes up, or lose your data if there’s an outage, or allow that data to be compromised\nwhen hackers try to break in—and if that bet doesn’t work out, your company\nmight go out of business. That’s what’s at stake when I refer to production-grade\ninfrastructure in this chapter.\nI’ve had the opportunity to work with hundreds of companies, and based on all of\nthese experiences, here’s roughly how long you should expect your next production-\ngrade infrastructure project to take:\n•If you want to deploy a service fully managed by a third party, such as running•\nMySQL using the AWS Relational Database Service (RDS), you can expect it to\ntake you one to two weeks to get that service ready for production.\n•If you want to run your own stateless distributed app, such as a cluster of Node.js•\napps that don’t store any data locally (e.g., they store all their data in RDS)\nrunning on top of an AWS Auto Scaling Group (ASG), that will take roughly\ntwice as long, or about two to four weeks to get ready for production.\n•If you want to run your own stateful distributed app, such as an Elasticsearch•\ncluster that runs on top of an ASG and stores data on local disks, that will be\nanother order-of-magnitude increase, or about two to four months to get ready\nfor production.\n•If you want to build out your entire architecture, including all of your apps, data•\nstores, load balancers, monitoring, alerting, security, and so on, that’s another\norder-of-magnitude (or two) increase, or about 6 to 36 months of work, with\n275\nsmall companies typically being closer to six months and larger companies typi‐\ncally taking several years.\nTable 8-1  shows a summary of this data.\nTable 8-1. How long it takes to build production-grade infrastructure from scratch\nType of infrastructure Example Time estimate\nManaged service Amazon RDS 1–2 weeks\nSelf-managed distributed system (stateless) A cluster of Node.js apps in an ASG 2–4 weeks\nSelf-managed distributed system (stateful) Elasticsearch cluster 2–4 months\nEntire architecture Apps, data stores, load balancers, monitoring, etc. 6–36 months\nIf you haven’t gone through the process of building out production-grade infrastruc‐\nture, you may be surprised by these numbers. I often hear reactions like, “How can\nit possibly take that long?” or “I can deploy a server on <cloud> in a few minutes.\nSurely it can’t take months to get the rest done!” And all too often, from many an\noverconfident engineer, “I’m sure those numbers apply to other people, but I will be\nable to get this done in a few days. ”\nAnd yet, anyone who has gone through a major cloud migration or assembled a\nbrand-new infrastructure from scratch knows that these numbers, if anything, are\noptimistic—a best-case scenario, really. If you don’t have people on your team with\ndeep expertise in building production-grade infrastructure, or if your team is being\npulled in a dozen different directions and you can’t find the time to focus on it, it\nmight take you significantly longer.\nIn this chapter, I’ll go over why it takes so long to build production-grade infrastruc‐\nture, what production grade really means, and what patterns work best for creating\nreusable, production-grade modules:\n•Why it takes so long to build production-grade infrastructure•\n•The production-grade infrastructure checklist•\n•Production-grade infrastructure modules•\n—Small modules—\n—Composable modules—\n—Testable modules—\n—Versioned modules—\n—Beyond Terraform modules—\n276 | Chapter 8: Production-Grade Terraform Code",3863
80-Why It Takes So Long to Build Production-Grade Infrastructure.pdf,80-Why It Takes So Long to Build Production-Grade Infrastructure,"1Douglas R. Hofstadter, Gödel, Escher, Bach: An Eternal Golden Braid , 20th anniversary ed. (New Y ork: Basic\nBooks, 1999).\n2Seth Godin, “Don’t Shave That Y ak!” Seth’s Blog, March 5, 2005, https://bit.ly/2OK45uL .\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\nWhy It Takes So Long to Build Production-Grade\nInfrastructure\nTime estimates for software projects are notoriously inaccurate. Time estimates for\nDevOps projects, doubly so. That quick tweak that you thought would take five\nminutes takes up the entire day; the minor new feature that you estimated at a day of\nwork takes two weeks; the app that you thought would be in production in two weeks\nis still not quite there six months later. Infrastructure and DevOps projects, perhaps\nmore than any other type of software, are the ultimate examples of Hofstadter’s Law:1\nHofstadter’s Law: It always takes longer than you expect, even when you take into\naccount Hofstadter’s Law.\nI think there are three major reasons for this. The first reason is that DevOps, as an\nindustry, is still in the Stone Age. I don’t mean that as an insult but rather in the sense\nthat the industry is still in its infancy. The terms “cloud computing, ” “infrastructure\nas code, ” and “DevOps” only appeared in the mid- to late-2000s, and tools like\nTerraform, Docker, Packer, and Kubernetes were all initially released in the mid- to\nlate-2010s. All of these tools and techniques are relatively new, and all of them are\nchanging rapidly. This means that they are not particularly mature and few people\nhave deep experience with them, so it’s no surprise that projects take longer than\nexpected.\nThe second reason is that DevOps seems to be particularly susceptible to yak shaving .\nIf you haven’t heard of “yak shaving” before, I assure you, this is a term that you will\ngrow to love (and hate). The best definition I’ve seen of this term comes from a blog\npost by Seth Godin:2\n“I want to wax the car today. ”\n“Oops, the hose is still broken from the winter. I’ll need to buy a new one at Home\nDepot. ”\n“But Home Depot is on the other side of the Tappan Zee bridge and getting there\nwithout my EZPass is miserable because of the tolls. ”\n“But, wait! I could borrow my neighbor’s EZPass… ”\nWhy It Takes So Long to Build Production-Grade Infrastructure | 277\n3Frederick P . Brooks Jr., The Mythical Man-Month: Essays on Software  Engineering , anniversary ed. (Reading,\nMA: Addison-Wesley Professional, 1995).“Bob won’t lend me his EZPass until I return the mooshi pillow my son borrowed,\nthough. ”\n“ And we haven’t returned it because some of the stuffing fell out and we need to get\nsome yak hair to restuff it. ”\nAnd the next thing you know, you’re at the zoo, shaving a yak, all so you can wax your\ncar.\nY ak shaving consists of all the tiny, seemingly unrelated tasks you must do before you\ncan do the task you originally wanted to do. If you develop software, and especially if\nyou work in the DevOps industry, you’ve probably seen this sort of thing a thousand\ntimes. Y ou go to deploy a fix for a small typo, only to uncover a bug in your app\nconfiguration. Y ou try to deploy a fix for the app configuration, but that’s blocked by\na TLS certificate issue. After spending hours on Stack Overflow, you try to roll out a\nfix for the TLS issue, but that fails due to a problem with your deployment system.\nY ou spend hours digging into that problem and find out it’s due to an out-of-date\nLinux version. The next thing you know, you’re updating the operating system on\nyour entire fleet of servers, all so you can deploy a “quick” one-character typo fix.\nDevOps seems to be especially prone to these sorts of yak-shaving incidents. In\npart, this is a consequence of the immaturity of DevOps technologies and modern\nsystem design, which often involves lots of tight coupling and duplication in the\ninfrastructure. Every change you make in the DevOps world is a bit like trying to pull\nout one wire from a box of tangled wires—it just tends to pull up everything else in\nthe box with it. In part, this is because the term “DevOps” covers an astonishingly\nbroad set of topics: everything from build to deployment to security and so on.\nThis brings us to the third reason why DevOps work takes so long. The first two\nreasons—DevOps is in the Stone Age and yak shaving—can be classified as accidental\ncomplexity. Accidental complexity  refers to the problems imposed by the particular\ntools and processes you’ve chosen, as opposed to essential complexity , which refers\nto the problems inherent in whatever it is that you’re working on.3 For example, if\nyou’re using C++ to write stock-trading algorithms, dealing with memory allocation\nbugs is accidental complexity: had you picked a different programming language\nwith automatic memory management, you wouldn’t have this as a problem at all.\nOn the other hand, figuring out an algorithm that can beat the market is essential\ncomplexity: you’ d have to solve this problem no matter what programming language\nyou picked.\n278 | Chapter 8: Production-Grade Terraform Code",5166
81-Small Modules.pdf,81-Small Modules,"The third reason why DevOps takes so long—the essential complexity of this prob‐\nlem—is that there is a genuinely long checklist of tasks that you must do to prepare\ninfrastructure for production. The problem is that the vast majority of developers\ndon’t know about most of the items on the checklist, so when they estimate a\nproject, they forget about a huge number of critical and time-consuming details. This\nchecklist is the focus of the next section.\nThe Production-Grade Infrastructure Checklist\nHere’s a fun experiment: go around your company and ask, “What are the require‐\nments for going to production?” In most companies, if you ask this question to\nfive people, you’ll get five different answers. One person will mention the need for\nmetrics and alerts; another will talk about capacity planning and high availability;\nsomeone else will go on a rant about automated tests and code reviews; yet another\nperson will bring up encryption, authentication, and server hardening; and if you’re\nlucky, someone might remember to bring up data backups and log aggregation. Most\ncompanies do not have a clear definition of the requirements for going to production,\nwhich means each piece of infrastructure is deployed a little differently and can be\nmissing some critical functionality.\nTo help improve this situation, I’ d like to share with you the Production-Grade\nInfrastructure Checklist , as shown in Table 8-2 . This list covers most of the key items\nthat you need to consider to deploy infrastructure to production.\nTable 8-2. The Production-Grade Infrastructure Checklist\nTask Description Example tools\nInstall Install the software binaries and all dependencies. Bash, Ansible, Docker,\nPacker\nConfigure Configure  the software at runtime. Includes port settings, TLS certs, service\ndiscovery, leaders, followers, replication, etc.Chef, Ansible, Kubernetes\nProvision Provision the infrastructure. Includes servers, load balancers, network\nconfiguration,  firewall  settings, IAM permissions, etc.Terraform, CloudFormation\nDeploy Deploy the service on top of the infrastructure. Roll out updates with no\ndowntime. Includes blue-green, rolling, and canary deployments.ASG, Kubernetes, ECS\nHigh availability Withstand outages of individual processes, servers, services, datacenters,\nand regions.Multi-datacenter, multi-\nregion\nScalability Scale up and down in response to load. Scale horizontally (more servers)\nand/or vertically (bigger servers).Auto scaling, replication\nPerformance Optimize CPU, memory, disk, network, and GPU usage. Includes query\ntuning, benchmarking, load testing, and profiling.Dynatrace, Valgrind,\nVisualVM\nNetworking Configure  static and dynamic IPs, ports, service discovery, firewalls,  DNS,\nSSH access, and VPN access.VPCs, firewalls,  Route 53\nThe Production-Grade Infrastructure Checklist | 279\nTask Description Example tools\nSecurity Encryption in transit (TLS) and on disk, authentication, authorization,\nsecrets management, server hardening.ACM, Let’s Encrypt, KMS,\nVault\nMetrics Availability metrics, business metrics, app metrics, server metrics, events,\nobservability, tracing, and alerting.CloudWatch, Datadog\nLogs Rotate logs on disk. Aggregate log data to a central location. Elastic Stack, Sumo Logic\nData backup Make backups of DBs, caches, and other data on a scheduled basis.\nReplicate to separate region/account.AWS Backup, RDS snapshots\nCost optimization Pick proper Instance types, use spot and reserved Instances, use auto\nscaling, and clean up unused resources.Auto scaling, Infracost\nDocumentation Document your code, architecture, and practices. Create playbooks to\nrespond to incidents.READMEs, wikis, Slack, IaC\nTests Write automated tests for your infrastructure code. Run tests after every\ncommit and nightly.Terratest, tflint,  OPA, InSpec\nMost developers are aware of the first few tasks: install, configure, provision, and\ndeploy. It’s all the ones that come after them that catch people off guard. For example,\ndid you think through the resilience of your service and what happens if a server goes\ndown? Or a load balancer goes down? Or an entire datacenter goes dark? Networking\ntasks are also notoriously tricky: setting up VPCs, VPNs, service discovery, and SSH\naccess are all essential tasks that can take months and yet are often entirely left out\nof many project plans and time estimates. Security tasks, such as encrypting data in\ntransit using TLS, dealing with authentication, and figuring out how to store secrets,\nare also often forgotten until the last minute.\nEvery time you’re working on a new piece of infrastructure, go through this checklist.\nNot every single piece of infrastructure needs every single item on the list, but you\nshould consciously and explicitly document which items you’ve implemented, which\nones you’ve decided to skip, and why.\nProduction-Grade Infrastructure Modules\nNow that you know the list of tasks that you need to do for each piece of infrastruc‐\nture, let’s talk about the best practices for building reusable modules to implement\nthese tasks. Here are the topics I’ll cover:\n•Small modules•\n•Composable modules•\n•Testable modules•\n•Versioned modules•\n•Beyond Terraform modules•\n280 | Chapter 8: Production-Grade Terraform Code\nSmall Modules\nDevelopers who are new to Terraform, and IaC in general, often define all of their\ninfrastructure for all of their environments (dev, stage, prod, etc.) in a single file or\nsingle module. As discussed in “State File Isolation” on page 93, this is a bad idea. In\nfact, I’ll go even further and make the following claim: large modules—modules that\ncontain more than a few hundred lines of code or that deploy more than a few closely\nrelated pieces of infrastructure—should be considered harmful.\nHere are just a few of the downsides of large modules:\nLarge modules are slow\nIf all of your infrastructure is defined in one Terraform module, running any\ncommand will take a long time. I’ve seen modules grow so large that terraform\nplan  takes 20 minutes to run!\nLarge modules are insecure\nIf all your infrastructure is managed in a single large module, to change anything,\nyou need permissions to access everything. This means that almost every user\nmust be an admin, which goes against the principle of least privilege .\nLarge modules are risky\nIf all your eggs are in one basket, a mistake anywhere could break everything.\nY ou might be making a minor change to a frontend app in staging, but due to a\ntypo or running the wrong command, you delete the production database.\nLarge modules are difficult  to understand\nThe more code you have in one place, the more difficult it is for any one person\nto understand it all. And when you don’t understand the infrastructure you’re\ndealing with, you end up making costly mistakes.\nLarge modules are difficult  to review\nReviewing a module that consists of several dozen lines of code is easy; reviewing\na module that consists of several thousand lines of code is nearly impossible.\nMoreover, terraform plan  not only takes longer to run, but if the output of the\nplan  command is several thousand lines, no one will bother to read it. And that\nmeans no one will notice that one little red line that means your database is being\ndeleted.\nLarge modules are difficult  to test\nTesting infrastructure code is hard; testing a large amount of infrastructure code\nis nearly impossible. I’ll come back to this point in Chapter 9 .\nProduction-Grade Infrastructure Modules | 281\n4Robert C. Martin, Clean Code: A Handbook of Agile Software  Craftsmanship , 1st ed. (Upper Saddle River, NJ:\nPrentice Hall, 2008).In short, you should build your code out of small modules that each do one thing.\nThis is not a new or controversial insight. Y ou’ve probably heard it many times\nbefore, albeit in slightly different contexts, such as this version from Clean Code :4\nThe first rule of functions is that they should be small. The second rule of functions is\nthat they should be smaller than that.\nImagine you were using a general-purpose programming language such as Java or\nPython or Ruby, and you came across a single function that was 20,000 lines  long—\nyou would immediately know this is a code smell. The better approach is to refactor\nthis code into a number of small, standalone functions that each do one thing. Y ou\nshould use the same strategy with Terraform.\nImagine that you came across the architecture shown in Figure 8-1 .\nFigure 8-1. A relatively complicated AWS architecture.\nIf this architecture was defined in a single Terraform module that was 20,000 lines\nlong, you should immediately think of it as a code smell. The better approach is to\nrefactor this module into a number of small, standalone modules that each do one\nthing, as shown in Figure 8-2 .\n282 | Chapter 8: Production-Grade Terraform Code\nFigure 8-2. A relatively complicated AWS architecture refactored into many small\nmodules.\nFor example, consider the webserver-cluster  module, which you last worked on\nin Chapter 5 . This module has become fairly large, as it is handling three somewhat\nunrelated tasks:\nAuto Scaling Group (ASG)\nThe webserver-cluster  module deploys an ASG that can do a zero-downtime,\nrolling deployment.\nApplication Load Balancer (ALB)\nThe webserver-cluster  deploys an ALB.\nHello, World app\nThe webserver-cluster  module also deploys a simple “Hello, World” app.\nLet’s refactor the code accordingly into three smaller modules:\nmodules/cluster/asg-rolling-deploy\nA generic, reusable, standalone module for deploying an ASG that can do a\nzero-downtime, rolling deployment.\nmodules/networking/alb\nA generic, reusable, standalone module for deploying an ALB.\nProduction-Grade Infrastructure Modules | 283\nmodules/services/hello-world-app\nA module specifically for deploying the “Hello, World” app, which uses the\nasg-rolling-deploy  and alb modules under the hood.\nBefore getting started, make sure to run terraform destroy  on any webserver-\ncluster  deployments you have from previous chapters. After you do that, you can\nstart putting together the asg-rolling-deploy  and alb modules. Create a new folder\nat modules/cluster/asg-rolling-deploy , and move the following resources from mod‐\nule/services/webserver-cluster/main.tf  to modules/cluster/asg-rolling-deploy/main.tf :\n•aws_launch_configuration•\n•aws_autoscaling_group•\n•aws_autoscaling_schedule  (both of them) •\n•aws_security_group  (for the Instances but not for the ALB) •\n•aws_security_group_rule  (just the one rule for the Instances but not those for •\nthe ALB)\n•aws_cloudwatch_metric_alarm  (both of them) •\nNext, move the following variables from module/services/webserver-cluster/variables.tf\nto modules/cluster/asg-rolling-deploy/variables.tf :\n•cluster_name•\n•ami•\n•instance_type•\n•min_size•\n•max_size•\n•enable_autoscaling•\n•custom_tags•\n•server_port•\nLet’s now move on to the ALB module. Create a new folder at modules/network‐\ning/alb , and move the following resources from module/services/webserver-cluster/\nmain.tf  to modules/networking/alb/main.tf :\n•aws_lb•\n•aws_lb_listener•\n•aws_security_group  (the one for the ALB but not for the Instances) •\n284 | Chapter 8: Production-Grade Terraform Code",11340
82-Composable Modules.pdf,82-Composable Modules,"5Peter H. Salus, A Quarter-Century of Unix  (New Y ork: Addison-Wesley Professional, 1994).•aws_security_group_rule  (both of the rules for the ALB but not the one for the •\nInstances)\nCreate modules/networking/alb/variables.tf , and define a single variable within:\nvariable  ""alb_name""  {\n  description  = ""The name to use for this ALB""\n  type        = string\n}\nUse this variable as the name  argument of the aws_lb  resource:\nresource  ""aws_lb"" ""example""  {\n  name               = var.alb_name\n  load_balancer_type  = ""application""\n  subnets             = data.aws_subnets.default.ids\n  security_groups     = [aws_security_group.alb.id ]\n}\nAnd the name  argument of the aws_security_group  resource:\nresource  ""aws_security_group"" ""alb""  {\n  name = var.alb_name\n}\nThis is a lot of code to shuffle around, so feel free to use the code examples for this\nchapter from GitHub .\nComposable Modules\nY ou now have two small modules— asg-rolling-deploy  and alb—that each do one\nthing and do it well. How do you make them work together? How do you build\nmodules that are reusable and composable? This question is not unique to Terraform\nbut is something programmers have been thinking about for decades. To quote Doug\nMcIlroy,5 the original developer of Unix pipes and a number of other Unix tools,\nincluding diff , sort , join , and tr:\nThis is the Unix philosophy: Write programs that do one thing and do it well. Write\nprograms to work together.\nOne way to do this is through function composition , in which you can take the outputs\nof one function and pass them as the inputs to another. For example, if you had the\nfollowing small functions in Ruby:\n# Simple function to do addition\ndef add(x, y)\n  return x + y\nend\nProduction-Grade Infrastructure Modules | 285\n# Simple function to do subtraction\ndef sub(x, y)\n  return x - y\nend\n# Simple function to do multiplication\ndef multiply (x, y)\n  return x * y\nend\nyou can use function composition to put them together by taking the outputs from\nadd and sub and passing them as the inputs to multiply :\n# Complex function that composes several simpler functions\ndef do_calculation (x, y)\n  return multiply (add(x, y), sub(x, y))\nend\nOne of the main ways to make functions composable is to minimize side effects :\nthat is, where possible, avoid reading state from the outside world and instead have\nit passed in via input parameters, and avoid writing state to the outside world and\ninstead return the result of your computations via output parameters. Minimizing\nside effects is one of the core tenets of functional programming because it makes\nthe code easier to reason about, easier to test, and easier to reuse. The reuse story is\nparticularly compelling, since function composition allows you to gradually build up\nmore complicated functions by combining simpler functions.\nAlthough you can’t avoid side effects when working with infrastructure code, you can\nstill follow the same basic principles in your Terraform modules: pass everything in\nthrough input variables, return everything through output variables, and build more\ncomplicated modules by combining simpler modules.\nOpen up modules/cluster/asg-rolling-deploy/variables.tf , and add four new input\nvariables:\nvariable  ""subnet_ids""  {\n  description  = ""The subnet IDs to deploy to""\n  type        = list(string)\n}\nvariable  ""target_group_arns""  {\n  description  = ""The ARNs of ELB target groups in which to register Instances""\n  type        = list(string)\n  default      = []\n}\nvariable  ""health_check_type""  {\n  description  = ""The type of health check to perform. Must be one of: EC2, ELB.""\n  type        = string\n  default      = ""EC2""\n286 | Chapter 8: Production-Grade Terraform Code\n}\nvariable  ""user_data""  {\n  description  = ""The User Data script to run in each Instance at boot""\n  type        = string\n  default      = null\n}\nThe first variable, subnet_ids , tells the asg-rolling-deploy  module what subnets\nto deploy into. Whereas the webserver-cluster  module was hardcoded to deploy\ninto the Default VPC and subnets, by exposing the subnet_ids  variable, you\nallow this module to be used with any VPC or subnets. The next two variables,\ntarget_group_arns  and health_check_type , configure how the ASG integrates with\nload balancers. Whereas the webserver-cluster  module had a built-in ALB, the\nasg-rolling-deploy  module is meant to be a generic module, so exposing the load-\nbalancer settings as input variables allows you to use the ASG with a wide variety of\nuse cases; e.g., no load balancer, one ALB, multiple NLBs, and so on.\nTake these three new input variables and pass them through to the aws_autoscal\ning_group  resource in modules/cluster/asg-rolling-deploy/main.tf , replacing the pre‐\nviously hardcoded settings that were referencing resources (e.g., the ALB) and\ndata sources (e.g., aws_subnets ) that we didn’t copy into the asg-rolling-deploy\nmodule:\nresource  ""aws_autoscaling_group"" ""example""  {\n  name                 = var.cluster_name\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = var.subnet_ids\n  # Configure integrations with a load balancer\n  target_group_arns     = var.target_group_arns\n  health_check_type     = var.health_check_type\n  min_size  = var.min_size\n  max_size  = var.max_size\n  # (...)\n}\nThe fourth variable, user_data , is for passing in a User Data script. Whereas the\nwebserver-cluster  module had a hardcoded User Data script that could only be\nused to deploy a “Hello, World” app, by taking in a User Data script as an input\nvariable, the asg-rolling-deploy  module can be used to deploy any app across\nan ASG. Pass this user_data  variable through to the aws_launch_configuration\nresource:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = var.ami\n  instance_type    = var.instance_type\nProduction-Grade Infrastructure Modules | 287\n  security_groups  = [aws_security_group.instance.id ]\n  user_data        = var.user_data\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n  }\n}\nY ou’ll also want to add a couple of useful output variables to modules/cluster/asg-\nrolling-deploy/outputs.tf :\noutput ""asg_name""  {\n  value       = aws_autoscaling_group.example.name\n  description  = ""The name of the Auto Scaling Group""\n}\noutput ""instance_security_group_id""  {\n  value       = aws_security_group.instance.id\n  description  = ""The ID of the EC2 Instance Security Group""\n}\nOutputting this data makes the asg-rolling-deploy  module even more reusable,\nsince consumers of the module can use these outputs to add new behaviors, such as\nattaching custom rules to the security group.\nFor similar reasons, you should add several output variables to modules/network‐\ning/alb/outputs.tf :\noutput ""alb_dns_name""  {\n  value       = aws_lb.example.dns_name\n  description  = ""The domain name of the load balancer""\n}\noutput ""alb_http_listener_arn""  {\n  value       = aws_lb_listener.http.arn\n  description  = ""The ARN of the HTTP listener""\n}\noutput ""alb_security_group_id""  {\n  value       = aws_security_group.alb.id\n  description  = ""The ALB Security Group ID""\n}\nY ou’ll see how to use these shortly.\nThe last step is to convert the webserver-cluster  module into a hello-world-app\nmodule that can deploy a “Hello, World” app using the asg-rolling-deploy  and\nalb modules. To do this, rename module/services/webserver-cluster  to module/services/\nhello-world-app . After all the changes in the previous steps, you should have only the\nfollowing resources and data sources left in module/services/hello-world-app/main.tf :\n288 | Chapter 8: Production-Grade Terraform Code\n•aws_lb_target_group•\n•aws_lb_listener_rule•\n•terraform_remote_state  (for the DB) •\n•aws_vpc•\n•aws_subnets•\nAdd the following variable to modules/services/hello-world-app/variables.tf :\nvariable  ""environment""  {\n  description  = ""The name of the environment we're deploying to""\n  type        = string\n}\nNow, add the asg-rolling-deploy  module that you created earlier to the hello-\nworld-app  module to deploy an ASG:\nmodule ""asg"" {\n  source  = ""../../cluster/asg-rolling-deploy""\n  cluster_name   = ""hello-world-${var.environment}""\n  ami           = var.ami\n  instance_type  = var.instance_type\n  user_data      = templatefile (""${path.module}/user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = data.terraform_remote_state.db.outputs.address\n    db_port      = data.terraform_remote_state.db.outputs.port\n    server_text  = var.server_text\n  })\n  min_size            = var.min_size\n  max_size            = var.max_size\n  enable_autoscaling  = var.enable_autoscaling\n  subnet_ids         = data.aws_subnets.default.ids\n  target_group_arns  = [aws_lb_target_group.asg.arn ]\n  health_check_type  = ""ELB""\n  custom_tags  = var.custom_tags\n}\nAnd add the alb module, also that you created earlier, to the hello-world-app\nmodule to deploy an ALB:\nmodule ""alb"" {\n  source  = ""../../networking/alb""\n  alb_name    = ""hello-world-${var.environment}""\n  subnet_ids  = data.aws_subnets.default.ids\n}\nProduction-Grade Infrastructure Modules | 289\nNote the use of the input variable environment  as a way to enforce a naming conven‐\ntion, so all of your resources will be namespaced based on the environment (e.g.,\nhello-world-stage , hello-world-prod ). This code also sets the new subnet_ids ,\ntarget_group_arns , health_check_type , and user_data  variables you added earlier\nto appropriate values.\nNext, you need to configure the ALB target group and listener rule for this\napp. Update the aws_lb_target_group  resource in modules/services/hello-world-app/\nmain.tf  to use environment  in its name :\nresource  ""aws_lb_target_group"" ""asg""  {\n  name     = ""hello-world-${var.environment}""\n  port     = var.server_port\n  protocol  = ""HTTP""\n  vpc_id    = data.aws_vpc.default.id\n  health_check  {\n    path                 = ""/""\n    protocol             = ""HTTP""\n    matcher              = ""200""\n    interval             = 15\n    timeout              = 3\n    healthy_threshold    = 2\n    unhealthy_threshold  = 2\n  }\n}\nNow, update the listener_arn  parameter of the aws_lb_listener_rule  resource to\npoint at the alb_http_listener_arn  output of the ALB module:\nresource  ""aws_lb_listener_rule"" ""asg""  {\n  listener_arn  = module.alb.alb_http_listener_arn\n  priority      = 100\n  condition  {\n    path_pattern  {\n      values  = [""*""]\n    }\n  }\n  action {\n    type              = ""forward""\n    target_group_arn  = aws_lb_target_group.asg.arn\n  }\n}\nFinally, pass through the important outputs from the asg-rolling-deploy  and alb\nmodules as outputs of the hello-world-app  module:\noutput ""alb_dns_name""  {\n  value       = module.alb.alb_dns_name\n290 | Chapter 8: Production-Grade Terraform Code",11001
83-Testable Modules.pdf,83-Testable Modules,"description  = ""The domain name of the load balancer""\n}\noutput ""asg_name""  {\n  value       = module.asg.asg_name\n  description  = ""The name of the Auto Scaling Group""\n}\noutput ""instance_security_group_id""  {\n  value       = module.asg.instance_security_group_id\n  description  = ""The ID of the EC2 Instance Security Group""\n}\nThis is function composition at work: you’re building up more complicated behavior\n(a “Hello, World” app) from simpler parts (ASG and ALB modules).\nTestable Modules\nAt this stage, you’ve written a whole lot of code in the form of three modules:\nasg-rolling-deploy , alb, and hello-world-app . The next step is to check that your\ncode actually works.\nThe modules you’ve created aren’t root modules meant to be deployed directly. To\ndeploy them, you need to write some Terraform code to plug in the arguments you\nwant, set up the provider , configure the backend , and so on. A great way to do this is\nto create an examples  folder that, as the name suggests, shows examples of how to use\nyour modules. Let’s try it out.\nCreate examples/asg/main.tf  with the following contents:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""asg"" {\n  source  = ""../../modules/cluster/asg-rolling-deploy""\n  cluster_name   = var.cluster_name\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n  min_size            = 1\n  max_size            = 1\n  enable_autoscaling  = false\n  subnet_ids         = data.aws_subnets.default.ids\n}\ndata ""aws_vpc"" ""default""  {\n  default  = true\n}\nProduction-Grade Infrastructure Modules | 291\ndata ""aws_subnets"" ""default""  {\n  filter {\n    name    = ""vpc-id""\n    values  = [data.aws_vpc.default.id ]\n  }\n}\ndata ""aws_ami"" ""ubuntu""  {\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\nThis bit of code uses the asg-rolling-deploy  module to deploy an ASG of size 1.\nTry it out by running terraform init  and terraform apply  and checking to see that\nit runs without errors and actually spins up an ASG. Now, add in a README.md  file\nwith these instructions, and suddenly this tiny little example takes on a whole lot of\npower. In just several files and lines of code, you now have the following:\nA manual test harness\nY ou can use this example code while working on the asg-rolling-deploy  mod‐\nule to repeatedly deploy and undeploy it by manually running terraform apply\nand terraform destroy  to check that it works as you expect.\nAn automated test harness\nAs you will see in Chapter 9 , this example code is also how you create automated\ntests for your modules. I typically recommend that tests go into the test folder.\nExecutable documentation\nIf you commit this example (including README.md ) into version control, other\nmembers of your team can find it, use it to understand how your module works,\nand take the module for a spin without writing a line of code. It’s both a way to\nteach the rest of your team and, if you add automated tests around it, a way to\nensure that your teaching materials always work as expected.\nEvery Terraform module you have in the modules  folder should have a corresponding\nexample in the examples  folder. And every example in the examples  folder should\nhave a corresponding test in the test folder. In fact, you’ll most likely have multiple\nexamples (and therefore multiple tests) for each module, with each example showing\ndifferent configurations and permutations of how that module can be used. For\nexample, you might want to add other examples for the asg-rolling-deploy  module\nthat show how to use it with auto scaling policies, how to hook up load balancers to\nit, how to set custom tags, and so on.\n292 | Chapter 8: Production-Grade Terraform Code\nPutting this all together, the folder structure for a typical modules  repo will look\nsomething like this:\nmodules\n └ examples\n   └ alb\n   └ asg-rolling-deploy\n     └ one-instance\n     └ auto-scaling\n     └ with-load-balancer\n     └ custom-tags\n   └ hello-world-app\n   └ mysql\n └ modules\n   └ alb\n   └ asg-rolling-deploy\n   └ hello-world-app\n   └ mysql\n └ test\n   └ alb\n   └ asg-rolling-deploy\n   └ hello-world-app\n   └ mysql\nAs an exercise for the reader, I leave it up to you to add lots of examples for the alb,\nasg-rolling-deploy , mysql , and hello-world-app  modules.\nA great practice to follow when developing a new module is to write the example\ncode first, before you write even a line of module code. If you begin with the\nimplementation, it’s too easy to become lost in the implementation details, and by\nthe time you resurface and make it back to the API, you end up with a module that\nis unintuitive and difficult to use. On the other hand, if you begin with the example\ncode, you’re free to think through the ideal user experience and come up with a\nclean API for your module and then work backward to the implementation. Because\nthe example code is the primary way of testing modules anyway, this is a form  of\nTest-Driven Development  (TDD); I’ll dive more into this topic in Chapter 9 , which is\nentirely dedicated to testing.\nIn this section, I’ll focus on creating self-validating modules : that is, modules that can\ncheck their own behavior to prevent certain types of bugs. Terraform has two ways of\ndoing this built in:\n•Validations•\n•Preconditions and postconditions•\nProduction-Grade Infrastructure Modules | 293\nValidations\nAs of Terraform 0.13, you can add validation blocks  to any input variable to perform\nchecks that go beyond basic type constraints. For example, you can add a validation\nblock to the instance_type  variable to ensure not only that the value the user passes\nin is a string (which is enforced by the type  constraint) but that the string has one of\ntwo allowed values from the AWS Free Tier:\nvariable  ""instance_type""  {\n  description  = ""The type of EC2 Instances to run (e.g. t2.micro)""\n  type        = string\n  validation  {\n    condition      = contains ([""t2.micro"", ""t3.micro"" ], var.instance_type )\n    error_message  = ""Only free tier is allowed: t2.micro | t3.micro.""\n  }\n}\nThe way a validation  block works is that the condition  parameter should evaluate\nto true  if the value is valid and false  otherwise. The error_message  parameter\nallows you to specify the message to show the user if they pass in an invalid value. For\nexample, here’s what happens if you try to set instance_type  to m4.large , which is\nnot in the AWS Free Tier:\n$ terraform apply -var instance_type=""m4.large""\n│ Error: Invalid value for variable\n│\n│   on main.tf line 17:\n│    1: variable ""instance_type"" {\n│     ├────────────────\n│     │ var.instance_type is ""m4.large""\n│\n│ Only free tier is allowed: t2.micro | t3.micro.\n│\n│ This was checked by the validation rule at main.tf:21,3-13.\nY ou can have multiple validation  blocks in each variable to check multiple\nconditions:\nvariable  ""min_size""  {\n  description  = ""The minimum number of EC2 Instances in the ASG""\n  type        = number\n  validation  {\n    condition      = var.min_size  > 0\n    error_message  = ""ASGs can't be empty or we'll have an outage!""\n  }\n  validation  {\n    condition      = var.min_size < = 10\n    error_message  = ""ASGs must have 10 or fewer instances to keep costs down.""\n294 | Chapter 8: Production-Grade Terraform Code\n  }\n}\nNote that validation  blocks have a major limitation: the condition  in a validation\nblock can only reference the surrounding input variable. If you try to reference any\nother input variables, local variables, resources, or data sources, you will get an error.\nSo while validation  blocks are useful for basic input sanitization, they can’t be used\nfor anything more complicated: for example, you can’t use them to do checks across\nmultiple variables (such as “exactly one of these two input variables must be set”) or\nany kind of dynamic checks (such as checking that the AMI the user requested uses\nthe x86_64 architecture). To do these sorts of more dynamic checks, you’ll need to\nuse precondition  and postcondition  blocks, as described next.\nPreconditions and postconditions\nAs of Terraform 1.2, you can add precondition  and postcondition  blocks to\nresources, data sources, and output variables to perform more dynamic checks. The\nprecondition  blocks are for catching errors before you run apply . For example, you\ncould use a precondition  block to do a more robust check that the instance_type\nthe user passes in is in the AWS Free Tier. In the previous section, you did this check\nusing a validation  block and a hardcoded list of instance types, but these sorts of\nlists quickly go out of date. Y ou can instead use the instance_type_data  data source\nto always get up-to-date information from AWS:\ndata ""aws_ec2_instance_type"" ""instance""  {\n  instance_type  = var.instance_type\n}\nAnd then you can add a precondition  block to the aws_launch_configuration\nresource to check that this instance type is eligible for the AWS Free Tier:\nresource  ""aws_launch_configuration"" ""example""  {\n  image_id         = var.ami\n  instance_type    = var.instance_type\n  security_groups  = [aws_security_group.instance.id ]\n  user_data        = var.user_data\n  # Required when using a launch configuration with an auto scaling group.\n  lifecycle  {\n    create_before_destroy  = true\n    precondition  {\n      condition      = data.aws_ec2_instance_type.instance.free_tier_eligible\n      error_message  = ""${var.instance_type} is not part of the AWS Free Tier!""\n    }\n  }\n}\nProduction-Grade Infrastructure Modules | 295\nJust like validation  blocks, precondition  blocks (and postcondition  blocks, as\nyou’ll see shortly) include a condition  that must evaluate to true  or false  and an\nerror_message  to show the user if the condition  evaluates to false . If you now try\nto run apply  with an instance type not in the AWS Free Tier, you’ll see your error\nmessage:\n$ terraform apply -var instance_type=""m4.large""\n│ Error: Resource precondition failed\n│\n│   on main.tf line 25, in resource ""aws_launch_configuration"" ""example"":\n│   18:    condition = data.aws_ec2_instance_type.instance.free_tier_eligible\n│     ├────────────────\n│     │ data.aws_ec2_instance_type.instance.free_tier_eligible is false\n│\n│ m4.large is not part of the AWS Free Tier!\nThe postcondition  blocks are for catching errors after you run apply . For example,\nyou can add a postcondition  block to the aws_autoscaling_group  resource to\ncheck that the ASG was deployed across more than one Availability Zone (AZ),\nthereby ensuring you can tolerate the failure of at least one AZ:\nresource  ""aws_autoscaling_group"" ""example""  {\n  name                 = var.cluster_name\n  launch_configuration  = aws_launch_configuration.example.name\n  vpc_zone_identifier   = var.subnet_ids\n  lifecycle  {\n    postcondition  {\n      condition      = length(self.availability_zones ) > 1\n      error_message  = ""You must use more than one AZ for high availability!""\n    }\n  }\n  # (...)\n}\nNote the use of the self  keyword in the condition  parameter. Self expressions  use the\nfollowing syntax:\nself.<ATTRIBUTE>\nY ou can use this special syntax solely in postcondition , connection , and provi\nsioner  blocks (you’ll see examples of the latter two later in this chapter) to refer to\nan output ATTRIBUTE  of the surrounding resource. If you tried to use the standard\naws_autoscaling_group.example.<ATTRIBUTE>  syntax, you’ d get a circular depend‐\nency error, as resources can’t have references to themselves, so the self expression is a\nworkaround added specifically for this sort of use case.\nIf you run apply  on this module, Terraform will deploy the module, but after, if it\nturns out that the subnets the user passed in via the subnet_ids  input variable were\n296 | Chapter 8: Production-Grade Terraform Code\nall in the same AZ, the postcondition  block will show an error. This way, you’ll\nalways be warned if your ASG isn’t configured for high availability.\nWhen to use validations, preconditions, and postconditions\nAs you can see, validation , precondition , and postcondition  blocks are all similar,\nso when should you use each one?\nUse validation  blocks for basic input sanitization\nUse validation  blocks in all of your production-grade modules to prevent users\nfrom passing invalid variables into your modules. The goal is to catch basic input\nerrors before  any changes have been deployed. Although precondition  blocks\nare more powerful, you should still use validation  blocks for checking variables\nwhenever possible, as validation  blocks are defined with the variables they\nvalidate, which leads to a more readable and maintainable API.\nUse precondition  blocks for checking basic assumptions\nUse precondition  blocks in all of your production-grade modules to check\nassumptions that must be true before  any changes have been deployed. This\nincludes any checks on variables you can’t do with validation  blocks (such as\nchecks that reference multiple variables or data sources) as well as checks on\nresources and data sources. The goal is to catch as many errors as early as you\ncan, before those errors can do any damage.\nUse postcondition  blocks for enforcing basic guarantees\nUse postcondition  blocks in all of your production-grade modules to check\nguarantees about how your module behaves after  changes have been deployed.\nThe goal is to give users of your module confidence that your module will\neither do what it says when they run apply  or exit with an error. It also gives\nmaintainers of that module a clearer signal of what behaviors you want this\nmodule to enforce, so those aren’t accidentally lost during a refactor.\nUse automated testing tools for enforcing more advanced assumptions and guarantees\nvalidation , precondition , and postcondition  blocks are all useful tools, but\nthey can only do basic checks. This is because you can only use data sources,\nresources, and language constructs built into Terraform to do these checks, and\nthose are often not enough for more advanced behavior. For example, if you\nbuilt a module to deploy a web service, you might want to add a check after\ndeployment that the web service is able to respond to HTTP requests. Y ou could\ntry to do this in a postcondition  block by making HTTP requests to the service\nusing Terraform’s http provider , but most deployments happen asynchronously,\nso you may need to retry the HTTP request multiple times, and there is no\nretry mechanism built into that provider. Moreover, if you deployed an internal\nweb service, it might not be accessible over the public internet, so you’ d need\nProduction-Grade Infrastructure Modules | 297",14906
84-Versioned Modules.pdf,84-Versioned Modules,"to connect to some internal network or VPN first, which is also tricky to do in\npure Terraform code. Therefore, to do more robust checks, you’ll want to use\nautomated testing tools such as OPA and Terratest, both of which you’ll see in\nChapter 9 .\nVersioned Modules\nThere are two types of versioning you’ll want to think through with modules:\n•Versioning of the module’s dependencies•\n•Versioning of the module itself•\nLet’s start with versioning of the module’s dependencies. Y our Terraform code has\nthree types of dependencies:\nTerraform core\nThe version of the terraform  binary you depend on\nProviders\nThe version of each provider your code depends on, such as the aws provider\nModules\nThe version of each module you depend on that are pulled in via module  blocks\nAs a general rule, you’ll want to practice versioning pinning  with all of your dependen‐\ncies. That means that you pin each of these three types of dependencies to a specific,\nfixed, known version. Deployments should be predictable and repeatable: if the code\ndidn’t change, then running apply  should always produce the same result, whether\nyou run it today or three months from now or three years from now. To make that\nhappen, you need to avoid pulling in new versions of dependencies accidentally.\nInstead, version upgrades should always be an explicit, deliberate action that is visible\nin the code you check into version control.\nLet’s go through how to do version pinning for the three types of Terraform\ndependencies.\nTo pin the version of the first type of dependency, your Terraform core version, you\ncan use the required_version  argument in your code. At a bare minimum, you\nshould require a specific major version of Terraform:\nterraform  {\n  # Require any 1.x version of Terraform\n  required_version  = "">= 1.0.0, < 2.0.0""\n}\nThis is critical, because each major release of Terraform is backward incompatible:\ne.g., the upgrade from 1.0.0 to 2.0.0 will likely include breaking changes, so you don’t\n298 | Chapter 8: Production-Grade Terraform Code\nwant to do it by accident. The preceding code will allow you to use only 1.x.x versions\nof Terraform with that module, so 1.0.0 and 1.2.3 will work, but if you try to use,\nperhaps accidentally, 0.14.3 or 2.0.0, and run terraform apply , you immediately get\nan error:\n$ terraform apply\nError: Unsupported Terraform Core version\nThis configuration does not support Terraform version 0.14.3. To proceed,\neither choose another supported Terraform version or update the root module's\nversion constraint. Version constraints are normally set for good reason, so\nupdating the constraint may lead to other errors or unexpected behavior.\nFor production-grade code, you may want to pin not only the major version but the\nminor and patch version too:\nterraform  {\n  # Require Terraform at exactly version 1.2.3\n  required_version  = ""1.2.3""\n}\nIn the past, before the Terraform 1.0.0 release, this was absolutely required, as every\nrelease of Terraform potentially included backward-incompatible changes, including\nto the state file format: e.g., a state file written by Terraform version 0.12.1 could not\nbe read by Terraform version 0.12.0. Fortunately, after the 1.0.0 release, this is no\nlonger the case: as per the officially published Terraform v1.0 Compatibility Promises ,\nupgrades between v1.x releases should require no changes to your code or workflows.\nThat said, you might still not want to upgrade to a new version of Terraform acciden‐\ntally. New versions introduce new features, and if some of your computers (developer\nworkstations and CI servers) start using those features but others are still on the old\nversions, you’ll run into issues. Moreover, new versions of Terraform may have bugs,\nand you’ll want to test that out in pre-production environments before trying it in\nproduction. Therefore, while pinning the major version is the bare minimum, I also\nrecommend pinning the minor and patch version and applying Terraform upgrades\nintentionally, carefully, and consistently throughout each environment.\nNote that, occasionally, you may have to use different versions of Terraform within\na single codebase. For example, perhaps you are testing out Terraform 1.2.3 in the\nstage environment, while the prod environment is still on Terraform 1.0.0. Having to\ndeal with multiple Terraform versions, whether on your own computer or on your CI\nservers, can be tricky. Fortunately, the open source tool tfenv , the Terraform version\nmanager, makes this much easier.\nAt its most basic level, you can use tfenv  to install and switch between multiple\nversions of Terraform. For example, you can use the tfenv install  command to\ninstall a specific version of Terraform:\nProduction-Grade Infrastructure Modules | 299\n$ tfenv install 1.2.3\nInstalling Terraform v1.2.3\nDownloading release tarball from\nhttps://releases.hashicorp.com/terraform/1.2.3/terraform_1.2.3_darwin_amd64.zip\nArchive:  tfenv_download.ZUS3Qn/terraform_1.2.3_darwin_amd64.zip\n  inflating: /opt/homebrew/Cellar/tfenv/2.2.2/versions/1.2.3/terraform\nInstallation of terraform v1.2.3 successful.\ntfenv on Apple Silicon (M1, M2)\nAs of June 2022, tfenv  did not install the proper version of Terra‐\nform on Apple Silicon, such as Macs running M1 or M2 processors\n(see this open issue for details ). The workaround is to set the\nTFENV_ARCH  environment variable to arm64 :\n$ export TFENV_ARCH=arm64\n$ tfenv install 1.2.3\nY ou can list the versions you have installed using the list  command:\n$ tfenv list\n  1.2.3\n  1.1.4\n  1.1.0\n* 1.0.0 (set by /opt/homebrew/Cellar/tfenv/2.2.2/version)\nAnd you can select the version of Terraform to use from that list using the use\ncommand:\n$ tfenv use 1.2.3\nSwitching default version to v1.2.3\nSwitching completed\nThese commands are all handy for working with multiple versions of Terraform,\nbut the real power of tfenv  is its support for .terraform-version  files. tfenv  will\nautomatically look for a .terraform-version  file in the current folder, as well as all\nthe parent folders, all the way up to the project root—that is, the version control\nroot (e.g., the folder with a .git folder in it)—and if it finds that file, any terraform\ncommand you run will automatically use the version defined in that file.\nFor example, if you wanted to try out Terraform 1.2.3 in the stage environment, while\nsticking with Terraform 1.0.0 in the prod environment, you could use the following\nfolder structure:\nlive\n └ stage\n   └ vpc\n   └ mysql\n   └ frontend-app\n   └ .terraform-version\n └ prod\n300 | Chapter 8: Production-Grade Terraform Code\n   └ vpc\n   └ mysql\n   └ frontend-app\n   └ .terraform-version\nInside of live/stage/.terraform-version , you would have the following:\n1.2.3\nAnd inside of live/prod/.terraform-version , you would have the following:\n1.0.0\nNow, any terraform  command you run in stage  or any subfolder will automati‐\ncally use Terraform 1.2.3. Y ou can check this by running the terraform version\ncommand:\n$ cd stage/vpc\n$ terraform version\nTerraform v1.2.3\nAnd similarly, any terraform  command you run in prod  will automatically use\nTerraform 1.0.0:\n$ cd prod/vpc\n$ terraform version\nTerraform v1.0.0\nThis works automatically on any developer workstation and in your CI server so long\nas everyone has tfenv  installed. If you’re a Terragrunt user, tgswitch  offers similar\nfunctionality to automatically pick the Terragrunt version based on a .terragrunt-\nversion  file.\nLet’s now turn our attention to the second type of dependency in your Terraform\ncode: providers. As you saw in Chapter 7 , to pin provider versions, you can use the\nrequired_providers  block :\nterraform  {\n  required_version  = "">= 1.0.0, < 2.0.0""\n  required_providers  {\n    aws = {\n      source   = ""hashicorp/aws""\n      version  = ""~> 4.0""\n    }\n  }\n}\nThis code pins the AWS Provider code to any 4.x version (the ~> 4.0  syntax is\nequivalent to >= 4.0, < 5.0 ). Again, the bare minimum is to pin to a specific major\nversion number to avoid accidentally pulling in backward-incompatible changes.\nWith Terraform 0.14.0 and above, you don’t need to pin minor or patch versions\nProduction-Grade Infrastructure Modules | 301\nfor providers, as this happens automatically due to the lock file. The first time you\nrun terraform init , Terraform creates a .terraform.lock.hcl  file, which records the\nfollowing information:\nThe exact version of each provider you used\nIf you check the .terraform.lock.hcl  file into version control (which you should!),\nthen in the future, if you run terraform init  again, on this computer or\nany other, Terraform will download the exact  same version of each provider.\nThat’s why you don’t need to pin the minor and patch version number in the\nrequired_providers  block, as that’s the default behavior anyway. If you want to\nexplicitly upgrade a provider version, you can update the version constraint in\nthe required_providers  block and run terraform init -upgrade . Terraform\nwill download new providers that match your version constraints and update\nthe .terraform.lock.hcl  file; you should review those updates and commit them to\nversion control.\nThe checksums for each provider\nTerraform records the checksum of each provider it downloads, and on subse‐\nquent runs of terraform init , it will show an error if the checksum changed.\nThis is a security measure to ensure someone can’t swap out provider code with\nmalicious code in the future. If the provider is cryptographically signed (most\nofficial HashiCorp providers are), Terraform will also validate the signature as an\nadditional check that the code can be trusted.\nLock Files with Multiple Operating Systems\nBy default, Terraform only records checksums for the platform\nyou ran init  on: for example, if you ran init  on Linux, then Terra‐\nform will only record the checksums for Linux provider binaries\nin .terraform.lock.hcl . If you check that file in and, later on, you\nrun init  on that code on a Mac, you’ll get an error, as the Mac\nchecksums won’t be in the .terraform.lock.hcl  file. If your team\nworks across multiple operating systems, you’ll need to run the\nterraform providers lock  command to record the checksums\nfor every platform you use:\nterraform providers lock \\n  -platform=windows_amd64 \ # 64-bit Windows\n  -platform=darwin_amd64 \  # 64-bit macOS\n  -platform=darwin_arm64 \  # 64-bit macOS (ARM)\n  -platform=linux_amd64     # 64-bit Linux\nFinally, let’s now look at the third type of dependencies: modules. As discussed in\n“Module Versioning” on page 133, I strongly recommend pinning module versions\n302 | Chapter 8: Production-Grade Terraform Code\nby using source  URLs (rather than local file paths) with the ref parameter set to a\nGit tag:\n  source  = ""git@github.com:foo/modules.git//services/hello-world-app?ref =v0.0.5""\nIf you use these sorts of URLs, Terraform will always download the exact same code\nfor the module every time you run terraform init .\nNow that you’ve seen how to version your code’s dependencies, let’s talk about how\nto version the code itself. As you saw in “Module Versioning”  on page 133, you can\nversion your code by using Git tags with semantic versioning:\n$ git tag -a ""v0.0.5"" -m ""Create new hello-world-app module""\n$ git push --follow-tags\nFor example, to deploy version v0.0.5  of your hello-world-app  module in the\nstaging environment, put the following code into live/stage/services/hello-world-app/\nmain.tf :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""hello_world_app""  {\n  # TODO: replace this with your own module URL and version!!\n  source  = ""git@github.com:foo/modules.git//services/hello-world-app?ref =v0.0.5""\n  server_text             = ""New server text""\n  environment             = ""stage""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\n  db_remote_state_key     = ""stage/data-stores/mysql/terraform.tfstate""\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\ndata ""aws_ami"" ""ubuntu""  {\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\nNext, pass through the ALB DNS name as an output in live/stage/services/hello-world-\napp/outputs.tf :\nProduction-Grade Infrastructure Modules | 303\n6Y ou can find the full details on publishing modules on the Terraform website .output ""alb_dns_name""  {\n  value       = module.hello_world_app .alb_dns_name\n  description  = ""The domain name of the load balancer""\n}\nNow you can deploy your versioned module by running terraform init  and terra\nform apply :\n$ terraform apply\n(...)\nApply complete! Resources: 13 added, 0 changed, 0 destroyed.\nOutputs:\nalb_dns_name = ""hello-world-stage-477699288.us-east-2.elb.amazonaws.com""\nIf that works well, you can then deploy the exact same version—and therefore\nthe exact same code—to other environments, including production. If you ever\nencounter an issue, versioning also gives you the option to roll back by deploying an\nolder version.\nAnother option for releasing modules is to publish them in the Terraform Reg‐\nistry. The Public Terraform Registry  includes hundreds of reusable, community-\nmaintained, open source modules for AWS, Google Cloud, Azure, and many other\nproviders. There are a few requirements to publish a module to the Public Terraform\nRegistry:6\n•The module must live in a public GitHub repo.•\n•The repo must be named terraform-<PROVIDER>-<NAME> , where PROVIDER  is the •\nprovider the module is targeting (e.g., aws) and NAME  is the name of the module\n(e.g., rds).\n•The module must follow a specific file structure, including defining Terraform•\ncode in the root of the repo, providing a README.md , and using the convention\nof main.tf , variables.tf , and outputs.tf  as filenames.\n•The repo must use Git tags with semantic versioning ( x.y.z ) for releases. •\nIf your module meets those requirements, you can share it with the world by logging\nin to the Terraform Registry with your GitHub account and using the web UI to\npublish the module. Once your modules are in the Registry, your team can use a web\nUI to discover modules and learn how to use them.\n304 | Chapter 8: Production-Grade Terraform Code",14517
85-Beyond Terraform Modules.pdf,85-Beyond Terraform Modules,"Terraform even supports a special syntax for consuming modules from the Terraform\nRegistry. Instead of long Git URLs with hard-to-spot ref parameters, you can use a\nspecial shorter registry URL in the source  argument and specify the version via a\nseparate version  argument using the following syntax:\nmodule ""<NAME>""  {\n  source   = ""<OWNER>/<REPO>/<PROVIDER>""\n  version  = ""<VERSION>""\n  # (...)\n}\nwhere NAME  is the identifier to use for the module in your Terraform code, OWNER\nis the owner of the GitHub repo (e.g., in github.com/foo/bar , the owner is foo),\nREPO  is the name of the GitHub repo (e.g., in github.com/foo/bar , the repo is bar),\nPROVIDER  is the provider you’re targeting (e.g., aws), and VERSION  is the version of the\nmodule to use. Here’s an example of how to use an open source RDS module from the\nTerraform Registry:\nmodule ""rds"" {\n  source   = ""terraform-aws-modules/rds/aws""\n  version  = ""4.4.0""\n  # (...)\n}\nIf you are a customer of HashiCorp’s Terraform Cloud or Terraform Enterprise, you\ncan have this same experience with a Private Terraform Registry—that is, a registry\nthat lives in your private Git repos and is only accessible to your team. This can be a\ngreat way to share modules within your company.\nBeyond Terraform Modules\nAlthough this book is all about Terraform, to build out your entire production-grade\ninfrastructure, you’ll need to use other tools, too, such as Docker, Packer, Chef,\nPuppet, and, of course, the duct tape, glue, and work horse of the DevOps world, the\ntrusty Bash script.\nMost of this code can reside in the modules  folder directly alongside your Terraform\ncode: e.g., you might have a modules/packer  folder that contains a Packer template\nand some Bash scripts you use to configure an AMI right next to the modules/asg-\nrolling-deploy  Terraform module you use to deploy that AMI.\nHowever, occasionally, you need to go further and run some non-Terraform code\n(e.g., a script) directly from a Terraform module. Sometimes, this is to integrate\nTerraform with another system (e.g., you’ve already used Terraform to configure User\nData scripts for execution on EC2 Instances); other times, it’s to work around a\nlimitation of Terraform, such as a missing provider API, or the inability to implement\nProduction-Grade Infrastructure Modules | 305\n7Y ou can find the full list of provisioners on the Terraform website .complicated logic due to Terraform’s declarative nature. If you search around, you can\nfind a few “escape hatches” within Terraform that make this possible:\n•Provisioners•\n•Provisioners with null_resource •\n•External data source•\nLet’s go through these one a time.\nProvisioners\nTerraform provisioners  are used to execute scripts either on the local machine or a\nremote machine when you run Terraform, typically to do the work of bootstrapping,\nconfiguration management, or cleanup. There are several different kinds of provi‐\nsioners, including local-exec  (execute a script on the local machine), remote-exec\n(execute a script on a remote resource), and file  (copy files to a remote resource).7\nY ou can add provisioners to a resource by using a provisioner  block. For example,\nhere is how you can use the local-exec  provisioner to execute a script on your local\nmachine:\nresource  ""aws_instance"" ""example""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n  provisioner  ""local-exec""  {\n    command  = ""echo \""Hello, World from $(uname -smp)\""""\n  }\n}\nWhen you run terraform apply  on this code, it prints “Hello, World from” and then\nthe local operating system details using the uname  command:\n$ terraform apply\n(...)\naws_instance.example (local-exec): Hello, World from Darwin x86_64 i386\n(...)\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n306 | Chapter 8: Production-Grade Terraform Code\nTrying out a remote-exec  provisioner is a little more complicated. To execute code\non a remote resource, such as an EC2 Instance, your Terraform client must be able to\ndo the following:\nCommunicate with the EC2 Instance over the network\nY ou already know how to allow this with a security group.\nAuthenticate to the EC2 Instance\nThe remote-exec  provisioner supports SSH and WinRM connections.\nSince the examples in this book have you launch Linux (Ubuntu) EC2 Instances,\nyou’ll want to use SSH authentication. And that means you’ll need to configure SSH\nkeys. Let’s begin by creating a security group that allows inbound connections to port\n22, the default port for SSH:\nresource  ""aws_security_group"" ""instance""  {\n  ingress {\n    from_port  = 22\n    to_port    = 22\n    protocol   = ""tcp""\n    # To make this example easy to try out, we allow all SSH connections.\n    # In real world usage, you should lock this down to solely trusted IPs.\n    cidr_blocks  = [""0.0.0.0/0"" ]\n  }\n}\nWith SSH keys, the normal process would be for you to generate an SSH key pair on\nyour computer, upload the public key to AWS, and store the private key somewhere\nsecure where your Terraform code can access it. However, to make it easier for you\nto try out this code, you can use a resource called tls_private_key  to automatically\ngenerate a private key:\n# To make this example easy to try out, we generate a private key in Terraform.\n# In real-world usage, you should manage SSH keys outside of Terraform.\nresource  ""tls_private_key"" ""example""  {\n  algorithm  = ""RSA""\n  rsa_bits   = 4096\n}\nThis private key is stored in Terraform state, which is not great for production use\ncases but is fine for this learning exercise. Next, upload the public key to AWS using\nthe aws_key_pair  resource:\nresource  ""aws_key_pair"" ""generated_key""  {\n  public_key  = tls_private_key.example.public_key_openssh\n}\nProduction-Grade Infrastructure Modules | 307\nFinally, let’s begin writing the code for the EC2 Instance:\ndata ""aws_ami"" ""ubuntu""  {\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\nresource  ""aws_instance"" ""example""  {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type           = ""t2.micro""\n  vpc_security_group_ids  = [aws_security_group.instance.id ]\n  key_name                = aws_key_pair.generated_key.key_name\n}\nJust about all of this code should be familiar to you: it’s using the aws_ami  data source\nto find Ubuntu AMI and using the aws_instance  resource to deploy that AMI on\na t2.micro  instance, associating that instance with the security group you created\nearlier. The only new item is the use of the key_name  attribute in the aws_instance\nresource to instruct AWS to associate your public key with this EC2 Instance. AWS\nwill add that public key to the server’s authorized_keys  file, which will allow you to\nSSH to that server with the corresponding private key.\nNext, add the remote-exec  provisioner to the aws_instance  resource:\nresource  ""aws_instance"" ""example""  {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type           = ""t2.micro""\n  vpc_security_group_ids  = [aws_security_group.instance.id ]\n  key_name                = aws_key_pair.generated_key.key_name\n  provisioner  ""remote-exec""  {\n    inline  = [""echo \""Hello, World from $(uname -smp)\"""" ]\n  }\n}\nThis looks nearly identical to the local-exec  provisioner, except you use an inline\nargument to pass a list of commands to execute, instead of a single command  argu‐\nment. Finally, you need to configure Terraform to use SSH to connect to this\nEC2 Instance when running the remote-exec  provisioner. Y ou do this by using a\nconnection  block:\nresource  ""aws_instance"" ""example""  {\n  ami                    = data.aws_ami.ubuntu.id\n  instance_type           = ""t2.micro""\n  vpc_security_group_ids  = [aws_security_group.instance.id ]\n  key_name                = aws_key_pair.generated_key.key_name\n308 | Chapter 8: Production-Grade Terraform Code\n  provisioner  ""remote-exec""  {\n    inline  = [""echo \""Hello, World from $(uname -smp)\"""" ]\n  }\n  connection  {\n    type         = ""ssh""\n    host         = self.public_ip\n    user         = ""ubuntu""\n    private_key  = tls_private_key.example.private_key_pem\n  }\n}\nThis connection  block tells Terraform to connect to the EC2 Instance’s public IP\naddress using SSH with ""ubuntu""  as the username (this is the default username\nfor the root user on Ubuntu AMIs) and the autogenerated private key. If you run\nterraform apply  on this code, you’ll see the following:\n$ terraform apply\n(...)\naws_instance.example: Creating...\naws_instance.example: Still creating... [10s elapsed]\naws_instance.example: Still creating... [20s elapsed]\naws_instance.example: Provisioning with 'remote-exec'...\naws_instance.example (remote-exec): Connecting to remote host via SSH...\naws_instance.example (remote-exec): Connecting to remote host via SSH...\naws_instance.example (remote-exec): Connecting to remote host via SSH...\n(... repeats a few more times ...)\naws_instance.example (remote-exec): Connecting to remote host via SSH...\naws_instance.example (remote-exec): Connected!\naws_instance.example (remote-exec): Hello, World from Linux x86_64 x86_64\nApply complete! Resources: 4 added, 0 changed, 0 destroyed.\nThe remote-exec  provisioner doesn’t know exactly when the EC2 Instance will be\nbooted and ready to accept connections, so it will retry the SSH connection multiple\ntimes until it succeeds or hits a timeout (the default timeout is five minutes, but you\ncan configure it). Eventually, the connection succeeds, and you get a “Hello, World”\nfrom the server.\nNote that, by default, when you specify a provisioner, it is a creation-time provisioner ,\nwhich means that it runs (a) during terraform apply , and (b) only during the\ninitial creation of a resource. The provisioner will not run on any subsequent calls to\nterraform apply , so creation-time provisioners are mainly useful for running initial\nbootstrap code. If you set the when = destroy  argument on a provisioner, it will be  a\nProduction-Grade Infrastructure Modules | 309\ndestroy-time provisioner , which will run after you run terraform destroy , just before\nthe resource is deleted.\nY ou can specify multiple provisioners on the same resource and Terraform will run\nthem one at a time, in order, from top to bottom. Y ou can use the on_failure\nargument to instruct Terraform how to handle errors from the provisioner: if set to\n""continue"" , Terraform will ignore the error and continue with resource creation or\ndestruction; if set to ""abort"" , Terraform will abort the creation or destruction.\nProvisioners Versus User Data\nY ou’ve now seen two different ways to execute scripts on a server using Terraform:\none is to use a remote-exec  provisioner, and the other is to use a User Data script. I’ve\ngenerally found User Data to be the more useful tool for the following reasons:\n•A remote-exec  provisioner requires that you open up SSH or WinRM access to •\nyour servers, which is more complicated to manage (as you saw earlier with all\nthe security group and SSH key work) and less secure than User Data, which\nsolely requires AWS API access (which you must have anyway when using\nTerraform to deploy to AWS).\n•Y ou can use User Data scripts with ASGs, ensuring that all servers in that ASG•\nexecute the script during boot, including servers launched due to an auto scaling\nor auto recovery event. Provisioners take effect only while Terraform is running\nand don’t work with ASGs at all.\n•The User Data script can be seen in the EC2 console (select an Instance, click•\nActions → Instance Settings → View/Change User Data), and you can find its\nexecution log on the EC2 Instance itself (typically in /var/log/cloud-init*.log ),\nboth of which are useful for debugging and neither of which is available with\nprovisioners.\nThe only real advantage to using a provisioner to execute code on an EC2 Instance\nis that User Data scripts are limited to a length of 16 KB, whereas provisioner scripts\ncan be arbitrarily long.\nProvisioners with null_resource\nProvisioners can be defined only within a resource, but sometimes, you want to exe‐\ncute a provisioner without tying it to a specific resource. Y ou can do this using some‐\nthing called the null_resource , which acts just like a normal Terraform resource,\nexcept that it doesn’t create anything. By defining provisioners on the null_resource ,\nyou can run your scripts as part of the Terraform lifecycle but without being attached\nto any “real” resource:\n310 | Chapter 8: Production-Grade Terraform Code\nresource  ""null_resource"" ""example""  {\n  provisioner  ""local-exec""  {\n    command  = ""echo \""Hello, World from $(uname -smp)\""""\n  }\n}\nThe null_resource  even has a handy argument called triggers , which takes in a\nmap of keys and values. Whenever the values change, the null_resource  will be\nre-created, therefore forcing any provisioners within it to be reexecuted. For example,\nif you want to execute a provisioner within a null_resource  every single time you\nrun terraform apply , you could use the uuid()  built-in function, which returns a\nnew, randomly generated UUID each time it’s called, within the triggers  argument:\nresource  ""null_resource"" ""example""  {\n  # Use UUID to force this null_resource to be recreated on every\n  # call to 'terraform apply'\n  triggers  = {\n    uuid  = uuid()\n  }\n  provisioner  ""local-exec""  {\n    command  = ""echo \""Hello, World from $(uname -smp)\""""\n  }\n}\nNow, every time you call terraform apply , the local-exec  provisioner will execute:\n$ terraform apply\n(...)\nnull_resource.example (local-exec): Hello, World from Darwin x86_64 i386\n$ terraform apply\nnull_resource.example (local-exec): Hello, World from Darwin x86_64 i386\nExternal data source\nProvisioners will typically be your go-to for executing scripts from Terraform, but\nthey aren’t always the correct fit. Sometimes, what you’re really looking to do is\nexecute a script to fetch some data and make that data available within the Terraform\ncode itself. To do this, you can use the external  data source, which allows an external\ncommand that implements a specific protocol to act as a data source.\nThe protocol is as follows:\n•Y ou can pass data from Terraform to the external program using the query •\nargument of the external  data source. The external program can read in these\narguments as JSON from stdin.\nProduction-Grade Infrastructure Modules | 311\n•The external program can pass data back to Terraform by writing JSON to•\nstdout. The rest of your Terraform code can then pull data out of this JSON by\nusing the result  output attribute of the external data source.\nHere’s an example:\ndata ""external"" ""echo""  {\n  program  = [""bash"", ""-c"", ""cat /dev/stdin"" ]\n  query = {\n    foo = ""bar""\n  }\n}\noutput ""echo"" {\n  value = data.external.echo.result\n}\noutput ""echo_foo""  {\n  value = data.external.echo.result.foo\n}\nThis example uses the external  data source to execute a Bash script that echoes\nback to stdout any data it receives on stdin. Therefore, any data you pass in via the\nquery  argument should come back as is via the result  output attribute. Here’s what\nhappens when you run terraform apply  on this code:\n$ terraform apply\n(...)\nApply complete! Resources: 0 added, 0 changed, 0 destroyed.\nOutputs:\necho = {\n  ""foo"" = ""bar""\n}\necho_foo = ""bar""\nY ou can see that data.external.<NAME>.result  contains the JSON returned by\nthe external program and that you can navigate within that JSON using the syntax\ndata.external.<NAME>.result.<PATH>  (e.g., data.external.echo.result.foo ).\nThe external  data source is a lovely escape hatch if you need to access data in your\nTerraform code and there’s no existing data source that knows how to retrieve that\ndata. However, be conservative with your use of external  data sources and all of the\nother Terraform “escape hatches, ” since they make your code less portable and more\nbrittle. For example, the external  data source code you just saw relies on Bash, which\nmeans you won’t be able to deploy that Terraform module from Windows.\n312 | Chapter 8: Production-Grade Terraform Code",16346
86-Conclusion.pdf,86-Conclusion,"Conclusion\nNow that you’ve seen all of the ingredients of creating production-grade Terraform\ncode, it’s time to put them together. The next time you begin to work on a new\nmodule, use the following process:\n1.Go through the production-grade infrastructure checklist in Table 8-2 , and 1.\nexplicitly identify the items you’ll be implementing and the items you’ll be\nskipping. Use the results of this checklist, plus Table 8-1 , to come up with a time\nestimate for your boss.\n2.Create an examples  folder, and write the example code first, using it to define the 2.\nbest user experience and cleanest API you can think of for your modules. Create\nan example for each important permutation of your module, and include enough\ndocumentation and reasonable defaults to make the example as easy to deploy as\npossible.\n3.Create a modules  folder, and implement the API you came up with as a collection 3.\nof small, reusable, composable modules. Use a combination of Terraform and\nother tools like Docker, Packer, and Bash to implement these modules. Make sure\nto pin the versions for all your dependencies, including Terraform core, your\nTerraform providers, and Terraform modules you depend on.\n4.Create a test folder, and write automated tests for each example. 4.\nThat last bullet point—writing automated tests for your infrastructure code—is what\nwe’ll focus on next, as we move on to Chapter 9 .\nConclusion | 313",1430
87-Manual Testing Basics.pdf,87-Manual Testing Basics,"CHAPTER 9\nHow to Test Terraform Code\nThe DevOps world is full of fear: fear of downtime, fear of data loss, fear of security\nbreaches. Every time you go to make a change, you’re always wondering, what will\nthis affect? Will it work the same way in every environment? Will this cause another\noutage? And if there is an outage, how late into the night will you need to stay up to\nfix it this time? As companies grow, there is more and more at stake, which makes the\ndeployment process even scarier, and even more error prone. Many companies try\nto mitigate this risk by doing deployments less frequently, but the result is that each\ndeployment is larger and actually more prone to breakage.\nIf you manage your infrastructure as code, you have a better way to mitigate risk:\ntests. The goal of testing is to give you the confidence to make changes. The key\nword here is confidence : no form of testing can guarantee that your code is free of\nbugs, so it’s more of a game of probability. If you can capture all of your infrastruc‐\nture and deployment processes as code, you can test that code in a pre-production\nenvironment, and if it works there, there’s a high probability that when you use the\nexact same code in production, it will work there, too. And in a world of fear and\nuncertainty, high probability and high confidence go a long way.\nIn this chapter, I’ll go over the process of testing infrastructure code, including both\nmanual testing and automated testing, with the bulk of the chapter spent on the latter:\n•Manual tests•\n—Manual testing basics—\n—Cleaning up after tests—\n•Automated tests•\n—Unit tests—\n—Integration tests—\n315\n—End-to-end tests—\n—Other testing approaches—\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\nManual Tests\nWhen  thinking about how to test Terraform code, it can be helpful to draw some\nparallels with how you would test code written in a general-purpose programming\nlanguage such as Ruby. Let’s say you were writing a simple web server in Ruby in\nweb-server.rb :\nclass WebServer  < WEBrick::HTTPServlet ::AbstractServlet\n  def do_GET(request, response )\n    case request.path\n    when ""/""\n      response .status = 200\n      response ['Content-Type' ] = 'text/plain'\n      response .body = 'Hello, World'\n    when ""/api""\n      response .status = 201\n      response ['Content-Type' ] = 'application/json'\n      response .body = '{""foo"":""bar""}'\n    else\n      response .status = 404\n      response ['Content-Type' ] = 'text/plain'\n      response .body = 'Not Found'\n    end\n  end\nend\nThis code will send a 200 response with the body “Hello, World” for the / URL, a\n201 response with a JSON body for the /api  URL, and a 404 for all other URLs. How\nwould you manually test this code? The typical answer is to add a bit of code to run\nthe web server on localhost:\n# This will only run if this script was called directly from the CLI, but\n# not if it was required from another file\nif __FILE__  == $0\n  # Run the server on localhost at port 8000\n  server = WEBrick::HTTPServer .new :Port => 8000\n  server.mount '/', WebServer\n  # Shut down the server on CTRL+C\n316 | Chapter 9: How to Test Terraform Code\n  trap 'INT' do server.shutdown  end\n  # Start the server\n  server.start\nend\nWhen you run this file from the CLI, it will start the web server on port 8000:\n$ ruby web-server.rb\n[2019-05-25 14:11:52] INFO  WEBrick 1.3.1\n[2019-05-25 14:11:52] INFO  ruby 2.3.7 (2018-03-28) [universal.x86_64-darwin17]\n[2019-05-25 14:11:52] INFO  WEBrick::HTTPServer#start: pid=19767 port=8000\nY ou can test this server using a web browser or curl :\n$ curl localhost:8000/\nHello, World\nManual Testing Basics\nWhat is the equivalent of this sort of manual testing with Terraform code? For\nexample, from the previous chapters, you already have Terraform code for deploying\nan ALB. Here’s a snippet from modules/networking/alb/main.tf :\nresource  ""aws_lb"" ""example""  {\n  name               = var.alb_name\n  load_balancer_type  = ""application""\n  subnets             = var.subnet_ids\n  security_groups     = [aws_security_group.alb.id ]\n}\nresource  ""aws_lb_listener"" ""http""  {\n  load_balancer_arn  = aws_lb.example.arn\n  port              = local.http_port\n  protocol           = ""HTTP""\n  # By default, return a simple 404 page\n  default_action  {\n    type  = ""fixed-response""\n    fixed_response  {\n      content_type  = ""text/plain""\n      message_body  = ""404: page not found""\n      status_code   = 404\n    }\n  }\n}\nresource  ""aws_security_group"" ""alb""  {\n  name = var.alb_name\n}\n# (...)\nManual Tests | 317\nIf you compare this code to the Ruby code, one difference should be fairly obvious:\nyou can’t deploy AWS ALBs, target groups, listeners, security groups, and all the other\ninfrastructure on your own computer.\nThis brings us to key testing takeaway #1 : when testing Terraform code, you can’t use\nlocalhost. This applies to most IaC tools, not just Terraform. The only practical way\nto do manual testing with Terraform is to deploy to a real environment (i.e., deploy\nto AWS). In other words, the way you’ve been manually running terraform apply\nand terraform destroy  throughout the book is how you do manual testing with\nTerraform.\nThis is one of the reasons why it’s essential to have easy-to-deploy examples in\nthe examples  folder for each module, as described in Chapter 8 . The easiest way\nto manually test the alb module is to use the example code you created for it in\nexamples/alb :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""alb"" {\n  source  = ""../../modules/networking/alb""\n  alb_name    = ""terraform-up-and-running""\n  subnet_ids  = data.aws_subnets.default.ids\n}\nAs you’ve done many times throughout the book, you deploy this example code using\nterraform apply :\n$ terraform apply\n(...)\nApply complete! Resources: 5 added, 0 changed, 0 destroyed.\nOutputs:\nalb_dns_name = ""hello-world-stage-477699288.us-east-2.elb.amazonaws.com""\nWhen the deployment is done, you can use a tool such as curl  to test, for example,\nthat the default action of the ALB is to return a 404:\n$ curl \\n  -s \\n  -o /dev/null \\n  -w ""%{http_code}"" \\n  hello-world-stage-477699288.us-east-2.elb.amazonaws.com\n404\n318 | Chapter 9: How to Test Terraform Code",6356
88-Unit Tests.pdf,88-Unit Tests,"1AWS doesn’t charge anything extra for additional AWS accounts, and if you use AWS Organizations, you can\ncreate multiple “child” accounts that all roll up their billing to a single root account, as you saw in Chapter 7 .\nValidating Infrastructure\nThe examples in this chapter use curl  and HTTP requests to vali‐\ndate that the infrastructure is working, because the infrastructure\nyou’re testing includes a load balancer that responds to HTTP\nrequests. For other types of infrastructure, you’ll need to replace\ncurl  and HTTP requests with a different form of validation. For\nexample, if your infrastructure code deploys a MySQL database,\nyou’ll need to use a MySQL client to validate it; if your infrastruc‐\nture code deploys a VPN server, you’ll need to use a VPN client\nto validate it; if your infrastructure code deploys a server that\nisn’t listening for requests at all, you might need to SSH to the\nserver and execute some commands locally to test it; and so on.\nSo although you can use the same basic test structure described in\nthis chapter with any type of infrastructure, the validation steps will\nchange depending on what you’re testing.\nIn short, when working with Terraform, every developer needs good example code\nto test and a real deployment environment (e.g., an AWS account) to use as an\nequivalent to localhost for running those tests. In the process of manual testing,\nyou’re likely to bring up and tear down a lot of infrastructure, and likely make lots of\nmistakes along the way, so this environment should be completely isolated from your\nother, more stable environments, such as staging, and especially production.\nTherefore, I strongly recommend that every team sets up an isolated sandbox envi‐\nronment , in which developers can bring up and tear down any infrastructure they\nwant without worrying about affecting others. In fact, to reduce the chances of\nconflicts between multiple developers (e.g., two developers trying to create a load\nbalancer with the same name), the gold standard is that each developer gets their own\ncompletely isolated sandbox environment. For example, if you’re using Terraform\nwith AWS, the gold standard is for each developer to have their own AWS account\nthat they can use to test anything they want.1\nCleaning Up After Tests\nHaving many sandbox environments is essential for developer productivity, but if\nyou’re not careful, you can end up with infrastructure running all over the place,\ncluttering up all of your environments, and costing you a lot of money.\nTo keep costs from spiraling out of control, key testing takeaway #2  is: regularly clean\nup your sandbox environments.\nManual Tests | 319\nAt a minimum, you should create a culture in which developers clean up whatever\nthey deploy when they are done testing by running terraform destroy . Depending\non your deployment environment, you might also be able to find tools that you can\nrun on a regular schedule (e.g., a cron job) to automatically clean up unused or old\nresources, such as cloud-nuke  and aws-nuke .\nFor example, a common pattern is to run cloud-nuke  as a cron job once per day\nin each sandbox environment to delete all resources that are more than 48 hours\nold, based on the assumption that any infrastructure a developer fired up for manual\ntesting is no longer necessary after a couple of days:\n$ cloud-nuke aws --older-than 48h\nWarning: Lots of Coding Ahead\nWriting automated tests for infrastructure code is not for the faint\nof heart. This automated testing section is arguably the most com‐\nplicated part of the book and does not make for light reading. If\nyou’re just skimming, feel free to skip this part. On the other hand,\nif you really want to learn how to test your infrastructure code, roll\nup your sleeves and get ready to write some code! Y ou don’t need\nto run any of the Ruby code (it’s just there to help build up your\nmental model), but you’ll want to write and run as much Go code\nas you can.\nAutomated Tests\nThe idea with automated testing is to write test code that validates that your real code\nbehaves the way it should. As you’ll see in Chapter 10 , you can set up a CI server\nto run these tests after every single commit and then immediately revert or fix any\ncommits that cause the tests to fail, thereby always keeping your code in a working\nstate.\nBroadly speaking, there are three types of automated tests:\nUnit tests\nUnit tests verify the functionality of a single, small unit of code. The definition  of\nunit varies, but in a general-purpose programming language, it’s typically a single\nfunction or class. Usually, any external dependencies—for example, databases,\nweb services, even the filesystem—are replaced with test doubles  or mocks  that\nallow you to finely control the behavior of those dependencies (e.g., by returning\na hardcoded response from a database mock) to test that your code handles a\nvariety of scenarios.\n320 | Chapter 9: How to Test Terraform Code\nIntegration tests\nIntegration tests verify that multiple units work together correctly. In a general-\npurpose programming language, an integration test consists of code that vali‐\ndates that several functions or classes work together correctly. Integration tests\ntypically use a mix of real dependencies and mocks: for example, if you’re testing\nthe part of your app that communicates with the database, you might want to\ntest it with a real database, but mock out other dependencies, such as the app’s\nauthentication system.\nEnd-to-end tests\nEnd-to-end tests involve running your entire architecture—for example, your\napps, your data stores, your load balancers—and validating that your system\nworks as a whole. Usually, these tests are done from the end-user’s perspective,\nsuch as using Selenium to automate interacting with your product via a web\nbrowser. End-to-end tests typically use real systems everywhere, without any\nmocks, in an architecture that mirrors production (albeit with fewer/smaller\nservers to save money).\nEach type of test serves a different purpose, and can catch different types of bugs,\nso you’ll likely want to use a mix of all three types. The purpose of unit tests is to\nhave tests that run quickly so that you can get fast feedback on your changes and\nvalidate a variety of different permutations to build up confidence that the basic\nbuilding blocks of your code (the individual units) work as expected. But just because\nindividual units work correctly in isolation doesn’t mean that they will work correctly\nwhen combined, so you need integration tests to ensure the basic building blocks\nfit together correctly. And just because different parts of your system work correctly\ndoesn’t mean they will work correctly when deployed in the real world, so you need\nend-to-end tests to validate that your code behaves as expected in conditions similar\nto production.\nLet’s now go through how to write each type of test for Terraform code.\nUnit Tests\nTo understand how to write unit tests for Terraform code, it’s helpful to first look at\nwhat it takes to write unit tests for a general-purpose programming language such as\nRuby. Take a look again at the Ruby web server code:\nclass WebServer  < WEBrick::HTTPServlet ::AbstractServlet\n  def do_GET(request, response )\n    case request.path\n    when ""/""\n      response .status = 200\n      response ['Content-Type' ] = 'text/plain'\n      response .body = 'Hello, World'\n    when ""/api""\nAutomated Tests | 321\n      response .status = 201\n      response ['Content-Type' ] = 'application/json'\n      response .body = '{""foo"":""bar""}'\n    else\n      response .status = 404\n      response ['Content-Type' ] = 'text/plain'\n      response .body = 'Not Found'\n    end\n  end\nend\nWriting a unit test that calls the do_GET  method directly is tricky, as you’ d have\nto either instantiate real WebServer , request , and response  objects, or create test\ndoubles of them, both of which require a fair bit of work. When you find it difficult\nto write unit tests, that’s often a code smell and indicates that the code needs to be\nrefactored. One way to refactor this Ruby code to make unit testing easier is to extract\nthe “handlers”—that is, the code that handles the /, /api , and not found paths—into\nits own Handlers  class:\nclass Handlers\n  def handle(path)\n    case path\n    when ""/""\n      [200, 'text/plain' , 'Hello, World' ]\n    when ""/api""\n      [201, 'application/json' , '{""foo"":""bar""}' ]\n    else\n      [404, 'text/plain' , 'Not Found' ]\n    end\n  end\nend\nThere are two key properties to notice about this new Handlers  class:\nSimple values as inputs\nThe Handlers  class does not depend on HTTPServer , HTTPRequest , or HTTPRes\nponse . Instead, all of its inputs are simple values, such as the path  of the URL,\nwhich is a string.\nSimple values as outputs\nInstead of setting values on a mutable HTTPResponse  object (a side effect), the\nmethods in the Handlers  class return the HTTP response as a simple value (an\narray that contains the HTTP status code, content type, and body).\nCode that takes in simple values as inputs and returns simple values as outputs is\ntypically easier to understand, update, and test. Let’s first update the WebServer  class\nto use the new Handlers  class to respond to requests:\nclass WebServer  < WEBrick::HTTPServlet ::AbstractServlet\n  def do_GET(request, response )\n322 | Chapter 9: How to Test Terraform Code\n    handlers  = Handlers .new\n    status_code , content_type , body = handlers .handle(request.path)\n    response .status = status_code\n    response ['Content-Type' ] = content_type\n    response .body = body\n  end\nend\nThis code calls the handle  method of the Handlers  class and sends back the status\ncode, content type, and body returned by that method as an HTTP response. As you\ncan see, using the Handlers  class is clean and simple. This same property will make\ntesting easy, too. Here are three unit tests that check each endpoint in the Handlers\nclass:\nclass TestWebServer  < Test::Unit::TestCase\n  def initialize (test_method_name )\n    super(test_method_name )\n    @handlers  = Handlers .new\n  end\n  def test_unit_hello\n    status_code , content_type , body = @handlers .handle(""/"")\n    assert_equal (200, status_code )\n    assert_equal ('text/plain' , content_type )\n    assert_equal ('Hello, World' , body)\n  end\n  def test_unit_api\n    status_code , content_type , body = @handlers .handle(""/api"")\n    assert_equal (201, status_code )\n    assert_equal ('application/json' , content_type )\n    assert_equal ('{""foo"":""bar""}' , body)\n  end\n  def test_unit_404\n    status_code , content_type , body = @handlers .handle(""/invalid-path"" )\n    assert_equal (404, status_code )\n    assert_equal ('text/plain' , content_type )\n    assert_equal ('Not Found' , body)\n  end\nend\nAnd here’s how you run the tests:\n$ ruby web-server-test.rb\nLoaded suite web-server-test\nFinished in 0.000572 seconds.\n-------------------------------------------\n3 tests, 9 assertions, 0 failures, 0 errors\n100% passed\n-------------------------------------------\nAutomated Tests | 323\n2In limited cases, it is possible to override the endpoints Terraform uses to communicate with providers, such\nas overriding the endpoints Terraform uses to talk to AWS to instead talk to a mocking tool called LocalStack .\nThis works for a small number of endpoints, but most Terraform code makes hundreds  of different API calls\nto the underlying provider, and mocking out all of them is impractical. Moreover, even if you do mock them\nall out, it’s not clear that the resulting unit test can give you much confidence that your code works correctly:\ne.g., if you create mock endpoints for ASGs and ALBs, your terraform apply  might succeed, but does that\ntell you anything useful about whether your code would have actually deployed a working app on top of that\ninfrastructure?In 0.0005272 seconds, you can now find out whether your web server code works as\nexpected. That’s the power of unit testing: a fast feedback loop that helps you build\nconfidence in your code.\nUnit testing Terraform code\nWhat is the equivalent of this sort of unit testing with Terraform code? The first\nstep is to identify what a “unit” is in the Terraform world. The closest equivalent to\na single function or class in Terraform is a single reusable module such as the alb\nmodule you created in Chapter 8 . How would you test this module?\nWith Ruby, to write unit tests, you needed to refactor the code so you could run\nit without complicated dependencies such as HTTPServer , HTTPRequest , or HTTPRes\nponse . If you think about what your Terraform code is doing—making API calls to\nAWS to create the load balancer, listeners, target groups, and so on—you’ll realize\nthat 99% of what this code is doing is communicating with complicated dependen‐\ncies! There’s no practical way to reduce the number of external dependencies to zero,\nand even if you could, you’ d effectively be left with no code to test.2\nThat brings us to key testing takeaway #3 : you cannot do pure  unit testing for\nTerraform code.\nBut don’t despair. Y ou can still build confidence that your Terraform code behaves as\nexpected by writing automated tests that use your code to deploy real infrastructure\ninto a real environment (e.g., into a real AWS account). In other words, unit tests for\nTerraform are really integration tests. However, I prefer to still call them unit tests to\nemphasize that the goal is to test a single unit (i.e., a single reusable module) to get\nfeedback as quickly as possible.\nThis means that the basic strategy for writing unit tests for Terraform is as follows:\n1.Create a small, standalone module.1.\n2.Create an easy-to-deploy example for that module.2.\n3.Run terraform apply  to deploy the example into a real environment. 3.\n4.Validate that what you just deployed works as expected. This step is specific to4.\nthe type of infrastructure you’re testing: for example, for an ALB, you’ d validate\n324 | Chapter 9: How to Test Terraform Code\nit by sending an HTTP request and checking that you receive back the expected\nresponse.\n5.Run terraform destroy  at the end of the test to clean up. 5.\nIn other words, you do exactly  the same steps as you would when doing manual\ntesting, but you capture those steps as code. In fact, that’s a good mental model for\ncreating  automated tests for your Terraform code: ask yourself, “How would I have\ntested this manually to be confident it works?” and then implement that test in code.\nY ou can use any programming language you want to write the test code. In this\nbook, all of the tests are written in the Go programming language to take advantage\nof an open source Go library called Terratest , which supports testing a wide variety\nof infrastructure-as-code tools (e.g., Terraform, Packer, Docker, Helm) across a wide\nvariety of environments (e.g., AWS, Google Cloud, Kubernetes). Terratest is a bit\nlike a Swiss Army knife, with hundreds of tools built in that make it significantly\neasier to test infrastructure code, including first-class support for the test strategy just\ndescribed, where you terraform apply  some code, validate that it works, and then\nrun terraform destroy  at the end to clean up.\nTo use Terratest, you need to do the following:\n1.Install Go  (minimum version 1.13). 1.\n2.Create a folder for your test code: e.g., a folder named test. 2.\n3.Run go mod init <NAME>  in the folder you just created, where NAME  is the 3.\nname to use for this test suite, typically in the format github.com/<ORG_NAME>/\n<PROJECT_NAME>  (e.g., go mod init github.com/brikis98/terraform-up-and-\nrunning ). This should create a go.mod  file, which is used to track the dependen‐\ncies of your Go code.\nAs a quick sanity check that your environment is set up correctly, create go_san‐\nity_test.go  in your new folder with the following contents:\npackage test\nimport (\n""fmt""\n""testing""\n)\nfunc TestGoIsWorking (t *testing.T) {\nfmt.Println()\nfmt.Println(""If you see this text, it's working!"" )\nfmt.Println()\n}\nAutomated Tests | 325\nRun this test using the go test  command:\ngo test -v\nThe -v flag means verbose, which ensures that the test always shows all log output.\nY ou should see output that looks something like this:\n=== RUN   TestGoIsWorking\nIf you see this text, it's working!\n--- PASS: TestGoIsWorking (0.00s)\nPASS\nok  github.com/brikis98/terraform-up-and-running-code 0.192s\nIf that’s working, feel free to delete go_sanity_test.go , and move on to writing a\nunit test for the alb module. Create alb_example_test.go  in your test folder with the\nfollowing skeleton of a unit test:\npackage test\nimport (\n""testing""\n)\nfunc TestAlbExample (t *testing.T) {\n}\nThe first step is to direct Terratest to where your Terraform code resides by using the\nterraform.Options  type:\npackage test\nimport (\n""github.com/gruntwork-io/terratest/modules/terraform""\n""testing""\n)\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n}\nNote that to test the alb module, you actually test the example code in your examples\nfolder (you should update the relative path in TerraformDir  to point to the folder\nwhere you created that example).\nThe next step in the automated test is to run terraform init  and terraform apply\nto deploy the code. Terratest has handy helpers for doing that:\n326 | Chapter 9: How to Test Terraform Code\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\nterraform .Init(t, opts)\nterraform .Apply(t, opts)\n}\nIn fact, running init  and apply  is such a common operation with Terratest that there\nis a convenient InitAndApply  helper method that does both in one command:\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n// Deploy the example\nterraform .InitAndApply (t, opts)\n}\nThe preceding code is already a fairly useful unit test, since it will run terraform\ninit  and terraform apply  and fail the test if those commands don’t complete\nsuccessfully (e.g., due to a problem with your Terraform code). However, you can go\neven further by making HTTP requests to the deployed load balancer and checking\nthat it returns the data you expect. To do that, you need a way to get the domain\nname of the deployed load balancer. Fortunately, that’s available as an output variable\nin the alb example:\noutput ""alb_dns_name""  {\n  value       = module.alb.alb_dns_name\n  description  = ""The domain name of the load balancer""\n}\nTerratest has helpers built in to read outputs from your Terraform code:\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n// Deploy the example\nterraform .InitAndApply (t, opts)\n// Get the URL of the ALB\nalbDnsName  := terraform .OutputRequired (t, opts, ""alb_dns_name"" )\nAutomated Tests | 327\nurl := fmt.Sprintf(""http://%s"" , albDnsName )\n}\nThe OutputRequired  function returns the output of the given name, or it fails the test\nif that output doesn’t exist or is empty. The preceding code builds a URL from this\noutput using the fmt.Sprintf  function that’s built into Go (don’t forget to import the\nfmt package). The next step is to make some HTTP requests to this URL using the\nhttp_helper  package (make sure to add github.com/gruntwork-io/terratest/mod\nules/http-helper  as an import):\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n// Deploy the example\nterraform .InitAndApply (t, opts)\n// Get the URL of the ALB\nalbDnsName  := terraform .OutputRequired (t, opts, ""alb_dns_name"" )\nurl := fmt.Sprintf(""http://%s"" , albDnsName )\n// Test that the ALB's default action is working and returns a 404\nexpectedStatus  := 404\nexpectedBody  := ""404: page not found""\nmaxRetries  := 10\ntimeBetweenRetries  := 10 * time.Second\nhttp_helper .HttpGetWithRetry (\nt,\nurl,\nnil,\nexpectedStatus ,\nexpectedBody ,\nmaxRetries ,\ntimeBetweenRetries ,\n)\n}\nThe http_helper.HttpGetWithRetry  method will make an HTTP GET request to\nthe URL you pass in and check that the response has the expected status code and\nbody. If it doesn’t, the method will retry up to the specified maximum number of\nretries, with the specified amount of time between retries. If it eventually achieves the\nexpected response, the test will pass; if the maximum number of retries is reached\nwithout the expected response, the test will fail. This sort of retry logic is very\ncommon in infrastructure testing, as there is usually a period of time between when\nterraform apply  finishes and when the deployed infrastructure is completely ready\n(i.e., it takes time for health checks to pass, DNS updates to propagate, and so on),\n328 | Chapter 9: How to Test Terraform Code\nand as you don’t know exactly how long that’ll take, the best option is to retry until it\nworks or you hit a timeout.\nThe last thing you need to do is to run terraform destroy  at the end of the test to\nclean up. As you can guess, there is a Terratest helper for this: terraform.Destroy .\nHowever, if you call terraform.Destroy  at the very end of the test, if any of the\ncode before that causes a test failure (e.g., HttpGetWithRetry  fails because the ALB is\nmisconfigured), the test code will exit before getting to terraform.Destroy , and the\ninfrastructure deployed for the test will never be cleaned up.\nTherefore, you want to ensure that you always  run terraform.Destroy , even if the\ntest fails. In many programming languages, this is done with a try / finally  or try /\nensure  construct, but in Go, this is done by using the defer  statement, which will\nguarantee that the code you pass to it will be executed when the surrounding function\nreturns (no matter how that return happens):\nfunc TestAlbExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n// Clean up everything at the end of the test\ndefer terraform .Destroy(t, opts)\n// Deploy the example\nterraform .InitAndApply (t, opts)\n// Get the URL of the ALB\nalbDnsName  := terraform .OutputRequired (t, opts, ""alb_dns_name"" )\nurl := fmt.Sprintf(""http://%s"" , albDnsName )\n// Test that the ALB's default action is working and returns a 404\nexpectedStatus  := 404\nexpectedBody  := ""404: page not found""\nmaxRetries  := 10\ntimeBetweenRetries  := 10 * time.Second\nhttp_helper .HttpGetWithRetry (\nt,\nurl,\nnil,\nexpectedStatus ,\nexpectedBody ,\nmaxRetries ,\ntimeBetweenRetries ,\n)\n}\nAutomated Tests | 329\nNote that the defer  is added early in the code, even before the call to terraform\n.InitAndApply , to ensure that nothing can cause the test to fail before getting to the\ndefer  statement and preventing it from queueing up the call to terraform.Destroy .\nOK, this unit test is finally ready to run!\nTerratest Version\nThe test code in this book was written with Terratest v0.39.0.\nTerratest is still a pre-1.0.0 tool, so newer releases may contain\nbackward-incompatible changes. To ensure the test examples in\nthis book work as written, I recommend installing Terratest specifi‐\ncally at version v0.39.0, and not the latest version. To do that, go\ninto go.mod  and add the following to the end of the file:\nrequire github.com/gruntwork-io/terratest v0.39.0\nSince this is a brand-new Go project, as a one-time action, you need to tell Go to\ndownload dependencies (including Terratest). The easiest way to do that at this stage\nis to run the following:\ngo mod tidy\nThis will download all your dependencies and create a go.sum  file to lock the exact\nversions you used.\nNext, since this test deploys infrastructure to AWS, before running the test, you\nneed to authenticate to your AWS account as usual (see “Other AWS Authentication\nOptions”  on page 44). Y ou saw earlier in this chapter that you should do manual\ntesting in a sandbox account; for automated testing, this is even more important, so I\nrecommend authenticating to a totally separate account. As your automated test suite\ngrows, you might be spinning up hundreds or thousands of resources in every test\nsuite, so keeping them isolated from everything else is essential.\nI typically recommend that teams have a completely separate environment (e.g.,\ncompletely separate AWS account) just for automated testing—separate even from\nthe sandbox environments you use for manual testing. That way, you can safely delete\nall resources that are more than a few hours old in the testing environment, based on\nthe assumption that no test will run that long.\nAfter you’ve authenticated to an AWS account that you can safely use for testing, you\ncan run the test, as follows:\n$ go test -v -timeout 30m\nTestAlbExample 2019-05-26T13:29:32+01:00 command.go:53:\nRunning command terraform with args [init -upgrade=false]\n(...)\n330 | Chapter 9: How to Test Terraform Code\nTestAlbExample 2019-05-26T13:29:33+01:00 command.go:53:\nRunning command terraform with args [apply -input=false -lock=false]\n(...)\nTestAlbExample 2019-05-26T13:32:06+01:00 command.go:121:\nApply complete! Resources: 5 added, 0 changed, 0 destroyed.\n(...)\nTestAlbExample 2019-05-26T13:32:06+01:00 command.go:53:\nRunning command terraform with args [output -no-color alb_dns_name]\n(...)\nTestAlbExample 2019-05-26T13:38:32+01:00 http_helper.go:27:\nMaking an HTTP GET call to URL\nhttp://terraform-up-and-running-1892693519.us-east-2.elb.amazonaws.com\n(...)\nTestAlbExample 2019-05-26T13:38:32+01:00 command.go:53:\nRunning command terraform with args\n[destroy -auto-approve -input=false -lock=false]\n(...)\nTestAlbExample 2019-05-26T13:39:16+01:00 command.go:121:\nDestroy complete! Resources: 5 destroyed.\n(...)\nPASS\nok  terraform-up-and-running 229.492s\nNote the use of the -timeout 30m  argument with go test . By default, Go imposes\na time limit of 10 minutes for tests, after which it forcibly kills the test run, not\nonly causing the tests to fail but also preventing the cleanup code (i.e., terraform\ndestroy ) from running. This ALB test should take closer to five minutes, but when‐\never running a Go test that deploys real infrastructure, it’s safer to set an extra-long\ntimeout to avoid the test being killed partway through and leaving all sorts of infra‐\nstructure still running.\nThe test will produce a lot of log output, but if you read through it carefully, you\nshould be able to spot all of the key stages of the test:\n1.Running terraform init 1.\n2.Running terraform apply 2.\nAutomated Tests | 331\n3.Reading output variables using terraform output 3.\n4.Repeatedly making HTTP requests to the ALB4.\n5.Running terraform destroy 5.\nIt’s nowhere near as fast as the Ruby unit tests, but in less than five minutes, you\ncan now automatically find out whether your alb module works as expected. This\nis about as fast of a feedback loop as you can get with infrastructure in AWS, and it\nshould give you a lot of confidence that your code works as expected.\nDependency injection\nLet’s now see what it would take to add a unit test for some slightly more complicated\ncode. Going back to the Ruby web server example once more, consider what would\nhappen if you needed to add a new /web-service  endpoint that made HTTP calls to\nan external dependency:\nclass Handlers\n  def handle(path)\n    case path\n    when ""/""\n      [200, 'text/plain' , 'Hello, World' ]\n    when ""/api""\n      [201, 'application/json' , '{""foo"":""bar""}' ]\n    when ""/web-service""\n      # New endpoint that calls a web service\n      uri = URI(""http://www.example.org"" )\n      response  = Net::HTTP.get_response (uri)\n      [response .code.to_i, response ['Content-Type' ], response .body]\n    else\n      [404, 'text/plain' , 'Not Found' ]\n    end\n  end\nend\nThe updated Handlers  class now handles the /web-service  URL by making an\nHTTP GET to example.org  and proxying the response. When you curl  this end‐\npoint, you get the following:\n$ curl localhost:8000/web-service\n<!doctype html>\n<html>\n<head>\n    <title>Example Domain</title>\n    <-- (...) -->\n</head>\n<body>\n<div>\n    <h1>Example Domain</h1>\n332 | Chapter 9: How to Test Terraform Code\n    <p>\n      This domain is established to be used for illustrative\n      examples in documents. You may use this domain in\n      examples without prior coordination or asking for permission.\n    </p>\n    <!-- (...) -->\n</div>\n</body>\n</html>\nHow would you add a unit test for this new method? If you tried to test the code as\nis, your unit tests would be subject to the behavior of an external dependency (in this\ncase, example.org ). This has a number of downsides:\n•If that dependency has an outage, your tests will fail, even though there’s nothing•\nwrong with your code.\n•If that dependency changed its behavior from time to time (e.g., returned a•\ndifferent response body), your tests would fail from time to time, and you’ d need\nto constantly keep updating the test code, even though there’s nothing wrong\nwith the implementation.\n•If that dependency were slow, your tests would be slow, which negates one of the•\nmain benefits of unit tests, the fast feedback loop.\n•If you wanted to test that your code handles various corner cases based on how•\nthat dependency behaves (e.g., does the code handle redirects?), you’ d have no\nway to do it without control of that external dependency.\nAlthough working with real dependencies might make sense for integration and\nend-to-end tests, with unit tests, you should try to minimize external dependencies as\nmuch as possible. The typical strategy for doing this is dependency injection , in which\nyou make it possible to pass in (or “inject”) external dependencies from outside your\ncode, rather than hardcoding them within your code.\nFor example, the Handlers  class shouldn’t need to deal with all of the details of how\nto call a web service. Instead, you can extract that logic into a separate WebService\nclass:\nclass WebService\n  def initialize (url)\n    @uri = URI(url)\n  end\n  def proxy\n    response  = Net::HTTP.get_response (@uri)\n    [response .code.to_i, response ['Content-Type' ], response .body]\n  end\nend\nAutomated Tests | 333\nThis class takes a URL as an input and exposes a proxy  method to proxy the HTTP\nGET response from that URL. Y ou can then update the Handlers  class to take a\nWebService  instance as an input and use that instance in the web_service  method:\nclass Handlers\n  def initialize (web_service )\n    @web_service  = web_service\n  end\n  def handle(path)\n    case path\n    when ""/""\n      [200, 'text/plain' , 'Hello, World' ]\n    when ""/api""\n      [201, 'application/json' , '{""foo"":""bar""}' ]\n    when ""/web-service""\n      # New endpoint that calls a web service\n      @web_service .proxy\n    else\n      [404, 'text/plain' , 'Not Found' ]\n    end\n  end\nend\nNow, in your implementation code, you can inject a real WebService  instance that\nmakes HTTP calls to example.org :\nclass WebServer  < WEBrick::HTTPServlet ::AbstractServlet\n  def do_GET(request, response )\n    web_service  = WebService .new(""http://www.example.org"" )\n    handlers  = Handlers .new(web_service )\n    status_code , content_type , body = handlers .handle(request.path)\n    response .status = status_code\n    response ['Content-Type' ] = content_type\n    response .body = body\n  end\nend\nIn your test code, you can create a mock version of the WebService  class that allows\nyou to specify a mock response to return:\nclass MockWebService\n  def initialize (response )\n    @response  = response\n  end\n  def proxy\n    @response\n  end\nend\n334 | Chapter 9: How to Test Terraform Code\nAnd now you can create an instance of this MockWebService  class and inject it into\nthe Handlers  class in your unit tests:\n  def test_unit_web_service\n    expected_status  = 200\n    expected_content_type  = 'text/html'\n    expected_body  = 'mock example.org'\n    mock_response  = [expected_status , expected_content_type , expected_body ]\n    mock_web_service  = MockWebService .new(mock_response )\n    handlers  = Handlers .new(mock_web_service )\n    status_code , content_type , body = handlers .handle(""/web-service"" )\n    assert_equal (expected_status , status_code )\n    assert_equal (expected_content_type , content_type )\n    assert_equal (expected_body , body)\n  end\nRerun the tests to make sure it all still works:\n$ ruby web-server-test.rb\nLoaded suite web-server-test\nStarted\n...\nFinished in 0.000645 seconds.\n--------------------------------------------\n4 tests, 12 assertions, 0 failures, 0 errors\n100% passed\n--------------------------------------------\nFantastic. Using dependency injection to minimize external dependencies allows you\nto write fast, reliable tests and check all the various corner cases. And since the\nthree test cases you added earlier are still passing, you can be confident that your\nrefactoring hasn’t broken anything.\nLet’s now turn our attention back to Terraform and see what dependency injection\nlooks like with Terraform modules, starting with the hello-world-app  module. If\nyou haven’t already, the first step is to create an easy-to-deploy example for it in the\nexamples  folder:\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nmodule ""hello_world_app""  {\n  source  = ""../../../modules/services/hello-world-app""\n  server_text  = ""Hello, World""\n  environment  = ""example""\n  db_remote_state_bucket  = ""(YOUR_BUCKET_NAME)""\nAutomated Tests | 335\n  db_remote_state_key     = ""examples/terraform.tfstate""\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\ndata ""aws_ami"" ""ubuntu""  {\n  most_recent  = true\n  owners       = [""099720109477"" ] # Canonical\n  filter {\n    name    = ""name""\n    values  = [""ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"" ]\n  }\n}\nThe dependency problem becomes apparent when you spot the parameters\ndb_remote_state_bucket  and db_remote_state_key : the hello-world-app  module\nassumes that you’ve already deployed the mysql  module and requires that you pass\nin the details of the S3 bucket where the mysql  module is storing state using these\ntwo parameters. The goal here is to create a unit test for the hello-world-app\nmodule, and although a pure unit test with 0 external dependencies isn’t possible with\nTerraform, it’s still a good idea to minimize external dependencies whenever possible.\nOne of the first steps with minimizing dependencies is to make it clearer what depen‐\ndencies your module has. A file-naming convention you might want to adopt is to\nmove all of the data sources and resources that represent external dependencies into\na separate dependencies.tf  file. For example, here’s what modules/services/hello-world-\napp/dependencies.tf  would look like:\ndata ""terraform_remote_state"" ""db""  {\n  backend  = ""s3""\n  config  = {\n    bucket  = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n    region  = ""us-east-2""\n  }\n}\ndata ""aws_vpc"" ""default""  {\n  default  = true\n}\ndata ""aws_subnets"" ""default""  {\n  filter {\n    name    = ""vpc-id""\n    values  = [data.aws_vpc.default.id ]\n336 | Chapter 9: How to Test Terraform Code\n  }\n}\nThis convention makes it easier for users of your code to know, at a glance, what this\ncode depends on in the outside world. In the case of the hello-world-app  module,\nyou can quickly see that it depends on a database, VPC, and subnets. So, how can you\ninject these dependencies from outside the module so that you can replace them at\ntest time? Y ou already know the answer to this: input variables.\nFor each of these dependencies, you should add a new input variable in modules/serv‐\nices/hello-world-app/variables.tf :\nvariable  ""vpc_id""  {\n  description  = ""The ID of the VPC to deploy into""\n  type        = string\n  default      = null\n}\nvariable  ""subnet_ids""  {\n  description  = ""The IDs of the subnets to deploy into""\n  type        = list(string)\n  default      = null\n}\nvariable  ""mysql_config""  {\n  description  = ""The config for the MySQL DB""\n  type        = object({\n    address  = string\n    port     = number\n  })\n  default      = null\n}\nThere’s now an input variable for the VPC ID, subnet IDs, and MySQL config. Each\nvariable specifies a default , so they are optional variables  that the user can set to\nsomething custom or omit to get the default  value. The default  for each variable is\nnull .\nNote that the mysql_config  variable uses the object  type constructor to create a\nnested type with address  and port  keys. This type is intentionally designed to match\nthe output types of the mysql  module:\noutput ""address""  {\n  value       = aws_db_instance.example.address\n  description  = ""Connect to the database at this endpoint""\n}\noutput ""port"" {\n  value       = aws_db_instance.example.port\n  description  = ""The port the database is listening on""\n}\nAutomated Tests | 337\nOne of the advantages of doing this is that, as soon as the refactor is complete, one of\nthe ways you’ll be able to use the hello-world-app  and mysql  modules together is as\nfollows:\nmodule ""hello_world_app""  {\n  source  = ""../../../modules/services/hello-world-app""\n  server_text             = ""Hello, World""\n  environment             = ""example""\n  # Pass all the outputs from the mysql module straight through!\n  mysql_config  = module.mysql\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\nmodule ""mysql"" {\n  source  = ""../../../modules/data-stores/mysql""\n  db_name      = var.db_name\n  db_username  = var.db_username\n  db_password  = var.db_password\n}\nBecause the type  of mysql_config  matches the type of the mysql  module outputs,\nyou can pass them all straight through in one line. And if the types are ever changed\nand no longer match, Terraform will give you an error right away so that you know\nto update them. This is not only function composition at work but also type-safe\nfunction composition.\nBut before that can work, you’ll need to finish refactoring the code. Because\nthe MySQL configuration can be passed in as an input, this means that the\ndb_remote_state_bucket  and db_remote_state_key  variables should now be\noptional, so set their default  values to null :\nvariable  ""db_remote_state_bucket""  {\n  description  = ""The name of the S3 bucket for the DB's Terraform state""\n  type        = string\n  default      = null\n}\nvariable  ""db_remote_state_key""  {\n  description  = ""The path in the S3 bucket for the DB's Terraform state""\n  type        = string\n  default      = null\n}\n338 | Chapter 9: How to Test Terraform Code\nNext, use the count  parameter to optionally create the three data sources in\nmodules/services/hello-world-app/dependencies.tf  based on whether the corresponding\ninput variable is set to null :\ndata ""terraform_remote_state"" ""db""  {\n  count = var.mysql_config  == null ? 1 : 0\n  backend  = ""s3""\n  config  = {\n    bucket  = var.db_remote_state_bucket\n    key    = var.db_remote_state_key\n    region  = ""us-east-2""\n  }\n}\ndata ""aws_vpc"" ""default""  {\n  count   = var.vpc_id  == null ? 1 : 0\n  default  = true\n}\ndata ""aws_subnets"" ""default""  {\n  count = var.subnet_ids  == null ? 1 : 0\n  filter {\n    name    = ""vpc-id""\n    values  = [data.aws_vpc.default.id ]\n  }\n}\nNow you need to update any references to these data sources to conditionally use\neither the input variable or the data source. Let’s capture these as local values:\nlocals {\n  mysql_config  = (\n    var.mysql_config  == null\n      ? data.terraform_remote_state.db[0].outputs\n      : var.mysql_config\n  )\n  vpc_id  = (\n    var.vpc_id  == null\n      ? data.aws_vpc.default[0].id\n      : var.vpc_id\n  )\n  subnet_ids  = (\n    var.subnet_ids  == null\n      ? data.aws_subnets.default[0].ids\n      : var.subnet_ids\n  )\n}\nAutomated Tests | 339\nNote that because the data sources use the count  parameters, they are now arrays,\nso any time you reference them, you need to use array lookup syntax (i.e., [0]). Go\nthrough the code, and anywhere you find a reference to one of these data sources,\nreplace it with a reference to one of the local variables you just added instead. Start by\nupdating the aws_subnets  data source to use local.vpc_id :\ndata ""aws_subnets"" ""default""  {\n  count = var.subnet_ids  == null ? 1 : 0\n  filter {\n    name    = ""vpc-id""\n    values  = [local.vpc_id ]\n  }\n}\nThen, set the subnet_ids  parameter of the alb module to local.subnet_ids :\nmodule ""alb"" {\n  source  = ""../../networking/alb""\n  alb_name    = ""hello-world-${var.environment}""\n  subnet_ids  = local.subnet_ids\n}\nIn the asg module, make the following updates: set the subnet_ids  parameter to\nlocal.subnet_ids , and in the user_data  variables, update db_address  and db_port\nto read data from local.mysql_config .\nmodule ""asg"" {\n  source  = ""../../cluster/asg-rolling-deploy""\n  cluster_name   = ""hello-world-${var.environment}""\n  ami           = var.ami\n  instance_type  = var.instance_type\n  user_data  = templatefile (""${path.module}/user-data.sh"" , {\n    server_port  = var.server_port\n    db_address   = local.mysql_config.address\n    db_port      = local.mysql_config.port\n    server_text  = var.server_text\n  })\n  min_size            = var.min_size\n  max_size            = var.max_size\n  enable_autoscaling  = var.enable_autoscaling\n  subnet_ids         = local.subnet_ids\n  target_group_arns  = [aws_lb_target_group.asg.arn ]\n  health_check_type  = ""ELB""\n  custom_tags  = var.custom_tags\n}\n340 | Chapter 9: How to Test Terraform Code\nFinally, update the vpc_id  parameter of the aws_lb_target_group  to use\nlocal.vpc_id :\nresource  ""aws_lb_target_group"" ""asg""  {\n  name     = ""hello-world-${var.environment}""\n  port     = var.server_port\n  protocol  = ""HTTP""\n  vpc_id    = local.vpc_id\n  health_check  {\n    path                 = ""/""\n    protocol             = ""HTTP""\n    matcher              = ""200""\n    interval             = 15\n    timeout              = 3\n    healthy_threshold    = 2\n    unhealthy_threshold  = 2\n  }\n}\nWith these updates, you can now choose to inject the VPC ID, subnet IDs, and/or\nMySQL config parameters into the hello-world-app  module, or omit any of those\nparameters, and the module will use an appropriate data source to fetch those values\nby itself. Let’s update the “Hello, World” app example to allow the MySQL config to\nbe injected but omit the VPC ID and subnet ID parameters because using the default\nVPC is good enough for testing. Add a new input variable to examples/hello-world-\napp/variables.tf :\nvariable  ""mysql_config""  {\n  description  = ""The config for the MySQL DB""\n  type = object({\n    address  = string\n    port     = number\n  })\n  default  = {\n    address  = ""mock-mysql-address""\n    port     = 12345\n  }\n}\nPass this variable through to the hello-world-app  module in examples/hello-world-\napp/main.tf :\nmodule ""hello_world_app""  {\n  source  = ""../../../modules/services/hello-world-app""\n  server_text  = ""Hello, World""\n  environment  = ""example""\n  mysql_config  = var.mysql_config\nAutomated Tests | 341\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\nY ou can now set this mysql_config  variable in a unit test to any value you want.\nCreate a unit test in test/hello_world_app_example_test.go  with the following contents:\nfunc TestHelloWorldAppExample (t *testing.T) {\nopts := &terraform .Options{\n// You should update this relative path to point at your\n// hello-world-app example directory!\nTerraformDir : ""../examples/hello-world-app/standalone"" ,\n}\n// Clean up everything at the end of the test\ndefer terraform .Destroy(t, opts)\nterraform .InitAndApply (t, opts)\nalbDnsName  := terraform .OutputRequired (t, opts, ""alb_dns_name"" )\nurl := fmt.Sprintf(""http://%s"" , albDnsName )\nmaxRetries  := 10\ntimeBetweenRetries  := 10 * time.Second\nhttp_helper .HttpGetWithRetryWithCustomValidation (\nt,\nurl,\nnil,\nmaxRetries ,\ntimeBetweenRetries ,\nfunc(status int, body string) bool {\nreturn status == 200 &&\nstrings.Contains (body, ""Hello, World"" )\n},\n)\n}\nThis code is nearly identical to the unit test for the alb example, with only two\ndifferences:\n•The TerraformDir  setting is pointing to the hello-world-app  example instead of •\nthe alb example (be sure to update the path as necessary for your filesystem).\n•Instead of using http_helper.HttpGetWithRetry  to check for a 404 response, •\nthe test is using the http_helper.HttpGetWithRetryWithCustomValidation\nmethod to check for a 200 response and a body that contains the text “Hello,\nWorld. ” That’s because the User Data script of the hello-world-app  module\n342 | Chapter 9: How to Test Terraform Code\nreturns a 200 OK response that includes not only the server text but also other\ntext, including HTML.\nThere’s just one new thing you’ll need to add to this test—set the mysql_config\nvariable:\nopts := &terraform .Options{\n// You should update this relative path to point at your\n// hello-world-app example directory!\nTerraformDir : ""../examples/hello-world-app/standalone"" ,\nVars: map[string]interface {}{\n""mysql_config"" : map[string]interface {}{\n""address"" : ""mock-value-for-test"" ,\n""port"":    3306,\n},\n},\n}\nThe Vars  parameter in terraform.Options  allows you to set variables in your Terra‐\nform code. This code is passing in some mock data for the mysql_config  variable.\nAlternatively, you could set this value to anything you want: for example, you could\nfire up a small, in-memory database at test time and set the address  to that database’s\nIP .\nRun this new test using go test , specifying the -run  argument to run just this test\n(otherwise, Go’s default behavior is to run all tests in the current folder, including the\nALB example test you created earlier):\n$ go test -v -timeout 30m -run TestHelloWorldAppExample\n(...)\nPASS\nok  terraform-up-and-running 204.113s\nIf all goes well, the test will run terraform apply , make repeated HTTP requests to\nthe load balancer, and, as soon as it gets back the expected response, run terraform\ndestroy  to clean everything up. All told, it should take only a few minutes, and you\nnow have a reasonable unit test for the “Hello, World” app.\nRunning tests in parallel\nIn the previous section, you ran just a single test using the -run  argument of the\ngo test  command. If you had omitted that argument, Go would’ve run all of\nyour tests—sequentially. Although four to five minutes to run a single test isn’t too\nbad for testing infrastructure code, if you have dozens of tests, and each one runs\nsequentially, it could take hours to run your entire test suite. To shorten the feedback\nloop, you want to run as many tests in parallel as you can.\nAutomated Tests | 343\nTo instruct Go to run your tests in parallel, the only change you need to make is\nto add t.Parallel()  to the top of each test. Here it is in test/hello_world_app_exam‐\nple_test.go :\nfunc TestHelloWorldAppExample (t *testing.T) {\nt.Parallel ()\nopts := &terraform .Options{\n// You should update this relative path to point at your\n// hello-world-app example directory!\nTerraformDir : ""../examples/hello-world-app/standalone"" ,\nVars: map[string]interface {}{\n""mysql_config"" : map[string]interface {}{\n""address"" : ""mock-value-for-test"" ,\n""port"":    3306,\n},\n},\n}\n// (...)\n}\nAnd similarly in test/alb_example_test.go :\nfunc TestAlbExample (t *testing.T) {\nt.Parallel ()\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\n}\n// (...)\n}\nIf you run go test  now, both of those tests will execute in parallel. However, there’s\none gotcha: some of the resources created by those tests—for example, the ASG,\nsecurity group, and ALB—use the same name, which will cause the tests to fail due\nto the name clashes. Even if you weren’t using t.Parallel()  in your tests, if multiple\npeople on your team were running the same tests or if you had tests running in a CI\nenvironment, these sorts of name clashes would be inevitable.\nThis leads to key testing takeaway #4 : you must namespace all of your resources.\nThat is, design modules and examples so that the name of every resource is (option‐\nally) configurable. With the alb example, this means that you need to make the name\nof the ALB configurable. Add a new input variable in examples/alb/variables.tf  with a\nreasonable default:\n344 | Chapter 9: How to Test Terraform Code\nvariable  ""alb_name""  {\n  description  = ""The name of the ALB and all its resources""\n  type        = string\n  default      = ""terraform-up-and-running""\n}\nNext, pass this value through to the alb module in examples/alb/main.tf :\nmodule ""alb"" {\n  source  = ""../../modules/networking/alb""\n  alb_name    = var.alb_name\n  subnet_ids  = data.aws_subnets.default.ids\n}\nNow, set this variable to a unique value in test/alb_example_test.go :\npackage test\nimport (\n""fmt""\n""github.com/stretchr/testify/require""\n""github.com/gruntwork-io/terratest/modules/http-helper""\n""github.com/gruntwork-io/terratest/modules/random""\n""github.com/gruntwork-io/terratest/modules/terraform""\n""testing""\n""time""\n)\nfunc TestAlbExample (t *testing.T) {\nt.Parallel ()\nopts := &terraform .Options{\n// You should update this relative path to point at your alb\n// example directory!\nTerraformDir : ""../examples/alb"" ,\nVars: map[string]interface {}{\n""alb_name"" : fmt.Sprintf(""test-%s"" , random.UniqueId ()),\n},\n}\n// (...)\n}\nThis code sets the alb_name  var to test-<RANDOM_ID> , where RANDOM_ID  is a random\nunique ID returned by the random.UniqueId()  helper in Terratest. This helper\nreturns a randomized, six-character base-62 string. The idea is that it’s a short\nidentifier  you can add to the names of most resources without hitting length-limit\nissues but random enough to make conflicts very unlikely ( 626 = 56+ billion\nAutomated Tests | 345\ncombinations ). This ensures that you can run a huge number of ALB tests in parallel\nwith no concern of having a name conflict.\nMake a similar change to the “Hello, World” app example, first by adding a new input\nvariable in examples/hello-world-app/variables.tf :\nvariable  ""environment""  {\n  description  = ""The name of the environment we're deploying to""\n  type        = string\n  default      = ""example""\n}\nThen by passing that variable through to the hello-world-app  module:\nmodule ""hello_world_app""  {\n  source  = ""../../../modules/services/hello-world-app""\n  server_text  = ""Hello, World""\n  environment  = var.environment\n  mysql_config  = var.mysql_config\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\nFinally, setting environment  to a value that includes random.UniqueId()  in test/\nhello_world_app_example_test.go :\nfunc TestHelloWorldAppExample (t *testing.T) {\nt.Parallel ()\nopts := &terraform .Options{\n// You should update this relative path to point at your\n// hello-world-app example directory!\nTerraformDir : ""../examples/hello-world-app/standalone"" ,\nVars: map[string]interface {}{\n""mysql_config"" : map[string]interface {}{\n""address"" : ""mock-value-for-test"" ,\n""port"":    3306,\n},\n""environment"" : fmt.Sprintf(""test-%s"" , random.UniqueId ()),\n},\n}\n// (...)\n}\n346 | Chapter 9: How to Test Terraform Code\nWith these changes complete, it should now be safe to run all your tests in parallel:\n$ go test -v -timeout 30m\nTestAlbExample 2019-05-26T17:57:21+01:00 (...)\nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...)\nTestAlbExample 2019-05-26T17:57:21+01:00 (...)\nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...)\nTestHelloWorldAppExample 2019-05-26T17:57:21+01:00 (...)\n(...)\nPASS\nok  terraform-up-and-running 216.090s\nY ou should see both tests running at the same time so that the entire test suite takes\nroughly as long as the slowest of the tests, rather than the combined time of all the\ntests running back to back.\nNote that, by default, the number of tests Go will run in parallel is equal to how many\nCPUs you have on your computer. So if you only have one CPU, then by default,\nthe tests will still run serially, rather than in parallel. Y ou can override this setting by\nsetting the GOMAXPROCS  environment variable or by passing the -parallel  argument\nto the go test  command. For example, to force Go to run up to two tests in parallel,\nyou would run the following:\n$ go test -v -timeout 30m -parallel 2\nRunning Tests in Parallel in the Same Folder\nOne other type of parallelism to take into account is what happens\nif you try to run multiple automated tests in parallel against the\nsame Terraform folder. For example, perhaps you’ d want to run\nseveral different tests against examples/hello-world-app , where each\ntest sets different values for the input variables before running\nterraform apply . If you try this, you’ll hit a problem: the tests will\nend up clashing because they all try to run terraform init  and\nend up overwriting one another’s .terraform  folder and Terraform\nstate files.\nIf you want to run multiple tests against the same folder in parallel,\nthe easiest solution is to have each test copy that folder to a unique\ntemporary folder, and run Terraform in the temporary folder to\navoid conflicts. Terratest, of course, has a built-in helper to do this\nfor you, and it even does it in a way that ensures that relative file\npaths within those Terraform modules work correctly: check out\nthe test_structure.CopyTerraformFolderToTemp  method and its\ndocumentation for details.\nAutomated Tests | 347",54865
89-Integration Tests.pdf,89-Integration Tests,"Integration Tests\nNow that you’ve got some unit tests in place, let’s move on to integration tests. Again,\nit’s helpful to start with the Ruby web server example to build up some intuition that\nyou can later apply to the Terraform code. To do an integration test of the Ruby web\nserver code, you need to do the following:\n1.Run the web server on localhost so that it listens on a port.1.\n2.Send HTTP requests to the web server.2.\n3.Validate you get back the responses you expect.3.\nLet’s create a helper method in web-server-test.rb  that implements these steps:\n  def do_integration_test (path, check_response )\n    port = 8000\n    server = WEBrick::HTTPServer .new :Port => port\n    server.mount '/', WebServer\n    begin\n      # Start the web server in a separate thread so it\n      # doesn't block the test\n      thread = Thread.new do\n        server.start\n      end\n      # Make an HTTP request to the web server at the\n      # specified path\n      uri = URI(""http://localhost: #{port}#{path}"")\n      response  = Net::HTTP.get_response (uri)\n      # Use the specified check_response lambda to validate\n      # the response\n      check_response .call(response )\n    ensure\n      # Shut the server and thread down at the end of the\n      # test\n      server.shutdown\n      thread.join\n    end\n  end\nThe do_integration_test  method configures the web server on port 8000, starts\nit in a background thread (so the web server doesn’t block the test from running),\nsends an HTTP GET to the path  specified, passes the HTTP response to the specified\ncheck_response  function for validation, and at the end of the test, shuts down the\nweb server. Here’s how you can use this method to write an integration test for the /\nendpoint of the web server:\n348 | Chapter 9: How to Test Terraform Code\n  def test_integration_hello\n    do_integration_test ('/', lambda { |response |\n      assert_equal (200, response .code.to_i)\n      assert_equal ('text/plain' , response ['Content-Type' ])\n      assert_equal ('Hello, World' , response .body)\n    })\n  end\nThis method calls the do_integration_test  method with the / path and passes it a\nlambda (essentially, an inline function) that checks the response was a 200 OK with\nthe body “Hello, World. ” The integration tests for the other endpoints are analogous.\nLet’s run all of the tests:\n$ ruby web-server-test.rb\n(...)\nFinished in 0.221561 seconds.\n--------------------------------------------\n8 tests, 24 assertions, 0 failures, 0 errors\n100% passed\n--------------------------------------------\nNote that before, with solely unit tests, the test suite took 0.000572 seconds to run,\nbut now, with integration tests, it takes 0.221561 seconds, a slowdown of roughly 387\ntimes. Of course, 0.221561 seconds is still blazing fast, but that’s only because the\nRuby web server code is intentionally a minimal example that doesn’t do much. The\nimportant thing here is not the absolute numbers but the relative trend: integration\ntests are typically slower than unit tests. I’ll come back to this point later.\nLet’s now turn our attention to integration tests for Terraform code. If a “unit” in\nTerraform is a single module, an integration test that validates how several units work\ntogether would need to deploy several modules and see that they work correctly. In\nthe previous section, you deployed the “Hello, World” app example with mock data\ninstead of a real MySQL DB. For an integration test, let’s deploy the MySQL module\nfor real and make sure the “Hello, World” app integrates with it correctly. Y ou should\nalready have just such code under live/stage/data-stores/mysql  and live/stage/services/\nhello-world-app . That is, you can create an integration test for (parts of) your staging\nenvironment.\nOf course, as mentioned earlier in the chapter, all automated tests should run in an\nisolated AWS account. So while you’re testing the code that is meant for staging,\nyou should authenticate to an isolated testing account and run the tests there. If\nyour modules have anything in them hardcoded for the staging environment, this is\nthe time to make those values configurable so you can inject test-friendly values. In\nparticular, in live/stage/data-stores/mysql/variables.tf , expose the database name via a\nnew db_name  input variable:\nAutomated Tests | 349\nvariable  ""db_name""  {\n  description  = ""The name to use for the database""\n  type        = string\n  default      = ""example_database_stage""\n}\nPass that value through to the mysql  module in live/stage/data-stores/mysql/main.tf :\nmodule ""mysql"" {\n  source  = ""../../../../modules/data-stores/mysql""\n  db_name      = var.db_name\n  db_username  = var.db_username\n  db_password  = var.db_password\n}\nLet’s now create the skeleton of the integration test in test/hello_world_integra‐\ntion_test.go  and fill in the implementation details later:\n// Replace these with the proper paths to your modules\nconst dbDirStage  = ""../live/stage/data-stores/mysql""\nconst appDirStage  = ""../live/stage/services/hello-world-app""\nfunc TestHelloWorldAppStage (t *testing.T) {\nt.Parallel ()\n// Deploy the MySQL DB\ndbOpts := createDbOpts (t, dbDirStage )\ndefer terraform .Destroy(t, dbOpts)\nterraform .InitAndApply (t, dbOpts)\n// Deploy the hello-world-app\nhelloOpts  := createHelloOpts (dbOpts, appDirStage )\ndefer terraform .Destroy(t, helloOpts )\nterraform .InitAndApply (t, helloOpts )\n// Validate the hello-world-app works\nvalidateHelloApp (t, helloOpts )\n}\nThe test is structured as follows: deploy mysql , deploy the hello-world-app , vali‐\ndate the app, undeploy the hello-world-app  (runs at the end due to defer ), and,\nfinally, undeploy mysql  (runs at the end due to defer ). The createDbOpts , create\nHelloOpts , and validateHelloApp  methods don’t exist yet, so let’s implement them\none at a time, starting with the createDbOpts  method:\nfunc createDbOpts (t *testing.T, terraformDir  string) *terraform .Options {\nuniqueId  := random.UniqueId ()\nreturn &terraform .Options{\nTerraformDir : terraformDir ,\nVars: map[string]interface {}{\n350 | Chapter 9: How to Test Terraform Code\n""db_name"" :     fmt.Sprintf(""test%s"" , uniqueId ),\n""db_username"" : ""admin"",\n""db_password"" : ""password"" ,\n},\n}\n}\nNot much new so far: the code points terraform.Options  at the passed-in directory\nand sets the db_name , db_username , and db_password  variables.\nThe next step is to deal with where this mysql  module will store its state. Up to now,\nthe backend  configuration has been set to hardcoded values:\n  backend ""s3"" {\n    # Replace this with your bucket name!\n    bucket          = ""terraform-up-and-running-state""\n    key            = ""stage/data-stores/mysql/terraform.tfstate""\n    region          = ""us-east-2""\n    # Replace this with your DynamoDB table name!\n    dynamodb_table  = ""terraform-up-and-running-locks""\n    encrypt         = true\n  }\nThese hardcoded values are a big problem for testing, because if you don’t change\nthem, you’ll end up overwriting the real state file for staging! One option is to use\nTerraform workspaces (as discussed in “Isolation via Workspaces” on page 94), but\nthat would still require access to the S3 bucket in the staging account, whereas you\nshould be running tests in a totally separate AWS account. The better option is to\nuse partial configuration, as introduced in “Limitations with Terraform’s Backends”\non page 91. Move the entire backend  configuration into an external file, such as\nbackend.hcl :\nbucket         = ""terraform-up-and-running-state""\nkey            = ""stage/data-stores/mysql/terraform.tfstate""\nregion         = ""us-east-2""\ndynamodb_table  = ""terraform-up-and-running-locks""\nencrypt        = true\nleaving the backend  configuration in live/stage/data-stores/mysql/main.tf  empty:\n  backend ""s3"" {\n  }\nWhen you’re deploying the mysql  module to the real staging environment, you tell\nTerraform to use the backend  configuration in backend.hcl  via the -backend-config\nargument:\n$ terraform init -backend-config=backend.hcl\nAutomated Tests | 351\nWhen you’re running tests on the mysql  module, you can tell Terratest to pass in\ntest-time-friendly values using the BackendConfig  parameter of terraform.Options :\nfunc createDbOpts (t *testing.T, terraformDir  string) *terraform .Options {\nuniqueId  := random.UniqueId ()\nbucketForTesting  := ""YOUR_S3_BUCKET_FOR_TESTING""\nbucketRegionForTesting  := ""YOUR_S3_BUCKET_REGION_FOR_TESTING""\ndbStateKey  := fmt.Sprintf(""%s/%s/terraform.tfstate"" , t.Name(), uniqueId )\nreturn &terraform .Options{\nTerraformDir : terraformDir ,\nVars: map[string]interface {}{\n""db_name"" :     fmt.Sprintf(""test%s"" , uniqueId ),\n""db_username"" : ""admin"",\n""db_password"" : ""password"" ,\n},\nBackendConfig : map[string]interface {}{\n""bucket"" :  bucketForTesting ,\n""region"" :  bucketRegionForTesting ,\n""key"":     dbStateKey ,\n""encrypt"" : true,\n},\n}\n}\nY ou’ll need to update the bucketForTesting  and bucketRegionForTesting  variables\nwith your own values. Y ou can create a single S3 bucket in your test AWS account to\nuse as a backend , as the key configuration (the path within the bucket) includes the\nuniqueId , which should be unique enough to have a different value for each test.\nThe next step is to make some updates to the hello-world-app  module in the\nstaging environment. Open live/stage/services/hello-world-app/variables.tf , and expose\nvariables for db_remote_state_bucket , db_remote_state_key , and environment :\nvariable  ""db_remote_state_bucket""  {\n  description  = ""The name of the S3 bucket for the database's remote state""\n  type        = string\n}\nvariable  ""db_remote_state_key""  {\n  description  = ""The path for the database's remote state in S3""\n  type        = string\n}\nvariable  ""environment""  {\n  description  = ""The name of the environment we're deploying to""\n  type        = string\n352 | Chapter 9: How to Test Terraform Code\n  default      = ""stage""\n}\nPass those values through to the hello-world-app  module in live/stage/services/hello-\nworld-app/main.tf :\nmodule ""hello_world_app""  {\n  source  = ""../../../../modules/services/hello-world-app""\n  server_text             = ""Hello, World""\n  environment             = var.environment\n  db_remote_state_bucket  = var.db_remote_state_bucket\n  db_remote_state_key     = var.db_remote_state_key\n  instance_type       = ""t2.micro""\n  min_size            = 2\n  max_size            = 2\n  enable_autoscaling  = false\n  ami                = data.aws_ami.ubuntu.id\n}\nNow you can implement the createHelloOpts  method:\nfunc createHelloOpts (\ndbOpts *terraform .Options,\nterraformDir  string) *terraform .Options {\nreturn &terraform .Options{\nTerraformDir : terraformDir ,\nVars: map[string]interface {}{\n""db_remote_state_bucket"" : dbOpts.BackendConfig [""bucket"" ],\n""db_remote_state_key"" :    dbOpts.BackendConfig [""key""],\n""environment"" :            dbOpts.Vars[""db_name"" ],\n},\n}\n}\nNote that db_remote_state_bucket  and db_remote_state_key  are set to the values\nused in the BackendConfig  for the mysql  module to ensure that the hello-world-app\nmodule is reading from the exact same state to which the mysql  module just wrote.\nThe environment  variable is set to the db_name  just so all the resources are name‐\nspaced the same way.\nFinally, you can implement the validateHelloApp  method:\nfunc validateHelloApp (t *testing.T, helloOpts  *terraform .Options) {\nalbDnsName  := terraform .OutputRequired (t, helloOpts , ""alb_dns_name"" )\nurl := fmt.Sprintf(""http://%s"" , albDnsName )\nmaxRetries  := 10\ntimeBetweenRetries  := 10 * time.Second\nAutomated Tests | 353\nhttp_helper .HttpGetWithRetryWithCustomValidation (\nt,\nurl,\nnil,\nmaxRetries ,\ntimeBetweenRetries ,\nfunc(status int, body string) bool {\nreturn status == 200 &&\nstrings.Contains (body, ""Hello, World"" )\n},\n)\n}\nThis method uses the http_helper  package, just as with the unit tests, except this\ntime, it’s with the http_helper.HttpGetWithRetryWithCustomValidation  method\nthat allows you to specify custom validation rules for the HTTP response status code\nand body. This is necessary to check that the HTTP response contains  the string\n“Hello, World, ” rather than equals that string exactly, as the User Data script in the\nhello-world-app  module returns an HTML response with other text in it as well.\nAlright, run the integration test to see whether it worked:\n$ go test -v -timeout 30m -run ""TestHelloWorldAppStage""\n(...)\nPASS\nok  terraform-up-and-running 795.63s\nExcellent, you now have an integration test that you can use to check that several\nof your modules work correctly together. This integration test is more complicated\nthan the unit test, and it takes more than twice as long (10–15 minutes rather than\n4–5 minutes). In general, there’s not much that you can do to make things faster —the\nbottleneck here is how long AWS takes to deploy and undeploy RDS, ASGs, ALBs,\netc.—but in certain circumstances, you might be able to make the test code do less\nusing test stages .\nTest stages\nIf you look at the code for your integration test, you may notice that it consists of five\ndistinct “stages”:\n1.Run terraform apply  on the mysql  module. 1.\n2.Run terraform apply  on the hello-world-app  module. 2.\n3.Run validations to make sure everything is working.3.\n4.Run terraform destroy  on the hello-world-app  module. 4.\n354 | Chapter 9: How to Test Terraform Code\n5.Run terraform destroy  on the mysql  module. 5.\nWhen you run these tests in a CI environment, you’ll want to run all of the stages,\nfrom start to finish. However, if you’re running these tests in your local dev environ‐\nment while iteratively making changes to the code, running all of these stages is\nunnecessary. For example, if you’re making changes only to the hello-world-app\nmodule, rerunning this entire test after every change means you’re paying the price\nof deploying and undeploying the mysql  module, even though none of your changes\naffect it. That adds 5 to 10 minutes of pure overhead to every test run.\nIdeally, the workflow would look more like this:\n1.Run terraform apply  on the mysql  module. 1.\n2.Run terraform apply  on the hello-world-app  module. 2.\n3.Now, you start doing iterative development:3.\na.Make a change to the hello-world-app  module. a.\nb.Rerun terraform apply  on the hello-world-app  module to deploy your b.\nupdates.\nc.Run validations to make sure everything is working.c.\nd.If everything works, move on to the next step. If not, go back to step 3a.d.\n4.Run terraform destroy  on the hello-world-app  module. 4.\n5.Run terraform destroy  on the mysql  module. 5.\nHaving the ability to quickly do that inner loop in step 3 is the key to fast, iterative\ndevelopment with Terraform. To support this, you need to break your test code into\nstages , in which you can choose the stages to execute and those that you can skip.\nTerratest supports this natively with the test_structure  package. The idea is that\nyou wrap each stage of your test in a function with a name, and you can then direct\nTerratest to skip some of those names by setting environment variables. Each test\nstage stores test data on disk so that it can be read back from disk on subsequent test\nruns. Let’s try this out on test/hello_world_integration_test.go , writing the skeleton of\nthe test first and then filling in the underlying methods later:\nfunc TestHelloWorldAppStageWithStages (t *testing.T) {\nt.Parallel ()\n// Store the function in a short variable name solely to make the\n// code examples fit better in the book.\nstage := test_structure .RunTestStage\n// Deploy the MySQL DB\ndefer stage(t, ""teardown_db"" , func() { teardownDb (t, dbDirStage ) })\nAutomated Tests | 355\nstage(t, ""deploy_db"" , func() { deployDb (t, dbDirStage ) })\n// Deploy the hello-world-app\ndefer stage(t, ""teardown_app"" , func() { teardownApp (t, appDirStage ) })\nstage(t, ""deploy_app"" , func() { deployApp (t, dbDirStage , appDirStage ) })\n// Validate the hello-world-app works\nstage(t, ""validate_app"" , func() { validateApp (t, appDirStage ) })\n}\nThe structure is the same as before—deploy mysql , deploy hello-world-app , validate\nhello-world-app , undeploy hello-world-app  (runs at the end due to defer ), unde‐\nploy mysql  (runs at the end due to defer )—except now, each stage is wrapped in\ntest_structure.RunTestStage . The RunTestStage  method takes three arguments:\nt\nThe first argument is the t value that Go passes as an argument to every automa‐\nted test. Y ou can use this value to manage test state. For example, you can fail the\ntest by calling t.Fail() .\nStage name\nThe second argument allows you to specify the name for this test stage. Y ou’ll see\nan example shortly of how to use this name to skip test stages.\nThe code to execute\nThe third argument is the code to execute for this test stage. This can be any\nfunction.\nLet’s now implement the functions for each test stage, starting with deployDb :\nfunc deployDb (t *testing.T, dbDir string) {\ndbOpts := createDbOpts (t, dbDir)\n// Save data to disk so that other test stages executed at a later\n// time can read the data back in\ntest_structure .SaveTerraformOptions (t, dbDir, dbOpts)\nterraform .InitAndApply (t, dbOpts)\n}\nJust as before, to deploy mysql , the code calls createDbOpts  and terraform\n.InitAndApply . The only new thing is that, in between those two steps, there is a call\nto test_structure.SaveTerraformOptions . This writes the data in dbOpts  to disk so\nthat other test stages can read it later on. For example, here’s the implementation of\nthe teardownDb  function:\nfunc teardownDb (t *testing.T, dbDir string) {\ndbOpts := test_structure .LoadTerraformOptions (t, dbDir)\ndefer terraform .Destroy(t, dbOpts)\n}\n356 | Chapter 9: How to Test Terraform Code\nThis function uses test_structure.LoadTerraformOptions  to load the dbOpts  data\nfrom disk that was earlier saved by the deployDb  function. The reason you need to\npass this data via the hard drive rather than passing it in memory is that you can\nrun each test stage as part of a different test run—and therefore, as part of a different\nprocess. As you’ll see a little later in this chapter, on the first few runs of go test ,\nyou might want to run deployDb  but skip teardownDb , and then in later runs do the\nopposite, running teardownDb  but skipping deployDb . To ensure that you’re using the\nsame database across all those test runs, you must store that database’s information on\ndisk.\nLet’s now implement the deployHelloApp  function:\nfunc deployApp (t *testing.T, dbDir string, helloAppDir  string) {\ndbOpts := test_structure .LoadTerraformOptions (t, dbDir)\nhelloOpts  := createHelloOpts (dbOpts, helloAppDir )\n// Save data to disk so that other test stages executed at a later\n// time can read the data back in\ntest_structure .SaveTerraformOptions (t, helloAppDir , helloOpts )\nterraform .InitAndApply (t, helloOpts )\n}\nThis function reuses the createHelloOpts  function from before and calls\nterraform.InitAndApply  on it. Again, the only new behavior is the use of\ntest_structure.LoadTerraformOptions  to load dbOpts  from disk and the use of\ntest_structure.SaveTerraformOptions  to save helloOpts  to disk. At this point,\nyou can probably guess what the implementation of the teardownApp  method looks\nlike:\nfunc teardownApp (t *testing.T, helloAppDir  string) {\nhelloOpts  := test_structure .LoadTerraformOptions (t, helloAppDir )\ndefer terraform .Destroy(t, helloOpts )\n}\nAnd the implementation of the validateApp  method:\nfunc validateApp (t *testing.T, helloAppDir  string) {\nhelloOpts  := test_structure .LoadTerraformOptions (t, helloAppDir )\nvalidateHelloApp (t, helloOpts )\n}\nSo, overall, the test code is identical to the original integration test, except each\nstage is wrapped in a call to test_structure.RunTestStage , and you need to do a\nlittle work to save and load data to and from disk. These simple changes unlock an\nimportant ability: you can instruct Terratest to skip any test stage called foo by setting\nthe environment variable SKIP_foo=true . Let’s go through a typical coding workflow\nto see how this works.\nAutomated Tests | 357\nY our first step will be to run the test but to skip both of the teardown stages so that\nthe mysql  and hello-world-app  modules stay deployed at the end of the test. Because\nthe teardown stages are called teardown_db  and teardown_app , you need to set the\nSKIP_teardown_db  and SKIP_teardown_app  environment variables, respectively, to\ndirect Terratest to skip those two stages:\n$ SKIP_teardown_db=true \\n  SKIP_teardown_app=true \\n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages'\n(...)\nThe 'SKIP_deploy_db' environment variable is not set,\nso executing stage 'deploy_db'.\n(...)\nThe 'deploy_app' environment variable is not set,\nso executing stage 'deploy_db'.\n(...)\nThe 'validate_app' environment variable is not set,\nso executing stage 'deploy_db'.\n(...)\nThe 'teardown_app' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nThe 'teardown_db' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nPASS\nok  terraform-up-and-running 423.650s\nNow you can start iterating on the hello-world-app  module, and each time you\nmake a change, you can rerun the tests, but this time, direct them to skip not only\nthe teardown stages but also the mysql  module deploy stage (because mysql  is still\nrunning) so that the only things that execute are deploy app  and the validations for\nthe hello-world-app  module:\n$ SKIP_teardown_db=true \\n  SKIP_teardown_app=true \\n  SKIP_deploy_db=true \\n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages'\n358 | Chapter 9: How to Test Terraform Code\n(...)\nThe 'SKIP_deploy_db' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nThe 'deploy_app' environment variable is not set,\nso executing stage 'deploy_db'.\n(...)\nThe 'validate_app' environment variable is not set,\nso executing stage 'deploy_db'.\n(...)\nThe 'teardown_app' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nThe 'teardown_db' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nPASS\nok  terraform-up-and-running 13.824s\nNotice how fast each of these test runs is now: instead of waiting 10 to 15 minutes\nafter every change, you can try out new changes in 10 to 60 seconds (depending on\nthe change). Given that you’re likely to rerun these stages dozens or even hundreds of\ntimes during development, the time savings can be massive.\nOnce the hello-world-app  module changes are working the way you expect, it’s time\nto clean everything up. Run the tests once more, this time skipping the deploy and\nvalidation stages so that only the teardown stages are executed:\n$ SKIP_deploy_db=true \\n  SKIP_deploy_app=true \\n  SKIP_validate_app=true \\n  go test -timeout 30m -run 'TestHelloWorldAppStageWithStages'\n(...)\nThe 'SKIP_deploy_db' environment variable is set,\nso skipping stage 'deploy_db'.\n(...)\nThe 'SKIP_deploy_app' environment variable is set,\nAutomated Tests | 359\nso skipping stage 'deploy_app'.\n(...)\nThe 'SKIP_validate_app' environment variable is set,\nso skipping stage 'validate_app'.\n(...)\nThe 'SKIP_teardown_app' environment variable is not set,\nso executing stage 'teardown_app'.\n(...)\nThe 'SKIP_teardown_db' environment variable is not set,\nso executing stage 'teardown_db'.\n(...)\nPASS\nok  terraform-up-and-running 340.02s\nUsing test stages lets you get rapid feedback from your automated tests, dramati‐\ncally increasing the speed and quality of iterative development. It won’t make any\ndifference in how long tests take in your CI environment, but the impact on the\ndevelopment environment is huge.\nRetries\nAfter you start running automated tests for your infrastructure code on a regular\nbasis, you’re likely to run into a problem: flaky tests. That is, tests occasionally will\nfail for transient reasons, such as an EC2 Instance occasionally failing to launch, or\na Terraform eventual consistency bug, or a TLS handshake error talking to S3. The\ninfrastructure world is a messy place, so you should expect intermittent failures in\nyour tests and handle them accordingly.\nTo make your tests a bit more resilient, you can add retries for known errors. For\nexample, while writing this book, I’ d occasionally get the following type of error,\nespecially when running many tests in parallel:\n* error loading the remote state: RequestError: send request failed\nPost https://xxx.amazonaws.com/: dial tcp xx.xx.xx.xx:443:\nconnect: connection refused\nTo make tests more reliable in the face of such errors, you can enable retries in Ter‐\nratest using the MaxRetries , TimeBetweenRetries , and RetryableTerraformErrors\narguments of terraform.Options :\nfunc createHelloOpts (\ndbOpts *terraform .Options,\nterraformDir  string) *terraform .Options {\n360 | Chapter 9: How to Test Terraform Code\nreturn &terraform .Options{\nTerraformDir : terraformDir ,\nVars: map[string]interface {}{\n""db_remote_state_bucket"" : dbOpts.BackendConfig [""bucket"" ],\n""db_remote_state_key"" :    dbOpts.BackendConfig [""key""],\n""environment"" :            dbOpts.Vars[""db_name"" ],\n},\n// Retry up to 3 times, with 5 seconds between retries,\n// on known errors\nMaxRetries :         3,\nTimeBetweenRetries : 5 * time.Second,\nRetryableTerraformErrors : map[string]string{\n""RequestError: send request failed"" : ""Throttling issue?"" ,\n},\n}\n}\nIn the RetryableTerraformErrors  argument, you can specify a map of known errors\nthat warrant a retry: the keys of the map are the error messages to look for in the\nlogs (you can use regular expressions here), and the values are additional information\nto display in the logs when Terratest matches one of these errors and kicks off a\nretry. Now, whenever your test code hits one of these known errors, you should see\na message in your logs, followed by a sleep of TimeBetweenRetries , and then your\ncommand will rerun:\n$ go test -v -timeout 30m\n(...)\nRunning command terraform with args [apply -input=false -lock=false\n-auto-approve]\n(...)\n* error loading the remote state: RequestError: send request failed\nPost https://s3.amazonaws.com/: dial tcp 11.22.33.44:443:\nconnect: connection refused\n(...)\n'terraform [apply]' failed with the error 'exit status code 1'\nbut this error was expected and warrants a retry. Further details:\nIntermittent error, possibly due to throttling?\n(...)\nRunning command terraform with args [apply -input=false -lock=false\n-auto-approve]\nAutomated Tests | 361",26782
90-End-to-End Tests.pdf,90-End-to-End Tests,"End-to-End Tests\nNow that you have unit tests and integration tests in place, the final type of tests\nthat you might want to add are end-to-end  tests. With the Ruby web server example,\nend-to-end tests might consist of deploying the web server and any data stores it\ndepends on and testing it from the web browser using a tool such as Selenium. The\nend-to-end tests for Terraform infrastructure will look similar: deploy everything into\nan environment that mimics production, and test it from the end-user’s perspective.\nAlthough you could write your end-to-end tests using the exact same strategy as the\nintegration tests—that is, create a few dozen test stages to run terraform apply , do\nsome validations, and then run terraform destroy —this is rarely done in practice.\nThe reason for this has to do with the test pyramid , which you can see in Figure 9-1 .\nFigure 9-1. The test pyramid.\nThe idea of the test pyramid is that you should typically be aiming for a large number\nof unit tests (the bottom of the pyramid), a smaller number of integration tests (the\nmiddle of the pyramid), and an even smaller number of end-to-end tests (the top of\nthe pyramid). This is because, as you go up the pyramid, the cost and complexity of\nwriting the tests, the brittleness of the tests, and the runtime of the tests all increase.\nThat gives us key testing takeaway #5 : smaller modules are easier and faster to test.\nY ou saw in the previous sections that it required a fair amount of work with name‐\nspacing, dependency injection, retries, error handling, and test stages to test even\na relatively simple hello-world-app  module. With larger and more complicated\ninfrastructure, this only becomes more difficult. Therefore, you want to do as much\nof your testing as low in the pyramid as you can because the bottom of the pyramid\noffers the fastest, most reliable feedback loop.\nIn fact, by the time you get to the top of the test pyramid, running tests to deploy a\ncomplicated architecture from scratch becomes untenable for two main reasons:\nToo slow\nDeploying your entire architecture from scratch and then undeploying it all\nagain can take a very long time: on the order of several hours. Test suites that\n362 | Chapter 9: How to Test Terraform Code\ntake that long provide relatively little value because the feedback loop is simply\ntoo slow. Y ou’ d probably run such a test suite only overnight, which means in\nthe morning you’ll get a report about a test failure, you’ll investigate for a while,\nsubmit a fix, and then wait for the next day to see whether it worked. That limits\nyou to roughly one bug fix attempt per day. In these sorts of situations, what\nactually happens is developers begin blaming others for test failures, convince\nmanagement to deploy despite the test failures, and eventually ignore the test\nfailures entirely.\nToo brittle\nAs mentioned in the previous section, the infrastructure world is messy. As\nthe amount of infrastructure you’re deploying goes up, the odds of hitting an\nintermittent, flaky issue goes up as well. For example, suppose that a single\nresource (such as an EC2 Instance) has a one-in-a-thousand chance (0.1%) of\nfailing due to an intermittent error (actual failure rates in the DevOps world are\nlikely higher). This means that the probability that a test that deploys a single\nresource runs without any intermittent errors is 99.9%. So what about a test\nthat deploys two resources? For that test to succeed, you need both resources to\ndeploy without intermittent errors, and to calculate those odds, you multiply the\nprobabilities: 99.9% × 99.9% = 99.8%. With three resources, the odds are 99.9% ×\n99.9% × 99.9% = 99.7%. With N resources, the formula is 99.9%N.\nSo now let’s consider different types of automated tests. If you had a unit test of\na single module that deployed, say, 20 resources, the odds of success are 99.9%20\n= 98.0%. This means that 2 test runs out of 100 will fail; if you add a few retries,\nyou can typically make these tests fairly reliable. Now, suppose that you had\nan integration test of 3 modules that deployed 60 resources. Now the odds of\nsuccess are 99.9%60 = 94.1%. Again, with enough retry logic, you can typically\nmake these tests stable enough to be useful. So what happens if you want to write\nan end-to-end test that deploys your entire infrastructure, which consists of 30\nmodules, or about 600 resources? The odds of success are 99.9%600 = 54.9%. This\nmeans that nearly half of your test runs will fail for transient reasons!\nY ou’ll be able to handle some of these errors with retries, but it quickly turns\ninto a never-ending game of whack-a-mole. Y ou add a retry for a TLS handshake\ntimeout, only to be hit by an APT repo downtime in your Packer template; you\nadd retries to the Packer build, only to have the build fail due to a Terraform\neventual-consistency bug; just as you are applying the Band-Aid to that, the build\nfails due to a brief GitHub outage. And because end-to-end tests take so long, you\nget only one attempt, maybe two, per day to fix these issues.\nIn practice, very few companies with complicated infrastructure run end-to-end tests\nthat deploy everything from scratch . Instead, the more common test strategy for\nend-to-end tests works as follows:\nAutomated Tests | 363",5349
91-Other Testing Approaches.pdf,91-Other Testing Approaches,"1.One time, you pay the cost of deploying a persistent, production-like environ‐1.\nment called “test, ” and you leave that environment running.\n2.Every time someone makes a change to your infrastructure code, the end-to-end2.\ntest does the following:\na.Applies the infrastructure change to the test environment.a.\nb.Runs validations against the test environment (e.g., uses Selenium to test yourb.\ncode from the end-user’s perspective) to make sure everything is working.\nBy changing your end-to-end test strategy to applying only incremental changes,\nyou’re reducing the number of resources that are being deployed at test time from\nseveral hundred to just a handful so that these tests will be faster and less brittle.\nMoreover, this approach to end-to-end testing more closely mimics how you’ll be\ndeploying those changes in production. After all, it’s not like you tear down and bring\nup your production environment from scratch to roll out each change. Instead, you\napply each change incrementally, so this style of end-to-end testing offers a huge\nadvantage: you can test not only that your infrastructure works correctly but also that\nthe deployment process  for that infrastructure works correctly, too.\nOther Testing Approaches\nMost of this chapter has focused on testing your Terraform code by doing a full apply\nand destroy  cycle. This is the gold standard of testing, but there are three other types\nof automated tests you can use:\n•Static analysis•\n•Plan testing•\n•Server testing•\nJust as unit, integration, and end-to-end tests each catch different types of bugs, each\nof the testing approaches just mentioned will catch different types of bugs as well,\nso you’ll most likely want to use several of these techniques together to get the best\nresults. Let’s go through these new categories one at a time.\nStatic analysis\nStatic analysis  is the most basic way to test your Terraform code: you parse the code\nand analyze it without actually executing it in any way. Table 9-1  shows some of\nthe tools in this space that work with Terraform and how they compare in terms of\npopularity and maturity, based on stats I gathered from GitHub in February 2022.\n364 | Chapter 9: How to Test Terraform Code\nTable 9-1. A comparison of popular static analysis tools for Terraform\nterraform validate tfsec tflint Terrascan\nBrief description Built-in Terraform\ncommandSpot potential security\nissuesPluggable Terraform\nlinterDetect compliance and\nsecurity violations\nLicense (same as Terraform) MIT MPL 2.0 Apache 2.0\nBacking company (same as Terraform) Aqua Security (none) Accurics\nStars (same as Terraform) 3,874 2,853 2,768\nContributors (same as Terraform) 96 77 63\nFirst release (same as Terraform) 2019 2016 2017\nLatest release (same as Terraform) v1.1.2 v0.34.1 v1.13.0\nBuilt-in checks Syntax checks only AWS, Azure, GCP,\nKubernetes, DigitalOcean,\netc.AWS, Azure, and\nGCPAWS, Azure, GCP,\nKubernetes, etc.\nCustom checks Not supported Defined  in YAML or JSON Defined  in a Go\npluginDefined  in Rego\nThe simplest of these tools is terraform validate , which is built into Terraform\nitself, which can catch syntax issues. For example, if you forgot to set the alb_name\nparameter in examples/alb , and you ran validate , you would get output similar to\nthe following:\n$ terraform validate\n│ Error: Missing required argument\n│\n│   on main.tf line 20, in module ""alb"":\n│   20: module ""alb"" {\n│\n│ The argument ""alb_name"" is required, but no definition was found.\nNote that validate  is limited solely to syntactic checks, whereas the other tools allow\nyou to enforce other types of policies. For example, you can use tools such as tfsec\nand tflint  to enforce policies, such as:\n•Security groups cannot be too open: e.g., block inbound rules that allow access•\nfrom all IPs (CIDR block 0.0.0.0/0 ).\n•All EC2 Instances must follow a specific tagging convention.•\nThe idea here is to define  your policies as code , so you can enforce your security,\ncompliance, and reliability requirements as code. In the next few sections, you’ll see\nseveral other policy as code tools.\nAutomated Tests | 365\nStrengths of static analysis tools\n•They run fast.•\n•They are easy to use.•\n•They are stable (no flaky tests).•\n•Y ou don’t need to authenticate to a real provider (e.g., to a real AWS account).•\n•Y ou don’t have to deploy/undeploy real resources.•\nWeaknesses of static analysis tools\n•They are very limited in the types of errors they can catch. Namely, they can only•\ncatch errors that can be determined from statically reading the code, without\nexecuting it: e.g., syntax errors, type errors, and a small subset of business logic\nerrors. For example, you can detect a policy violation for static values, such as a\nsecurity group hardcoded to allow inbound access from CIDR block 0.0.0.0/0 ,\nbut you can’t detect policy violations from dynamic values, such as the same\nsecurity group but with the inbound CIDR block being read in from a variable or\nfile.\n•These tests aren’t checking functionality, so it’s possible for all the checks to pass•\nand the infrastructure still doesn’t work!\nPlan testing\nAnother  way to test your code is to run terraform plan  and to analyze the plan\noutput. Since you’re executing the code, this is more than static analysis, but it’s less\nthan a unit or integration test, as you’re not executing the code fully: in particular,\nplan  executes the read steps (e.g., fetching state, executing data sources) but not the\nwrite steps (e.g., creating or modifying resources). Table 9-2  shows some of the tools\nthat do plan  testing and how they compare in terms of popularity and maturity,\nbased on stats I gathered from GitHub in February 2022.\nTable 9-2. A comparison of popular plan testing tools for Terraform\nTerratest Open Policy\nAgent (OPA)HashiCorp Sentinel Checkov terraform-\ncompliance\nBrief description Go library for IaC\ntestingGeneral-\npurpose policy\nenginePolicy-as-code for\nHashiCorp enterprise\nproductsPolicy-as-code for\neveryoneBDD test\nframework for\nTerraform\nLicense Apache 2.0 Apache 2.0 Commercial /\nproprietary licenseApache 2.0 MIT\nBacking company Gruntwork Styra HashiCorp Bridgecrew (none)\nStars 5,888 6,207 (not open source) 3,758 1,104\nContributors 157 237 (not open source) 199 36\nFirst release 2016 2016 2017 2019 2018\n366 | Chapter 9: How to Test Terraform Code\nTerratest Open Policy\nAgent (OPA)HashiCorp Sentinel Checkov terraform-\ncompliance\nLatest release v0.40.0 v0.37.1 v0.18.5 2.0.810 1.3.31\nBuilt-in checks None None None AWS, Azure, GCP,\nKubernetes, etc.None\nCustom checks Defined  in Go Defined  in Rego Defined  in Sentinel Defined  in Python or\nYAMLDefined  in BDD\nSince you’re already familiar with Terratest, let’s take a quick look at how you can\nuse it to do plan  testing on the code in examples/alb . If you ran terraform plan\nmanually, here’s a snippet of the output you’ d get:\nTerraform will perform the following actions:\n  # module.alb.aws_lb.example will be created\n  + resource ""aws_lb"" ""example"" {\n      + arn                        = (known after apply)\n      + load_balancer_type         = ""application""\n      + name                       = ""test-4Ti6CP""\n      (...)\n    }\n  (...)\nPlan: 5 to add, 0 to change, 0 to destroy.\nHow can you test this output programmatically? Here’s the basic structure of a test\nthat uses Terratest’s InitAndPlan  helper to run init  and plan  automatically:\nfunc TestAlbExamplePlan (t *testing.T ) {\nt.Parallel ()\nalbName : = fmt.Sprintf (""test-%s"" , random.UniqueId ())\nopts := &terraform.Options {\n// You should update this relative  path to point at your alb\n// example directory!\nTerraformDir:  ""../examples/alb"" ,\nVars: map[string]interface {}{\n""alb_name"" : albName,\n},\n}\nplanString : = terraform.InitAndPlan (t, opts)\n}\nEven this minimal test offers some value, in that it validates that your code can\nsuccessfully run plan , which checks that the syntax is valid and that all the read API\ncalls work. But you can go even further. One small improvement is to check that you\nAutomated Tests | 367\nget the expected counts at the end of the plan: “5 to add, 0 to change, 0 to destroy. ”\nY ou can do this using the GetResourceCount  helper\n// An example of how to check the plan output's  add/change/destroy  counts\nresourceCounts : = terraform.GetResourceCount (t, planString )\nrequire.Equal (t, 5, resourceCounts.Add )\nrequire.Equal (t, 0, resourceCounts.Change )\nrequire.Equal (t, 0, resourceCounts.Destroy )\nY ou can do an even more thorough check by using the InitAndPlanAndShowWith\nStructNoLogTempPlanFile  helper to parse the plan  output into a struct , which\ngives you programmatic access to all the values and changes in that plan  output.\nFor example, you could check that the plan  output includes the aws_lb  resource at\naddress module.alb.aws_lb.example  and that the name  attribute of this resource is\nset to the expected value, as follows:\n// An example of how to check specific  values in the plan output\nplanStruct : =\nterraform.InitAndPlanAndShowWithStructNoLogTempPlanFile (t, opts)\nalb, exists : =\nplanStruct.ResourcePlannedValuesMap [""module.alb.aws_lb.example"" ]\nrequire.True (t, exists, ""aws_lb resource must exist"" )\nname, exists : = alb.AttributeValues [""name""]\nrequire.True (t, exists, ""missing name parameter"" )\nrequire.Equal (t, albName, name)\nThe strength of Terratest’s approach to plan testing is that it’s extremely flexible, as\nyou can write arbitrary Go code to check whatever you want. But this very same\nfactor is also, in some ways, a weakness, as it makes it harder to get started.\nSome teams prefer a more declarative language for defining their policies as code.\nIn the last few years, Open Policy Agent (OPA) has become a  popular policy-as-code\ntool, as it allows your to capture you company’s policies as code in a declarative\nlanguage called Rego.\nFor example, many companies have tagging policies they want to enforce. A com‐\nmon one with Terraform code is to ensure that every resource that is managed\nby Terraform has a ManagedBy = terraform  tag. Here is a simple policy called\nenforce_tagging.rego  you could use to check for this tag:\npackage terraform\nallow {\n   resource_change := input.resource_changes [_]\n   resource_change .change.after.tags[""ManagedBy"" ]\n}\n368 | Chapter 9: How to Test Terraform Code\nThis policy will look through the changes in a terraform plan  output, extract the\ntag ManagedBy , and set an OPA variable called allow  to true  if that tag is set or\nundefined  otherwise.\nNow, consider the following Terraform module:\nresource  ""aws_instance"" ""example""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n}\nThis module is not setting the required ManagedBy  tag. How can we catch that with\nOPA?\nThe first step is to run terraform plan  and to save the output to a plan file:\n$ terraform plan -out tfplan.binary\nOPA only operates on JSON, so the next step is to convert the plan file to JSON using\nthe terraform show  command:\n$ terraform show -json tfplan.binary > tfplan.json\nFinally, you can run the opa eval  command to check this plan file against the\nenforce_tagging.rego  policy:\n$ opa eval \\n  --data enforce_tagging.rego \\n  --input tfplan.json \\n  --format pretty \\n  data.terraform.allow\nundefined\nSince the ManagedBy  tag was not set, the output from OPA is undefined . Now, try\nsetting the ManagedBy  tag:\nresource  ""aws_instance"" ""example""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n  tags = {\n    ManagedBy  = ""terraform""\n  }\n}\nRerun terraform plan , terraform show , and opa eval :\n$ terraform plan -out tfplan.binary\n$ terraform show -json tfplan.binary > tfplan.json\n$ opa eval \\n  --data enforce_tagging.rego \\nAutomated Tests | 369\n  --input tfplan.json \\n  --format pretty \\n  data.terraform.allow\ntrue\nThis time, the output is true , which means the policy has passed.\nUsing tools like OPA, you can enforce your company’s requirements by creating a\nlibrary of such policies and setting up a CI/CD pipeline that runs these policies\nagainst your Terraform modules after every commit.\nStrengths of plan testing tools\n•They run fast—not quite as fast as pure static analysis but much faster than unit•\nor integration tests.\n•They are somewhat easy to use—not quite as easy as pure static analysis but•\nmuch easier than unit or integration tests.\n•They are stable (few flaky tests)—not quite as stable as pure static analysis but•\nmuch more stable than unit or integration tests.\n•Y ou don’t have to deploy/undeploy real resources.•\nWeaknesses of plan testing tools\n•They are limited in the types of errors they can catch. They can catch more than•\npure static analysis but nowhere near as many errors as unit and integration\ntesting.\n•Y ou have to authenticate to a real provider (e.g., to a real AWS account). This is•\nrequired for plan  to work.\n•These tests aren’t checking functionality, so it’s possible for all the checks to pass•\nand the infrastructure still doesn’t work!\nServer testing\nThere are a set of testing tools that are focused on testing that your servers (including\nvirtual servers) have been properly configured. I’m not aware of any common name\nfor these sorts of tools, so I’ll call it server testing . These are not general-purpose\ntools for testing all aspects of your Terraform code. In fact, most of these tools were\noriginally built to be used with configuration management tools, such as Chef and\nPuppet, which were entirely focused on launching servers. However, as Terraform has\ngrown in popularity, it’s now very common to use it to launch servers, and these tools\ncan be helpful for validating that the servers you launched are working. Table 9-3\nshows some of the tools that do server testing and how they compare in terms of\npopularity and maturity, based on stats I gathered from GitHub in February 2022.\n370 | Chapter 9: How to Test Terraform Code\nTable 9-3. A comparison of popular server testing tools\nInSpec Serverspec Goss\nBrief description Auditing and testing framework RSpec tests for your servers Quick and easy server testing/validation\nLicense Apache 2.0 MIT Apache 2.0\nBacking company Chef (none) (none)\nStars 2,472 2,426 4,607\nContributors 279 128 89\nFirst release 2016 2013 2015\nLatest release v4.52.9 v2.42.0 v0.3.16\nBuilt-in checks None None None\nCustom checks Defined  in a Ruby-based DSL Defined  in a Ruby-based DSL Defined  in YAML\nMost of these tools provide a simple domain-specific  language  (DSL) for checking that\nthe servers you’ve deployed conform to some sort of specification. For example, if\nyou were testing a Terraform module that deployed an EC2 Instance, you could use\nthe following inspec  code to validate that the Instance has proper permissions on\nspecific files, has certain dependencies installed, and is listening on a specific port:\ndescribe  file('/etc/myapp.conf' ) do\n  it { should exist }\n  its('mode') { should cmp 0644 }\nend\ndescribe  apache_conf  do\n  its('Listen' ) { should cmp 8080 }\nend\ndescribe  port(8080) do\n  it { should be_listening  }\nend\nStrengths of server testing tools\n•They make it easy to validate specific properties of servers. The DSLs these tools•\noffer are much easier to use for common checks than doing it all from scratch.\n•Y ou can build up a library of policy checks. Because each individual check is•\nquick to write, per the previous bullet point, these tools tend to be a good way\nto validate a checklist of requirements, especially around compliance (e.g., PCI\ncompliance, HIPAA compliance, etc.).\n•They can catch many types of errors. Since you actually have to run apply  and •\nyou validate a real, running server, these types of tests catch far more types of\nerrors than pure static analysis or plan testing.\nAutomated Tests | 371",16011
92-Conclusion.pdf,92-Conclusion,"Weaknesses of server testing tools\n•They are not as fast. These tests only work on servers that are deployed, so you•\nhave to run the full apply  (and perhaps destroy ) cycle, which can take a long\ntime.\n•They are not as stable (some flaky tests). Since you have to run apply  and wait •\nfor real servers to deploy, you will hit various intermittent issues and occasionally\nhave flaky tests.\n•Y ou have to authenticate to a real provider (e.g., to a real AWS account). This is•\nrequired for the apply  to work to deploy the servers, plus, these server testing\ntools all require additional authentication methods—e.g., SSH—to connect to the\nservers you’re testing.\n•Y ou have to deploy/undeploy real resources. This takes time and costs money.•\n•They only thoroughly check that servers work and not other types of infrastruc‐•\nture.\n•These tests aren’t checking functionality, so it’s possible for all the checks to pass•\nand the infrastructure still doesn’t work!\nConclusion\nEverything in the infrastructure world is continuously changing: Terraform, Packer,\nDocker, Kubernetes, AWS, Google Cloud, Azure, and so on are all moving targets.\nThis means that infrastructure code rots very quickly. Or to put it another way:\nInfrastructure code without automated tests is broken.\nI mean this both as an aphorism and as a literal statement. Every single time I’ve gone\nto write infrastructure code, no matter how much effort I’ve put into keeping the\ncode clean, testing it manually, and doing code reviews, as soon as I’ve taken the time\nto write automated tests, I’ve found numerous nontrivial bugs. Something magical\nhappens when you take the time to automate the testing process and, almost without\nexception, it flushes out problems that you otherwise would’ve never found yourself\n(but your customers would’ve). And not only do you find these bugs when you first\nadd automated tests, but if you run your tests after every commit, you’ll keep finding\nbugs over time, especially as the DevOps world changes all around you.\nThe automated tests I’ve added to my infrastructure code have caught bugs not\nonly in my own code but also in the tools I was using, including nontrivial bugs in\nTerraform, Packer, Elasticsearch, Kafka, AWS, and so on. Writing automated tests as\nshown in this chapter is not easy: it takes considerable effort to write these tests, it\ntakes even more effort to maintain them and add enough retry logic to make them\nreliable, and it takes still more effort to keep your test environment clean to keep\ncosts in check. But it’s all worth it.\n372 | Chapter 9: How to Test Terraform Code\nWhen I build a module to deploy a data store, for example, after every commit to\nthat repo, my tests fire up a dozen copies of that data store in various configurations,\nwrite data, read data, and then tear everything back down. Each time those tests pass,\nthat gives me huge confidence that my code still works. If nothing else, the automated\ntests let me sleep better. Those hours I spent dealing with retry logic and eventual\nconsistency pay off in the hours I won’t be spending at 3 a.m. dealing with an outage.\nThis Book Has Tests, Too!\nAll of the code examples in this book have tests, too. Y ou can find\nall of the code examples, and all of their corresponding tests, at\nGitHub .\nThroughout this chapter, you saw the basic process of testing Terraform code, includ‐\ning the following key takeaways:\nWhen testing Terraform code, you can’t use localhost\nTherefore, you need to do all of your manual testing by deploying real resources\ninto one or more isolated sandbox environments.\nYou cannot do pure unit testing for Terraform code\nTherefore, you have to do all of your automated testing by writing code that\ndeploys real resources into one or more isolated sandbox environments.\nRegularly clean up your sandbox environments\nOtherwise, the environments will become unmanageable, and costs will spiral\nout of control.\nYou must namespace all of your resources\nThis ensures that multiple tests running in parallel do not conflict with one\nanother.\nSmaller modules are easier and faster to test\nThis was one of the key takeaways in Chapter 8 , and it’s worth repeating in this\nchapter, too: smaller modules are easier to create, maintain, use, and test.\nY ou also saw a number of different testing approaches throughout this chapter: unit\ntesting, integration testing, end-to-end testing, static analysis, and so on. Table 9-4\nshows the trade-offs between these different types of tests.\nConclusion | 373\nTable 9-4. A comparison of testing approaches (more black squares is better)\nStatic\nanalysisPlan testing Server\ntestingUnit tests Integration\ntestsEnd-to-end\ntests\nFast to run ■■■■■ ■■■■□ ■■■□□ ■■□□□ ■□□□□ □□□□□\nCheap to run ■■■■■ ■■■■□ ■■■□□ ■■□□□ ■□□□□ □□□□□\nStable and reliable ■■■■■ ■■■■□ ■■■□□ ■■□□□ ■□□□□ □□□□□\nEasy to use ■■■■■ ■■■■□ ■■■□□ ■■□□□ ■□□□□ □□□□□\nCheck syntax ■■■■■ ■■■■■ ■■■■■ ■■■■■ ■■■■■ ■■■■■\nCheck policies ■■□□□ ■■■■□ ■■■■□ ■■■■■ ■■■■■ ■■■■■\nCheck servers work □□□□□ □□□□□ ■■■■■ ■■■■■ ■■■■■ ■■■■■\nCheck other infrastructure\nworks□□□□□ □□□□□ ■■□□□ ■■■■□ ■■■■■ ■■■■■\nCheck all the infrastructure\nworks together□□□□□ □□□□□ □□□□□ ■□□□□ ■■■□□ ■■■■■\nSo which testing approach should you use? The answer is: a mix of all of them! Each\ntype of test has strengths and weaknesses, so you have to combine multiple types of\ntests to be confident your code works as expected. That doesn’t mean that you use\nall the different types of tests in equal proportion: recall the test pyramid and how,\nin general, you’ll typically want lots of unit tests, fewer integration tests, and only a\nsmall number of high-value end-to-end tests. Moreover, you don’t have to add all the\ndifferent types of tests at once. Instead, pick the ones that give you the best bang for\nyour buck and add those first. Almost any testing is better than none, so if all you can\nadd for now is static analysis, then use that as a starting point, and build on top of it\nincrementally.\nLet’s now move on to Chapter 10 , where you’ll see how to incorporate Terraform code\nand your automated test code into your team’s workflow, including how to manage\nenvironments, how to configure a CI/CD pipeline, and more.\n374 | Chapter 9: How to Test Terraform Code",6340
93-Adopting IaC in Your Team.pdf,93-Adopting IaC in Your Team,"CHAPTER 10\nHow to Use Terraform as a Team\nAs you’ve been reading this book and working through the code samples, you’ve most\nlikely been working by yourself. In the real world, you’ll most likely be working as\npart of a team, which introduces a number of new challenges. Y ou may need to find\na way to convince your team to use Terraform and other infrastructure-as-code (IaC)\ntools. Y ou may need to deal with multiple people concurrently trying to understand,\nuse, and modify the Terraform code you write. And you may need to figure out how\nto fit Terraform into the rest of your tech stack and make it a part of your company’s\nworkflow.\nIn this chapter, I’ll dive into the key processes you need to put in place to make\nTerraform and IaC work for your team:\n•Adopting infrastructure as code in your team•\n•A workflow for deploying application code•\n•A workflow for deploying infrastructure code•\n•Putting it all together•\nLet’s go through these topics one at a time.\nExample Code\nAs a reminder, you can find all of the code examples in the book on\nGitHub .\n375",1083
94-Convince Your Boss.pdf,94-Convince Your Boss,"Adopting IaC in Your Team\nIf your team is used to managing all of your infrastructure by hand, switching to\ninfrastructure as code requires more than just introducing a new tool or technology.\nIt also requires changing the culture and processes of the team. Changing culture\nand process is a significant undertaking, especially at larger companies. Because every\nteam’s culture and process is a little different, there’s no one-size-fits-all way to do it,\nbut here are a few tips that will be useful in most situations:\n•Convince your boss•\n•Work incrementally•\n•Give your team the time to learn•\nConvince Your Boss\nI’ve seen this story play out many times at many companies: a developer discovers\nTerraform, becomes inspired by what it can do, shows up to work full of enthusiasm\nand excitement, shows Terraform to everyone… and the boss says “no. ” The devel‐\noper, of course, becomes frustrated and discouraged. Why doesn’t everyone else see\nthe benefits of this? We could automate everything! We could avoid so many bugs!\nHow else can we pay down all this tech debt? How can you all be so blind??\nThe problem is that although this developer sees all the benefits of adopting an IaC\ntool such as Terraform, they aren’t seeing all the costs. Here are just a few of the costs\nof adopting IaC:\nSkills gap\nThe move to IaC means that your Ops team will need to spend most of its time\nwriting large amounts of code: Terraform modules, Go tests, Chef recipes, and\nso on. Whereas some Ops engineers are comfortable with coding all day and\nwill love the change, others will find this a tough transition. Many Ops engineers\nand sysadmins are used to making changes manually, with perhaps an occasional\nshort script here or there, and the move to doing software engineering nearly full\ntime might require learning a number of new skills or hiring new people.\nNew tools\nSoftware developers can become attached to the tools they use; some are nearly\nreligious about it. Every time you introduce a new tool, some developers will\nbe thrilled at the opportunity to learn something new, but others will prefer to\nstick to what they know and may resist having to invest lots of time and energy\nlearning new languages and techniques.\n376 | Chapter 10: How to Use Terraform as a Team\nChange in mindset\nIf your team members are used to managing infrastructure manually, they are\nused to making all of their changes directly : for example, by SSHing to a server\nand executing a few commands. The move to IaC requires a shift in mindset\nwhere you make all of your changes indirectly , first by editing code, then check‐\ning it in, and then letting some automated process apply the changes. This layer\nof indirection can be frustrating; for simple tasks, it’ll feel slower than the direct\noption, especially when you’re still learning a new IaC tool and are not efficient\nwith it.\nOpportunity cost\nIf you choose to invest your time and resources in one project, you are implicitly\nchoosing not to invest that time and resources in other projects. What projects\nwill have to be put on hold so that you can migrate to IaC? How important are\nthose projects?\nSome developers on your team will look at this list and become excited. But many\nothers will groan—including your boss. Learning new skills, mastering new tools, and\nadopting new mindsets may or may not be beneficial, but one thing is certain: it is\nnot free. Adopting IaC is a significant investment, and as with any investment, you\nneed to consider not only the potential upside but also the potential downsides.\nY our boss in particular will be sensitive to the opportunity cost. One of the key\nresponsibilities of any manager is to make sure the team is working on the highest-\npriority projects. When you show up and excitedly start talking about Terraform,\nwhat your boss might really be hearing is, “Oh no, this sounds like a massive under‐\ntaking. How much time is it going to take?” It’s not that your boss is blind to what\nTerraform can do, but if you are spending time on that, you might not have time to\ndeploy the new app the search team has been asking about for months, or to prepare\nfor the Payment Card Industry (PCI) audit, or to dig into the outage from last week.\nSo, if you want to convince your boss that your team should adopt IaC, your goal\nis not to prove that it has value but that it will bring more value to your team than\nanything else you could work on during that time.\nOne of the least effective ways to do this is to just list the features of your favorite\nIaC tool: for example, Terraform is declarative, it’s popular, it’s open source. This\nis one of many areas where developers would do well to learn from salespeople.\nMost salespeople know that focusing on features is typically an ineffective way to sell\nproducts. A slightly better technique is to focus on benefits: that is, instead of talking\nabout what a product can do (“product X can do Y!”), you should talk about what\nthe customer can do by using that product (“you can do Y by using product X!”). In\nother words, show the customer what new superpowers your product can give them.\nAdopting IaC in Your Team | 377\nFor example, instead of telling your boss that Terraform is declarative, talk about how\nyour infrastructure will be far easier to maintain. Instead of talking about the fact\nthat Terraform is popular, talk about how you’ll be able to leverage lots of existing\nmodules and plugins to get things done faster. And instead of explaining to your boss\nthat Terraform is open source, help your boss see how much easier it will be to hire\nnew developers for the team from a large, active open source community.\nFocusing on benefits is a great start, but the best salespeople know an even more\neffective strategy: focus on the problems. If you watch a great salesperson talking\nto a customer, you’ll notice that it’s actually the customer that does most of the\ntalking. The salesperson spends most of their time listening and looking for one\nspecific thing: What is the key problem that customer is trying to solve? What’s the\nbiggest pain point? Instead of trying to sell some sort of features or benefits, the best\nsalespeople try to solve their customer’s problems. If that solution happens to include\nthe product they are selling, all the better, but the real focus is on problem solving,\nnot selling.\nTalk to your boss and try to understand the most important problems they are\nworking on that quarter or that year. Y ou might find that those problems would not\nbe solved by IaC. And that’s OK! It might be slightly heretical for the author of a book\non Terraform to say this, but not every team needs IaC. Adopting IaC has a relatively\nhigh cost, and although it will pay off in the long term for some scenarios, it won’t\nfor others; for example, if you’re at a tiny startup with just one Ops person, or you’re\nworking on a prototype that might be thrown away in a few months, or you’re just\nworking on a side project for fun, managing infrastructure by hand is often the right\nchoice. Sometimes, even if IaC would be a great fit for your team, it won’t be the\nhighest priority, and given limited resources, working on other projects might still be\nthe right choice.\nIf you do find that one of the key problems your boss is focused on can be solved\nwith IaC, then your goal is to show your boss what that world looks like. For\nexample, perhaps the biggest issue your boss is focused on this quarter is improving\nuptime. Y ou’ve had numerous outages the last few months, many hours of downtime,\ncustomers are complaining, and the CEO is breathing down your manager’s neck,\nchecking in daily to see how things are going. Y ou dig in and find out that more\nthan half of these outages were caused by a manual error during deployment: e.g.,\nsomeone accidentally skipped an important step during the rollout process, or a\nserver was misconfigured, or the infrastructure in staging didn’t match what you had\nin production.\nNow, when you talk to your boss, instead of talking about Terraform features or\nbenefits, lead with the following: “I have an idea for how to reduce our outages by\nhalf. ” I guarantee this will get your boss’s attention. Use this opportunity to paint a\npicture for your boss of a world in which your deployment process is fully automated,\n378 | Chapter 10: How to Use Terraform as a Team",8457
95-Give Your Team the Time to Learn.pdf,95-Give Your Team the Time to Learn,"1The Standish Group, “CHAOS Manifesto 2013: Think Big, Act Small, ” 2013, https://oreil.ly/ydaWQ .\n2Dan Milstein, “How to Survive a Ground-Up Rewrite Without Losing Y our Sanity, ” OnStartups.com, April 8,\n2013, https://oreil.ly/nOGrU .reliable, and repeatable so that the manual errors that caused half of your previous\noutages are no longer possible. Not only that, but if deployment is automated, you\ncan also add automated tests, reducing outages further and allowing the whole com‐\npany to deploy twice as often. Let your boss dream of being the one to tell the\nCEO that they’ve managed to cut outages in half and double deployments. And then\nmention that, based on your research, you believe you can deliver this future world\nusing Terraform.\nThere’s no guarantee that your boss will say yes, but your odds are quite a bit higher\nwith this approach. And your odds get even better if you work incrementally.\nWork Incrementally\nOne of the most important lessons I’ve learned in my career is that most large\nsoftware projects fail. Whereas roughly 3 out of 4 small IT projects (less than $1\nmillion) are completed successfully, only 1 out of 10 large projects (greater than $10\nmillion) are completed on time and on budget, and more than one-third of large\nprojects are never completed at all.1\nThis is why I always get worried when I see a team try to not only adopt IaC but to do\nso all at once, across a huge amount of infrastructure, across every team, and often as\npart of an even bigger initiative. I can’t help but shake my head when I see the CEO\nor CTO of a large company give marching orders that everything must be migrated\nto the cloud, the old datacenters must be shut down, and that everyone will “do\nDevOps” (whatever that means), all within six months. I’m not exaggerating when I\nsay that I’ve seen this pattern several dozen times, and without exception, every single\none of these initiatives has failed. Inevitably, two to three years later, every one of\nthese companies is still working on the migration, the old datacenter is still running,\nand no one can tell whether they are really doing DevOps.\nIf you want to successfully adopt IaC, or if you want to succeed at any other type of\nmigration project, the only sane way to do it is incrementally. The key to incremental‐\nism is not just splitting up the work into a series of small steps but splitting up the\nwork in such a way that every step brings its own value—even if the later steps never\nhappen.\nTo understand why this is so important, consider the opposite, false incrementalism .2\nSuppose that you do a huge migration project, broken up into several small steps,\nbut the project doesn’t offer any real value until the very final step is completed. For\nexample, the first step is to rewrite the frontend, but you don’t launch it, because it\nAdopting IaC in Your Team | 379\nrelies on a new backend. Then, you rewrite the backend, but you don’t launch that\neither, because it doesn’t work until data is migrated to a new data store. And then,\nfinally, the last step is to do the data migration. Only after this last step do you finally\nlaunch everything and begin realizing any value from doing all this work. Waiting\nuntil the very end of a project to get any value is a big risk. If that project is canceled\nor put on hold or significantly changed partway through, you might get zero value\nout of it, despite a lot of investment.\nIn fact, this is exactly what happens with many large migration projects. The project\nis big to begin with, and like most software projects, it takes much longer than\nexpected. During that time, market conditions change, or the original stakeholders\nlose patience (e.g., the CEO was OK with spending three months to clean up tech\ndebt, but after 12 months, it’s time to begin shipping new products), and the project\nends up getting canceled before completion. With false incrementalism, this gives you\nthe worst possible outcome: you’ve paid a huge cost and received absolutely nothing\nin return.\nTherefore, incrementalism is essential. Y ou want each part of the project to deliver\nsome value so that even if the project doesn’t finish, no matter what step you got\nto, it was still worth doing. The best way to accomplish this is to focus on solving\none, small, concrete problem at a time. For example, instead of trying to do a “big\nbang” migration to the cloud, try to identify one, small, specific app or team that is\nstruggling, and work to migrate just them. Or instead of trying to do a “big bang”\nmove to “DevOps, ” try to identify a single, small, concrete problem (e.g., outages\nduring deployment) and put in place a solution for that specific problem (e.g.,\nautomate the most problematic deployment with Terraform).\nIf you can get a quick win by fixing one real, concrete problem right away, and\nmaking one team successful, you’ll begin to build momentum. That team can become\nyour cheerleader and help convince other teams to migrate, too. Fixing the specific\ndeployment issue can make the CEO happy and get you support to use IaC for\nmore projects. This will allow you to go for another quick win, and another one\nafter that. And if you can keep repeating this process—delivering value early and\noften—you’ll be far more likely to succeed at the larger migration effort. But even if\nthe larger migration doesn’t work out, at least one team is more successful now and\none deployment process works better, so it was still worth the investment.\nGive Your Team the Time to Learn\nI hope that, at this point, it’s clear that adopting IaC can be a significant investment.\nIt’s not something that will happen overnight. It’s not something that will happen\nmagically, just because the manager gives you a nod. It will happen only through\na deliberate effort of getting everyone on board, making learning resources (e.g.,\n380 | Chapter 10: How to Use Terraform as a Team\ndocumentation, video tutorials, and, of course, this book!) available, and providing\ndedicated time for team members to ramp up.\nIf your team doesn’t get the time and resources that it needs, then your IaC migration\nis unlikely to be successful. No matter how nice your code is, if your entire team isn’t\non board with it, here’s how it will play out:\n1.One developer on the team is passionate about IaC and spends a few months1.\nwriting beautiful Terraform code and using it to deploy lots of infrastructure.\n2.The developer is happy and productive, but unfortunately, the rest of the team2.\ndid not get the time to learn and adopt Terraform.\n3.Then, the inevitable happens: an outage. One of your team members needs to3.\ndeal with it, and they have two options: either (A) fix the outage the way they’ve\nalways done it, by making changes manually, which takes a few minutes, or (B)\nfix the outage by using Terraform, but they aren’t familiar with it, so this could\ntake hours or days. Y our team members are probably reasonable, rational people\nand will almost always choose option A.\n4.Now, as a result of the manual change, the Terraform code no longer matches4.\nwhat’s actually deployed. Therefore, next time someone on your team tries to use\nTerraform, there’s a chance that they will get a weird error. If they do, they will\nlose trust in the Terraform code and once again fall back to option A, making\nmore manual changes. This makes the code even more out of sync with reality,\nso the odds of the next person getting a weird Terraform error are even higher,\nand you quickly get into a cycle in which team members make more and more\nmanual changes.\n5.In a remarkably short time, everyone is back to doing everything manually, the5.\nTerraform code is completely unusable, and the months spent writing it are a\ntotal waste.\nThis scenario isn’t hypothetical but something I’ve seen happen at many different\ncompanies. They have large, expensive codebases full of beautiful Terraform code that\nare just gathering dust. To avoid this scenario, you need to not only convince your\nboss that you should use Terraform but also give everyone on the team the time they\nneed to learn the tool and internalize how to use it so that when the next outage\nhappens, it’s easier to fix it in code than it is to do it by hand.\nOne thing that can help teams adopt IaC faster is to have a well-defined process for\nusing it. When you’re learning or using IaC on a small team, running it ad hoc on\na developer’s computer is good enough. But as your company and IaC usage grows,\nyou’ll want to define a more systematic, repeatable, automated workflow for how\ndeployments happen.\nAdopting IaC in Your Team | 381",8695
96-A Workflow for Deploying Application Code.pdf,96-A Workflow for Deploying Application Code,,0
97-Run the Code Locally.pdf,97-Run the Code Locally,"A Workflow  for Deploying Application Code\nIn this section, I’ll introduce a typical workflow for taking application code (e.g., a\nRuby on Rails or Java/Spring app) from development all the way to production. This\nworkflow is reasonably well understood in the DevOps industry, so you’ll probably\nbe familiar with parts of it. Later in this chapter, I’ll talk about a workflow for taking\ninfrastructure code (e.g., Terraform modules) from development to production. This\nworkflow is not nearly as well known in the industry, so it will be helpful to compare\nthat workflow side by side with the application workflow to understand how to\ntranslate each application code step to an analogous infrastructure code step.\nHere’s what the application code workflow looks like:\n1.Use version control.1.\n2.Run the code locally.2.\n3.Make code changes.3.\n4.Submit changes for review.4.\n5.Run automated tests.5.\n6.Merge and release.6.\n7.Deploy.7.\nLet’s go through these steps one at a time.\nUse Version Control\nAll of your code should be in version control. No exceptions. It was the #1 item on\nthe classic Joel Test  when Joel Spolsky created it more than 20 years ago, and the only\nthings that have changed since then are that (a) with tools like GitHub, it’s easier than\never to use version control and (b) you can represent more and more things as code.\nThis includes documentation (e.g., a README written in Markdown), application\nconfiguration (e.g., a config file written in YAML), specifications (e.g., test code\nwritten with RSpec), tests (e.g., automated tests written with JUnit), databases (e.g.,\nschema migrations written in ActiveRecord), and of course, infrastructure.\nAs in the rest of this book, I’m going to assume that you’re using Git for version\ncontrol. For example, here is how you can check out the code repo for this book:\n$ git clone https://github.com/brikis98/terraform-up-and-running-code.git\nBy default, this checks out the main  branch of your repo, but you’ll most likely do\nall of your work in a separate branch. Here’s how you can create a branch called\nexample-feature  and switch to it by using the git checkout  command:\n382 | Chapter 10: How to Use Terraform as a Team",2225
98-Deploy.pdf,98-Deploy,"$ cd terraform-up-and-running-code\n$ git checkout -b example-feature\nSwitched to a new branch 'example-feature'\nRun the Code Locally\nNow  that the code is on your computer, you can run it locally. Y ou may recall the\nRuby web server example from Chapter 9 , which you can run as follows:\n$ cd code/ruby/10-terraform/team\n$ ruby web-server.rb\n[2019-06-15 15:43:17] INFO  WEBrick 1.3.1\n[2019-06-15 15:43:17] INFO  ruby 2.3.7 (2018-03-28) [universal.x86_64-darwin17]\n[2019-06-15 15:43:17] INFO  WEBrick::HTTPServer#start: pid=28618 port=8000\nNow you can manually test it with curl :\n$ curl http://localhost:8000\nHello, World\nAlternatively, you can run the automated tests:\n$ ruby web-server-test.rb\n(...)\nFinished in 0.633175 seconds.\n--------------------------------------------\n8 tests, 24 assertions, 0 failures, 0 errors\n100% passed\n--------------------------------------------\nThe key thing to notice is that both manual and automated tests for application code\ncan run completely locally on your own computer. Y ou’ll see later in this chapter that\nthis is not true for the same part of the workflow for infrastructure changes.\nMake Code Changes\nNow that you can run the application code, you can begin making changes. This is an\niterative process in which you make a change, rerun your manual or automated tests\nto see whether the change worked, make another change, rerun the tests, and so on.\nFor example, you can change the output of web-server.rb  to “Hello, World v2, ” restart\nthe server, and see the result:\n$ curl http://localhost:8000\nHello, World v2\nY ou might also update and rerun the automated tests. The idea in this part of the\nworkflow is to optimize the feedback loop so that the time between making a change\nand seeing whether it worked is minimized.\nA Workflow  for Deploying Application Code | 383\nAs you work, you should regularly be committing your code, with clear commit\nmessages explaining the changes you’ve made:\n$ git commit -m ""Updated Hello, World text""\nSubmit Changes for Review\nEventually, the code and tests will work the way you want them to, so it’s time to\nsubmit your changes for a code review. Y ou can do this with a separate code review\ntool (e.g., Phabricator or Review Board) or, if you’re using GitHub, you can create\na pull request . There are several different ways to create a pull request. One of the\neasiest is to git push  your example-feature  branch back to origin  (that is, back to\nGitHub itself), and GitHub will automatically print out a pull request URL in the log\noutput:\n$ git push origin example-feature\n(...)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: Create a pull request for 'example-feature' on GitHub by visiting:\nremote:      https://github.com/<OWNER>/<REPO>/pull/new/example-feature\nremote:\nOpen that URL in your browser, fill out the pull request title and description, and\nthen click Create. Y our team members will now be able to review the changes, as\nshown in Figure 10-1 .\nFigure 10-1. Your team members can review your code changes in a GitHub pull request.\n384 | Chapter 10: How to Use Terraform as a Team\nRun Automated Tests\nY ou should set up commit hooks to run automated tests for every commit you\npush to your version control system. The most common way to do this is to use a\ncontinuous integration  (CI) server, such as Jenkins, CircleCI, or GitHub Actions. Most\npopular CI servers have integrations built in specifically for GitHub, so not only does\nevery commit automatically run tests, but the output of those tests shows up in the\npull request itself, as shown in Figure 10-2 .\nY ou can see in Figure 10-2  that CircleCI has run unit tests, integration tests, end-\nto-end tests, and some static analysis checks (in the form of security vulnerability\nscanning using a tool called snyk ) against the code in the branch, and everything\npassed.\nFigure 10-2. GitHub pull request showing automated test results from CircleCI.\nA Workflow  for Deploying Application Code | 385\nMerge and Release\nY our  team members should review your code changes, looking for potential bugs,\nenforcing coding guidelines (more on this later in the chapter), checking that the\nexisting tests passed, and ensuring that you’ve added tests for any new behavior. If\neverything looks good, your code can be merged into the main  branch.\nThe next step is to release the code. If you’re using  immutable infrastructure practices\n(as discussed in “Server Templating Tools” on page 7), releasing application code\nmeans packaging that code into a new, immutable, versioned artifact. Depending on\nhow you want to package and deploy your application, the artifact can be a new\nDocker image, a new virtual machine image (e.g., new AMI), a new .jar file, a new .tar\nfile, etc. Whatever format you pick, make sure the artifact is immutable (i.e., you\nnever change it) and that it has a unique version number (so you can distinguish this\nartifact from all of the others).\nFor example, if you are packaging your application using Docker, you can store the\nversion number in a Docker tag. Y ou could use the ID of the commit (the sha1 hash)\nas the tag so that you can map the Docker image you’re deploying back to the exact\ncode it contains:\n$ commit_id=$(git rev-parse HEAD)\n$ docker build -t brikis98/ruby-web-server:$commit_id .\nThe preceding code will build a new Docker image called brikis98/ruby-web-\nserver  and tag it with the ID of the most recent commit, which will look something\nlike 92e3c6380ba6d1e8c9134452ab6e26154e6ad849 . Later on, if you’re debugging an\nissue in a Docker image, you can see the exact code it contains by checking out the\ncommit ID the Docker image has as a tag:\n$ git checkout 92e3c6380ba6d1e8c9134452ab6e26154e6ad849\nHEAD is now at 92e3c63 Updated Hello, World text\nOne downside to commit IDs is that they aren’t very readable or memorable. An\nalternative is to create a Git tag:\n$ git tag -a ""v0.0.4"" -m ""Update Hello, World text""\n$ git push --follow-tags\nA tag is a pointer to a specific Git commit but with a friendlier name. Y ou can use this\nGit tag on your Docker images:\n$ git_tag=$(git describe --tags)\n$ docker build -t brikis98/ruby-web-server:$git_tag .\nThus, when you’re debugging, check out the code at a specific tag:\n$ git checkout v0.0.4\nNote: checking out 'v0.0.4'.\n386 | Chapter 10: How to Use Terraform as a Team\n(...)\nHEAD is now at 92e3c63 Updated Hello, World text\nDeploy\nNow that you have a versioned artifact, it’s time to deploy it. There are many different\nways to deploy application code, depending on the type of application, how you\npackage it, how you want to run it, your architecture, what tools you’re using, and so\non. Here are a few of the key considerations:\n•Deployment tooling•\n•Deployment strategies•\n•Deployment server•\n•Promotion across environments•\nDeployment tooling\nThere are many different tools that you can use to deploy your application, depend‐\ning on how you package it and how you want to run it. Here are a few examples:\nTerraform\nAs you’ve seen in this book, you can use Terraform to deploy certain types of\napplications. For example, in earlier chapters, you created a module called asg-\nrolling-deploy  that could do a zero-downtime rolling deployment across an\nASG. If you package your application as an AMI (e.g., using Packer), you could\ndeploy new AMI versions with the asg-rolling-deploy  module by updating the\nami parameter in your Terraform code and running terraform apply .\nOrchestration tools\nThere are a number of orchestration tools designed to deploy and manage appli‐\ncations, such as Kubernetes (arguably the most popular Docker orchestration\ntool), Amazon ECS, HashiCorp Nomad, and Apache Mesos. In Chapter 7 , you\nsaw an example of how to use Kubernetes to deploy Docker containers.\nScripts\nTerraform and most orchestration tools support only a limited set of deployment\nstrategies (discussed in the next section). If you have more complicated require‐\nments, you may have to write custom scripts to implement these requirements.\nDeployment strategies\nThere are a number of different strategies that you can use for application deploy‐\nment, depending on your requirements. Suppose that you have five copies of the old\nA Workflow  for Deploying Application Code | 387\nversion of your app running, and you want to roll out a new version. Here are a few\nof the most common strategies you can use:\nRolling deployment with replacement\nTake  down one of the old copies of the app, deploy a new copy to replace it,\nwait for the new copy to come up and pass health checks, start sending the\nnew copy live traffic, and then repeat the process until all of the old copies have\nbeen replaced. Rolling deployment with replacement ensures that you never have\nmore than five copies of the app running, which can be useful if you have limited\ncapacity (e.g., if each copy of the app runs on a physical server) or if you’re\ndealing with a stateful system where each app has a unique identity (e.g., this is\noften the case with consensus systems, such as Apache ZooKeeper). Note that\nthis deployment strategy can work with larger batch sizes (you can replace more\nthan one copy of the app at a time if you can handle the load and won’t lose data\nwith fewer apps running) and that during deployment, you will have both the old\nand new versions of the app running at the same time.\nRolling deployment without replacement\nDeploy one new copy of the app, wait for the new copy to come up and pass\nhealth checks, start sending the new copy live traffic, undeploy an old copy of\nthe app, and then repeat the process until all the old copies have been replaced.\nRolling deployment without replacement works only if you have flexible capacity\n(e.g., your apps run in the cloud, where you can spin up new virtual servers any\ntime you want) and if your application can tolerate more than five copies of it\nrunning at the same time. The advantage is that you never have less than five\ncopies of the app running, so you’re not running at a reduced capacity during\ndeployment. Note that this deployment strategy can also work with larger batch\nsizes (if you have the capacity for it, you can deploy five new copies all at once)\nand that during deployment, you will have both the old and new versions of the\napp running at the same time.\nBlue-green deployment\nDeploy five new copies of the app, wait for all of them to come up and pass\nhealth checks, shift all live traffic to the new copies at the same time, and then\nundeploy the old copies. Blue-green deployment works only if you have flexible\ncapacity (e.g., your apps run in the cloud, where you can spin up new virtual\nservers any time you want) and if your application can tolerate more than five\ncopies of it running at the same time. The advantage is that only one version of\nyour app is visible to users at any given time and that you never have less than\nfive copies of the app running, so you’re not running at a reduced capacity during\ndeployment.\n388 | Chapter 10: How to Use Terraform as a Team\nCanary deployment\nDeploy  one new copy of the app, wait for it to come up and pass health checks,\nstart sending live traffic to it, and then pause the deployment. During the pause,\ncompare the new copy of the app, called the “canary, ” to one of the old copies,\ncalled the “control. ” Y ou can compare the canary and control across a variety of\ndimensions: CPU usage, memory usage, latency, throughput, error rates in the\nlogs, HTTP response codes, and so on. Ideally, there’s no way to tell the two\nservers apart, which should give you confidence that the new code works just\nfine. In that case, you unpause the deployment and use one of the rolling deploy‐\nment strategies to complete it. On the other hand, if you spot any differences,\nthen that may be a sign of problems in the new code, and you can cancel the\ndeployment and undeploy the canary before the problem becomes worse.\nThe name comes from the “canary in a coal mine” concept, where miners would\ntake canary birds with them down into the tunnels, and if the tunnels filled with\ndangerous gases (e.g., carbon monoxide), those gases would affect the canary\nbefore the miners, thus providing an early warning to the miners that something\nwas wrong and that they needed to exit immediately, before more damage was\ndone. The canary deployment offers similar benefits, giving you a systematic way\nto test new code in production in a way that, if something goes wrong, you get a\nwarning early on, when it has affected only a small portion of your users and you\nstill have enough time to react and prevent further damage.\nCanary deployments are often combined with feature toggles , in which you wrap\nall new features in an if-statement. By default, the if-statement defaults to false,\nso the new feature is toggled off when you initially deploy the code. Because all\nnew functionality is off, when you deploy the canary server, it should behave\nidentically to the control, and any differences can be automatically flagged as\na problem and trigger a rollback. If there were no problems, later on you can\nenable the feature toggle for a portion of your users via an internal web interface.\nFor example, you might initially enable the new feature only for employees; if\nthat works well, you can enable it for 1% of users; if that’s still working well, you\ncan ramp it up to 10%; and so on. If at any point there’s a problem, you can\nuse the feature toggle to ramp the feature back down. This process allows you to\nseparate deployment  of new code from release  of new features.\nDeployment server\nY ou should run the deployment from a CI server and not from a developer’s com‐\nputer. This has the following benefits:\nFully automated\nTo run deployments from a CI server, you’ll be forced to fully automate all\ndeployment steps. This ensures that your deployment process is captured as\nA Workflow  for Deploying Application Code | 389",14138
99-Use Version Control.pdf,99-Use Version Control,"code, that you don’t miss any steps accidentally due to manual error, and that the\ndeployment is fast and repeatable.\nConsistent environment\nIf developers run deployments from their own computers, you’ll run into bugs\ndue to differences in how their computer is configured: for example, different\noperating systems, different dependency versions (different versions of Terra‐\nform), different configurations, and differences in what’s actually being deployed\n(e.g., the developer accidentally deploys a change that wasn’t committed to ver‐\nsion control). Y ou can eliminate all of these issues by deploying everything from\nthe same CI server.\nBetter permissions management\nInstead of giving every developer permissions to deploy, you can give solely the\nCI server those permissions (especially for the production environment). It’s a lot\neasier to enforce good security practices for a single server than it is to do for\nnumerous developers with production access.\nPromotion across environments\nIf you’re using immutable infrastructure practices, the way to roll out new changes is\nto promote the exact same versioned artifact from one environment to another. For\nexample, if you have dev, staging, and production environments, to roll out v0.0.4  of\nyour app, you would do the following:\n1.Deploy v0.0.4  of the app to dev. 1.\n2.Run your manual and automated tests in dev.2.\n3.If v0.0.4  works well in dev, repeat steps 1 and 2 to deploy v0.0.4  to staging (this 3.\nis known as promoting  the artifact).\n4.If v0.0.4  works well in staging, repeat steps 1 and 2 again to promote v0.0.4  to 4.\nprod.\nBecause you’re running the exact same artifact everywhere, there’s a good chance that\nif it works in one environment, it will work in another. And if you do hit any issues,\nyou can roll back anytime by deploying an older artifact version.\nA Workflow  for Deploying Infrastructure Code\nNow that you’ve seen the workflow for deploying application code, it’s time to dive\ninto the workflow for deploying infrastructure code. In this section, when I say\n“infrastructure code, ” I mean code written with any IaC tool (including, of course,\nTerraform) that you can use to deploy arbitrary infrastructure changes beyond a\n390 | Chapter 10: How to Use Terraform as a Team\nsingle application: for example, deploying databases, load balancers, network configu‐\nrations, DNS settings, and so on.\nHere’s what the infrastructure code workflow looks like:\n1.Use version control1.\n2.Run the code locally2.\n3.Make code changes3.\n4.Submit changes for review4.\n5.Run automated tests5.\n6.Merge and release6.\n7.Deploy7.\nOn the surface, it looks identical to the application workflow, but under the hood,\nthere are important differences. Deploying infrastructure code changes is more com‐\nplicated, and the techniques are not as well understood, so being able to relate each\nstep back to the analogous step from the application code workflow should make it\neasier to follow along. Let’s dive in.\nUse Version Control\nJust as with your application code, all of your infrastructure code should be in version\ncontrol. This means that you’ll use git clone  to check out your code, just as before.\nHowever, version control for infrastructure code has a few extra requirements:\n•Live repo and modules  repo •\n•Golden Rule of Terraform•\n•The trouble with branches•\nLive repo and modules repo\nAs discussed in Chapter 4 , you will typically want at least two separate version\ncontrol repositories for your Terraform code: one repo for modules and one repo\nfor live infrastructure. The repository for modules is where you create your reusable,\nversioned modules, such as all the modules you built in the previous chapters of\nthis book ( cluster/asg-rolling-deploy , data-stores/mysql , networking/alb , and\nservices/hello-world-app ). The repository for live infrastructure defines the live\ninfrastructure you’ve deployed in each environment (dev, stage, prod, etc.).\nOne pattern that works well is to have one infrastructure team in your company\nthat specializes in creating reusable, robust, production-grade modules. This team\nA Workflow  for Deploying Infrastructure Code | 391\n3See the Gruntwork Infrastructure as Code Library .can create remarkable leverage for your company by building a library of modules\nthat implement the ideas from Chapter 8 ; that is, each module has a composable\nAPI, is thoroughly documented (including executable documentation in the examples\nfolder), has a comprehensive suite of automated tests, is versioned, and implements\nall of your company’s requirements from the production-grade infrastructure check‐\nlist (i.e., security, compliance, scalability, high availability, monitoring, and so on).\nIf you build such a library (or you buy one off the shelf3), all the other teams at\nyour company will be able to consume these modules, a bit like a service catalog, to\ndeploy and manage their own infrastructure, without (a) each team having to spend\nmonths assembling that infrastructure from scratch or (b) the Ops team becoming\na bottleneck because it must deploy and manage the infrastructure for every team.\nInstead, the Ops team can spend most of its time writing infrastructure code, and\nall of the other teams will be able to work independently, using these modules to\nget themselves up and running. And because every team is using the same canonical\nmodules under the hood, as the company grows and requirements change, the Ops\nteam can push out new versions of the modules to all teams, ensuring everything\nstays consistent and maintainable.\nOr it will be maintainable, as long as you follow the Golden Rule of Terraform.\nThe Golden Rule of Terraform\nHere’s a quick way to check the health of your Terraform code: go into your live\nrepository, pick several folders at random, and run terraform plan  in each one. If\nthe output is always “no changes, ” that’s great, because it means that your infrastruc‐\nture code matches what’s actually deployed. If the output sometimes shows a small\ndiff, and you hear the occasional excuse from your team members (“Oh, right, I\ntweaked that one thing by hand and forgot to update the code”), your code doesn’t\nmatch reality, and you might soon be in trouble. If terraform plan  fails completely\nwith weird errors, or every plan  shows a gigantic diff, your Terraform code has no\nrelation at all to reality and is likely useless.\nThe gold standard, or what you’re really aiming for, is what I call The Golden Rule of\nTerraform :\nThe main branch of the live repository should be a 1:1 representation of what’s actually\ndeployed in production.\nLet’s break this sentence down, starting at the end and working our way back:\n392 | Chapter 10: How to Use Terraform as a Team\n“…what’s actually deployed”\nThe only way to ensure that the Terraform code in the live repository is an up-\nto-date representation of what’s actually deployed is to never make out-of-band\nchanges . After you begin using Terraform, do not make changes via a web UI, or\nmanual API calls, or any other mechanism. As you saw in Chapter 5 , out-of-band\nchanges not only lead to complicated bugs, but they also void many of the\nbenefits you get from using IaC in the first place.\n“…a 1:1 representation… ”\nIf I browse your live repository, I should be able to see, from a quick scan, what\nresources have been deployed in what environments. That is, every resource\nshould have a 1:1 match with some line of code checked into the live repo. This\nseems obvious at first glance, but it’s surprisingly easy to get it wrong. One way\nto get it wrong, as I just mentioned, is to make out-of-band changes so that the\ncode is there, but the live infrastructure is different. A more subtle way to get it\nwrong is to use Terraform workspaces to manage environments so that the live\ninfrastructure is there, but the code isn’t. That is, if you use workspaces, your live\nrepo will have only one copy of the code, even though you may have 3 or 30\nenvironments deployed with it. From merely looking at the code, there will be\nno way to know what’s actually deployed, which will lead to mistakes and make\nmaintenance complicated. Therefore, as described in “Isolation via Workspaces”\non page 94, instead of using workspaces to manage environments, you want\neach environment defined in a separate folder, using separate files, so that you\ncan see exactly what environments have been deployed just by browsing the live\nrepository. Later in this chapter, you’ll see how to do this with minimal copying\nand pasting.\n“The  main branch… ”\nY ou should have to look at only a single branch to understand what’s actually\ndeployed in production. Typically, that branch will be main . This means that all\nchanges that affect the production environment should go directly into main  (you\ncan create a separate branch but only to create a pull request with the intention\nof merging that branch into main ), and you should run terraform apply  only\nfor the production environment against the main  branch. In the next section, I’ll\nexplain why.\nThe trouble with branches\nIn Chapter 3 , you saw that you can use the locking mechanisms built into Terraform\nbackends to ensure that if two team members are running terraform apply  at the\nsame time on the same set of Terraform configurations, their changes do not over‐\nwrite each other. Unfortunately, this only solves part of the problem. Even though\nTerraform backends provide locking for Terraform state, they cannot help you with\nlocking at the level of the Terraform code itself. In particular, if two team members\nA Workflow  for Deploying Infrastructure Code | 393\nare deploying the same code to the same environment but from different branches,\nyou’ll run into conflicts that locking can’t prevent.\nFor example, suppose that one of your team members, Anna, makes some changes\nto the Terraform configurations for an app called “foo” that consists of a single EC2\nInstance:\nresource  ""aws_instance"" ""foo""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n}\nThe app is getting a lot of traffic, so Anna decides to change the instance_type  from\nt2.micro  to t2.medium :\nresource  ""aws_instance"" ""foo""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.medium""\n}\nHere’s what Anna sees when she runs terraform plan :\n$ terraform plan\n(...)\nTerraform will perform the following actions:\n  # aws_instance.foo will be updated in-place\n  ~ resource ""aws_instance"" ""foo"" {\n        ami                          = ""ami-0fb653ca2d3203ac1""\n        id                           = ""i-096430d595c80cb53""\n        instance_state               = ""running""\n      ~ instance_type                = ""t2.micro"" -> ""t2.medium""\n        (...)\n    }\nPlan: 0 to add, 1 to change, 0 to destroy.\nThose changes look good, so she deploys them to staging.\nIn the meantime, Bill comes along and also starts making changes to the Terraform\nconfigurations for the same app but on a different branch. All Bill wants to do is to\nadd a tag to the app:\nresource  ""aws_instance"" ""foo""  {\n  ami           = data.aws_ami.ubuntu.id\n  instance_type  = ""t2.micro""\n  tags = {\n    Name  = ""foo""\n  }\n}\n394 | Chapter 10: How to Use Terraform as a Team",11356
100-Submit Changes for Review.pdf,100-Submit Changes for Review,"Note that Anna’s changes are already deployed in staging, but because they are on\na different branch, Bill’s code still has the instance_type  set to the old value of\nt2.micro . Here’s what Bill sees when he runs the plan  command (the following log\noutput is truncated for readability):\n$ terraform plan\n(...)\nTerraform will perform the following actions:\n  # aws_instance.foo will be updated in-place\n  ~ resource ""aws_instance"" ""foo"" {\n        ami                          = ""ami-0fb653ca2d3203ac1""\n        id                           = ""i-096430d595c80cb53""\n        instance_state               = ""running""\n      ~ instance_type                = ""t2.medium"" -> ""t2.micro""\n      + tags                         = {\n          + ""Name"" = ""foo""\n        }\n        (...)\n    }\nPlan: 0 to add, 1 to change, 0 to destroy.\nUh oh, he’s about to undo Anna’s instance_type  change! If Anna is still testing\nin staging, she’ll be very confused when the server suddenly redeploys and starts\nbehaving differently. The good news is that if Bill diligently reads the plan  output,\nhe can spot the error before it affects Anna. Nevertheless, the point of the example is\nto highlight what happens when you deploy changes to a shared environment from\ndifferent branches.\nThe locking from Terraform backends doesn’t help here, because the conflict has\nnothing to do with concurrent modifications to the state file; Bill and Anna might\nbe applying their changes weeks apart, and the problem would be the same. The\nunderlying cause is that branching and Terraform are a bad combination. Terraform\nis implicitly a mapping from Terraform code to infrastructure deployed in the real\nworld. Because there’s only one real world, it doesn’t make much sense to have\nmultiple branches of your Terraform code. So for any shared environment (e.g., stage,\nprod), always deploy from a single branch.\nRun the Code Locally\nNow that you’ve got the code checked out onto your computer, the next step is to\nrun it. The gotcha with Terraform is that, unlike application code, you don’t have\n“localhost”; for example, you can’t deploy an AWS ASG onto your own laptop. As\ndiscussed in “Manual Testing Basics” on page 317, the only way to manually test\nA Workflow  for Deploying Infrastructure Code | 395\nTerraform code is to run it in a sandbox environment, such as an AWS account\ndedicated for developers (or better yet, one AWS account for each developer).\nOnce you have a sandbox environment, to test manually, you run terraform apply :\n$ terraform apply\n(...)\nApply complete! Resources: 5 added, 0 changed, 0 destroyed.\nOutputs:\nalb_dns_name = ""hello-world-stage-477699288.us-east-2.elb.amazonaws.com""\nAnd you verify the deployed infrastructure works by using tools such as curl :\n$ curl hello-world-stage-477699288.us-east-2.elb.amazonaws.com\nHello, World\nTo run automated tests written in Go, you use go test  in a sandbox account dedica‐\nted to testing:\n$ go test -v -timeout 30m\n(...)\nPASS\nok  terraform-up-and-running 229.492s\nMake Code Changes\nNow that you can run your Terraform code, you can iteratively begin to make\nchanges, just as with application code. Every time you make a change, you can rerun\nterraform apply  to deploy those changes and rerun curl  to see whether those\nchanges worked:\n$ curl hello-world-stage-477699288.us-east-2.elb.amazonaws.com\nHello, World v2\nOr you can rerun go test  to make sure the tests are still passing:\n$ go test -v -timeout 30m\n(...)\nPASS\nok  terraform-up-and-running 229.492s\nThe only difference from application code is that infrastructure code tests typically\ntake longer, so you’ll want to put more thought into how you can shorten the test\ncycle so that you can get feedback on your changes as quickly as possible. In “Test\n396 | Chapter 10: How to Use Terraform as a Team\nstages”  on page 354, you saw that you can use these test stages to rerun only specific\nstages of a test suite, dramatically shortening the feedback loop.\nAs you make changes, be sure to regularly commit your work:\n$ git commit -m ""Updated Hello, World text""\nSubmit Changes for Review\nAfter your code is working the way you expect, you can create a pull request to get\nyour code reviewed, just as you would with application code. Y our team will review\nyour code changes, looking for bugs as well as enforcing coding guidelines . Whenever\nyou’re writing code as a team, regardless of what type of code you’re writing, you\nshould define guidelines for everyone to follow. One of my favorite definitions of\n“clean code” comes from an interview I did with Nick Dellamaggiore for my earlier\nbook, Hello, Startup :\nIf I look at a single file and it’s written by 10 different engineers, it should be almost\nindistinguishable which part was written by which person. To me, that is clean code.\nThe way you do that is through code reviews and publishing your style guide, your\npatterns, and your language idioms. Once you learn them, everybody is way more\nproductive because you all know how to write code the same way. At that point, it’s\nmore about what you’re writing and not how you write it.\n—Nick Dellamaggiore, Infrastructure Lead at Coursera\nThe Terraform coding guidelines that make sense for each team will be different, so\nhere, I’ll list a few of the common ones that are useful for most teams:\n•Documentation•\n•Automated tests•\n•File layout•\n•Style guide•\nDocumentation\nIn some sense, Terraform code is, in and of itself, a form of documentation. It\ndescribes in a simple language exactly what infrastructure you deployed and how\nthat infrastructure is configured. However, there is no such thing as self-documenting\ncode. Although well-written code can tell you what  it does, no programming lan‐\nguage that I’m aware of (including Terraform) can tell you why it does it.\nThis is why all software, including IaC, needs documentation beyond the code itself.\nThere are several types of documentation that you can consider and have your team\nmembers require as part of code reviews:\nA Workflow  for Deploying Infrastructure Code | 397\n4Writing the README first is called Readme-Driven Development .Written documentation\nMost Terraform modules should have a README that explains what the module\ndoes, why it exists, how to use it, and how to modify it. In fact, you may want to\nwrite the README first, before any of the actual Terraform code, because that\nwill force you to consider what  you’re building and why you’re building it before\nyou dive into the code and get lost in the details of how to build it.4 Spending 20\nminutes writing a README can often save you hours of writing code that solves\nthe wrong problem. Beyond the basic README, you might also want to have\ntutorials, API documentation, wiki pages, and design documents that go deeper\ninto how the code works and why it was built this way.\nCode documentation\nWithin  the code itself, you can use comments as a form of documentation.\nTerraform treats any text that begins with a hash ( #) as a comment. Don’t use\ncomments to explain what the code does; the code should do that itself. Only\ninclude comments to offer information that can’t be expressed in code, such as\nhow the code is meant to be used or why the code uses a particular design choice.\nTerraform also allows every input and output variable to declare a description\nparameter, which is a great place to describe how that variable should be used.\nExample code\nAs discussed in Chapter 8 , every Terraform module should include example\ncode that shows how that module is meant to be used. This is a great way to\nhighlight the intended usage patterns and give your users a way to try your\nmodule without having to write any code, and it’s the main way to add automated\ntests for the module.\nAutomated tests\nAll of Chapter 9  focuses on testing Terraform code, so I won’t repeat any of that here,\nother than to say that infrastructure code without tests is broken. Therefore, one of\nthe most important comments you can make in any code review is “How did you test\nthis?”\nFile layout\nY our team should define conventions for where Terraform code is stored and the\nfile layout you use. Because the file layout for Terraform also determines the way\nTerraform state is stored, you should be especially mindful of how file layout affects\nyour ability to provide isolation guarantees, such as ensuring that changes in a staging\nenvironment cannot accidentally cause problems in production. In a code review, you\nmight want to enforce the file layout described in “Isolation via File Layout” on page\n398 | Chapter 10: How to Use Terraform as a Team",8701
101-Deploy.pdf,101-Deploy,"100, which provides isolation between different environments (e.g., stage and prod)\nand different components (e.g., a network topology for the entire environment and a\nsingle app within that environment).\nStyle guide\nEvery team should enforce a set of conventions about code style, including the\nuse of whitespace, newlines, indentation, curly braces, variable naming, and so on.\nAlthough programmers love to debate spaces versus tabs and where the curly brace\nshould go, the more important thing is that you are consistent throughout your\ncodebase.\nTerraform has a built-in fmt command  that can reformat code to a consistent style\nautomatically:\n$ terraform fmt\nI recommend running this command as part of a commit hook to ensure that all code\ncommitted to version control uses a consistent style.\nRun Automated Tests\nJust as with application code, your infrastructure code should have commit hooks\nthat kick off automated tests in a CI server after every commit and show the results\nof those tests in the pull request. Y ou already saw how to write unit tests, integration\ntests, and end-to-end tests for your Terraform code in Chapter 9 . There’s one other\ncritical type of test you should run: terraform plan . The rule here is simple:\nAlways run plan  before apply .\nTerraform shows the plan  output automatically when you run apply , so what this\nrule really means is that you should always pause and read the plan  output! Y ou’ d be\namazed at the type of errors you can catch by taking 30 seconds to scan the “diff ” you\nget as an output. A great way to encourage this behavior is by integrating plan  into\nyour code review flow. For  example, Atlantis  is an open source tool that automatically\nruns terraform plan  on commits and adds the plan  output to pull requests as a\ncomment, as shown in Figure 10-3 .\nA Workflow  for Deploying Infrastructure Code | 399\nFigure 10-3. Atlantis can automatically add the output of the terraform plan  com‐\nmand as a comment on your pull requests.\nTerraform Cloud and Terraform Enterprise, HashiCorp’s paid tools, both support\nrunning plan  automatically on pull requests as well.\nMerge and Release\nAfter your team members have had a chance to review the code changes and plan\noutput and all the tests have passed, you can merge your changes into the main\nbranch and release the code. Similar to application code, you can use Git tags to\ncreate a versioned release:\n$ git tag -a ""v0.0.6"" -m ""Updated hello-world-example text""\n$ git push --follow-tags\n400 | Chapter 10: How to Use Terraform as a Team\nWhereas with application code, you often have a separate artifact to deploy, such as\na Docker image or VM image, since Terraform natively supports downloading code\nfrom Git, the repository at a specific tag is the immutable, versioned artifact you will\nbe deploying.\nDeploy\nNow  that you have an immutable, versioned artifact, it’s time to deploy it. Here are a\nfew of the key considerations for deploying Terraform code:\n•Deployment tooling•\n•Deployment strategies•\n•Deployment server•\n•Promote artifacts across environments•\nDeployment tooling\nWhen deploying Terraform code, Terraform itself is the main tool that you use.\nHowever, there are a few other tools that you might find useful:\nAtlantis\nThe open source tool you saw earlier can not only add the plan  output to your\npull requests but also allows you to trigger a terraform apply  when you add a\nspecial comment to your pull request. Although this provides a convenient web\ninterface for Terraform deployments, be aware that it doesn’t support versioning,\nwhich can make maintenance and debugging for larger projects more difficult.\nTerraform Cloud and Terraform Enterprise\nHashiCorp’s paid products provide a web UI that you can use to run terraform\nplan  and terraform apply  as well as manage variables, secrets, and access\npermissions.\nTerragrunt\nThis  is an open source wrapper for Terraform that fills in some gaps in Terra‐\nform. Y ou’ll see how to use it a bit later in this chapter to deploy versioned\nTerraform code across multiple environments with minimal copying and pasting.\nScripts\nAs always, you can write scripts in a general-purpose programming language\nsuch as Python or Ruby or Bash to customize how you use Terraform.\nA Workflow  for Deploying Infrastructure Code | 401\nDeployment strategies\nFor most types of infrastructure changes, Terraform doesn’t offer any built-in deploy‐\nment strategies: for example, there’s no way to do a blue-green deployment for a VPC\nchange, and there’s no way to feature toggle a database change. Y ou’re essentially limi‐\nted to terraform apply , which either works or it doesn’t. A small subset of changes\ndo support deployment strategies, such as the zero-downtime rolling deployment in\nthe asg-rolling-deploy  module you built in previous chapters, but these are the\nexceptions and not the norm.\nDue to these limitations, it’s critical to take into account what happens when a\ndeployment goes wrong. With an application deployment, many types of errors are\ncaught by the deployment strategy; for example, if the app fails to pass health checks,\nthe load balancer will never send it live traffic, so users won’t be affected. Moreover,\nthe rolling deployment or blue-green deployment strategy can automatically roll back\nto the previous version of the app in case of errors.\nTerraform, on the other hand, does not roll back automatically in case of errors . In\npart, that’s because there is no reasonable way to roll back many types of infrastruc‐\nture changes: for example, if an app deployment failed, it’s almost always safe to roll\nback to an older version of the app, but if the Terraform change you were deploying\nfailed, and that change was to delete a database or terminate a server, you can’t easily\nroll that back!\nTherefore, you should expect errors to happen and ensure you have a first-class way\nto deal with them:\nRetries\nCertain types of Terraform errors are transient and go away if you rerun terra\nform apply . The deployment tooling you use with Terraform should detect\nthese known errors and automatically retry after a brief pause. Terragrunt has\nautomatic retries  on known errors as a built-in feature.\nTerraform state errors\nOccasionally, Terraform will fail to save state after running terraform apply .\nFor example, if you lose internet connectivity partway through an apply , not\nonly will the apply  fail, but Terraform won’t be able to write the updated state\nfile to your remote backend (e.g., to Amazon S3). In these cases, Terraform will\nsave the state file on disk in a file called errored.tfstate . Make sure that your CI\nserver does not delete these files (e.g., as part of cleaning up the workspace after a\nbuild)! If you can still access this file after a failed deployment, as soon as internet\nconnectivity is restored, you can push this file to your remote backend (e.g., to\nS3) using the state push  command so that the state information isn’t lost:\n$ terraform state push errored.tfstate\n402 | Chapter 10: How to Use Terraform as a Team\n5See 10 real-world stories of how we’ve compromised CI/CD pipelines  for some eye-opening examples.Errors releasing locks\nOccasionally, Terraform will fail to release a lock. For example, if your CI server\ncrashes in the middle of a terraform apply , the state will remain permanently\nlocked. Anyone else who tries to run apply  on the same module will get an\nerror message saying the state is locked and showing the ID of the lock. If you’re\nabsolutely sure this is an accidentally leftover lock, you can forcibly release it\nusing the force-unlock  command, passing it the ID of the lock from that error\nmessage:\n$ terraform force-unlock <LOCK_ID>\nDeployment server\nJust as with your application code, all of your infrastructure code changes should\nbe applied from a CI server and not from a developer’s computer. Y ou can run\nterraform  from Jenkins, CircleCI, GitHub Actions, Terraform Cloud, Terraform\nEnterprise, Atlantis, or any other reasonably secure automated platform. This gives\nyou the same benefits as with application code: it forces you to fully automate\nyour deployment process, it ensures deployment always happens from a consistent\nenvironment, and it gives you better control over who has permissions to access\nproduction environments.\nThat said, permissions to deploy infrastructure code are quite a bit trickier than\nfor application code. With application code, you can usually give your CI server a\nminimal, fixed set of permissions to deploy your apps; for example, to deploy to an\nASG, the CI server typically needs only a few specific ec2 and autoscaling  permis‐\nsions. However, to be able to deploy arbitrary infrastructure code changes (e.g., your\nTerraform code might try to deploy a database or a VPC or an entirely new AWS\naccount), the CI server needs arbitrary permissions—that is, admin permissions. And\nthat’s a problem.\nThe reason it’s a problem is that CI servers are (a) notoriously hard to secure,5 (b)\naccessible to all the developers at your company, and (c) used to execute arbitrary\ncode. Adding permanent admin permissions to this mix is just asking for trouble!\nY ou’ d effectively be giving every single person on your team admin permissions and\nturning your CI server into a very high-value target for attackers.\nThere are a few things you can do to minimize this risk:\nLock the CI server down\nMake it accessible solely over HTTPs, require all users to be authenticated, and\nfollow server-hardening practices (e.g., lock down the firewall, install fail2ban,\nenable audit logging, etc.).\nA Workflow  for Deploying Infrastructure Code | 403\n6Check out Gruntwork Pipelines  for a real-world example of this worker pattern.Don’t expose your CI server on the public internet\nThat is, run the CI server in private subnets, without any public IP , so that it’s\naccessible only over a VPN connection. That way, only users with valid network\naccess (e.g., via a VPN certificate) can access your CI server at all. Note that this\ndoes have a drawback: webhooks from external systems won’t work. For example,\nGitHub won’t automatically be able to trigger builds in your CI server; instead,\nyou’ll need to configure your CI server to poll your version control system for\nupdates. This is a small price to pay for a significantly more secure CI server.\nEnforce an approval workflow\nConfigure your CI/CD pipeline to require that every deployment be approved by\nat least one person (other than the person who requested the deployment in the\nfirst place). During this approval step, the reviewer should be able to see both the\ncode changes and the plan  output, as one final check that things look OK before\napply  runs. This ensures that every deployment, code change, and plan  output\nhas had at least two sets of eyes on it.\nDon’t give the CI server permanent credentials\nAs you saw in Chapter 6 , instead of manually managed, permanent credentials\n(e.g., AWS access keys copy/pasted into your CI server), you should prefer to\nuse authentication mechanisms that use temporary credentials, such as IAM roles\nand OIDC.\nDon’t give the CI server admin credentials\nInstead, isolate the admin credentials to a totally separate, isolated worker : e.g., a\nseparate server, a separate container, etc. That worker should be extremely locked\ndown, so no developers have access to it at all, and the only thing it allows is\nfor the CI server to trigger that worker via an extremely limited remote API. For\nexample, that worker’s API may only allow you to run specific commands (e.g.,\nterraform plan  and terraform apply ), in specific repos (e.g., your live repo), in\nspecific branches (e.g., the main  branch), and so on. This way, even if an attacker\ngets access to your CI server, they still won’t have access to the admin credentials,\nand all they can do is request a deployment on some code that’s already in your\nversion control system, which isn’t nearly as much of a catastrophe as leaking the\nadmin credentials fully.6\n404 | Chapter 10: How to Use Terraform as a Team\n7Credit for how to promote Terraform code across environments goes to Kief Morris: Using Pipelines to\nManage Environments with Infrastructure as Code .Promote artifacts across environments\nJust as with application artifacts, you’ll want to promote your immutable, versioned\ninfrastructure artifacts from environment to environment: for example, promote\nv0.0.6  from dev to stage to prod.7 The rule here is also simple:\nAlways test Terraform changes in pre-prod before prod.\nBecause everything is automated with Terraform anyway, it doesn’t cost you much\nextra effort to try a change in staging before production, but it will catch a huge\nnumber of errors. Testing in pre-prod is especially important because, as mentioned\nearlier in this chapter, Terraform does not roll back changes in case of errors. If you\nrun terraform apply  and something goes wrong, you must fix it yourself. This is\neasier and less stressful to do if you catch the error in a pre-prod environment rather\nthan prod.\nThe process for promoting Terraform code across environments is similar to the\nprocess of promoting application artifacts, except there is an extra approval step,\nas mentioned in the previous section, where you run terraform plan  and have\nsomeone manually review the output and approve the deployment. This step isn’t\nusually necessary for application deployments, as most application deployments are\nsimilar and relatively low risk. However, every infrastructure deployment can be\ncompletely different, and mistakes can be very costly (e.g., deleting a database), so\nhaving one last chance to look at the plan  output and review it is well worth the time.\nHere’s what the process looks like for promoting, for instance, v0.0.6  of a Terraform\nmodule across the dev, stage, and prod environments:\n1.Update the dev environment to v0.0.6 , and run terraform plan . 1.\n2.Prompt someone to review and approve the plan; for example, send an automa‐2.\nted message via Slack.\n3.If the plan is approved, deploy v0.0.6  to dev by running terraform apply . 3.\n4.Run your manual and automated tests in dev.4.\n5.If v0.0.6  works well in dev, repeat steps 1–4 to promote v0.0.6  to staging. 5.\n6.If v0.0.6  works well in staging, repeat steps 1–4 again to promote v0.0.6  to 6.\nproduction.\nA Workflow  for Deploying Infrastructure Code | 405\nOne important issue to deal with is all the code duplication between environments in\nthe live repo. For example, consider the live repo shown in Figure 10-4 .\nFigure 10-4. File layout with a large number of copy/pasted environments and modules\nwithin each environment.\nThis live repo has a large number of regions, and within each region, a large number\nof modules, most of which are copied and pasted. Sure, each module has a main.tf\nthat references a module in your modules  repo, so it’s not as much copying and\npasting as it could be, but even if all you’re doing is instantiating a single module,\nthere is still a large amount of boilerplate that needs to be duplicated between each\nenvironment:\n406 | Chapter 10: How to Use Terraform as a Team\n•The provider  configuration •\n•The backend  configuration •\n•The input variables to pass to the module•\n•The output variables to proxy from the module•\nThis can add up to dozens or hundreds of lines of mostly identical code in each\nmodule, copied and pasted into each environment. To make this code more DRY ,\nand to make it easier to promote Terraform code across environments, you can use\nthe open source tool I’ve mentioned earlier called Terragrunt. Terragrunt is a thin\nwrapper for Terraform, which means that you run all of the standard terraform\ncommands, except you use terragrunt  as the binary:\n$ terragrunt plan\n$ terragrunt apply\n$ terragrunt output\nTerragrunt will run Terraform with the command you specify, but based on config‐\nuration you specify in a terragrunt.hcl  file, you can get some extra behavior. In\nparticular, Terragrunt allows you to define all of your Terraform code exactly once in\nthe modules  repo, whereas in the live repo, you will have solely terragrunt.hcl  files that\nprovide a DRY way to configure and deploy each module in each environment. This\nwill result in a live repo with far fewer files and lines of code, as shown in Figure 10-5 .\nTo get started, install Terragrunt by following the instructions on the Terragrunt\nwebsite . Next, add a provider  configuration to modules/data-stores/mysql/main.tf\nand modules/services/hello-world-app/main.tf :\nprovider  ""aws"" {\n  region  = ""us-east-2""\n}\nA Workflow  for Deploying Infrastructure Code | 407\nFigure 10-5. Use Terragrunt in your live repos to reduce the amount of code duplication.\nCommit these changes and release a new version of your modules  repo:\n$ git add modules/data-stores/mysql/main.tf\n$ git add modules/services/hello-world-app/main.tf\n$ git commit -m ""Update mysql and hello-world-app for Terragrunt""\n$ git tag -a ""v0.0.7"" -m ""Update Hello, World text""\n$ git push --follow-tags\nNow, head over to the live repo, and delete all the .tf files. Y ou’re going to replace all\nthat copied and pasted Terraform code with a single terragrunt.hcl  file for each mod‐\nule. For example, here’s terragrunt.hcl  for live/stage/data-stores/mysql/terragrunt.hcl :\nterraform  {\n  source  = ""github.com/<OWNER>/modules//data-stores/mysql?ref =v0.0.7""\n}\ninputs = {\n  db_name  = ""example_stage""\n  # Set the username using the TF_VAR_db_username environment variable\n408 | Chapter 10: How to Use Terraform as a Team\n  # Set the password using the TF_VAR_db_password environment variable\n}\nAs you can see, terragrunt.hcl  files use the same HashiCorp Configuration Language\n(HCL) syntax as Terraform itself. When you run terragrunt apply  and it finds the\nsource  parameter in a terragrunt.hcl  file, Terragrunt will do the following:\n1.Check out the URL specified in source  to a temporary folder. This supports the 1.\nsame URL syntax as the source  parameter of Terraform modules, so you can use\nlocal file paths, Git URLs, versioned Git URLs (with a ref parameter, as in the\npreceding example), and so on.\n2.Run terraform apply  in the temporary folder, passing it the input variables that 2.\nyou’ve specified in the inputs = { … }  block.\nThe benefit of this approach is that the code in the live repo is reduced to just a single\nterragrunt.hcl  file per module, which contains only a pointer to the module to use (at\na specific version), plus the input variables to set for that specific environment. That’s\nabout as DRY as you can get.\nTerragrunt also helps you keep your backend  configuration DRY . Instead of having\nto define the bucket , key, dynamodb_table , and so on in every single module, you\ncan define it in a single terragrunt.hcl  file per environment. For example, create the\nfollowing in live/stage/terragrunt.hcl :\nremote_state  {\n  backend  = ""s3""\n  generate  = {\n    path       = ""backend.tf""\n    if_exists  = ""overwrite""\n  }\n  config  = {\n    bucket          = ""<YOUR BUCKET>""\n    key            = ""${path_relative_to_include()}/terraform.tfstate""\n    region          = ""us-east-2""\n    encrypt         = true\n    dynamodb_table  = ""<YOUR_TABLE>""\n  }\n}\nFrom this one remote_state  block, Terragrunt can generate the backend  configura‐\ntion dynamically for each of your modules, writing the configuration in config  to the\nfile specified via the generate  param. Note that the key value in config  uses a Terra‐\ngrunt built-in function called path_relative_to_include() , which will return the\nrelative path between this root terragrunt.hcl  file and any child module that includes\nA Workflow  for Deploying Infrastructure Code | 409\nit. For example, to include this root file in live/stage/data-stores/mysql/terragrunt.hcl ,\nadd an include  block:\nterraform  {\n  source  = ""github.com/<OWNER>/modules//data-stores/mysql?ref =v0.0.7""\n}\ninclude {\n  path = find_in_parent_folders ()\n}\ninputs = {\n  db_name  = ""example_stage""\n  # Set the username using the TF_VAR_db_username environment variable\n  # Set the password using the TF_VAR_db_password environment variable\n}\nThe include  block finds the root terragrunt.hcl  using the Terragrunt built-in function\nfind_in_parent_folders() , automatically inheriting all the settings from that parent\nfile, including the remote_state  configuration. The result is that this mysql  module\nwill use all the same backend  settings as the root file, and the key value will automat‐\nically resolve to data-stores/mysql/terraform.tfstate . This means that your Terraform\nstate will be stored in the same folder structure as your live repo, which will make it\neasy to know which module produced which state files.\nTo deploy this module, run terragrunt apply :\n$ terragrunt apply --terragrunt-log-level debug\nDEBU[0001] Reading Terragrunt config file at terragrunt.hcl\nDEBU[0001] Included config live/stage/terragrunt.hcl\nDEBU[0001] Downloading Terraform configurations into .terragrunt-cache\nDEBU[0001] Generated file backend.tf\nDEBU[0013] Running command: terraform init\n(...)\nInitializing the backend...\nSuccessfully configured the backend ""s3""! Terraform will automatically\nuse this backend unless the backend configuration changes.\n(...)\nDEBU[0024] Running command: terraform apply\n(...)\nTerraform will perform the following actions:\n(...)\n410 | Chapter 10: How to Use Terraform as a Team\nPlan: 5 to add, 0 to change, 0 to destroy.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n  Enter a value: yes\n(...)\nApply complete! Resources: 5 added, 0 changed, 0 destroyed.\nNormally, Terragrunt only shows the log output from Terraform itself, but as I\nincluded --terragrunt-log-level debug , the preceding output shows what Terra‐\ngrunt does under the hood:\n1.Read the terragrunt.hcl  file in the mysql  folder where you ran apply . 1.\n2.Pull in all the settings from the included root terragrunt.hcl  file. 2.\n3.Download the Terraform code specified in the source  URL into the .terragrunt- 3.\ncache  scratch folder.\n4.Generate a backend.tf  file with your backend  configuration. 4.\n5.Detect that init  has not been run and run it automatically (Terragrunt will even 5.\ncreate your S3 bucket and DynamoDB table automatically if they don’t already\nexist).\n6.Run apply  to deploy changes. 6.\nNot bad for a couple of tiny terragrunt.hcl  files!\nY ou can now deploy the hello-world-app  module in staging by adding live/stage/\nservices/hello-world-app/terragrunt.hcl  and running terragrunt apply :\nterraform  {\n  source  = ""github.com/<OWNER>/modules//services/hello-world-app?ref =v0.0.7""\n}\ninclude {\n  path = find_in_parent_folders ()\n}\ndependency  ""mysql"" {\n  config_path  = ""../../data-stores/mysql""\n}\ninputs = {\n  environment  = ""stage""\n  ami         = ""ami-0fb653ca2d3203ac1""\nA Workflow  for Deploying Infrastructure Code | 411",23246
102-Putting It All Together.pdf,102-Putting It All Together,"min_size  = 2\n  max_size  = 2\n  enable_autoscaling  = false\n  mysql_config  = dependency.mysql.outputs\n}\nThis terragrunt.hcl  file uses the source  URL and inputs  just as you saw before and\nuses include  to pull in the settings from the root terragrunt.hcl  file, so it will inherit\nthe same backend  settings, except for the key, which will be automatically set to\nservices/hello-world-app/terraform.tfstate , just as you’ d expect. The one new thing in\nthis terragrunt.hcl  file is the dependency  block:\ndependency  ""mysql"" {\n  config_path  = ""../../data-stores/mysql""\n}\nThis is a Terragrunt feature that can be used to automatically read the output vari‐\nables of another Terragrunt module, so you can pass them as input variables to the\ncurrent module, as follows:\n  mysql_config  = dependency.mysql.outputs\nIn other words, dependency  blocks are an alternative to using terra\nform_remote_state  data sources to pass data between modules. While terra\nform_remote_state  data sources have the advantage of being native to Terraform,\nthe drawback is that they make your modules more tightly coupled together, as each\nmodule needs to know how other modules store state. Using Terragrunt dependency\nblocks allows your modules to expose generic inputs like mysql_config  and vpc_id ,\ninstead of using data sources, which makes the modules less tightly coupled and\neasier to test and reuse.\nOnce you’ve got hello-world-app  working in staging, create analogous terragrunt.hcl\nfiles in live/prod  and promote the exact same v0.0.7  artifact to production by run‐\nning terragrunt apply  in each module.\nPutting It All Together\nY ou’ve now seen how to take both application code and infrastructure code from\ndevelopment all the way through to production. Table 10-1  shows an overview of the\ntwo workflows side by side.\n412 | Chapter 10: How to Use Terraform as a Team\nTable 10-1. Application and infrastructure code workflows\nApplication code Infrastructure code\nUse version control•git clone•\n•One repo per app•\n•Use branches••git clone•\n•live and modules  repos •\n•Don’t use branches•\nRun the code locally•Run on localhost•\n•ruby web-server.rb•\n•ruby web-server-test.rb••Run in a sandbox environment•\n•terraform apply•\n•go test•\nMake code changes•Change the code•\n•ruby web-server.rb•\n•ruby web-server-test.rb••Change the code•\n•terraform apply•\n•go test•\n•Use test stages•\nSubmit changes for\nreview•Submit a pull request•\n•Enforce coding guidelines••Submit a pull request•\n•Enforce coding guidelines•\nRun automated tests•Tests run on CI server•\n•Unit tests•\n•Integration tests•\n•End-to-end tests•\n•Static analysis••Tests run on CI server•\n•Unit tests•\n•Integration tests•\n•End-to-end tests•\n•Static analysis•\n•terraform plan•\nMerge and release•git tag•\n•Create versioned, immutable•\nartifact•git tag•\n•Use repo with tag as versioned, immutable artifact•\nDeploy•Deploy with Terraform,•\norchestration tool (e.g.,\nKubernetes, Mesos), scripts\n•Many deployment strategies:•\nrolling deployment, blue-green,\ncanary\n•Run deployment on a CI server•\n•Give CI server limited permissions•\n•Promote immutable, versioned•\nartifacts across environments\n•Once a pull request is merged,•\ndeploy automatically•Deploy with Terraform, Atlantis, Terraform Cloud,•\nTerraform Enterprise, Terragrunt, scripts\n•Limited deployment strategies (make sure to handle•\nerrors: retries, errored.tfstate !)\n•Run deployment on a CI server•\n•Give CI server temporary credentials solely to invoke•\na separate, locked-down worker that has admin\npermissions\n•Promote immutable, versioned artifacts across•\nenvironments\n•Once a pull request is merged, go through an approval•\nworkflow  where someone checks the plan  output one\nlast time, and then deploy automatically\nIf you follow this process, you will be able to run application and infrastructure code\nin dev, test it, review it, package it into versioned, immutable artifacts, and promote\nthose artifacts from environment to environment, as shown in Figure 10-6 .\nPutting It All Together | 413",4107
103-Conclusion.pdf,103-Conclusion,"Figure 10-6. Promoting an immutable, versioned artifact of Terraform code from envi‐\nronment to environment.\nConclusion\nIf you’ve made it to this point in the book, you now know just about everything you\nneed to use Terraform in the real world, including how to write Terraform code; how\nto manage Terraform state; how to create reusable modules with Terraform; how to\ndo loops, if-statements, and deployments; how to manage secrets; how to work with\nmultiple regions, accounts, and clouds; how to write production-grade Terraform\ncode; how to test your Terraform code; and how to use Terraform as a team. Y ou’ve\nworked through examples of deploying and managing servers, clusters of servers,\nload balancers, databases, scheduled actions, CloudWatch alarms, IAM users, reusa‐\nble modules, zero-downtime deployment, AWS Secrets Manager, Kubernetes clusters,\nautomated tests, and more. Phew! Just don’t forget to run terraform destroy  in each\nmodule when you’re all done.\n414 | Chapter 10: How to Use Terraform as a Team\nThe power of Terraform, and more generally, IaC, is that you can manage all the\noperational concerns around an application using the same coding principles as the\napplication itself. This allows you to apply the full power of software engineering to\nyour infrastructure, including modules, code reviews, version control, and automated\ntesting.\nIf you use Terraform correctly, your team will be able to deploy faster and respond to\nchanges more quickly. Hopefully, deployments will become routine and boring—and\nin the world of operations, boring is a very good thing. And if you really do your job\nright, rather than spending all your time managing infrastructure by hand, your team\nwill be able to spend more and more time improving that infrastructure, allowing you\nto go even faster.\nThis is the end of the book but just the beginning of your journey with Terraform.\nTo learn more about Terraform, IaC, and DevOps, head over to Appendix  for a list of\nrecommended reading. And if you’ve got feedback or questions, I’ d love to hear from\nyou at jim@ybrikman.com . Thank you for reading!\nConclusion | 415",2156
104-Appendix. Recommended Reading.pdf,104-Appendix. Recommended Reading,,0
105-Blogs.pdf,105-Blogs,"APPENDIX\nRecommended Reading\nThe following are some of the best resources I’ve found on DevOps and infrastruc‐\nture as code, including books, blog posts, newsletters, and talks.\nBooks\n•Infrastructure as Code: Dynamic Systems for the Cloud Age  by Kief Morris •\n(O’Reilly)\n•Site Reliability Engineering: How Google Runs Production Systems  by Betsy Beyer, •\nChris Jones, Jennifer Petoff, and Niall Richard Murphy (O’Reilly)\n•The DevOps Handbook: How To Create World-Class Agility, Reliability, and Secu‐ •\nrity in Technology Organizations  by Gene Kim, Jez Humble, Patrick Debois, and\nJohn Willis (IT Revolution Press)\n•Designing Data-Intensive Applications  by Martin Kleppmann (O’Reilly) •\n•Continuous Delivery: Reliable Software  Releases through Build, Test, and •\nDeployment  Automation  by Jez Humble and David Farley (Addison-Wesley\nProfessional)\n•Release It! Design and Deploy Production-Ready Software  by Michael T. Nygard •\n(The Pragmatic Bookshelf)\n•Kubernetes in Action  by Marko Luksa (Manning) •\n•Leading the Transformation: Applying Agile and DevOps Principles at Scale  by •\nGary Gruver and Tommy Mouser (IT Revolution Press)\n•Visible Ops Handbook  by Kevin Behr, Gene Kim, and George Spafford (Informa‐ •\ntion Technology Process Institute)\n•Effective  DevOps  by Jennifer Davis and Ryn Daniels (O’Reilly) •\n417",1351
106-Newsletters.pdf,106-Newsletters,"•Lean Enterprise  by Jez Humble, Joanne Molesky, Barry O’Reilly (O’Reilly) •\n•Hello, Startup: A Programmer’s Guide to Building Products, Technologies, and•\nTeams  by Y evgeniy Brikman (O’Reilly)\nBlogs\n•High Scalability•\n•Code as Craft•\n•AWS News Blog•\n•Kitchen Soap•\n•Paul Hammant’s blog•\n•Martin Fowler’s blog•\n•Gruntwork Blog•\n•Y evgeniy Brikman blog•\nTalks\n•“Reusable, Composable, Battle-Tested Terraform Modules”  by Y evgeniy Brikman •\n•“5 Lessons Learned from Writing Over 300,000 Lines of Infrastructure Code”  by •\nY evgeniy Brikman\n•“ Automated Testing for Terraform, Docker, Packer, Kubernetes, and More”  by •\nY evgeniy Brikman\n•“Infrastructure as Code: Running Microservices on AWS using Docker, Terra‐•\nform, and ECS”  by Y evgeniy Brikman\n•“ Agility Requires Safety”  by Y evgeniy Brikman •\n•“ Adopting Continuous Delivery”  by Jez Humble •\n•“Continuously Deploying Culture”  by Michael Rembetsy and Patrick McDonnell •\n•“10+ Deploys Per Day: Dev and Ops Cooperation at Flickr”  by John Allspaw and •\nPaul Hammond\n•“Why Google Stores Billions of Lines of Code in a Single Repository”  by Rachel •\nPotvin\n•“The Language of the System”  by Rich Hickey •\n•“Real Software Engineering”  by Glenn Vanderburg •\n418 | Appendix: Recommended Reading",1282
107-Online Forums.pdf,107-Online Forums,Newsletters\n•DevOps Weekly•\n•Gruntwork Newsletter•\n•Terraform: Up & Running Newsletter•\n•Terraform Weekly Newsletter•\nOnline Forums\n•Terraform subforum of HashiCorp Discuss•\n•Terraform subreddit•\n•DevOps subreddit•\nRecommended Reading | 419,249
108-Index.pdf,108-Index,"Index\nA\nAccess Key IDs (in AWS), 42, 44, 196-199,\n200-201\naccessing secrets, interfaces for, 195\n(see also secrets management)\naccidental complexity, 278\naccounts (in AWS)\nconfiguring multiple, 238-245\nsetup, 40-42\nad hoc scripts, 4-5\nagent versus agentless software, 28-29\nALB (Application Load Balancer), 71-78,\n283-285\naliases, 227\nconfiguration aliases, 247-248\nfor Gmail, 241\nKMS CMK aliases, 210\nwhen to use, 236-237, 245\nAmazon S3 (Simple Storage Service), as remote\nbackend, 85-91\nAmazon Web Services (see AWS)\nAMI (Amazon Machine Image)\nami parameter, 46\nPacker template, 10-11\nzero-downtime deployment, 169-179\nAMI IDs, managing, 229-230\nami parameter, 14, 46\nAnsible, comparison with other IaC tools,\n20-36\napplication code workflow\nautomated tests, 385\nchanging code, 383-384\ncomparison with infrastructure code work‐\nflow, 412deployment, 387-390\nlocally run code, 383\nmerge and release, 386\nreviewing code changes, 384\nsteps in, 382\nversion control, 382\nApplication Load Balancer (ALB), 71-78,\n283-285\narguments for resources, 45\narrays\nlookup syntax, 144\nof resources, 145\nASGs (Auto Scaling Groups), 66-70\ninstance refresh, 181-184\nscheduled actions, 127-129\nzero-downtime deployment, 171-179\nassume role policies, 201\nAtlantis, 399-401\nauditing multiple AWS accounts, 239\nauthentication (see secrets management)\nauto healing, 12\nauto scaling, 12, 127-129, 160-162\nautomated tests, 398\nin application code workflow, 385\nend-to-end tests, 362-364\nin infrastructure code workflow, 399-400\nintegration tests\nwith multiple modules, 349-354\nretries in, 360-361\nRuby comparison, 348-349\nstages of, 354-360\nplan testing, 366-370\npurpose of, 320\nserver testing, 370-372\n421\nstatic analysis, 364-366\ntypes of, 320-321\nunit tests\ndependency injection, 332-343\nRuby comparison, 321-324\nrunning in parallel, 343-347\nwith Terratest, 324-332\nwhen to use, 297\nAvailability Zones (AZs), 45, 179-180\nAWS (Amazon Web Services)\naccounts\nconfiguring multiple, 238-245\nsetup, 40-42\nAZs (Availability Zones), 45, 179-180\nbenefits of, 39\nconfiguring as provider, 45\nregions, 45, 226-237\naws eks update-kubeconfig command, 270\nAWS Provider, xx, 222\nAZs (Availability Zones), 45, 179-180\nB\nbackends\nconfiguring with Terragrunt, 409-411\nlocal backends, 84\nremote backends\nlimitations of, 91-93\nshared storage with, 84-91\nBash scripts, 305\nas ad hoc scripts, 4-5\nexternalizing, 111-113\nbest practices for testing, 373\nblocking public access to S3 buckets, 87\nblue-green deployment, 388\nbranches, reasons to avoid, 393-395\nbucket parameter, 86, 88\nbuilt-in functions, explained, 111-113\nbus factor, 16\nC\ncanary deployment, 389\ncentralized secret stores, 194\nchanging code\nin application code workflow, 383-384\nin infrastructure code workflow, 396\nchecklist for production-grade infrastructure,\n279-280\nChef, comparison with other IaC tools, 20-36\nchild accounts, creating, 239-241CI servers\nautomated tests, 385\nas deployment servers, 389, 403-404\nmachine user authentication, 199-206, 219\nCI/CD workflow\nfor application code\nautomated tests, 385\nchanging code, 383-384\ncomparison with infrastructure code\nworkflow, 412\ndeployment, 387-390\nlocally run code, 383\nmerge and release, 386\nreviewing code changes, 384\nsteps in, 382\nversion control, 382\nCircleCI, 199-201\nEC2 Instances with IAM roles, 201-204\nGitHub Actions, 204-206\nfor infrastructure code\nautomated tests, 399-400\nchanging code, 396\ncomparison with application code work‐\nflow, 412\ndeployment, 401-412\nlocally run code, 395\nmerge and release, 400-401\nreviewing code changes, 397-399\nsteps in, 390\nversion control, 391-395\nCIDR blocks, 54, 366\nCircleCI, 199-201, 385\nCircleCI Context, 200\nCLB (Classic Load Balancer), 71\ncleaning up manual tests, 319-320\nCLI (command-line interface), 195\ncloud providers (see providers)\nCloudFormation, comparison with other IaC\ntools, 20-36\ncluster of web servers\ndeploying, 66-70\nrefactoring, 283-285\nclusters (Kubernetes), 12\ndeploying Docker containers, 264-272\ninspecting, 262-264\nCMK (Customer Managed Key), 209\ncode examples\nfair use permissions, xx-xxi\nlocation of, xix-xx\n422 | Index\nversions used for, xx\ncoding guidelines, 397-399\ncollaboration using Git repositories, 51\ncombining IaC tools, 34-36\ncommand history, avoiding writing to, 197\ncommand-line interface (CLI), 195\ncomments, as documentation, 398\ncommunity size in IaC tool comparison, 31-33\ncomparison of IaC tools, 20-36\nagent versus agentless, 28-29\ncombining tools, 34-36\nconfiguration management versus provi‐\nsioning, 21\ngeneral-purpose versus domain-specific lan‐\nguage, 25-26\nlarge versus small community, 31-33\nmaster versus masterless, 26-27\nmature versus cutting edge, 33\nmutable versus immutable infrastructure,\n21-22\npaid versus free, 30-31\nprocedural versus declarative language,\n22-25\ncomposable modules for production-grade\ninfrastructure, 285-291\nconcat built-in function, 165\nconditionals\ncount parameter\nif-else-statements with, 163-165\nif-statements with, 160-162\nfor_each and for expressions, 165-167\nif string directive, 167-168\nternary syntax, 99, 161, 165\ntypes and purpose of, 160\nconfigurable web servers, deploying, 60-66\nconfiguration aliases, 247-248\nconfiguration drift, 2\nconfiguration files\nexplained, 18\nnaming conventions, 102-103\nconfiguration management tools, 5-7\ncombining with provisioning, 34\nprovisioning tools versus, 21\ncontainer engines, 9\ncontainers, 9\ndeploying with EKS (Elastic Kubernetes Ser‐\nvice), 264-272\nexplained, 250-252\nPods, 13, 257-258control plane (Kubernetes), 253, 266\ncost in IaC tool comparison, 30-31\ncount parameter\nconditionals with\nif-else-statements, 163-165\nif-statements, 160-162\nlimitations, 147-149, 179-180\nloops with, 142-149\ncreate_before_destroy lifecycle setting, 68, 141,\n171, 181, 188\ncreation-time provisioners, 309\ncredentials (see secrets)\nCustomer Managed Key (CMK), 209\ncustomer secrets, 193\nD\ndata sources\nexternal, 311-312\nreplicating in multiple AWS regions,\n230-236\nsecrets management, 206\nwith encrypted files, 209-213\nwith environment variables, 207-209\nwith secret stores, 213-217\nfor subnets, 69-70\nterraform_remote_state, 105-113\ndeclarative language\nprocedural versus, 22-25\nTerraform as, 141\ndeclaring variables, 60-66\ndefault parameter, 60\nDefault VPC (in AWS), 42, 59, 69\ndefault_tags block, 155\ndefining listeners, 72\ndependencies\nimplicit, 55\nin Terragrunt, 412\ntypes of, 298\nversioning pinning, 298-303\ndependency injection, 332-343\ndepends_on parameter, 65\ndeploying\napplication code, 387-390\nDocker with EKS (Elastic Kubernetes Ser‐\nvice), 264-272\ninfrastructure code, 401-412\nload balancers, 70-78\nmodules in multiple AWS regions, 230-236\nresources in multiple AWS regions, 228-230\nIndex | 423\nservers\ncluster of web servers, 66-70\nconfigurable web servers, 60-66\nsingle servers, 44-52\nweb servers, 52-59\nwith zero-downtime deployment, 169-179\ndeployment servers, 389, 403-404\ndeployment strategies, 387-389, 402-403\ndeployment tools, 387, 401\nDeployments (Kubernetes), 13, 254-259\ndescription parameter, 60, 64\ndestroy-time provisioners, 309\nDevOps, 1-3\nproject time estimates, 277-279\nresources for information, 417-419\nDocker\ncontainers (see containers)\ndeploying with EKS (Elastic Kubernetes Ser‐\nvice), 264-272\nexplained, 249-252\ninstalling, 249\npurpose of, 11\nDocker Hub, 250\ndocker kill command, 263\ndocker ps command, 262\nDocker Registry, 250\ndocker rm command, 252\ndocker run command, 249\ndocker start command, 251\ndocumentation\nIaC as, 16\nfor Terraform, xix, 46\ntypes of, 397-398\nDRY (Don't Repeat Y ourself) principle, 60, 93,\n407\nDSL (domain-specific language) versus GPL\n(general-purpose programming language),\n25-26\nDynamoDB tables\nlimitations of remote backends, 91-93\nlocking with, 87\ndynamodb_table parameter, 89\nE\nEC2 Instances, 45, 49\n(see also servers)\ndeploying in multiple AWS regions, 228-230\nIAM roles for, 201-204\nEKS (Elastic Kubernetes Service), 264-272ELB (Elastic Load Balancer) service, 70\nenabling\nserver-side encryption, 87\nversioning in Amazon S3, 86\nencrypt parameter, 89\nencryption\nsecrets management with, 209-213\nserver-side, enabling, 87\nend-to-end tests, 321, 362-364\nenforcing tagging standards, 155\nenvironment variables, secrets management\nwith, 107, 197-199, 207-209\nenvironments\nfolders for, 101-102\npromotion across, 390, 405-412\nsandbox, 319-320, 330, 396\nerrors in deployment, 402-403\nessential complexity, 278\nexample code, as documentation, 398\nexamples\nfair use permissions, xx-xxi\nlocation of, xix-xx\nversions used for, xx\nexecuting scripts\nfor external data source, 311-312\nwith provisioners, 306-310\nexpressions, 55\nexternal data source, 311-312\nexternalizing Bash scripts, 111-113\nF\nfair use permissions for code examples, xx-xxi\nfalse incrementalism, 379\nfeature toggles, 389\nfile layout\nconventions for, 398\nisolating state files, 100-105\nfile paths\nlocal, 138\npath references for, 129-130\nfile-based secret stores, 194\nfolders\nfor environments, 101-102\nrunning commands across multiple, 104\nfor expressions\nconditionals with, 165-167\nloops with, 156-158\nfor string directive, loops with, 158-159\nformat built-in function, 111\n424 | Index\nfor_each expressions\nconditionals with, 165-167\nlimitations, 179-180\nloops with, 149-154\nfunction composition, 285-291\nG\nget deployments command, 262, 271\nget nodes command, 254, 270\nget pods command, 262, 271\nget services command, 263, 271\ngit commit command, 52\ngit push command, 52\nGit repositories\ncollaborating with teammates, 51\nSSH authentication for, 137\nGitHub Actions, 204-206\nGitHub, pull requests, 384\n.gitignore file, 51\nGmail aliases, 241\nGo, installing, 325\nGolden Rule of Terraform, 392-393\nGPL (general-purpose programming language)\nversus DSL (domain-specific language),\n25-26\nH\nHCL (HashiCorp Configuration Language), 44\nhealth checks, 74\nheredoc syntax, 54, 167\nhistory files, avoiding writing to, 197\nHofstadter's Law, 277\nhuman users, secrets management and, 197-199\nhypervisors, 8\nI\nIaC (infrastructure as code), 275\n(see also production-grade infrastructure)\nad hoc scripts, 4-5\nbenefits of, 16-17\nconfiguration management tools, 5-7\nimporting into state files, 185\norchestration tools, 12-14\nout of band resources, 185\nprovisioning tools, 14\npurpose of, 3\nrefactoring, 186-189\nresources for information, 417-419server templating tools, 7-11\nteam adoption of\nincrementalism in, 379-380\nlearning curve, 380-381\nreturn on investment, 376-379\ntool comparison, 20-36\nagent versus agentless, 28-29\ncombining tools, 34-36\nconfiguration management versus provi‐\nsioning, 21\ngeneral-purpose versus domain-specific\nlanguage, 25-26\nlarge versus small community, 31-33\nmaster versus masterless, 26-27\nmature versus cutting edge, 33\nmutable versus immutable infrastruc‐\nture, 21-22\npaid versus free, 30-31\nprocedural versus declarative language,\n22-25\nvalidating, 319\nIAM (Identity and Access Management) ser‐\nvice, 40\nIAM OIDC identity providers, 204\nIAM Policies, 41\nIAM roles\nchild account authentication, 243\ncross-account authentication, 244\nfor EC2 Instances, 201-204\nIAM users\nchanging policies, 163-165\ncreating, 40, 142-152\nerrors, 184-186\nidempotence, 6, 47, 89\nif string directive, conditionals with, 167-168\nif-else-statements with count parameter,\n163-165\nif-statements with count parameter, 160-162\nimages (of servers), 7-11\nimmutable infrastructure\nexplained, 11\nmutable versus, 21-22\npromotion across environments, 390, 405\nreleasing application code, 386\nworkflows, 412-413\nimplicit dependencies, 55\nimporting infrastructure, 185\nincrementalism in IaC (infrastructure as code)\nadoption, 379-380\nIndex | 425\ninfrastructure as code (see IaC)\ninfrastructure code workflow\nautomated tests, 399-400\nchanging code, 396\ncomparison with application code work‐\nflow, 412\ndeployment, 401-412\nlocally run code, 395\nmerge and release, 400-401\nreviewing code changes, 397-399\nsteps in, 390\nversion control, 391-395\ninfrastructure secrets, 194\ninline blocks\ncreating multiple with for_each expressions,\n152-154\nseparate resources versus, 130-132\ninput variables, 60-64\nin composable modules, 286-291\nin modules, 121-124\ninspecting Kubernetes clusters, 262-264\ninstalling\nDocker, 249\nGo, 325\nkubectl, 254\nproviders, 223-225\nTerraform, 43-44, 300\nTerragrunt, 407\ninstance metadata endpoints, 203\ninstance profiles, 202\ninstance refresh, 181-184\ninstance_type parameter, 46\nintegration tests\npurpose of, 321\nretries in, 360-361\nRuby comparison, 348-349\nstages of, 354-360\nwith multiple modules, 349-354\ninterfaces, accessing secrets, 195\ninterpolation, 64\nisolating\nmultiple AWS accounts, 238\nat network level, 133\nstate files, 83\npurpose of, 93-94\nvia file layout, 100-105\nvia workspaces, 94-100K\nkernel, 9\nkernel space, 9\nkey parameter, 88\nkey policies, 209\nKMS (key management system), 194\nkubectl, 254, 262-263\nKubernetes\nclusters, 12\ndeploying Docker containers, 264-272\ninspecting, 262-264\ncontrol plane, 253, 266\nEKS (Elastic Kubernetes Service), 264-272\nexplained, 252-264\nkubectl, 254, 262-263\nobjects in, 254\nPods, 13, 257-258\nrunning, 253\nworker nodes, 253, 266\nKubernetes Deployment, 13, 254-259\nKubernetes Service, 255, 259-261\nL\nlarge modules, limitations of, 281\nlaunch configurations, 67\nlaunch templates, 67\nlearning curve in IaC (infrastructure as code)\nadoption, 380-381\nlength built-in function, 144\nlifecycle settings, 68, 86, 171\nlist comprehensions (Python), 156\nlistener rules, creating, 75\nlisteners, defining, 72\nliterals, 55\nlive repository, 391-393, 406-412\nload balancing, 12, 70-78\nlocal backends, 84\nlocal file paths, 138\nlocal names, 224\nlocal references, 126\nlocal values in modules, 125-127\nlocally run code\nin application code workflow, 383\nin infrastructure code workflow, 395\nlock files, 302\nlocking\nwith DynamoDB tables, 87\nwith remote backends, 84\nstate files, 83\n426 | Index\nversion control disadvantages of, 84\nlookup syntax for arrays, 144\nloops\ncount parameter, 142-149\nfor expressions, 156-158\nfor string directive, 158-159\nfor_each expressions, 149-154\ntypes and purpose of, 142\nM\nmachine users, secrets management and,\n199-206, 219\nmanaged node groups, 266-268\nManaged Policies (in AWS), 41\nmanual tests\ncleanup, 319-320\nexplained, 317-319\nRuby comparison, 316-317\nmaps of resources, 150\nmaster versus masterless servers, 26-27\nmaturity in IaC tool comparison, 33\nmerging code\nin application code workflow, 386\nin infrastructure code workflow, 400-401\nmocks, 320\nmodules\nauthenticating multiple AWS accounts,\n242-244\nconfiguring for Kubernetes, 261-262\ncount parameter, 146-147\ncreating for Kubernetes, 255-261\ncreating reusable, 118-120\ndeploying in multiple AWS regions, 230-236\nfor_each expressions, 152\ninline blocks versus separate resources,\n130-132\ninput variables, 121-124\nintegration tests, 349-354\nlocal values, 125-127\nfor multiple providers, 245-248\noutput variables, 127-129\npath references, 129-130\nfor production-grade infrastructure\ncomposable modules, 285-291\nnon-Terraform code in, 305-312\nsmall modules, 281-285\ntestable modules, 291-298\nversioned modules, 298-305\npublishing, 304-305purpose of, 92, 117\nrepository for, 391-392\nself-validating, 293\nprecondition/postcondition blocks in,\n295-297\nvalidation blocks in, 294-295\ntagging standards, 155\ntypes of, 118, 245\nunit tests for, 324-332\nversioning, 133-138\nversioning pinning, 302-303\nmultiple AWS accounts, configuring, 238-245\nmultiple AWS regions, configuring, 226-237\nmultiple folders, running commands across,\n104\nmultiple inline blocks, creating with for_each\nexpressions, 152-154\nmultiple modules, integration tests with,\n349-354\nmultiple providers\nDocker, explained, 249-252\nEKS (Elastic Kubernetes Service), 264-272\nKubernetes, explained, 252-264\nmultiple AWS accounts, 238-245\nmultiple AWS regions, 226-237\nmultiple copies of different providers,\n248-272\nmultiple copies of same provider, 226-248\nreusable modules for, 245-248\nmutable versus immutable infrastructure, 21-22\nN\nnaming\nconfiguration files, 102-103\nresources, 49\nnetwork isolation, 133\nnetwork security\nCIDR blocks, 54, 366\ndata sources for subnets, 69\nDefault VPC (in AWS), 42\nnetwork isolation, 133\nport numbers and, 53\nsecurity groups, creating, 54, 73\nVPC subnets, 59\nNLB (Network Load Balancer), 71\nnon-Terraform code, running in modules,\n305-312\nnull_resource, 310-311\nIndex | 427\nO\nobjects (Kubernetes), 254\nOIDC (Open ID Connect), 204-206\none built-in function, 165\nOPA (Open Policy Agent), 368-370\nOpenStack Heat, comparison with other IaC\ntools, 20-36\norchestration tools, 12-14\ncombining with provisioning and server\ntemplating, 35-36\nfor deployment, 387\npurpose of, 252\nout of band resources, 185\noutput variables, 64-66\nin composable modules, 286-291\nin modules, 127-129\nP\nPacker, 7-11\nparallel unit tests, running, 343-347\npartial configurations, 92, 351-352\npassing secrets via environment variables, 107,\n197-199\npath references in modules, 129-130\npermissions for code examples, xx-xxi\npersonal secrets, 193\nplain text, avoiding secrets in, 192-193\nplan files, secrets management, 218-219\nplan testing, 366-370, 399\nplugins, providers as, 222-223\nPod Template, 257-258\nPods, 13, 257-258\npolicy-as-code tools, 368-370\nport numbers, 53\npostcondition blocks, 295-297\nprecondition blocks, 295-297\npreferred local names, 224\nprevent_destroy parameter, 86\nprivate API, state files as, 83\nprivate Git repositories, 137\nprivate keys, 194\nprivate subnets, 60\nPrivate Terraform Registry, 305\nprocedural versus declarative language, 22-25\nproduction-grade infrastructure\nchecklist for, 279-280\ndefinition of, 275\nmodules for\ncomposable modules, 285-291non-Terraform code in, 305-312\nsmall modules, 281-285\ntestable modules, 291-298\nversioned modules, 298-305\ntime estimates for, 275-279\npromotion across environments, 390, 405-412\nproviders, 249\n(see also multiple providers)\nconfiguring, 45, 225-226\nmultiple AWS accounts, 238-245\nmultiple AWS regions, 226-237\nin reusable modules, 245-248\ninstalling, 223-225\nas plugins, 222-223\nsecrets management, 196-197\nhuman users and, 197-199\nmachine users and, 199-206, 219\ntransparent portability, 19\nversioning pinning, 301-302\nprovisioners\nexecuting scripts, 306-310\nwith null_resource, 310-311\nUser Data scripts versus, 310\nprovisioning tools, 14\ncombining with configuration management,\n34\ncombining with orchestration and server\ntemplating, 35-36\ncombining with server templating, 35-35\nconfiguration management tools versus, 21\npublic access to S3 buckets, blocking, 87\npublic keys, 194\npublic subnets, 59\nPublic Terraform Registry, 304-305\npublishing modules, 304-305\npull requests, 384\nPulumi, comparison with other IaC tools, 20-36\nPuppet, comparison with other IaC tools, 20-36\nR\nrandom integers, generating, 180\nRDS (Relational Database Service), 105\nREADME files, 398\nreconciliation loops, 254\nrefactoring\nTerraform code, 186-189\nweb server clusters, 283-285\nreferences, 55\nregion parameter, 89\n428 | Index\nregions (in AWS), 45, 226-237\nRego, 368-370\nRelational Database Service (RDS), 105\nrelative filepaths, 129\nreleasing code\nin application code workflow, 386\nin infrastructure code workflow, 400-401\nremote backends\nlimitations of, 91-93\nshared storage with, 84-91\nremote procedure calls (RPCs), 222\nremoving resources, 79-80\nreplicas, 13\nreplicating data sources in multiple AWS\nregions, 230-236\nrepositories\nsharing, 51\nSSH authentication for, 137\nfor version control, 391-392\nrequired_providers block, 224-225\nrequired_version parameter, 298\nresource attribute references, 55\nresource dependencies, 105\nresources\narrays of, 145\ncreating, 45\ncreating out of band, 185\ndeploying in multiple AWS regions, 228-230\nlifecycle settings, 68\nmaps of, 150\nnaming, 49\nnull_resource, 310-311\nremoving, 79-80\nsecrets management, 206\nwith encrypted files, 209-213\nwith environment variables, 207-209\nwith secret stores, 213-217\nseparate resources versus inline blocks,\n130-132\ntagging standards, 155\nresources for information, 417-419\nretries\nin integration tests, 360-361\nwith known errors, 402\nreturn on investment for IaC (infrastructure as\ncode), 376-379\nreusability of code, 17\nreusable modules, 245\ncreating, 118-120for multiple providers, 245-248\nreviewing code changes\nin application code workflow, 384\nfor infrastructure code, 397-399\nrolling deployments, 7, 388-388\nroot accounts, 239\nroot modules, 118, 245\nroot users, 40\nport numbers and, 53\nRPCs (remote procedure calls), 222\nrunning\nKubernetes, 253\nparallel unit tests, 343-347\nS\nS3 buckets\ncreating, 85-86\nlimitations of remote backends, 91-93\npublic access, blocking, 87\nserver-side encryption, enabling, 87\nversioning, enabling, 86\nsandbox environments, 319-320, 330, 396\nscheduled actions, 127, 160\nscripts\nfor deployment, 387, 401\nexecuting\nfor external data source, 311-312\nwith provisioners, 306-310\nSecret Access Keys (in AWS), 42, 44\nsecret stores, secrets management with,\n213-217\nsecrets\naccess interface for, 195\navoiding in plain text, 192-193\npassing via environment variables, 107\nwith remote backends, 85\nstorage methods, 194-195\ntypes of, 193-194\nversion control disadvantages, 84\nsecrets management\nCI/CD workflow\nCircleCI, 199-201\nEC2 Instances with IAM roles, 201-204\nGitHub Actions, 204-206\nmultiple AWS accounts, 238, 242-244\nfor plan files, 218-219\nfor providers, 196-197\nhuman users and, 197-199\nmachine users and, 199-206, 219\nIndex | 429\nfor resources and data sources, 206\nwith encrypted files, 209-213\nwith environment variables, 207-209\nwith secret stores, 213-217\nrules of, 192-193\nfor state files, 217-218\nTerraform authentication options, 44\ntool comparison, 195-196\nsecurity (see network security; secrets manage‐\nment)\nsecurity groups, creating, 54, 73\nselector block, 259\nself-validating modules, 293\nprecondition/postcondition blocks in,\n295-297\nvalidation blocks in, 294-295\nsemantic versioning, 136\nsensitive parameter, 61, 65\nseparate resources versus inline blocks, 130-132\nserver templating tools, 7-11\ncombining with orchestration and provi‐\nsioning, 35-36\ncombining with provisioning, 35\nserver testing, 370-372\nserver-side encryption, enabling, 87\nservers, deploying\ncluster of web servers, 66-70\nconfigurable web servers, 60-66\nsingle servers, 44-52\nweb servers, 52-59\nservice discovery, 12\nshared storage for state files, 83-91\nwith remote backends, 84-91\nversion control disadvantages, 83-84\nsharing Git repositories with teammates, 51\nsmall modules for production-grade infrastruc‐\nture, 281-285\nsnowflake servers, 2\nsoftware delivery, 1\nsops, 212\nsplat expression, 145\nSpolsky, Joel, 382\nSSH authentication for Git repositories, 137\nstages of integration tests, 354-360\nstate files\nexplained, 82-83\nimporting infrastructure, 185\nisolating, 83\npurpose of, 93-94via file layout, 100-105\nvia workspaces, 94-100\nlocking, 83\nas private API, 83\nsecrets management, 217-218\nshared storage for, 83-91\nwith remote backends, 84-91\nversion control disadvantages, 83-84\nterraform_remote_state data source,\n105-113\nstatic analysis, 364-366\nstoring secrets, 194-195\nstring directives, 158\nstrip markers, 168\nstructural types, 62\nstyle guidelines, 399\nsubnets\ndata sources for, 69-70\nof VPCs, 59\nsubnets parameter, 72\nsubnet_ids parameter, 69\nT\ntagging standards, enforcing, 155\ntarget groups, creating, 74\nTDD (Test-Driven Development), 293\nteam adoption of IaC\nincrementalism in, 379-380\nlearning curve, 380-381\nreturn on investment, 376-379\ntemplatefile built-in function, 111-113, 129\nternary syntax, 99, 161, 165\nTerraform\nauthentication options, 44\nchanges, 2017-2019, xvi-xviii\ncomparison with other IaC tools, 20-36\nagent versus agentless, 28-29\ncombining tools, 34-36\nconfiguration management versus provi‐\nsioning, 21\ngeneral-purpose versus domain-specific\nlanguage, 25-26\nlarge versus small community, 31-33\nmaster versus masterless, 26-27\nmature versus cutting edge, 33\nmutable versus immutable infrastruc‐\nture, 21-22\npaid versus free, 30-31\n430 | Index\nprocedural versus declarative language,\n22-25\ncore, 222, 298-301\nas declarative language, 141\nas deployment tool, 387\ndocumentation, xix, 46\nGolden Rule of Terraform, 392-393\ninstalling, 43-44, 300\nnew features, xiv-xvi\noperational overview, 17-19\npurpose of, ix\nrefactoring, 186-189\ntransparent portability, 19\nversion used, xx\nterraform apply command, 48\nTerraform Cloud, 401\nTerraform configurations, 18\nterraform console command, 111\nterraform destroy command, 79\nTerraform Enterprise, 401\nterraform fmt command, 399\nterraform graph command, 56\nterraform import command, 185\nterraform init command, 47, 89\nterraform output command, 65\nterraform plan command, 47, 184-186,\n366-370, 399\nTerraform state files (see state files)\nterraform validate command, 365\nterraform version command, 301\nterraform workspace list command, 97\nterraform workspace new command, 96\nterraform workspace select command, 97\nterraform workspace show command, 95\nterraform_remote_state data source, 105-113\nTerragrunt, 93, 401\ninstalling, 407\npromotion across environments, 407-412\ntgswitch, 301\nTerratest, 248\nintegration tests with, 355-360\nplan tests with, 367-368\nretries in integration tests, 360-361\nunit tests with, 324-332\nversions of, 330\ntest doubles, 320\ntest pyramid, 362\nTest-Driven Development (TDD), 293testable modules for production-grade infra‐\nstructure, 291-298\ntesting\nautomated tests, 398\nin application code workflow, 385\nend-to-end tests, 362-364\nin infrastructure code workflow, 399-400\nintegration tests, 348-361\nplan testing, 366-370\npurpose of, 320\nserver testing, 370-372\nstatic analysis, 364-366\ntypes of, 320-321\nunit tests, 321-347\nwhen to use, 297\nbest practices, 373\ncomparison of approaches, 373\nmanual tests\ncleanup, 319-320\nexplained, 317-319\nRuby comparison, 316-317\ntfenv, 299-301\ntgswitch, 301\ntime estimates for production-grade infrastruc‐\nture, 275-279\ntransparent portability, 19\ntype constraints, 61\ntype parameter, 61\nU\nUbuntu, 249\nUI (user interface), 195\nunit tests\ndependency injection, 332-343\npurpose of, 320\nRuby comparison, 321-324\nrunning in parallel, 343-347\nwith Terratest, 324-332\nunits, defined, 320\nUser Data scripts versus provisioners, 310\nuser interface (UI), 195\nuser space, 9\nuser_data parameter, 14, 53\nuser_data_replace_on_change parameter, 54\nV\nVagrant, purpose of, 11\nvalidating\ncode, 17\nIndex | 431\ninfrastructure, 319\nvalidation blocks, 294-295\nvalidation parameter, 61\nvalues built-in function, 151\nvariable references, 64\nvariables, 121\n(see also input variables; output variables)\ndeclaring, 60-66\nversion control, 51\nin application code workflow, 382\ndisadvantages of, 83-84\nimportance of, 17\nin infrastructure code workflow, 391-395\nsecrets in, 192\nversioning\nin Amazon S3, 85, 86\nmodules, 133-138\nfor production-grade infrastructure,\n298-305\nversioning pinning, 298-303\nVM images, 8\nVMs (virtual machines), 8\nVPCs (virtual private clouds)Default VPC (in AWS), 42\nnetwork isolation, 133\nsubnets, 59\nW\nweb servers\ndeploying, 52-59\ncluster of web servers, 66-70\nconfigurable web servers, 60-66\nrefactoring, 283-285\nworker nodes (Kubernetes), 253, 266\nworkflows (see CI/CD workflow)\nworkspaces\nisolating state files, 94-100\nlimitations of, 99\nY\nyak shaving, 277-278\nZ\nzero-downtime deployment, 169-179\nlimitations, 181-184\n432 | Index",28026
109-About the Author.pdf,109-About the Author,,0
110-Colophon.pdf,110-Colophon,"About the Author\nY evgeniy (Jim) Brikman  loves programming, writing, speaking, traveling, and lifting\nheavy things. He is the cofounder of Gruntwork, a company that provides DevOps\nas a Service. He’s also the author of another book published by O’Reilly Media\ncalled Hello, Startup: A Programmer’s Guide to Building Products, Technologies, and\nTeams . Previously, he worked as a software engineer at LinkedIn, TripAdvisor, Cisco\nSystems, and Thomson Financial and got his BS and master’s degrees at Cornell\nUniversity. For more info, check out ybrikman.com .\nColophon\nThe animal on the cover of Terraform: Up and Running  is a flying dragon lizard\n(Draco volans ), a small reptile so named for its ability to glide using winglike flaps\nof skin known as patagia . The patagia are brightly colored and allow the animal to\nglide for up to eight meters. The flying dragon lizard is commonly found in many\nSoutheast Asian countries, including Indonesia, Vietnam, Thailand, the Philippines,\nand Singapore.\nFlying dragon lizards feed on insects and can grow to more than 20 centimeters in\nlength. They live primarily in forested regions, gliding from tree to tree to find prey\nand avoid predators. Females descend from the trees only to lay their eggs in hidden\nholes in the ground. The males are highly territorial and will chase rivals from tree to\ntree.\nAlthough once thought to be poisonous, flying dragon lizards pose no threat to\nhumans and are sometimes kept as pets. They are not currently threatened or\nendangered. Many of the animals on O’Reilly covers are endangered; all of them\nare important to the world.\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\nfrom Johnson’s Natural History . The cover fonts are Gilroy Semibold and Guardian\nSans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐\ndensed; and the code font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant Answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175",2206
