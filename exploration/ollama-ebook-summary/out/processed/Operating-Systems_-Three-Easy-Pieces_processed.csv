Title,Text,Character Count
Introduction. Preface,"Preface To Everyone Welcome to this book. We hope you’ll enjoy reading it as much as we e njoyed writing it. The book is called Operating Systems: Three Easy Pieces (available athttp://www.ostep.org ), and the title is obviously an homage to one of the greatest sets of lecture notes ever created, by one Richard Fe ynman on the topic of Physics [F96]. While this book will undoubtedly fall short of t he high standard set by that famous physicist, perhaps it will be good enough for you i n your quest to understand what operating systems (and more generally, systems ) are all about. The three easy pieces refer to the three major thematic elements the book is organized around: virtualization ,concurrency ,a n d persistence . In discussing these concepts, we’ll end up discussing most of the important thi ngs an operating system does; hopefully, you’ll also have some fun along the way. L earning new things is fun, right? At least, it should be. Each major concept is divided into a set of chapters, most of whi ch present a particular problem and then show how to solve it. The chapters are short, and try (as best as possible) to reference the source material where th e ideas really came from. One of our goals in writing this book is to make the paths of h istory as clear as possible, as we think that helps a student understand what is ,w h a tw a s ,a n d what will be more clearly. In this case, seeing how the sausage w as made is nearly as important as understanding what the sausage is good for1. There are a couple devices we use throughout the book which are pro bably worth introducing here. The ﬁrst is the crux of the problem. Anytime we are trying to solve a problem, we ﬁrst try to state what the most impor tant issue is; such a crux of the problem is explicitly called out in the text, and hopefully solved via the techniques, algorithms, and ideas presented in the res t of the text. In many places, we’ll explain how a system works by showing its b ehavior over time. These timelines are at the essence of understanding; if you know what happens, for example, when a process page faults, you are on your w ay to truly understanding how virtual memory operates. If you comprehend wha tt a k e sp l a c e when a journaling ﬁle system writes a block to disk, you have ta ken the ﬁrst steps towards mastery of storage systems. There are also numerous asides and tipsthroughout the text, adding a little color to the mainline presentation. Asides tend to discuss somet hing relevant (but perhaps not essential) to the main text; tips tend to be general lessons that can be 1Hint: eating. Or if you’re a vegetarian, running away from. iii iv applied to systems you build. An index at the end of the book lists all of these tips and asides (as well as cruces, the odd plural of crux) for your conve nience. We use one of the oldest didactic methods, the dialogue , throughout the book, as a way of presenting some of the material in a different light. These are used to introduce the major thematic concepts (in a peachy way, as we wil ls e e ) ,a sw e l la s to review material every now and then.",3119
Introduction. Preface,"They are also a chance to write in a more humorous style. Whether you ﬁnd them useful, or humorous, well, that’ sa n o t h e r matter entirely. At the beginning of each major section, we’ll ﬁrst present an abstraction that an operating system provides, and then work in subsequent chapte rs on the mecha- nisms, policies, and other support needed to provide the abstr action. Abstractions are fundamental to all aspects of Computer Science, so it is perha ps no surprise that they are also essential in operating systems. Throughout the chapters, we try to use real code (not pseudocode )w h e r ep o s - sible, so for virtually all examples, you should be able to type th em up yourself and run them. Running real code on real systems is the best way to learn about operat- ing systems, so we encourage you to do so when you can. We are also making code available at https://github.com/remzi-arpacidusseau/ostep-code for your viewing pleasure. In various parts of the text, we have sprinkled in a few homeworks to ensure that you are understanding what is going on. Many of these homew orks are little simulations of pieces of the operating system; you should download the home- works, and run them to quiz yourself. The homework simulators have t he follow- ing feature: by giving them a different random seed, you can ge nerate a virtually inﬁnite set of problems; the simulators can also be told to solve the problems for you. Thus, you can test and re-test yourself until you have achiev ed a good level of understanding. The most important addendum to this book is a set of projects in which you learn about how real systems work by designing, implementing, an d testing your own code. All projects (as well as the code examples, mentioned ab ove) are in theCp r o g r a m m i n gl a n g u a g e [KR88]; C is a simple and powerful language that underlies most operating systems, and thus worth adding to your to ol-chest of languages. Two types of projects are available (see the onlin ea p p e n d i xf o ri d e a s ) . The ﬁrst are systems programming projects; these projects are great for those who are new to C and U NIXand want to learn how to do low-level C programming. The second type are based on a real operating system kernel de veloped at MIT called xv6 [CK+08]; these projects are great for students that already have some C and want to get their hands dirty inside the OS. At Wisconsin, w e’ve run the course in three different ways: either all systems programming, all xv6 programming, or a mix of both. We are slowly making project descriptions, and a testing frame work, avail- able. See https://github.com/remzi-arpacidusseau/ostep-projec ts for more information. If not part of a class, this will give you a chance to do these projects on your own, to better learn the material. Unfortunatel y, you don’t have a TA to bug when you get stuck, but not everything in life can be fre e (but books can be.). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG v To Educators If you are an instructor or professor who wishes to use this book ,p l e a s ef e e l free to do so. As you may have noticed, they are free and availabl eo n - l i n ef r o m the following web page: http://www.ostep.org You can also purchase a printed copy from lulu.com .L o o kf o ri to nt h ew e b page above.",3305
Introduction. Preface,"The (current) proper citation for the book is as follows: Operating Systems: Three Easy Pieces Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau Arpaci-Dusseau Books August, 2018 (Version 1.00) http://www.ostep.org The course divides fairly well across a 15-week semester, in wh ich you can cover most of the topics within at a reasonable level of depth. Cramming the course into a 10-week quarter probably requires dropping some de tail from each of the pieces. There are also a few chapters on virtual machine mo nitors, which we usually squeeze in sometime during the semester, either right at end of the large section on virtualization, or near the end as an aside. One slightly unusual aspect of the book is that concurrency, a top ic at the front of many OS books, is pushed off herein until the student has built an understand- ing of virtualization of the CPU and of memory. In our experience in teaching this course for nearly 20 years, students have a hard time underst anding how the concurrency problem arises, or why they are trying to solve it , if they don’t yet un- derstand what an address space is, what a process is, or why co ntext switches can occur at arbitrary points in time. Once they do understand these concepts, how- ever, introducing the notion of threads and the problems that a rise due to them becomes rather easy, or at least, easier. As much as is possible, we use a chalkboard (or whiteboard) to deli ver a lec- ture. On these more conceptual days, we come to class with a few majo ri d e a s and examples in mind and use the board to present them. Handouts are us eful to give the students concrete problems to solve based on the mate rial. On more practical days, we simply plug a laptop into the projector and s how real code; this style works particularly well for concurrency lectures as well as for any discus- sion sections where you show students code that is relevant fo rt h e i rp r o j e c t s .W e don’t generally use slides to present material, but have now made as e ta v a i l a b l e for those who prefer that style of presentation. If you’d like a copy of any of these materials, please drop us an ema il. We have already shared them with many others around the world, and others have contributed their materials as well. One last request: if you use the free online chapters, please jus tlink to them, instead of making a local copy. This helps us track usage (over 1 mi llion chapters downloaded in the past few years.) and also ensures students ge tt h el a t e s t( a n d greatest?) version. c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES vi To Students If you are a student reading this book, thank you. It is an honor f or us to provide some material to help you in your pursuit of knowledge about operating systems. We both think back fondly towards some textbooks of our un dergraduate days (e.g., Hennessy and Patterson [HP90], the classic book on computer architec- ture) and hope this book will become one of those positive memorie s for you. You may have noticed this book is free and available online2.",3069
Introduction. Preface,"There is one major reason for this: textbooks are generally too expensive. This b ook, we hope, is the ﬁrst of a new wave of free materials to help those in pursuit of the ir education, regardless of which part of the world they come from or how much th ey are willing to spend for a book. Failing that, it is one free book, which is better than none. We also hope, where possible, to point you to the original sour ces of much of the material in the book: the great papers and persons who ha ve shaped the ﬁeld of operating systems over the years. Ideas are not pulled o ut of the air; they come from smart and hard-working people (including numerous Turing -award winners3), and thus we should strive to celebrate those ideas and people where possible. In doing so, we hopefully can better understand the r evolutions that have taken place, instead of writing texts as if those thoughts have always been present [K62]. Further, perhaps such references will encourag ey o ut od i gd e e p e r on your own; reading the famous papers of our ﬁeld is certainly one of the best ways to learn. 2A digression here: “free” in the way we use it here does not mean open source, and it does not mean the book is not copyrighted with the usual protections – i t is. What it means is that you can download the chapters and use them to learn about operating systems. Why not an open-source book, just like Linux is an open-source kernel? Well, w e believe it is important for a book to have a single voice throughout, and have worked hard to prov ide such a voice. When you’re reading it, the book should kind of feel like a dialogue w ith the person explaining something to you. Hence, our approach. 3The Turing Award is the highest award in Computer Science; it is like the N obel Prize, except that you have never heard of it. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG vii Acknowledgments This section will contain thanks to those who helped us put the b ook together. The important thing for now: your name could go here. But, you have to help. So send us some feedback and help debug this book. And you could be famous .O r , at least, have your name in some book. The people who have helped so far include: Aaron Gember (Colgate), Aashrith H Govindraj (USF), Abhinav Mehra, Abhirami Senthilkumaran*, Adam Dresche r* (WUSTL), Adam Eggum, Aditya Venkataraman, Adriana Iamnitchi and class (USF), Ahm ad Jarara, Ahmed Fikri*, Ajaykrishna Raghavan, Akiel Khan, Alex Curtis, Alex Wyler , Alex Zhao (U. Colorado at Colorado Springs), Ali Razeen (Duke), Alistair Martin, AmirBehzad Eslami, Anand Mundada, Andrew Mahler, Andrew Valencik (Saint Mary’s), Angela Demke Brown (Toront o), Antonella Bernobich (UoPeople)*, Arek Bulski, B. Brahmananda Reddy (Minnesota), Ba la Subrahmanyam Kambala, Bart Miller, Ben Kushigian (U. Mass), Benita Bose, Biswaj it Mazumder (Clemson), Bobby Jack, Bj ¨orn Lindberg, Brandon Harshe (U. Minn), Brennan Payne, Brian Gorman, Brian Kroth, Caleb Sumner (Southern Adventist), Cara Lauritzen, Charlott e Kissinger, Cheng Su, Chien-Chung Shen (Delaware)*, Christian Stober, Christoph Jaeger, C.J.",3115
Introduction. Preface,"Stanbridge (Memorial U. of Newfoundland), Cody Hanson, Constantinos Georgiades, Dakota Cr ane (U. Washington- Tacoma), Dan Soendergaard (U. Aarhus), Dan Tsafrir (Technion), Danilo Brus chi (Universita Degli Studi Di Milano), Darby Asher Noam Haller, David Hanle (Grinnel l), David Hartman, Deepika Muthukumar, Demir Delic, Dennis Zhou, Dheeraj Shetty (North C arolina State), Do- rian Arnold (New Mexico), Dustin Metzler, Dustin Passofaro, Eduardo Stelmaszczyk, Emad Sadeghi, Emil Hessman, Emily Jacobson, Emmett Witchel (Texas), Eric F reudenthal (UTEP), Eric Johansson, Erik Turk, Ernst Biersack (France), Fangjun Kuang (U. Stut tgart), Feng Zhang (IBM), Finn Kuusisto*, Giovanni Lagorio (DIBRIS), Glenn Bruns (CSU Monter ey Bay), Glen Granzow (College of Idaho), Guilherme Baptista, Hamid Reza Ghasem i, Hao Chen, Henry Abbey, Hilmar G ´ustafsson (Aalborg University), Hrishikesh Amur, Huanchen Zhang*, Hu seyin Sular, Hugo Diaz, Ilya Oblomkov, Itai Hass (Toronto), Jackson “Jake ” Haenchen (Texas), Ja- gannathan Eachambadi, Jake Gillberg, Jakob Olandt, James Earley, James Perry (U. Michigan- Dearborn)*, Jan Reineke (Universit ¨at des Saarlandes), Jason MacLafferty (Southern Adven- tist), Jason Waterman (Vassar), Jay Lim, Jerod Weinman (Grinnell), Jhih-Cheng Luo, Jiao Dong (Rutgers), Jia-Shen Boon, Jiawen Bao, Jingxin Li, Joe Jean (NYU), Joel Kuntz (S aint Mary’s), Joel Sommers (Colgate), John Brady (Grinnell), John Komenda, Jonathan Perry (MI T), Joshua Carpenter (NCSU), Jun He, Karl Wallinger, Kartik Singhal, Katherine D udenas, Katie Coyle (Georgia Tech), Kaushik Kannan, Kemal Bıc ¸akcı, Kevin Liu*, Lanyue Lu, Laura Xu, Lei Tian (U. Nebraska-Lincoln), Leonardo Medici (U. Milan), Leslie Schultz, Liang Yin, Lihao Wang, Looserof, Manav Batra (IIIT-Delhi), Manu Awasthi (Samsung), Marcel van der H olst, Marco Guazzone (U. Piemonte Orientale), Mart Oskamp, Martha Ferris, Masashi Kis hikawa (Sony), Matt Reichoff, Mattia Monga (U. Milan), Matty Williams, Meng Huang, Michael Machtel (Hochschule Konstanz), Michael Walﬁsh (NYU), Michael Wu (UCLA), Mike Griepentrog , Ming Chen (Stonybrook), Mohammed Alali (Delaware), Mohamed Omran (GUST), Muruga n Kan- daswamy, Nadeem Shaikh, Natasha Eilbert, Natasha Stopa, Nathan D ipiazza, Nathan Sulli- van, Neeraj Badlani (N.C. State), Neil Perry, Nelson Gomez, Nghia H uynh (Texas), Nicholas Mandal, Nick Weinandt, Patel Pratyush Ashesh (BITS-Pilani), Patricio Jara, Pav le Kostovic, Perry Kivolowitz, Peter Peterson (Minnesota), Pieter Kockx, Radford Sm ith, Riccardo Mut- schlechner, Ripudaman Singh, Robert Ord ´o˜nez and class (Southern Adventist), Roger Watten- hofer (ETH), Rohan Das (Toronto)*, Rohan Pasalkar (Minnesota), Rohan Puri, Ross Aiken, Rus- lan Kiselev, Ryland Herrick, Sam Kelly, Sam Noh (UNIST), Samer Al-K iswany, Sandeep Um- madi (Minnesota), Sankaralingam Panneerselvam, Satish Chebrolu (NetAp p), Satyanarayana c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES viii Shanmugam*, Scott Catlin, Scott Lee (UCLA), Seth Pollen, Sharad Punuganti , Shreevatsa R., Simon Pratt (Waterloo), Sivaraman Sivaraman*, Song Jiang (Wayne Stat e), Spencer Harston (Weber State), Srinivasan Thirunarayanan*, Stefan Dekanski, Stephen B ye, Suriyhaprakhas Balaram Sankari, Sy Jin Cheah, Teri Zhao (EMC), Thanumalayan S. Pillai, Thom as Griebel, Thomas Scrace, Tianxia Bai, Tong He, Tongxin Zheng, Tony Adkins, Torin Ru deen (Princeton), Tuo Wang, Tyler Couto, Varun Vats, Vikas Goel, Waciuma Wanjohi, W illiam Royle (Grinnell), Xiang Peng, Xu Di, Yifan Hao, Yuanyuan Chen, Yubin Ruan, Yudong Sun, Yue Z huo (Texas A&M), Yufui Ren, Zef RosnBrick, Zeyuan Hu (Texas), ZiHan Zheng (USTC), Zuyu Zhang.",3651
Introduction. Preface,"Special thanks to those marked with an asterisk above, who hav eg o n ea b o v ea n d beyond in their suggestions for improvement. In addition, a hearty thanks to Professor Joe Meehean (Lynch burg) for his de- tailed notes on each chapter, to Professor Jerod Weinman (Gri nnell) and his entire class for their incredible booklets, to Professor Chien-Ch ung Shen (Delaware) for his invaluable and detailed reading and comments, to Adam Dresch er (WUSTL) for his careful reading and suggestions, to Glen Granzow (Coll ege of Idaho) for his incredibly detailed comments and tips, Michael Walﬁsh (NYU) fo r his enthusiasm and detailed suggestions for improvement, Peter Peterson (UMD) for his many bits of useful feedback and commentary, Mark Kampe (Pomona) for det ailed crit- icism (we only wish we could ﬁx all suggestions.), and Youjip Won (Hanyang) for his translation work into Korean(.) and numerous insightful sugg estions. All have helped these authors immeasurably in the reﬁnement of the material sh e r e i n . Also, many thanks to the hundreds of students who have taken 537 ov er the years. In particular, the Fall ’08 class who encouraged the ﬁrs tw r i t t e nf o r mo f these notes (they were sick of not having any kind of textbook t o read — pushy students.), and then praised them enough for us to keep going (in cluding one hi- larious “ZOMG. You should totally write a textbook.” comment in our course evaluations that year). Ag r e a td e b to ft h a n k si sa l s oo w e dt ot h eb r a v ef e ww h ot o o kt h e xv6 project lab course, much of which is now incorporated into the main 537 cour se. From Spring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Ton yG r e g e r s o n ,M i c h a e l Griepentrog, Tyler Harter, Ryan Kroiss, Eric Radzikowski, Wesley Reardan, Rajiv Vaidyanathan, and Christopher Waclawik. From Fall ’09: Nic k Bearson, Aaron Brown, Alex Bird, David Capel, Keith Gould, Tom Grim, Jeffrey Hug o, Brandon Johnson, John Kjell, Boyan Li, James Loethen, Will McCardell ,R y a nS z a r o l e t t a ,S i - mon Tso, and Ben Yule. From Spring ’10: Patrick Blesi, Aidan Denn is-Oehling, Paras Doshi, Jake Friedman, Benjamin Frisch, Evan Hanson, Pik kili Hemanth, Michael Jeung, Alex Langenfeld, Scott Rick, Mike Treffert, Ga rret Staus, Brennan Wall, Hans Werner, Soo-Young Yang, and Carlos Grifﬁn (almost) . Although they do not directly help with the book, our graduate stud ents have taught us much of what we know about systems. We talk with them regular ly while they are at Wisconsin, but they do all the real work — and b y telling us about what they are doing, we learn new things every week. This list includes the follow- ing collection of current and former students and post-docs wit hw h o mw eh a v e published papers; an asterisk marks those who received a Ph.D. under our guid- ance: Abhishek Rajimwale, Aishwarya Ganesan, Andrew Krioukov, Ao Ma, Brian Forney, Chris Dragga, Deepak Ramamurthi, Dennis Zhou, Edward Oa kes, Flo- rentina Popovici*, Hariharan Gopalakrishnan, Haryadi S. G unawi*, James Nugent, Joe Meehean*, John Bent*, Jun He, Kevin Houck, Lanyue Lu*, Lakshmi Bairava- sundaram*, Laxman Visampalli, Leo Arulraj*, Leon Yang, Meenali Rung ta, Muthian OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG ix Sivathanu*, Nathan Burnett*, Nitin Agrawal*, Ram Alagappan, Sa mer Al-Kiswany, Scott Hendrickson, Sriram Subramanian*, Stephen Todd Jones* , Stephen Sturde- vant, Sudarsun Kannan, Suli Yang*, Swaminathan Sundararaman*, Sw etha Krish- nan, Thanh Do*, Thanumalayan S. Pillai*, Timothy Denehy*, Tyle rH a r t e r * ,V e n k a t Venkataramani, Vijay Chidambaram*, Vijayan Prabhakaran*, Yi ying Zhang*, Yupu Zhang*, Yuvraj Patel, Zev Weiss*.",3704
Introduction. Preface,"Our graduate students have largely been funded by the National Sc ience Foun- dation (NSF), the Department of Energy Ofﬁce of Science (DOE), and by industry grants. We are especially grateful to the NSF for their support over many years, as our research has shaped the content of many chapters herein. We thank Thomas Griebel, who demanded a better cover for the boo k. Al- though we didn’t take his speciﬁc suggestion (a dinosaur, can yo ub e l i e v ei t ? ) ,t h e beautiful picture of Halley’s comet would not be found on the cover w ithout him. A ﬁnal debt of gratitude is also owed to Aaron Brown, who ﬁrst too k this course many years ago (Spring ’09), then took the xv6 lab course (Fall ’0 9), and ﬁnally was a graduate teaching assistant for the course for two years or so (Fall ’10 through Spring ’12). His tireless work has vastly improved the state o ft h ep r o j e c t s( p a r - ticularly those in xv6 land) and thus has helped better the learn ing experience for countless undergraduates and graduates here at Wisconsin. As Aaro n would say (in his usual succinct manner): “Thx.” c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES x Final Words Yeats famously said “Education is not the ﬁlling of a pail but the l ighting of a ﬁre.” He was right but wrong at the same time4.Y o ud oh a v et o“ ﬁ l lt h ep a i l ”ab i t , and these notes are certainly here to help with that part of yo ur education; after all, when you go to interview at Google, and they ask you a trick ques tion about how to use semaphores, it might be good to actually know what a semaphor ei s ,r i g h t ? But Yeats’s larger point is obviously on the mark: the real point of education is to get you interested in something, to learn something more ab out the subject matter on your own and not just what you have to digest to get a good grade in some class. As one of our fathers (Remzi’s dad, Vedat Arpaci) used to say, “Learn beyond the classroom”. We created these notes to spark your interest in operating sys tems, to read more about the topic on your own, to talk to your professor about all the exciting re- search that is going on in the ﬁeld, and even to get involved wi th that research. It is a great ﬁeld(.), full of exciting and wonderful ideas that hav e shaped computing history in profound and important ways. And while we understand t his ﬁre won’t light for all of you, we hope it does for many, or even a few. Becaus eo n c et h a tﬁ r e is lit, well, that is when you truly become capable of doing somet hing great. And thus the real point of the educational process: to go forth, to s tudy many new and fascinating topics, to learn, to mature, and most importantly, t o ﬁnd something that lights a ﬁre for you. Andrea and Remzi Married couple Professors of Computer Science at the University of Wisconsin Chief Lighters of Fires, hopefully5 4If he actually said this; as with many famous quotes, the history of thi sg e mi sm u r k y . 5If this sounds like we are admitting some past history as arsonists ,y o ua r ep r o b a b l y missing the point. Probably. If this sounds cheesy, well, that’s becaus e it is, but you’ll just have to forgive us for that. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG xi References [CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai Zeldovich. From: http://pdos.csail.mit.edu/6.828/2008/index.html .xv6 was developed as a port of the original UNIXversion 6 and represents a beautiful, clean, and simple way to understand a modern operating system. [F96] “Six Easy Pieces: Essentials Of Physics Explained By Its Most Bril liant Teacher” by Richard P. Feynman. Basic Books, 1996. This book reprints the six easiest chapters of Feynman’s Lectures on Physics, from 1963. If you like Physics, it is a fantastic read . [HP90] “Computer Architecture a Quantitative Approach” (1st ed.) by Da vid A. Patterson and John L. Hennessy . Morgan-Kaufman, 1990. A book that encouraged each of us at our undergraduate institutions to pursue graduate studies; we later both had the pleasure of working with Patterson, who greatly shaped the foundations of our research careers. [KR88] “The C Programming Language” by Brian Kernighan and Dennis Ritchie. Prentice- Hall, April 1988. The C programming reference that everyone should have, by the people wh o invented the language. [K62] “The Structure of Scientiﬁc Revolutions” by Thomas S. Kuhn. Univers ity of Chicago Press, 1962. A great and famous read about the fundamentals of the scientiﬁc process. Mop-up w ork, anomaly, crisis, and revolution. We are mostly destined to do mop-up work, alas. c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4655
Table of Contents,"Contents To Everyone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii To Educators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v To Students . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . vii Final Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi 1A D i a l o g u e o n t h e B o o k 1 2I n t r o d u c t i o n t o O p e r a t i n g S y s t e m s 3 2.1 Virtualizing The CPU . . . . . . . . . . . . . . . . . . . . . 5 2.2 Virtualizing Memory . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Concurrency . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.4 Persistence . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.5 Design Goals . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.6 Some History . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Homework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 IV i r t u a l i z a t i o n 21 3A D i a l o g u e o n V i r t u a l i z a t i o n 2 3 4T h e A b s t r a c t i o n : T h e P r o c e s s 2 5 4.1 The Abstraction: A Process . . . . . . . . . . . . . . . . . . 26 4.2 Process API . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3 Process Creation: A Little More Detail . . . . . . . . . . . . 28 4.4 Process States . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.5 Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 xiii xiv CONTENTS Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 35 5I n t e r l u d e : P r o c e s s A P I 37 5.1 The fork() System Call . . . . . . . . . . . . . . . . . . . 37 5.2 The wait() System Call . . . . . . . . . . . . . . . . . . . 39 5.3 Finally, The exec() System Call . . . . . . . . . . . . . . . 40 5.4 Why? Motivating The API . . . . . . . . . . . . . . . . . . . 41 5.5 Process Control And Users . . . . . . . . . . . . . . . . . . 44 5.6 Useful Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6M e c h a n i s m : L i m i t e d D i r e c t E x e c u t i o n 4 9 6.1 Basic Technique: Limited Direct Execution . . . . . . . . . 49 6.2 Problem #1: Restricted Operations . . . . . . . . . . . . . . 50 6.3 Problem #2: Switching Between Processes . . . . . . . . . . 55 6.4 Worried About Concurrency? . . . . . . . . . . . . . . . . . 59 6.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 Homework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 63 7S c h e d u l i n g : I n t r o d u c t i o n 65 7.1 Workload Assumptions . . . . . . . . . . . . . . . . . . . . 65 7.2 Scheduling Metrics . . . . . . . . . . . . . . . . . . . . . . . 66 7.3 First In, First Out (FIFO) . . . . . . . . . . . . . . . . . . . . 66 7.4 Shortest Job First (SJF) . . . . . . . . . . . . . . . . . . . . . 68 7.5 Shortest Time-to-Completion First (STCF) . . . . . . . . . . 69 7.6 A New Metric: Response Time . . . . . . . . . . . . . . . . 70 7.7 Round Robin . . . . . . . . . . . . . . . . . . . . . . . . . . 71 7.8 Incorporating I/O . . . . . . . . . . . . . . . . . . . . . . . 73 7.9 No More Oracle . . . . . . . . . . . . . . . . . . . . . . . . . 74 7.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 76 8S c h e d u l i n g : The Multi-Level Feedback Queue 77 8.1 MLFQ: Basic Rules . . . . . . . . . . . . . . . . . . . . . . . 78 8.2 Attempt #1: How To Change Priority .",4260
Table of Contents,". . . . . . . . . . . 79 8.3 Attempt #2: The Priority Boost . . . . . . . . . . . . . . . . 83 8.4 Attempt #3: Better Accounting . . . . . . . . . . . . . . . . 84 8.5 Tuning MLFQ And Other Issues . . . . . . . . . . . . . . . 84 8.6 MLFQ: Summary . . . . . . . . . . . . . . . . . . . . . . . . 86 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 88 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONTENTS xv 9S c h e d u l i n g : P r o p o r t i o n a l S h a r e 8 9 9.1 Basic Concept: Tickets Represent Your Share . . . . . . . . 89 9.2 Ticket Mechanisms . . . . . . . . . . . . . . . . . . . . . . . 91 9.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . 92 9.4 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 9.5 How To Assign Tickets? . . . . . . . . . . . . . . . . . . . . 94 9.6 Why Not Deterministic? . . . . . . . . . . . . . . . . . . . . 94 9.7 The Linux Completely Fair Scheduler (CFS) . . . . . . . . . 95 9.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 102 10 Multiprocessor Scheduling (Advanced) 103 10.1 Background: Multiprocessor Architecture . . . . . . . . . . 104 10.2 Don’t Forget Synchronization . . . . . . . . . . . . . . . . . 106 10.3 One Final Issue: Cache Afﬁnity . . . . . . . . . . . . . . . . 107 10.4 Single-Queue Scheduling . . . . . . . . . . . . . . . . . . . 107 10.5 Multi-Queue Scheduling . . . . . . . . . . . . . . . . . . . . 109 10.6 Linux Multiprocessor Schedulers . . . . . . . . . . . . . . . 112 10.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 114 11 Summary Dialogue on CPU Virtualization 117 12 A Dialogue on Memory Virtualization 119 13 The Abstraction: Address Spaces 121 13.1 Early Systems . . . . . . . . . . . . . . . . . . . . . . . . . . 121 13.2 Multiprogramming and Time Sharing . . . . . . . . . . . . 122 13.3 The Address Space . . . . . . . . . . . . . . . . . . . . . . . 123 13.4 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 13.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 14 Interlude: Memory API 131 14.1 Types of Memory . . . . . . . . . . . . . . . . . . . . . . . . 131 14.2 The malloc() Call . . . . . . . . . . . . . . . . . . . . . . 132 14.3 The free() Call . . . . . . . . . . . . . . . . . . . . . . . . 134 14.4 Common Errors . . . . . . . . . . . . . . . . . . . . . . . . 134 14.5 Underlying OS Support . . . . . . . . . . . . . . . . . . . . 137 14.6 Other Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES xvi CONTENTS 15 Mechanism: Address Translation 141 15.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 142 15.2 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 15.3 Dynamic (Hardware-based) Relocation . . . . . . . . . . . 145 15.4 Hardware Support: A Summary . . . . . . . . . . . . . . . 148 15.5 Operating System Issues . . . . . . . . . . . . . . . . . . . . 149 15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 154 16 Segmentation 155 16.1 Segmentation: Generalized Base/Bounds . . . . . . . . . . 155 16.2 Which Segment Are We Referring To? . . . . . . . . . . . . 158 16.3 What About The Stack?",4155
Table of Contents,. . . . . . . . . . . . . . . . . . . . 159 16.4 Support for Sharing . . . . . . . . . . . . . . . . . . . . . . 160 16.5 Fine-grained vs. Coarse-grained Segmentation . . . . . . . 16 1 16.6 OS Support . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 16.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 165 17 Free-Space Management 167 17.1 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . 168 17.2 Low-level Mechanisms . . . . . . . . . . . . . . . . . . . . 169 17.3 Basic Strategies . . . . . . . . . . . . . . . . . . . . . . . . . 177 17.4 Other Approaches . . . . . . . . . . . . . . . . . . . . . . . 179 17.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 183 18 Paging: Introduction 185 18.1 A Simple Example And Overview . . . . . . . . . . . . . . 185 18.2 Where Are Page Tables Stored? . . . . . . . . . . . . . . . . 189 18.3 What’s Actually In The Page Table? . . . . . . . . . . . . . 190 18.4 Paging: Also Too Slow . . . . . . . . . . . . . . . . . . . . . 191 18.5 A Memory Trace . . . . . . . . . . . . . . . . . . . . . . . . 192 18.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 197 19 Paging: Faster Translations (TLBs) 199 19.1 TLB Basic Algorithm . . . . . . . . . . . . . . . . . . . . . . 199 19.2 Example: Accessing An Array . . . . . . . . . . . . . . . . 201 19.3 Who Handles The TLB Miss? . . . . . . . . . . . . . . . . . 203 19.4 TLB Contents: What’s In There? . . . . . . . . . . . . . . . 205 19.5 TLB Issue: Context Switches . . . . . . . . . . . . . . . . . 206 19.6 Issue: Replacement Policy . . . . . . . . . . . . . . . . . . . 208 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONTENTS xvii 19.7 A Real TLB Entry . . . . . . . . . . . . . . . . . . . . . . . . 209 19.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 Homework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 212 20 Paging: Smaller Tables 215 20.1 Simple Solution: Bigger Pages . . . . . . . . . . . . . . . . 215 20.2 Hybrid Approach: Paging and Segments . . . . . . . . . . 216 20.3 Multi-level Page Tables . . . . . . . . . . . . . . . . . . . . 219 20.4 Inverted Page Tables . . . . . . . . . . . . . . . . . . . . . . 226 20.5 Swapping the Page Tables to Disk . . . . . . . . . . . . . . 227 20.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 229 21 Beyond Physical Memory: Mechanisms 231 21.1 Swap Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 21.2 The Present Bit . . . . . . . . . . . . . . . . . . . . . . . . . 233 21.3 The Page Fault . . . . . . . . . . . . . . . . . . . . . . . . . 234 21.4 What If Memory Is Full? . . . . . . . . . . . . . . . . . . . . 235 21.5 Page Fault Control Flow . . . . . . . . . . . . . . . . . . . . 236 21.6 When Replacements Really Occur . . . . . . . . . . . . . . 237 21.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239 Homework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 240 22 Beyond Physical Memory: Policies 243 22.1 Cache Management . . . . . . . . . . . . . . . . . . . . . . 243 22.2 The Optimal Replacement Policy . . . . . . . . . . . . . . . 244 22.3 A Simple Policy: FIFO . . . . . . . . . . . . . . . . . . . . . 246 22.4 Another Simple Policy: Random . . . . . . . . . . . . . . . 248 22.5 Using History: LRU . . . . . . . . . . . . . . . . . . . . . . 249 22.6 Workload Examples . . . . . . . . . . . . . . . . . . . . . .,4218
Table of Contents,"250 22.7 Implementing Historical Algorithms . . . . . . . . . . . . . 253 22.8 Approximating LRU . . . . . . . . . . . . . . . . . . . . . . 254 22.9 Considering Dirty Pages . . . . . . . . . . . . . . . . . . . . 255 22.10 Other VM Policies . . . . . . . . . . . . . . . . . . . . . . . 256 22.11 Thrashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 22.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 259 23 Complete Virtual Memory Systems 261 23.1 VAX/VMS Virtual Memory . . . . . . . . . . . . . . . . . . 262 23.2 The Linux Virtual Memory System . . . . . . . . . . . . . . 268 23.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES xviii CONTENTS 24 Summary Dialogue on Memory Virtualization 279 II Concurrency 283 25 A Dialogue on Concurrency 285 26 Concurrency: An Introduction 287 26.1 Why Use Threads? . . . . . . . . . . . . . . . . . . . . . . . 288 26.2 An Example: Thread Creation . . . . . . . . . . . . . . . . 289 26.3 Why It Gets Worse: Shared Data . . . . . . . . . . . . . . . 292 26.4 The Heart Of The Problem: Uncontrolled Scheduling . . . 294 26.5 The Wish For Atomicity . . . . . . . . . . . . . . . . . . . . 296 26.6 One More Problem: Waiting For Another . . . . . . . . . . 298 26.7 Summary: Why in OS Class? . . . . . . . . . . . . . . . . . 298 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 301 27 Interlude: Thread API 303 27.1 Thread Creation . . . . . . . . . . . . . . . . . . . . . . . . 303 27.2 Thread Completion . . . . . . . . . . . . . . . . . . . . . . . 304 27.3 Locks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307 27.4 Condition Variables . . . . . . . . . . . . . . . . . . . . . . 309 27.5 Compiling and Running . . . . . . . . . . . . . . . . . . . . 311 27.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 314 28 Locks 315 28.1 Locks: The Basic Idea . . . . . . . . . . . . . . . . . . . . . 315 28.2 Pthread Locks . . . . . . . . . . . . . . . . . . . . . . . . . . 316 28.3 Building A Lock . . . . . . . . . . . . . . . . . . . . . . . . 317 28.4 Evaluating Locks . . . . . . . . . . . . . . . . . . . . . . . . 317 28.5 Controlling Interrupts . . . . . . . . . . . . . . . . . . . . . 318 28.6 A Failed Attempt: Just Using Loads/Stores . . . . . . . . . 319 28.7 Building Working Spin Locks with Test-And-Set . . . . . . 320 28.8 Evaluating Spin Locks . . . . . . . . . . . . . . . . . . . . . 322 28.9 Compare-And-Swap . . . . . . . . . . . . . . . . . . . . . . 323 28.10 Load-Linked and Store-Conditional . . . . . . . . . . . . . 324 28.11 Fetch-And-Add . . . . . . . . . . . . . . . . . . . . . . . . . 326 28.12 Too Much Spinning: What Now? . . . . . . . . . . . . . . . 327 28.13 A Simple Approach: Just Yield, Baby . . . . . . . . . . . . . 328 28.14 Using Queues: Sleeping Instead Of Spinning . . . . . . . . 329 28.15 Different OS, Different Support . . . . . . . . . . . . . . . . 332 28.16 Two-Phase Locks . . . . . . . . . . . . . . . . . . . . . . . . 332 28.17 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONTENTS xix Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 336 29 Lock-based Concurrent Data Structures 337 29.1 Concurrent Counters . . . . . . . . . . . . . . . . . . . . . . 337 29.2 Concurrent Linked Lists . . . . . . . . . . . . . . . . . . . . 342 29.3 Concurrent Queues . . . . . . . . . . . . . . . . . . . . . . . 345 29.4 Concurrent Hash Table .",4105
Table of Contents,". . . . . . . . . . . . . . . . . . . 346 29.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 350 30 Condition Variables 351 30.1 Deﬁnition and Routines . . . . . . . . . . . . . . . . . . . . 352 30.2 The Producer/Consumer (Bounded Buffer) Problem . . . . 355 30.3 Covering Conditions . . . . . . . . . . . . . . . . . . . . . . 363 30.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 366 31 Semaphores 367 31.1 Semaphores: A Deﬁnition . . . . . . . . . . . . . . . . . . . 367 31.2 Binary Semaphores (Locks) . . . . . . . . . . . . . . . . . . 369 31.3 Semaphores For Ordering . . . . . . . . . . . . . . . . . . . 370 31.4 The Producer/Consumer (Bounded Buffer) Problem . . . . 372 31.5 Reader-Writer Locks . . . . . . . . . . . . . . . . . . . . . . 376 31.6 The Dining Philosophers . . . . . . . . . . . . . . . . . . . 378 31.7 How To Implement Semaphores . . . . . . . . . . . . . . . 381 31.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 384 32 Common Concurrency Problems 385 32.1 What Types Of Bugs Exist? . . . . . . . . . . . . . . . . . . 385 32.2 Non-Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . 386 32.3 Deadlock Bugs . . . . . . . . . . . . . . . . . . . . . . . . . 389 32.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 400 33 Event-based Concurrency (Advanced) 401 33.1 The Basic Idea: An Event Loop . . . . . . . . . . . . . . . . 401 33.2 An Important API: select() (orpoll() )......... 4 0 2 33.3 Using select() ........................ 4 0 3 33.4 Why Simpler? No Locks Needed . . . . . . . . . . . . . . . 404 33.5 A Problem: Blocking System Calls . . . . . . . . . . . . . . 405 33.6 A Solution: Asynchronous I/O . . . . . . . . . . . . . . . . 405 33.7 Another Problem: State Management . . . . . . . . . . . . 408 c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES xx CONTENTS 33.8 What Is Still Difﬁcult With Events . . . . . . . . . . . . . . 409 33.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 411 34 Summary Dialogue on Concurrency 413 III Persistence 415 35 A Dialogue on Persistence 417 36 I/O Devices 419 36.1 System Architecture . . . . . . . . . . . . . . . . . . . . . . 419 36.2 A Canonical Device . . . . . . . . . . . . . . . . . . . . . . 421 36.3 The Canonical Protocol . . . . . . . . . . . . . . . . . . . . 422 36.4 Lowering CPU Overhead With Interrupts . . . . . . . . . . 423 36.5 More Efﬁcient Data Movement With DMA . . . . . . . . . 424 36.6 Methods Of Device Interaction . . . . . . . . . . . . . . . . 425 36.7 Fitting Into The OS: The Device Driver . . . . . . . . . . . . 426 36.8 Case Study: A Simple IDE Disk Driver . . . . . . . . . . . . 427 36.9 Historical Notes . . . . . . . . . . . . . . . . . . . . . . . . 430 36.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431 37 Hard Disk Drives 433 37.1 The Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 433 37.2 Basic Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 434 37.3 A Simple Disk Drive . . . . . . . . . . . . . . . . . . . . . . 435 37.4 I/O Time: Doing The Math . . . . . . . . . . . . . . . . . . 438 37.5 Disk Scheduling . . . . . . . . . . . . . . . . . . . . . . . . 442 37.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . .",4135
Table of Contents,"446 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 448 38 Redundant Arrays of Inexpensive Disks (RAIDs) 449 38.1 Interface And RAID Internals . . . . . . . . . . . . . . . . . 450 38.2 Fault Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 38.3 How To Evaluate A RAID . . . . . . . . . . . . . . . . . . . 451 38.4 RAID Level 0: Striping . . . . . . . . . . . . . . . . . . . . . 452 38.5 RAID Level 1: Mirroring . . . . . . . . . . . . . . . . . . . . 455 38.6 RAID Level 4: Saving Space With Parity . . . . . . . . . . . 458 38.7 RAID Level 5: Rotating Parity . . . . . . . . . . . . . . . . 462 38.8 RAID Comparison: A Summary . . . . . . . . . . . . . . . 463 38.9 Other Interesting RAID Issues . . . . . . . . . . . . . . . . 464 38.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONTENTS xxi Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 466 39 Interlude: Files and Directories 467 39.1 Files And Directories . . . . . . . . . . . . . . . . . . . . . . 467 39.2 The File System Interface . . . . . . . . . . . . . . . . . . . 469 39.3 Creating Files . . . . . . . . . . . . . . . . . . . . . . . . . . 469 39.4 Reading And Writing Files . . . . . . . . . . . . . . . . . . 470 39.5 Reading And Writing, But Not Sequentially . . . . . . . . . 472 39.6 Shared File Table Entries: fork() And dup() ....... 4 7 5 39.7 Writing Immediately With fsync() ............. 4 7 7 39.8 Renaming Files . . . . . . . . . . . . . . . . . . . . . . . . . 478 39.9 Getting Information About Files . . . . . . . . . . . . . . . 479 39.10 Removing Files . . . . . . . . . . . . . . . . . . . . . . . . . 480 39.11 Making Directories . . . . . . . . . . . . . . . . . . . . . . . 480 39.12 Reading Directories . . . . . . . . . . . . . . . . . . . . . . 481 39.13 Deleting Directories . . . . . . . . . . . . . . . . . . . . . . 482 39.14 Hard Links . . . . . . . . . . . . . . . . . . . . . . . . . . . 482 39.15 Symbolic Links . . . . . . . . . . . . . . . . . . . . . . . . . 484 39.16 Permission Bits And Access Control Lists . . . . . . . . . . 485 39.17 Making And Mounting A File System . . . . . . . . . . . . 488 39.18 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 40 File System Implementation 493 40.1 The Way To Think . . . . . . . . . . . . . . . . . . . . . . . 493 40.2 Overall Organization . . . . . . . . . . . . . . . . . . . . . . 494 40.3 File Organization: The Inode . . . . . . . . . . . . . . . . . 496 40.4 Directory Organization . . . . . . . . . . . . . . . . . . . . 501 40.5 Free Space Management . . . . . . . . . . . . . . . . . . . . 501 40.6 Access Paths: Reading and Writing . . . . . . . . . . . . . . 502 40.7 Caching and Buffering . . . . . . . . . . . . . . . . . . . . . 506 40.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 510 41 Locality and The Fast File System 511 41.1 The Problem: Poor Performance . . . . . . . . . . . . . . . 511 41.2 FFS: Disk Awareness Is The Solution . . . . . . . . . . . . . 513 41.3 Organizing Structure: The Cylinder Group . . . . . . . . . 513 41.4 Policies: How To Allocate Files and Directories . . . . . . . 515 41.5 Measuring File Locality . . . . . . . . . . . . . . . . . . . . 517 41.6 The Large-File Exception . . . . . . . . . . . . . . . . . . . 518 41.7 A Few Other Things About FFS . . . . . . . . . . . . . . . . 520 41.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523 Homework (Simulation) .",4085
Table of Contents,". . . . . . . . . . . . . . . . . . . . . . 524 c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES xxii CONTENTS 42 Crash Consistency: FSCK and Journaling 525 42.1 A Detailed Example . . . . . . . . . . . . . . . . . . . . . . 526 42.2 Solution #1: The File System Checker . . . . . . . . . . . . 529 42.3 Solution #2: Journaling (or Write-Ahead Logging) . . . . . 531 42.4 Solution #3: Other Approaches . . . . . . . . . . . . . . . . 541 42.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 545 43 Log-structured File Systems 547 43.1 Writing To Disk Sequentially . . . . . . . . . . . . . . . . . 548 43.2 Writing Sequentially And Effectively . . . . . . . . . . . . . 54 9 43.3 How Much To Buffer? . . . . . . . . . . . . . . . . . . . . . 550 43.4 Problem: Finding Inodes . . . . . . . . . . . . . . . . . . . 551 43.5 Solution Through Indirection: The Inode Map . . . . . . . 551 43.6 Completing The Solution: The Checkpoint Region . . . . . 553 43.7 Reading A File From Disk: A Recap . . . . . . . . . . . . . 553 43.8 What About Directories? . . . . . . . . . . . . . . . . . . . 554 43.9 A New Problem: Garbage Collection . . . . . . . . . . . . . 555 43.10 Determining Block Liveness . . . . . . . . . . . . . . . . . . 556 43.11 A Policy Question: Which Blocks To Clean, And When? . . 557 43.12 Crash Recovery And The Log . . . . . . . . . . . . . . . . . 558 43.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 561 44 Flash-based SSDs 563 44.1 Storing a Single Bit . . . . . . . . . . . . . . . . . . . . . . . 563 44.2 From Bits to Banks/Planes . . . . . . . . . . . . . . . . . . 564 44.3 Basic Flash Operations . . . . . . . . . . . . . . . . . . . . . 565 44.4 Flash Performance And Reliability . . . . . . . . . . . . . . 567 44.5 From Raw Flash to Flash-Based SSDs . . . . . . . . . . . . 568 44.6 FTL Organization: A Bad Approach . . . . . . . . . . . . . 569 44.7 A Log-Structured FTL . . . . . . . . . . . . . . . . . . . . . 570 44.8 Garbage Collection . . . . . . . . . . . . . . . . . . . . . . . 572 44.9 Mapping Table Size . . . . . . . . . . . . . . . . . . . . . . 574 44.10 Wear Leveling . . . . . . . . . . . . . . . . . . . . . . . . . 579 44.11 SSD Performance And Cost . . . . . . . . . . . . . . . . . . 579 44.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 585 45 Data Integrity and Protection 587 45.1 Disk Failure Modes . . . . . . . . . . . . . . . . . . . . . . . 587 45.2 Handling Latent Sector Errors . . . . . . . . . . . . . . . . 589 45.3 Detecting Corruption: The Checksum . . . . . . . . . . . . 590 45.4 Using Checksums . . . . . . . . . . . . . . . . . . . . . . . 593 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONTENTS xxiii 45.5 A New Problem: Misdirected Writes . . . . . . . . . . . . . 594 45.6 One Last Problem: Lost Writes . . . . . . . . . . . . . . . . 595 45.7 Scrubbing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595 45.8 Overheads Of Checksumming . . . . . . . . . . . . . . . . 596 45.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 598 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 46 Summary Dialogue on Persistence 601 47 A Dialogue on Distribution 603 48 Distributed Systems 605 48.1 Communication Basics . . . . . . . . . . . . . . . . . . . . . 606 48.2 Unreliable Communication Layers . . . . . . . . . . . . . .",3990
Table of Contents,"607 48.3 Reliable Communication Layers . . . . . . . . . . . . . . . 609 48.4 Communication Abstractions . . . . . . . . . . . . . . . . . 611 48.5 Remote Procedure Call (RPC) . . . . . . . . . . . . . . . . . 613 48.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619 Homework (Code) . . . . . . . . . . . . . . . . . . . . . . . . . . . 620 49 Sun’s Network File System (NFS) 621 49.1 A Basic Distributed File System . . . . . . . . . . . . . . . . 622 49.2 On To NFS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623 49.3 Focus: Simple And Fast Server Crash Recovery . . . . . . . 623 49.4 Key To Fast Crash Recovery: Statelessness . . . . . . . . . 624 49.5 The NFSv2 Protocol . . . . . . . . . . . . . . . . . . . . . . 625 49.6 From Protocol To Distributed File System . . . . . . . . . . 627 49.7 Handling Server Failure With Idempotent Operations . . . 62 9 49.8 Improving Performance: Client-side Caching . . . . . . . . 63 1 49.9 The Cache Consistency Problem . . . . . . . . . . . . . . . 631 49.10 Assessing NFS Cache Consistency . . . . . . . . . . . . . . 633 49.11 Implications On Server-Side Write Buffering . . . . . . . . 63 3 49.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637 Homework (Measurement) . . . . . . . . . . . . . . . . . . . . . . 638 50 The Andrew File System (AFS) 639 50.1 AFS Version 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 639 50.2 Problems with Version 1 . . . . . . . . . . . . . . . . . . . . 641 50.3 Improving the Protocol . . . . . . . . . . . . . . . . . . . . 642 50.4 AFS Version 2 . . . . . . . . . . . . . . . . . . . . . . . . . . 642 50.5 Cache Consistency . . . . . . . . . . . . . . . . . . . . . . . 644 50.6 Crash Recovery . . . . . . . . . . . . . . . . . . . . . . . . . 646 50.7 Scale And Performance Of AFSv2 . . . . . . . . . . . . . . 646 c⃝2008–18, A RPACI -DUSSEAUTHREE EASY PIECES xxiv CONTENTS 50.8 AFS: Other Improvements . . . . . . . . . . . . . . . . . . . 649 50.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651 Homework (Simulation) . . . . . . . . . . . . . . . . . . . . . . . 652 51 Summary Dialogue on Distribution 653 General Index 655 Asides 667 Tips 671 Cruces 675 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2507
1. Dialogue,"1 A Dialogue on the Book Professor: Welcome to this book. It’s called Operating Systems in Three Easy Pieces , and I am here to teach you the things you need to know about oper ating systems. I am called “Professor”; who are you? Student: Hi Professor. I am called “Student”, as you might have guessed. An d I am here and ready to learn. Professor: Sounds good. Any questions? Student: Sure. Why is it called “Three Easy Pieces”? Professor: That’s an easy one. Well, you see, there are these great lectures on Physics by Richard Feynman... Student: Oh. The guy who wrote “Surely You’re Joking, Mr. Feynman”, right? Great book. Is this going to be hilarious like that book was? Professor: Um... well, no. That book was great, and I’m glad you’ve read it. Hopefully this book is more like his notes on Physics. Some of the basics w ere summed up in a book called “Six Easy Pieces”. He was talking about Phys ics; we’re going to do Three Easy Pieces on the ﬁne topic of Operating Syst ems. This is appropriate, as Operating Systems are about half as hard as Phy sics. Student: Well, I liked physics, so that is probably good. What are those pieces? Professor: They are the three key ideas we’re going to learn about: virtualiza- tion,concurrency , and persistence . In learning about these ideas, we’ll learn all about how an operating system works, including how it decides wha t program to run next on a CPU, how it handles memory overload in a virtual memo ry sys- tem, how virtual machine monitors work, how to manage information o n disks, and even a little about how to build a distributed system that works wh en parts have failed. That sort of stuff. Student: I have no idea what you’re talking about, really. Professor: Good. That means you are in the right class. Student: I have another question: what’s the best way to learn this stuff? Professor: Excellent query. Well, each person needs to ﬁgure this out on their 1 2 A D IALOGUE ON THE BOOK own, of course, but here is what I would do: go to class, to hear the professor introduce the material. Then, at the end of every week, read thes e notes, to help the ideas sink into your head a bit better. Of course, some time later (hint: before the exam.), read the notes again to ﬁrm up your knowledge. Of cour se, your pro- fessor will no doubt assign some homeworks and projects, so you sho uld do those; in particular, doing projects where you write real code to solve real problems is the best way to put the ideas within these notes into action. As Confu cius said... Student: Oh, I know. ’I hear and I forget. I see and I remember. I do and I understand.’ Or something like that. Professor: (surprised) How did you know what I was going to say?. Student: It seemed to follow. Also, I am a big fan of Confucius, and an even bigger fan of Xunzi, who actually is a better source for this quote1. Professor: (stunned) Well, I think we are going to get along just ﬁne. Just ﬁne indeed. Student: Professor – just one more question, if I may. What are these dialogue s for? I mean, isn’t this just supposed to be a book? Why not present t he material directly? Professor: Ah, good question, good question. Well, I think it is sometimes useful to pull yourself outside of a narrative and think a bit; these d ialogues are those times. So you and I are going to work together to make sense of all of these pretty complex ideas. Are you up for it? Student: So we have to think? Well, I’m up for that. I mean, what else do I have to do anyhow? It’s not like I have much of a life outside of this book. Professor: Me neither, sadly. So let’s get to work. 1According to this website (http://www.barrypopik.com/index.php/ne wyork city/ entry/tell meand iforget teach meand imay remember involve meand iwill lear/), Confucian philosopher Xunzi said “Not having heard something is not as goo d as having heard it; having heard it is not as good as having seen it; having seen it i s not as good as knowing it; knowing it is not as good as putting it into practice.” Later on, the wisdom got attached to Confucius for some reason. Thanks to Jiao Dong (Rutgers) for tell ing us. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4172
2. Introduction,"2 Introduction to Operating Systems If you are taking an undergraduate operating systems course, you should already have some idea of what a computer program does when it runs. If not, this book (and the corresponding course) is going to be difﬁcu lt — so you should probably stop reading this book, or run to the near- est bookstore and quickly consume the necessary background mater ial before continuing (both Patt & Patel [PP03] and Bryant & O’Hallar on [BOH10] are pretty great books). So what happens when a program runs? Well, a running program does one very simple thing: it executes i n- structions. Many millions (and these days, even billions) of tim es ev- ery second, the processor fetches an instruction from memory, decodes it (i.e., ﬁgures out which instruction this is), and executes it (i.e., it does the thing that it is supposed to do, like add two numbers together , access memory, check a condition, jump to a function, and so forth). After it is done with this instruction, the processor moves on to the next instr uction, and so on, and so on, until the program ﬁnally completes1. Thus, we have just described the basics of the Von Neumann model of computing2. Sounds simple, right? But in this class, we will be learning that while a program runs, a lot of other wild things are going on with the primary goal of making the system easy to use . There is a body of software, in fact, that is responsible for making it easy to run programs (even allowing you to seemingly run many at t he same time), allowing programs to share memory, enabling program s to interact with devices, and other fun stuff like that. That body of software 1Of course, modern processors do many bizarre and frightening things under neath the hood to make programs run faster, e.g., executing multiple instru ctions at once, and even issu- ing and completing them out of order. But that is not our concern here; we ar e just concerned with the simple model most programs assume: that instructions see mingly execute one at a time, in an orderly and sequential fashion. 2Von Neumann was one of the early pioneers of computing systems. He al so did pioneer- ing work on game theory and atomic bombs, and played in the NBA for six years. OK, one of those things isn’t true. 1 2 INTRODUCTION TO OPERATING SYSTEMS THECRUX OF THE PROBLEM : HOWTOVIRTUALIZE RESOURCES One central question we will answer in this book is quite simple: how does the operating system virtualize resources? This is the cru x of our problem. Why the OS does this is not the main question, as the answer should be obvious: it makes the system easier to use. Thus, we focu s on thehow: what mechanisms and policies are implemented by the OS to attain virtualization? How does the OS do so efﬁciently? What ha rdware support is needed? We will use the “crux of the problem”, in shaded boxes such as this one, as a way to call out speciﬁc problems we are trying to solve in buil ding an operating system. Thus, within a note on a particular topic, you may ﬁnd one or more cruces (yes, this is the proper plural) which highlight the problem.",3105
2. Introduction,"The details within the chapter, of course, present the solution, or at least the basic parameters of a solution. is called the operating system (OS)3, as it is in charge of making sure the system operates correctly and efﬁciently in an easy-to-use man ner. The primary way the OS does this is through a general technique t hat we call virtualization . That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms it into a more gen- eral, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a virtual machine . Of course, in order to allow users to tell the OS what to do and thus make use of the features of the virtual machine (such as running a pro- gram, or allocating memory, or accessing a ﬁle), the OS also provid es some interfaces (APIs) that you can call. A typical OS, in fact, e xports a few hundred system calls that are available to applications. Because the OS provides these calls to run programs, access memory and de vices, and other related actions, we also sometimes say that the OS provi des a standard library to applications. Finally, because virtualization allows many programs to run (t hus shar- ing the CPU), and many programs to concurrently access their own in- structions and data (thus sharing memory), and many programs to access devices (thus sharing disks and so forth), the OS is sometimes k nown as aresource manager . Each of the CPU, memory, and disk is a resource of the system; it is thus the operating system’s role to manage those re- sources, doing so efﬁciently or fairly or indeed with many other pos sible goals in mind. To understand the role of the OS a little bit better , let’s take a look at some examples. 3Another early name for the OS was the supervisor or even the master control program . Apparently, the latter sounded a little overzealous (see the movi e Tron for details) and thus, thankfully, “operating system” caught on instead. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 3 1#include <stdio.h> 2#include <stdlib.h> 3#include <sys/time.h> 4#include <assert.h> 5#include \""common.h\"" 6 7int 8main(int argc, char *argv[]) 9{ 10 if (argc .= 2) { 11 fprintf(stderr, \""usage: cpu <string> \""); 12 exit(1); 13 } 14 char*str = argv[1]; 15 while (1) { 16 Spin(1); 17 printf(\"" percents \"", str); 18 } 19 return 0; 20} Figure 2.1: Simple Example: Code That Loops And Prints ( cpu.c ) 2.1 Virtualizing The CPU Figure 2.1 depicts our ﬁrst program. It doesn’t do much. In fact, a ll it does is call Spin() , a function that repeatedly checks the time and returns once it has run for a second. Then, it prints out the string that the user passed in on the command line, and repeats, forever. Let’s say we save this ﬁle as cpu.c and decide to compile and run it on a system with a single processor (or CPU as we will sometimes call it). Here is what we will see: prompt> gcc -o cpu cpu.c -Wall prompt> ./cpu \""A\"" A A A A ˆC prompt> Not too interesting of a run — the system begins running the progra m, which repeatedly checks the time until a second has elapsed. O nce a sec- ond has passed, the code prints the input string passed in by the user (in this example, the letter “A”), and continues.",3277
2. Introduction,"Note the progr am will run forever; only by pressing “Control-c” (which on U NIX-based systems will terminate the program running in the foreground) can we hal t the program. Now, let’s do the same thing, but this time, let’s run many differ ent in- stances of this same program. Figure 2.2 shows the results of this slightly more complicated example. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 INTRODUCTION TO OPERATING SYSTEMS prompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D & [1] 7353 [2] 7354 [3] 7355 [4] 7356 A B D C A B D C A C B D ... Figure 2.2: Running Many Programs At Once Well, now things are getting a little more interesting. Even th ough we have only one processor, somehow all four of these programs seem to be running at the same time. How does this magic happen?4 It turns out that the operating system, with some help from the har d- ware, is in charge of this illusion , i.e., the illusion that the system has a very large number of virtual CPUs. Turning a single CPU (or smal l set of them) into a seemingly inﬁnite number of CPUs and thus allowing many programs to seemingly run at once is what we call virtualizing the CPU , the focus of the ﬁrst major part of this book. Of course, to run programs, and stop them, and otherwise tell the O S which programs to run, there need to be some interfaces (APIs) t hat you can use to communicate your desires to the OS. We’ll talk about thes e APIs throughout this book; indeed, they are the major way in which m ost users interact with operating systems. You might also notice that the ability to run multiple programs a t once raises all sorts of new questions. For example, if two programs wan t to run at a particular time, which should run? This question is answered by apolicy of the OS; policies are used in many different places within an OS to answer these types of questions, and thus we will study the m as we learn about the basic mechanisms that operating systems implement (such as the ability to run multiple programs at once). Hence th e role of the OS as a resource manager . 4Note how we ran four processes at the same time, by using the &symbol. Doing so runs a job in the background in the tcsh shell, which means that the user is able to immediately issue their next command, which in this case is another program to run. The semi-colo n between commands allows us to run multiple programs at the same time in tcsh . If you’re using a different shell (e.g., bash ), it works slightly differently; read documentation online for detai ls. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 5 1#include <unistd.h> 2#include <stdio.h> 3#include <stdlib.h> 4#include \""common.h\"" 5 6int 7main(int argc, char *argv[]) 8{ 9int*p = malloc(sizeof(int)); // a1 10 assert(p .= NULL); 11 printf(\""( percentd) address pointed to by p:  percentp \"", 12 getpid(), p); // a2 13 *p = 0; // a3 14 while (1) { 15 Spin(1); 16 *p =*p + 1; 17 printf(\""( percentd) p:  percentd \"", getpid(), *p); // a4 18 } 19 return 0; 20} Figure 2.3: A Program That Accesses Memory ( mem.c ) 2.2 Virtualizing Memory Now let’s consider memory. The model of physical memory pre- sented by modern machines is very simple. Memory is just an arra y of bytes; to read memory, one must specify an address to be able to access the data stored there; to write (orupdate ) memory, one must also specify the data to be written to the given address. Memory is accessed all the time when a program is running. A pro- gram keeps all of its data structures in memory, and accesses th em through various instructions, like loads and stores or other explicit inst ructions that access memory in doing their work.",3697
2. Introduction,"Don’t forget that each instr uc- tion of the program is in memory too; thus memory is accessed on each instruction fetch. Let’s take a look at a program (in Figure 2.3) that allocates some mem - ory by calling malloc() . The output of this program can be found here: prompt> ./mem (2134) address pointed to by p: 0x200000 (2134) p: 1 (2134) p: 2 (2134) p: 3 (2134) p: 4 (2134) p: 5 ˆC The program does a couple of things. First, it allocates some memory (line a1). Then, it prints out the address of the memory (a2), and then puts the number zero into the ﬁrst slot of the newly allocated mem ory (a3). Finally, it loops, delaying for a second and incrementing t he value stored at the address held in p.With every print statement, it also prints out what is called the process identiﬁer (the PID) of the running program. This PID is unique per running process. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 INTRODUCTION TO OPERATING SYSTEMS prompt> ./mem &; ./mem & [1] 24113 [2] 24114 (24113) address pointed to by p: 0x200000 (24114) address pointed to by p: 0x200000 (24113) p: 1 (24114) p: 1 (24114) p: 2 (24113) p: 2 (24113) p: 3 (24114) p: 3 (24113) p: 4 (24114) p: 4 ... Figure 2.4: Running The Memory Program Multiple Times Again, this ﬁrst result is not too interesting. The newly alloca ted mem- ory is at address 0x200000 . As the program runs, it slowly updates the value and prints out the result. Now, we again run multiple instances of this same program to see what happens (Figure 2.4). We see from the example that each ru nning program has allocated memory at the same address ( 0x200000 ), and yet each seems to be updating the value at 0x200000 independently. It is as if each running program has its own private memory, instead of sha ring the same physical memory with other running programs5. Indeed, that is exactly what is happening here as the OS is virtualiz- ing memory . Each process accesses its own private virtual address space (sometimes just called its address space ), which the OS somehow maps onto the physical memory of the machine. A memory reference withi n one running program does not affect the address space of other proces ses (or the OS itself); as far as the running program is concerned, it has phys- ical memory all to itself. The reality, however, is that physic al memory is a shared resource, managed by the operating system. Exactly how all of this is accomplished is also the subject of the ﬁrst part of this b ook, on the topic of virtualization . 2.3 Concurrency Another main theme of this book is concurrency . We use this concep- tual term to refer to a host of problems that arise, and must be add ressed, when working on many things at once (i.e., concurrently) in the sa me program. The problems of concurrency arose ﬁrst within the operati ng system itself; as you can see in the examples above on virtualiza tion, the OS is juggling many things at once, ﬁrst running one process, the n an- other, and so forth. As it turns out, doing so leads to some deep and interesting problems.",3058
2. Introduction,"5For this example to work, you need to make sure address-space rando mization is dis- abled; randomization, as it turns out, can be a good defense against ce rtain kinds of security ﬂaws. Read more about it on your own, especially if you want to lea rn how to break into computer systems via stack-smashing attacks. Not that we would recom mend such a thing... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 7 1#include <stdio.h> 2#include <stdlib.h> 3#include \""common.h\"" 4 5volatile int counter = 0; 6int loops; 7 8void*worker(void *arg) { 9int i; 10 for (i = 0; i < loops; i++) { 11 counter++; 12 } 13 return NULL; 14} 15 16int 17main(int argc, char *argv[]) 18{ 19 if (argc .= 2) { 20 fprintf(stderr, \""usage: threads <value> \""); 21 exit(1); 22 } 23 loops = atoi(argv[1]); 24 pthread_t p1, p2; 25 printf(\""Initial value :  percentd \"", counter); 26 27 Pthread_create(&p1, NULL, worker, NULL); 28 Pthread_create(&p2, NULL, worker, NULL); 29 Pthread_join(p1, NULL); 30 Pthread_join(p2, NULL); 31 printf(\""Final value :  percentd \"", counter); 32 return 0; 33} Figure 2.5: A Multi-threaded Program ( threads.c ) Unfortunately, the problems of concurrency are no longer limited just to the OS itself. Indeed, modern multi-threaded programs exhibit the same problems. Let us demonstrate with an example of a multi-threaded program (Figure 2.5). Although you might not understand this example fully at the momen t (and we’ll learn a lot more about it in later chapters, in the secti on of the book on concurrency), the basic idea is simple. The main program cr eates two threads usingPthread create()6. You can think of a thread as a function running within the same memory space as other functions , with more than one of them active at a time. In this example, each threa d starts running in a routine called worker() , in which it simply increments a counter in a loop for loops number of times. Below is a transcript of what happens when we run this program wit h the input value for the variable loops set to 1000. The value of loops 6The actual call should be to lower-case pthread create() ; the upper-case version is our own wrapper that calls pthread create() and makes sure that the return code indicates that the call succeeded. See the code for details. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 INTRODUCTION TO OPERATING SYSTEMS THECRUX OF THE PROBLEM : HOWTOBUILD CORRECT CONCURRENT PROGRAMS When there are many concurrently executing threads within th e same memory space, how can we build a correctly working program? What primitives are needed from the OS? What mechanisms should be pro- vided by the hardware? How can we use them to solve the problems of concurrency? determines how many times each of the two workers will increment the shared counter in a loop. When the program is run with the value of loops set to 1000, what do you expect the ﬁnal value of counter to be? prompt> gcc -o thread thread.c -Wall -pthread prompt> ./thread 1000 Initial value : 0 Final value : 2000 As you probably guessed, when the two threads are ﬁnished, the ﬁ nal value of the counter is 2000, as each thread incremented the coun ter 1000 times.",3202
2. Introduction,"Indeed, when the input value of loops is set to N, we would expect the ﬁnal output of the program to be 2N. But life is not so simple, as it turns out. Let’s run the same program, but with higher value s for loops , and see what happens: prompt> ./thread 100000 Initial value : 0 Final value : 143012 // huh?? prompt> ./thread 100000 Initial value : 0 Final value : 137298 // what the?? In this run, when we gave an input value of 100,000, instead of ge tting a ﬁnal value of 200,000, we instead ﬁrst get 143,012. Then, whe n we run the program a second time, we not only again get the wrong value, but also a different value than the last time. In fact, if you run the program over and over with high values of loops , you may ﬁnd that sometimes you even get the right answer. So why is this happening? As it turns out, the reason for these odd and unusual outcomes relate to how instructions are executed, which is one at a time. Unfortun ately, a key part of the program above, where the shared counter is increme nted, takes three instructions: one to load the value of the counter from m em- ory into a register, one to increment it, and one to store it back in to mem- ory. Because these three instructions do not execute atomically (all at once), strange things can happen. It is this problem of concurrency that we will address in great detail in the second part of this book. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 9 1#include <stdio.h> 2#include <unistd.h> 3#include <assert.h> 4#include <fcntl.h> 5#include <sys/types.h> 6 7int 8main(int argc, char *argv[]) 9{ 10 int fd = open(\""/tmp/file\"", O_WRONLY | O_CREAT | O_TRUNC, S_I RWXU); 11 assert(fd > -1); 12 int rc = write(fd, \""hello world \"", 13); 13 assert(rc == 13); 14 close(fd); 15 return 0; 16} Figure 2.6: A Program That Does I/O ( io.c ) 2.4 Persistence The third major theme of the course is persistence . In system memory, data can be easily lost, as devices such as DRAM store values in a volatile manner; when power goes away or the system crashes, any data in me m- ory is lost. Thus, we need hardware and software to be able to store data persistently ; such storage is thus critical to any system as users care a great deal about their data. The hardware comes in the form of some kind of input/output orI/O device; in modern systems, a hard drive is a common repository for long- lived information, although solid-state drives (SSDs ) are making head- way in this arena as well. The software in the operating system that usually manages the d isk is called the ﬁle system ; it is thus responsible for storing any ﬁles the user creates in a reliable and efﬁcient manner on the disks of the sys tem. Unlike the abstractions provided by the OS for the CPU and memory, the OS does not create a private, virtualized disk for each appli cation. Rather, it is assumed that often times, users will want to share informa- tion that is in ﬁles. For example, when writing a C program, you mig ht ﬁrst use an editor (e.g., Emacs7) to create and edit the C ﬁle ( emacs -nw main.c ). Once done, you might use the compiler to turn the source code into an executable (e.g., gcc -o main main.c ). When you’re ﬁnished, you might run the new executable (e.g., ./main ).",3271
2. Introduction,"Thus, you can see how ﬁles are shared across different processes. First, Emacs crea tes a ﬁle that serves as input to the compiler; the compiler uses that input ﬁl e to create a new executable ﬁle (in many steps — take a compiler course for de tails); ﬁnally, the new executable is then run. And thus a new program i s born. To understand this better, let’s look at some code. Figure 2.6 pres ents code to create a ﬁle ( /tmp/file ) that contains the string “hello world”. 7You should be using Emacs. If you are using vi, there is probably som ething wrong with you. If you are using something that is not a real code editor, that is e ven worse. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 INTRODUCTION TO OPERATING SYSTEMS THECRUX OF THE PROBLEM : HOWTOSTORE DATA PERSISTENTLY The ﬁle system is the part of the OS in charge of managing persist ent data. What techniques are needed to do so correctly? What mechanism s and policies are required to do so with high performance? How is reli ability achieved, in the face of failures in hardware and software? To accomplish this task, the program makes three calls into the oper- ating system. The ﬁrst, a call to open() , opens the ﬁle and creates it; the second,write() , writes some data to the ﬁle; the third, close() , sim- ply closes the ﬁle thus indicating the program won’t be writing an y more data to it. These system calls are routed to the part of the operating sys- tem called the ﬁle system , which then handles the requests and returns some kind of error code to the user. You might be wondering what the OS does in order to actually write to disk. We would show you but you’d have to promise to close your eyes ﬁrst; it is that unpleasant. The ﬁle system has to do a fai r bit of work: ﬁrst ﬁguring out where on disk this new data will reside, and the n keep- ing track of it in various structures the ﬁle system maintains. Doing so requires issuing I/O requests to the underlying storage devi ce, to either read existing structures or update (write) them. As anyone who has writ- ten a device driver8knows, getting a device to do something on your behalf is an intricate and detailed process. It requires a dee p knowledge of the low-level device interface and its exact semantics. Fort unately, the OS provides a standard and simple way to access devices through its sys- tem calls. Thus, the OS is sometimes seen as a standard library . Of course, there are many more details in how devices are accesse d, and how ﬁle systems manage data persistently atop said devices . For performance reasons, most ﬁle systems ﬁrst delay such writes for a while, hoping to batch them into larger groups. To handle the problems of sys- tem crashes during writes, most ﬁle systems incorporate some kin d of intricate write protocol, such as journaling orcopy-on-write , carefully ordering writes to disk to ensure that if a failure occurs durin g the write sequence, the system can recover to reasonable state afterwar ds. To make different common operations efﬁcient, ﬁle systems employ many di ffer- ent data structures and access methods, from simple lists to com plex b- trees.",3148
2. Introduction,"If all of this doesn’t make sense yet, good. We’ll be talking a bout all of this quite a bit more in the third part of this book on persistence , where we’ll discuss devices and I/O in general, and then disks , RAIDs, and ﬁle systems in great detail. 8A device driver is some code in the operating system that knows how to d eal with a speciﬁc device. We will talk more about devices and device drivers later. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 11 2.5 Design Goals So now you have some idea of what an OS actually does: it takes phys- icalresources , such as a CPU, memory, or disk, and virtualizes them. It handles tough and tricky issues related to concurrency . And it stores ﬁles persistently , thus making them safe over the long-term. Given that we want to build such a system, we want to have some goals in mind to h elp focus our design and implementation and make trade-offs as neces sary; ﬁnding the right set of trade-offs is a key to building systems. One of the most basic goals is to build up some abstractions in order to make the system convenient and easy to use. Abstractions are fun- damental to everything we do in computer science. Abstraction makes it possible to write a large program by dividing it into small an d under- standable pieces, to write such a program in a high-level lang uage like C9without thinking about assembly, to write code in assembly with out thinking about logic gates, and to build a processor out of gates wit hout thinking too much about transistors. Abstraction is so fundamen tal that sometimes we forget its importance, but we won’t here; thus, in eac h sec- tion, we’ll discuss some of the major abstractions that have develop ed over time, giving you a way to think about pieces of the OS. One goal in designing and implementing an operating system is t o provide high performance ; another way to say this is our goal is to mini- mize the overheads of the OS. Virtualization and making the system easy to use are well worth it, but not at any cost; thus, we must strive t o pro- vide virtualization and other OS features without excessive ove rheads. These overheads arise in a number of forms: extra time (more instr uc- tions) and extra space (in memory or on disk). We’ll seek solutions th at minimize one or the other or both, if possible. Perfection, however, i s not always attainable, something we will learn to notice and (wher e appro- priate) tolerate. Another goal will be to provide protection between applications, as well as between the OS and applications. Because we wish to all ow many programs to run at the same time, we want to make sure that t he malicious or accidental bad behavior of one does not harm others; we certainly don’t want an application to be able to harm the OS itse lf (as that would affect allprograms running on the system). Protection is at the heart of one of the main principles underlying an operating sy stem, which is that of isolation ; isolating processes from one another is the key to protection and thus underlies much of what an OS must do.",3096
2. Introduction,"The operating system must also run non-stop; when it fails, allappli- cations running on the system fail as well. Because of this depen dence, operating systems often strive to provide a high degree of reliability . As operating systems grow evermore complex (sometimes containing mi l- lions of lines of code), building a reliable operating system is qu ite a chal- 9Some of you might object to calling C a high-level language. Remember t his is an OS course, though, where we’re simply happy not to have to code in assembl y all the time. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 INTRODUCTION TO OPERATING SYSTEMS lenge — and indeed, much of the on-going research in the ﬁeld (inc luding some of our own work [BS+09, SS+10]) focuses on this exact problem. Other goals make sense: energy-efﬁciency is important in our increas- ingly green world; security (an extension of protection, really) against malicious applications is critical, especially in these high ly-networked times; mobility is increasingly important as OSes are run on smaller and smaller devices. Depending on how the system is used, the OS wil l have different goals and thus likely be implemented in at least sli ghtly differ- ent ways. However, as we will see, many of the principles we will present on how to build an OS are useful on a range of different devices. 2.6 Some History Before closing this introduction, let us present a brief history of how operating systems developed. Like any system built by humans, good ideas accumulated in operating systems over time, as engineer s learned what was important in their design. Here, we discuss a few major devel- opments. For a richer treatment, see Brinch Hansen’s excellent history of operating systems [BH00]. Early Operating Systems: Just Libraries In the beginning, the operating system didn’t do too much. Basic ally, it was just a set of libraries of commonly-used functions; for examp le, instead of having each programmer of the system write low-level I /O handling code, the “OS” would provide such APIs, and thus make lif e easier for the developer. Usually, on these old mainframe systems, one program ran at a time , as controlled by a human operator. Much of what you think a modern OS would do (e.g., deciding what order to run jobs in) was performe d by this operator. If you were a smart developer, you would be nice to thi s operator, so that they might move your job to the front of the queue. This mode of computing was known as batch processing, as a number of jobs were set up and then run in a “batch” by the operator. Compute rs, as of that point, were not used in an interactive manner, because of cost: it was simply too expensive to let a user sit in front of the compute r and use it, as most of the time it would just sit idle then, costing the f acility hundreds of thousands of dollars per hour [BH00]. Beyond Libraries: Protection In moving beyond being a simple library of commonly-used services , op- erating systems took on a more central role in managing machines.",3043
2. Introduction,"O ne important aspect of this was the realization that code run on behal f of the OS was special; it had control of devices and thus should be treate d dif- ferently than normal application code. Why is this? Well, imagi ne if you OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 13 allowed any application to read from anywhere on the disk; the noti on of privacy goes out the window, as any program could read any ﬁle. Thus , implementing a ﬁle system (to manage your ﬁles) as a library makes little sense. Instead, something else was needed. Thus, the idea of a system call was invented, pioneered by the Atlas computing system [K+61,L78]. Instead of providing OS routines a s a li- brary (where you just make a procedure call to access them), the idea here was to add a special pair of hardware instructions and hardware state to make the transition into the OS a more formal, controlled process. The key difference between a system call and a procedure call i s that a system call transfers control (i.e., jumps) into the OS while simultane- ously raising the hardware privilege level . User applications run in what is referred to as user mode which means the hardware restricts what ap- plications can do; for example, an application running in user mod e can’t typically initiate an I/O request to the disk, access any phy sical memory page, or send a packet on the network. When a system call is initia ted (usually through a special hardware instruction called a trap), the hard- ware transfers control to a pre-speciﬁed trap handler (that the OS set up previously) and simultaneously raises the privilege level to kernel mode . In kernel mode, the OS has full access to the hardware of the syst em and thus can do things like initiate an I/O request or make more memor y available to a program. When the OS is done servicing the reques t, it passes control back to the user via a special return-from-trap instruction, which reverts to user mode while simultaneously passing control back to where the application left off. The Era of Multiprogramming Where operating systems really took off was in the era of computing b e- yond the mainframe, that of the minicomputer . Classic machines like the PDP family from Digital Equipment made computers hugely mor e affordable; thus, instead of having one mainframe per large orga nization, now a smaller collection of people within an organization could likel y have their own computer. Not surprisingly, one of the major impacts of this drop in cost was an increase in developer activity; more smar t people got their hands on computers and thus made computer systems do more interesting and beautiful things. In particular, multiprogramming became commonplace due to the de- sire to make better use of machine resources. Instead of just run ning one job at a time, the OS would load a number of jobs into memory and switch rapidly between them, thus improving CPU utilization. This sw itching was particularly important because I/O devices were slow; hav ing a pro- gram wait on the CPU while its I/O was being serviced was a waste of CPU time.",3134
2. Introduction,"Instead, why not switch to another job and run it for a whi le? The desire to support multiprogramming and overlap in the prese nce of I/O and interrupts forced innovation in the conceptual developm ent of operating systems along a number of directions. Issues such as memory c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 INTRODUCTION TO OPERATING SYSTEMS protection became important; we wouldn’t want one program to be able to access the memory of another program. Understanding how to deal with the concurrency issues introduced by multiprogramming was also critical; making sure the OS was behaving correctly despite t he presence of interrupts is a great challenge. We will study these issues and related topics later in the book. One of the major practical advances of the time was the introducti on of the U NIX operating system, primarily thanks to Ken Thompson (and Dennis Ritchie) at Bell Labs (yes, the phone company). U NIXtook many good ideas from different operating systems (particularly from M ultics [O72], and some from systems like TENEX [B+72] and the Berkeley Time- Sharing System [S+68]), but made them simpler and easier to use. Soon this team was shipping tapes containing U NIX source code to people around the world, many of whom then got involved and added to the system themselves; see the Aside (next page) for more detail10. The Modern Era Beyond the minicomputer came a new type of machine, cheaper, fas ter, and for the masses: the personal computer , orPCas we call it today. Led by Apple’s early machines (e.g., the Apple II) and the IBM PC, t his new breed of machine would soon become the dominant force in computing, as their low-cost enabled one machine per desktop instead of a shar ed minicomputer per workgroup. Unfortunately, for operating systems, the PC at ﬁrst represent ed a great leap backwards, as early systems forgot (or never knew of) t he lessons learned in the era of minicomputers. For example, early op erat- ing systems such as DOS (the Disk Operating System , from Microsoft ) didn’t think memory protection was important; thus, a malicious (or per- haps just a poorly-programmed) application could scribble all ove r mem- ory. The ﬁrst generations of the Mac OS (v9 and earlier) took a coopera- tive approach to job scheduling; thus, a thread that accidenta lly got stuck in an inﬁnite loop could take over the entire system, forcing a reboot . The painful list of OS features missing in this generation of system s is long, too long for a full discussion here. Fortunately, after some years of suffering, the old features of mi ni- computer operating systems started to ﬁnd their way onto the des ktop. For example, Mac OS X/macOS has U NIXat its core, including all of the features one would expect from such a mature system. Windows has s im- ilarly adopted many of the great ideas in computing history, star ting in particular with Windows NT, a great leap forward in Microsoft OS t ech- nology. Even today’s cell phones run operating systems (such as Lin ux) that are much more like what a minicomputer ran in the 1970s than what 10We’ll use asides and other related text boxes to call attention to various items that don’t quite ﬁt the main ﬂow of the text.",3234
2. Introduction,"Sometimes, we’ll even use them j ust to make a joke, because why not have a little fun along the way? Yes, many of the jokes are bad. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 15 ASIDE : THEIMPORTANCE OF UNIX It is difﬁcult to overstate the importance of U NIX in the history of oper- ating systems. Inﬂuenced by earlier systems (in particular , the famous Multics system from MIT), U NIXbrought together many great ideas and made a system that was both simple and powerful. Underlying the original “Bell Labs” U NIX was the unifying principle of building small powerful programs that could be connected togethe r to form larger workﬂows. The shell , where you type commands, provided primitives such as pipes to enable such meta-level programming, and thus it became easy to string together programs to accomplish a b ig- ger task. For example, to ﬁnd lines of a text ﬁle that have the word “foo” in them, and then to count how many such lines exist, you would type:grep foo file.txt|wc -l , thus using the grep andwc(word count) programs to achieve your task. The U NIX environment was friendly for programmers and developers alike, also providing a compiler for the new C programming language . Making it easy for programmers to write their own programs, as wel l as share them, made U NIX enormously popular. And it probably helped a lot that the authors gave out copies for free to anyone who asked, an e arly form of open-source software . Also of critical importance was the accessibility and readabi lity of the code. Having a beautiful, small kernel written in C invited oth ers to play with the kernel, adding new and cool features. For example, an en ter- prising group at Berkeley, led by Bill Joy , made a wonderful distribution (the Berkeley Systems Distribution , orBSD ) which had some advanced virtual memory, ﬁle system, and networking subsystems. Joy lat er co- founded Sun Microsystems . Unfortunately, the spread of U NIXwas slowed a bit as companies tried to assert ownership and proﬁt from it, an unfortunate (but common) res ult of lawyers getting involved. Many companies had their own varian ts: SunOS from Sun Microsystems, AIX from IBM, HPUX (a.k.a. “H-Pucks”) from HP , and IRIX from SGI. The legal wrangling among AT&T/Bell Labs and these other players cast a dark cloud over U NIX, and many wondered if it would survive, especially as Windows was introduc ed and took over much of the PC market... a PC ran in the 1980s (thank goodness); it is good to see that the good ideas developed in the heyday of OS development have found their w ay into the modern world. Even better is that these ideas continue t o de- velop, providing more features and making modern systems even be tter for users and applications. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 INTRODUCTION TO OPERATING SYSTEMS ASIDE : ANDTHEN CAME LINUX Fortunately for U NIX, a young Finnish hacker named Linus Torvalds de- cided to write his own version of U NIX which borrowed heavily on the principles and ideas behind the original system, but not from th e code base, thus avoiding issues of legality. He enlisted help from ma ny others around the world, took advantage of the sophisticated GNU tools that already existed [G85], and soon Linux was born (as well as the modern open-source software movement). As the internet era came into place, most companies (such as Googl e, Amazon, Facebook, and others) chose to run Linux, as it was free and could be readily modiﬁed to suit their needs; indeed, it is hard to imag- ine the success of these new companies had such a system not exist ed. As smart phones became a dominant user-facing platform, Linux f ound a stronghold there too (via Android), for many of the same reasons. An d Steve Jobs took his U NIX-based NeXTStep operating environment with him to Apple, thus making U NIX popular on desktops (though many users of Apple technology are probably not even aware of this fact). Thus UNIX lives on, more important today than ever before.",4048
2. Introduction,"The computing gods, if you believe in them, should be thanked for this wonderful ou t- come. 2.7 Summary Thus, we have an introduction to the OS. Today’s operating systems make systems relatively easy to use, and virtually all operat ing systems you use today have been inﬂuenced by the developments we will dis cuss throughout the book. Unfortunately, due to time constraints, there are a number of pa rts of the OS we won’t cover in the book. For example, there is a lot of net- working code in the operating system; we leave it to you to take the net- working class to learn more about that. Similarly, graphics devices are particularly important; take the graphics course to expand you r knowl- edge in that direction. Finally, some operating system books talk a great deal about security ; we will do so in the sense that the OS must provide protection between running programs and give users the ability to pro- tect their ﬁles, but we won’t delve into deeper security issues that one might ﬁnd in a security course. However, there are many important topics that we will cover, incl ud- ing the basics of virtualization of the CPU and memory, concurrenc y, and persistence via devices and ﬁle systems. Don’t worry. While the re is a lot of ground to cover, most of it is quite cool, and at the end of the road, you’ll have a new appreciation for how computer systems really work. Now get to work. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTRODUCTION TO OPERATING SYSTEMS 17 References [BS+09] “Tolerating File-System Mistakes with EnvyFS” by L. Bair avasundaram, S. Sundarara- man, A. Arpaci-Dusseau, R. Arpaci-Dusseau. USENIX ’09, San Diego , CA, June 2009. A fun paper about using multiple ﬁle systems at once to tolerate a mistake in any one of the m. [BH00] “The Evolution of Operating Systems” by P . Brinch Hansen. In ’Cl assic Operating Systems: From Batch Processing to Distributed Systems.’ Springe r-Verlag, New York, 2000. This essay provides an intro to a wonderful collection of papers about histori cally signiﬁcant systems. [B+72] “TENEX, A Paged Time Sharing System for the PDP-10” by D. Bobrow, J. Burchﬁel, D. Murphy, R. Tomlinson. CACM, Volume 15, Number 3, March 1972. TENEX has much of the machinery found in modern operating systems; read more about it to see how mu ch innovation was already in place in the early 1970’s. [B75] “The Mythical Man-Month” by F. Brooks. Addison-Wesley, 1975. A classic text on software engineering; well worth the read. [BOH10] “Computer Systems: A Programmer’s Perspective” by R. Bry ant and D. O’Hallaron. Addison-Wesley, 2010. Another great intro to how computer systems work. Has a little bit of overlap with this book — so if you’d like, you can skip the last few chapters of that book, or sim ply read them to get a different perspective on some of the same material. After all, one good w ay to build up your own knowledge is to hear as many other perspectives as possible, and then develop your own opinion and thoughts on the matter. You know, by thinking. [G85] “The GNU Manifesto” by R.",3065
2. Introduction,"Stallman. 1985. www.gnu.org/gnu/manifesto.html . A huge part of Linux’s success was no doubt the presence of an excellen t compiler, gcc, and other relevant pieces of open software, thanks to the GNU effort headed by Stallman. S tallman is a visionary when it comes to open source, and this manifesto lays out his thoughts as to why. [K+61] “One-Level Storage System” by T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner. IRE Transactions on Electronic Computers, April 1962. The Atlas pioneered much of what you see in modern systems. However, this paper is not the best read. If you were to on ly read one, you might try the historical perspective below [L78]. [L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com- munications of the ACM, Volume 21:1, January 1978. A nice piece of history on the early devel- opment of computer systems and the pioneering efforts of the Atlas. Of course, one could go back and read the Atlas papers themselves, but this paper provides a great overvie w and adds some historical perspective. [O72] “The Multics System: An Examination of its Structure” by Elliot t Organick. MIT Press, 1972. A great overview of Multics. So many good ideas, and yet it was an over-design ed system, shooting for too much, and thus never really worked. A classic example of what F red Brooks would call the “second-system effect” [B75]. [PP03] “Introduction to Computing Systems: From Bits and Gates to C a nd Beyond” by Yale N. Patt, Sanjay J. Patel. McGraw-Hill, 2003. One of our favorite intro to computing systems books. Starts at transistors and gets you all the way up to C; the early material is particularly great. [RT74] “The U NIXTime-Sharing System” by Dennis M. Ritchie, Ken Thompson. CACM, Vol- ume 17: 7, July 1974. A great summary of UNIXwritten as it was taking over the world of computing, by the people who wrote it. [S68] “SDS 940 Time-Sharing System” by Scientiﬁc Data Systems. TECH NICAL MANUAL, SDS 90 11168, August 1968. Yes, a technical manual was the best we could ﬁnd. But it is fascinating to read these old system documents, and see how much was already in place in the late 1960’s. One of the minds behind the Berkeley Time-Sharing System (which eventually b ecame the SDS system) was Butler Lampson, who later won a Turing award for his contributions in systems . [SS+10] “Membrane: Operating System Support for Restartable File Systems” by S. Sundarara- man, S. Subramanian, A. Rajimwale, A. Arpaci-Dusseau, R. Arpaci-Du sseau, M. Swift. FAST ’10, San Jose, CA, February 2010. The great thing about writing your own class notes: you can ad- vertise your own research. But this paper is actually pretty neat — when a ﬁl e system hits a bug and crashes, Membrane auto-magically restarts it, all without applications or the rest of the system being affected. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 INTRODUCTION TO OPERATING SYSTEMS Homework Most (and eventually, all) chapters of this book have homework sec- tions at the end. Doing these homeworks is important, as each lets y ou, the reader, gain more experience with the concepts presented w ithin the chapter. There are two types of homeworks.",3204
2. Introduction,"The ﬁrst is based on simulation . A simulation of a computer system is just a simple program that pret ends to do some of the interesting parts of what a real system does, and the n re- port some output metrics to show how the system behaves. For example , a hard drive simulator might take a series of requests, simulat e how long they would take to get serviced by a hard drive with certain per formance characteristics, and then report the average latency of the re quests. The cool thing about simulations is they let you easily explore how systems behave without the difﬁculty of running a real system. Indeed, they even let you create systems that cannot exist in the real wor ld (for example, a hard drive with unimaginably fast performance), a nd thus see the potential impact of future technologies. Of course, simulations are not without their downsides. By their v ery nature, simulations are just approximations of how a real system b ehaves. If an important aspect of real-world behavior is omitted, the simu lation will report bad results. Thus, results from a simulation should a lways be treated with some suspicion. In the end, how a system behaves in t he real world is what matters. The second type of homework requires interaction with real-world code . Some of these homeworks are measurement focused, whereas oth- ers just require some small-scale development and experiment ation. Both are just small forays into the larger world you should be getting i nto, which is how to write systems code in C on U NIX-based systems. Indeed, larger-scale projects, which go beyond these homeworks, are nee ded to push you in this direction; thus, beyond just doing homeworks, we st rongly recommend you do projects to solidify your systems skills. See this page (https://github.com/remzi-arpacidusseau/ostep-projec ts) for some projects. To do these homeworks, you likely have to be on a U NIX-based ma- chine, running either Linux, macOS, or some similar system. It s hould also have a C compiler installed (e.g., gcc) as well as Python. You should also know how to edit code in a real code editor of some kind. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2173
Part I Virtualization,Part I Virtualization 1,23
3. Dialogue,"3 A Dialogue on Virtualization Professor: And thus we reach the ﬁrst of our three pieces on operating system s: virtualization . Student: But what is virtualization, oh noble professor? Professor: Imagine we have a peach. Student: A peach? (incredulous) Professor: Yes, a peach. Let us call that the physical peach. But we have many eaters who would like to eat this peach. What we would like to present t o each eater is their own peach, so that they can be happy. We call the pea ch we give eaters virtual peaches; we somehow create many of these virtual peaches out o f the one physical peach. And the important thing: in this illusion, it look s to each eater like they have a physical peach, but in reality they don’t. Student: So you are sharing the peach, but you don’t even know it? Professor: Right. Exactly. Student: But there’s only one peach. Professor: Yes. And...? Student: Well, if I was sharing a peach with somebody else, I think I would notice. Professor: Ah yes. Good point. But that is the thing with many eaters; most of the time they are napping or doing something else, and thus, you c an snatch that peach away and give it to someone else for a while. And thus we cre ate the illusion of many virtual peaches, one peach for each person. Student: Sounds like a bad campaign slogan. You are talking about computers, right Professor? Professor: Ah, young grasshopper, you wish to have a more concrete example . Good idea. Let us take the most basic of resources, the CPU. Assu me there is one physical CPU in a system (though now there are often two or four or m ore). What virtualization does is take that single CPU and make it look like many virtu al CPUs to the applications running on the system. Thus, while each app lication 3 4 A D IALOGUE ON VIRTUALIZATION thinks it has its own CPU to use, there is really only one. And thus the O S has created a beautiful illusion: it has virtualized the CPU. Student: Wow. That sounds like magic. Tell me more. How does that work? Professor: In time, young student, in good time. Sounds like you are ready to begin. Student: I am. Well, sort of. I must admit, I’m a little worried you are going to start talking about peaches again. Professor: Don’t worry too much; I don’t even like peaches. And thus we be- gin... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2331
4. Processes,"4 The Abstraction: The Process In this chapter, we discuss one of the most fundamental abstract ions that the OS provides to users: the process . The deﬁnition of a process, infor- mally, is quite simple: it is a running program [V+65,BH70]. The program itself is a lifeless thing: it just sits there on the disk, a bun ch of instructions (and maybe some static data), waiting to spring into action. It is the oper- ating system that takes these bytes and gets them running, tr ansforming the program into something useful. It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where you might lik e to run a web browser, mail program, a game, a music player, and so forth. In fact, a typical system may be seemingly running tens or even hundreds of processes at the same time. Doing so makes the system easy to us e, as one never need be concerned with whether a CPU is available; one s imply runs programs. Hence our challenge: THECRUX OF THE PROBLEM : HOWTOPROVIDE THEILLUSION OFMANY CPU S? Although there are only a few physical CPUs available, how can th e OS provide the illusion of a nearly-endless supply of said CPUs? The OS creates this illusion by virtualizing the CPU. By running one process, then stopping it and running another, and so forth, the O S can promote the illusion that many virtual CPUs exist when in fact th ere is only one physical CPU (or a few). This basic technique, known as time sharing of the CPU, allows users to run as many concurrent processes as they would like; the potential cost is performance, as each will r un more slowly if the CPU(s) must be shared. To implement virtualization of the CPU, and to implement it wel l, the OS will need both some low-level machinery and some high-level in - telligence. We call the low-level machinery mechanisms ; mechanisms are low-level methods or protocols that implement a needed piece of functionality. For example, we’ll learn later how to implement a context 1 2 THEABSTRACTION : THEPROCESS TIP: USETIME SHARING (AND SPACE SHARING ) Time sharing is a basic technique used by an OS to share a resource. By allowing the resource to be used for a little while by one entity, a nd then a little while by another, and so forth, the resource in question ( e.g., the CPU, or a network link) can be shared by many. The counterpart of ti me sharing is space sharing , where a resource is divided (in space) among those who wish to use it. For example, disk space is naturally a s pace- shared resource; once a block is assigned to a ﬁle, it is normally n ot as- signed to another ﬁle until the user deletes the original ﬁle. switch , which gives the OS the ability to stop running one program and start running another on a given CPU; this time-sharing mechanism is employed by all modern OSes. On top of these mechanisms resides some of the intelligence in the OS, in the form of policies . Policies are algorithms for making some kind of decision within the OS. For example, given a number of possi- ble programs to run on a CPU, which program should the OS run? A scheduling policy in the OS will make this decision, likely using histori- cal information (e.g., which program has run more over the last min ute?), workload knowledge (e.g., what types of programs are run), and per for- mance metrics (e.g., is the system optimizing for interactive performance, or throughput?) to make its decision.",3440
4. Processes,"4.1 The Abstraction: A Process The abstraction provided by the OS of a running program is somethin g we will call a process . As we said above, a process is simply a running program; at any instant in time, we can summarize a process by ta king an inventory of the different pieces of the system it accesses or aff ects during the course of its execution. To understand what constitutes a process, we thus have to under stand itsmachine state : what a program can read or update when it is running. At any given time, what parts of the machine are important to the execu- tion of this program? One obvious component of machine state that comprises a process is itsmemory . Instructions lie in memory; the data that the running pro- gram reads and writes sits in memory as well. Thus the memory tha t the process can address (called its address space ) is part of the process. Also part of the process’s machine state are registers ; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process. Note that there are some particularly special registers that f orm part of this machine state. For example, the program counter (PC) (sometimes called the instruction pointer orIP) tells us which instruction of the pro- gram is currently being executed; similarly a stack pointer and associated OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : THEPROCESS 3 TIP: SEPARATE POLICY ANDMECHANISM In many operating systems, a common design paradigm is to separa te high-level policies from their low-level mechanisms [L+75]. Y ou can think of the mechanism as providing the answer to a how question about a system; for example, how does an operating system perform a context switch? The policy provides the answer to a which question; for example, which process should the operating system run right now? Separating the two allows one easily to change policies without having to rethin k the mechanism and is thus a form of modularity , a general software design principle. frame pointer are used to manage the stack for function parameters, local variables, and return addresses. Finally, programs often access persistent storage devices too. Su chI/O information might include a list of the ﬁles the process currently has open. 4.2 Process API Though we defer discussion of a real process API until a subsequen t chapter, here we ﬁrst give some idea of what must be included in a ny interface of an operating system. These APIs, in some form, are av ailable on any modern operating system. •Create: An operating system must include some method to cre- ate new processes. When you type a command into the shell, or double-click on an application icon, the OS is invoked to create a new process to run the program you have indicated. •Destroy: As there is an interface for process creation, systems also provide an interface to destroy processes forcefully. Of course, many processes will run and just exit by themselves when complete; w hen they don’t, however, the user may wish to kill them, and thus an in - terface to halt a runaway process is quite useful. •Wait: Sometimes it is useful to wait for a process to stop running; thus some kind of waiting interface is often provided. •Miscellaneous Control: Other than killing or waiting for a process, there are sometimes other controls that are possible. For example, most operating systems provide some kind of method to suspend a process (stop it from running for a while) and then resume it (con- tinue it running). •Status: There are usually interfaces to get some status information about a process as well, such as how long it has run for, or what state it is in.",3691
4. Processes,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 THEABSTRACTION : THEPROCESS Memory CPU Diskcode static data heap stack Process code static data ProgramLoading: Takes on-disk program and reads it into the address space of process Figure 4.1: Loading: From Program To Process 4.3 Process Creation: A Little More Detail One mystery that we should unmask a bit is how programs are trans- formed into processes. Speciﬁcally, how does the OS get a program up and running? How does process creation actually work? The ﬁrst thing that the OS must do to run a program is to load its code and any static data (e.g., initialized variables) into memor y, into the ad- dress space of the process. Programs initially reside on disk (or, in some modern systems, ﬂash-based SSDs ) in some kind of executable format ; thus, the process of loading a program and static data into memory r e- quires the OS to read those bytes from disk and place them in memor y somewhere (as shown in Figure 4.1). In early (or simple) operating systems, the loading process is don eea- gerly , i.e., all at once before running the program; modern OSes perform the process lazily , i.e., by loading pieces of code or data only as they are needed during program execution. To truly understand how lazy l oading of pieces of code and data works, you’ll have to understand more about the machinery of paging and swapping , topics we’ll cover in the future when we discuss the virtualization of memory. For now, just rememb er that before running anything, the OS clearly must do some work to get the important program bits from disk into memory. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : THEPROCESS 5 Once the code and static data are loaded into memory, there are a f ew other things the OS needs to do before running the process. Some mem - ory must be allocated for the program’s run-time stack (or just stack ). As you should likely already know, C programs use the stack for local variables, function parameters, and return addresses; the O S allocates this memory and gives it to the process. The OS will also likely i nitial- ize the stack with arguments; speciﬁcally, it will ﬁll in the parameters to themain() function, i.e., argc and theargv array. The OS may also allocate some memory for the program’s heap . In C programs, the heap is used for explicitly requested dynamical ly-allocated data; programs request such space by calling malloc() and free it ex- plicitly by calling free() . The heap is needed for data structures such as linked lists, hash tables, trees, and other interesting data structures. The heap will be small at ﬁrst; as the program runs, and requests mor e mem- ory via the malloc() library API, the OS may get involved and allocate more memory to the process to help satisfy such calls. The OS will also do some other initialization tasks, particular ly as re- lated to input/output (I/O). For example, in U NIXsystems, each process by default has three open ﬁle descriptors , for standard input, output, and error; these descriptors let programs easily read input from the terminal and print output to the screen.",3146
4. Processes,"We’ll learn more about I/O, ﬁle des crip- tors, and the like in the third part of the book on persistence . By loading the code and static data into memory, by creating and i ni- tializing a stack, and by doing other work as related to I/O setup , the OS has now (ﬁnally) set the stage for program execution. It thus has on e last task: to start the program running at the entry point, namely main() . By jumping to the main() routine (through a specialized mechanism that we will discuss next chapter), the OS transfers control of the CP U to the newly-created process, and thus the program begins its execut ion. 4.4 Process States Now that we have some idea of what a process is (though we will continue to reﬁne this notion), and (roughly) how it is created, le t us talk about the different states a process can be in at a given time. The notion that a process can be in one of these states arose in early computer s ystems [DV66,V+65]. In a simpliﬁed view, a process can be in one of three states: •Running : In the running state, a process is running on a processor. This means it is executing instructions. •Ready : In the ready state, a process is ready to run but for some reason the OS has chosen not to run it at this given moment. •Blocked : In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 THEABSTRACTION : THEPROCESS Running Ready BlockedDescheduled Scheduled I/O: initiate I/O: done Figure 4.2: Process: State Transitions If we were to map these states to a graph, we would arrive at the d i- agram in Figure 4.2. As you can see in the diagram, a process can b e moved between the ready and running states at the discretion of t he OS. Being moved from ready to running means the process has been sched- uled ; being moved from running to ready means the process has been descheduled . Once a process has become blocked (e.g., by initiating an I/O operation), the OS will keep it as such until some event occurs (e.g., I/O completion); at that point, the process moves to the ready stat e again (and potentially immediately to running again, if the OS so de cides). Let’s look at an example of how two processes might transition through some of these states. First, imagine two processes running, eac h of which only use the CPU (they do no I/O). In this case, a trace of the stat e of each process might look like this (Figure 4.3). Time Process 0 Process 1 Notes 1 Running Ready 2 Running Ready 3 Running Ready 4 Running Ready Process 0now done 5 – Running 6 – Running 7 – Running 8 – Running Process 1now done Figure 4.3: Tracing Process State: CPU Only In this next example, the ﬁrst process issues an I/O after runn ing for some time. At that point, the process is blocked, giving the other p rocess a chance to run.",3022
4. Processes,"Figure 4.4 shows a trace of this scenario. More speciﬁcally, Process 0initiates an I/O and becomes blocked wait- ing for it to complete; processes become blocked, for example, when read- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : THEPROCESS 7 Time Process 0 Process 1 Notes 1 Running Ready 2 Running Ready 3 Running Ready Process 0initiates I/O 4 Blocked Running Process 0is blocked, 5 Blocked Running so Process 1runs 6 Blocked Running 7 Ready Running I/O done 8 Ready Running Process 1now done 9 Running – 10 Running – Process 0now done Figure 4.4: Tracing Process State: CPU and I/O ing from a disk or waiting for a packet from a network. The OS recog- nizes Process 0is not using the CPU and starts running Process 1. While Process 1is running, the I/O completes, moving Process 0back to ready. Finally, Process 1ﬁnishes, and Process 0runs and then is done. Note that there are many decisions the OS must make, even in this simple example. First, the system had to decide to run Process 1while Process 0issued an I/O; doing so improves resource utilization by keep- ing the CPU busy. Second, the system decided not to switch back to Process 0when its I/O completed; it is not clear if this is a good deci- sion or not. What do you think? These types of decisions are made by th e OSscheduler , a topic we will discuss a few chapters in the future. 4.5 Data Structures The OS is a program, and like any program, it has some key data stru c- tures that track various relevant pieces of information. To trac k the state of each process, for example, the OS likely will keep some kind of pro- cess list for all processes that are ready and some additional informa- tion to track which process is currently running. The OS must al so track, in some way, blocked processes; when an I/O event completes, the O S should make sure to wake the correct process and ready it to run ag ain. Figure 4.5 shows what type of information an OS needs to track about each process in the xv6 kernel [CK+08]. Similar process structu res exist in “real” operating systems such as Linux, Mac OS X, or Windows; l ook them up and see how much more complex they are. From the ﬁgure, you can see a couple of important pieces of informa- tion the OS tracks about a process. The register context will hold, for a stopped process, the contents of its registers. When a process is s topped, its registers will be saved to this memory location; by restoring these reg- isters (i.e., placing their values back into the actual phys ical registers), the OS can resume running the process. We’ll learn more about this tec hnique known as a context switch in future chapters. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 THEABSTRACTION : THEPROCESS // the registers xv6 will save and restore // to stop and subsequently restart a process struct context { int eip; int esp; int ebx; int ecx; int edx; int esi; int edi; int ebp; }; // the different states a process can be in enum proc_state { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE }; // the information xv6 tracks about each process // including its register context and state struct proc { char*mem; // Start of process memory uint sz; // Size of process memory char*kstack; // Bottom of kernel stack // for this process enum proc_state state; // Process state int pid; // Process ID struct proc *parent; // Parent process void*chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory struct context context; // Switch here to run process struct trapframe *tf; // Trap frame for the // current interrupt }; Figure 4.5: The xv6 Proc Structure You can also see from the ﬁgure that there are some other states a pr o- cess can be in, beyond running, ready, and blocked.",3842
4. Processes,"Sometimes a sy stem will have an initial state that the process is in when it is being created. Also, a process could be placed in a ﬁnal state where it has exited but has not yet been cleaned up (in UNIX-based systems, this is cal led the zombie state1). This ﬁnal state can be useful as it allows other processes (usually the parent that created the process) to examine the return code of the process and see if the just-ﬁnished process executed succ essfully (usually, programs return zero in U NIX-based systems when they have accomplished a task successfully, and non-zero otherwise). Wh en ﬁn- ished, the parent will make one ﬁnal call (e.g., wait() ) to wait for the completion of the child, and to also indicate to the OS that it can clean up any relevant data structures that referred to the now-extinc t process. 1Yes, the zombie state. Just like real zombies, these zombies are relatively easy to kill. However, different techniques are usually recommended. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : THEPROCESS 9 ASIDE : DATA STRUCTURE — T HEPROCESS LIST Operating systems are replete with various important data structures that we will discuss in these notes. The process list (also called the task list) is the ﬁrst such structure. It is one of the simpler ones, but cer tainly any OS that has the ability to run multiple programs at once will have something akin to this structure in order to keep track of all the running programs in the system. Sometimes people refer to the individual struc- ture that stores information about a process as a Process Control Block (PCB ), a fancy way of talking about a C structure that contains informa - tion about each process (also sometimes called a process descriptor ). ASIDE : KEYPROCESS TERMS •The process is the major OS abstraction of a running program. At any point in time, the process can be described by its state: the con- tents of memory in its address space , the contents of CPU registers (including the program counter and stack pointer , among others), and information about I/O (such as open ﬁles which can be read or written). •The process API consists of calls programs can make related to pro- cesses. Typically, this includes creation, destruction, and other use- ful calls. •Processes exist in one of many different process states , including running, ready to run, and blocked. Different events (e.g., g etting scheduled or descheduled, or waiting for an I/O to complete) tran - sition a process from one of these states to the other. •Aprocess list contains information about all processes in the sys- tem. Each entry is found in what is sometimes called a process control block (PCB ), which is really just a structure that contains information about a speciﬁc process. 4.6 Summary We have introduced the most basic abstraction of the OS: the process . It is quite simply viewed as a running program. With this concep tual view in mind, we will now move on to the nitty-gritty: the low-leve l mechanisms needed to implement processes, and the higher-le vel poli- cies required to schedule them in an intelligent way. By combi ning mech- anisms and policies, we will build up our understanding of how an op er- ating system virtualizes the CPU.",3247
4. Processes,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 THEABSTRACTION : THEPROCESS References [BH70] “The Nucleus of a Multiprogramming System” by Per Brinch Hansen. C ommunica- tions of the ACM, Volume 13:4, April 1970. This paper introduces one of the ﬁrst microkernels in operating systems history, called Nucleus. The idea of smaller, more mi nimal systems is a theme that rears its head repeatedly in OS history; it all began with Brinch Hansen’s work described herein. [CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai Zeldovich. From: https://github.com/mit-pdos/xv6-public. The coolest real and little OS in the world. Download and play with it to learn more about the details of how operating syste ms actually work. We have been using an older version (2012-01-30-1-g1c41342) and hen ce some examples in the book may not match the latest in the source. [DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis, Earl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966 . This paper deﬁned many of the early terms and concepts around building multiprogramme d systems. [L+75] “Policy/mechanism separation in Hydra” by R. Levin, E. Cohen, W . Corwin, F. Pollack, W. Wulf. SOSP ’75, Austin, Texas, November 1975. An early paper about how to structure operat- ing systems in a research OS known as Hydra. While Hydra never became a mainstream OS, some of its ideas inﬂuenced OS designers. [V+65] “Structure of the Multics Supervisor” by V .A. Vyssotsky, F . J. Corbato, R. M. Graham. Fall Joint Computer Conference, 1965. An early paper on Multics, which described many of the basic ideas and terms that we ﬁnd in modern systems. Some of the vision behind comp uting as a utility are ﬁnally being realized in modern cloud systems. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : THEPROCESS 11 Homework (Simulation) This program, process-run.py , allows you to see how process states change as programs run and either use the CPU (e.g., perform an a dd instruction) or do I/O (e.g., send a request to a disk and wait for it to complete). See the README for details. Questions 1. Runprocess-run.py with the following ﬂags: -l 5:100,5:100 . What should the CPU utilization be (e.g., the percent of time the CPU i s in use?) Why do you know this? Use the -cand-pﬂags to see if you were right. 2. Now run with these ﬂags: ./process-run.py -l 4:100,1:0 . These ﬂags specify one process with 4 instructions (all to use the CPU) , and one that simply issues an I/O and waits for it to be done. How long doe s it take to complete both processes? Use -cand-pto ﬁnd out if you were right. 3. Switch the order of the processes: ./process-run.py -l 1:0,4:100 . What happens now? Does switching the order matter? Why? (As alw ays, use-cand-pto see if you were right) 4. We’ll now explore some of the other ﬂags. One important ﬂag is -S, which determines how the system reacts when a process issues an I/O. Wi th the ﬂag set to SWITCH ON END, the system will NOT switch to another pro- cess while one is doing I/O, instead waiting until the process is completely ﬁnished. What happens when you run the following two processe s (-l 1:0,4:100 -c -S SWITCH ONEND), one doing I/O and the other doing CPU work? 5. Now, run the same processes, but with the switching behavior s et to switch to another process whenever one is WAITING for I/O ( -l 1:0,4:100 -c -S SWITCH ONIO). What happens now? Use -cand-pto conﬁrm that you are right. 6. One other important behavior is what to do when an I/O complet es. With -I IORUNLATER , when an I/O completes, the process that issued it is not necessarily run right away; rather, whatever was running at th e time keeps running. What happens when you run this combination of processe s? (Run ./process-run.py -l 3:0,5:100,5:100,5:100 -S SWITCH ONIO -I IORUNLATER -c -p ) Are system resources being effectively utilized? 7. Now run the same processes, but with -I IORUNIMMEDIATE set, which immediately runs the process that issued the I/O. How does this beh avior differ? Why might running a process that just completed an I/O aga in be a good idea? 8. Now run with some randomly generated processes: -s 1 -l 3:50,3:50 or-s 2 -l 3:50,3:50 or-s 3 -l 3:50,3:50 . See if you can pre- dict how the trace will turn out. What happens when you use the ﬂag -I IORUNIMMEDIATE vs.-I IORUNLATER ? What happens when you use -S SWITCH ONIOvs.-S SWITCH ONEND? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4533
5. Process API,"5 Interlude: Process API ASIDE : INTERLUDES Interludes will cover more practical aspects of systems, inclu ding a par- ticular focus on operating system APIs and how to use them. If you don ’t like practical things, you could skip these interludes. But you should like practical things, because, well, they are generally useful in real life; com- panies, for example, don’t usually hire you for your non-practical s kills. In this interlude, we discuss process creation in U NIX systems. U NIX presents one of the most intriguing ways to create a new process wi th a pair of system calls: fork() andexec() . A third routine, wait() , can be used by a process wishing to wait for a process it has create d to complete. We now present these interfaces in more detail, with a few simple examples to motivate us. And thus, our problem: CRUX: HOWTOCREATE ANDCONTROL PROCESSES What interfaces should the OS present for process creation and con - trol? How should these interfaces be designed to enable powerful func- tionality, ease of use, and high performance? 5.1 Thefork() System Call Thefork() system call is used to create a new process [C63]. How- ever, be forewarned: it is certainly the strangest routine you w ill ever call1. More speciﬁcally, you have a running program whose code looks like what you see in Figure 5.1; examine the code, or better yet, t ype it in and run it yourself. 1Well, OK, we admit that we don’t know that for sure; who knows what routine s you call when no one is looking? But fork() is pretty odd, no matter how unusual your routine- calling patterns are. 1 2 INTERLUDE : PROCESS API 1#include <stdio.h> 2#include <stdlib.h> 3#include <unistd.h> 4 5int main(int argc, char *argv[]) { 6printf(\""hello world (pid: percentd) \"", (int) getpid()); 7int rc = fork(); 8if (rc < 0) { // fork failed; exit 9 fprintf(stderr, \""fork failed \""); 10 exit(1); 11 } else if (rc == 0) { // child (new process) 12 printf(\""hello, I am child (pid: percentd) \"", (int) getpid()); 13 } else { // parent goes down this path (main) 14 printf(\""hello, I am parent of  percentd (pid: percentd) \"", 15 rc, (int) getpid()); 16 } 17 return 0; 18} 19 Figure 5.1: Callingfork() (p1.c ) When you run this program (called p1.c ), you’ll see the following: prompt> ./p1 hello world (pid:29146) hello, I am parent of 29147 (pid:29146) hello, I am child (pid:29147) prompt> Let us understand what happened in more detail in p1.c . When it ﬁrst started running, the process prints out a hello world messa ge; in- cluded in that message is its process identiﬁer , also known as a PID. The process has a PID of 29146; in U NIX systems, the PID is used to name the process if one wants to do something with the process, such as ( for example) stop it from running. So far, so good. Now the interesting part begins. The process calls the fork() system call, which the OS provides as a way to create a new process. The od d part: the process that is created is an (almost) exact copy of the calling pro- cess. That means that to the OS, it now looks like there are two copies of the program p1running, and both are about to return from the fork() system call. The newly-created process (called the child , in contrast to the creating parent ) doesn’t start running at main() , like you might expect (note, the “hello, world” message only got printed out once); rather , it just comes into life as if it had called fork() itself.",3419
5. Process API,"You might have noticed: the child isn’t an exact copy. Speciﬁcally, al- though it now has its own copy of the address space (i.e., its own priv ate memory), its own registers, its own PC, and so forth, the value it r eturns to the caller of fork() is different. Speciﬁcally, while the parent receives the PID of the newly-created child, the child receives a retur n code of zero. This differentiation is useful, because it is simple the n to write the code that handles the two different cases (as above). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : PROCESS API 3 1#include <stdio.h> 2#include <stdlib.h> 3#include <unistd.h> 4#include <sys/wait.h> 5 6int main(int argc, char *argv[]) { 7printf(\""hello world (pid: percentd) \"", (int) getpid()); 8int rc = fork(); 9if (rc < 0) { // fork failed; exit 10 fprintf(stderr, \""fork failed \""); 11 exit(1); 12 } else if (rc == 0) { // child (new process) 13 printf(\""hello, I am child (pid: percentd) \"", (int) getpid()); 14 } else { // parent goes down this path (main) 15 int rc_wait = wait(NULL); 16 printf(\""hello, I am parent of  percentd (rc_wait: percentd) (pid: percentd) \"", 17 rc, rc_wait, (int) getpid()); 18 } 19 return 0; 20} 21 Figure 5.2: Callingfork() Andwait() (p2.c ) You might also have noticed: the output (of p1.c ) is not deterministic . When the child process is created, there are now two active proce sses in the system that we care about: the parent and the child. Assumi ng we are running on a system with a single CPU (for simplicity), then either the child or the parent might run at that point. In our example (ab ove), the parent did and thus printed out its message ﬁrst. In other ca ses, the opposite might happen, as we show in this output trace: prompt> ./p1 hello world (pid:29146) hello, I am child (pid:29147) hello, I am parent of 29147 (pid:29146) prompt> The CPU scheduler , a topic we’ll discuss in great detail soon, deter- mines which process runs at a given moment in time; because the s ched- uler is complex, we cannot usually make strong assumptions about w hat it will choose to do, and hence which process will run ﬁrst. This non- determinism , as it turns out, leads to some interesting problems, par- ticularly in multi-threaded programs ; hence, we’ll see a lot more non- determinism when we study concurrency in the second part of the book. 5.2 Thewait() System Call So far, we haven’t done much: just created a child that prints out a message and exits. Sometimes, as it turns out, it is quite useful for a parent to wait for a child process to ﬁnish what it has been doing. This task is accomplished with the wait() system call (or its more complete siblingwaitpid() ); see Figure 5.2 for details. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 INTERLUDE : PROCESS API In this example ( p2.c ), the parent process calls wait() to delay its execution until the child ﬁnishes executing. When the child i s done, wait() returns to the parent. Adding await() call to the code above makes the output determin- istic.",3039
5. Process API,"Can you see why? Go ahead, think about it. (waiting for you to think .... and done) Now that you have thought a bit, here is the output: prompt> ./p2 hello world (pid:29266) hello, I am child (pid:29267) hello, I am parent of 29267 (rc_wait:29267) (pid:29266) prompt> With this code, we now know that the child will always print ﬁrst. Why do we know that? Well, it might simply run ﬁrst, as before, an d thus print before the parent. However, if the parent does happen to run ﬁrst, it will immediately call wait() ; this system call won’t return until the child has run and exited2. Thus, even when the parent runs ﬁrst, it politely waits for the child to ﬁnish running, then wait() returns, and then the parent prints its message. 5.3 Finally, The exec() System Call A ﬁnal and important piece of the process creation API is the exec() system call3. This system call is useful when you want to run a program that is different from the calling program. For example, callin gfork() inp2.c is only useful if you want to keep running copies of the same program. However, often you want to run a different program;exec() does just that (Figure 5.3, page 5). In this example, the child process calls execvp() in order to run the programwc, which is the word counting program. In fact, it runs wcon the source ﬁle p3.c , thus telling us how many lines, words, and bytes are found in the ﬁle: prompt> ./p3 hello world (pid:29383) hello, I am child (pid:29384) 29 107 1030 p3.c hello, I am parent of 29384 (rc_wait:29384) (pid:29383) prompt> 2There are a few cases where wait() returns before the child exits; read the man page for more details, as always. And beware of any absolute and unquali ﬁed statements this book makes, such as “the child will always print ﬁrst” or “U NIXis the best thing in the world, even better than ice cream.” 3On Linux, there are six variants of exec() :execl, execlp(), execle(), execv(), execvp() , andexecvpe() . Read the man pages to learn more. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : PROCESS API 5 1#include <stdio.h> 2#include <stdlib.h> 3#include <unistd.h> 4#include <string.h> 5#include <sys/wait.h> 6 7int main(int argc, char *argv[]) { 8printf(\""hello world (pid: percentd) \"", (int) getpid()); 9int rc = fork(); 10 if (rc < 0) { // fork failed; exit 11 fprintf(stderr, \""fork failed \""); 12 exit(1); 13 } else if (rc == 0) { // child (new process) 14 printf(\""hello, I am child (pid: percentd) \"", (int) getpid()); 15 char*myargs[3]; 16 myargs[0] = strdup(\""wc\""); // program: \""wc\"" (word count) 17 myargs[1] = strdup(\""p3.c\""); // argument: file to count 18 myargs[2] = NULL; // marks end of array 19 execvp(myargs[0], myargs); // runs word count 20 printf(\""this shouldn’t print out\""); 21 } else { // parent goes down this path (main) 22 int rc_wait = wait(NULL); 23 printf(\""hello, I am parent of  percentd (rc_wait: percentd) (pid: percentd) \"", 24 rc, rc_wait, (int) getpid()); 25 } 26 return 0; 27} 28 Figure 5.3: Callingfork() ,wait() , Andexec() (p3.c ) Thefork() system call is strange; its partner in crime, exec() , is not so normal either.",3111
5. Process API,"What it does: given the name of an executable (e .g.,wc), and some arguments (e.g., p3.c ), itloads code (and static data) from that executable and overwrites its current code segment (and curre nt static data) with it; the heap and stack and other parts of the memory spa ce of the program are re-initialized. Then the OS simply runs that p rogram, passing in any arguments as the argv of that process. Thus, it does not create a new process; rather, it transforms the currently runn ing program (formerly p3) into a different running program ( wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns. 5.4 Why? Motivating The API Of course, one big question you might have: why would we build such an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of fork() andexec() is essential in building a U NIX shell, because it lets the shell run code after the call to fork() butbefore the call to exec() ; this code can alter the environment of the about-to-be-run program, and thus enables a va riety of interesting features to be readily built. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 INTERLUDE : PROCESS API TIP: GETTING ITRIGHT (LAMPSON ’SLAW) As Lampson states in his well-regarded “Hints for Computer Syste ms Design” [L83], “ Get it right . Neither abstraction nor simplicity is a sub- stitute for getting it right.” Sometimes, you just have to do the r ight thing, and when you do, it is way better than the alternatives. There ar e lots of ways to design APIs for process creation; however, the combinati on offork() andexec() are simple and immensely powerful. Here, the UNIXdesigners simply got it right. And because Lampson so often “got it right”, we name the law in his honor. The shell is just a user program4. It shows you a prompt and then waits for you to type something into it. You then type a command (i.e ., the name of an executable program, plus any arguments) into it; in most cases, the shell then ﬁgures out where in the ﬁle system the exe cutable resides, calls fork() to create a new child process to run the command, calls some variant of exec() to run the command, and then waits for the command to complete by calling wait() . When the child completes, the shell returns from wait() and prints out a prompt again, ready for your next command. The separation of fork() andexec() allows the shell to do a whole bunch of useful things rather easily. For example: prompt> wc p3.c > newfile.txt In the example above, the output of the program wcisredirected into the output ﬁle newfile.txt (the greater-than sign is how said redirec- tion is indicated). The way the shell accomplishes this task is quite sim- ple: when the child is created, before calling exec() , the shell closes standard output and opens the ﬁle newfile.txt . By doing so, any out- put from the soon-to-be-running program wcare sent to the ﬁle instead of the screen. Figure 5.4 (page 7) shows a program that does exactly this.",3058
5. Process API,"The re ason this redirection works is due to an assumption about how the operati ng system manages ﬁle descriptors. Speciﬁcally, U NIXsystems start looking for free ﬁle descriptors at zero. In this case, STDOUT FILENO will be the ﬁrst available one and thus get assigned when open() is called. Subse- quent writes by the child process to the standard output ﬁle des criptor, for example by routines such as printf() , will then be routed transpar- ently to the newly-opened ﬁle instead of the screen. Here is the output of running the p4.c program: prompt> ./p4 prompt> cat p4.output 32 109 846 p4.c prompt> 4And there are lots of shells; tcsh ,bash , andzsh to name a few. You should pick one, read its man pages, and learn more about it; all U NIXexperts do. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : PROCESS API 7 1#include <stdio.h> 2#include <stdlib.h> 3#include <unistd.h> 4#include <string.h> 5#include <fcntl.h> 6#include <sys/wait.h> 7 8int main(int argc, char *argv[]) { 9int rc = fork(); 10 if (rc < 0) { // fork failed; exit 11 fprintf(stderr, \""fork failed \""); 12 exit(1); 13 } else if (rc == 0) { // child: redirect standard output to a fil e 14 close(STDOUT_FILENO); 15 open(\""./p4.output\"", O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU ); 16 17 // now exec \""wc\""... 18 char*myargs[3]; 19 myargs[0] = strdup(\""wc\""); // program: \""wc\"" (word count) 20 myargs[1] = strdup(\""p4.c\""); // argument: file to count 21 myargs[2] = NULL; // marks end of array 22 execvp(myargs[0], myargs); // runs word count 23 } else { // parent goes down this path (main) 24 int rc_wait = wait(NULL); 25 } 26 return 0; 27} Figure 5.4: All Of The Above With Redirection ( p4.c ) You’ll notice (at least) two interesting tidbits about this outpu t. First, whenp4is run, it looks as if nothing has happened; the shell just prints the command prompt and is immediately ready for your next command. However, that is not the case; the program p4did indeed call fork() to create a new child, and then run the wcprogram via a call to execvp() . You don’t see any output printed to the screen because it has been r edi- rected to the ﬁle p4.output . Second, you can see that when we cat the output ﬁle, all the expected output from running wcis found. Cool, right? UNIX pipes are implemented in a similar way, but with the pipe() system call. In this case, the output of one process is connected to an in- kernel pipe (i.e., queue), and the input of another process is connected to that same pipe; thus, the output of one process seamlessly is us ed as input to the next, and long and useful chains of commands can be st rung together. As a simple example, consider looking for a word in a ﬁle, a nd then counting how many times said word occurs; with pipes and the u til- itiesgrep andwc, it is easy — just type grep -o foo file | wc -l into the command prompt and marvel at the result. Finally, while we just have sketched out the process API at a hig h level, there is a lot more detail about these calls out there to be learned and digested; we’ll learn more, for example, about ﬁle descriptors wh en we talk about ﬁle systems in the third part of the book. For now, sufﬁce i t to say that the fork()/exec() combination is a powerful way to create and manipulate processes.",3271
5. Process API,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 INTERLUDE : PROCESS API ASIDE : RTFM — R EAD THEMANPAGES Many times in this book, when referring to a particular system c all or library call, we’ll tell you to read the manual pages , or man pages for short. Man pages are the original form of documentation that exist on UNIX systems; realize that they were created before the thing call edthe web existed. Spending some time reading man pages is a key step in the growth of a systems programmer; there are tons of useful tidbits hidden in those pages. Some particularly useful pages to read are the man pages for whichever shell you are using (e.g., tcsh, orbash ), and certainly for any system calls your program makes (in order to see what return valu es and error conditions exist). Finally, reading the man pages can save you some embarrassment . When you ask colleagues about some intricacy of fork() , they may simply reply: “RTFM.” This is your colleagues’ way of gently urging you t o Read The Man pages. The F in RTFM just adds a little color to the phrase ... 5.5 Process Control And Users Beyondfork() ,exec() , andwait() , there are a lot of other inter- faces for interacting with processes in U NIX systems. For example, the kill() system call is used to send signals to a process, including di- rectives to pause, die, and other useful imperatives. For conve nience, in most U NIX shells, certain keystroke combinations are conﬁgured to deliver a speciﬁc signal to the currently running process; for example, control-c sends a SIGINT (interrupt) to the process (normally terminating it) and control-z sends a SIGTSTP (stop) signal thus pausing the process in mid-execution (you can resume it later with a command, e.g., t hefg built-in command found in many shells). The entire signals subsystem provides a rich infrastructure to deliver external events to processes, including ways to receive and p rocess those signals within individual processes, and ways to send signal s to individ- ual processes as well as entire process groups . To use this form of com- munication, a process should use the signal() system call to “catch” various signals; doing so ensures that when a particular signa l is deliv- ered to a process, it will suspend its normal execution and run a p articu- lar piece of code in response to the signal. Read elsewhere [SR05] to learn more about signals and their many intricacies. This naturally raises the question: who can send a signal to a p rocess, and who cannot? Generally, the systems we use can have multipl e people using them at the same time; if one of these people can arbitraril y send signals such as SIGINT (to interrupt a process, likely terminating it), the usability and security of the system will be compromised. As a re sult, modern systems include a strong conception of the notion of a user . The user, after entering a password to establish credentials, log s in to gain access to system resources. The user may then launch one or many p ro- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : PROCESS API 9 ASIDE : THESUPERUSER (ROOT ) A system generally needs a user who can administer the system, and is not limited in the way most users are. Such a user should be able to k ill an arbitrary process (e.g., if it is abusing the system in some w ay), even though that process was not started by this user. Such a user should also be able to run powerful commands such as shutdown (which, unsurpris- ingly, shuts down the system). In U NIX-based systems, these special abil- ities are given to the superuser (sometimes called root). While most users can’t kill other users processes, the superuser can.",3687
5. Process API,"Being root is much like being Spider-Man: with great power comes great responsibility [ QI15]. Thus, to increase security (and avoid costly mistakes), it’s usually better to be a regular user; if you do need to be root, tread carefully, as all of the destructive powers of the computing world are now at your ﬁngertips . cesses, and exercise full control over them (pause them, kill th em, etc.). Users generally can only control their own processes; it is the job of the operating system to parcel out resources (such as CPU, memory, an d disk) to each user (and their processes) to meet overall system goals. 5.6 Useful Tools There are many command-line tools that are useful as well. For exa m- ple, using the pscommand allows you to see which processes are run- ning; read the man pages for some useful ﬂags to pass to ps. The tooltop is also quite helpful, as it displays the processes of the syste m and how much CPU and other resources they are eating up. Humorously, many times when you run it, top claims it is the top resource hog; perhaps it is a bit of an egomaniac. The command kill can be used to send arbitrary signals to processes, as can the slightly more user friendly killall . Be sure to use these carefully; if you accidentally kill your wind ow manager, the computer you are sitting in front of may become quite difﬁcult t o use. Finally, there are many different kinds of CPU meters you can us e to get a quick glance understanding of the load on your system; for exa mple, we always keep MenuMeters (from Raging Menace software) running on our Macintosh toolbars, so we can see how much CPU is being utilized at any moment in time. In general, the more information about what i s going on, the better. 5.7 Summary We have introduced some of the APIs dealing with U NIXprocess cre- ation:fork() ,exec() , andwait() . However, we have just skimmed the surface. For more detail, read Stevens and Rago [SR05], of cours e, particularly the chapters on Process Control, Process Relationsh ips, and Signals. There is much to extract from the wisdom therein. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 INTERLUDE : PROCESS API ASIDE : KEYPROCESS API T ERMS •Each process has a name; in most systems, that name is a number known as a process ID (PID). •The fork() system call is used in U NIXsystems to create a new pro- cess. The creator is called the parent ; the newly created process is called the child . As sometimes occurs in real life [J16], the child process is a nearly identical copy of the parent. •The wait() system call allows a parent to wait for its child to com- plete execution. •The exec() family of system calls allows a child to break free from its similarity to its parent and execute an entirely new progr am. •A U NIX shell commonly uses fork() ,wait() , andexec() to launch user commands; the separation of fork and exec enables fea - tures like input/output redirection ,pipes , and other cool features, all without changing anything about the programs being run. •Process control is available in the form of signals , which can cause jobs to stop, continue, or even terminate. •Which processes can be controlled by a particular person is encap - sulated in the notion of a user ; the operating system allows multiple users onto the system, and ensures users can only control their own processes. •Asuperuser can control all processes (and indeed do many other things); this role should be assumed infrequently and with cau tion for security reasons.",3507
5. Process API,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : PROCESS API 11 References [C63] “A Multiprocessor System Design” by Melvin E. Conway. AFIPS ’6 3 Fall Joint Computer Conference. New York, USA 1963 An early paper on how to design multiprocessing systems; may be the ﬁrst place the term fork() was used in the discussion of spawning new processes. [DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis and Earl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966. A classic paper that outlines the basics of multiprogrammed computer systems. Undoubte dly had great inﬂuence on Project MAC, Multics, and eventually UNIX. [J16] “They could be twins.” by Phoebe Jackson-Edwards. The Daily Mail. March 1, 2016. Available: www.dailymail.co.uk/femail/article-3469189/Photos-c hildren- look-IDENTICAL-parents-age-sweep-web.html .This hard-hitting piece of journalism shows a bunch of weirdly similar child/parent photos and is frankly kind of mesm erizing. Go ahead, waste two minutes of your life and check it out. But don’t forget to come back here. Th is, in a microcosm, is the danger of surﬁng the web. [L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems Review, Volume 15:5, October 1983. Lampson’s famous hints on how to design computer systems. You should read it at some point in your life, and probably at many points in your life. [QI15] “With Great Power Comes Great Responsibility” by The Quote Inv estigator. Available: https://quoteinvestigator.com/2015/07/23/great-powe r.The quote investigator concludes that the earliest mention of this concept is 1793, in a collection of decrees made at the French National Convention. The speciﬁc quote: “Ils doivent envisager qu’une gr ande responsabilit est la suite insparable d’un grand pouvoir”, which roughly translates to “They m ust consider that great responsibility follows inseparably from great power.” Only in 1962 di d the following words appear in Spider-Man: “...with great power there must also come–great responsib ility.” So it looks like the French Revolution gets credit for this one, not Stan Lee. Sorry, Stan. [SR05] “Advanced Programming in the U NIX Environment” by W. Richard Stevens, Stephen A. Rago. Addison-Wesley, 2005. All nuances and subtleties of using UNIX APIs are found herein. Buy this book. Read it. And most importantly, live it. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 INTERLUDE : PROCESS API ASIDE : CODING HOMEWORKS Coding homeworks are small exercises where you write code to run on a real machine to get some experience with some basic operating sys- tem APIs. After all, you are (probably) a computer scientist, an d there- fore should like to code, right? (if you don’t, there is always CS the ory, but that’s pretty hard) Of course, to truly become an expert, you h ave to spend more than a little time hacking away at the machine; inde ed, ﬁnd every excuse you can to write some code and see how it works. Spend the time, and become the wise master you know you can be.",3075
5. Process API,"Homework (Code) In this homework, you are to gain some familiarity with the process management APIs about which you just read. Don’t worry – it’s even more fun than it sounds. You’ll in general be much better off if you ﬁn d as much time as you can to write some code, so why not start now? Questions 1. Write a program that calls fork() . Before calling fork() , have the main process access a variable (e.g., x) and set its value to something (e.g., 100). What value is the variable in the child process? What happens t o the vari- able when both the child and parent change the value of x? 2. Write a program that opens a ﬁle (with the open() system call) and then callsfork() to create a new process. Can both the child and parent ac- cess the ﬁle descriptor returned by open() ? What happens when they are writing to the ﬁle concurrently, i.e., at the same time? 3. Write another program using fork() . The child process should print “hello”; the parent process should print “goodbye”. You should try to en sure that the child process always prints ﬁrst; can you do this without calling wait() in the parent? 4. Write a program that calls fork() and then calls some form of exec() to run the program /bin/ls. See if you can try all of the variants of exec() , including (on Linux) execl(), execle(), execlp(), execv(), execvp(), andexecvpe() . Why do you think there are so many variants of the same basic call? 5. Now write a program that uses wait() to wait for the child process to ﬁnish in the parent. What does wait() return? What happens if you use wait() in the child? 6. Write a slight modiﬁcation of the previous program, this time us ingwaitpid() instead of wait() . When would waitpid() be useful? 7. Write a program that creates a child process, and then in th e child closes standard output ( STDOUTFILENO ). What happens if the child calls printf() to print some output after closing the descriptor? 8. Write a program that creates two children, and connects th e standard output of one to the standard input of the other, using the pipe() system call. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2120
6. Direct Execution,"6 Mechanism: Limited Direct Execution In order to virtualize the CPU, the operating system needs to som ehow share the physical CPU among many jobs running seemingly at the same time. The basic idea is simple: run one process for a little while , then run another one, and so forth. By time sharing the CPU in this manner, virtualization is achieved. There are a few challenges, however, in building such virtual ization machinery. The ﬁrst is performance : how can we implement virtualiza- tion without adding excessive overhead to the system? The second is control : how can we run processes efﬁciently while retaining control over the CPU? Control is particularly important to the OS, as it is in ch arge of resources; without control, a process could simply run forever and t ake over the machine, or access information that it should not be allowed to access. Obtaining high performance while maintaining control is thus one of the central challenges in building an operating system. THECRUX: HOWTOEFFICIENTLY VIRTUALIZE THECPU W ITHCONTROL The OS must virtualize the CPU in an efﬁcient manner while ret aining control over the system. To do so, both hardware and operating-syst em support will be required. The OS will often use a judicious bit of h ard- ware support in order to accomplish its work effectively. 6.1 Basic Technique: Limited Direct Execution To make a program run as fast as one might expect, not surprisingl y OS developers came up with a technique, which we call limited direct execution . The “direct execution” part of the idea is simple: just run the program directly on the CPU. Thus, when the OS wishes to start a p ro- gram running, it creates a process entry for it in a process list, allocates some memory for it, loads the program code into memory (from disk), lo- cates its entry point (i.e., the main() routine or something similar), jumps 1 2 MECHANISM : LIMITED DIRECT EXECUTION OS Program Create entry for process list Allocate memory for program Load program into memory Set up stack with argc/argv Clear registers Execute callmain() Run main() Execute return from main Free memory of process Remove from process list Figure 6.1: Direct Execution Protocol (Without Limits) to it, and starts running the user’s code. Figure 6.1 shows this b asic di- rect execution protocol (without any limits, yet), using a normal c all and return to jump to the program’s main() and later to get back into the kernel. Sounds simple, no? But this approach gives rise to a few problems in our quest to virtualize the CPU. The ﬁrst is simple: if we jus t run a program, how can the OS make sure the program doesn’t do anything that we don’t want it to do, while still running it efﬁciently? Th e second: when we are running a process, how does the operating system stop it from running and switch to another process, thus implementing t hetime sharing we require to virtualize the CPU? In answering these questions below, we’ll get a much better sens e of what is needed to virtualize the CPU. In developing these tech niques, we’ll also see where the “limited” part of the name arises from; w ithout limits on running programs, the OS wouldn’t be in control of anything and thus would be “just a library” — a very sad state of affairs for an aspiring operating system.",3291
6. Direct Execution,"6.2 Problem #1: Restricted Operations Direct execution has the obvious advantage of being fast; the prog ram runs natively on the hardware CPU and thus executes as quickly as one would expect. But running on the CPU introduces a problem: what if the process wishes to perform some kind of restricted operation, su ch as issuing an I/O request to a disk, or gaining access to more sys tem resources such as CPU or memory? THECRUX: HOWTOPERFORM RESTRICTED OPERATIONS A process must be able to perform I/O and some other restricted oper - ations, but without giving the process complete control over the sys tem. How can the OS and hardware work together to do so? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 3 ASIDE : W HYSYSTEM CALLS LOOK LIKEPROCEDURE CALLS You may wonder why a call to a system call, such as open() orread() , looks exactly like a typical procedure call in C; that is, if it look s just like a procedure call, how does the system know it’s a system call, and do all the right stuff? The simple reason: it isa procedure call, but hidden in- side that procedure call is the famous trap instruction. More spe ciﬁcally, when you call open() (for example), you are executing a procedure call into the C library. Therein, whether for open() or any of the other sys- tem calls provided, the library uses an agreed-upon calling con vention with the kernel to put the arguments to open in well-known locati ons (e.g., on the stack, or in speciﬁc registers), puts the system- call number into a well-known location as well (again, onto the stack or a regis ter), and then executes the aforementioned trap instruction. The code in the library after the trap unpacks return values and returns cont rol to the program that issued the system call. Thus, the parts of the C lib rary that make system calls are hand-coded in assembly, as they need to c arefully follow convention in order to process arguments and return values c or- rectly, as well as execute the hardware-speciﬁc trap instru ction. And now you know why you personally don’t have to write assembly code to trap into an OS; somebody has already written that assembly for you. One approach would simply be to let any process do whatever it wan ts in terms of I/O and other related operations. However, doing so would prevent the construction of many kinds of systems that are desira ble. For example, if we wish to build a ﬁle system that checks permissi ons before granting access to a ﬁle, we can’t simply let any user process is sue I/Os to the disk; if we did, a process could simply read or write the ent ire disk and thus all protections would be lost. Thus, the approach we take is to introduce a new processor mode, known as user mode ; code that runs in user mode is restricted in what it can do. For example, when running in user mode, a process can’t issu e I/O requests; doing so would result in the processor raising an ex ception; the OS would then likely kill the process. In contrast to user mode is kernel mode , which the operating system (or kernel) runs in.",3087
6. Direct Execution,"In this mode, code that runs can do what it lik es, in- cluding privileged operations such as issuing I/O requests an d executing all types of restricted instructions. We are still left with a challenge, however: what should a user p ro- cess do when it wishes to perform some kind of privileged operation , such as reading from disk? To enable this, virtually all modern hard- ware provides the ability for user programs to perform a system call . Pioneered on ancient machines such as the Atlas [K+61,L78], sy stem calls allow the kernel to carefully expose certain key pieces of funct ionality to user programs, such as accessing the ﬁle system, creating and destroy- ing processes, communicating with other processes, and allocati ng more c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 MECHANISM : LIMITED DIRECT EXECUTION TIP: USEPROTECTED CONTROL TRANSFER The hardware assists the OS by providing different modes of exec ution. Inuser mode , applications do not have full access to hardware resources. Inkernel mode , the OS has access to the full resources of the machine. Special instructions to trap into the kernel and return-from-trap back to user-mode programs are also provided, as well as instructions th at allow the OS to tell the hardware where the trap table resides in memory. memory. Most operating systems provide a few hundred calls (see t he POSIX standard for details [P10]); early Unix systems exposed a more concise subset of around twenty calls. To execute a system call, a program must execute a special trap instruc- tion. This instruction simultaneously jumps into the kernel an d raises the privilege level to kernel mode; once in the kernel, the system c an now per- form whatever privileged operations are needed (if allowed), an d thus do the required work for the calling process. When ﬁnished, the OS c alls a special return-from-trap instruction, which, as you might expect, returns into the calling user program while simultaneously reducing t he privi- lege level back to user mode. The hardware needs to be a bit careful when executing a trap, i n that it must make sure to save enough of the caller’s registers in order to be able to return correctly when the OS issues the return-from-trap in struction. On x86, for example, the processor will push the program counter, ﬂ ags, and a few other registers onto a per-process kernel stack ; the return-from- trap will pop these values off the stack and resume execution of th e user- mode program (see the Intel systems manuals [I11] for details). Other hardware systems use different conventions, but the basic conc epts are similar across platforms. There is one important detail left out of this discussion: how does th e trap know which code to run inside the OS? Clearly, the calling pr ocess can’t specify an address to jump to (as you would when making a pro- cedure call); doing so would allow programs to jump anywhere into the kernel which clearly is a Very Bad Idea1. Thus the kernel must carefully control what code executes upon a trap. The kernel does so by setting up a trap table at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to conﬁgure machine hardware as need be. One of the ﬁrst things t he OS thus does is to tell the hardware what code to run when certain ex cep- tional events occur. For example, what code should run when a hard- disk interrupt takes place, when a keyboard interrupt occurs, or when a program makes a system call?",3506
6. Direct Execution,"The OS informs the hardware of the locations of these trap handlers , usually with some kind of special in- 1Imagine jumping into code to access a ﬁle, but just after a permission check; in fact, it is likely such an ability would enable a wily programmer to get the k ernel to run arbitrary code sequences [S07]. In general, try to avoid Very Bad Ideas like this one . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 5 OS @ boot Hardware (kernel mode) initialize trap table remember address of... syscall handler OS @ run Hardware Program (kernel mode) (user mode) Create entry for process list Allocate memory for program Load program into memory Setup user stack with argv Fill kernel stack with reg/PC return-from-trap restore regs from kernel stack move to user mode jump to main Run main() ... Call system call trap into OS save regs to kernel stack move to kernel mode jump to trap handler Handle trap Do work of syscall return-from-trap restore regs from kernel stack move to user mode jump to PC after trap ... return from main trap (viaexit() ) Free memory of process Remove from process list Figure 6.2: Limited Direct Execution Protocol struction. Once the hardware is informed, it remembers the loca tion of these handlers until the machine is next rebooted, and thus the hardware knows what to do (i.e., what code to jump to) when system calls and other exceptional events take place. To specify the exact system call, a system-call number is usually as- signed to each system call. The user code is thus responsible for placing the desired system-call number in a register or at a speciﬁed l ocation on the stack; the OS, when handling the system call inside the tra p handler, examines this number, ensures it is valid, and, if it is, exec utes the corre- sponding code. This level of indirection serves as a form of protection ; user code cannot specify an exact address to jump to, but rather m ust request a particular service via number. One last aside: being able to execute the instruction to tell t he hard- ware where the trap tables are is a very powerful capability. T hus, as you might have guessed, it is also a privileged operation. If you try to exe- cute this instruction in user mode, the hardware won’t let you, and you c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 MECHANISM : LIMITED DIRECT EXECUTION TIP: BEWARY OFUSERINPUTS INSECURE SYSTEMS Even though we have taken great pains to protect the OS during sy stem calls (by adding a hardware trapping mechanism, and ensurin g all calls to the OS are routed through it), there are still many other aspects to imple- menting a secure operating system that we must consider. One of these is the handling of arguments at the system call boundary; the OS must check what the user passes in and ensure that arguments are pr operly speciﬁed, or otherwise reject the call. For example, with a write() system call, the user speciﬁes an address of a buffer as a source of the write call. If the user (either accid entally or maliciously) passes in a “bad” address (e.g., one inside the k ernel’s portion of the address space), the OS must detect this and reject the call.",3197
6. Direct Execution,"Otherwise, it would be possible for a user to read all of kernel mem ory; given that kernel (virtual) memory also usually includes all of the physi- cal memory of the system, this small slip would enable a program to read the memory of any other process in the system. In general, a secure system must treat user inputs with great suspicion. Not doing so will undoubtedly lead to easily hacked software, a de spair- ing sense that the world is an unsafe and scary place, and the los s of job security for the all-too-trusting OS developer. can probably guess what will happen (hint: adios, offending prog ram). Point to ponder: what horrible things could you do to a system if you could install your own trap table? Could you take over the machine? The timeline (with time increasing downward, in Figure 6.2) s umma- rizes the protocol. We assume each process has a kernel stack wher e reg- isters (including general purpose registers and the program c ounter) are saved to and restored from (by the hardware) when transitioning into and out of the kernel. There are two phases in the limited direct execution ( LDE ) protocol. In the ﬁrst (at boot time), the kernel initializes the trap tabl e, and the CPU remembers its location for subsequent use. The kernel does so via a privileged instruction (all privileged instructions are hig hlighted in bold). In the second (when running a process), the kernel sets up a few t hings (e.g., allocating a node on the process list, allocating memory) be fore us- ing a return-from-trap instruction to start the execution of the process; this switches the CPU to user mode and begins running the proces s. When the process wishes to issue a system call, it traps back in to the OS, which handles it and once again returns control via a return-from -trap to the process. The process then completes its work, and returns f rom main() ; this usually will return into some stub code which will properl y exit the program (say, by calling the exit() system call, which traps into the OS). At this point, the OS cleans up and we are done. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 7 6.3 Problem #2: Switching Between Processes The next problem with direct execution is achieving a switch be tween processes. Switching between processes should be simple, right ? The OS should just decide to stop one process and start another. What’s t he big deal? But it actually is a little bit tricky: speciﬁcally , if a process is running on the CPU, this by deﬁnition means the OS is notrunning. If the OS is not running, how can it do anything at all? (hint: it can ’t) While this sounds almost philosophical, it is a real problem: there is cl early no way for the OS to take an action if it is not running on the CPU. Thus w e arrive at the crux of the problem. THECRUX: HOWTOREGAIN CONTROL OFTHECPU How can the operating system regain control of the CPU so that it can switch between processes? A Cooperative Approach: Wait For System Calls One approach that some systems have taken in the past (for exampl e, early versions of the Macintosh operating system [M11], or the old X erox Alto system [A79]) is known as the cooperative approach. In this style, the OS trusts the processes of the system to behave reasonably. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task. Thus, you might ask, how does a friendly process give up the CPU in this utopian world? Most processes, as it turns out, transfer contr ol of the CPU to the OS quite frequently by making system calls , for example, to open a ﬁle and subsequently read it, or to send a message to anot her machine, or to create a new process. Systems like this often inclu de an explicit yield system call, which does nothing except to transfer control to the OS so it can run other processes. Applications also transfer control to the OS when they do somethi ng illegal. For example, if an application divides by zero, or tries to access memory that it shouldn’t be able to access, it will generate a trap to the OS. The OS will then have control of the CPU again (and likely termi nate the offending process). TIP: DEALING WITHAPPLICATION MISBEHAVIOR Operating systems often have to deal with misbehaving process es, those that either through design (maliciousness) or accident (bugs) attempt to do something that they shouldn’t.",4430
6. Direct Execution,"In modern systems, the way the O S tries to handle such malfeasance is to simply terminate the of fender. One strike and you’re out. Perhaps brutal, but what else should the OS do when you try to access memory illegally or execute an illegal ins truction? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 MECHANISM : LIMITED DIRECT EXECUTION Thus, in a cooperative scheduling system, the OS regains control of the CPU by waiting for a system call or an illegal operation of some ki nd to take place. You might also be thinking: isn’t this passive ap proach less than ideal? What happens, for example, if a process (whether ma licious, or just full of bugs) ends up in an inﬁnite loop, and never makes a sy stem call? What can the OS do then? A Non-Cooperative Approach: The OS Takes Control Without some additional help from the hardware, it turns out the OS can’t do much at all when a process refuses to make system calls (or mis takes) and thus return control to the OS. In fact, in the cooperative approa ch, your only recourse when a process gets stuck in an inﬁnite loop is to resort to the age-old solution to all problems in computer systems: reboot the machine . Thus, we again arrive at a subproblem of our general quest to gain control of the CPU. THECRUX: HOWTOGAIN CONTROL WITHOUT COOPERATION How can the OS gain control of the CPU even if processes are not being cooperative? What can the OS do to ensure a rogue process does not tak e over the machine? The answer turns out to be simple and was discovered by a number of people building computer systems many years ago: a timer interrupt [M+63]. A timer device can be programmed to raise an interrupt every so many milliseconds; when the interrupt is raised, the curre ntly running process is halted, and a pre-conﬁgured interrupt handler in the OS runs. At this point, the OS has regained control of the CPU, and thus can d o what it pleases: stop the current process, and start a differen t one. As we discussed before with system calls, the OS must inform the hardware of which code to run when the timer interrupt occurs; th us, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is of course a privi leged operation. Once the timer has begun, the OS can thus feel safe in that control will eventually be returned to it, and thus the OS is fre e to run user programs. The timer can also be turned off (also a privileg ed opera- tion), something we will discuss later when we understand concu rrency in more detail. TIP: USETHETIMER INTERRUPT TOREGAIN CONTROL The addition of a timer interrupt gives the OS the ability to run again on a CPU even if processes act in a non-cooperative fashion. Thus, th is hardware feature is essential in helping the OS maintain cont rol of the machine. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 9 TIP: REBOOT ISUSEFUL Earlier on, we noted that the only solution to inﬁnite loops (and simi lar behaviors) under cooperative preemption is to reboot the machine. While you may scoff at this hack, researchers have shown that reboot (or in gen- eral, starting over some piece of software) can be a hugely useful tool in building robust systems [C+04]. Speciﬁcally, reboot is useful because it moves software back to a k nown and likely more tested state. Reboots also reclaim stale or leake d re- sources (e.g., memory) which may otherwise be hard to handle. Fi nally, reboots are easy to automate. For all of these reasons, it is not uncomm on in large-scale cluster Internet services for system managem ent software to periodically reboot sets of machines in order to reset them and t hus obtain the advantages listed above.",3740
6. Direct Execution,"Thus, next time you reboot, you are not just enacting some ugly hack. Rather, you are using a time-tested approach to improving the be havior of a computer system. Well done. Note that the hardware has some responsibility when an interrup t oc- curs, in particular to save enough of the state of the program that was running when the interrupt occurred such that a subsequent re turn-from- trap instruction will be able to resume the running program corr ectly. This set of actions is quite similar to the behavior of the hardwar e during an explicit system-call trap into the kernel, with various re gisters thus getting saved (e.g., onto a kernel stack) and thus easily rest ored by the return-from-trap instruction. Saving and Restoring Context Now that the OS has regained control, whether cooperatively via a s ys- tem call, or more forcefully via a timer interrupt, a decision has to be made: whether to continue running the currently-running proc ess, or switch to a different one. This decision is made by a part of the ope rating system known as the scheduler ; we will discuss scheduling policies in great detail in the next few chapters. If the decision is made to switch, the OS then executes a low-lev el piece of code which we refer to as a context switch . A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process (onto its kernel stack, for example) and restore a few for the soon-to-be-executing process (from its ker nel stack). By doing so, the OS thus ensures that when the return-fr om-trap instruction is ﬁnally executed, instead of returning to the pr ocess that was running, the system resumes execution of another process. To save the context of the currently-running process, the OS wil l ex- ecute some low-level assembly code to save the general purpose re gis- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 MECHANISM : LIMITED DIRECT EXECUTION OS @ boot Hardware (kernel mode) initialize trap table remember addresses of... syscall handler timer handler start interrupt timer start timer interrupt CPU in X ms OS @ run Hardware Program (kernel mode) (user mode) Process A ... timer interrupt save regs(A) to k-stack(A) move to kernel mode jump to trap handler Handle the trap Callswitch() routine save regs(A) to proc-struct(A) restore regs(B) from proc-struct(B) switch to k-stack(B) return-from-trap (into B) restore regs(B) from k-stack(B) move to user mode jump to B’s PC Process B ... Figure 6.3: Limited Direct Execution Protocol (Timer Interrupt) ters, PC, and the kernel stack pointer of the currently-runnin g process, and then restore said registers, PC, and switch to the kernel s tack for the soon-to-be-executing process. By switching stacks, the kernel enters the call to the switch code in the context of one process (the one that was in- terrupted) and returns in the context of another (the soon-to-be-e xecuting one). When the OS then ﬁnally executes a return-from-trap inst ruction, the soon-to-be-executing process becomes the currently-runnin g process.",3088
6. Direct Execution,"And thus the context switch is complete. A timeline of the entire process is shown in Figure 6.3. In this ex ample, Process A is running and then is interrupted by the timer inter rupt. The hardware saves its registers (onto its kernel stack) and ente rs the kernel (switching to kernel mode). In the timer interrupt handler, t he OS decides to switch from running Process A to Process B. At that point, it cal ls the switch() routine, which carefully saves current register values (int o the process structure of A), restores the registers of Process B (from i ts process structure entry), and then switches contexts , speciﬁcally by changing the stack pointer to use B’s kernel stack (and not A’s). Finally, the O S returns- from-trap, which restores B’s registers and starts running it. Note that there are two types of register saves/restores that ha ppen during this protocol. The ﬁrst is when the timer interrupt occurs ; in this OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 11 1# void swtch(struct context **old, struct context *new); 2# 3# Save current register context in old 4# and then load register context from new. 5.globl swtch 6swtch: 7# Save old registers 8movl 4( percentesp),  percenteax # put old ptr into eax 9popl 0( percenteax) # save the old IP 10movl  percentesp, 4( percenteax) # and stack 11movl  percentebx, 8( percenteax) # and other registers 12movl  percentecx, 12( percenteax) 13movl  percentedx, 16( percenteax) 14movl  percentesi, 20( percenteax) 15movl  percentedi, 24( percenteax) 16movl  percentebp, 28( percenteax) 17 18# Load new registers 19movl 4( percentesp),  percenteax # put new ptr into eax 20movl 28( percenteax),  percentebp # restore other registers 21movl 24( percenteax),  percentedi 22movl 20( percenteax),  percentesi 23movl 16( percenteax),  percentedx 24movl 12( percenteax),  percentecx 25movl 8( percenteax),  percentebx 26movl 4( percenteax),  percentesp # stack is switched here 27pushl 0( percenteax) # return addr put in place 28ret # finally return into new ctxt Figure 6.4: The xv6 Context Switch Code case, the user registers of the running process are implicitly saved by the hardware , using the kernel stack of that process. The second is when the OS decides to switch from A to B; in this case, the kernel registers are ex- plicitly saved by the software (i.e., the OS), but this time into memory in the process structure of the process. The latter action moves the s ystem from running as if it just trapped into the kernel from A to as if i t just trapped into the kernel from B. To give you a better sense of how such a switch is enacted, Figure 6 .4 shows the context switch code for xv6. See if you can make sense of it (you’ll have to know a bit of x86, as well as some xv6, to do so). The context structures old andnew are found in the old and new process’s process structures, respectively. 6.4 Worried About Concurrency? Some of you, as attentive and thoughtful readers, may be now think- ing: “Hmm...",3019
6. Direct Execution,"what happens when, during a system call, a timer interrupt occurs?” or “What happens when you’re handling one interrupt and a n- other one happens? Doesn’t that get hard to handle in the kernel?” Good questions — we really have some hope for you yet. The answer is yes, the OS does indeed need to be concerned as to wh at c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 MECHANISM : LIMITED DIRECT EXECUTION ASIDE : HOWLONG CONTEXT SWITCHES TAKE A natural question you might have is: how long does something like a context switch take? Or even a system call? For those of you that are cu- rious, there is a tool called lmbench [MS96] that measures exactly those things, as well as a few other performance measures that might b e rele- vant. Results have improved quite a bit over time, roughly tracking pr ocessor performance. For example, in 1996 running Linux 1.3.37 on a 200- MHz P6 CPU, system calls took roughly 4 microseconds, and a context swit ch roughly 6 microseconds [MS96]. Modern systems perform almost an or- der of magnitude better, with sub-microsecond results on system s with 2- or 3-GHz processors. It should be noted that not all operating-system actions track CPU per- formance. As Ousterhout observed, many OS operations are memory intensive, and memory bandwidth has not improved as dramatical ly as processor speed over time [O90]. Thus, depending on your workload, buying the latest and greatest processor may not speed up your OS a s much as you might hope. happens if, during interrupt or trap handling, another interr upt occurs. This, in fact, is the exact topic of the entire second piece of this book, on concurrency ; we’ll defer a detailed discussion until then. To whet your appetite, we’ll just sketch some basics of how the OS handles these tricky situations. One simple thing an OS might do is dis- able interrupts during interrupt processing; doing so ensures that when one interrupt is being handled, no other one will be delivered to the CPU. Of course, the OS has to be careful in doing so; disabling interru pts for too long could lead to lost interrupts, which is (in technical ter ms) bad. Operating systems also have developed a number of sophisticate d locking schemes to protect concurrent access to internal data structu res. This enables multiple activities to be on-going within the ker nel at the same time, particularly useful on multiprocessors. As we’ll see in the next piece of this book on concurrency, though, such locking can be com - plicated and lead to a variety of interesting and hard-to-ﬁnd b ugs. 6.5 Summary We have described some key low-level mechanisms to implement C PU virtualization, a set of techniques which we collectively refe r to as limited direct execution . The basic idea is straightforward: just run the program you want to run on the CPU, but ﬁrst make sure to set up the hardwar e so as to limit what the process can do without OS assistance. This general approach is taken in real life as well. For example , those of you who have children, or, at least, have heard of children, may be familiar with the concept of baby prooﬁng a room: locking cabinets con- taining dangerous stuff and covering electrical sockets.",3210
6. Direct Execution,"When the room is OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 13 ASIDE : KEYCPU V IRTUALIZATION TERMS (MECHANISMS ) •The CPU should support at least two modes of execution: a re- stricted user mode and a privileged (non-restricted) kernel mode . •Typical user applications run in user mode, and use a system call totrap into the kernel to request operating system services. •The trap instruction saves register state carefully, change s the hard- ware status to kernel mode, and jumps into the OS to a pre-speci ﬁed destination: the trap table . •When the OS ﬁnishes servicing a system call, it returns to the user program via another special return-from-trap instruction, which re- duces privilege and returns control to the instruction after th e trap that jumped into the OS. •The trap tables must be set up by the OS at boot time, and make sure that they cannot be readily modiﬁed by user programs. All of this is part of the limited direct execution protocol which runs programs efﬁciently but without loss of OS control. •Once a program is running, the OS must use hardware mechanisms to ensure the user program does not run forever, namely the timer interrupt . This approach is a non-cooperative approach to CPU scheduling. •Sometimes the OS, during a timer interrupt or system call, might wish to switch from running the current process to a different on e, a low-level technique known as a context switch . thus readied, you can let your baby roam freely, secure in the know ledge that the most dangerous aspects of the room have been restricted. In an analogous manner, the OS “baby proofs” the CPU, by ﬁrst (dur- ing boot time) setting up the trap handlers and starting an inte rrupt timer, and then by only running processes in a restricted mode. By doing s o, the OS can feel quite assured that processes can run efﬁciently, on ly requir- ing OS intervention to perform privileged operations or when they have monopolized the CPU for too long and thus need to be switched out. We thus have the basic mechanisms for virtualizing the CPU in p lace. But a major question is left unanswered: which process should we r un at a given time? It is this question that the scheduler must answe r, and thus the next topic of our study. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 MECHANISM : LIMITED DIRECT EXECUTION References [A79] “Alto User’s Handbook” by Xerox. Xerox Palo Alto Research C enter, September 1979. Available: http://history-computer.com/Library/AltoUsersHandbo ok.pdf .An amazing system, way ahead of its time. Became famous because Steve Jobs visited, took notes, and built Lisa and eventually Mac. [C+04] “Microreboot — A Technique for Cheap Recovery” by G. Candea, S. Ka wamoto, Y. Fujiki, G. Friedman, A. Fox. OSDI ’04, San Francisco, CA, December 2 004. An excellent paper pointing out how far one can go with reboot in building more robust systems. [I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual” b y Volume 3A and 3B: System Programming Guide.",3050
6. Direct Execution,"Intel Corporation, January 2011. This is just a boring manual, but sometimes those are useful. [K+61] “One-Level Storage System” by T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner. IRE Transactions on Electronic Computers, April 1962. The Atlas pioneered much of what you see in modern systems. However, this paper is not the best one to read. If you wer e to only read one, you might try the historical perspective below [L78]. [L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com- munications of the ACM, 21:1, January 1978. A history of the early development of computers and the pioneering efforts of Atlas. [M+63] “A Time-Sharing Debugging System for a Small Computer” by J. McCa rthy, S. Boilen, E. Fredkin, J. C. R. Licklider. AFIPS ’63 (Spring), May, 1963, New York , USA. An early paper about time-sharing that refers to using a timer interrupt; the quote that discu sses it: “The basic task of the channel 17 clock routine is to decide whether to remove the current us er from core and if so to decide which user program to swap in as he goes out.” [MS96] “lmbench: Portable tools for performance analysis” by Larry McVoy a nd Carl Staelin. USENIX Annual Technical Conference, January 1996. A fun paper about how to measure a number of different things about your OS and its performance. Download lmbench and give it a try. [M11] “Mac OS 9” by Apple Computer, Inc.. January 2011. http://en.wikipedia.org/wiki/ MacOS9.You can probably even ﬁnd an OS 9 emulator out there if you want to; check it out, it’ s a fun little Mac. [O90] “Why Aren’t Operating Systems Getting Faster as Fast as Hardwa re?” by J. Ouster- hout. USENIX Summer Conference, June 1990. A classic paper on the nature of operating system performance. [P10] “The Single UNIX Speciﬁcation, Version 3” by The Open Group, May 2010 . Available: http://www.unix.org/version3/ .This is hard and painful to read, so probably avoid it if you can. Like, unless someone is paying you to read it. Or, you’re just so curi ous you can’t help it. [S07] “The Geometry of Innocent Flesh on the Bone: Return-into-libc without Function Calls (on the x86)” by Hovav Shacham. CCS ’07, October 2007. One of those awesome, mind-blowing ideas that you’ll see in research from time to time. The author shows that if you c an jump into code arbitrarily, you can essentially stitch together any code sequence you like (gi ven a large code base); read the paper for the details. The technique makes it even harder to defend again st malicious attacks, alas. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : LIMITED DIRECT EXECUTION 15 Homework (Measurement) ASIDE : M EASUREMENT HOMEWORKS Measurement homeworks are small exercises where you write code t o run on a real machine, in order to measure some aspect of OS or hardwa re performance. The idea behind such homeworks is to give you a littl e bit of hands-on experience with a real operating system. In this homework, you’ll measure the costs of a system call and contex t switch. Measuring the cost of a system call is relatively easy. For example, you could repeatedly call a simple system call (e.g., performin g a 0-byte read), and time how long it takes; dividing the time by the numbe r of iterations gives you an estimate of the cost of a system call.",3319
6. Direct Execution,"One thing you’ll have to take into account is the precision and acc u- racy of your timer. A typical timer that you can use is gettimeofday() ; read the man page for details. What you’ll see there is that gettimeofday() returns the time in microseconds since 1970; however, this does n ot mean that the timer is precise to the microsecond. Measure back-to-b ack calls togettimeofday() to learn something about how precise the timer re- ally is; this will tell you how many iterations of your null system- call test you’ll have to run in order to get a good measurement result. If gettimeofday() is not precise enough for you, you might look into using therdtsc instruction available on x86 machines. Measuring the cost of a context switch is a little trickier. The l mbench benchmark does so by running two processes on a single CPU, and se t- ting up two U NIX pipes between them; a pipe is just one of many ways processes in a U NIXsystem can communicate with one another. The ﬁrst process then issues a write to the ﬁrst pipe, and waits for a read on the second; upon seeing the ﬁrst process waiting for something to read from the second pipe, the OS puts the ﬁrst process in the blocked state , and switches to the other process, which reads from the ﬁrst pipe and then writes to the second. When the second process tries to read from th e ﬁrst pipe again, it blocks, and thus the back-and-forth cycle of commu nication continues. By measuring the cost of communicating like this repe atedly, lmbench can make a good estimate of the cost of a context switch. You can try to re-create something similar here, using pipes, or pe rhaps some other communication mechanism such as U NIXsockets. One difﬁculty in measuring context-switch cost arises in syst ems with more than one CPU; what you need to do on such a system is ensure that your context-switching processes are located on the same processor . For- tunately, most operating systems have calls to bind a process to a partic- ular processor; on Linux, for example, the schedsetaffinity() call is what you’re looking for. By ensuring both processes are on the same processor, you are making sure to measure the cost of the OS stopping one process and restoring another on the same CPU. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",2300
8. Multi-level Feedback,"8 Scheduling: The Multi-Level Feedback Queue In this chapter, we’ll tackle the problem of developing one of the mos t well-known approaches to scheduling, known as the Multi-level Feed- back Queue (MLFQ) . The Multi-level Feedback Queue (MLFQ) sched- uler was ﬁrst described by Corbato et al. in 1962 [C+62] in a sys tem known as the Compatible Time-Sharing System (CTSS), and this work, along with later work on Multics, led the ACM to award Corbato its highest honor, the T uring Award . The scheduler has subsequently been reﬁned throughout the years to the implementations you will encou nter in some modern systems. The fundamental problem MLFQ tries to address is two-fold. Firs t, it would like to optimize turnaround time , which, as we saw in the previous note, is done by running shorter jobs ﬁrst; unfortunately, the OS d oesn’t generally know how long a job will run for, exactly the knowledge tha t algorithms like SJF (or STCF) require. Second, MLFQ would like to mak e a system feel responsive to interactive users (i.e., users si tting and staring at the screen, waiting for a process to ﬁnish), and thus minimiz eresponse time; unfortunately, algorithms like Round Robin reduce response tim e but are terrible for turnaround time. Thus, our problem: given th at we in general do not know anything about a process, how can we build a scheduler to achieve these goals? How can the scheduler learn, as the system runs, the characteristics of the jobs it is running, and thus make better scheduling decisions? THECRUX: HOWTOSCHEDULE WITHOUT PERFECT KNOWLEDGE ? How can we design a scheduler that both minimizes response time f or interactive jobs while also minimizing turnaround time withou ta priori knowledge of job length? 1 2SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE TIP: LEARN FROM HISTORY The multi-level feedback queue is an excellent example of a sy stem that learns from the past to predict the future. Such approaches are c om- mon in operating systems (and many other places in Computer Scienc e, including hardware branch predictors and caching algorithms ). Such approaches work when jobs have phases of behavior and are thus pre- dictable; of course, one must be careful with such techniques, a s they can easily be wrong and drive a system to make worse decisions than th ey would have with no knowledge at all. 8.1 MLFQ: Basic Rules To build such a scheduler, in this chapter we will describe th e basic algorithms behind a multi-level feedback queue; although the speciﬁcs of many implemented MLFQs differ [E95], most approaches are simi lar. In our treatment, the MLFQ has a number of distinct queues , each assigned a different priority level . At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run. Of course, more than one job may be on a given queue, and thus have thesame priority. In this case, we will just use round-robin scheduling among those jobs.",3051
8. Multi-level Feedback,"Thus, we arrive at the ﬁrst two basic rules for MLFQ: •Rule 1: If Priority(A) >Priority(B), A runs (B doesn’t). •Rule 2: If Priority(A) =Priority(B), A & B run in RR. The key to MLFQ scheduling therefore lies in how the scheduler s ets priorities. Rather than giving a ﬁxed priority to each job, MLFQ varies the priority of a job based on its observed behavior . If, for example, a job repeatedly relinquishes the CPU while waiting for input from t he key- board, MLFQ will keep its priority high, as this is how an interac tive process might behave. If, instead, a job uses the CPU intensive ly for long periods of time, MLFQ will reduce its priority. In this way, MLFQ will try tolearn about processes as they run, and thus use the history of the job to predict its future behavior. If we were to put forth a picture of what the queues might look like a t a given instant, we might see something like the following (Figu re 8.1). In the ﬁgure, two jobs (A and B) are at the highest priority level , while job C is in the middle and Job D is at the lowest priority. Given our curr ent knowledge of how MLFQ works, the scheduler would just alternate ti me slices between A and B because they are the highest priority job s in the system; poor jobs C and D would never even get to run — an outrage. Of course, just showing a static snapshot of some queues does not re- ally give you an idea of how MLFQ works. What we need is to under- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE 3 Q1Q2Q3Q4Q5Q6Q7Q8 [Low Priority][High Priority] DCA B Figure 8.1: MLFQ Example stand how job priority changes over time. And that, in a surprise only to those who are reading a chapter from this book for the ﬁrst time, i s exactly what we will do next. 8.2 Attempt #1: How To Change Priority We now must decide how MLFQ is going to change the priority level of a job (and thus which queue it is on) over the lifetime of a job. To do this, we must keep in mind our workload: a mix of interactive jobs th at are short-running (and may frequently relinquish the CPU), a nd some longer-running “CPU-bound” jobs that need a lot of CPU time but whe re response time isn’t important. Here is our ﬁrst attempt at a priori ty- adjustment algorithm: •Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). •Rule 4a: If a job uses up an entire time slice while running, its pri- ority is reduced (i.e., it moves down one queue). •Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level. Example 1: A Single Long-Running Job Let’s look at some examples. First, we’ll look at what happens when th ere has been a long running job in the system. Figure 8.2 shows what ha ppens to this job over time in a three-queue scheduler. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE Q2 Q1 Q0 0 50 100 150 200 Figure 8.2: Long-running Job Over Time As you can see in the example, the job enters at the highest priori ty (Q2). After a single time-slice of 10 ms, the scheduler reduce s the job’s priority by one, and thus the job is on Q1. After running at Q1 for a ti me slice, the job is ﬁnally lowered to the lowest priority in the syst em (Q0), where it remains.",3309
8. Multi-level Feedback,"Pretty simple, no? Example 2: Along Came A Short Job Now let’s look at a more complicated example, and hopefully see how MLFQ tries to approximate SJF. In this example, there are two job s: A, which is a long-running CPU-intensive job, and B, which is a shor t-running interactive job. Assume A has been running for some time, and the n B ar- rives. What will happen? Will MLFQ approximate SJF for B? Figure 8.3 plots the results of this scenario. A (shown in black) i s run- ning along in the lowest-priority queue (as would any long-runnin g CPU- intensive jobs); B (shown in gray) arrives at time T= 100 , and thus is Q0Q1Q2 0 50 100 150 200 Figure 8.3: Along Came An Interactive Job OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE 5 Q0Q1Q2 0 50 100 150 200 Figure 8.4: A Mixed I/O-intensive and CPU-intensive Workload inserted into the highest queue; as its run-time is short (only 20 ms), B completes before reaching the bottom queue, in two time slices; t hen A resumes running (at low priority). From this example, you can hopefully understand one of the major goals of the algorithm: because it doesn’t know whether a job will be a short job or a long-running job, it ﬁrst assumes it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queu es, and thus soon prove itself to be a long-running more batch-like proc ess. In this manner, MLFQ approximates SJF. Example 3: What About I/O? Let’s now look at an example with some I/O. As Rule 4b states above, if a process gives up the processor before using up its time slice, we k eep it at the same priority level. The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say by waiting for user input f rom the keyboard or mouse), it will relinquish the CPU before its time slice is complete; in such case, we don’t wish to penalize the job and thus s imply keep it at the same level. Figure 8.4 shows an example of how this works, with an interactive job B (shown in gray) that needs the CPU only for 1 ms before performing a n I/O competing for the CPU with a long-running batch job A (shown in black). The MLFQ approach keeps B at the highest priority becau se B keeps releasing the CPU; if B is an interactive job, MLFQ furth er achieves its goal of running interactive jobs quickly. Problems With Our Current MLFQ We thus have a basic MLFQ. It seems to do a fairly good job, sharing the CPU fairly between long-running jobs, and letting short or I/O-i ntensive interactive jobs run quickly. Unfortunately, the approach we h ave devel- oped thus far contains serious ﬂaws. Can you think of any? (This is where you pause and think as deviously as you can) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE Q0Q1Q2 0 50 100 150 200Q0Q1Q2 0 50 100 150 200 Boost Boost Boost Boost Figure 8.5: Without (Left) and With (Right) Priority Boost First, there is the problem of starvation : if there are “too many” in- teractive jobs in the system, they will combine to consume allCPU time, and thus long-running jobs will never receive any CPU time (they starve ).",3272
8. Multi-level Feedback,"We’d like to make some progress on these jobs even in this scenario. Second, a smart user could rewrite their program to game the sched- uler. Gaming the scheduler generally refers to the idea of doing some - thing sneaky to trick the scheduler into giving you more than you r fair share of the resource. The algorithm we have described is suscep tible to the following attack: before the time slice is over, issue an I/O op eration (to some ﬁle you don’t care about) and thus relinquish the CPU; doing so allows you to remain in the same queue, and thus gain a higher per cent- age of CPU time. When done right (e.g., by running for 99 percent of a time s lice before relinquishing the CPU), a job could nearly monopolize the CP U. Finally, a program may change its behavior over time; what was CPU- bound may transition to a phase of interactivity. With our curren t ap- proach, such a job would be out of luck and not be treated like the other interactive jobs in the system. TIP: SCHEDULING MUST BESECURE FROM ATTACK You might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distri buted stor- age system’s I/O request handling [Y+18]), is not a security concern, but in increasingly many cases, it is exactly that. Consider the m odern dat- acenter, in which users from around the world share CPUs, memorie s, networks, and storage systems; without care in policy design and en- forcement, a single user may be able to adversely harm others an d gain advantage for itself. Thus, scheduling policy forms an importan t part of the security of a system, and should be carefully constructed. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE 7 Q0Q1Q2 0 50 100 150 200 250 300Q0Q1Q2 0 50 100 150 200 250 300 Figure 8.6: Without (Left) and With (Right) Gaming Tolerance 8.3 Attempt #2: The Priority Boost Let’s try to change the rules and see if we can avoid the problem of starvation. What could we do in order to guarantee that CPU-bound jobs will make some progress (even if it is not much?). The simple idea here is to periodically boost the priority of all the jobs in system. There are many ways to achieve this, but let’s just d o some- thing simple: throw them all in the topmost queue; hence, a new ru le: •Rule 5: After some time period S, move all the jobs in the system to the topmost queue. Our new rule solves two problems at once. First, processes are gua r- anteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus ev entu- ally receive service. Second, if a CPU-bound job has become intera ctive, the scheduler treats it properly once it has received the priori ty boost. Let’s see an example. In this scenario, we just show the behavior of a long-running job when competing for the CPU with two short-runni ng interactive jobs. Two graphs are shown in Figure 8.5 (page 6). O n the left, there is no priority boost, and thus the long-running job gets star ved once the two short jobs arrive; on the right, there is a priority boost eve ry 50 ms (which is likely too small of a value, but used here for the exam ple), and thus we at least guarantee that the long-running job will ma ke some progress, getting boosted to the highest priority every 50 ms and thus getting to run periodically.",3404
8. Multi-level Feedback,"Of course, the addition of the time period Sleads to the obvious ques- tion: what should Sbe set to? John Ousterhout, a well-regarded systems researcher [O11], used to call such values in systems voo-doo constants , because they seemed to require some form of black magic to set the m cor- rectly. Unfortunately, Shas that ﬂavor. If it is set too high, long-running jobs could starve; too low, and interactive jobs may not get a proper s hare of the CPU. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE Q2 Q1 Q0 0 50 100 150 200 250 300 Figure 8.7: Lower Priority, Longer Quanta 8.4 Attempt #3: Better Accounting We now have one more problem to solve: how to prevent gaming of our scheduler? The real culprit here, as you might have guessed , are Rules 4a and 4b, which let a job retain its priority by relinquis hing the CPU before the time slice expires. So what should we do? The solution here is to perform better accounting of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a pr o- cess used at a given level, the scheduler should keep track; onc e a process has used its allotment, it is demoted to the next priority queue. Whether it uses the time slice in one long burst or many small ones does not mat ter. We thus rewrite Rules 4a and 4b to the following single rule: •Rule 4: Once a job uses up its time allotment at a given level (re- gardless of how many times it has given up the CPU), its priority i s reduced (i.e., it moves down one queue). Let’s look at an example. Figure 8.6 (page 7) shows what happens when a workload tries to game the scheduler with the old Rules 4a a nd 4b (on the left) as well the new anti-gaming Rule 4. Without any prot ection from gaming, a process can issue an I/O just before a time slice en ds and thus dominate CPU time. With such protections in place, regardl ess of the I/O behavior of the process, it slowly moves down the queues, and thus cannot gain an unfair share of the CPU. 8.5 Tuning MLFQ And Other Issues A few other issues arise with MLFQ scheduling. One big question is how to parameterize such a scheduler. For example, how many queues should there be? How big should the time slice be per queue? How ofte n should priority be boosted in order to avoid starvation and account for changes in behavior? There are no easy answers to these questi ons, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE 9 TIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT ’SLAW) Avoiding voo-doo constants is a good idea whenever possible. Unfor- tunately, as in the example above, it is often difﬁcult. One coul d try to make the system learn a good value, but that too is not straightforw ard. The frequent result: a conﬁguration ﬁle ﬁlled with default par ameter val- ues that a seasoned administrator can tweak when something isn’t quite working correctly. As you can imagine, these are often left unmodi ﬁed, and thus we are left to hope that the defaults work well in the ﬁel d.",3170
8. Multi-level Feedback,"This tip brought to you by our old OS professor, John Ousterhout, and hence we call it Ousterhout’s Law . For example, most MLFQ variants allow for varying time-slice len gth across different queues. The high-priority queues are usuall y given short time slices; they are comprised of interactive jobs, after all, and thus quickly alternating between them makes sense (e.g., 10 or few er millisec- onds). The low-priority queues, in contrast, contain long-runnin g jobs that are CPU-bound; hence, longer time slices work well (e.g., 1 00s of ms). Figure 8.7 (page 8) shows an example in which two jobs run for 20 ms at the highest queue (with a 10-ms time slice), 40 ms in the m iddle (20-ms time slice), and with a 40-ms time slice at the lowest. The Solaris MLFQ implementation — the Time-Sharing scheduling class, or TS — is particularly easy to conﬁgure; it provides a set of tables that determine exactly how the priority of a process is altered th rough- out its lifetime, how long each time slice is, and how often to boost th e priority of a job [AD00]; an administrator can muck with this tabl e in or- der to make the scheduler behave in different ways. Default v alues for the table are 60 queues, with slowly increasing time-slice le ngths from 20 milliseconds (highest priority) to a few hundred millisecon ds (lowest), and priorities boosted around every 1 second or so. Other MLFQ schedulers don’t use a table or the exact rules descri bed in this chapter; rather they adjust priorities using mathema tical formu- lae. For example, the FreeBSD scheduler (version 4.3) uses a form ula to calculate the current priority level of a job, basing it on how much CPU the process has used [LM+89]; in addition, usage is decayed over time, providing the desired priority boost in a different manner than d escribed herein. See Epema’s paper for an excellent overview of such decay-usage algorithms and their properties [E95]. Finally, many schedulers have a few other features that you mig ht en- counter. For example, some schedulers reserve the highest prior ity levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system. Some systems also allow s ome user advice to help set priorities; for example, by using the command-line utilitynice you can increase or decrease the priority of a job (somewhat) and thus increase or decrease its chances of running at any give n time. See the man page for more. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE TIP: USEADVICE WHERE POSSIBLE As the operating system rarely knows what is best for each and eve ry process of the system, it is often useful to provide interfaces to allow users or administrators to provide some hints to the OS. We often call such hints advice , as the OS need not necessarily pay attention to it, but rather might take the advice into account in order to make a better deci sion. Such hints are useful in many parts of the OS, including the sched uler (e.g., with nice ), memory manager (e.g., madvise ), and ﬁle system (e.g., informed prefetching and caching [P+95]).",3160
8. Multi-level Feedback,"8.6 MLFQ: Summary We have described a scheduling approach known as the Multi-Lev el Feedback Queue (MLFQ). Hopefully you can now see why it is called that: it has multiple levels of queues, and uses feedback to determine the priority of a given job. History is its guide: pay attention to how job s behave over time and treat them accordingly. The reﬁned set of MLFQ rules, spread throughout the chapter, are re- produced here for your viewing pleasure: •Rule 1: If Priority(A) >Priority(B), A runs (B doesn’t). •Rule 2: If Priority(A) =Priority(B), A & B run in round-robin fash- ion using the time slice (quantum length) of the given queue. •Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). •Rule 4: Once a job uses up its time allotment at a given level (re- gardless of how many times it has given up the CPU), its priority i s reduced (i.e., it moves down one queue). •Rule 5: After some time period S, move all the jobs in the system to the topmost queue. MLFQ is interesting for the following reason: instead of demandin g a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to ac hieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and m akes progress for long-running CPU-intensive workloads. For this reas on, many systems, including BSD U NIX derivatives [LM+89, B86], Solaris [M06], and Windows NT and subsequent Windows operating systems [CS97] use a form of MLFQ as their base scheduler. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE 11 References [AD00] “Multilevel Feedback Queue Scheduling in Solaris” by Andrea A rpaci-Dusseau. Avail- able: http://www.ostep.org/Citations/notes-solaris.pdf. A great short set of notes by one of the authors on the details of the Solaris scheduler. OK, we are probably biased in th is description, but the notes are pretty darn good. [B86] “The Design of the U NIXOperating System” by M.J. Bach. Prentice-Hall, 1986. One of the classic old books on how a real UNIXoperating system is built; a deﬁnite must-read for kernel hackers. [C+62] “An Experimental Time-Sharing System” by F. J. Corbato, M. M. D aggett, R. C. Daley. IFIPS 1962. A bit hard to read, but the source of many of the ﬁrst ideas in multi-level fee dback schedul- ing. Much of this later went into Multics, which one could argue was the most inﬂuential operating system of all time. [CS97] “Inside Windows NT” by Helen Custer and David A. Solomon. Micro soft Press, 1997. The NT book, if you want to learn about something other than UNIX. Of course, why would you? OK, we’re kidding; you might actually work for Microsoft some day you know. [E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors” by D .H.J. Epema. SIG- METRICS ’95. A nice paper on the state of the art of scheduling back in the mid 1990s, inclu ding a good overview of the basic approach behind decay-usage schedulers.",3081
8. Multi-level Feedback,"[LM+89] “The Design and Implementation of the 4.3BSD U NIXOperating System” by S.J. Lef- ﬂer, M.K. McKusick, M.J. Karels, J.S. Quarterman. Addison-Wesley, 1989 .Another OS classic, written by four of the main people behind BSD. The later versions of this book, w hile more up to date, don’t quite match the beauty of this one. [M06] “Solaris Internals: Solaris 10 and OpenSolaris Kernel Architectu re” by Richard Mc- Dougall. Prentice-Hall, 2006. A good book about Solaris and how it works. [O11] “John Ousterhout’s Home Page” by John Ousterhout. www.stanford.edu/˜ouster/ . The home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of taking graduate operating systems from Ousterhout while in graduate school; ind eed, this is where the two co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus, you really can blame Ousterhout for this entire mess you’re in. [P+95] “Informed Prefetching and Caching” by R.H. Patterson, G.A. Gibson, E. G inting, D. Stodolsky, J. Zelenka. SOSP ’95, Copper Mountain, Colorado, October 1995. A fun paper about some very cool ideas in ﬁle systems, including how applications can give th e OS advice about what ﬁles it is accessing and how it plans to access them. [Y+18] “Principled Schedulability Analysis for Distributed Storag e Systems using Thread Ar- chitecture Models” by Suli Yang, Jing Liu, Andrea C. Arpaci-Dusseau, Re mzi H. Arpaci- Dusseau. OSDI ’18, San Diego, California. A recent work of our group that demonstrates the difﬁculty of scheduling I/O requests within modern distributed storage systems such as Hive/HDFS, Cassandra, MongoDB, and Riak. Without care, a single user might be able to mon opolize system re- sources. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12SCHEDULING : THEMULTI -LEVEL FEEDBACK QUEUE Homework (Simulation) This program, mlfq.py , allows you to see how the MLFQ scheduler presented in this chapter behaves. See the README for details. Questions 1. Run a few randomly-generated problems with just two jobs and tw o queues; compute the MLFQ execution trace for each. Make your life easier by limit- ing the length of each job and turning off I/Os. 2. How would you run the scheduler to reproduce each of the examples i n the chapter? 3. How would you conﬁgure the scheduler parameters to behave just l ike a round-robin scheduler? 4. Craft a workload with two jobs and scheduler parameters so th at one job takes advantage of the older Rules 4a and 4b (turned on with the -Sﬂag) to game the scheduler and obtain 99 percent of the CPU over a particular t ime interval. 5. Given a system with a quantum length of 10 ms in its highest queue, how of- ten would you have to boost jobs back to the highest priority le vel (with the -Bﬂag) in order to guarantee that a single long-running (and pote ntially- starving) job gets at least 5 percent of the CPU? 6. One question that arises in scheduling is which end of a queue to add a job that just ﬁnished I/O; the -Iﬂag changes this behavior for this scheduling simulator. Play around with some workloads and see if you can see t he effect of this ﬂag. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3218
9. Lottery Scheduling,"9 Scheduling: Proportional Share In this chapter, we’ll examine a different type of scheduler kn own as a proportional-share scheduler, also sometimes referred to as a fair-share scheduler. Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might in stead try to guarantee that each job obtain a certain percentage of CPU time. An excellent early example of proportional-share scheduling is found in research by Waldspurger and Weihl [WW94], and is known as lottery scheduling ; however, the idea is certainly older [KL88]. The basic idea is quite simple: every so often, hold a lottery to determine whic h process should get to run next; processes that should run more often should b e given more chances to win the lottery. Easy, no? Now, onto the detai ls. But not before our crux: CRUX: HOWTOSHARE THECPU P ROPORTIONALLY How can we design a scheduler to share the CPU in a proportional manner? What are the key mechanisms for doing so? How effective ar e they? 9.1 Basic Concept: Tickets Represent Your Share Underlying lottery scheduling is one very basic concept: tickets , which are used to represent the share of a resource that a process (or use r or whatever) should receive. The percent of tickets that a process has repre- sents its share of the system resource in question. Let’s look at an example. Imagine two processes, A and B, and furth er that A has 75 tickets while B has only 25. Thus, what we would like is for A to receive 75 percent of the CPU and B the remaining 25 percent. Lottery scheduling achieves this probabilistically (but not d eterminis- tically) by holding a lottery every so often (say, every time sli ce). Holding a lottery is straightforward: the scheduler must know how many tot al tickets there are (in our example, there are 100). The schedul er then picks 1 2 S CHEDULING : PROPORTIONAL SHARE TIP: USERANDOMNESS One of the most beautiful aspects of lottery scheduling is its use ofran- domness . When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so. Random approaches has at least three advantages over more tradit ional decisions. First, random often avoids strange corner-case behav iors that a more traditional algorithm may have trouble handling. For examp le, consider the LRU replacement policy (studied in more detail in a future chapter on virtual memory); while often a good replacement algorit hm, LRU attains worst-case performance for some cyclic-sequential work- loads. Random, on the other hand, has no such worst case. Second, random also is lightweight, requiring little state to t rack alter- natives. In a traditional fair-share scheduling algorithm, t racking how much CPU each process has received requires per-process accoun ting, which must be updated after running each process. Doing so rand omly necessitates only the most minimal of per-process state (e.g., t he number of tickets each has). Finally, random can be quite fast. As long as generating a random num- ber is quick, making the decision is also, and thus random can be u sed in a number of places where speed is required. Of course, the fas ter the need, the more random tends towards pseudo-random.",3256
9. Lottery Scheduling,"a winning ticket, which is a number from 0 to 991. Assuming A holds tickets 0 through 74 and B 75 through 99, the winning ticket simp ly de- termines whether A or B runs. The scheduler then loads the state of that winning process and runs it. Here is an example output of a lottery scheduler’s winning ticket s: 63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 Here is the resulting schedule: A A A A A A A A A A A A A A A B B B B As you can see from the example, the use of randomness in lottery scheduling leads to a probabilistic correctness in meeting th e desired pro- portion, but no guarantee. In our example above, B only gets to run 4 out of 20 time slices (20 percent), instead of the desired 25 percent allocation. How ever, the longer these two jobs compete, the more likely they are to achi eve the desired percentages. 1Computer Scientists always start counting at 0. It is so odd to non-comp uter-types that famous people have felt obliged to write about why we do it this way [D 82]. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 3 TIP: USETICKETS TOREPRESENT SHARES One of the most powerful (and basic) mechanisms in the design of lot tery (and stride) scheduling is that of the ticket . The ticket is used to represent a process’s share of the CPU in these examples, but can be applied much more broadly. For example, in more recent work on virtual memory man- agement for hypervisors, Waldspurger shows how tickets can be us ed to represent a guest operating system’s share of memory [W02]. Thus , if you are ever in need of a mechanism to represent a proportion of ownershi p, this concept just might be ... (wait for it) ... the ticket. 9.2 Ticket Mechanisms Lottery scheduling also provides a number of mechanisms to mani p- ulate tickets in different and sometimes useful ways. One way is with the concept of ticket currency . Currency allows a user with a set of tick- ets to allocate tickets among their own jobs in whatever currency they would like; the system then automatically converts said curren cy into the correct global value. For example, assume users A and B have each been given 100 ticke ts. User A is running two jobs, A1 and A2, and gives them each 500 tic kets (out of 1000 total) in A’s currency. User B is running only 1 job and gi ves it 10 tickets (out of 10 total). The system converts A1’s and A2’s all ocation from 500 each in A’s currency to 50 each in the global currency; si milarly, B1’s 10 tickets is converted to 100 tickets. The lottery is then h eld over the global ticket currency (200 total) to determine which job runs. User A -> 500 (A’s currency) to A1 -> 50 (global currency) -> 500 (A’s currency) to A2 -> 50 (global currency) User B -> 10 (B’s currency) to B1 -> 100 (global currency) Another useful mechanism is ticket transfer . With transfers, a process can temporarily hand off its tickets to another process. This abi lity is especially useful in a client/server setting, where a clien t process sends a message to a server asking it to do some work on the client’s behal f.",3084
9. Lottery Scheduling,"To speed up the work, the client can pass the tickets to the serv er and thus try to maximize the performance of the server while the ser ver is handling the client’s request. When ﬁnished, the server then transfers the tickets back to the client and all is as before. Finally, ticket inﬂation can sometimes be a useful technique. With inﬂation, a process can temporarily raise or lower the number of tic kets it owns. Of course, in a competitive scenario with processes that do not trust one another, this makes little sense; one greedy process cou ld give itself a vast number of tickets and take over the machine. Rathe r, inﬂation can be applied in an environment where a group of processes trust on e another; in such a case, if any one process knows it needs more CPU ti me, it can boost its ticket value as a way to reﬂect that need to the sy stem, all without communicating with any other processes. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 4 S CHEDULING : PROPORTIONAL SHARE 1// counter: used to track if we’ve found the winner yet 2int counter = 0; 3 4// winner: use some call to a random number generator to 5// get a value, between 0 and the total # of tickets 6int winner = getrandom(0, totaltickets); 7 8// current: use this to walk through the list of jobs 9node_t*current = head; 10while (current) { 11counter = counter + current->tickets; 12if (counter > winner) 13 break; // found the winner 14current = current->next; 15} 16// ’current’ is the winner: schedule it... Figure 9.1: Lottery Scheduling Decision Code 9.3 Implementation Probably the most amazing thing about lottery scheduling is the s im- plicity of its implementation. All you need is a good random number generator to pick the winning ticket, a data structure to trac k the pro- cesses of the system (e.g., a list), and the total number of ticke ts. Let’s assume we keep the processes in a list. Here is an example c om- prised of three processes, A, B, and C, each with some number of tic kets. headJob:A Tix:100Job:B Tix:50Job:C Tix:250NULL To make a scheduling decision, we ﬁrst have to pick a random numb er (the winner) from the total number of tickets (400)2Let’s say we pick the number 300. Then, we simply traverse the list, with a simple c ounter used to help us ﬁnd the winner (Figure 9.1). The code walks the list of processes, adding each ticket value to counter until the value exceeds winner . Once that is the case, the current list el- ement is the winner. With our example of the winning ticket bein g 300, the following takes place. First, counter is incremented to 100 to ac- count for A’s tickets; because 100 is less than 300, the loop continu es. Thencounter would be updated to 150 (B’s tickets), still less than 300 and thus again we continue. Finally, counter is updated to 400 (clearly greater than 300), and thus we break out of the loop with current point- ing at C (the winner). 2Surprisingly, as pointed out by Bj ¨orn Lindberg, this can be challenging to do correctly; for more details, see http://stackoverflow.com/questions/2509679/ how-to-generate-a-random-number-from-within-a-range .",3118
9. Lottery Scheduling,"OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 5 1 10 100 10000.00.20.40.60.81.0 Job LengthUnfairness (Average) Figure 9.2: Lottery Fairness Study To make this process most efﬁcient, it might generally be best t o or- ganize the list in sorted order, from the highest number of ticket s to the lowest. The ordering does not affect the correctness of the algorith m; however, it does ensure in general that the fewest number of list itera- tions are taken, especially if there are a few processes that pos sess most of the tickets. 9.4 An Example To make the dynamics of lottery scheduling more understandable , we now perform a brief study of the completion time of two jobs competing against one another, each with the same number of tickets (100) a nd same run time ( R, which we will vary). In this scenario, we’d like for each job to ﬁnish at roughly the same time, but due to the randomness of lottery scheduling, sometimes one job ﬁnishes before the other. To quantify this difference, we de ﬁne a simple unfairness metric ,Uwhich is simply the time the ﬁrst job com- pletes divided by the time that the second job completes. For exam ple, ifR= 10 , and the ﬁrst job ﬁnishes at time 10 (and the second job at 20), U=10 20= 0.5. When both jobs ﬁnish at nearly the same time, Uwill be quite close to 1. In this scenario, that is our goal: a perfectly fa ir scheduler would achieve U= 1. Figure 9.2 plots the average unfairness as the length of the two jobs (R) is varied from 1 to 1000 over thirty trials (results are genera ted via the simulator provided at the end of the chapter). As you can see from th e graph, when the job length is not very long, average unfairness c an be quite severe. Only as the jobs run for a signiﬁcant number of time slices does the lottery scheduler approach the desired outcome. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 6 S CHEDULING : PROPORTIONAL SHARE 9.5 How To Assign Tickets? One problem we have not addressed with lottery scheduling is: how to assign tickets to jobs? This problem is a tough one, because of cou rse how the system behaves is strongly dependent on how tickets are al lo- cated. One approach is to assume that the users know best; in suc h a case, each user is handed some number of tickets, and a user can a llocate tickets to any jobs they run as desired. However, this solution is a non- solution: it really doesn’t tell you what to do. Thus, given a set of job s, the “ticket-assignment problem” remains open. 9.6 Why Not Deterministic? You might also be wondering: why use randomness at all? As we saw above, while randomness gets us a simple (and approximately corr ect) scheduler, it occasionally will not deliver the exact right prop ortions, es- pecially over short time scales. For this reason, Waldspurger in vented stride scheduling , a deterministic fair-share scheduler [W95]. Stride scheduling is also straightforward. Each job in the syst em has a stride, which is inverse in proportion to the number of tickets i t has. In our example above, with jobs A, B, and C, with 100, 50, and 250 tick ets, respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. For example, if we divide 10,000 by each of those ticket values, we ob tain the following stride values for A, B, and C: 100, 200, and 40. We ca ll this value the stride of each process; every time a process runs, we will increment a counter for it (called its pass value) by its stride to track its global progress. The scheduler then uses the stride and pass to determine whic h pro- cess should run next. The basic idea is simple: at any given tim e, pick the process to run that has the lowest pass value so far; when you r un a process, increment its pass counter by its stride. A pseudocode imple- mentation is provided by Waldspurger [W95]: curr = remove_min(queue); // pick client with min pass schedule(curr); // run for quantum curr->pass += curr->stride; // update pass using stride insert(queue, curr); // return curr to queue In our example, we start with three processes (A, B, and C), with stride values of 100, 200, and 40, and all with pass values initially a t 0. Thus, at ﬁrst, any of the processes might run, as their pass values are eq ually low. Assume we pick A (arbitrarily; any of the processes with equal l ow pass values can be chosen).",4420
9. Lottery Scheduling,"A runs; when ﬁnished with the time slice , we update its pass value to 100. Then we run B, whose pass value is t hen set to 200. Finally, we run C, whose pass value is incremented t o 40. At this point, the algorithm will pick the lowest pass value, which is C’s, and OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 7 Pass(A) Pass(B) Pass(C) Who Runs? (stride=100) (stride=200) (stride=40) 0 0 0 A 100 0 0 B 100 200 0 C 100 200 40 C 100 200 80 C 100 200 120 A 200 200 120 C 200 200 160 C 200 200 200 ... Figure 9.3: Stride Scheduling: A Trace run it, updating its pass to 80 (C’s stride is 40, as you recall). Then C will run again (still the lowest pass value), raising its pass to 12 0. A will run now, updating its pass to 200 (now equal to B’s). Then C will run tw ice more, updating its pass to 160 then 200. At this point, all pass v alues are equal again, and the process will repeat, ad inﬁnitum. Figure 9.3 traces the behavior of the scheduler over time. As we can see from the ﬁgure, C ran ﬁve times, A twice, and B just once, exactly in proportion to their ticket values of 250, 100, and 50. Lot- tery scheduling achieves the proportions probabilistically ove r time; stride scheduling gets them exactly right at the end of each scheduli ng cycle. So you might be wondering: given the precision of stride schedulin g, why use lottery scheduling at all? Well, lottery scheduling ha s one nice property that stride scheduling does not: no global state. Imagi ne a new job enters in the middle of our stride scheduling example above; w hat should its pass value be? Should it be set to 0? If so, it will monopoliz e the CPU. With lottery scheduling, there is no global state per p rocess; we simply add a new process with whatever tickets it has, updat e the single global variable to track how many total tickets we have, a nd go from there. In this way, lottery makes it much easier to incorpora te new processes in a sensible manner. 9.7 The Linux Completely Fair Scheduler (CFS) Despite these earlier works in fair-share scheduling, the cu rrent Linux approach achieves similar goals in an alternate manner. The sc heduler, entitled the Completely Fair Scheduler (orCFS ) [J09], implements fair- share scheduling, but does so in a highly efﬁcient and scalabl e manner. To achieve its efﬁciency goals, CFS aims to spend very little t ime mak- ing scheduling decisions, through both its inherent design and its clever use of data structures well-suited to the task. Recent studie s have shown that scheduler efﬁciency is surprisingly important; speciﬁ cally, in a study of Google datacenters, Kanev et al. show that even after aggressi ve opti- mization, scheduling uses about 5 percent of overall datacenter CPU tim e. Re- ducing that overhead as much as possible is thus a key goal in moder n scheduler architecture. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 8 S CHEDULING : PROPORTIONAL SHARE 0 50 100 150 200 250 TimeABCDABCDA B A B A B Figure 9.4: CFS Simple Example Basic Operation Whereas most schedulers are based around the concept of a ﬁxed tim e slice, CFS operates a bit differently. Its goal is simple: to fa irly divide a CPU evenly among all competing processes.",3234
9. Lottery Scheduling,"It does so through a simp le counting-based technique known as virtual runtime (vruntime ). As each process runs, it accumulates vruntime . In the most basic case, each process’s vruntime increases at the same rate, in proportion with physical (real) time. When a scheduling decision occurs, CFS will pick the process with the lowestvruntime to run next. This raises a question: how does the scheduler know when to stop the currently running process, and run the next one? The tension here is clear: if CFS switches too often, fairness is increased, as CFS will ensure that each process receives its share of CPU even over miniscule t ime win- dows, but at the cost of performance (too much context switching); i f CFS switches less often, performance is increased (reduced contex t switching), but at the cost of near-term fairness. CFS manages this tension through various control parameters. The ﬁrst isschedlatency . CFS uses this value to determine how long one process should run before considering a switch (effectively det ermining its time slice but in a dynamic fashion). A typical schedlatency value is 48 (milliseconds); CFS divides this value by the number ( n) of processes running on the CPU to determine the time slice for a process, and t hus ensures that over this period of time, CFS will be completely fair . For example, if there are n= 4 processes running, CFS divides the value ofschedlatency bynto arrive at a per-process time slice of 12 ms. CFS then schedules the ﬁrst job and runs it until it has used 12 ms of (virtual) runtime, and then checks to see if there is a job wit h lower vruntime to run instead. In this case, there is, and CFS would switch to one of the three other jobs, and so forth. Figure 9.4 shows an examp le where the four jobs (A, B, C, D) each run for two time slices in this fashion; two of them (C, D) then complete, leaving just two remaining, wh ich then each run for 24 ms in round-robin fashion. But what if there are “too many” processes running? Wouldn’t that lead to too small of a time slice, and thus too many context switche s? Good question. And the answer is yes. To address this issue, CFS adds another parameter, mingranularity , which is usually set to a value like 6 ms. CFS will never set the time slice OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 9 of a process to less than this value, ensuring that not too much tim e is spent in scheduling overhead. For example, if there are ten processes running, our original cal cula- tion would divide schedlatency by ten to determine the time slice (result: 4.8 ms). However, because of mingranularity , CFS will set the time slice of each process to 6 ms instead. Although CFS won’t (q uite) be perfectly fair over the target scheduling latency ( schedlatency ) of 48 ms, it will be close, while still achieving high CPU efﬁcien cy. Note that CFS utilizes a periodic timer interrupt, which means it can only make decisions at ﬁxed time intervals. This interrupt goes off fre- quently (e.g., every 1 ms), giving CFS a chance to wake up and d etermine if the current job has reached the end of its run. If a job has a time slice that is not a perfect multiple of the timer interrupt interval, that is OK; CFS tracks vruntime precisely, which means that over the long haul, it will eventually approximate ideal sharing of the CPU.",3375
9. Lottery Scheduling,"Weighting (Niceness) CFS also enables controls over process priority, enabling users or admin- istrators to give some processes a higher share of the CPU. It does t his not with tickets, but through a classic U NIX mechanism known as the nice level of a process. The nice parameter can be set anywhere from -2 0 to +19 for a process, with a default of 0. Positive nice values impl ylower priority and negative values imply higher priority; when you’re too nice, you just don’t get as much (scheduling) attention, alas. CFS maps the nice value of each process to a weight , as shown here: static const int prio_to_weight[40] = { /*-20*/ 88761, 71755, 56483, 46273, 36291, /*-15*/ 29154, 23254, 18705, 14949, 11916, /*-10*/ 9548, 7620, 6100, 4904, 3906, /*-5*/ 3121, 2501, 1991, 1586, 1277, /*0*/ 1024, 820, 655, 526, 423, /*5*/ 335, 272, 215, 172, 137, /*10*/ 110, 87, 70, 56, 45, /*15*/ 36, 29, 23, 18, 15, }; These weights allow us to compute the effective time slice of eac h pro- cess (as we did before), but now accounting for their priority diff erences. The formula used to do so is as follows: timeslice k=weightk/summationtextn−1 i=0weighti·schedlatency (9.1) Let’s do an example to see how this works. Assume there are two jobs , A and B. A, because its our most precious job, is given a higher prior ity by c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 10 S CHEDULING : PROPORTIONAL SHARE assigning it a nice value of -5; B, because we hates it3, just has the default priority (nice value equal to 0). This means weight A(from the table) is 3121, whereas weight Bis 1024. If you then compute the time slice of each job, you’ll ﬁnd that A’s time slice is about3 4ofschedlatency (hence, 36 ms), and B’s about1 4(hence, 12 ms). In addition to generalizing the time slice calculation, the wa y CFS cal- culatesvruntime must also be adapted. Here is the new formula, which takes the actual run time that process ihas accrued ( runtime i) and scales it inversely by the weight of the process. In our running example , A’s vruntime will accumulate at one-third the rate of B’s. vruntime i=vruntime i+weight0 weighti·runtime i (9.2) One smart aspect of the construction of the table of weights above is that the table preserves CPU proportionality ratios when the dif ference in nice values is constant. For example, if process A instead had a n ice value of 5 (not -5), and process B had a nice value of 10 (not 0), CFS would schedule them in exactly the same manner as before. Run through the math yourself to see why. Using Red-Black Trees One major focus of CFS is efﬁciency, as stated above. For a schedule r, there are many facets of efﬁciency, but one of them is as simple as this: when the scheduler has to ﬁnd the next job to run, it should do so a s quickly as possible. Simple data structures like lists don’t sca le: modern systems sometimes are comprised of 1000s of processes, and thus se arch- ing through a long-list every so many milliseconds is wasteful. CFS addresses this by keeping processes in a red-black tree [B72]. A red-black tree is one of many types of balanced trees; in contrast to a simple binary tree (which can degenerate to list-like perfor mance un- der worst-case insertion patterns), balanced trees do a littl e extra work to maintain low depths, and thus ensure that operations are logar ithmic (and not linear) in time.",3371
9. Lottery Scheduling,"CFS does not keep allprocess in this structure; rather, only running (or runnable) processes are kept therein. If a process goes to sle ep (say, waiting on an I/O to complete, or for a network packet to arrive), it is removed from the tree and kept track of elsewhere. Let’s look at an example to make this more clear. Assume there are t en jobs, and that they have the following values of vruntime : 1, 5, 9, 10, 14, 18, 17, 21, 22, and 24. If we kept these jobs in an ordered list, ﬁn ding the next job to run would be simple: just remove the ﬁrst element. Howe ver, when placing that job back into the list (in order), we would have to scan 3Yes, yes, we are using bad grammar here on purpose, please don’t s end in a bug ﬁx. Why? Well, just a most mild of references to the Lord of the Rings, and ou r favorite anti-hero Gollum, nothing to get too excited about. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 11 1 59 1014 18 17 22 21 24 Figure 9.5: CFS Red-Black Tree the list, looking for the right spot to insert it, an O(n)operation. Any search is also quite inefﬁcient, also taking linear time on av erage. Keeping the same values in a red-black tree makes most operation s more efﬁcient, as depicted in Figure 9.5. Processes are ordered in the tree byvruntime , and most operations (such as insertion and deletion) are logarithmic in time, i.e., O(logn). Whennis in the thousands, logarith- mic is noticeably more efﬁcient than linear. Dealing With I/O And Sleeping Processes One problem with picking the lowest vruntime to run next arises with jobs that have gone to sleep for a long period of time. Imagine two pro- cesses, A and B, one of which (A) runs continuously, and the other (B ) which has gone to sleep for a long period of time (say, 10 seconds). Wh en B wakes up, its vruntime will be 10 seconds behind A’s, and thus (if we’re not careful), B will now monopolize the CPU for the next 10 sec- onds while it catches up, effectively starving A. CFS handles this case by altering the vruntime of a job when it wakes up. Speciﬁcally, CFS sets the vruntime of that job to the minimum value found in the tree (remember, the tree only contains running jobs) [B+18]. In this way, CFS avoids starvation, but not without a cost: jobs that sleep for short periods of time frequently do not ever get their fair shar e of the CPU [AC97]. Other CFS Fun CFS has many other features, too many to discuss at this point in t he book. It includes numerous heuristics to improve cache performan ce, has strategies for handling multiple CPUs effectively (as discu ssed later in the book), can schedule across large groups of processes (instead of tre ating each process as an independent entity), and many other interes ting fea- tures. Read recent research, starting with Bouron [B+18], to l earn more. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 12 S CHEDULING : PROPORTIONAL SHARE TIP: USEEFFICIENT DATA STRUCTURES WHEN APPROPRIATE In many cases, a list will do. In many cases, it will not.",3040
9. Lottery Scheduling,"Knowing w hich data structure to use when is a hallmark of good engineering. In t he case discussed herein, simple lists found in earlier schedulers s imply do not work well on modern systems, particular in the heavily loaded ser vers found in datacenters. Such systems contain thousands of active pr o- cesses; searching through a long list to ﬁnd the next job to run on e ach core every few milliseconds would waste precious CPU cycles. A be tter structure was needed, and CFS provided one by adding an excelle nt im- plementation of a red-black tree. More generally, when picking a data structure for a system you are building, carefully consider its access pat- terns and its frequency of usage; by understanding these, you w ill be able to implement the right structure for the task at hand. 9.8 Summary We have introduced the concept of proportional-share scheduling a nd brieﬂy discussed three approaches: lottery scheduling, stri de scheduling, and the Completely Fair Scheduler (CFS) of Linux. Lottery uses ran dom- ness in a clever way to achieve proportional share; stride does so deter- ministically. CFS, the only “real” scheduler discussed in thi s chapter, is a bit like weighted round-robin with dynamic time slices, but bu ilt to scale and perform well under load; to our knowledge, it is the most widely used fair-share scheduler in existence today. No scheduler is a panacea, and fair-share schedulers have th eir fair share of problems. One issue is that such approaches do not partic ularly mesh well with I/O [AC97]; as mentioned above, jobs that perform I /O occasionally may not to get their fair share of CPU. Another issue i s that they leave open the hard problem of ticket or priority assignment, i.e., how do you know how many tickets your browser should be allocated, or to what nice value to set your text editor? Other general-purpos e sched- ulers (such as the MLFQ we discussed previously, and other simi lar Linux schedulers) handle these issues automatically and thus may b e more eas- ily deployed. The good news is that there are many domains in which these prob- lems are not the dominant concern, and proportional-share schedul ers are used to great effect. For example, in a virtualized data center (or cloud ), where you might like to assign one-quarter of your CPU cycles to the Windows VM and the rest to your base Linux installation, pr opor- tional sharing can be simple and effective. The idea can also b e extended to other resources; see Waldspurger [W02] for further details on how to proportionally share memory in VMWare’s ESX Server. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG SCHEDULING : PROPORTIONAL SHARE 13 References [AC97] “Extending Proportional-Share Scheduling to a Network of Works tations” by Andrea C. Arpaci-Dusseau and David E. Culler. PDPTA’97, June 1997. A paper by one of the authors on how to extend proportional-share scheduling to work better in a clustered env ironment. [B+18] “The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS” by J.",3021
9. Lottery Scheduling,"B ouron, S. Chevalley, B. Lepers, W. Zwaenepoel, R. Gouicem, J. Lawall, G. Muller, J. Sope na. USENIX ATC ’18, July 2018, Boston, Massachusetts. A recent, detailed work comparing Linux CFS and the FreeBSD schedulers. An excellent overview of each scheduler is also provi ded. The result of the comparison: inconclusive (in some cases CFS was better, and in others, ULE (the BSD sche duler), was. Sometimes in life there are no easy answers. [B72] “Symmetric binary B-Trees: Data Structure And Maintenance Algor ithms” by Rudolf Bayer. Acta Informatica, Volume 1, Number 4, December 1972. A cool balanced tree introduced before you were born (most likely). One of many balanced trees out there; study your algorithms book for more alternatives. [D82] “Why Numbering Should Start At Zero” by Edsger Dijkstra, Au gust 1982. Available: http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.P DF.A short note from E. Dijkstra, one of the pioneers of computer science. We’ll be hearing much m ore on this guy in the section on Concurrency. In the meanwhile, enjoy this note, which include s this motivating quote: “One of my colleagues — not a computing scientist — accused a number of younger computing scientists of ’pedantry’ because they started numbering at zero.” The note explains why d oing so is logical. [K+15] “Proﬁling A Warehouse-scale Computer” by S. Kanev, P . Ranganat han, J. P . Darago, K. Hazelwood, T. Moseley, G. Wei, D. Brooks. ISCA ’15, June, 2015 , Portland, Oregon. A fascinating study of where the cycles go in modern data centers, which are increasingly where most of computing happens. Almost 20 percent of CPU time is spent in the operating system , 5 percent in the scheduler alone. [J09] “Inside The Linux 2.6 Completely Fair Scheduler” by M. Tim Jones. De cember 15, 2009. http://ostep.org/Citations/inside-cfs.pdf .A simple overview of CFS from its ear- lier days. CFS was created by Ingo Molnar in a short burst of creativity whic h led to a 100K kernel patch developed in 62 hours. [KL88] “A Fair Share Scheduler” by J. Kay and P . Lauder. CACM, Volume 3 1 Issue 1, January 1988. An early reference to a fair-share scheduler. [WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Ma nagement” by Carl A. Waldspurger and William E. Weihl. OSDI ’94, November 1994. The landmark paper on lottery scheduling that got the systems community re-energized about schedulin g, fair sharing, and the power of simple randomized algorithms. [W95] “Lottery and Stride Scheduling: Flexible Proportional-Share R esource Management” by Carl A. Waldspurger. Ph.D. Thesis, MIT, 1995. The award-winning thesis of Waldspurger’s that outlines lottery and stride scheduling. If you’re thinking of writing a Ph. D. dissertation at some point, you should always have a good example around, to give you something to strive for: this is such a good one. [W02] “Memory Resource Management in VMware ESX Server” by Carl A. Wa ldspurger. OSDI ’02, Boston, Massachusetts. The paper to read about memory management in VMMs (a.k.a., hypervisors). In addition to being relatively easy to read, the paper contains n umerous cool ideas about this new type of VMM-level memory management. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 14 S CHEDULING : PROPORTIONAL SHARE Homework (Simulation) This program, lottery.py , allows you to see how a lottery scheduler works. See the README for details. Questions 1. Compute the solutions for simulations with 3 jobs and random see ds of 1, 2, and 3. 2. Now run with two speciﬁc jobs: each of length 10, but one (job 0 ) with just 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100 ). What happens when the number of tickets is so imbalanced? Will job 0 ev er run before job 1 completes? How often? In general, what does such a t icket imbalance do to the behavior of lottery scheduling? 3. When running with two jobs of length 100 and equal ticket allo cations of 100 (-l 100:100,100:100 ), how unfair is the scheduler? Run with some dif- ferent random seeds to determine the (probabilistic) answer ; let unfairness be determined by how much earlier one job ﬁnishes than the other. 4. How does your answer to the previous question change as the quan tum size (-q) gets larger? 5. Can you make a version of the graph that is found in the chapter ? What else would be worth exploring? How would the graph look with a str ide scheduler? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",4446
10. Multi-CPU Scheduling,"10 Multiprocessor Scheduling (Advanced) This chapter will introduce the basics of multiprocessor scheduling . As this topic is relatively advanced, it may be best to cover it after you have studied the topic of concurrency in some detail (i.e., the second m ajor “easy piece” of the book). After years of existence only in the high-end of the computing spe c- trum, multiprocessor systems are increasingly commonplace, and have found their way into desktop machines, laptops, and even mobile d e- vices. The rise of the multicore processor, in which multiple CPU cores are packed onto a single chip, is the source of this proliferation; these chips have become popular as computer architects have had a difﬁ cult time making a single CPU much faster without using (way) too muc h power. And thus we all now have a few CPUs available to us, which i s a good thing, right? Of course, there are many difﬁculties that arise with the arri val of more than a single CPU. A primary one is that a typical application (i .e., some C program you wrote) only uses a single CPU; adding more CPUs does not make that single application run faster. To remedy this proble m, you’ll have to rewrite your application to run in parallel , perhaps using threads (as discussed in great detail in the second piece of this book). Mu lti- threaded applications can spread work across multiple CPUs and thus run faster when given more CPU resources. ASIDE : ADVANCED CHAPTERS Advanced chapters require material from a broad swath of the book t o truly understand, while logically ﬁtting into a section that i s earlier than said set of prerequisite materials. For example, this chapter on multipro- cessor scheduling makes much more sense if you’ve ﬁrst read the mi ddle piece on concurrency; however, it logically ﬁts into the part of th e book on virtualization (generally) and CPU scheduling (speciﬁcal ly). Thus, it is recommended such chapters be covered out of order; in this case, after the second piece of the book. 1 2 MULTIPROCESSOR SCHEDULING (ADVANCED ) MemoryCPU Cache Figure 10.1: Single CPU With Cache Beyond applications, a new problem that arises for the operating s ys- tem is (not surprisingly.) that of multiprocessor scheduling . Thus far we’ve discussed a number of principles behind single-processor schedul- ing; how can we extend those ideas to work on multiple CPUs? What new problems must we overcome? And thus, our problem: CRUX: HOWTOSCHEDULE JOBS ONMULTIPLE CPU S How should the OS schedule jobs on multiple CPUs? What new prob- lems arise? Do the same old techniques work, or are new ideas requ ired? 10.1 Background: Multiprocessor Architecture To understand the new issues surrounding multiprocessor sched ul- ing, we have to understand a new and fundamental difference b etween single-CPU hardware and multi-CPU hardware. This differen ce centers around the use of hardware caches (e.g., Figure 10.1), and exactly how data is shared across multiple processors. We now discuss this is sue fur- ther, at a high level.",3024
10. Multi-CPU Scheduling,"Details are available elsewhere [CSG99 ], in particular in an upper-level or perhaps graduate computer architecture c ourse. In a system with a single CPU, there are a hierarchy of hardware caches that in general help the processor run programs faster. Caches are small, fast memories that (in general) hold copies of popular data that is found in the main memory of the system. Main memory, in contrast, holds allof the data, but access to this larger memory is slower. By keep- ing frequently accessed data in a cache, the system can make t he large, slow memory appear to be a fast one. As an example, consider a program that issues an explicit load in struc- tion to fetch a value from memory, and a simple system with only a si ngle CPU; the CPU has a small cache (say 64 KB) and a large main memory . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 3 MemoryCPU CPU Cache Cache Bus Figure 10.2: Two CPUs With Caches Sharing Memory The ﬁrst time a program issues this load, the data resides in mai n mem- ory, and thus takes a long time to fetch (perhaps in the tens of nan osec- onds, or even hundreds). The processor, anticipating that the da ta may be reused, puts a copy of the loaded data into the CPU cache. If the pr ogram later fetches this same data item again, the CPU ﬁrst checks f or it in the cache; if it ﬁnds it there, the data is fetched much more quickl y (say, just a few nanoseconds), and thus the program runs faster. Caches are thus based on the notion of locality , of which there are two kinds: temporal locality and spatial locality . The idea behind tem- poral locality is that when a piece of data is accessed, it is like ly to be accessed again in the near future; imagine variables or even i nstructions themselves being accessed over and over again in a loop. The idea b e- hind spatial locality is that if a program accesses a data item a t address x, it is likely to access data items near xas well; here, think of a program streaming through an array, or instructions being executed one a fter the other. Because locality of these types exist in many programs, ha rdware systems can make good guesses about which data to put in a cache an d thus work well. Now for the tricky part: what happens when you have multiple pro- cessors in a single system, with a single shared main memory, as we see in Figure 10.2? As it turns out, caching with multiple CPUs is much more compli- cated. Imagine, for example, that a program running on CPU 1 read s a data item (with value D) at address A; because the data is not in the cache on CPU 1, the system fetches it from main memory, and gets th e valueD. The program then modiﬁes the value at address A, just updat- ing its cache with the new value D′; writing the data through all the way to main memory is slow, so the system will (usually) do that late r. Then assume the OS decides to stop running the program and move it to CP U 2. The program then re-reads the value at address A; there is no such data c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 MULTIPROCESSOR SCHEDULING (ADVANCED ) CPU 2’s cache, and thus the system fetches the value from main me mory, and gets the old value Dinstead of the correct value D′.",3248
10. Multi-CPU Scheduling,"Oops. This general problem is called the problem of cache coherence , and there is a vast research literature that describes many diff erent subtleties involved with solving the problem [SHW11]. Here, we will skip all of the nuance and make some major points; take a computer architecture c lass (or three) to learn more. The basic solution is provided by the hardware: by monitoring mem- ory accesses, hardware can ensure that basically the “right t hing” hap- pens and that the view of a single shared memory is preserved. On e way to do this on a bus-based system (as described above) is to use an old technique known as bus snooping [G83]; each cache pays attention to memory updates by observing the bus that connects them to main me m- ory. When a CPU then sees an update for a data item it holds in its ca che, it will notice the change and either invalidate its copy (i.e., remove it from its own cache) or update it (i.e., put the new value into its cache too). Write-back caches, as hinted at above, make this more compli cated (because the write to main memory isn’t visible until later), b ut you can imagine how the basic scheme might work. 10.2 Don’t Forget Synchronization Given that the caches do all of this work to provide coherence, do p ro- grams (or the OS itself) have to worry about anything when they ac cess shared data? The answer, unfortunately, is yes, and is documen ted in great detail in the second piece of this book on the topic of concurrenc y. While we won’t get into the details here, we’ll sketch/review som e of the basic ideas here (assuming you’re familiar with concurrency). When accessing (and in particular, updating) shared data it ems or structures across CPUs, mutual exclusion primitives (such as locks) should likely be used to guarantee correctness (other approaches, suc h as build- inglock-free data structures, are complex and only used on occasion; see the chapter on deadlock in the piece on concurrency for details ). For example, assume we have a shared queue being accessed on multi ple CPUs concurrently. Without locks, adding or removing elements fr om the queue concurrently will not work as expected, even with the u nder- lying coherence protocols; one needs locks to atomically update the data structure to its new state. To make this more concrete, imagine this code sequence, which is used to remove an element from a shared linked list, as we see in Figur e 10.3. Imagine if threads on two CPUs enter this routine at the same tim e. If Thread 1 executes the ﬁrst line, it will have the current valu e ofhead stored in its tmp variable; if Thread 2 then executes the ﬁrst line as well, it also will have the same value of head stored in its own private tmp variable (tmp is allocated on the stack, and thus each thread will have its own private storage for it). Thus, instead of each thread remov ing an element from the head of the list, each thread will try to remov e the OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 5 1typedef struct __Node_t { 2int value; 3struct __Node_t *next; 4} Node_t; 5 6int List_Pop() { 7Node_t*tmp = head; // remember old head ...",3167
10. Multi-CPU Scheduling,"8int value = head->value; // ... and its value 9head = head->next; // advance head to next pointer 10 free(tmp); // free old head 11 return value; // return value at head 12} Figure 10.3: Simple List Delete Code same head element, leading to all sorts of problems (such as an at tempted double free of the head element at line 4, as well as potentially r eturning the same data value twice). The solution, of course, is to make such routines correct via lock- ing. In this case, allocating a simple mutex (e.g., pthread mutext m;) and then adding a lock(&m) at the beginning of the routine and anunlock(&m) at the end will solve the problem, ensuring that the code will execute as desired. Unfortunately, as we will see, such a n approach is not without problems, in particular with regards to performance . Speciﬁ- cally, as the number of CPUs grows, access to a synchronized shar ed data structure becomes quite slow. 10.3 One Final Issue: Cache Afﬁnity One ﬁnal issue arises in building a multiprocessor cache sched uler, known as cache afﬁnity [TTG95]. This notion is simple: a process, when run on a particular CPU, builds up a fair bit of state in the cache s (and TLBs) of the CPU. The next time the process runs, it is often advan ta- geous to run it on the same CPU, as it will run faster if some of its st ate is already present in the caches on that CPU. If, instead, one ru ns a pro- cess on a different CPU each time, the performance of the process w ill be worse, as it will have to reload the state each time it runs (note i t will run correctly on a different CPU thanks to the cache coherence protocol s of the hardware). Thus, a multiprocessor scheduler should conside r cache afﬁnity when making its scheduling decisions, perhaps prefe rring to keep a process on the same CPU if at all possible. 10.4 Single-Queue Scheduling With this background in place, we now discuss how to build a sched - uler for a multiprocessor system. The most basic approach is to sim ply reuse the basic framework for single processor scheduling, by pu tting all jobs that need to be scheduled into a single queue; we call this single- queue multiprocessor scheduling orSQMS for short. This approach has the advantage of simplicity; it does not require much work to t ake an existing policy that picks the best job to run next and adapt it t o work on more than one CPU (where it might pick the best two jobs to run, if t here are two CPUs, for example). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 MULTIPROCESSOR SCHEDULING (ADVANCED ) However, SQMS has obvious shortcomings. The ﬁrst problem is a lack ofscalability . To ensure the scheduler works correctly on multiple CPUs, the developers will have inserted some form of locking into the code, as described above. Locks ensure that when SQMS code accesses the si ngle queue (say, to ﬁnd the next job to run), the proper outcome arises. Locks, unfortunately, can greatly reduce performance, partic ularly as the number of CPUs in the systems grows [A91]. As contention for suc h a single lock increases, the system spends more and more time in l ock overhead and less time doing the work the system should be doing (not e: it would be great to include a real measurement of this in here som eday).",3262
10. Multi-CPU Scheduling,"The second main problem with SQMS is cache afﬁnity. For example, let us assume we have ﬁve jobs to run ( A,B,C,D,E) and four processors. Our scheduling queue thus looks like this: Queue A B C D E NULL Over time, assuming each job runs for a time slice and then anothe r job is chosen, here is a possible job schedule across CPUs: CPU 3CPU 2CPU 1CPU 0 DCBAECBAEDBAEDCAEDCB  ... (repeat) ... ... (repeat) ... ... (repeat) ... ... (repeat) ... Because each CPU simply picks the next job to run from the globall y- shared queue, each job ends up bouncing around from CPU to CPU, thu s doing exactly the opposite of what would make sense from the stand- point of cache afﬁnity. To handle this problem, most SQMS schedulers include some kind of afﬁnity mechanism to try to make it more likely that process wil l continue to run on the same CPU if possible. Speciﬁcally, one might provide a fﬁn- ity for some jobs, but move others around to balance load. For example, imagine the same ﬁve jobs scheduled as follows: CPU 3CPU 2CPU 1CPU 0 DDDDECCCECBBEBBAEAAA  ... (repeat) ... ... (repeat) ... ... (repeat) ... ... (repeat) ... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 7 In this arrangement, jobs Athrough Dare not moved across proces- sors, with only job Emigrating from CPU to CPU, thus preserving afﬁn- ity for most. You could then decide to migrate a different job the ne xt time through, thus achieving some kind of afﬁnity fairness as we ll. Im- plementing such a scheme, however, can be complex. Thus, we can see the SQMS approach has its strengths and weak- nesses. It is straightforward to implement given an existing single-CPU scheduler, which by deﬁnition has only a single queue. However, it does not scale well (due to synchronization overheads), and it does not r eadily preserve cache afﬁnity. 10.5 Multi-Queue Scheduling Because of the problems caused in single-queue schedulers, som e sys- tems opt for multiple queues, e.g., one per CPU. We call this appr oach multi-queue multiprocessor scheduling (orMQMS ). In MQMS, our basic scheduling framework consists of multiple sche dul- ing queues. Each queue will likely follow a particular schedul ing disci- pline, such as round robin, though of course any algorithm can be use d. When a job enters the system, it is placed on exactly one scheduli ng queue, according to some heuristic (e.g., random, or picking one w ith fewer jobs than others). Then it is scheduled essentially inde pendently, thus avoiding the problems of information sharing and synchroniza tion found in the single-queue approach. For example, assume we have a system where there are just two CP Us (labeled CPU 0 and CPU 1), and some number of jobs enter the system : A,B,C, andDfor example. Given that each CPU has a scheduling queue now, the OS has to decide into which queue to place each job. It mi ght do something like this: Q0 A C Q1 B D Depending on the queue scheduling policy, each CPU now has two jobs to choose from when deciding what should run. For example, with round robin , the system might produce a schedule that looks like this: CPU 1CPU 0 AACCAACCAACC BBDDBBDDBBDD  ...",3167
10. Multi-CPU Scheduling,"... MQMS has a distinct advantage of SQMS in that it should be inher- ently more scalable. As the number of CPUs grows, so too does the num - ber of queues, and thus lock and cache contention should not become a central problem. In addition, MQMS intrinsically provides cac he afﬁnity; c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 MULTIPROCESSOR SCHEDULING (ADVANCED ) jobs stay on the same CPU and thus reap the advantage of reusing ca ched contents therein. But, if you’ve been paying attention, you might see that we have a n ew problem, which is fundamental in the multi-queue based approa ch:load imbalance . Let’s assume we have the same set up as above (four jobs, two CPUs), but then one of the jobs (say C) ﬁnishes. We now have the following scheduling queues: Q0 A Q1 B D If we then run our round-robin policy on each queue of the system, we will see this resulting schedule: CPU 1CPU 0 AAAAAAAAAAAA BBDDBBDDBBDD  ... ... As you can see from this diagram, Agets twice as much CPU as Band D, which is not the desired outcome. Even worse, let’s imagine that b oth AandCﬁnish, leaving just jobs BandDin the system. The scheduling queues will look like this: Q0 Q1 B D As a result, CPU 0 will be left idle. (insert dramatic and sinister music here) And hence our CPU usage timeline looks sad: CPU 0 CPU 1 BBDDBBDDBBDD  ... So what should a poor multi-queue multiprocessor scheduler do? How can we overcome the insidious problem of load imbalance and defeat t he evil forces of ... the Decepticons1? How do we stop asking questions that are hardly relevant to this otherwise wonderful book? 1Little known fact is that the home planet of Cybertron was destroyed b y bad CPU scheduling decisions. And now let that be the ﬁrst and last reference to Trans formers in this book, for which we sincerely apologize. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 9 CRUX: HOWTODEAL WITHLOAD IMBALANCE How should a multi-queue multiprocessor scheduler handle load im - balance, so as to better achieve its desired scheduling goals ? The obvious answer to this query is to move jobs around, a technique which we (once again) refer to as migration . By migrating a job from one CPU to another, true load balance can be achieved. Let’s look at a couple of examples to add some clarity. Once again, we have a situation where one CPU is idle and the other has some jobs. Q0 Q1 B D In this case, the desired migration is easy to understand: the OS should simply move one of BorDto CPU 0. The result of this single job migra- tion is evenly balanced load and everyone is happy. A more tricky case arises in our earlier example, where Awas left alone on CPU 0 and BandDwere alternating on CPU 1: Q0 A Q1 B D In this case, a single migration does not solve the problem. What would you do in this case? The answer, alas, is continuous migrati on of one or more jobs. One possible solution is to keep switching jobs, as we see in the following timeline. In the ﬁgure, ﬁrst Ais alone on CPU 0, andBandDalternate on CPU 1. After a few time slices, Bis moved to compete with Aon CPU 0, while Denjoys a few time slices alone on CPU 1. And thus load is balanced: CPU 0 CPU 1AAAABABABBBB BDBDDDDDADAD  ...",3237
10. Multi-CPU Scheduling,"... Of course, many other possible migration patterns exist. But now f or the tricky part: how should the system decide to enact such a mig ration? One basic approach is to use a technique known as work stealing [FLR98]. With a work-stealing approach, a (source) queue that i s low on jobs will occasionally peek at another (target) queue, to see how full it is. If the target queue is (notably) more full than the source q ueue, the source will “steal” one or more jobs from the target to help balance l oad. Of course, there is a natural tension in such an approach. If you look around at other queues too often, you will suffer from high overhead and have trouble scaling, which was the entire purpose of implem enting c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 MULTIPROCESSOR SCHEDULING (ADVANCED ) the multiple queue scheduling in the ﬁrst place. If, on the othe r hand, you don’t look at other queues very often, you are in danger of suffering from severe load imbalances. Finding the right threshold remai ns, as is common in system policy design, a black art. 10.6 Linux Multiprocessor Schedulers Interestingly, in the Linux community, no common solution has ap- proached to building a multiprocessor scheduler. Over time, th ree dif- ferent schedulers arose: the O(1) scheduler, the Completely F air Sched- uler (CFS), and the BF Scheduler (BFS)2. See Meehean’s dissertation for an excellent overview of the strengths and weaknesses of said sc hedulers [M11]; here we just summarize a few of the basics. Both O(1) and CFS use multiple queues, whereas BFS uses a singl e queue, showing that both approaches can be successful. Of course , there are many other details which separate these schedulers. For ex ample, the O(1) scheduler is a priority-based scheduler (similar to the MLFQ dis- cussed before), changing a process’s priority over time and then s chedul- ing those with highest priority in order to meet various scheduli ng objec- tives; interactivity is a particular focus. CFS, in contrast, i s a deterministic proportional-share approach (more like Stride scheduling, as dis cussed earlier). BFS, the only single-queue approach among the three, i s also proportional-share, but based on a more complicated scheme known as Earliest Eligible Virtual Deadline First (EEVDF) [SA96]. Re ad more about these modern algorithms on your own; you should be able to understand how they work now. 10.7 Summary We have seen various approaches to multiprocessor scheduling. T he single-queue approach (SQMS) is rather straightforward to buil d and bal- ances load well but inherently has difﬁculty with scaling to m any pro- cessors and cache afﬁnity. The multiple-queue approach (MQMS) scales better and handles cache afﬁnity well, but has trouble with loa d imbal- ance and is more complicated. Whichever approach you take, there is no simple answer: building a general purpose scheduler remains a daunting task, as small code changes can lead to large behavioral differ ences. Only undertake such an exercise if you know exactly what you are doing, or, at least, are getting paid a large amount of money to do so.",3139
10. Multi-CPU Scheduling,"2Look up what BF stands for on your own; be forewarned, it is not for the faint of heart. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 11 References [A90] “The Performance of Spin Lock Alternatives for Shared-Memory Multipr ocessors” by Thomas E. Anderson. IEEE TPDS Volume 1:1, January 1990. A classic paper on how different locking alternatives do and don’t scale. By Tom Anderson, very well known re searcher in both systems and networking. And author of a very ﬁne OS textbook, we must say. [B+10] “An Analysis of Linux Scalability to Many Cores Abstract” by S ilas Boyd-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek , Robert Morris, Nick- olai Zeldovich. OSDI ’10, Vancouver, Canada, October 2010. A terriﬁc modern paper on the difﬁculties of scaling Linux to many cores. [CSG99] “Parallel Computer Architecture: A Hardware/Software Ap proach” by David E. Culler, Jaswinder Pal Singh, and Anoop Gupta. Morgan Kaufmann, 1999. A treasure ﬁlled with details about parallel machines and algorithms. As Mark Hill humorously observes on the jacket, the book contains more information than most research papers. [FLR98] “The Implementation of the Cilk-5 Multithreaded Language” by Matteo Frigo, Charles E. Leiserson, Keith Randall. PLDI ’98, Montreal, Canada, June 1998. Cilk is a lightweight language and runtime for writing parallel programs, and an excellent examp le of the work-stealing paradigm. [G83] “Using Cache Memory To Reduce Processor-Memory Trafﬁc” by James R. G oodman. ISCA ’83, Stockholm, Sweden, June 1983. The pioneering paper on how to use bus snooping, i.e., paying attention to requests you see on the bus, to build a cache coherence protoc ol. Goodman’s research over many years at Wisconsin is full of cleverness, this being but one e xample. [M11] “Towards Transparent CPU Scheduling” by Joseph T. Meehean. Doctoral Dissertation at University of Wisconsin—Madison, 2011. A dissertation that covers a lot of the details of how modern Linux multiprocessor scheduling works. Pretty awesome. But, as co-ad visors of Joe’s, we may be a bit biased here. [SHW11] “A Primer on Memory Consistency and Cache Coherence” by Daniel J. Sor in, Mark D. Hill, and David A. Wood. Synthesis Lectures in Computer Architect ure. Morgan and Clay- pool Publishers, May 2011. A deﬁnitive overview of memory consistency and multiprocessor caching. Required reading for anyone who likes to know way too much about a given topic. [SA96] “Earliest Eligible Virtual Deadline First: A Flexible and Accurate Mechanism for Pro- portional Share Resource Allocation” by Ion Stoica and Hussein Abde l-Wahab. Technical Re- port TR-95-22, Old Dominion University, 1996. A tech report on this cool scheduling idea, from Ion Stoica, now a professor at U.C. Berkeley and world expert in networking, di stributed systems, and many other things. [TTG95] “Evaluating the Performance of Cache-Afﬁnity Scheduling in Shared-Memo ry Mul- tiprocessors” by Josep Torrellas, Andrew Tucker, Anoop Gupta. Jou rnal of Parallel and Dis- tributed Computing, Volume 24:2, February 1995.",3129
10. Multi-CPU Scheduling,"This is not the ﬁrst paper on the topic, but it has citations to earlier work, and is a more readable and practical paper than some of the ear lier queuing- based analysis papers. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 MULTIPROCESSOR SCHEDULING (ADVANCED ) Homework (Simulation) In this homework, we’ll use multi.py to simulate a multi-processor CPU scheduler, and learn about some of its details. Read the rela ted README for more information about the simulator and its options. Questions 1. To start things off, let’s learn how to use the simulator to stud y how to build an effective multi-processor scheduler. The ﬁrst simulation wil l run just one job, which has a run-time of 30, and a working-set size of 200. Run this job (called job ’a’ here) on one simulated CPU as follows: ./multi.py -n 1 -L a:30:200 . How long will it take to complete? Turn on the -cﬂag to see a ﬁnal answer, and the -tﬂag to see a tick-by-tick trace of the job and how it is scheduled. 2. Now increase the cache size so as to make the job’s working se t (size=200) ﬁt into the cache (which, by default, is size=100); for example, run ./multi.py -n 1 -L a:30:200 -M 300 . Can you predict how fast the job will run once it ﬁts in cache? (hint: remember the key parameter of the war mrate, which is set by the -r ﬂag) Check your answer by running with the s olve ﬂag (-c) enabled. 3. One cool thing about multi.py is that you can see more detail about what is going on with different tracing ﬂags. Run the same simulation a s above, but this time with time left tracing enabled ( -T). This ﬂag shows both the job that was scheduled on a CPU at each time step, as well as how much run-time that job has left after each tick has run. What do you not ice about how that second column decreases? 4. Now add one more bit of tracing, to show the status of each CPU c ache for each job, with the -Cﬂag. For each job, each cache will either show a blank space (if the cache is cold for that job) or a ’w’ (if the cache i s warm for that job). At what point does the cache become warm for job ’a’ in this simple example? What happens as you change the warmup time parameter ( -w) to lower or higher values than the default? 5. At this point, you should have a good idea of how the simulator wo rks for a single job running on a single CPU. But hey, isn’t this a multi-pro cessor CPU scheduling chapter? Oh yeah. So let’s start working with mul tiple jobs. Speciﬁcally, let’s run the following three jobs on a two-CPU s ystem (i.e., type ./multi.py -n 2 -L a:100:100,b:100:50,c:100:50 ) Can you pre- dict how long this will take, given a round-robin centralized scheduler? Use -cto see if you were right, and then dive down into details with -tto see a step-by-step and then -Cto see whether caches got warmed effectively for these jobs. What do you notice? 6. Now we’ll apply some explicit controls to study cache afﬁnity , as described in the chapter. To do this, you’ll need the -Aﬂag. This ﬂag can be used to limit which CPUs the scheduler can place a particular job upon. I n this case, let’s use it to place jobs ’b’ and ’c’ on CPU 1, while restr icting ’a’ to CPU 0.",3164
10. Multi-CPU Scheduling,"This magic is accomplished by typing this ./multi.py -n 2 -L a:100:100,b:100:50, c:100:50 -A a:0,b:1,c:1 ; don’t forget to turn on various tracing options to see what is really happening . Can you OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MULTIPROCESSOR SCHEDULING (ADVANCED ) 13 predict how fast this version will run? Why does it do better? W ill other combinations of ’a’, ’b’, and ’c’ onto the two processors run fa ster or slower? 7. One interesting aspect of caching multiprocessors is the op portunity for better-than-expected speed up of jobs when using multiple CPUs (an d their caches) as compared to running jobs on a single processor. Spec iﬁcally, when you run on NCPUs, sometimes you can speed up by more than a fac- tor ofN, a situation entitled super-linear speedup . To experiment with this, use the job description here ( -L a:100:100,b:100:100,c:100:100 ) with a small cache ( -M 50 ) to create three jobs. Run this on systems with 1, 2, and 3 CPUs (-n 1 ,-n 2 ,-n 3 ). Now, do the same, but with a larger per-CPU cache of size 100. What do you notice about performance as the numb er of CPUs scales? Use -cto conﬁrm your guesses, and other tracing ﬂags to dive even deeper. 8. One other aspect of the simulator worth studying is the per-CP U scheduling option, the -pﬂag. Run with two CPUs again, and this three job conﬁgu- ration (-L a:100:100,b:100:50,c:100:50 ). How does this option do, as opposed to the hand-controlled afﬁnity limits you put in pla ce above? How does performance change as you alter the ’peek interval’ ( -P) to lower or higher values? How does this per-CPU approach work as the numb er of CPUs scales? 9. Finally, feel free to just generate random workloads and se e if you can pre- dict their performance on different numbers of processors, cac he sizes, and scheduling options. If you do this, you’ll soon be a multi-processor schedul- ing master , which is a pretty awesome thing to be. Good luck. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",2008
11. Summary Dialogue on CPU Virtualization,"11 Summary Dialogue on CPU Virtualization Professor: So, Student, did you learn anything? Student: Well, Professor, that seems like a loaded question. I think you only want me to say “yes.” Professor: That’s true. But it’s also still an honest question. Come on, give a professor a break, will you? Student: OK, OK. I think I did learn a few things. First, I learned a little about how the OS virtualizes the CPU. There are a bunch of important mechanisms that I had to understand to make sense of this: traps and trap ha ndlers, timer interrupts, and how the OS and the hardware have to carefully save and restore state when switching between processes. Professor: Good, good. Student: All those interactions do seem a little complicated though; how can I learn more? Professor: Well, that’s a good question. I think there is no substitute for doing; just reading about these things doesn’t quite give you the proper s ense. Do the class projects and I bet by the end it will all kind of make sense. Student: Sounds good. What else can I tell you? Professor: Well, did you get some sense of the philosophy of the OS in your quest to understand its basic machinery? Student: Hmm... I think so. It seems like the OS is fairly paranoid. It wants to make sure it stays in charge of the machine. While it wants a progra m to run as efﬁciently as possible (and hence the whole reasoning behind limited direct execution ), the OS also wants to be able to say “Ah. Not so fast my friend” in case of an errant or malicious process. Paranoia rules the day, an d certainly keeps the OS in charge of the machine. Perhaps that is why we think o f the OS as a resource manager. Professor: Yes indeed — sounds like you are starting to put it together. Nice. Student: Thanks. 1 2 S UMMARY DIALOGUE ON CPU V IRTUALIZATION Professor: And what about the policies on top of those mechanisms — any interesting lessons there? Student: Some lessons to be learned there for sure. Perhaps a little obvious, b ut obvious can be good. Like the notion of bumping short jobs to the fron t of the queue — I knew that was a good idea ever since the one time I was buyin g some gum at the store, and the guy in front of me had a credit card that wo uldn’t work. He was no short job, let me tell you. Professor: That sounds oddly rude to that poor fellow. What else? Student: Well, that you can build a smart scheduler that tries to be like SJF and RR all at once — that MLFQ was pretty neat. Building up a real sch eduler seems difﬁcult. Professor: Indeed it is. That’s why there is still controversy to this day over which scheduler to use; see the Linux battles between CFS, BFS, an d the O(1) scheduler, for example. And no, I will not spell out the full name of BFS . Student: And I won’t ask you to. These policy battles seem like they could rage forever; is there really a right answer? Professor: Probably not. After all, even our own metrics are at odds: if your scheduler is good at turnaround time, it’s bad at response time, an d vice versa. As Lampson said, perhaps the goal isn’t to ﬁnd the best solution, bu t rather to avoid disaster. Student: That’s a little depressing. Professor: Good engineering can be that way. And it can also be uplifting. It’s just your perspective on it, really. I personally think being prag matic is a good thing, and pragmatists realize that not all problems have clean and easy solutions. Anything else that caught your fancy? Student: I really liked the notion of gaming the scheduler; it seems like that might be something to look into when I’m next running a job on Amazon’s EC2 service. Maybe I can steal some cycles from some other unsuspect ing (and more importantly, OS-ignorant) customer. Professor: It looks like I might have created a monster. Professor Frankenste in is not what I’d like to be called, you know. Student: But isn’t that the idea? To get us excited about something, so much so that we look into it on our own? Lighting ﬁres and all that? Professor: I guess so. But I didn’t think it would work. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4083
7. CPU Scheduling,"7 Scheduling: Introduction By now low-level mechanisms of running processes (e.g., context switch- ing) should be clear; if they are not, go back a chapter or two, and r ead the description of how that stuff works again. However, we have yet to u n- derstand the high-level policies that an OS scheduler employs. We will now do just that, presenting a series of scheduling policies (sometimes called disciplines ) that various smart and hard-working people have de- veloped over the years. The origins of scheduling, in fact, predate computer systems; e arly approaches were taken from the ﬁeld of operations management and a p- plied to computers. This reality should be no surprise: assemb ly lines and many other human endeavors also require scheduling, and ma ny of the same concerns exist therein, including a laser-like desi re for efﬁciency. And thus, our problem: THECRUX: HOWTODEVELOP SCHEDULING POLICY How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metri cs are important? What basic approaches have been used in the earlies t of com- puter systems? 7.1 Workload Assumptions Before getting into the range of possible policies, let us ﬁrst ma ke a number of simplifying assumptions about the processes running i n the system, sometimes collectively called the workload . Determining the workload is a critical part of building policies, and the more you kn ow about workload, the more ﬁne-tuned your policy can be. The workload assumptions we make here are mostly unrealistic, bu t that is alright (for now), because we will relax them as we go, and even- tually develop what we will refer to as ... (dramatic pause) ... 1 2 SCHEDULING : INTRODUCTION afully-operational scheduling discipline1. We will make the following assumptions about the processes, some- times called jobs , that are running in the system: 1. Each job runs for the same amount of time. 2. All jobs arrive at the same time. 3. Once started, each job runs to completion. 4. All jobs only use the CPU (i.e., they perform no I/O) 5. The run-time of each job is known. We said many of these assumptions were unrealistic, but just as some animals are more equal than others in Orwell’s Animal Farm [O45], some assumptions are more unrealistic than others in this chapter. I n particu- lar, it might bother you that the run-time of each job is known: this would make the scheduler omniscient, which, although it would be grea t (prob- ably), is not likely to happen anytime soon. 7.2 Scheduling Metrics Beyond making workload assumptions, we also need one more thing to enable us to compare different scheduling policies: a scheduling met- ric. A metric is just something that we use to measure something, and there are a number of different metrics that make sense in sche duling. For now, however, let us also simplify our life by simply having a s in- gle metric: turnaround time . The turnaround time of a job is deﬁned as the time at which the job completes minus the time at which the job arrived in the system. More formally, the turnaround time Tturnaround is: Tturnaround =Tcompletion −Tarrival (7.1) Because we have assumed that all jobs arrive at the same time, f or now Tarrival= 0 and hence Tturnaround =Tcompletion .",3272
7. CPU Scheduling,"This fact will change as we relax the aforementioned assumptions. You should note that turnaround time is a performance metric, which will be our primary focus this chapter. Another metric of interes t isfair- ness , as measured (for example) by Jain’s Fairness Index [J91]. Perfor- mance and fairness are often at odds in scheduling; a scheduler , for ex- ample, may optimize performance but at the cost of preventing a fe w jobs from running, thus decreasing fairness. This conundrum shows u s that life isn’t always perfect. 7.3 First In, First Out (FIFO) The most basic algorithm we can implement is known as First In, First Out (FIFO ) scheduling or sometimes First Come, First Served (FCFS ). 1Said in the same way you would say “A fully-operational Death Star .” OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : INTRODUCTION 3 FIFO has a number of positive properties: it is clearly simple an d thus easy to implement. And, given our assumptions, it works pretty w ell. Let’s do a quick example together. Imagine three jobs arrive in t he system, A, B, and C, at roughly the same time ( Tarrival= 0). Because FIFO has to put some job ﬁrst, let’s assume that while they all arr ived simultaneously, A arrived just a hair before B which arrived ju st a hair before C. Assume also that each job runs for 10 seconds. What will t he average turnaround time be for these jobs? 0 20 40 60 80 100 120 TimeABC Figure 7.1: FIFO Simple Example From Figure 7.1, you can see that A ﬁnished at 10, B at 20, and C at 3 0. Thus, the average turnaround time for the three jobs is simply10+20+30 3= 20. Computing turnaround time is as easy as that. Now let’s relax one of our assumptions. In particular, let’s relax as - sumption 1, and thus no longer assume that each job runs for the sam e amount of time. How does FIFO perform now? What kind of workload could you construct to make FIFO perform poorly? (think about this before reading on ... keep thinking ... got it?.) Presumably you’ve ﬁgured this out by now, but just in case, let’s do an example to show how jobs of different lengths can lead to trouble for FIFO scheduling. In particular, let’s again assume three jobs (A, B, and C), but this time A runs for 100 seconds while B and C run for 10 each . 0 20 40 60 80 100 120 TimeA BC Figure 7.2: Why FIFO Is Not That Great As you can see in Figure 7.2, Job A runs ﬁrst for the full 100 seconds before B or C even get a chance to run. Thus, the average turnaroun d time for the system is high: a painful 110 seconds (100+110+120 3= 110 ). This problem is generally referred to as the convoy effect [B+79], where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer. This scheduling scen ario might remind you of a single line at a grocery store and what you feel like w hen c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 SCHEDULING : INTRODUCTION TIP: THEPRINCIPLE OF SJF Shortest Job First represents a general scheduling principle t hat can be applied to any system where the perceived turnaround time per customer (or, in our case, a job) matters. Think of any line you have waited in : if the establishment in question cares about customer satisfacti on, it is likely they have taken SJF into account.",3287
7. CPU Scheduling,"For example, grocery stores common ly have a “ten-items-or-less” line to ensure that shoppers with on ly a few things to purchase don’t get stuck behind the family preparing for some upcoming nuclear winter. you see the person in front of you with three carts full of provisions an d a checkbook out; it’s going to be a while2. So what should we do? How can we develop a better algorithm to deal with our new reality of jobs that run for different amounts of ti me? Think about it ﬁrst; then read on. 7.4 Shortest Job First (SJF) It turns out that a very simple approach solves this problem; in fa ct it is an idea stolen from operations research [C54,PV56] and appl ied to scheduling of jobs in computer systems. This new scheduling dis cipline is known as Shortest Job First (SJF) , and the name should be easy to remember because it describes the policy quite completely: it runs the shortest job ﬁrst, then the next shortest, and so on. 0 20 40 60 80 100 120 TimeBC A Figure 7.3: SJF Simple Example Let’s take our example above but with SJF as our scheduling policy. Figure 7.3 shows the results of running A, B, and C. Hopefully the dia- gram makes it clear why SJF performs much better with regards to aver- age turnaround time. Simply by running B and C before A, SJF reduce s average turnaround from 110 seconds to 50 (10+20+120 3= 50 ), more than a factor of two improvement. In fact, given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm. How- ever, you are in a systems class, not theory or operations research; no proofs are allowed. 2Recommended action in this case: either quickly switch to a different li ne, or take a long, deep, and relaxing breath. That’s right, breathe in, breathe out. It wi ll be OK, don’t worry. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : INTRODUCTION 5 ASIDE : PREEMPTIVE SCHEDULERS In the old days of batch computing, a number of non-preemptive sched- ulers were developed; such systems would run each job to completi on before considering whether to run a new job. Virtually all modern sched- ulers are preemptive , and quite willing to stop one process from run- ning in order to run another. This implies that the scheduler em ploys the mechanisms we learned about previously; in particular, the sc heduler can perform a context switch , stopping one running process temporarily and resuming (or starting) another. Thus we arrive upon a good approach to scheduling with SJF, but our assumptions are still fairly unrealistic. Let’s relax another . In particular, we can target assumption 2, and now assume that jobs can arrive at any time instead of all at once. What problems does this lead to? (Another pause to think ... are you thinking? Come on, you can do it ) Here we can illustrate the problem again with an example. This time, assume A arrives at t= 0 and needs to run for 100 seconds, whereas B and C arrive at t= 10 and each need to run for 10 seconds. With pure SJF, we’d get the schedule seen in Figure 7.4.",3058
7. CPU Scheduling,"0 20 40 60 80 100 120 TimeA BC[B,C arrive] Figure 7.4: SJF With Late Arrivals From B and C As you can see from the ﬁgure, even though B and C arrived shortly after A, they still are forced to wait until A has completed, and thus suffer the same convoy problem. Average turnaround time for these three j obs is 103.33 seconds (100+(110 −10)+(120 −10) 3). What can a scheduler do? 7.5 Shortest Time-to-Completion First (STCF) To address this concern, we need to relax assumption 3 (that jobs must run to completion), so let’s do that. We also need some machinery w ithin the scheduler itself. As you might have guessed, given our prev ious dis- cussion about timer interrupts and context switching, the sche duler can certainly do something else when B and C arrive: it can preempt job A and decide to run another job, perhaps continuing A later. SJF by ou r deﬁ- nition is a non-preemptive scheduler, and thus suffers from the problems described above. Fortunately, there is a scheduler which does exactly that: add preemp- tion to SJF, known as the Shortest Time-to-Completion First (STCF ) or Preemptive Shortest Job First (PSJF ) scheduler [CK68]. Any time a new c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 SCHEDULING : INTRODUCTION 0 20 40 60 80 100 120 TimeABC A[B,C arrive] Figure 7.5: STCF Simple Example job enters the system, the STCF scheduler determines which of th e re- maining jobs (including the new job) has the least time left, an d schedules that one. Thus, in our example, STCF would preempt A and run B and C to completion; only when they are ﬁnished would A’s remaining time be scheduled. Figure 7.5 shows an example. The result is a much-improved average turnaround time: 50 secon ds ((120−0)+(20−10)+(30 −10) 3). And as before, given our new assumptions, STCF is provably optimal; given that SJF is optimal if all jobs arriv e at the same time, you should probably be able to see the intuition beh ind the optimality of STCF. 7.6 A New Metric: Response Time Thus, if we knew job lengths, and that jobs only used the CPU, and ou r only metric was turnaround time, STCF would be a great policy. In fa ct, for a number of early batch computing systems, these types of sche duling algorithms made some sense. However, the introduction of time-sha red machines changed all that. Now users would sit at a terminal and de- mand interactive performance from the system as well. And thus , a new metric was born: response time . We deﬁne response time as the time from when the job arrives in a system to the ﬁrst time it is scheduled3. More formally: Tresponse=Tfirstrun−Tarrival (7.2) For example, if we had the schedule above (with A arriving at tim e 0, and B and C at time 10), the response time of each job is as follows: 0 f or job A, 0 for B, and 10 for C (average: 3.33). As you might be thinking, STCF and related disciplines are not pa r- ticularly good for response time. If three jobs arrive at the same t ime, for example, the third job has to wait for the previous two jobs to ru nin their entirety before being scheduled just once.",3075
7. CPU Scheduling,"While great for turnaround time, this approach is quite bad for response time and interacti vity. In- deed, imagine sitting at a terminal, typing, and having to wa it 10 seconds 3Some deﬁne it slightly differently, e.g., to also include the time unt il the job produces some kind of “response”; our deﬁnition is the best-case version of t his, essentially assuming that the job produces a response instantaneously. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : INTRODUCTION 7 0 5 10 15 20 25 30 TimeA B C Figure 7.6: SJF Again (Bad for Response Time) 0 5 10 15 20 25 30 TimeABCABCABCABCABC Figure 7.7: Round Robin (Good For Response Time) to see a response from the system just because some other job got sche d- uled in front of yours: not too pleasant. Thus, we are left with another problem: how can we build a schedul er that is sensitive to response time? 7.7 Round Robin To solve this problem, we will introduce a new scheduling algorit hm, classically referred to as Round-Robin (RR) scheduling [K64]. The basic idea is simple: instead of running jobs to completion, RR runs a job for a time slice (sometimes called a scheduling quantum ) and then switches to the next job in the run queue. It repeatedly does so until the j obs are ﬁnished. For this reason, RR is sometimes called time-slicing . Note that the length of a time slice must be a multiple of the timer-interr upt period; thus if the timer interrupts every 10 milliseconds, the time s lice could be 10, 20, or any other multiple of 10 ms. To understand RR in more detail, let’s look at an example. Assume three jobs A, B, and C arrive at the same time in the system, and t hat they each wish to run for 5 seconds. An SJF scheduler runs each job t o completion before running another (Figure 7.6). In contrast, RR w ith a time-slice of 1 second would cycle through the jobs quickly (Figur e 7.7). The average response time of RR is:0+1+2 3= 1; for SJF, average re- sponse time is:0+5+10 3= 5. As you can see, the length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time m etric. However, making the time slice too short is problematic: suddenl y the cost of context switching will dominate overall performance. Thus , de- ciding on the length of the time slice presents a trade-off to a sy stem de- signer, making it long enough to amortize the cost of switching without making it so long that the system is no longer responsive. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 SCHEDULING : INTRODUCTION TIP: AMORTIZATION CANREDUCE COSTS The general technique of amortization is commonly used in systems when there is a ﬁxed cost to some operation. By incurring that cost l ess often (i.e., by performing the operation fewer times), the total c ost to the system is reduced. For example, if the time slice is set to 10 ms , and the context-switch cost is 1 ms, roughly 10 percent of time is spent context sw itch- ing and is thus wasted. If we want to amortize this cost, we can increase the time slice, e.g., to 100 ms. In this case, less than 1 percent of tim e is spent context switching, and thus the cost of time-slicing has been am ortized. Note that the cost of context switching does not arise solely from the OS actions of saving and restoring a few registers.",3329
7. CPU Scheduling,"When programs run, they build up a great deal of state in CPU caches, TLBs, branch p redictors, and other on-chip hardware. Switching to another job causes this s tate to be ﬂushed and new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost [MB91] . RR, with a reasonable time slice, is thus an excellent schedul er if re- sponse time is our only metric. But what about our old friend turnarou nd time? Let’s look at our example above again. A, B, and C, each with ru n- ning times of 5 seconds, arrive at the same time, and RR is the sch eduler with a (long) 1-second time slice. We can see from the picture abov e that A ﬁnishes at 13, B at 14, and C at 15, for an average of 14. Pretty aw ful. It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric. Intuitively, this should make se nse: what RR is doing is stretching out each job as long as it can, by only runni ng each job for a short bit before moving to the next. Because turnaroun d time only cares about when jobs ﬁnish, RR is nearly pessimal, eve n worse than simple FIFO in many cases. More generally, any policy (such as RR) that is fair, i.e., that evenly di- vides the CPU among active processes on a small time scale, will p erform poorly on metrics such as turnaround time. Indeed, this is an inhe rent trade-off: if you are willing to be unfair, you can run shorter jobs to com- pletion, but at the cost of response time; if you instead value fair ness, response time is lowered, but at the cost of turnaround time. This t ype of trade-off is common in systems; you can’t have your cake and eat it too4. We have developed two types of schedulers. The ﬁrst type (SJF, STC F) optimizes turnaround time, but is bad for response time. The secon d type (RR) optimizes response time but is bad for turnaround. And we sti ll have two assumptions which need to be relaxed: assumption 4 (th at jobs do no I/O), and assumption 5 (that the run-time of each job is known ). Let’s tackle those assumptions next. 4A saying that confuses people, because it should be “You can’t keep your cake and eat it too” (which is kind of obvious, no?). Amazingly, there is a wikipedia pa ge about this saying; even more amazingly, it is kind of fun to read [W15]. As they say in Ita lian, you can’t Avere la botte piena e la moglie ubriaca. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : INTRODUCTION 9 TIP: OVERLAP ENABLES HIGHER UTILIZATION When possible, overlap operations to maximize the utilization of sys- tems. Overlap is useful in many different domains, including when per- forming disk I/O or sending messages to remote machines; in eith er case, starting the operation and then switching to other work is a good ide a, and improves the overall utilization and efﬁciency of the system . 7.8 Incorporating I/O First we will relax assumption 4 — of course all programs perform I/O. Imagine a program that didn’t take any input: it would produc e the same output each time. Imagine one without output: it is the prover bial tree falling in the forest, with no one to see it; it doesn’t matter that it ran.",3177
7. CPU Scheduling,"A scheduler clearly has a decision to make when a job initiates a n I/O request, because the currently-running job won’t be using the C PU dur- ing the I/O; it is blocked waiting for I/O completion. If the I/O is sent to a hard disk drive, the process might be blocked for a few millisec onds or longer, depending on the current I/O load of the drive. Thus, the s ched- uler should probably schedule another job on the CPU at that time. The scheduler also has to make a decision when the I/O completes . When that occurs, an interrupt is raised, and the OS runs and mov es the process that issued the I/O from blocked back to the ready sta te. Of course, it could even decide to run the job at that point. How should t he OS treat each job? To understand this issue better, let us assume we have two jobs , A and B, which each need 50 ms of CPU time. However, there is one obvious difference: A runs for 10 ms and then issues an I/O request (ass ume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 m s and performs no I/O. The scheduler runs A ﬁrst, then B after (Figur e 7.8). 0 20 40 60 80 100 120 140 TimeA A A A A B BBBB CPU Disk Figure 7.8: Poor Use Of Resources Assume we are trying to build a STCF scheduler. How should such a scheduler account for the fact that A is broken up into 5 10-ms sub -jobs, whereas B is just a single 50-ms CPU demand? Clearly, just run ning one job and then the other without considering how to take I/O into accou nt makes little sense. A common approach is to treat each 10-ms sub-job of A as an indepen- dent job. Thus, when the system starts, its choice is whether to schedule c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 SCHEDULING : INTRODUCTION 0 20 40 60 80 100 120 140 TimeA A A A A B B B B B CPU Disk Figure 7.9: Overlap Allows Better Use Of Resources a 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the short er one, in this case A. Then, when the ﬁrst sub-job of A has completed, only B is left, and it begins running. Then a new sub-job of A is submit ted, and it preempts B and runs for 10 ms. Doing so allows for overlap , with the CPU being used by one process while waiting for the I/O of anothe r process to complete; the system is thus better utilized (see Fi gure 7.9). And thus we see how a scheduler might incorporate I/O. By treatin g each CPU burst as a job, the scheduler makes sure processes that are “in- teractive” get run frequently. While those interactive jobs a re performing I/O, other CPU-intensive jobs run, thus better utilizing the p rocessor. 7.9 No More Oracle With a basic approach to I/O in place, we come to our ﬁnal assump- tion: that the scheduler knows the length of each job. As we said be fore, this is likely the worst assumption we could make. In fact, in a ge neral- purpose OS (like the ones we care about), the OS usually knows very little about the length of each job. Thus, how can we build an approach that be- haves like SJF/STCF without such a priori knowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good? 7.10 Summary We have introduced the basic ideas behind scheduling and deve loped two families of approaches. The ﬁrst runs the shortest job remain ing and thus optimizes turnaround time; the second alternates between all jobs and thus optimizes response time.",3393
7. CPU Scheduling,"Both are bad where the other is g ood, alas, an inherent trade-off common in systems. We have also seen how we might incorporate I/O into the picture, but have still not solved the prob- lem of the fundamental inability of the OS to see into the future . Shortly, we will see how to overcome this problem, by building a scheduler t hat uses the recent past to predict the future. This scheduler is known as the multi-level feedback queue , and it is the topic of the next chapter. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SCHEDULING : INTRODUCTION 11 References [B+79] “The Convoy Phenomenon” by M. Blasgen, J. Gray, M. Mitoma, T. Price. ACM Op - erating Systems Review, 13:2, April 1979. Perhaps the ﬁrst reference to convoys, which occurs in databases as well as the OS. [C54] “Priority Assignment in Waiting Line Problems” by A. Cobham. Jou rnal of Operations Research, 2:70, pages 70–76, 1954. The pioneering paper on using an SJF approach in scheduling the repair of machines. [K64] “Analysis of a Time-Shared Processor” by Leonard Kleinrock. Nav al Research Logistics Quarterly, 11:1, pages 59–73, March 1964. May be the ﬁrst reference to the round-robin scheduling algorithm; certainly one of the ﬁrst analyses of said approach to scheduling a ti me-shared system. [CK68] “Computer Scheduling Methods and their Countermeasures” by Ed ward G. Coffman and Leonard Kleinrock. AFIPS ’68 (Spring), April 1968. An excellent early introduction to and analysis of a number of basic scheduling disciplines. [J91] “The Art of Computer Systems Performance Analysis: Techniques for Ex perimental De- sign, Measurement, Simulation, and Modeling” by R. Jain. Interscience, Ne w York, April 1991. The standard text on computer systems measurement. A great reference for y our library, for sure. [O45] “Animal Farm” by George Orwell. Secker and Warburg (London), 1945 .A great but depressing allegorical book about power and its corruptions. Some say it is a c ritique of Stalin and the pre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs. [PV56] “Machine Repair as a Priority Waiting-Line Problem” by Thomas E. Phipp s Jr., W. R. Van Voorhis. Operations Research, 4:1, pages 76–86, February 195 6.Follow-on work that gen- eralizes the SJF approach to machine repair from Cobham’s original work; also postu lates the utility of an STCF approach in such an environment. Speciﬁcally, “There are certain types of repair work, ... involving much dismantling and covering the ﬂoor with nuts and bolts, which certainly should not be interrupted once undertaken; in other cases it would be inadvisable to contin ue work on a long job if one or more short ones became available (p.81).” [MB91] “The effect of context switches on cache performance” by Jeffrey C. Mogul, A nita Borg. ASPLOS, 1991. A nice study on how cache performance can be affected by context switching; less of an issue in today’s systems where processors issue billions of instruction s per second but context-switches still happen in the millisecond time range. [W15] “You can’t have your cake and eat it” by Authors: Unknown.. Wikiped ia (as of Decem- ber 2015).http://en.wikipedia.org/wiki/You can’thaveyourcakeandeatit. The best part of this page is reading all the similar idioms from other languag es. In Tamil, you can’t “have both the moustache and drink the soup.” c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 SCHEDULING : INTRODUCTION Homework (Simulation) This program, scheduler.py , allows you to see how different sched- ulers perform under scheduling metrics such as response time, turnaround time, and total wait time. See the README for details. Questions 1. Compute the response time and turnaround time when running three j obs of length 200 with the SJF and FIFO schedulers. 2. Now do the same but with jobs of different lengths: 100, 200, a nd 300. 3. Now do the same, but also with the RR scheduler and a time-slice o f 1. 4. For what types of workloads does SJF deliver the same turnaro und times as FIFO? 5. For what types of workloads and quantum lengths does SJF deli ver the same response times as RR? 6. What happens to response time with SJF as job lengths increa se? Can you use the simulator to demonstrate the trend? 7. What happens to response time with RR as quantum lengths incre ase? Can you write an equation that gives the worst-case response time, givenNjobs? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4438
12. A Dialogue on Memory Virtualization,"12 A Dialogue on Memory Virtualization Student: So, are we done with virtualization? Professor: No. Student: Hey, no reason to get so excited; I was just asking a question. Stud ents are supposed to do that, right? Professor: Well, professors do always say that, but really they mean this: ask questions, ifthey are good questions, and you have actually put a little thought into them. Student: Well, that sure takes the wind out of my sails. Professor: Mission accomplished. In any case, we are not nearly done with virtualization. Rather, you have just seen how to virtualize the CPU, but really there is a big monster waiting in the closet: memory. Virtualizing memor y is complicated and requires us to understand many more intricate det ails about how the hardware and OS interact. Student: That sounds cool. Why is it so hard? Professor: Well, there are a lot of details, and you have to keep them straight in your head to really develop a mental model of what is going on. We’ll s tart simple, with very basic techniques like base/bounds, and slowly add co mplexity to tackle new challenges, including fun topics like TLBs and multi-level pa ge tables. Eventually, we’ll be able to describe the workings of a fully-func tional modern virtual memory manager. Student: Neat. Any tips for the poor student, inundated with all of this infor- mation and generally sleep-deprived? Professor: For the sleep deprivation, that’s easy: sleep more (and party less ). For understanding virtual memory, start with this: every address generated by a user program is a virtual address . The OS is just providing an illusion to each process, speciﬁcally that it has its own large and private memo ry; with some hardware help, the OS will turn these pretend virtual addres ses into real physical addresses, and thus be able to locate the desired informat ion. 1 2 A D IALOGUE ON MEMORY VIRTUALIZATION Student: OK, I think I can remember that... (to self) every address from a us er program is virtual, every address from a user program is virtual, eve ry ... Professor: What are you mumbling about? Student: Oh nothing.... (awkward pause) ... Anyway, why does the OS wan t to provide this illusion again? Professor: Mostly ease of use : the OS will give each program the view that it has a large contiguous address space to put its code and data into; thus, as a programmer, you never have to worry about things like “where sho uld I store this variable?” because the virtual address space of the program is lar ge and has lots of room for that sort of thing. Life, for a programmer, becomes much more tricky if you have to worry about ﬁtting all of your code data into a small, cro wded memory. Student: Why else? Professor: Well, isolation andprotection are big deals, too. We don’t want one errant program to be able to read, or worse, overwrite, some other program’s memory, do we? Student: Probably not. Unless it’s a program written by someone you don’t like. Professor: Hmmm.... I think we might need to add a class on morals and ethics to your schedule for next semester. Perhaps OS class isn’t getting the right mes- sage across. Student: Maybe we should. But remember, it’s not me who taught us that the proper OS response to errant process behavior is to kill the offendin g process. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3333
13. Address Spaces,"13 The Abstraction: Address Spaces In the early days, building computer systems was easy. Why, you ask? Because users didn’t expect much. It is those darned users with their expectations of “ease of use”, “high performance”, “reliabilit y”, etc., that really have led to all these headaches. Next time you meet one of t hose computer users, thank them for all the problems they have caused . 13.1 Early Systems From the perspective of memory, early machines didn’t provide muc h of an abstraction to users. Basically, the physical memory of the machine looked something like what you see in Figure 13.1. The OS was a set of routines (a library, really) that sat in memory (start- ing at physical address 0 in this example), and there would be on e run- ning program (a process) that currently sat in physical memory ( starting at physical address 64k in this example) and used the rest of me mory. There were few illusions here, and the user didn’t expect much f rom the OS. Life was sure easy for OS developers in those days, wasn’t it? max64KB0KB Current Program (code, data, etc.)Operating System (code, data, etc.) Figure 13.1: Operating Systems: The Early Days 1 2 THEABSTRACTION : ADDRESS SPACES 512KB448KB384KB320KB256KB192KB128KB64KB0KB (free)(free)(free)(free)Operating System (code, data, etc.) Process A (code, data, etc.)Process B (code, data, etc.)Process C (code, data, etc.) Figure 13.2: Three Processes: Sharing Memory 13.2 Multiprogramming and Time Sharing After a time, because machines were expensive, people began t o share machines more effectively. Thus the era of multiprogramming was born [DV66], in which multiple processes were ready to run at a give n time, and the OS would switch between them, for example when one decide d to perform an I/O. Doing so increased the effective utilization of the CPU. Such increases in efﬁciency were particularly important in those days where each machine cost hundreds of thousands or even million s of dollars (and you thought your Mac was expensive.). Soon enough, however, people began demanding more of machines, and the era of time sharing was born [S59, L60, M62, M83]. Speciﬁcally, many realized the limitations of batch computing, particularl y on pro- grammers themselves [CV65], who were tired of long (and hence i neffec- tive) program-debug cycles. The notion of interactivity became impor- tant, as many users might be concurrently using a machine, eac h waiting for (or hoping for) a timely response from their currently-executi ng tasks. One way to implement time sharing would be to run one process for a short while, giving it full access to all memory (Figure 13.1, p age 1), then stop it, save all of its state to some kind of disk (including all of p hysical memory), load some other process’s state, run it for a while, and thus implement some kind of crude sharing of the machine [M+63]. Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grows. While saving and restoring regis ter-level state (the PC, general-purpose registers, etc.) is relative ly fast, saving the entire contents of memory to disk is brutally non-performant. Thu s, what we’d rather do is leave processes in memory while switching betw een them, allowing the OS to implement time sharing efﬁciently (F igure 13.2).",3311
13. Address Spaces,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : ADDRESS SPACES 3 16KB15KB2KB1KB0KB Stack(free)HeapProgram Codethe code segment: where instructions live the heap segment: contains malloc’d data dynamic data structures (it grows downward) (it grows upward) the stack segment: contains local variables arguments to routines,  return values, etc. Figure 13.3: An Example Address Space In the diagram, there are three processes (A, B, and C) and each of them have a small part of the 512KB physical memory carved out for them. Assuming a single CPU, the OS chooses to run one of the process es (say A), while the others (B and C) sit in the ready queue waitin g to run. As time sharing became more popular, you can probably guess that new demands were placed on the operating system. In particular , allow- ing multiple programs to reside concurrently in memory makes protec- tion an important issue; you don’t want a process to be able to read, or worse, write some other process’s memory. 13.3 The Address Space However, we have to keep those pesky users in mind, and doing so requires the OS to create an easy to use abstraction of physical memory. We call this abstraction the address space , and it is the running program’s view of memory in the system. Understanding this fundamental O S ab- straction of memory is key to understanding how memory is virtuali zed. The address space of a process contains all of the memory state of the running program. For example, the code of the program (the instruc- tions) have to live in memory somewhere, and thus they are in the a d- dress space. The program, while it is running, uses a stack to keep track of where it is in the function call chain as well as to allocate loca l variables and pass parameters and return values to and from routines. Fin ally, the heap is used for dynamically-allocated, user-managed memory, such as that you might receive from a call to malloc() in C ornew in an object- oriented language such as C++ or Java. Of course, there are other t hings in there too (e.g., statically-initialized variables), but for now let us just assume those three components: code, stack, and heap. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 THEABSTRACTION : ADDRESS SPACES In the example in Figure 13.3 (page 3), we have a tiny address s pace (only 16KB)1. The program code lives at the top of the address space (starting at 0 in this example, and is packed into the ﬁrst 1K of the ad- dress space). Code is static (and thus easy to place in memory), so we can place it at the top of the address space and know that it won’t need an y more space as the program runs. Next, we have the two regions of the address space that may grow (and shrink) while the program runs. Those are the heap (at the t op) and the stack (at the bottom). We place them like this because each w ishes to be able to grow, and by putting them at opposite ends of the address space, we can allow such growth: they just have to grow in opposite directions.",3018
13. Address Spaces,"The heap thus starts just after the code (at 1KB) an d grows downward (say when a user requests more memory via malloc() ); the stack starts at 16KB and grows upward (say when a user makes a pr oce- dure call). However, this placement of stack and heap is just a c onvention; you could arrange the address space in a different way if you’d lik e (as we’ll see later, when multiple threads co-exist in an address space, no nice way to divide the address space like this works anymore, al as). Of course, when we describe the address space, what we are desc rib- ing is the abstraction that the OS is providing to the running program. The program really isn’t in memory at physical addresses 0 throug h 16KB; rather it is loaded at some arbitrary physical address(es). Ex amine pro- cesses A, B, and C in Figure 13.2; there you can see how each proces s is loaded into memory at a different address. And hence the problem : THECRUX: HOWTOVIRTUALIZE MEMORY How can the OS build this abstraction of a private, potentially la rge address space for multiple running processes (all sharing mem ory) on top of a single, physical memory? When the OS does this, we say the OS is virtualizing memory , because the running program thinks it is loaded into memory at a particul ar ad- dress (say 0) and has a potentially very large address space (s ay 32-bits or 64-bits); the reality is quite different. When, for example, process A in Figure 13.2 tries to perform a load at address 0 (which we will call a virtual address ), somehow the OS, in tandem with some hardware support, will have to make sure the loa d doesn’t actually go to physical address 0 but rather to physica l address 320KB (where A is loaded into memory). This is the key to virtual ization of memory, which underlies every modern computer system in the wor ld. 1We will often use small examples like this because (a) it is a pain t o represent a 32-bit address space and (b) the math is harder. We like simple math. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : ADDRESS SPACES 5 TIP: THEPRINCIPLE OFISOLATION Isolation is a key principle in building reliable systems. If t wo entities are properly isolated from one another, this implies that one can fail w ith- out affecting the other. Operating systems strive to isolate pr ocesses from each other and in this way prevent one from harming the other. By us ing memory isolation, the OS further ensures that running programs c annot affect the operation of the underlying OS. Some modern OS’s take iso- lation even further, by walling off pieces of the OS from other piec es of the OS. Such microkernels [BH70, R+89, S+03] thus may provide greater reliability than typical monolithic kernel designs. 13.4 Goals Thus we arrive at the job of the OS in this set of notes: to virtualiz e memory. The OS will not only virtualize memory, though; it will do s o with style. To make sure the OS does so, we need some goals to guide u s. We have seen these goals before (think of the Introduction), and we ’ll see them again, but they are certainly worth repeating.",3086
13. Address Spaces,"One major goal of a virtual memory (VM) system is transparency2. The OS should implement virtual memory in a way that is invisibl e to the running program. Thus, the program shouldn’t be aware of the fa ct that memory is virtualized; rather, the program behaves as if i t has its own private physical memory. Behind the scenes, the OS (and har dware) does all the work to multiplex memory among many different jobs, an d hence implements the illusion. Another goal of VM is efﬁciency . The OS should strive to make the virtualization as efﬁcient as possible, both in terms of time (i.e., not mak- ing programs run much more slowly) and space (i.e., not using too mu ch memory for structures needed to support virtualization). In imp lement- ing time-efﬁcient virtualization, the OS will have to rely on h ardware support, including hardware features such as TLBs (which we w ill learn about in due course). Finally, a third VM goal is protection . The OS should make sure to protect processes from one another as well as the OS itself from pro- cesses. When one process performs a load, a store, or an instruction f etch, it should not be able to access or affect in any way the memory conten ts of any other process or the OS itself (that is, anything outside its address space). Protection thus enables us to deliver the property of isolation among processes; each process should be running in its own isolated co- coon, safe from the ravages of other faulty or even malicious processe s. 2This usage of transparency is sometimes confusing; some students think tha t “being transparent” means keeping everything out in the open, i.e., what gove rnment should be like. Here, it means the opposite: that the illusion provided by the OS shou ld not be visible to ap- plications. Thus, in common usage, a transparent system is one that is ha rd to notice, not one that responds to requests as stipulated by the Freedom of Informat ion Act. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 THEABSTRACTION : ADDRESS SPACES ASIDE : EVERY ADDRESS YOUSEEISVIRTUAL Ever write a C program that prints out a pointer? The value you see (some large number, often printed in hexadecimal), is a virtual address . Ever wonder where the code of your program is found? You can print that out too, and yes, if you can print it, it also is a virtual addre ss. In fact, any address you can see as a programmer of a user-level progr am is a virtual address. It’s only the OS, through its tricky techniq ues of virtualizing memory, that knows where in the physical memory of t he machine these instructions and data values lie. So never forget : if you print out an address in a program, it’s a virtual one, an illusion of h ow things are laid out in memory; only the OS (and the hardware) knows the real truth. Here’s a little program ( va.c ) that prints out the locations of the main() routine (where code lives), the value of a heap-allocated value r eturned frommalloc() , and the location of an integer on the stack: 1#include <stdio.h> 2#include <stdlib.h> 3int main(int argc, char *argv[]) { 4printf(\""location of code :  percentp \"", (void *) main); 5printf(\""location of heap :  percentp \"", (void *) malloc(1)); 6int x = 3; 7printf(\""location of stack :  percentp \"", (void *) &x); 8return x; 9} When run on a 64-bit Mac, we get the following output: location of code : 0x1095afe50 location of heap : 0x1096008c0 location of stack : 0x7fff691aea64 From this, you can see that code comes ﬁrst in the address space, th en the heap, and the stack is all the way at the other end of this larg e virtual space.",3604
13. Address Spaces,"All of these addresses are virtual, and will be transla ted by the OS and hardware in order to fetch values from their true physical l ocations. In the next chapters, we’ll focus our exploration on the basic mecha- nisms needed to virtualize memory, including hardware and operatin g systems support. We’ll also investigate some of the more relevant poli- cies that you’ll encounter in operating systems, including how to mana ge free space and which pages to kick out of memory when you run low on space. In doing so, we’ll build up your understanding of how a modern virtual memory system really works3. 3Or, we’ll convince you to drop the course. But hold on; if you make it throu gh VM, you’ll likely make it all the way. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : ADDRESS SPACES 7 13.5 Summary We have seen the introduction of a major OS subsystem: virtual mem - ory. The VM system is responsible for providing the illusion of a lar ge, sparse, private address space to programs, which hold all of the ir instruc- tions and data therein. The OS, with some serious hardware help, w ill take each of these virtual memory references, and turn them int o physi- cal addresses, which can be presented to the physical memory i n order to fetch the desired information. The OS will do this for many proces ses at once, making sure to protect programs from one another, as well as pr o- tect the OS. The entire approach requires a great deal of mechani sm (lots of low-level machinery) as well as some critical policies to work; we’ll start from the bottom up, describing the critical mechanisms ﬁr st. And thus we proceed. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 THEABSTRACTION : ADDRESS SPACES References [BH70] “The Nucleus of a Multiprogramming System” by Per Brinch Hansen. C ommunica- tions of the ACM, 13:4, April 1970. The ﬁrst paper to suggest that the OS, or kernel, should be a minimal and ﬂexible substrate for building customized operating systems ; this theme is revisited throughout OS research history. [CV65] “Introduction and Overview of the Multics System” by F. J. Corba to, V . A. Vyssotsky. Fall Joint Computer Conference, 1965. A great early Multics paper. Here is the great quote about time sharing: “The impetus for time-sharing ﬁrst arose from professional p rogrammers because of their constant frustration in debugging programs at batch processing installations. Thus, the original goal was to time-share computers to allow simultaneous access by several persons wh ile giving to each of them the illusion of having the whole machine at his disposal.” [DV66] “Programming Semantics for Multiprogrammed Computations” b y Jack B. Dennis, Earl C. Van Horn. Communications of the ACM, Volume 9, Number 3, March 1 966. An early paper (but not the ﬁrst) on multiprogramming. [L60] “Man-Computer Symbiosis” by J. C. R. Licklider. IRE Transactio ns on Human Factors in Electronics, HFE-1:1, March 1960. A funky paper about how computers and people are going to enter into a symbiotic age; clearly well ahead of its time but a fascinating read none theless. [M62] “Time-Sharing Computer Systems” by J.",3159
13. Address Spaces,"McCarthy. Management and the Co mputer of the Future, MIT Press, Cambridge, MA, 1962. Probably McCarthy’s earliest recorded paper on time sharing. In another paper [M83], he claims to have been thinking of the ide a since 1957. McCarthy left the systems area and went on to become a giant in Artiﬁcial Intel ligence at Stanford, including the creation of the LISP programming language. See McCarthy’s hom e page for more info: http://www-formal.stanford.edu/jmc/ [M+63] “A Time-Sharing Debugging System for a Small Computer” by J. McCa rthy, S. Boilen, E. Fredkin, J. C. R. Licklider. AFIPS ’63 (Spring), New York, NY, May 19 63.A great early example of a system that swapped program memory to the “drum” when the program wasn’t r unning, and then back into “core” memory when it was about to be run. [M83] “Reminiscences on the History of Time Sharing” by John McCarthy. 1983. Available: http://www-formal.stanford.edu/jmc/history/timesharing/times haring.html. A terriﬁc his- torical note on where the idea of time-sharing might have come fzshortm, includ ing some doubts towards those who cite Strachey’s work [S59] as the by pioneering work in this area.. [NS07] “Valgrind: A Framework for Heavyweight Dynamic Binary Instr umentation” by N. Nethercote, J. Seward. PLDI 2007, San Diego, California, June 2007. Valgrind is a lifesaver of a program for those who use unsafe languages like C. Read this paper to learn about its very cool binary instrumentation techniques – it’s really quite impressive. [R+89] “Mach: A System Software kernel” by R. Rashid, D. Julin, D. Orr, R. Sanzi, R. Baron, A. Forin, D. Golub, M. Jones. COMPCON ’89, February 1989. Although not the ﬁrst project on microkernels per se, the Mach project at CMU was well-known and inﬂuential; it still lives today deep in the bowels of Mac OS X. [S59] “Time Sharing in Large Fast Computers” by C. Strachey. Proceed ings of the International Conference on Information Processing, UNESCO, June 1959. One of the earliest references on time sharing. [S+03] “Improving the Reliability of Commodity Operating System s” by M. M. Swift, B. N. Bershad, H. M. Levy. SOSP ’03. The ﬁrst paper to show how microkernel-like thinking can improve operating system reliability. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEABSTRACTION : ADDRESS SPACES 9 Homework (Code) In this homework, we’ll just learn about a few useful tools to examin e virtual memory usage on Linux-based systems. This will only be a brief hint at what is possible; you’ll have to dive deeper on your own to tru ly become an expert (as always.). Questions 1. The ﬁrst Linux tool you should check out is the very simple tool free . First, typeman free and read its entire manual page; it’s short, don’t worry. 2. Now, run free , perhaps using some of the arguments that might be useful (e.g.,-m, to display memory totals in megabytes). How much memory is in your system? How much is free? Do these numbers match your intuition? 3. Next, create a little program that uses a certain amount of memory , called memory-user.c . This program should take one command-line argument: the number of megabytes of memory it will use. When run, it should allo- cate an array, and constantly stream through the array, touchi ng each entry. The program should do this indeﬁnitely, or, perhaps, for a cer tain amount of time also speciﬁed at the command line. 4. Now, while running your memory-user program, also (in a different ter- minal window, but on the same machine) run the free tool. How do the memory usage totals change when your program is running? How about when you kill the memory-user program? Do the numbers match your ex- pectations? Try this for different amounts of memory usage. What h appens when you use really large amounts of memory? 5. Let’s try one more tool, known as pmap . Spend some time, and read the pmap manual page in detail. 6. To usepmap , you have to know the process ID of the process you’re inter- ested in. Thus, ﬁrst run ps auxw to see a list of all processes; then, pick an interesting one, such as a browser. You can also use your memory-user program in this case (indeed, you can even have that program c allgetpid() and print out its PID for your convenience). 7. Now run pmap on some of these processes, using various ﬂags (like -X) to reveal many details about the process. What do you see? How man y different entities make up a modern address space, as opposed to our simple conception of code/stack/heap? 8. Finally, let’s run pmap on your your memory-user program, with different amounts of used memory. What do you see here? Does the output from pmap match your expectations? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4700
14. Memory API,"14 Interlude: Memory API In this interlude, we discuss the memory allocation interfaces in U NIX systems. The interfaces provided are quite simple, and hence the chapter is short and to the point1. The main problem we address is this: CRUX: HOWTOALLOCATE ANDMANAGE MEMORY In U NIX/C programs, understanding how to allocate and manage memory is critical in building robust and reliable software. Wh at inter- faces are commonly used? What mistakes should be avoided? 14.1 Types of Memory In running a C program, there are two types of memory that are allo- cated. The ﬁrst is called stack memory, and allocations and deallocations of it are managed implicitly by the compiler for you, the programmer; for this reason it is sometimes called automatic memory. Declaring memory on the stack in C is easy. For example, let’s say y ou need some space in a function func() for an integer, called x. To declare such a piece of memory, you just do something like this: void func() { int x; // declares an integer on the stack ... } The compiler does the rest, making sure to make space on the stack when you call into func() . When you return from the function, the com- piler deallocates the memory for you; thus, if you want some informat ion to live beyond the call invocation, you had better not leave that in forma- tion on the stack. It is this need for long-lived memory that gets us to the second typ e of memory, called heap memory, where all allocations and deallocations 1Indeed, we hope all chapters are. But this one is shorter and pointier, w e think. 1 2 INTERLUDE : M EMORY API areexplicitly handled by you, the programmer. A heavy responsibility, no doubt. And certainly the cause of many bugs. But if you are care ful and pay attention, you will use such interfaces correctly and wi thout too much trouble. Here is an example of how one might allocate an intege r on the heap: void func() { int*x = (int *) malloc(sizeof(int)); ... } A couple of notes about this small code snippet. First, you might no- tice that both stack and heap allocation occur on this line: ﬁrst th e com- piler knows to make room for a pointer to an integer when it sees your declaration of said pointer ( int*x); subsequently, when the program callsmalloc() , it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program. Because of its explicit nature, and because of its more varied us age, heap memory presents more challenges to both users and systems. Thus, it is the focus of the remainder of our discussion. 14.2 The malloc() Call Themalloc() call is quite simple: you pass it a size asking for some room on the heap, and it either succeeds and gives you back a pointer to the newly-allocated space, or fails and returns NULL2. The manual page shows what you need to do to use malloc; type man malloc at the command line and you will see: #include <stdlib.h> ... void*malloc(size_t size); From this information, you can see that all you need to do is include the header ﬁle stdlib.h to use malloc. In fact, you don’t really need to even do this, as the C library, which all C programs link with by default, has the code for malloc() inside of it; adding the header just lets the compiler check whether you are calling malloc() correctly (e.g., passing the right number of arguments to it, of the right type). The single parameter malloc() takes is of type sizetwhich sim- ply describes how many bytes you need.",3531
14. Memory API,"However, most programmers do not type in a number here directly (such as 10); indeed, it wou ld be considered poor form to do so. Instead, various routines and macros ar e utilized. For example, to allocate space for a double-precision ﬂ oating point value, you simply do this: double*d = (double *) malloc(sizeof(double)); 2Note thatNULL in C isn’t really anything special at all, just a macro for the value zer o. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : M EMORY API 3 TIP: W HEN INDOUBT , TRYITOUT If you aren’t sure how some routine or operator you are using behaves, there is no substitute for simply trying it out and making sure i t behaves as you expect. While reading the manual pages or other documentat ion is useful, how it works in practice is what matters. Write some cod e and test it. That is no doubt the best way to make sure your code behave s as you desire. Indeed, that is what we did to double-check the thin gs we were saying about sizeof() were actually true. Wow, that’s lot of double -ing. This invocation of malloc() uses the sizeof() operator to request the right amount of space; in C, this is generally thought of as a compile-time operator, meaning that the actual size is known at compile time and thus a number (in this case, 8, for a double) is substituted as the argument to malloc() . For this reason, sizeof() is correctly thought of as an operator and not a function call (a function call would take place at run time). You can also pass in the name of a variable (and not just a type) to sizeof() , but in some cases you may not get the desired results, so be careful. For example, let’s look at the following code snippet: int*x = malloc(10 *sizeof(int)); printf(\"" percentd \"", sizeof(x)); In the ﬁrst line, we’ve declared space for an array of 10 integers , which is ﬁne and dandy. However, when we use sizeof() in the next line, it returns a small value, such as 4 (on 32-bit machines) or 8 (on 64 -bit machines). The reason is that in this case, sizeof() thinks we are sim- ply asking how big a pointer to an integer is, not how much memory we have dynamically allocated. However, sometimes sizeof() does work as you might expect: int x[10]; printf(\"" percentd \"", sizeof(x)); In this case, there is enough static information for the compiler t o know that 40 bytes have been allocated. Another place to be careful is with strings. When declaring sp ace for a string, use the following idiom: malloc(strlen(s) + 1) , which gets the length of the string using the function strlen() , and adds 1 to it in order to make room for the end-of-string character. Using sizeof() may lead to trouble here. You might also notice that malloc() returns a pointer to type void . Doing so is just the way in C to pass back an address and let the pr o- grammer decide what to do with it. The programmer further help s out by using what is called a cast; in our example above, the programmer casts the return type of malloc() to a pointer to a double . Casting doesn’t really accomplish anything, other than tell the compiler and other c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 INTERLUDE : M EMORY API programmers who might be reading your code: “yeah, I know what I’m doing.” By casting the result of malloc() , the programmer is just giving some reassurance; the cast is not needed for the correctness.",3368
14. Memory API,"14.3 The free() Call As it turns out, allocating memory is the easy part of the equation; knowing when, how, and even if to free memory is the hard part. To f ree heap memory that is no longer in use, programmers simply call free() : int*x = malloc(10 *sizeof(int)); ... free(x); The routine takes one argument, a pointer returned by malloc() . Thus, you might notice, the size of the allocated region is not passe d in by the user, and must be tracked by the memory-allocation librar y itself. 14.4 Common Errors There are a number of common errors that arise in the use of malloc() andfree() . Here are some we’ve seen over and over again in teaching the undergraduate operating systems course. All of these examp les com- pile and run with nary a peep from the compiler; while compiling a C program is necessary to build a correct C program, it is far from su fﬁ- cient, as you will learn (often in the hard way). Correct memory management has been such a problem, in fact, that many newer languages have support for automatic memory manage- ment . In such languages, while you call something akin to malloc() to allocate memory (usually new or something similar to allocate a new object), you never have to call something to free space; rather, agarbage collector runs and ﬁgures out what memory you no longer have refer- ences to and frees it for you. Forgetting To Allocate Memory Many routines expect memory to be allocated before you call them. F or example, the routine strcpy(dst, src) copies a string from a source pointer to a destination pointer. However, if you are not careful, y ou might do this: char*src = \""hello\""; char*dst; // oops. unallocated strcpy(dst, src); // segfault and die When you run this code, it will likely lead to a segmentation fault3, which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY. 3Although it sounds arcane, you will soon learn why such an illegal me mory access is called a segmentation fault; if that isn’t incentive to read on, what is? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : M EMORY API 5 TIP: ITCOMPILED ORITRAN/negationslash=ITISCORRECT Just because a program compiled(.) or even ran once or many times cor- rectly does not mean the program is correct. Many events may have c on- spired to get you to a point where you believe it works, but then some - thing changes and it stops. A common student reaction is to say (or y ell) “But it worked before.” and then blame the compiler, operating sys tem, hardware, or even (dare we say it) the professor. But the problem i s usu- ally right where you think it would be, in your code. Get to work and debug it before you blame those other components. In this case, the proper code might instead look like this: char*src = \""hello\""; char*dst = (char *) malloc(strlen(src) + 1); strcpy(dst, src); // work properly Alternately, you could use strdup() and make your life even easier. Read thestrdup man page for more information. Not Allocating Enough Memory A related error is not allocating enough memory, sometimes called a buffer overﬂow . In the example above, a common error is to make almost enough room for the destination buffer.",3202
14. Memory API,"char*src = \""hello\""; char*dst = (char *) malloc(strlen(src)); // too small. strcpy(dst, src); // work properly Oddly enough, depending on how malloc is implemented and many other details, this program will often run seemingly correctly. In some cases, when the string copy executes, it writes one byte too far p ast the end of the allocated space, but in some cases this is harmless, pe rhaps overwriting a variable that isn’t used anymore. In some cases, th ese over- ﬂows can be incredibly harmful, and in fact are the source of many secu- rity vulnerabilities in systems [W06]. In other cases, the ma lloc library allocated a little extra space anyhow, and thus your program actu ally doesn’t scribble on some other variable’s value and works quite ﬁne. In even other cases, the program will indeed fault and crash. And t hus we learn another valuable lesson: even though it ran correctly once, doesn’t mean it’s correct. Forgetting to Initialize Allocated Memory With this error, you call malloc() properly, but forget to ﬁll in some val- ues into your newly-allocated data type. Don’t do this. If you do for get, your program will eventually encounter an uninitialized read , where it c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 INTERLUDE : M EMORY API reads from the heap some data of unknown value. Who knows what might be in there? If you’re lucky, some value such that the progra m still works (e.g., zero). If you’re not lucky, something random and harmfu l. Forgetting To Free Memory Another common error is known as a memory leak , and it occurs when you forget to free memory. In long-running applications or systems (such as the OS itself), this is a huge problem, as slowly leaking memor y even- tually leads one to run out of memory, at which point a restart is req uired. Thus, in general, when you are done with a chunk of memory, you should make sure to free it. Note that using a garbage-collected langu age doesn’t help here: if you still have a reference to some chunk of memory, no garbage collector will ever free it, and thus memory leaks remai n a prob- lem even in more modern languages. In some cases, it may seem like not calling free() is reasonable. For example, your program is short-lived, and will soon exit; in this c ase, when the process dies, the OS will clean up all of its allocated pa ges and thus no memory leak will take place per se. While this certainl y “works” (see the aside on page 7), it is probably a bad habit to develop, so be wary of choosing such a strategy. In the long run, one of your goals as a pro- grammer is to develop good habits; one of those habits is understand ing how you are managing memory, and (in languages like C), freeing t he blocks you have allocated. Even if you can get away with not doing so, it is probably good to get in the habit of freeing each and every byt e you explicitly allocate. Freeing Memory Before You Are Done With It Sometimes a program will free memory before it is ﬁnished using it; such a mistake is called a dangling pointer , and it, as you can guess, is also a bad thing. The subsequent use can crash the program, or overwrit e valid memory (e.g., you called free() , but then called malloc() again to allocate something else, which then recycles the errantly-fr eed memory). Freeing Memory Repeatedly Programs also sometimes free memory more than once; this is known as thedouble free .",3404
14. Memory API,"The result of doing so is undeﬁned. As you can imag- ine, the memory-allocation library might get confused and do all sorts of weird things; crashes are a common outcome. Callingfree() Incorrectly One last problem we discuss is the call of free() incorrectly. After all, free() expects you only to pass to it one of the pointers you received frommalloc() earlier. When you pass in some other value, bad things can (and do) happen. Thus, such invalid frees are dangerous and of course should also be avoided. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : M EMORY API 7 ASIDE : W HYNOMEMORY ISLEAKED ONCE YOUR PROCESS EXITS When you write a short-lived program, you might allocate some space usingmalloc() . The program runs and is about to complete: is there need to call free() a bunch of times just before exiting? While it seems wrong not to, no memory will be “lost” in any real sense. The reason is simple: there are really two levels of memory management in the system. The ﬁrst level of memory management is performed by the OS, which hands out memory to processes when they run, and takes it back whe n processes exit (or otherwise die). The second level of management iswithin each process, for example within the heap when you call malloc() andfree() . Even if you fail to call free() (and thus leak memory in the heap), the operating system will reclaim allthe memory of the process (including those pages for code, stack, and, as relev ant here, heap) when the program is ﬁnished running. No matter what the s tate of your heap in your address space, the OS takes back all of those pag es when the process dies, thus ensuring that no memory is lost despi te the fact that you didn’t free it. Thus, for short-lived programs, leaking memory often does not cause any operational problems (though it may be considered poor form). When you write a long-running server (such as a web server or database man- agement system, which never exit), leaked memory is a much big ger is- sue, and will eventually lead to a crash when the application r uns out of memory. And of course, leaking memory is an even larger issue insi de one particular program: the operating system itself. Showing us on ce again: those who write the kernel code have the toughest job of all. .. Summary As you can see, there are lots of ways to abuse memory. Because of fre - quent errors with memory, a whole ecosphere of tools have developed to help ﬁnd such problems in your code. Check out both purify [HJ92] and valgrind [SN05]; both are excellent at helping you locate the source of your memory-related problems. Once you become accustomed to using these powerful tools, you will wonder how you survived without them. 14.5 Underlying OS Support You might have noticed that we haven’t been talking about system calls when discussing malloc() andfree() . The reason for this is sim- ple: they are not system calls, but rather library calls. Thus the malloc li- brary manages space within your virtual address space, but it self is built on top of some system calls which call into the OS to ask for more mem- ory or release some back to the system.",3143
14. Memory API,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 INTERLUDE : M EMORY API One such system call is called brk, which is used to change the loca- tion of the program’s break : the location of the end of the heap. It takes one argument (the address of the new break), and thus either inc reases or decreases the size of the heap based on whether the new break is l arger or smaller than the current break. An additional call sbrk is passed an increment but otherwise serves a similar purpose. Note that you should never directly call either brk orsbrk . They are used by the memory-allocation library; if you try to use them, you will likely make something go (horribly) wrong. Stick to malloc() and free() instead. Finally, you can also obtain memory from the operating system via t he mmap() call. By passing in the correct arguments, mmap() can create an anonymous memory region within your program — a region which is not associated with any particular ﬁle but rather with swap space , something we’ll discuss in detail later on in virtual memory. This memory ca n then also be treated like a heap and managed as such. Read the manua l page ofmmap() for more details. 14.6 Other Calls There are a few other calls that the memory-allocation library su p- ports. For example, calloc() allocates memory and also zeroes it be- fore returning; this prevents some errors where you assume that m emory is zeroed and forget to initialize it yourself (see the paragrap h on “unini- tialized reads” above). The routine realloc() can also be useful, when you’ve allocated space for something (say, an array), and then nee d to add something to it: realloc() makes a new larger region of memory, copies the old region into it, and returns the pointer to the new re gion. 14.7 Summary We have introduced some of the APIs dealing with memory allocation. As always, we have just covered the basics; more details are ava ilable elsewhere. Read the C book [KR88] and Stevens [SR05] (Chapter 7) f or more information. For a cool modern paper on how to detect and correct many of these problems automatically, see Novark et al. [N+07]; t his paper also contains a nice summary of common problems and some neat ideas on how to ﬁnd and ﬁx them. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : M EMORY API 9 References [HJ92] “Purify: Fast Detection of Memory Leaks and Access Errors” by R. Hastings, B. Joyce. USENIX Winter ’92. The paper behind the cool Purify tool, now a commercial product. [KR88] “The C Programming Language” by Brian Kernighan, Dennis Ritchie. Pre ntice-Hall 1988. The C book, by the developers of C. Read it once, do some programming, then re ad it again, and then keep it near your desk or wherever you program. [N+07] “Exterminator: Automatically Correcting Memory Errors wi th High Probability” by G. Novark, E. D. Berger, B. G. Zorn. PLDI 2007, San Diego, Califor nia. A cool paper on ﬁnding and correcting memory errors automatically, and a great overview of many common e rrors in C and C++ programs. An extended version of this paper is available CACM (Volume 5 1, Issue 12, December 2008).",3122
14. Memory API,"[SN05] “Using Valgrind to Detect Undeﬁned Value Errors with Bit-pre cision” by J. Seward, N. Nethercote. USENIX ’05. How to use valgrind to ﬁnd certain types of errors. [SR05] “Advanced Programming in the U NIX Environment” by W. Richard Stevens, Stephen A. Rago. Addison-Wesley, 2005. We’ve said it before, we’ll say it again: read this book many times and use it as a reference whenever you are in doubt. The authors are always su rprised at how each time they read something in this book, they learn something new, even after many year s of C programming. [W06] “Survey on Buffer Overﬂow Attacks and Countermeasures” by T. Werthman. Avail- able: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOverﬂow.pdf. A nice survey of buffer overﬂows and some of the security problems they cause. Refers to man y of the famous exploits. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 INTERLUDE : M EMORY API Homework (Code) In this homework, you will gain some familiarity with memory allo- cation. First, you’ll write some buggy programs (fun.). Then, you’ll use some tools to help you ﬁnd the bugs you inserted. Then, you will reali ze how awesome these tools are and use them in the future, thus making yourself more happy and productive. The tools are the debugger (e. g., gdb), and a memory-bug detector called valgrind [SN05]. Questions 1. First, write a simple program called null.c that creates a pointer to an integer, sets it to NULL , and then tries to dereference it. Compile this into an executable called null . What happens when you run this program? 2. Next, compile this program with symbol information included (w ith the-g ﬂag). Doing so let’s put more information into the executable, ena bling the debugger to access more useful information about variable names and the like. Run the program under the debugger by typing gdb null and then, oncegdb is running, typing run. What does gdb show you? 3. Finally, use the valgrind tool on this program. We’ll use the memcheck tool that is a part of valgrind to analyze what happens. Run this by typing in the following: valgrind --leak-check=yes null . What happens when you run this? Can you interpret the output from the tool? 4. Write a simple program that allocates memory using malloc() but forgets to free it before exiting. What happens when this program runs? Can you usegdb to ﬁnd any problems with it? How about valgrind (again with the--leak-check=yes ﬂag)? 5. Write a program that creates an array of integers called data of size 100 usingmalloc ; then, set data[100] to zero. What happens when you run this program? What happens when you run this program using valgrind ? Is the program correct? 6. Create a program that allocates an array of integers (as ab ove), frees them, and then tries to print the value of one of the elements of the arr ay. Does the program run? What happens when you use valgrind on it? 7. Now pass a funny value to free (e.g., a pointer in the middle of t he array you allocated above). What happens? Do you need tools to ﬁnd t his type of problem? 8. Try out some of the other interfaces to memory allocation. For e xample, cre- ate a simple vector-like data structure and related routines tha t userealloc() to manage the vector. Use an array to store the vectors elements; when a user adds an entry to the vector, use realloc() to allocate more space for it. How well does such a vector perform? How does it compare to a li nked list? Usevalgrind to help you ﬁnd bugs. 9. Spend more time and read about using gdb andvalgrind . Knowing your tools is critical; spend the time and learn how to become an exper t debugger in the U NIXand C environment. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3712
15. Address Translation,"15 Mechanism: Address Translation In developing the virtualization of the CPU, we focused on a genera l mechanism known as limited direct execution (orLDE ). The idea be- hind LDE is simple: for the most part, let the program run directl y on the hardware; however, at certain key points in time (such as when a process issues a system call, or a timer interrupt occurs), arrange so t hat the OS gets involved and makes sure the “right” thing happens. Thus, the OS, with a little hardware support, tries its best to get out of the wa y of the running program, to deliver an efﬁcient virtualization; however, by inter- posing at those critical points in time, the OS ensures that it maintai ns control over the hardware. Efﬁciency and control together are two of the main goals of any modern operating system. In virtualizing memory, we will pursue a similar strategy, at taining both efﬁciency and control while providing the desired virtuali zation. Ef- ﬁciency dictates that we make use of hardware support, which at ﬁrst will be quite rudimentary (e.g., just a few registers) but wi ll grow to be fairly complex (e.g., TLBs, page-table support, and so forth, a s you will see). Control implies that the OS ensures that no application is allowed to access any memory but its own; thus, to protect applications fr om one another, and the OS from applications, we will need help from the h ard- ware here too. Finally, we will need a little more from the VM syste m, in terms of ﬂexibility ; speciﬁcally, we’d like for programs to be able to use their address spaces in whatever way they would like, thus mak ing the system easier to program. And thus we arrive at the reﬁned crux : THECRUX: HOWTOEFFICIENTLY ANDFLEXIBLY VIRTUALIZE MEMORY How can we build an efﬁcient virtualization of memory? How do we provide the ﬂexibility needed by applications? How do we main tain control over which memory locations an application can access, and t hus ensure that application memory accesses are properly restrict ed? How do we do all of this efﬁciently? 1 2 MECHANISM : ADDRESS TRANSLATION The generic technique we will use, which you can consider an add ition to our general approach of limited direct execution, is something that is referred to as hardware-based address translation , or just address trans- lation for short. With address translation, the hardware transforms ea ch memory access (e.g., an instruction fetch, load, or store), chang ing the vir- tual address provided by the instruction to a physical address where the desired information is actually located. Thus, on each and every memory reference, an address translation is performed by the hardwar e to redirect application memory references to their actual locations in memor y. Of course, the hardware alone cannot virtualize memory, as it jus t pro- vides the low-level mechanism for doing so efﬁciently. The OS mu st get involved at key points to set up the hardware so that the correct t rans- lations take place; it must thus manage memory , keeping track of which locations are free and which are in use, and judiciously interve ning to maintain control over how memory is used.",3150
15. Address Translation,"Once again the goal of all of this work is to create a beautiful illu- sion : that the program has its own private memory, where its own code and data reside. Behind that virtual reality lies the ugly ph ysical truth: that many programs are actually sharing memory at the same time , as the CPU (or CPUs) switches between running one program and the ne xt. Through virtualization, the OS (with the hardware’s help) turn s the ugly machine reality into something that is a useful, powerful, and easy to use abstraction. 15.1 Assumptions Our ﬁrst attempts at virtualizing memory will be very simple, almost laughably so. Go ahead, laugh all you want; pretty soon it will be t he OS laughing at you, when you try to understand the ins and outs of TLBs , multi-level page tables, and other technical wonders. Don’t lik e the idea of the OS laughing at you? Well, you may be out of luck then; that’s jus t how the OS rolls. Speciﬁcally, we will assume for now that the user’s address space must be placed contiguously in physical memory. We will also assume, for sim- plicity, that the size of the address space is not too big; speciﬁ cally, that it is less than the size of physical memory . Finally, we will also assume that each address space is exactly the same size . Don’t worry if these assump- tions sound unrealistic; we will relax them as we go, thus achiev ing a realistic virtualization of memory. 15.2 An Example To understand better what we need to do to implement address t rans- lation, and why we need such a mechanism, let’s look at a simple exa m- ple. Imagine there is a process whose address space is as indica ted in Figure 15.1. What we are going to examine here is a short code sequ ence OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 3 TIP: INTERPOSITION ISPOWERFUL Interposition is a generic and powerful technique that is often u sed to great effect in computer systems. In virtualizing memory, the hardware will interpose on each memory access, and translate each virtua l address issued by the process to a physical address where the desired i nforma- tion is actually stored. However, the general technique of inter position is much more broadly applicable; indeed, almost any well-deﬁned i nterface can be interposed upon, to add new functionality or improve some othe r aspect of the system. One of the usual beneﬁts of such an approach i s transparency ; the interposition often is done without changing the client of the interface, thus requiring no changes to said client. that loads a value from memory, increments it by three, and then s tores the value back into memory. You can imagine the C-language repr esen- tation of this code might look like this: void func() { int x = 3000; // thanks, Perry. x = x + 3; // this is the line of code we are interested in ... The compiler turns this line of code into assembly, which might l ook something like this (in x86 assembly). Use objdump on Linux or otool on a Mac to disassemble it: 128: movl 0x0( percentebx),  percenteax ;load 0+ebx into eax 132: addl $0x03,  percenteax ;add 3 to eax register 135: movl  percenteax, 0x0( percentebx) ;store eax back to mem This code snippet is relatively straightforward; it presumes that the address of xhas been placed in the register ebx, and then loads the value at that address into the general-purpose register eax using themovl in- struction (for “longword” move). The next instruction adds 3 to eax, and the ﬁnal instruction stores the value in eax back into memory at that same location. In Figure 15.1 (page 4), observe how both the code and data are laid out in the process’s address space; the three-instruction code se quence is located at address 128 (in the code section near the top), and the v alue of the variable xat address 15 KB (in the stack near the bottom). In the ﬁgure, the initial value of xis 3000, as shown in its location on the stack.",3917
15. Address Translation,"When these instructions run, from the perspective of the process , the following memory accesses take place. •Fetch instruction at address 128 •Execute this instruction (load from address 15 KB) •Fetch instruction at address 132 •Execute this instruction (no memory reference) •Fetch the instruction at address 135 •Execute this instruction (store to address 15 KB) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 MECHANISM : ADDRESS TRANSLATION 16KB15KB14KB4KB3KB2KB1KB0KB Stack(free)HeapProgram Code128 132 135movl 0x0( percentebx), percenteax addl 0x03,  percenteax movl  percenteax,0x0( percentebx) 3000 Figure 15.1: A Process And Its Address Space From the program’s perspective, its address space starts at address 0 and grows to a maximum of 16 KB; all memory references it generate s should be within these bounds. However, to virtualize memory, th e OS wants to place the process somewhere else in physical memory, not nec- essarily at address 0. Thus, we have the problem: how can we relocate this process in memory in a way that is transparent to the process? How can we provide the illusion of a virtual address space starting a t 0, when in reality the address space is located at some other physical ad dress? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 5 64KB48KB32KB16KB0KB (not in use)(not in use)Operating System StackCode Heap (allocated but not in use) Relocated Process Figure 15.2: Physical Memory with a Single Relocated Process An example of what physical memory might look like once this pro- cess’s address space has been placed in memory is found in Figure 15.2. In the ﬁgure, you can see the OS using the ﬁrst slot of physical mem ory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB. The othe r two slots are free (16 KB-32 KB and 48 KB-64 KB). 15.3 Dynamic (Hardware-based) Relocation To gain some understanding of hardware-based address transla tion, we’ll ﬁrst discuss its ﬁrst incarnation. Introduced in the ﬁrst time-sharing machines of the late 1950’s is a simple idea referred to as base and bounds ; the technique is also referred to as dynamic relocation ; we’ll use both terms interchangeably [SS74]. Speciﬁcally, we’ll need two hardware registers within each CP U: one is called the base register, and the other the bounds (sometimes called a limit register). This base-and-bounds pair is going to allow us to pla ce the address space anywhere we’d like in physical memory, and do so w hile ensuring that the process can only access its own address space. In this setup, each program is written and compiled as if it is loa ded at address zero. However, when a program starts running, the OS dec ides where in physical memory it should be loaded and sets the base reg ister to that value. In the example above, the OS decides to load the pr ocess at physical address 32 KB and thus sets the base register to this value. Interesting things start to happen when the process is runnin g.",3052
15. Address Translation,"Now, when any memory reference is generated by the process, it is translated by the processor in the following manner: physical address = virtual address + base c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 MECHANISM : ADDRESS TRANSLATION ASIDE : SOFTWARE -BASED RELOCATION In the early days, before hardware support arose, some systems pe r- formed a crude form of relocation purely via software methods. The basic technique is referred to as static relocation , in which a piece of soft- ware known as the loader takes an executable that is about to be run and rewrites its addresses to the desired offset in physical memor y. For example, if an instruction was a load from address 1000 into a r eg- ister (e.g., movl 1000,  percenteax ), and the address space of the program was loaded starting at address 3000 (and not 0, as the program thi nks), the loader would rewrite the instruction to offset each address b y 3000 (e.g.,movl 4000,  percenteax ). In this way, a simple static relocation of the process’s address space is achieved. However, static relocation has numerous problems. First and most i m- portantly, it does not provide protection, as processes can generat e bad addresses and thus illegally access other process’s or even OS me mory; in general, hardware support is likely needed for true protection [ WL+93]. Another negative is that once placed, it is difﬁcult to later re locate an ad- dress space to another location [M65]. Each memory reference generated by the process is a virtual address ; the hardware in turn adds the contents of the base register to th is address and the result is a physical address that can be issued to the memory system. To understand this better, let’s trace through what happens wh en a single instruction is executed. Speciﬁcally, let’s look at one ins truction from our earlier sequence: 128: movl 0x0( percentebx),  percenteax The program counter (PC) is set to 128; when the hardware needs t o fetch this instruction, it ﬁrst adds the value to the base regi ster value of 32 KB (32768) to get a physical address of 32896; the hardware then fetches the instruction from that physical address. Next, the processor begins executing the instruction. At some point, the process the n issues the load from virtual address 15 KB, which the processor takes and again adds to the base register (32 KB), getting the ﬁnal physical a ddress of 47 KB and thus the desired contents. Transforming a virtual address into a physical address is exa ctly the technique we refer to as address translation ; that is, the hardware takes a virtual address the process thinks it is referencing and tran sforms it into a physical address which is where the data actually resides. Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, th e technique is often referred to as dynamic relocation [M65]. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 7 TIP: HARDWARE -BASED DYNAMIC RELOCATION With dynamic relocation, a little hardware goes a long way. Namel y, a base register is used to transform virtual addresses (generated b y the pro- gram) into physical addresses. A bounds (orlimit ) register ensures that such addresses are within the conﬁnes of the address space.",3338
15. Address Translation,"Toge ther they provide a simple and efﬁcient virtualization of memory. Now you might be asking: what happened to that bounds (limit) reg - ister? After all, isn’t this the base andbounds approach? Indeed, it is. As you might have guessed, the bounds register is there to help wit h protec- tion. Speciﬁcally, the processor will ﬁrst check that the memory r eference iswithin bounds to make sure it is legal; in the simple example above, the bounds register would always be set to 16 KB. If a process generat es a vir- tual address that is greater than the bounds, or one that is negat ive, the CPU will raise an exception, and the process will likely be term inated. The point of the bounds is thus to make sure that all addresses gen erated by the process are legal and within the “bounds” of the process. We should note that the base and bounds registers are hardware st ruc- tures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the memory management unit (MMU) ; as we develop more sophisticated memory- management techniques, we will be adding more circuitry to th e MMU. A small aside about bound registers, which can be deﬁned in one of two ways. In one way (as above), it holds the sizeof the address space, and thus the hardware checks the virtual address against it ﬁ rst before adding the base. In the second way, it holds the physical address of the end of the address space, and thus the hardware ﬁrst adds the ba se and then makes sure the address is within bounds. Both methods are log ically equivalent; for simplicity, we’ll usually assume the former me thod. Example Translations To understand address translation via base-and-bounds in more detail, let’s take a look at an example. Imagine a process with an address s pace of size 4 KB (yes, unrealistically small) has been loaded at phys ical address 16 KB. Here are the results of a number of address translations: Virtual Address Physical Address 0→ 16 KB 1 KB→ 17 KB 3000→ 19384 4400→ Fault (out of bounds) As you can see from the example, it is easy for you to simply add the base address to the virtual address (which can rightly be vie wed as an offset into the address space) to get the resulting physical addres s. Only if the virtual address is “too big” or negative will the result b e a fault, causing an exception to be raised. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 MECHANISM : ADDRESS TRANSLATION ASIDE : DATA STRUCTURE — T HEFREE LIST The OS must track which parts of free memory are not in use, so as to be able to allocate memory to processes. Many different data str uctures can of course be used for such a task; the simplest (which we will a ssume here) is a free list , which simply is a list of the ranges of the physical memory which are not currently in use. 15.4 Hardware Support: A Summary Let us now summarize the support we need from the hardware (also see Figure 15.3, page 9). First, as discussed in the chapter on CPU virtual- ization, we require two different CPU modes.",3065
15. Address Translation,"The OS runs in privileged mode (orkernel mode ), where it has access to the entire machine; appli- cations run in user mode , where they are limited in what they can do. A single bit, perhaps stored in some kind of processor status word , indi- cates which mode the CPU is currently running in; upon certain s pecial occasions (e.g., a system call or some other kind of exception or inter rupt), the CPU switches modes. The hardware must also provide the base and bounds registers them- selves; each CPU thus has an additional pair of registers, part of the mem- ory management unit (MMU ) of the CPU. When a user program is run- ning, the hardware will translate each address, by adding th e base value to the virtual address generated by the user program. The hard ware must also be able to check whether the address is valid, which is ac complished by using the bounds register and some circuitry within the CPU. The hardware should provide special instructions to modify the b ase and bounds registers, allowing the OS to change them when diffe rent processes run. These instructions are privileged ; only in kernel (or priv- ileged) mode can the registers be modiﬁed. Imagine the havoc a us er process could wreak1if it could arbitrarily change the base register while running. Imagine it. And then quickly ﬂush such dark thoughts from your mind, as they are the ghastly stuff of which nightmares are made. Finally, the CPU must be able to generate exceptions in situations where a user program tries to access memory illegally (with an a ddress that is “out of bounds”); in this case, the CPU should stop executin g the user program and arrange for the OS “out-of-bounds” exception handler to run. The OS handler can then ﬁgure out how to react, in this cas e likely terminating the process. Similarly, if a user program tries to c hange the values of the (privileged) base and bounds registers, the CPU s hould raise an exception and run the “tried to execute a privileged operati on while in user mode” handler. The CPU also must provide a method to inform it of the location of these handlers; a few more privileged instruc tions are thus needed. 1Is there anything other than “havoc” that can be “wreaked”? [W17] OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 9 Hardware Requirements Notes Privileged mode Needed to prevent user-mode processes from executing privileged operations Base/bounds registers Need pair of registers per CPU to support address translation and bounds checks Ability to translate virtual addresses Circuitry to do translations and check and check if within bounds limits; in this case, quite simple Privileged instruction(s) to OS must be able to set these values update base/bounds before letting a user program run Privileged instruction(s) to register OS must be able to tell hardware what exception handlers code to run if exception occurs Ability to raise exceptions When processes try to access privileged instructions or out-of-bounds memory Figure 15.3: Dynamic Relocation: Hardware Requirements 15.5 Operating System Issues Just as the hardware provides new features to support dynamic r elo- cation, the OS now has new issues it must handle; the combination of hardware support and OS management leads to the implementati on of a simple virtual memory. Speciﬁcally, there are a few critical junctures where the OS must get involved to implement our base-and-bounds ver- sion of virtual memory. First, the OS must take action when a process is created, ﬁnding space for its address space in memory. Fortunately, given our assumpti ons that each address space is (a) smaller than the size of physical mem ory and (b) the same size, this is quite easy for the OS; it can simply vie w physical memory as an array of slots, and track whether each one is free or in use. When a new process is created, the OS will have to search a d ata structure (often called a free list ) to ﬁnd room for the new address space and then mark it used.",4007
15. Address Translation,"With variable-sized address spaces, l ife is more complicated, but we will leave that concern for future chapters . Let’s look at an example. In Figure 15.2 (page 5), you can see the OS using the ﬁrst slot of physical memory for itself, and that it has r elocated the process from the example above into the slot starting at physi cal mem- ory address 32 KB. The other two slots are free (16 KB-32 KB and 48 K B- 64 KB); thus, the free list should consist of these two entries. Second, the OS must do some work when a process is terminated (i.e., when it exits gracefully, or is forcefully killed because it mi sbehaved), reclaiming all of its memory for use in other processes or the OS. Upon termination of a process, the OS thus puts its memory back on the fre e list, and cleans up any associated data structures as need be. Third, the OS must also perform a few additional steps when a cont ext switch occurs. There is only one base and bounds register pair on ea ch CPU, after all, and their values differ for each running progra m, as each program is loaded at a different physical address in memory. Thu s, the OS must save and restore the base-and-bounds pair when it switches be- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 MECHANISM : ADDRESS TRANSLATION OS Requirements Notes Memory management Need to allocate memory for new processes; Reclaim memory from terminated processes; Generally manage memory via free list Base/bounds management Must set base/bounds properly upon context switch Exception handling Code to run when exceptions arise; likely action is to terminate offending process Figure 15.4: Dynamic Relocation: Operating System Responsibilities tween processes. Speciﬁcally, when the OS decides to stop runni ng a pro- cess, it must save the values of the base and bounds registers to memory, in some per-process structure such as the process structure orprocess control block (PCB). Similarly, when the OS resumes a running process (or runs it the ﬁrst time), it must set the values of the base and b ounds on the CPU to the correct values for this process. We should note that when a process is stopped (i.e., not running), i t is possible for the OS to move an address space from one location in mem- ory to another rather easily. To move a process’s address space, th e OS ﬁrst deschedules the process; then, the OS copies the address s pace from the current location to the new location; ﬁnally, the OS updates t he saved base register (in the process structure) to point to the new loca tion. When the process is resumed, its (new) base register is restored, an d it begins running again, oblivious that its instructions and data are now i n a com- pletely new spot in memory. Fourth, the OS must provide exception handlers , or functions to be called, as discussed above; the OS installs these handlers at boot time (via privileged instructions). For example, if a process tries to ac cess mem- ory outside its bounds, the CPU will raise an exception; the OS mus t be prepared to take action when such an exception arises.",3071
15. Address Translation,"The common reac- tion of the OS will be one of hostility: it will likely terminate the offending process. The OS should be highly protective of the machine it is ru nning, and thus it does not take kindly to a process trying to access memor y or execute instructions that it shouldn’t. Bye bye, misbehaving p rocess; it’s been nice knowing you. Figure 15.5 (page 11) illustrates much of the hardware/OS int eraction in a timeline. The ﬁgure shows what the OS does at boot time to ready the machine for use, and then what happens when a process (Proces s A) starts running; note how its memory translations are handled b y the hardware with no OS intervention. At some point, a timer interru pt oc- curs, and the OS switches to Process B, which executes a “bad loa d” (to an illegal memory address); at that point, the OS must get involved , termi- nating the process and cleaning up by freeing B’s memory and remov ing its entry from the process table. As you can see from the diagram, w e are still following the basic approach of limited direct execution . In most cases, the OS just sets up the hardware appropriately and lets the process run directly on the CPU; only when the process misbehaves does the OS have to become involved. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 11 OS @ boot Hardware (kernel mode) initialize trap table remember addresses of... system call handler timer handler illegal mem-access handler illegal instruction handler start interrupt timer start timer; interrupt after X ms initialize process table initialize free list OS @ run Hardware Program (kernel mode) (user mode) To start process A: allocate entry in process table allocate memory for process set base/bounds registers return-from-trap (into A) restore registers of A move to user mode jump to A’s (initial) PC Process A runs Fetch instruction Translate virtual address and perform fetch Execute instruction If explicit load/store: Ensure address is in-bounds; Translate virtual address and perform load/store ... Timer interrupt move to kernel mode Jump to interrupt handler Handle the trap Callswitch() routine save regs(A) to proc-struct(A) (including base/bounds) restore regs(B) from proc-struct(B) (including base/bounds) return-from-trap (into B) restore registers of B move to user mode jump to B’s PC Process B runs Execute bad load Load is out-of-bounds; move to kernel mode jump to trap handler Handle the trap Decide to terminate process B de-allocate B’s memory free B’s entry in process table Figure 15.5: Limited Direct Execution Protocol (Dynamic Relocation) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 MECHANISM : ADDRESS TRANSLATION 15.6 Summary In this chapter, we have extended the concept of limited direct exe- cution with a speciﬁc mechanism used in virtual memory, known as ad- dress translation . With address translation, the OS can control each and every memory access from a process, ensuring the accesses stay w ithin the bounds of the address space. Key to the efﬁciency of this tech nique is hardware support, which performs the translation quickly for each ac- cess, turning virtual addresses (the process’s view of memory) i nto phys- ical ones (the actual view). All of this is performed in a way that istrans- parent to the process that has been relocated; the process has no idea it s memory references are being translated, making for a wonderful illusion.",3440
15. Address Translation,"We have also seen one particular form of virtualization, known as b ase and bounds or dynamic relocation. Base-and-bounds virtualizati on is quite efﬁcient , as only a little more hardware logic is required to add a base register to the virtual address and check that the addre ss generated by the process is in bounds. Base-and-bounds also offers protection ; the OS and hardware combine to ensure no process can generate memory references outside its own address space. Protection is certain ly one of the most important goals of the OS; without it, the OS could not control the machine (if processes were free to overwrite memory, they cou ld eas- ily do nasty things like overwrite the trap table and take over t he system). Unfortunately, this simple technique of dynamic relocation does have its inefﬁciencies. For example, as you can see in Figure 15.2 (p age 5), the relocated process is using physical memory from 32 KB to 48 KB; how- ever, because the process stack and heap are not too big, all of the space between the two is simply wasted . This type of waste is usually called in- ternal fragmentation , as the space inside the allocated unit is not all used (i.e., is fragmented) and thus wasted. In our current approach , although there might be enough physical memory for more processes, we are cu r- rently restricted to placing an address space in a ﬁxed-size d slot and thus internal fragmentation can arise2. Thus, we are going to need more so- phisticated machinery, to try to better utilize physical me mory and avoid internal fragmentation. Our ﬁrst attempt will be a slight gen eralization of base and bounds known as segmentation , which we will discuss next. 2A different solution might instead place a ﬁxed-sized stack within the address space, just below the code region, and a growing heap below that. However, thi s limits ﬂexibility by making recursion and deeply-nested function calls challenging, and thus i s something we hope to avoid. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MECHANISM : ADDRESS TRANSLATION 13 References [M65] “On Dynamic Program Relocation” by W.C. McGee. IBM Systems Journal , Volume 4:3, 1965, pages 184–199. This paper is a nice summary of early work on dynamic relocation, as well as some basics on static relocation. [P90] “Relocating loader for MS-DOS .EXE executable ﬁles” by Kenneth D. A . Pillay. Micro- processors & Microsystems archive, Volume 14:7 (September 1990). An example of a relocating loader for MS-DOS. Not the ﬁrst one, but just a relatively modern example of how such a system works. [SS74] “The Protection of Information in Computer Systems” by J. Salt zer and M. Schroeder. CACM, July 1974. From this paper: “The concepts of base-and-bound register and hardware-interp reted descriptors appeared, apparently independently, between 1957 and 195 9 on three projects with diverse goals. At M.I.T., McCarthy suggested the base-and-bound idea as part of the mem ory protection system necessary to make time-sharing feasible. IBM independently developed th e base-and-bound register as a mechanism to permit reliable multiprogramming of the Stretch (7030) comp uter system.",3156
15. Address Translation,"At Burroughs, R. Barton suggested that hardware-interpreted descriptors would provide di rect support for the naming scope rules of higher level languages in the B5000 computer system.” W e found this quote on Mark Smotherman’s cool history pages [S04]; see them for more information. [S04] “System Call Support” by Mark Smotherman. May 2004. people.cs.clemson.edu/ ˜mark/syscall.html .A neat history of system call support. Smotherman has also collected some early history on items like interrupts and other fun aspects of computing history . See his web pages for more details. [WL+93] “Efﬁcient Software-based Fault Isolation” by Robert Wahbe , Steven Lucco, Thomas E. Anderson, Susan L. Graham. SOSP ’93. A terriﬁc paper about how you can use compiler support to bound memory references from a program, without hardware support. The p aper sparked renewed interest in software techniques for isolation of memory references. [W17] Answer to footnote: “Is there anything other than havoc that can be wre aked?” by Waciuma Wanjohi. October 2017. Amazingly, this enterprising reader found the answer via google’s Ngram viewing tool (available at the following URL: http://books.google.com/ngrams ). The answer, thanks to Mr. Wanjohi: “It’s only since about 1970 that ’wreak havoc’ has been more popular than ’wreak vengeance’. In the 1800s, the word wreak was almost always f ollowed by ’his/their vengeance’.” Apparently, when you wreak, you are up to no good, but at least w reakers have some options now. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 MECHANISM : ADDRESS TRANSLATION Homework (Simulation) The program relocation.py allows you to see how address trans- lations are performed in a system with base and bounds registers . See the README for details. Questions 1. Run with seeds 1, 2, and 3, and compute whether each virtual addr ess gen- erated by the process is in or out of bounds. If in bounds, compute th e translation. 2. Run with these ﬂags: -s 0 -n 10 . What value do you have set -l(the bounds register) to in order to ensure that all the generated vi rtual addresses are within bounds? 3. Run with these ﬂags: -s 1 -n 10 -l 100 . What is the maximum value that base can be set to, such that the address space still ﬁts in to physical memory in its entirety? 4. Run some of the same problems above, but with larger address spac es (-a) and physical memories ( -p). 5. What fraction of randomly-generated virtual addresses are valid, as a func- tion of the value of the bounds register? Make a graph from runnin g with different random seeds, with limit values ranging from 0 up to th e maxi- mum size of the address space. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2709
16. Segmentation,"16 Segmentation So far we have been putting the entire address space of each proce ss in memory. With the base and bounds registers, the OS can easily re locate processes to different parts of physical memory. However, you mig ht have noticed something interesting about these address spaces of ours: there is a big chunk of “free” space right in the middle, betwee n the stack and the heap. As you can imagine from Figure 16.1, although the space between t he stack and heap is not being used by the process, it is still takin g up phys- ical memory when we relocate the entire address space somewhere in physical memory; thus, the simple approach of using a base and bou nds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t ﬁt into mem ory; thus, base and bounds is not as ﬂexible as we would like. And thus: THECRUX: HOWTOSUPPORT A L ARGE ADDRESS SPACE How do we support a large address space with (potentially) a lot of free space between the stack and the heap? Note that in our examp les, with tiny (pretend) address spaces, the waste doesn’t seem too b ad. Imag- ine, however, a 32-bit address space (4 GB in size); a typical p rogram will only use megabytes of memory, but still would demand that the enti re address space be resident in memory. 16.1 Segmentation: Generalized Base/Bounds To solve this problem, an idea was born, and it is called segmenta- tion. It is quite an old idea, going at least as far back as the very ear ly 1960’s [H61, G62]. The idea is simple: instead of having just one base and bounds pair in our MMU, why not have a base and bounds pair per logical segment of the address space? A segment is just a contiguous portion of the address space of a particular length, and in our canon ical 1 2 SEGMENTATION 16KB15KB14KB6KB5KB4KB3KB2KB1KB0KB Program Code Heap (free) Stack Figure 16.1: An Address Space (Again) address space, we have three logically-different segments: code, stack, and heap. What segmentation allows the OS to do is to place each on e of those segments in different parts of physical memory, and thus avoid ﬁlling physical memory with unused virtual address space. Let’s look at an example. Assume we want to place the address spac e from Figure 16.1 into physical memory. With a base and bounds pai r per segment, we can place each segment independently in physical mem- ory. For example, see Figure 16.2 (page 3); there you see a 64KB ph ysical memory with those three segments in it (and 16KB reserved for the OS). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEGMENTATION 3 64KB48KB32KB16KB0KB (not in use)(not in use) (not in use)Operating System Stack Code Heap Figure 16.2: Placing Segments In Physical Memory As you can see in the diagram, only used memory is allocated space in physical memory, and thus large address spaces with large a mounts of unused address space (which we sometimes call sparse address spaces ) can be accommodated. The hardware structure in our MMU required to support segmenta - tion is just what you’d expect: in this case, a set of three base and bounds register pairs. Figure 16.3 below shows the register values for the exam- ple above; each bounds register holds the size of a segment. Segment Base Size Code 32K 2K Heap 34K 2K Stack 28K 2K Figure 16.3: Segment Register Values You can see from the ﬁgure that the code segment is placed at physi cal address 32KB and has a size of 2KB and the heap segment is placed at 34KB and also has a size of 2KB.",3528
16. Segmentation,"Let’s do an example translation, using the address space in Fig ure 16.1. Assume a reference is made to virtual address 100 (which is in the code segment). When the reference takes place (say, on an instruct ion fetch), the hardware will add the base value to the offset into this segment (100 in this case) to arrive at the desired physical address: 100 + 32 KB, or 32868. It will then check that the address is within bounds (100 is les s than 2KB), ﬁnd that it is, and issue the reference to physical memory addr ess 32868. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 SEGMENTATION ASIDE : THESEGMENTATION FAULT The term segmentation fault or violation arises from a memory acces s on a segmented machine to an illegal address. Humorously, the te rm persists, even on machines with no support for segmentation at al l. Or not so humorously, if you can’t ﬁgure out why your code keeps faulting. Now let’s look at an address in the heap, virtual address 4200 (aga in refer to Figure 16.1). If we just add the virtual address 4200 to the base of the heap (34KB), we get a physical address of 39016, which is notthe correct physical address. What we need to ﬁrst do is extract th eoffset into the heap, i.e., which byte(s) in this segment the address refers to. Because the heap starts at virtual address 4KB (4096), the offset of 420 0 is actually 4200 minus 4096, or 104. We then take this offset (104) and add it to the base register physical address (34K) to get the desired resu lt: 34920. What if we tried to refer to an illegal address, such as 7KB whi ch is be- yond the end of the heap? You can imagine what will happen: the har d- ware detects that the address is out of bounds, traps into the OS, l ikely leading to the termination of the offending process. And now you know the origin of the famous term that all C programmers learn to dread : the segmentation violation orsegmentation fault . 16.2 Which Segment Are We Referring To? The hardware uses segment registers during translation. How d oes it know the offset into a segment, and to which segment an address r efers? One common approach, sometimes referred to as an explicit approach, is to chop up the address space into segments based on the top few b its of the virtual address; this technique was used in the VAX/VMS system [LL82]. In our example above, we have three segments; thus we ne ed two bits to accomplish our task. If we use the top two bits of our 14-bit v irtual address to select the segment, our virtual address looks like th is: 13 12 11 10 9 8 7 6 5 4 3 2 1 0 Segment Offset In our example, then, if the top two bits are 00, the hardware know s the virtual address is in the code segment, and thus uses the cod e base and bounds pair to relocate the address to the correct physical l ocation. If the top two bits are 01, the hardware knows the address is in th e heap, and thus uses the heap base and bounds. Let’s take our example hea p virtual address from above (4200) and translate it, just to mak e sure this is clear. The virtual address 4200, in binary form, can be seen here: 13 012 111 010 09 08 07 06 15 14 03 12 01 00 0 Segment Offset OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEGMENTATION 5 As you can see from the picture, the top two bits (01) tell the hard ware which segment we are referring to. The bottom 12 bits are the offset into the segment: 0000 0110 1000, or hex 0x068, or 104 in decimal. Thu s, the hardware simply takes the ﬁrst two bits to determine which se gment reg- ister to use, and then takes the next 12 bits as the offset into t he segment. By adding the base register to the offset, the hardware arrive s at the ﬁ- nal physical address. Note the offset eases the bounds check too: w e can simply check if the offset is less than the bounds; if not, the add ress is ille- gal. Thus, if base and bounds were arrays (with one entry per seg ment), the hardware would be doing something like this to obtain the desi red physical address: 1// get top 2 bits of 14-bit VA 2Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT 3// now get offset 4Offset = VirtualAddress & OFFSET_MASK 5if (Offset >= Bounds[Segment]) 6RaiseException(PROTECTION_FAULT) 7else 8PhysAddr = Base[Segment] + Offset 9Register = AccessMemory(PhysAddr) In our running example, we can ﬁll in values for the constants abov e.",4334
16. Segmentation,"Speciﬁcally, SEGMASK would be set to 0x3000 ,SEGSHIFT to12, and OFFSETMASK to0xFFF . You may also have noticed that when we use the top two bits, and we only have three segments (code, heap, stack), one segment of the a ddress space goes unused. Thus, some systems put code in the same segmen t as the heap and thus use only one bit to select which segment to use [ LL82]. There are other ways for the hardware to determine which segmen t a particular address is in. In the implicit approach, the hardware deter- mines the segment by noticing how the address was formed. If, for e x- ample, the address was generated from the program counter (i.e. , it was an instruction fetch), then the address is within the code segm ent; if the address is based off of the stack or base pointer, it must be in the s tack segment; any other address must be in the heap. 16.3 What About The Stack? Thus far, we’ve left out one important component of the address space : the stack. The stack has been relocated to physical address 28 KB in the di- agram above, but with one critical difference: it grows backwards . In phys- ical memory, it starts at 28KB and grows back to 26KB, correspondi ng to virtual addresses 16KB to 14KB; translation must proceed diff erently. The ﬁrst thing we need is a little extra hardware support. Inst ead of just base and bounds values, the hardware also needs to know whi ch way the segment grows (a bit, for example, that is set to 1 when the se gment grows in the positive direction, and 0 for negative). Our updated view of what the hardware tracks is seen in Figure 16.4. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 SEGMENTATION Segment Base Size Grows Positive? Code 32K 2K 1 Heap 34K 2K 1 Stack 28K 2K 0 Figure 16.4: Segment Registers (With Negative-Growth Support) With the hardware understanding that segments can grow in the neg- ative direction, the hardware must now translate such virtual addresses slightly differently. Let’s take an example stack virtual ad dress and trans- late it to understand the process. In this example, assume we wish to access virtual address 15K B, which should map to physical address 27KB. Our virtual address, in b inary form, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The ha rd- ware uses the top two bits (11) to designate the segment, but th en we are left with an offset of 3KB. To obtain the correct negative offset, w e must subtract the maximum segment size from 3KB: in this example, a seg- ment can be 4KB, and thus the correct negative offset is 3KB minu s 4KB which equals -1KB. We simply add the negative offset (-1KB) to the base (28KB) to arrive at the correct physical address: 27KB. The bou nds check can be calculated by ensuring the absolute value of the negativ e offset is less than the segment’s size. 16.4 Support for Sharing As support for segmentation grew, system designers soon realized that they could realize new types of efﬁciencies with a little more ha rdware support. Speciﬁcally, to save memory, sometimes it is useful to share certain memory segments between address spaces.",3096
16. Segmentation,"In particul ar,code sharing is common and still in use in systems today. To support sharing, we need a little extra support from the hardw are, in the form of protection bits . Basic support adds a few bits per segment, indicating whether or not a program can read or write a segment, or p er- haps execute code that lies within the segment. By setting a cod e segment to read-only, the same code can be shared across multiple process es, with- out worry of harming isolation; while each process still thinks tha t it is ac- cessing its own private memory, the OS is secretly sharing memor y which cannot be modiﬁed by the process, and thus the illusion is preserv ed. An example of the additional information tracked by the hardware (and OS) is shown in Figure 16.5. As you can see, the code segment is set to read and execute, and thus the same physical segment in memory could be mapped into multiple virtual address spaces. Segment Base Size Grows Positive? Protection Code 32K 2K 1 Read-Execute Heap 34K 2K 1 Read-Write Stack 28K 2K 0 Read-Write Figure 16.5: Segment Register Values (with Protection) OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEGMENTATION 7 With protection bits, the hardware algorithm described earlie r would also have to change. In addition to checking whether a virtual address is within bounds, the hardware also has to check whether a partic ular access is permissible. If a user process tries to write to a read-only s egment, or execute from a non-executable segment, the hardware should rai se an exception, and thus let the OS deal with the offending process. 16.5 Fine-grained vs. Coarse-grained Segmentation Most of our examples thus far have focused on systems with just a few segments (i.e., code, stack, heap); we can think of this seg mentation ascoarse-grained , as it chops up the address space into relatively large, coarse chunks. However, some early systems (e.g., Multics [CV6 5,DD68]) were more ﬂexible and allowed for address spaces to consist of a lar ge number of smaller segments, referred to as ﬁne-grained segmentation. Supporting many segments requires even further hardware supp ort, with a segment table of some kind stored in memory. Such segment ta- bles usually support the creation of a very large number of segmen ts, and thus enable a system to use segments in more ﬂexible ways than w e have thus far discussed. For example, early machines like the Burr oughs B5000 had support for thousands of segments, and expected a compiler to c hop code and data into separate segments which the OS and hardware would then support [RK68]. The thinking at the time was that by havin g ﬁne- grained segments, the OS could better learn about which segmen ts are in use and which are not and thus utilize main memory more effective ly. 16.6 OS Support You now should have a basic idea as to how segmentation works. Pieces of the address space are relocated into physical memory a s the system runs, and thus a huge savings of physical memory is achie ved relative to our simpler approach with just a single base/bounds pair for the entire address space. Speciﬁcally, all the unused space b etween the stack and the heap need not be allocated in physical memory, allow ing us to ﬁt more address spaces into physical memory.",3275
16. Segmentation,"However, segmentation raises a number of new issues. We’ll ﬁrst d e- scribe the new OS issues that must be addressed. The ﬁrst is an old one: what should the OS do on a context switch? You should have a good guess by now: the segment registers must be saved and restored. Clearly, each process has its own virtual address space, and the OS must m ake sure to set up these registers correctly before letting the proc ess run again. The second, and more important, issue is managing free space in p hys- ical memory. When a new address space is created, the OS has to b e able to ﬁnd space in physical memory for its segments. Previousl y, we assumed that each address space was the same size, and thus ph ysical memory could be thought of as a bunch of slots where processes would ﬁt in. Now, we have a number of segments per process, and each segm ent might be a different size. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 SEGMENTATION 64KB56KB48KB40KB32KB24KB16KB8KB0KB Operating SystemNot Compacted (not in use) (not in use) (not in use)Allocated Allocated Allocated 64KB56KB48KB40KB32KB24KB16KB8KB0KB (not in use)AllocatedOperating SystemCompacted Figure 16.6: Non-compacted and Compacted Memory The general problem that arises is that physical memory quickl y be- comes full of little holes of free space, making it difﬁcult to all ocate new segments, or to grow existing ones. We call this problem external frag- mentation [R69]; see Figure 16.6 (left). In the example, a process comes along and wishes to allocate a 20KB segment. In that example, there is 24KB free, but not in one conti guous segment (rather, in three non-contiguous chunks). Thus, the OS cannot satisfy the 20KB request. One solution to this problem would be to compact physical memory by rearranging the existing segments. For example, the OS coul d stop whichever processes are running, copy their data to one contiguou s re- gion of memory, change their segment register values to point to t he new physical locations, and thus have a large free extent of memor y with which to work. By doing so, the OS enables the new allocation reques t to succeed. However, compaction is expensive, as copying segmen ts is memory-intensive and generally uses a fair amount of processor ti me. See Figure 16.6 (right) for a diagram of compacted physical memory . A simpler approach is to use a free-list management algorithm t hat tries to keep large extents of memory available for allocation. Th ere are literally hundreds of approaches that people have taken, inclu ding clas- sic algorithms like best-ﬁt (which keeps a list of free spaces and returns the one closest in size that satisﬁes the desired allocation to th e requester), worst-ﬁt ,ﬁrst-ﬁt , and more complex schemes like buddy algorithm [K68]. An excellent survey by Wilson et al. is a good place to start if you w ant to learn more about such algorithms [W+95], or you can wait until we cov er some of the basics ourselves in a later chapter. Unfortunately, t hough, no matter how smart the algorithm, external fragmentation will st ill exist; thus, a good algorithm simply attempts to minimize it. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEGMENTATION 9 TIP: IF1000 S OLUTIONS EXIST, NOGREAT ONEDOES The fact that so many different algorithms exist to try to mini mize exter- nal fragmentation is indicative of a stronger underlying truth : there is no one “best” way to solve the problem.",3443
16. Segmentation,"Thus, we settle for something r ea- sonable and hope it is good enough. The only real solution (as we will see in forthcoming chapters) is to avoid the problem altogether, b y never allocating memory in variable-sized chunks. 16.7 Summary Segmentation solves a number of problems, and helps us build a more effective virtualization of memory. Beyond just dynamic relocat ion, seg- mentation can better support sparse address spaces, by avoidin g the huge potential waste of memory between logical segments of the address space. It is also fast, as doing the arithmetic segmentation requires is easy and well-suited to hardware; the overheads of translation are mini mal. A fringe beneﬁt arises too: code sharing. If code is placed within a sepa- rate segment, such a segment could potentially be shared across multiple running programs. However, as we learned, allocating variable-sized segments i n mem- ory leads to some problems that we’d like to overcome. The ﬁrst, as di s- cussed above, is external fragmentation. Because segments ar e variable- sized, free memory gets chopped up into odd-sized pieces, and th us sat- isfying a memory-allocation request can be difﬁcult. One can tr y to use smart algorithms [W+95] or periodically compact memory, but the p rob- lem is fundamental and hard to avoid. The second and perhaps more important problem is that segmentati on still isn’t ﬂexible enough to support our fully generalized, spa rse address space. For example, if we have a large but sparsely-used heap a ll in one logical segment, the entire heap must still reside in memory in order to be accessed. In other words, if our model of how the address space is bei ng used doesn’t exactly match how the underlying segmentation has b een designed to support it, segmentation doesn’t work very well. We th us need to ﬁnd some new solutions. Ready to ﬁnd them? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 SEGMENTATION References [CV65] “Introduction and Overview of the Multics System” by F. J. Corba to, V . A. Vyssotsky. Fall Joint Computer Conference, 1965. One of ﬁve papers presented on Multics at the Fall Joint Computer Conference; oh to be a ﬂy on the wall in that room that day. [DD68] “Virtual Memory, Processes, and Sharing in Multics” by Robert C . Daley and Jack B. Dennis. Communications of the ACM, Volume 11:5, May 1968. An early paper on how to perform dynamic linking in Multics, which was way ahead of its time. Dynamic linkin g ﬁnally found its way back into systems about 20 years later, as the large X-windows libraries deman ded it. Some say that these large X11 libraries were MIT’s revenge for removing support for dynamic linking in early versions ofUNIX. [G62] “Fact Segmentation” by M. N. Greenﬁeld. Proceedings of the SJCC, Vo lume 21, May 1962. Another early paper on segmentation; so early that it has no references to other work. [H61] “Program Organization and Record Keeping for Dynamic Storage” b y A. W. Holt. Com- munications of the ACM, Volume 4:10, October 1961. An incredibly early and difﬁcult to read paper about segmentation and some of its uses.",3110
16. Segmentation,"[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” by Intel. 2009. Avail- able: http://www.intel.com/products/processor/manuals. Try reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little bit. [K68] “The Art of Computer Programming: Volume I” by Donald Knuth. Add ison-Wesley, 1968. Knuth is famous not only for his early books on the Art of Computer Programming but for his typesetting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to typeset this very book. His tomes on algorithms are a great early refe rence to many of the algorithms that underly computing systems today. [L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems Review, 15:5, October 1983. A treasure-trove of sage advice on how to build systems. Hard to read in one sitting; take it in a little at a time, like a ﬁne wine, or a reference manual . [LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Henry M. Levy, Peter H. Lipman. IEEE Computer, Volume 15:3, March 1982. A classic memory management system, with lots of common sense in its design. We’ll study it in more detai l in a later chapter. [RK68] “Dynamic Storage Allocation Systems” by B. Randell and C.J. Kuehner. Communica- tions of the ACM, Volume 11:5, May 1968. A nice overview of the differences between paging and segmentation, with some historical discussion of various machines. [R69] “A note on storage fragmentation and program segmentation” by Brian Randell. Com- munications of the ACM, Volume 12:7, July 1969. One of the earliest papers to discuss fragmenta- tion. [W+95] “Dynamic Storage Allocation: A Survey and Critical Review” by Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles. International Workshop on Memo ry Management, Scotland, UK, September 1995. A great survey paper on memory allocators. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEGMENTATION 11 Homework (Simulation) This program allows you to see how address translations are perform ed in a system with segmentation. See the README for details. Questions 1. First let’s use a tiny address space to translate some addres ses. Here’s a sim- ple set of parameters with a few different random seeds; can yo u translate the addresses? segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0 segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1 segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2 2. Now, let’s see if we understand this tiny address space we’v e constructed (using the parameters from the question above). What is the high est legal virtual address in segment 0? What about the lowest legal virtual address in segment 1? What are the lowest and highest illegal addresses in this entire address space? Finally, how would you run segmentation.py with the -Aﬂag to test if you are right? 3. Let’s say we have a tiny 16-byte address space in a 128-byte physical mem- ory. What base and bounds would you set up so as to get the simulator t o generate the following translation results for the speciﬁed address stream: valid, valid, violation, ..., violation, valid, valid? Assume the following pa- rameters: segmentation.py -a 16 -p 128 -A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 --b0 ? --l0 ? --b1 ? --l1 ? 4. Assume we want to generate a problem where roughly 90 percent of the ran domly- generated virtual addresses are valid (not segmentation viol ations). How should you conﬁgure the simulator to do so? Which parameters are imp or- tant to getting this outcome? 5. Can you run the simulator such that no virtual addresses are vali d? How? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",3716
17. Free Space Management,"17 Free-Space Management In this chapter, we take a small detour from our discussion of virtu al- izing memory to discuss a fundamental aspect of any memory manag e- ment system, whether it be a malloc library (managing pages of a pro- cess’s heap) or the OS itself (managing portions of the address spa ce of a process). Speciﬁcally, we will discuss the issues surrounding free-space management . Let us make the problem more speciﬁc. Managing free space can ce r- tainly be easy, as we will see when we discuss the concept of paging . It is easy when the space you are managing is divided into ﬁxed-size d units; in such a case, you just keep a list of these ﬁxed-sized units; wh en a client requests one of them, return the ﬁrst entry. Where free-space management becomes more difﬁcult (and inter est- ing) is when the free space you are managing consists of variable -sized units; this arises in a user-level memory-allocation library ( as inmalloc() andfree() ) and in an OS managing physical memory when using seg- mentation to implement virtual memory. In either case, the problem that exists is known as external fragmentation : the free space gets chopped into little pieces of different sizes and is thus fragmented; subsequent re- quests may fail because there is no single contiguous space tha t can sat- isfy the request, even though the total amount of free space excee ds the size of the request. free used free 0 10 20 30 The ﬁgure shows an example of this problem. In this case, the total free space available is 20 bytes; unfortunately, it is fragme nted into two chunks of size 10 each. As a result, a request for 15 bytes will fa il even though there are 20 bytes free. And thus we arrive at the problem ad- dressed in this chapter. 1 2 FREE-SPACE MANAGEMENT CRUX: HOWTOMANAGE FREE SPACE How should free space be managed, when satisfying variable-si zed re- quests? What strategies can be used to minimize fragmentati on? What are the time and space overheads of alternate approaches? 17.1 Assumptions Most of this discussion will focus on the great history of allocators found in user-level memory-allocation libraries. We draw on Wils on’s excellent survey [W+95] but encourage interested readers to go to the source document itself for more details1. We assume a basic interface such as that provided by malloc() and free() . Speciﬁcally, void*malloc(size t size) takes a single pa- rameter,size , which is the number of bytes requested by the applica- tion; it hands back a pointer (of no particular type, or a void pointer in C lingo) to a region of that size (or greater). The complementary rou tine void free(void *ptr) takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when fre eing the space, does not inform the library of its size; thus, the library m ust be able to ﬁgure out how big a chunk of memory is when handed just a pointer to it. We’ll discuss how to do this a bit later on in the chapter. The space that this library manages is known historically as th eheap , and the generic data structure used to manage free space in th e heap is some kind of free list . This structure contains references to all of the free chunks of space in the managed region of memory. Of course, this dat a structure need not be a list per se , but just some kind of data structure to track free space.",3371
17. Free Space Management,"We further assume that primarily we are concerned with external frag- mentation , as described above. Allocators could of course also have the problem of internal fragmentation ; if an allocator hands out chunks of memory bigger than that requested, any unasked for (and thus un used) space in such a chunk is considered internal fragmentation (because the waste occurs inside the allocated unit) and is another example of space waste. However, for the sake of simplicity, and because it is the more in- teresting of the two types of fragmentation, we’ll mostly focus on ex ternal fragmentation. We’ll also assume that once memory is handed out to a client, it can not be relocated to another location in memory. For example, if a program callsmalloc() and is given a pointer to some space within the heap, that memory region is essentially “owned” by the program (and can not be moved by the library) until the program returns it via a corres pond- ing call to free() . Thus, no compaction of free space is possible, which 1It is nearly 80 pages long; thus, you really have to be interested. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 3 would be useful to combat fragmentation2. Compaction could, however, be used in the OS to deal with fragmentation when implementing seg- mentation (as discussed in said chapter on segmentation). Finally, we’ll assume that the allocator manages a contiguous reg ion of bytes. In some cases, an allocator could ask for that region to grow; for example, a user-level memory-allocation library might call into the kernel to grow the heap (via a system call such as sbrk ) when it runs out of space. However, for simplicity, we’ll just assume that the reg ion is a single ﬁxed size throughout its life. 17.2 Low-level Mechanisms Before delving into some policy details, we’ll ﬁrst cover some com- mon mechanisms used in most allocators. First, we’ll discuss the b asics of splitting and coalescing, common techniques in most any allocator . Sec- ond, we’ll show how one can track the size of allocated regions quickly and with relative ease. Finally, we’ll discuss how to build a si mple list inside the free space to keep track of what is free and what isn’t . Splitting and Coalescing A free list contains a set of elements that describe the free spa ce still re- maining in the heap. Thus, assume the following 30-byte heap: free used free 0 10 20 30 The free list for this heap would have two elements on it. One entr y de- scribes the ﬁrst 10-byte free segment (bytes 0-9), and one ent ry describes the other free segment (bytes 20-29): headaddr:0 len:10addr:20 len:10NULL As described above, a request for anything greater than 10 byte s will fail (returning NULL); there just isn’t a single contiguous chu nk of mem- ory of that size available. A request for exactly that size (10 by tes) could be satisﬁed easily by either of the free chunks. But what happe ns if the request is for something smaller than 10 bytes? Assume we have a request for just a single byte of memory.",3043
17. Free Space Management,"In this case, the allocator will perform an action known as splitting : it will ﬁnd 2Once you hand a pointer to a chunk of memory to a C program, it is generally d ifﬁcult to determine all references (pointers) to that region, which may be stor ed in other variables or even in registers at a given point in execution. This may not be the ca se in more strongly- typed, garbage-collected languages, which would thus enable compacti on as a technique to combat fragmentation. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 FREE-SPACE MANAGEMENT a free chunk of memory that can satisfy the request and split it i nto two. The ﬁrst chunk it will return to the caller; the second chunk wi ll remain on the list. Thus, in our example above, if a request for 1 byte were made, and the allocator decided to use the second of the two elements on th e list to satisfy the request, the call to malloc() would return 20 (th e address of the 1-byte allocated region) and the list would end up looking like this: headaddr:0 len:10addr:21 len:9NULL In the picture, you can see the list basically stays intact; th e only change is that the free region now starts at 21 instead of 20, and the leng th of that free region is now just 93. Thus, the split is commonly used in allocators when requests are smaller than the size of any particular free chunk. A corollary mechanism found in many allocators is known as coalesc- ingof free space. Take our example from above once more (free 10 bytes, used 10 bytes, and another free 10 bytes). Given this (tiny) heap, what happens when an application call s free(10), thus returning the space in the middle of the heap? If we simply add this free space back into our list without too much thinking, we might end up with a list that looks like this: headaddr:10 len:10addr:0 len:10addr:20 len:10NULL Note the problem: while the entire heap is now free, it is seeming ly divided into three chunks of 10 bytes each. Thus, if a user requ ests 20 bytes, a simple list traversal will not ﬁnd such a free chunk, a nd return failure. What allocators do in order to avoid this problem is coalesce free sp ace when a chunk of memory is freed. The idea is simple: when returni ng a free chunk in memory, look carefully at the addresses of the chunk you are returning as well as the nearby chunks of free space; if the newly- freed space sits right next to one (or two, as in this example) exi sting free chunks, merge them into a single larger free chunk. Thus, wit h coalesc- ing, our ﬁnal list should look like this: headaddr:0 len:30NULL Indeed, this is what the heap list looked like at ﬁrst, before any allo- cations were made. With coalescing, an allocator can better ensu re that large free extents are available for the application. 3This discussion assumes that there are no headers, an unrealistic but simplifying assump- tion we make for now. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 5 ptrThe header used by malloc library The 20 bytes returned to caller Figure 17.1: An Allocated Region Plus Header size: 20 magic: 1234567hptr ptr The 20 bytes returned to caller Figure 17.2: Speciﬁc Contents Of The Header Tracking The Size Of Allocated Regions You might have noticed that the interface to free(void *ptr) does not take a size parameter; thus it is assumed that given a pointe r, the malloc library can quickly determine the size of the region of mem ory being freed and thus incorporate the space back into the free li st. To accomplish this task, most allocators store a little bit of extra infor- mation in a header block which is kept in memory, usually just before the handed-out chunk of memory.",3672
17. Free Space Management,"Let’s look at an example again (Fig- ure 17.1). In this example, we are examining an allocated block of size 20 bytes, pointed to by ptr; imagine the user called malloc() and stored the results in ptr, e.g.,ptr = malloc(20); . The header minimally contains the size of the allocated region (i n this case, 20); it may also contain additional pointers to speed up de alloca- tion, a magic number to provide additional integrity checking, and other information. Let’s assume a simple header which contains the siz e of the region and a magic number, like this: typedef struct __header_t { int size; int magic; } header_t; The example above would look like what you see in Figure 17.2. When c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 FREE-SPACE MANAGEMENT the user calls free(ptr) , the library then uses simple pointer arithmetic to ﬁgure out where the header begins: void free(void *ptr) { header_t *hptr = (void *)ptr - sizeof(header_t); ... After obtaining such a pointer to the header, the library can ea sily de- termine whether the magic number matches the expected value as a san- ity check ( assert(hptr->magic == 1234567) ) and calculate the to- tal size of the newly-freed region via simple math (i.e., addin g the size of the header to size of the region). Note the small but critical det ail in the last sentence: the size of the free region is the size of the heade r plus the size of the space allocated to the user. Thus, when a user reques tsNbytes of memory, the library does not search for a free chunk of size N; rather, it searches for a free chunk of size Nplus the size of the header. Embedding A Free List Thus far we have treated our simple free list as a conceptual ent ity; it is just a list describing the free chunks of memory in the heap. But how do we build such a list inside the free space itself? In a more typical list, when allocating a new node, you would just ca ll malloc() when you need space for the node. Unfortunately, within the memory-allocation library, you can’t do this. Instead, you need to build the list inside the free space itself. Don’t worry if this sounds a little weird; it is, but not so weird that you can’t do it. Assume we have a 4096-byte chunk of memory to manage (i.e., the heap is 4KB). To manage this as a free list, we ﬁrst have to init ialize said list; initially, the list should have one entry, of size 4096 (mi nus the header size). Here is the description of a node of the list: typedef struct __node_t { int size; struct __node_t *next; } node_t; Now let’s look at some code that initializes the heap and puts the ﬁrs t element of the free list inside that space. We are assuming tha t the heap is built within some free space acquired via a call to the system c allmmap() ; this is not the only way to build such a heap but serves us well in t his example. Here is the code: // mmap() returns a pointer to a chunk of free space node_t*head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0); head->size = 4096 - sizeof(node_t); head->next = NULL; After running this code, the status of the list is that it has a si ngle entry, of size 4088. Yes, this is a tiny heap, but it serves as a ﬁne exam ple for us OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 7 size: 4088 next: 0 ...head [virtual address: 16KB] header: size field header: next field (NULL is 0) the rest of the 4KB chunk Figure 17.3: A Heap With One Free Chunk size: 100 magic: 1234567 .",3478
17. Free Space Management,". . size: 3980 next: 0 . . .ptr[virtual address: 16KB] headThe 100 bytes now allocated The free 3980 byte chunk Figure 17.4: A Heap: After One Allocation here. Thehead pointer contains the beginning address of this range; let’s assume it is 16KB (though any virtual address would be ﬁne). Vis ually, the heap thus looks like what you see in Figure 17.3. Now, let’s imagine that a chunk of memory is requested, say of size 100 bytes. To service this request, the library will ﬁrst ﬁnd a chunk that is large enough to accommodate the request; because there is only one free chunk (size: 4088), this chunk will be chosen. Then, the chunk will be split into two: one chunk big enough to service the request (and header , as described above), and the remaining free chunk. Assuming a n 8-byte header (an integer size and an integer magic number), the spa ce in the heap now looks like what you see in Figure 17.4. Thus, upon the request for 100 bytes, the library allocated 108 b ytes out of the existing one free chunk, returns a pointer (marked ptr in the ﬁgure above) to it, stashes the header information immediately before the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 FREE-SPACE MANAGEMENT size: 100 magic: 1234567 . . . size: 100 magic: 1234567 . . . size: 100 magic: 1234567 . . . size: 3764 next: 0 . . .sptr[virtual address: 16KB] head100 bytes still allocated 100 bytes still allocated  (but about to be freed) 100-bytes still allocated The free 3764-byte chunk Figure 17.5: Free Space With Three Chunks Allocated allocated space for later use upon free() , and shrinks the one free node in the list to 3980 bytes (4088 minus 108). Now let’s look at the heap when there are three allocated regions, ea ch of 100 bytes (or 108 including the header). A visualization of thi s heap is shown in Figure 17.5. As you can see therein, the ﬁrst 324 bytes of the heap are now allo- cated, and thus we see three headers in that space as well as th ree 100- byte regions being used by the calling program. The free list re mains uninteresting: just a single node (pointed to by head ), but now only 3764 bytes in size after the three splits. But what happens when th e calling program returns some memory via free() ? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 9 size: 100 magic: 1234567 . . . size: 100 next: 16708 . . . size: 100 magic: 1234567 . . . size: 3764 next: 0 . . .[virtual address: 16KB] head sptr100 bytes still allocated (now a free chunk of memory) 100-bytes still allocated The free 3764-byte chunk Figure 17.6: Free Space With Two Chunks Allocated In this example, the application returns the middle chunk of al located memory, by calling free(16500) (the value 16500 is arrived upon by adding the start of the memory region, 16384, to the 108 of the prev ious chunk and the 8 bytes of the header for this chunk). This value is shown in the previous diagram by the pointer sptr . The library immediately ﬁgures out the size of the free region, a nd then adds the free chunk back onto the free list. Assuming we in sert at the head of the free list, the space now looks like this (Figure 17.",3148
17. Free Space Management,"6). And now we have a list that starts with a small free chunk (100 by tes, pointed to by the head of the list) and a large free chunk (3764 by tes). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 FREE-SPACE MANAGEMENT size: 100 next: 16492 . . . size: 100 next: 16708 . . . size: 100 next: 16384 . . . size: 3764 next: 0 . . .[virtual address: 16KB] head(now free) (now free) (now free) The free 3764-byte chunk Figure 17.7: A Non-Coalesced Free List Our list ﬁnally has more than one element on it. And yes, the free s pace is fragmented, an unfortunate but common occurrence. One last example: let’s assume now that the last two in-use chun ks are freed. Without coalescing, you might end up with a free list that is highly fragmented (see Figure 17.7). As you can see from the ﬁgure, we now have a big mess. Why? Simple, we forgot to coalesce the list. Although all of the memory is free, it is chopped up into pieces, thus appearing as a fragmented memory d espite not being one. The solution is simple: go through the list and merge neighboring chunks; when ﬁnished, the heap will be whole again . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 11 Growing The Heap We should discuss one last mechanism found within many allocation li- braries. Speciﬁcally, what should you do if the heap runs out of spa ce? The simplest approach is just to fail. In some cases this is the on ly option, and thus returning NULL is an honorable approach. Don’t feel bad. Y ou tried, and though you failed, you fought the good ﬁght. Most traditional allocators start with a small-sized heap and th en re- quest more memory from the OS when they run out. Typically, this me ans they make some kind of system call (e.g., sbrk in most U NIXsystems) to grow the heap, and then allocate the new chunks from there. To ser vice thesbrk request, the OS ﬁnds free physical pages, maps them into the address space of the requesting process, and then returns the v alue of the end of the new heap; at that point, a larger heap is available , and the request can be successfully serviced. 17.3 Basic Strategies Now that we have some machinery under our belt, let’s go over some basic strategies for managing free space. These approaches ar e mostly based on pretty simple policies that you could think up yourself; t ry it before reading and see if you come up with all of the alternatives ( or maybe some new ones.). The ideal allocator is both fast and minimizes fragmentation. Un fortu- nately, because the stream of allocation and free requests can b e arbitrary (after all, they are determined by the programmer), any parti cular strat- egy can do quite badly given the wrong set of inputs. Thus, we wil l not describe a “best” approach, but rather talk about some basics an d discuss their pros and cons. Best Fit The best ﬁt strategy is quite simple: ﬁrst, search through the free list a nd ﬁnd chunks of free memory that are as big or bigger than the reques ted size. Then, return the one that is the smallest in that group of ca ndidates; this is the so called best-ﬁt chunk (it could be called smalles t ﬁt too).",3134
17. Free Space Management,"One pass through the free list is enough to ﬁnd the correct block to ret urn. The intuition behind best ﬁt is simple: by returning a block tha t is close to what the user asks, best ﬁt tries to reduce wasted space. How ever, there is a cost; naive implementations pay a heavy performance penalt y when performing an exhaustive search for the correct free block. Worst Fit The worst ﬁt approach is the opposite of best ﬁt; ﬁnd the largest chunk and return the requested amount; keep the remaining (large) c hunk on the free list. Worst ﬁt tries to thus leave big chunks free inst ead of lots of c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 FREE-SPACE MANAGEMENT small chunks that can arise from a best-ﬁt approach. Once again , how- ever, a full search of free space is required, and thus this app roach can be costly. Worse, most studies show that it performs badly, leading t o excess fragmentation while still having high overheads. First Fit The ﬁrst ﬁt method simply ﬁnds the ﬁrst block that is big enough and returns the requested amount to the user. As before, the remain ing free space is kept free for subsequent requests. First ﬁt has the advantage of speed — no exhaustive search of all the free spaces are necessary — but sometimes pollutes the beginni ng of the free list with small objects. Thus, how the allocator manages the free list’s order becomes an issue. One approach is to use address-based ordering ; by keeping the list ordered by the address of the free space, coal escing becomes easier, and fragmentation tends to be reduced. Next Fit Instead of always beginning the ﬁrst-ﬁt search at the beginni ng of the list, thenext ﬁt algorithm keeps an extra pointer to the location within the list where one was looking last. The idea is to spread the searche s for free space throughout the list more uniformly, thus avoiding spli ntering of the beginning of the list. The performance of such an approach is quite similar to ﬁrst ﬁt, as an exhaustive search is once again avoide d. Examples Here are a few examples of the above strategies. Envision a free l ist with three elements on it, of sizes 10, 30, and 20 (we’ll ignore headers and other details here, instead just focusing on how strategies operate): head 10 30 20 NULL Assume an allocation request of size 15. A best-ﬁt approach would search the entire list and ﬁnd that 20 was the best ﬁt, as it is t he smallest free space that can accommodate the request. The resulting fre e list: head 10 30 5 NULL As happens in this example, and often happens with a best-ﬁt ap - proach, a small free chunk is now left over. A worst-ﬁt approach is s imilar but instead ﬁnds the largest chunk, in this example 30. The re sulting list: head 10 15 20 NULL OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 13 The ﬁrst-ﬁt strategy, in this example, does the same thing as w orst-ﬁt, also ﬁnding the ﬁrst free block that can satisfy the request. T he difference is in the search cost; both best-ﬁt and worst-ﬁt look through the ent ire list; ﬁrst-ﬁt only examines free chunks until it ﬁnds one that ﬁts, th us reducing search cost.",3129
17. Free Space Management,"These examples just scratch the surface of allocation policies. More detailed analysis with real workloads and more complex allocator b ehav- iors (e.g., coalescing) are required for a deeper understandin g. Perhaps something for a homework section, you say? 17.4 Other Approaches Beyond the basic approaches described above, there have been a h ost of suggested techniques and algorithms to improve memory allocat ion in some way. We list a few of them here for your consideration (i.e., to m ake you think about a little more than just best-ﬁt allocation). Segregated Lists One interesting approach that has been around for some time is the use ofsegregated lists . The basic idea is simple: if a particular application has one (or a few) popular-sized request that it makes, keep a sep arate list just to manage objects of that size; all other requests are f orwarded to a more general memory allocator. The beneﬁts of such an approach are obvious. By having a chunk of memory dedicated for one particular size of requests, fragmenta tion is much less of a concern; moreover, allocation and free requests can b e served quite quickly when they are of the right size, as no compl icated search of a list is required. Just like any good idea, this approach introduces new complication s into a system as well. For example, how much memory should one ded- icate to the pool of memory that serves specialized requests of a gi ven size, as opposed to the general pool? One particular allocator, the slab allocator by uber-engineer Jeff Bonwick (which was designed for use in the Solaris kernel), handles this issue in a rather nice way [B9 4]. Speciﬁcally, when the kernel boots up, it allocates a number of object caches for kernel objects that are likely to be requested frequently ( such as locks, ﬁle-system inodes, etc.); the object caches thus are eac h segregated free lists of a given size and serve memory allocation and free req uests quickly. When a given cache is running low on free space, it requ ests some slabs of memory from a more general memory allocator (the to- tal amount requested being a multiple of the page size and the obj ect in question). Conversely, when the reference counts of the objects w ithin a given slab all go to zero, the general allocator can reclaim the m from the specialized allocator, which is often done when the VM system needs more memory. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 FREE-SPACE MANAGEMENT ASIDE : GREAT ENGINEERS AREREALLY GREAT Engineers like Jeff Bonwick (who not only wrote the slab allocator m en- tioned herein but also was the lead of an amazing ﬁle system, ZFS) are the heart of Silicon Valley. Behind almost any great product or tech nol- ogy is a human (or small group of humans) who are way above average in their talents, abilities, and dedication. As Mark Zuckerb erg (of Face- book) says: “Someone who is exceptional in their role is not just a litt le better than someone who is pretty good. They are 100 times better. ” This is why, still today, one or two people can start a company that chang es the face of the world forever (think Google, Apple, or Facebook).",3157
17. Free Space Management,"Work hard and you might become such a “100x” person as well. Failing th at, work with such a person; you’ll learn more in a day than most learn in a month. Failing that, feel sad. The slab allocator also goes beyond most segregated list approache s by keeping free objects on the lists in a pre-initialized state . Bonwick shows that initialization and destruction of data structures is costly [B94]; by keeping freed objects in a particular list in their initial ized state, the slab allocator thus avoids frequent initialization and destruc tion cycles per object and thus lowers overheads noticeably. Buddy Allocation Because coalescing is critical for an allocator, some approaches h ave been designed around making coalescing simple. One good example is fou nd in the binary buddy allocator [K65]. In such a system, free memory is ﬁrst conceptually thought of as one big space of size 2N. When a request for memory is made, the search for free space recursively divides free space by two until a block that is big enough to accommodate the request is found (and a further split in to two would result in a space that is too small). At this point, the requ ested block is returned to the user. Here is an example of a 64KB free sp ace getting divided in the search for a 7KB block: 64 KB 32 KB 32 KB 16 KB 16 KB 8 KB 8 KB OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 15 In the example, the leftmost 8KB block is allocated (as indicate d by the darker shade of gray) and returned to the user; note that this sc heme can suffer from internal fragmentation , as you are only allowed to give out power-of-two-sized blocks. The beauty of buddy allocation is found in what happens when that block is freed. When returning the 8KB block to the free list, th e allocator checks whether the “buddy” 8KB is free; if so, it coalesces the t wo blocks into a 16KB block. The allocator then checks if the buddy of the 16K B block is still free; if so, it coalesces those two blocks. This recu rsive coa- lescing process continues up the tree, either restoring the ent ire free space or stopping when a buddy is found to be in use. The reason buddy allocation works so well is that it is simple to de - termine the buddy of a particular block. How, you ask? Think about t he addresses of the blocks in the free space above. If you think caref ully enough, you’ll see that the address of each buddy pair only differs by a single bit; which bit is determined by the level in the buddy tree. And thus you have a basic idea of how binary buddy allocation schemes wor k. For more detail, as always, see the Wilson survey [W+95]. Other Ideas One major problem with many of the approaches described above is th eir lack of scaling . Speciﬁcally, searching lists can be quite slow. Thus, advanced allocators use more complex data structures to address these costs, trading simplicity for performance. Examples include b alanced bi- nary trees, splay trees, or partially-ordered trees [W+95]. Given that modern systems often have multiple processors and run multi-threaded workloads (something you’ll learn about in great d etail in the section of the book on Concurrency), it is not surprising that a lot of effort has been spent making allocators work well on multiprocess or- based systems.",3299
17. Free Space Management,"Two wonderful examples are found in Berger et al . [B+00] and Evans [E06]; check them out for the details. These are but two of the thousands of ideas people have had over time about memory allocators; read on your own if you are curious. Failing that, read about how the glibc allocator works [S15], to give you a sen se of what the real world is like. 17.5 Summary In this chapter, we’ve discussed the most rudimentary forms of me m- ory allocators. Such allocators exist everywhere, linked into eve ry C pro- gram you write, as well as in the underlying OS which is managin g mem- ory for its own data structures. As with many systems, there are m any trade-offs to be made in building such a system, and the more you k now about the exact workload presented to an allocator, the more you could do to tune it to work better for that workload. Making a fast, space-e fﬁcient, scalable allocator that works well for a broad range of workloads rema ins an on-going challenge in modern computer systems. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 FREE-SPACE MANAGEMENT References [B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Appli cations” by Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, Paul R. Wilson. ASPLOS -IX, November 2000. Berger and company’s excellent allocator for multiprocessor systems. Bey ond just being a fun paper, also used in practice. [B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator” by Je ff Bonwick. USENIX ’94. A cool paper about how to build an allocator for an operating system kernel, and a great example of how to specialize for particular common object sizes. [E06] “A Scalable Concurrent malloc(3) Implementation for FreeBSD” b y Jason Evans. April, 2006. http://people.freebsd.org/˜jasone/jemalloc/bsdcan2006 /jemalloc.pdf. A detailed look at how to build a real modern allocator for use in multiprocessors. The “jemalloc ” allocator is in widespread use today, within FreeBSD, NetBSD, Mozilla Firefox, and within Facebook. [K65] “A Fast Storage Allocator” by Kenneth C. Knowlton. Communicati ons of the ACM, Volume 8:10, October 1965. The common reference for buddy allocation. Random strange fact: Knuth gives credit for the idea not to Knowlton but to Harry Markowitz, a Nobel-prize wi nning economist. Another strange fact: Knuth communicates all of his emails via a secretary; he doesn’t send email himself, rather he tells his secretary what email to send and then the secr etary does the work of emailing. Last Knuth fact: he created TeX, the tool used to typeset this book. It is an amazing pie ce of software4. [S15] “Understanding glibc malloc” by Sploitfun. February, 2015. s ploitfun.wordpress.com/ 2015/02/10/understanding-glibc-malloc/. A deep dive into how glibc malloc works. Amazingly detailed and a very cool read. [W+95] “Dynamic Storage Allocation: A Survey and Critical Review” by Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles. International Workshop on Memo ry Management, Scotland, UK, September 1995. An excellent and far-reaching survey of many facets of memory allocation.",3109
17. Free Space Management,"Far too much detail to go into in this tiny chapter. 4Actually we use LaTeX, which is based on Lamport’s additions to Te X, but close enough. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG FREE-SPACE MANAGEMENT 17 Homework (Simulation) The program, malloc.py , lets you explore the behavior of a simple free-space allocator as described in the chapter. See the READM E for details of its basic operation. Questions 1. First run with the ﬂags -n 10 -H 0 -p BEST -s 0 to generate a few random allocations and frees. Can you predict what alloc()/ free() will re- turn? Can you guess the state of the free list after each request? What do you notice about the free list over time? 2. How are the results different when using a WORST ﬁt policy to s earch the free list (-p WORST )? What changes? 3. What about when using FIRST ﬁt ( -p FIRST )? What speeds up when you use ﬁrst ﬁt? 4. For the above questions, how the list is kept ordered can aff ect the time it takes to ﬁnd a free location for some of the policies. Use the d ifferent free list orderings ( -l ADDRSORT ,-l SIZESORT+ ,-l SIZESORT- ) to see how the policies and the list orderings interact. 5. Coalescing of a free list can be quite important. Increase th e number of random allocations (say to -n 1000 ). What happens to larger allocation requests over time? Run with and without coalescing (i.e., witho ut and with the-Cﬂag). What differences in outcome do you see? How big is the free list over time in each case? Does the ordering of the list matter in this case? 6. What happens when you change the percent allocated fracti on-Pto higher than 50? What happens to allocations as it nears 100? What abo ut as the percent nears 0? 7. What kind of speciﬁc requests can you make to generate a highl y-fragmented free space? Use the -Aﬂag to create fragmented free lists, and see how dif- ferent policies and options change the organization of the f ree list. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",1978
18. Introduction to Paging,"18 Paging: Introduction It is sometimes said that the operating system takes one of two app roaches when solving most any space-management problem. The ﬁrst approa ch is to chop things up into variable-sized pieces, as we saw with segmenta- tion in virtual memory. Unfortunately, this solution has inherent di fﬁcul- ties. In particular, when dividing a space into different-s ize chunks, the space itself can become fragmented , and thus allocation becomes more challenging over time. Thus, it may be worth considering the second approach: to chop up space into ﬁxed-sized pieces. In virtual memory, we call this idea paging , and it goes back to an early and important system, the Atlas [KE+ 62, L78]. Instead of splitting up a process’s address space into some numbe r of variable-sized logical segments (e.g., code, heap, stack), w e divide it into ﬁxed-sized units, each of which we call a page . Correspondingly, we view physical memory as an array of ﬁxed-sized slots called page frames ; each of these frames can contain a single virtual-memory page. Our ch allenge: THECRUX: HOWTOVIRTUALIZE MEMORY WITHPAGES How can we virtualize memory with pages, so as to avoid the prob- lems of segmentation? What are the basic techniques? How do we ma ke those techniques work well, with minimal space and time overhea ds? 18.1 A Simple Example And Overview To help make this approach more clear, let’s illustrate it with a simple example. Figure 18.1 (page 2) presents an example of a tiny add ress space, only 64 bytes total in size, with four 16-byte pages (virtual pag es 0, 1, 2, and 3). Real address spaces are much bigger, of course, commonly 3 2 bits and thus 4-GB of address space, or even 64 bits1; in the book, we’ll often use tiny examples to make them easier to digest. 1A 64-bit address space is hard to imagine, it is so amazingly large. An analogy might help: if you think of a 32-bit address space as the size of a tennis court, a 64-bit address space is about the size of Europe(.). 1 2 PAGING : INTRODUCTION 644832160 (page 3)(page 2)(page 1)(page 0 of the address space) Figure 18.1: A Simple 64-byte Address Space Physical memory, as shown in Figure 18.2, also consists of a numbe r of ﬁxed-sized slots, in this case eight page frames (making for a 128-byte physical memory, also ridiculously small). As you can see in the diagram, the pages of the virtual address space have been placed at diff erent loca- tions throughout physical memory; the diagram also shows the OS us ing some of physical memory for itself. Paging, as we will see, has a number of advantages over our previou s approaches. Probably the most important improvement will be ﬂexibil- ity: with a fully-developed paging approach, the system will be ab le to support the abstraction of an address space effectively, regar dless of how a process uses the address space; we won’t, for example, make assu mp- tions about the direction the heap and stack grow and how they are us ed. Another advantage is the simplicity of free-space management that pag- ing affords.",3049
18. Introduction to Paging,"For example, when the OS wishes to place our tiny 64- byte address space into our eight-page physical memory, it simply ﬁ nds four free pages; perhaps the OS keeps a free list of all free pages for this, and just grabs the ﬁrst four free pages off of this list. In the exampl e, the OS 1281129680644832160 page frame 7page frame 6page frame 5page frame 4page frame 3page frame 2page frame 1page frame 0 of physical memory reserved for OS (unused) page 3 of AS page 0 of AS (unused) page 2 of AS (unused) page 1 of AS Figure 18.2: A 64-Byte Address Space In A 128-Byte Physical Memory OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 3 has placed virtual page 0 of the address space (AS) in physical f rame 3, virtual page 1 of the AS in physical frame 7, page 2 in frame 5, an d page 3 in frame 2. Page frames 1, 4, and 6 are currently free. To record where each virtual page of the address space is placed in physical memory, the operating system usually keeps a per-process data structure known as a page table . The major role of the page table is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page r esides. For our simple example (Figure 18.2, page 2), the page table woul d thus have the following four entries: (Virtual Page 0 →Physical Frame 3), (VP 1→PF 7), (VP 2 →PF 5), and (VP 3 →PF 2). It is important to remember that this page table is a per-process data structure (most page table structures we discuss are per-proc ess struc- tures; an exception we’ll touch on is the inverted page table ). If another process were to run in our example above, the OS would have to manag e a different page table for it, as its virtual pages obviously map todifferent physical pages (modulo any sharing going on). Now, we know enough to perform an address-translation example. Let’s imagine the process with that tiny address space (64 byte s) is per- forming a memory access: movl <virtual address>,  percenteax Speciﬁcally, let’s pay attention to the explicit load of the data f rom address<virtual address> into the register eax (and thus ignore the instruction fetch that must have happened prior). Totranslate this virtual address that the process generated, we have to ﬁrst split it into two components: the virtual page number (VPN) , and theoffset within the page. For this example, because the virtual addres s space of the process is 64 bytes, we need 6 bits total for our virtual address (26= 64 ). Thus, our virtual address can be conceptualized as follows: Va5 Va4 Va3 Va2 Va1 Va0 In this diagram, Va5 is the highest-order bit of the virtual add ress, and Va0 the lowest-order bit. Because we know the page size (16 bytes ), we can further divide the virtual address as follows: Va5 Va4 Va3 Va2 Va1 Va0VPN offset The page size is 16 bytes in a 64-byte address space; thus we ne ed to be able to select 4 pages, and the top 2 bits of the address do just that. Thus, we have a 2-bit virtual page number (VPN). The remainin g bits tell us which byte of the page we are interested in, 4 bits in this cas e; we call this the offset.",3149
18. Introduction to Paging,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 PAGING : INTRODUCTION When a process generates a virtual address, the OS and hardwar e must combine to translate it into a meaningful physical addre ss. For ex- ample, let us assume the load above was to virtual address 21: movl 21,  percenteax Turning “21” into binary form, we get “010101”, and thus we can ex- amine this virtual address and see how it breaks down into a virt ual page number (VPN) and offset: 0 1 0 1 0 1VPN offset Thus, the virtual address “21” is on the 5th (“0101”th) byte of v irtual page “01” (or 1). With our virtual page number, we can now index our page table and ﬁnd which physical frame virtual page 1 reside s within. In the page table above the physical frame number (PFN ) (also sometimes called the physical page number orPPN ) is 7 (binary 111). Thus, we can translate this virtual address by replacing the VPN with the PFN and then issue the load to physical memory (Figure 18.3). Note the offset stays the same (i.e., it is not translated), beca use the offset just tells us which byte within the page we want. Our ﬁnal physical address is 1110101 (117 in decimal), and is exactly where we w ant our load to fetch data from (Figure 18.2, page 2). With this basic overview in mind, we can now ask (and hopefully, answer) a few basic questions you may have about paging. For examp le, where are these page tables stored? What are the typical conten ts of the page table, and how big are the tables? Does paging make the syst em (too) slow? These and other beguiling questions are answered, at l east in part, in the text below. Read on. 0 1 0 1 0 1VPN offset 1 1 1 0 1 0 1Address Translation PFN offsetVirtual Address Physical Address Figure 18.3: The Address Translation Process OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 5 1281129680644832160 page frame 7page frame 6page frame 5page frame 4page frame 3page frame 2page frame 1page frame 0 of physical memory (unused) page 3 of AS page 0 of AS (unused) page 2 of AS (unused) page 1 of ASpage table: 3 7 5 2 Figure 18.4: Example: Page Table in Kernel Physical Memory 18.2 Where Are Page Tables Stored? Page tables can get terribly large, much bigger than the smal l segment table or base/bounds pair we have discussed previously. For exam ple, imagine a typical 32-bit address space, with 4KB pages. This virtual ad- dress splits into a 20-bit VPN and 12-bit offset (recall that 1 0 bits would be needed for a 1KB page size, and just add two more to get to 4KB). A 20-bit VPN implies that there are 220translations that the OS would have to manage for each process (that’s roughly a million); assumi ng we need 4 bytes per page table entry (PTE) to hold the physical translation plus any other useful stuff, we get an immense 4MB of memory neede d for each page table. That is pretty large. Now imagine there are 100 processes running: this means the OS would need 400MB of memory just for all those address translations. Even in the modern era, w here machines have gigabytes of memory, it seems a little crazy to us e a large chunk of it just for translations, no?",3139
18. Introduction to Paging,"And we won’t even think about how big such a page table would be for a 64-bit address space; that wou ld be too gruesome and perhaps scare you off entirely. Because page tables are so big, we don’t keep any special on-chip hard- ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in memory somewhere. Let’s assume for now that the page tables live in physical memory t hat the OS manages; later we’ll see that much of OS memory itself can b e vir- tualized, and thus page tables can be stored in OS virtual memor y (and even swapped to disk), but that is too confusing right now, so we’l l ig- nore it. In Figure 18.4 is a picture of a page table in OS memory; se e the tiny set of translations in there? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 PAGING : INTRODUCTION 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 PFN G PAT D A PCD PWT U/S R/W P Figure 18.5: An x86 Page Table Entry (PTE) 18.3 What’s Actually In The Page Table? Let’s talk a little about page table organization. The page table is just a data structure that is used to map virtual addresses (or real ly, virtual page numbers) to physical addresses (physical frame number s). Thus, any data structure could work. The simplest form is called a linear page table , which is just an array. The OS indexes the array by the virtual page number (VPN), and looks up the page-table entry (PTE) at that in dex in order to ﬁnd the desired physical frame number (PFN). For now, we will assume this simple linear structure; in later chapters, we w ill make use of more advanced data structures to help solve some problems with pa ging. As for the contents of each PTE, we have a number of different bits in there worth understanding at some level. A valid bit is common to indicate whether the particular translation is valid; for exa mple, when a program starts running, it will have code and heap at one end of it s address space, and the stack at the other. All the unused space in-between will be marked invalid , and if the process tries to access such memory, it will generate a trap to the OS which will likely terminate the process. Thus, the valid bit is crucial for supporting a sparse address s pace; by simply marking all the unused pages in the address space inva lid, we remove the need to allocate physical frames for those pages and th us save a great deal of memory. We also might have protection bits , indicating whether the page could be read from, written to, or executed from. Again, accessing a pag e in a way not allowed by these bits will generate a trap to the OS. There are a couple of other bits that are important but we won’t talk about much for now. A present bit indicates whether this page is in phys- ical memory or on disk (i.e., it has been swapped out ). We will under- stand this machinery further when we study how to swap parts of the address space to disk to support address spaces that are large r than phys- ical memory; swapping allows the OS to free up physical memory by moving rarely-used pages to disk.",3135
18. Introduction to Paging,"A dirty bit is also common, indicating whether the page has been modiﬁed since it was brought into memor y. Areference bit (a.k.a. accessed bit ) is sometimes used to track whether a page has been accessed, and is useful in determining which p ages are popular and thus should be kept in memory; such knowledge is criti cal during page replacement , a topic we will study in great detail in subse- quent chapters. Figure 18.5 shows an example page table entry from the x86 archi tec- ture [I09]. It contains a present bit (P); a read/write bit (R/ W) which determines if writes are allowed to this page; a user/supervi sor bit (U/S) OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 7 which determines if user-mode processes can access the page; a few bits (PWT, PCD, PAT, and G) that determine how hardware caching work s for these pages; an accessed bit (A) and a dirty bit (D); and ﬁnall y, the page frame number (PFN) itself. Read the Intel Architecture Manuals [I09] for more details on x8 6 pag- ing support. Be forewarned, however; reading manuals such as th ese, while quite informative (and certainly necessary for those who write code to use such page tables in the OS), can be challenging at ﬁrst. A little pa- tience, and a lot of desire, is required. 18.4 Paging: Also Too Slow With page tables in memory, we already know that they might be too big. As it turns out, they can slow things down too. For example, take our simple instruction: movl 21,  percenteax Again, let’s just examine the explicit reference to address 2 1 and not worry about the instruction fetch. In this example, we’ll assume the hard- ware performs the translation for us. To fetch the desired data, the system must ﬁrst translate the virtual address (21) into the correct physical ad- dress (117). Thus, before fetching the data from address 117, t he system must ﬁrst fetch the proper page table entry from the process’s pag e table, perform the translation, and then load the data from physical mem ory. To do so, the hardware must know where the page table is for the currently-running process. Let’s assume for now that a single page-table base register contains the physical address of the starting location of the page table. To ﬁnd the location of the desired PTE, the hardware w ill thus perform the following functions: VPN = (VirtualAddress & VPN_MASK) >> SHIFT PTEAddr = PageTableBaseRegister + (VPN *sizeof(PTE)) In our example, VPNMASK would be set to 0x30 (hex 30, or binary 110000) which picks out the VPN bits from the full virtual addre ss;SHIFT is set to 4 (the number of bits in the offset), such that we move the VPN bits down to form the correct integer virtual page number. For exa m- ple, with virtual address 21 (010101), and masking turns thi s value into 010000; the shift turns it into 01, or virtual page 1, as desire d. We then use this value as an index into the array of PTEs pointed to by the pag e table base register. Once this physical address is known, the hardware can fetch th e PTE from memory, extract the PFN, and concatenate it with the offset f rom the virtual address to form the desired physical address. Speciﬁc ally, you can think of the PFN being left-shifted by SHIFT , and then bitwise OR’d with the offset to form the ﬁnal address as follows: offset = VirtualAddress & OFFSET_MASK PhysAddr = (PFN << SHIFT) | offset c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 PAGING : INTRODUCTION 1// Extract the VPN from the virtual address 2VPN = (VirtualAddress & VPN_MASK) >> SHIFT 3 4// Form the address of the page-table entry (PTE) 5PTEAddr = PTBR + (VPN *sizeof(PTE)) 6 7// Fetch the PTE 8PTE = AccessMemory(PTEAddr) 9 10// Check if process can access the page 11if (PTE.Valid == False) 12 RaiseException(SEGMENTATION_FAULT) 13else if (CanAccess(PTE.ProtectBits) == False) 14 RaiseException(PROTECTION_FAULT) 15else 16 // Access is OK: form physical address and fetch it 17 offset = VirtualAddress & OFFSET_MASK 18 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset 19 Register = AccessMemory(PhysAddr) Figure 18.6: Accessing Memory With Paging Finally, the hardware can fetch the desired data from memory an d put it into register eax.",4195
18. Introduction to Paging,"The program has now succeeded at loading a value from memory. To summarize, we now describe the initial protocol for what happen s on each memory reference. Figure 18.6 shows the basic approach. F or every memory reference (whether an instruction fetch or an expl icit load or store), paging requires us to perform one extra memory referenc e in order to ﬁrst fetch the translation from the page table. That is a lot of work. Extra memory references are costly, and in this case will l ikely slow down the process by a factor of two or more. And now you can hopefully see that there are tworeal problems that we must solve. Without careful design of both hardware and softwar e, page tables will cause the system to run too slowly, as well as ta ke up too much memory. While seemingly a great solution for our memory virtualization needs, these two crucial problems must ﬁrst be overcome. 18.5 A Memory Trace Before closing, we now trace through a simple memory access exam- ple to demonstrate all of the resulting memory accesses that occu r when using paging. The code snippet (in C, in a ﬁle called array.c ) that we are interested in is as follows: int array[1000]; ... for (i = 0; i < 1000; i++) array[i] = 0; We compile array.c and run it with the following commands: OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 9 ASIDE : DATA STRUCTURE — T HEPAGE TABLE One of the most important data structures in the memory managemen t subsystem of a modern OS is the page table . In general, a page table stores virtual-to-physical address translations , thus letting the system know where each page of an address space actually resides in phy sical memory. Because each address space requires such translation s, in gen- eral there is one page table per process in the system. The exact structure of the page table is either determined by the hardware (older sy stems) or can be more ﬂexibly managed by the OS (modern systems). prompt> gcc -o array array.c -Wall -O prompt> ./array Of course, to truly understand what memory accesses this code sn ip- pet (which simply initializes an array) will make, we’ll have to know (or assume) a few more things. First, we’ll have to disassemble the result- ing binary (using objdump on Linux, or otool on a Mac) to see what assembly instructions are used to initialize the array in a loop . Here is the resulting assembly code: 1024 movl $0x0,( percentedi, percenteax,4) 1028 incl  percenteax 1032 cmpl $0x03e8, percenteax 1036 jne 0x1024 The code, if you know a little x86, is actually quite easy to understand2. The ﬁrst instruction moves the value zero (shown as $0x0 ) into the vir- tual memory address of the location of the array; this address is com puted by taking the contents of  percentedi and adding  percenteax multiplied by four to it. Thus, percentedi holds the base address of the array, whereas  percenteax holds the array index ( i); we multiply by four because the array is an array of inte- gers, each of size four bytes. The second instruction increments the array index held in  percenteax , and the third instruction compares the contents of that register to t he hex value0x03e8 , or decimal 1000. If the comparison shows that two val- ues are not yet equal (which is what the jne instruction tests), the fourth instruction jumps back to the top of the loop.",3343
18. Introduction to Paging,"To understand which memory accesses this instruction sequenc e makes (at both the virtual and physical levels), we’ll have to assume something about where in virtual memory the code snippet and array are found , as well as the contents and location of the page table. For this example, we assume a virtual address space of size 64KB (un- realistically small). We also assume a page size of 1KB. 2We are cheating a little bit here, assuming each instruction is four byt es in size for sim- plicity; in actuality, x86 instructions are variable-sized. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 PAGING : INTRODUCTION 0 10 20 30 40 50102410741124 Memory AccessCode (VA)400004005040100Array (VA)10241074112411741224 Page Table (PA) 409641464196 Code (PA)723272827332 Array (PA)mov inc cmp jnemovPageTable[1]PageTable[39] Figure 18.7: A Virtual (And Physical) Memory Trace All we need to know now are the contents of the page table, and its location in physical memory. Let’s assume we have a linear (array -based) page table and that it is located at physical address 1KB (1024 ). As for its contents, there are just a few virtual pages we need to worry about having mapped for this example. First, there is the virtu al page the code lives on. Because the page size is 1KB, virtual address 102 4 resides on the second page of the virtual address space (VPN=1, as VPN=0 i s the ﬁrst page). Let’s assume this virtual page maps to physica l frame 4 (VPN 1→PFN 4). Next, there is the array itself. Its size is 4000 bytes (1000 i ntegers), and we assume that it resides at virtual addresses 40000 throu gh 44000 (not including the last byte). The virtual pages for this decim al range are VPN=39 ... VPN=42. Thus, we need mappings for these pages. Let ’s as- sume these virtual-to-physical mappings for the example: (VPN 39 →PFN 7), (VPN 40 →PFN 8), (VPN 41 →PFN 9), (VPN 42 →PFN 10). We are now ready to trace the memory references of the program. When it runs, each instruction fetch will generate two memory r eferences: one to the page table to ﬁnd the physical frame that the instruc tion resides within, and one to the instruction itself to fetch it to the CPU f or process- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 11 ing. In addition, there is one explicit memory reference in the f orm of themov instruction; this adds another page table access ﬁrst (to tran slate the array virtual address to the correct physical one) and then the array access itself. The entire process, for the ﬁrst ﬁve loop iterations, is depicted i n Fig- ure 18.7 (page 10). The bottom most graph shows the instruction mem ory references on the y-axis in black (with virtual addresses on th e left, and the actual physical addresses on the right); the middle graph shows array accesses in dark gray (again with virtual on left and physical on right); ﬁ- nally, the topmost graph shows page table memory accesses in ligh t gray (just physical, as the page table in this example resides in p hysical mem- ory). The x-axis, for the entire trace, shows memory accesses acr oss the ﬁrst ﬁve iterations of the loop; there are 10 memory accesses per loop , which includes four instruction fetches, one explicit update of memory, and ﬁve page table accesses to translate those four fetches and one explicit update.",3327
18. Introduction to Paging,"See if you can make sense of the patterns that show up in this visu- alization. In particular, what will change as the loop continues to run beyond these ﬁrst ﬁve iterations? Which new memory locations will be accessed? Can you ﬁgure it out? This has just been the simplest of examples (only a few lines of C c ode), and yet you might already be able to sense the complexity of under stand- ing the actual memory behavior of real applications. Don’t worry: it deﬁ- nitely gets worse, because the mechanisms we are about to introd uce only complicate this already complex machinery. Sorry3. 18.6 Summary We have introduced the concept of paging as a solution to our chal- lenge of virtualizing memory. Paging has many advantages over p revi- ous approaches (such as segmentation). First, it does not lead to e xternal fragmentation, as paging (by design) divides memory into ﬁxed -sized units. Second, it is quite ﬂexible, enabling the sparse use of vi rtual ad- dress spaces. However, implementing paging support without care will lead to a slower machine (with many extra memory accesses to access the p age table) as well as memory waste (with memory ﬁlled with page tabl es in- stead of useful application data). We’ll thus have to think a lit tle harder to come up with a paging system that not only works, but works well. The next two chapters, fortunately, will show us how to do so. 3We’re not really sorry. But, we are sorry about not being sorry, i f that makes sense. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 PAGING : INTRODUCTION References [KE+62] “One-level Storage System” by T. Kilburn, D.B.G. Edwards , M.J. Lanigan, F.H. Sum- ner. IRE Trans. EC-11, 2, 1962. Reprinted in Bell and Newell, “Comp uter Structures: Readings and Examples”. McGraw-Hill, New York, 1971. The Atlas pioneered the idea of dividing memory into ﬁxed-sized pages and in many senses was an early form of the memory-managem ent ideas we see in modern computer systems. [I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” Intel, 2009. Available: http://www.intel.com/products/processor/manuals. In particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B: System Programmin g Guide Part 2”. [L78] “The Manchester Mark I and Atlas: A Historical Perspective” by S. H. L avington. Com- munications of the ACM, Volume 21:1, January 1978. This paper is a great retrospective of some of the history of the development of some important computer systems. As we someti mes forget in the US, many of these new ideas came from overseas. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : INTRODUCTION 13 Homework (Simulation) In this homework, you will use a simple program, which is known as paging-linear-translate.py , to see if you understand how simple virtual-to-physical address translation works with linear pa ge tables. See the README for details. Questions 1. Before doing any translations, let’s use the simulator to stud y how linear page tables change size given different parameters. Compute th e size of linear page tables as different parameters change. Some sugges ted inputs are below; by using the -v flag , you can see how many page-table entries are ﬁlled. First, to understand how linear page table size cha nges as the address space grows: paging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0 paging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0 Then, to understand how linear page table size changes as page size grows: paging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0 Before running any of these, try to think about the expected tren ds. How should page-table size change as the address space grows? As th e page size grows? Why shouldn’t we just use really big pages in general? 2. Now let’s do some translations. Start with some small examples, and change the number of pages that are allocated to the address space with the-u flag . For example: paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100 What happens as you increase the percentage of pages that are allocated in each address space? 3. Now let’s try some different random seeds, and some differen t (and some- times quite crazy) address-space parameters, for variety: paging-linear-translate.py -P 8 -a 32 -p 1024 -v -s 1 paging-linear-translate.py -P 8k -a 32k -p 1m -v -s 2 paging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3 Which of these parameter combinations are unrealistic? Why? 4. Use the program to try out some other problems. Can you ﬁnd the li mits of where the program doesn’t work anymore? For example, what happe ns if the address-space size is bigger than physical memory? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",5095
19. Translation Lookaside Buffers,"19 Paging: Faster Translations (TLBs) Using paging as the core mechanism to support virtual memory can lead to high performance overheads. By chopping the address space in to small, ﬁxed-sized units (i.e., pages), paging requires a large amou nt of mapping information. Because that mapping information is generally stor ed in physical memory, paging logically requires an extra memory looku p for each virtual address generated by the program. Going to memory f or translation information before every instruction fetch or explic it load or store is prohibitively slow. And thus our problem: THECRUX: HOWTOSPEED UPADDRESS TRANSLATION How can we speed up address translation, and generally avoid the extra memory reference that paging seems to require? What har dware support is required? What OS involvement is needed? When we want to make things fast, the OS usually needs some help . And help often comes from the OS’s old friend: the hardware. To speed address translation, we are going to add what is called (for hist orical rea- sons [CP78]) a translation-lookaside buffer , orTLB [CG68, C95]. A TLB is part of the chip’s memory-management unit (MMU ), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache . Upon each virtual memory reference, the hardware ﬁrst checks the TLB to see if th e desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Be- cause of their tremendous performance impact, TLBs in a real sen se make virtual memory possible [C95]. 1 2 P AGING : FASTER TRANSLATIONS (TLB S) 1VPN = (VirtualAddress & VPN_MASK) >> SHIFT 2(Success, TlbEntry) = TLB_Lookup(VPN) 3if (Success == True) // TLB Hit 4if (CanAccess(TlbEntry.ProtectBits) == True) 5 Offset = VirtualAddress & OFFSET_MASK 6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 7 Register = AccessMemory(PhysAddr) 8else 9 RaiseException(PROTECTION_FAULT) 10else // TLB Miss 11PTEAddr = PTBR + (VPN *sizeof(PTE)) 12PTE = AccessMemory(PTEAddr) 13if (PTE.Valid == False) 14 RaiseException(SEGMENTATION_FAULT) 15else if (CanAccess(PTE.ProtectBits) == False) 16 RaiseException(PROTECTION_FAULT) 17else 18 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 19 RetryInstruction() Figure 19.1: TLB Control Flow Algorithm 19.1 TLB Basic Algorithm Figure 19.1 shows a rough sketch of how hardware might handle a virtual address translation, assuming a simple linear page table (i.e., the page table is an array) and a hardware-managed TLB (i.e., the hardware handles much of the responsibility of page table accesses; we’ll explain more about this below). The algorithm the hardware follows works like this: ﬁrst, extrac t the virtual page number (VPN) from the virtual address (Line 1 in F igure 19.1), and check if the TLB holds the translation for this VPN (Line 2). I f it does, we have a TLB hit , which means the TLB holds the translation. Success. We can now extract the page frame number (PFN) from the relevant TLB entry, concatenate that onto the offset from the original virtual address, and form the desired physical address (PA), and access memory ( Lines 5–7), assuming protection checks do not fail (Line 4).",3265
19. Translation Lookaside Buffers,"If the CPU does not ﬁnd the translation in the TLB (a TLB miss ), we have some more work to do. In this example, the hardware accesses t he page table to ﬁnd the translation (Lines 11–12), and, assumin g that the virtual memory reference generated by the process is valid and accessi- ble (Lines 13, 15), updates the TLB with the translation (Line 18). These set of actions are costly, primarily because of the extra memory re ference needed to access the page table (Line 12). Finally, once the TL B is up- dated, the hardware retries the instruction; this time, the t ranslation is found in the TLB, and the memory reference is processed quickly. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 3 The TLB, like all caches, is built on the premise that in the comm on case, translations are found in the cache (i.e., are hits). If s o, little over- head is added, as the TLB is found near the processing core and is d e- signed to be quite fast. When a miss occurs, the high cost of pagin g is incurred; the page table must be accessed to ﬁnd the translat ion, and an extra memory reference (or more, with more complex page tables) re sults. If this happens often, the program will likely run noticeably mor e slowly; memory accesses, relative to most CPU instructions, are quite c ostly, and TLB misses lead to more memory accesses. Thus, it is our hope to avoi d TLB misses as much as we can. 19.2 Example: Accessing An Array To make clear the operation of a TLB, let’s examine a simple virtua l address trace and see how a TLB can improve its performance. In th is example, let’s assume we have an array of 10 4-byte integers in m emory, starting at virtual address 100. Assume further that we have a small 8-bit virtual address space, with 16-byte pages; thus, a virtual a ddress breaks down into a 4-bit VPN (there are 16 virtual pages) and a 4-bit off set (there are 16 bytes on each of those pages). Figure 19.2 (page 4) shows the array laid out on the 16 16-byte pag es of the system. As you can see, the array’s ﬁrst entry ( a[0] ) begins on (VPN=06, offset=04); only three 4-byte integers ﬁt onto that pa ge. The array continues onto the next page (VPN=07), where the next four entries (a[3] ...a[6] ) are found. Finally, the last three entries of the 10-entry array (a[7] ...a[9] ) are located on the next page of the address space (VPN=08). Now let’s consider a simple loop that accesses each array element, something that would look like this in C: int sum = 0; for (i = 0; i < 10; i++) { sum += a[i]; } For the sake of simplicity, we will pretend that the only memory ac - cesses the loop generates are to the array (ignoring the variabl esiand sum, as well as the instructions themselves). When the ﬁrst array element (a[0] ) is accessed, the CPU will see a load to virtual address 100. Th e hardware extracts the VPN from this (VPN=06), and uses that to check the TLB for a valid translation. Assuming this is the ﬁrst time t he pro- gram accesses the array, the result will be a TLB miss. The next access is to a[1] , and there is some good news here: a TLB hit.",3117
19. Translation Lookaside Buffers,"Because the second element of the array is packed next to th e ﬁrst, it lives on the same page; because we’ve already accessed this pag e when accessing the ﬁrst element of the array, the translation is alr eady loaded c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 4 P AGING : FASTER TRANSLATIONS (TLB S) VPN = 15VPN = 14VPN = 13VPN = 12VPN = 11VPN = 10VPN = 09VPN = 08VPN = 07VPN = 06VPN = 05VPN = 04VPN = 03VPN = 02VPN = 01VPN = 0000 04 08 12 16Offset a[0] a[1] a[2] a[3] a[4] a[5] a[6] a[7] a[8] a[9] Figure 19.2: Example: An Array In A Tiny Address Space into the TLB. And hence the reason for our success. Access to a[2] en- counters similar success (another hit), because it too lives on t he same page asa[0] anda[1] . Unfortunately, when the program accesses a[3] , we encounter an- other TLB miss. However, once again, the next entries ( a[4] ...a[6] ) will hit in the TLB, as they all reside on the same page in memory. Finally, access to a[7] causes one last TLB miss. The hardware once again consults the page table to ﬁgure out the location of this virt ual page in physical memory, and updates the TLB accordingly. The ﬁnal t wo ac- cesses (a[8] anda[9] ) receive the beneﬁts of this TLB update; when the hardware looks in the TLB for their translations, two more hits res ult. Let us summarize TLB activity during our ten accesses to the ar ray: miss , hit, hit, miss , hit, hit, hit, miss , hit, hit. Thus, our TLB hit rate , which is the number of hits divided by the total number of accesse s, is 70 percent. Although this is not too high (indeed, we desire hit rates th at ap- proach 100 percent), it is non-zero, which may be a surprise. Even though this is the ﬁrst time the program accesses the array, the TLB improve s per- formance due to spatial locality . The elements of the array are packed tightly into pages (i.e., they are close to one another in space ), and thus only the ﬁrst access to an element on a page yields a TLB miss. Also note the role that page size plays in this example. If the pa ge size OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 5 TIP: USECACHING WHEN POSSIBLE Caching is one of the most fundamental performance techniques in com- puter systems, one that is used again and again to make the “comm on- case fast” [HP06]. The idea behind hardware caches is to take advantage oflocality in instruction and data references. There are usually two typ es of locality: temporal locality and spatial locality . With temporal locality, the idea is that an instruction or data item that has been recent ly accessed will likely be re-accessed soon in the future. Think of loop variab les or in- structions in a loop; they are accessed repeatedly over time. Wit h spatial locality, the idea is that if a program accesses memory at addres sx, it will likely soon access memory near x. Imagine here streaming through an array of some kind, accessing one element and then the next. Of cou rse, these properties depend on the exact nature of the program, and th us are not hard-and-fast laws but more like rules of thumb.",3107
19. Translation Lookaside Buffers,"Hardware caches, whether for instructions, data, or address tr anslations (as in our TLB) take advantage of locality by keeping copies of memor y in small, fast on-chip memory. Instead of having to go to a (slow) mem ory to satisfy a request, the processor can ﬁrst check if a nearby cop y exists in a cache; if it does, the processor can access it quickly (i.e., in a few CPU cycles) and avoid spending the costly time it takes to acces s memory (many nanoseconds). You might be wondering: if caches (like the TLB) are so great, wh y don’t we just make bigger caches and keep all of our data in them? Unfor- tunately, this is where we run into more fundamental laws like those of physics. If you want a fast cache, it has to be small, as issues l ike the speed-of-light and other physical constraints become relevant . Any large cache by deﬁnition is slow, and thus defeats the purpose. Thus, w e are stuck with small, fast caches; the question that remains is how to best use them to improve performance. had simply been twice as big (32 bytes, not 16), the array acces s would suffer even fewer misses. As typical page sizes are more like 4 KB, these types of dense, array-based accesses achieve excellent TLB p erformance, encountering only a single miss per page of accesses. One last point about TLB performance: if the program, soon after thi s loop completes, accesses the array again, we’d likely see an even bet- ter result, assuming that we have a big enough TLB to cache the n eeded translations: hit, hit, hit, hit, hit, hit, hit, hit, hit, hit . In this case, the TLB hit rate would be high because of temporal locality , i.e., the quick re-referencing of memory items in time . Like any cache, TLBs rely upon both spatial and temporal locality for success, which are program proper- ties. If the program of interest exhibits such locality (and man y programs do), the TLB hit rate will likely be high. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 6 P AGING : FASTER TRANSLATIONS (TLB S) 1VPN = (VirtualAddress & VPN_MASK) >> SHIFT 2(Success, TlbEntry) = TLB_Lookup(VPN) 3if (Success == True) // TLB Hit 4if (CanAccess(TlbEntry.ProtectBits) == True) 5 Offset = VirtualAddress & OFFSET_MASK 6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 7 Register = AccessMemory(PhysAddr) 8else 9 RaiseException(PROTECTION_FAULT) 10else // TLB Miss 11RaiseException(TLB_MISS) Figure 19.3: TLB Control Flow Algorithm (OS Handled) 19.3 Who Handles The TLB Miss? One question that we must answer: who handles a TLB miss? Two an - swers are possible: the hardware, or the software (OS). In the olde n days, the hardware had complex instruction sets (sometimes called CISC , for complex-instruction set computers) and the people who built the hard- ware didn’t much trust those sneaky OS people. Thus, the hardwar e would handle the TLB miss entirely. To do this, the hardware ha s to know exactly where the page tables are located in memory (via a page- table base register , used in Line 11 in Figure 19.1), as well as their exact format ; on a miss, the hardware would “walk” the page table, ﬁnd the cor- rect page-table entry and extract the desired translation, u pdate the TLB with the translation, and retry the instruction. An example of a n “older” architecture that has hardware-managed TLBs is the Intel x86 architec- ture, which uses a ﬁxed multi-level page table (see the next chapter for details); the current page table is pointed to by the CR3 regis ter [I09]. More modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9 [WG00], both RISC or reduced-instruction set computers) have what is known as a software-managed TLB . On a TLB miss, the hardware sim- ply raises an exception (line 11 in Figure 19.3), which pauses the current instruction stream, raises the privilege level to kernel mode , and jumps to a trap handler . As you might guess, this trap handler is code within the OS that is written with the express purpose of handling TLB m isses. When run, the code will lookup the translation in the page table, u se spe- cial “privileged” instructions to update the TLB, and return from the trap; at this point, the hardware retries the instruction (resultin g in a TLB hit).",4209
19. Translation Lookaside Buffers,"Let’s discuss a couple of important details. First, the return-f rom-trap instruction needs to be a little different than the return-fr om-trap we saw before when servicing a system call. In the latter case, the re turn-from- trap should resume execution at the instruction after the trap into the OS, just as a return from a procedure call returns to the instruction imme- diately following the call into the procedure. In the former case , when returning from a TLB miss-handling trap, the hardware must re sume ex- ecution at the instruction that caused the trap; this retry thus lets the in- OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 7 ASIDE : RISC VS. CISC In the 1980’s, a great battle took place in the computer architect ure com- munity. On one side was the CISC camp, which stood for Complex Instruction Set Computing ; on the other side was RISC , for Reduced Instruction Set Computing [PS81]. The RISC side was spear-headed by David Patterson at Berkeley and John Hennessy at Stanford (who ar e also co-authors of some famous books [HP06]), although later John Cocke was recognized with a Turing award for his earliest work on RISC [CM00] . CISC instruction sets tend to have a lot of instructions in them, an d each instruction is relatively powerful. For example, you might see a string copy, which takes two pointers and a length and copies bytes from s ource to destination. The idea behind CISC was that instructions shoul d be high-level primitives, to make the assembly language itsel f easier to use, and to make code more compact. RISC instruction sets are exactly the opposite. A key observation b ehind RISC is that instruction sets are really compiler targets, and a ll compil- ers really want are a few simple primitives that they can use t o gener- ate high-performance code. Thus, RISC proponents argued, let’s ri p out as much from the hardware as possible (especially the microcode) , and make what’s left simple, uniform, and fast. In the early days, RISC chips made a huge impact, as they were not iceably faster [BC91]; many papers were written; a few companies were formed (e.g., MIPS and Sun). However, as time progressed, CISC manufact urers such as Intel incorporated many RISC techniques into the core of th eir processors, for example by adding early pipeline stages that tr ansformed complex instructions into micro-instructions which could then b e pro- cessed in a RISC-like manner. These innovations, plus a growing n umber of transistors on each chip, allowed CISC to remain competitive. Th e end result is that the debate died down, and today both types of process ors can be made to run fast. struction run again, this time resulting in a TLB hit. Thus, de pending on how a trap or exception was caused, the hardware must save a diffe rent PC when trapping into the OS, in order to resume properly when the time to do so arrives. Second, when running the TLB miss-handling code, the OS needs to be extra careful not to cause an inﬁnite chain of TLB misses to occur . Many solutions exist; for example, you could keep TLB miss handlers in p hysi- cal memory (where they are unmapped and not subject to address trans- lation), or reserve some entries in the TLB for permanently-vali d transla- tions and use some of those permanent translation slots for the handl er code itself; these wired translations always hit in the TLB.",3412
19. Translation Lookaside Buffers,"The primary advantage of the software-managed approach is ﬂexibil- ity: the OS can use any data structure it wants to implement the pa ge c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 8 P AGING : FASTER TRANSLATIONS (TLB S) ASIDE : TLB V ALID BIT/negationslash=PAGE TABLE VALID BIT A common mistake is to confuse the valid bits found in a TLB with thos e found in a page table. In a page table, when a page-table entry ( PTE) is marked invalid, it means that the page has not been allocated by the process, and should not be accessed by a correctly-working program . The usual response when an invalid page is accessed is to trap to th e OS, which will respond by killing the process. A TLB valid bit, in contrast, simply refers to whether a TLB ent ry has a valid translation within it. When a system boots, for example, a c ommon initial state for each TLB entry is to be set to invalid, becaus e no address translations are yet cached there. Once virtual memory is enab led, and once programs start running and accessing their virtual addre ss spaces, the TLB is slowly populated, and thus valid entries soon ﬁll the TL B. The TLB valid bit is quite useful when performing a context swit ch too, as we’ll discuss further below. By setting all TLB entries to in valid, the system can ensure that the about-to-be-run process does not accid entally use a virtual-to-physical translation from a previous process. table, without necessitating hardware change. Another advan tage is sim- plicity , as seen in the TLB control ﬂow (line 11 in Figure 19.3, in contrast to lines 11–19 in Figure 19.1). The hardware doesn’t do much on a m iss: just raise an exception and let the OS TLB miss handler do the re st. 19.4 TLB Contents: What’s In There? Let’s look at the contents of the hardware TLB in more detail. A typic al TLB might have 32, 64, or 128 entries and be what is called fully associa- tive. Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to ﬁnd the desired translation. A TLB entry might look like this: VPN PFN other bits Note that both the VPN and PFN are present in each entry, as a tran s- lation could end up in any of these locations (in hardware terms, th e TLB is known as a fully-associative cache). The hardware searches the entries in parallel to see if there is a match. More interesting are the “other bits”. For example, the TLB common ly has a valid bit, which says whether the entry has a valid translation or not. Also common are protection bits, which determine how a page can be accessed (as in the page table). For example, code pages migh t be marked read and execute , whereas heap pages might be marked read and write . There may also be a few other ﬁelds, including an address-space identiﬁer , adirty bit , and so forth; see below for more information. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 9 19.5 TLB Issue: Context Switches With TLBs, some new issues arise when switching between proces ses (and hence address spaces). Speciﬁcally, the TLB contains vir tual-to-physical translations that are only valid for the currently running proce ss; these translations are not meaningful for other processes. As a result, when switching from one process to another, the hardware or OS (or both) mu st be careful to ensure that the about-to-be-run process does not acc identally use translations from some previously run process.",3504
19. Translation Lookaside Buffers,"To understand this situation better, let’s look at an example. Wh en one process (P1) is running, it assumes the TLB might be caching tr anslations that are valid for it, i.e., that come from P1’s page table. Assume , for this example, that the 10th virtual page of P1 is mapped to physical frame 100. In this example, assume another process (P2) exists, and the OS soon might decide to perform a context switch and run it. Assume here that the 10th virtual page of P2 is mapped to physical frame 170. If e ntries for both processes were in the TLB, the contents of the TLB would be: VPN PFN valid prot 10 100 1 rwx — — 0 — 10 170 1 rwx — — 0 — In the TLB above, we clearly have a problem: VPN 10 translates to either PFN 100 (P1) or PFN 170 (P2), but the hardware can’t disti nguish which entry is meant for which process. Thus, we need to do some mor e work in order for the TLB to correctly and efﬁciently support virtu aliza- tion across multiple processes. And thus, a crux: THECRUX: HOWTOMANAGE TLB C ONTENTS ONA C ONTEXT SWITCH When context-switching between processes, the translations i n the TLB for the last process are not meaningful to the about-to-be-run proc ess. What should the hardware or OS do in order to solve this problem? There are a number of possible solutions to this problem. One ap- proach is to simply ﬂush the TLB on context switches, thus emptying it before running the next process. On a software-based system, this can be accomplished with an explicit (and privileged) hardwa re instruc- tion; with a hardware-managed TLB, the ﬂush could be enacted wh en the page-table base register is changed (note the OS must change t he PTBR on a context switch anyhow). In either case, the ﬂush operation sim ply sets all valid bits to 0, essentially clearing the contents of t he TLB. By ﬂushing the TLB on each context switch, we now have a working solution, as a process will never accidentally encounter the wron g trans- c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 10 P AGING : FASTER TRANSLATIONS (TLB S) lations in the TLB. However, there is a cost: each time a process ru ns, it must incur TLB misses as it touches its data and code pages. If th e OS switches between processes frequently, this cost may be high. To reduce this overhead, some systems add hardware support to en - able sharing of the TLB across context switches. In particular, some hard- ware systems provide an address space identiﬁer (ASID ) ﬁeld in the TLB. You can think of the ASID as a process identiﬁer (PID), but usu- ally it has fewer bits (e.g., 8 bits for the ASID versus 32 bits for a PID). If we take our example TLB from above and add ASIDs, it is clear processes can readily share the TLB: only the ASID ﬁeld is needed to dif- ferentiate otherwise identical translations. Here is a depic tion of a TLB with the added ASID ﬁeld: VPN PFN valid prot ASID 10 100 1 rwx 1 — — 0 — — 10 170 1 rwx 2 — — 0 — — Thus, with address-space identiﬁers, the TLB can hold transl ations from different processes at the same time without any confusion.",3059
19. Translation Lookaside Buffers,"O f course, the hardware also needs to know which process is current ly run- ning in order to perform translations, and thus the OS must, on a con text switch, set some privileged register to the ASID of the current p rocess. As an aside, you may also have thought of another case where two entries of the TLB are remarkably similar. In this example, th ere are two entries for two different processes with two different VPNs th at point to thesame physical page: VPN PFN valid prot ASID 10 101 1 r-x 1 — — 0 — — 50 101 1 r-x 2 — — 0 — — This situation might arise, for example, when two processes share a page (a code page, for example). In the example above, Process 1 is shar- ing physical page 101 with Process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of i ts ad- dress space. Sharing of code pages (in binaries, or shared librar ies) is useful as it reduces the number of physical pages in use, thus r educing memory overheads. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 11 19.6 Issue: Replacement Policy As with any cache, and thus also with the TLB, one more issue that we must consider is cache replacement . Speciﬁcally, when we are installing a new entry in the TLB, we have to replace an old one, and thus the question: which one to replace? THECRUX: HOWTODESIGN TLB R EPLACEMENT POLICY Which TLB entry should be replaced when we add a new TLB entry? The goal, of course, being to minimize the miss rate (or increase hit rate ) and thus improve performance. We will study such policies in some detail when we tackle the prob lem of swapping pages to disk; here we’ll just highlight a few typic al policies. One common approach is to evict the least-recently-used orLRU entry. LRU tries to take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been u sed is a good candidate for eviction. Another typical approach is to use a random pol- icy, which evicts a TLB mapping at random. Such a policy is useful due to its simplicity and ability to avoid corner-case behaviors; f or example, a “reasonable” policy such as LRU behaves quite unreasonably wh en a program loops over n+ 1 pages with a TLB of size n; in this case, LRU misses upon every access, whereas random does much better. 19.7 A Real TLB Entry Finally, let’s brieﬂy look at a real TLB. This example is from the M IPS R4000 [H93], a modern system that uses software-managed TLBs; a slightly simpliﬁed MIPS TLB entry can be seen in Figure 19.4. The MIPS R4000 supports a 32-bit address space with 4KB pages. Thus, we would expect a 20-bit VPN and 12-bit offset in our typical virt ual ad- dress. However, as you can see in the TLB, there are only 19 bits for the VPN; as it turns out, user addresses will only come from half the ad dress space (the rest reserved for the kernel) and hence only 19 bits of VPN are needed. The VPN translates to up to a 24-bit physical fram e number (PFN), and hence can support systems with up to 64GB of (physica l) main memory ( 2244KB pages).",3108
19. Translation Lookaside Buffers,"There are a few other interesting bits in the MIPS TLB. We see a global bit (G), which is used for pages that are globally-shared among p rocesses. Thus, if the global bit is set, the ASID is ignored. We also see the 8-bit ASID , which the OS can use to distinguish between address spaces ( as 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 VPN G ASID PFN C D V Figure 19.4: A MIPS TLB Entry c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 12 P AGING : FASTER TRANSLATIONS (TLB S) TIP: RAM I SN’TALWAYS RAM (C ULLER ’SLAW) The term random-access memory , orRAM , implies that you can access any part of RAM just as quickly as another. While it is generally good to think of RAM in this way, because of hardware/OS features such a s the TLB, accessing a particular page of memory may be costly, partic ularly if that page isn’t currently mapped by your TLB. Thus, it is alwa ys good to remember the implementation tip: RAM isn’t always RAM . Some- times randomly accessing your address space, particularly if the number of pages accessed exceeds the TLB coverage, can lead to severe p erfor- mance penalties. Because one of our advisors, David Culler, used to al- ways point to the TLB as the source of many performance problems, we name this law in his honor: Culler’s Law . described above). One question for you: what should the OS do if the re are more than 256 ( 28) processes running at a time? Finally, we see 3 Coherence (C) bits, which determine how a page is cached by the hardware (a bit beyond the scope of these notes); a dirty bit which is marked when the page has been written to (we’ll see the use of this later); a valid bit which tells the hardware if there is a valid translation prese nt in the entry. There is also a page mask ﬁeld (not shown), which supports multiple page sizes; we’ll see later why having larger pages might be useful . Finally, some of the 64 bits are unused (shaded gray in the diagram). MIPS TLBs usually have 32 or 64 of these entries, most of which are used by user processes as they run. However, a few are reserved f or the OS. A wired register can be set by the OS to tell the hardware how many slots of the TLB to reserve for the OS; the OS uses these reserved ma p- pings for code and data that it wants to access during critical t imes, where a TLB miss would be problematic (e.g., in the TLB miss handler). Because the MIPS TLB is software managed, there needs to be ins truc- tions to update the TLB. The MIPS provides four such instructions :TLBP , which probes the TLB to see if a particular translation is in the re;TLBR , which reads the contents of a TLB entry into registers; TLBWI , which re- places a speciﬁc TLB entry; and TLBWR , which replaces a random TLB entry. The OS uses these instructions to manage the TLB’s conten ts. It is of course critical that these instructions are privileged ; imagine what a user process could do if it could modify the contents of the TLB (hint : just about anything, including take over the machine, run its own mal icious “OS”, or even make the Sun disappear).",3106
19. Translation Lookaside Buffers,"19.8 Summary We have seen how hardware can help us make address translation faster. By providing a small, dedicated on-chip TLB as an addre ss-translation cache, most memory references will hopefully be handled without having to access the page table in main memory. Thus, in the common case, OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 13 the performance of the program will be almost as if memory isn’t bein g virtualized at all, an excellent achievement for an operating system, and certainly essential to the use of paging in modern systems. However, TLBs do not make the world rosy for every program that exists. In particular, if the number of pages a program accesse s in a short period of time exceeds the number of pages that ﬁt into the TLB, th e pro- gram will generate a large number of TLB misses, and thus run qu ite a bit more slowly. We refer to this phenomenon as exceeding the TLB cov- erage , and it can be quite a problem for certain programs. One solution, as we’ll discuss in the next chapter, is to include support for la rger page sizes; by mapping key data structures into regions of the progra m’s ad- dress space that are mapped by larger pages, the effective cov erage of the TLB can be increased. Support for large pages is often exploited by pro- grams such as a database management system (aDBMS ), which have certain data structures that are both large and randomly-acce ssed. One other TLB issue worth mentioning: TLB access can easily be- come a bottleneck in the CPU pipeline, in particular with what i s called a physically-indexed cache . With such a cache, address translation has to take place before the cache is accessed, which can slow things down quite a bit. Because of this potential problem, people have looked into al l sorts of clever ways to access caches with virtual addresses, thus avoiding the expensive step of translation in the case of a cache hit. Such a virtually- indexed cache solves some performance problems, but introduces new issues into hardware design as well. See Wiggins’s ﬁne survey f or more details [W03]. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 14 P AGING : FASTER TRANSLATIONS (TLB S) References [BC91] “Performance from Architecture: Comparing a RISC and a CISC with S imilar Hard- ware Organization” by D. Bhandarkar and Douglas W. Clark. Communicat ions of the ACM, September 1991. A great and fair comparison between RISC and CISC. The bottom line: on similar hardware, RISC was about a factor of three better in performance. [CM00] “The evolution of RISC technology at IBM” by John Cocke, V . Markstei n. IBM Journal of Research and Development, 44:1/2. A summary of the ideas and work behind the IBM 801, which many consider the ﬁrst true RISC microprocessor. [C95] “The Core of the Black Canyon Computer Corporation” by John Coule ur. IEEE Annals of History of Computing, 17:4, 1995. In this fascinating historical note, Couleur talks about how he invented the TLB in 1964 while working for GE, and the fortuitous collaboration th at thus ensued with the Project MAC folks at MIT.",3118
19. Translation Lookaside Buffers,"[CG68] “Shared-access Data Processing System” by John F. Couleur, Edwa rd L. Glaser. Patent 3412382, November 1968. The patent that contains the idea for an associative memory to store address translations. The idea, according to Couleur, came in 1964. [CP78] “The architecture of the IBM System/370” by R.P . Case, A. Padegs . Communications of the ACM. 21:1, 73-96, January 1978. Perhaps the ﬁrst paper to use the term translation lookaside buffer . The name arises from the historical name for a cache, which was a lookaside buffer as called by those developing the Atlas system at the University of Manchester; a cache of ad dress translations thus became a translation lookaside buffer . Even though the term lookaside buffer fell out of favor, TLB seems to have stuck, for whatever reason. [H93] “MIPS R4000 Microprocessor User’s Manual”. by Joe Heinrich. Prentice-H all, June 1993. Available: http://cag.csail.mit.edu/raw/ . documents/R4400 Uman book Ed2.pdf A manual, one that is surprisingly readable. Or is it? [HP06] “Computer Architecture: A Quantitative Approach” by John Hennessy and David Pat- terson. Morgan-Kaufmann, 2006. A great book about computer architecture. We have a particular attachment to the classic ﬁrst edition. [I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” by Intel, 2009. Avail- able: http://www.intel.com/products/processor/manuals. In particular, pay attention to “Vol- ume 3A: System Programming Guide” Part 1 and “Volume 3B: System Programming Guide Part 2”. [PS81] “RISC-I: A Reduced Instruction Set VLSI Computer” by D.A. Pat terson and C.H. Se- quin. ISCA ’81, Minneapolis, May 1981. The paper that introduced the term RISC, and started the avalanche of research into simplifying computer chips for performance. [SB92] “CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking” by Rafael H. Saavedra-Barrera. EECS Department, Univ ersity of California, Berkeley. Technical Report No. UCB/CSD-92-684, February 1992.. A great dissertation about how to predict execution time of applications by breaking them down into constituen t pieces and knowing the cost of each piece. Probably the most interesting part that comes out of this work is the tool to measure details of the cache hierarchy (described in Chapter 5). Make sure to check ou t the wonderful diagrams therein. [W03] “A Survey on the Interaction Between Caching, Translation and Pro tection” by Adam Wiggins. University of New South Wales TR UNSW-CSE-TR-0321, August , 2003. An excellent survey of how TLBs interact with other parts of the CPU pipeline, namely hard ware caches. [WG00] “The SPARC Architecture Manual: Version 9” by David L. Weaver a nd Tom Germond. SPARC International, San Jose, California, September 2000. Avail able:www.sparc.org/ standards/SPARCV9.pdf .Another manual. I bet you were hoping for a more fun citation to end this chapter. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG PAGING : FASTER TRANSLATIONS (TLB S) 15 Homework (Measurement) In this homework, you are to measure the size and cost of accessing a TLB. The idea is based on work by Saavedra-Barrera [SB92], who de - veloped a simple but beautiful method to measure numerous aspec ts of cache hierarchies, all with a very simple user-level program .",3297
19. Translation Lookaside Buffers,"Read his work for more details. The basic idea is to access some number of pages within a large da ta structure (e.g., an array) and to time those accesses. For exam ple, let’s say the TLB size of a machine happens to be 4 (which would be very smal l, but useful for the purposes of this discussion). If you write a progr am that touches 4 or fewer pages, each access should be a TLB hit, and thus relatively fast. However, once you touch 5 pages or more, repeatedl y in a loop, each access will suddenly jump in cost, to that of a TLB miss. The basic code to loop through an array once should look like this: int jump = PAGESIZE / sizeof(int); for (i = 0; i < NUMPAGES *jump; i += jump) { a[i] += 1; } In this loop, one integer per page of the array ais updated, up to the number of pages speciﬁed by NUMPAGES . By timing such a loop repeat- edly (say, a few hundred million times in another loop around this on e, or however many loops are needed to run for a few seconds), you can time how long each access takes (on average). By looking for jumps in cost a s NUMPAGES increases, you can roughly determine how big the ﬁrst-level TLB is, determine whether a second-level TLB exists (and how bi g it is if it does), and in general get a good sense of how TLB hits and misses ca n affect performance. Figure 19.5 (page 16) shows the average time per access as the n umber of pages accessed in the loop is increased. As you can see in the gra ph, when just a few pages are accessed (8 or fewer), the average acc ess time is roughly 5 nanoseconds. When 16 or more pages are accessed, there is a sudden jump to about 20 nanoseconds per access. A ﬁnal jump in cos t occurs at around 1024 pages, at which point each access takes arou nd 70 nanoseconds. From this data, we can conclude that there is a two-le vel TLB hierarchy; the ﬁrst is quite small (probably holding betwe en 8 and 16 entries); the second is larger but slower (holding roughly 512 entries). The overall difference between hits in the ﬁrst-level TLB and misses is quite large, roughly a factor of fourteen. TLB performance matter s. Questions 1. For timing, you’ll need to use a timer (e.g., gettimeofday() ). How pre- cise is such a timer? How long does an operation have to take in or der for you to time it precisely? (this will help determine how many times , in a loop, you’ll have to repeat a page access in order to time it succe ssfully) c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 16 P AGING : FASTER TRANSLATIONS (TLB S) 1 4 16 64 256 1024020406080TLB Size Measurement Number Of PagesTime Per Access (ns) Figure 19.5: Discovering TLB Sizes and Miss Costs 2. Write the program, called tlb.c , that can roughly measure the cost of ac- cessing each page. Inputs to the program should be: the number of p ages to touch and the number of trials. 3. Now write a script in your favorite scripting language (csh, python, etc.) to run this program, while varying the number of pages accessed from 1 up to a few thousand, perhaps incrementing by a factor of two per iter ation. Run the script on different machines and gather some data. How many t rials are needed to get reliable measurements? 4. Next, graph the results, making a graph that looks similar to th e one above. Use a good tool like ploticus or evenzplot . Visualization usually makes the data much easier to digest; why do you think that is? 5. One thing to watch out for is compiler optimization. Compilers do all sorts of clever things, including removing loops which increment val ues that no other part of the program subsequently uses. How can you ensure th e com- piler does not remove the main loop above from your TLB size estima tor? 6. Another thing to watch out for is the fact that most systems toda y ship with multiple CPUs, and each CPU, of course, has its own TLB hierarchy. T o really get good measurements, you have to run your code on just one CP U, instead of letting the scheduler bounce it from one CPU to the ne xt. How can you do that? (hint: look up “pinning a thread” on Google for some clues) What will happen if you don’t do this, and the code moves f rom one CPU to the other? 7. Another issue that might arise relates to initialization. If you don’t initialize the array aabove before accessing it, the ﬁrst time you access it will be very expensive, due to initial access costs such as demand zeroin g. Will this affect your code and its timing? What can you do to counterbalanc e these potential costs? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",4523
20. Advanced Page Tables,"20 Paging: Smaller Tables We now tackle the second problem that paging introduces: page tab les are too big and thus consume too much memory. Let’s start out with a linear page table. As you might recall1, linear page tables get pretty big. Assume again a 32-bit address space ( 232bytes), with 4KB ( 212byte) pages and a 4-byte page-table entry. An address space thus ha s roughly one million virtual pages in it (232 212); multiply by the page-table entry size and you see that our page table is 4MB in size. Recall also: we usua lly have one page table for every process in the system. With a hundred active processes (not uncommon on a modern system), we will be allocating hundreds of megabytes of memory just for page tables. As a result, we are in search of some techniques to reduce this heavy burden. Th ere are a lot of them, so let’s get going. But not before our crux: CRUX: HOWTOMAKE PAGE TABLES SMALLER ? Simple array-based page tables (usually called linear page t ables) are too big, taking up far too much memory on typical systems. How can we make page tables smaller? What are the key ideas? What inefﬁc iencies arise as a result of these new data structures? 20.1 Simple Solution: Bigger Pages We could reduce the size of the page table in one simple way: use bigger pages. Take our 32-bit address space again, but this ti me assume 16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset . As- suming the same size for each PTE (4 bytes), we now have 218entries in our linear page table and thus a total size of 1MB per page table, a factor 1Or indeed, you might not; this paging thing is getting out of control, no? That said, always make sure you understand the problem you are solving before moving onto the solution; indeed, if you understand the problem, you can often derive the solut ion yourself. Here, the problem should be clear: simple linear (array-based) page tables are too big. 1 2 PAGING : SMALLER TABLES ASIDE : M ULTIPLE PAGE SIZES As an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64) now support multiple page sizes. Usually, a small (4KB or 8KB) pa ge size is used. However, if a “smart” application requests it, a s ingle large page (e.g., of size 4MB) can be used for a speciﬁc portion of the addr ess space, enabling such applications to place a frequently-use d (and large) data structure in such a space while consuming only a single TLB en- try. This type of large page usage is common in database manageme nt systems and other high-end commercial applications. The main r eason for multiple page sizes is not to save page table space, however; it is to reduce pressure on the TLB, enabling a program to access more of it s ad- dress space without suffering from too many TLB misses. However, as researchers have shown [N+02], using multiple page sizes mak es the OS virtual memory manager notably more complex, and thus large page s are sometimes most easily used simply by exporting a new interfa ce to applications to request large pages directly.",3027
20. Advanced Page Tables,"of four reduction in size of the page table (not surprisingly, the r eduction exactly mirrors the factor of four increase in page size). The major problem with this approach, however, is that big pages l ead to waste within each page, a problem known as internal fragmentation (as the waste is internal to the unit of allocation). Applications thus end up allocating pages but only using little bits and pieces of each , and mem- ory quickly ﬁlls up with these overly-large pages. Thus, most sy stems use relatively small page sizes in the common case: 4KB (as in x86) or 8KB (as in SPARCv9). Our problem will not be solved so simply, alas. 20.2 Hybrid Approach: Paging and Segments Whenever you have two reasonable but different approaches to som e- thing in life, you should always examine the combination of the two to see if you can obtain the best of both worlds. We call such a combinati on a hybrid . For example, why eat just chocolate or plain peanut butter when you can instead combine the two in a lovely hybrid known as the Rees e’s Peanut Butter Cup [M28]? Years ago, the creators of Multics (in particular Jack Dennis) c hanced upon such an idea in the construction of the Multics virtual memory sys- tem [M07]. Speciﬁcally, Dennis had the idea of combining paging and segmentation in order to reduce the memory overhead of page tables . We can see why this might work by examining a typical linear pag e ta- ble in more detail. Assume we have an address space in which the used portions of the heap and stack are small. For the example, we use a t iny 16KB address space with 1KB pages (Figure 20.1); the page tab le for this address space is in Figure 20.2. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 3 code heap stackVirtual Address Space Physical Memory 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 Figure 20.1: A 16KB Address Space With 1KB Pages This example assumes the single code page (VPN 0) is mapped to physical page 10, the single heap page (VPN 4) to physical pag e 23, and the two stack pages at the other end of the address space (VPNs 14 and 15) are mapped to physical pages 28 and 4, respectively. As you can see from the picture, most of the page table is unused, full of invalid entries. What a waste. And this is for a tiny 16KB address space. Imagine the page table of a 32-bit address space and all the potential waste d space in there. Actually, don’t imagine such a thing; it’s far too gruesome . PFN valid prot present dirty 10 1 r-x 1 0 - 0 — - - - 0 — - - - 0 — - - 23 1 rw- 1 1 - 0 — - - - 0 — - - - 0 — - - - 0 — - - - 0 — - - - 0 — - - - 0 — - - - 0 — - - - 0 — - - 28 1 rw- 1 1 4 1 rw- 1 1 Figure 20.2: A Page Table For 16KB Address Space c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 PAGING : SMALLER TABLES Thus, our hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logica l seg- ment? In this example, we might thus have three page tables, on e for the code, heap, and stack parts of the address space.",3144
20. Advanced Page Tables,"Now, remember with segmentation, we had a base register that told us where each segment lived in physical memory, and a bound orlimit register that told us the size of said segment. In our hybrid, we s till have those structures in the MMU; here, we use the base not to point to t he segment itself but rather to hold the physical address of the page table of that segment. The bounds register is used to indicate the end of the p age table (i.e., how many valid pages it has). Let’s do a simple example to clarify. Assume a 32-bit virtual a ddress space with 4KB pages, and an address space split into four segm ents. We’ll only use three segments for this example: one for code, one for heap, and one for stack. To determine which segment an address refers to, we’ll use the t op two bits of the address space. Let’s assume 00 is the unused segm ent, with 01 for code, 10 for the heap, and 11 for the stack. Thus, a virtu al address looks like this: 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0 Seg VPN Offset In the hardware, assume that there are thus three base/bounds pairs, one each for code, heap, and stack. When a process is running, the b ase register for each of these segments contains the physical addre ss of a lin- ear page table for that segment; thus, each process in the syste m now has three page tables associated with it. On a context switch, these regi sters must be changed to reﬂect the location of the page tables of the new ly- running process. On a TLB miss (assuming a hardware-managed TLB, i.e., where t he hardware is responsible for handling TLB misses), the hardwar e uses the segment bits ( SN) to determine which base and bounds pair to use. The hardware then takes the physical address therein and combine s it with the VPN as follows to form the address of the page table entry (PTE) : SN = (VirtualAddress & SEG_MASK) >> SN_SHIFT VPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT AddressOfPTE = Base[SN] + (VPN *sizeof(PTE)) This sequence should look familiar; it is virtually identical t o what we saw before with linear page tables. The only difference, of cours e, is the use of one of three segment base registers instead of the single pa ge table base register. The critical difference in our hybrid scheme is the presence of a bounds register per segment; each bounds register holds the value of th e maxi- mum valid page in the segment. For example, if the code segment i s using its ﬁrst three pages (0, 1, and 2), the code segment page t able will only have three entries allocated to it and the bounds register w ill be set OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 5 TIP: USEHYBRIDS When you have two good and seemingly opposing ideas, you should always see if you can combine them into a hybrid that manages to achieve the best of both worlds. Hybrid corn species, for example, are known to be more robust than any naturally-occurring species. Of course, not all hybrids are a good idea; see the Zeedonk (or Zonkey), which is a cross of a Zebra and a Donkey.",3077
20. Advanced Page Tables,"If you don’t believe such a creature exists, l ook it up, and prepare to be amazed. to 3; memory accesses beyond the end of the segment will generate an ex- ception and likely lead to the termination of the process. In this manner, our hybrid approach realizes a signiﬁcant memory savings compar ed to the linear page table; unallocated pages between the stack an d the heap no longer take up space in a page table (just to mark them as not va lid). However, as you might notice, this approach is not without problems. First, it still requires us to use segmentation; as we discuss ed before, seg- mentation is not quite as ﬂexible as we would like, as it assumes a certain usage pattern of the address space; if we have a large but spars ely-used heap, for example, we can still end up with a lot of page table wast e. Second, this hybrid causes external fragmentation to arise aga in. While most of memory is managed in page-sized units, page tables now can be of arbitrary size (in multiples of PTEs). Thus, ﬁnding free spa ce for them in memory is more complicated. For these reasons, people continued t o look for better ways to implement smaller page tables. 20.3 Multi-level Page Tables A different approach doesn’t rely on segmentation but attacks the same problem: how to get rid of all those invalid regions in the page tabl e in- stead of keeping them all in memory? We call this approach a multi-level page table , as it turns the linear page table into something like a tree. T his approach is so effective that many modern systems employ it (e.g ., x86 [BOH10]). We now describe this approach in detail. The basic idea behind a multi-level page table is simple. Fir st, chop up the page table into page-sized units; then, if an entire page of page-table entries (PTEs) is invalid, don’t allocate that page of the page ta ble at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory . The page directory thus either can be used to tell you where a page of the pa ge table is, or that the entire page of the page table contains no val id pages. Figure 20.3 shows an example. On the left of the ﬁgure is the clas sic linear page table; even though most of the middle regions of the add ress space are not valid, we still require page-table space allocat ed for those regions (i.e., the middle two pages of the page table). On the ri ght is a multi-level page table. The page directory marks just two pag es of the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 PAGING : SMALLER TABLESvalidprotPFN 1 rx 12 1 rx 13 0 - - 1 rw 100 0 - - 0 - - 0 - - 0 - - 0 - - 0 - - 0 - - 0 - - 0 - - 0 - - 1 rw 86 1 rw 15Linear Page Table PTBR 201 PFN 201 PFN 202 PFN 203 PFN 204 validprotPFN 1 rx 12 1 rx 13 0 - - 1 rw 100 0 - - 0 - - 1 rw 86 1 rw 15[Page 1 of PT: Not Allocated] [Page 2 of PT: Not Allocated] PFN 201 PFN 204Multi-level Page Table PDBR 200 valid PFN 1 201 0 - 0 - 1 204PFN 200 The Page Directory Figure 20.3: Linear (Left) And Multi-Level (Right) Page Tables page table as valid (the ﬁrst and last); thus, just those two pa ges of the page table reside in memory. And thus you can see one way to visual ize what a multi-level table is doing: it just makes parts of the lin ear page table disappear (freeing those frames for other uses), and trac ks which pages of the page table are allocated with the page directory. The page directory, in a simple two-level table, contains one ent ry per page of the page table. It consists of a number of page directory entries (PDE ). A PDE (minimally) has a valid bit and a page frame number (PFN), similar to a PTE. However, as hinted at above, the meanin g of this valid bit is slightly different: if the PDE is valid, it m eans that at least one of the pages of the page table that the entry points to (via the P FN) is valid, i.e., in at least one PTE on that page pointed to by this P DE, the valid bit in that PTE is set to one. If the PDE is not valid (i.e., e qual to zero), the rest of the PDE is not deﬁned.",4067
20. Advanced Page Tables,"Multi-level page tables have some obvious advantages over approa ches we’ve seen thus far. First, and perhaps most obviously, the multi -level ta- ble only allocates page-table space in proportion to the amount of ad dress space you are using; thus it is generally compact and supports sp arse ad- dress spaces. Second, if carefully constructed, each portion of the page table ﬁt s neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a p age table. Contrast this to a simple (non-paged) linear page table2, which is just an array of PTEs indexed by VPN; with such a structure, t he en- tire linear page table must reside contiguously in physical me mory. For a large page table (say 4MB), ﬁnding such a large chunk of unuse d con- tiguous free physical memory can be quite a challenge. With a mu lti-level 2We are making some assumptions here, i.e., that all page tables re side in their entirety in physical memory (i.e., they are not swapped to disk); we’ll soon rel ax this assumption. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 7 TIP: UNDERSTAND TIME-SPACE TRADE -OFFS When building a data structure, one should always consider time-space trade-offs in its construction. Usually, if you wish to make access to a par- ticular data structure faster, you will have to pay a space-us age penalty for the structure. structure, we add a level of indirection through use of the page directory, which points to pieces of the page table; that indirection allows us to place page-table pages wherever we would like in physical memory. It should be noted that there is a cost to multi-level tables; on a T LB miss, two loads from memory will be required to get the right tran slation information from the page table (one for the page directory, and one f or the PTE itself), in contrast to just one load with a linear page ta ble. Thus, the multi-level table is a small example of a time-space trade-off . We wanted smaller tables (and got them), but not for free; although i n the common case (TLB hit), performance is obviously identical, a TLB m iss suffers from a higher cost with this smaller table. Another obvious negative is complexity . Whether it is the hardware or OS handling the page-table lookup (on a TLB miss), doing so is undou bt- edly more involved than a simple linear page-table lookup. Often we are willing to increase complexity in order to improve performance or reduce overheads; in the case of a multi-level table, we make page-tab le lookups more complicated in order to save valuable memory. A Detailed Multi-Level Example To understand the idea behind multi-level page tables bette r, let’s do an example. Imagine a small address space of size 16KB, with 64-b yte pages. Thus, we have a 14-bit virtual address space, with 8 bits for th e VPN and 6 bits for the offset. A linear page table would have 28(256) entries, even if only a small portion of the address space is in use. Figure 20.4 p resents one example of such an address space.",3070
20. Advanced Page Tables,"stackstack(free)(free)... all free ...(free)(free)heapheap(free)(free)codecode 1111 11111111 11101111 11011111 11000000 01110000 01100000 01010000 01000000 00110000 00100000 00010000 0000 ................ Figure 20.4: A 16KB Address Space With 64-byte Pages c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 PAGING : SMALLER TABLES TIP: BEWARY OFCOMPLEXITY System designers should be wary of adding complexity into their s ys- tem. What a good systems builder does is implement the least compl ex system that achieves the task at hand. For example, if disk spa ce is abun- dant, you shouldn’t design a ﬁle system that works hard to use as fe w bytes as possible; similarly, if processors are fast, it is bett er to write a clean and understandable module within the OS than perhaps th e most CPU-optimized, hand-assembled code for the task at hand. Be war y of needless complexity, in prematurely-optimized code or other form s; such approaches make systems harder to understand, maintain, and debug. As Antoine de Saint-Exupery famously wrote: “Perfection is ﬁnall y at- tained not when there is no longer anything to add, but when ther e is no longer anything to take away.” What he didn’t write: “It’s a lot ea sier to say something about perfection than to actually achieve it.” In this example, virtual pages 0 and 1 are for code, virtual page s 4 and 5 for the heap, and virtual pages 254 and 255 for the stack; the re st of the pages of the address space are unused. To build a two-level page table for this address space, we start with our full linear page table and break it up into page-sized unit s. Recall our full table (in this example) has 256 entries; assume each PTE is 4 bytes in size. Thus, our page table is 1KB (256 ×4 bytes) in size. Given that we have 64-byte pages, the 1KB page table can be divided into 1 6 64-byte pages; each page can hold 16 PTEs. What we need to understand now is how to take a VPN and use it to index ﬁrst into the page directory and then into the page of the p age table. Remember that each is an array of entries; thus, all we need to ﬁ gure out is how to construct the index for each from pieces of the VPN. Let’s ﬁrst index into the page directory. Our page table in this example is small: 256 entries, spread across 16 pages. The page direct ory needs one entry per page of the page table; thus, it has 16 entries. As a re sult, we need four bits of the VPN to index into the directory; we use the top four bits of the VPN, as follows: 13 12 11 10 9 8 7 6 5 4 3 2 1 0VPN offset Page Directory Index Once we extract the page-directory index (PDIndex for short) from the VPN, we can use it to ﬁnd the address of the page-directory en try (PDE) with a simple calculation: PDEAddr = PageDirBase + (PDIndex *sizeof(PDE)) . This results in our page directory, which we now ex- amine to make further progress in our translation. If the page-directory entry is marked invalid, we know that the access is invalid, and thus raise an exception. If, however, the PDE is valid, OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 9 we have more work to do.",3125
20. Advanced Page Tables,"Speciﬁcally, we now have to fetch the page- table entry (PTE) from the page of the page table pointed to by thi s page- directory entry. To ﬁnd this PTE, we have to index into the porti on of the page table using the remaining bits of the VPN: 13 12 11 10 9 8 7 6 5 4 3 2 1 0VPN offset Page Directory Index Page Table Index This page-table index (PTIndex for short) can then be used to index into the page table itself, giving us the address of our PTE: PTEAddr = (PDE.PFN << SHIFT) + (PTIndex *sizeof(PTE)) Note that the page-frame number (PFN) obtained from the page-di rectory entry must be left-shifted into place before combining it with the page- table index to form the address of the PTE. To see if this all makes sense, we’ll now ﬁll in a multi-level pag e ta- ble with some actual values, and translate a single virtual ad dress. Let’s begin with the page directory for this example (left side of Figure 20.5). In the ﬁgure, you can see that each page directory entry (PDE) de - scribes something about a page of the page table for the address sp ace. In this example, we have two valid regions in the address space (at the beginning and end), and a number of invalid mappings in-betwe en. In physical page 100 (the physical frame number of the 0th page of the page table), we have the ﬁrst page of 16 page table entries for th e ﬁrst 16 VPNs in the address space. See Figure 20.5 (middle part) for the contents of this portion of the page table. Page Directory Page of PT (@PFN:100) Page of PT (@PFN:101) PFN valid? PFN valid prot PFN valid prot 100 1 10 1 r-x — 0 — — 0 23 1 r-x — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 80 1 rw- — 0 — — 0 59 1 rw- — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — — 0 — — 0 — 0 — 55 1 rw- 101 1 — 0 — 45 1 rw- Figure 20.5: A Page Directory, And Pieces Of Page Table c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 PAGING : SMALLER TABLES This page of the page table contains the mappings for the ﬁrst 16 VPNs; in our example, VPNs 0 and 1 are valid (the code segment), a s are 4 and 5 (the heap). Thus, the table has mapping information f or each of those pages. The rest of the entries are marked invalid. The other valid page of the page table is found inside PFN 101. Thi s page contains mappings for the last 16 VPNs of the address space; see Figure 20.5 (right) for details. In the example, VPNs 254 and 255 (the stack) have valid mappin gs. Hopefully, what we can see from this example is how much space sav ings are possible with a multi-level indexed structure. In this ex ample, instead of allocating the full sixteen pages for a linear page table, we allocate only three : one for the page directory, and two for the chunks of the page table that have valid mappings. The savings for large (32-bit or 64-b it) address spaces could obviously be much greater. Finally, let’s use this information in order to perform a translat ion. Here is an address that refers to the 0th byte of VPN 254: 0x3F80 , or 11 1111 1000 0000 in binary. Recall that we will use the top 4 bits of the VPN to index into the page directory.",3160
20. Advanced Page Tables,"Thus, 1111 will choose the last (15th, if you start at the 0th) entry of the page directory above. This points us to a valid pa ge of the page table located at address 101. We then use the next 4 bits of the VPN ( 1110 ) to index into that page of the page table and ﬁnd the desired PTE. 1110 is the next-to-last (14th) entry on the pa ge, and tells us that page 254 of our virtual address space is mapped at p hysi- cal page 55. By concatenating PFN=55 (or hex 0x37 ) with offset=000000, we can thus form our desired physical address and issue the requ est to the memory system: PhysAddr = (PTE.PFN << SHIFT) + offset = 00 1101 1100 0000 = 0x0DC0 . You should now have some idea of how to construct a two-level page table, using a page directory which points to pages of the page ta ble. Un- fortunately, however, our work is not done. As we’ll now discuss, some- times two levels of page table is not enough. More Than Two Levels In our example thus far, we’ve assumed that multi-level page ta bles only have two levels: a page directory and then pieces of the page tab le. In some cases, a deeper tree is possible (and indeed, needed). Let’s take a simple example and use it to show why a deeper multi- level table can be useful. In this example, assume we have a 30 -bit virtual address space, and a small (512 byte) page. Thus our virtual ad dress has a 21-bit virtual page number component and a 9-bit offset. Remember our goal in constructing a multi-level page table: to m ake each piece of the page table ﬁt within a single page. Thus far, w e’ve only considered the page table itself; however, what if the page dir ectory gets too big? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 11 To determine how many levels are needed in a multi-level table to make all pieces of the page table ﬁt within a page, we start by de termining how many page-table entries ﬁt within a page. Given our page siz e of 512 bytes, and assuming a PTE size of 4 bytes, you should see that you ca n ﬁt 128 PTEs on a single page. When we index into a page of the page tab le, we can thus conclude we’ll need the least signiﬁcant 7 bits ( log2128) of the VPN as an index: 29282726252423222120191817161514131211109876543210VPN offset Page Directory Index Page Table Index What you also might notice from the diagram above is how many bits are left into the (large) page directory: 14. If our page direct ory has214 entries, it spans not one page but 128, and thus our goal of making ev ery piece of the multi-level page table ﬁt into a page vanishes. To remedy this problem, we build a further level of the tree, by s plit- ting the page directory itself into multiple pages, and then a dding another page directory on top of that, to point to the pages of the page direct ory. We can thus split up our virtual address as follows: 29282726252423222120191817161514131211109876543210VPN offset PD Index 0 PD Index 1 Page Table Index Now, when indexing the upper-level page directory, we use the v ery top bits of the virtual address ( PD Index 0 in the diagram); this index can be used to fetch the page-directory entry from the top-level page di- rectory. If valid, the second level of the page directory is consul ted by combining the physical frame number from the top-level PDE and t he next part of the VPN ( PD Index 1 ). Finally, if valid, the PTE address can be formed by using the page-table index combined with the ad dress from the second-level PDE.",3467
20. Advanced Page Tables,"Whew. That’s a lot of work. And all just to look something up in a multi-level table. The Translation Process: Remember the TLB To summarize the entire process of address translation using a t wo-level page table, we once again present the control ﬂow in algorithmic for m (Figure 20.6). The ﬁgure shows what happens in hardware (assu ming a hardware-managed TLB) upon every memory reference. As you can see from the ﬁgure, before any of the complicated multi- level page table access occurs, the hardware ﬁrst checks the T LB; upon c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 PAGING : SMALLER TABLES 1VPN = (VirtualAddress & VPN_MASK) >> SHIFT 2(Success, TlbEntry) = TLB_Lookup(VPN) 3if (Success == True) // TLB Hit 4if (CanAccess(TlbEntry.ProtectBits) == True) 5 Offset = VirtualAddress & OFFSET_MASK 6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 7 Register = AccessMemory(PhysAddr) 8else 9 RaiseException(PROTECTION_FAULT) 10else // TLB Miss 11 // first, get page directory entry 12 PDIndex = (VPN & PD_MASK) >> PD_SHIFT 13 PDEAddr = PDBR + (PDIndex *sizeof(PDE)) 14 PDE = AccessMemory(PDEAddr) 15 if (PDE.Valid == False) 16 RaiseException(SEGMENTATION_FAULT) 17 else 18 // PDE is valid: now fetch PTE from page table 19 PTIndex = (VPN & PT_MASK) >> PT_SHIFT 20 PTEAddr = (PDE.PFN << SHIFT) + (PTIndex *sizeof(PTE)) 21 PTE = AccessMemory(PTEAddr) 22 if (PTE.Valid == False) 23 RaiseException(SEGMENTATION_FAULT) 24 else if (CanAccess(PTE.ProtectBits) == False) 25 RaiseException(PROTECTION_FAULT) 26 else 27 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 28 RetryInstruction() Figure 20.6: Multi-level Page Table Control Flow a hit, the physical address is formed directly without accessing the page table at all, as before. Only upon a TLB miss does the hardware nee d to perform the full multi-level lookup. On this path, you can see the cost of our traditional two-level page table: two additional memory acce sses to look up a valid translation. 20.4 Inverted Page Tables An even more extreme space savings in the world of page tables is found with inverted page tables . Here, instead of having many page tables (one per process of the system), we keep a single page tabl e that has an entry for each physical page of the system. The entry tells us which process is using this page, and which virtual page of that proces s maps to this physical page. Finding the correct entry is now a matter of searching through thi s data structure. A linear scan would be expensive, and thus a ha sh table is often built over the base structure to speed up lookups. The Powe rPC is one example of such an architecture [JM98]. More generally, inverted page tables illustrate what we’ve sa id from the beginning: page tables are just data structures. You can d o lots of crazy things with data structures, making them smaller or big ger, making them slower or faster. Multi-level and inverted page tables ar e just two examples of the many things one could do. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 13 20.5 Swapping the Page Tables to Disk Finally, we discuss the relaxation of one ﬁnal assumption. Thus f ar, we have assumed that page tables reside in kernel-owned physi cal mem- ory. Even with our many tricks to reduce the size of page tables, i t is still possible, however, that they may be too big to ﬁt into memory all at once. Thus, some systems place such page tables in kernel virtual memory , thereby allowing the system to swap some of these page tables to disk when memory pressure gets a little tight. We’ll talk more about th is in a future chapter (namely, the case study on VAX/VMS), once we und er- stand how to move pages in and out of memory in more detail.",3716
20. Advanced Page Tables,"20.6 Summary We have now seen how real page tables are built; not necessarily j ust as linear arrays but as more complex data structures. The trade -offs such tables present are in time and space — the bigger the table, th e faster a TLB miss can be serviced, as well as the converse — and thus the r ight choice of structure depends strongly on the constraints of the give n envi- ronment. In a memory-constrained system (like many older systems), smal l struc- tures make sense; in a system with a reasonable amount of memory an d with workloads that actively use a large number of pages, a bigge r ta- ble that speeds up TLB misses might be the right choice. With sof tware- managed TLBs, the entire space of data structures opens up to th e delight of the operating system innovator (hint: that’s you). What new stru c- tures can you come up with? What problems do they solve? Think of these questions as you fall asleep, and dream the big dreams tha t only operating-system developers can dream. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 PAGING : SMALLER TABLES References [BOH10] “Computer Systems: A Programmer’s Perspective” by Randal E. Bryant and David R. O’Hallaron. Addison-Wesley, 2010. We have yet to ﬁnd a good ﬁrst reference to the multi-level page table. However, this great textbook by Bryant and O’Hallaron dives into the details of x86, which at least is an early system that used such structures. It’s also just a great b ook to have. [JM98] “Virtual Memory: Issues of Implementation” by Bruce Jacob, Tre vor Mudge. IEEE Computer, June 1998. An excellent survey of a number of different systems and their approac h to virtualizing memory. Plenty of details on x86, PowerPC, MIPS, and other archi tectures. [LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Hank Levy, P . Lipman. IEEE Computer, Vol. 15, No. 3, March 1982. A terriﬁc paper about a real virtual memory manager in a classic operating system, VMS. So terriﬁc, in fact, that we’ll use it to review everything we’ve learned about virtual memory thus far a few chapters from now. [M28] “Reese’s Peanut Butter Cups” by Mars Candy Corporation. Publis hed at stores near you. Apparently these ﬁne confections were invented in 1928 by Harry Burne tt Reese, a former dairy farmer and shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If true, Hershey and Reese probably hate each other’s guts, as any two chocolate baron s should. [N+02] “Practical, Transparent Operating System Support for Superpa ges” by Juan Navarro, Sitaram Iyer, Peter Druschel, Alan Cox. OSDI ’02, Boston, Massachuse tts, October 2002. A nice paper showing all the details you have to get right to incorporate large pages, or superpages , into a modern OS. Not as easy as you might think, alas. [M07] “Multics: History” Available: http://www.multicians.org/history.html .This amazing web site provides a huge amount of history on the Multics system, certai nly one of the most inﬂuential systems in OS history. The quote from therein: “Jack Dennis of M IT contributed inﬂuential architectural ideas to the beginning of Multics, especially the idea of com bining paging and segmenta- tion.” (from Section 1.2.1) OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG PAGING : SMALLER TABLES 15 Homework (Simulation) This fun little homework tests if you understand how a multi-leve l page table works. And yes, there is some debate over the use of the t erm “fun” in the previous sentence. The program is called, perhaps unsur- prisingly: paging-multilevel-translate.py ; see the README for details. Questions 1. With a linear page table, you need a single register to loca te the page table, assuming that hardware does the lookup upon a TLB miss. How many registers do you need to locate a two-level page table? A thre e-level table? 2. Use the simulator to perform translations given random seeds 0, 1, and 2, and check your answers using the -cﬂag. How many memory references are needed to perform each lookup? 3. Given your understanding of how cache memory works, how do you t hink memory references to the page table will behave in the cache? Wi ll they lead to lots of cache hits (and thus fast accesses?) Or lots of misses (and thus slow accesses)? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4332
21. Swapping Mechanisms,"21 Beyond Physical Memory: Mechanisms Thus far, we’ve assumed that an address space is unrealistica lly small and ﬁts into physical memory. In fact, we’ve been assuming that every address space of every running process ﬁts into memory. We will n ow relax these big assumptions, and assume that we wish to support many concurrently-running large address spaces. To do so, we require an additional level in the memory hierarchy . Thus far, we have assumed that all pages reside in physical me mory. However, to support large address spaces, the OS will need a pla ce to stash away portions of address spaces that currently aren’t in gr eat de- mand. In general, the characteristics of such a location are tha t it should have more capacity than memory; as a result, it is generally slow er (if it were faster, we would just use it as memory, no?). In modern system s, this role is usually served by a hard disk drive . Thus, in our memory hierarchy, big and slow hard drives sit at the bottom, with memory just above. And thus we arrive at the crux of the problem: THECRUX: HOWTOGOBEYOND PHYSICAL MEMORY How can the OS make use of a larger, slower device to transparentl y pro- vide the illusion of a large virtual address space? One question you might have: why do we want to support a single large address space for a process? Once again, the answer is conv enience and ease of use. With a large address space, you don’t have to worry about if there is room enough in memory for your program’s data struc- tures; rather, you just write the program naturally, allocatin g memory as needed. It is a powerful illusion that the OS provides, and makes your life vastly simpler. You’re welcome. A contrast is found in older sy stems that used memory overlays , which required programmers to manually move pieces of code or data in and out of memory as they were needed [D97]. Try imagining what this would be like: before calling a f unction or accessing some data, you need to ﬁrst arrange for the code or data to be in memory; yuck. 1 2 BEYOND PHYSICAL MEMORY : M ECHANISMS ASIDE : STORAGE TECHNOLOGIES We’ll delve much more deeply into how I/O devices actually work la ter (see the chapter on I/O devices). So be patient. And of course the s lower device need not be a hard disk, but could be something more modern such as a Flash-based SSD. We’ll talk about those things too. For now, just assume we have a big and relatively-slow device which we c an use to help us build the illusion of a very large virtual memory, even bigger than physical memory itself. Beyond just a single process, the addition of swap space allows the OS to support the illusion of a large virtual memory for multiple concu rrently- running processes. The invention of multiprogramming (running multi- ple programs “at once”, to better utilize the machine) almost de manded the ability to swap out some pages, as early machines clearly cou ld not hold all the pages needed by all processes at once. Thus, the combi na- tion of multiprogramming and ease-of-use leads us to want to supp ort using more memory than is physically available.",3109
21. Swapping Mechanisms,"It is something that all modern VM systems do; it is now something we will learn more about. 21.1 Swap Space The ﬁrst thing we will need to do is to reserve some space on the di sk for moving pages back and forth. In operating systems, we general ly refer to such space as swap space , because we swap pages out of memory to it and swap pages into memory from it. Thus, we will simply assume that the OS can read from and write to the swap space, in page-sized u nits. To do so, the OS will need to remember the disk address of a given page. The size of the swap space is important, as ultimately it determ ines the maximum number of memory pages that can be in use by a system a t a given time. Let us assume for simplicity that it is very large for now. In the tiny example (Figure 21.1), you can see a little example of a 4- page physical memory and an 8-page swap space. In the example, three processes (Proc 0, Proc 1, and Proc 2) are actively sharing physic al mem- ory; each of the three, however, only have some of their valid pages i n memory, with the rest located in swap space on disk. A fourth proces s (Proc 3) has all of its pages swapped out to disk, and thus clearly isn’t currently running. One block of swap remains free. Even from thi s tiny example, hopefully you can see how using swap space allows the sys tem to pretend that memory is larger than it actually is. We should note that swap space is not the only on-disk location for swapping trafﬁc. For example, assume you are running a program b inary (e.g.,ls, or your own compiled main program). The code pages from this binary are initially found on disk, and when the program runs, th ey are loaded into memory (either all at once when the program starts exe cution, OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : M ECHANISMS 3 Physical MemoryPFN 0 Proc 0 [VPN 0]PFN 1 Proc 1 [VPN 2]PFN 2 Proc 1 [VPN 3]PFN 3 Proc 2 [VPN 0] Swap SpaceProc 0 [VPN 1]Block 0 Proc 0 [VPN 2]Block 1 [Free]Block 2 Proc 1 [VPN 0]Block 3 Proc 1 [VPN 1]Block 4 Proc 3 [VPN 0]Block 5 Proc 2 [VPN 1]Block 6 Proc 3 [VPN 1]Block 7 Figure 21.1: Physical Memory and Swap Space or, as in modern systems, one page at a time when needed). However, if the system needs to make room in physical memory for other needs, it can safely re-use the memory space for these code pages, knowing t hat it can later swap them in again from the on-disk binary in the ﬁle sy stem. 21.2 The Present Bit Now that we have some space on the disk, we need to add some ma- chinery higher up in the system in order to support swapping pag es to and from the disk. Let us assume, for simplicity, that we have a s ystem with a hardware-managed TLB. Recall ﬁrst what happens on a memory reference. The running pro- cess generates virtual memory references (for instruction fet ches, or data accesses), and, in this case, the hardware translates them i nto physical addresses before fetching the desired data from memory. Remember that the hardware ﬁrst extracts the VPN from the virt ual address, checks the TLB for a match (a TLB hit ), and if a hit, produces the resulting physical address and fetches it from memory.",3166
21. Swapping Mechanisms,"This is hopefully the common case, as it is fast (requiring no additional memory acc esses). If the VPN is not found in the TLB (i.e., a TLB miss ), the hardware locates the page table in memory (using the page table base register ) and looks up the page table entry (PTE) for this page using the VPN as an index. If the page is valid and present in physical memory , the hardware extracts the PFN from the PTE, installs it in the TLB, and retries the instruction, this time generating a TLB hit; so far, so good. If we wish to allow pages to be swapped to disk, however, we must add even more machinery. Speciﬁcally, when the hardware looks in the PTE, it may ﬁnd that the page is not present in physical memory. The way the hardware (or the OS, in a software-managed TLB approach) dete r- mines this is through a new piece of information in each page-tabl e entry, known as the present bit . If the present bit is set to one, it means the page is present in physical memory and everything proceeds as a bove; if it is set to zero, the page is notin memory but rather on disk somewhere. The act of accessing a page that is not in physical memory is commonl y referred to as a page fault . c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 BEYOND PHYSICAL MEMORY : M ECHANISMS ASIDE : SWAPPING TERMINOLOGY ANDOTHER THINGS Terminology in virtual memory systems can be a little confusing a nd vari- able across machines and operating systems. For example, a page fault more generally could refer to any reference to a page table that generates a fault of some kind: this could include the type of fault we are dis cussing here, i.e., a page-not-present fault, but sometimes can refer to illegal mem- ory accesses. Indeed, it is odd that we call what is deﬁnitely a l egal access (to a page mapped into the virtual address space of a process, bu t simply not in physical memory at the time) a “fault” at all; really, it s hould be called a page miss . But often, when people say a program is “page fault- ing”, they mean that it is accessing parts of its virtual addre ss space that the OS has swapped out to disk. We suspect the reason that this behavior became known as a “fault ” re- lates to the machinery in the operating system to handle it. Wh en some- thing unusual happens, i.e., when something the hardware does n’t know how to handle occurs, the hardware simply transfers control to th e OS, hoping it can make things better. In this case, a page that a proc ess wants to access is missing from memory; the hardware does the only thing it can, which is raise an exception, and the OS takes over from there . As this is identical to what happens when a process does something i llegal, it is perhaps not surprising that we term the activity a “fault .” Upon a page fault, the OS is invoked to service the page fault. A p artic- ular piece of code, known as a page-fault handler , runs, and must service the page fault, as we now describe. 21.3 The Page Fault Recall that with TLB misses, we have two types of systems: hard ware- managed TLBs (where the hardware looks in the page table to ﬁnd t he desired translation) and software-managed TLBs (where the OS does). In either type of system, if a page is not present, the OS is put in ch arge to handle the page fault. The appropriately-named OS page-fault handler runs to determine what to do. Virtually all systems handle pag e faults in software; even with a hardware-managed TLB, the hardware tru sts the OS to manage this important duty. If a page is not present and has been swapped to disk, the OS will need to swap the page into memory in order to service the page fault.",3645
21. Swapping Mechanisms,"T hus, a question arises: how will the OS know where to ﬁnd the desired pag e? In many systems, the page table is a natural place to store such in formation. Thus, the OS could use the bits in the PTE normally used for data su ch as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the PTE to ﬁnd the address, and issues the re quest to disk to fetch the page into memory. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : M ECHANISMS 5 ASIDE : W HYHARDWARE DOESN ’THANDLE PAGE FAULTS We know from our experience with the TLB that hardware designers are loathe to trust the OS to do much of anything. So why do they trust t he OS to handle a page fault? There are a few main reasons. First, p age faults to disk are slow; even if the OS takes a long time to handle a fault, executing tons of instructions, the disk operation itself is trad itionally so slow that the extra overheads of running software are minimal. Sec ond, to be able to handle a page fault, the hardware would have to und erstand swap space, how to issue I/Os to the disk, and a lot of other details which it currently doesn’t know much about. Thus, for both reasons of perfor- mance and simplicity, the OS handles page faults, and even ha rdware types can be happy. When the disk I/O completes, the OS will then update the page ta ble to mark the page as present, update the PFN ﬁeld of the page-tab le entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction. This next attempt may generate a TLB mi ss, which would then be serviced and update the TLB with the translation ( one could alternately update the TLB when servicing the page faul t to avoid this step). Finally, a last restart would ﬁnd the translation i n the TLB and thus proceed to fetch the desired data or instruction from memory a t the translated physical address. Note that while the I/O is in ﬂight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes whi le the page fault is being serviced. Because I/O is expensive, this overlap of the I/O (page fault) of one process and the execution of another is ye t another way a multiprogrammed system can make the most effectiv e use of its hardware. 21.4 What If Memory Is Full? In the process described above, you may notice that we assumed the re is plenty of free memory in which to page in a page from swap space. Of course, this may not be the case; memory may be full (or close to it ). Thus, the OS might like to ﬁrst page out one or more pages to make room for the new page(s) the OS is about to bring in. The process of picki ng a page to kick out, or replace is known as the page-replacement policy . As it turns out, a lot of thought has been put into creating a good page - replacement policy, as kicking out the wrong page can exact a gre at cost on program performance. Making the wrong decision can cause a pro- gram to run at disk-like speeds instead of memory-like speeds; in cur- rent technology that means a program could run 10,000 or 100,000 ti mes slower.",3122
21. Swapping Mechanisms,"Thus, such a policy is something we should study in some det ail; indeed, that is exactly what we will do in the next chapter. For now, it is good enough to understand that such a policy exists, built on top of th e mechanisms described here. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 BEYOND PHYSICAL MEMORY : M ECHANISMS 1VPN = (VirtualAddress & VPN_MASK) >> SHIFT 2(Success, TlbEntry) = TLB_Lookup(VPN) 3if (Success == True) // TLB Hit 4if (CanAccess(TlbEntry.ProtectBits) == True) 5 Offset = VirtualAddress & OFFSET_MASK 6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 7 Register = AccessMemory(PhysAddr) 8else 9 RaiseException(PROTECTION_FAULT) 10else // TLB Miss 11 PTEAddr = PTBR + (VPN *sizeof(PTE)) 12 PTE =AccessMemory(PTEAddr) 13 if (PTE.Valid == False) 14 RaiseException(SEGMENTATION_FAULT) 15 else 16 if (CanAccess(PTE.ProtectBits) == False) 17 RaiseException(PROTECTION_FAULT) 18 else if (PTE.Present == True) 19 // assuming hardware-managed TLB 20 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 21 RetryInstruction() 22 else if (PTE.Present == False) 23 RaiseException(PAGE_FAULT) Figure 21.2: Page-Fault Control Flow Algorithm (Hardware) 21.5 Page Fault Control Flow With all of this knowledge in place, we can now roughly sketch the complete control ﬂow of memory access. In other words, when some- body asks you “what happens when a program fetches some data from memory?”, you should have a pretty good idea of all the different pos- sibilities. See the control ﬂow in Figures 21.2 and 21.3 for more det ails; the ﬁrst ﬁgure shows what the hardware does during translation, and the second what the OS does upon a page fault. From the hardware control ﬂow diagram in Figure 21.2, notice that there are now three important cases to understand when a TLB mis s oc- curs. First, that the page was both present and valid (Lines 18–21); in this case, the TLB miss handler can simply grab the PFN from the PTE, retry the instruction (this time resulting in a TLB hit), and t hus continue as described (many times) before. In the second case (Lines 22– 23), the page fault handler must be run; although this was a legitimate page for the process to access (it is valid, after all), it is not present in physical memory. Third (and ﬁnally), the access could be to an invalid pa ge, due for example to a bug in the program (Lines 13–14). In this case, n o other bits in the PTE really matter; the hardware traps this invali d access, and the OS trap handler runs, likely terminating the offending pr ocess. From the software control ﬂow in Figure 21.3, we can see what the OS roughly must do in order to service the page fault. First, the OS must ﬁnd a physical frame for the soon-to-be-faulted-in page to reside wi thin; if there is no such page, we’ll have to wait for the replacement alg orithm to run and kick some pages out of memory, thus freeing them for use here . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : M ECHANISMS 7 1PFN = FindFreePhysicalPage() 2if (PFN == -1) // no free page found 3PFN = EvictPage() // run replacement algorithm 4DiskRead(PTE.DiskAddr, PFN) // sleep (waiting for I/O) 5PTE.present = True // update page table with present 6PTE.PFN = PFN // bit and translation (PFN) 7RetryInstruction() // retry instruction Figure 21.3: Page-Fault Control Flow Algorithm (Software) With a physical frame in hand, the handler then issues the I/O request to read in the page from swap space.",3455
21. Swapping Mechanisms,"Finally, when that slow opera tion completes, the OS updates the page table and retries the instr uction. The retry will result in a TLB miss, and then, upon another retry, a T LB hit, at which point the hardware will be able to access the desired ite m. 21.6 When Replacements Really Occur Thus far, the way we’ve described how replacements occur assume s that the OS waits until memory is entirely full, and only then re places (evicts) a page to make room for some other page. As you can imagine, this is a little bit unrealistic, and there are many reasons for the OS to keep a small portion of memory free more proactively. To keep a small amount of memory free, most operating systems thus have some kind of high watermark (HW ) and low watermark (LW) to help decide when to start evicting pages from memory. How this wor ks is as follows: when the OS notices that there are fewer than LW pages avail- able, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The back- ground thread, sometimes called the swap daemon orpage daemon1, then goes to sleep, happy that it has freed some memory for running pro- cesses and the OS to use. By performing a number of replacements at once, new performance optimizations become possible. For example, many systems will cluster orgroup a number of pages and write them out at once to the swap parti- tion, thus increasing the efﬁciency of the disk [LL82]; as we wi ll see later when we discuss disks in more detail, such clustering reduces seek and rotational overheads of a disk and thus increases performance noti ceably. To work with the background paging thread, the control ﬂow in Figur e 21.3 should be modiﬁed slightly; instead of performing a replace ment directly, the algorithm would instead simply check if there ar e any free pages available. If not, it would inform the background paging th read that free pages are needed; when the thread frees up some pages , it would re-awaken the original thread, which could then page in the des ired page and go about its work. 1The word “daemon”, usually pronounced “demon”, is an old term for a back ground thread or process that does something useful. Turns out (once again.) that the source of the term is Multics [CS94]. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 BEYOND PHYSICAL MEMORY : M ECHANISMS TIP: DOWORK INTHEBACKGROUND When you have some work to do, it is often a good idea to do it in the background to increase efﬁciency and to allow for grouping of opera- tions. Operating systems often do work in the background; for exam ple, many systems buffer ﬁle writes in memory before actually writi ng the data to disk. Doing so has many possible beneﬁts: increased dis k efﬁ- ciency, as the disk may now receive many writes at once and thus b etter be able to schedule them; improved latency of writes, as the app lication thinks the writes completed quite quickly; the possibility of w ork reduc- tion, as the writes may need never to go to disk (i.e., if the ﬁle is deleted); and better use of idle time , as the background work may possibly be done when the system is otherwise idle, thus better utilizing t he hard- ware [G+95].",3230
21. Swapping Mechanisms,"21.7 Summary In this brief chapter, we have introduced the notion of accessing more memory than is physically present within a system. To do so req uires more complexity in page-table structures, as a present bit (of some kind) must be included to tell us whether the page is present in memor y or not. When not, the operating system page-fault handler runs to service the page fault , and thus arranges for the transfer of the desired page from disk to memory, perhaps ﬁrst replacing some pages in memory to ma ke room for those soon to be swapped in. Recall, importantly (and amazingly.), that these actions all t ake place transparently to the process. As far as the process is concerned, it is just accessing its own private, contiguous virtual memory. Behind th e scenes, pages are placed in arbitrary (non-contiguous) locations in phys ical mem- ory, and sometimes they are not even present in memory, requiring a fetch from disk. While we hope that in the common case a memory access is fast, in some cases it will take multiple disk operations to serv ice it; some- thing as simple as performing a single instruction can, in the w orst case, take many milliseconds to complete. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : M ECHANISMS 9 References [CS94] “Take Our Word For It” by F. Corbato, R. Steinberg. www.takeourword.com/TOW146 (Page 4). Richard Steinberg writes: “Someone has asked me the origin of the word daemon as it applies to computing. Best I can tell based on my research, the word was ﬁrst used by people on your team at Project MAC using the IBM 7094 in 1963.” Professor Corbato replies: “O ur use of the word daemon was inspired by the Maxwell’s daemon of physics and thermodynamics (my bac kground is in physics). Maxwell’s daemon was an imaginary agent which helped sort molecules of di fferent speeds and worked tirelessly in the background. We fancifully began to use the word daemon to d escribe background pro- cesses which worked tirelessly to perform system chores.” [D97] “Before Memory Was Virtual” by Peter Denning. In the Beginning: Reco llections of Software Pioneers, Wiley, November 1997. An excellent historical piece by one of the pioneers of virtual memory and working sets. [G+95] “Idleness is not sloth” by Richard Golding, Peter Bosch, Carl Sta elin, Tim Sullivan, John Wilkes. USENIX ATC ’95, New Orleans, Louisiana. A fun and easy-to-read discussion of how idle time can be better used in systems, with lots of good examples. [LL82] “Virtual Memory Management in the VAX/VMS Operating System” by Hank Levy, P . Lipman. IEEE Computer, Vol. 15, No. 3, March 1982. Not the ﬁrst place where page clustering was used, but a clear and simple explanation of how such a mechanism works. We s ure cite this paper a lot. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 BEYOND PHYSICAL MEMORY : M ECHANISMS Homework (Measurement) This homework introduces you to a new tool, vmstat , and how it can be used to understand memory, CPU, and I/O usage. Read the assoc i- ated README and examine the code in mem.c before proceeding to the exercises and questions below.",3142
21. Swapping Mechanisms,"Questions 1. First, open two separate terminal connections to the same machine, so that you can easily run something in one window and the other. Now, in one window, run vmstat 1 , which shows statistics about machine usage every second. Read the man page, the associated README, an d any other information you need so that you can understand its output. Leave this window running vmstat for the rest of the exercises below. Now, we will run the program mem.c but with very little memory usage. This can be accomplished by typing ./mem 1 (which uses only 1 MB of memory). How do the CPU usage statistics change when running mem? Do the numbers in the user time column make sense? How does this change when running more than one instance of mem at once? 2. Let’s now start looking at some of the memory statistics while runningmem. We’ll focus on two columns: swpd (the amount of virtual memory used) and free (the amount of idle memory). Run ./mem 1024 (which allocates 1024 MB) and watch how these values change. Then kill the running pro gram (by typing control-c) and watch again how the values change. W hat do you notice about the values? In particular, how does the free column change when the program exits? Does the amount of free memory increase by t he expected amount when mem exits? 3. We’ll next look at the swap columns (siandso), which indicate how much swapping is taking place to and from the disk. Of course, to act ivate these, you’ll need to run mem with large amounts of memory. First, examine how much free memory is on your Linux system (for example, by typing cat /proc/meminfo ; typeman proc for details on the /proc ﬁle system and the types of information you can ﬁnd there). One of the ﬁrst ent ries in /proc/meminfo is the total amount of memory in your system. Let’s as- sume it’s something like 8 GB of memory; if so, start by running mem 4000 (about 4 GB) and watching the swap in/out columns. Do they ever giv e non-zero values? Then, try with 5000 ,6000 , etc. What happens to these values as the program enters the second loop (and beyond), as c ompared to the ﬁrst loop? How much data (total) are swapped in and out during t he second, third, and subsequent loops? (do the numbers make sense?) 4. Do the same experiments as above, but now watch the other statis tics (such as CPU utilization, and block I/O statistics). How do they cha nge when mem is running? 5. Now let’s examine performance. Pick an input for mem that comfortably ﬁts in memory (say 4000 if the amount of memory on the system is 8 GB). How long does loop 0 take (and subsequent loops 1, 2, etc.)? Now p ick a size comfortably beyond the size of memory (say 12000 again assuming 8 GB of OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : M ECHANISMS 11 memory). How long do the loops take here? How do the bandwidth num- bers compare? How different is performance when constantly sw apping versus ﬁtting everything comfortably in memory? Can you make a gra ph, with the size of memory used by mem on the x-axis, and the bandwidth of accessing said memory on the y-axis? Finally, how does the perfo rmance of the ﬁrst loop compare to that of subsequent loops, for both the ca se where everything ﬁts in memory and where it doesn’t? 6. Swap space isn’t inﬁnite. You can use the tool swapon with the-sﬂag to see how much swap space is available. What happens if you try to r unmem with increasingly large values, beyond what seems to be availa ble in swap? At what point does the memory allocation fail? 7. Finally, if you’re advanced, you can conﬁgure your system to us e different swap devices using swapon andswapoff . Read the man pages for details. If you have access to different hardware, see how the performa nce of swap- ping changes when swapping to a classic hard drive, a ﬂash-ba sed SSD, and even a RAID array. How much can swapping performance be improved vi a newer devices? How close can you get to in-memory performance? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4018
22. Swapping Policies,"22 Beyond Physical Memory: Policies In a virtual memory manager, life is easy when you have a lot of free memory. A page fault occurs, you ﬁnd a free page on the free-page li st, and assign it to the faulting page. Hey, Operating System, cong ratula- tions. You did it again. Unfortunately, things get a little more interesting when litt le memory is free. In such a case, this memory pressure forces the OS to start paging outpages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the replacement policy of the OS; historically, it was one of the most important decisions the earl y vir- tual memory systems made, as older systems had little physical memory. Minimally, it is an interesting set of policies worth knowing a li ttle more about. And thus our problem: THECRUX: HOWTODECIDE WHICH PAGE TOEVICT How can the OS decide which page (or pages) to evict from memory? This decision is made by the replacement policy of the system, wh ich usu- ally follows some general principles (discussed below) but also includes certain tweaks to avoid corner-case behaviors. 22.1 Cache Management Before diving into policies, we ﬁrst describe the problem we are trying to solve in more detail. Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. Thus, our goal in picking a replaceme nt policy for this cache is to minimize the number of cache misses , i.e., to minimize the number of times that we have to fetch a page from dis k. Alternately, one can view our goal as maximizing the number of cache hits, i.e., the number of times a page that is accessed is found in mem ory. Knowing the number of cache hits and misses let us calculate the av- erage memory access time (AMAT ) for a program (a metric computer architects compute for hardware caches [HP06]). Speciﬁcally, given these values, we can compute the AMAT of a program as follows: AMAT =TM+(PMiss·TD) (22.1) 1 2 BEYOND PHYSICAL MEMORY : POLICIES whereTMrepresents the cost of accessing memory, TDthe cost of ac- cessing disk, and PMiss the probability of not ﬁnding the data in the cache (a miss); PMiss varies from 0.0 to 1.0, and sometimes we refer to a percent miss rate instead of a probability (e.g., a 10 percent miss ra te means PMiss= 0.10). Note you always pay the cost of accessing the data in memory; when you miss, however, you must additionally pay the cost of fetching the data from disk. For example, let us imagine a machine with a (tiny) address spa ce: 4KB, with 256-byte pages. Thus, a virtual address has two comp onents: a 4-bit VPN (the most-signiﬁcant bits) and an 8-bit offset (the l east-signiﬁcant bits). Thus, a process in this example can access 24or 16 total virtual pages. In this example, the process generates the following mem ory ref- erences (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x 300, 0x400, 0x500, 0x600, 0x700, 0x800, 0x900. These virtual addresses refer t o the ﬁrst byte of each of the ﬁrst ten pages of the address space (the page number being the ﬁrst hex digit of each virtual address). Let us further assume that every page except virtual page 3 is already in memory. Thus, our sequence of memory references will encounter the following behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit , hit. We can compute the hit rate (the percent of references found in memory): 90 percent, as 9 out of 10 references are in memory.",3483
22. Swapping Policies,"The miss rate is thus 10 percent ( PMiss= 0.1). In general, PHit+PMiss= 1.0; hit rate plus miss rate sum to 100 percent. To calculate AMAT, we need to know the cost of accessing memory and the cost of accessing disk. Assuming the cost of accessing mem ory (TM) is around 100 nanoseconds, and the cost of accessing disk ( TD) is about 10 milliseconds, we have the following AMAT: 100ns+0.1·10ms, which is 100ns+ 1ms, or 1.0001 ms, or about 1 millisecond. If our hit rate had instead been 99.9 percent ( Pmiss= 0.001), the result is quite different: AMAT is 10.1 microseconds, or roughly 100 times faster. As the hit rate approaches 100 percent, AMAT approaches 100 nanoseconds. Unfortunately, as you can see in this example, the cost of disk acc ess is so high in modern systems that even a tiny miss rate will quic kly dom- inate the overall AMAT of running programs. Clearly, we need to a void as many misses as possible or run slowly, at the rate of the disk. On e way to help with this is to carefully develop a smart policy, as we now do. 22.2 The Optimal Replacement Policy To better understand how a particular replacement policy works , it would be nice to compare it to the best possible replacement polic y. As it turns out, such an optimal policy was developed by Belady many years ago [B66] (he originally called it MIN). The optimal replaceme nt policy leads to the fewest number of misses overall. Belady showed that a sim- ple (but, unfortunately, difﬁcult to implement.) approach tha t replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 3 TIP: COMPARING AGAINST OPTIMAL ISUSEFUL Although optimal is not very practical as a real policy, it is incr edibly useful as a comparison point in simulation or other studies. Saying t hat your fancy new algorithm has a 80 percent hit rate isn’t meaningful in is olation; saying that optimal achieves an 82 percent hit rate (and thus your new a pproach is quite close to optimal) makes the result more meaningful and g ives it context. Thus, in any study you perform, knowing what the optimal i s lets you perform a better comparison, showing how much improvement is still possible, and also when you can stop making your policy better, because it is close enough to the ideal [AD03]. Hopefully, the intuition behind the optimal policy makes sense. Think about it like this: if you have to throw out some page, why not throw out the one that is needed the furthest from now? By doing so, you are essentially saying that all the other pages in the cache are mor e important than the one furthest out. The reason this is true is simple: you wi ll refer to the other pages before you refer to the one furthest out. Let’s trace through a simple example to understand the decision s the optimal policy makes. Assume a program accesses the following str eam of virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1.",3021
22. Swapping Policies,"Figure 22.1 shows t he behavior of optimal, assuming a cache that ﬁts three pages. In the ﬁgure, you can see the following actions. Not surprisingly, the ﬁrst three accesses are misses, as the cache begins in an empt y state; such a miss is sometimes referred to as a cold-start miss (orcompulsory miss ). Then we refer again to pages 0 and 1, which both hit in the cache. Finally, we reach another miss (to page 3), but this time the cache is ful l; a re- placement must take place. Which begs the question: which pag e should we replace? With the optimal policy, we examine the future for ea ch page currently in the cache (0, 1, and 2), and see that 0 is accessed almost imme- diately, 1 is accessed a little later, and 2 is accessed furth est in the future. Thus the optimal policy has an easy choice: evict page 2, resulti ng in pages 0, 1, and 3 in the cache. The next three references are hi ts, but then Resulting Access Hit/Miss? Evict Cache State 0 Miss 0 1 Miss 0, 1 2 Miss 0, 1, 2 0 Hit 0, 1, 2 1 Hit 0, 1, 2 3 Miss 2 0, 1, 3 0 Hit 0, 1, 3 3 Hit 0, 1, 3 1 Hit 0, 1, 3 2 Miss 3 0, 1, 2 1 Hit 0, 1, 2 Figure 22.1: Tracing The Optimal Policy c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 BEYOND PHYSICAL MEMORY : POLICIES ASIDE : TYPES OF CACHE MISSES In the computer architecture world, architects sometimes ﬁnd i t useful to characterize misses by type, into one of three categories: com pulsory, capacity, and conﬂict misses, sometimes called the Three C’s [H87]. A compulsory miss (orcold-start miss [EF78]) occurs because the cache is empty to begin with and this is the ﬁrst reference to the item; in con- trast, a capacity miss occurs because the cache ran out of space and had to evict an item to bring a new item into the cache. The third ty pe of miss (a conﬂict miss ) arises in hardware because of limits on where an item can be placed in a hardware cache, due to something known as set- associativity ; it does not arise in the OS page cache because such caches are always fully-associative , i.e., there are no restrictions on where in memory a page can be placed. See H&P for details [HP06]. we get to page 2, which we evicted long ago, and suffer another mis s. Here the optimal policy again examines the future for each page i n the cache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which is about to be accessed), we’ll be OK. The example shows page 3 get ting evicted, although 0 would have been a ﬁne choice too. Finally, we hi t on page 1 and the trace completes. We can also calculate the hit rate for the cache: with 6 hits and 5 misses, the hit rate isHits Hits+Misseswhich is6 6+5or 54.5 percent. You can also compute the hit rate modulo compulsory misses (i.e., ignore the ﬁrstmiss to a given page), resulting in a 85.7 percent hit rate. Unfortunately, as we saw before in the development of scheduling policies, the future is not generally known; you can’t build the opt imal policy for a general-purpose operating system1. Thus, in developing a real, deployable policy, we will focus on approaches that ﬁnd some ot her way to decide which page to evict.",3124
22. Swapping Policies,"The optimal policy will thus s erve only as a comparison point, to know how close we are to “perfect”. 22.3 A Simple Policy: FIFO Many early systems avoided the complexity of trying to approach optimal and employed very simple replacement policies. For exam ple, some systems used FIFO (ﬁrst-in, ﬁrst-out) replacement, where pages were simply placed in a queue when they enter the system; when a re- placement occurs, the page on the tail of the queue (the “ﬁrst-in ” page) is evicted. FIFO has one great strength: it is quite simple to imp lement. Let’s examine how FIFO does on our example reference stream (Figur e 22.2, page 5). We again begin our trace with three compulsory mis ses to pages 0, 1, and 2, and then hit on both 0 and 1. Next, page 3 is refer enced, causing a miss; the replacement decision is easy with FIFO: pi ck the page 1If you can, let us know. We can become rich together. Or, like the scientist s who “discov- ered” cold fusion, widely scorned and mocked [FP89]. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 5 Resulting Access Hit/Miss? Evict Cache State 0 Miss First-in → 0 1 Miss First-in → 0, 1 2 Miss First-in → 0, 1, 2 0 Hit First-in → 0, 1, 2 1 Hit First-in → 0, 1, 2 3 Miss 0 First-in → 1, 2, 3 0 Miss 1 First-in → 2, 3, 0 3 Hit First-in → 2, 3, 0 1 Miss 2 First-in → 3, 0, 1 2 Miss 3 First-in → 0, 1, 2 1 Hit First-in → 0, 1, 2 Figure 22.2: Tracing The FIFO Policy that was the “ﬁrst one” in (the cache state in the ﬁgure is kept i n FIFO order, with the ﬁrst-in page on the left), which is page 0. Unfort unately, our next access is to page 0, causing another miss and replaceme nt (of page 1). We then hit on page 3, but miss on 1 and 2, and ﬁnally hit on 3 . Comparing FIFO to optimal, FIFO does notably worse: a 36.4 percent hit rate (or 57.1 percent excluding compulsory misses). FIFO simply can’t d eter- mine the importance of blocks: even though page 0 had been accesse d a number of times, FIFO still kicks it out, simply because it was the ﬁrst one brought into memory. ASIDE : BELADY ’SANOMALY Belady (of the optimal policy) and colleagues found an interestin g refer- ence stream that behaved a little unexpectedly [BNS69]. The m emory- reference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5. The replacem ent policy they were studying was FIFO. The interesting part: how the cac he hit rate changed when moving from a cache size of 3 to 4 pages. In general, you would expect the cache hit rate to increase (get better) when the cache gets larger. But in this case, with FIFO, it get s worse. Cal- culate the hits and misses yourself and see. This odd behavior is generally referred to as Belady’s Anomaly (to the chagrin of his co-authors). Some other policies, such as LRU, don’t suffer from this problem. Can you guess why? As it turns out, LRU has what is known as a stack prop- erty [M+70]. For algorithms with this property, a cache of size N+ 1 naturally includes the contents of a cache of size N. Thus, when increas- ing the cache size, hit rate will either stay the same or improve .",3076
22. Swapping Policies,"FIFO and Random (among others) clearly do not obey the stack property, and th us are susceptible to anomalous behavior. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 BEYOND PHYSICAL MEMORY : POLICIES Resulting Access Hit/Miss? Evict Cache State 0 Miss 0 1 Miss 0, 1 2 Miss 0, 1, 2 0 Hit 0, 1, 2 1 Hit 0, 1, 2 3 Miss 0 1, 2, 3 0 Miss 1 2, 3, 0 3 Hit 2, 3, 0 1 Miss 3 2, 0, 1 2 Hit 2, 0, 1 1 Hit 2, 0, 1 Figure 22.3: Tracing The Random Policy 22.4 Another Simple Policy: Random Another similar replacement policy is Random, which simply pic ks a random page to replace under memory pressure. Random has propert ies similar to FIFO; it is simple to implement, but it doesn’t reall y try to be too intelligent in picking which blocks to evict. Let’s look at how R andom does on our famous example reference stream (see Figure 22.3). Of course, how Random does depends entirely upon how lucky (or unlucky) Random gets in its choices. In the example above, Random does a little better than FIFO, and a little worse than optimal. In fa ct, we can run the Random experiment thousands of times and determine how it does in general. Figure 22.4 shows how many hits Random achieves ov er 10,000 trials, each with a different random seed. As you can see , some- times (just over 40 percent of the time), Random is as good as optimal, achie ving 6 hits on the example trace; sometimes it does much worse, achievi ng 2 hits or fewer. How Random does depends on the luck of the draw. 0 1 2 3 4 5 6 701020304050 Number of HitsFrequency Figure 22.4: Random Performance Over 10,000 Trials OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 7 Resulting Access Hit/Miss? Evict Cache State 0 Miss LRU → 0 1 Miss LRU → 0, 1 2 Miss LRU → 0, 1, 2 0 Hit LRU → 1, 2, 0 1 Hit LRU → 2, 0, 1 3 Miss 2 LRU → 0, 1, 3 0 Hit LRU → 1, 3, 0 3 Hit LRU → 1, 0, 3 1 Hit LRU → 0, 3, 1 2 Miss 0 LRU → 3, 1, 2 1 Hit LRU → 3, 2, 1 Figure 22.5: Tracing The LRU Policy 22.5 Using History: LRU Unfortunately, any policy as simple as FIFO or Random is likely to have a common problem: it might kick out an important page, one that is about to be referenced again. FIFO kicks out the page that was ﬁrst brought in; if this happens to be a page with important code or data structures upon it, it gets thrown out anyhow, even though it will s oon be paged back in. Thus, FIFO, Random, and similar policies are not l ikely to approach optimal; something smarter is needed. As we did with scheduling policy, to improve our guess at the futu re, we once again lean on the past and use history as our guide. For example, if a program has accessed a page in the near past, it is likely to access it again in the near future. One type of historical information a page-replacement policy coul d use is frequency ; if a page has been accessed many times, perhaps it should not be replaced as it clearly has some value. A more commonly- used property of a page is its recency of access; the more recently a page has been accessed, perhaps the more likely it will be accessed again.",3068
22. Swapping Policies,"This family of policies is based on what people refer to as the prin- ciple of locality [D70], which basically is just an observation about pro- grams and their behavior. What this principle says, quite sim ply, is that programs tend to access certain code sequences (e.g., in a loop) a nd data structures (e.g., an array accessed by the loop) quite frequen tly; we should thus try to use history to ﬁgure out which pages are important, an d keep those pages in memory when it comes to eviction time. And thus, a family of simple historically-based algorithms are born. The Least-Frequently-Used (LFU ) policy replaces the least-frequently- used page when an eviction must take place. Similarly, the Least-Recently- Used (LRU ) policy replaces the least-recently-used page. These algo- rithms are easy to remember: once you know the name, you know exactl y what it does, which is an excellent property for a name. To better understand LRU, let’s examine how LRU does on our exam- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 BEYOND PHYSICAL MEMORY : POLICIES ASIDE : TYPES OF LOCALITY There are two types of locality that programs tend to exhibit. Th e ﬁrst is known as spatial locality , which states that if a page Pis accessed, it is likely the pages around it (say P−1orP+ 1) will also likely be accessed. The second is temporal locality , which states that pages that have been accessed in the near past are likely to be accessed a gain in the near future. The assumption of the presence of these types of local ity plays a large role in the caching hierarchies of hardware syste ms, which deploy many levels of instruction, data, and address-translat ion caching to help programs run fast when such locality exists. Of course, the principle of locality , as it is often called, is no hard-and- fast rule that all programs must obey. Indeed, some programs acce ss memory (or disk) in rather random fashion and don’t exhibit much or any locality in their access streams. Thus, while locality is a good thing to keep in mind while designing caches of any kind (hardware or soft ware), it does not guarantee success. Rather, it is a heuristic that often proves useful in the design of computer systems. ple reference stream. Figure 22.5 (page 7) shows the results. From the ﬁgure, you can see how LRU can use history to do better than statel ess policies such as Random or FIFO. In the example, LRU evicts page 2 when it ﬁrst has to replace a page, because 0 and 1 have been accesse d more re- cently. It then replaces page 0 because 1 and 3 have been acces sed more recently. In both cases, LRU’s decision, based on history, turns ou t to be correct, and the next references are thus hits. Thus, in our exa mple, LRU does as well as possible, matching optimal in its performance2. We should also note that the opposites of these algorithms exist: Most- Frequently-Used (MFU ) and Most-Recently-Used (MRU ). In most cases (not all.), these policies do not work well, as they ignore the locali ty most programs exhibit instead of embracing it. 22.6 Workload Examples Let’s look at a few more examples in order to better understand how some of these policies behave. Here, we’ll examine more complex work- loads instead of small traces. However, even these workloads are great ly simpliﬁed; a better study would include application traces.",3353
22. Swapping Policies,"Our ﬁrst workload has no locality, which means that each referen ce is to a random page within the set of accessed pages. In this simp le ex- ample, the workload accesses 100 unique pages over time, choosing the next page to refer to at random; overall, 10,000 pages are acces sed. In the experiment, we vary the cache size from very small (1 page) to e nough to hold all the unique pages (100 page), in order to see how each pol icy behaves over the range of cache sizes. 2OK, we cooked the results. But sometimes cooking is necessary to prov e a point. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 9 0 20 40 60 80 1000 percent20 percent40 percent60 percent80 percent100 percentThe No-Locality Workload Cache Size (Blocks)Hit Rate OPT LRU FIFO RAND Figure 22.6: The No-Locality Workload Figure 22.6 plots the results of the experiment for optimal, LRU, Ran- dom, and FIFO. The y-axis of the ﬁgure shows the hit rate that each policy achieves; the x-axis varies the cache size as described above . We can draw a number of conclusions from the graph. First, when there is no locality in the workload, it doesn’t matter much which r ealistic policy you are using; LRU, FIFO, and Random all perform the same, w ith the hit rate exactly determined by the size of the cache. Second, when the cache is large enough to ﬁt the entire workload, it also doesn’t matter which policy you use; all policies (even Random) converge to a 100 percent hit rate when all the referenced blocks ﬁt in cache. Finally, you ca n see that optimal performs noticeably better than the realistic policies ; peeking into the future, if it were possible, does a much better job of replacem ent. The next workload we examine is called the “80-20” workload, whic h exhibits locality: 80 percent of the references are made to 20 percent of the pa ges (the “hot” pages); the remaining 20 percent of the references are made to th e re- maining 80 percent of the pages (the “cold” pages). In our workload, there are a total 100 unique pages again; thus, “hot” pages are referred t o most of the time, and “cold” pages the remainder. Figure 22.7 (page 10 ) shows how the policies perform with this workload. As you can see from the ﬁgure, while both random and FIFO do rea- sonably well, LRU does better, as it is more likely to hold onto the h ot pages; as those pages have been referred to frequently in the p ast, they are likely to be referred to again in the near future. Optimal once again does better, showing that LRU’s historical information is not perfe ct. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 BEYOND PHYSICAL MEMORY : POLICIES 0 20 40 60 80 1000 percent20 percent40 percent60 percent80 percent100 percentThe 80-20 Workload Cache Size (Blocks)Hit Rate OPT LRU FIFO RAND Figure 22.7: The 80-20 Workload You might now be wondering: is LRU’s improvement over Random and FIFO really that big of a deal? The answer, as usual, is “it d epends.” If each miss is very costly (not uncommon), then even a small increas e in hit rate (reduction in miss rate) can make a huge difference on perf ormance. If misses are not so costly, then of course the beneﬁts possible wit h LRU are not nearly as important.",3230
22. Swapping Policies,"Let’s look at one ﬁnal workload. We call this one the “looping sequen- tial” workload, as in it, we refer to 50 pages in sequence, start ing at 0, then 1, ..., up to page 49, and then we loop, repeating those acces ses, for a total of 10,000 accesses to 50 unique pages. The last graph in Fi gure 22.8 shows the behavior of the policies under this workload. This workload, common in many applications (including important commercial applications such as databases [CD85]), represen ts a worst- case for both LRU and FIFO. These algorithms, under a looping-sequ ential workload, kick out older pages; unfortunately, due to the looping na ture of the workload, these older pages are going to be accessed sooner tha n the pages that the policies prefer to keep in cache. Indeed, ev en with a cache of size 49, a looping-sequential workload of 50 pages result s in a 0 percent hit rate. Interestingly, Random fares notably better, not q uite ap- proaching optimal, but at least achieving a non-zero hit rate. T urns out that random has some nice properties; one such property is not havin g weird corner-case behaviors. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 11 0 20 40 60 80 1000 percent20 percent40 percent60 percent80 percent100 percentThe Looping-Sequential Workload Cache Size (Blocks)Hit Rate OPT LRU FIFO RAND Figure 22.8: The Looping Workload 22.7 Implementing Historical Algorithms As you can see, an algorithm such as LRU can generally do a better job than simpler policies like FIFO or Random, which may throw out important pages. Unfortunately, historical policies present u s with a new challenge: how do we implement them? Let’s take, for example, LRU. To implement it perfectly, we nee d to do a lot of work. Speciﬁcally, upon each page access (i.e., each memory access, whether an instruction fetch or a load or store), we must up date some data structure to move this page to the front of the list (i.e. , the MRU side). Contrast this to FIFO, where the FIFO list of pages is only accessed when a page is evicted (by removing the ﬁrst-in page) or when a new page is added to the list (to the last-in side). To keep tra ck of which pages have been least- and most-recently used, the system has to do some accounting work on every memory reference. Clearly, without great care, such accounting could greatly reduce performance. One method that could help speed this up is to add a little bit of ha rd- ware support. For example, a machine could update, on each page ac cess, a time ﬁeld in memory (for example, this could be in the per-proces s page table, or just in some separate array in memory, with one entry per phys- ical page of the system). Thus, when a page is accessed, the tim e ﬁeld would be set, by hardware, to the current time. Then, when repl acing a page, the OS could simply scan all the time ﬁelds in the system t o ﬁnd the least-recently-used page. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 BEYOND PHYSICAL MEMORY : POLICIES Unfortunately, as the number of pages in a system grows, scannin g a huge array of times just to ﬁnd the absolute least-recently-us ed page is prohibitively expensive. Imagine a modern machine with 4GB of m em- ory, chopped into 4KB pages. This machine has 1 million pages, an d thus ﬁnding the LRU page will take a long time, even at modern CPU spee ds.",3373
22. Swapping Policies,"Which begs the question: do we really need to ﬁnd the absolute old est page to replace? Can we instead survive with an approximation? CRUX: HOWTOIMPLEMENT ANLRU R EPLACEMENT POLICY Given that it will be expensive to implement perfect LRU, can we ap- proximate it in some way, and still obtain the desired behavior? 22.8 Approximating LRU As it turns out, the answer is yes: approximating LRU is more fea- sible from a computational-overhead standpoint, and indeed it is what many modern systems do. The idea requires some hardware support, in the form of a use bit (sometimes called the reference bit ), the ﬁrst of which was implemented in the ﬁrst system with paging, the Atl as one- level store [KE+62]. There is one use bit per page of the system, a nd the use bits live in memory somewhere (they could be in the per-proces s page tables, for example, or just in an array somewhere). Whenever a p age is referenced (i.e., read or written), the use bit is set by hardw are to 1. The hardware never clears the bit, though (i.e., sets it to 0); tha t is the respon- sibility of the OS. How does the OS employ the use bit to approximate LRU? Well, there could be a lot of ways, but with the clock algorithm [C69], one simple approach was suggested. Imagine all the pages of the system arr anged in a circular list. A clock hand points to some particular page to begin with (it doesn’t really matter which). When a replacement must occur , the OS checks if the currently-pointed to page Phas a use bit of 1 or 0. If 1, this implies that page Pwas recently used and thus is nota good candidate for replacement. Thus, the use bit for Pset to 0 (cleared), and the clock hand is incremented to the next page ( P+ 1). The algorithm continues until it ﬁnds a use bit that is set to 0, implying this page has n ot been recently used (or, in the worst case, that all pages have been an d that we have now searched through the entire set of pages, clearing all t he bits). Note that this approach is not the only way to employ a use bit to approximate LRU. Indeed, any approach which periodically clea rs the use bits and then differentiates between which pages have us e bits of 1 versus 0 to decide which to replace would be ﬁne. The clock algori thm of Corbato’s was just one early approach which met with some success, a nd had the nice property of not repeatedly scanning through all of mem ory looking for an unused page. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 13 0 20 40 60 80 1000 percent20 percent40 percent60 percent80 percent100 percentThe 80-20 Workload Cache Size (Blocks)Hit Rate OPT LRU FIFO RAND Clock Figure 22.9: The 80-20 Workload With Clock The behavior of a clock algorithm variant is shown in Figure 22.9. T his variant randomly scans pages when doing a replacement; when it en- counters a page with a reference bit set to 1, it clears the bit ( i.e., sets it to 0); when it ﬁnds a page with the reference bit set to 0, it choos es it as its victim. As you can see, although it doesn’t do quite as well as p erfect LRU, it does better than approaches that don’t consider history at a ll.",3143
22. Swapping Policies,"22.9 Considering Dirty Pages One small modiﬁcation to the clock algorithm (also originally sug - gested by Corbato [C69]) that is commonly made is the additional c on- sideration of whether a page has been modiﬁed or not while in memory. The reason for this: if a page has been modiﬁed and is thus dirty , it must be written back to disk to evict it, which is expensive. If it h as not been modiﬁed (and is thus clean ), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O. Thus, some VM systems prefer to evict clean pages over dirty pages. To support this behavior, the hardware should include a modiﬁed bit (a.k.a. dirty bit ). This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm. The clock al gorithm, for example, could be changed to scan for pages that are both unuse d and clean to evict ﬁrst; failing to ﬁnd those, then for unused pa ges that are dirty, and so forth. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 BEYOND PHYSICAL MEMORY : POLICIES 22.10 Other VM Policies Page replacement is not the only policy the VM subsystem employs (though it may be the most important). For example, the OS also has to decide when to bring a page into memory. This policy, sometimes called thepage selection policy (as it was called by Denning [D70]), presents the OS with some different options. For most pages, the OS simply uses demand paging , which means the OS brings the page into memory when it is accessed, “on demand” a s it were. Of course, the OS could guess that a page is about to be use d, and thus bring it in ahead of time; this behavior is known as prefetching and should only be done when there is reasonable chance of success. For example, some systems will assume that if a code page Pis brought into memory, that code page P+1will likely soon be accessed and thus should be brought into memory too. Another policy determines how the OS writes pages out to disk. Of course, they could simply be written out one at a time; however, man y systems instead collect a number of pending writes together in m emory and write them to disk in one (more efﬁcient) write. This behavi or is usually called clustering or simply grouping of writes, and is effective because of the nature of disk drives, which perform a single larg e write more efﬁciently than many small ones. 22.11 Thrashing Before closing, we address one ﬁnal question: what should the OS do when memory is simply oversubscribed, and the memory demands of t he set of running processes simply exceeds the available physica l memory? In this case, the system will constantly be paging, a condition s ometimes referred to as thrashing [D70]. Some earlier operating systems had a fairly sophisticated set of m ech- anisms to both detect and cope with thrashing when it took place. F or example, given a set of processes, a system could decide not to run a sub- set of processes, with the hope that the reduced set of processes’ working sets (the pages that they are using actively) ﬁt in memory and thus c an make progress. This approach, generally known as admission control , states that it is sometimes better to do less work well than to tr y to do everything at once poorly, a situation we often encounter in real li fe as well as in modern computer systems (sadly). Some current systems take more a draconian approach to memory overload. For example, some versions of Linux run an out-of-memory killer when memory is oversubscribed; this daemon chooses a memory- intensive process and kills it, thus reducing memory in a none-t oo-subtle manner. While successful at reducing memory pressure, this a pproach can have problems, if, for example, it kills the X server and thu s renders any applications requiring the display unusable.",3831
22. Swapping Policies,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 15 22.12 Summary We have seen the introduction of a number of page-replacement (an d other) policies, which are part of the VM subsystem of all modern ope rat- ing systems. Modern systems add some tweaks to straightforward LRU approximations like clock; for example, scan resistance is an important part of many modern algorithms, such as ARC [MM03]. Scan-resista nt al- gorithms are usually LRU-like but also try to avoid the worst-ca se behav- ior of LRU, which we saw with the looping-sequential workload. Thus , the evolution of page-replacement algorithms continues. However, in many cases the importance of said algorithms has de- creased, as the discrepancy between memory-access and disk- access times has increased. Because paging to disk is so expensive, the cos t of frequent paging is prohibitive. Thus, the best solution to excessive pag ing is often a simple (if intellectually unsatisfying) one: buy more memory . c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 BEYOND PHYSICAL MEMORY : POLICIES References [AD03] “Run-Time Adaptation in River” by Remzi H. Arpaci-Dusseau . ACM TOCS, 21:1, February 2003. A summary of one of the authors’ dissertation work on a system named River, where he learned that comparison against the ideal is an important technique for syste m designers. [B66] “A Study of Replacement Algorithms for Virtual-Storage Comp uter” by Laszlo A. Be- lady. IBM Systems Journal 5(2): 78-101, 1966. The paper that introduces the simple way to compute the optimal behavior of a policy (the MIN algorithm). [BNS69] “An Anomaly in Space-time Characteristics of Certain Progra ms Running in a Paging Machine” by L. A. Belady, R. A. Nelson, G. S. Shedler. Communications of the ACM, 12:6, June 1969. Introduction of the little sequence of memory references known as Belady’s An omaly. How do Nelson and Shedler feel about this name, we wonder? [CD85] “An Evaluation of Buffer Management Strategies for Relatio nal Database Systems” by Hong-Tai Chou, David J. DeWitt. VLDB ’85, Stockholm, Sweden, Augu st 1985. A famous database paper on the different buffering strategies you should use under a number of common database access patterns. The more general lesson: if you know something about a workload, you can tailor policies to do better than the general-purpose ones usually found in the OS. [C69] “A Paging Experiment with the Multics System” by F.J. Corbato. I ncluded in a Festschrift published in honor of Prof. P .M. Morse. MIT Press, Cambridge, MA, 1969. The original (and hard to ﬁnd.) reference to the clock algorithm, though not the ﬁrst usage of a us e bit. Thanks to H. Balakrishnan of MIT for digging up this paper for us. [D70] “Virtual Memory” by Peter J. Denning. Computing Surveys, Vol. 2 , No. 3, September 1970. Denning’s early and famous survey on virtual memory systems. [EF78] “Cold-start vs. Warm-start Miss Ratios” by Malcolm C. Easto n, Ronald Fagin. Commu- nications of the ACM, 21:10, October 1978.",3049
22. Swapping Policies,"A good discussion of cold- vs. warm-start misses. [FP89] “Electrochemically Induced Nuclear Fusion of Deuterium” by Mar tin Fleischmann, Stanley Pons. Journal of Electroanalytical Chemistry, Volume 26, Num ber 2, Part 1, April, 1989. The famous paper that would have revolutionized the world in providing an easy w ay to generate nearly-inﬁnite power from jars of water with a little metal in them. Unfortunately , the results pub- lished (and widely publicized) by Pons and Fleischmann were impossi ble to reproduce, and thus these two well-meaning scientists were discredited (and certainly, mocked). Th e only guy really happy about this result was Marvin Hawkins, whose name was left off this paper even thoug h he participated in the work, thus avoiding association with one of the biggest scientiﬁc goofs of the 20th c entury. [HP06] “Computer Architecture: A Quantitative Approach” by John Hennessy and David Patterson. Morgan-Kaufmann, 2006. A marvelous book about computer architecture. Read it. [H87] “Aspects of Cache Memory and Instruction Buffer Performance” by Mark D . Hill. Ph.D. Dissertation, U.C. Berkeley, 1987. Mark Hill, in his dissertation work, introduced the Three C’s, which later gained wide popularity with its inclusion in H&P [HP06]. The q uote from therein: “I have found it useful to partition misses ... into three components intuitively b ased on the cause of the misses (page 49).” [KE+62] “One-level Storage System” by T. Kilburn, D.B.G. Edwards , M.J. Lanigan, F.H. Sum- ner. IRE Trans. EC-11:2, 1962. Although Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the use bits in large memories was not a problem the auth ors solved. [M+70] “Evaluation Techniques for Storage Hierarchies” by R. L. Matts on, J. Gecsei, D. R. Slutz, I. L. Traiger. IBM Systems Journal, Volume 9:2, 1970. A paper that is mostly about how to simulate cache hierarchies efﬁciently; certainly a classic in that reg ard, as well for its excellent discussion of some of the properties of various replacement algorithms. Can you ﬁgure out w hy the stack property might be useful for simulating a lot of different-sized caches at once? [MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache” by Nimrod Megid do and Dharmendra S. Modha. FAST 2003, February 2003, San Jose, California .An excellent modern paper about replacement algorithms, which includes a new policy, ARC , that is now used in some systems. Recognized in 2014 as a “Test of Time” award winner by the storage systems community at the FAST ’14 conference. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG BEYOND PHYSICAL MEMORY : POLICIES 17 Homework (Simulation) This simulator, paging-policy.py , allows you to play around with different page-replacement policies. See the README for detai ls. Questions 1. Generate random addresses with the following arguments: -s 0 -n 10 , -s 1 -n 10 , and-s 2 -n 10 . Change the policy from FIFO, to LRU, to OPT. Compute whether each access in said address traces are hit s or misses. 2. For a cache of size 5, generate worst-case address referen ce streams for each of the following policies: FIFO, LRU, and MRU (worst-cas e reference streams cause the most misses possible. For the worst case refere nce streams, how much bigger of a cache is needed to improve performance dramati cally and approach OPT? 3. Generate a random trace (use python or perl). How would you exp ect the different policies to perform on such a trace? 4. Now generate a trace with some locality. How can you generat e such a trace? How does LRU perform on it? How much better than RAND is LRU? How does CLOCK do? How about CLOCK with different numbers of clock bits? 5. Use a program like valgrind to instrument a real application and gen- erate a virtual page reference stream. For example, running valgrind --tool=lackey --trace-mem=yes ls will output a nearly-complete reference trace of every instruction and data reference made b y the program ls. To make this useful for the simulator above, you’ll have to ﬁrst tra ns- form each virtual memory reference into a virtual page-number refe rence (done by masking off the offset and shifting the resulting bits downward). How big of a cache is needed for your application trace in order to satisfy a large fraction of requests? Plot a graph of its working set as t he size of the cache increases. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4436
23. Complete VM Systems,"23 Complete Virtual Memory Systems Before we end our study of virtualizing memory, let us take a closer look at how entire virtual memory systems are put together. We’ve seen key elements of such systems, including numerous page-table desi gns, inter- actions with the TLB (sometimes, even handled by the OS itself) , and strategies for deciding which pages to keep in memory and which to kick out. However, there are many other features that comprise a comple te virtual memory system, including numerous features for perform ance, functionality, and security. And thus, our crux: THECRUX: HOWTOBUILD A C OMPLETE VM S YSTEM What features are needed to realize a complete virtual memory s ys- tem? How do they improve performance, increase security, or other wise improve the system? We’ll do this by covering two systems. The ﬁrst is one of the earli- est examples of a “modern” virtual memory manager, that found in t he V AX/VMS operating system [LL82], as developed in the 1970’s and early 1980’s; a surprising number of techniques and approaches from th is sys- tem survive to this day, and thus it it is well worth studying. Som e ideas, even those that are 50 years old, are still worth knowing, a thought that is well known to those in most other ﬁelds (e.g., Physics), but has to be stated in technology-driven disciplines (e.g., Computer Scien ce). The second is that of Linux , for reasons that should be obvious. Linux is a widely used system, and runs effectively on systems as sma ll and underpowered as phones to the most scalable multicore systems fou nd in modern datacenters. Thus, its VM system must be ﬂexible enou gh to run successfully in all of those scenarios. We will discuss each system to illustrate how concepts brought forth in earlier chapters come tog ether in a complete memory manager. 1 2 C OMPLETE VIRTUAL MEMORY SYSTEMS 23.1 VAX/VMS Virtual Memory The VAX-11 minicomputer architecture was introduced in the la te 1970’s byDigital Equipment Corporation (DEC ). DEC was a massive player in the computer industry during the era of the mini-computer; un fortu- nately, a series of bad decisions and the advent of the PC slowly (b ut surely) led to their demise [C03]. The architecture was real ized in a num- ber of implementations, including the VAX-11/780 and the less powerful VAX-11/750. The OS for the system was known as VAX/VMS (or just plain VMS), one of whose primary architects was Dave Cutler, who later led th e effort to develop Microsoft’s Windows NT [C93]. VMS had the general prob- lem that it would be run on a broad range of machines, including ver y inexpensive VAXen (yes, that is the proper plural) to extreme ly high-end and powerful machines in the same architecture family. Thus, the OS had to have mechanisms and policies that worked (and worked well) ac ross this huge range of systems. As an additional issue, VMS is an excellent example of software i nno- vations used to hide some of the inherent ﬂaws of the architecture . Al- though the OS often relies on the hardware to build efﬁcient abst ractions and illusions, sometimes the hardware designers don’t quite get every- thing right; in the VAX hardware, we’ll see a few examples of thi s, and what the VMS operating system does to build an effective, workin g sys- tem despite these hardware ﬂaws. Memory Management Hardware The VAX-11 provided a 32-bit virtual address space per process , divided into 512-byte pages. Thus, a virtual address consisted of a 23- bit VPN and a 9-bit offset. Further, the upper two bits of the VPN were us ed to differentiate which segment the page resided within; thus, the system was a hybrid of paging and segmentation, as we saw previously.",3690
23. Complete VM Systems,"The lower-half of the address space was known as “process space” a nd is unique to each process. In the ﬁrst half of process space (known asP0), the user program is found, as well as a heap which grows downward. In the second half of process space ( P1), we ﬁnd the stack, which grows upwards. The upper-half of the address space is known as system space (S), although only half of it is used. Protected OS code and data resid e here, and the OS is in this way shared across processes. One major concern of the VMS designers was the incredibly small s ize of pages in the VAX hardware (512 bytes). This size, chosen for hi storical reasons, has the fundamental problem of making simple linear pa ge ta- bles excessively large. Thus, one of the ﬁrst goals of the VMS desi gners was to ensure that VMS would not overwhelm memory with page tables . The system reduced the pressure page tables place on memory in t wo ways. First, by segmenting the user address space into two, th e VAX-11 provides a page table for each of these regions ( P0andP1) per process; OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 3 ASIDE : THECURSE OFGENERALITY Operating systems often have a problem known as the curse of gener- ality , where they are tasked with general support for a broad class of applications and systems. The fundamental result of the curse is that the OS is not likely to support any one installation very well. In the c ase of VMS, the curse was very real, as the VAX-11 architecture was re alized in a number of different implementations. It is no less real toda y, where Linux is expected to run well on your phone, a TV set-top box, a laptop computer, desktop computer, and a high-end server running thous ands of processes in a cloud-based datacenter. thus, no page-table space is needed for the unused portion of the a ddress space between the stack and the heap. The base and bounds regis ters are used as you would expect; a base register holds the address of t he page table for that segment, and the bounds holds its size (i.e., number of page-table entries). Second, the OS reduces memory pressure even further by placing u ser page tables (for P0andP1, thus two per process) in kernel virtual mem- ory. Thus, when allocating or growing a page table, the kernel all ocates space out of its own virtual memory, in segment S. If memory comes un- der severe pressure, the kernel can swap pages of these page ta bles out to disk, thus making physical memory available for other uses. Putting page tables in kernel virtual memory means that addre ss trans- lation is even further complicated. For example, to translate a virtual ad- dress inP0orP1, the hardware has to ﬁrst try to look up the page-table entry for that page in its page table (the P0orP1page table for that pro- cess); in doing so, however, the hardware may ﬁrst have to consult the system page table (which lives in physical memory); with that transla- tion complete, the hardware can learn the address of the page of th e page table, and then ﬁnally learn the address of the desired memory a ccess.",3104
23. Complete VM Systems,"All of this, fortunately, is made faster by the VAX’s hardware-m anaged TLBs, which usually (hopefully) circumvent this laborious looku p. A Real Address Space One neat aspect of studying VMS is that we can see how a real addre ss space is constructed (Figure 23.1. Thus far, we have assumed a simple address space of just user code, user data, and user heap, but as we can see above, a real address space is notably more complex. For example, the code segment never begins at page 0. This page, instead, is marked inaccessible, in order to provide some suppor t for de- tecting null-pointer accesses. Thus, one concern when designing an ad- dress space is support for debugging, which the inaccessible z ero page provides here in some form. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 C OMPLETE VIRTUAL MEMORY SYSTEMS Page 0: Invalid User Code User Heap User Stack Trap Tables Kernel Data Kernel Code Kernel Heap UnusedSystem (S)User (P1)User (P0)0 230 231 232 Figure 23.1: The V AX/VMS Address Space Perhaps more importantly, the kernel virtual address space (i .e., its data structures and code) is a part of each user address space. O n a con- text switch, the OS changes the P0andP1registers to point to the ap- propriate page tables of the soon-to-be-run process; however, it doe s not change the Sbase and bound registers, and as a result the “same” kernel structures are mapped into each user address space. The kernel is mapped into each address space for a number of reas ons. This construction makes life easier for the kernel; when, for exa mple, the OS is handed a pointer from a user program (e.g., on a write() system call), it is easy to copy data from that pointer to its own structur es. The OS is naturally written and compiled, without worry of where the d ata it is accessing comes from. If in contrast the kernel were located entirely in physical memory, it would be quite hard to do things like swap pages of the page table to disk; if the kernel were given its own addres s space, OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 5 ASIDE : W HYNULL POINTER ACCESSES CAUSE SEGFAULTS You should now have a good understanding of exactly what happens on a null-pointer dereference. A process generates a virtual add ress of 0, by doing something like this: int*p = NULL; // set p = 0 *p = 10; // try to store 10 to virtual addr 0 The hardware tries to look up the VPN (also 0 here) in the TLB, and suf- fers a TLB miss. The page table is consulted, and the entry for VP N 0 is found to be marked invalid. Thus, we have an invalid access, which transfers control to the OS, which likely terminates the process (on U NIX systems, processes are sent a signal which allows them to react to such a fault; if uncaught, however, the process is killed). moving data between user applications and the kernel would agai n be complicated and painful. With this construction (now used widel y), the kernel appears almost as a library to applications, albeit a pr otected one.",3029
23. Complete VM Systems,"One last point about this address space relates to protection. Cl early, the OS does not want user applications reading or writing OS data or code. Thus, the hardware must support different protection leve ls for pages to enable this. The VAX did so by specifying, in protecti on bits in the page table, what privilege level the CPU must be at in ord er to access a particular page. Thus, system data and code are set to a higher level of protection than user data and code; an attempted access t o such information from user code will generate a trap into the OS, and (you guessed it) the likely termination of the offending process. Page Replacement The page table entry (PTE) in VAX contains the following bits: a v alid bit, a protection ﬁeld (4 bits), a modify (or dirty) bit, a ﬁeld res erved for OS use (5 bits), and ﬁnally a physical frame number (PFN) to st ore the location of the page in physical memory. The astute reader might n ote: noreference bit . Thus, the VMS replacement algorithm must make do without hardware support for determining which pages are activ e. The developers were also concerned about memory hogs , programs that use a lot of memory and make it hard for other programs to run. Most of the policies we have looked at thus far are susceptible to su ch hogging; for example, LRU is a global policy that doesn’t share memory fairly among processes. To address these two problems, the developers came up with the seg- mented FIFO replacement policy [RL81]. The idea is simple: each process has a maximum number of pages it can keep in memory, known as its res- ident set size (RSS ). Each of these pages is kept on a FIFO list; when a c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 C OMPLETE VIRTUAL MEMORY SYSTEMS ASIDE : EMULATING REFERENCE BITS As it turns out, you don’t need a hardware reference bit in order to g et some notion of which pages are in use in a system. In fact, in the ear ly 1980’s, Babaoglu and Joy showed that protection bits on the VAX can be used to emulate reference bits [BJ81]. The basic idea: if you w ant to gain some understanding of which pages are actively being used in a s ystem, mark all of the pages in the page table as inaccessible (but kee p around the information as to which pages are really accessible by the p rocess, perhaps in the “reserved OS ﬁeld” portion of the page table entry ). When a process accesses a page, it will generate a trap into the OS; th e OS will then check if the page really should be accessible, and if so, re vert the page to its normal protections (e.g., read-only, or read-write). At the time of a replacement, the OS can check which pages remain marked in acces- sible, and thus get an idea of which pages have not been recently used. The key to this “emulation” of reference bits is reducing overhe ad while still obtaining a good idea of page usage. The OS must not be too aggre s- sive in marking pages inaccessible, or overhead would be too high . The OS also must not be too passive in such marking, or all pages will e nd up referenced; the OS will again have no good idea which page to evi ct.",3116
23. Complete VM Systems,"process exceeds its RSS, the “ﬁrst-in” page is evicted. FIFO cle arly does not need any support from the hardware, and is thus easy to implem ent. Of course, pure FIFO does not perform particularly well, as we saw earlier. To improve FIFO’s performance, VMS introduced two second- chance lists where pages are placed before getting evicted from memory, speciﬁcally a global clean-page free list and dirty-page list . When a process Pexceeds its RSS, a page is removed from its per-process FIFO; if cle an (not modiﬁed), it is placed on the end of the clean-page list; if di rty (mod- iﬁed), it is placed on the end of the dirty-page list. If another process Qneeds a free page, it takes the ﬁrst free page off of the global clean list. However, if the original process Pfaults on that page before it is reclaimed, Preclaims it from the free (or dirty) list, thus avoiding a costly disk access. The bigger these global second-ch ance lists are, the closer the segmented FIFO algorithm performs to LRU [RL 81]. Another optimization used in VMS also helps overcome the small pag e size in VMS. Speciﬁcally, with such small pages, disk I/O durin g swap- ping could be highly inefﬁcient, as disks do better with large transfers. To make swapping I/O more efﬁcient, VMS adds a number of optimiza - tions, but most important is clustering . With clustering, VMS groups large batches of pages together from the global dirty list, and wr ites them to disk in one fell swoop (thus making them clean). Clustering is used in most modern systems, as the freedom to place pages anywhere wi thin swap space lets the OS group pages, perform fewer and bigger wri tes, and thus improve performance. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 7 Other Neat Tricks VMS had two other now-standard tricks: demand zeroing and copy-on - write. We now describe these lazy optimizations. One form of laziness in VMS (and most modern systems) is demand zeroing of pages. To un- derstand this better, let’s consider the example of adding a pag e to your address space, say in your heap. In a naive implementation, the OS re- sponds to a request to add a page to your heap by ﬁnding a page in ph ys- ical memory, zeroing it (required for security; otherwise you’d be able to see what was on the page from when some other process used it.), and then mapping it into your address space (i.e., setting up the p age table to refer to that physical page as desired). But the naive implem entation can be costly, particularly if the page does not get used by the proces s. With demand zeroing, the OS instead does very little work when th e page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or write s the page, a trap into the OS takes place. When handling the trap, the OS n otices (usually through some bits marked in the “reserved for OS” portion of the page table entry) that this is actually a demand-zero page; a t this point, the OS does the needed work of ﬁnding a physical page, zeroing it, a nd mapping it into the process’s address space. If the process neve r accesses the page, all such work is avoided, and thus the virtue of demand z eroing.",3240
23. Complete VM Systems,"Another cool optimization found in VMS (and again, in virtually eve ry modern OS) is copy-on-write (COW for short). The idea, which goes at least back to the TENEX operating system [BB+72], is simple: w hen the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the target address space and mark it read- only in both address spaces. If both address spaces only read the p age, no further action is taken, and thus the OS has realized a fast copy without actually moving any data. If, however, one of the address spaces does indeed try to write to t he page, it will trap into the OS. The OS will then notice that the pa ge is a COW page, and thus (lazily) allocate a new page, ﬁll it with the data, and map this new page into the address space of the faulting process . The process then continues and now has its own private copy of the page. COW is useful for a number of reasons. Certainly any sort of shared library can be mapped copy-on-write into the address spaces of m any processes, saving valuable memory space. In U NIX systems, COW is even more critical, due to the semantics of fork() andexec() . As you might recall, fork() creates an exact copy of the address space of the caller; with a large address space, making such a copy is sl ow and data intensive. Even worse, most of the address space is immedia tely over-written by a subsequent call to exec() , which overlays the calling process’s address space with that of the soon-to-be-exec’d program. By instead performing a copy-on-write fork() , the OS avoids much of the needless copying and thus retains the correct semantics while improving performance. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 C OMPLETE VIRTUAL MEMORY SYSTEMS TIP: BELAZY Being lazy can be a virtue in both life as well as in operating sys tems. Laziness can put off work until later, which is beneﬁcial withi n an OS for a number of reasons. First, putting off work might reduce the late ncy of the current operation, thus improving responsiveness; for examp le, op- erating systems often report that writes to a ﬁle succeeded imm ediately, and only write them to disk later in the background. Second, and mor e importantly, laziness sometimes obviates the need to do the work at all; for example, delaying a write until the ﬁle is deleted removes t he need to do the write at all. Laziness is also good in life: for example, by putting off your OS project, you may ﬁnd that the project speciﬁcation bugs a re worked out by your fellow classmates; however, the class project is un- likely to get canceled, so being too lazy may be problematic, le ading to a late project, bad grade, and a sad professor. Don’t make professors s ad. 23.2 The Linux Virtual Memory System We’ll now discuss some of the more interesting aspects of the Linux VM system. Linux development has been driven forward by real en gi- neers solving real problems encountered in production, and thus a large number of features have slowly been incorporated into what is now a fully functional, feature-ﬁlled virtual memory system. While we won’t be able to discuss every aspect of Linux VM, we’ll touch on the most important ones, especially where it has gone beyond what is found in classic VM systems such as VAX/VMS. We’ll also tr y to highlight commonalities between Linux and older systems. For this discussion, we’ll focus on Linux for Intel x86. While Linux can and does run on many different processor architectures, Linux on x 86 is its most dominant and important deployment, and thus the focus of our attention.",3599
23. Complete VM Systems,"The Linux Address Space Much like other modern operating systems, and also like VAX/VMS, a Linux virtual address space1consists of a user portion (where user program code, stack, heap, and other parts reside) and a kernel p ortion (where kernel code, stacks, heap, and other parts reside). Lik e those other systems, upon a context switch, the user portion of the currently- running address space changes; the kernel portion is the same across proc esses. Like those other systems, a program running in user mode cannot acc ess kernel virtual pages; only by trapping into the kernel and tra nsitioning to privileged mode can such memory be accessed. 1Until recent changes, due to security threats, that is. Read the subsecti ons below about Linux security for details on this modiﬁcation. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 9 Page 0: Invalid User Code User Heap User Stack Kernel (Logical) Kernel (Virtual)User Kernel0x00000000 0xC0000000 Figure 23.2: The Linux Address Space In classic 32-bit Linux (i.e., Linux with a 32-bit virtual ad dress space), the split between user and kernel portions of the address space t akes place at address 0xC0000000 , or three-quarters of the way through the address space. Thus, virtual addresses 0through0xBFFFFFFF are user virtual addresses; the remaining virtual addresses ( 0xC0000000 through 0xFFFFFFFF ) are in the kernel’s virtual address space. 64-bit Linux has a similar split but at slightly different points. Figure 23.2 s hows a depiction of a typical (simpliﬁed) address space. One slightly interesting aspect of Linux is that it contains tw o types of kernel virtual addresses. The ﬁrst are known as kernel logical addresses [O16]. This is what you would consider the normal virtual address space of the kernel; to get more memory of this type, kernel code merely ne eds to callkmalloc . Most kernel data structures live here, such as page ta- bles, per-process kernel stacks, and so forth. Unlike most other memory in the system, kernel logical memory cannot be swapped to disk. The most interesting aspect of kernel logical addresses is thei r con- nection to physical memory. Speciﬁcally, there is a direct mapp ing be- tween kernel logical addresses and the ﬁrst portion of physical m emory. Thus, kernel logical address 0xC0000000 translates to physical address 0x00000000 ,0xC0000FFF to0x00000FFF , and so forth. This direct mapping has two implications. The ﬁrst is that it is simple to t ranslate back and forth between kernel logical addresses and physical a ddresses; as a result, these addresses are often treated as if they are in deed physi- cal. The second is that if a chunk of memory is contiguous in kernel l og- ical address space, it is also contiguous in physical memory. Th is makes memory allocated in this part of the kernel’s address space suita ble for operations which need contiguous physical memory to work correctly , such as I/O transfers to and from devices via directory memory access (DMA ) (something we’ll learn about in the third part of this book). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 C OMPLETE VIRTUAL MEMORY SYSTEMS The other type of kernel address is a kernel virtual address . To get memory of this type, kernel code calls a different allocator, vmalloc , which returns a pointer to a virtually contiguous region of the des ired size. Unlike kernel logical memory, kernel virtual memory is us ually not contiguous; each kernel virtual page may map to non-contiguous ph ysi- cal pages (and is thus not suitable for DMA). However, such memory is easier to allocate as a result, and thus used for large buffers w here ﬁnding a contiguous large chunk of physical memory would be challenging.",3747
23. Complete VM Systems,"In 32-bit Linux, one other reason for the existence of kernel virtu al addresses is that they enable the kernel to address more than ( roughly) 1 GB of memory. Years ago, machines had much less memory than this, a nd enabling access to more than 1 GB was not an issue. However, techn ology progressed, and soon there was a need to enable the kernel to use l arger amounts of memory. Kernel virtual addresses, and their disconne ction from a strict one-to-one mapping to physical memory, make this poss ible. However, with the move to 64-bit Linux, the need is less urgent, because the kernel is not conﬁned to only the last 1 GB of the virtual addres s space. Page Table Structure Because we are focused on Linux for x86, our discussion will center on the type of page-table structure provided by x86, as it determi nes what Linux can and cannot do. As mentioned before, x86 provides a hardwa re- managed, multi-level page table structure, with one page tab le per pro- cess; the OS simply sets up mappings in its memory, points a priv ileged register at the start of the page directory, and the hardware ha ndles the rest. The OS gets involved, as expected, at process creation, de letion, and upon context switches, making sure in each case that the correct page table is being used by the hardware MMU to perform translations . Probably the biggest change in recent years is the move from 32-b it x86 to 64-bit x86, as brieﬂy mentioned above. As seen in the VAX/ VMS system, 32-bit address spaces have been around for a long time, a nd as technology changed, they were ﬁnally starting to become a real l imit for programs. Virtual memory makes it easy to program systems, but w ith modern systems containing many GB of memory, 32 bits were no longer enough to refer to each of them. Thus, the next leap became neces sary. Moving to a 64-bit address affects page table structure in x86 in the expected manner. Because x86 uses a multi-level page table, current 64- bit systems use a four-level table. The full 64-bit nature of th e virtual address space is not yet in use, however, rather only the bottom 48 b its. Thus, a virtual address can be viewed as follows: 63 47 31 15 0 Unused P1 P2 P3 P4 Offset OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 11 As you can see in the picture, the top 16 bits of a virtual address a re unused (and thus play no role in translation), the bottom 12 bits ( due to the 4-KB page size) are used as the offset (and hence just used d irectly, and not translated), leaving the middle 36 bits of virtual addr ess to take part in the translation. The P1 portion of the address is used to in dex into the topmost page directory, and the translation proceeds from ther e, one level at a time, until the actual page of the page table is index ed by P4, yielding the desired page table entry. As system memories grow even larger, more parts of this voluminous address space will become enabled, leading to ﬁve-level and e ventually six-level page-table tree structures. Imagine that: a simp le page table lookup requiring six levels of translation, just to ﬁgure out wher e in mem- ory a certain piece of data resides.",3181
23. Complete VM Systems,"Large Page Support Intel x86 allows for the use of multiple page sizes, not just the st andard 4- KB page. Speciﬁcally, recent designs support 2-MB and even 1-G B pages in hardware. Thus, over time, Linux has evolved to allow applica tions to utilize these huge pages (as they are called in the world of Linux). Using huge pages, as hinted at earlier, leads to numerous bene ﬁts. As seen in VAX/VMS, doing so reduces the number of mappings that are needed in the page table; the larger the pages, the fewer the m appings. However, fewer page-table entries is not the driving force behi nd huge pages; rather, it’s better TLB behavior and related performanc e gains. When a process actively uses a large amount of memory, it quickly ﬁlls up the TLB with translations. If those translations are for 4 -KB pages, only a small amount of total memory can be accessed without inducing TLB misses. The result, for modern “big memory” workloads running on machines with many GBs of memory, is a noticeable performance cost ; recent research shows that some applications spend 10 percent of their c ycles servicing TLB misses [B+13]. Huge pages allow a process to access a large tract of memory with- out TLB misses, by using fewer slots in the TLB, and thus is the ma in advantage. However, there are other beneﬁts to huge pages: the re is a shorter TLB-miss path, meaning that when a TLB miss does occur, i t is serviced more quickly. In addition, allocation can be quite fast (in certain scenarios), a small but sometimes important beneﬁt. One interesting aspect of Linux support for huge pages is how it wa s done incrementally. At ﬁrst, Linux developers knew such suppor t was only important for a few applications, such as large databases wi th strin- gent performance demands. Thus, the decision was made to allow a ppli- cations to explicitly request memory allocations with large pag es (either through the mmap() orshmget() calls). In this way, most applications would be unaffected (and continue to use only 4-KB pages; a few de - manding applications would have to be changed to use these inte rfaces, but for them it would be worth the pain. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 C OMPLETE VIRTUAL MEMORY SYSTEMS TIP: CONSIDER INCREMENTALISM Many times in life, you are encouraged to be a revolutionary. “Thi nk big.”, they say. “Change the world.”, they scream. And you can see why it is appealing; in some cases, big changes are needed, and thu s pushing hard for them makes a lot of sense. And, if you try it this way, at lea st they might stop yelling at you. However, in many cases, a slower, more incremental approach migh t be the right thing to do. The Linux huge page example in this chapt er is an example of engineering incrementalism; instead of taking t he stance of a fundamentalist and insisting large pages were the way of th e future, developers took the measured approach of ﬁrst introducing special ized support for it, learning more about its upsides and downsides, and , only when there was real reason for it, adding more generic support for a ll applications. Incrementalism, while sometimes scorned, often leads to slow, t hought- ful, and sensible progress.",3208
23. Complete VM Systems,"When building systems, such an ap proach might just be the thing you need. Indeed, this may be true in lif e as well. More recently, as the need for better TLB behavior is more common among many applications, Linux developers have added transparent huge page support. When this feature is enabled, the operating syst em auto- matically looks for opportunities to allocate huge pages (usually 2 MB, but on some systems, 1 GB) without requiring application modiﬁcat ion. Huge pages are not without their costs. The biggest potential cost is internal fragmentation , i.e., a page that is large but sparsely used. This form of waste can ﬁll memory with large but little used pages. Swap ping, if enabled, also does not work well with huge pages, sometimes gre atly amplifying the amount of I/O a system does. Overhead of allocation can also be bad (in some other cases). Overall, one thing is clear : the 4- KB page size which served systems so well for so many years is not the universal solution it once was; growing memory sizes demand that w e consider large pages and other solutions as part of a necessary evol ution of VM systems. Linux’s slow adoption of this hardware-based technol ogy is evidence of the coming change. The Page Cache To reduce costs of accessing persistent storage (the focus of the t hird part of this book), most systems use aggressive caching subsystems to keep popular data items in memory. Linux, in this regard, is no diffe rent than traditional operating systems. The Linux page cache is uniﬁed, keeping pages in memory from three primary sources: memory-mapped ﬁles , ﬁle data and metadata from de- vices (usually accessed by directing read() andwrite() calls to the ﬁle system), and heap and stack pages that comprise each process (s ometimes called anonymous memory , because there is no named ﬁle underneath of OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 13 ASIDE : THEUBIQUITY OFMEMORY -MAPPING Memory mapping predates Linux by some years, and is used in many places within Linux and other modern systems. The idea is simpl e: by callingmmap() on an already opened ﬁle descriptor, a process is returned a pointer to the beginning of a region of virtual memory where the con - tents of the ﬁle seem to be located. By then using that pointer, a p rocess can access any part of the ﬁle with a simple pointer dereference . Accesses to parts of a memory-mapped ﬁle that have not yet been br ought into memory trigger page faults , at which point the OS will page in the relevant data and make it accessible by updating the page tab le of the process accordingly (i.e., demand paging ). Every regular Linux process uses memory-mapped ﬁles, even the code inmain() does not call mmap() directly, because of how Linux loads code from the executable and shared library code into memory. Bel ow is the (highly abbreviated) output of the pmap command line tool, which shows what different mapping comprise the virtual address spa ce of a running program (the shell, in this example, tcsh ). The output shows four columns: the virtual address of the mapping, its size, the p rotection bits of the region, and the source of the mapping: 0000000000400000 372K r-x-- tcsh 00000000019d5000 1780K rw--- [anon ] 00007f4e7cf06000 1792K r-x-- libc-2.23.so 00007f4e7d2d0000 36K r-x-- libcrypt-2.23.so 00007f4e7d508000 148K r-x-- libtinfo.so.5.9 00007f4e7d731000 152K r-x-- ld-2.23.so 00007f4e7d932000 16K rw--- [stack ] As you can see from this output, the code from the tcsh binary, as well as code from libc ,libcrypt ,libtinfo , and code from the dynamic linker itself ( ld.so ) are all mapped into the address space. Also present are two anonymous regions, the heap (the second entry, labeled anon ) and the stack (labeled stack ).",3784
23. Complete VM Systems,"Memory-mapped ﬁles provide a straight- forward and efﬁcient way for the OS to construct a modern address s pace. it, but rather swap space). These entities are kept in a page cache hash table , allowing for quick lookup when said data is needed. The page cache tracks if entries are clean (read but not updated) or dirty (a.k.a., modiﬁed ). Dirty data is periodically written to the back- ing store (i.e., to a speciﬁc ﬁle for ﬁle data, or to swap space for a nony- mous regions) by background threads (called pdflush ), thus ensuring that modiﬁed data eventually is written back to persistent st orage. This background activity either takes place after a certain time p eriod or if too many pages are considered dirty (both conﬁgurable parameters) . In some cases, a system runs low on memory, and Linux has to decide which pages to kick out of memory to free up space. To do so, Linux us es c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 C OMPLETE VIRTUAL MEMORY SYSTEMS a modiﬁed form of 2Qreplacement [JS94], which we describe here. The basic idea is simple: standard LRU replacement is effect ive, but can be subverted by certain common access patterns. For example , if a process repeatedly accesses a large ﬁle (especially one that i s nearly the size of memory, or larger), LRU will kick every other ﬁle out of memory . Even worse: retaining portions of this ﬁle in memory isn’t useful, a s they are never re-referenced before getting kicked out of memory. The Linux version of the 2Q replacement algorithm solves this prob - lem by keeping two lists, and dividing memory between them. Wh en accessed for the ﬁrst time, a page is placed on one queue (called A1in the original paper, but the inactive list in Linux); when it is re-referenced, the page is promoted to the other queue (called Aqin the original, but the ac- tive list in Linux). When replacement needs to take place, the candida te for replacement is taken from the inactive list. Linux also per iodically moves pages from the bottom of the active list to the inactive list, keeping the active list to about two-thirds of the total page cache size [G 04]. Linux would ideally manage these lists in perfect LRU order, bu t, as discussed in earlier chapters, doing so is costly. Thus, as wit h many OSes, an approximation of LRU (similar to clock replacement) is used. This 2Q approach generally behaves quite a bit like LRU, but not ably handles the case where a cyclic large-ﬁle access occurs by conﬁ ning the pages of that cyclic access to the inactive list. Because said pages are never re-referenced before getting kicked out of memory, they do not ﬂus h out other useful pages found in the active list. Security And Buffer Overﬂows Probably the biggest difference between modern VM systems (Li nux, So- laris, or one of the BSD variants) and ancient ones (VAX/VMS) is the emphasis on security in the modern era. Protection has always bee n a serious concern for operating systems, but with machines more in ter- connected than ever, it is no surprise that developers have imp lemented a variety of defensive countermeasures to halt those wily hacke rs from gaining control of systems. One major threat is found in buffer overﬂow attacks [W18], which can be used against normal user programs and even the kernel itself . The idea of these attacks is to ﬁnd a bug in the target system which lets t he attacker inject arbitrary data into the target’s address space.",3450
23. Complete VM Systems,"Such vu lnerabilities sometime arise because the developer assumes (erroneously) tha t an in- put will not be overly long, and thus (trustingly) copies the inpu t into a buffer; because the input is in fact too long, it overﬂows the buff er, thus overwriting memory of the target. Code as innocent as the below can b e the source of the problem: int some_function(char *input) { char dest_buffer[100]; strcpy(dest_buffer, input); // oops, unbounded copy. } OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 15 In many cases, such an overﬂow is not catastrophic, e.g., bad inpu t innocently given to a user program or even the OS will probably cau se it to crash, but no worse. However, malicious programmers can caref ully craft the input that overﬂows the buffer so as to inject their own code into the targeted system, essentially allowing them to take i t over and do their own bidding. If successful upon a network-connected use r pro- gram, attackers can run arbitrary computations or even rent out c ycles on the compromised system; if successful upon the operating system itself, the attack can access even more resources, and is a form of what is c alled privilege escalation (i.e., user code gaining kernel access rights). If you can’t guess, these are all Bad Things. The ﬁrst and most simple defense against buffer overﬂow is to pre vent execution of any code found within certain regions of an address spa ce (e.g., within the stack). The NX bit (for No-eXecute), introduced by AMD into their version of x86 (a similar XD bit is now available on Inte l’s), is one such defense; it just prevents execution from any page which has this bit set in its corresponding page table entry. The approach prev ents code, injected by an attacker into the target’s stack, from being exe cuted, and thus mitigates the problem. However, clever attackers are ... clever, and even when injec ted code cannot be added explicitly by the attacker, arbitrary code seq uences can be executed by malicious code. The idea is known, in its most gener al form, as a return-oriented programming (ROP ) [S07], and really it is quite brilliant. The observation behind ROP is that there are l ots of bits of code ( gadgets , in ROP terminology) within any program’s address space, especially C programs that link with the voluminous C library. T hus, an attacker can overwrite the stack such that the return addre ss in the currently executing function points to a desired malicious ins truction (or series of instructions), followed by a return instruction. By str inging to- gether a large number of gadgets (i.e., ensuring each return j umps to the next gadget), the attacker can execute arbitrary code. Amazi ng. To defend against ROP (including its earlier form, the return-to-libc attack [S+04]), Linux (and other systems) add another defense, known asaddress space layout randomization (ASLR ). Instead of placing code, stack, and the heap at ﬁxed locations within the virtual addres s space, the OS randomizes their placement, thus making it quite challeng ing to craft the intricate code sequence required to implement this class of attacks. Most attacks on vulnerable user programs will thus cause crashe s, but not be able to gain control of the running program.",3285
23. Complete VM Systems,"Interestingly, you can observe this randomness in practice rat her eas- ily. Here’s a piece of code that demonstrates it on a modern Linux sys tem: int main(int argc, char *argv[]) { int stack = 0; printf(\"" percentp \"", &stack); return 0; } c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 C OMPLETE VIRTUAL MEMORY SYSTEMS This code just prints out the (virtual) address of a variable on th e stack. In older non-ASLR systems, this value would be the same each time. But, as you can see below, the value changes with each run: prompt> ./random 0x7ffd3e55d2b4 prompt> ./random 0x7ffe1033b8f4 prompt> ./random 0x7ffe45522e94 ASLR is such a useful defense for user-level programs that it has also been incorporated into the kernel, in a feature unimaginative ly called ker- nel address space layout randomization (KASLR ). However, it turns out the kernel may have even bigger problems to handle, as we discu ss next. Other Security Problems: Meltdown And Spectre As we write these words (August, 2018), the world of systems secu rity has been turned upside down by two new and related attacks. The ﬁrst is called Meltdown , and the second Spectre . They were discovered at about the same time by four different groups of researchers/engi neers, and have led to deep questioning of the fundamental protections of fered by computer hardware and the OS above. See meltdownattack.com andspectreattack.com for papers describing each attack in detail. Spectre is considered the more problematic of the two. The general weakness exploited in each of these attacks is that the CPUs found in modern systems perform all sorts of crazy behind-the - scenes tricks to improve performance. One class of technique th at lies at the core of the problem is called speculative execution , in which the CPU guesses which instructions will soon be executed in the futu re, and starts executing them ahead of time. If the guesses are correct , the pro- gram runs faster; if not, the CPU undoes their effects on archite ctural state (e.g., registers) tries again, this time going down the right p ath. The problem with speculation is that it tends to leave traces of i ts ex- ecution in various parts of the system, such as processor caches, b ranch predictors, etc. And thus the problem: as the authors of the attac ks show, such state can make vulnerable the contents of memory, even memor y that we thought was protected by the MMU. One avenue to increasing kernel protection was thus to remove as much of the kernel address space from each user process and inste ad have a separate kernel page table for most kernel data (called kernel page- table isolation , orKPTI ) [G+17]. Thus, instead of mapping the kernel’s code and data structures into each process, only the barest mini mum is kept therein; when switching into the kernel, then, a switch to the kernel page table is now needed. Doing so improves security and avoids som e OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG COMPLETE VIRTUAL MEMORY SYSTEMS 17 attack vectors, but at a cost: performance.",3050
23. Complete VM Systems,"Switching page table s is costly. Ah, the costs of security: convenience andperformance. Unfortunately, KPTI doesn’t solve all of the security problems lai d out above, just some of them. And simple solutions, such as turning off s pec- ulation, would make little sense, because systems would run thou sands of times slower. Thus, it is an interesting time to be alive, if s ystems secu- rity is your thing. To truly understand these attacks, you’ll (likely) have to lea rn a lot more ﬁrst. Begin by understanding modern computer architectur e, as found in advanced books on the topic, focusing on speculation and all t he mechanisms needed to implement it. Deﬁnitely read about the M eltdown and Spectre attacks, at the websites mentioned above; they actu ally also include a useful primer on speculation, so perhaps are not a bad p lace to start. And study the operating system for further vulnerabili ties. Who knows what problems remain? 23.3 Summary You have now seen a top-to-bottom review of two virtual memory sys- tems. Hopefully, most of the details were easy to follow, as you shoul d have already had a good understanding of the basic mechanisms an d policies. More detail on VAX/VMS is available in the excellent ( and short) paper by Levy and Lipman [LL82]. We encourage you to read it, as i t is a great way to see what the source material behind these chapter s is like. You have also learned a bit about Linux. While a large and complex system, it inherits many good ideas from the past, many of which we have not had room to discuss in detail. For example, Linux performs lazy copy-on-write copying of pages upon fork() , thus lowering overheads by avoiding unnecessary copying. Linux also demand zeroes page s (us- ing memory-mapping of the /dev/zero device), and has a background swap daemon ( swapd ) that swaps pages to disk to reduce memory pres- sure. Indeed, the VM is ﬁlled with good ideas taken from the past, and also includes many of its own innovations. To learn more, check out these reasonable (but, alas, outdated) b ooks [BC05,G04]. We encourage you to read them on your own, as we can only provide the merest drop from what is an ocean of complexity. But, you’ve got to start somewhere. What is any ocean, but a multitude of drops? [M04] c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 C OMPLETE VIRTUAL MEMORY SYSTEMS References [B+13] “Efﬁcient Virtual Memory for Big Memory Servers” by A. Basu, J. Gandhi, J. Chang, M. D. Hill, M. M. Swift. ISCA ’13, June 2013, Tel-Aviv, Israel. A recent work showing that TLBs matter, consuming 10 percent of cycles for large-memory workloads. The solution: one m assive segment to hold large data sets. We go backward, so that we can go forward. [BB+72] “TENEX, A Paged Time Sharing System for the PDP-10” by D. G. Bobro w, J. D. Burch- ﬁel, D. L. Murphy, R. S. Tomlinson. CACM, Volume 15, March 1972. An early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of those; also an inspiration for other aspects of modern systems, including process management, virtual me mory, and ﬁle systems.",3110
23. Complete VM Systems,"[BJ81] “Converting a Swap-Based System to do Paging in an Architecture L acking Page-Reference Bits” by O. Babaoglu, W. N. Joy. SOSP ’81, Paciﬁc Grove, California, D ecember 1981. How to exploit existing protection machinery to emulate reference bits, from a g roup at Berkeley working on their own version of UNIX: the Berkeley Systems Distribution (BSD ). The group was inﬂuential in the development of virtual memory, ﬁle systems, and networking. [BC05] “Understanding the Linux Kernel” by D. P . Bovet, M. Cesati. O’Re illy Media, Novem- ber 2005. One of the many books you can ﬁnd on Linux, which are out of date, but still worthwhi le. [C03] “The Innovator’s Dilemma” by Clayton M. Christenson. Harper Pape rbacks, January 2003. A fantastic book about the disk-drive industry and how new innovations disrupt ex isting ones. A good read for business majors and computer scientists alike. Provides insi ght on how large and successful companies completely fail. [C93] “Inside Windows NT” by H. Custer, D. Solomon. Microsoft Press, 19 93.The book about Windows NT that explains the system top to bottom, in more detail than you might like. But seriously, a pretty good book. [G04] “Understanding the Linux Virtual Memory Manager” by M. Gorman. Prent ice Hall, 2004. An in-depth look at Linux VM, but alas a little out of date. [G+17] “KASLR is Dead: Long Live KASLR” by D. Gruss, M. Lipp, M. Schwa rz, R. Fell- ner, C. Maurice, S. Mangard. Engineering Secure Software and Systems, 20 17. Available: https://gruss.cc/files/kaiser.pdf Excellent info on KASLR, KPTI, and beyond. [JS94] “2Q: A Low Overhead High Performance Buffer Management Replacement Al gorithm” by T. Johnson, D. Shasha. VLDB ’94, Santiago, Chile. A simple but effective approach to building page replacement. [LL82] “Virtual Memory Management in the VAX/VMS Operating System” by H. Levy, P . Lipman. IEEE Computer, Volume 15:3, March 1982. Read the original source of most of this material. Particularly important if you wish to go to graduate school, where all y ou do is read papers, work, read some more papers, work more, eventually write a paper, and then work some more. [M04] “Cloud Atlas” by D. Mitchell. Random House, 2004. It’s hard to pick a favorite book. There are too many. Each is great in its own unique way. But it’d be hard for these author s not to pick “Cloud Atlas”, a fantastic, sprawling epic about the human condition, from where the th e last quote of this chapter is lifted. If you are smart – and we think you are – you should stop reading obscure commentary in the references and instead read “Cloud Atlas”; you’ll thank us later. [O16] “Virtual Memory and Linux” by A. Ott. Embedded Linux Conference, Ap ril 2016. https://events.static.linuxfound.org/sites/events/ﬁles/sli des/elc 2016 mem.pdf . A useful set of slides which gives an overview of the Linux VM. [RL81] “Segmented FIFO Page Replacement” by R. Turner, H. Levy. SIG METRICS ’81, Las Vegas, Nevada, September 1981. A short paper that shows for some workloads, segmented FIFO can approach the performance of LRU. [S07] “The Geometry of Innocent Flesh on the Bone: Return-into-libc without Function Calls (on the x86)” by H. Shacham. CCS ’07, October 2007. A generalization of return-to-libc. Dr. Beth Garner said in Basic Instinct, “She’s crazy. She’s brilliant.” We might s ay the same about ROP . [S+04] “On the Effectiveness of Address-space Randomization” by H. Shacha m, M. Page, B. Pfaff, E. J. Goh, N. Modadugu, D. Boneh. CCS ’04, October 2004. A description of the return-to- libc attack and its limits. Start reading, but be wary: the rabbit hole of system s security is deep... OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",3695
24. Summary Dialogue on Memory Virtualization,"24 Summary Dialogue on Memory Virtualization Student: (Gulps) Wow, that was a lot of material. Professor: Yes, and? Student: Well, how am I supposed to remember it all? You know, for the exam? Professor: Goodness, I hope that’s not why you are trying to remember it. Student: Why should I then? Professor: Come on, I thought you knew better. You’re trying to learn some- thing here, so that when you go off into the world, you’ll understand how systems actually work. Student: Hmm... can you give an example? Professor: Sure. One time back in graduate school, my friends and I were measuring how long memory accesses took, and once in a while the num bers were way higher than we expected; we thought all the data was ﬁttin g nicely into the second-level hardware cache, you see, and thus should have been really fast to access. Student: (nods) Professor: We couldn’t ﬁgure out what was going on. So what do you do in such a case? Easy, ask a professor. So we went and asked one of our pro fessors, who looked at the graph we had produced, and simply said “TLB”. Aha. Of course, TLB misses. Why didn’t we think of that? Having a good model of how v irtual memory works helps diagnose all sorts of interesting performance p roblems. Student: I think I see. I’m trying to build these mental models of how things work, so that when I’m out there working on my own, I won’t be surp rised when a system doesn’t quite behave as expected. I should even be able t o anticipate how the system will work just by thinking about it. Professor: Exactly. So what have you learned? What’s in your mental model of how virtual memory works? Student: Well, I think I now have a pretty good idea of what happens when memory is referenced by a process, which, as you’ve said many times, happens 1 2 S UMMARY DIALOGUE ON MEMORY VIRTUALIZATION on each instruction fetch as well as explicit loads and stores. Professor: Sounds good — tell me more. Student: Well, one thing I’ll always remember is that the addresses we see in a user program, written in C for example... Professor: What other language is there? Student: (continuing) ... Yes, I know you like C. So do I. Anyhow, as I was saying, I now really know that all addresses that we can observe wit hin a program are virtual addresses; that I, as a programmer, am just given this illusion of where data and code are in memory. I used to think it was cool that I could p rint the address of a pointer, but now I ﬁnd it frustrating — it’s just a virtual a ddress. I can’t see the real physical address where the data lives. Professor: Nope, the OS deﬁnitely hides that from you. What else? Student: Well, I think the TLB is a really key piece, providing the system with a small hardware cache of address translations. Page tables are u sually quite large and hence live in big and slow memories. Without that TLB, progra ms would certainly run a great deal more slowly. Seems like the TLB truly m akes virtualizing memory possible. I couldn’t imagine building a system withou t one. And I shudder at the thought of a program with a working set that e xceeds the coverage of the TLB: with all those TLB misses, it would be hard to wa tch. Professor: Yes, cover the eyes of the children. Beyond the TLB, what did you learn? Student: I also now understand that the page table is one of those data stru ctures you need to know about; it’s just a data structure, though, and t hat means almost any structure could be used. We started with simple structures, lik e arrays (a.k.a. linear page tables), and advanced all the way up to multi-level tables (which look like trees), and even crazier things like pageable page tables in kerne l virtual memory. All to save a little space in memory. Professor: Indeed. Student: And here’s one more important thing: I learned that the address t rans- lation structures need to be ﬂexible enough to support what progra mmers want to do with their address spaces. Structures like the multi-level tab le are perfect in this sense; they only create table space when the user needs a po rtion of the address space, and thus there is little waste. Earlier attempts, like the simple base and bounds register, just weren’t ﬂexible enough; the structures need to match what users expect and want out of their virtual memory system. Professor: That’s a nice perspective. What about all of the stuff we learned about swapping to disk? Student: Well, it’s certainly fun to study, and good to know how page replace- ment works. Some of the basic policies are kind of obvious (like LRU, for ex- ample), but building a real virtual memory system seems more intere sting, like we saw in the VMS case study. But somehow, I found the mechanisms m ore interesting, and the policies less so. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUMMARY DIALOGUE ON MEMORY VIRTUALIZATION 3 Professor: Oh, why is that? Student: Well, as you said, in the end the best solution to policy problems is simple: buy more memory. But the mechanisms you need to understa nd to know how stuff really works. Speaking of which... Professor: Yes? Student: Well, my machine is running a little slowly these days... and memory certainly doesn’t cost that much... Professor: Oh ﬁne, ﬁne. Here’s a few bucks. Go and get yourself some DRAM, cheapskate. Student: Thanks professor. I’ll never swap to disk again — or, if I do, at least I’ll know what’s actually going on. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",5449
Part II Concurrency,Part II Concurrency 1,21
25. A Dialogue on Concurrency,"25 A Dialogue on Concurrency Professor: And thus we reach the second of our three pillars of operating sys- tems: concurrency . Student: I thought there were four pillars...? Professor: Nope, that was in an older version of the book. Student: Umm... OK. So what is concurrency, oh wonderful professor? Professor: Well, imagine we have a peach — Student: (interrupting) Peaches again. What is it with you and peaches? Professor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, “Do I dare to eat a peach”, and all that fun stuff? Student: Oh yes. In English class in high school. Great stuff. I really liked the part where — Professor: (interrupting) This has nothing to do with that — I just like peaches. Anyhow, imagine there are a lot of peaches on a table, and a lot of peo ple who wish to eat them. Let’s say we did it this way: each eater ﬁrst identiﬁes a peach visually, and then tries to grab it and eat it. What is wrong with this app roach? Student: Hmmm... seems like you might see a peach that somebody else also sees. If they get there ﬁrst, when you reach out, no peach for you . Professor: Exactly. So what should we do about it? Student: Well, probably develop a better way of going about this. Maybe form a line, and when you get to the front, grab a peach and get on with it. Professor: Good. But what’s wrong with your approach? Student: Sheesh, do I have to do all the work? Professor: Yes. Student: OK, let me think. Well, we used to have many people grabbing for peaches all at once, which is faster. But in my way, we just go one at a t ime, which is correct, but quite a bit slower. The best kind of approach wo uld be fast and correct, probably. 3 4 A D IALOGUE ON CONCURRENCY Professor: You are really starting to impress. In fact, you just told us everythin g we need to know about concurrency. Well done. Student: I did? I thought we were just talking about peaches. Remember, this is usually the part where you make it about computers again. Professor: Indeed. My apologies. One must never forget the concrete. Well, as it turns out, there are certain types of programs that we call multi-threaded applications; each thread is kind of like an independent agent running around in this program, doing things on the program’s behalf. But these thr eads access memory, and for them, each spot of memory is kind of like one of those peaches. If we don’t coordinate access to memory between threads, the pro gram won’t work as expected. Make sense? Student: Kind of. But why do we talk about this in an OS class? Isn’t that just application programming? Professor: Good question. A few reasons, actually. First, the OS must support multi-threaded applications with primitives such as locks andcondition vari- ables , which we’ll talk about soon. Second, the OS itself was the ﬁrst concu rrent program — it must access its own memory very carefully or many stran ge and terrible things will happen. Really, it can get quite grisly. Student: I see. Sounds interesting. There are more details, I imagine? Professor: Indeed there are... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3114
26. Concurrency and Threads,"26 Concurrency: An Introduction Thus far, we have seen the development of the basic abstractions that the OS performs. We have seen how to take a single physical CPU and tu rn it into multiple virtual CPUs , thus enabling the illusion of multiple pro- grams running at the same time. We have also seen how to create t he illusion of a large, private virtual memory for each process; this abstrac- tion of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk). In this note, we introduce a new abstraction for a single running p ro- cess: that of a thread . Instead of our classic view of a single point of execution within a program (i.e., a single PC where instruction s are be- ing fetched from and executed), a multi-threaded program has more than one point of execution (i.e., multiple PCs, each of which is being f etched and executed from). Perhaps another way to think of this is that e ach thread is very much like a separate process, except for one diffe rence: they share the same address space and thus can access the same data. The state of a single thread is thus very similar to that of a proce ss. It has a program counter (PC) that tracks where the program is fet ch- ing instructions from. Each thread has its own private set of regi sters it uses for computation; thus, if there are two threads that are run ning on a single processor, when switching from running one (T1) to runni ng the other (T2), a context switch must take place. The context switch between threads is quite similar to the context switch between process es, as the register state of T1 must be saved and the register state of T2 re stored before running T2. With processes, we saved state to a process control block (PCB) ; now, we’ll need one or more thread control blocks (TCBs) to store the state of each thread of a process. There is one major diff erence, though, in the context switch we perform between threads as compa red to processes: the address space remains the same (i.e., there is no need to switch which page table we are using). One other major difference between threads and processes concer ns the stack. In our simple model of the address space of a classic proc ess (which we can now call a single-threaded process), there is a single stack, usually residing at the bottom of the address space (Figure 26.1 , left). 1 2 C ONCURRENCY : ANINTRODUCTION 16KB15KB2KB1KB0KB Stack(free)HeapProgram Codethe code segment: where instructions live the heap segment: contains malloc’d data dynamic data structures (it grows downward) (it grows upward) the stack segment: contains local variables arguments to routines,  return values, etc. 16KB15KB2KB1KB0KB Stack (1)Stack (2)(free) (free)HeapProgram Code Figure 26.1: Single-Threaded And Multi-Threaded Address Spaces However, in a multi-threaded process, each thread runs indepe ndently and of course may call into various routines to do whatever work it i s do- ing. Instead of a single stack in the address space, there will be one per thread. Let’s say we have a multi-threaded process that has two threads in it; the resulting address space looks different (Figure 26.",3249
26. Concurrency and Threads,"1, right). In this ﬁgure, you can see two stacks spread throughout the addre ss space of the process. Thus, any stack-allocated variables, par ameters, re- turn values, and other things that we put on the stack will be pla ced in what is sometimes called thread-local storage, i.e., the stack of the rele- vant thread. You might also notice how this ruins our beautiful address space l ay- out. Before, the stack and heap could grow independently and troub le only arose when you ran out of room in the address space. Here, we no longer have such a nice situation. Fortunately, this is usual ly OK, as stacks do not generally have to be very large (the exception bei ng in pro- grams that make heavy use of recursion). 26.1 Why Use Threads? Before getting into the details of threads and some of the problems you might have in writing multi-threaded programs, let’s ﬁrst ans wer a more simple question. Why should you use threads at all? As it turns out, there are at least two major reasons you should use threads. The ﬁrst is simple: parallelism . Imagine you are writing a pro- gram that performs operations on very large arrays, for example, a dding two large arrays together, or incrementing the value of each ele ment in the array by some amount. If you are running on just a single proces- sor, the task is straightforward: just perform each operation and be done. However, if you are executing the program on a system with multipl e OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 3 processors, you have the potential of speeding up this process consi der- ably by using the processors to each perform a portion of the work. The task of transforming your standard single-threaded program into a pro- gram that does this sort of work on multiple CPUs is called paralleliza- tion, and using a thread per CPU to do this work is a natural and typic al way to make programs run faster on modern hardware. The second reason is a bit more subtle: to avoid blocking program progress due to slow I/O. Imagine that you are writing a program th at performs different types of I/O: either waiting to send or recei ve a mes- sage, for an explicit disk I/O to complete, or even (implicitly) for a page fault to ﬁnish. Instead of waiting, your program may wish to do som e- thing else, including utilizing the CPU to perform computation , or even issuing further I/O requests. Using threads is a natural way to avoid getting stuck; while one thread in your program waits (i.e., is b locked waiting for I/O), the CPU scheduler can switch to other threads , which are ready to run and do something useful. Threading enables overlap of I/O with other activities within a single program, much like multipro- gramming did for processes across programs; as a result, many modern server-based applications (web servers, database manageme nt systems, and the like) make use of threads in their implementations. Of course, in either of the cases mentioned above, you could use mult i- pleprocesses instead of threads. However, threads share an address space and thus make it easy to share data, and hence are a natural choi ce when constructing these types of programs. Processes are a more sound ch oice for logically separate tasks where little sharing of data struc tures in mem- ory is needed.",3310
26. Concurrency and Threads,"26.2 An Example: Thread Creation Let’s get into some of the details. Say we wanted to run a program that creates two threads, each of which does some independent wor k, in this case printing “A” or “B”. The code is shown in Figure 26.2 (pa ge 4). The main program creates two threads, each of which will run the functionmythread() , though with different arguments (the string Aor B). Once a thread is created, it may start running right away (d epending on the whims of the scheduler); alternately, it may be put in a “r eady” but not “running” state and thus not run yet. Of course, on a multiproce ssor, the threads could even be running at the same time, but let’s not w orry about this possibility quite yet. After creating the two threads (let’s call them T1 and T2), the main thread calls pthread join() , which waits for a particular thread to complete. It does so twice, thus ensuring T1 and T2 will run and c om- plete before ﬁnally allowing the main thread to run again; when it does, it will print “main: end” and exit. Overall, three threads we re employed during this run: the main thread, T1, and T2. Let us examine the possible execution ordering of this little prog ram. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 4 C ONCURRENCY : ANINTRODUCTION 1#include <stdio.h> 2#include <assert.h> 3#include <pthread.h> 4#include \""common.h\"" 5#include \""common_threads.h\"" 6 7void*mythread(void *arg) { 8printf(\"" percents \"", (char *) arg); 9return NULL; 10} 11 12int 13main(int argc, char *argv[]) { 14pthread_t p1, p2; 15int rc; 16printf(\""main: begin \""); 17Pthread_create(&p1, NULL, mythread, \""A\""); 18Pthread_create(&p2, NULL, mythread, \""B\""); 19// join waits for the threads to finish 20Pthread_join(p1, NULL); 21Pthread_join(p2, NULL); 22printf(\""main: end \""); 23return 0; 24} Figure 26.2: Simple Thread Creation Code (t0.c) In the execution diagram (Figure 26.3, page 5), time increase s in the down- wards direction, and each column shows when a different thread ( the main one, or Thread 1, or Thread 2) is running. Note, however, that this ordering is not the only possible ordering. In fact, given a sequence of instructions, there are quite a few, d epending on which thread the scheduler decides to run at a given point. For e xample, once a thread is created, it may run immediately, which would le ad to the execution shown in Figure 26.4 (page 5). We also could even see “B” printed before “A”, if, say, the sched uler decided to run Thread 2 ﬁrst even though Thread 1 was created ea rlier; there is no reason to assume that a thread that is created ﬁrst w ill run ﬁrst. Figure 26.5 (page 5) shows this ﬁnal execution ordering, with Th read 2 getting to strut its stuff before Thread 1. As you might be able to see, one way to think about thread creation is that it is a bit like making a function call; however, instead of ﬁrst ex- ecuting the function and then returning to the caller, the sys tem instead creates a new thread of execution for the routine that is being cal led, and it runs independently of the caller, perhaps before returning from the cre- ate, but perhaps much later.",3131
26. Concurrency and Threads,"What runs next is determined by t he OS scheduler , and although the scheduler likely implements some sensible algorithm, it is hard to know what will run at any given moment in t ime. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 5 main Thread 1 Thread2 starts running prints “main: begin” creates Thread 1 creates Thread 2 waits for T1 runs prints “A” returns waits for T2 runs prints “B” returns prints “main: end” Figure 26.3: Thread Trace (1) main Thread 1 Thread2 starts running prints “main: begin” creates Thread 1 runs prints “A” returns creates Thread 2 runs prints “B” returns waits for T1 returns immediately; T1 is done waits for T2 returns immediately; T2 is done prints “main: end” Figure 26.4: Thread Trace (2) main Thread 1 Thread2 starts running prints “main: begin” creates Thread 1 creates Thread 2 runs prints “B” returns waits for T1 runs prints “A” returns waits for T2 returns immediately; T2 is done prints “main: end” Figure 26.5: Thread Trace (3) c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 6 C ONCURRENCY : ANINTRODUCTION As you also might be able to tell from this example, threads make life complicated: it is already hard to tell what will run when. Comp uters are hard enough to understand without concurrency. Unfortunately, with concurrency, it simply gets worse. Much worse. 26.3 Why It Gets Worse: Shared Data The simple thread example we showed above was useful in showing how threads are created and how they can run in different orders d epend- ing on how the scheduler decides to run them. What it doesn’t show you , though, is how threads interact when they access shared data. Let us imagine a simple example where two threads wish to upda te a global shared variable. The code we’ll study is in Figure 26.6 (p age 7). Here are a few notes about the code. First, as Stevens suggests [SR0 5], we wrap the thread creation and join routines to simply exit on fai lure; for a program as simple as this one, we want to at least notice an err or occurred (if it did), but not do anything very smart about it (e.g ., just exit). Thus, Pthread create() simply calls pthread create() and makes sure the return code is 0; if it isn’t, Pthread create() just prints a message and exits. Second, instead of using two separate function bodies for the worker threads, we just use a single piece of code, and pass the thread a n argu- ment (in this case, a string) so we can have each thread print a different letter before its messages. Finally, and most importantly, we can now look at what each worker is trying to do: add a number to the shared variable counter , and do so 10 million times (1e7) in a loop. Thus, the desired ﬁnal result is: 2 0,000,000. We now compile and run the program, to see how it behaves. Some- times, everything works how we might expect: prompt> gcc -o main main.c -Wall -pthread prompt> ./main main: begin (counter = 0) A: begin B: begin A: done B: done main: done with both (counter = 20000000) Unfortunately, when we run this code, even on a single processor, w e don’t necessarily get the desired result. Sometimes, we get: prompt> ./main main: begin (counter = 0) A: begin B: begin A: done B: done main: done with both (counter = 19345221) OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 7 1#include <stdio.h> 2#include <pthread.h> 3#include \""common.h\"" 4#include \""common_threads.h\"" 5 6static volatile int counter = 0; 7 8// 9// mythread() 10// 11// Simply adds 1 to counter repeatedly, in a loop 12// No, this is not how you would add 10,000,000 to 13// a counter, but it shows the problem nicely. 14// 15void*mythread(void *arg) { 16printf(\"" percents: begin \"", (char *) arg); 17int i; 18for (i = 0; i < 1e7; i++) { 19 counter = counter + 1; 20} 21printf(\"" percents: done \"", (char *) arg); 22return NULL; 23} 24 25// 26// main() 27// 28// Just launches two threads (pthread_create) 29// and then waits for them (pthread_join) 30// 31int main(int argc, char *argv[]) { 32pthread_t p1, p2; 33printf(\""main: begin (counter =  percentd) \"", counter); 34Pthread_create(&p1, NULL, mythread, \""A\""); 35Pthread_create(&p2, NULL, mythread, \""B\""); 36 37// join waits for the threads to finish 38Pthread_join(p1, NULL); 39Pthread_join(p2, NULL); 40printf(\""main: done with both (counter =  percentd) \"", counter); 41return 0; 42} Figure 26.6: Sharing Data: Uh Oh (t1.c) c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 8 C ONCURRENCY : ANINTRODUCTION TIP: KNOW ANDUSEYOUR TOOLS You should always learn new tools that help you write, debug, and un - derstand computer systems.",4616
26. Concurrency and Threads,"Here, we use a neat tool called a disassem- bler. When you run a disassembler on an executable, it shows you what assembly instructions make up the program. For example, if we wi sh to understand the low-level code to update a counter (as in our examp le), we runobjdump (Linux) to see the assembly code: prompt> objdump -d main Doing so produces a long listing of all the instructions in the progr am, neatly labeled (particularly if you compiled with the -gﬂag), which in- cludes symbol information in the program. The objdump program is just one of many tools you should learn how to use; a debugger like gdb, memory proﬁlers like valgrind orpurify , and of course the compiler itself are others that you should spend time to learn more about; th e better you are at using your tools, the better systems you’ll be able to buil d. Let’s try it one more time, just to see if we’ve gone crazy. After all , aren’t computers supposed to produce deterministic results, as you have been taught?. Perhaps your professors have been lying to you? (gasp) prompt> ./main main: begin (counter = 0) A: begin B: begin A: done B: done main: done with both (counter = 19221041) Not only is each run wrong, but also yields a different result. A big question remains: why does this happen? 26.4 The Heart Of The Problem: Uncontrolled Scheduling To understand why this happens, we must understand the code se - quence that the compiler generates for the update to counter . In this case, we wish to simply add a number (1) to counter . Thus, the code sequence for doing so might look something like this (in x86); mov 0x8049a1c,  percenteax add $0x1,  percenteax mov  percenteax, 0x8049a1c This example assumes that the variable counter is located at address 0x8049a1c. In this three-instruction sequence, the x86 mov instruction is used ﬁrst to get the memory value at the address and put it into r egister eax. Then, the add is performed, adding 1 (0x1) to the contents of the eax register, and ﬁnally, the contents of eax are stored back into memory at the same address. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 9 Let us imagine one of our two threads (Thread 1) enters this region of code, and is thus about to increment counter by one. It loads the value ofcounter (let’s say it’s 50 to begin with) into its register eax. Thus, eax=50 for Thread 1. Then it adds one to the register; thus eax=51 . Now, something unfortunate happens: a timer interrupt goes off; t hus, the OS saves the state of the currently running thread (its PC, its registers including eax, etc.) to the thread’s TCB. Now something worse happens: Thread 2 is chosen to run, and it en- ters this same piece of code. It also executes the ﬁrst instruct ion, getting the value of counter and putting it into its eax (remember: each thread when running has its own private registers; the registers are virtualized by the context-switch code that saves and restores them). The va lue of counter is still 50 at this point, and thus Thread 2 has eax=50 . Let’s then assume that Thread 2 executes the next two instructions, increment- ingeax by 1 (thus eax=51 ), and then saving the contents of eax into counter (address 0x8049a1c).",3217
26. Concurrency and Threads,"Thus, the global variable counter now has the value 51. Finally, another context switch occurs, and Thread 1 resumes ru nning. Recall that it had just executed the mov andadd, and is now about to perform the ﬁnal mov instruction. Recall also that eax=51 . Thus, the ﬁnal mov instruction executes, and saves the value to memory; the counte r is set to 51 again. Put simply, what has happened is this: the code to increment counter has been run twice, but counter , which started at 50, is now only equal to 51. A “correct” version of this program should have resulted in t he variablecounter equal to 52. Let’s look at a detailed execution trace to understand the problem bet- ter. Assume, for this example, that the above code is loaded at add ress 100 in memory, like the following sequence (note for those of you used t o nice, RISC-like instruction sets: x86 has variable-length in structions; this mov instruction takes up 5 bytes of memory, and the add only 3): 100 mov 0x8049a1c,  percenteax 105 add $0x1,  percenteax 108 mov  percenteax, 0x8049a1c With these assumptions, what happens is shown in Figure 26.7 (p age 10). Assume the counter starts at value 50, and trace through th is example to make sure you understand what is going on. What we have demonstrated here is called a race condition (or, more speciﬁcally, a data race ): the results depend on the timing execution of the code. With some bad luck (i.e., context switches that occur at un- timely points in the execution), we get the wrong result. In fact , we may get a different result each time; thus, instead of a nice deterministic com- putation (which we are used to from computers), we call this resu ltinde- terminate , where it is not known what the output will be and it is indeed likely to be different across runs. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 10 C ONCURRENCY : ANINTRODUCTION (after instruction) OS Thread 1 Thread 2 PC  percenteax counter before critical section 100 0 50 mov 0x8049a1c,  percenteax 105 50 50 add $0x1,  percenteax 108 51 50 interrupt save T1’s state restore T2’s state 100 0 50 mov 0x8049a1c,  percenteax 105 50 50 add $0x1,  percenteax 108 51 50 mov  percenteax, 0x8049a1c 113 51 51 interrupt save T2’s state restore T1’s state 108 51 51 mov  percenteax, 0x8049a1c 113 51 51 Figure 26.7: The Problem: Up Close and Personal Because multiple threads executing this code can result in a r ace con- dition, we call this code a critical section . A critical section is a piece of code that accesses a shared variable (or more generally, a share d resource) and must not be concurrently executed by more than one thread. What we really want for this code is what we call mutual exclusion . This property guarantees that if one thread is executing withi n the critical section, the others will be prevented from doing so. Virtually all of these terms, by the way, were coined by Edsger D ijk- stra, who was a pioneer in the ﬁeld and indeed won the Turing Awar d because of this and other work; see his 1968 paper on “Cooperating Se- quential Processes” [D68] for an amazingly clear description of the prob- lem.",3132
26. Concurrency and Threads,"We’ll be hearing more about Dijkstra in this section of the book. 26.5 The Wish For Atomicity One way to solve this problem would be to have more powerful in- structions that, in a single step, did exactly whatever we nee ded done and thus removed the possibility of an untimely interrupt. For ex ample, what if we had a super instruction that looked like this: memory-add 0x8049a1c, $0x1 Assume this instruction adds a value to a memory location, and the hardware guarantees that it executes atomically ; when the instruction executed, it would perform the update as desired. It could not be i nter- rupted mid-instruction, because that is precisely the guara ntee we receive from the hardware: when an interrupt occurs, either the instru ction has not run at all, or it has run to completion; there is no in-between s tate. Hardware can be a beautiful thing, no? Atomically, in this context, means “as a unit”, which sometimes we take as “all or none.” What we’d like is to execute the three instr uction sequence atomically: OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 11 TIP: USEATOMIC OPERATIONS Atomic operations are one of the most powerful underlying technique s in building computer systems, from the computer architecture, to concur- rent code (what we are studying here), to ﬁle systems (which we ’ll study soon enough), database management systems, and even distribut ed sys- tems [L+93]. The idea behind making a series of actions atomic is simply expressed with the phrase “all or nothing”; it should either appear as if al l of the ac- tions you wish to group together occurred, or that none of them occurred , with no in-between state visible. Sometimes, the grouping of man y ac- tions into a single atomic action is called a transaction , an idea devel- oped in great detail in the world of databases and transaction proc essing [GR92]. In our theme of exploring concurrency, we’ll be using synchronizat ion primitives to turn short sequences of instructions into atomic b locks of execution, but the idea of atomicity is much bigger than that, as we will see. For example, ﬁle systems use techniques such as journalin g or copy- on-write in order to atomically transition their on-disk state, c ritical for operating correctly in the face of system failures. If that doesn ’t make sense, don’t worry — it will, in some future chapter. mov 0x8049a1c,  percenteax add $0x1,  percenteax mov  percenteax, 0x8049a1c As we said, if we had a single instruction to do this, we could jus t issue that instruction and be done. But in the general case, we w on’t have such an instruction. Imagine we were building a concurrent B-t ree, and wished to update it; would we really want the hardware to suppor t an “atomic update of B-tree” instruction? Probably not, at least in a sane instruction set. Thus, what we will instead do is ask the hardware for a few usefu l instructions upon which we can build a general set of what we call syn- chronization primitives . By using this hardware support, in combina- tion with some help from the operating system, we will be able to bu ild multi-threaded code that accesses critical sections in a sync hronized and controlled manner, and thus reliably produces the correct resul t despite the challenging nature of concurrent execution.",3319
26. Concurrency and Threads,"Pretty awesome , right? This is the problem we will study in this section of the book. It is a wonderful and hard problem, and should make your mind hurt (a bit) . If it doesn’t, then you don’t understand. Keep working until your hea d hurts; you then know you’re headed in the right direction. At that p oint, take a break; we don’t want your head hurting too much. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 12 C ONCURRENCY : ANINTRODUCTION THECRUX: HOWTOSUPPORT SYNCHRONIZATION What support do we need from the hardware in order to build use- ful synchronization primitives? What support do we need from the OS? How can we build these primitives correctly and efﬁciently? How can programs use them to get the desired results? 26.6 One More Problem: Waiting For Another This chapter has set up the problem of concurrency as if only one typ e of interaction occurs between threads, that of accessing shared variables and the need to support atomicity for critical sections. As it tur ns out, there is another common interaction that arises, where one thread must wait for another to complete some action before it continues. This in ter- action arises, for example, when a process performs a disk I/O and is put to sleep; when the I/O completes, the process needs to be roused f rom its slumber so it can continue. Thus, in the coming chapters, we’ll be not only studying how to buil d support for synchronization primitives to support atomicity but a lso for mechanisms to support this type of sleeping/waking interacti on that is common in multi-threaded programs. If this doesn’t make sense rig ht now, that is OK. It will soon enough, when you read the chapter on con- dition variables . If it doesn’t by then, well, then it is less OK, and you should read that chapter again (and again) until it does make se nse. 26.7 Summary: Why in OS Class? Before wrapping up, one question that you might have is: why are we studying this in OS class? “History” is the one-word answer; the OS was the ﬁrst concurrent program, and many techniques were created for use within the OS. Later, with multi-threaded processes, application prog ram- mers also had to consider such things. For example, imagine the case where there are two processes run ning. Assume they both call write() to write to the ﬁle, and both wish to append the data to the ﬁle (i.e., add the data to the end of the ﬁl e, thus increasing its length). To do so, both must allocate a new block, r ecord in the inode of the ﬁle where this block lives, and change the size of the ﬁle to reﬂect the new larger size (among other things; we’ll lear n more about ﬁles in the third part of the book). Because an interrupt may occur at any time, the code that updates these shared structures (e. g., a bitmap for allocation, or the ﬁle’s inode) are critical sections; thus, OS d esign- ers, from the very beginning of the introduction of the interrupt, had to worry about how the OS updates internal structures. An untimely inter- rupt causes all of the problems described above.",3045
26. Concurrency and Threads,"Not surprisingl y, page tables, process lists, ﬁle system structures, and virtually every kernel data structure has to be carefully accessed, with the proper synch ronization primitives, to work correctly. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 13 ASIDE : KEYCONCURRENCY TERMS CRITICAL SECTION , RACE CONDITION , INDETERMINATE , M UTUAL EXCLUSION These four terms are so central to concurrent code that we thought it worth while to call them out explicitly. See some of Dijkstra’s earl y work [D65,D68] for more details. •Acritical section is a piece of code that accesses a shared resource, usually a variable or data structure. •Arace condition (ordata race [NM92]) arises if multiple threads of execution enter the critical section at roughly the same time; b oth attempt to update the shared data structure, leading to a sur prising (and perhaps undesirable) outcome. •Anindeterminate program consists of one or more race conditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic , something we usually expect from computer systems. •To avoid these problems, threads should use some kind of mutual exclusion primitives; doing so guarantees that only a single thread ever enters a critical section, thus avoiding races, and resul ting in deterministic program outputs. c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES 14 C ONCURRENCY : ANINTRODUCTION References [D65] “Solution of a problem in concurrent programming control” by E. W. Dijkstra. Commu- nications of the ACM, 8(9):569, September 1965. Pointed to as the ﬁrst paper of Dijkstra’s where he outlines the mutual exclusion problem and a solution. The solution, howeve r, is not widely used; advanced hardware and OS support is needed, as we will see in the coming c hapters. [D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available at this site: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.Dijkstra has an amaz- ing number of his old papers, notes, and thoughts recorded (for posterity) on thi s website at the last place he worked, the University of Texas. Much of his foundational work, howeve r, was done years earlier while he was at the Technische Hochshule of Eindhoven (THE), in cluding this famous paper on “cooperating sequential processes”, which basically outlines all of the thi nking that has to go into writ- ing multi-threaded programs. Dijkstra discovered much of this while worki ng on an operating system named after his school: the “THE” operating system (said “T”, “H”, “E”, and n ot like the word “the”). [GR92] “Transaction Processing: Concepts and Techniques” by Jim Gray and And reas Reuter. Morgan Kaufmann, September 1992. This book is the bible of transaction processing, written by one of the legends of the ﬁeld, Jim Gray. It is, for this reason, also considered Ji m Gray’s “brain dump”, in which he wrote down everything he knows about how database management system s work.",3027
26. Concurrency and Threads,"Sadly, Gray passed away tragically a few years back, and many of us lost a friend and gre at mentor, including the co-authors of said book, who were lucky enough to interact with Gray during thei r graduate school years. [L+93] “Atomic Transactions” by Nancy Lynch, Michael Merritt, William Wei hl, Alan Fekete. Morgan Kaufmann, August 1993. A nice text on some of the theory and practice of atomic transac- tions for distributed systems. Perhaps a bit formal for some, but lots of good mate rial is found herein. [NM92] “What Are Race Conditions? Some Issues and Formalizations” by Robert H. B. Netzer and Barton P . Miller. ACM Letters on Programming Languages and Syst ems, Volume 1:1, March 1992. An excellent discussion of the different types of races found in concu rrent programs. In this chapter (and the next few), we focus on data races, but later we will broaden to discuss general races as well. [SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen A. Rago. Addison-Wesley, 2005. As we’ve said many times, buy this book, and read it, in little chunks, preferably before going to bed. This way, you will actually fall as leep more quickly; more im- portantly, you learn a little more about how to become a serious UNIXprogrammer. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CONCURRENCY : ANINTRODUCTION 15 Homework (Simulation) This program, x86.py , allows you to see how different thread inter- leavings either cause or avoid race conditions. See the README for d e- tails on how the program works, then answer the questions below. Questions 1. Let’s examine a simple program, “loop.s”. First, just read and un derstand it. Then, run it with these arguments ( ./x86.py -p loop.s -t 1 -i 100 -R dx ) This speciﬁes a single thread, an interrupt every 100 instruc tions, and tracing of register  percentdx. What will  percentdx be during the run? Use the -c ﬂag to check your answers; the answers, on the left, show the va lue of the register (or memory value) after the instruction on the right has run. 2. Same code, different ﬂags: ( ./x86.py -p loop.s -t 2 -i 100 -a dx=3,dx=3 -R dx ) This speciﬁes two threads, and initializes each  percentdx to 3. What values will  percentdx see? Run with -cto check. Does the presence of multiple threads affect your calculations? Is there a race in thi s code? 3. Run this: ./x86.py -p loop.s -t 2 -i 3 -r -a dx=3,dx=3 -R dx This makes the interrupt interval small/random; use different se eds (-s) to see different interleavings. Does the interrupt frequency ch ange anything? 4. Now, a different program, looping-race-nolock.s , which accesses a shared variable located at address 2000; we’ll call this var iablevalue . Run it with a single thread to conﬁrm your understanding: ./x86.py -p looping-race-nolock.s -t 1 -M 2000 What isvalue (i.e., at mem- ory address 2000) throughout the run? Use -cto check. 5. Run with multiple iterations/threads: ./x86.py -p looping-race-nolock.s -t 2 -a bx=3 -M 2000 Why does each thread loop three times? What is ﬁnal value of value ? 6. Run with random interrupt intervals: ./x86.py -p looping-race-nolock.s -t 2 -M 2000 -i 4 -r -s 0 with different seeds ( -s 1 ,-s 2 , etc.) Can you tell by looking at the thread interleaving what the ﬁn al value of value will be? Does the timing of the interrupt matter? Where can it saf ely occur? Where not? In other words, where is the critical sectio n exactly? 7. Now examine ﬁxed interrupt intervals: ./x86.py -p looping-race-nolock.s -a bx=1 -t 2 -M 2000 -i 1 What will the ﬁnal value of the shared variablevalue be? What about when you change -i 2 ,-i 3 , etc.? For which interrupt intervals does the program give the “correct ” answer? 8. Run the same for more loops (e.g., set -a bx=100 ). What interrupt inter- vals (-i) lead to a correct outcome? Which intervals are surprising? 9. One last program: wait-for-me.s . Run:./x86.py -p wait-for-me.s -a ax=1,ax=0 -R ax -M 2000 This sets the  percentax register to 1 for thread 0, and 0 for thread 1, and watches  percentax and memory location 2000. How should the code behave? How is the value at location 2000 being us ed by the threads? What will its ﬁnal value be? 10. Now switch the inputs: ./x86.py -p wait-for-me.s -a ax=0,ax=1 -R ax -M 2000 How do the threads behave? What is thread 0 doing? How would changing the interrupt interval (e.g., -i 1000 , or perhaps to use random intervals) change the trace outcome? Is the program e fﬁciently using the CPU? c/circlecopyrt2008–19, A RPACI -DUSSEAUTHREE EASY PIECES",4543
27. Thread API,"27 Interlude: Thread API This chapter brieﬂy covers the main portions of the thread API. Ea ch part will be explained further in the subsequent chapters, as we s how how to use the API. More details can be found in various books and online sources [B89, B97, B+96, K+96]. We should note that the subseque nt chap- ters introduce the concepts of locks and condition variables more sl owly, with many examples; this chapter is thus better used as a refe rence. CRUX: HOWTOCREATE ANDCONTROL THREADS What interfaces should the OS present for thread creation and con trol? How should these interfaces be designed to enable ease of use as w ell as utility? 27.1 Thread Creation The ﬁrst thing you have to be able to do to write a multi-threade d program is to create new threads, and thus some kind of thread cre ation interface must exist. In POSIX, it is easy: #include <pthread.h> int pthread_create( pthread_t * thread, const pthread_attr_t *attr, void* (*start_routine)(void *), void* arg); This declaration might look a little complex (particularly if you haven’t used function pointers in C), but actually it’s not too bad. There a re four arguments: thread ,attr ,startroutine , andarg. The ﬁrst, thread , is a pointer to a structure of type pthread t; we’ll use this structure to interact with this thread, and thus we need to pa ss it to pthread create() in order to initialize it. 1 2 INTERLUDE : THREAD API The second argument, attr , is used to specify any attributes this thread might have. Some examples include setting the stack size or perh aps in- formation about the scheduling priority of the thread. An attribu te is initialized with a separate call to pthread attrinit() ; see the man- ual page for details. However, in most cases, the defaults will b e ﬁne; in this case, we will simply pass the value NULL in. The third argument is the most complex, but is really just askin g: which function should this thread start running in? In C, we call this afunction pointer , and this one tells us the following is expected: a function name (startroutine ), which is passed a single argument of type void*(as indicated in the parentheses after startroutine ), and which returns a value of type void*(i.e., a void pointer ). If this routine instead required an integer argument, instea d of a void pointer, the declaration would look like this: int pthread_create(..., // first two args are the same void*(*start_routine)(int), int arg); If instead the routine took a void pointer as an argument, but retur ned an integer, it would look like this: int pthread_create(..., // first two args are the same int ( *start_routine)(void *), void*arg); Finally, the fourth argument, arg, is exactly the argument to be passed to the function where the thread begins execution. You might ask : why do we need these void pointers? Well, the answer is quite simple : having a void pointer as an argument to the function startroutine allows us to pass in anytype of argument; having it as a return value allows the thread to return anytype of result.",3047
27. Thread API,"Let’s look at an example in Figure 27.1. Here we just create a thre ad that is passed two arguments, packaged into a single type we d eﬁne our- selves (myargt). The thread, once created, can simply cast its argument to the type it expects and thus unpack the arguments as desire d. And there it is. Once you create a thread, you really have another live executing entity, complete with its own call stack, runni ng within the same address space as all the currently existing threads in the pr ogram. The fun thus begins. 27.2 Thread Completion The example above shows how to create a thread. However, what happens if you want to wait for a thread to complete? You need to do something special in order to wait for completion; in particular, you must call the routine pthread join() . int pthread_join(pthread_t thread, void **value_ptr); OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : THREAD API 3 1#include <pthread.h> 2 3typedef struct __myarg_t { 4int a; 5int b; 6} myarg_t; 7 8void*mythread(void *arg) { 9myarg_t *m = (myarg_t *) arg; 10 printf(\"" percentd  percentd \"", m->a, m->b); 11 return NULL; 12} 13 14int 15main(int argc, char *argv[]) { 16 pthread_t p; 17 int rc; 18 19 myarg_t args; 20 args.a = 10; 21 args.b = 20; 22 rc = pthread_create(&p, NULL, mythread, &args); 23 ... 24} Figure 27.1: Creating a Thread This routine takes two arguments. The ﬁrst is of type pthread t, and is used to specify which thread to wait for. This variable is in itialized by the thread creation routine (when you pass a pointer to it as an arg ument topthread create() ); if you keep it around, you can use it to wait for that thread to terminate. The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is deﬁned to return a pointer to void; because the pthread join() routine changes the value of the passed in argument, you need to pass in a pointer to that val ue, not just the value itself. Let’s look at another example (Figure 27.2, page 4). In the code, a single thread is again created, and passed a couple of argument s via the myargtstructure. To return values, the myretttype is used. Once the thread is ﬁnished running, the main thread, which has bee n waiting inside of the pthread join() routine1, then returns, and we can access the values returned from the thread, namely whatever is in myrett. A few things to note about this example. First, often times we don’t have to do all of this painful packing and unpacking of argument s. For example, if we just create a thread with no arguments, we can p assNULL in as an argument when the thread is created. Similarly, we can passNULL intopthread join() if we don’t care about the return value. Second, if we are just passing in a single value (e.g., an int), w e don’t 1Note we use wrapper functions here; speciﬁcally, we call Malloc(), Pthread join(), and Pthread create(), which just call their similarly-named lower-case versions and m ake sure the routines did not return anything unexpected. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 INTERLUDE : THREAD API 1#include <stdio.h> 2#include <pthread.h> 3#include <assert.h> 4#include <stdlib.h> 5 6typedef struct __myarg_t { 7int a; 8int b; 9} myarg_t; 10 11typedef struct __myret_t { 12 int x; 13 int y; 14} myret_t; 15 16void*mythread(void *arg) { 17 myarg_t *m = (myarg_t *) arg; 18 printf(\"" percentd  percentd \"", m->a, m->b); 19 myret_t *r = Malloc(sizeof(myret_t)); 20 r->x = 1; 21 r->y = 2; 22 return (void *) r; 23} 24 25int 26main(int argc, char *argv[]) { 27 pthread_t p; 28 myret_t *m; 29 30 myarg_t args = {10, 20}; 31 Pthread_create(&p, NULL, mythread, &args); 32 Pthread_join(p, (void **) &m); 33 printf(\""returned  percentd  percentd \"", m->x, m->y); 34 free(m); 35 return 0; 36} Figure 27.2: Waiting for Thread Completion have to package it up as an argument.",3881
27. Thread API,"Figure 27.3 (page 5) shows an example. In this case, life is a bit simpler, as we don’t have to p ackage arguments and return values inside of structures. Third, we should note that one has to be extremely careful with how values are returned from a thread. In particular, never retur n a pointer which refers to something allocated on the thread’s call stack. I f you do, what do you think will happen? (think about it.) Here is an exampl e of a dangerous piece of code, modiﬁed from the example in Figure 27.3. 1void*mythread(void *arg) { 2myarg_t *m = (myarg_t *) arg; 3printf(\"" percentd  percentd \"", m->a, m->b); 4myret_t r; // ALLOCATED ON STACK: BAD. 5r.x = 1; 6r.y = 2; 7return (void *) &r; 8} OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : THREAD API 5 void*mythread(void *arg) { int m = (int) arg; printf(\"" percentd \"", m); return (void *) (arg + 1); } int main(int argc, char *argv[]) { pthread_t p; int rc, m; Pthread_create(&p, NULL, mythread, (void *) 100); Pthread_join(p, (void **) &m); printf(\""returned  percentd \"", m); return 0; } Figure 27.3: Simpler Argument Passing to a Thread In this case, the variable ris allocated on the stack of mythread . How- ever, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all.), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results. C ertainly, when you print out the values you think you returned, you’ll probably (but not necessarily.) be surprised. Try it and ﬁnd out for yoursel f2. Finally, you might notice that the use of pthread create() to create a thread, followed by an immediate call to pthread join() , is a pretty strange way to create a thread. In fact, there is an easier way to accom- plish this exact task; it’s called a procedure call . Clearly, we’ll usually be creating more than just one thread and waiting for it to complete, other- wise there is not much purpose to using threads at all. We should note that not all code that is multi-threaded uses the joi n routine. For example, a multi-threaded web server might creat e a number of worker threads, and then use the main thread to accept reques ts and pass them to the workers, indeﬁnitely. Such long-lived programs thus may not need to join. However, a parallel program that creates thr eads to execute a particular task (in parallel) will likely use joi n to make sure all such work completes before exiting or moving onto the next stage of computation. 27.3 Locks Beyond thread creation and join, probably the next most useful set of functions provided by the POSIX threads library are those for provi ding mutual exclusion to a critical section via locks . The most basic pair of routines to use for this purpose is provided by the following: int pthread_mutex_lock(pthread_mutex_t *mutex); int pthread_mutex_unlock(pthread_mutex_t *mutex); 2Fortunately the compiler gcc will likely complain when you write code like this, which is yet another reason to pay attention to compiler warnings.",3044
27. Thread API,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 INTERLUDE : THREAD API The routines should be easy to understand and use. When you have a region of code that is a critical section , and thus needs to be protected to ensure correct operation, locks are quite useful. You can probably imag- ine what the code looks like: pthread_mutex_t lock; pthread_mutex_lock(&lock); x = x + 1; // or whatever your critical section is pthread_mutex_unlock(&lock); The intent of the code is as follows: if no other thread holds the lock whenpthread mutexlock() is called, the thread will acquire the lock and enter the critical section. If another thread does indeed hol d the lock, the thread trying to grab the lock will not return from the call un til it has acquired the lock (implying that the thread holding the lock has released it via the unlock call). Of course, many threads may be stuck wai ting inside the lock acquisition function at a given time; only the thr ead with the lock acquired, however, should call unlock. Unfortunately, this code is broken, in two important ways. The ﬁr st problem is a lack of proper initialization . All locks must be properly initialized in order to guarantee that they have the correct va lues to begin with and thus work as desired when lock and unlock are called. With POSIX threads, there are two ways to initialize locks. One way to do this is to use PTHREAD MUTEXINITIALIZER , as follows: pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; Doing so sets the lock to the default values and thus makes the loc k usable. The dynamic way to do it (i.e., at run time) is to make a call to pthread mutexinit() , as follows: int rc = pthread_mutex_init(&lock, NULL); assert(rc == 0); // always check success. The ﬁrst argument to this routine is the address of the lock itsel f, whereas the second is an optional set of attributes. Read more about the attr ibutes yourself; passing NULL in simply uses the defaults. Either way works, but we usually use the dynamic (latter) method. Note that a correspon ding call topthread mutexdestroy() should also be made, when you are done with the lock; see the manual page for all of details. The second problem with the code above is that it fails to check err or codes when calling lock and unlock. Just like virtually any libr ary rou- tine you call in a U NIX system, these routines can also fail. If your code doesn’t properly check error codes, the failure will happen silen tly, which in this case could allow multiple threads into a critical secti on. Minimally, use wrappers, which assert that the routine succeeded (e.g., as in Fig- ure 27.4); more sophisticated (non-toy) programs, which can’t sim ply exit when something goes wrong, should check for failure and do somethin g appropriate when the lock or unlock does not succeed. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : THREAD API 7 // Use this to keep your code clean but check for failures // Only use if exiting program is OK upon failure void Pthread_mutex_lock(pthread_mutex_t *mutex) { int rc = pthread_mutex_lock(mutex); assert(rc == 0); } Figure 27.4: An Example Wrapper The lock and unlock routines are not the only routines within the pthreads library to interact with locks. In particular, here are two more routines which may be of interest: int pthread_mutex_trylock(pthread_mutex_t *mutex); int pthread_mutex_timedlock(pthread_mutex_t *mutex, struct timespec *abs_timeout); These two calls are used in lock acquisition. The trylock version re- turns failure if the lock is already held; the timedlock version of acquir- ing a lock returns after a timeout or after acquiring the lock, whi chever happens ﬁrst. Thus, the timedlock with a timeout of zero degener ates to the trylock case.",3760
27. Thread API,"Both of these versions should generally be avoi ded; however, there are a few cases where avoiding getting stuck (pe rhaps in- deﬁnitely) in a lock acquisition routine can be useful, as we’ll s ee in future chapters (e.g., when we study deadlock). 27.4 Condition Variables The other major component of any threads library, and certainly th e case with POSIX threads, is the presence of a condition variable . Con- dition variables are useful when some kind of signaling must tak e place between threads, if one thread is waiting for another to do someth ing be- fore it can continue. Two primary routines are used by programs wi shing to interact in this way: int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex); int pthread_cond_signal(pthread_cond_t *cond); To use a condition variable, one has to in addition have a lock that i s associated with this condition. When calling either of the above r outines, this lock should be held. The ﬁrst routine, pthread condwait() , puts the calling thread to sleep, and thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thre ad might care about. A typical usage looks like this: pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; pthread_cond_t cond = PTHREAD_COND_INITIALIZER; Pthread_mutex_lock(&lock); while (ready == 0) Pthread_cond_wait(&cond, &lock); Pthread_mutex_unlock(&lock); c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 INTERLUDE : THREAD API In this code, after initialization of the relevant lock and condit ion3, a thread checks to see if the variable ready has yet been set to something other than zero. If not, the thread simply calls the wait routine i n order to sleep until some other thread wakes it. The code to wake a thread, which would run in some other thread, looks like this: Pthread_mutex_lock(&lock); ready = 1; Pthread_cond_signal(&cond); Pthread_mutex_unlock(&lock); A few things to note about this code sequence. First, when signal ing (as well as when modifying the global variable ready ), we always make sure to have the lock held. This ensures that we don’t accidental ly intro- duce a race condition into our code. Second, you might notice that the wait call takes a lock as its second parameter, whereas the signal call only takes a condition. The r eason for this difference is that the wait call, in addition to puttin g the call- ing thread to sleep, releases the lock when putting said caller to sleep. Imagine if it did not: how could the other thread acquire the lock an d signal it to wake up? However, before returning after being woken, the pthread condwait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the lock acquire at the b eginning of the wait sequence, and the lock release at the end, it holds the lock. One last oddity: the waiting thread re-checks the condition in a while loop, instead of a simple if statement. We’ll discuss this issue i n detail when we study condition variables in a future chapter, but in ge neral, using a while loop is the simple and safe thing to do.",3117
27. Thread API,"Although it re checks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread ; in such a case, without rechecking, the waiting thread will continue t hinking that the condition has changed even though it has not. It is safer thus t o view waking up as a hint that something might have changed, rather t han an absolute fact. Note that sometimes it is tempting to use a simple ﬂag to signal b e- tween two threads, instead of a condition variable and associate d lock. For example, we could rewrite the waiting code above to look more like this in the waiting code: while (ready == 0) ; // spin The associated signaling code would look like this: ready = 1; 3Note that one could use pthread condinit() (and correspond- ing the pthread conddestroy() call) instead of the static initializer PTHREAD CONDINITIALIZER . Sound like more work? It is. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : THREAD API 9 Don’t ever do this, for the following reasons. First, it performs poorl y in many cases (spinning for a long time just wastes CPU cycles). Second, it is error prone. As recent research shows [X+10], it is surpris ingly easy to make mistakes when using ﬂags (as above) to synchronize betw een threads; in that study, roughly half the uses of these ad hoc synchroniza- tions were buggy. Don’t be lazy; use condition variables even when you think you can get away without doing so. If condition variables sound confusing, don’t worry too much (yet) – we’ll be covering them in great detail in a subsequent chapter. Until then, it should sufﬁce to know that they exist and to have some idea how an d why they are used. 27.5 Compiling and Running All of the code examples in this chapter are relatively easy to g et up and running. To compile them, you must include the header pthread.h in your code. On the link line, you must also explicitly link with the pthreads library, by adding the -pthread ﬂag. For example, to compile a simple multi-threaded program, all you have to do is the following: prompt> gcc -o main main.c -Wall -pthread As long as main.c includes the pthreads header, you have now suc- cessfully compiled a concurrent program. Whether it works or not, a s usual, is a different matter entirely. 27.6 Summary We have introduced the basics of the pthread library, includin g thread creation, building mutual exclusion via locks, and signaling a nd waiting via condition variables. You don’t need much else to write robust an d efﬁcient multi-threaded code, except patience and a great de al of care. We now end the chapter with a set of tips that might be useful to you when you write multi-threaded code (see the aside on the following page for details). There are other aspects of the API that are interes ting; if you want more information, type man -k pthread on a Linux system to see over one hundred APIs that make up the entire interface. Howe ver, the basics discussed herein should enable you to build sophisti cated (and hopefully, correct and performant) multi-threaded programs. T he hard part with threads is not the APIs, but rather the tricky logic of h ow you build concurrent programs.",3213
27. Thread API,"Read on to learn more. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 INTERLUDE : THREAD API ASIDE : THREAD API G UIDELINES There are a number of small but important things to remember whe n you use the POSIX thread library (or really, any thread library) to build a multi-threaded program. They are: •Keep it simple. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interac tions lead to bugs. •Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum. Each interaction shoul d be carefully thought out and constructed with tried and true ap- proaches (many of which we will learn about in the coming chap- ters). •Initialize locks and condition variables. Failure to do so will lead to code that sometimes works and sometimes fails in very strange ways. •Check your return codes. Of course, in any C and U NIXprogram- ming you do, you should be checking each and every return code, and it’s true here as well. Failure to do so will lead to bizarre and hard to understand behavior, making you likely to (a) scream, ( b) pull some of your hair out, or (c) both. •Be careful with how you pass arguments to, and return values from, threads. In particular, any time you are passing a reference to a variable allocated on the stack, you are probably doing something wrong. •Each thread has its own stack. As related to the point above, please remember that each thread has its own stack. Thus, if you have a locally-allocated variable inside of some function a thread is ex e- cuting, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the value s must be in the heap or otherwise some locale that is globally accessible. •Always use condition variables to signal between threads. While it is often tempting to use a simple ﬂag, don’t do it. •Use the manual pages. On Linux, in particular, the pthread man pages are highly informative and discuss much of the nuances pr e- sented here, often in even more detail. Read them carefully. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : THREAD API 11 References [B89] “An Introduction to Programming with Threads” by Andrew D. Birr ell. DEC Techni- cal Report, January, 1989. Available: https://birrell.org/and rew/papers/035-Threads.pdf A classic but older introduction to threaded programming. Still a worthwhile read, and freely available. [B97] “Programming with POSIX Threads” by David R. Butenhof. Addison-Wes ley, May 1997. Another one of these books on threads. [B+96] “PThreads Programming: by A POSIX Standard for Better Multiproces sing. ” Dick Buttlar, Jacqueline Farrell, Bradford Nichols. O’Reilly, Septemb er 1996 A reasonable book from the excellent, practical publishing house O’Reilly. Our bookshelves ce rtainly contain a great deal of books from this company, including some excellent offerings on Perl, Python , and Javascript (particularly Crockford’s “Javascript: The Good Parts”.) [K+96] “Programming With Threads” by Steve Kleiman, Devang Shah, Bart Smaalders.",3114
27. Thread API,"Pren- tice Hall, January 1996. Probably one of the better books in this space. Get it at your local library. Or steal it from your mother. More seriously, just ask your mother for it – she’l l let you borrow it, don’t worry. [X+10] “Ad Hoc Synchronization Considered Harmful” by Weiwei Xiong, Soyeon Park, Jiaqi Zhang, Yuanyuan Zhou, Zhiqiang Ma. OSDI 2010, Vancouver, Canada. This paper shows how seemingly simple synchronization code can lead to a surprising number of bugs. Use condition variables and do the signaling correctly. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 INTERLUDE : THREAD API Homework (Code) In this section, we’ll write some simple multi-threaded program s and use a speciﬁc tool, called helgrind , to ﬁnd problems in these programs. Read the README in the homework download for details on how to build the programs and run helgrind. Questions 1. First build main-race.c . Examine the code so you can see the (hopefully obvious) data race in the code. Now run helgrind (by typing valgrind --tool=helgrind main-race ) to see how it reports the race. Does it point to the right lines of code? What other information does i t give to you? 2. What happens when you remove one of the offending lines of co de? Now add a lock around one of the updates to the shared variable, and t hen around both. What does helgrind report in each of these cases? 3. Now let’s look at main-deadlock.c . Examine the code. This code has a problem known as deadlock (which we discuss in much more depth in a forthcoming chapter). Can you see what problem it might have? 4. Now run helgrind on this code. What does helgrind report? 5. Now run helgrind onmain-deadlock-global.c . Examine the code; does it have the same problem that main-deadlock.c has? Should helgrind be reporting the same error? What does this tell you about tools likehelgrind ? 6. Let’s next look at main-signal.c . This code uses a variable ( done ) to signal that the child is done and that the parent can now conti nue. Why is this code inefﬁcient? (what does the parent end up spending it s time doing, particularly if the child thread takes a long time to complete?) 7. Now run helgrind on this program. What does it report? Is the code correct? 8. Now look at a slightly modiﬁed version of the code, which is f ound in main-signal-cv.c . This version uses a condition variable to do the sig- naling (and associated lock). Why is this code preferred to t he previous version? Is it correctness, or performance, or both? 9. Once again run helgrind onmain-signal-cv . Does it report any errors? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2633
28. Locks,"28 Locks From the introduction to concurrency, we saw one of the fundamental problems in concurrent programming: we would like to execute a se ries of instructions atomically, but due to the presence of interrupt s on a single processor (or multiple threads executing on multiple processors c oncur- rently), we couldn’t. In this chapter, we thus attack this probl em directly, with the introduction of something referred to as a lock . Programmers annotate source code with locks, putting them around critical sec tions, and thus ensure that any such critical section executes as if i t were a sin- gle atomic instruction. 28.1 Locks: The Basic Idea As an example, assume our critical section looks like this, the ca nonical update of a shared variable: balance = balance + 1; Of course, other critical sections are possible, such as adding a n ele- ment to a linked list or other more complex updates to shared struc tures, but we’ll just keep to this simple example for now. To use a lock, we add some code around the critical section like this: 1lock_t mutex; // some globally-allocated lock ’mutex’ 2... 3lock(&mutex); 4balance = balance + 1; 5unlock(&mutex); A lock is just a variable, and thus to use one, you must declare a lock variable of some kind (such as mutex above). This lock variable (or just “lock” for short) holds the state of the lock at any instant in time. I t is ei- ther available (orunlocked orfree) and thus no thread holds the lock, or acquired (orlocked orheld ), and thus exactly one thread holds the lock and presumably is in a critical section. We could store other infor mation in the data type as well, such as which thread holds the lock, or a q ueue 1 2 LOCKS for ordering lock acquisition, but information like that is hidden from the user of the lock. The semantics of the lock() andunlock() routines are simple. Call- ing the routine lock() tries to acquire the lock; if no other thread holds the lock (i.e., it is free), the thread will acquire the lock and enter the crit- ical section; this thread is sometimes said to be the owner of the lock. If another thread then calls lock() on that same lock variable ( mutex in this example), it will not return while the lock is held by anothe r thread; in this way, other threads are prevented from entering the crit ical section while the ﬁrst thread that holds the lock is in there. Once the owner of the lock calls unlock() , the lock is now available (free) again. If no other threads are waiting for the lock (i.e., no other thread has called lock() and is stuck therein), the state of the lock is simply changed to free. If there are waiting threads (stuck i nlock() ), one of them will (eventually) notice (or be informed of) this change of the lock’s state, acquire the lock, and enter the critical section. Locks provide some minimal amount of control over scheduling to programmers. In general, we view threads as entities created by the pro- grammer but scheduled by the OS, in any fashion that the OS chooses . Locks yield some of that control back to the programmer; by putting a lock around a section of code, the programmer can guarantee that no more than a single thread can ever be active within that code. Th us locks help transform the chaos that is traditional OS scheduling into a more controlled activity.",3312
28. Locks,"28.2 Pthread Locks The name that the POSIX library uses for a lock is a mutex , as it is used to provide mutual exclusion between threads, i.e., if one thread is in the critical section, it excludes the others from entering until it has completed the section. Thus, when you see the following POSIX threads code, you should understand that it is doing the same thing as above (we aga in use our wrappers that check for errors upon lock and unlock): 1pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; 2 3Pthread_mutex_lock(&lock); // wrapper; exits on failure 4balance = balance + 1; 5Pthread_mutex_unlock(&lock); You might also notice here that the POSIX version passes a variabl e to lock and unlock, as we may be using different locks to protect different variables. Doing so can increase concurrency: instead of one big lock that is used any time any critical section is accessed (a coarse-grained locking strategy), one will often protect different data and data struc tures with different locks, thus allowing more threads to be in locked code at once (a more ﬁne-grained approach). OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 3 28.3 Building A Lock By now, you should have some understanding of how a lock works, from the perspective of a programmer. But how should we build a lock? What hardware support is needed? What OS support? It is this set of questions we address in the rest of this chapter. THECRUX: HOWTOBUILD A L OCK How can we build an efﬁcient lock? Efﬁcient locks provide mutual exclusion at low cost, and also might attain a few other properties we discuss below. What hardware support is needed? What OS support ? To build a working lock, we will need some help from our old friend, the hardware, as well as our good pal, the OS. Over the years, a num- ber of different hardware primitives have been added to the in struction sets of various computer architectures; while we won’t study how th ese instructions are implemented (that, after all, is the topic of a computer architecture class), we will study how to use them in order to bu ild a mu- tual exclusion primitive like a lock. We will also study how the O S gets involved to complete the picture and enable us to build a sophist icated locking library. 28.4 Evaluating Locks Before building any locks, we should ﬁrst understand what our goal s are, and thus we ask how to evaluate the efﬁcacy of a particular l ock implementation. To evaluate whether a lock works (and works well ), we should ﬁrst establish some basic criteria. The ﬁrst is whether the lock does its basic task, which is to provide mutual exclusion . Basically, does the lock work, preventing multiple threads from entering a critica l section? The second is fairness . Does each thread contending for the lock get a fair shot at acquiring it once it is free? Another way to look at thi s is by examining the more extreme case: does any thread contending f or the lock starve while doing so, thus never obtaining it? The ﬁnal criterion is performance , speciﬁcally the time overheads added by using the lock.",3067
28. Locks,"There are a few different cases that are worth con- sidering here. One is the case of no contention; when a single thr ead is running and grabs and releases the lock, what is the overhead of do- ing so? Another is the case where multiple threads are contendin g for the lock on a single CPU; in this case, are there performance conce rns? Fi- nally, how does the lock perform when there are multiple CPUs invol ved, and threads on each contending for the lock? By comparing these dif fer- ent scenarios, we can better understand the performance impac t of using various locking techniques, as described below. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LOCKS 28.5 Controlling Interrupts One of the earliest solutions used to provide mutual exclusion was to disable interrupts for critical sections; this solution was i nvented for single-processor systems. The code would look like this: 1void lock() { 2DisableInterrupts(); 3} 4void unlock() { 5EnableInterrupts(); 6} Assume we are running on such a single-processor system. By turn - ing off interrupts (using some kind of special hardware instruc tion) be- fore entering a critical section, we ensure that the code inside the critical section will notbe interrupted, and thus will execute as if it were atomic. When we are ﬁnished, we re-enable interrupts (again, via a ha rdware in- struction) and thus the program proceeds as usual. The main positive of this approach is its simplicity. You certain ly don’t have to scratch your head too hard to ﬁgure out why this works. With out interruption, a thread can be sure that the code it executes wil l execute and that no other thread will interfere with it. The negatives, unfortunately, are many. First, this approach requires us to allow any calling thread to perform a privileged operation (turning interrupts on and off), and thus trust that this facility is not abused. As you already know, any time we are required to trust an arbitrary pro- gram, we are probably in trouble. Here, the trouble manifests in numer- ous ways: a greedy program could call lock() at the beginning of its execution and thus monopolize the processor; worse, an errant or mali - cious program could call lock() and go into an endless loop. In this latter case, the OS never regains control of the system, and ther e is only one recourse: restart the system. Using interrupt disabling a s a general- purpose synchronization solution requires too much trust in appli cations. Second, the approach does not work on multiprocessors. If multiple threads are running on different CPUs, and each try to enter th e same critical section, it does not matter whether interrupts are dis abled; threads will be able to run on other processors, and thus could enter the cri tical section. As multiprocessors are now commonplace, our general soluti on will have to do better than this. Third, turning off interrupts for extended periods of time can le ad to interrupts becoming lost, which can lead to serious systems prob lems. Imagine, for example, if the CPU missed the fact that a disk dev ice has ﬁnished a read request. How will the OS know to wake the process wa it- ing for said read?",3178
28. Locks,"Finally, and probably least important, this approach can be ine fﬁcient. Compared to normal instruction execution, code that masks or unmas ks interrupts tends to be executed slowly by modern CPUs. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 5 1typedef struct __lock_t { int flag; } lock_t; 2 3void init(lock_t *mutex) { 4// 0 -> lock is available, 1 -> held 5mutex->flag = 0; 6} 7 8void lock(lock_t *mutex) { 9while (mutex->flag == 1) // TEST the flag 10 ; // spin-wait (do nothing) 11mutex->flag = 1; // now SET it. 12} 13 14void unlock(lock_t *mutex) { 15mutex->flag = 0; 16} Figure 28.1: First Attempt: A Simple Flag For these reasons, turning off interrupts is only used in limited con- texts as a mutual-exclusion primitive. For example, in some cas es an operating system itself will use interrupt masking to guaran tee atom- icity when accessing its own data structures, or at least to pre vent cer- tain messy interrupt handling situations from arising. This u sage makes sense, as the trust issue disappears inside the OS, which alwa ys trusts itself to perform privileged operations anyhow. 28.6 A Failed Attempt: Just Using Loads/Stores To move beyond interrupt-based techniques, we will have to rel y on CPU hardware and the instructions it provides us to build a prope r lock. Let’s ﬁrst try to build a simple lock by using a single ﬂag variab le. In this failed attempt, we’ll see some of the basic ideas needed to build a lock, and (hopefully) see why just using a single variable and acces sing it via normal loads and stores is insufﬁcient. In this ﬁrst attempt (Figure 28.1), the idea is quite simple: use a simple variable (flag ) to indicate whether some thread has possession of a lock. The ﬁrst thread that enters the critical section will call lock() , which tests whether the ﬂag is equal to 1 (in this case, it is not), and then sets the ﬂag to 1 to indicate that the thread now holds the lock. When ﬁnished with the critical section, the thread calls unlock() and clears the ﬂag, thus indicating that the lock is no longer held. If another thread happens to call lock() while that ﬁrst thread is in the critical section, it will simply spin-wait in the while loop for that thread to call unlock() and clear the ﬂag. Once that ﬁrst thread does so, the waiting thread will fall out of the while loop, set the ﬂag to 1 for itself, and proceed into the critical section. Unfortunately, the code has two problems: one of correctness, and a n- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 LOCKS Thread 1 Thread 2 calllock() while (ﬂag == 1) interrupt: switch to Thread 2 calllock() while (ﬂag == 1) ﬂag = 1; interrupt: switch to Thread 1 ﬂag = 1; // set ﬂag to 1 (too.) Figure 28.2: Trace: No Mutual Exclusion other of performance. The correctness problem is simple to see once you get used to thinking about concurrent programming. Imagine the code interleaving in Figure 28.2; assume flag=0 to begin. As you can see from this interleaving, with timely (untimely?) inter- rupts, we can easily produce a case where both threads set the ﬂag to 1 and both threads are thus able to enter the critical section.",3151
28. Locks,"Th is behavior is what professionals call “bad” – we have obviously failed to prov ide the most basic requirement: providing mutual exclusion. The performance problem, which we will address more later on, is t he fact that the way a thread waits to acquire a lock that is alread y held: it endlessly checks the value of ﬂag, a technique known as spin-waiting . Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread t hat the waiter is waiting for cannot even run (at least, until a context s witch oc- curs). Thus, as we move forward and develop more sophisticated solu - tions, we should also consider ways to avoid this kind of waste. 28.7 Building Working Spin Locks with Test-And-Set Because disabling interrupts does not work on multiple processors , and because simple approaches using loads and stores (as shown ab ove) don’t work, system designers started to invent hardware support for lock- ing. The earliest multiprocessor systems, such as the Burrough s B5000 in the early 1960’s [M82], had such support; today all systems provi de this type of support, even for single CPU systems. The simplest bit of hardware support to understand is known as a test-and-set (oratomic exchange1) instruction. We deﬁne what the test- and-set instruction does via the following C code snippet: 1int TestAndSet(int *old_ptr, int new) { 2int old = *old_ptr; // fetch old value at old_ptr 3*old_ptr = new; // store ’new’ into old_ptr 4return old; // return the old value 5} 1Each architecture that supports test-and-set calls it by a different name . On SPARC it is called the load/store unsigned byte instruction ( ldstub ); on x86 it is the locked version of the atomic exchange ( xchg ). OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 7 ASIDE : DEKKER ’SANDPETERSON ’SALGORITHMS In the 1960’s, Dijkstra posed the concurrency problem to his frie nds, and one of them, a mathematician named Theodorus Jozef Dekker, came up with a solution [D68]. Unlike the solutions we discuss here, whic h use special hardware instructions and even OS support, Dekker’s algorithm uses just loads and stores (assuming they are atomic with respec t to each other, which was true on early hardware). Dekker’s approach was later reﬁned by Peterson [P81]. Once agai n, just loads and stores are used, and the idea is to ensure that two thre ads never enter a critical section at the same time. Here is Peterson’s algorithm (for two threads); see if you can understand the code. What are the flag and turn variables used for? int flag[2]; int turn; void init() { // indicate you intend to hold the lock w/ ’flag’ flag[0] = flag[1] = 0; // whose turn is it? (thread 0 or 1) turn = 0; } void lock() { // ’self’ is the thread ID of caller flag[self] = 1; // make it other thread’s turn turn = 1 - self; while ((flag[1-self] == 1) && (turn == 1 - self)) ; // spin-wait while it’s not your turn } void unlock() { // simply undo your intent flag[self] = 0; } For some reason, developing locks that work without special hardwar e support became all the rage for a while, giving theory-types a lot of prob- lems to work on.",3180
28. Locks,"Of course, this line of work became quite useless wh en people realized it is much easier to assume a little hardware s upport (and indeed that support had been around from the earliest days of mult ipro- cessing). Further, algorithms like the ones above don’t work on mod- ern hardware (due to relaxed memory consistency models), thus m aking them even less useful than they were before. Yet more research r elegated to the dustbin of history... c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 LOCKS 1typedef struct __lock_t { 2int flag; 3} lock_t; 4 5void init(lock_t *lock) { 6// 0: lock is available, 1: lock is held 7lock->flag = 0; 8} 9 10void lock(lock_t *lock) { 11while (TestAndSet(&lock->flag, 1) == 1) 12 ; // spin-wait (do nothing) 13} 14 15void unlock(lock_t *lock) { 16lock->flag = 0; 17} Figure 28.3: A Simple Spin Lock Using Test-and-set What the test-and-set instruction does is as follows. It returns the old value pointed to by the ptr, and simultaneously updates said value to new. The key, of course, is that this sequence of operations is performe d atomically . The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simul taneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock , as we now examine in Figure 28.3. Or better yet: ﬁgure it out ﬁrst yours elf. Let’s make sure we understand why this lock works. Imagine ﬁrst t he case where a thread calls lock() and no other thread currently holds the lock; thus, flag should be 0. When the thread calls TestAndSet(flag, 1), the routine will return the old value of flag , which is 0; thus, the call- ing thread, which is testing the value of ﬂag, will not get caught spinning in the while loop and will acquire the lock. The thread will also a tomi- cally setthe value to 1, thus indicating that the lock is now held. When the thread is ﬁnished with its critical section, it calls unlock() to set the ﬂag back to zero. The second case we can imagine arises when one thread already ha s the lock held (i.e., flag is 1). In this case, this thread will call lock() and then callTestAndSet(flag, 1) as well. This time, TestAndSet() will return the old value at ﬂag, which is 1 (because the lock is h eld), while simultaneously setting it to 1 again. As long as the lock is held by another thread, TestAndSet() will repeatedly return 1, and thus this thread will spin and spin until the lock is ﬁnally released. Wh en the ﬂag is ﬁnally set to 0 by some other thread, this thread will call TestAndSet() again, which will now return 0 while atomically setting the val ue to 1 and thus acquire the lock and enter the critical section. By making both the test (of the old lock value) and set(of the new OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 9 TIP: THINK ABOUT CONCURRENCY ASA M ALICIOUS SCHEDULER From this example, you might get a sense of the approach you need to take to understand concurrent execution.",3057
28. Locks,"What you should try to d o is to pretend you are a malicious scheduler , one that interrupts threads at the most inopportune of times in order to foil their feeble attempts at b uilding synchronization primitives. What a mean scheduler you are. Alt hough the exact sequence of interrupts may be improbable , it is possible , and that is all we need to demonstrate that a particular approach does not w ork. It can be useful to think maliciously. (at least, sometimes) value) a single atomic operation, we ensure that only one thread ac quires the lock. And that’s how to build a working mutual exclusion primit ive. You may also now understand why this type of lock is usually referr ed to as a spin lock . It is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work corre ctly on a single processor, it requires a preemptive scheduler (i.e., one that will interrupt a thread via a timer, in order to run a different thread, from time to time). Without preemption, spin locks don’t make much sens e on a single CPU, as a thread spinning on a CPU will never relinquis h it. 28.8 Evaluating Spin Locks Given our basic spin lock, we can now evaluate how effective it is along our previously described axes. The most important aspect of a lock iscorrectness : does it provide mutual exclusion? The answer here is yes: the spin lock only allows a single thread to enter the critical se ction at a time. Thus, we have a correct lock. The next axis is fairness . How fair is a spin lock to a waiting thread? Can you guarantee that a waiting thread will ever enter the cri tical sec- tion? The answer here, unfortunately, is bad news: spin locks don ’t pro- vide any fairness guarantees. Indeed, a thread spinning may spin forever, under contention. Simple spin locks (as discussed thus far) are n ot fair and may lead to starvation. The ﬁnal axis is performance . What are the costs of using a spin lock? To analyze this more carefully, we suggest thinking about a few different cases. In the ﬁrst, imagine threads competing for the lock on a sin gle processor; in the second, consider threads spread out across many C PUs. For spin locks, in the single CPU case, performance overheads can be quite painful; imagine the case where the thread holding th e lock is preempted within a critical section. The scheduler might the n run every other thread (imagine there are N−1others), each of which tries to ac- quire the lock. In this case, each of those threads will spin for th e duration of a time slice before giving up the CPU, a waste of CPU cycles. However, on multiple CPUs, spin locks work reasonably well (if the number of threads roughly equals the number of CPUs). The thinki ng c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 LOCKS 1int CompareAndSwap(int *ptr, int expected, int new) { 2int actual = *ptr; 3if (actual == expected) 4 *ptr = new; 5return actual; 6} Figure 28.4: Compare-and-swap goes as follows: imagine Thread A on CPU 1 and Thread B on CPU 2, both contending for a lock. If Thread A (CPU 1) grabs the lock, and th en Thread B tries to, B will spin (on CPU 2). However, presumably the crit- ical section is short, and thus soon the lock becomes available, and is ac- quired by Thread B. Spinning to wait for a lock held on another proces sor doesn’t waste many cycles in this case, and thus can be effectiv e.",3400
28. Locks,"28.9 Compare-And-Swap Another hardware primitive that some systems provide is known as thecompare-and-swap instruction (as it is called on SPARC, for exam- ple), or compare-and-exchange (as it called on x86). The C pseudocode for this single instruction is found in Figure 28.4. The basic idea is for compare-and-swap to test whether the valu e at the address speciﬁed by ptr is equal to expected ; if so, update the memory location pointed to by ptr with the new value. If not, do nothing. In either case, return the actual value at that memory location, th us allowing the code calling compare-and-swap to know whether it succeeded or not. With the compare-and-swap instruction, we can build a lock in a m an- ner quite similar to that with test-and-set. For example, we c ould just replace the lock() routine above with the following: 1void lock(lock_t *lock) { 2while (CompareAndSwap(&lock->flag, 0, 1) == 1) 3 ; // spin 4} The rest of the code is the same as the test-and-set example above . This code works quite similarly; it simply checks if the ﬂag is 0 and if so, atomically swaps in a 1 thus acquiring the lock. Threads that try to acquire the lock while it is held will get stuck spinning until the lock is ﬁnally released. If you want to see how to really make a C-callable x86-version of compare-and-swap, the code sequence (from [S05]) might be usefu l2. Finally, as you may have sensed, compare-and-swap is a more power - ful instruction than test-and-set. We will make some use of this power in 2github.com/remzi-arpacidusseau/ostep-code/tree/mast er/threads-locks OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 11 the future when we brieﬂy delve into topics such as lock-free synchro- nization [H91]. However, if we just build a simple spin lock with it, its behavior is identical to the spin lock we analyzed above. 28.10 Load-Linked and Store-Conditional Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture [H93] , for example, theload-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures. The C pseudocode f or these instructions is as found in Figure 28.5. Alpha, PowerPC, a nd ARM provide similar instructions [W09]. The load-linked operates much like a typical load instruction, a nd sim- ply fetches a value from memory and places it in a register. The k ey differ- ence comes with the store-conditional, which only succeeds (and u pdates the value stored at the address just load-linked from) if no inte rvening store to the address has taken place. In the case of success, the store- conditional returns 1 and updates the value at ptr tovalue ; if it fails, the value at ptr isnotupdated and 0 is returned. As a challenge to yourself, try thinking about how to build a lock u sing load-linked and store-conditional. Then, when you are ﬁnished, l ook at the code below which provides one simple solution. Do it. The solution is in Figure 28.6. Thelock() code is the only interesting piece. First, a thread spins waiting for the ﬂag to be set to 0 (and thus indicate the lock is not held). Once so, the thread tries to acquire the lock via the store-condit ional; if it succeeds, the thread has atomically changed the ﬂag’s value to 1 and thus can proceed into the critical section.",3345
28. Locks,"Note how failure of the store-conditional might arise. One thread c alls lock() and executes the load-linked, returning 0 as the lock is not held . Before it can attempt the store-conditional, it is interrupted a nd another thread enters the lock code, also executing the load-linked ins truction, 1int LoadLinked(int *ptr) { 2return*ptr; 3} 4 5int StoreConditional(int *ptr, int value) { 6if (no update to *ptr since LoadLinked to this address) { 7 *ptr = value; 8 return 1; // success. 9} else { 10 return 0; // failed to update 11} 12} Figure 28.5: Load-linked And Store-conditional c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 LOCKS 1void lock(lock_t *lock) { 2while (1) { 3 while (LoadLinked(&lock->flag) == 1) 4 ; // spin until it’s zero 5 if (StoreConditional(&lock->flag, 1) == 1) 6 return; // if set-it-to-1 was a success: all done 7 // otherwise: try it all over again 8} 9} 10 11void unlock(lock_t *lock) { 12lock->flag = 0; 13} Figure 28.6: Using LL/SC To Build A Lock and also getting a 0 and continuing. At this point, two threads h ave each executed the load-linked and each are about to attempt the store- conditional. The key feature of these instructions is that only one of these threads will succeed in updating the ﬂag to 1 and thus acquire the lock; the second thread to attempt the store-conditional will fail (be cause the other thread updated the value of ﬂag between its load-linked an d store- conditional) and thus have to try to acquire the lock again. In class a few years ago, undergraduate student David Capel su g- gested a more concise form of the above, for those of you who enjoy short-circuiting boolean conditionals. See if you can ﬁgure out why i t is equivalent. It certainly is shorter. 1void lock(lock_t *lock) { 2while (LoadLinked(&lock->flag) || 3 .StoreConditional(&lock->flag, 1)) 4 ; // spin 5} 28.11 Fetch-And-Add One ﬁnal hardware primitive is the fetch-and-add instruction, which atomically increments a value while returning the old value at a partic- ular address. The C pseudocode for the fetch-and-add instructi on looks like this: 1int FetchAndAdd(int *ptr) { 2int old = *ptr; 3*ptr = old + 1; 4return old; 5} OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 13 TIP: LESSCODE ISBETTER CODE (LAUER ’SLAW) Programmers tend to brag about how much code they wrote to do some- thing. Doing so is fundamentally broken. What one should brag abou t, rather, is how little code one wrote to accomplish a given task. Short, concise code is always preferred; it is likely easier to unders tand and has fewer bugs. As Hugh Lauer said, when discussing the construct ion of the Pilot operating system: “If the same people had twice as much time, they could produce as good of a system in half the code.” [L81] We’ll ca ll this Lauer’s Law , and it is well worth remembering. So next time you’re bragging about how much code you wrote to ﬁnish the assignment, thi nk again, or better yet, go back, rewrite, and make the code as clea r and con- cise as possible.",3021
28. Locks,"In this example, we’ll use fetch-and-add to build a more intere sting ticket lock , as introduced by Mellor-Crummey and Scott [MS91]. The lock and unlock code is found in Figure 28.7 (page 14). Instead of a single value, this solution uses a ticket and turn va riable in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it ﬁrst does an atomic fetch-an d-add on the ticket value; that value is now considered this thread’s “t urn” (myturn ). The globally shared lock->turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. Unlock is accomplished simply by incrementing the turn such that the next waiting th read (if there is one) can now enter the critical section. Note one important difference with this solution versus our previou s attempts: it ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the future (on ce those in front of it have passed through the critical section and released the lock). In our previous attempts, no such guarantee existed; a thread s pinning on test-and-set (for example) could spin forever even as other thr eads acquire and release the lock. 28.12 Too Much Spinning: What Now? Our simple hardware-based locks are simple (only a few lines of c ode) and they work (you could even prove that if you’d like to, by writing some code), which are two excellent properties of any system or code . However, in some cases, these solutions can be quite inefﬁcient. Imagine you are running two threads on a single processor. Now imagine that one thread (thread 0) is in a critical section and thus has a lock h eld, and unfortunately gets interrupted. The second thread (thread 1) now tries to acquire the lock, but ﬁnds that it is held. Thus, it begins to sp in. And spin. Then it spins some more. And ﬁnally, a timer interrupt goes off, th read 0 is run again, which releases the lock, and ﬁnally (the next ti me it runs, c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 LOCKS 1typedef struct __lock_t { 2int ticket; 3int turn; 4} lock_t; 5 6void lock_init(lock_t *lock) { 7lock->ticket = 0; 8lock->turn = 0; 9} 10 11void lock(lock_t *lock) { 12int myturn = FetchAndAdd(&lock->ticket); 13while (lock->turn .= myturn) 14 ; // spin 15} 16 17void unlock(lock_t *lock) { 18lock->turn = lock->turn + 1; 19} Figure 28.7: Ticket Locks say), thread 1 won’t have to spin so much and will be able to acqui re the lock. Thus, any time a thread gets caught spinning in a situati on like this, it wastes an entire time slice doing nothing but checking a valu e that isn’t going to change. The problem gets worse with Nthreads contending for a lock; N−1time slices may be wasted in a similar manner, simply spinning and waiting for a single thread to release the lock. An d thus, our next problem: THECRUX: HOWTOAVOID SPINNING How can we develop a lock that doesn’t needlessly waste time spin- ning on the CPU?",3057
28. Locks,"Hardware support alone cannot solve the problem. We’ll need OS sup- port too. Let’s now ﬁgure out just how that might work. 28.13 A Simple Approach: Just Yield, Baby Hardware support got us pretty far: working locks, and even (as wi th the case of the ticket lock) fairness in lock acquisition. However , we still have a problem: what to do when a context switch occurs in a critic al section, and threads start to spin endlessly, waiting for the i nterrupted (lock-holding) thread to be run again? Our ﬁrst try is a simple and friendly approach: when you are going to spin, instead give up the CPU to another thread. Or, as Al Davis might say, “just yield, baby.” [D91]. Figure 28.8 (page 15) present s the approach. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 15 1void init() { 2flag = 0; 3} 4 5void lock() { 6while (TestAndSet(&flag, 1) == 1) 7 yield(); // give up the CPU 8} 9 10void unlock() { 11flag = 0; 12} Figure 28.8: Lock With Test-and-set And Yield In this approach, we assume an operating system primitive yield() which a thread can call when it wants to give up the CPU and let a n- other thread run. A thread can be in one of three states (running, ready, or blocked); yield is simply a system call that moves the caller f rom the running state to the ready state, and thus promotes another thread to running. Thus, the yielding process essentially deschedules itself. Think about the example with two threads on one CPU; in this case, our yield-based approach works quite well. If a thread happens t o call lock() and ﬁnd a lock held, it will simply yield the CPU, and thus the other thread will run and ﬁnish its critical section. In this si mple case, the yielding approach works well. Let us now consider the case where there are many threads (say 10 0) contending for a lock repeatedly. In this case, if one thread acqu ires the lock and is preempted before releasing it, the other 99 will e ach call lock() , ﬁnd the lock held, and yield the CPU. Assuming some kind of round-robin scheduler, each of the 99 will execute this run-an d-yield pattern before the thread holding the lock gets to run again. Whi le better than our spinning approach (which would waste 99 time slices spi nning), this approach is still costly; the cost of a context switch can be su bstantial, and there is thus plenty of waste. Worse, we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repea tedly enter and exit the critical section. We clearly will need an ap proach that addresses this problem directly. 28.14 Using Queues: Sleeping Instead Of Spinning The real problem with our previous approaches is that they leave t oo much to chance. The scheduler determines which thread runs n ext; if the scheduler makes a bad choice, a thread runs that must eithe r spin waiting for the lock (our ﬁrst approach), or yield the CPU immediat ely (our second approach). Either way, there is potential for waste an d no prevention of starvation. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 LOCKS 1typedef struct __lock_t { 2int flag; 3int guard; 4queue_t *q; 5} lock_t; 6 7void lock_init(lock_t *m) { 8m->flag = 0; 9m->guard = 0; 10queue_init(m->q); 11} 12 13void lock(lock_t *m) { 14while (TestAndSet(&m->guard, 1) == 1) 15 ; //acquire guard lock by spinning 16if (m->flag == 0) { 17 m->flag = 1; // lock is acquired 18 m->guard = 0; 19} else { 20 queue_add(m->q, gettid()); 21 m->guard = 0; 22 park(); 23} 24} 25 26void unlock(lock_t *m) { 27while (TestAndSet(&m->guard, 1) == 1) 28 ; //acquire guard lock by spinning 29if (queue_empty(m->q)) 30 m->flag = 0; // let go of lock; no one wants it 31else 32 unpark(queue_remove(m->q)); // hold lock 33 // (for next thread.) 34m->guard = 0; 35} Figure 28.9: Lock With Queues, Test-and-set, Yield, And Wakeup Thus, we must explicitly exert some control over which thread nex t gets to acquire the lock after the current holder releases it. T o do this, we will need a little more OS support, as well as a queue to keep trac k of which threads are waiting to acquire the lock.",4104
28. Locks,"For simplicity, we will use the support provided by Solaris, in ter ms of two calls:park() to put a calling thread to sleep, and unpark(threadID) to wake a particular thread as designated by threadID . These two rou- tines can be used in tandem to build a lock that puts a caller to s leep if it tries to acquire a held lock and wakes it when the lock is free. Le t’s look at the code in Figure 28.9 to understand one possible use of such prim itives. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 17 ASIDE : M ORE REASON TOAVOID SPINNING : PRIORITY INVERSION One good reason to avoid spin locks is performance: as described in t he main text, if a thread is interrupted while holding a lock, other threads that use spin locks will spend a large amount of CPU time just wait ing for the lock to become available. However, it turns out there is anothe r inter- esting reason to avoid spin locks on some systems: correctness. The prob- lem to be wary of is known as priority inversion , which unfortunately is an intergalactic scourge, occurring on Earth [M15] and Mars [R9 7]. Let’s assume there are two threads in a system. Thread 2 (T2) ha s a high scheduling priority, and Thread 1 (T1) has lower priority. In th is example, let’s assume that the CPU scheduler will always run T2 over T1, i f indeed both are runnable; T1 only runs when T2 is not able to do so (e.g., w hen T2 is blocked on I/O). Now, the problem. Assume T2 is blocked for some reason. So T1 runs, grabs a spin lock, and enters a critical section. T2 now becomes un blocked (perhaps because an I/O completed), and the CPU scheduler imm edi- ately schedules it (thus descheduling T1). T2 now tries to acq uire the lock, and because it can’t (T1 holds the lock), it just keeps spinning. Because the lock is a spin lock, T2 spins forever, and the system is hung. Just avoiding the use of spin locks, unfortunately, does not avoid th e problem of inversion (alas). Imagine three threads, T1, T2, and T3, with T3 at the highest priority, and T1 the lowest. Imagine now that T1 grabs a lock. T3 then starts, and because it is higher priority than T1 , runs im- mediately (preempting T1). T3 tries to acquire the lock that T 1 holds, but gets stuck waiting, because T1 still holds it. If T2 starts to r un, it will have higher priority than T1, and thus it will run. T3, which is high er priority than T2, is stuck waiting for T1, which may never run now that T2 i s run- ning. Isn’t it sad that the mighty T3 can’t run, while lowly T2 cont rols the CPU? Having high priority just ain’t what it used to be. You can address the priority inversion problem in a number of ways. In the speciﬁc case where spin locks cause the problem, you can avoid us- ing spin locks (described more below). More generally, a higher- priority thread waiting for a lower-priority thread can temporarily boost t he lower thread’s priority, thus enabling it to run and overcoming th e in- version, a technique known as priority inheritance . A last solution is simplest: ensure all threads have the same priority.",3060
28. Locks,"We do a couple of interesting things in this example. First, we c ombine the old test-and-set idea with an explicit queue of lock waiters to make a more efﬁcient lock. Second, we use a queue to help control who gets th e lock next and thus avoid starvation. You might notice how the guard is used (Figure 28.9, page 16), bas i- cally as a spin-lock around the ﬂag and queue manipulations the l ock is using. This approach thus doesn’t avoid spin-waiting entirely; a thread c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 LOCKS might be interrupted while acquiring or releasing the lock, an d thus cause other threads to spin-wait for this one to run again. However, the time spent spinning is quite limited (just a few instructions insi de the lock and unlock code, instead of the user-deﬁned critical section), and t hus this approach may be reasonable. Second, you might notice that in lock() , when a thread can not ac- quire the lock (it is already held), we are careful to add oursel ves to a queue (by calling the gettid() function to get the thread ID of the current thread), set guard to 0, and yield the CPU. A question f or the reader: What would happen if the release of the guard lock came after the park() , and not before? Hint: something bad. You might also notice the interesting fact that the ﬂag does not ge t set back to 0 when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity. When a thread is woken up, it wi ll be as if it is returning from park() ; however, it does not hold the guard at that point in the code and thus cannot even try to set the ﬂag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; ﬂag is not set to 0 in-between. Finally, you might notice the perceived race condition in the solu tion, just before the call to park() . With just the wrong timing, a thread will be about to park, assuming that it should sleep until the lock is n o longer held. A switch at that time to another thread (say, a thread hold ing the lock) could lead to trouble, for example, if that thread then rele ased the lock. The subsequent park by the ﬁrst thread would then sleep for ever (potentially), a problem sometimes called the wakeup/waiting race . Solaris solves this problem by adding a third system call: setpark() . By calling this routine, a thread can indicate it is about to park. If it then happens to be interrupted and another thread calls unpark bef ore park is actually called, the subsequent park returns immediately i nstead of sleep- ing. The code modiﬁcation, inside of lock() , is quite small: 1queue_add(m->q, gettid()); 2setpark(); // new code 3m->guard = 0; A different solution could pass the guard into the kernel. In tha t case, the kernel could take precautions to atomically release the lock and de- queue the running thread. 28.15 Different OS, Different Support We have thus far seen one type of support that an OS can provide in order to build a more efﬁcient lock in a thread library. Other OS’s p rovide similar support; the details vary.",3105
28. Locks,"For example, Linux provides a futex which is similar to the Solaris in- terface but provides more in-kernel functionality. Speciﬁcall y, each futex has associated with it a speciﬁc physical memory location, as wel l as a OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 19 1void mutex_lock (int *mutex) { 2int v; 3/*Bit 31 was clear, we got the mutex (the fastpath) */ 4if (atomic_bit_test_set (mutex, 31) == 0) 5return; 6atomic_increment (mutex); 7while (1) { 8 if (atomic_bit_test_set (mutex, 31) == 0) { 9 atomic_decrement (mutex); 10 return; 11 } 12 /*We have to waitFirst make sure the futex value 13 we are monitoring is truly negative (locked). */ 14 v =*mutex; 15 if (v >= 0) 16 continue; 17 futex_wait (mutex, v); 18} 19} 20 21void mutex_unlock (int *mutex) { 22/*Adding 0x80000000 to counter results in 0 if and 23 only if there are not other interested threads */ 24if (atomic_add_zero (mutex, 0x80000000)) 25return; 26 27/*There are other threads waiting for this mutex, 28 wake one of them up. */ 29futex_wake (mutex); 30} Figure 28.10: Linux-based Futex Locks per-futex in-kernel queue. Callers can use futex calls (des cribed below) to sleep and wake as need be. Speciﬁcally, two calls are available. The call to futexwait(address, expected) puts the calling thread to sleep, assuming the value at address is equal to expected . If it is notequal, the call returns immediately. The call to the routine futexwake(address) wakes one thread that is wait- ing on the queue. The usage of these calls in a Linux mutex is shown in Figure 28.10 (page 19). This code snippet from lowlevellock.h in the nptl library (part of the gnu libc library) [L09] is interesting for a few reasons. Fi rst, it uses a single integer to track both whether the lock is held or not (the hi gh bit of the integer) and the number of waiters on the lock (all the other b its). Thus, if the lock is negative, it is held (because the high bit i s set and that bit determines the sign of the integer). Second, the code snippet shows how to optimize for the common case, c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 20 LOCKS speciﬁcally when there is no contention for the lock; with only one t hread acquiring and releasing a lock, very little work is done (the atom ic bit test-and-set to lock and an atomic add to release the lock). See if you can puzzle through the rest of this “real-world” lock to un - derstand how it works. Do it and become a master of Linux locking, or a t least somebody who listens when a book tells you to do something3. 28.16 Two-Phase Locks One ﬁnal note: the Linux approach has the ﬂavor of an old approach that has been used on and off for years, going at least as far back to Dahm Locks in the early 1960’s [M82], and is now referred to as a two-phase lock . A two-phase lock realizes that spinning can be useful, partic ularly if the lock is about to be released. So in the ﬁrst phase, the lock sp ins for a while, hoping that it can acquire the lock. However, if the lock is not acquired during the ﬁrst spin phase, a sec- ond phase is entered, where the caller is put to sleep, and only w oken up when the lock becomes free later. The Linux lock above is a form of suc h a lock, but it only spins once; a generalization of this could spin in a loop for a ﬁxed amount of time before using futex support to sleep.",3347
28. Locks,"Two-phase locks are yet another instance of a hybrid approach, where combining two good ideas may indeed yield a better one. Of course, whether it does depends strongly on many things, including the h ard- ware environment, number of threads, and other workload details. As always, making a single general-purpose lock, good for all possibl e use cases, is quite a challenge. 28.17 Summary The above approach shows how real locks are built these days: some hardware support (in the form of a more powerful instruction) plus s ome operating system support (e.g., in the form of park() andunpark() primitives on Solaris, or futex on Linux). Of course, the details differ, and the exact code to perform such locking is usually highly tuned. C heck out the Solaris or Linux code bases if you want to see more details; they a re a fascinating read [L09, S09]. Also see David et al.’s excellen t work for a comparison of locking strategies on modern multiprocessors [D+13]. 3Like buy a print copy of OSTEP. Even though the book is available for free online, wouldn’t you just love a hard cover for your desk? Or, better yet, te n copies to share with friends and family? And maybe one extra copy to throw at an enemy? (the book isheavy, and thus chucking it is surprisingly effective) OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOCKS 21 References [D91] “Just Win, Baby: Al Davis and His Raiders” by Glenn Dickey. Har court, 1991. The book about Al Davis and his famous quote. Or, we suppose, the book is more about Al Davi s and the Raiders, and not so much the quote. To be clear: we are not recommending this book, we ju st needed a citation. [D+13] “Everything You Always Wanted to Know about Synchronization bu t Were Afraid to Ask” by Tudor David, Rachid Guerraoui, Vasileios Trigonakis. S OSP ’13, Nemacolin Wood- lands Resort, Pennsylvania, November 2013. An excellent paper comparing many different ways to build locks using hardware primitives. Great to see how many ideas work on m odern hardware. [D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available online here: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.One of the early semi- nal papers. Discusses how Dijkstra posed the original concurrency probl em, and Dekker’s solution. [H93] “MIPS R4000 Microprocessor User’s Manual” by Joe Heinrich. Prentice-Ha ll, June 1993. Available: http://cag.csail.mit.edu/raw/documents/R4400 Uman book Ed2.pdf. The old MIPS user’s manual. Download it while it still exists. [H91] “Wait-free Synchronization” by Maurice Herlihy. ACM TOPLAS, Volume 13: 1, January 1991. A landmark paper introducing a different approach to building concurre nt data structures. Be- cause of the complexity involved, some of these ideas have been slow to gain acc eptance in deployment. [L81] “Observations on the Development of an Operating System” by Hu gh Lauer. SOSP ’81, Paciﬁc Grove, California, December 1981. A must-read retrospective about the development of the Pilot OS, an early PC operating system. Fun and full of insights.",3052
28. Locks,"[L09] “glibc 2.9 (include Linux pthreads implementation)” by Many author s.. Available here: http://ftp.gnu.org/gnu/glibc .In particular, take a look at the nptl subdirectory where you will ﬁnd most of the pthread support in Linux today. [M82] “The Architecture of the Burroughs B5000: 20 Years Later and Still Ahead of the Times?” by A. Mayer. 1982. Available: www.ajwm.net/amayer/papers/B5000.html .“It (RDLK) is an indivisible operation which reads from and writes into a memory location .” RDLK is thus test- and-set. Dave Dahm created spin locks (“Buzz Locks”) and a two-phase lock called “Dah m Locks.” [M15] “OSSpinLock Is Unsafe” by J. McCall. mjtsai.com/blog/2015/12/16/osspinlock -is-unsafe .Calling OSSpinLock on a Mac is unsafe when using threads of different p riorities – you might spin forever. So be careful, Mac fanatics, even your mighty syste m can be less than perfect... [MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multip rocessors” by John M. Mellor-Crummey and M. L. Scott. ACM TOCS, Volume 9, Issue 1, Febr uary 1991. An excellent and thorough survey on different locking algorithms. Howev er, no operating systems support is used, just fancy hardware instructions. [P81] “Myths About the Mutual Exclusion Problem” by G.L. Peterson. Inform ation Processing Letters, 12(3), pages 115–116, 1981. Peterson’s algorithm introduced here. [R97] “What Really Happened on Mars?” by Glenn E. Reeves. research.microsoft.com/ en-us/um/people/mbj/Mars Pathfinder/Authoritative Account.html .A descrip- tion of priority inversion on Mars Pathﬁnder. Concurrent code correctness m atters, especially in space. [S05] “Guide to porting from Solaris to Linux on x86” by Ajay Sood, April 29, 2005. Available: http://www.ibm.com/developerworks/linux/library/l-s olar/ . [S09] “OpenSolaris Thread Library” by Sun.. Code: src.opensolaris.org/source/xref/ onnv/onnv-gate/usr/src/lib/libc/port/threads/synch. c.Pretty interesting, al- though who knows what will happen now that Oracle owns Sun. Thanks to Mike Swift for the pointer. [W09] “Load-Link, Store-Conditional” by Many authors.. en.wikipedia.org/wiki/Load- Link/Store-Conditional .Can you believe we referenced Wikipedia? But, we found the infor- mation there and it felt wrong not to. Further, it was useful, listing the ins tructions for the different ar- chitectures: ldll/stlcandldql/stqc(Alpha),lwarx/stwcx (PowerPC), ll/sc (MIPS), andldrex/strex (ARM). Actually Wikipedia is pretty amazing, so don’t be so harsh, OK? [WG00] “The SPARC Architecture Manual: Version 9” by D. Weaver, T. Ger mond. SPARC In- ternational, 2000. http://www.sparc.org/standards/SPARCV9.pdf .Seedevelopers. sun.com/solaris/articles/atomic sparc/ for more on atomics. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 22 LOCKS Homework (Simulation) This program, x86.py , allows you to see how different thread inter- leavings either cause or avoid race conditions. See the README for d e- tails on how the program works and answer the questions below. Questions 1. Examine flag.s .",3048
28. Locks,"This code “implements” locking with a single memory ﬂag. Can you understand the assembly? 2. When you run with the defaults, does flag.s work? Use the -Mand-R ﬂags to trace variables and registers (and turn on -cto see their values). Can you predict what value will end up in flag ? 3. Change the value of the register  percentbx with the-aﬂag (e.g.,-a bx=2,bx=2 if you are running just two threads). What does the code do? How d oes it change your answer for the question above? 4. Setbxto a high value for each thread, and then use the -iﬂag to generate different interrupt frequencies; what values lead to a bad outco mes? Which lead to good outcomes? 5. Now let’s look at the program test-and-set.s . First, try to understand the code, which uses the xchg instruction to build a simple locking primi- tive. How is the lock acquire written? How about lock release? 6. Now run the code, changing the value of the interrupt interval (-i) again, and making sure to loop for a number of times. Does the code always wo rk as expected? Does it sometimes lead to an inefﬁcient use of the CPU? How could you quantify that? 7. Use the -Pﬂag to generate speciﬁc tests of the locking code. For example, run a schedule that grabs the lock in the ﬁrst thread, but then tri es to acquire it in the second. Does the right thing happen? What else should you test? 8. Now let’s look at the code in peterson.s , which implements Peterson’s algorithm (mentioned in a sidebar in the text). Study the code an d see if you can make sense of it. 9. Now run the code with different values of -i. What kinds of different be- havior do you see? Make sure to set the thread IDs appropriatel y (using-a bx=0,bx=1 for example) as the code assumes it. 10. Can you control the scheduling (with the -Pﬂag) to “prove” that the code works? What are the different cases you should show hold? Thin k about mutual exclusion and deadlock avoidance. 11. Now study the code for the ticket lock in ticket.s . Does it match the code in the chapter? Then run with the following ﬂags: -a bx=1000,bx=1000 (causing each thread to loop through the critical section 1000 times). Watch what happens; do the threads spend much time spin-waiting for th e lock? 12. How does the code behave as you add more threads? 13. Now examine yield.s , in which a yield instruction enables one thread to yield control of the CPU (realistically, this would be an OS primitive, but for the simplicity, we assume an instruction does the task). Find a scenario wheretest-and-set.s wastes cycles spinning, but yield.s does not. How many instructions are saved? In what scenarios do these sav ings arise? 14. Finally, examine test-and-test-and-set.s . What does this lock do? What kind of savings does it introduce as compared to test-and-set.s ? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",2828
29. Locked Data Structures,"29 Lock-based Concurrent Data Structures Before moving beyond locks, we’ll ﬁrst describe how to use locks in som e common data structures. Adding locks to a data structure to make it us- able by threads makes the structure thread safe . Of course, exactly how such locks are added determines both the correctness and perform ance of the data structure. And thus, our challenge: CRUX: HOWTOADDLOCKS TODATA STRUCTURES When given a particular data structure, how should we add locks t o it, in order to make it work correctly? Further, how do we add locks s uch that the data structure yields high performance, enabling ma ny threads to access the structure at once, i.e., concurrently ? Of course, we will be hard pressed to cover all data structures or all methods for adding concurrency, as this is a topic that has been st udied for years, with (literally) thousands of research papers publi shed about it. Thus, we hope to provide a sufﬁcient introduction to the type of think- ing required, and refer you to some good sources of material for furth er inquiry on your own. We found Moir and Shavit’s survey to be a great source of information [MS04]. 29.1 Concurrent Counters One of the simplest data structures is a counter. It is a structu re that is commonly used and has a simple interface. We deﬁne a simple non - concurrent counter in Figure 29.1. Simple But Not Scalable As you can see, the non-synchronized counter is a trivial data str ucture, requiring a tiny amount of code to implement. We now have our next challenge: how can we make this code thread safe ? Figure 29.2 shows how we do so. 1 2 LOCK -BASED CONCURRENT DATA STRUCTURES 1typedef struct __counter_t { 2int value; 3} counter_t; 4 5void init(counter_t *c) { 6c->value = 0; 7} 8 9void increment(counter_t *c) { 10 c->value++; 11} 12 13void decrement(counter_t *c) { 14 c->value--; 15} 16 17int get(counter_t *c) { 18 return c->value; 19} Figure 29.1: A Counter Without Locks 1typedef struct __counter_t { 2int value; 3pthread_mutex_t lock; 4} counter_t; 5 6void init(counter_t *c) { 7c->value = 0; 8Pthread_mutex_init(&c->lock, NULL); 9} 10 11void increment(counter_t *c) { 12 Pthread_mutex_lock(&c->lock); 13 c->value++; 14 Pthread_mutex_unlock(&c->lock); 15} 16 17void decrement(counter_t *c) { 18 Pthread_mutex_lock(&c->lock); 19 c->value--; 20 Pthread_mutex_unlock(&c->lock); 21} 22 23int get(counter_t *c) { 24 Pthread_mutex_lock(&c->lock); 25 int rc = c->value; 26 Pthread_mutex_unlock(&c->lock); 27 return rc; 28} Figure 29.2: A Counter With Locks This concurrent counter is simple and works correctly. In fact, i t fol- lows a design pattern common to the simplest and most basic concurr ent data structures: it simply adds a single lock, which is acquir ed when call- ing a routine that manipulates the data structure, and is rele ased when returning from the call. In this manner, it is similar to a data structure built with monitors [BH73], where locks are acquired and released auto- matically as you call and return from object methods.",3024
29. Locked Data Structures,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 3 1 2 3 4051015 ThreadsTime (seconds)Precise Approximate Figure 29.3: Performance of Traditional vs. Approximate Counters At this point, you have a working concurrent data structure. The p rob- lem you might have is performance. If your data structure is too sl ow, you’ll have to do more than just add a single lock; such optimization s, if needed, are thus the topic of the rest of the chapter. Note that if t he data structure is nottoo slow, you are done. No need to do something fancy if something simple will work. To understand the performance costs of the simple approach, we ru n a benchmark in which each thread updates a single shared counte r a ﬁxed number of times; we then vary the number of threads. Figure 29.3 shows the total time taken, with one to four threads active; each threa d updates the counter one million times. This experiment was run upon an iMa c with four Intel 2.7 GHz i5 CPUs; with more CPUs active, we hope to g et more total work done per unit time. From the top line in the ﬁgure (labeled ’Precise’), you can see that the performance of the synchronized counter scales poorly. Whereas a s ingle thread can complete the million counter updates in a tiny amount of time (roughly 0.03 seconds), having two threads each update the coun ter one million times concurrently leads to a massive slowdown (taking ov er 5 seconds.). It only gets worse with more threads. Ideally, you’d like to see the threads complete just as quickly on mul- tiple processors as the single thread does on one. Achieving this e nd is called perfect scaling ; even though more work is done, it is done in par- allel, and hence the time taken to complete the task is not incre ased. Scalable Counting Amazingly, researchers have studied how to build more scalabl e coun- ters for years [MS04]. Even more amazing is the fact that scalabl e coun- ters matter, as recent work in operating system performance ana lysis has shown [B+10]; without scalable counting, some workloads running on Linux suffer from serious scalability problems on multicore mach ines. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LOCK -BASED CONCURRENT DATA STRUCTURES Time L1L2L3L4G 0 0 0 0 0 0 1 0 0 1 1 0 2 1 0 2 1 0 3 2 0 3 1 0 4 3 0 3 2 0 5 4 1 3 3 0 6 5→0 1 3 4 5 (from L1) 7 0 2 4 5 →0 10 (from L4) Figure 29.4: Tracing the Approximate Counters Many techniques have been developed to attack this problem. We ’ll describe one approach known as an approximate counter [C06]. The approximate counter works by representing a single logical c ounter via numerous local physical counters, one per CPU core, as well as a single global counter. Speciﬁcally, on a machine with four CPUs, there are four local counters and one global one. In addition to these counters, the re are also locks: one for each local counter1, and one for the global counter. The basic idea of approximate counting is as follows. When a thread running on a given core wishes to increment the counter, it incre ments its local counter; access to this local counter is synchronized via th e corre- sponding local lock. Because each CPU has its own local counter, thr eads across CPUs can update local counters without contention, and thus up- dates to the counter are scalable. However, to keep the global counter up to date (in case a thread wi shes to read its value), the local values are periodically transfer red to the global counter, by acquiring the global lock and incrementing it by the local counter’s value; the local counter is then reset to zero.",3614
29. Locked Data Structures,"How often this local-to-global transfer occurs is determined by a t hresh- oldS. The smaller Sis, the more the counter behaves like the non-scalable counter above; the bigger Sis, the more scalable the counter, but the fur- ther off the global value might be from the actual count. One could s im- ply acquire all the local locks and the global lock (in a speciﬁed or der, to avoid deadlock) to get an exact value, but that is not scalable. To make this clear, let’s look at an example (Figure 29.4). In thi s ex- ample, the threshold Sis set to 5, and there are threads on each of four CPUs updating their local counters L1...L4. The global counter value (G) is also shown in the trace, with time increasing downward. At e ach time step, a local counter may be incremented; if the local value reaches the threshold S, the local value is transferred to the global counter and the local counter is reset. The lower line in Figure 29.3 (labeled ’Approximate’, on page 3) sh ows the performance of approximate counters with a threshold Sof1024 . Per- formance is excellent; the time taken to update the counter four million times on four processors is hardly higher than the time taken to up date it one million times on one processor. 1We need the local locks because we assume there may be more than one threa d on each core. If, instead, only one thread ran on each core, no local lock would be ne eded. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 5 1typedef struct __counter_t { 2int global; // global count 3pthread_mutex_t glock; // global lock 4int local[NUMCPUS]; // local count (per cpu) 5pthread_mutex_t llock[NUMCPUS]; // ... and locks 6int threshold; // update frequency 7} counter_t; 8 9// init: record threshold, init locks, init values 10// of all local counts and global count 11void init(counter_t *c, int threshold) { 12 c->threshold = threshold; 13 c->global = 0; 14 pthread_mutex_init(&c->glock, NULL); 15 int i; 16 for (i = 0; i < NUMCPUS; i++) { 17 c->local[i] = 0; 18 pthread_mutex_init(&c->llock[i], NULL); 19 } 20} 21 22// update: usually, just grab local lock and update local amo unt 23// once local count has risen by ’threshold’, grab global 24// lock and transfer local values to it 25void update(counter_t *c, int threadID, int amt) { 26 int cpu = threadID  percent NUMCPUS; 27 pthread_mutex_lock(&c->llock[cpu]); 28 c->local[cpu] += amt; // assumes amt > 0 29 if (c->local[cpu] >= c->threshold) { // transfer to global 30 pthread_mutex_lock(&c->glock); 31 c->global += c->local[cpu]; 32 pthread_mutex_unlock(&c->glock); 33 c->local[cpu] = 0; 34 } 35 pthread_mutex_unlock(&c->llock[cpu]); 36} 37 38// get: just return global amount (which may not be perfect) 39int get(counter_t *c) { 40 pthread_mutex_lock(&c->glock); 41 int val = c->global; 42 pthread_mutex_unlock(&c->glock); 43 return val; // only approximate. 44} Figure 29.5: Approximate Counter Implementation Figure 29.6 shows the importance of the threshold value S, with four threads each incrementing the counter 1 million times on four CPU s.",3076
29. Locked Data Structures,"IfS is low, performance is poor (but the global count is always quite acc urate); ifSis high, performance is excellent, but the global count lags (by at most the number of CPUs multiplied by S). This accuracy/performance trade- off is what approximate counters enable. A rough version of an approximate counter is found in Figure 29.5. Read it, or better yet, run it yourself in some experiments to bet ter under- stand how it works. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 LOCK -BASED CONCURRENT DATA STRUCTURES 1 2 4 8 16 32 64 128 256 1024 512051015 Approximation Factor (S)Time (seconds) Figure 29.6: Scaling Approximate Counters 29.2 Concurrent Linked Lists We next examine a more complicated structure, the linked list. Let’s start with a basic approach once again. For simplicity, we’ll omit some of the obvious routines that such a list would have and just focus on conc ur- rent insert; we’ll leave it to the reader to think about lookup, de lete, and so forth. Figure 29.7 shows the code for this rudimentary data str ucture. As you can see in the code, the code simply acquires a lock in the ins ert routine upon entry, and releases it upon exit. One small tricky i ssue arises ifmalloc() happens to fail (a rare case); in this case, the code must also release the lock before failing the insert. This kind of exceptional control ﬂow has been shown to be quite error prone; a recent study of Linux kernel patches found that a huge fr action of bugs (nearly 40 percent) are found on such rarely-taken code paths (ind eed, this observation sparked some of our own research, in which we removed all memory-failing paths from a Linux ﬁle system, resulting in a mor e robust system [S+11]). Thus, a challenge: can we rewrite the insert and lookup routines to re- main correct under concurrent insert but avoid the case where th e failure path also requires us to add the call to unlock? The answer, in this case, is yes. Speciﬁcally, we can rearrang e the code a bit so that the lock and release only surround the actual critic al section in the insert code, and that a common exit path is used in the lookup c ode. The former works because part of the lookup actually need not be locke d; assuming that malloc() itself is thread-safe, each thread can call into it without worry of race conditions or other concurrency bugs. Only when updating the shared list does a lock need to be held. See Figure 29 .8 for the details of these modiﬁcations. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 7 1// basic node structure 2typedef struct __node_t { 3int key; 4struct __node_t *next; 5} node_t; 6 7// basic list structure (one used per list) 8typedef struct __list_t { 9node_t *head; 10 pthread_mutex_t lock; 11} list_t; 12 13void List_Init(list_t *L) { 14 L->head = NULL; 15 pthread_mutex_init(&L->lock, NULL); 16} 17 18int List_Insert(list_t *L, int key) { 19 pthread_mutex_lock(&L->lock); 20 node_t*new = malloc(sizeof(node_t)); 21 if (new == NULL) { 22 perror(\""malloc\""); 23 pthread_mutex_unlock(&L->lock); 24 return -1; // fail 25 } 26 new->key = key; 27 new->next = L->head; 28 L->head = new; 29 pthread_mutex_unlock(&L->lock); 30 return 0; // success 31} 32 33int List_Lookup(list_t *L, int key) { 34 pthread_mutex_lock(&L->lock); 35 node_t*curr = L->head; 36 while (curr) { 37 if (curr->key == key) { 38 pthread_mutex_unlock(&L->lock); 39 return 0; // success 40 } 41 curr = curr->next; 42 } 43 pthread_mutex_unlock(&L->lock); 44 return -1; // failure 45} Figure 29.7: Concurrent Linked List As for the lookup routine, it is a simple code transformation to jump out of the main search loop to a single return path.",3685
29. Locked Data Structures,"Doing so again re - duces the number of lock acquire/release points in the code, and t hus decreases the chances of accidentally introducing bugs (such as forget- ting to unlock before returning) into the code. Scaling Linked Lists Though we again have a basic concurrent linked list, once again w e are in a situation where it does not scale particularly well. One technique that researchers have explored to enable more concurrency with in a list is c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 LOCK -BASED CONCURRENT DATA STRUCTURES 1void List_Init(list_t *L) { 2L->head = NULL; 3pthread_mutex_init(&L->lock, NULL); 4} 5 6void List_Insert(list_t *L, int key) { 7// synchronization not needed 8node_t*new = malloc(sizeof(node_t)); 9if (new == NULL) { 10 perror(\""malloc\""); 11 return; 12 } 13 new->key = key; 14 15 // just lock critical section 16 pthread_mutex_lock(&L->lock); 17 new->next = L->head; 18 L->head = new; 19 pthread_mutex_unlock(&L->lock); 20} 21 22int List_Lookup(list_t *L, int key) { 23 int rv = -1; 24 pthread_mutex_lock(&L->lock); 25 node_t*curr = L->head; 26 while (curr) { 27 if (curr->key == key) { 28 rv = 0; 29 break; 30 } 31 curr = curr->next; 32 } 33 pthread_mutex_unlock(&L->lock); 34 return rv; // now both success and failure 35} Figure 29.8: Concurrent Linked List: Rewritten something called hand-over-hand locking (a.k.a. lock coupling ) [MS04]. The idea is pretty simple. Instead of having a single lock for the entire list, you instead add a lock per node of the list. When traversing t he list, the code ﬁrst grabs the next node’s lock and then releases th e current node’s lock (which inspires the name hand-over-hand). Conceptually, a hand-over-hand linked list makes some sense; i t en- ables a high degree of concurrency in list operations. However, in prac- tice, it is hard to make such a structure faster than the simpl e single lock approach, as the overheads of acquiring and releasing locks for ea ch node of a list traversal is prohibitive. Even with very large lists, and a large number of threads, the concurrency enabled by allowing multipl e on- going traversals is unlikely to be faster than simply grabbin g a single lock, performing an operation, and releasing it. Perhaps some kin d of hy- brid (where you grab a new lock every so many nodes) would be worth investigating. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 9 TIP: M ORE CONCURRENCY ISN’TNECESSARILY FASTER If the scheme you design adds a lot of overhead (for example, by acqu ir- ing and releasing locks frequently, instead of once), the fact t hat it is more concurrent may not be important. Simple schemes tend to work well, especially if they use costly routines rarely. Adding more locks and com- plexity can be your downfall. All of that said, there is one way to r eally know: build both alternatives (simple but less concurrent, and complex but more concurrent) and measure how they do. In the end, you can’t cheat on performance; your idea is either faster, or it isn’t.",3053
29. Locked Data Structures,"TIP: BEWARY OFLOCKS ANDCONTROL FLOW A general design tip, which is useful in concurrent code as well as elsewhere, is to be wary of control ﬂow changes that lead to functi on re- turns, exits, or other similar error conditions that halt the exec ution of a function. Because many functions will begin by acquiring a loc k, al- locating some memory, or doing other similar stateful operations, wh en errors arise, the code has to undo all of the state before returnin g, which is error-prone. Thus, it is best to structure code to minimize th is pattern. 29.3 Concurrent Queues As you know by now, there is always a standard method to make a concurrent data structure: add a big lock. For a queue, we’ll skip that approach, assuming you can ﬁgure it out. Instead, we’ll take a look at a slightly more concurrent queue desi gned by Michael and Scott [MS98]. The data structures and code used for t his queue are found in Figure 29.9 on the following page. If you study this code carefully, you’ll notice that there are two l ocks, one for the head of the queue, and one for the tail. The goal of these two locks is to enable concurrency of enqueue and dequeue operations. In the common case, the enqueue routine will only access the tail lock , and dequeue only the head lock. One trick used by Michael and Scott is to add a dummy node (allo- cated in the queue initialization code); this dummy enables th e separa- tion of head and tail operations. Study the code, or better yet, type i t in, run it, and measure it, to understand how it works deeply. Queues are commonly used in multi-threaded applications. Howev er, the type of queue used here (with just locks) often does not complete ly meet the needs of such programs. A more fully developed bounded queue, that enables a thread to wait if the queue is either emp ty or overly full, is the subject of our intense study in the next chapter on con dition variables. Watch for it. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 L OCK -BASED CONCURRENT DATA STRUCTURES 1typedef struct __node_t { 2int value; 3struct __node_t *next; 4} node_t; 5 6typedef struct __queue_t { 7node_t *head; 8node_t *tail; 9pthread_mutex_t headLock; 10 pthread_mutex_t tailLock; 11} queue_t; 12 13void Queue_Init(queue_t *q) { 14 node_t*tmp = malloc(sizeof(node_t)); 15 tmp->next = NULL; 16 q->head = q->tail = tmp; 17 pthread_mutex_init(&q->headLock, NULL); 18 pthread_mutex_init(&q->tailLock, NULL); 19} 20 21void Queue_Enqueue(queue_t *q, int value) { 22 node_t*tmp = malloc(sizeof(node_t)); 23 assert(tmp .= NULL); 24 tmp->value = value; 25 tmp->next = NULL; 26 27 pthread_mutex_lock(&q->tailLock); 28 q->tail->next = tmp; 29 q->tail = tmp; 30 pthread_mutex_unlock(&q->tailLock); 31} 32 33int Queue_Dequeue(queue_t *q, int*value) { 34 pthread_mutex_lock(&q->headLock); 35 node_t*tmp = q->head; 36 node_t*newHead = tmp->next; 37 if (newHead == NULL) { 38 pthread_mutex_unlock(&q->headLock); 39 return -1; // queue was empty 40 } 41 *value = newHead->value; 42 q->head = newHead; 43 pthread_mutex_unlock(&q->headLock); 44 free(tmp); 45 return 0; 46} Figure 29.9: Michael and Scott Concurrent Queue 29.4 Concurrent Hash Table We end our discussion with a simple and widely applicable concur rent data structure, the hash table.",3272
29. Locked Data Structures,"We’ll focus on a simple hash tabl e that does not resize; a little more work is required to handle resizing, wh ich we leave as an exercise for the reader (sorry.). This concurrent hash table is straightforward, is built using the con- current lists we developed earlier, and works incredibly well . The reason OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 11 1#define BUCKETS (101) 2 3typedef struct __hash_t { 4list_t lists[BUCKETS]; 5} hash_t; 6 7void Hash_Init(hash_t *H) { 8int i; 9for (i = 0; i < BUCKETS; i++) { 10 List_Init(&H->lists[i]); 11 } 12} 13 14int Hash_Insert(hash_t *H, int key) { 15 int bucket = key  percent BUCKETS; 16 return List_Insert(&H->lists[bucket], key); 17} 18 19int Hash_Lookup(hash_t *H, int key) { 20 int bucket = key  percent BUCKETS; 21 return List_Lookup(&H->lists[bucket], key); 22} Figure 29.10: A Concurrent Hash Table for its good performance is that instead of having a single lock for th e en- tire structure, it uses a lock per hash bucket (each of which is r epresented by a list). Doing so enables many concurrent operations to take pl ace. Figure 29.11 shows the performance of the hash table under concur - rent updates (from 10,000 to 50,000 concurrent updates from eac h of four threads, on the same iMac with four CPUs). Also shown, for the sake of comparison, is the performance of a linked list (with a single loc k). As you can see from the graph, this simple concurrent hash table s cales magniﬁcently; the linked list, in contrast, does not. 0 10 20 30 40051015 Inserts (Thousands)Time (seconds)Simple Concurrent List Concurrent Hash Table Figure 29.11: Scaling Hash Tables c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 L OCK -BASED CONCURRENT DATA STRUCTURES TIP: AVOID PREMATURE OPTIMIZATION (KNUTH ’SLAW) When building a concurrent data structure, start with the most basic ap- proach, which is to add a single big lock to provide synchronized a ccess. By doing so, you are likely to build a correct lock; if you then ﬁnd that it suffers from performance problems, you can reﬁne it, thus only mak ing it fast if need be. As Knuth famously stated, “Premature optimization is the root of all evil.” Many operating systems utilized a single lock when ﬁrst transi tioning to multiprocessors, including Sun OS and Linux. In the latter, t his lock even had a name, the big kernel lock (BKL ). For many years, this sim- ple approach was a good one, but when multi-CPU systems became the norm, only allowing a single active thread in the kernel at a time became a performance bottleneck. Thus, it was ﬁnally time to add the opt imiza- tion of improved concurrency to these systems. Within Linux, the more straightforward approach was taken: replace one lock with many. Within Sun, a more radical decision was made: build a brand new operating sys- tem, known as Solaris, that incorporates concurrency more fundamen - tally from day one. Read the Linux and Solaris kernel books for more information about these fascinating systems [BC05, MM00].",3049
29. Locked Data Structures,"29.5 Summary We have introduced a sampling of concurrent data structures, fr om counters, to lists and queues, and ﬁnally to the ubiquitous and heavily- used hash table. We have learned a few important lessons along th e way: to be careful with acquisition and release of locks around control ﬂ ow changes; that enabling more concurrency does not necessarily in crease performance; that performance problems should only be remedied on ce they exist. This last point, of avoiding premature optimization , is cen- tral to any performance-minded developer; there is no value in making something faster if doing so will not improve the overall performan ce of the application. Of course, we have just scratched the surface of high performanc e structures. See Moir and Shavit’s excellent survey for more informa tion, as well as links to other sources [MS04]. In particular, you might be inter- ested in other structures (such as B-trees); for this knowledge , a database class is your best bet. You also might be interested in techniqu es that don’t use traditional locks at all; such non-blocking data structures are some- thing we’ll get a taste of in the chapter on common concurrency bugs, but frankly this topic is an entire area of knowledge requiring m ore study than is possible in this humble book. Find out more on your own if you are interested (as always.). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCK -BASED CONCURRENT DATA STRUCTURES 13 References [B+10] “An Analysis of Linux Scalability to Many Cores” by Silas Boy d-Wickizer, Austin T. Clements, Yandong Mao, Aleksey Pesterev, M. Frans Kaashoek, Robert Mo rris, Nickolai Zel- dovich . OSDI ’10, Vancouver, Canada, October 2010. A great study of how Linux performs on multicore machines, as well as some simple solutions. Includes a neat sloppy counter to solve one form of the scalable counting problem. [BH73] “Operating System Principles” by Per Brinch Hansen. Prentice-Hall, 1 973. Available: http://portal.acm.org/citation.cfm?id=540365 .One of the ﬁrst books on operating systems; certainly ahead of its time. Introduced monitors as a concurrency pri mitive. [BC05] “Understanding the Linux Kernel (Third Edition)” by Daniel P . Bovet and Marco Cesati. O’Reilly Media, November 2005. The classic book on the Linux kernel. You should read it. [C06] “The Search For Fast, Scalable Counters” by Jonathan Corbet. Feb ruary 1, 2006. Avail- able:https://lwn.net/Articles/170003 .LWN has many wonderful articles about the latest in Linux This article is a short description of scalable approximate countin g; read it, and others, to learn more about the latest in Linux. [L+13] “A Study of Linux File System Evolution” by Lanyue Lu, Andre a C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, Shan Lu. FAST ’13, San Jose, CA, Februar y 2013. Our paper that studies every patch to Linux ﬁle systems over nearly a decade. Lots of fun ﬁ ndings in there; read it to see. The work was painful to do though; the poor graduate student, Lanyue Lu, had to look through every single patch by hand in order to understand what they did.",3088
29. Locked Data Structures,"[MS98] “Nonblocking Algorithms and Preemption-safe Locking on by Multipro grammed Shared- memory Multiprocessors. ” M. Michael, M. Scott. Journal of Parallel and Di stributed Com- puting, Vol. 51, No. 1, 1998 Professor Scott and his students have been at the forefront of concurrent algorithms and data structures for many years; check out his web page, numer ous papers, or books to ﬁnd out more. [MS04] “Concurrent Data Structures” by Mark Moir and Nir Shavit. In Handb ook of Data Structures and Applications (Editors D. Metha and S.Sahni). Chapman and Ha ll/CRC Press, 2004. Available: www.cs.tau.ac.il/˜shanir/concurrent-data-structures .pdf . A short but relatively comprehensive reference on concurrent data str uctures. Though it is missing some of the latest works in the area (due to its age), it remains an incredibly use ful reference. [MM00] “Solaris Internals: Core Kernel Architecture” by Jim Mauro and Richa rd McDougall. Prentice Hall, October 2000. The Solaris book. You should also read this, if you want to learn about something other than Linux. [S+11] “Making the Common Case the Only Case with Anticipatory Memory Al location” by Swaminathan Sundararaman, Yupu Zhang, Sriram Subramanian, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau . FAST ’11, San Jose, CA, February 2011. Our work on removing possibly-failing allocation calls from kernel code paths. By allocating all potenti ally needed memory before doing any work, we avoid failure deep down in the storage stack. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 L OCK -BASED CONCURRENT DATA STRUCTURES Homework (Code) In this homework, you’ll gain some experience with writing concur- rent code and measuring its performance. Learning to build code that performs well is a critical skill and thus gaining a little exp erience here with it is quite worthwhile. Questions 1. We’ll start by redoing the measurements within this chapter. Use the call gettimeofday() to measure time within your program. How accurate is this timer? What is the smallest interval it can measure? Gain con ﬁdence in its workings, as we will need it in all subsequent questions. Y ou can also look into other timers, such as the cycle counter available on x86 via the rdtsc instruction. 2. Now, build a simple concurrent counter and measure how long it tak es to increment the counter many times as the number of threads increases . How many CPUs are available on the system you are using? Does this numbe r impact your measurements at all? 3. Next, build a version of the sloppy counter. Once again, measure its per- formance as the number of threads varies, as well as the threshol d. Do the numbers match what you see in the chapter? 4. Build a version of a linked list that uses hand-over-hand loc king [MS04], as cited in the chapter. You should read the paper ﬁrst to understa nd how it works, and then implement it. Measure its performance. When does a hand- over-hand list work better than a standard list as shown in th e chapter? 5. Pick your favorite interesting data structure, such as a B-tre e or other slightly more interested structure. Implement it, and start with a simple lo cking strategy such as a single lock. Measure its performance as the numb er of concurrent threads increases. 6. Finally, think of a more interesting locking strategy for t his favorite data structure of yours. Implement it, and measure its performance. How do es it compare to the straightforward locking approach? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3529
30. Condition Variables,"30 Condition Variables Thus far we have developed the notion of a lock and seen how one can be properly built with the right combination of hardware and OS supp ort. Unfortunately, locks are not the only primitives that are needed to build concurrent programs. In particular, there are many cases where a thread wishes to c heck whether a condition is true before continuing its execution. For example, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a join() ); how should such a wait be implemented? Let’s look at Figure 30.1. 1void*child(void *arg) { 2printf(\""child \""); 3// XXX how to indicate we are done? 4return NULL; 5} 6 7int main(int argc, char *argv[]) { 8printf(\""parent: begin \""); 9pthread_t c; 10 Pthread_create(&c, NULL, child, NULL); // create child 11 // XXX how to wait for child? 12 printf(\""parent: end \""); 13 return 0; 14} Figure 30.1: A Parent Waiting For Its Child What we would like to see here is the following output: parent: begin child parent: end We could try using a shared variable, as you see in Figure 30.2. T his solution will generally work, but it is hugely inefﬁcient as the parent spins and wastes CPU time. What we would like here instead is some way t o put the parent to sleep until the condition we are waiting for (e. g., the child is done executing) comes true. 1 2 CONDITION VARIABLES 1volatile int done = 0; 2 3void*child(void *arg) { 4printf(\""child \""); 5done = 1; 6return NULL; 7} 8 9int main(int argc, char *argv[]) { 10 printf(\""parent: begin \""); 11 pthread_t c; 12 Pthread_create(&c, NULL, child, NULL); // create child 13 while (done == 0) 14 ; // spin 15 printf(\""parent: end \""); 16 return 0; 17} Figure 30.2: Parent Waiting For Child: Spin-based Approach THECRUX: HOWTOWAITFORA C ONDITION In multi-threaded programs, it is often useful for a thread to wa it for some condition to become true before proceeding. The simple approac h, of just spinning until the condition becomes true, is grossly inef ﬁcient and wastes CPU cycles, and in some cases, can be incorrect. Thus , how should a thread wait for a condition? 30.1 Deﬁnition and Routines To wait for a condition to become true, a thread can make use of what is known as a condition variable . A condition variable is an explicit queue that threads can put themselves on when some state of execu tion (i.e., some condition ) is not as desired (by waiting on the condition); some other thread, when it changes said state, can then wake one ( or more) of those waiting threads and thus allow them to continue (by sig- naling on the condition). The idea goes back to Dijkstra’s use of “private semaphores” [D68]; a similar idea was later named a “condition v ariable” by Hoare in his work on monitors [H74]. To declare such a condition variable, one simply writes somethin g like this:pthread condt c; , which declares cas a condition variable (note: proper initialization is also required). A condition vari able has two operations associated with it: wait() andsignal() .",3050
30. Condition Variables,"Thewait() call is executed when a thread wishes to put itself to sleep; the signal() call is executed when a thread has changed something in the program a nd thus wants to wake a sleeping thread waiting on this condition. Sp eciﬁ- cally, the POSIX calls look like this: pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m); pthread_cond_signal(pthread_cond_t *c); OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 3 1int done = 0; 2pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; 3pthread_cond_t c = PTHREAD_COND_INITIALIZER; 4 5void thr_exit() { 6Pthread_mutex_lock(&m); 7done = 1; 8Pthread_cond_signal(&c); 9Pthread_mutex_unlock(&m); 10} 11 12void*child(void *arg) { 13 printf(\""child \""); 14 thr_exit(); 15 return NULL; 16} 17 18void thr_join() { 19 Pthread_mutex_lock(&m); 20 while (done == 0) 21 Pthread_cond_wait(&c, &m); 22 Pthread_mutex_unlock(&m); 23} 24 25int main(int argc, char *argv[]) { 26 printf(\""parent: begin \""); 27 pthread_t p; 28 Pthread_create(&p, NULL, child, NULL); 29 thr_join(); 30 printf(\""parent: end \""); 31 return 0; 32} Figure 30.3: Parent Waiting For Child: Use A Condition Variable We will often refer to these as wait() andsignal() for simplicity. One thing you might notice about the wait() call is that it also takes a mutex as a parameter; it assumes that this mutex is locked when wait() is called. The responsibility of wait() is to release the lock and put the calling thread to sleep (atomically); when the thread wakes u p (after some other thread has signaled it), it must re-acquire the lock befor e returning to the caller. This complexity stems from the desire to prevent certain race conditions from occurring when a thread is trying to put itse lf to sleep. Let’s take a look at the solution to the join problem (Figure 30 .3) to understand this better. There are two cases to consider. In the ﬁrst, the parent create s the child thread but continues running itself (assume we have only a sing le pro- cessor) and thus immediately calls into thrjoin() to wait for the child thread to complete. In this case, it will acquire the lock, chec k if the child is done (it is not), and put itself to sleep by calling wait() (hence releas- ing the lock). The child will eventually run, print the messag e “child”, and callthrexit() to wake the parent thread; this code just grabs the lock, sets the state variable done , and signals the parent thus waking it. Finally, the parent will run (returning from wait() with the lock held), unlock the lock, and print the ﬁnal message “parent: end”. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 CONDITION VARIABLES In the second case, the child runs immediately upon creation, se ts done to 1, calls signal to wake a sleeping thread (but there is none, so it just returns), and is done. The parent then runs, calls thrjoin() , sees thatdone is 1, and thus does not wait and returns. One last note: you might observe the parent uses a while loop instead of just anifstatement when deciding whether to wait on the condition.",3040
30. Condition Variables,"While this does not seem strictly necessary per the logic of the pr ogram, it is always a good idea, as we will see below. To make sure you understand the importance of each piece of the threxit() andthrjoin() code, let’s try a few alternate implemen- tations. First, you might be wondering if we need the state varia bledone . What if the code looked like the example below? Would this work? 1void thr_exit() { 2Pthread_mutex_lock(&m); 3Pthread_cond_signal(&c); 4Pthread_mutex_unlock(&m); 5} 6 7void thr_join() { 8Pthread_mutex_lock(&m); 9Pthread_cond_wait(&c, &m); 10 Pthread_mutex_unlock(&m); 11} Unfortunately this approach is broken. Imagine the case where t he child runs immediately and calls threxit() immediately; in this case, the child will signal, but there is no thread asleep on the condi tion. When the parent runs, it will simply call wait and be stuck; no thre ad will ever wake it. From this example, you should appreciate the importance of the state variable done ; it records the value the threads are interested in knowing. The sleeping, waking, and locking all are built around it. Here is another poor implementation. In this example, we imagine that one does not need to hold a lock in order to signal and wait. What problem could occur here? Think about it. 1void thr_exit() { 2done = 1; 3Pthread_cond_signal(&c); 4} 5 6void thr_join() { 7if (done == 0) 8 Pthread_cond_wait(&c); 9} The issue here is a subtle race condition. Speciﬁcally, if the pa rent calls thrjoin() and then checks the value of done , it will see that it is 0 and thus try to go to sleep. But just before it calls wait to go to sle ep, the parent is interrupted, and the child runs. The child changes the sta te variable done to 1 and signals, but no thread is waiting and thus no thread is woken. When the parent runs again, it sleeps forever, which is s ad. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 5 TIP: ALWAYS HOLD THELOCK WHILE SIGNALING Although it is strictly not necessary in all cases, it is likely simplest and best to hold the lock while signaling when using condition variab les. The example above shows a case where you must hold the lock for correct- ness; however, there are some other cases where it is likely OK not to, but probably is something you should avoid. Thus, for simplicity, hold the lock when calling signal . The converse of this tip, i.e., hold the lock when calling wait, is not just a tip, but rather mandated by the semantics of wait, because wa it always (a) assumes the lock is held when you call it, (b) releases said l ock when putting the caller to sleep, and (c) re-acquires the lock just before return- ing. Thus, the generalization of this tip is correct: hold the lock when calling signal or wait , and you will always be in good shape. Hopefully, from this simple join example, you can see some of the ba- sic requirements of using condition variables properly. To make sure you understand, we now go through a more complicated example: the pro- ducer/consumer orbounded-buffer problem. 30.2 The Producer/Consumer (Bounded Buffer) Problem The next synchronization problem we will confront in this chapter is known as the producer/consumer problem, or sometimes as the bounded buffer problem, which was ﬁrst posed by Dijkstra [D72]. Indeed, it was this very producer/consumer problem that led Dijkstra and his c o-workers to invent the generalized semaphore (which can be used as eith er a lock or a condition variable) [D01]; we will learn more about semaphores later.",3536
30. Condition Variables,"Imagine one or more producer threads and one or more consumer threads. Producers generate data items and place them in a buf fer; con- sumers grab said items from the buffer and consume them in some wa y. This arrangement occurs in many real systems. For example, in a multi-threaded web server, a producer puts HTTP requests int o a work queue (i.e., the bounded buffer); consumer threads take reque sts out of this queue and process them. A bounded buffer is also used when you pipe the output of one pro- gram into another, e.g., grep foo file.txt | wc -l . This example runs two processes concurrently; grep writes lines from file.txt with the string foo in them to what it thinks is standard output; the U NIX shell redirects the output to what is called a U NIX pipe (created by the pipe system call). The other end of this pipe is connected to the stan- dard input of the process wc, which simply counts the number of lines in the input stream and prints out the result. Thus, the grep process is the producer; the wcprocess is the consumer; between them is an in-kernel bounded buffer; you, in this example, are just the happy user. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 CONDITION VARIABLES 1int buffer; 2int count = 0; // initially, empty 3 4void put(int value) { 5assert(count == 0); 6count = 1; 7buffer = value; 8} 9 10int get() { 11 assert(count == 1); 12 count = 0; 13 return buffer; 14} Figure 30.4: The Put And Get Routines (Version 1) 1void*producer(void *arg) { 2int i; 3int loops = (int) arg; 4for (i = 0; i < loops; i++) { 5 put(i); 6} 7} 8 9void*consumer(void *arg) { 10 int i; 11 while (1) { 12 int tmp = get(); 13 printf(\"" percentd \"", tmp); 14 } 15} Figure 30.5: Producer/Consumer Threads (Version 1) Because the bounded buffer is a shared resource, we must of course require synchronized access to it, lest1a race condition arise. To begin to understand this problem better, let us examine some actual code . The ﬁrst thing we need is a shared buffer, into which a producer puts data, and out of which a consumer takes data. Let’s just use a singl e integer for simplicity (you can certainly imagine placing a poi nter to a data structure into this slot instead), and the two inner routi nes to put a value into the shared buffer, and to get a value out of the buffe r. See Figure 30.4 for details. Pretty simple, no? The put() routine assumes the buffer is empty (and checks this with an assertion), and then simply puts a val ue into the shared buffer and marks it full by setting count to 1. Theget() routine does the opposite, setting the buffer to empty (i.e., setting count to 0) and returning the value. Don’t worry that this shared buffer has just a single entry; later, we’ll generalize it to a queue that can hol d multiple entries, which will be even more fun than it sounds. Now we need to write some routines that know when it is OK to access the buffer to either put data into it or get data out of it. The condi tions for this should be obvious: only put data into the buffer when count is zero 1This is where we drop some serious Old English on you, and the subjunctiv e form.",3138
30. Condition Variables,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 7 1int loops; // must initialize somewhere... 2cond_t cond; 3mutex_t mutex; 4 5void*producer(void *arg) { 6int i; 7for (i = 0; i < loops; i++) { 8 Pthread_mutex_lock(&mutex); // p1 9 if (count == 1) // p2 10 Pthread_cond_wait(&cond, &mutex); // p3 11 put(i); // p4 12 Pthread_cond_signal(&cond); // p5 13 Pthread_mutex_unlock(&mutex); // p6 14 } 15} 16 17void*consumer(void *arg) { 18 int i; 19 for (i = 0; i < loops; i++) { 20 Pthread_mutex_lock(&mutex); // c1 21 if (count == 0) // c2 22 Pthread_cond_wait(&cond, &mutex); // c3 23 int tmp = get(); // c4 24 Pthread_cond_signal(&cond); // c5 25 Pthread_mutex_unlock(&mutex); // c6 26 printf(\"" percentd \"", tmp); 27 } 28} Figure 30.6: Producer/Consumer: Single CV And If Statement (i.e., when the buffer is empty), and only get data from the buff er when count is one (i.e., when the buffer is full). If we write the synchroni zation code such that a producer puts data into a full buffer, or a consume r gets data from an empty one, we have done something wrong (and in this code, an assertion will ﬁre). This work is going to be done by two types of threads, one set of which we’ll call the producer threads, and the other set which we’ll call con- sumer threads. Figure 30.5 shows the code for a producer that puts an integer into the shared buffer loops number of times, and a consumer that gets the data out of that shared buffer (forever), each time printing out the data item it pulled from the shared buffer. A Broken Solution Now imagine that we have just a single producer and a single consu mer. Obviously the put() andget() routines have critical sections within them, asput() updates the buffer, and get() reads from it. However, putting a lock around the code doesn’t work; we need something more. Not surprisingly, that something more is some condition variables . In this (broken) ﬁrst try (Figure 30.6), we have a single condition vari ablecond and associated lock mutex . Let’s examine the signaling logic between producers and consume rs. When a producer wants to ﬁll the buffer, it waits for it to be empt y (p1– p3). The consumer has the exact same logic, but waits for a differ ent c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 CONDITION VARIABLES Tc1 State T c2 State Tp State Count Comment c1 Running Ready Ready 0 c2 Running Ready Ready 0 c3 Sleep Ready Ready 0 Nothing to get Sleep Ready p1 Running 0 Sleep Ready p2 Running 0 Sleep Ready p4 Running 1 Buffer now full Ready Ready p5 Running 1 T c1awoken Ready Ready p6 Running 1 Ready Ready p1 Running 1 Ready Ready p2 Running 1 Ready Ready p3 Sleep 1 Buffer full; sleep Ready c1 Running Sleep 1 T c2sneaks in ... Ready c2 Running Sleep 1 Ready c4 Running Sleep 0 ... and grabs data Ready c5 Running Ready 0 T pawoken Ready c6 Running Ready 0 c4 Running Ready Ready 0 Oh oh. No data Figure 30.7: Thread Trace: Broken Solution (Version 1) condition: fullness (c1–c3). With just a single producer and a single consumer, the code in Fig ure 30.6 works.",3057
30. Condition Variables,"However, if we have more than one of these threads (e.g. , two consumers), the solution has two critical problems. What are they? ... (pause here to think) ... Let’s understand the ﬁrst problem, which has to do with the ifstate- ment before the wait. Assume there are two consumers ( Tc1andTc2) and one producer ( Tp). First, a consumer ( Tc1) runs; it acquires the lock (c1), checks if any buffers are ready for consumption (c2), and ﬁnding that none are, waits (c3) (which releases the lock). Then the producer ( Tp) runs. It acquires the lock (p1), checks if all buffers are full (p2), and ﬁnding that not to be the case, goes ah ead and ﬁlls the buffer (p4). The producer then signals that a buffer h as been ﬁlled (p5). Critically, this moves the ﬁrst consumer ( Tc1) from sleeping on a condition variable to the ready queue; Tc1is now able to run (but not yet running). The producer then continues until realizing t he buffer is full, at which point it sleeps (p6, p1–p3). Here is where the problem occurs: another consumer ( Tc2) sneaks in and consumes the one existing value in the buffer (c1, c2, c4, c5 , c6, skip- ping the wait at c3 because the buffer is full). Now assume Tc1runs; just before returning from the wait, it re-acquires the lock and then returns. It then callsget() (c4), but there are no buffers to consume. An assertion triggers, and the code has not functioned as desired. Clearly, w e should have somehow prevented Tc1from trying to consume because Tc2snuck in and consumed the one value in the buffer that had been produced . Fig- ure 30.7 shows the action each thread takes, as well as its sched uler state (Ready, Running, or Sleeping) over time. The problem arises for a simple reason: after the producer woke Tc1, butbeforeTc1ever ran, the state of the bounded buffer changed (thanks to Tc2). Signaling a thread only wakes them up; it is thus a hint that the state OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 9 1int loops; 2cond_t cond; 3mutex_t mutex; 4 5void*producer(void *arg) { 6int i; 7for (i = 0; i < loops; i++) { 8 Pthread_mutex_lock(&mutex); // p1 9 while (count == 1) // p2 10 Pthread_cond_wait(&cond, &mutex); // p3 11 put(i); // p4 12 Pthread_cond_signal(&cond); // p5 13 Pthread_mutex_unlock(&mutex); // p6 14 } 15} 16 17void*consumer(void *arg) { 18 int i; 19 for (i = 0; i < loops; i++) { 20 Pthread_mutex_lock(&mutex); // c1 21 while (count == 0) // c2 22 Pthread_cond_wait(&cond, &mutex); // c3 23 int tmp = get(); // c4 24 Pthread_cond_signal(&cond); // c5 25 Pthread_mutex_unlock(&mutex); // c6 26 printf(\"" percentd \"", tmp); 27 } 28} Figure 30.8: Producer/Consumer: Single CV And While of the world has changed (in this case, that a value has been plac ed in the buffer), but there is no guarantee that when the woken thread r uns, the state will still be as desired. This interpretation of what a signal means is often referred to as Mesa semantics , after the ﬁrst research that built a condition variable in such a manner [LR80]; the contrast, refe rred to as Hoare semantics , is harder to build but provides a stronger guarantee that the woken thread will run immediately upon being woken [H74 ].",3183
30. Condition Variables,"Virtually every system ever built employs Mesa semantics. Better, But Still Broken: While, Not If Fortunately, this ﬁx is easy (Figure 30.8): change the ifto awhile . Think about why this works; now consumer Tc1wakes up and (with the lock held) immediately re-checks the state of the shared variable (c2). If the buffer is empty at that point, the consumer simply goes back to sl eep (c3). The corollary ifis also changed to a while in the producer (p2). Thanks to Mesa semantics, a simple rule to remember with condi tion variables is to always use while loops . Sometimes you don’t have to re- check the condition, but it is always safe to do so; just do it and b e happy. However, this code still has a bug, the second of two problems men- tioned above. Can you see it? It has something to do with the fact th at there is only one condition variable. Try to ﬁgure out what the probl em is, before reading ahead. DO IT. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 CONDITION VARIABLES Tc1 State T c2 State Tp State Count Comment c1 Running Ready Ready 0 c2 Running Ready Ready 0 c3 Sleep Ready Ready 0 Nothing to get Sleep c1 Running Ready 0 Sleep c2 Running Ready 0 Sleep c3 Sleep Ready 0 Nothing to get Sleep Sleep p1 Running 0 Sleep Sleep p2 Running 0 Sleep Sleep p4 Running 1 Buffer now full Ready Sleep p5 Running 1 T c1awoken Ready Sleep p6 Running 1 Ready Sleep p1 Running 1 Ready Sleep p2 Running 1 Ready Sleep p3 Sleep 1 Must sleep (full) c2 Running Sleep Sleep 1 Recheck condition c4 Running Sleep Sleep 0 T c1grabs data c5 Running Ready Sleep 0 Oops. Woke T c2 c6 Running Ready Sleep 0 c1 Running Ready Sleep 0 c2 Running Ready Sleep 0 c3 Sleep Ready Sleep 0 Nothing to get Sleep c2 Running Sleep 0 Sleep c3 Sleep Sleep 0 Everyone asleep... Figure 30.9: Thread Trace: Broken Solution (Version 2) ... (another pause for you to think, or close your eyes for a bit) ... Let’s conﬁrm you ﬁgured it out correctly, or perhaps let’s conﬁrm that you are now awake and reading this part of the book. The problem oc- curs when two consumers run ﬁrst ( Tc1andTc2) and both go to sleep (c3). Then, the producer runs, puts a value in the buffer, and wakes on e of the consumers (say Tc1). The producer then loops back (releasing and reac- quiring the lock along the way) and tries to put more data in the bu ffer; because the buffer is full, the producer instead waits on the con dition (thus sleeping). Now, one consumer is ready to run ( Tc1), and two threads are sleeping on a condition ( Tc2andTp). We are about to cause a problem: things are getting exciting. The consumer Tc1then wakes by returning from wait() (c3), re-checks the condition (c2), and ﬁnding the buffer full, consumes the val ue (c4). This consumer then, critically, signals on the condition (c5), w aking only onethread that is sleeping. However, which thread should it wake? Because the consumer has emptied the buffer, it clearly should wake the producer. However, if it wakes the consumer Tc2(which is deﬁnitely possible, depending on how the wait queue is managed), we have a p rob- lem. Speciﬁcally, the consumer Tc2will wake up and ﬁnd the buffer empty (c2), and go back to sleep (c3). The producer Tp, which has a value to put into the buffer, is left sleeping. The other consumer thr ead,Tc1, also goes back to sleep.",3313
30. Condition Variables,"All three threads are left sleeping, a clear bug; see Figure 30.9 for the brutal step-by-step of this terrible calam ity. Signaling is clearly needed, but must be more directed. A consum er should not wake other consumers, only producers, and vice-versa. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 11 1cond_t empty, fill; 2mutex_t mutex; 3 4void*producer(void *arg) { 5int i; 6for (i = 0; i < loops; i++) { 7 Pthread_mutex_lock(&mutex); 8 while (count == 1) 9 Pthread_cond_wait(&empty, &mutex); 10 put(i); 11 Pthread_cond_signal(&fill); 12 Pthread_mutex_unlock(&mutex); 13 } 14} 15 16void*consumer(void *arg) { 17 int i; 18 for (i = 0; i < loops; i++) { 19 Pthread_mutex_lock(&mutex); 20 while (count == 0) 21 Pthread_cond_wait(&fill, &mutex); 22 int tmp = get(); 23 Pthread_cond_signal(&empty); 24 Pthread_mutex_unlock(&mutex); 25 printf(\"" percentd \"", tmp); 26 } 27} Figure 30.10: Producer/Consumer: Two CVs And While The Single Buffer Producer/Consumer Solution The solution here is once again a small one: use twocondition variables, instead of one, in order to properly signal which type of thread shou ld wake up when the state of the system changes. Figure 30.10 shows the resulting code. In the code above, producer threads wait on the condition empty , and signals ﬁll. Conversely, consumer threads wait on ﬁlland signal empty . By doing so, the second problem above is avoided by design: a consumer can never accidentally wake a consumer, and a producer can neve r acci- dentally wake a producer. The Correct Producer/Consumer Solution We now have a working producer/consumer solution, albeit not a fully general one. The last change we make is to enable more concurrenc y and efﬁciency; speciﬁcally, we add more buffer slots, so that mult iple values can be produced before sleeping, and similarly multiple value s can be consumed before sleeping. With just a single producer and consum er, this approach is more efﬁcient as it reduces context switches; with m ultiple producers or consumers (or both), it even allows concurrent producin g or consuming to take place, thus increasing concurrency. Fortun ately, it is a small change from our current solution. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 CONDITION VARIABLES 1int buffer[MAX]; 2int fill_ptr = 0; 3int use_ptr = 0; 4int count = 0; 5 6void put(int value) { 7buffer[fill_ptr] = value; 8fill_ptr = (fill_ptr + 1)  percent MAX; 9count++; 10} 11 12int get() { 13 int tmp = buffer[use_ptr]; 14 use_ptr = (use_ptr + 1)  percent MAX; 15 count--; 16 return tmp; 17} Figure 30.11: The Correct Put And Get Routines 1cond_t empty, fill; 2mutex_t mutex; 3 4void*producer(void *arg) { 5int i; 6for (i = 0; i < loops; i++) { 7 Pthread_mutex_lock(&mutex); // p1 8 while (count == MAX) // p2 9 Pthread_cond_wait(&empty, &mutex); // p3 10 put(i); // p4 11 Pthread_cond_signal(&fill); // p5 12 Pthread_mutex_unlock(&mutex); // p6 13 } 14} 15 16void*consumer(void *arg) { 17 int i; 18 for (i = 0; i < loops; i++) { 19 Pthread_mutex_lock(&mutex); // c1 20 while (count == 0) // c2 21 Pthread_cond_wait(&fill, &mutex); // c3 22 int tmp = get(); // c4 23 Pthread_cond_signal(&empty); // c5 24 Pthread_mutex_unlock(&mutex); // c6 25 printf(\"" percentd \"", tmp); 26 } 27} Figure 30.12: The Correct Producer/Consumer Synchronization The ﬁrst change for this correct solution is within the buffer str ucture itself and the corresponding put() andget() (Figure 30.11).",3457
30. Condition Variables,"We also slightly change the conditions that producers and consumers che ck in or- der to determine whether to sleep or not. Figure 30.12 shows the c orrect waiting and signaling logic. A producer only sleeps if all buffe rs are cur- rently ﬁlled (p2); similarly, a consumer only sleeps if all buf fers are cur- rently empty (c2). And thus we solve the producer/consumer probl em; time to sit back and drink a cold one. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 13 TIP: USEWHILE (NOTIF) FORCONDITIONS When checking for a condition in a multi-threaded program, using awhile loop is always correct; using an ifstatement only might be, depending on the semantics of signaling. Thus, always use while and your code will behave as expected. Using while loops around conditional checks also handles the case where spurious wakeups occur. In some thread packages, due to de- tails of the implementation, it is possible that two threads get woken up though just a single signal has taken place [L11]. Spurious wake ups are further reason to re-check the condition a thread is waiting on. 30.3 Covering Conditions We’ll now look at one more example of how condition variables can be used. This code study is drawn from Lampson and Redell’s paper on Pilot [LR80], the same group who ﬁrst implemented the Mesa semantics described above (the language they used was Mesa, hence the na me). The problem they ran into is best shown via simple example, in th is case in a simple multi-threaded memory allocation library. Fig ure 30.13 shows a code snippet which demonstrates the issue. As you might see in the code, when a thread calls into the memory allocation code, it might have to wait in order for more memory to be- come free. Conversely, when a thread frees memory, it signals th at more memory is free. However, our code above has a problem: which waiting thread (there can be more than one) should be woken up? Consider the following scenario. Assume there are zero bytes fre e; threadTacallsallocate(100) , followed by thread Tbwhich asks for less memory by calling allocate(10) . BothTaandTbthus wait on the condition and go to sleep; there aren’t enough free bytes to satis fy either of these requests. At that point, assume a third thread, Tc, callsfree(50) . Unfortu- nately, when it calls signal to wake a waiting thread, it migh t not wake the correct waiting thread, Tb, which is waiting for only 10 bytes to be freed;Tashould remain waiting, as not enough memory is yet free. Thus, the code in the ﬁgure does not work, as the thread waking other threa ds does not know which thread (or threads) to wake up. The solution suggested by Lampson and Redell is straightforward : re- place thepthread condsignal() call in the code above with a call to pthread condbroadcast() , which wakes up allwaiting threads. By doing so, we guarantee that any threads that should be woken are. T he downside, of course, can be a negative performance impact, as we m ight needlessly wake up many other waiting threads that shouldn’t (y et) be awake.",3058
30. Condition Variables,"Those threads will simply wake up, re-check the conditi on, and then go immediately back to sleep. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 CONDITION VARIABLES 1// how many bytes of the heap are free? 2int bytesLeft = MAX_HEAP_SIZE; 3 4// need lock and condition too 5cond_t c; 6mutex_t m; 7 8void* 9allocate(int size) { 10 Pthread_mutex_lock(&m); 11 while (bytesLeft < size) 12 Pthread_cond_wait(&c, &m); 13 void*ptr = ...; // get mem from heap 14 bytesLeft -= size; 15 Pthread_mutex_unlock(&m); 16 return ptr; 17} 18 19void free(void *ptr, int size) { 20 Pthread_mutex_lock(&m); 21 bytesLeft += size; 22 Pthread_cond_signal(&c); // whom to signal?? 23 Pthread_mutex_unlock(&m); 24} Figure 30.13: Covering Conditions: An Example Lampson and Redell call such a condition a covering condition , as it covers all the cases where a thread needs to wake up (conservati vely); the cost, as we’ve discussed, is that too many threads might be wok en. The astute reader might also have noticed we could have used thi s ap- proach earlier (see the producer/consumer problem with only a sin gle condition variable). However, in that case, a better solution was avail- able to us, and thus we used it. In general, if you ﬁnd that your pr ogram only works when you change your signals to broadcasts (but you don’t think it should need to), you probably have a bug; ﬁx it. But in case s like the memory allocator above, broadcast may be the most straightforwa rd solution available. 30.4 Summary We have seen the introduction of another important synchronization primitive beyond locks: condition variables. By allowing thread s to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, including the fa mous (and still important) producer/consumer problem, as well as cove ring conditions. A more dramatic concluding sentence would go here, su ch as “He loved Big Brother” [O49]. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG CONDITION VARIABLES 15 References [D68] “Cooperating sequential processes” by Edsger W. Dijkstra . 1968. Available online here: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.P DF.Another classic from Di- jkstra; reading his early works on concurrency will teach you much of what you n eed to know. [D72] “Information Streams Sharing a Finite Buffer” by E.W. Dijkstr a. Information Processing Letters 1: 179180, 1972. Available: http://www.cs.utexas.e du/users/EWD/ewd03xx/EWD329.PDF The famous paper that introduced the producer/consumer problem. [D01] “My recollections of operating system design” by E.W. Dijkstr a. April, 2001. Avail- able:http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303. PDF.A fascinating read for those of you interested in how the pioneers of our ﬁeld came up with some v ery basic and fundamental concepts, including ideas like “interrupts” and even “a stac k”. [H74] “Monitors: An Operating System Structuring Concept” by C.A.R. H oare. Communica- tions of the ACM, 17:10, pages 549–557, October 1974.",3041
30. Condition Variables,"Hoare did a fair amount of theoretical work in concurrency. However, he is still probably most known for his work on Qu icksort, the coolest sorting algorithm in the world, at least according to these authors. [L11] “Pthread cond signal Man Page” by Mysterious author. March, 2011. Available online: http://linux.die.net/man/3/pthread condsignal .The Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to race con ditions within the sig- nal/wakeup code. [LR80] “Experience with Processes and Monitors in Mesa” by B.W. Lampso n, D.R. Redell. Communications of the ACM. 23:2, pages 105-117, February 1980. A terriﬁc paper about how to actually implement signaling and condition variables in a real system, l eading to the term “Mesa” semantics for what it mzshortns to be woken up; the older semantics, developed by Tony Hoare [H74], then became known as “Hoare” semantics, which is hard to say out loud in class wi th a straight face. [O49] “1984” by George Orwell. Secker and Warburg, 1949. A little heavy-handed, but of course a must read. That said, we kind of gave away the ending by quoting the last sente nce. Sorry. And if the government is reading this, let us just say that we think that the govern ment is “double plus good”. Hear that, our pals at the NSA? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 CONDITION VARIABLES Homework (Code) This homework lets you explore some real code that uses locks and condition variables to implement various forms of the producer/con sumer queue discussed in the chapter. You’ll look at the real code, run it in various conﬁgurations, and use it to learn about what works and wha t doesn’t, as well as other intricacies. Read the README for detail s. Questions 1. Our ﬁrst question focuses on main-two-cvs-while.c (the working so- lution). First, study the code. Do you think you have an understa nding of what should happen when you run the program? 2. Run with one producer and one consumer, and have the producer pro duce a few values. Start with a buffer (size 1), and then increase it. How does the behavior of the code change with larger buffers? (or does it?) What would you predict numfull to be with different buffer sizes (e.g., -m 10 ) and different numbers of produced items (e.g., -l 100 ), when you change the consumer sleep string from default (no sleep) to -C 0,0,0,0,0,0,1 ? 3. If possible, run the code on different systems (e.g., a Mac an d Linux). Do you see different behavior across these systems? 4. Let’s look at some timings. How long do you think the followin g execution, with one producer, three consumers, a single-entry shared buffe r, and each consumer pausing at point c3for a second, will take? ./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,1,0,0,0:0,0,0,1,0,0,0:0,0,0,1,0,0, 0 -l 10 -v -t 5. Now change the size of the shared buffer to 3 ( -m 3 ). Will this make any difference in the total time? 6. Now change the location of the sleep to c6(this models a consumer taking something off the queue and then doing something with it), again us ing a single-entry buffer. What time do you predict in this case? ./main-two-cvs-while -p 1 -c 3 -m 1 -C 0,0,0,0,0,0,1:0,0,0,0,0,0,1:0,0,0,0,0,0, 1 -l 10 -v -t 7. Finally, change the buffer size to 3 again ( -m 3 ). What time do you predict now? 8. Now let’s look at main-one-cv-while.c . Can you conﬁgure a sleep string, assuming a single producer, one consumer, and a buffer of si ze 1, to cause a problem with this code? 9. Now change the number of consumers to two. Can you construct slee p strings for the producer and the consumers so as to cause a problem in the code? 10. Now examine main-two-cvs-if.c . Can you cause a problem to happen in this code? Again consider the case where there is only one co nsumer, and then the case where there is more than one. 11. Finally, examine main-two-cvs-while-extra-unlock.c . What prob- lem arises when you release the lock before doing a put or a get? Can you reliably cause such a problem to happen, given the sleep string s? What bad thing can happen? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4129
31. Semaphores,"31 Semaphores As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency problems. One of the ﬁrst people to realize this years ago was Edsger Dijkstra (though it is hard to know the exact history [GR92]), known among other things for his famous “shortest paths” algorithm in graph theory [D59], an e arly polemic on structured programming entitled “Goto Statements Cons id- ered Harmful” [D68a] (what a great title.), and, in the case we will study here, the introduction of a synchronization primitive called the semaphore [D68b,D72]. Indeed, Dijkstra and colleagues invented the se maphore as a single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables. THECRUX: HOWTOUSESEMAPHORES How can we use semaphores instead of locks and condition variables? What is the deﬁnition of a semaphore? What is a binary semaphore? I s it straightforward to build a semaphore out of locks and condition va ri- ables? To build locks and condition variables out of semaphores? 31.1 Semaphores: A Deﬁnition A semaphore is an object with an integer value that we can manipu late with two routines; in the POSIX standard, these routines are semwait() andsempost()1. Because the initial value of the semaphore deter- mines its behavior, before calling any other routine to interact with the semaphore, we must ﬁrst initialize it to some value, as the code i n Figure 31.1 does. 1Historically, semwait() was called P() by Dijkstra and sempost() called V(). P() comes from “prolaag”, a contraction of “probeer” (Dutch for “try”) and “verlaag” (“de- crease”); V() comes from the Dutch word “verhoog” which means “increase” (tha nks to Mart Oskamp for this information). Sometimes, people call them down and up. U se the Dutch versions to impress your friends, or confuse them, or both. 1 2 SEMAPHORES 1#include <semaphore.h> 2sem_t s; 3sem_init(&s, 0, 1); Figure 31.1: Initializing A Semaphore In the ﬁgure, we declare a semaphore s and initialize it to the v alue 1 by passing 1 in as the third argument. The second argument to seminit() will be set to 0 in all of the examples we’ll see; this indicates t hat the semaphore is shared between threads in the same process. See the man page for details on other usages of semaphores (namely, how they can be used to synchronize access across different processes), which require a different value for that second argument. After a semaphore is initialized, we can call one of two functions to interact with it, semwait() orsempost() . The behavior of these two functions is seen in Figure 31.2. For now, we are not concerned with the implementation of these rou- tines, which clearly requires some care; with multiple threa ds calling into semwait() andsempost() , there is the obvious need for managing these critical sections. We will now focus on how to usethese primitives; later we may discuss how they are built. We should discuss a few salient aspects of the interfaces here.",3052
31. Semaphores,"First, we can see that semwait() will either return right away (because the value of the semaphore was one or higher when we called semwait() ), or it will cause the caller to suspend execution waiting for a subseq uent post. Of course, multiple calling threads may call into semwait() , and thus all be queued waiting to be woken. Second, we can see that sempost() does not wait for some particular condition to hold like semwait() does. Rather, it simply increments the value of the semaphore and then, if there is a thread waiting to b e woken, wakes one of them up. Third, the value of the semaphore, when negative, is equal to th e num- ber of waiting threads [D68b]. Though the value generally isn’t seen by users of the semaphores, this invariant is worth knowing and perh aps can help you remember how a semaphore functions. Don’t worry (yet) about the seeming race conditions possible within the semaphore; assume that the actions they make are performed a tomi- cally. We will soon use locks and condition variables to do just thi s. 1int sem_wait(sem_t *s) { 2decrement the value of semaphore s by one 3wait if value of semaphore s is negative 4} 5 6int sem_post(sem_t *s) { 7increment the value of semaphore s by one 8if there are one or more threads waiting, wake one 9} Figure 31.2: Semaphore: Deﬁnitions Of Wait And Post OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 3 1sem_t m; 2sem_init(&m, 0, X); // initialize semaphore to X; what shoul d X be? 3 4sem_wait(&m); 5// critical section here 6sem_post(&m); Figure 31.3: A Binary Semaphore (That Is, A Lock) 31.2 Binary Semaphores (Locks) We are now ready to use a semaphore. Our ﬁrst use will be one with which we are already familiar: using a semaphore as a lock. See Fi gure 31.3 for a code snippet; therein, you’ll see that we simply surroun d the critical section of interest with a semwait() /sempost() pair. Criti- cal to making this work, though, is the initial value of the semap horem (initialized to Xin the ﬁgure). What should Xbe? ... (Try thinking about it before going on) ... Looking back at deﬁnition of the semwait() andsempost() rou- tines above, we can see that the initial value should be 1. To make this clear, let’s imagine a scenario with two threads. The ﬁrst thread (Thread 0) calls semwait() ; it will ﬁrst decrement the value of the semaphore, changing it to 0. Then, it will wait only if the va lue is notgreater than or equal to 0. Because the value is 0, semwait() will simply return and the calling thread will continue; Thread 0 i s now free to enter the critical section. If no other thread tries to acquire the lock while Thread 0 is inside the critical section, when it calls sempost() , it will simply restore the value of the semaphore to 1 (and not wake a waiti ng thread, because there are none). Figure 31.4 shows a trace of thi s scenario. A more interesting case arises when Thread 0 “holds the lock” (i. e., it has called semwait() but not yet called sempost() ), and another thread (Thread 1) tries to enter the critical section by calli ngsemwait() . In this case, Thread 1 will decrement the value of the semaphore to -1, and thus wait (putting itself to sleep and relinquishing the proc essor). When Thread 0 runs again, it will eventually call sempost() , incrementing the value of the semaphore back to zero, and then wake the waiting thr ead (Thread 1), which will then be able to acquire the lock for itsel f. When Thread 1 ﬁnishes, it will again increment the value of the sema phore, restoring it to 1 again. Value of Semaphore Thread 0 Thread 1 1 1 call semwait() 0 semwait() returns 0 (crit sect) 0 call sempost() 1 sempost() returns Figure 31.4: Thread Trace: Single Thread Using A Semaphore c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 SEMAPHORES Value Thread 0 State Thread 1 State 1 Running Ready 1 call semwait() Running Ready 0semwait() returns Running Ready 0(crit sect: begin) Running Ready 0 Interrupt; Switch →T1 Ready Running 0 Ready call semwait() Running -1 Ready decrement sem Running -1 Ready (sem<0)→sleep Sleeping -1 Running Switch→T0 Sleeping -1(crit sect: end) Running Sleeping -1 call sempost() Running Sleeping 0increment sem Running Sleeping 0wake(T1) Running Ready 0sempost() returns Running Ready 0 Interrupt; Switch →T1 Ready Running 0 Ready semwait() returns Running 0 Ready (crit sect) Running 0 Ready call sempost() Running 1 Ready sempost() returns Running Figure 31.5: Thread Trace: Two Threads Using A Semaphore Figure 31.5 shows a trace of this example. In addition to thread a ctions, the ﬁgure shows the scheduler state of each thread: Running, Ready (i.e., runnable but not running), and Sleeping. Note in particular tha t Thread 1 goes into the sleeping state when it tries to acquire the alrea dy-held lock; only when Thread 0 runs again can Thread 1 be awoken and potential ly run again. If you want to work through your own example, try a scenario where multiple threads queue up waiting for a lock.",4986
31. Semaphores,"What would the valu e of the semaphore be during such a trace? Thus we are able to use semaphores as locks. Because locks only hav e two states (held and not held), we sometimes call a semaphore use d as a lock a binary semaphore . Note that if you are using a semaphore only in this binary fashion, it could be implemented in a simpler man ner than the generalized semaphores we present here. 31.3 Semaphores For Ordering Semaphores are also useful to order events in a concurrent program . For example, a thread may wish to wait for a list to become non-empt y, so it can delete an element from it. In this pattern of usage, we of ten ﬁnd one thread waiting for something to happen, and another thread making that something happen and then signaling that it has happened, thus wak- ing the waiting thread. We are thus using the semaphore as an ordering primitive (similar to our use of condition variables earlier). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 5 1sem_t s; 2 3void* 4child(void *arg) { 5printf(\""child \""); 6sem_post(&s); // signal here: child is done 7return NULL; 8} 9 10int 11main(int argc, char *argv[]) { 12 sem_init(&s, 0, X); // what should X be? 13 printf(\""parent: begin \""); 14 pthread_t c; 15 Pthread_create(&c, NULL, child, NULL); 16 sem_wait(&s); // wait here for child 17 printf(\""parent: end \""); 18 return 0; 19} Figure 31.6: A Parent Waiting For Its Child A simple example is as follows. Imagine a thread creates another thread and then wants to wait for it to complete its execution (Fi gure 31.6). When this program runs, we would like to see the following: parent: begin child parent: end The question, then, is how to use a semaphore to achieve this effe ct; as it turns out, the answer is relatively easy to understand. As y ou can see in the code, the parent simply calls semwait() and the child sempost() to wait for the condition of the child ﬁnishing its execution to bec ome true. However, this raises the question: what should the initia l value of this semaphore be? (Again, think about it here, instead of reading ahead) The answer, of course, is that the value of the semaphore should be s et to is 0. There are two cases to consider. First, let us assume th at the parent creates the child but the child has not run yet (i.e., it is sitt ing in a ready queue but not running). In this case (Figure 31.7, page 6), the parent will callsemwait() before the child has called sempost() ; we’d like the parent to wait for the child to run. The only way this will happen is if the value of the semaphore is not greater than 0; hence, 0 is the initi al value. The parent runs, decrements the semaphore (to -1), then waits (sleeping). When the child ﬁnally runs, it will call sempost() , increment the value of the semaphore to 0, and wake the parent, which will then retur n from semwait() and ﬁnish the program. The second case (Figure 31.8, page 6) occurs when the child runs to completion before the parent gets a chance to call semwait() . In this case, the child will ﬁrst call sempost() , thus incrementing the value of the semaphore from 0 to 1. When the parent then gets a chance to ru n, it will call semwait() and ﬁnd the value of the semaphore to be 1; the parent will thus decrement the value (to 0) and return from semwait() without waiting, also achieving the desired effect. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 SEMAPHORES Value Parent State Child State 0create(Child) Running (Child exists; is runnable) Ready 0 call semwait() Running Ready -1decrement sem Running Ready -1(sem<0)→sleep Sleeping Ready -1 Switch→Child Sleeping child runs Running -1 Sleeping call sempost() Running 0 Sleeping increment sem Running 0 Ready wake(Parent) Running 0 Ready sempost() returns Running 0 Ready Interrupt; Switch →Parent Ready 0semwait() returns Running Ready Figure 31.7: Thread Trace: Parent Waiting For Child (Case 1) Value Parent State Child State 0create(Child) Running (Child exists; is runnable) Ready 0 Interrupt; Switch →Child Ready child runs Running 0 Ready call sempost() Running 1 Ready increment sem Running 1 Ready wake(nobody) Running 1 Ready sempost() returns Running 1parent runs Running Interrupt; Switch →Parent Ready 1 call semwait() Running Ready 0decrement sem Running Ready 0(sem≥0)→awake Running Ready 0semwait() returns Running Ready Figure 31.8: Thread Trace: Parent Waiting For Child (Case 2) 31.4 The Producer/Consumer (Bounded Buffer) Problem The next problem we will confront in this chapter is known as the pro- ducer/consumer problem, or sometimes as the bounded buffer problem [D72].",4602
31. Semaphores,"This problem is described in detail in the previous chap ter on con- dition variables; see there for details. First Attempt Our ﬁrst attempt at solving the problem introduces two semaphore s,empty andfull , which the threads will use to indicate when a buffer entry ha s been emptied or ﬁlled, respectively. The code for the put and get routines is in Figure 31.9, and our attempt at solving the producer and cons umer problem is in Figure 31.10. In this example, the producer ﬁrst waits for a buffer to become em pty in order to put data into it, and the consumer similarly waits for a buffer to become ﬁlled before using it. Let us ﬁrst imagine that MAX=1 (there is only one buffer in the array), and see if this works. Imagine again there are two threads, a producer and a consumer. Let us examine a speciﬁc scenario on a single CPU. Assume the consum er gets to run ﬁrst. Thus, the consumer will hit Line C1 in Figure 3 1.10, callingsemwait(&full) . Because full was initialized to the value 0, OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 7 1int buffer[MAX]; 2int fill = 0; 3int use = 0; 4 5void put(int value) { 6buffer[fill] = value; // Line F1 7fill = (fill + 1)  percent MAX; // Line F2 8} 9 10int get() { 11 int tmp = buffer[use]; // Line G1 12 use = (use + 1)  percent MAX; // Line G2 13 return tmp; 14} Figure 31.9: The Put And Get Routines 1sem_t empty; 2sem_t full; 3 4void*producer(void *arg) { 5int i; 6for (i = 0; i < loops; i++) { 7 sem_wait(&empty); // Line P1 8 put(i); // Line P2 9 sem_post(&full); // Line P3 10 } 11} 12 13void*consumer(void *arg) { 14 int i, tmp = 0; 15 while (tmp .= -1) { 16 sem_wait(&full); // Line C1 17 tmp = get(); // Line C2 18 sem_post(&empty); // Line C3 19 printf(\"" percentd \"", tmp); 20 } 21} 22 23int main(int argc, char *argv[]) { 24 // ... 25 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith... 26 sem_init(&full, 0, 0); // ... and 0 are full 27 // ... 28} Figure 31.10: Adding The Full And Empty Conditions the call will decrement full (to -1), block the consumer, and wait for another thread to call sempost() onfull , as desired. Assume the producer then runs. It will hit Line P1, thus callin g the semwait(&empty) routine. Unlike the consumer, the producer will continue through this Line, because empty was initialized to t he value MAX (in this case, 1). Thus, empty will be decremented to 0 and the producer will put a data value into the ﬁrst entry of buffer (Lin e P2). The producer will then continue on to P3 and call sempost(&full) , chang- ing the value of the full semaphore from -1 to 0 and waking the consu mer (e.g., move it from blocked to ready). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 SEMAPHORES In this case, one of two things could happen. If the producer contin - ues to run, it will loop around and hit Line P1 again. This time, how - ever, it would block, as the empty semaphore’s value is 0. If the pr oducer instead was interrupted and the consumer began to run, it would call semwait(&full) (Line C1) and ﬁnd that the buffer was indeed full and thus consume it.",3097
31. Semaphores,"In either case, we achieve the desired beh avior. You can try this same example with more threads (e.g., multiple pro- ducers, and multiple consumers). It should still work. Let us now imagine that MAX is greater than 1 (say MAX = 10). For this example, let us assume that there are multiple producers and m ultiple consumers. We now have a problem: a race condition. Do you see where it occurs? (take some time and look for it) If you can’t see it, here’s a h int: look more closely at the put() and get() code. OK, let’s understand the issue. Imagine two producers (Pa and P b) both calling into put() at roughly the same time. Assume produce r Pa gets to run ﬁrst, and just starts to ﬁll the ﬁrst buffer entry (ﬁll = 0 at Line F1). Before Pa gets a chance to increment the ﬁll counter to 1, it is in terrupted. Producer Pb starts to run, and at Line F1 it also puts its data in to the 0th element of buffer, which means that the old data there is over written. This is a no-no; we don’t want any data from the producer to be lost. A Solution: Adding Mutual Exclusion As you can see, what we’ve forgotten here is mutual exclusion . The ﬁlling of a buffer and incrementing of the index into the buffer is a critical section, and thus must be guarded carefully. So let’s use our frie nd the binary semaphore and add some locks. Figure 31.11 shows our attemp t. Now we’ve added some locks around the entire put()/get() parts of the code, as indicated by the NEW LINE comments. That seems like the right idea, but it also doesn’t work. Why? Deadlock. Why does deadl ock occur? Take a moment to consider it; try to ﬁnd a case where deadloc k arises. What sequence of steps must happen for the program to dea dlock? Avoiding Deadlock OK, now that you ﬁgured it out, here is the answer. Imagine two thr eads, one producer and one consumer. The consumer gets to run ﬁrst. It acquires the mutex (Line C0), and then calls semwait() on the full semaphore (Line C1); because there is no data yet, this call ca uses the consumer to block and thus yield the CPU; importantly, though, th e con- sumer still holds the lock. A producer then runs. It has data to produce and if it were able to run, it would be able to wake the consumer thread and all would be good. Un - fortunately, the ﬁrst thing it does is call semwait() on the binary mutex semaphore (Line P0). The lock is already held. Hence, the produc er is now stuck waiting too. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 9 1sem_t empty; 2sem_t full; 3sem_t mutex; 4 5void*producer(void *arg) { 6int i; 7for (i = 0; i < loops; i++) { 8 sem_wait(&mutex); // Line P0 (NEW LINE) 9 sem_wait(&empty); // Line P1 10 put(i); // Line P2 11 sem_post(&full); // Line P3 12 sem_post(&mutex); // Line P4 (NEW LINE) 13 } 14} 15 16void*consumer(void *arg) { 17 int i; 18 for (i = 0; i < loops; i++) { 19 sem_wait(&mutex); // Line C0 (NEW LINE) 20 sem_wait(&full); // Line C1 21 int tmp = get(); // Line C2 22 sem_post(&empty); // Line C3 23 sem_post(&mutex); // Line C4 (NEW LINE) 24 printf(\"" percentd \"", tmp); 25 } 26} 27 28int main(int argc, char *argv[]) { 29 // ... 30 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith... 31 sem_init(&full, 0, 0); // ... and 0 are full 32 sem_init(&mutex, 0, 1); // mutex=1 because it is a lock (NEW L INE) 33 // ...",3320
31. Semaphores,"34} Figure 31.11: Adding Mutual Exclusion (Incorrectly) There is a simple cycle here. The consumer holds the mutex and is waiting for the someone to signal full. The producer could signal full but iswaiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock. At Last, A Working Solution To solve this problem, we simply must reduce the scope of the lock. F ig- ure 31.12 shows the correct solution. As you can see, we simply move t he mutex acquire and release to be just around the critical secti on; the full and empty wait and signal code is left outside. The result is a si mple and working bounded buffer, a commonly-used pattern in multi-threa ded programs. Understand it now; use it later. You will thank us for ye ars to come. Or at least, you will thank us when the same question is as ked on the ﬁnal exam. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 SEMAPHORES 1sem_t empty; 2sem_t full; 3sem_t mutex; 4 5void*producer(void *arg) { 6int i; 7for (i = 0; i < loops; i++) { 8 sem_wait(&empty); // Line P1 9 sem_wait(&mutex); // Line P1.5 (MOVED MUTEX HERE...) 10 put(i); // Line P2 11 sem_post(&mutex); // Line P2.5 (... AND HERE) 12 sem_post(&full); // Line P3 13 } 14} 15 16void*consumer(void *arg) { 17 int i; 18 for (i = 0; i < loops; i++) { 19 sem_wait(&full); // Line C1 20 sem_wait(&mutex); // Line C1.5 (MOVED MUTEX HERE...) 21 int tmp = get(); // Line C2 22 sem_post(&mutex); // Line C2.5 (... AND HERE) 23 sem_post(&empty); // Line C3 24 printf(\"" percentd \"", tmp); 25 } 26} 27 28int main(int argc, char *argv[]) { 29 // ... 30 sem_init(&empty, 0, MAX); // MAX buffers are empty to begin w ith... 31 sem_init(&full, 0, 0); // ... and 0 are full 32 sem_init(&mutex, 0, 1); // mutex=1 because it is a lock 33 // ... 34} Figure 31.12: Adding Mutual Exclusion (Correctly) 31.5 Reader-Writer Locks Another classic problem stems from the desire for a more ﬂexible loc k- ing primitive that admits that different data structure acc esses might re- quire different kinds of locking. For example, imagine a number of con- current list operations, including inserts and simple lookups. While in- serts change the state of the list (and thus a traditional criti cal section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to p ro- ceed concurrently. The special type of lock we will now develop to s up- port this type of operation is known as a reader-writer lock [CHP71]. The code for such a lock is available in Figure 31.13. The code is pretty simple. If some thread wants to update the dat a structure in question, it should call the new pair of synchroniza tion op- erations:rwlockacquire writelock() , to acquire a write lock, and rwlockrelease writelock() , to release it. Internally, these simply use thewritelock semaphore to ensure that only a single writer can ac- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 11 1typedef struct _rwlock_t { 2sem_t lock; // binary semaphore (basic lock) 3sem_t writelock; // used to allow ONE writer or MANY readers 4int readers; // count of readers reading in critical section 5} rwlock_t; 6 7void rwlock_init(rwlock_t *rw) { 8rw->readers = 0; 9sem_init(&rw->lock, 0, 1); 10sem_init(&rw->writelock, 0, 1); 11} 12 13void rwlock_acquire_readlock(rwlock_t *rw) { 14sem_wait(&rw->lock); 15rw->readers++; 16if (rw->readers == 1) 17 sem_wait(&rw->writelock); // first reader acquires write lock 18sem_post(&rw->lock); 19} 20 21void rwlock_release_readlock(rwlock_t *rw) { 22sem_wait(&rw->lock); 23rw->readers--; 24if (rw->readers == 0) 25 sem_post(&rw->writelock); // last reader releases writel ock 26sem_post(&rw->lock); 27} 28 29void rwlock_acquire_writelock(rwlock_t *rw) { 30sem_wait(&rw->writelock); 31} 32 33void rwlock_release_writelock(rwlock_t *rw) { 34sem_post(&rw->writelock); 35} Figure 31.13: A Simple Reader-Writer Lock quire the lock and thus enter the critical section to update the data struc- ture in question.",4050
31. Semaphores,"More interesting is the pair of routines to acquire and release r ead locks. When acquiring a read lock, the reader ﬁrst acquires lock and then increments the readers variable to track how many readers are currently inside the data structure. The important step then taken within rwlockacquire readlock() occurs when the ﬁrst reader acquires the lock; in that case, the reader also acquires the write lock b y calling semwait() on thewritelock semaphore, and then releasing the lock by calling sempost() . Thus, once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wish es to acquire the write lock will have to wait until allreaders are ﬁnished; the last one to exit the critical section calls sempost() on “writelock” and thus enables a waiting writer to acquire the lock. This approach works (as desired), but does have some negatives, e spe- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 SEMAPHORES TIP: SIMPLE ANDDUMB CANBEBETTER (HILL’SLAW) You should never underestimate the notion that the simple and dum b approach can be the best one. With locking, sometimes a simple spi n lock works best, because it is easy to implement and fast. Although s omething like reader/writer locks sounds cool, they are complex, and comple x can mean slow. Thus, always try the simple and dumb approach ﬁrst. This idea, of appealing to simplicity, is found in many places. One early source is Mark Hill’s dissertation [H87], which studied how to de sign caches for CPUs. Hill found that simple direct-mapped caches w orked better than fancy set-associative designs (one reason is that i n caching, simpler designs enable faster lookups). As Hill succinctly su mmarized his work: “Big and dumb is better.” And thus we call this simila r advice Hill’s Law . cially when it comes to fairness. In particular, it would be rel atively easy for readers to starve writers. More sophisticated solutions to th is prob- lem exist; perhaps you can think of a better implementation? Hin t: think about what you would need to do to prevent more readers from enterin g the lock once a writer is waiting. Finally, it should be noted that reader-writer locks should be us ed with some caution. They often add more overhead (especially with m ore sophisticated implementations), and thus do not end up speedin g up performance as compared to just using simple and fast locking pr imi- tives [CB08]. Either way, they showcase once again how we can use semaphores in an interesting and useful way. 31.6 The Dining Philosophers One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem [D71]. The prob- lem is famous because it is fun and somewhat intellectually int eresting; however, its practical utility is low. However, its fame forces i ts inclu- sion here; indeed, you might be asked about it on some interview, an d you’d really hate your OS professor if you miss that question and don’t get the job. Conversely, if you get the job, please feel free to sen d your OS professor a nice note, or some stock options.",3137
31. Semaphores,"The basic setup for the problem is this (as shown in Figure 31.14) : as- sume there are ﬁve “philosophers” sitting around a table. Betwe en each pair of philosophers is a single fork (and thus, ﬁve total). The phi loso- phers each have times where they think, and don’t need any forks, and times where they eat. In order to eat, a philosopher needs two fork s, both the one on their left and the one on their right. The contention for the se forks, and the synchronization problems that ensue, are what mak es this a problem we study in concurrent programming. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 13 P0P1 P2 P3 P4f0f1f2 f3 f4 Figure 31.14: The Dining Philosophers Here is the basic loop of each philosopher: while (1) { think(); getforks(); eat(); putforks(); } The key challenge, then, is to write the routines getforks() and putforks() such that there is no deadlock, no philosopher starves and never gets to eat, and concurrency is high (i.e., as many philos ophers can eat at the same time as possible). Following Downey’s solutions [D08], we’ll use a few helper functions to get us towards a solution. They are: int left(int p) { return p; } int right(int p) { return (p + 1)  percent 5; } When philosopher pwishes to refer to the fork on their left, they sim- ply callleft(p) . Similarly, the fork on the right of a philosopher pis referred to by calling right(p) ; the modulo operator therein handles the one case where the last philosopher ( p=4) tries to grab the fork on their right, which is fork 0. We’ll also need some semaphores to solve this problem. Let us assum e we have ﬁve, one for each fork: semt forks[5] . c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 SEMAPHORES 1void getforks() { 2sem_wait(forks[left(p)]); 3sem_wait(forks[right(p)]); 4} 5 6void putforks() { 7sem_post(forks[left(p)]); 8sem_post(forks[right(p)]); 9} Figure 31.15: Thegetforks() Andputforks() Routines Broken Solution We attempt our ﬁrst solution to the problem. Assume we initialize each semaphore (in the forks array) to a value of 1. Assume also that each philosopher knows its own number ( p). We can thus write the getforks() andputforks() routine as shown in Figure 31.15. The intuition behind this (broken) solution is as follows. To acqui re the forks, we simply grab a “lock” on each one: ﬁrst the one on the left , and then the one on the right. When we are done eating, we release t hem. Simple, no? Unfortunately, in this case, simple means broken. Ca n you see the problem that arises? Think about it. The problem is deadlock . If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right , each will be stuck holding one fork and waiting for another, forever. Spec iﬁ- cally, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philos opher 2 grabs fork 2, philosopher 3 grabs fork 3, and philosopher 4 grabs for k 4; all the forks are acquired, and all the philosophers are stuck wa iting for a fork that another philosopher possesses.",3037
31. Semaphores,"We’ll study deadlock in m ore detail soon; for now, it is safe to say that this is not a working soluti on. A Solution: Breaking The Dependency The simplest way to attack this problem is to change how forks are ac- quired by at least one of the philosophers; indeed, this is how Dijk stra himself solved the problem. Speciﬁcally, let’s assume that phil osopher 4 (the highest numbered one) acquires the forks in a different order. The code to do so is as follows: 1void getforks() { 2if (p == 4) { 3sem_wait(forks[right(p)]); 4sem_wait(forks[left(p)]); 5} else { 6sem_wait(forks[left(p)]); 7sem_wait(forks[right(p)]); 8} 9} Because the last philosopher tries to grab right before left, th ere is no situation where each philosopher grabs one fork and is stuck waiti ng for another; the cycle of waiting is broken. Think through the ramiﬁc ations of this solution, and convince yourself that it works. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 15 1typedef struct __Zem_t { 2int value; 3pthread_cond_t cond; 4pthread_mutex_t lock; 5} Zem_t; 6 7// only one thread can call this 8void Zem_init(Zem_t *s, int value) { 9s->value = value; 10 Cond_init(&s->cond); 11 Mutex_init(&s->lock); 12} 13 14void Zem_wait(Zem_t *s) { 15 Mutex_lock(&s->lock); 16 while (s->value <= 0) 17 Cond_wait(&s->cond, &s->lock); 18 s->value--; 19 Mutex_unlock(&s->lock); 20} 21 22void Zem_post(Zem_t *s) { 23 Mutex_lock(&s->lock); 24 s->value++; 25 Cond_signal(&s->cond); 26 Mutex_unlock(&s->lock); 27} Figure 31.16: Implementing Zemaphores With Locks And CVs There are other “famous” problems like this one, e.g., the cigarette smoker’s problem or the sleeping barber problem . Most of them are just excuses to think about concurrency; some of them have fascin ating names. Look them up if you are interested in learning more, or just g et- ting more practice thinking in a concurrent manner [D08]. 31.7 How To Implement Semaphores Finally, let’s use our low-level synchronization primitives, loc ks and condition variables, to build our own version of semaphores called . .. (drum roll here) ...Zemaphores . This task is fairly straightforward, as you can see in Figure 31.16. As you can see from the ﬁgure, we use just one lock and one condition variable, plus a state variable to track the value of the semap hore. Study the code for yourself until you really understand it. Do it. One subtle difference between our Zemaphore and pure semaphore s as deﬁned by Dijkstra is that we don’t maintain the invariant th at the value of the semaphore, when negative, reﬂects the number of wai ting threads; indeed, the value will never be lower than zero. This b ehavior is easier to implement and matches the current Linux implement ation. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 SEMAPHORES TIP: BECAREFUL WITHGENERALIZATION The abstract technique of generalization can thus be quite use ful in sys- tems design, where one good idea can be made slightly broader and t hus solve a larger class of problems. However, be careful when genera lizing; as Lampson warns us “Don’t generalize; generalizations are gene rally wrong” [L83].",3134
31. Semaphores,"One could view semaphores as a generalization of locks and condition variables; however, is such a generalization needed? And, giv en the dif- ﬁculty of realizing a condition variable on top of a semaphore, perha ps this generalization is not as general as you might think. Curiously, building condition variables out of semaphores is a muc h trickier proposition. Some highly experienced concurrent program mers tried to do this in the Windows environment, and many different bugs ensued [B04]. Try it yourself, and see if you can ﬁgure out why bui lding condition variables out of semaphores is more challenging than it m ight appear. 31.8 Summary Semaphores are a powerful and ﬂexible primitive for writing concu r- rent programs. Some programmers use them exclusively, shunning locks and condition variables, due to their simplicity and utility. In this chapter, we have presented just a few classic problems and solu- tions. If you are interested in ﬁnding out more, there are many othe r ma- terials you can reference. One great (and free reference) is A llen Downey’s book on concurrency and programming with semaphores [D08]. This book has lots of puzzles you can work on to improve your understand- ing of both semaphores in speciﬁc and concurrency in general. Bec oming a real concurrency expert takes years of effort; going beyond what you learn in this class is undoubtedly the key to mastering such a t opic. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SEMAPHORES 17 References [B04] “Implementing Condition Variables with Semaphores” by Andr ew Birrell. December 2004. An interesting read on how difﬁcult implementing CVs on top of semaphores r eally is, and the mistakes the author and co-workers made along the way. Particularly relevant because the group had done a ton of concurrent programming; Birrell, for example, is known for (amon g other things) writing various thread-programming guides. [CB08] “Real-world Concurrency” by Bryan Cantrill, Jeff Bonwick. ACM Qu eue. Volume 6, No. 5. September 2008. A nice article by some kernel hackers from a company formerly known as Sun on the real problems faced in concurrent code. [CHP71] “Concurrent Control with Readers and Writers” by P .J. Courto is, F. Heymans, D.L. Parnas. Communications of the ACM, 14:10, October 1971. The introduction of the reader-writer problem, and a simple solution. Later work introduced more complex solutions , skipped here because, well, they are pretty complex. [D59] “A Note on Two Problems in Connexion with Graphs” by E. W. Dijk stra. Numerische Mathematik 1, 269271, 1959. Available: http://www-m3.ma.tum.de/twiki/pub/MN0506/ WebHome/dijkstra.pdf .Can you believe people worked on algorithms in 1959? We can’t. Even before computers were any fun to use, these people had a sense that they woul d transform the world... [D68a] “Go-to Statement Considered Harmful” by E.W. Dijkstra. CA CM, volume 11(3), March 1968.http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.P DF.Sometimes thought as the beginning of the ﬁeld of software engineering. [D68b] “The Structure of the THE Multiprogramming System” by E.W.",3115
31. Semaphores,"Dij kstra. CACM, vol- ume 11(5), 1968. One of the earliest papers to point out that systems work in computer science is an engaging intellectual endeavor. Also argues strongly for modularity in the form of layered systems. [D72] “Information Streams Sharing a Finite Buffer” by E.W. Dijkstr a. Information Processing Letters 1, 1972. http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.P DF.Did Dijkstra invent everything? No, but maybe close. He certainly was the ﬁr st to clearly write down what the problems were in concurrent code. However, it is true that practitione rs in operating system design knew of many of the problems described by Dijkstra, so perhaps giving him too much credit would be a misrepresentation of history. [D08] “The Little Book of Semaphores” by A.B. Downey. Available at the following site: http://greenteapress.com/semaphores/ .A nice (and free.) book about semaphores. Lots of fun problems to solve, if you like that sort of thing. [D71] “Hierarchical ordering of sequential processes” by E.W. Dijk stra. Available online here: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.P DF.Presents numerous con- currency problems, including the Dining Philosophers. The wikip edia page about this problem is also quite informative. [GR92] “Transaction Processing: Concepts and Techniques” by Jim Gray, Andr eas Reuter. Morgan Kaufmann, September 1992. The exact quote that we ﬁnd particularly humorous is found on page 485, at the top of Section 8.8: “The ﬁrst multiprocessors, circa 196 0, had test and set instruc- tions ... presumably the OS implementors worked out the appropriate algorithms , although Dijkstra is generally credited with inventing semaphores many years later.” Oh, sn ap. [H87] “Aspects of Cache Memory and Instruction Buffer Performance” by Mark D . Hill. Ph.D. Dissertation, U.C. Berkeley, 1987. Hill’s dissertation work, for those obsessed with caching in early systems. A great example of a quantitative dissertation. [L83] “Hints for Computer Systems Design” by Butler Lampson. ACM Op erating Systems Review, 15:5, October 1983. Lampson, a famous systems researcher, loved using hints in the design of computer systems. A hint is something that is often correct but can be wron g; in this use, a signal() is telling a waiting thread that it changed the condition that the waiter was waiting on, but not to trust that the condition will be in the desired state when the waiting thread wakes up. In this paper about hints for designing systems, one of Lampson’s general hints is that you shou ld use hints. It is not as confusing as it sounds. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 SEMAPHORES Homework (Code) In this homework, we’ll use semaphores to solve some well-known concurrency problems. Many of these are taken from Downey’s excell ent “Little Book of Semaphores”2, which does a good job of pulling together a number of classic problems as well as introducing a few new vari ants; interested readers should check out the Little Book for more fun. Each of the following questions provides a code skeleton; your job is to ﬁll in the code to make it work given semaphores. On Linux, you will be using native semaphores; on a Mac (where there is no sema phore support), you’ll have to ﬁrst build an implementation (using lock s and condition variables, as described in the chapter).",3357
31. Semaphores,"Good luck. Questions 1. The ﬁrst problem is just to implement and test a solution to the fork/join problem , as described in the text. Even though this solution is describe d in the text, the act of typing it in on your own is worthwhile; even B ach would rewrite Vivaldi, allowing one soon-to-be master to learn fro m an existing one. Seefork-join.c for details. Add the call sleep(1) to the child to ensure it is working. 2. Let’s now generalize this a bit by investigating the rendezvous problem . The problem is as follows: you have two threads, each of which are about to enter the rendezvous point in the code. Neither should exit th is part of the code before the other enters it. Consider using two semapho res for this task, and see rendezvous.c for details. 3. Now go one step further by implementing a general solution to barrier syn- chronization . Assume there are two points in a sequential piece of code, calledP1andP2. Putting a barrier between P1andP2guarantees that all threads will execute P1before any one thread executes P2. Your task: write the code to implement a barrier() function that can be used in this man- ner. It is safe to assume you know N(the total number of threads in the running program) and that all Nthreads will try to enter the barrier. Again, you should likely use two semaphores to achieve the solution, and some other integers to count things. See barrier.c for details. 4. Now let’s solve the reader-writer problem , also as described in the text. In this ﬁrst take, don’t worry about starvation. See the code in reader-writer.c for details. Add sleep() calls to your code to demonstrate it works as you expect. Can you show the existence of the starvation problem? 5. Let’s look at the reader-writer problem again, but this time , worry about starvation. How can you ensure that all readers and writers ev entually make progress? See reader-writer-nostarve.c for details. 6. Use semaphores to build a no-starve mutex , in which any thread that tries to acquire the mutex will eventually obtain it. See the code in mutex-nostarve.c for more information. 7. Liked these problems? See Downey’s free text for more just like them. And don’t forget, have fun. But, you always do when you write code, n o? 2Available: http://greenteapress.com/semaphores/downey08semapho res.pdf . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2367
32. Concurrency Bugs,"32 Common Concurrency Problems Researchers have spent a great deal of time and effort looking int o con- currency bugs over many years. Much of the early work focused on deadlock , a topic which we’ve touched on in the past chapters but will now dive into deeply [C+71]. More recent work focuses on studying other types of common concurrency bugs (i.e., non-deadlock bugs). I n this chapter, we take a brief look at some example concurrency prob lems found in real code bases, to better understand what problems to l ook out for. And thus our central issue for this chapter: CRUX: HOWTOHANDLE COMMON CONCURRENCY BUGS Concurrency bugs tend to come in a variety of common patterns. Knowing which ones to look out for is the ﬁrst step to writing more ro- bust, correct concurrent code. 32.1 What Types Of Bugs Exist? The ﬁrst, and most obvious, question is this: what types of concur- rency bugs manifest in complex, concurrent programs? This ques tion is difﬁcult to answer in general, but fortunately, some others hav e done the work for us. Speciﬁcally, we rely upon a study by Lu et al. [L+08], w hich analyzes a number of popular concurrent applications in great de tail to understand what types of bugs arise in practice. The study focuses on four major and important open-source applica- tions: MySQL (a popular database management system), Apache (a well- known web server), Mozilla (the famous web browser), and OpenOfﬁ ce (a free version of the MS Ofﬁce suite, which some people actually u se). In the study, the authors examine concurrency bugs that have be en found and ﬁxed in each of these code bases, turning the developers’ work into a quantitative bug analysis; understanding these results ca n help you un- derstand what types of problems actually occur in mature code bas es. 1 2 COMMON CONCURRENCY PROBLEMS Application What it does Non-Deadlock Deadlock MySQL Database Server 14 9 Apache Web Server 13 4 Mozilla Web Browser 41 16 OpenOfﬁce Ofﬁce Suite 6 2 Total 74 31 Figure 32.1: Bugs In Modern Applications Figure 32.1 shows a summary of the bugs Lu and colleagues studied . From the ﬁgure, you can see that there were 105 total bugs, most of wh ich were not deadlock (74); the remaining 31 were deadlock bugs. Fur ther, you can see the number of bugs studied from each application; whil e OpenOfﬁce only had 8 total concurrency bugs, Mozilla had nearly 6 0. We now dive into these different classes of bugs (non-deadlock, d ead- lock) a bit more deeply. For the ﬁrst class of non-deadlock bugs, we u se examples from the study to drive our discussion. For the second cla ss of deadlock bugs, we discuss the long line of work that has been done in either preventing, avoiding, or handling deadlock. 32.2 Non-Deadlock Bugs Non-deadlock bugs make up a majority of concurrency bugs, accord- ing to Lu’s study. But what types of bugs are these? How do they ari se? How can we ﬁx them? We now discuss the two major types of non- deadlock bugs found by Lu et al.: atomicity violation bugs and order violation bugs. Atomicity-Violation Bugs The ﬁrst type of problem encountered is referred to as an atomicity vi- olation .",3134
32. Concurrency Bugs,"Here is a simple example, found in MySQL. Before reading the explanation, try ﬁguring out what the bug is. Do it. 1Thread 1:: 2if (thd->proc_info) { 3... 4fputs(thd->proc_info, ...); 5... 6} 7 8Thread 2:: 9thd->proc_info = NULL; In the example, two different threads access the ﬁeld procinfo in the structure thd. The ﬁrst thread checks if the value is non-NULL and then prints its value; the second thread sets it to NULL. Clear ly, if the ﬁrst thread performs the check but then is interrupted before t he call to fputs , the second thread could run in-between, thus setting the point er to NULL; when the ﬁrst thread resumes, it will crash, as a NULL pointer will be dereferenced by fputs . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 3 The more formal deﬁnition of an atomicity violation, according to Lu et al, is this: “The desired serializability among multiple m emory accesses is violated (i.e. a code region is intended to be atomic, but the at omicity is not enforced during execution).” In our example above, the code h as anatomicity assumption (in Lu’s words) about the check for non-NULL of procinfo and the usage of procinfo in thefputs() call; when the assumption is incorrect, the code will not work as desired. Finding a ﬁx for this type of problem is often (but not always) strai ght- forward. Can you think of how to ﬁx the code above? In this solution, we simply add locks around the shared-variable ref- erences, ensuring that when either thread accesses the procinfo ﬁeld, it has a lock held ( procinfolock ). Of course, any other code that ac- cesses the structure should also acquire this lock before doing s o. 1pthread_mutex_t proc_info_lock = PTHREAD_MUTEX_INITIAL IZER; 2 3Thread 1:: 4pthread_mutex_lock(&proc_info_lock); 5if (thd->proc_info) { 6... 7fputs(thd->proc_info, ...); 8... 9} 10pthread_mutex_unlock(&proc_info_lock); 11 12Thread 2:: 13pthread_mutex_lock(&proc_info_lock); 14thd->proc_info = NULL; 15pthread_mutex_unlock(&proc_info_lock); Order-Violation Bugs Another common type of non-deadlock bug found by Lu et al. is known as an order violation . Here is another simple example; once again, see if you can ﬁgure out why the code below has a bug in it. 1Thread 1:: 2void init() { 3... 4mThread = PR_CreateThread(mMain, ...); 5... 6} 7 8Thread 2:: 9void mMain(...) { 10 ... 11 mState = mThread->State; 12 ... 13} As you probably ﬁgured out, the code in Thread 2 seems to assume that the variable mThread has already been initialized (and is not NULL); however, if Thread 2 runs immediately once created, the value of mThread will not be set when it is accessed within mMain() in Thread 2, and will c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 COMMON CONCURRENCY PROBLEMS likely crash with a NULL-pointer dereference. Note that we ass ume the value ofmThread is initially NULL; if not, even stranger things could happen as arbitrary memory locations are accessed through the de refer- ence in Thread 2. The more formal deﬁnition of an order violation is this: “The desired order between two (groups of) memory accesses is ﬂipped (i.e., Ashould always be executed before B, but the order is not enforced during execu- tion)” [L+08]. The ﬁx to this type of bug is generally to enforce ordering.",3281
32. Concurrency Bugs,"As we discussed in detail previously, using condition variables is an easy and robust way to add this style of synchronization into modern code bas es. In the example above, we could thus rewrite the code as follows: 1pthread_mutex_t mtLock = PTHREAD_MUTEX_INITIALIZER; 2pthread_cond_t mtCond = PTHREAD_COND_INITIALIZER; 3int mtInit = 0; 4 5Thread 1:: 6void init() { 7... 8mThread = PR_CreateThread(mMain, ...); 9 10// signal that the thread has been created... 11pthread_mutex_lock(&mtLock); 12mtInit = 1; 13pthread_cond_signal(&mtCond); 14pthread_mutex_unlock(&mtLock); 15... 16} 17 18Thread 2:: 19void mMain(...) { 20 ... 21 // wait for the thread to be initialized... 22 pthread_mutex_lock(&mtLock); 23 while (mtInit == 0) 24 pthread_cond_wait(&mtCond, &mtLock); 25 pthread_mutex_unlock(&mtLock); 26 27 mState = mThread->State; 28 ... 29} In this ﬁxed-up code sequence, we have added a lock ( mtLock ) and corresponding condition variable ( mtCond ), as well as a state variable (mtInit ). When the initialization code runs, it sets the state of mtInit to 1 and signals that it has done so. If Thread 2 had run before this point, it will be waiting for this signal and corresponding state chang e; if it runs later, it will check the state and see that the initialization has already oc- curred (i.e., mtInit is set to 1), and thus continue as is proper. Note that we could likely use mThread as the state variable itself, but do not do so for the sake of simplicity here. When ordering matters between t hreads, condition variables (or semaphores) can come to the rescue. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 5 Non-Deadlock Bugs: Summary A large fraction (97 percent) of non-deadlock bugs studied by Lu et al. ar e either atomicity or order violations. Thus, by carefully thinking about t hese types of bug patterns, programmers can likely do a better job of av oiding them. Moreover, as more automated code-checking tools develop, they should likely focus on these two types of bugs as they constitute su ch a large fraction of non-deadlock bugs found in deployment. Unfortunately, not all bugs are as easily ﬁxed as the examples w e looked at above. Some require a deeper understanding of what the pro- gram is doing, or a larger amount of code or data structure reorganiza tion to ﬁx. Read Lu et al.’s excellent (and readable) paper for more de tails. 32.3 Deadlock Bugs Beyond the concurrency bugs mentioned above, a classic problem th at arises in many concurrent systems with complex locking protocols i s known asdeadlock . Deadlock occurs, for example, when a thread (say Thread 1) is holding a lock ( L1) and waiting for another one ( L2); unfortunately, the thread (Thread 2) that holds lock L2is waiting for L1to be released. Here is a code snippet that demonstrates such a potential deadloc k: Thread 1: Thread 2: pthread_mutex_lock(L1); pthread_mutex_lock(L2); pthread_mutex_lock(L2); pthread_mutex_lock(L1); Note that if this code runs, deadlock does not necessarily occur; ra ther, it may occur, if, for example, Thread 1 grabs lock L1and then a context switch occurs to Thread 2.",3140
32. Concurrency Bugs,"At that point, Thread 2 grabs L2, and tries to acquireL1. Thus we have a deadlock, as each thread is waiting for the other and neither can run. See Figure 32.2 for a graphical depict ion; the presence of a cycle in the graph is indicative of the deadlock. The ﬁgure should make the problem clear. How should programmers write code so as to handle deadlock in some way? CRUX: HOWTODEAL WITHDEADLOCK How should we build systems to prevent, avoid, or at least detect a nd recover from deadlock? Is this a real problem in systems today? Why Do Deadlocks Occur? As you may be thinking, simple deadlocks such as the one above seem readily avoidable. For example, if Thread 1 and 2 both made sure t o grab locks in the same order, the deadlock would never arise. So why do de ad- locks happen? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 COMMON CONCURRENCY PROBLEMS Thread 1 Thread 2Lock L1 Lock L2 HoldsHoldsWanted byWanted by Figure 32.2: The Deadlock Dependency Graph One reason is that in large code bases, complex dependencies ari se between components. Take the operating system, for example. The vir- tual memory system might need to access the ﬁle system in order t o page in a block from disk; the ﬁle system might subsequently require a page of memory to read the block into and thus contact the virtual memory system. Thus, the design of locking strategies in large system s must be carefully done to avoid deadlock in the case of circular dependen cies that may occur naturally in the code. Another reason is due to the nature of encapsulation . As software de- velopers, we are taught to hide details of implementations and t hus make software easier to build in a modular way. Unfortunately, such m odular- ity does not mesh well with locking. As Jula et al. point out [J+08], some seemingly innocuous interfaces almost invite you to deadlock. For exam- ple, take the Java Vector class and the method AddAll() . This routine would be called as follows: Vector v1, v2; v1.AddAll(v2); Internally, because the method needs to be multi-thread safe , locks for both the vector being added to (v1) and the parameter (v2) need t o be acquired. The routine acquires said locks in some arbitrary orde r (say v1 then v2) in order to add the contents of v2 to v1. If some other thread callsv2.AddAll(v1) at nearly the same time, we have the potential for deadlock, all in a way that is quite hidden from the calling appl ication. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 7 Conditions for Deadlock Four conditions need to hold for a deadlock to occur [C+71]: •Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock). •Hold-and-wait: Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for additional re- sources (e.g., locks that they wish to acquire). •No preemption: Resources (e.g., locks) cannot be forcibly removed from threads that are holding them. •Circular wait: There exists a circular chain of threads such that each thread holds one or more resources (e.g., locks) that are being re- quested by the next thread in the chain.",3186
32. Concurrency Bugs,"If any of these four conditions are not met, deadlock cannot occur. Thus, we ﬁrst explore techniques to prevent deadlock; each of these strate- gies seeks to prevent one of the above conditions from arising and th us is one approach to handling the deadlock problem. Prevention Circular Wait Probably the most practical prevention technique (and certain ly one that is frequently employed) is to write your locking code such that you never induce a circular wait. The most straightforward way to do that is to pro- vide a total ordering on lock acquisition. For example, if there are only two locks in the system ( L1andL2), you can prevent deadlock by always acquiringL1beforeL2. Such strict ordering ensures that no cyclical wait arises; hence, no deadlock. Of course, in more complex systems, more than two locks will ex- ist, and thus total lock ordering may be difﬁcult to achieve (and per- haps is unnecessary anyhow). Thus, a partial ordering can be a useful way to structure lock acquisition so as to avoid deadlock. An exce llent real example of partial lock ordering can be seen in the memory map - ping code in Linux [T+94]; the comment at the top of the source code reveals ten different groups of lock acquisition orders, includi ng simple ones such as “ imutex beforeimmapmutex ” and more complex orders such as “immapmutex beforeprivate lock beforeswaplock before mapping->tree lock ”. As you can imagine, both total and partial ordering require caref ul design of locking strategies and must be constructed with great care. Fur- ther, ordering is just a convention, and a sloppy programmer can ea sily ignore the locking protocol and potentially cause deadlock. Finall y, lock c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 COMMON CONCURRENCY PROBLEMS TIP: ENFORCE LOCK ORDERING BYLOCK ADDRESS In some cases, a function must grab two (or more) locks; thus, we know we must be careful or deadlock could arise. Imagine a function tha t is called as follows: dosomething(mutex t*m1, mutex t*m2). If the code always grabs m1beforem2(or always m2beforem1), it could deadlock, because one thread could call dosomething(L1, L2) while another thread could call dosomething(L2, L1) . To avoid this particular issue, the clever programmer can use t headdress of each lock as a way of ordering lock acquisition. By acquiring locks in either high-to-low or low-to-high address order, dosomething() can guarantee that it always acquires locks in the same order, rega rdless of which order they are passed in. The code would look something like th is: if (m1 > m2) { // grab locks in high-to-low address order pthread_mutex_lock(m1); pthread_mutex_lock(m2); } else { pthread_mutex_lock(m2); pthread_mutex_lock(m1); } // Code assumes that m1 .= m2 (it is not the same lock) By using this simple technique, a programmer can ensure a simp le and efﬁcient deadlock-free implementation of multi-lock acquisit ion. ordering requires a deep understanding of the code base, and how v ari- ous routines are called; just one mistake could result in the “D” w ord1.",3068
32. Concurrency Bugs,"Hold-and-wait The hold-and-wait requirement for deadlock can be avoided by acq uiring all locks at once, atomically. In practice, this could be achieve d as follows: 1pthread_mutex_lock(prevention); // begin lock acquisiti on 2pthread_mutex_lock(L1); 3pthread_mutex_lock(L2); 4... 5pthread_mutex_unlock(prevention); // end By ﬁrst grabbing the lock prevention , this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided. Of course, it requires that an y time any thread grabs a lock, it ﬁrst acquires the global prevention l ock. For example, if another thread was trying to grab locks L1andL2in a dif- ferent order, it would be OK, because it would be holding the preve ntion lock while doing so. 1Hint: “D” stands for “Deadlock”. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 9 Note that the solution is problematic for a number of reasons. As before, encapsulation works against us: when calling a routine, this ap- proach requires us to know exactly which locks must be held and to ac- quire them ahead of time. This technique also is likely to decr ease con- currency as all locks must be acquired early on (at once) instead of when they are truly needed. No Preemption Because we generally view locks as held until unlock is called, multiple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide a more ﬂ ex- ible set of interfaces to help avoid this situation. Speciﬁcally , the routine pthread mutextrylock() either grabs the lock (if it is available) and returns success or returns an error code indicating the lock is he ld; in the latter case, you can try again later if you want to grab that lock. Such an interface could be used as follows to build a deadlock-free , ordering-robust lock acquisition protocol: 1top: 2pthread_mutex_lock(L1); 3if (pthread_mutex_trylock(L2) .= 0) { 4pthread_mutex_unlock(L1); 5goto top; 6} Note that another thread could follow the same protocol but grab the locks in the other order ( L2thenL1) and the program would still be dead- lock free. One new problem does arise, however: livelock . It is possible (though perhaps unlikely) that two threads could both be repeat edly at- tempting this sequence and repeatedly failing to acquire bot h locks. In this case, both systems are running through this code sequence ov er and over again (and thus it is not a deadlock), but progress is not being made, hence the name livelock. There are solutions to the livelock probl em, too: for example, one could add a random delay before looping back and try- ing the entire thing over again, thus decreasing the odds of repe ated in- terference among competing threads. One point about this solution: it skirts around the hard parts of usi ng a trylock approach. The ﬁrst problem that would likely exist agai n arises due to encapsulation: if one of these locks is buried in some routine that is getting called, the jump back to the beginning becomes more c omplex to implement.",3102
32. Concurrency Bugs,"If the code had acquired some resources (other than L1) along the way, it must make sure to carefully release them as we ll; for example, if after acquiring L1, the code had allocated some memory, it would have to release that memory upon failure to acquire L2, before jumping back to the top to try the entire sequence again. Howeve r, in limited circumstances (e.g., the Java vector method mentioned earlier), this type of approach could work well. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 COMMON CONCURRENCY PROBLEMS You might also notice that this approach doesn’t really addpreemption (the forcible action of taking a lock away from a thread that owns it) , but rather uses the trylock approach to allow a developer to back ou t of lock ownership (i.e., preempt their own ownership) in a graceful way. However, it is a practical approach, and thus we include it here , despite its imperfection in this regard. Mutual Exclusion The ﬁnal prevention technique would be to avoid the need for mutua l exclusion at all. In general, we know this is difﬁcult, because the code we wish to run does indeed have critical sections. So what can we do? Herlihy had the idea that one could design various data structur es without locks at all [H91, H93]. The idea behind these lock-free (and related wait-free ) approaches here is simple: using powerful hardware instructions, you can build data structures in a manner that doe s not re- quire explicit locking. As a simple example, let us assume we have a compare-and-swap i n- struction, which as you may recall is an atomic instruction provid ed by the hardware that does the following: 1int CompareAndSwap(int *address, int expected, int new) { 2if (*address == expected) { 3*address = new; 4return 1; // success 5} 6return 0; // failure 7} Imagine we now wanted to atomically increment a value by a certa in amount. We could do it as follows: 1void AtomicIncrement(int *value, int amount) { 2do { 3int old = *value; 4} while (CompareAndSwap(value, old, old + amount) == 0); 5} Instead of acquiring a lock, doing the update, and then releasin g it, we have instead built an approach that repeatedly tries to updat e the value to the new amount and uses the compare-and-swap to do so. In this man ner, no lock is acquired, and no deadlock can arise (though livelock is still a possibility). Let us consider a slightly more complex example: list insertion. Here is code that inserts at the head of a list: 1void insert(int value) { 2node_t*n = malloc(sizeof(node_t)); 3assert(n .= NULL); 4n->value = value; 5n->next = head; 6head = n; 7} OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 11 This code performs a simple insertion, but if called by multiple threads at the “same time”, has a race condition (see if you can ﬁgure out wh y). Of course, we could solve this by surrounding this code with a lock acqu ire and release: 1void insert(int value) { 2node_t*n = malloc(sizeof(node_t)); 3assert(n .= NULL); 4n->value = value; 5pthread_mutex_lock(listlock); // begin critical section 6n->next = head; 7head = n; 8pthread_mutex_unlock(listlock); // end critical section 9} In this solution, we are using locks in the traditional manner2. Instead, let us try to perform this insertion in a lock-free manner simply using the compare-and-swap instruction. Here is one possible approach: 1void insert(int value) { 2node_t*n = malloc(sizeof(node_t)); 3assert(n .= NULL); 4n->value = value; 5do { 6n->next = head; 7} while (CompareAndSwap(&head, n->next, n) == 0); 8} The code here updates the next pointer to point to the current hea d, and then tries to swap the newly-created node into position as th e new head of the list.",3710
32. Concurrency Bugs,"However, this will fail if some other thread succ essfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head. Of course, building a useful list requires more than just a list insert, and not surprisingly building a list that you can insert into, de lete from, and perform lookups on in a lock-free manner is non-trivial. Read th e rich literature on lock-free and wait-free synchronization to l earn more [H01, H91, H93]. Deadlock Avoidance via Scheduling Instead of deadlock prevention, in some scenarios deadlock avoidance is preferable. Avoidance requires some global knowledge of which locks various threads might grab during their execution, and subseq uently sched- ules said threads in a way as to guarantee no deadlock can occur. For example, assume we have two processors and four threads which must be scheduled upon them. Assume further we know that Thread 2The astute reader might be asking why we grabbed the lock so late, inste ad of right when entering insert() ; can you, astute reader, ﬁgure out why that is likely correct? What assumptions does the code make, for example, about the call to malloc() ? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 COMMON CONCURRENCY PROBLEMS 1 (T1) grabs locks L1andL2(in some order, at some point during its execution), T2 grabs L1andL2as well, T3 grabs just L2, and T4 grabs no locks at all. We can show these lock acquisition demands of the thre ads in tabular form: T1 T2 T3 T4 L1 yes yes no no L2 yes yes yes no A smart scheduler could thus compute that as long as T1 and T2 are not run at the same time, no deadlock could ever arise. Here is one s uch schedule: CPU 1 CPU 2 T1 T2T3 T4 Note that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even though T3 grabs lock L2, it can never cause a deadlock by running con- currently with other threads because it only grabs one lock. Let’s look at one more example. In this one, there is more contention for the same resources (again, locks L1andL2), as indicated by the fol- lowing contention table: T1 T2 T3 T4 L1 yes yes yes no L2 yes yes yes no In particular, threads T1, T2, and T3 all need to grab both locks L1and L2at some point during their execution. Here is a possible schedule that guarantees that no deadlock could ever occur: CPU 1 CPU 2 T1 T2 T3T4 As you can see, static scheduling leads to a conservative approa ch where T1, T2, and T3 are all run on the same processor, and thus the total time to complete the jobs is lengthened considerably. Thoug h it may have been possible to run these tasks concurrently, the fear of d eadlock prevents us from doing so, and the cost is performance. One famous example of an approach like this is Dijkstra’s Banker’s Al- gorithm [D64], and many similar approaches have been describe d in the literature. Unfortunately, they are only useful in very limit ed environ- ments, for example, in an embedded system where one has full know l- edge of the entire set of tasks that must be run and the locks that t hey need. Further, such approaches can limit concurrency, as we sa w in the second example above. Thus, avoidance of deadlock via scheduling is not a widely-used general-purpose solution.",3212
32. Concurrency Bugs,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 13 TIP: DON’TALWAYS DOITPERFECTLY (TOMWEST’SLAW) Tom West, famous as the subject of the classic computer-industry book Soul of a New Machine [K81], says famously: “Not everything worth doing is worth doing well”, which is a terriﬁc engineering maxim. If a bad thing happens rarely, certainly one should not spend a great dea l of effort to prevent it, particularly if the cost of the bad thing occurrin g is small. If, on the other hand, you are building a space shuttle, and the cos t of something going wrong is the space shuttle blowing up, well, perh aps you should ignore this piece of advice. Some readers object: “This sounds like your are suggesting mediocr ity as a solution.” Perhaps they are right, that we should be careful w ith advice such as this. However, our experience tells us that in th e world of engineering, with pressing deadlines and other real-world con cerns, one will always have to decide which aspects of a system to build we ll and which to put aside for another day. The hard part is knowing which to do when, a bit of insight only gained through experience and dedi cation to the task at hand. Detect and Recover One ﬁnal general strategy is to allow deadlocks to occasionally oc cur, and then take some action once such a deadlock has been detected. For ex am- ple, if an OS froze once a year, you would just reboot it and get happil y (or grumpily) on with your work. If deadlocks are rare, such a non-solut ion is indeed quite pragmatic. Many database systems employ deadlock detection and recovery te ch- niques. A deadlock detector runs periodically, building a resou rce graph and checking it for cycles. In the event of a cycle (deadlock), th e system needs to be restarted. If more intricate repair of data structu res is ﬁrst required, a human being may be involved to ease the process. More detail on database concurrency, deadlock, and related issu es can be found elsewhere [B+87, K87]. Read these works, or better yet, take a course on databases to learn more about this rich and interesting topic. 32.4 Summary In this chapter, we have studied the types of bugs that occur in c on- current programs. The ﬁrst type, non-deadlock bugs, are surpri singly common, but often are easier to ﬁx. They include atomicity violati ons, in which a sequence of instructions that should have been execut ed to- gether was not, and order violations, in which the needed order bet ween two threads was not enforced. We have also brieﬂy discussed deadlock: why it occurs, and what can be done about it. The problem is as old as concurrency itself, and ma ny c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 COMMON CONCURRENCY PROBLEMS hundreds of papers have been written about the topic. The best sol u- tion in practice is to be careful, develop a lock acquisition order , and thus prevent deadlock from occurring in the ﬁrst place. Wait-fr ee ap- proaches also have promise, as some wait-free data structures a re now ﬁnding their way into commonly-used libraries and critical sy stems, in- cluding Linux. However, their lack of generality and the comple xity to develop a new wait-free data structure will likely limit the ov erall util- ity of this approach. Perhaps the best solution is to develop new con cur- rent programming models: in systems such as MapReduce (from Googl e) [GD02], programmers can describe certain types of parallel com putations without any locks whatsoever.",3505
32. Concurrency Bugs,"Locks are problematic by their very na- ture; perhaps we should seek to avoid using them unless we truly must. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG COMMON CONCURRENCY PROBLEMS 15 References [B+87] “Concurrency Control and Recovery in Database Systems” by Phili p A. Bernstein, Vas- sos Hadzilacos, Nathan Goodman. Addison-Wesley, 1987. The classic text on concurrency in database management systems. As you can tell, understanding concurrency, deadlock, and other topics in the world of databases is a world unto itself. Study it and ﬁnd out for yoursel f. [C+71] “System Deadlocks” by E.G. Coffman, M.J. Elphick, A. Shoshani. AC M Computing Surveys, 3:2, June 1971. The classic paper outlining the conditions for deadlock and how you might go about dealing with it. There are certainly some earlier papers on this topic ; see the references within this paper for details. [D64] “Een algorithme ter voorkoming van de dodelijke omarming” by Edsger Dijkstra. 1964. Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/ EWD108.PDF. Indeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the ﬁrst to note its existence, at least in written form. However, he called it the “deadly em brace”, which (thankfully) did not catch on. [GD02] “MapReduce: Simpliﬁed Data Processing on Large Clusters” b y Sanjay Ghemawhat, Jeff Dean. OSDI ’04, San Francisco, CA, October 2004. The MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for performing such computations on clusters of generally unreliable machines. [H01] “A Pragmatic Implementation of Non-blocking Linked-lists” by T im Harris. Interna- tional Conference on Distributed Computing (DISC), 2001. A relatively modern example of the difﬁculties of building something as simple as a concurrent linked li st without locks. [H91] “Wait-free Synchronization” by Maurice Herlihy . ACM TOPLAS, 13:1, Ja nuary 1991. Herlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These approaches tend to be complex and hard, often more difﬁcult than using locks cor rectly, probably limiting their success in the real world. [H93] “A Methodology for Implementing Highly Concurrent Data Objects” by Maurice Her- lihy. ACM TOPLAS, 15:5, November 1993. A nice overview of lock-free and wait-free structures. Both approaches eschew locks, but wait-free approaches are harder to realize, as th ey try to ensure than any operation on a concurrent structure will terminate in a ﬁnite number of ste ps (e.g., no unbounded looping). [J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlo cks” by Horatiu Jula, Daniel Tralamazza, Cristian Zamﬁr, George Candea. OSDI ’0 8, San Diego, CA, December 2008. An excellent recent paper on deadlocks and how to avoid getting caught in the s ame ones over and over again in a particular system. [K81] “Soul of a New Machine” by Tracy Kidder. Backbay Books, 2000 (rep rint of 1980 ver- sion). A must-read for any systems builder or engineer, detailing the early days of how a team inside Data General (DG), led by Tom West, worked to produce a “new machine.” Kidde r’s other books are also excellent, including “Mountains beyond Mountains.” Or maybe you don ’t agree with us, comma?",3291
32. Concurrency Bugs,"[K87] “Deadlock Detection in Distributed Databases” by Edgar K napp. ACM Computing Sur- veys, 19:4, December 1987. An excellent overview of deadlock detection in distributed database sys- tems. Also points to a number of other related works, and thus is a good place to start your reading. [L+08] “Learning from Mistakes — A Comprehensive Study on Real World Concurrency Bug Characteristics” by Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou. ASPLOS ’08, March 2008, Seattle, Washington. The ﬁrst in-depth study of concurrency bugs in real software, and the basis for this chapter. Look at Y.Y. Zhou’s or Shan Lu’s web pages for many more intere sting papers on bugs. [T+94] “Linux File Memory Map Code” by Linus Torvalds and many others. A vailable online at:http://lxr.free-electrons.com/source/mm/filemap.c .Thanks to Michael Wal- ﬁsh (NYU) for pointing out this precious example. The real world, as you can s ee in this ﬁle, can be a bit more complex than the simple clarity found in textbooks... c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 COMMON CONCURRENCY PROBLEMS Homework (Code) This homework lets you explore some real code that deadlocks (or avoids deadlock). The different versions of code correspond to diff erent approaches to avoiding deadlock in a simpliﬁed vectoradd() routine. See the README for details on these programs and their common sub- strate. Questions 1. First let’s make sure you understand how the programs generall y work, and some of the key options. Study the code in vector-deadlock.c , as well as inmain-common.c and related ﬁles. Now, run ./vector-deadlock -n 2 -l 1 -v , which instantiates two threads (-n 2 ), each of which does one vector add ( -l 1 ), and does so in verbose mode ( -v). Make sure you understand the output. How does the output change from run to run? 2. Now add the -dﬂag, and change the number of loops ( -l) from 1 to higher numbers. What happens? Does the code (always) deadlock? 3. How does changing the number of threads ( -n) change the outcome of the program? Are there any values of -nthat ensure no deadlock occurs? 4. Now examine the code in vector-global-order.c . First, make sure you understand what the code is trying to do; do you understand why t he code avoids deadlock? Also, why is there a special case in this vectoradd() routine when the source and destination vectors are the same? 5. Now run the code with the following ﬂags: -t -n 2 -l 100000 -d . How long does the code take to complete? How does the total time c hange when you increase the number of loops, or the number of threads? 6. What happens if you turn on the parallelism ﬂag ( -p)? How much would you expect performance to change when each thread is working on adding different vectors (which is what -penables) versus working on the same ones? 7. Now let’s study vector-try-wait.c . First make sure you understand the code. Is the ﬁrst call to pthread mutextrylock() really needed? Now run the code. How fast does it run compared to the global order ap- proach? How does the number of retries, as counted by the code, ch ange as the number of threads increases? 8. Now let’s look at vector-avoid-hold-and-wait.c . What is the main problem with this approach? How does its performance compare t o the other versions, when running both with -pand without it? 9. Finally, let’s look at vector-nolock.c . This version doesn’t use locks at all; does it provide the exact same semantics as the other versio ns? Why or why not? 10. Now compare its performance to the other versions, both whe n threads are working on the same two vectors (no -p) and when each thread is working on separate vectors ( -p). How does this no-lock version perform? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3747
33. Event-based Concurrency,"33 Event-based Concurrency (Advanced) Thus far, we’ve written about concurrency as if the only way to bui ld concurrent applications is to use threads. Like many things in life, this is not completely true. Speciﬁcally, a different style of concurr ent pro- gramming is often used in both GUI-based applications [O96] as w ell as some types of internet servers [PDZ99]. This style, known as event-based concurrency , has become popular in some modern systems, including server-side frameworks such as node.js [N13], but its roots are found in C/U NIXsystems that we’ll discuss below. The problem that event-based concurrency addresses is two-fold . The ﬁrst is that managing concurrency correctly in multi-threade d applica- tions can be challenging; as we’ve discussed, missing locks, de adlock, and other nasty problems can arise. The second is that in a multi- threaded application, the developer has little or no control over what is sch eduled at a given moment in time; rather, the programmer simply create s threads and then hopes that the underlying OS schedules them in a reason able manner across available CPUs. Given the difﬁculty of building a general- purpose scheduler that works well in all cases for all workloads, s ome- times the OS will schedule work in a manner that is less than opti mal. And thus, we have ... THECRUX: HOWTOBUILD CONCURRENT SERVERS WITHOUT THREADS How can we build a concurrent server without using threads, and t hus retain control over concurrency as well as avoid some of the problems that seem to plague multi-threaded applications? 33.1 The Basic Idea: An Event Loop The basic approach we’ll use, as stated above, is called event-based concurrency . The approach is quite simple: you simply wait for some- thing (i.e., an “event”) to occur; when it does, you check what ty pe of 1 2 EVENT -BASED CONCURRENCY (ADVANCED ) event it is and do the small amount of work it requires (which may i n- clude issuing I/O requests, or scheduling other events for futu re han- dling, etc.). That’s it. Before getting into the details, let’s ﬁrst examine what a canon ical event-based server looks like. Such applications are based aroun d a sim- ple construct known as the event loop . Pseudocode for an event loop looks like this: while (1) { events = getEvents(); for (e in events) processEvent(e); } It’s really that simple. The main loop simply waits for something t o do (by calling getEvents() in the code above) and then, for each event re- turned, processes them, one at a time; the code that processes eac h event is known as an event handler . Importantly, when a handler processes an event, it is the only activity taking place in the system; th us, deciding which event to handle next is equivalent to scheduling. This explicit con- trol over scheduling is one of the fundamental advantages of the ev ent- based approach. But this discussion leaves us with a bigger question: how exactl y does an event-based server determine which events are taking pla ce, in par- ticular with regards to network and disk I/O? Speciﬁcally, how c an an event server tell if a message has arrived for it?",3131
33. Event-based Concurrency,"33.2 An Important API: select() (orpoll() ) With that basic event loop in mind, we next must address the ques tion of how to receive events. In most systems, a basic API is availabl e, via either theselect() orpoll() system calls. What these interfaces enable a program to do is simple: check w hether there is any incoming I/O that should be attended to. For example, imag- ine that a network application (such as a web server) wishes to c heck whether any network packets have arrived, in order to service t hem. These system calls let you do exactly that. Takeselect() for example. The manual page (on a Mac) describes the API in this manner: int select(int nfds, fd_set*restrict readfds, fd_set*restrict writefds, fd_set*restrict errorfds, struct timeval *restrict timeout); The actual description from the man page: select() examines the I/O de- scriptor sets whose addresses are passed in readfds, writefds, an d errorfds to see if some of their descriptors are ready for reading, are ready for writ ing, or have OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG EVENT -BASED CONCURRENCY (ADVANCED ) 3 ASIDE : BLOCKING VS . NON-BLOCKING INTERFACES Blocking (or synchronous ) interfaces do all of their work before returning to the caller; non-blocking (or asynchronous ) interfaces begin some work but return immediately, thus letting whatever work that need s to be done get done in the background. The usual culprit in blocking calls is I/O of some kind. For exampl e, if a call must read from disk in order to complete, it might block, wait ing for the I/O request that has been sent to the disk to return. Non-blocking interfaces can be used in any style of programming ( e.g., with threads), but are essential in the event-based approach , as a call that blocks will halt all progress. an exceptional condition pending, respectively. The ﬁrst nfds descr iptors are checked in each set, i.e., the descriptors from 0 through nfds-1 in t he descriptor sets are examined. On return, select() replaces the given descrip tor sets with subsets consisting of those descriptors that are ready for the re quested operation. select() returns the total number of ready descriptors in all the sets. A couple of points about select() . First, note that it lets you check whether descriptors can be read from as well as written to; the former lets a server determine that a new packet has arrived and is in need of processing, whereas the latter lets the service know when it is OK to reply (i.e., the outbound queue is not full). Second, note the timeout argument. One common usage here is to set the timeout to NULL, which causes select() to block indeﬁnitely, until some descriptor is ready. However, more robust servers will usually specify some kind of timeout; one common technique is to set the time out to zero, and thus use the call to select() to return immediately. Thepoll() system call is quite similar. See its manual page, or Stevens and Rago [SR05], for details. Either way, these basic primitives give us a way to build a non- blocking event loop, which simply checks for incoming packets, reads from s ockets with messages upon them, and replies as needed.",3169
33. Event-based Concurrency,"33.3 Using select() To make this more concrete, let’s examine how to use select() to see which network descriptors have incoming messages upon them. Fig ure 33.1 shows a simple example. This code is actually fairly simple to understand. After some i nitial- ization, the server enters an inﬁnite loop. Inside the loop, it use s the FDZERO() macro to ﬁrst clear the set of ﬁle descriptors, and then uses FDSET() to include all of the ﬁle descriptors from minFD tomaxFD in the set. This set of descriptors might represent, for example, a ll of the net- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 EVENT -BASED CONCURRENCY (ADVANCED ) 1#include <stdio.h> 2#include <stdlib.h> 3#include <sys/time.h> 4#include <sys/types.h> 5#include <unistd.h> 6 7int main(void) { 8// open and set up a bunch of sockets (not shown) 9// main loop 10 while (1) { 11 // initialize the fd_set to all zero 12 fd_set readFDs; 13 FD_ZERO(&readFDs); 14 15 // now set the bits for the descriptors 16 // this server is interested in 17 // (for simplicity, all of them from min to max) 18 int fd; 19 for (fd = minFD; fd < maxFD; fd++) 20 FD_SET(fd, &readFDs); 21 22 // do the select 23 int rc = select(maxFD+1, &readFDs, NULL, NULL, NULL); 24 25 // check which actually have data using FD_ISSET() 26 int fd; 27 for (fd = minFD; fd < maxFD; fd++) 28 if (FD_ISSET(fd, &readFDs)) 29 processFD(fd); 30 } 31} Figure 33.1: Simple Code Using select() work sockets to which the server is paying attention. Finally, t he server callsselect() to see which of the connections have data available upon them. By then using FDISSET() in a loop, the event server can see which of the descriptors have data ready and process the incoming data. Of course, a real server would be more complicated than this, and require logic to use when sending messages, issuing disk I/O, and many other details. For further information, see Stevens and Rago [SR05 ] for API information, or Pai et. al or Welsh et al. for a good overview of the general ﬂow of event-based servers [PDZ99, WCB01]. 33.4 Why Simpler? No Locks Needed With a single CPU and an event-based application, the problems found in concurrent programs are no longer present. Speciﬁcally, beca use only one event is being handled at a time, there is no need to acquire or release locks; the event-based server cannot be interrupted by another thread be- cause it is decidedly single threaded. Thus, concurrency bug s common in threaded programs do not manifest in the basic event-based app roach. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG EVENT -BASED CONCURRENCY (ADVANCED ) 5 TIP: DON’TBLOCK INEVENT -BASED SERVERS Event-based servers enable ﬁne-grained control over scheduli ng of tasks. However, to maintain such control, no call that blocks the execut ion of the caller can ever be made; failing to obey this design tip wil l result in a blocked event-based server, frustrated clients, and seriou s questions as to whether you ever read this part of the book. 33.5 A Problem: Blocking System Calls Thus far, event-based programming sounds great, right? You prog ram a simple loop, and handle events as they arise.",3145
33. Event-based Concurrency,"You don’t even need t o think about locking. But there is an issue: what if an event requ ires that you issue a system call that might block? For example, imagine a request comes from a client into a server t o read a ﬁle from disk and return its contents to the requesting cl ient (much like a simple HTTP request). To service such a request, some ev ent han- dler will eventually have to issue an open() system call to open the ﬁle, followed by a series of read() calls to read the ﬁle. When the ﬁle is read into memory, the server will likely start sending the results to the client. Both theopen() andread() calls may issue I/O requests to the stor- age system (when the needed metadata or data is not in memory alre ady), and thus may take a long time to service. With a thread-based se rver, this is no issue: while the thread issuing the I/O request suspend s (waiting for the I/O to complete), other threads can run, thus enabling th e server to make progress. Indeed, this natural overlap of I/O and other computa- tion is what makes thread-based programming quite natural and straight- forward. With an event-based approach, however, there are no other threa ds to run: just the main event loop. And this implies that if an event h andler issues a call that blocks, the entire server will do just that: block until the call completes. When the event loop blocks, the system sits idle, and thus is a huge potential waste of resources. We thus have a rule that mu st be obeyed in event-based systems: no blocking calls are allowed. 33.6 A Solution: Asynchronous I/O To overcome this limit, many modern operating systems have intro- duced new ways to issue I/O requests to the disk system, refer red to generically as asynchronous I/O . These interfaces enable an application to issue an I/O request and return control immediately to the ca ller, be- fore the I/O has completed; additional interfaces enable an app lication to determine whether various I/Os have completed. For example, let us examine the interface provided on a Mac (other systems have similar APIs). The APIs revolve around a basic str ucture, c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 EVENT -BASED CONCURRENCY (ADVANCED ) thestruct aiocb orAIO control block in common terminology. A simpliﬁed version of the structure looks like this (see the manua l pages for more information): struct aiocb { int aio_fildes; / *File descriptor */ off_t aio_offset; / *File offset */ volatile void *aio_buf; / *Location of buffer */ size_t aio_nbytes; / *Length of transfer */ }; To issue an asynchronous read to a ﬁle, an application should ﬁrst ﬁll in this structure with the relevant information: the ﬁle de scriptor of the ﬁle to be read ( aiofildes ), the offset within the ﬁle ( aiooffset ) as well as the length of the request ( aionbytes ), and ﬁnally the tar- get memory location into which the results of the read should be copi ed (aiobuf). After this structure is ﬁlled in, the application must issue t he asyn- chronous call to read the ﬁle; on a Mac, this API is simply the asyn- chronous read API: int aio_read(struct aiocb *aiocbp); This call tries to issue the I/O; if successful, it simply ret urns right away and the application (i.e., the event-based server) can c ontinue with its work.",3286
33. Event-based Concurrency,"There is one last piece of the puzzle we must solve, however. How can we tell when an I/O is complete, and thus that the buffer (pointe d to by aiobuf) now has the requested data within it? One last API is needed. On a Mac, it is referred to (somewhat conf us- ingly) asaioerror() . The API looks like this: int aio_error(const struct aiocb *aiocbp); This system call checks whether the request referred to by aiocbp has completed. If it has, the routine returns success (indicated b y a zero); if not, EINPROGRESS is returned. Thus, for every outstanding asy n- chronous I/O, an application can periodically poll the system via a call toaioerror() to determine whether said I/O has yet completed. One thing you might have noticed is that it is painful to check wh ether an I/O has completed; if a program has tens or hundreds of I/Os issu ed at a given point in time, should it simply keep checking each of th em repeatedly, or wait a little while ﬁrst, or ... ? To remedy this issue, some systems provide an approach based on th e interrupt . This method uses U NIX signals to inform applications when an asynchronous I/O completes, thus removing the need to repeate dly ask the system. This polling vs. interrupts issue is seen in de vices too, as you will see (or already have seen) in the chapter on I/O devices. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG EVENT -BASED CONCURRENCY (ADVANCED ) 7 ASIDE : UNIXSIGNALS A huge and fascinating infrastructure known as signals is present in all mod- ern U NIX variants. At its simplest, signals provide a way to communicate wi th a process. Speciﬁcally, a signal can be delivered to an applic ation; doing so stops the application from whatever it is doing to run a signal handler , i.e., some code in the application to handle that signal. When ﬁnished, the pro cess just resumes its previous behavior. Each signal has a name, such as HUP (hang up), INT (interrupt), SEGV (seg- mentation violation), etc; see the manual page for details. Int erestingly, sometimes it is the kernel itself that does the signaling. For example, wh en your program en- counters a segmentation violation, the OS sends it a SIGSEGV (prepending SIG to signal names is common); if your program is conﬁgured to catch th at signal, you can actually run some code in response to this erroneous progr am behavior (which can be useful for debugging). When a signal is sent to a pro cess not conﬁg- ured to handle that signal, some default behavior is enacted; fo r SEGV , the process is killed. Here is a simple program that goes into an inﬁnite loop, but has ﬁ rst set up a signal handler to catch SIGHUP: #include <stdio.h> #include <signal.h> void handle(int arg) { printf(\""stop wakin’ me up... \""); } int main(int argc, char *argv[]) { signal(SIGHUP, handle); while (1) ; // doin’ nothin’ except catchin’ some sigs return 0; } You can send signals to it with the kill command line tool (yes, this is an odd and aggressive name). Doing so will interrupt the main while loo p in the program and run the handler code handle() : prompt> ./main & [3] 36705 prompt> kill -HUP 36705 stop wakin’ me up... prompt> kill -HUP 36705 stop wakin’ me up... prompt> kill -HUP 36705 stop wakin’ me up...",3217
33. Event-based Concurrency,"There is a lot more to learn about signals, so much that a single cha pter, much less a single page, does not nearly sufﬁce. As always, there is o ne great source: Stevens and Rago [SR05]. Read more if interested. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 EVENT -BASED CONCURRENCY (ADVANCED ) In systems without asynchronous I/O, the pure event-based appr oach cannot be implemented. However, clever researchers have deri ved meth- ods that work fairly well in their place. For example, Pai et al. [ PDZ99] describe a hybrid approach in which events are used to process n etwork packets, and a thread pool is used to manage outstanding I/Os. Re ad their paper for details. 33.7 Another Problem: State Management Another issue with the event-based approach is that such code is gen- erally more complicated to write than traditional thread-base d code. The reason is as follows: when an event handler issues an asynchronous I/O, it must package up some program state for the next event handler t o use when the I/O ﬁnally completes; this additional work is not needed in thread-based programs, as the state the program needs is on the s tack of the thread. Adya et al. call this work manual stack management , and it is fundamental to event-based programming [A+02]. To make this point more concrete, let’s look at a simple example in which a thread-based server needs to read from a ﬁle descriptor (fd) and, once complete, write the data that it read from the ﬁle to a network socket descriptor ( sd). The code (ignoring error checking) looks like this: int rc = read(fd, buffer, size); rc = write(sd, buffer, size); As you can see, in a multi-threaded program, doing this kind of work is trivial; when the read() ﬁnally returns, the code immediately knows which socket to write to because that information is on the stack of the thread (in the variable sd). In an event-based system, life is not so easy. To perform the sam e task, we’d ﬁrst issue the read asynchronously, using the AIO calls des cribed above. Let’s say we then periodically check for completion of the rea d using the aioerror() call; when that call informs us that the read is complete, how does the event-based server know what to do? The solution, as described by Adya et al. [A+02], is to use an old p ro- gramming language construct known as a continuation [FHK84]. Though it sounds complicated, the idea is rather simple: basically, r ecord the needed information to ﬁnish processing this event in some data st ruc- ture; when the event happens (i.e., when the disk I/O complete s), look up the needed information and process the event. In this speciﬁc case, the solution would be to record the socket de- scriptor (sd) in some kind of data structure (e.g., a hash table), indexed by the ﬁle descriptor ( fd). When the disk I/O completes, the event han- dler would use the ﬁle descriptor to look up the continuation, which will return the value of the socket descriptor to the caller. At this p oint (ﬁ- nally), the server can then do the last bit of work to write the da ta to the socket.",3081
33. Event-based Concurrency,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG EVENT -BASED CONCURRENCY (ADVANCED ) 9 33.8 What Is Still Difﬁcult With Events There are a few other difﬁculties with the event-based approac h that we should mention. For example, when systems moved from a single CPU to multiple CPUs, some of the simplicity of the event-based a p- proach disappeared. Speciﬁcally, in order to utilize more than on e CPU, the event server has to run multiple event handlers in parall el; when do- ing so, the usual synchronization problems (e.g., critical sect ions) arise, and the usual solutions (e.g., locks) must be employed. Thus, on mod - ern multicore systems, simple event handling without locks is n o longer possible. Another problem with the event-based approach is that it does not integrate well with certain kinds of systems activity, such a spaging . For example, if an event-handler page faults, it will block, and t hus the server will not make progress until the page fault completes. Even thoug h the server has been structured to avoid explicit blocking, this type of implicit blocking due to page faults is hard to avoid and thus can lead to l arge performance problems when prevalent. A third issue is that event-based code can be hard to manage over time, as the exact semantics of various routines changes [A+02]. For ex ample, if a routine changes from non-blocking to blocking, the event hand ler that calls that routine must also change to accommodate its new n ature, by ripping itself into two pieces. Because blocking is so disa strous for event-based servers, a programmer must always be on the lookout for such changes in the semantics of the APIs each event uses. Finally, though asynchronous disk I/O is now possible on most plat- forms, it has taken a long time to get there [PDZ99], and it never quite integrates with asynchronous network I/O in as simple and unifor m a manner as you might think. For example, while one would simply lik e to use the select() interface to manage all outstanding I/Os, usually some combination of select() for networking and the AIO calls for disk I/O are required. 33.9 Summary We’ve presented a bare bones introduction to a different style of c on- currency based on events. Event-based servers give control of sc hedul- ing to the application itself, but do so at some cost in complexity and difﬁculty of integration with other aspects of modern systems (e. g., pag- ing). Because of these challenges, no single approach has emer ged as best; thus, both threads and events are likely to persist as tw o different approaches to the same concurrency problem for many years to come. Read some research papers (e.g., [A+02, PDZ99, vB+03, WCB01] ) or bet- ter yet, write some event-based code, to learn more. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 EVENT -BASED CONCURRENCY (ADVANCED ) References [A+02] “Cooperative Task Management Without Manual Stack Management” b y Atul Adya, Jon Howell, Marvin Theimer, William J. Bolosky, John R. Douceur. USEN IX ATC ’02, Monterey, CA, June 2002.",3057
33. Event-based Concurrency,"This gem of a paper is the ﬁrst to clearly articulate some of the difﬁculties of event-based concurrency, and suggests some simple solutions, as well explores the e ven crazier idea of combining the two types of concurrency management into a single application. [FHK84] “Programming With Continuations” by Daniel P . Friedman, Chri stopher T. Haynes, Eugene E. Kohlbecker. In Program Transformation and Programming Envir onments, Springer Verlag, 1984. The classic reference to this old idea from the world of programming langu ages. Now increasingly popular in some modern languages. [N13] “Node.js Documentation” by the folks who built node.js. Availa ble:nodejs.org/api . One of the many cool new frameworks that help you readily build web services and applications. Every modern systems hacker should be proﬁcient in frameworks such as this one (and likely, more than one). Spend the time and do some development in one of these worlds and become an ex pert. [O96] “Why Threads Are A Bad Idea (for most purposes)” by John Ousterhout. I nvited Talk at USENIX ’96, San Diego, CA, January 1996. A great talk about how threads aren’t a great match for GUI-based applications (but the ideas are more general). Ousterhout formed m any of these opinions while he was developing Tcl/Tk, a cool scripting language and toolkit that made it 100x easier to develop GUI-based applications than the state of the art at the time. While the Tk GUI toolkit live s on (in Python for example), Tcl seems to be slowly dying (unfortunately). [PDZ99] “Flash: An Efﬁcient and Portable Web Server” by Vivek S. Pai, Pe ter Druschel, Willy Zwaenepoel. USENIX ’99, Monterey, CA, June 1999. A pioneering paper on how to structure web servers in the then-burgeoning Internet era. Read it to understand the basics as well as to see the authors’ ideas on how to build hybrids when support for asynchronous I/O is l acking. [SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen A. Rago. Addison-Wesley, 2005. Once again, we refer to the classic must-have-on-your-bookshelf book of UNIXsystems programming. If there is some detail you need to know, it is in here. [vB+03] “Capriccio: Scalable Threads for Internet Services” by Rob von Behren, Jeremy Condit, Feng Zhou, George C. Necula, Eric Brewer. SOSP ’03, Lake George, N ew York, October 2003. A paper about how to make threads work at extreme scale; a counter to all the event-base d work ongoing at the time. [WCB01] “SEDA: An Architecture for Well-Conditioned, Scalable Interne t Services” by Matt Welsh, David Culler, and Eric Brewer. SOSP ’01, Banff, Canada, Octobe r 2001. A nice twist on event-based serving that combines threads, queues, and event-based hand ling into one streamlined whole. Some of these ideas have found their way into the infrastructures of comp anies such as Google, Amazon, and elsewhere. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG EVENT -BASED CONCURRENCY (ADVANCED ) 11 Homework (Code) In this (short) homework, you’ll gain some experience with event- based code and some of its key concepts.",3089
33. Event-based Concurrency,"Good luck. Questions 1. First, write a simple server that can accept and serve TCP co nnections. You’ll have to poke around the Internet a bit if you don’t already know how to do this. Build this to serve exactly one request at a time; have each r equest be very simple, e.g., to get the current time of day. 2. Now, add the select() interface. Build a main program that can accept multiple connections, and an event loop that checks which ﬁle d escriptors have data on them, and then read and process those requests. Mak e sure to carefully test that you are using select() correctly. 3. Next, let’s make the requests a little more interesting, to mimic a simple web or ﬁle server. Each request should be to read the contents of a ﬁl e (named in the request), and the server should respond by reading the ﬁle i nto a buffer, and then returning the contents to the client. Use the standard open() , read() ,close() system calls to implement this feature. Be a little careful here: if you leave this running for a long time, someone may ﬁgure out how to use it to read all the ﬁles on your computer. 4. Now, instead of using standard I/O system calls, use the asyn chronous I/O interfaces as described in the chapter. How hard was it to inc orporate asyn- chronous interfaces into your program? 5. For fun, add some signal handling to your code. One common use of si gnals is to poke a server to reload some kind of conﬁguration ﬁle, or ta ke some other kind of administrative action. Perhaps one natural way t o play around with this is to add a user-level ﬁle cache to your server, which s tores recently accessed ﬁles. Implement a signal handler that clears the cach e when the signal is sent to the server process. 6. Finally, we have the hard part: how can you tell if the effor t to build an asynchronous, event-based approach are worth it? Can you cre ate an ex- periment to show the beneﬁts? How much implementation complexity di d your approach add? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",2014
34. Summary Dialogue on Concurrency,"34 Summary Dialogue on Concurrency Professor: So, does your head hurt now? Student: (taking two Motrin tablets) Well, some. It’s hard to think about all t he ways threads can interleave. Professor: Indeed it is. I am always amazed that when concurrent execution is involved, just a few lines of code can become nearly impossible to unders tand. Student: Me too. It’s kind of embarrassing, as a Computer Scientist, not to be able to make sense of ﬁve lines of code. Professor: Oh, don’t feel too badly. If you look through the ﬁrst papers on con - current algorithms, they are sometimes wrong. And the authors o ften professors. Student: (gasps) Professors can be ... umm... wrong? Professor: Yes, it is true. Though don’t tell anybody — it’s one of our trade secrets. Student: I am sworn to secrecy. But if concurrent code is so hard to think ab out, and so hard to get right, how are we supposed to write correct con current code? Professor: Well that is the real question, isn’t it? I think it starts with a few simple things. First, keep it simple. Avoid complex interactions betwee n threads, and use well-known and tried-and-true ways to manage thread inte ractions. Student: Like simple locking, and maybe a producer-consumer queue? Professor: Exactly. Those are common paradigms, and you should be able to produce the working solutions given what you’ve learned. Second, o nly use con- currency when absolutely needed; avoid it if at all possible. There is n othing worse than premature optimization of a program. Student: I see — why add threads if you don’t need them? Professor: Exactly. Third, if you really need parallelism, seek it in other sim- pliﬁed forms. For example, the Map-Reduce method for writing parallel d ata analysis code is an excellent example of achieving parallelism without hav ing to handle any of the horriﬁc complexities of locks, condition variables, an d the other nasty things we’ve talked about. 1 2 SUMMARY DIALOGUE ON CONCURRENCY Student: Map-Reduce, huh? Sounds interesting — I’ll have to read more abou t it on my own. Professor: Good. You should. In the end, you’ll have to do a lot of that, as what we learn together can only serve as the barest introduction t o the wealth of knowledge that is out there. Read, read, and read some more. And then try things out, write some code, and then write some more too. As Gladwell talk s about in his book “Outliers”, you need to put roughly 10,000 hours into some thing in order to become a real expert. You can’t do that all inside of class time. Student: Wow, I’m not sure if that is depressing, or uplifting. But I’ll assume the latter, and get to work. Time to write some more concurrent co de... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2736
Part III Persistence,Part III Persistence 1,22
35. A Dialogue on Persistence,"35 A Dialogue on Persistence Professor: And thus we reach the third of our four ... err... three pillars of operating systems: persistence . Student: Did you say there were three pillars, or four? What is the fourth? Professor: No. Just three, young student, just three. Trying to keep it simple here. Student: OK, ﬁne. But what is persistence, oh ﬁne and noble professor? Professor: Actually, you probably know what it means in the traditional sense, right? As the dictionary would say: “a ﬁrm or obstinate continuance in a course of action in spite of difﬁculty or opposition.” Student: It’s kind of like taking your class: some obstinance required. Professor: Ha. Yes. But persistence here means something else. Let me explain . Imagine you are outside, in a ﬁeld, and you pick a — Student: (interrupting) I know. A peach. From a peach tree. Professor: I was going to say apple, from an apple tree. Oh well; we’ll do it your way, I guess. Student: (stares blankly) Professor: Anyhow, you pick a peach; in fact, you pick many many peaches, but you want to make them last for a long time. Winter is hard and cruel in Wisconsin, after all. What do you do? Student: Well, I think there are some different things you can do. You can pickle it. Or bake a pie. Or make a jam of some kind. Lots of fun. Professor: Fun? Well, maybe. Certainly, you have to do a lot more work to make the peach persist . And so it is with information as well; making information persist, despite computer crashes, disk failures, or power outage s is a tough and interesting challenge. Student: Nice segue; you’re getting quite good at that. Professor: Thanks. A professor can always use a few kind words, you know. 3 4 A D IALOGUE ON PERSISTENCE Student: I’ll try to remember that. I guess it’s time to stop talking peaches, and start talking computers? Professor: Yes, it is that time... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",1913
36.  IO Devices,"36 I/O Devices Before delving into the main content of this part of the book (on persi s- tence), we ﬁrst introduce the concept of an input/output (I/O) device and show how the operating system might interact with such an entity . I/O is quite critical to computer systems, of course; imagine a program without any input (it produces the same result each time); now imagine a pro- gram with no output (what was the purpose of it running?). Clearl y, for computer systems to be interesting, both input and output are re quired. And thus, our general problem: CRUX: HOWTOINTEGRATE I/O I NTO SYSTEMS How should I/O be integrated into systems? What are the general mechanisms? How can we make them efﬁcient? 36.1 System Architecture To begin our discussion, let’s look at a “classical” diagram of a typ - ical system (Figure 36.1). The picture shows a single CPU atta ched to the main memory of the system via some kind of memory bus or inter- connect. Some devices are connected to the system via a general I/O bus , which in many modern systems would be PCI (or one of its many deriva- tives); graphics and some other higher-performance I/O device s might be found here. Finally, even lower down are one or more of what we call a peripheral bus , such as SCSI ,SATA , orUSB . These connect slow devices to the system, including disks ,mice , and keyboards . One question you might ask is: why do we need a hierarchical stru c- ture like this? Put simply: physics, and cost. The faster a bus is, the shorter it must be; thus, a high-performance memory bus does not ha ve much room to plug devices and such into it. In addition, engineer ing a bus for high performance is quite costly. Thus, system designe rs have adopted this hierarchical approach, where components that dema nd high performance (such as the graphics card) are nearer the CPU. Low er per- 1 2 I/O D EVICES GraphicsMemory CPU Memory Bus (proprietary) General I/O Bus (e.g., PCI) Peripheral I/O Bus (e.g., SCSI, SATA, USB) Figure 36.1: Prototypical System Architecture formance components are further away. The beneﬁts of placing dis ks and other slow devices on a peripheral bus are manifold; in particula r, you can place a large number of devices on it. Of course, modern systems increasingly use specialized chips ets and faster point-to-point interconnects to improve performance. Fig ure 36.2 (page 3) shows an approximate diagram of Intel’s Z270 Chipset [H1 7]. Along the top, the CPU connects most closely to the memory system, but also has a high-performance connection to the graphics card (and thus, the display) to enable gaming (oh, the horror.) and other gra phics- intensive applications. The CPU connects to an I/O chip via Intel’s proprietary DMI (Direct Media Interface ), and the rest of the devices connect to this chip via a number of different interconnects. On the right, one or more hard d rives connect to the system via the eSATA interface; ATA (the AT Attachment , in reference to providing connection to the IBM PC AT), then SATA (for Serial ATA ), and now eSATA (for external SATA ) represent an evolu- tion of storage interfaces over the past decades, with each step f orward increasing performance to keep pace with modern storage device s. Below the I/O chip are a number of USB (Universal Serial Bus ) con- nections, which in this depiction enable a keyboard and mouse to b e at- tached to the computer. On many modern systems, USB is used for low performance devices such as these. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 3 PCIe GraphicsMemory Interconnect Graphics CPU MemoryDMI I/O ChipeSATA DiskDiskDiskDiskUSBKeyboard MousePCIe Network Figure 36.2: Modern System Architecture Finally, on the left, other higher performance devices can be con nected to the system via PCIe (Peripheral Component Interconnect Express ). In this diagram, a network interface is attached to the system he re; higher performance storage devices (such as NVMe persistent storage devices) are often connected here.",4013
36.  IO Devices,"36.2 A Canonical Device Let us now look at a canonical device (not a real one), and use this device to drive our understanding of some of the machinery requir ed to make device interaction efﬁcient. From Figure 36.3 (page 4), w e can see that a device has two important components. The ﬁrst is the hardw are interface it presents to the rest of the system. Just like a piece of softwar e, hardware must also present some kind of interface that allows th e system software to control its operation. Thus, all devices have some spec iﬁed interface and protocol for typical interaction. The second part of any device is its internal structure . This part of the device is implementation speciﬁc and is responsible for imp lement- ing the abstraction the device presents to the system. Very si mple devices will have one or a few hardware chips to implement their function ality; more complex devices will include a simple CPU, some general pur pose memory, and other device-speciﬁc chips to get their job done. For e xam- ple, modern RAID controllers might consist of hundreds of thousands of lines of ﬁrmware (i.e., software within a hardware device) to implement its functionality. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 I/O D EVICES Other Hardware-specific ChipsMemory (DRAM or SRAM or both)Micro-controller (CPU)Registers Status Command Data Interface Internals Figure 36.3: A Canonical Device 36.3 The Canonical Protocol In the picture above, the (simpliﬁed) device interface is comp rised of three registers: a status register, which can be read to see the current sta- tus of the device; a command register, to tell the device to perform a cer- tain task; and a data register to pass data to the device, or get data from the device. By reading and writing these registers, the opera ting system can control device behavior. Let us now describe a typical interaction that the OS might have with the device in order to get the device to do something on its behalf . The protocol is as follows: While (STATUS == BUSY) ; // wait until device is not busy Write data to DATA register Write command to COMMAND register (Doing so starts the device and executes the command) While (STATUS == BUSY) ; // wait until device is done with your request The protocol has four steps. In the ﬁrst, the OS waits until the dev ice is ready to receive a command by repeatedly reading the status re gister; we call this polling the device (basically, just asking it what is going on). Sec- ond, the OS sends some data down to the data register; one can imagi ne that if this were a disk, for example, that multiple writes woul d need to take place to transfer a disk block (say 4KB) to the device. Whe n the main CPU is involved with the data movement (as in this example protocol ), we refer to it as programmed I/O (PIO) . Third, the OS writes a command to the command register; doing so implicitly lets the device kn ow that both the data is present and that it should begin working on the com- mand. Finally, the OS waits for the device to ﬁnish by again poll ing it in a loop, waiting to see if it is ﬁnished (it may then get an error c ode to indicate success or failure).",3180
36.  IO Devices,"This basic protocol has the positive aspect of being simple and work - ing. However, there are some inefﬁciencies and inconveniences involved. The ﬁrst problem you might notice in the protocol is that polling seem s inefﬁcient; speciﬁcally, it wastes a great deal of CPU time ju st waiting for the (potentially slow) device to complete its activity, instea d of switching to another ready process and thus better utilizing the CPU. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 5 THECRUX: HOWTOAVOID THECOSTS OFPOLLING How can the OS check device status without frequent polling, and thus lower the CPU overhead required to manage the device? 36.4 Lowering CPU Overhead With Interrupts The invention that many engineers came upon years ago to improve this interaction is something we’ve seen already: the interrupt . Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task. When the de vice is ﬁnally ﬁnished with the operation, it will raise a hardware i nterrupt, causing the CPU to jump into the OS at a predetermined interrupt service routine (ISR) or more simply an interrupt handler . The handler is just a piece of operating system code that will ﬁnish the request (for ex ample, by reading data and perhaps an error code from the device) and wak e the process waiting for the I/O, which can then proceed as desired. Interrupts thus allow for overlap of computation and I/O, which is key for improved utilization. This timeline shows the problem: CPU Disk 1111111111ppppp11111 In the diagram, Process 1 runs on the CPU for some time (indicated b y a repeated 1on the CPU line), and then issues an I/O request to the disk to read some data. Without interrupts, the system simply spins , polling the status of the device repeatedly until the I/O is complete (i ndicated by ap). The disk services the request and ﬁnally Process 1 can run ag ain. If instead we utilize interrupts and allow for overlap, the OS ca n do something else while waiting for the disk: CPU Disk 11111111112222211111 In this example, the OS runs Process 2 on the CPU while the disk se r- vices Process 1’s request. When the disk request is ﬁnished, an interrupt occurs, and the OS wakes up Process 1 and runs it again. Thus, both the CPU and the disk are properly utilized during the middle stret ch of time. Note that using interrupts is not always the best solution. For example, imagine a device that performs its tasks very quickly: the ﬁrs t poll usually ﬁnds the device to be done with task. Using an interrupt in this case will actually slow down the system: switching to another process, handling the interrupt, and switching back to the issuing process is expen sive. Thus, if a device is fast, it may be best to poll; if it is slow, interrupts , which allow c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 I/O D EVICES TIP: INTERRUPTS NOTALWAYS BETTER THAN PIO Although interrupts allow for overlap of computation and I/O, they on ly really make sense for slow devices.",3066
36.  IO Devices,"Otherwise, the cost of interr upt han- dling and context switching may outweigh the beneﬁts interrup ts pro- vide. There are also cases where a ﬂood of interrupts may overload a sys- tem and lead it to livelock [MR96]; in such cases, polling provid es more control to the OS in its scheduling and thus is again useful. overlap, are best. If the speed of the device is not known, or sometim es fast and sometimes slow, it may be best to use a hybrid that polls for a little while and then, if the device is not yet ﬁnished, uses in terrupts. This two-phased approach may achieve the best of both worlds. Another reason not to use interrupts arises in networks [MR96]. W hen a huge stream of incoming packets each generate an interrupt, i t is pos- sible for the OS to livelock , that is, ﬁnd itself only processing interrupts and never allowing a user-level process to run and actually ser vice the re- quests. For example, imagine a web server that experiences a l oad burst because it became the top-ranked entry on hacker news [H18]. In this case, it is better to occasionally use polling to better control wh at is hap- pening in the system and allow the web server to service some req uests before going back to the device to check for more packet arrivals. Another interrupt-based optimization is coalescing . In such a setup, a device which needs to raise an interrupt ﬁrst waits for a bit be fore deliv- ering the interrupt to the CPU. While waiting, other requests may soon complete, and thus multiple interrupts can be coalesced into a single in- terrupt delivery, thus lowering the overhead of interrupt proce ssing. Of course, waiting too long will increase the latency of a request, a common trade-off in systems. See Ahmad et al. [A+11] for an excellent su mmary. 36.5 More Efﬁcient Data Movement With DMA Unfortunately, there is one other aspect of our canonical protocol tha t requires our attention. In particular, when using programmed I /O (PIO) to transfer a large chunk of data to a device, the CPU is once agai n over- burdened with a rather trivial task, and thus wastes a lot of tim e and effort that could better be spent running other processes. This t imeline illustrates the problem: CPU Disk 1111111111ccc2222211 In the timeline, Process 1 is running and then wishes to write s ome data to the disk. It then initiates the I/O, which must copy the data fr om memory to the device explicitly, one word at a time (marked cin the diagram). When the copy is complete, the I/O begins on the disk and the CPU ca n ﬁnally be used for something else. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 7 THECRUX: HOWTOLOWER PIO O VERHEADS With PIO, the CPU spends too much time moving data to and from devices by hand. How can we ofﬂoad this work and thus allow the CPU to be more effectively utilized? The solution to this problem is something we refer to as Direct Mem- ory Access (DMA) . A DMA engine is essentially a very speciﬁc device within a system that can orchestrate transfers between devic es and main memory without much CPU intervention. DMA works as follows. To transfer data to the device, for example, the OS would program the DMA engine by telling it where the data live s in memory, how much data to copy, and which device to send it to.",3286
36.  IO Devices,"At tha t point, the OS is done with the transfer and can proceed with other w ork. When the DMA is complete, the DMA controller raises an interrupt , and the OS thus knows the transfer is complete. The revised timelin e: CPU DMA Disk 11111111112222222211 ccc From the timeline, you can see that the copying of data is now handle d by the DMA controller. Because the CPU is free during that time, the OS can do something else, here choosing to run Process 2. Process 2 thu s gets to use more CPU before Process 1 runs again. 36.6 Methods Of Device Interaction Now that we have some sense of the efﬁciency issues involved with performing I/O, there are a few other problems we need to handle t o incorporate devices into modern systems. One problem you may have noticed thus far: we have not really said anything about how the OS ac- tually communicates with the device. Thus, the problem: THECRUX: HOWTOCOMMUNICATE WITHDEVICES How should the hardware communicate with a device? Should there be explicit instructions? Or are there other ways to do it? Over time, two primary methods of device communication have de- veloped. The ﬁrst, oldest method (used by IBM mainframes for many years) is to have explicit I/O instructions . These instructions specify a way for the OS to send data to speciﬁc device registers and thus allow the construction of the protocols described above. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 I/O D EVICES For example, on x86, the inandout instructions can be used to com- municate with devices. For example, to send data to a device, t he caller speciﬁes a register with the data in it, and a speciﬁc port which names the device. Executing the instruction leads to the desired behav ior. Such instructions are usually privileged . The OS controls devices, and the OS thus is the only entity allowed to directly communicate wi th them. Imagine if any program could read or write the disk, for example: t otal chaos (as always), as any user program could use such a loophole to ga in complete control over the machine. The second method to interact with devices is known as memory- mapped I/O . With this approach, the hardware makes device registers available as if they were memory locations. To access a particul ar register, the OS issues a load (to read) or store (to write) the address; the hardware then routes the load/store to the device instead of main memory. There is not some great advantage to one approach or the other. The memory-mapped approach is nice in that no new instructions are n eeded to support it, but both approaches are still in use today. 36.7 Fitting Into The OS: The Device Driver One ﬁnal problem we will discuss: how to ﬁt devices, each of which have very speciﬁc interfaces, into the OS, which we would like t o keep as general as possible. For example, consider a ﬁle system. We’d l ike to build a ﬁle system that worked on top of SCSI disks, IDE disks, USB keychain drives, and so forth, and we’d like the ﬁle system to be relatively oblivious to all of the details of how to issue a read or write request to these difference types of drives.",3121
36.  IO Devices,"Thus, our problem: THECRUX: HOWTOBUILD A D EVICE -NEUTRAL OS How can we keep most of the OS device-neutral, thus hiding the de- tails of device interactions from major OS subsystems? The problem is solved through the age-old technique of abstraction . At the lowest level, a piece of software in the OS must know in detai l how a device works. We call this piece of software a device driver , and any speciﬁcs of device interaction are encapsulated within. Let us see how this abstraction might help OS design and impleme n- tation by examining the Linux ﬁle system software stack. Figur e 36.4 is a rough and approximate depiction of the Linux software organizati on. As you can see from the diagram, a ﬁle system (and certainly, an a ppli- cation above) is completely oblivious to the speciﬁcs of which disk class it is using; it simply issues block read and write requests to t he generic block layer, which routes them to the appropriate device driver , which handles the details of issuing the speciﬁc request. Although s impliﬁed, the diagram shows how such detail can be hidden from most of the OS. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 9 Application File System Raw Generic Block Layer Device Driver [SCSI, ATA, etc.]POSIX API [open, read, write, close, etc.] Generic Block Interface [block read/write] Specific Block Interface [protocol-specific read/write] user kernel mode Figure 36.4: The File System Stack The diagram also shows a raw interface to devices, which enables spe- cial applications (such as a ﬁle-system checker , described later [AD14], or a disk defragmentation tool) to directly read and write blocks without using the ﬁle abstraction. Most systems provide this type of inte rface to support these low-level storage management applications. Note that the encapsulation seen above can have its downside as we ll. For example, if there is a device that has many special capabil ities, but has to present a generic interface to the rest of the kernel, th ose special capabilities will go unused. This situation arises, for examp le, in Linux with SCSI devices, which have very rich error reporting; because other block devices (e.g., ATA/IDE) have much simpler error handlin g, all that higher levels of software ever receive is a generic EIO (generic IO error) error code; any extra detail that SCSI may have provided is thus lost to the ﬁle system [G08]. Interestingly, because device drivers are needed for any dev ice you might plug into your system, over time they have come to represen t a huge percentage of kernel code. Studies of the Linux kernel revea l that over 70 percent of OS code is found in device drivers [C01]; for Windows-bas ed systems, it is likely quite high as well. Thus, when people tel l you that the OS has millions of lines of code, what they are really saying is tha t the OS has millions of lines of device-driver code. Of course, for any give n in- stallation, most of that code may not be active (i.e., only a few devi ces are connected to the system at a time).",3044
36.  IO Devices,"Perhaps more depressingly, as drivers are often written by “amateurs” (instead of full-time kernel d evelopers), they tend to have many more bugs and thus are a primary contribut or to kernel crashes [S03]. 36.8 Case Study: A Simple IDE Disk Driver To dig a little deeper here, let’s take a quick look at an actual de vice: an IDE disk drive [L94]. We summarize the protocol as described in t his ref- erence [W10]; we’ll also peek at the xv6 source code for a simple ex ample of a working IDE driver [CK+08]. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 I/O D EVICES Control Register: Address 0x3F6 = 0x08 (0000 1RE0): R=reset, E=0 means \""enable interrupt\"" Command Block Registers: Address 0x1F0 = Data Port Address 0x1F1 = Error Address 0x1F2 = Sector Count Address 0x1F3 = LBA low byte Address 0x1F4 = LBA mid byte Address 0x1F5 = LBA hi byte Address 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive Address 0x1F7 = Command/status Status Register (Address 0x1F7): 7 6 5 4 3 2 1 0 BUSY READY FAULT SEEK DRQ CORR IDDEX ERROR Error Register (Address 0x1F1): (check when Status ERROR== 1) 7 6 5 4 3 2 1 0 BBK UNC MC IDNF MCR ABRT T0NF AMNF BBK = Bad Block UNC = Uncorrectable data error MC = Media Changed IDNF = ID mark Not Found MCR = Media Change Requested ABRT = Command aborted T0NF = Track 0 Not Found AMNF = Address Mark Not Found Figure 36.5: The IDE Interface An IDE disk presents a simple interface to the system, consist ing of four types of register: control, command block, status, and error. T hese registers are available by reading or writing to speciﬁc “I/O addresses” (such as0x3F6 below) using (on x86) the inandout I/O instructions. The basic protocol to interact with the device is as follows, assum ing it has already been initialized. •Wait for drive to be ready. Read Status Register (0x1F7) until drive is READY and not BUSY. •Write parameters to command registers. Write the sector count, logical block address (LBA) of the sectors to be accessed, and dri ve number (master=0x00 or slave=0x10, as IDE permits just two dr ives) to command registers (0x1F2-0x1F6). •Start the I/O. by issuing read/write to command register. Write READ—WRITE command to command register (0x1F7). •Data transfer (for writes): Wait until drive status is READY and DRQ (drive request for data); write data to data port. •Handle interrupts. In the simplest case, handle an interrupt for each sector transferred; more complex approaches allow batching and thus one ﬁnal interrupt when the entire transfer is complet e. •Error handling. After each operation, read the status register. If the ERROR bit is on, read the error register for details. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 11 static int ide_wait_ready() { while (((int r = inb(0x1f7)) & IDE_BSY) || .(r & IDE_DRDY)) ; // loop until drive isn’t busy } static void ide_start_request(struct buf *b) { ide_wait_ready(); outb(0x3f6, 0); // generate interrupt outb(0x1f2, 1); // how many sectors? outb(0x1f3, b->sector & 0xff); // LBA goes here ... outb(0x1f4, (b->sector >> 8) & 0xff); // ... and here outb(0x1f5, (b->sector >> 16) & 0xff); // ...",3131
36.  IO Devices,"and here. outb(0x1f6, 0xe0 | ((b->dev&1)<<4) | ((b->sector>>24)&0x 0f)); if(b->flags & B_DIRTY){ outb(0x1f7, IDE_CMD_WRITE); // this is a WRITE outsl(0x1f0, b->data, 512/4); // transfer data too. } else { outb(0x1f7, IDE_CMD_READ); // this is a READ (no data) } } void ide_rw(struct buf *b) { acquire(&ide_lock); for (struct buf **pp = &ide_queue; *pp; pp=&( *pp)->qnext) ; // walk queue *pp = b; // add request to end if (ide_queue == b) // if q is empty ide_start_request(b); // send req to disk while ((b->flags & (B_VALID|B_DIRTY)) .= B_VALID) sleep(b, &ide_lock); // wait for completion release(&ide_lock); } void ide_intr() { struct buf *b; acquire(&ide_lock); if (.(b->flags & B_DIRTY) && ide_wait_ready() >= 0) insl(0x1f0, b->data, 512/4); // if READ: get data b->flags |= B_VALID; b->flags &= ˜B_DIRTY; wakeup(b); // wake waiting process if ((ide_queue = b->qnext) .= 0) // start next request ide_start_request(ide_queue); // (if one exists) release(&ide_lock); } Figure 36.6: The xv6 IDE Disk Driver (Simpliﬁed) Most of this protocol is found in the xv6 IDE driver (Figure 36.6), which (after initialization) works through four primary functi ons. The ﬁrst isiderw() , which queues a request (if there are others pending), or issues it directly to the disk (via idestartrequest() ); in either case, the routine waits for the request to complete and the calli ng pro- cess is put to sleep. The second is idestartrequest() , which is used to send a request (and perhaps data, in the case of a write) to the disk; theinandout x86 instructions are called to read and write device c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 I/O D EVICES registers, respectively. The start request routine uses the third function, idewaitready() , to ensure the drive is ready before issuing a request to it. Finally, ideintr() is invoked when an interrupt takes place; it reads data from the device (if the request is a read, not a write) , wakes the process waiting for the I/O to complete, and (if there are more req uests in the I/O queue), launches the next I/O via idestartrequest() . 36.9 Historical Notes Before ending, we include a brief historical note on the origin of som e of these fundamental ideas. If you are interested in learning m ore, read Smotherman’s excellent summary [S08]. Interrupts are an ancient idea, existing on the earliest of mac hines. For example, the UNIVAC in the early 1950’s had some form of interrupt vec- toring, although it is unclear in exactly which year this featu re was avail- able [S08]. Sadly, even in its infancy, we are beginning to lose t he origins of computing history. There is also some debate as to which machine ﬁrst introduced th e idea of DMA. For example, Knuth and others point to the DYSEAC (a “mo- bile” machine, which at the time meant it could be hauled in a tr ailer), whereas others think the IBM SAGE may have been the ﬁrst [S08]. Ei - ther way, by the mid 50’s, systems with I/O devices that communi cated directly with memory and interrupted the CPU when ﬁnished exi sted.",3046
36.  IO Devices,"The history here is difﬁcult to trace because the inventions ar e tied to real, and sometimes obscure, machines. For example, some think t hat the Lincoln Labs TX-2 machine was ﬁrst with vectored interrupts [S0 8], but this is hardly clear. Because the ideas are relatively obvious — no Einsteinian leap is re- quired to come up with the idea of letting the CPU do something els e while a slow I/O is pending — perhaps our focus on “who ﬁrst?” is mis - guided. What is certainly clear: as people built these early m achines, it became obvious that I/O support was needed. Interrupts, DMA, an d re- lated ideas are all direct outcomes of the nature of fast CPUs and s low devices; if you were there at the time, you might have had simila r ideas. 36.10 Summary You should now have a very basic understanding of how an OS inter- acts with a device. Two techniques, the interrupt and DMA, ha ve been introduced to help with device efﬁciency, and two approaches t o access- ing device registers, explicit I/O instructions and memory-m apped I/O, have been described. Finally, the notion of a device driver has b een pre- sented, showing how the OS itself can encapsulate low-level det ails and thus make it easier to build the rest of the OS in a device-neutr al fashion. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG I/O D EVICES 13 References [A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device I O” by Irfan Ahmad, Ajay Gulati, Ali Mashtizadeh. USENIX ’11. A terriﬁc survey of interrupt coalescing in traditional and virtualized environments. [AD14] “Operating Systems: Three Easy Pieces” (Chapters: Crash Consis tency: FSCK and Journaling and Log-Structured File Systems) by Remzi Arpaci-Dussea u and Andrea Arpaci- Dusseau. Arpaci-Dusseau Books, 2014. A description of a ﬁle-system checker and how it works, which requires low-level access to disk devices not normally provide d by the ﬁle system directly. [C01] “An Empirical Study of Operating System Errors” by Andy Chou, Junfeng Yang, Ben- jamin Chelf, Seth Hallem, Dawson Engler. SOSP ’01. One of the ﬁrst papers to systematically explore how many bugs are in modern operating systems. Among other neat ﬁndi ngs, the authors show that device drivers have something like seven times more bugs than mainli ne kernel code. [CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai Zeldovich. From: http://pdos.csail.mit.edu/6.828/2008/inde x.html. Seeide.c for the IDE device driver, with a few more details therein. [D07] “What Every Programmer Should Know About Memory” by Ulrich Dre pper. Novem- ber, 2007. Available: http://www.akkadia.org/drepper/cpumemory.pdf .A fantastic read about modern memory systems, starting at DRAM and going all the way up to vir tualization and cache-optimized algorithms. [G08] “EIO: Error-handling is Occasionally Correct” by Haryadi Gunawi, C indy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, Ben Liblit. FAST ’ 08, San Jose, CA, February 2008. Our own work on building a tool to ﬁnd code in Linux ﬁle systems that does not han dle error return properly.",3110
36.  IO Devices,"We found hundreds and hundreds of bugs, many of whic h have now been ﬁxed. [H17] “Intel Core i7-7700K review: Kaby Lake Debuts for Desktop” by Joel Hruska. January 3, 2017.www.extremetech.com/extreme/241950-intels-core-i7-7 700k-reviewed-kaby -lake-debuts-desktop .An in-depth review of a recent Intel chipset, including CPUs and the I/O subsystem. [H18] “Hacker News” by Many contributors. Available: https://news. ycombinator.com. One of the better aggregrators for tech-related stuff. Once back in 2014, this book be came a highly-ranked entry, leading to 1 million chapter downloads in just one day. Sadly, we have yet to re-experience such a high. [L94] “AT Attachment Interface for Disk Drives” by Lawrence J. Lamers. Re ference number: ANSI X3.221, 1994. Available: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf . A rather dry document about device interfaces. Read it at your own peril. [MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel” by Jeffrey Mogul, K. K. Ramakrishnan. USENIX ’96, San Diego, CA, January 1996. Mogul and colleagues did a great deal of pioneering work on web server network performance. This paper is but one example. [S08] “Interrupts” by Mark Smotherman. July ’08. Available: http://people.cs.clemson.edu/ ˜mark/interrupts.html .A treasure trove of information on the history of interrupts, DMA, and related early ideas in computing. [S03] “Improving the Reliability of Commodity Operating Systems ” by Michael M. Swift, Brian N. Bershad, Henry M. Levy. SOSP ’03. Swift’s work revived interest in a more microkernel-like approach to operating systems; minimally, it ﬁnally gave some good reasons why address-space based protection could be useful in a modern OS. [W10] “Hard Disk Driver” by Washington State Course Homepage. A vailable online at this site:http://eecs.wsu.edu/˜cs460/cs560/HDdriver.html .A nice summary of a simple IDE disk drive’s interface and how to build a device driver for it. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",2007
37. Hard Disk Drives,"37 Hard Disk Drives The last chapter introduced the general concept of an I/O device and showed you how the OS might interact with such a beast. In this cha pter, we dive into more detail about one device in particular: the hard disk drive . These drives have been the main form of persistent data storage in computer systems for decades and much of the development of ﬁle sys - tem technology (coming soon) is predicated on their behavior. Thus, it is worth understanding the details of a disk’s operation before bui lding the ﬁle system software that manages it. Many of these details a re avail- able in excellent papers by Ruemmler and Wilkes [RW92] and An derson, Dykes, and Riedel [ADR03]. CRUX: HOWTOSTORE ANDACCESS DATA ONDISK How do modern hard-disk drives store data? What is the interface ? How is the data actually laid out and accessed? How does disk sched ul- ing improve performance? 37.1 The Interface Let’s start by understanding the interface to a modern disk dri ve. The basic interface for all modern drives is straightforward. The d rive consists of a large number of sectors (512-byte blocks), each of which can be read or written. The sectors are numbered from 0ton−1on a disk with n sectors. Thus, we can view the disk as an array of sectors; 0ton−1is thus the address space of the drive. Multi-sector operations are possible; indeed, many ﬁle systems will read or write 4KB at a time (or more). However, when updating the di sk, the only guarantee drive manufacturers make is that a single 5 12-byte write is atomic (i.e., it will either complete in its entirety or it won’t com- plete at all); thus, if an untimely power loss occurs, only a portion of a larger write may complete (sometimes called a torn write ). 1 2 HARD DISKDRIVES 0111098 7 6 5 4321Spindle Figure 37.1: A Disk With Just A Single Track There are some assumptions most clients of disk drives make, but that are not speciﬁed directly in the interface; Schlosser and G anger have called this the “unwritten contract” of disk drives [SG04]. Spec iﬁcally, one can usually assume that accessing two blocks1near one-another within the drive’s address space will be faster than accessing two bl ocks that are far apart. One can also usually assume that accessing blocks i n a contigu- ous chunk (i.e., a sequential read or write) is the fastest acce ss mode, and usually much faster than any more random access pattern. 37.2 Basic Geometry Let’s start to understand some of the components of a modern disk. We start with a platter , a circular hard surface on which data is stored persistently by inducing magnetic changes to it. A disk may h ave one or more platters; each platter has 2 sides, each of which is calle d asur- face. These platters are usually made of some hard material (such as aluminum), and then coated with a thin magnetic layer that ena bles the drive to persistently store bits even when the drive is powered off. The platters are all bound together around the spindle , which is con- nected to a motor that spins the platters around (while the drive is pow- ered on) at a constant (ﬁxed) rate.",3103
37. Hard Disk Drives,"The rate of rotation is often meas ured in rotations per minute (RPM) , and typical modern values are in the 7,200 RPM to 15,000 RPM range. Note that we will often be interested in the time of a single rotation, e.g., a drive that rotates at 10,000 RPM means that a single rotation takes about 6 milliseconds (6 ms). Data is encoded on each surface in concentric circles of sectors; w e call one such concentric circle a track . A single surface contains many thou- sands and thousands of tracks, tightly packed together, with hu ndreds of tracks ﬁtting into the width of a human hair. To read and write from the surface, we need a mechanism that all ows us to either sense (i.e., read) the magnetic patterns on the di sk or to in- duce a change in (i.e., write) them. This process of reading and writing is accomplished by the disk head ; there is one such head per surface of the drive. The disk head is attached to a single disk arm , which moves across the surface to position the head over the desired track. 1We, and others, often use the terms block and sector interchangeably, assuming the reader will know exactly what is meant per context. Sorry about this. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 3 Head Arm0111098 7 6 5 4321SpindleRotates this way Figure 37.2: A Single Track Plus A Head 37.3 A Simple Disk Drive Let’s understand how disks work by building up a model one track at a time. Assume we have a simple disk with a single track (Figur e 37.1). This track has just 12 sectors, each of which is 512 bytes in size (our typical sector size, recall) and addressed therefore by the nu mbers 0 through 11. The single platter we have here rotates around the spindle, to which a motor is attached. Of course, the track by itself isn’t too intere sting; we want to be able to read or write those sectors, and thus we need a di sk head, attached to a disk arm, as we now see (Figure 37.2). In the ﬁgure, the disk head, attached to the end of the arm, is pos i- tioned over sector 6, and the surface is rotating counter-clockwis e. Single-track Latency: The Rotational Delay To understand how a request would be processed on our simple, one- track disk, imagine we now receive a request to read block 0. How s hould the disk service this request? In our simple disk, the disk doesn’t have to do much. In particula r, it must just wait for the desired sector to rotate under the disk hea d. This wait happens often enough in modern drives, and is an important en ough component of I/O service time, that it has a special name: rotational de- lay(sometimes rotation delay , though that sounds weird). In the exam- ple, if the full rotational delay is R, the disk has to incur a rotational delay of aboutR 2to wait for 0 to come under the read/write head (if we start at 6). A worst-case request on this single track would be to sector 5, causing nearly a full rotational delay in order to service such a request . Multiple Tracks: Seek Time So far our disk just has a single track, which is not too realistic; modern disks of course have many millions. Let’s thus look at ever-so-sligh tly more realistic disk surface, this one with three tracks (Figur e 37.3, left). In the ﬁgure, the head is currently positioned over the innermost track (which contains sectors 24 through 35); the next track over contai ns the next set of sectors (12 through 23), and the outermost track contain s the ﬁrst sectors (0 through 11). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 HARD DISKDRIVES 011109 8 7 6 5 4 32112232221 20 19 18 17 16 1514132435343332 31 30 29 28272625SpindleRotates this way SeekRemaining rotation 3210 11 10 9 8 7 65415141312 23 22 21 20 19 1817162726252435 34 33 32 31302928SpindleRotates this way Figure 37.3: Three Tracks Plus A Head (Right: With Seek) To understand how the drive might access a given sector, we now tr ace what would happen on a request to a distant sector, e.g., a read to sector 11. To service this read, the drive has to ﬁrst move the disk arm to the cor- rect track (in this case, the outermost one), in a process known as a seek . Seeks, along with rotations, are one of the most costly disk operations. The seek, it should be noted, has many phases: ﬁrst an acceleration phase as the disk arm gets moving; then coasting as the arm is moving at full speed, then deceleration as the arm slows down; ﬁnally settling as the head is carefully positioned over the correct track. The settling time is often quite signiﬁcant, e.g., 0.5 to 2 ms, as the drive must b e certain to ﬁnd the right track (imagine if it just got close instead.).",4611
37. Hard Disk Drives,"After the seek, the disk arm has positioned the head over the righ t track. A depiction of the seek is found in Figure 37.3 (right). As we can see, during the seek, the arm has been moved to the desi red track, and the platter of course has rotated, in this case about 3 s ectors. Thus, sector 9 is just about to pass under the disk head, and we mu st only endure a short rotational delay to complete the transfer. When sector 11 passes under the disk head, the ﬁnal phase of I/O will take place, known as the transfer , where data is either read from or written to the surface. And thus, we have a complete picture of I /O time: ﬁrst a seek, then waiting for the rotational delay, and ﬁnally th e transfer. Some Other Details Though we won’t spend too much time on it, there are some other inter- esting details about how hard drives operate. Many drives employ some kind of track skew to make sure that sequential reads can be properly serviced even when crossing track boundaries. In our simple exa mple disk, this might appear as seen in Figure 37.4. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 5 Track skew: 2 blocks011109 8 7 6 5 4 32122212019 18 17 16 15 14 1312233231302928 27 26 25 24353433SpindleRotates this way Figure 37.4: Three Tracks: Track Skew Of 2 Sectors are often skewed like this because when switching from one track to another, the disk needs time to reposition the head (eve n to neigh- boring tracks). Without such skew, the head would be moved to the n ext track but the desired next block would have already rotated unde r the head, and thus the drive would have to wait almost the entire rota tional delay to access the next block. Another reality is that outer tracks tend to have more sectors tha n inner tracks, which is a result of geometry; there is simply more room out there. These tracks are often referred to as multi-zoned disk drives, where the disk is organized into multiple zones, and where a zone is con- secutive set of tracks on a surface. Each zone has the same number of sectors per track, and outer zones have more sectors than inner zone s. Finally, an important part of any modern disk drive is its cache , for historical reasons sometimes called a track buffer . This cache is just some small amount of memory (usually around 8 or 16 MB) which the drive can use to hold data read from or written to the disk. For example, w hen reading a sector from the disk, the drive might decide to read in all of the sectors on that track and cache them in its memory; doing so allows t he drive to quickly respond to any subsequent requests to the sam e track. On writes, the drive has a choice: should it acknowledge the writ e has completed when it has put the data in its memory, or after the writ e has actually been written to disk? The former is called write back caching (or sometimes immediate reporting ), and the latter write through . Write back caching sometimes makes the drive appear “faster”, but c an be dan- gerous; if the ﬁle system or applications require that data be wr itten to disk in a certain order for correctness, write-back caching can lead to problems (read the chapter on ﬁle-system journaling for details ).",3199
37. Hard Disk Drives,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 HARD DISKDRIVES ASIDE : DIMENSIONAL ANALYSIS Remember in Chemistry class, how you solved virtually every prob - lem by simply setting up the units such that they canceled out, and some- how the answers popped out as a result? That chemical magic is know n by the highfalutin name of dimensional analysis and it turns out it is useful in computer systems analysis too. Let’s do an example to see how dimensional analysis works and why it is useful. In this case, assume you have to ﬁgure out how long, in mil- liseconds, a single rotation of a disk takes. Unfortunately, you ar e given only the RPM of the disk, or rotations per minute . Let’s assume we’re talking about a 10K RPM disk (i.e., it rotates 10,000 times per m inute). How do we set up the dimensional analysis so that we get time per r ota- tion in milliseconds? To do so, we start by putting the desired units on the left; in thi s case, we wish to obtain the time (in milliseconds) per rotation, so that is ex- actly what we write down:Time(ms) 1Rotation. We then write down everything we know, making sure to cancel units where possible. First, we ob tain 1minute 10,000Rotations(keeping rotation on the bottom, as that’s where it is on the left), then transform minutes into seconds with60seconds 1minute, and then ﬁnally transform seconds in milliseconds with1000ms 1second. The ﬁnal result is the following (with units nicely canceled): Time(ms) 1Rot.=1✘✘✘minute 10,000Rot.·60✘✘✘seconds 1✘✘✘minute·1000ms 1✘✘✘second=60,000ms 10,000Rot.=6ms Rotation As you can see from this example, dimensional analysis makes wha t seems intuitive into a simple and repeatable process. Beyond t he RPM calculation above, it comes in handy with I/O analysis regul arly. For example, you will often be given the transfer rate of a disk, e. g., 100 MB/second, and then asked: how long does it take to transfer a 512 KB block (in milliseconds)? With dimensional analysis, it’s easy: Time(ms) 1Request=512✟✟KB 1Request·1✟✟MB 1024✟✟KB·1✘✘✘second 100✟✟MB·1000ms 1✘✘✘second=5ms Request 37.4 I/O Time: Doing The Math Now that we have an abstract model of the disk, we can use a little analysis to better understand disk performance. In particul ar, we can now represent I/O time as the sum of three major components: TI/O=Tseek+Trotation +Ttransfer (37.1) OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 7 Cheetah 15K.5 Barracuda Capacity 300 GB 1 TB RPM 15,000 7,200 Average Seek 4 ms 9 ms Max Transfer 125 MB/s 105 MB/s Platters 4 4 Cache 16 MB 16/32 MB Connects via SCSI SATA Figure 37.5: Disk Drive Specs: SCSI Versus SATA Note that the rate of I/O ( RI/O), which is often more easily used for comparison between drives (as we will do below), is easily comput ed from the time. Simply divide the size of the transfer by the time i t took: RI/O=SizeTransfer TI/O(37.2) To get a better feel for I/O time, let us perform the following calc u- lation. Assume there are two workloads we are interested in.",3023
37. Hard Disk Drives,"The ﬁrst, known as the random workload, issues small (e.g., 4KB) reads to random locations on the disk. Random workloads are common in many impor- tant applications, including database management systems. The second, known as the sequential workload, simply reads a large number of sec- tors consecutively from the disk, without jumping around. Sequent ial access patterns are quite common and thus important as well. To understand the difference in performance between random an d se- quential workloads, we need to make a few assumptions about the di sk drive ﬁrst. Let’s look at a couple of modern disks from Seagate. The ﬁrs t, known as the Cheetah 15K.5 [S09b], is a high-performance SCSI driv e. The second, the Barracuda [S09a], is a drive built for capacity. Details on both are found in Figure 37.5. As you can see, the drives have quite different characteristi cs, and in many ways nicely summarize two important components of the dis k drive market. The ﬁrst is the “high performance” drive market , where drives are engineered to spin as fast as possible, deliver low s eek times, and transfer data quickly. The second is the “capacity” marke t, where cost per byte is the most important aspect; thus, the drives are s lower but pack as many bits as possible into the space available. From these numbers, we can start to calculate how well the drive s would do under our two workloads outlined above. Let’s start by looking at the random workload. Assuming each 4 KB read occurs at a random location on disk, we can calculate how long each such read would take . On the Cheetah: Tseek= 4ms, T rotation = 2ms, T transfer = 30microsecs (37.3) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 HARD DISKDRIVES TIP: USEDISKS SEQUENTIALLY When at all possible, transfer data to and from disks in a sequen tial man- ner. If sequential is not possible, at least think about transfe rring data in large chunks: the bigger, the better. If I/O is done in littl e random pieces, I/O performance will suffer dramatically. Also, user s will suffer. Also, you will suffer, knowing what suffering you have wrought wit h your careless random I/Os. The average seek time (4 milliseconds) is just taken as the ave rage time reported by the manufacturer; note that a full seek (from one end of the surface to the other) would likely take two or three times longer. The average rotational delay is calculated from the RPM directly. 1 5000 RPM is equal to 250 RPS (rotations per second); thus, each rotation tak es 4 ms. On average, the disk will encounter a half rotation and thus 2 ms i s the average time. Finally, the transfer time is just the size of th e transfer over the peak transfer rate; here it is vanishingly small (30 microseconds ; note that we need 1000 microseconds just to get 1 millisecond.). Thus, from our equation above, TI/Ofor the Cheetah roughly equals 6 ms. To compute the rate of I/O, we just divide the size of the tran sfer by the average time, and thus arrive at RI/Ofor the Cheetah under the random workload of about 0.66MB/s. The same calculation for the Bar- racuda yields a TI/Oof about 13.2 ms, more than twice as slow, and thus a rate of about 0.31MB/s.",3189
37. Hard Disk Drives,"Now let’s look at the sequential workload. Here we can assume there is a single seek and rotation before a very long transfer. For simpl icity, assume the size of the transfer is 100 MB. Thus, TI/Ofor the Cheetah and Barracuda is about 800 ms and 950 ms, respectively. The rates of I/O are thus very nearly the peak transfer rates of 125 MB/s and 105 MB/s, respectively. Figure 37.6 summarizes these numbers. The ﬁgure shows us a number of important things. First, and most importantly, there is a huge gap in drive performance between r andom and sequential workloads, almost a factor of 200 or so for the Cheetah and more than a factor 300 difference for the Barracuda. And thus we arrive at the most obvious design tip in the history of computing. A second, more subtle point: there is a large difference in perfor mance between high-end “performance” drives and low-end “capacity ” drives. For this reason (and others), people are often willing to pay top doll ar for the former while trying to get the latter as cheaply as possible . Cheetah Barracuda RI/O Random 0.66 MB/s 0.31 MB/s RI/O Sequential 125 MB/s 105 MB/s Figure 37.6: Disk Drive Performance: SCSI Versus SATA OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 9 ASIDE : COMPUTING THE“AVERAGE ” SEEK In many books and papers, you will see average disk-seek time cit ed as being roughly one-third of the full seek time. Where does this c ome from? Turns out it arises from a simple calculation based on average see k distance , not time. Imagine the disk as a set of tracks, from 0toN. The seek distance between any two tracks xandyis thus computed as the absolute value of the difference between them: |x−y|. To compute the average seek distance, all you need to do is to ﬁrs t add up all possible seek distances: N/summationdisplay x=0N/summationdisplay y=0|x−y|. (37.4) Then, divide this by the number of different possible seeks: N2. To compute the sum, we’ll just use the integral form: /integraldisplayN x=0/integraldisplayN y=0|x−y|dydx. (37.5) To compute the inner integral, let’s break out the absolute value : /integraldisplayx y=0(x−y)dy+/integraldisplayN y=x(y−x)dy. (37.6) Solving this leads to (xy−1 2y2)/vextendsingle/vextendsinglex 0+ (1 2y2−xy)/vextendsingle/vextendsingleN xwhich can be sim- pliﬁed to (x2−Nx+1 2N2).Now we have to compute the outer integral: /integraldisplayN x=0(x2−Nx+1 2N2)dx, (37.7) which results in: (1 3x3−N 2x2+N2 2x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleN 0=N3 3. (37.8) Remember that we still have to divide by the total number of seek s (N2) to compute the average seek distance: (N3 3)/(N2) =1 3N. Thus the average seek distance on a disk, over all possible seeks, is one-t hird the full distance. And now when you hear that an average seek is one-t hird of a full seek, you’ll know where it came from. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 HARD DISKDRIVES 011109 8 7 6 5 4 32112232221 20 19 18 17 16 1514132435343332 31 30 29 28272625SpindleRotates this way Figure 37.7: SSTF: Scheduling Requests 21 And 2 37.5 Disk Scheduling Because of the high cost of I/O, the OS has historically played a rol e in deciding the order of I/Os issued to the disk. More speciﬁcally, given a set of I/O requests, the disk scheduler examines the requests and decides which one to schedule next [SCO90, JW91]. Unlike job scheduling, where the length of each job is usually un - known, with disk scheduling, we can make a good guess at how long a “job” (i.e., disk request) will take. By estimating the seek and possi- ble rotational delay of a request, the disk scheduler can know how l ong each request will take, and thus (greedily) pick the one that w ill take the least time to service ﬁrst. Thus, the disk scheduler will try to follow the principle of SJF (shortest job ﬁrst) in its operation. SSTF: Shortest Seek Time First One early disk scheduling approach is known as shortest-seek-time-ﬁrst (SSTF ) (also called shortest-seek-ﬁrst orSSF). SSTF orders the queue of I/O requests by track, picking requests on the nearest track t o complete ﬁrst. For example, assuming the current position of the head is ove r the inner track, and we have requests for sectors 21 (middle track) and 2 (outer track), we would then issue the request to 21 ﬁrst, wait f or it to complete, and then issue the request to 2 (Figure 37.7). SSTF works well in this example, seeking to the middle track ﬁrst and then the outer track.",4463
37. Hard Disk Drives,"However, SSTF is not a panacea, for the following reasons. First, the drive geometry is not available to the host OS; rather, it sees an array of blocks. Fortunately, this problem is rather ea sily ﬁxed. Instead of SSTF, an OS can simply implement nearest-block-ﬁrst (NBF ), which schedules the request with the nearest block address ne xt. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 11 The second problem is more fundamental: starvation . Imagine in our example above if there were a steady stream of requests to the in- ner track, where the head currently is positioned. Requests to any other tracks would then be ignored completely by a pure SSTF approach. And thus the crux of the problem: CRUX: HOWTOHANDLE DISKSTARVATION How can we implement SSTF-like scheduling but avoid starvation? Elevator (a.k.a. SCAN or C-SCAN) The answer to this query was developed some time ago (see [CKR72 ] for example), and is relatively straightforward. The algorith m, originally called SCAN , simply moves back and forth across the disk servicing re- quests in order across the tracks. Let’s call a single pass across the disk (from outer to inner tracks, or inner to outer) a sweep . Thus, if a request comes for a block on a track that has already been serviced on this sw eep of the disk, it is not handled immediately, but rather queued un til the next sweep (in the other direction). SCAN has a number of variants, all of which do about the same thing. For example, Coffman et al. introduced F-SCAN , which freezes the queue to be serviced when it is doing a sweep [CKR72]; this action plac es re- quests that come in during the sweep into a queue to be serviced later. Doing so avoids starvation of far-away requests, by delaying the servic- ing of late-arriving (but nearer by) requests. C-SCAN is another common variant, short for Circular SCAN . In- stead of sweeping in both directions across the disk, the algorith m only sweeps from outer-to-inner, and then resets at the outer track to begin again. Doing so is a bit more fair to inner and outer tracks, as pur e back- and-forth SCAN favors the middle tracks, i.e., after servicing the outer track, SCAN passes through the middle twice before coming back to the outer track again. For reasons that should now be clear, the SCAN algorithm (and its cousins) is sometimes referred to as the elevator algorithm, because it behaves like an elevator which is either going up or down and not jus t servicing requests to ﬂoors based on which ﬂoor is closer. Imagine h ow annoying it would be if you were going down from ﬂoor 10 to 1, and somebody got on at 3 and pressed 4, and the elevator went up to 4 be- cause it was “closer” than 1. As you can see, the elevator algorith m, when used in real life, prevents ﬁghts from taking place on elevators . In disks, it just prevents starvation. Unfortunately, SCAN and its cousins do not represent the best sch edul- ing technology. In particular, SCAN (or SSTF even) do not actually ad here as closely to the principle of SJF as they could.",3051
37. Hard Disk Drives,"In particular, th ey ignore rotation. And thus, another crux: c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 HARD DISKDRIVES CRUX: HOWTOACCOUNT FORDISKROTATION COSTS How can we implement an algorithm that more closely approximates SJ F by taking both seek and rotation into account? 011109 8 7 6 5 4 32112232221 20 19 18 17 16 1514132435343332 31 30 29 28272625SpindleRotates this way Figure 37.8: SSTF: Sometimes Not Good Enough SPTF: Shortest Positioning Time First Before discussing shortest positioning time ﬁrst orSPTF scheduling (some- times also called shortest access time ﬁrst orSATF ), which is the solution to our problem, let us make sure we understand the problem in more d e- tail. Figure 37.8 presents an example. In the example, the head is currently positioned over sector 30 on t he inner track. The scheduler thus has to decide: should it sched ule sector 16 (on the middle track) or sector 8 (on the outer track) for its next req uest. So which should it service next? The answer, of course, is “it depends”. In engineering, it turn s out “it depends” is almost always the answer, reﬂecting that trad e-offs are part of the life of the engineer; such maxims are also good in a pinc h, e.g., when you don’t know an answer to your boss’s question, you might want to try this gem. However, it is almost always better to know why it depends, which is what we discuss here. What it depends on here is the relative time of seeking as compare d to rotation. If, in our example, seek time is much higher than rota tional delay, then SSTF (and variants) are just ﬁne. However, imagine i f seek is quite a bit faster than rotation. Then, in our example, it would ma ke more sense to seek further to service request 8 on the outer track than it would to perform the shorter seek to the middle track to service 16, wh ich has to rotate all the way around before passing under the disk head. On modern drives, as we saw above, both seek and rotation are roughly OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 13 TIP: ITALWAYS DEPENDS (LIVNY ’SLAW) Almost any question can be answered with “it depends”, as our coll eague Miron Livny always says. However, use with caution, as if you answ er too many questions this way, people will stop asking you questions a lto- gether. For example, somebody asks: “want to go to lunch?” You rep ly: “it depends, are youcoming along?” equivalent (depending, of course, on the exact requests), and t hus SPTF is useful and improves performance. However, it is even more difﬁ cult to implement in an OS, which generally does not have a good idea wher e track boundaries are or where the disk head currently is (in a rot ational sense). Thus, SPTF is usually performed inside a drive, descri bed below. Other Scheduling Issues There are many other issues we do not discuss in this brief descr iption of basic disk operation, scheduling, and related topics. One suc h is- sue is this: where is disk scheduling performed on modern systems? In older systems, the operating system did all the scheduling; af ter looking through the set of pending requests, the OS would pick the best one , and issue it to the disk.",3182
37. Hard Disk Drives,"When that request completed, the next one w ould be chosen, and so forth. Disks were simpler then, and so was life. In modern systems, disks can accommodate multiple outstanding r e- quests, and have sophisticated internal schedulers themsel ves (which can implement SPTF accurately; inside the disk controller, all rel evant details are available, including exact head position). Thus, the OS sc heduler usu- ally picks what it thinks the best few requests are (say 16) an d issues them all to disk; the disk then uses its internal knowledge of head pos ition and detailed track layout information to service said requests in t he best pos- sible (SPTF) order. Another important related task performed by disk schedulers is I/O merging . For example, imagine a series of requests to read blocks 33, then 8, then 34, as in Figure 37.8. In this case, the scheduler should merge the requests for blocks 33 and 34 into a single two-block request; any re- ordering that the scheduler does is performed upon the merged req uests. Merging is particularly important at the OS level, as it reduc es the num- ber of requests sent to the disk and thus lowers overheads. One ﬁnal problem that modern schedulers address is this: how long should the system wait before issuing an I/O to disk? One might n aively think that the disk, once it has even a single I/O, should immedi ately issue the request to the drive; this approach is called work-conserving , as the disk will never be idle if there are requests to serve. Howe ver, research onanticipatory disk scheduling has shown that sometimes it is better to wait for a bit [ID01], in what is called a non-work-conserving approach. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 HARD DISKDRIVES By waiting, a new and “better” request may arrive at the disk, and thus overall efﬁciency is increased. Of course, deciding when to wa it, and for how long, can be tricky; see the research paper for details, or che ck out the Linux kernel implementation to see how such ideas are trans itioned into practice (if you are the ambitious sort). 37.6 Summary We have presented a summary of how disks work. The summary is actually a detailed functional model; it does not describe the am azing physics, electronics, and material science that goes into act ual drive de- sign. For those interested in even more details of that nature, we suggest a different major (or perhaps minor); for those that are happy with this model, good. We can now proceed to using the model to build more in- teresting systems on top of these incredible devices. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG HARD DISKDRIVES 15 References [ADR03] “More Than an Interface: SCSI vs. ATA” by Dave Anderson, Jim Dyke s, Erik Riedel. FAST ’03, 2003. One of the best recent-ish references on how modern disk drives really work; a must read for anyone interested in knowing more. [CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times” E.G . Coffman, L.A. Klimko, B. Ryan SIAM Journal of Computing, September 1972, Vol 1 .",3051
37. Hard Disk Drives,"No 3. Some of the early work in the ﬁeld of disk scheduling. [HK+17] “The Unwritten Contract of Solid State Drives” by Jun He, Sud arsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, Belgr ade, Serbia, April 2017. We take the idea of the unwritten contract, and extend it to SSDs. Using SSDs well s eems as complicated than hard drives, and sometimes more so. [ID01] “Anticipatory Scheduling: A Disk-scheduling Framework To Overco me Deceptive Idle- ness In Synchronous I/O” by Sitaram Iyer, Peter Druschel. SOSP ’01, Octobe r 2001. A cool paper showing how waiting can improve disk scheduling: better requests may be on their way. [JW91] “Disk Scheduling Algorithms Based On Rotational Position” by D. Jacobson, J. Wilkes. Technical Report HPL-CSP-91-7rev1, Hewlett-Packard, February 1991. A more modern take on disk scheduling. It remains a technical report (and not a published pap er) because the authors were scooped by Seltzer et al. [S90]. [RW92] “An Introduction to Disk Drive Modeling” by C. Ruemmler, J. W ilkes. IEEE Computer, 27:3, March 1994. A terriﬁc introduction to the basics of disk operation. Some pieces are out of date, but most of the basics remain. [SCO90] “Disk Scheduling Revisited” by Margo Seltzer, Peter Chen, John Ou sterhout. USENIX 1990. A paper that talks about how rotation matters too in the world of disk scheduling. [SG04] “MEMS-based storage devices and standard disk interfaces: A squ are peg in a round hole?” Steven W. Schlosser, Gregory R. Ganger FAST ’04, pp. 87-100, 2004 While the MEMS aspect of this paper hasn’t yet made an impact, the discussion of the contract be tween ﬁle systems and disks is wonderful and a lasting contribution. We later build on this work to stu dy the “Unwritten Contract of Solid State Drives” [HK+17] [S09a] “Barracuda ES.2 data sheet” by Seagate, Inc.. Available at this website, at least, it was: http://www.seagate.com/docs/pdf/datasheet/disc/ds_b arracuda_es.pdf .A data sheet; read at your own risk. Risk of what? Boredom. [S09b] “Cheetah 15K.5” by Seagate, Inc.. Available at this websit e, we’re pretty sure it is: http://www.seagate.com/docs/pdf/datasheet/disc/ds-c heetah-15k-5-us.pdf . See above commentary on data sheets. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 HARD DISKDRIVES Homework (Simulation) This homework uses disk.py to familiarize you with how a modern hard drive works. It has a lot of different options, and unlike most of the other simulations, has a graphical animator to show you exactl y what happens when the disk is in action. See the README for details. 1. Compute the seek, rotation, and transfer times for the follow ing sets of re- quests:-a 0 ,-a 6 ,-a 30 ,-a 7,30,8 , and ﬁnally -a 10,11,12,13 . 2. Do the same requests above, but change the seek rate to differe nt values: -S 2,-S 4 ,-S 8 ,-S 10 ,-S 40 ,-S 0.1 . How do the times change? 3. Do the same requests above, but change the rotation rate: -R 0.1 ,-R 0.5 , -R 0.01 . How do the times change? 4. FIFO is not always best, e.g., with the request stream -a 7,30,8 , what or- der should the requests be processed in? Run the shortest seek-t ime ﬁrst (SSTF) scheduler ( -p SSTF ) on this workload; how long should it take (seek, rotation, transfer) for each request to be served?",3288
37. Hard Disk Drives,"5. Now use the shortest access-time ﬁrst (SATF) scheduler ( -p SATF ). Does it make any difference for -a 7,30,8 workload? Find a set of requests where SATF outperforms SSTF; more generally, when is SATF better than SSTF? 6. Here is a request stream to try: -a 10,11,12,13 . What goes poorly when it runs? Try adding track skew to address this problem ( -o skew ). Given the default seek rate, what should the skew be to maximize performan ce? What about for different seek rates (e.g., -S 2 ,-S 4 )? In general, could you write a formula to ﬁgure out the skew? 7. Specify a disk with different density per zone, e.g., -z 10,20,30 , which speciﬁes the angular difference between blocks on the outer, mi ddle, and inner tracks. Run some random requests (e.g., -a -1 -A 5,-1,0 , which speciﬁes that random requests should be used via the -a -1 ﬂag and that ﬁve requests ranging from 0 to the max be generated), and compute t he seek, rotation, and transfer times. Use different random seed s. What is the bandwidth (in sectors per unit time) on the outer, middle, and inn er tracks? 8. A scheduling window determines how many requests the disk can e xamine at once. Generate random workloads (e.g., -A 1000,-1,0 , with different seeds) and see how long the SATF scheduler takes when the sched uling win- dow is changed from 1 up to the number of requests. How big of a windo w is needed to maximize performance? Hint: use the -cﬂag and don’t turn on graphics ( -G) to run these quickly. When the scheduling window is set to 1, does it matter which policy you are using? 9. Create a series of requests to starve a particular request, as suming an SATF policy. Given that sequence, how does it perform if you use a bounded SATF (BSATF ) scheduling approach? In this approach, you specify the scheduling window (e.g., -w 4 ); the scheduler only moves onto the next window of requests when allrequests in the current window have been ser- viced. Does this solve starvation? How does it perform, as comp ared to SATF? In general, how should a disk make this trade-off between perfor- mance and starvation avoidance? 10. All the scheduling policies we have looked at thus far are greedy ; they pick the next best option instead of looking for an optimal schedule. Can you ﬁnd a set of requests in which greedy is not optimal? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2366
38. Redundant Disk Arrays RAID,"38 Redundant Arrays of Inexpensive Disks (RAIDs) When we use a disk, we sometimes wish it to be faster; I/O operati ons are slow and thus can be the bottleneck for the entire system. Whe n we use a disk, we sometimes wish it to be larger; more and more data is being put online and thus our disks are getting fuller and fuller. Whe n we use a disk, we sometimes wish for it to be more reliable; when a disk fa ils, if our data isn’t backed up, all that valuable data is gone. CRUX: HOWTOMAKE A L ARGE , FAST, RELIABLE DISK How can we make a large, fast, and reliable storage system? What are the key techniques? What are trade-offs between different ap proaches? In this chapter, we introduce the Redundant Array of Inexpensive Disks better known as RAID [P+88], a technique to use multiple disks in concert to build a faster, bigger, and more reliable disk syste m. The term was introduced in the late 1980s by a group of researchers at U.C. Berke- ley (led by Professors David Patterson and Randy Katz and then st udent Garth Gibson); it was around this time that many different rese archers si- multaneously arrived upon the basic idea of using multiple disk s to build a better storage system [BG88, K86,K88,PB86,SG86]. Externally, a RAID looks like a disk: a group of blocks one can read or write. Internally, the RAID is a complex beast, consisting of m ultiple disks, memory (both volatile and non-), and one or more processors to manage the system. A hardware RAID is very much like a computer system, specialized for the task of managing a group of disks. RAIDs offer a number of advantages over a single disk. One advan- tage is performance . Using multiple disks in parallel can greatly speed up I/O times. Another beneﬁt is capacity . Large data sets demand large disks. Finally, RAIDs can improve reliability ; spreading data across mul- tiple disks (without RAID techniques) makes the data vulnera ble to the loss of a single disk; with some form of redundancy , RAIDs can tolerate the loss of a disk and keep operating as if nothing were wrong. 1 2 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) TIP: TRANSPARENCY ENABLES DEPLOYMENT When considering how to add new functionality to a system, one shou ld always consider whether such functionality can be added transparently , in a way that demands no changes to the rest of the system. Requi ring a complete rewrite of the existing software (or radical hardware c hanges) lessens the chance of impact of an idea. RAID is a perfect exampl e, and certainly its transparency contributed to its success; admi nistrators could install a SCSI-based RAID storage array instead of a SCSI disk, and t he rest of the system (host computer, OS, etc.) did not have to change on e bit to start using it. By solving this problem of deployment , RAID was made more successful from day one. Amazingly, RAIDs provide these advantages transparently to systems that use them, i.e., a RAID just looks like a big disk to the host sy stem. The beauty of transparency, of course, is that it enables one to simpl y replace a disk with a RAID and not change a single line of software; the oper at- ing system and client applications continue to operate without m odiﬁca- tion. In this manner, transparency greatly improves the deployability of RAID, enabling users and administrators to put a RAID to use wi thout worries of software compatibility. We now discuss some of the important aspects of RAIDs. We begin with the interface, fault model, and then discuss how one can eva luate a RAID design along three important axes: capacity, reliabilit y, and perfor- mance. We then discuss a number of other issues that are importan t to RAID design and implementation. 38.1 Interface And RAID Internals To a ﬁle system above, a RAID looks like a big, (hopefully) fast, an d (hopefully) reliable disk. Just as with a single disk, it pres ents itself as a linear array of blocks, each of which can be read or written by the ﬁle system (or other client). When a ﬁle system issues a logical I/O request to the RAID, the RAID internally must calculate which disk (or disks) to access in or der to com- plete the request, and then issue one or more physical I/Os to do so. The exact nature of these physical I/Os depends on the RAID level, a s we will discuss in detail below. However, as a simple example, consider a RAID that keeps two copies of each block (each one on a separate disk); wh en writing to such a mirrored RAID system, the RAID will have to perform two physical I/Os for every one logical I/O it is issued. A RAID system is often built as a separate hardware box, with a st an- dard connection (e.g., SCSI, or SATA) to a host. Internally, however, RAIDs are fairly complex, consisting of a microcontroller that run s ﬁrmware to direct the operation of the RAID, volatile memory such as DRAM to buffer data blocks as they are read and written, and in some ca ses, OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 3 non-volatile memory to buffer writes safely and perhaps even sp ecial- ized logic to perform parity calculations (useful in some RAID le vels, as we will also see below). At a high level, a RAID is very much a spe cial- ized computer system: it has a processor, memory, and disks; howev er, instead of running applications, it runs specialized software designed to operate the RAID. 38.2 Fault Model To understand RAID and compare different approaches, we must h ave a fault model in mind. RAIDs are designed to detect and recover f rom certain kinds of disk faults; thus, knowing exactly which faul ts to expect is critical in arriving upon a working design.",5669
38. Redundant Disk Arrays RAID,"The ﬁrst fault model we will assume is quite simple, and has bee n called the fail-stop fault model [S84]. In this model, a disk can be in exactly one of two states: working or failed. With a working disk, a ll blocks can be read or written. In contrast, when a disk has failed , we assume it is permanently lost. One critical aspect of the fail-stop model is what it assumes abou t fault detection. Speciﬁcally, when a disk has failed, we assume that this is easily detected. For example, in a RAID array, we would assume t hat the RAID controller hardware (or software) can immediately observe w hen a disk has failed. Thus, for now, we do not have to worry about more complex “silent” failures such as disk corruption. We also do not have to worry about a sin- gle block becoming inaccessible upon an otherwise working disk (s ome- times called a latent sector error). We will consider these more c omplex (and unfortunately, more realistic) disk faults later. 38.3 How To Evaluate A RAID As we will soon see, there are a number of different approaches to building a RAID. Each of these approaches has different charac teristics which are worth evaluating, in order to understand their stren gths and weaknesses. Speciﬁcally, we will evaluate each RAID design along three axe s. The ﬁrst axis is capacity ; given a set of Ndisks each with Bblocks, how much useful capacity is available to clients of the RAID? Without re dundancy, the answer is N·B; in contrast, if we have a system that keeps two copies of each block (called mirroring ), we obtain a useful capacity of (N·B)/2. Different schemes (e.g., parity-based ones) tend to fall in b etween. The second axis of evaluation is reliability . How many disk faults can the given design tolerate? In alignment with our fault model, we assume only that an entire disk can fail; in later chapters (i.e., on da ta integrity), we’ll think about how to handle more complex failure modes. Finally, the third axis is performance . Performance is somewhat chal- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) lenging to evaluate, because it depends heavily on the workload pre- sented to the disk array. Thus, before evaluating performance , we will ﬁrst present a set of typical workloads that one should consider. We now consider three important RAID designs: RAID Level 0 (stri p- ing), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity-b ased re- dundancy). The naming of each of these designs as a “level” stem s from the pioneering work of Patterson, Gibson, and Katz at Berkeley [P+ 88]. 38.4 RAID Level 0: Striping The ﬁrst RAID level is actually not a RAID level at all, in that t here is no redundancy. However, RAID level 0, or striping as it is better known, serves as an excellent upper-bound on performance and capacity and thus is worth understanding. The simplest form of striping will stripe blocks across the disks of the system as follows (assume here a 4-disk array): Disk 0 Disk 1 Disk 2 Disk 3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Figure 38.1: RAID-0: Simple Striping From Figure 38.1, you get the basic idea: spread the blocks of the a rray across the disks in a round-robin fashion.",3218
38. Redundant Disk Arrays RAID,"This approach is design ed to extract the most parallelism from the array when requests are m ade for contiguous chunks of the array (as in a large, sequential read, f or exam- ple). We call the blocks in the same row a stripe ; thus, blocks 0, 1, 2, and 3 are in the same stripe above. In the example, we have made the simplifying assumption that on ly 1 block (each of say size 4KB) is placed on each disk before moving on to the next. However, this arrangement need not be the case. For exa mple, we could arrange the blocks across disks as in Figure 38.2: Disk 0 Disk 1 Disk 2 Disk 3 0 2 4 6 chunk size: 1 3 5 7 2 blocks 8 10 12 14 9 11 13 15 Figure 38.2: Striping With A Bigger Chunk Size In this example, we place two 4KB blocks on each disk before moving on to the next disk. Thus, the chunk size of this RAID array is 8KB, and a stripe thus consists of 4 chunks or 32KB of data. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 5 ASIDE : THERAID M APPING PROBLEM Before studying the capacity, reliability, and performance c haracteristics of the RAID, we ﬁrst present an aside on what we call the mapping prob- lem. This problem arises in all RAID arrays; simply put, given a log ical block to read or write, how does the RAID know exactly which physica l disk and offset to access? For these simple RAID levels, we do not need much sophistication i n order to correctly map logical blocks onto their physical locations . Take the ﬁrst striping example above (chunk size = 1 block = 4KB). In t his case, given a logical block address A, the RAID can easily compute the d esired disk and offset with two simple equations: Disk = A  percent number_of_disks Offset = A / number_of_disks Note that these are all integer operations (e.g., 4 / 3 = 1 not 1.333 33...). Let’s see how these equations work for a simple example. Imagine in the ﬁrst RAID above that a request arrives for block 14. Given that th ere are 4 disks, this would mean that the disk we are interested in is (1 4  percent 4 = 2): disk 2. The exact block is calculated as (14 / 4 = 3): block 3. Thus , block 14 should be found on the fourth block (block 3, starting at 0) of the th ird disk (disk 2, starting at 0), which is exactly where it is. You can think about how these equations would be modiﬁed to support different chunk sizes. Try it. It’s not too hard. Chunk Sizes Chunk size mostly affects performance of the array. For example, a small chunk size implies that many ﬁles will get striped across many disks, thus increasing the parallelism of reads and writes to a single ﬁle ; however, the positioning time to access blocks across multiple disks increas es, because the positioning time for the entire request is determined by the maximum of the positioning times of the requests across all drives. A big chunk size, on the other hand, reduces such intra-ﬁle para l- lelism, and thus relies on multiple concurrent requests to ach ieve high throughput. However, large chunk sizes reduce positioning time ; if, for example, a single ﬁle ﬁts within a chunk and thus is placed on a s ingle disk, the positioning time incurred while accessing it will ju st be the po- sitioning time of a single disk. Thus, determining the “best” chunk size is hard to do, as it req uires a great deal of knowledge about the workload presented to the disk sy stem [CL95]. For the rest of this discussion, we will assume that the a rray uses a chunk size of a single block (4KB). Most arrays use larger chunk sizes (e.g., 64 KB), but for the issues we discuss below, the exact chu nk size does not matter; thus we use a single block for the sake of simplicit y.",3668
38. Redundant Disk Arrays RAID,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) Back To RAID-0 Analysis Let us now evaluate the capacity, reliability, and performanc e of striping. From the perspective of capacity, it is perfect: given Ndisks each of size Bblocks, striping delivers N·Bblocks of useful capacity. From the stand- point of reliability, striping is also perfect, but in the bad w ay: any disk failure will lead to data loss. Finally, performance is excell ent: all disks are utilized, often in parallel, to service user I/O requests . Evaluating RAID Performance In analyzing RAID performance, one can consider two different p erfor- mance metrics. The ﬁrst is single-request latency . Understanding the la- tency of a single I/O request to a RAID is useful as it reveals how much parallelism can exist during a single logical I/O operation. Th e second issteady-state throughput of the RAID, i.e., the total bandwidth of many concurrent requests. Because RAIDs are often used in high-per formance environments, the steady-state bandwidth is critical, and t hus will be the main focus of our analyses. To understand throughput in more detail, we need to put forth some workloads of interest. We will assume, for this discussion, that t here are two types of workloads: sequential and random . With a sequential workload, we assume that requests to the array come in large conti gu- ous chunks; for example, a request (or series of requests) that ac cesses 1 MB of data, starting at block xand ending at block (x+1 MB), would be deemed sequential. Sequential workloads are common in many envir on- ments (think of searching through a large ﬁle for a keyword), and t hus are considered important. For random workloads, we assume that each request is rather small , and that each request is to a different random location on disk. For exam- ple, a random stream of requests may ﬁrst access 4KB at logical ad dress 10, then at logical address 550,000, then at 20,100, and so fort h. Some im- portant workloads, such as transactional workloads on a database ma n- agement system (DBMS), exhibit this type of access pattern, an d thus it is considered an important workload. Of course, real workloads are not so simple, and often have a mix of sequential and random-seeming components as well as behaviors in- between the two. For simplicity, we just consider these two possi bilities. As you can tell, sequential and random workloads will result in wi dely different performance characteristics from a disk. With sequ ential access, a disk operates in its most efﬁcient mode, spending little time s eeking and waiting for rotation and most of its time transferring data. With r andom access, just the opposite is true: most time is spent seeking and waiting for rotation and relatively little time is spent transferring d ata. To capture this difference in our analysis, we will assume that a disk can transfer data atSMB/s under a sequential workload, and RMB/s when under a random workload.",3028
38. Redundant Disk Arrays RAID,"In general, Sis much greater than R(i.e.,S≫R). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 7 To make sure we understand this difference, let’s do a simple e xercise. Speciﬁcally, let’s calculate SandRgiven the following disk characteris- tics. Assume a sequential transfer of size 10 MB on average, and a random transfer of 10 KB on average. Also, assume the following disk chara cter- istics: Average seek time 7 ms Average rotational delay 3 ms Transfer rate of disk 50 MB/s To compute S, we need to ﬁrst ﬁgure out how time is spent in a typical 10 MB transfer. First, we spend 7 ms seeking, and then 3 ms rotat ing. Finally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of a sec ond, or 200 ms, spent in transfer. Thus, for each 10 MB request, we spen d 210 ms completing the request. To compute S, we just need to divide: S=Amount of Data Time to access=10MB 210ms= 47.62MB/s As we can see, because of the large time spent transferring dat a,Sis very near the peak bandwidth of the disk (the seek and rotational costs have been amortized). We can compute Rsimilarly. Seek and rotation are the same; we then compute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0. 195 ms. R=Amount of Data Time to access=10KB 10.195ms= 0.981MB/s As we can see, Ris less than 1 MB/s, and S/R is almost 50. Back To RAID-0 Analysis, Again Let’s now evaluate the performance of striping. As we said above, i t is generally good. From a latency perspective, for example, the lat ency of a single-block request should be just about identical to that of a s ingle disk; after all, RAID-0 will simply redirect that request to one of it s disks. From the perspective of steady-state throughput, we’d expect to get the full bandwidth of the system. Thus, throughput equals N(the num- ber of disks) multiplied by S(the sequential bandwidth of a single disk). For a large number of random I/Os, we can again use all of the disks, and thus obtain N·RMB/s. As we will see below, these values are both the simplest to calculate and will serve as an upper bound in com parison with other RAID levels. 38.5 RAID Level 1: Mirroring Our ﬁrst RAID level beyond striping is known as RAID level 1, or mirroring. With a mirrored system, we simply make more than one cop y of each block in the system; each copy should be placed on a separate disk, of course. By doing so, we can tolerate disk failures. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) In a typical mirrored system, we will assume that for each logica l block, the RAID keeps two physical copies of it. Here is an exampl e: Disk 0 Disk 1 Disk 2 Disk 3 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 Figure 38.3: Simple RAID-1: Mirroring In the example, disk 0 and disk 1 have identical contents, and d isk 2 and disk 3 do as well; the data is striped across these mirror pai rs. In fact, you may have noticed that there are a number of different ways to p lace block copies across the disks.",3030
38. Redundant Disk Arrays RAID,"The arrangement above is a common one and is sometimes called RAID-10 or (RAID 1+0 ) because it uses mirrored pairs (RAID-1) and then stripes (RAID-0) on top of them; another c om- mon arrangement is RAID-01 (orRAID 0+1 ), which contains two large striping (RAID-0) arrays, and then mirrors (RAID-1) on top of the m. For now, we will just talk about mirroring assuming the above layout. When reading a block from a mirrored array, the RAID has a choice: i t can read either copy. For example, if a read to logical block 5 is is sued to the RAID, it is free to read it from either disk 2 or disk 3. When wr iting a block, though, no such choice exists: the RAID must update both copies of the data, in order to preserve reliability. Do note, though, th at these writes can take place in parallel; for example, a write to logic al block 5 could proceed to disks 2 and 3 at the same time. RAID-1 Analysis Let us assess RAID-1. From a capacity standpoint, RAID-1 is exp ensive; with the mirroring level = 2, we only obtain half of our peak useful c a- pacity. With Ndisks ofBblocks, RAID-1 useful capacity is (N·B)/2. From a reliability standpoint, RAID-1 does well. It can tolerate the fail- ure of any one disk. You may also notice RAID-1 can actually do bett er than this, with a little luck. Imagine, in the ﬁgure above, tha t disk 0 and disk 2 both failed. In such a situation, there is no data loss. More gen- erally, a mirrored system (with mirroring level of 2) can tolerat e 1 disk failure for certain, and up to N/2 failures depending on which d isks fail. In practice, we generally don’t like to leave things like this t o chance; thus most people consider mirroring to be good for handling a single failu re. Finally, we analyze performance. From the perspective of the la tency of a single read request, we can see it is the same as the latency on a single disk; all the RAID-1 does is direct the read to one of its copies. A w rite is a little different: it requires two physical writes to comp lete before it is done. These two writes happen in parallel, and thus the time will be roughly equivalent to the time of a single write; however, becau se the logical write must wait for both physical writes to complete, it s uffers the worst-case seek and rotational delay of the two requests, and thu s (on average) will be slightly higher than a write to a single disk . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 9 ASIDE : THERAID C ONSISTENT -UPDATE PROBLEM Before analyzing RAID-1, let us ﬁrst discuss a problem that ari ses in any multi-disk RAID system, known as the consistent-update problem [DAA05]. The problem occurs on a write to any RAID that has to up- date multiple disks during a single logical operation. In this c ase, let us assume we are considering a mirrored disk array. Imagine the write is issued to the RAID, and then the RAID deci des that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the reque st to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0 completed (but clearly the re quest to disk 1 did not, as it was never issued).",3263
38. Redundant Disk Arrays RAID,"The result of this untimely power loss is that the two copies of the b lock are now inconsistent ; the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of bot h disks to change atomically , i.e., either both should end up as the new version or neither. The general way to solve this problem is to use a write-ahead log of some kind to ﬁrst record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it. By taking this appr oach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 ca se) are out of sync. One last note: because logging to disk on every write is prohibiti vely expensive, most RAID hardware includes a small amount of non-vola tile RAM (e.g., battery-backed) where it performs this type of loggi ng. Thus, consistent update is provided without the high cost of logging to di sk. To analyze steady-state throughput, let us start with the seq uential workload. When writing out to disk sequentially, each logical wr ite must result in two physical writes; for example, when we write logic al block 0 (in the ﬁgure above), the RAID internally would write it to both disk 0 and disk 1. Thus, we can conclude that the maximum bandwidth ob - tained during sequential writing to a mirrored array is (N 2·S), or half the peak bandwidth. Unfortunately, we obtain the exact same performance during a se - quential read. One might think that a sequential read could do better, because it only needs to read one copy of the data, not both. However, let’s use an example to illustrate why this doesn’t help much. Im agine we need to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s say we issue the read of 0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the re ad of 3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1, and 3, respectively. One might naively think that because we are utilizing all disks, we are achieving the full bandwidth of the array. To see that this is not (necessarily) the case, however, conside r the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) requests a single disk receives (say disk 0). First, it gets a request for block 0; then, it gets a request for block 4 (skipping block 2). In f act, each disk receives a request for every other block. While it is rotatin g over the skipped block, it is not delivering useful bandwidth to the cli ent. Thus, each disk will only deliver half its peak bandwidth. And thus, the se- quential read will only obtain a bandwidth of (N 2·S) MB/s. Random reads are the best case for a mirrored RAID. In this case, w e can distribute the reads across all the disks, and thus obtain t he full pos- sible bandwidth. Thus, for random reads, RAID-1 delivers N·RMB/s. Finally, random writes perform as you might expect:N 2·RMB/s. Each logical write must turn into two physical writes, and thus whi le all the disks will be in use, the client will only perceive this as half the available bandwidth. Even though a write to logical block xturns into two parallel writes to two different physical disks, the bandwidth of many small re- quests only achieves half of what we saw with striping.",3428
38. Redundant Disk Arrays RAID,"As we wil l soon see, getting half the available bandwidth is actually prett y good. 38.6 RAID Level 4: Saving Space With Parity We now present a different method of adding redundancy to a disk a r- ray known as parity . Parity-based approaches attempt to use less capac- ity and thus overcome the huge space penalty paid by mirrored sys tems. They do so at a cost, however: performance. Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 Figure 38.4: RAID-4 With Parity Here is an example ﬁve-disk RAID-4 system (Figure 38.4). For e ach stripe of data, we have added a single parity block that stores the redun- dant information for that stripe of blocks. For example, parity bloc k P1 has redundant information that it calculated from blocks 4, 5, 6, and 7. To compute parity, we need to use a mathematical function that e n- ables us to withstand the loss of any one block from our stripe. It tur ns out the simple function XOR does the trick quite nicely. For a given set of bits, the XOR of all of those bits returns a 0 if there are an even nu mber of 1’s in the bits, and a 1 if there are an odd number of 1’s. For example: C0 C1 C2 C3 P 0 0 1 1 XOR(0,0,1,1) = 0 0 1 0 0 XOR(0,1,0,0) = 1 OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 11 In the ﬁrst row (0,0,1,1), there are two 1’s (C2, C3), and thus XO R of all of those values will be 0 (P); similarly, in the second row ther e is only one 1 (C1), and thus the XOR must be 1 (P). You can remember this in a simple way: that the number of 1s in any row must be an even (not odd) number; that is the invariant that the RAID must maintain in order for parity to be correct. From the example above, you might also be able to guess how parity information can be used to recover from a failure. Imagine the colu mn la- beled C2 is lost. To ﬁgure out what values must have been in the col umn, we simply have to read in all the other values in that row (includ ing the XOR’d parity bit) and reconstruct the right answer. Speciﬁcally, assume the ﬁrst row’s value in column C2 is lost (it is a 1); by reading the ot her values in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parit y column P), we get the values 0, 0, 1, and 0. Because we know that XO R keeps an even number of 1’s in each row, we know what the missing dat a must be: a 1. And that is how reconstruction works in a XOR-based pa r- ity scheme. Note also how we compute the reconstructed value: we j ust XOR the data bits and the parity bits together, in the same way t hat we calculated the parity in the ﬁrst place. Now you might be wondering: we are talking about XORing all of these bits, and yet from above we know that the RAID places 4KB (or larger) blocks on each disk; how do we apply XOR to a bunch of blocks to compute the parity? It turns out this is easy as well. Simply pe rform a bitwise XOR across each bit of the data blocks; put the result of ea ch bit- wise XOR into the corresponding bit slot in the parity block. For ex ample, if we had blocks of size 4 bits (yes, this is still quite a bit smal ler than a 4KB block, but you get the picture), they might look something like this: Block0 Block1 Block2 Block3 Parity 00 10 11 10 11 10 01 00 01 10 As you can see from the ﬁgure, the parity is computed for each bit of each block and the result placed in the parity block.",3402
38. Redundant Disk Arrays RAID,"RAID-4 Analysis Let us now analyze RAID-4. From a capacity standpoint, RAID-4 us es 1 disk for parity information for every group of disks it is protecting . Thus, our useful capacity for a RAID group is (N−1)·B. Reliability is also quite easy to understand: RAID-4 tolerat es 1 disk failure and no more. If more than one disk is lost, there is simply n o way to reconstruct the lost data. Finally, there is performance. This time, let us start by anal yzing steady- state throughput. Sequential read performance can utilize all of the disks except for the parity disk, and thus deliver a peak effective b andwidth of (N−1)·SMB/s (an easy case). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 4 5 6 7 P1 8 9 10 11 P2 12 13 14 15 P3 Figure 38.5: Full-stripe Writes In RAID-4 To understand the performance of sequential writes, we must ﬁr st un- derstand how they are done. When writing a big chunk of data to dis k, RAID-4 can perform a simple optimization known as a full-stripe write . For example, imagine the case where the blocks 0, 1, 2, and 3 have been sent to the RAID as part of a write request (Figure 38.5). In this case, the RAID can simply calculate the new value of P0 ( by performing an XOR across the blocks 0, 1, 2, and 3) and then write a ll of the blocks (including the parity block) to the ﬁve disks above in parallel (highlighted in gray in the ﬁgure). Thus, full-stripe write s are the most efﬁcient way for RAID-4 to write to disk. Once we understand the full-stripe write, calculating the p erformance of sequential writes on RAID-4 is easy; the effective bandwidt h is also (N−1)·SMB/s. Even though the parity disk is constantly in use during the operation, the client does not gain performance advantage from it. Now let us analyze the performance of random reads. As you can also see from the ﬁgure above, a set of 1-block random reads will be sprea d across the data disks of the system but not the parity disk. Thus, the effective performance is: (N−1)·RMB/s. Random writes, which we have saved for last, present the most in- teresting case for RAID-4. Imagine we wish to overwrite block 1 i n the example above. We could just go ahead and overwrite it, but that w ould leave us with a problem: the parity block P0 would no longer accura tely reﬂect the correct parity value of the stripe; in this example, P0 must also be updated. How can we update it both correctly and efﬁciently? It turns out there are two methods. The ﬁrst, known as additive parity , requires us to do the following. To compute the value of the new par ity block, read in all of the other data blocks in the stripe in paralle l (in the example, blocks 0, 2, and 3) and XOR those with the new block (1). T he result is your new parity block. To complete the write, you can the n write the new data and new parity to their respective disks, also in parallel. The problem with this technique is that it scales with the numb er of disks, and thus in larger RAIDs requires a high number of reads to com- pute parity.",3119
38. Redundant Disk Arrays RAID,"Thus, the subtractive parity method. For example, imagine this string of bits (4 data bits, one parity ): C0 C1 C2 C3 P 0 0 1 1 XOR(0,0,1,1) = 0 Let’s imagine that we wish to overwrite bit C2 with a new value wh ich we will call C2 new. The subtractive method works in three steps. First, we read in the old data at C2 (C2 old= 1) and the old parity (P old= 0). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 13 Then, we compare the old data and the new data; if they are the sam e (e.g., C2 new = C2old), then we know the parity bit will also remain the same (i.e., P new= Pold). If, however, they are different, then we must ﬂip the old parity bit to the opposite of its current state, that is, if (Pold== 1), Pnewwill be set to 0; if (P old== 0), P newwill be set to 1. We can express this whole mess neatly with XOR (where ⊕is the XOR operator): Pnew= (Cold⊕Cnew)⊕Pold (38.1) Because we are dealing with blocks, not bits, we perform this cal cula- tion over all the bits in the block (e.g., 4096 bytes in each block m ultiplied by 8 bits per byte). Thus, in most cases, the new block will be dif ferent than the old block and thus the new parity block will too. You should now be able to ﬁgure out when we would use the additive parity calculation and when we would use the subtractive method . Think about how many disks would need to be in the system so that the addi tive method performs fewer I/Os than the subtractive method; what is the cross-over point? For this performance analysis, let us assume we are using the su btrac- tive method. Thus, for each write, the RAID has to perform 4 physi cal I/Os (two reads and two writes). Now imagine there are lots of wri tes submitted to the RAID; how many can RAID-4 perform in parallel? To understand, let us again look at the RAID-4 layout (Figure 38.6) . Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 ∗4 5 6 7+P1 8 9 10 11 P2 12∗13 14 15+P3 Figure 38.6: Example: Writes To 4, 13, And Respective Parity Blocks Now imagine there were 2 small writes submitted to the RAID-4 a t about the same time, to blocks 4 and 13 (marked with∗in the diagram). The data for those disks is on disks 0 and 1, and thus the read and wr ite to data could happen in parallel, which is good. The problem that a rises is with the parity disk; both the requests have to read the rela ted parity blocks for 4 and 13, parity blocks 1 and 3 (marked with+). Hopefully, the issue is now clear: the parity disk is a bottleneck under this ty pe of work- load; we sometimes thus call this the small-write problem for parity- based RAIDs. Thus, even though the data disks could be accessed in parallel, the parity disk prevents any parallelism from mate rializing; all writes to the system will be serialized because of the parity d isk. Because the parity disk has to perform two I/Os (one read, one write) per l ogical I/O, we can compute the performance of small random writes in RAID -4 by computing the parity disk’s performance on those two I/Os, and t hus we achieve (R/2)MB/s. RAID-4 throughput under random small writes is terrible; it does not improve as you add disks to the system. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) We conclude by analyzing I/O latency in RAID-4. As you now know, a single read (assuming no failure) is just mapped to a single disk, and thus its latency is equivalent to the latency of a single disk r equest. The latency of a single write requires two reads and then two write s; the reads can happen in parallel, as can the writes, and thus total laten cy is about twice that of a single disk (with some differences because we ha ve to wait for both reads to complete and thus get the worst-case positioning t ime, but then the updates don’t incur seek cost and thus may be a better -than- average positioning cost). 38.7 RAID Level 5: Rotating Parity To address the small-write problem (at least, partially), Pa tterson, Gib- son, and Katz introduced RAID-5. RAID-5 works almost identicall y to RAID-4, except that it rotates the parity block across drives (Figure 38.7). Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 0 1 2 3 P0 5 6 7 P1 4 10 11 P2 8 9 15 P3 12 13 14 P4 16 17 18 19 Figure 38.7: RAID-5 With Rotated Parity As you can see, the parity block for each stripe is now rotated across the disks, in order to remove the parity-disk bottleneck for RAID -4. RAID-5 Analysis Much of the analysis for RAID-5 is identical to RAID-4.",4503
38. Redundant Disk Arrays RAID,"For examp le, the effective capacity and failure tolerance of the two levels are identical. So are sequential read and write performance. The latency of a sin gle request (whether a read or a write) is also the same as RAID-4. Random read performance is a little better, because we can now ut ilize all disks. Finally, random write performance improves noticeab ly over RAID-4, as it allows for parallelism across requests. Imagine a write to block 1 and a write to block 10; this will turn into requests to di sk 1 and disk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for block 10 and its parity). Thus, they can proceed in parallel. In fact, we can generally assume that given a large number of random reques ts, we will be able to keep all the disks about evenly busy. If that is t he case, then our total bandwidth for small writes will beN 4·RMB/s. The factor of four loss is due to the fact that each RAID-5 write still genera tes 4 total I/O operations, which is simply the cost of using parity-based RA ID. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 15 RAID-0 RAID-1 RAID-4 RAID-5 Capacity N·B(N·B)/2 (N−1)·B(N−1)·B Reliability 0 1 (for sure) 1 1 N 2(if lucky) Throughput Sequential Read N·S(N/2)·S(N−1)·S(N−1)·S Sequential Write N·S(N/2)·S(N−1)·S(N−1)·S Random Read N·R N ·R(N−1)·R N ·R Random Write N·R(N/2)·R1 2·RN 4R Latency Read T T T T Write T T 2T 2T Figure 38.8: RAID Capacity, Reliability, and Performance Because RAID-5 is basically identical to RAID-4 except in th e few cases where it is better, it has almost completely replaced RAID-4 in the market- place. The only place where it has not is in systems that know they will never perform anything other than a large write, thus avoiding t he small- write problem altogether [HLM94]; in those cases, RAID-4 is some times used as it is slightly simpler to build. 38.8 RAID Comparison: A Summary We now summarize our simpliﬁed comparison of RAID levels in Fig- ure 38.8. Note that we have omitted a number of details to simplif y our analysis. For example, when writing in a mirrored system, the a verage seek time is a little higher than when writing to just a single disk, because the seek time is the max of two seeks (one on each disk). Thus, rand om write performance to two disks will generally be a little less than random write performance of a single disk. Also, when updating the pari ty disk in RAID-4/5, the ﬁrst read of the old parity will likely cause a f ull seek and rotation, but the second write of the parity will only result in rotation. However, the comparison in Figure 38.8 does capture the essentia l dif- ferences, and is useful for understanding tradeoffs across RAI D levels. For the latency analysis, we simply use Tto represent the time that a request to a single disk would take. To conclude, if you strictly want performance and do not care about reliability, striping is obviously best. If, however, you want r andom I/O performance and reliability, mirroring is the best; the cost you pay is in lost capacity. If capacity and reliability are your main goals, then RAID- 5 is the winner; the cost you pay is in small-write performance. F inally, if you are always doing sequential I/O and want to maximize capa city, RAID-5 also makes the most sense. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 38.9 Other Interesting RAID Issues There are a number of other interesting ideas that one could (and p er- haps should) discuss when thinking about RAID.",3587
38. Redundant Disk Arrays RAID,"Here are some thi ngs we might eventually write about. For example, there are many other RAID designs, including Leve ls 2 and 3 from the original taxonomy, and Level 6 to tolerate multiple d isk faults [C+04]. There is also what the RAID does when a disk fail s; some- times it has a hot spare sitting around to ﬁll in for the failed disk. What happens to performance under failure, and performance during recon- struction of the failed disk? There are also more realistic faul t models, to take into account latent sector errors orblock corruption [B+08], and lots of techniques to handle such faults (see the data integrit y chapter for details). Finally, you can even build RAID as a software layer: such soft- ware RAID systems are cheaper but have other problems, including the consistent-update problem [DAA05]. 38.10 Summary We have discussed RAID. RAID transforms a number of independen t disks into a large, more capacious, and more reliable single ent ity; impor- tantly, it does so transparently, and thus hardware and softwa re above is relatively oblivious to the change. There are many possible RAID levels to choose from, and the exact RAID level to use depends heavily on what is important to the end -user. For example, mirrored RAID is simple, reliable, and generally provides good performance but at a high capacity cost. RAID-5, in contrast, is reliable and better from a capacity standpoint, but performs qu ite poorly when there are small writes in the workload. Picking a RAID and s etting its parameters (chunk size, number of disks, etc.) properly for a particular workload is challenging, and remains more of an art than a science . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG REDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) 17 References [B+08] “An Analysis of Data Corruption in the Storage Stack” by La kshmi N. Bairavasun- daram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau , Remzi H. Arpaci- Dusseau. FAST ’08, San Jose, CA, February 2008. Our own work analyzing how often disks actu- ally corrupt your data. Not often, but sometimes. And thus something a reliabl e storage system must consider. [BJ88] “Disk Shadowing” by D. Bitton and J. Gray. VLDB 1988. One of the ﬁrst papers to discuss mirroring, herein called “shadowing”. [CL95] “Striping in a RAID level 5 disk array” by Peter M. Chen and Edw ard K. Lee. SIGMET- RICS 1995. A nice analysis of some of the important parameters in a RAID-5 disk array. [C+04] “Row-Diagonal Parity for Double Disk Failure Correction” by P . Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar. FAST ’04, February 2004. Though not the ﬁrst paper on a RAID system with two disks for parity, it is a recent and highly-un derstandable version of said idea. Read it to learn more. [DAA05] “Journal-guided Resynchronization for Software RAID” by Timo thy E. Denehy, A. Arpaci-Dusseau, R. Arpaci-Dusseau. FAST 2005. Our own work on the consistent-update problem. Here we solve it for Software RAID by integrating the journaling machinery of the ﬁle system above with the software RAID beneath it.",3102
38. Redundant Disk Arrays RAID,"[HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau, Michael Malcolm. USENIX Winter 1994, San Francisco, California, 1994. The sparse paper intro- ducing a landmark product in storage, the write-anywhere ﬁle layout or WAFL ﬁle system that underlies the NetApp ﬁle server. [K86] “Synchronized Disk Interleaving” by M.Y. Kim. IEEE Transactions o n Computers, Vol- ume C-35: 11, November 1986. Some of the earliest work on RAID is found here. [K88] “Small Disk Arrays – The Emerging Approach to High Performance” b y F. Kurzweil. Presentation at Spring COMPCON ’88, March 1, 1988, San Francisco, Californi a.Another early RAID reference. [P+88] “Redundant Arrays of Inexpensive Disks” by D. Patterson, G. Gi bson, R. Katz. SIG- MOD 1988. This is considered theRAID paper, written by famous authors Patterson, Gibson, and Katz. The paper has since won many test-of-time awards and ushered in the RAID era, i ncluding the name RAID itself. [PB86] “Providing Fault Tolerance in Parallel Secondary Storage Syst ems” by A. Park, K. Bal- asubramaniam. Department of Computer Science, Princeton, CS-TR-O57-86, N ovember 1986. Another early work on RAID. [SG86] “Disk Striping” by K. Salem, H. Garcia-Molina. IEEE Internati onal Conference on Data Engineering, 1986. And yes, another early RAID work. There are a lot of these, which kind of came out of the woodwork when the RAID paper was published in SIGMOD. [S84] “Byzantine Generals in Action: Implementing Fail-Stop Processor s” by F.B. Schneider. ACM Transactions on Computer Systems, 2(2):145154, May 1984. Finally, a paper that is not about RAID. This paper is actually about how systems fail, and how to make something behave in a fail-stop manner. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 R EDUNDANT ARRAYS OF INEXPENSIVE DISKS (RAID S) Homework (Simulation) This section introduces raid.py , a simple RAID simulator you can use to shore up your knowledge of how RAID systems work. See the README for details. Questions 1. Use the simulator to perform some basic RAID mapping tests. Run wit h different levels (0, 1, 4, 5) and see if you can ﬁgure out the mappi ngs of a set of requests. For RAID-5, see if you can ﬁgure out the difference be tween left- symmetric and left-asymmetric layouts. Use some different random s eeds to generate different problems than above. 2. Do the same as the ﬁrst problem, but this time vary the chunk size w ith-C. How does chunk size change the mappings? 3. Do the same as above, but use the -rﬂag to reverse the nature of each problem. 4. Now use the reverse ﬂag but increase the size of each request wi th the -Sﬂag. Try specifying sizes of 8k, 12k, and 16k, while varying t he RAID level. What happens to the underlying I/O pattern when the siz e of the re- quest increases? Make sure to try this with the sequential workl oad too (-W sequential ); for what request sizes are RAID-4 and RAID-5 much more I/O efﬁcient? 5. Use the timing mode of the simulator ( -t) to estimate the performance of 100 random reads to the RAID, while varying the RAID levels, usin g 4 disks. 6. Do the same as above, but increase the number of disks. How does t he performance of each RAID level scale as the number of disks increa ses? 7. Do the same as above, but use all writes ( -w 100 ) instead of reads. How does the performance of each RAID level scale now? Can you do a ro ugh estimate of the time it will take to complete the workload of 100 r andom writes? 8. Run the timing mode one last time, but this time with a sequential wo rkload (-W sequential ). How does the performance vary with RAID level, and when doing reads versus writes? How about when varying the size of each request? What size should you write to a RAID when using RAID-4 or RAID-5? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3832
39. Files and Directories,"39 Interlude: Files and Directories Thus far we have seen the development of two key operating system ab- stractions: the process, which is a virtualization of the CPU, an d the ad- dress space, which is a virtualization of memory. In tandem, the se two abstractions allow a program to run as if it is in its own private, i solated world; as if it has its own processor (or processors); as if it has its ow n memory. This illusion makes programming the system much easier and thus is prevalent today not only on desktops and servers but increa singly on all programmable platforms including mobile phones and the lik e. In this section, we add one more critical piece to the virtualiza tion puz- zle:persistent storage . A persistent-storage device, such as a classic hard disk drive or a more modern solid-state storage device , stores informa- tion permanently (or at least, for a long time). Unlike memory, whos e contents are lost when there is a power loss, a persistent-storage device keeps such data intact. Thus, the OS must take extra care with such a device: this is where users keep data that they really care ab out. CRUX: HOWTOMANAGE A P ERSISTENT DEVICE How should the OS manage a persistent device? What are the APIs? What are the important aspects of the implementation? Thus, in the next few chapters, we will explore critical techn iques for managing persistent data, focusing on methods to improve perform ance and reliability. We begin, however, with an overview of the API: the in- terfaces you’ll expect to see when interacting with a U NIXﬁle system. 39.1 Files And Directories Two key abstractions have developed over time in the virtualiza tion of storage. The ﬁrst is the ﬁle. A ﬁle is simply a linear array of bytes, each of which you can read or write. Each ﬁle has some kind of low-level name , usually a number of some kind; often, the user is not aware of 1 2 INTERLUDE : FILES AND DIRECTORIES this name (as we will see). For historical reasons, the low-level name of a ﬁle is often referred to as its inode number . We’ll be learning a lot more about inodes in future chapters; for now, just assume that each ﬁl e has an inode number associated with it. In most systems, the OS does not know much about the structure of the ﬁle (e.g., whether it is a picture, or a text ﬁle, or C code); ra ther, the responsibility of the ﬁle system is simply to store such data per sistently on disk and make sure that when you request the data again, you get what you put there in the ﬁrst place. Doing so is not as simple as it seems. The second abstraction is that of a directory . A directory, like a ﬁle, also has a low-level name (i.e., an inode number), but its conten ts are quite speciﬁc: it contains a list of (user-readable name, low-l evel name) pairs. For example, let’s say there is a ﬁle with the low-level na me “10”, and it is referred to by the user-readable name of “foo”. The dire ctory that “foo” resides in thus would have an entry (“foo”, “10”) that ma ps the user-readable name to the low-level name.",3040
39. Files and Directories,"Each entry in a d irectory refers to either ﬁles or other directories. By placing directori es within other directories, users are able to build an arbitrary directory tree (or directory hierarchy ), under which all ﬁles and directories are stored. / foo bar.txtbar foo bar bar.txt Figure 39.1: An Example Directory Tree The directory hierarchy starts at a root directory (in U NIX-based sys- tems, the root directory is simply referred to as /) and uses some kind ofseparator to name subsequent sub-directories until the desired ﬁle or directory is named. For example, if a user created a directory foo in the root directory /, and then created a ﬁle bar.txt in the directory foo, we could refer to the ﬁle by its absolute pathname , which in this case would be/foo/bar.txt . See Figure 39.1 for a more complex directory tree; valid directories in the example are /, /foo, /bar, /bar/bar, /bar/foo and valid ﬁles are /foo/bar.txt and/bar/foo/bar.txt . Directories and ﬁles can have the same name as long as they are in dif- ferent locations in the ﬁle-system tree (e.g., there are two ﬁl es named bar.txt in the ﬁgure, /foo/bar.txt and/bar/foo/bar.txt ). You may also notice that the ﬁle name in this example often has two OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 3 TIP: THINK CAREFULLY ABOUT NAMING Naming is an important aspect of computer systems [SK09]. In U NIX systems, virtually everything that you can think of is named th rough the ﬁle system. Beyond just ﬁles, devices, pipes, and even process es [K84] can be found in what looks like a plain old ﬁle system. This uniformi ty of naming eases your conceptual model of the system, and makes the system simpler and more modular. Thus, whenever creating a sys tem or interface, think carefully about what names you are using. parts:bar andtxt, separated by a period. The ﬁrst part is an arbitrary name, whereas the second part of the ﬁle name is usually used to i ndi- cate the type of the ﬁle, e.g., whether it is C code (e.g., .c), or an image (e.g.,.jpg ), or a music ﬁle (e.g., .mp3 ). However, this is usually just a convention : there is usually no enforcement that the data contained in a ﬁle named main.c is indeed C source code. Thus, we can see one great thing provided by the ﬁle system: a conv e- nient way to name all the ﬁles we are interested in. Names are important in systems as the ﬁrst step to accessing any resource is being a ble to name it. In U NIXsystems, the ﬁle system thus provides a uniﬁed way to access ﬁles on disk, USB stick, CD-ROM, many other devices, and in fact m any other things, all located under the single directory tree. 39.2 The File System Interface Let’s now discuss the ﬁle system interface in more detail. We’ll s tart with the basics of creating, accessing, and deleting ﬁles. You may think this is straightforward, but along the way we’ll discover the mys terious call that is used to remove ﬁles, known as unlink() . Hopefully, by the end of this chapter, this mystery won’t be so mysterious to you.",3047
39. Files and Directories,"39.3 Creating Files We’ll start with the most basic of operations: creating a ﬁle. This can be accomplished with the open system call; by calling open() and passing it theOCREAT ﬂag, a program can create a new ﬁle. Here is some exam- ple code to create a ﬁle called “foo” in the current working direct ory. int fd = open(\""foo\"", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S _IWUSR); The routine open() takes a number of different ﬂags. In this exam- ple, the second parameter creates the ﬁle ( OCREAT ) if it does not exist, ensures that the ﬁle can only be written to ( OWRONLY ), and, if the ﬁle already exists, truncates it to a size of zero bytes thus removi ng any exist- ing content ( OTRUNC ). The third parameter speciﬁes permissions, in this case making the ﬁle readable and writable by the owner. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 INTERLUDE : FILES AND DIRECTORIES ASIDE : THECREAT()SYSTEM CALL The older way of creating a ﬁle is to call creat() , as follows: int fd = creat(\""foo\""); // option: add second flag to set permi ssions You can think of creat() asopen() with the following ﬂags: OCREAT | O WRONLY | O TRUNC . Because open() can create a ﬁle, the usage of creat() has somewhat fallen out of favor (indeed, it could just be implemented as a library call to open() ); however, it does hold a special place in U NIXlore. Speciﬁcally, when Ken Thompson was asked what he would do differently if he were redesigning U NIX, he replied: “I’d spell creat with an e.” One important aspect of open() is what it returns: a ﬁle descriptor . A ﬁle descriptor is just an integer, private per process, and is u sed in U NIX systems to access ﬁles; thus, once a ﬁle is opened, you use the ﬁle de- scriptor to read or write the ﬁle, assuming you have permission to do so. In this way, a ﬁle descriptor is a capability [L84], i.e., an opaque handle that gives you the power to perform certain operations. Another way to think of a ﬁle descriptor is as a pointer to an object of type ﬁle; once you have such an object, you can call other “methods” to access the ﬁle , like read() andwrite() (we’ll see how to do so below). As stated above, ﬁle descriptors are managed by the operating sy stem on a per-process basis. This means some kind of simple structure ( e.g., an array) is kept in the proc structure on U NIXsystems. Here is the relevant piece from the xv6 kernel [CK+08]: struct proc { ... struct file *ofile[NOFILE]; // Open files ... }; A simple array (with a maximum of NOFILE open ﬁles) tracks which ﬁles are opened on a per-process basis. Each entry of the array is a ctually just a pointer to a struct file , which will be used to track information about the ﬁle being read or written; we’ll discuss this further b elow. 39.4 Reading And Writing Files Once we have some ﬁles, of course we might like to read or write them . Let’s start by reading an existing ﬁle. If we were typing at a com mand line, we might just use the program cat to dump the contents of the ﬁle to the screen. prompt> echo hello > foo prompt> cat foo hello prompt> OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 5 TIP: USESTRACE (ANDSIMILAR TOOLS ) Thestrace tool provides an awesome way to see what programs are up to. By running it, you can trace which system calls a program make s, see the arguments and return codes, and generally get a very good ide a of what is going on.",3419
39. Files and Directories,"The tool also takes some arguments which can be quite useful. For e x- ample,-ffollows any fork’d children too; -treports the time of day at each call; -e trace=open,close,read,write only traces calls to those system calls and ignores all others. There are many more powe rful ﬂags — read the man pages and ﬁnd out how to harness this wonderful tool. In this code snippet, we redirect the output of the program echo to the ﬁlefoo, which then contains the word “hello” in it. We then use cat to see the contents of the ﬁle. But how does the cat program access the ﬁlefoo? To ﬁnd this out, we’ll use an incredibly useful tool to trace the sy s- tem calls made by a program. On Linux, the tool is called strace ; other systems have similar tools (see dtruss on a Mac, or truss on some older UNIXvariants). What strace does is trace every system call made by a program while it runs, and dump the trace to the screen for you to s ee. Here is an example of using strace to ﬁgure out what cat is doing (some calls removed for readability): prompt> strace cat foo ... open(\""foo\"", O_RDONLY|O_LARGEFILE) = 3 read(3, \""hello \"", 4096) = 6 write(1, \""hello \"", 6) = 6 hello read(3, \""\"", 4096) = 0 close(3) = 0 ... prompt> The ﬁrst thing that cat does is open the ﬁle for reading. A couple of things we should note about this; ﬁrst, that the ﬁle is only opened for reading (not writing), as indicated by the ORDONLY ﬂag; second, that the 64-bit offset be used ( OLARGEFILE ); third, that the call to open() succeeds and returns a ﬁle descriptor, which has the value of 3. Why does the ﬁrst call to open() return 3, not 0 or perhaps 1 as you might expect? As it turns out, each running process already has three ﬁles open, standard input (which the process can read to receiv e input), standard output (which the process can write to in order to dump i nfor- mation to the screen), and standard error (which the process can write error messages to). These are represented by ﬁle descriptors 0, 1, and 2, respectively. Thus, when you ﬁrst open another ﬁle (as cat does above), it will almost certainly be ﬁle descriptor 3. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 INTERLUDE : FILES AND DIRECTORIES After the open succeeds, cat uses theread() system call to repeat- edly read some bytes from a ﬁle. The ﬁrst argument to read() is the ﬁle descriptor, thus telling the ﬁle system which ﬁle to read; a pr ocess can of course have multiple ﬁles open at once, and thus the descriptor en ables the operating system to know which ﬁle a particular read refers to. The second argument points to a buffer where the result of the read() will be placed; in the system-call trace above, strace shows the resul ts of the read in this spot (“hello”). The third argument is the size of the buff er, which in this case is 4 KB. The call to read() returns successfully as well, here returning the number of bytes it read (6, which includes 5 for th e letters in the word “hello” and one for an end-of-line marker). At this point, you see another interesting result of the strace: a single call to thewrite() system call, to the ﬁle descriptor 1. As we mentioned above, this descriptor is known as the standard output, and thus i s used to write the word “hello” to the screen as the program cat is meant to do. But does it call write() directly?",3333
39. Files and Directories,"Maybe (if it is highly optimized). But if not, what cat might do is call the library routine printf() ; in- ternally,printf() ﬁgures out all the formatting details passed to it, and eventually writes to standard output to print the results to t he screen. Thecat program then tries to read more from the ﬁle, but since there are no bytes left in the ﬁle, the read() returns 0 and the program knows that this means it has read the entire ﬁle. Thus, the program ca llsclose() to indicate that it is done with the ﬁle “foo”, passing in the corre sponding ﬁle descriptor. The ﬁle is thus closed, and the reading of it thus complete. Writing a ﬁle is accomplished via a similar set of steps. First, a ﬁle is opened for writing, then the write() system call is called, perhaps repeatedly for larger ﬁles, and then close() . Usestrace to trace writes to a ﬁle, perhaps of a program you wrote yourself, or by tracing the dd utility, e.g., dd if=foo of=bar . 39.5 Reading And Writing, But Not Sequentially Thus far, we’ve discussed how to read and write ﬁles, but all acc ess has been sequential ; that is, we have either read a ﬁle from the beginning to the end, or written a ﬁle out from beginning to end. Sometimes, however, it is useful to be able to read or write to a spe - ciﬁc offset within a ﬁle; for example, if you build an index over a t ext document, and use it to look up a speciﬁc word, you may end up reading from some random offsets within the document. To do so, we will use thelseek() system call. Here is the function prototype: off_t lseek(int fildes, off_t offset, int whence); The ﬁrst argument is familiar (a ﬁle descriptor). The second ar gu- ment is the offset , which positions the ﬁle offset to a particular location within the ﬁle. The third argument, called whence for historical reasons, determines exactly how the seek is performed. From the man page: OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 7 ASIDE : DATA STRUCTURE — T HEOPEN FILETABLE Each process maintains an array of ﬁle descriptors, each of which refers to an entry in the system-wide open ﬁle table . Each entry in this table tracks which underlying ﬁle the descriptor refers to, the curr ent offset, and other relevant details such as whether the ﬁle is readable or writable. If whence is SEEK_SET, the offset is set to offset bytes. If whence is SEEK_CUR, the offset is set to its current location plus offset bytes. If whence is SEEK_END, the offset is set to the size of the file plus offset bytes. As you can tell from this description, for each ﬁle a process opens, t he OS tracks a “current” offset, which determines where the next read or write will begin reading from or writing to within the ﬁle. Thus , part of the abstraction of an open ﬁle is that it has a current offset, whi ch is updated in one of two ways. The ﬁrst is when a read or write of N bytes takes place, Nis added to the current offset; thus each read or write implicitly updates the offset. The second is explicitly withlseek , which changes the offset as speciﬁed above. The offset, as you might have guessed, is kept in that struct file we saw earlier, as referenced from the struct proc . Here is a (simpli- ﬁed) xv6 deﬁnition of the structure: struct file { int ref; char readable; char writable; struct inode *ip; uint off; }; As you can see in the structure, the OS can use this to determine whether the opened ﬁle is readable or writable (or both), which un der- lying ﬁle it refers to (as pointed to by the struct inode pointerip), and the current offset ( off).",3572
39. Files and Directories,"There is also a reference count ( ref), which we will discuss further below. These ﬁle structures represent all of the currently opened ﬁle s in the system; together, they are sometimes referred to as the open ﬁle table . The xv6 kernel just keeps these as an array as well, with one lock per entry, as shown here: struct { struct spinlock lock; struct file file[NFILE]; } ftable; Let’s make this a bit clearer with a few examples. First, let’s t rack a process that opens a ﬁle (of size 300 bytes) and reads it by callin g the read() system call repeatedly, each time reading 100 bytes. Here is a trace of the relevant system calls, along with the values retur ned by each c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 INTERLUDE : FILES AND DIRECTORIES system call, and the value of the current offset in the open ﬁle ta ble for this ﬁle access: Return Current System Calls Code Offset fd = open(\""file\"", O RDONLY); 3 0 read(fd, buffer, 100); 100 100 read(fd, buffer, 100); 100 200 read(fd, buffer, 100); 100 300 read(fd, buffer, 100); 0 300 close(fd); 0 – There are a couple of items of interest to note from the trace. First , you can see how the current offset gets initialized to zero when t he ﬁle is opened. Next, you can see how it is incremented with each read() by the process; this makes it easy for a process to just keep calling read() to get the next chunk of the ﬁle. Finally, you can see how at the end , an attempted read() past the end of the ﬁle returns zero, thus indicating to the process that it has read the ﬁle in its entirety. Second, let’s trace a process that opens the same ﬁle twice and issues a read to each of them. OFT[10] OFT[11] Return Current Current System Calls Code Offset Offset fd1 = open(\""file\"", O RDONLY); 3 0 – fd2 = open(\""file\"", O RDONLY); 4 0 0 read(fd1, buffer1, 100); 100 100 0 read(fd2, buffer2, 100); 100 100 100 close(fd1); 0 – 100 close(fd2); 0 – – In this example, two ﬁle descriptors are allocated ( 3and4), and each refers to a different entry in the open ﬁle table (in this example, entries 10 and11, as shown in the table heading; OFT stands for Open File Table). If you trace through what happens, you can see how each current offs et is updated independently. In one ﬁnal example, a process uses lseek() to reposition the current offset before reading; in this case, only a single open ﬁle table e ntry is needed (as with the ﬁrst example). Return Current System Calls Code Offset fd = open(\""file\"", O RDONLY); 3 0 lseek(fd, 200, SEEK SET); 200 200 read(fd, buffer, 50); 50 250 close(fd); 0 – Here, thelseek() call ﬁrst sets the current offset to 200. The subse- quentread() then reads the next 50 bytes, and updates the current offset accordingly. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 9 ASIDE : CALLING LSEEK()DOES NOTPERFORM A D ISKSEEK The poorly-named system call lseek() confuses many a student try- ing to understand disks and how the ﬁle systems atop them work. Do not confuse the two.",3015
39. Files and Directories,"The lseek() call simply changes a variable in OS memory that tracks, for a particular process, at which offset its next read or write will start. A disk seek occurs when a read or write issued to the disk is not on the same track as the last read or write, and thus nec es- sitates a head movement. Making this even more confusing is the f act that calling lseek() to read or write from/to random parts of a ﬁle, and then reading/writing to those random parts, will indeed lead t o more disk seeks. Thus, calling lseek() can certainly lead to a seek in an up- coming read or write, but absolutely does not cause any disk I/O to oc cur itself. 39.6 Shared File Table Entries: fork() Anddup() In many cases (as in the examples shown above), the mapping of ﬁle descriptor to an entry in the open ﬁle table is a one-to-one mapping . For example, when a process runs, it might decide to open a ﬁle, read it, and then close it; in this example, the ﬁle will have a unique entry in the open ﬁle table. Even if some other process reads the same ﬁle at the sam e time, each will have its own entry in the open ﬁle table. In this way, ea ch logical reading or writing of a ﬁle is independent, and each has its own cu rrent offset while it accesses the given ﬁle. However, there are a few interesting cases where an entry in th e open ﬁle table is shared . One of those cases occurs when a parent process creates a child process with fork() . Figure 39.2 shows a small code snippet in which a parent creates a child and then waits for it to complete. The child adjusts the current offset via a call to lseek() and then exits. Finally the parent, after waiting for the child, checks the current offset and prints out its value. int main(int argc, char *argv[]) { int fd = open(\""file.txt\"", O_RDONLY); assert(fd >= 0); int rc = fork(); if (rc == 0) { rc = lseek(fd, 10, SEEK_SET); printf(\""child: offset  percentd \"", rc); } else if (rc > 0) { (void) wait(NULL); printf(\""parent: offset  percentd \"", (int) lseek(fd, 0, SEEK_CUR )); } return 0; } Figure 39.2: Shared Parent/Child File Table Entries ( fork-seek.c ) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 INTERLUDE : FILES AND DIRECTORIES Parent File Descriptors 3: Child File Descriptors 3:Open File Table refcnt: 2 off: 10 inode: Inode #1000 (file.txt) Figure 39.3: Processes Sharing An Open File Table Entry When we run this program, we see the following output: prompt> ./fork-seek child: offset 10 parent: offset 10 prompt> Figure 39.3 shows the relationships that connect each processes private descriptor arrays, the shared open ﬁle table entry, and the ref erence from it to the underlying ﬁle-system inode. Note that we ﬁnally make use of thereference count here. When a ﬁle table entry is shared, its reference count is incremented; only when both processes close the ﬁle (or exi t) will the entry be removed. Sharing open ﬁle table entries across parent and child is occasion ally useful. For example, if you create a number of processes that are c ooper- atively working on a task, they can write to the same output ﬁle wi thout any extra coordination. For more on what is shared by processes when fork() is called, please see the man pages. One other interesting, and perhaps more useful, case of sharing occurs with thedup() system call (and its very similar cousins, dup2() and evendup3() ). Thedup() call allows a process to create a new ﬁle descriptor that refers to the same underlying open ﬁle as an existing descript or. Figure 39.4 shows a small code snippet that shows how dup() can be used. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 11 int main(int argc, char *argv[]) { int fd = open(\""README\"", O_RDONLY); assert(fd >= 0); int fd2 = dup(fd); // now fd and fd2 can be used interchangeably return 0; } Figure 39.4: Shared File Table Entry With dup() (dup.c ) Thedup() call (and, in particular, dup2() ) are useful when writing a U NIX shell and performing operations like output redirection; spend some time and think about why.",4050
39. Files and Directories,"And now, you are thinking: why didn’t they tell me this when I was doing the shell project? Oh well, you c an’t get everything in the right order, even in an incredible book about ope rating systems. Sorry. 39.7 Writing Immediately With fsync() Most times when a program calls write() , it is just telling the ﬁle system: please write this data to persistent storage, at some p oint in the future. The ﬁle system, for performance reasons, will buffer such writes in memory for some time (say 5 seconds, or 30); at that later point in time, the write(s) will actually be issued to the storage devi ce. From the perspective of the calling application, writes seem to complet e quickly, and only in rare cases (e.g., the machine crashes after the write() call but before the write to disk) will data be lost. However, some applications require something more than this even - tual guarantee. For example, in a database management system (DBMS), development of a correct recovery protocol requires the ability to f orce writes to disk from time to time. To support these types of applications, most ﬁle systems provide s ome additional control APIs. In the U NIXworld, the interface provided to ap- plications is known as fsync(int fd) . When a process calls fsync() for a particular ﬁle descriptor, the ﬁle system responds by forci ng all dirty (i.e., not yet written) data to disk, for the ﬁle referred to by t he speciﬁed ﬁle descriptor. The fsync() routine returns once all of these writes are complete. Here is a simple example of how to use fsync() . The code opens the ﬁlefoo, writes a single chunk of data to it, and then calls fsync() to ensure the writes are forced immediately to disk. Once the fsync() returns, the application can safely move on, knowing that the dat a has been persisted (if fsync() is correctly implemented, that is). int fd = open(\""foo\"", O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S _IWUSR); assert(fd > -1); int rc = write(fd, buffer, size); assert(rc == size); rc = fsync(fd); assert(rc == 0); c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 INTERLUDE : FILES AND DIRECTORIES Interestingly, this sequence does not guarantee everything t hat you might expect; in some cases, you also need to fsync() the directory that contains the ﬁle foo. Adding this step ensures not only that the ﬁle itself is on disk, but that the ﬁle, if newly created, also is durably a part of the directory. Not surprisingly, this type of detail is often overlooke d, leading to many application-level bugs [P+13,P+14]. 39.8 Renaming Files Once we have a ﬁle, it is sometimes useful to be able to give a ﬁle a different name. When typing at the command line, this is accomp lished withmvcommand; in this example, the ﬁle foo is renamed bar: prompt> mv foo bar Usingstrace , we can see that mvuses the system call rename(char *old, char *new) , which takes precisely two arguments: the original name of the ﬁle ( old) and the new name ( new). One interesting guarantee provided by the rename() call is that it is (usually) implemented as an atomic call with respect to system crashes; if the system crashes during the renaming, the ﬁle will eithe r be named the old name or the new name, and no odd in-between state can arise . Thus,rename() is critical for supporting certain kinds of applications that require an atomic update to ﬁle state.",3351
39. Files and Directories,"Let’s be a little more speciﬁc here. Imagine that you are using a ﬁ le ed- itor (e.g., emacs), and you insert a line into the middle of a ﬁle. The ﬁle’s name, for the example, is foo.txt . The way the editor might update the ﬁle to guarantee that the new ﬁle has the original contents plus the line inserted is as follows (ignoring error-checking for simplicity) : int fd = open(\""foo.txt.tmp\"", O_WRONLY|O_CREAT|O_TRUNC, S_IRUSR|S_IWUSR); write(fd, buffer, size); // write out new version of file fsync(fd); close(fd); rename(\""foo.txt.tmp\"", \""foo.txt\""); What the editor does in this example is simple: write out the new version of the ﬁle under a temporary name ( foo.txt.tmp ), force it to disk with fsync() , and then, when the application is certain the new ﬁle metadata and contents are on the disk, rename the temporary ﬁ le to the original ﬁle’s name. This last step atomically swaps the new ﬁle into place, while concurrently deleting the old version of the ﬁle, an d thus an atomic ﬁle update is achieved. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 13 39.9 Getting Information About Files Beyond ﬁle access, we expect the ﬁle system to keep a fair amount of information about each ﬁle it is storing. We generally call such data about ﬁles metadata . To see the metadata for a certain ﬁle, we can use the stat() orfstat() system calls. These calls take a pathname (or ﬁle descriptor) to a ﬁle and ﬁll in a stat structure as seen here: struct stat { dev_t st_dev; / *ID of device containing file */ ino_t st_ino; / *inode number */ mode_t st_mode; / *protection */ nlink_t st_nlink; / *number of hard links */ uid_t st_uid; / *user ID of owner */ gid_t st_gid; / *group ID of owner */ dev_t st_rdev; / *device ID (if special file) */ off_t st_size; / *total size, in bytes */ blksize_t st_blksize; / *blocksize for filesystem I/O */ blkcnt_t st_blocks; / *number of blocks allocated */ time_t st_atime; / *time of last access */ time_t st_mtime; / *time of last modification */ time_t st_ctime; / *time of last status change */ }; You can see that there is a lot of information kept about each ﬁle, in- cluding its size (in bytes), its low-level name (i.e., inode nu mber), some ownership information, and some information about when the ﬁle was accessed or modiﬁed, among other things. To see this information, y ou can use the command line tool stat : prompt> echo hello > file prompt> stat file File: ‘file’ Size: 6 Blocks: 8 IO Block: 4096 regular file Device: 811h/2065d Inode: 67158084 Links: 1 Access: (0640/-rw-r-----) Uid: (30686/ remzi) Gid: (30686 / remzi) Access: 2011-05-03 15:50:20.157594748 -0500 Modify: 2011-05-03 15:50:20.157594748 -0500 Change: 2011-05-03 15:50:20.157594748 -0500 As it turns out, each ﬁle system usually keeps this type of inform ation in a structure called an inode1. We’ll be learning a lot more about inodes when we talk about ﬁle system implementation. For now, you should ju st think of an inode as a persistent data structure kept by the ﬁle s ystem that has information like we see above inside of it. All inodes reside on d isk; a copy of active ones are usually cached in memory to speed up acces s.",3195
39. Files and Directories,"1Some ﬁle systems call these structures similar, but slightly dif ferent, names, such as dnodes; the basic idea is similar however. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 INTERLUDE : FILES AND DIRECTORIES 39.10 Removing Files At this point, we know how to create ﬁles and access them, either s e- quentially or not. But how do you delete ﬁles? If you’ve used U NIX, you probably think you know: just run the program rm. But what system call doesrmuse to remove a ﬁle? Let’s use our old friend strace again to ﬁnd out. Here we remove that pesky ﬁle “foo”: prompt> strace rm foo ... unlink(\""foo\"") = 0 ... We’ve removed a bunch of unrelated cruft from the traced output, leaving just a single call to the mysteriously-named system c allunlink() . As you can see, unlink() just takes the name of the ﬁle to be removed, and returns zero upon success. But this leads us to a great puzz le: why is this system call named “unlink”? Why not just “remove” or “del ete”. To understand the answer to this puzzle, we must ﬁrst underst and more than just ﬁles, but also directories. 39.11 Making Directories Beyond ﬁles, a set of directory-related system calls enable you t o make, read, and delete directories. Note you can never write to a direc tory di- rectly; because the format of the directory is considered ﬁle sys tem meta- data, you can only update a directory indirectly by, for example, creating ﬁles, directories, or other object types within it. In this way, t he ﬁle system makes sure that the contents of the directory always are as expec ted. To create a directory, a single system call, mkdir() , is available. The eponymous mkdir program can be used to create such a directory. Let’s take a look at what happens when we run the mkdir program to make a simple directory called foo: prompt> strace mkdir foo ... mkdir(\""foo\"", 0777) = 0 ... prompt> When such a directory is created, it is considered “empty”, alt hough it does have a bare minimum of contents. Speciﬁcally, an empty direc tory has two entries: one entry that refers to itself, and one entry t hat refers to its parent. The former is referred to as the “.” (dot) director y, and the latter as “..” (dot-dot). You can see these directories by passin g a ﬂag (-a) to the program ls: OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 15 TIP: BEWARY OFPOWERFUL COMMANDS The program rmprovides us with a great example of powerful com- mands, and how sometimes too much power can be a bad thing. For example, to remove a bunch of ﬁles at once, you can type something li ke: prompt> rm * where the *will match all ﬁles in the current directory. But sometimes you want to also delete the directories too, and in fact all of their contents. You can do this by telling rmto recursively descend into each directory, and remove its contents too: prompt> rm -rf * Where you get into trouble with this small string of characters i s when you issue the command, accidentally, from the root directory of a ﬁle sys- tem, thus removing every ﬁle and directory from it.",3074
39. Files and Directories,"Oops. Thus, remember the double-edged sword of powerful commands; whil e they give you the ability to do a lot of work with a small number of keystrokes, they also can quickly and readily do a great deal of harm. prompt> ls -a ./ ../ prompt> ls -al total 8 drwxr-x--- 2 remzi remzi 6 Apr 30 16:17 ./ drwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../ 39.12 Reading Directories Now that we’ve created a directory, we might wish to read one too. Indeed, that is exactly what the program lsdoes. Let’s write our own little tool like lsand see how it is done. Instead of just opening a directory as if it were a ﬁle, we instead use a new set of calls. Below is an example program that prints the cont ents of a directory. The program uses three calls, opendir() ,readdir() , andclosedir() , to get the job done, and you can see how simple the interface is; we just use a simple loop to read one directory entry at a time, and print out the name and inode number of each ﬁle in the directory . int main(int argc, char *argv[]) { DIR*dp = opendir(\"".\""); assert(dp .= NULL); struct dirent *d; while ((d = readdir(dp)) .= NULL) { printf(\"" percentlu  percents \"", (unsigned long) d->d_ino, d->d_name); } closedir(dp); return 0; } c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 INTERLUDE : FILES AND DIRECTORIES The declaration below shows the information available within eac h directory entry in the struct dirent data structure: struct dirent { char d_name[256]; / *filename */ ino_t d_ino; / *inode number */ off_t d_off; / *offset to the next dirent */ unsigned short d_reclen; / *length of this record */ unsigned char d_type; / *type of file */ }; Because directories are light on information (basically, just m apping the name to the inode number, along with a few other details), a pr ogram may want to call stat() on each ﬁle to get more information on each, such as its length or other detailed information. Indeed, this is exactly whatlsdoes when you pass it the -lﬂag; trystrace onlswith and without that ﬂag to see for yourself. 39.13 Deleting Directories Finally, you can delete a directory with a call to rmdir() (which is used by the program of the same name, rmdir ). Unlike ﬁle deletion, however, removing directories is more dangerous, as you could poten- tially delete a large amount of data with a single command. Thus, rmdir() has the requirement that the directory be empty (i.e., only has “.” and “..” entries) before it is deleted. If you try to delete a non-empty di rectory, the call tormdir() simply will fail. 39.14 Hard Links We now come back to the mystery of why removing a ﬁle is performed viaunlink() , by understanding a new way to make an entry in the ﬁle system tree, through a system call known as link() . Thelink() system call takes two arguments, an old pathname and a new one; w hen you “link” a new ﬁle name to an old one, you essentially create anoth er way to refer to the same ﬁle. The command-line program lnis used to do this, as we see in this example: prompt> echo hello > file prompt> cat file hello prompt> ln file file2 prompt> cat file2 hello Here we created a ﬁle with the word “hello” in it, and called the ﬁ le file2.",3177
39. Files and Directories,"We then create a hard link to that ﬁle using the lnprogram. After this, we can examine the ﬁle by either opening file orfile2 . 2Note how creative the authors of this book are. We also used to have a ca t named “Cat” (true story). However, she died, and we now have a hamster named “Hammy .” Update: Hammy is now dead too. The pet bodies are piling up. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 17 The waylink works is that it simply creates another name in the di- rectory you are creating the link to, and refers it to the same inode number (i.e., low-level name) of the original ﬁle. The ﬁle is not copied in any way; rather, you now just have two human names ( file andfile2 ) that both refer to the same ﬁle. We can even see this in the directory itse lf, by print- ing out the inode number of each ﬁle: prompt> ls -i file file2 67158084 file 67158084 file2 prompt> By passing the -iﬂag tols, it prints out the inode number of each ﬁle (as well as the ﬁle name). And thus you can see what link really h as done: just make a new reference to the same exact inode number (67158 084 in this example). By now you might be starting to see why unlink() is calledunlink() . When you create a ﬁle, you are really doing twothings. First, you are making a structure (the inode) that will track virtually all r elevant infor- mation about the ﬁle, including its size, where its blocks are on d isk, and so forth. Second, you are linking a human-readable name to that ﬁle, and putting that link into a directory. After creating a hard link to a ﬁle, to the ﬁle system, there is no dif- ference between the original ﬁle name ( file ) and the newly created ﬁle name (file2 ); indeed, they are both just links to the underlying meta- data about the ﬁle, which is found in inode number 67158084. Thus, to remove a ﬁle from the ﬁle system, we call unlink() . In the example above, we could for example remove the ﬁle named file , and still access the ﬁle without difﬁculty: prompt> rm file removed ‘file’ prompt> cat file2 hello The reason this works is because when the ﬁle system unlinks ﬁle , it checks a reference count within the inode number. This reference count (sometimes called the link count ) allows the ﬁle system to track how many different ﬁle names have been linked to this particular inode. When unlink() is called, it removes the “link” between the human-readable name (the ﬁle that is being deleted) to the given inode number, and decre- ments the reference count; only when the reference count reache s zero does the ﬁle system also free the inode and related data blocks, a nd thus truly “delete” the ﬁle. You can see the reference count of a ﬁle using stat() of course. Let’s see what it is when we create and delete hard links to a ﬁle. In t his exam- ple, we’ll create three links to the same ﬁle, and then delete t hem. Watch the link count. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 INTERLUDE : FILES AND DIRECTORIES prompt> echo hello > file prompt> stat file ...",3037
39. Files and Directories,"Inode: 67158084 Links: 1 ... prompt> ln file file2 prompt> stat file ... Inode: 67158084 Links: 2 ... prompt> stat file2 ... Inode: 67158084 Links: 2 ... prompt> ln file2 file3 prompt> stat file ... Inode: 67158084 Links: 3 ... prompt> rm file prompt> stat file2 ... Inode: 67158084 Links: 2 ... prompt> rm file2 prompt> stat file3 ... Inode: 67158084 Links: 1 ... prompt> rm file3 39.15 Symbolic Links There is one other type of link that is really useful, and it is cal led a symbolic link or sometimes a soft link . As it turns out, hard links are somewhat limited: you can’t create one to a directory (for fear that you will create a cycle in the directory tree); you can’t hard link to ﬁles in other disk partitions (because inode numbers are only unique wit hin a particular ﬁle system, not across ﬁle systems); etc. Thus, a ne w type of link called the symbolic link was created. To create such a link, you can use the same program ln, but with the -sﬂag. Here is an example: prompt> echo hello > file prompt> ln -s file file2 prompt> cat file2 hello As you can see, creating a soft link looks much the same, and the orig - inal ﬁle can now be accessed through the ﬁle name file as well as the symbolic link name file2 . However, beyond this surface similarity, symbolic links are ac tually quite different from hard links. The ﬁrst difference is that a symbolic link is actually a ﬁle itself, of a different type. We’ve alread y talked about regular ﬁles and directories; symbolic links are a third type t he ﬁle system knows about. A stat on the symlink reveals all: prompt> stat file ... regular file ... prompt> stat file2 ... symbolic link ... Runninglsalso reveals this fact. If you look closely at the ﬁrst char- acter of the long-form of the output from ls, you can see that the ﬁrst OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 19 character in the left-most column is a -for regular ﬁles, a dfor directo- ries, and an lfor soft links. You can also see the size of the symbolic link (4 bytes in this case), as well as what the link points to (the ﬁl e named file ). prompt> ls -al drwxr-x--- 2 remzi remzi 29 May 3 19:10 ./ drwxr-x--- 27 remzi remzi 4096 May 3 15:14 ../ -rw-r----- 1 remzi remzi 6 May 3 19:10 file lrwxrwxrwx 1 remzi remzi 4 May 3 19:10 file2 -> file The reason that file2 is 4 bytes is because the way a symbolic link is formed is by holding the pathname of the linked-to ﬁle as the data of the link ﬁle. Because we’ve linked to a ﬁle named file , our link ﬁle file2 is small (4 bytes). If we link to a longer pathname, our link ﬁle w ould be bigger: prompt> echo hello > alongerfilename prompt> ln -s alongerfilename file3 prompt> ls -al alongerfilename file3 -rw-r----- 1 remzi remzi 6 May 3 19:17 alongerfilename lrwxrwxrwx 1 remzi remzi 15 May 3 19:17 file3 -> alongerfilen ame Finally, because of the way symbolic links are created, they le ave the possibility for what is known as a dangling reference : prompt> echo hello > file prompt> ln -s file file2 prompt> cat file2 hello prompt> rm file prompt> cat file2 cat: file2: No such file or directory As you can see in this example, quite unlike hard links, removin g the original ﬁle named file causes the link to point to a pathname that no longer exists.",3284
39. Files and Directories,"39.16 Permission Bits And Access Control Lists The abstraction of a process provided two central virtualization s: of the CPU and of memory. Each of these gave the illusion to a process th at it had its own private CPU and its own private memory; in reality, the OS underneath used various techniques to share limited physica l resources among competing entities in a safe and secure manner. The ﬁle system also presents a virtual view of a disk, transform ing it from a bunch of raw blocks into much more user-friendly ﬁles and di - rectories, as described within this chapter. However, the abs traction is notably different from that of the CPU and memory, in that ﬁles are com- monly shared among different users and processes and are not (always) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 20 INTERLUDE : FILES AND DIRECTORIES private. Thus, a more comprehensive set of mechanisms for enabli ng var- ious degrees of sharing are usually present within ﬁle systems . The ﬁrst form of such mechanisms is the classic U NIXpermission bits . To see permissions for a ﬁle foo.txt , just type: prompt> ls -l foo.txt -rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt We’ll just pay attention to the ﬁrst part of this output, namely th e -rw-r--r-- . The ﬁrst character here just shows the type of the ﬁle: -for a regular ﬁle (which foo.txt is),dfor a directory, lfor a symbolic link, and so forth; this is (mostly) not related to permissions, so we’ll ignore it for now. We are interested in the permission bits, which are represent ed by the next nine characters ( rw-r--r-- ). These bits determine, for each regular ﬁle, directory, and other entities, exactly who can access it a nd how. The permissions consist of three groupings: what the owner of the ﬁle can do to it, what someone in a group can do to the ﬁle, and ﬁnally, what anyone (sometimes referred to as other ) can do. The abilities the owner, group member, or others can have include the ability to read the ﬁ le, write it, or execute it. In the example above, the ﬁrst three characters of the output of ls show that the ﬁle is both readable and writable by the owner ( rw-), and only readable by members of the group wheel and also by anyone else in the system ( r-- followed by r--). The owner of the ﬁle can readily change these permissions, for exa m- ple by using the chmod command (to change the ﬁle mode ). To remove the ability for anyone except the owner to access the ﬁle, you could type: prompt> chmod 600 foo.txt This command enables the readable bit (4) and writable bit (2) for the owner (OR’ing them together yields the 6 above), but set the group a nd other permission bits to 0 and 0, respectively, thus setting th e permissions torw------- . The execute bit is particularly interesting. For regular ﬁle s, its presence determines whether a program can be run or not. For example, if we h ave a simple shell script called hello.csh , we may wish to run it by typing: prompt> ./hello.csh hello, from shell world. However, if we don’t set the execute bit properly for this ﬁle, the f ol- lowing happens: prompt> chmod 600 hello.csh prompt> ./hello.csh ./hello.csh: Permission denied.",3170
39. Files and Directories,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 21 ASIDE : SUPERUSER FORFILESYSTEMS Which user is allowed to do privileged operations to help admini ster the ﬁle system? For example, if an inactive user’s ﬁles need to be de leted to save space, who has the rights to do so? On local ﬁle systems, the common default is for there to be some kind of superuser (i.e., root) who can access all ﬁles regardless of privileges. In a distributed ﬁle system such as AFS (which has access control l ists), a group called system:administrators contains users that are trusted to do so. In both cases, these trusted users represent an inhere nt secu- rity risk; if an attacker is able to somehow impersonate such a us er, the attacker can access all the information in the system, thus viol ating ex- pected privacy and protection guarantees. For directories, the execute bit behaves a bit differently. Spe ciﬁcally, it enables a user (or group, or everyone) to do things like change d i- rectories (i.e., cd) into the given directory, and, in combination with the writable bit, create ﬁles therein. The best way to learn more a bout this: play around with it yourself. Don’t worry, you (probably) won’t mess anything up too badly. Beyond permissions bits, some ﬁle systems, including the distr ibuted ﬁle system known as AFS (discussed in a later chapter), includ ing more sophisticated controls. AFS, for example, does this in the form of an ac- cess control list (ACL ) per directory. Access control lists are a more gen- eral and powerful way to represent exactly who can access a giv en re- source. In a ﬁle system, this enables a user to create a very spe ciﬁc list of who can and cannot read a set of ﬁles, in contrast to the somewhat li mited owner/group/everyone model of permissions bits described above. For example, here are the access controls for a private directory i n one author’s AFS account, as shown by the fs listacl command: prompt> fs listacl private Access list for private is Normal rights: system:administrators rlidwka remzi rlidwka The listing shows that both the system administrators and the us er remzi can lookup, insert, delete, and administer ﬁles in this direct ory, as well as read, write, and lock those ﬁles. To allow someone (in this case, the other author) to access to this d i- rectory, user remzi can just type the following command. prompt> fs setacl private/ andrea rl There goes remzi ’s privacy. But now you have learned an even more important lesson: there can be no secrets in a good marriage, even within the ﬁle system3. 3Married happily since 1996, if you were wondering. We know, you weren’ t. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 22 INTERLUDE : FILES AND DIRECTORIES TIP: BEWARY OFTOCTTOU In 1974, McPhee noticed a problem in computer systems. Speciﬁ- cally, McPhee noted that “... if there exists a time interval b etween a validity-check and the operation connected with that validit y-check, [and,] through multitasking, the validity-check variables can deliberately be changed during this time interval, resulting in an invali d operation be- ing performed by the control program.” We today call this the Time Of Check To Time Of Use (TOCTTOU ) problem, and alas, it still can occur.",3283
39. Files and Directories,"A simple example, as described by Bishop and Dilger [BD96], sh ows how a user can trick a more trusted service and thus cause trouble. I magine, for example, that a mail service runs as root (and thus has privil ege to access all ﬁles on a system). This service appends an incoming m essage to a user’s inbox ﬁle as follows. First, it calls lstat() to get informa- tion about the ﬁle, speciﬁcally ensuring that it is actually ju st a regular ﬁle owned by the target user, and not a link to another ﬁle that the mail server should not be updating. Then, after the check succeeds, the server updates the ﬁle with the new message. Unfortunately, the gap between the check and the update leads to a prob- lem: the attacker (in this case, the user who is receiving the mail, and thus has permissions to access the inbox) switches the inbox ﬁle (via a call torename() ) to point to a sensitive ﬁle such as /etc/passwd (which holds information about users and their passwords). If this switc h hap- pens at just the right time (between the check and the access) , the server will blithely update the sensitive ﬁle with the contents of the mail. The attacker can now write to the sensitive ﬁle by sending an email , an esca- lation in privilege; by updating /etc/passwd , the attacker can add an account with root privileges and thus gain control of the system. There are not any simple and great solutions to the TOCTTOU proble m [T+08]. One approach is to reduce the number of services that ne ed root privileges to run, which helps. The ONOFOLLOW ﬂag makes it so that open() will fail if the target is a symbolic link, thus avoiding attack s that require said links. More radicial approaches, such as usi ng a trans- actional ﬁle system [H+18], would solve the problem, there aren’t many transactional ﬁle systems in wide deployment. Thus, the usual (lame) advice: careful when you write code that runs with high privile ges. 39.17 Making And Mounting A File System We’ve now toured the basic interfaces to access ﬁles, directorie s, and certain types of special types of links. But there is one more topic we should discuss: how to assemble a full directory tree from many un der- lying ﬁle systems. This task is accomplished via ﬁrst making ﬁ le systems, and then mounting them to make their contents accessible. To make a ﬁle system, most ﬁle systems provide a tool, usually re- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 23 ferred to as mkfs (pronounced “make fs”), that performs exactly this task. The idea is as follows: give the tool, as input, a device (such as a d isk par- tition, e.g., /dev/sda1 ) and a ﬁle system type (e.g., ext3), and it simply writes an empty ﬁle system, starting with a root directory, onto t hat disk partition. And mkfs said, let there be a ﬁle system. However, once such a ﬁle system is created, it needs to be made ac - cessible within the uniform ﬁle-system tree. This task is ach ieved via the mount program (which makes the underlying system call mount() to do the real work). What mount does, quite simply is take an existing direc- tory as a target mount point and essentially paste a new ﬁle system onto the directory tree at that point.",3215
39. Files and Directories,"An example here might be useful. Imagine we have an unmounted ext3 ﬁle system, stored in device partition /dev/sda1 , that has the fol- lowing contents: a root directory which contains two sub-directori es,a andb, each of which in turn holds a single ﬁle named foo. Let’s say we wish to mount this ﬁle system at the mount point /home/users . We would type something like this: prompt> mount -t ext3 /dev/sda1 /home/users If successful, the mount would thus make this new ﬁle system ava il- able. However, note how the new ﬁle system is now accessed. To look at the contents of the root directory, we would use lslike this: prompt> ls /home/users/ a b As you can see, the pathname /home/users/ now refers to the root of the newly-mounted directory. Similarly, we could access direc toriesa andbwith the pathnames /home/users/a and/home/users/b . Fi- nally, the ﬁles named foo could be accessed via /home/users/a/foo and/home/users/b/foo . And thus the beauty of mount: instead of having a number of separate ﬁle systems, mount uniﬁes all ﬁle sy stems into one tree, making naming uniform and convenient. To see what is mounted on your system, and at which points, simply run themount program. You’ll see something like this: /dev/sda1 on / type ext3 (rw) proc on /proc type proc (rw) sysfs on /sys type sysfs (rw) /dev/sda5 on /tmp type ext3 (rw) /dev/sda7 on /var/vice/cache type ext3 (rw) tmpfs on /dev/shm type tmpfs (rw) AFS on /afs type afs (rw) This crazy mix shows that a whole number of different ﬁle systems , including ext3 (a standard disk-based ﬁle system), the proc ﬁ le system (a ﬁle system for accessing information about current processes), t mpfs (a ﬁle system just for temporary ﬁles), and AFS (a distributed ﬁle system) are all glued together onto this one machine’s ﬁle-system tree. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 24 INTERLUDE : FILES AND DIRECTORIES ASIDE : KEYFILESYSTEM TERMS •Aﬁleis an array of bytes which can be created, read, written, and deleted. It has a low-level name (i.e., a number) that refers t o it uniquely. The low-level name is often called an i-number . •Adirectory is a collection of tuples, each of which contains a human-readable name and low-level name to which it maps. Each entry refers either to another directory or to a ﬁle. Each direct ory also has a low-level name (i-number) itself. A directory alway s has two special entries: the .entry, which refers to itself, and the .. entry, which refers to its parent. •Adirectory tree ordirectory hierarchy organizes all ﬁles and direc- tories into a large tree, starting at the root. •To access a ﬁle, a process must use a system call (usually, open() ) to request permission from the operating system. If permission i s granted, the OS returns a ﬁle descriptor , which can then be used for read or write access, as permissions and intent allow. •Each ﬁle descriptor is a private, per-process entity, which re fers to an entry in the open ﬁle table . The entry therein tracks which ﬁle this access refers to, the current offset of the ﬁle (i.e., which part of the ﬁle the next read or write will access), and other relevant information. •Calls toread() andwrite() naturally update the current offset; otherwise, processes can use lseek() to change its value, enabling random access to different parts of the ﬁle.",3332
39. Files and Directories,"•To force updates to persistent media, a process must use fsync() or related calls. However, doing so correctly while maintaining high performance is challenging [P+14], so think carefully w hen doing so. •To have multiple human-readable names in the ﬁle system refe r to the same underlying ﬁle, use hard links orsymbolic links . Each is useful in different circumstances, so consider their stre ngths and weaknesses before usage. And remember, deleting a ﬁle is just per- forming that one last unlink() of it from the directory hierarchy. •Most ﬁle systems have mechanisms to enable and disable sharin g. A rudimentary form of such controls are provided by permissions bits; more sophisticated access control lists allow for more precise control over exactly who can access and manipulate information. 39.18 Summary The ﬁle system interface in U NIXsystems (and indeed, in any system) is seemingly quite rudimentary, but there is a lot to understa nd if you wish to master it. Nothing is better, of course, than simply usin g it (a lot). So please do so. Of course, read more; as always, Stevens [SR05] is th e place to begin. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG INTERLUDE : FILES AND DIRECTORIES 25 References [BD96] “Checking for Race Conditions in File Accesses” by Matt Bishop, Michae l Dilger. Com- puting Systems 9:2, 1996. A great description of the TOCTTOU problem and its presence in ﬁle systems. [CK+08] “The xv6 Operating System” by Russ Cox, Frans Kaashoek, Robe rt Morris, Nickolai Zeldovich. From: https://github.com/mit-pdos/xv6-public. As mentioned before, a cool and simple Unix implementation. We have been using an older version (2012-0 1-30-1-g1c41342) and hence some examples in the book may not match the latest in the source. [H+18] “TxFS: Leveraging File-System Crash Consistency to Provide A CID Transactions” by Y. Hu, Z. Zhu, I. Neal, Y. Kwon, T. Cheng, V . Chidambaram, E. Witchel. U SENIX ATC ’18, June 2018. The best paper at USENIX ATC ’18, and a good recent place to start to learn about transactional ﬁle systems. [K84] “Processes as Files” by Tom J. Killian. USENIX, June 1984. The paper that introduced the /proc ﬁle system, where each process can be treated as a ﬁle within a pseud o ﬁle system. A clever idea that you can still see in modern UNIXsystems. [L84] “Capability-Based Computer Systems” by Henry M. Levy. Dig ital Press, 1984. Available: http://homes.cs.washington.edu/˜levy/capabook. An excellent overview of early capability-based systems. [P+13] “Towards Efﬁcient, Portable Application-Level Consistency” by Thanumalayan S. Pil- lai, Vijay Chidambaram, Joo-Young Hwang, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci- Dusseau. HotDep ’13, November 2013. Our own work that shows how readily applications can make mistakes in committing data to disk; in particular, assumptions about the ﬁle sy stem creep into applications and thus make the applications work correctly only if they are runnin g on a speciﬁc ﬁle system. [P+14] “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications” by Thanumalayan S.",3110
39. Files and Directories,"Pillai, Vijay Chidambaram, Ramnat than Alagappan, Samer Al-Kiswany, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. OSDI ’14, Broom- ﬁeld, Colorado, October 2014. The full conference paper on this topic – with many more details and interesting tidbits than the ﬁrst workshop paper above. [SK09] “Principles of Computer System Design” by Jerome H. Saltzer and M. Frans Kaashoek. Morgan-Kaufmann, 2009. This tour de force of systems is a must-read for anybody interested in the ﬁeld. It’s how they teach systems at MIT. Read it once, and then read it a few more times to let it all soak in. [SR05] “Advanced Programming in the U NIXEnvironment” by W. Richard Stevens and Stephen A. Rago. Addison-Wesley, 2005. We have probably referenced this book a few hundred thousand times. It is that useful to you, if you care to become an awesome systems programm er. [T+08] “Portably Solving File TOCTTOU Races with Hardness Ampliﬁcatio n” by D. Tsafrir, T. Hertz, D. Wagner, D. Da Silva. FAST ’08, San Jose, California, 200 8.Not the paper that introduced TOCTTOU, but a recent-ish and well-done description of the problem and a w ay to solve the problem in a portable manner. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 26 INTERLUDE : FILES AND DIRECTORIES Homework (Code) In this homework, we’ll just familiarize ourselves with how the AP Is described in the chapter work. To do so, you’ll just write a few dif ferent programs, mostly based on various U NIXutilities. Questions 1.Stat: Write your own version of the command line program stat , which simply calls the stat() system call on a given ﬁle or directory. Print out ﬁle size, number of blocks allocated, reference (link) count, and s o forth. What is the link count of a directory, as the number of entries in the di rectory changes? Useful interfaces: stat() 2.List Files: Write a program that lists ﬁles in the given directory. When c alled without any arguments, the program should just print the ﬁle names. When invoked with the -lﬂag, the program should print out information about each ﬁle, such as the owner, group, permissions, and other infor mation ob- tained from the stat() system call. The program should take one addi- tional argument, which is the directory to read, e.g., myls -l directory . If no directory is given, the program should just use the current w orking di- rectory. Useful interfaces: stat() ,opendir() ,readdir() ,getcwd() . 3.Tail: Write a program that prints out the last few lines of a ﬁle. The p ro- gram should be efﬁcient, in that it seeks to near the end of the ﬁ le, reads in a block of data, and then goes backwards until it ﬁnds the reques ted num- ber of lines; at this point, it should print out those lines from beginning to the end of the ﬁle. To invoke the program, one should type: mytail -n file , wherenis the number of lines at the end of the ﬁle to print. Useful interfaces: stat() ,lseek() ,open() ,read() ,close() . 4.Recursive Search: Write a program that prints out the names of each ﬁle and directory in the ﬁle system tree, starting at a given poin t in the tree. For example, when run without arguments, the program should start with t he current working directory and print its contents, as well as t he contents of any sub-directories, etc., until the entire tree, root at the C WD, is printed. If given a single argument (of a directory name), use that as the root of the tree instead. Reﬁne your recursive search with more fun options, simil ar to the powerfulfind command line tool. Useful interfaces: you ﬁgure it out. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3602
40. File System Implementation,"40 File System Implementation In this chapter, we introduce a simple ﬁle system implementat ion, known asvsfs (the Very Simple File System ). This ﬁle system is a simpliﬁed version of a typical U NIX ﬁle system and thus serves to introduce some of the basic on-disk structures, access methods, and various poli cies that you will ﬁnd in many ﬁle systems today. The ﬁle system is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the ﬁle system work better (though we will want to pa y at- tention to device characteristics to make sure the ﬁle system works well). Because of the great ﬂexibility we have in building a ﬁle syste m, many different ones have been built, literally from AFS (the Andrew File Sys- tem) [H+88] to ZFS (Sun’s Zettabyte File System) [B07]. All of the se ﬁle systems have different data structures and do some things bet ter or worse than their peers. Thus, the way we will be learning about ﬁle sy stems is through case studies: ﬁrst, a simple ﬁle system (vsfs) in this chapter to introduce most concepts, and then a series of studies of real ﬁle sy stems to understand how they can differ in practice. THECRUX: HOWTOIMPLEMENT A S IMPLE FILESYSTEM How can we build a simple ﬁle system? What structures are neede d on the disk? What do they need to track? How are they accessed? 40.1 The Way To Think To think about ﬁle systems, we usually suggest thinking about t wo different aspects of them; if you understand both of these aspect s, you probably understand how the ﬁle system basically works. The ﬁrst is the data structures of the ﬁle system. In other words, what types of on-disk structures are utilized by the ﬁle system to org anize its data and metadata? The ﬁrst ﬁle systems we’ll see (including v sfs below) employ simple structures, like arrays of blocks or other objects, w hereas 1 2 FILESYSTEM IMPLEMENTATION ASIDE : M ENTAL MODELS OFFILESYSTEMS As we’ve discussed before, mental models are what you are really t rying to develop when learning about systems. For ﬁle systems, your men tal model should eventually include answers to questions like: wha t on-disk structures store the ﬁle system’s data and metadata? What happ ens when a process opens a ﬁle? Which on-disk structures are accessed dur ing a read or write? By working on and improving your mental model, you develop an abstract understanding of what is going on, instead of j ust trying to understand the speciﬁcs of some ﬁle-system code (thoug h that is also useful, of course.). more sophisticated ﬁle systems, like SGI’s XFS, use more complicate d tree-based structures [S+96]. The second aspect of a ﬁle system is its access methods . How does it map the calls made by a process, such as open() ,read() ,write() , etc., onto its structures? Which structures are read during t he execution of a particular system call? Which are written? How efﬁciently are all of these steps performed? If you understand the data structures and access methods of a ﬁle sys- tem, you have developed a good mental model of how it truly works, a key part of the systems mindset.",3149
40. File System Implementation,"Try to work on developing your ment al model as we delve into our ﬁrst implementation. 40.2 Overall Organization We now develop the overall on-disk organization of the data struc- tures of the vsfs ﬁle system. The ﬁrst thing we’ll need to do is di vide the disk into blocks ; simple ﬁle systems use just one block size, and that’s exactly what we’ll do here. Let’s choose a commonly-used size of 4 KB. Thus, our view of the disk partition where we’re building our ﬁle sy s- tem is simple: a series of blocks, each of size 4 KB. The blocks are a d- dressed from 0 to N−1, in a partition of size N4-KB blocks. Assume we have a really small disk, with just 64 blocks: 0 78 1516 2324 31 32 3940 4748 5556 63 Let’s now think about what we need to store in these blocks to build a ﬁle system. Of course, the ﬁrst thing that comes to mind is user data. In fact, most of the space in any ﬁle system is (and should be) user data. Let’s call the region of the disk we use for user data the data region , and, OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 3 again for simplicity, reserve a ﬁxed portion of the disk for these b locks, say the last 56 of 64 blocks on the disk: 0 7D 8DDDDDDD 15D 16DDDDDDD 23D 24DDDDDDD 31 D 32DDDDDDD 39D 40DDDDDDD 47D 48DDDDDDD 55D 56DDDDDDD 63Data Region Data Region As we learned about (a little) last chapter, the ﬁle system has to track information about each ﬁle. This information is a key piece of metadata , and tracks things like which data blocks (in the data region) com prise a ﬁle, the size of the ﬁle, its owner and access rights, access and modify times, and other similar kinds of information. To store this inform ation, ﬁle systems usually have a structure called an inode (we’ll read more about inodes below). To accommodate inodes, we’ll need to reserve some space on the disk for them as well. Let’s call this portion of the disk the inode table , which simply holds an array of on-disk inodes. Thus, our on-disk image now looks like this picture, assuming that we use 5 of our 64 blocks for in odes (denoted by I’s in the diagram): 0IIIII 7D 8DDDDDDD 15D 16DDDDDDD 23D 24DDDDDDD 31 D 32DDDDDDD 39D 40DDDDDDD 47D 48DDDDDDD 55D 56DDDDDDD 63Data Region Data RegionInodes We should note here that inodes are typically not that big, for exam ple 128 or 256 bytes. Assuming 256 bytes per inode, a 4-KB block can hol d 16 inodes, and our ﬁle system above contains 80 total inodes. In our simp le ﬁle system, built on a tiny 64-block partition, this number repr esents the maximum number of ﬁles we can have in our ﬁle system; however, do note that the same ﬁle system, built on a larger disk, could simpl y allocate a larger inode table and thus accommodate more ﬁles. Our ﬁle system thus far has data blocks (D), and inodes (I), but a few things are still missing. One primary component that is still n eeded, as you might have guessed, is some way to track whether inodes or data blocks are free or allocated. Such allocation structures are thus a requisite element in any ﬁle system. Many allocation-tracking methods are possible, of course.",3108
40. File System Implementation,"For exam - ple, we could use a free list that points to the ﬁrst free block, which then points to the next free block, and so forth. We instead choose a simp le and popular structure known as a bitmap , one for the data region (the data bitmap ), and one for the inode table (the inode bitmap ). A bitmap is a c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 FILESYSTEM IMPLEMENTATION simple structure: each bit is used to indicate whether the cor responding object/block is free (0) or in-use (1). And thus our new on-disk lay out, with an inode bitmap (i) and a data bitmap (d): 0idIIIII 7D 8DDDDDDD 15D 16DDDDDDD 23D 24DDDDDDD 31 D 32DDDDDDD 39D 40DDDDDDD 47D 48DDDDDDD 55D 56DDDDDDD 63Data Region Data RegionInodes You may notice that it is a bit of overkill to use an entire 4-KB block for these bitmaps; such a bitmap can track whether 32K objects are allocated, and yet we only have 80 inodes and 56 data blocks. However, we just u se an entire 4-KB block for each of these bitmaps for simplicity. The careful reader (i.e., the reader who is still awake) may h ave no- ticed there is one block left in the design of the on-disk structur e of our very simple ﬁle system. We reserve this for the superblock , denoted by an S in the diagram below. The superblock contains information abou t this particular ﬁle system, including, for example, how many i nodes and data blocks are in the ﬁle system (80 and 56, respectively in th is instance), where the inode table begins (block 3), and so forth. It will like ly also include a magic number of some kind to identify the ﬁle system ty pe (in this case, vsfs). S 0idIIIII 7D 8DDDDDDD 15D 16DDDDDDD 23D 24DDDDDDD 31 D 32DDDDDDD 39D 40DDDDDDD 47D 48DDDDDDD 55D 56DDDDDDD 63Data Region Data RegionInodes Thus, when mounting a ﬁle system, the operating system will rea d the superblock ﬁrst, to initialize various parameters, and th en attach the volume to the ﬁle-system tree. When ﬁles within the volume are a ccessed, the system will thus know exactly where to look for the needed on-di sk structures. 40.3 File Organization: The Inode One of the most important on-disk structures of a ﬁle system is the inode ; virtually all ﬁle systems have a structure similar to this. The name inode is short for index node , the historical name given to it in U NIX [RT74] and possibly earlier systems, used because these nodes were orig- inally arranged in an array, and the array indexed into when accessing a particular inode. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 5 ASIDE : DATA STRUCTURE — T HEINODE The inode is the generic name that is used in many ﬁle systems to de- scribe the structure that holds the metadata for a given ﬁle, su ch as its length, permissions, and the location of its constituent blocks. T he name goes back at least as far as U NIX (and probably further back to Multics if not earlier systems); it is short for index node , as the inode number is used to index into an array of on-disk inodes in order to ﬁnd the inod e of that number. As we’ll see, design of the inode is one key part of ﬁle system design.",3120
40. File System Implementation,"Most modern systems have some kind of structure li ke this for every ﬁle they track, but perhaps call them different things (such as dnodes, fnodes, etc.). Each inode is implicitly referred to by a number (called the i-number ), which we’ve earlier called the low-level name of the ﬁle. In vsfs (and other simple ﬁle systems), given an i-number, you should direct ly be able to calculate where on the disk the corresponding inode is located. For ex- ample, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks) and thus consisting of 80 inodes (assuming each inode is 256 bytes ); fur- ther assume that the inode region starts at 12KB (i.e, the super block starts at 0KB, the inode bitmap is at address 4KB, the data bitmap at 8K B, and thus the inode table comes right after). In vsfs, we thus have th e following layout for the beginning of the ﬁle system partition (in closeup vi ew): Super i-bmap d-bmap 0KB 4KB 8KB 12KB 16KB 20KB 24KB 28KB 32KBThe Inode Table (Closeup) 0123 4567 891011 1213141516171819 20212223 24252627 2829303132333435 36373839 40414243 4445464748495051 52535455 56575859 6061626364656667 68697071 72737475 76777879iblock 0 iblock 1 iblock 2 iblock 3 iblock 4 To read inode number 32, the ﬁle system would ﬁrst calculate the off- set into the inode region ( 32·sizeof(inode)or8192 ), add it to the start address of the inode table on disk ( inodeStartAddr =12KB), and thus arrive upon the correct byte address of the desired block of inodes: 20KB. Recall that disks are not byte addressable, but rather consist of a large number of addressable sectors, usually 512 bytes. Thus, to fet ch the block of inodes that contains inode 32, the ﬁle system would issue a read t o sec- tor20×1024 512, or 40, to fetch the desired inode block. More generally, the sector address sector of the inode block can be calculated as follows: blk = (inumber *sizeof(inode_t)) / blockSize; sector = ((blk *blockSize) + inodeStartAddr) / sectorSize; Inside each inode is virtually all of the information you need about a ﬁle: its type (e.g., regular ﬁle, directory, etc.), its size, the number of blocks allocated to it, protection information (such as who owns the ﬁle, as well c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 FILESYSTEM IMPLEMENTATION Size Name What is this inode ﬁeld for? 2 mode can this ﬁle be read/written/executed? 2 uid who owns this ﬁle? 4 size how many bytes are in this ﬁle? 4 time what time was this ﬁle last accessed? 4 ctime what time was this ﬁle created? 4 mtime what time was this ﬁle last modiﬁed? 4 dtime what time was this inode deleted? 2 gid which group does this ﬁle belong to? 2 links count how many hard links are there to this ﬁle? 4 blocks how many blocks have been allocated to this ﬁle? 4 ﬂags how should ext2 use this inode? 4 osd1 an OS-dependent ﬁeld 60 block a set of disk pointers (15 total) 4 generation ﬁle version (used by NFS) 4 ﬁle acl a new permissions model beyond mode bits 4 dir acl called access control lists Figure 40.1: Simpliﬁed Ext2 Inode as who can access it), some time information, including when the ﬁle was created, modiﬁed, or last accessed, as well as information about w here its data blocks reside on disk (e.g., pointers of some kind).",3239
40. File System Implementation,"We refer t o all such information about a ﬁle as metadata ; in fact, any information inside the ﬁle system that isn’t pure user data is often referred to as s uch. An example inode from ext2 [P09] is shown in Figure 40.11. One of the most important decisions in the design of the inode is how it refers to where data blocks are. One simple approach would be t o have one or more direct pointers (disk addresses) inside the inode; each pointer refers to one disk block that belongs to the ﬁle. Such an app roach is limited: for example, if you want to have a ﬁle that is really b ig (e.g., bigger than the block size multiplied by the number of direct poi nters in the inode), you are out of luck. The Multi-Level Index To support bigger ﬁles, ﬁle system designers have had to introd uce dif- ferent structures within inodes. One common idea is to have a spe cial pointer known as an indirect pointer . Instead of pointing to a block that contains user data, it points to a block that contains more pointers , each of which point to user data. Thus, an inode may have some ﬁxed numbe r of direct pointers (e.g., 12), and a single indirect pointer. If a ﬁle grows large enough, an indirect block is allocated (from the data-block region of the disk), and the inode’s slot for an indirect pointer is set to poin t to it. Assuming 4-KB blocks and 4-byte disk addresses, that adds anot her 1024 pointers; the ﬁle can grow to be (12+1024) ·4Kor 4144KB. 1Type info is kept in the directory entry, and thus is not found in the inode it self. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 7 TIP: CONSIDER EXTENT -BASED APPROACHES A different approach is to use extents instead of pointers. An extent is simply a disk pointer plus a length (in blocks); thus, instead of requiring a pointer for every block of a ﬁle, all one needs is a pointer and a leng th to specify the on-disk location of a ﬁle. Just a single extent is li miting, as one may have trouble ﬁnding a contiguous chunk of on-disk free space when allocating a ﬁle. Thus, extent-based ﬁle systems often al low for more than one extent, thus giving more freedom to the ﬁle system du ring ﬁle allocation. In comparing the two approaches, pointer-based approaches are t he most ﬂexible but use a large amount of metadata per ﬁle (particularl y for large ﬁles). Extent-based approaches are less ﬂexible but more compa ct; in par- ticular, they work well when there is enough free space on the dis k and ﬁles can be laid out contiguously (which is the goal for virtually a ny ﬁle allocation policy anyhow). Not surprisingly, in such an approach, you might want to support even larger ﬁles. To do so, just add another pointer to the inode: t hedou- ble indirect pointer . This pointer refers to a block that contains pointers to indirect blocks, each of which contain pointers to data blocks. A dou- ble indirect block thus adds the possibility to grow ﬁles with an additional 1024·1024 or 1-million 4KB blocks, in other words supporting ﬁles that are over 4GB in size.",3044
40. File System Implementation,"You may want even more, though, and we bet you know where this is headed: the triple indirect pointer . Overall, this imbalanced tree is referred to as the multi-level index ap- proach to pointing to ﬁle blocks. Let’s examine an example with tw elve direct pointers, as well as both a single and a double indirect bl ock. As- suming a block size of 4 KB, and 4-byte pointers, this structure c an accom- modate a ﬁle of just over 4 GB in size (i.e., (12+1024+10242)×4KB). Can you ﬁgure out how big of a ﬁle can be handled with the addition of a triple-indirect block? (hint: pretty big) Many ﬁle systems use a multi-level index, including commonly- used ﬁle systems such as Linux ext2 [P09] and ext3, NetApp’s WAFL, a s well as the original U NIXﬁle system. Other ﬁle systems, including SGI XFS and Linux ext4, use extents instead of simple pointers; see the earlier aside for details on how extent-based schemes work (they are akin to segme nts in the discussion of virtual memory). You might be wondering: why use an imbalanced tree like this? Wh y not a different approach? Well, as it turns out, many researcher s have studied ﬁle systems and how they are used, and virtually every time they ﬁnd certain “truths” that hold across the decades. One such ﬁnd ing is that most ﬁles are small . This imbalanced design reﬂects such a reality; if most ﬁles are indeed small, it makes sense to optimize for this ca se. Thus, with a small number of direct pointers (12 is a typical number), an inode c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 FILESYSTEM IMPLEMENTATION Most ﬁles are small ˜2K is the most common size Average ﬁle size is growing Almost 200K is the average Most bytes are stored in large ﬁles A few big ﬁles use most of space File systems contains lots of ﬁles Almost 100K on average File systems are roughly half full Even as disks grow, ﬁle systems remain ˜50 percent full Directories are typically small Many have few entries; most have 20 or fewer Figure 40.2: File System Measurement Summary can directly point to 48 KB of data, needing one (or more) indirect b locks for larger ﬁles. See Agrawal et. al [A+07] for a recent-ish study ; Figure 40.2 summarizes those results. Of course, in the space of inode design, many other possibilities e x- ist; after all, the inode is just a data structure, and any data structure that stores the relevant information, and can query it effectively, is sufﬁcient. As ﬁle system software is readily changed, you should be willing to ex- plore different designs should workloads or technologies change. 40.4 Directory Organization In vsfs (as in many ﬁle systems), directories have a simple orga niza- tion; a directory basically just contains a list of (entry name, i node num- ber) pairs. For each ﬁle or directory in a given directory, there i s a string and a number in the data block(s) of the directory. For each string , there may also be a length (assuming variable-sized names). For example, assume a directory dir (inode number 5) has three ﬁles in it (foo,bar, andfoobarisaprettylongname ), with inode num- bers 12, 13, and 24 respectively. The on-disk data for dir might look like: inum | reclen | strlen | name 5 12 2 .",3198
40. File System Implementation,"2 12 3 .. 12 12 4 foo 13 12 4 bar 24 36 28 foobar_is_a_pretty_longname In this example, each entry has an inode number, record length ( the total bytes for the name plus any left over space), string length (the actual length of the name), and ﬁnally the name of the entry. Note that ea ch di- rectory has two extra entries, .“dot” and ..“dot-dot”; the dot directory is just the current directory (in this example, dir), whereas dot-dot is the parent directory (in this case, the root). Deleting a ﬁle (e.g., calling unlink() ) can leave an empty space in the middle of the directory, and hence there should be some way to m ark that as well (e.g., with a reserved inode number such as zero). Su ch a delete is one reason the record length is used: a new entry may reu se an old, bigger entry and thus have extra space within. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 9 ASIDE : LINKED -BASED APPROACHES Another simpler approach in designing inodes is to use a linked list . Thus, inside an inode, instead of having multiple pointers, you j ust need one, to point to the ﬁrst block of the ﬁle. To handle larger ﬁles, ad d an- other pointer at the end of that data block, and so on, and thus you can support large ﬁles. As you might have guessed, linked ﬁle allocation performs poorly for some workloads; think about reading the last block of a ﬁle, for examp le, or just doing random access. Thus, to make linked allocation work be tter, some systems will keep an in-memory table of link information, ins tead of storing the next pointers with the data blocks themselves. The table is indexed by the address of a data block D; the content of an entry is simplyD’s next pointer, i.e., the address of the next block in a ﬁle which followsD. A null-value could be there too (indicating an end-of-ﬁle), or some other marker to indicate that a particular block is free. Ha ving such a table of next pointers makes it so that a linked allocation schem e can effectively do random ﬁle accesses, simply by ﬁrst scanning t hrough the (in memory) table to ﬁnd the desired block, and then accessing ( on disk) it directly. Does such a table sound familiar? What we have described is the b asic structure of what is known as the ﬁle allocation table , orFAT ﬁle system. Yes, this classic old Windows ﬁle system, before NTFS [C94], is b ased on a simple linked-based allocation scheme. There are other differ ences from a standard U NIXﬁle system too; for example, there are no inodes per se, but rather directory entries which store metadata about a ﬁle an d refer directly to the ﬁrst block of said ﬁle, which makes creating har d links impossible. See Brouwer [B02] for more of the inelegant details. You might be wondering where exactly directories are stored. Oft en, ﬁle systems treat directories as a special type of ﬁle. Thus, a d irectory has an inode, somewhere in the inode table (with the type ﬁeld of the in ode marked as “directory” instead of “regular ﬁle”). The directory has data blocks pointed to by the inode (and perhaps, indirect blocks); th ese data blocks live in the data block region of our simple ﬁle system. Our on- disk structure thus remains unchanged.",3199
40. File System Implementation,"We should also note again that this simple linear list of director y en- tries is not the only way to store such information. As before, any da ta structure is possible. For example, XFS [S+96] stores directorie s in B-tree form, making ﬁle create operations (which have to ensure that a ﬁ le name has not been used before creating it) faster than systems with s imple lists that must be scanned in their entirety. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 FILESYSTEM IMPLEMENTATION ASIDE : FREE SPACE MANAGEMENT There are many ways to manage free space; bitmaps are just one w ay. Some early ﬁle systems used free lists , where a single pointer in the super block was kept to point to the ﬁrst free block; inside that block th e next free pointer was kept, thus forming a list through the free blocks of the system. When a block was needed, the head block was used and the l ist updated accordingly. Modern ﬁle systems use more sophisticated data structures. For e xample, SGI’s XFS [S+96] uses some form of a B-tree to compactly represent which chunks of the disk are free. As with any data structure, differ ent time- space trade-offs are possible. 40.5 Free Space Management A ﬁle system must track which inodes and data blocks are free, an d which are not, so that when a new ﬁle or directory is allocated, it c an ﬁnd space for it. Thus free space management is important for all ﬁle systems. In vsfs, we have two simple bitmaps for this task. For example, when we create a ﬁle, we will have to allocate an inod e for that ﬁle. The ﬁle system will thus search through the bitmap for an in- ode that is free, and allocate it to the ﬁle; the ﬁle system will h ave to mark the inode as used (with a 1) and eventually update the on-disk bi tmap with the correct information. A similar set of activities take pl ace when a data block is allocated. Some other considerations might also come into play when allocating data blocks for a new ﬁle. For example, some Linux ﬁle systems, suc h as ext2 and ext3, will look for a sequence of blocks (say 8) that are f ree when a new ﬁle is created and needs data blocks; by ﬁnding such a se- quence of free blocks, and then allocating them to the newly-cre ated ﬁle, the ﬁle system guarantees that a portion of the ﬁle will be contigu ous on the disk, thus improving performance. Such a pre-allocation policy is thus a commonly-used heuristic when allocating space for data bl ocks. 40.6 Access Paths: Reading and Writing Now that we have some idea of how ﬁles and directories are stored on disk, we should be able to follow the ﬂow of operation during the activ ity of reading or writing a ﬁle. Understanding what happens on this access path is thus the second key in developing an understanding of how a ﬁle system works; pay attention. For the following examples, let us assume that the ﬁle system has been mounted and thus that the superblock is already in memory. Every thing else (i.e., inodes, directories) is still on the disk. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 11 data inode root foo bar root foo bar bar bar bitmap bitmap inode inode inode data data data data data [0] [1] [2] read read open(bar) read read read read read() read write read read() read write read read() read write Figure 40.3: File Read Timeline (Time Increasing Downward) Reading A File From Disk In this simple example, let us ﬁrst assume that you want to simp ly open a ﬁle (e.g., /foo/bar ), read it, and then close it.",3499
40. File System Implementation,"For this simple example, let’s assume the ﬁle is just 12KB in size (i.e., 3 blocks). When you issue an open(\""/foo/bar\"", O RDONLY) call, the ﬁle sys- tem ﬁrst needs to ﬁnd the inode for the ﬁle bar, to obtain some basic in- formation about the ﬁle (permissions information, ﬁle size, etc.) . To do so, the ﬁle system must be able to ﬁnd the inode, but all it has right now is the full pathname. The ﬁle system must traverse the pathname and thus locate the desired inode. All traversals begin at the root of the ﬁle system, in the root directory which is simply called /. Thus, the ﬁrst thing the FS will read from disk is the inode of the root directory. But where is this inode? To ﬁnd an inode, we must know its i-number. Usually, we ﬁnd the i-number of a ﬁle or directory in its parent directory; the root has no parent (by deﬁ nition). Thus, the root inode number must be “well known”; the FS must know what it is when the ﬁle system is mounted. In most U NIX ﬁle systems, the root inode number is 2. Thus, to begin the process, the FS reads in the block that contains inode number 2 (the ﬁrst inode block). Once the inode is read in, the FS can look inside of it to ﬁnd pointers to data blocks, which contain the contents of the root directory. The FS will thus use these on-disk pointers to read through the directory, in this case looking for an entry for foo. By reading in one or more directory data blocks, it will ﬁnd the entry for foo; once found, the FS will also hav e found the inode number of foo (say it is 44) which it will need next. The next step is to recursively traverse the pathname until t he desired inode is found. In this example, the FS reads the block containing the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 FILESYSTEM IMPLEMENTATION ASIDE : READS DON’TACCESS ALLOCATION STRUCTURES We’ve seen many students get confused by allocation structures s uch as bitmaps. In particular, many often think that when you are simp ly read- ing a ﬁle, and not allocating any new blocks, that the bitmap will still be consulted. This is not true. Allocation structures, such as bi tmaps, are only accessed when allocation is needed. The inodes, director ies, and indirect blocks have all the information they need to complete a r ead re- quest; there is no need to make sure a block is allocated when the inode already points to it. inode offoo and then its directory data, ﬁnally ﬁnding the inode number ofbar. The ﬁnal step of open() is to readbar’s inode into memory; the FS then does a ﬁnal permissions check, allocates a ﬁle descriptor for this process in the per-process open-ﬁle table, and returns it to the user. Once open, the program can then issue a read() system call to read from the ﬁle. The ﬁrst read (at offset 0 unless lseek() has been called) will thus read in the ﬁrst block of the ﬁle, consulting the inode to ﬁnd the location of such a block; it may also update the inode with a new l ast- accessed time. The read will further update the in-memory open ﬁle table for this ﬁle descriptor, updating the ﬁle offset such that the ne xt read will read the second ﬁle block, etc.",3130
40. File System Implementation,"At some point, the ﬁle will be closed. There is much less work to be done here; clearly, the ﬁle descriptor should be deallocated, bu t for now, that is all the FS really needs to do. No disk I/Os take place. A depiction of this entire process is found in Figure 40.3 (page 11 ); time increases downward in the ﬁgure. In the ﬁgure, the open cau ses numerous reads to take place in order to ﬁnally locate the inode of t he ﬁle. Afterwards, reading each block requires the ﬁle system to ﬁrs t consult the inode, then read the block, and then update the inode’s last-acce ssed-time ﬁeld with a write. Spend some time and understand what is going on. Also note that the amount of I/O generated by the open is propor- tional to the length of the pathname. For each additional director y in the path, we have to read its inode as well as its data. Making this w orse would be the presence of large directories; here, we only have to r ead one block to get the contents of a directory, whereas with a large dire ctory, we might have to read many data blocks to ﬁnd the desired entry. Ye s, life can get pretty bad when reading a ﬁle; as you’re about to ﬁnd out, wr iting out a ﬁle (and especially, creating a new one) is even worse. Writing A File To Disk Writing to a ﬁle is a similar process. First, the ﬁle must be open ed (as above). Then, the application can issue write() calls to update the ﬁle with new contents. Finally, the ﬁle is closed. Unlike reading, writing to the ﬁle may also allocate a block (unless the block is being overwritten, for example). When writing out a n ew ﬁle, each write not only has to write data to disk but has to ﬁrst d ecide OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 13 data inode root foo bar root foo bar bar bar bitmap bitmap inode inode inode data data data data data [0] [1] [2] read read read read create read (/foo/bar) write write read write write read read write() write write write read read write() write write write read read write() write write write Figure 40.4: File Creation Timeline (Time Increasing Downward) which block to allocate to the ﬁle and thus update other structur es of the disk accordingly (e.g., the data bitmap and inode). Thus, each write to a ﬁle logically generates ﬁve I/Os: one to read the data bitmap (w hich is then updated to mark the newly-allocated block as used), one to w rite the bitmap (to reﬂect its new state to disk), two more to read and th en write the inode (which is updated with the new block’s location), and ﬁna lly one to write the actual block itself. The amount of write trafﬁc is even worse when one considers a sim- ple and common operation such as ﬁle creation. To create a ﬁle, the ﬁ le system must not only allocate an inode, but also allocate space wit hin the directory containing the new ﬁle. The total amount of I/O trafﬁ c to do so is quite high: one read to the inode bitmap (to ﬁnd a free inod e), one write to the inode bitmap (to mark it allocated), one write to t he new inode itself (to initialize it), one to the data of the directory ( to link the high-level name of the ﬁle to its inode number), and one read and w rite to the directory inode to update it. If the directory needs to grow to ac- commodate the new entry, additional I/Os (i.e., to the data bitm ap, and the new directory block) will be needed too.",3353
40. File System Implementation,"All that just to creat e a ﬁle. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 FILESYSTEM IMPLEMENTATION Let’s look at a speciﬁc example, where the ﬁle /foo/bar is created, and three blocks are written to it. Figure 40.4 (page 13) shows w hat hap- pens during the open() (which creates the ﬁle) and during each of three 4KB writes. In the ﬁgure, reads and writes to the disk are grouped under whi ch system call caused them to occur, and the rough ordering they mig ht take place in goes from top to bottom of the ﬁgure. You can see how much work it is to create the ﬁle: 10 I/Os in this case, to walk the pat hname and then ﬁnally create the ﬁle. You can also see that each alloca ting write costs 5 I/Os: a pair to read and update the inode, another pair to r ead and update the data bitmap, and then ﬁnally the write of the dat a itself. How can a ﬁle system accomplish any of this with reasonable efﬁcie ncy? THECRUX: HOWTOREDUCE FILESYSTEM I/O C OSTS Even the simplest of operations like opening, reading, or writing a ﬁle incurs a huge number of I/O operations, scattered over the disk. W hat can a ﬁle system do to reduce the high costs of doing so many I/Os? 40.7 Caching and Buffering As the examples above show, reading and writing ﬁles can be expe n- sive, incurring many I/Os to the (slow) disk. To remedy what wou ld clearly be a huge performance problem, most ﬁle systems aggress ively use system memory (DRAM) to cache important blocks. Imagine the open example above: without caching, every ﬁle open would require at least two reads for every level in the directory hierarchy (one to read the inode of the directory in question, and at least one t o read its data). With a long pathname (e.g., /1/2/3/ ... /100/ﬁle.t xt), the ﬁle system would literally perform hundreds of reads just to open the ﬁle. Early ﬁle systems thus introduced a ﬁxed-size cache to hold popular blocks. As in our discussion of virtual memory, strategies such as LRU and different variants would decide which blocks to keep in cac he. This ﬁxed-size cache would usually be allocated at boot time to be rough ly 10 percent of total memory. This static partitioning of memory, however, can be wasteful; what if the ﬁle system doesn’t need 10 percent of memory at a given point in time? With the ﬁxed-size approach described above, unused pages in t he ﬁle cache cannot be re-purposed for some other use, and thus go to waste . Modern systems, in contrast, employ a dynamic partitioning approach. Speciﬁcally, many modern operating systems integrate virtual memory pages and ﬁle system pages into a uniﬁed page cache [S00]. In this way, memory can be allocated more ﬂexibly across virtual memory and ﬁle system, depending on which needs more memory at a given time. Now imagine the ﬁle open example with caching. The ﬁrst open may generate a lot of I/O trafﬁc to read in directory inode and data, bu t sub- OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 15 TIP: UNDERSTAND STATIC VS. DYNAMIC PARTITIONING When dividing a resource among different clients/users, you ca n use either static partitioning ordynamic partitioning . The static approach simply divides the resource into ﬁxed proportions once; for exampl e, if there are two possible users of memory, you can give some ﬁxed fract ion of memory to one user, and the rest to the other. The dynamic approac h is more ﬂexible, giving out differing amounts of the resource over t ime; for example, one user may get a higher percentage of disk bandwid th for a period of time, but then later, the system may switch and decid e to give a different user a larger fraction of available disk bandwidth .",3677
40. File System Implementation,"Each approach has its advantages. Static partitioning ensures each user receives some share of the resource, usually delivers more predi ctable performance, and is often easier to implement. Dynamic partit ioning can achieve better utilization (by letting resource-hungry user s consume oth- erwise idle resources), but can be more complex to implement, an d can lead to worse performance for users whose idle resources get consum ed by others and then take a long time to reclaim when needed. As is of - ten the case, there is no best method; rather, you should think ab out the problem at hand and decide which approach is most suitable. Inde ed, shouldn’t you always be doing that? sequent ﬁle opens of that same ﬁle (or ﬁles in the same directory) w ill mostly hit in the cache and thus no I/O is needed. Let us also consider the effect of caching on writes. Whereas rea d I/O can be avoided altogether with a sufﬁciently large cache, writ e trafﬁc has to go to disk in order to become persistent. Thus, a cache does not s erve as the same kind of ﬁlter on write trafﬁc that it does for reads. Tha t said, write buffering (as it is sometimes called) certainly has a number of per- formance beneﬁts. First, by delaying writes, the ﬁle system c anbatch some updates into a smaller set of I/Os; for example, if an inode bi tmap is updated when one ﬁle is created and then updated moments late r as another ﬁle is created, the ﬁle system saves an I/O by delaying the write after the ﬁrst update. Second, by buffering a number of writes in memory, the system can then schedule the subsequent I/Os and thus increase per- formance. Finally, some writes are avoided altogether by delayi ng them; for example, if an application creates a ﬁle and then deletes it , delaying the writes to reﬂect the ﬁle creation to disk avoids them entirely. In this case, laziness (in writing blocks to disk) is a virtue. For the reasons above, most modern ﬁle systems buffer writes in mem - ory for anywhere between ﬁve and thirty seconds, representing y et an- other trade-off: if the system crashes before the updates have b een prop- agated to disk, the updates are lost; however, by keeping write s in mem- ory longer, performance can be improved by batching, scheduling , and even avoiding writes. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 FILESYSTEM IMPLEMENTATION TIP: UNDERSTAND THEDURABILITY /PERFORMANCE TRADE -OFF Storage systems often present a durability/performance trade- off to users. If the user wishes data that is written to be immediate ly durable, the system must go through the full effort of committing the newly - written data to disk, and thus the write is slow (but safe). Howe ver, if the user can tolerate the loss of a little data, the system can buf fer writes in memory for some time and write them later to the disk (in the bac k- ground). Doing so makes writes appear to complete quickly, thus im- proving perceived performance; however, if a crash occurs, writ es not yet committed to disk will be lost, and hence the trade-off.",3059
40. File System Implementation,"To un derstand how to make this trade-off properly, it is best to understand wha t the ap- plication using the storage system requires; for example, whil e it may be tolerable to lose the last few images downloaded by your web browser , losing part of a database transaction that is adding money to your b ank account may be less tolerable. Unless you’re rich, of course; in tha t case, why do you care so much about hoarding every last penny? Some applications (such as databases) don’t enjoy this trade-off. T hus, to avoid unexpected data loss due to write buffering, they simp ly force writes to disk, by calling fsync() , by using direct I/O interfaces that work around the cache, or by using the raw disk interface and avoiding the ﬁle system altogether2. While most applications live with the trade- offs made by the ﬁle system, there are enough controls in place to g et the system to do what you want it to, should the default not be satisfyi ng. 40.8 Summary We have seen the basic machinery required in building a ﬁle sy stem. There needs to be some information about each ﬁle (metadata), usu ally stored in a structure called an inode. Directories are just a spe ciﬁc type of ﬁle that store name →inode-number mappings. And other structures are needed too; for example, ﬁle systems often use a structure suc h as a bitmap to track which inodes or data blocks are free or allocated. The terriﬁc aspect of ﬁle system design is its freedom; the ﬁle s ystems we explore in the coming chapters each take advantage of this fre edom to optimize some aspect of the ﬁle system. There are also clearly many policy decisions we have left unexplored. For example, when a new ﬁle is created, where should it be placed on disk? This policy and othe rs will also be the subject of future chapters. Or will they?3 2Take a database class to learn more about old-school databases and their former insis- tence on avoiding the OS and controlling everything themselves. But watch o ut. Those database types are always trying to bad mouth the OS. Shame on you, database people. Shame. 3Cue mysterious music that gets you even more intrigued about the topic of ﬁle systems. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FILESYSTEM IMPLEMENTATION 17 References [A+07] “A Five-Year Study of File-System Metadata” by Nitin Agra wal, William J. Bolosky, John R. Douceur, Jacob R. Lorch. FAST ’07, San Jose, California, Februar y 2007. An excellent recent analysis of how ﬁle systems are actually used. Use the bibliography within to follow the trail of ﬁle-system analysis papers back to the early 1980s. [B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available from: http://www.ostep.org/Citations/zfs_last.pdf .One of the most recent important ﬁle systems, full of features and awesomeness. We should have a chapter on it, and perhaps soon will. [B02] “The FAT File System” by Andries Brouwer. September, 2002. Available online at: http://www.win.tue.nl/˜aeb/linux/fs/fat/fat.html .A nice clean description of FAT.",3033
40. File System Implementation,"The ﬁle system kind, not the bacon kind. Though you have to admit, bacon fat p robably tastes better. [C94] “Inside the Windows NT File System” by Helen Custer. Microsoft Press, 1994. A short book about NTFS; there are probably ones with more technical details elsewher e. [H+88] “Scale and Performance in a Distributed File System” by John H. H oward, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Rober t N. Sidebotham, Michael J. West.. ACM TOCS, Volume 6:1, February 1988. A classic distributed ﬁle system; we’ll be learning more about it later, don’t worry. [P09] “The Second Extended File System: Internal Layout” by Dave Poirie r. 2009.Available: http://www.nongnu.org/ext2-doc/ext2.html .Some details on ext2, a very simple Linux ﬁle system based on FFS, the Berkeley Fast File System. We’ll be readin g about it in the next chapter. [RT74] “The U NIX Time-Sharing System” by M. Ritchie, K. Thompson. CACM Volume 17:7, 1974. The original paper about UNIX. Read it to see the underpinnings of much of modern operating systems. [S00] “UBC: An Efﬁcient Uniﬁed I/O and Memory Caching Subsystem for NetB SD” by Chuck Silvers. FREENIX, 2000. A nice paper about NetBSD’s integration of ﬁle-system buffer caching and the virtual-memory page cache. Many other systems do the same type of thing. [S+96] “Scalability in the XFS File System” by Adan Sweeney, Doug Doucette, Wei Hu, Curtis Anderson, Mike Nishimoto, Geoff Peck. USENIX ’96, January 1996, San D iego, California. The ﬁrst attempt to make scalability of operations, including things like havin g millions of ﬁles in a directory, a central focus. A great example of pushing an idea to the extrem e. The key idea behind this ﬁle system: everything is a tree. We should have a chapter on this ﬁle sys tem too. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 FILESYSTEM IMPLEMENTATION Homework (Simulation) Use this tool, vsfs.py , to study how ﬁle system state changes as var- ious operations take place. The ﬁle system begins in an empty sta te, with just a root directory. As the simulation takes place, various opera tions are performed, thus slowly changing the on-disk state of the ﬁle syst em. See the README for details. Questions 1. Run the simulator with some different random seeds (say 17, 18 , 19, 20), and see if you can ﬁgure out which operations must have taken place between each state change. 2. Now do the same, using different random seeds (say 21, 22, 23, 24), except run with the -rﬂag, thus making you guess the state change while being shown the operation. What can you conclude about the inode and data-block allocation algorithms, in terms of which blocks they prefer to allocate? 3. Now reduce the number of data blocks in the ﬁle system, to very low numbers (say two), and run the simulator for a hundred or so requests. What types of ﬁles end up in the ﬁle system in this hig hly- constrained layout? What types of operations would fail? 4. Now do the same, but with inodes. With very few inodes, what types of operations can succeed? Which will usually fail? What i s the ﬁnal state of the ﬁle system likely to be? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",3186
41. Fast File System FFS,"41 Locality and The Fast File System When the U NIXoperating system was ﬁrst introduced, the U NIXwizard himself Ken Thompson wrote the ﬁrst ﬁle system. Let’s call that th e “old UNIXﬁle system”, and it was really simple. Basically, its data st ructures looked like this on the disk: S Inodes Data The super block (S) contained information about the entire ﬁle syst em: how big the volume is, how many inodes there are, a pointer to the hea d of a free list of blocks, and so forth. The inode region of the disk conta ined all the inodes for the ﬁle system. Finally, most of the disk was tak en up by data blocks. The good thing about the old ﬁle system was that it was simple, and supported the basic abstractions the ﬁle system was trying to d eliver: ﬁles and the directory hierarchy. This easy-to-use system was a rea l step for- ward from the clumsy, record-based storage systems of the past, a nd the directory hierarchy was a true advance over simpler, one-level hierarchies provided by earlier systems. 41.1 The Problem: Poor Performance The problem: performance was terrible. As measured by Kirk McK u- sick and his colleagues at Berkeley [MJLF84], performance sta rted off bad and got worse over time, to the point where the ﬁle system was deliv ering only 2 percent of overall disk bandwidth. The main issue was that the old U NIXﬁle system treated the disk like it was a random-access memory; data was spread all over the place wi thout regard to the fact that the medium holding the data was a disk, a nd thus had real and expensive positioning costs. For example, the data b locks of a ﬁle were often very far away from its inode, thus inducing an exp ensive seek whenever one ﬁrst read the inode and then the data blocks of a ﬁ le (a pretty common operation). 1 2 LOCALITY AND THEFAST FILESYSTEM Worse, the ﬁle system would end up getting quite fragmented , as the free space was not carefully managed. The free list would end up point- ing to a bunch of blocks spread across the disk, and as ﬁles got alloc ated, they would simply take the next free block. The result was that a logi- cally contiguous ﬁle would be accessed by going back and forth acros s the disk, thus reducing performance dramatically. For example, imagine the following data block region, which contai ns four ﬁles (A, B, C, and D), each of size 2 blocks: A1 A2 B1 B2 C1 C2 D1 D2 If B and D are deleted, the resulting layout is: A1 A2 C1 C2 As you can see, the free space is fragmented into two chunks of tw o blocks, instead of one nice contiguous chunk of four. Let’s say you now wish to allocate a ﬁle E, of size four blocks: A1 A2 E1 E2 C1 C2 E3 E4 You can see what happens: E gets spread across the disk, and as a result, when accessing E, you don’t get peak (sequential) perfor mance from the disk. Rather, you ﬁrst read E1 and E2, then seek, then re ad E3 and E4. This fragmentation problem happened all the time in the old UNIX ﬁle system, and it hurt performance. A side note: this problem is exactly what disk defragmentation tools help with; they reorganize on- disk data to place ﬁles contiguously and make free space for one or a few contiguous regions, moving data around and then rewriting inodes a nd such to reﬂect the changes.",3231
41. Fast File System FFS,"One other problem: the original block size was too small (512 bytes ). Thus, transferring data from the disk was inherently inefﬁci ent. Smaller blocks were good because they minimized internal fragmentation (waste within the block), but bad for transfer as each block might requi re a posi- tioning overhead to reach it. Thus, the problem: THECRUX: HOWTOORGANIZE ON-DISK DATA TOIMPROVE PERFORMANCE How can we organize ﬁle system data structures so as to improve pe r- formance? What types of allocation policies do we need on top of those data structures? How do we make the ﬁle system “disk aware”? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 3 41.2 FFS: Disk Awareness Is The Solution A group at Berkeley decided to build a better, faster ﬁle syste m, which they cleverly called the Fast File System (FFS) . The idea was to design the ﬁle system structures and allocation policies to be “disk aw are” and thus improve performance, which is exactly what they did. FFS t hus ush- ered in a new era of ﬁle system research; by keeping the same interface to the ﬁle system (the same APIs, including open() ,read() ,write() , close() , and other ﬁle system calls) but changing the internal implemen- tation , the authors paved the path for new ﬁle system construction, work that continues today. Virtually all modern ﬁle systems adhere t o the ex- isting interface (and thus preserve compatibility with appl ications) while changing their internals for performance, reliability, or othe r reasons. 41.3 Organizing Structure: The Cylinder Group The ﬁrst step was to change the on-disk structures. FFS divide s the disk into a number of cylinder groups . A single cylinder is a set of tracks on different surfaces of a hard drive that are the same distance from the center of the drive; it is called a cylinder because of its clear resemblance to the so-called geometrical shape. FFS aggregates Nconsecutive cylin- ders into a group, and thus the entire disk can thus be viewed as a collec- tion of cylinder groups. Here is a simple example, showing the four outer most tracks of a drive with six platters, and a cylinder group tha t consists of three cylinders: Single track (e.g., dark gray)Cylinder: Tracks at same distance from center of drive across different surfaces (all tracks with same color) Cylinder Group: Set of N consecutive cylinders (if N=3, first group does not include black track) Note that modern drives do not export enough information for the ﬁle system to truly understand whether a particular cylinde r is in use; as discussed previously [AD14a], disks export a logical addres s space of blocks and hide details of their geometry from clients. Thus, mode rn ﬁle c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LOCALITY AND THEFAST FILESYSTEM systems (such as Linux ext2, ext3, and ext4) instead organize the drive into block groups , each of which is just a consecutive portion of the disk’s address space. The picture below illustrates an example wher e every 8 blocks are organized into a different block group (note that real g roups would consist of many more blocks): Group 0 Group 1 Group 2 Whether you call them cylinder groups or block groups, these groups are the central mechanism that FFS uses to improve performance . Crit- ically, by placing two ﬁles within the same group, FFS can ensu re that accessing one after the other will not result in long seeks across t he disk. To use these groups to store ﬁles and directories, FFS needs to ha ve the ability to place ﬁles and directories into a group, and track al l necessary information about them therein. To do so, FFS includes all the str uctures you might expect a ﬁle system to have within each group, e.g., sp ace for inodes, data blocks, and some structures to track whether each of those are allocated or free. Here is a depiction of what FFS keeps within a single cylinder group: Sib db Inodes Data Let’s now examine the components of this single cylinder group in more detail. FFS keeps a copy of the super block (S) in each group for reliability reasons. The super block is needed to mount the ﬁle s ystem; by keeping multiple copies, if one copy becomes corrupt, you can sti ll mount and access the ﬁle system by using a working replica.",4279
41. Fast File System FFS,"Within each group, FFS needs to track whether the inodes and dat a blocks of the group are allocated. A per-group inode bitmap (ib) and data bitmap (db) serve this role for inodes and data blocks in each group. Bitmaps are an excellent way to manage free space in a ﬁle syst em be- cause it is easy to ﬁnd a large chunk of free space and allocate it to a ﬁle, perhaps avoiding some of the fragmentation problems of the free lis t in the old ﬁle system. Finally, the inode and data block regions are just like those in the pre- vious very-simple ﬁle system (VSFS). Most of each cylinder group, a s usual, is comprised of data blocks. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 5 ASIDE : FFS F ILECREATION As an example, think about what data structures must be update d when a ﬁle is created; assume, for this example, that the user creat es a new ﬁle /foo/bar.txt and that the ﬁle is one block long (4KB). The ﬁle is new, and thus needs a new inode; thus, both the inode bitmap and the new ly- allocated inode will be written to disk. The ﬁle also has data in it and thus it too must be allocated; the data bitmap and a data block wil l thus (eventually) be written to disk. Hence, at least four writes t o the current cylinder group will take place (recall that these writes may b e buffered in memory for a while before they take place). But this is not all. I n particular, when creating a new ﬁle, you must also place the ﬁl e in the ﬁle-system hierarchy, i.e., the directory must be updated. Sp eciﬁcally, the parent directory foo must be updated to add the entry for bar.txt ; this update may ﬁt in an existing data block of foo or require a new block to be allocated (with associated data bitmap). The inode of foo must also be updated, both to reﬂect the new length of the directory as well as to update time ﬁelds (such as last-modiﬁed-time). Overall, it i s a lot of work just to create a new ﬁle. Perhaps next time you do so, you should be m ore thankful, or at least surprised that it all works so well. 41.4 Policies: How To Allocate Files and Directories With this group structure in place, FFS now has to decide how to pl ace ﬁles and directories and associated metadata on disk to improve p erfor- mance. The basic mantra is simple: keep related stuff together (and its corol- lary, keep unrelated stuff far apart ). Thus, to obey the mantra, FFS has to decide what is “related” an d place it within the same block group; conversely, unrelated ite ms should be placed into different block groups. To achieve this end, FFS makes use of a few simple placement heuristics. The ﬁrst is the placement of directories. FFS employs a simple ap - proach: ﬁnd the cylinder group with a low number of allocated direc - tories (to balance directories across groups) and a high number of free inodes (to subsequently be able to allocate a bunch of ﬁles), and put the directory data and inode in that group. Of course, other heuristic s could be used here (e.g., taking into account the number of free data b locks).",3056
41. Fast File System FFS,"For ﬁles, FFS does two things. First, it makes sure (in the gener al case) to allocate the data blocks of a ﬁle in the same group as its inode, th us preventing long seeks between inode and data (as in the old ﬁle sy stem). Second, it places all ﬁles that are in the same directory in the cy linder group of the directory they are in. Thus, if a user creates four ﬁle s,/a/b , /a/c ,/a/d , andb/f, FFS would try to place the ﬁrst three near one another (same group) and the fourth far away (in some other group). Let’s look at an example of such an allocation. In the example, as- sume that there are only 10 inodes and 10 data blocks in each group ( both c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 LOCALITY AND THEFAST FILESYSTEM unrealistically small numbers), and that the three director ies (the root di- rectory/,/a, and/b) and four ﬁles ( /a/c, /a/d, /a/e, /b/f ) are placed within them per the FFS policies. Assume the regular ﬁl es are each two blocks in size, and that the directories have just a single b lock of data. For this ﬁgure, we use the obvious symbols for each ﬁle or directory (i .e., /for the root directory, afor/a,ffor/b/f , and so forth). group inodes data 0 /--------- /--------- 1 acde------ accddee--- 2 bf-------- bff------- 3 ---------- ---------- 4 ---------- ---------- 5 ---------- ---------- 6 ---------- ---------- 7 ---------- ---------- ... Note that the FFS policy does two positive things: the data blocks of each ﬁle are near each ﬁle’s inode, and ﬁles in the same directory are near one another (namely, /a/c ,/a/d , and/a/e are all in Group 1, and directory/band its ﬁle /b/f are near one another in Group 2). In contrast, let’s now look at an inode allocation policy that simply spreads inodes across groups, trying to ensure that no group’s inod e table ﬁlls up quickly. The ﬁnal allocation might thus look something lik e this: group inodes data 0 /--------- /--------- 1 a--------- a--------- 2 b--------- b--------- 3 c--------- cc-------- 4 d--------- dd-------- 5 e--------- ee-------- 6 f--------- ff-------- 7 ---------- ---------- ... As you can see from the ﬁgure, while this policy does indeed keep ﬁl e (and directory) data near its respective inode, ﬁles within a d irectory are arbitrarily spread around the disk, and thus name-based local ity is not preserved. Access to ﬁles /a/c ,/a/d , and/a/e now spans three groups instead of one as per the FFS approach. The FFS policy heuristics are not based on extensive studies of ﬁl e- system trafﬁc or anything particularly nuanced; rather, the y are based on good old-fashioned common sense (isn’t that what CS stands for after all?)1. Files in a directory areoften accessed together: imagine compil- ing a bunch of ﬁles and then linking them into a single executab le. Be- cause such namespace-based locality exists, FFS will often im prove per- formance, making sure that seeks between related ﬁles are nic e and short. 1Some people refer to common sense as horse sense , especially people who work regu- larly with horses. However, we have a feeling that this idiom may be l ost as the “mechanized horse”, a.k.a.",3135
41. Fast File System FFS,"the car, gains in popularity. What will they invent next ? A ﬂying machine??.. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 7 0 2 4 6 8 100 percent20 percent40 percent60 percent80 percent100 percentFFS Locality Path DifferenceCumulative FrequencyTrace Random Figure 41.1: FFS Locality For SEER Traces 41.5 Measuring File Locality To understand better whether these heuristics make sense, l et’s ana- lyze some traces of ﬁle system access and see if indeed there is n amespace locality. For some reason, there doesn’t seem to be a good study of this topic in the literature. Speciﬁcally, we’ll use the SEER traces [K94] and analyze how “far away” ﬁle accesses were from one another in the directory tree. For ex- ample, if ﬁle fis opened, and then re-opened next in the trace (before any other ﬁles are opened), the distance between these two opens in the directory tree is zero (as they are the same ﬁle). If a ﬁle fin directory dir (i.e.,dir/f ) is opened, and followed by an open of ﬁle gin the same directory (i.e., dir/g ), the distance between the two ﬁle accesses is one, as they share the same directory but are not the same ﬁle. Our dis tance metric, in other words, measures how far up the directory tree you h ave to travel to ﬁnd the common ancestor of two ﬁles; the closer they are in the tree, the lower the metric. Figure 41.1 shows the locality observed in the SEER traces over all workstations in the SEER cluster over the entirety of all traces. T he graph plots the difference metric along the x-axis, and shows the cumu lative percentage of ﬁle opens that were of that difference along the y-a xis. Speciﬁcally, for the SEER traces (marked “Trace” in the graph), you can see that about 7 percent of ﬁle accesses were to the ﬁle that was opened pr evi- ously, and that nearly 40 percent of ﬁle accesses were to either the sam e ﬁle or to one in the same directory (i.e., a difference of zero or one). Thu s, the FFS locality assumption seems to make sense (at least for these t races). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 LOCALITY AND THEFAST FILESYSTEM Interestingly, another 25 percent or so of ﬁle accesses were to ﬁles tha t had a distance of two. This type of locality occurs when the user has stru ctured a set of related directories in a multi-level fashion and consist ently jumps between them. For example, if a user has a src directory and builds object ﬁles ( .oﬁles) into an obj directory, and both of these directories are sub-directories of a main proj directory, a common access pattern will beproj/src/foo.c followed by proj/obj/foo.o . The distance between these two accesses is two, as proj is the common ancestor. FFS does notcapture this type of locality in its policies, and thus more seeki ng will occur between such accesses. For comparison, the graph also shows locality for a “Random” trace. The random trace was generated by selecting ﬁles from within an existing SEER trace in random order, and calculating the distance metric between these randomly-ordered accesses. As you can see, there is less n amespace locality in the random traces, as expected.",3156
41. Fast File System FFS,"However, because ev entually every ﬁle shares a common ancestor (e.g., the root), there is some loc ality, and thus random is useful as a comparison point. 41.6 The Large-File Exception In FFS, there is one important exception to the general policy of ﬁle placement, and it arises for large ﬁles. Without a different ru le, a large ﬁle would entirely ﬁll the block group it is ﬁrst placed within (a nd maybe others). Filling a block group in this manner is undesirable, as it prevents subsequent “related” ﬁles from being placed within this block group, and thus may hurt ﬁle-access locality. Thus, for large ﬁles, FFS does the following. After some number of blocks are allocated into the ﬁrst block group (e.g., 12 blocks, or t he num- ber of direct pointers available within an inode), FFS places th e next “large” chunk of the ﬁle (e.g., those pointed to by the ﬁrst indirect block ) in an- other block group (perhaps chosen for its low utilization). Then, th e next chunk of the ﬁle is placed in yet another different block group, an d so on. Let’s look at some diagrams to understand this policy better. With out the large-ﬁle exception, a single large ﬁle would place all of it s blocks into one part of the disk. We investigate a small example of a ﬁle ( /a) with 30 blocks in an FFS conﬁgured with 10 inodes and 40 data blocks per grou p. Here is the depiction of FFS without the large-ﬁle exception: group inodes data 0 /a-------- /aaaaaaaaa aaaaaaaaaa aaaaaaaaaa a--------- 1 ---------- ---------- ---------- ---------- ---------- 2 ---------- ---------- ---------- ---------- ---------- ... As you can see in the picture, /aﬁlls up most of the data blocks in Group 0, whereas other groups remain empty. If some other ﬁles are n ow created in the root directory ( /), there is not much room for their data in the group. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 9 With the large-ﬁle exception (here set to ﬁve blocks in each chu nk), FFS instead spreads the ﬁle spread across groups, and the resultin g utilization within any one group is not too high: group inodes data 0 /a-------- /aaaaa---- ---------- ---------- ---------- 1 ---------- aaaaa----- ---------- ---------- ---------- 2 ---------- aaaaa----- ---------- ---------- ---------- 3 ---------- aaaaa----- ---------- ---------- ---------- 4 ---------- aaaaa----- ---------- ---------- ---------- 5 ---------- aaaaa----- ---------- ---------- ---------- 6 ---------- ---------- ---------- ---------- ---------- ... The astute reader (that’s you) will note that spreading blocks of a ﬁle across the disk will hurt performance, particularly in the rel atively com- mon case of sequential ﬁle access (e.g., when a user or applicati on reads chunks 0 through 29 in order). And you are right, oh astute reader of ours. But you can address this problem by choosing chunk size caref ully. Speciﬁcally, if the chunk size is large enough, the ﬁle system w ill spend most of its time transferring data from disk and just a (relative ly) little time seeking between chunks of the block.",3084
41. Fast File System FFS,"This process of reducin g an overhead by doing more work per overhead paid is called amortization and is a common technique in computer systems. Let’s do an example: assume that the average positioning time (i .e., seek and rotation) for a disk is 10 ms. Assume further that the dis k trans- fers data at 40 MB/s. If your goal was to spend half our time seekin g between chunks and half our time transferring data (and thus a chieve 50 percent of peak disk performance), you would thus need to spend 10 ms transferring data for every 10 ms positioning. So the question bec omes: how big does a chunk have to be in order to spend 10 ms in transfer? Easy, just use our old friend, math, in particular the dimension al analysis mentioned in the chapter on disks [AD14a]: 40✘✘MB ✟✟sec·1024KB 1✘✘MB·1✟✟sec 1000✟✟ms·10✟✟ms= 409.6KB (41.1) Basically, what this equation says is this: if you transfer dat a at 40 MB/s, you need to transfer only 409.6KB every time you seek in orde r to spend half your time seeking and half your time transferring. Si milarly, you can compute the size of the chunk you would need to achieve 90 percent of peak bandwidth (turns out it is about 3.69MB), or even 99 percent of peak bandwidth (40.6MB.). As you can see, the closer you want to get to p eak, the bigger these chunks get (see Figure 41.2 for a plot of these va lues). FFS did not use this type of calculation in order to spread large ﬁl es across groups, however. Instead, it took a simple approach, based on the structure of the inode itself. The ﬁrst twelve direct blocks wer e placed in the same group as the inode; each subsequent indirect block, a nd all the blocks it pointed to, was placed in a different group. With a bl ock size of 4KB, and 32-bit disk addresses, this strategy implies that every c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 LOCALITY AND THEFAST FILESYSTEM 0 percent 25 percent 50 percent 75 percent 100 percent1K32K1M10MThe Challenges of Amortization Percent Bandwidth (Desired)Log(Chunk Size Needed)50 percent, 409.6K90 percent, 3.69M Figure 41.2: Amortization: How Big Do Chunks Have To Be? 1024 blocks of the ﬁle (4MB) were placed in separate groups, the l one exception being the ﬁrst 48KB of the ﬁle as pointed to by direct poi nters. Note that the trend in disk drives is that transfer rate improve s fairly rapidly, as disk manufacturers are good at cramming more bits in to the same surface, but the mechanical aspects of drives related to seeks (disk arm speed and the rate of rotation) improve rather slowly [P98]. Th e implication is that over time, mechanical costs become relative ly more expensive, and thus, to amortize said costs, you have to transfe r more data between seeks. 41.7 A Few Other Things About FFS FFS introduced a few other innovations too. In particular, the desi gn- ers were extremely worried about accommodating small ﬁles; as it turned out, many ﬁles were 2KB or so in size back then, and using 4KB block s, while good for transferring data, was not so good for space efﬁciency . This internal fragmentation could thus lead to roughly half the disk be- ing wasted for a typical ﬁle system.",3152
41. Fast File System FFS,"The solution the FFS designers hit upon was simple and solved the problem. They decided to introduce sub-blocks , which were 512-byte little blocks that the ﬁle system could allocate to ﬁles. Thus, i f you created a small ﬁle (say 1KB in size), it would occupy two sub-blocks and t hus not waste an entire 4KB block. As the ﬁle grew, the ﬁle system will c ontinue allocating 512-byte blocks to it until it acquires a full 4KB of d ata. At that point, FFS will ﬁnd a 4KB block, copy the sub-blocks into it, and free the sub-blocks for future use. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 11 0111098 7 6 5 4321Spindle 0115104 9 3 8 2716Spindle Figure 41.3: FFS: Standard Versus Parameterized Placement You might observe that this process is inefﬁcient, requiring a l ot of ex- tra work for the ﬁle system (in particular, a lot of extra I/O to per form the copy). And you’d be right again. Thus, FFS generally avoided this pes- simal behavior by modifying the libc library; the library would buffer writes and then issue them in 4KB chunks to the ﬁle system, thu s avoid- ing the sub-block specialization entirely in most cases. A second neat thing that FFS introduced was a disk layout that was optimized for performance. In those times (before SCSI and other more modern device interfaces), disks were much less sophisticate d and re- quired the host CPU to control their operation in a more hands-on way. A problem arose in FFS when a ﬁle was placed on consecutive sectors of the disk, as on the left in Figure 41.3. In particular, the problem arose during sequential reads. FFS would ﬁrst issue a read to block 0; by the time the read was complete, an d FFS issued a read to block 1, it was too late: block 1 had rotated under t he head and now the read to block 1 would incur a full rotation. FFS solved this problem with a different layout, as you can see on th e right in Figure 41.3. By skipping over every other block (in the e xample), FFS has enough time to request the next block before it went past t he disk head. In fact, FFS was smart enough to ﬁgure out for a particu lar disk how many blocks it should skip in doing layout in order to avoid the extra rotations; this technique was called parameterization , as FFS would ﬁgure out the speciﬁc performance parameters of the disk and use those to decide on the exact staggered layout scheme. You might be thinking: this scheme isn’t so great after all. In f act, you will only get 50 percent of peak bandwidth with this type of layout, becau se you have to go around each track twice just to read each block once. For- tunately, modern disks are much smarter: they internally rea d the entire track in and buffer it in an internal disk cache (often called a track buffer for this very reason). Then, on subsequent reads to the track, th e disk will just return the desired data from its cache. File systems thus no longer have to worry about these incredibly low-level details. Abstra ction and higher-level interfaces can be a good thing, when designed prop erly.",3066
41. Fast File System FFS,"Some other usability improvements were added as well. FFS was one of the ﬁrst ﬁle systems to allow for long ﬁle names , thus enabling more expressive names in the ﬁle system instead of the traditional ﬁ xed-size approach (e.g., 8 characters). Further, a new concept was intr oduced c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 LOCALITY AND THEFAST FILESYSTEM TIP: M AKE THESYSTEM USABLE Probably the most basic lesson from FFS is that not only did it intro- duce the conceptually good idea of disk-aware layout, but it also a dded a number of features that simply made the system more usable. Lon g ﬁle names, symbolic links, and a rename operation that worked atomica lly all improved the utility of a system; while hard to write a resea rch pa- per about (imagine trying to read a 14-pager about “The Symbolic L ink: Hard Link’s Long Lost Cousin”), such small features made FFS more u se- ful and thus likely increased its chances for adoption. Making a system usable is often as or more important than its deep technical innova tions. called a symbolic link . As discussed in a previous chapter [AD14b] , hard links are limited in that they both could not point to director ies (for fear of introducing loops in the ﬁle system hierarchy) and that th ey can only point to ﬁles within the same volume (i.e., the inode number m ust still be meaningful). Symbolic links allow the user to create an “alias” to any other ﬁle or directory on a system and thus are much more ﬂexible . FFS also introduced an atomic rename() operation for renaming ﬁles. Usability improvements, beyond the basic technology, also like ly gained FFS a stronger user base. 41.8 Summary The introduction of FFS was a watershed moment in ﬁle system his- tory, as it made clear that the problem of ﬁle management was one of t he most interesting issues within an operating system, and showed how one might begin to deal with that most important of devices, the hard disk. Since that time, hundreds of new ﬁle systems have developed, but still today many ﬁle systems take cues from FFS (e.g., Linux ext2 and e xt3 are obvious intellectual descendants). Certainly all modern syst ems account for the main lesson of FFS: treat the disk like it’s a disk. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LOCALITY AND THEFAST FILESYSTEM 13 References [AD14a] “Operating Systems: Three Easy Pieces” (Chapter: Hard Disk Dr ives) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2 014. There is no way you should be reading about FFS without having ﬁrst understood hard drives in some detail. If you try to do so, please instead go directly to jail; do not pass go, and, critically, d o not collect 200 much-needed simoleons. [AD14b] “Operating Systems: Three Easy Pieces” (Chapter: File System Implementation) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau . Arpaci-Dusseau Bo oks, 2014. As above, it makes little sense to read this chapter unless you have read (and understood) th e chapter on ﬁle system implementation.",3027
41. Fast File System FFS,"Otherwise, we’ll be throwing around terms like “i node” and “indirect block” and you’ll be like “huh?” and that is no fun for either of us. [K94] “The Design of the SEER Predictive Caching System” by G. H. Kuenning. MOBICOMM ’94, Santa Cruz, California, December 1994. According to Kuenning, this is the best overview of the SEER project, which led to (among other things) the collection of these traces. [MJLF84] “A Fast File System for U NIX” by Marshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry. ACM TOCS, 2:3, August 1984. McKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to ﬁle systems, much of whi ch was based on his work building FFS. In his acceptance speech, he discussed the ori ginal FFS software: only 1200 lines of code. Modern versions are a little more complex, e.g., the BSD FF S descendant now is in the 50-thousand lines-of-code range. [P98] “Hardware Technology Trends and Database Opportunities” by Dav id A. Patterson. Keynote Lecture at SIGMOD ’98, June 1998. A great and simple overview of disk technology trends and how they change over time. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 LOCALITY AND THEFAST FILESYSTEM Homework (Simulation) This section introduces ffs.py , a simple FFS simulator you can use to understand better how FFS-based ﬁle and directory allocation w ork. See the README for details on how to run the simulator. Questions 1. Examine the ﬁle in.largefile , and then run the simulator with ﬂag -f in.largefile and-L 4 . The latter sets the large-ﬁle exception to 4 blocks. What will the resulting allocation look like? Run with -cto check. 2. Now run with -L 30 . What do you expect to see? Once again, turn on -c to see if you were right. You can also use -Sto see exactly which blocks were allocated to the ﬁle /a. 3. Now we will compute some statistics about the ﬁle. The ﬁrst is so mething we call ﬁlespan , which is the max distance between any two data blocks of the ﬁle or between the inode and any data block. Calculate the ﬁ lespan of /a. Runffs.py -f in.largefile -L 4 -T -c to see what it is. Do the same with -L 100 . What difference do you expect in ﬁlespan as the large-ﬁle exception parameter changes from low values to high v alues? 4. Now let’s look at a new input ﬁle, in.manyfiles . How do you think the FFS policy will lay these ﬁles out across groups? (you can run wit h-vto see what ﬁles and directories are created, or just cat in.manyfiles ). Run the simulator with -cto see if you were right. 5. A metric to evaluate FFS is called dirspan . This metric calculates the spread of ﬁles within a particular directory, speciﬁcally the max dis tance between the inodes and data blocks of all ﬁles in the directory and the inode and data block of the directory itself. Run with in.manyfiles and the-Tﬂag, and calculate the dirspan of the three directories. Run with -cto check. How good of a job does FFS do in minimizing dirspan? 6. Now change the size of the inode table per group to 5 ( -I 5 ). How do you think this will change the layout of the ﬁles? Run with -cto see if you were right. How does it affect the dirspan? 7. Which group should FFS place inode of a new directory in? The d efault (simulator) policy looks for the group with the most free inodes. A different policy looks for a set of groups with the most free inodes. For exa mple, if you run with -A 2 , when allocating a new directory, the simulator will look at groups in pairs and pick the best pair for the allocation. Run ./ffs.py -f in.manyfiles -I 5 -A 2 -c to see how allocation changes with this strategy. How does it affect dirspan? Why might this poli cy be good? 8. One last policy change we will explore relates to ﬁle fragmen tation. Run ./ffs.py -f in.fragmented -v and see if you can predict how the ﬁles that remain are allocated. Run with -cto conﬁrm your answer. What is interesting about the data layout of ﬁle /i? Why is it problematic? 9. A new policy, which we call contiguous allocation (-C), tries to ensure that each ﬁle is allocated contiguously. Speciﬁcally, with -C n , the ﬁle system tries to ensure that ncontiguous blocks are free within a group before al- locating a block. Run ./ffs.py -f in.fragmented -v -C 2 -c to see the difference. How does layout change as the parameter pas sed to-C increases? Finally, how does -Caffect ﬁlespan and dirspan? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4443
42. FSCK and Journaling,"42 Crash Consistency: FSCK and Journaling As we’ve seen thus far, the ﬁle system manages a set of data struc tures to implement the expected abstractions: ﬁles, directories, and all of the other metadata needed to support the basic abstraction that we expec t from a ﬁle system. Unlike most data structures (for example, those foun d in memory of a running program), ﬁle system data structures must persist , i.e., they must survive over the long haul, stored on devices that retain data despite power loss (such as hard disks or ﬂash-based SSDs). One major challenge faced by a ﬁle system is how to update persis - tent data structures despite the presence of a power loss orsystem crash . Speciﬁcally, what happens if, right in the middle of updating on -disk structures, someone trips over the power cord and the machine loses power? Or the operating system encounters a bug and crashes? Bec ause of power losses and crashes, updating a persistent data structu re can be quite tricky, and leads to a new and interesting problem in ﬁle system implementation, known as the crash-consistency problem . This problem is quite simple to understand. Imagine you have to up- date two on-disk structures, AandB, in order to complete a particular operation. Because the disk only services a single request at a t ime, one of these requests will reach the disk ﬁrst (either AorB). If the system crashes or loses power after one write completes, the on-disk struc ture will be left in an inconsistent state. And thus, we have a problem that all ﬁle systems need to solve: THECRUX: HOWTOUPDATE THEDISKDESPITE CRASHES The system may crash or lose power between any two writes, and thus the on-disk state may only partially get updated. After th e crash, the system boots and wishes to mount the ﬁle system again (in order to access ﬁles and such). Given that crashes can occur at arbitra ry points in time, how do we ensure the ﬁle system keeps the on-disk image i n a reasonable state? 1 2 C RASH CONSISTENCY : FSCK AND JOURNALING In this chapter, we’ll describe this problem in more detail, and look at some methods ﬁle systems have used to overcome it. We’ll begin by examining the approach taken by older ﬁle systems, known as fsck or the ﬁle system checker . We’ll then turn our attention to another approach, known as journaling (also known as write-ahead logging ), a technique which adds a little bit of overhead to each write but recovers more quickly from crashes or power losses. We will discuss the basic machinery of journaling, including a few different ﬂavors of journaling that Linux ext3 [T98,PAA05] (a relatively modern journaling ﬁle system) impl ements. 42.1 A Detailed Example To kick off our investigation of journaling, let’s look at an example. We’ll need to use a workload that updates on-disk structures in some way. Assume here that the workload is simple: the append of a sing le data block to an existing ﬁle. The append is accomplished by open ing the ﬁle, calling lseek() to move the ﬁle offset to the end of the ﬁle, and then issuing a single 4KB write to the ﬁle before closing it.",3103
42. FSCK and Journaling,"Let’s also assume we are using standard simple ﬁle system stru ctures on the disk, similar to ﬁle systems we have seen before. This tin y example includes an inode bitmap (with just 8 bits, one per inode), a data bitmap (also 8 bits, one per data block), inodes (8 total, numbered 0 to 7, and spread across four blocks), and data blocks (8 total, numbered 0 to 7). Here is a diagram of this ﬁle system: Inode BmapData BmapInodes Data Blocks I[v1]Da If you look at the structures in the picture, you can see that a sing le inode is allocated (inode number 2), which is marked in the inode bitma p, and a single allocated data block (data block 4), also marked in the da ta bitmap. The inode is denoted I[v1], as it is the ﬁrst version of this inode; i t will soon be updated (due to the workload described above). Let’s peek inside this simpliﬁed inode too. Inside of I[v1], we see : owner : remzi permissions : read-write size : 1 pointer : 4 pointer : null pointer : null pointer : null In this simpliﬁed inode, the size of the ﬁle is 1(it has one block al- located), the ﬁrst direct pointer points to block 4(the ﬁrst data block of OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 3 the ﬁle, Da), and all three other direct pointers are set to null (indicating that they are not used). Of course, real inodes have many more ﬁeld s; see previous chapters for more information. When we append to the ﬁle, we are adding a new data block to it, an d thus must update three on-disk structures: the inode (which mu st point to the new block and record the new larger size due to the append) , the new data block Db, and a new version of the data bitmap (call it B[v 2]) to indicate that the new data block has been allocated. Thus, in the memory of the system, we have three blocks which we must write to disk. The updated inode (inode version 2, or I[v2] for short) now looks like this: owner : remzi permissions : read-write size : 2 pointer : 4 pointer : 5 pointer : null pointer : null The updated data bitmap (B[v2]) now looks like this: 00001100. F inally, there is the data block (Db), which is just ﬁlled with whatever it is users put into ﬁles. Stolen music perhaps? What we would like is for the ﬁnal on-disk image of the ﬁle system to look like this: Inode BmapData BmapInodes Data Blocks I[v2]Da Db To achieve this transition, the ﬁle system must perform three s epa- rate writes to the disk, one each for the inode (I[v2]), bitmap (B [v2]), and data block (Db). Note that these writes usually don’t happen imme di- ately when the user issues a write() system call; rather, the dirty in- ode, bitmap, and new data will sit in main memory (in the page cache orbuffer cache ) for some time ﬁrst; then, when the ﬁle system ﬁnally decides to write them to disk (after say 5 seconds or 30 seconds), the ﬁle system will issue the requisite write requests to the disk. U nfortunately, a crash may occur and thus interfere with these updates to the d isk. In particular, if a crash happens after one or two of these writes ha ve taken place, but not all three, the ﬁle system could be left in a funny s tate. Crash Scenarios To understand the problem better, let’s look at some example crash sce- narios.",3243
42. FSCK and Journaling,"Imagine only a single write succeeds; there are thus th ree possible outcomes, which we list here: c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 C RASH CONSISTENCY : FSCK AND JOURNALING •Just the data block (Db) is written to disk. In this case, the data is on disk, but there is no inode that points to it and no bitmap that even says the block is allocated. Thus, it is as if the write neve r occurred. This case is not a problem at all, from the perspective of ﬁle-system crash consistency1. •Just the updated inode (I[v2]) is written to disk. In this case, the inode points to the disk address (5) where Db was about to be writ- ten, but Db has not yet been written there. Thus, if we trust tha t pointer, we will read garbage data from the disk (the old contents of disk address 5). Further, we have a new problem, which we call a ﬁle-system in- consistency . The on-disk bitmap is telling us that data block 5 has not been allocated, but the inode is saying that it has. The disag ree- ment between the bitmap and the inode is an inconsistency in the data structures of the ﬁle system; to use the ﬁle system, we mus t somehow resolve this problem (more on that below). •Just the updated bitmap (B[v2]) is written to disk. In this case, the bitmap indicates that block 5 is allocated, but there is no inode that points to it. Thus the ﬁle system is inconsistent again; if left unre- solved, this write would result in a space leak , as block 5 would never be used by the ﬁle system. There are also three more crash scenarios in this attempt to wri te three blocks to disk. In these cases, two writes succeed and the last one fails: •The inode (I[v2]) and bitmap (B[v2]) are written to disk, but not data (Db). In this case, the ﬁle system metadata is completely con- sistent: the inode has a pointer to block 5, the bitmap indicates that 5 is in use, and thus everything looks OK from the perspective of the ﬁle system’s metadata. But there is one problem: 5 has garbag e in it again. •The inode (I[v2]) and the data block (Db) are written, but not the bitmap (B[v2]). In this case, we have the inode pointing to the cor- rect data on disk, but again have an inconsistency between the i n- ode and the old version of the bitmap (B1). Thus, we once again need to resolve the problem before using the ﬁle system. •The bitmap (B[v2]) and data block (Db) are written, but not th e inode (I[v2]). In this case, we again have an inconsistency between the inode and the data bitmap. However, even though the block was written and the bitmap indicates its usage, we have no ide a which ﬁle it belongs to, as no inode points to the ﬁle. 1However, it might be a problem for the user, who just lost some data . OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 5 The Crash Consistency Problem Hopefully, from these crash scenarios, you can see the many proble ms that can occur to our on-disk ﬁle system image because of crashes: we can have inconsistency in ﬁle system data structures; we can have space leaks; we can return garbage data to a user; and so forth.",3108
42. FSCK and Journaling,"What we’d like to do ideally is move the ﬁle system from one consistent state (e.g., be fore the ﬁle got appended to) to another atomically (e.g., after the inode, bitmap, and new data block have been written to disk). Unfortunately, w e can’t do this easily because the disk only commits one write at a time, a nd crashes or power loss may occur between these updates. We call thi s general problem the crash-consistency problem (we could also call it the consistent-update problem ). 42.2 Solution #1: The File System Checker Early ﬁle systems took a simple approach to crash consistency. Ba si- cally, they decided to let inconsistencies happen and then ﬁx them later (when rebooting). A classic example of this lazy approach is found in a tool that does this: fsck2.fsck is a U NIX tool for ﬁnding such inconsis- tencies and repairing them [M86]; similar tools to check and re pair a disk partition exist on different systems. Note that such an approach can’t ﬁx all problems; consider, for example, the case above where the ﬁle system looks consistent but the inode points to garbage data. The only real goal is to make sure the ﬁle system metadata is internally consiste nt. The toolfsck operates in a number of phases, as summarized in McKusick and Kowalski’s paper [MK96]. It is run before the ﬁle system is mounted and made available ( fsck assumes that no other ﬁle-system activity is on-going while it runs); once ﬁnished, the on-disk ﬁl e system should be consistent and thus can be made accessible to users. Here is a basic summary of what fsck does: •Superblock: fsck ﬁrst checks if the superblock looks reasonable, mostly doing sanity checks such as making sure the ﬁle system si ze is greater than the number of blocks that have been allocated. Us u- ally the goal of these sanity checks is to ﬁnd a suspect (corrupt) superblock; in this case, the system (or administrator) may dec ide to use an alternate copy of the superblock. •Free blocks: Next,fsck scans the inodes, indirect blocks, double indirect blocks, etc., to build an understanding of which block s are currently allocated within the ﬁle system. It uses this knowle dge to produce a correct version of the allocation bitmaps; thus, if the re is any inconsistency between bitmaps and inodes, it is resolved by trusting the information within the inodes. The same type of chec k is performed for all the inodes, making sure that all inodes that l ook like they are in use are marked as such in the inode bitmaps. 2Pronounced either “eff-ess-see-kay”, “eff-ess-check”, or, if you don’t like the tool, “eff- suck”. Yes, serious professional people use this term. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 C RASH CONSISTENCY : FSCK AND JOURNALING •Inode state: Each inode is checked for corruption or other prob- lems. For example, fsck makes sure that each allocated inode has a valid type ﬁeld (e.g., regular ﬁle, directory, symbolic link , etc.). If there are problems with the inode ﬁelds that are not easily ﬁxed, the inode is considered suspect and cleared by fsck ; the inode bitmap is correspondingly updated. •Inode links: fsck also veriﬁes the link count of each allocated in- ode. As you may recall, the link count indicates the number of dif- ferent directories that contain a reference (i.e., a link) to t his par- ticular ﬁle. To verify the link count, fsck scans through the en- tire directory tree, starting at the root directory, and builds i ts own link counts for every ﬁle and directory in the ﬁle system.",3516
42. FSCK and Journaling,"If ther e is a mismatch between the newly-calculated count and that foun d within an inode, corrective action must be taken, usually by ﬁxi ng the count within the inode. If an allocated inode is discovered but no directory refers to it, it is moved to the lost+found directory. •Duplicates: fsck also checks for duplicate pointers, i.e., cases where two different inodes refer to the same block. If one inode is obvi- ously bad, it may be cleared. Alternately, the pointed-to block could be copied, thus giving each inode its own copy as desired. •Bad blocks: A check for bad block pointers is also performed while scanning through the list of all pointers. A pointer is considered “bad” if it obviously points to something outside its valid range, e.g., it has an address that refers to a block greater than the p arti- tion size. In this case, fsck can’t do anything too intelligent; it just removes (clears) the pointer from the inode or indirect block. •Directory checks: fsck does not understand the contents of user ﬁles; however, directories hold speciﬁcally formatted informat ion created by the ﬁle system itself. Thus, fsck performs additional integrity checks on the contents of each directory, making sure t hat “.” and “..” are the ﬁrst entries, that each inode referred to i n a directory entry is allocated, and ensuring that no directory is linked to more than once in the entire hierarchy. As you can see, building a working fsck requires intricate knowledge of the ﬁle system; making sure such a piece of code works correctly i n all cases can be challenging [G+08]. However, fsck (and similar a pproaches) have a bigger and perhaps more fundamental problem: they are too slow . With a very large disk volume, scanning the entire disk to ﬁnd a ll the allocated blocks and read the entire directory tree may take man y minutes or hours. Performance of fsck , as disks grew in capacity and RAIDs grew in popularity, became prohibitive (despite recent advan ces [M+13]). At a higher level, the basic premise of fsck seems just a tad irra- tional. Consider our example above, where just three blocks are wr itten to the disk; it is incredibly expensive to scan the entire dis k to ﬁx prob- lems that occurred during an update of just three blocks. This si tuation is akin to dropping your keys on the ﬂoor in your bedroom, and then com- OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 7 mencing a search-the-entire-house-for-keys recovery algorithm, starting in the basement and working your way through every room. It works but is wasteful. Thus, as disks (and RAIDs) grew, researchers and p ractitioners started to look for other solutions. 42.3 Solution #2: Journaling (or Write-Ahead Logging) Probably the most popular solution to the consistent update problem is to steal an idea from the world of database management systems . That idea, known as write-ahead logging , was invented to address exactly this type of problem. In ﬁle systems, we usually call write-ahead log ging jour- naling for historical reasons.",3073
42. FSCK and Journaling,"The ﬁrst ﬁle system to do this was Cedar [H87], though many modern ﬁle systems use the idea, including L inux ext3 and ext4, reiserfs, IBM’s JFS, SGI’s XFS, and Windows NTFS. The basic idea is as follows. When updating the disk, before over- writing the structures in place, ﬁrst write down a little note ( somewhere else on the disk, in a well-known location) describing what you are about to do. Writing this note is the “write ahead” part, and we write i t to a structure that we organize as a “log”; hence, write-ahead loggi ng. By writing the note to disk, you are guaranteeing that if a crash takes places during the update (overwrite) of the structures you are u pdating, you can go back and look at the note you made and try again; thus, you will know exactly what to ﬁx (and how to ﬁx it) after a crash, inst ead of having to scan the entire disk. By design, journaling thus ad ds a bit of work during updates to greatly reduce the amount of work require d during recovery. We’ll now describe how Linux ext3 , a popular journaling ﬁle system, incorporates journaling into the ﬁle system. Most of the on-disk st ruc- tures are identical to Linux ext2 , e.g., the disk is divided into block groups, and each block group contains an inode bitmap, data bitmap, inodes , and data blocks. The new key structure is the journal itself, which occupies some small amount of space within the partition or on another device. Thus, an ext2 ﬁle system (without journaling) looks like this: Super Group 0 Group 1 . . . Group N Assuming the journal is placed within the same ﬁle system imag e (though sometimes it is placed on a separate device, or as a ﬁle wit hin the ﬁle system), an ext3 ﬁle system with a journal looks like this : Super Journal Group 0 Group 1 . . . Group N The real difference is just the presence of the journal, and of cou rse, how it is used. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 C RASH CONSISTENCY : FSCK AND JOURNALING Data Journaling Let’s look at a simple example to understand how data journaling works. Data journaling is available as a mode with the Linux ext3 ﬁle sy stem, from which much of this discussion is based. Say we have our canonical update again, where we wish to write the inode (I[v2]), bitmap (B[v2]), and data block (Db) to disk agai n. Before writing them to their ﬁnal disk locations, we are now ﬁrst going to write them to the log (a.k.a. journal). This is what this will look like i n the log:JournalTxB I[v2] B[v2] Db TxE You can see we have written ﬁve blocks here. The transaction begi n (TxB) tells us about this update, including information about th e pend- ing update to the ﬁle system (e.g., the ﬁnal addresses of the bl ocks I[v2], B[v2], and Db), and some kind of transaction identiﬁer (TID ). The mid- dle three blocks just contain the exact contents of the blocks them selves; this is known as physical logging as we are putting the exact physical contents of the update in the journal (an alternate idea, logical logging , puts a more compact logical representation of the update in the jour nal, e.g., “this update wishes to append data block Db to ﬁle X”, whi ch is a little more complex but can save space in the log and perhaps impr ove performance).",3234
42. FSCK and Journaling,"The ﬁnal block (TxE) is a marker of the end of this tr ansac- tion, and will also contain the TID. Once this transaction is safely on disk, we are ready to overwrit e the old structures in the ﬁle system; this process is called checkpointing . Thus, to checkpoint the ﬁle system (i.e., bring it up to date with the pend- ing update in the journal), we issue the writes I[v2], B[v2], a nd Db to their disk locations as seen above; if these writes complete succ essfully, we have successfully checkpointed the ﬁle system and are basi cally done. Thus, our initial sequence of operations: 1.Journal write: Write the transaction, including a transaction-begin block, all pending data and metadata updates, and a transacti on- end block, to the log; wait for these writes to complete. 2.Checkpoint: Write the pending metadata and data updates to their ﬁnal locations in the ﬁle system. In our example, we would write TxB, I[v2], B[v2], Db, and TxE to t he journal ﬁrst. When these writes complete, we would complete the u pdate by checkpointing I[v2], B[v2], and Db, to their ﬁnal locations on disk. Things get a little trickier when a crash occurs during the wri tes to the journal. Here, we are trying to write the set of blocks in the t ransac- tion (e.g., TxB, I[v2], B[v2], Db, TxE) to disk. One simple way to do this would be to issue each one at a time, waiting for each to complete, a nd then issuing the next. However, this is slow. Ideally, we’d like to issue OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 9 ASIDE : FORCING WRITES TODISK To enforce ordering between two disk writes, modern ﬁle systems have to take a few extra precautions. In olden times, forcing ordering between two writes, AandB, was easy: just issue the write of Ato the disk, wait for the disk to interrupt the OS when the write is complete, and t hen issue the write of B. Things got slightly more complex due to the increased use of write caches within disks. With write buffering enabled (sometimes calle dimmediate reporting ), a disk will inform the OS the write is complete when it simply has been placed in the disk’s memory cache, and has not yet reache d disk. If the OS then issues a subsequent write, it is not guaran teed to reach the disk after previous writes; thus ordering between wr ites is not preserved. One solution is to disable write buffering. However , more modern systems take extra precautions and issue explicit write barriers ; such a barrier, when it completes, guarantees that all writes issued before the barrier will reach disk before any writes issued after the barrier. All of this machinery requires a great deal of trust in the correc t oper- ation of the disk. Unfortunately, recent research shows that some disk manufacturers, in an effort to deliver “higher performing” di sks, explic- itly ignore write-barrier requests, thus making the disks se emingly run faster but at the risk of incorrect operation [C+13, R+11]. As Kah an said, the fast almost always beats out the slow, even if the fast is wrong .",3063
42. FSCK and Journaling,"all ﬁve block writes at once, as this would turn ﬁve writes into a s ingle sequential write and thus be faster. However, this is unsafe, for the fol- lowing reason: given such a big write, the disk internally may p erform scheduling and complete small pieces of the big write in any orde r. Thus, the disk internally may (1) write TxB, I[v2], B[v2], and TxE a nd only later (2) write Db. Unfortunately, if the disk loses power between (1) and (2), this is what ends up on disk: JournalTxB id=1I[v2] B[v2] ?? TxE id=1 Why is this a problem? Well, the transaction looks like a valid tra ns- action (it has a begin and an end with matching sequence number s). Fur- ther, the ﬁle system can’t look at that fourth block and know it is wron g; after all, it is arbitrary user data. Thus, if the system now re boots and runs recovery, it will replay this transaction, and ignorantly copy the con- tents of the garbage block ’??’ to the location where Db is supposed t o live. This is bad for arbitrary user data in a ﬁle; it is much wors e if it hap- pens to a critical piece of ﬁle system, such as the superblock, w hich could render the ﬁle system unmountable. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 C RASH CONSISTENCY : FSCK AND JOURNALING ASIDE : OPTIMIZING LOGWRITES You may have noticed a particular inefﬁciency of writing to the l og. Namely, the ﬁle system ﬁrst has to write out the transaction-be gin block and contents of the transaction; only after these writes complete can the ﬁle system send the transaction-end block to disk. The performa nce im- pact is clear, if you think about how a disk works: usually an extra rota- tion is incurred (think about why). One of our former graduate students, Vijayan Prabhakaran, had a simple idea to ﬁx this problem [P+05]. When writing a transaction to th e journal, include a checksum of the contents of the journal in the begin and e nd blocks. Doing so enables the ﬁle system to write the entire tran saction at once, without incurring a wait; if, during recovery, the ﬁle sys tem sees a mismatch in the computed checksum versus the stored checksum in the transaction, it can conclude that a crash occurred during th e write of the transaction and thus discard the ﬁle-system update. Thu s, with a small tweak in the write protocol and recovery system, a ﬁle syste m can achieve faster common-case performance; on top of that, the system is slightly more reliable, as any reads from the journal are now prote cted by a checksum. This simple ﬁx was attractive enough to gain the notice of Linux ﬁ le sys- tem developers, who then incorporated it into the next generati on Linux ﬁle system, called (you guessed it.) Linux ext4 . It now ships on mil- lions of machines worldwide, including the Android handheld pla tform. Thus, every time you write to disk on many Linux-based systems, a little code developed at Wisconsin makes your system a little faster and more reliable. To avoid this problem, the ﬁle system issues the transactional w rite in two steps.",3026
42. FSCK and Journaling,"First, it writes all blocks except the TxE block to th e journal, issuing these writes all at once. When these writes complete, t he journal will look something like this (assuming our append workload again) :JournalTxB id=1I[v2] B[v2] Db When those writes complete, the ﬁle system issues the write of th e TxE block, thus leaving the journal in this ﬁnal, safe state: JournalTxB id=1I[v2] B[v2] Db TxE id=1 An important aspect of this process is the atomicity guarantee pr o- vided by the disk. It turns out that the disk guarantees that an y 512-byte OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 11 write will either happen or not (and never be half-written); th us, to make sure the write of TxE is atomic, one should make it a single 512-byt e block. Thus, our current protocol to update the ﬁle system, with each of it s three phases labeled: 1.Journal write: Write the contents of the transaction (including TxB, metadata, and data) to the log; wait for these writes to complete . 2.Journal commit: Write the transaction commit block (containing TxE) to the log; wait for write to complete; transaction is said to be committed . 3.Checkpoint: Write the contents of the update (metadata and data) to their ﬁnal on-disk locations. Recovery Let’s now understand how a ﬁle system can use the contents of the jour - nal to recover from a crash. A crash may happen at any time during this sequence of updates. If the crash happens before the transacti on is writ- ten safely to the log (i.e., before Step 2 above completes), then our job is easy: the pending update is simply skipped. If the crash ha ppens af- ter the transaction has committed to the log, but before the check point is complete, the ﬁle system can recover the update as follows. When the system boots, the ﬁle system recovery process will scan the log and look for transactions that have committed to the disk; these transac tions are thus replayed (in order), with the ﬁle system again attempting to write out the blocks in the transaction to their ﬁnal on-disk locations. T his form of logging is one of the simplest forms there is, and is called redo logging . By recovering the committed transactions in the journal, the ﬁle system ensures that the on-disk structures are consistent, and thus c an proceed by mounting the ﬁle system and readying itself for new requests . Note that it is ﬁne for a crash to happen at any point during check- pointing, even after some of the updates to the ﬁnal locations of the blocks have completed. In the worst case, some of these updates are simpl y per- formed again during recovery. Because recovery is a rare operati on (only taking place after an unexpected system crash), a few redund ant writes are nothing to worry about3. Batching Log Updates You might have noticed that the basic protocol could add a lot of extra disk trafﬁc. For example, imagine we create two ﬁles in a row, ca lled file1 andfile2 , in the same directory. To create one ﬁle, one has to update a number of on-disk structures, minimally including : the in- ode bitmap (to allocate a new inode), the newly-created inode of th e ﬁle, 3Unless you worry about everything, in which case we can’t help you.",3228
42. FSCK and Journaling,"Stop worrying so much, it is unhealthy. But now you’re probably worried about over-wo rrying. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 C RASH CONSISTENCY : FSCK AND JOURNALING the data block of the parent directory containing the new director y en- try, and the parent directory inode (which now has a new modiﬁcati on time). With journaling, we logically commit all of this informati on to the journal for each of our two ﬁle creations; because the ﬁles are i n the same directory, and assuming they even have inodes within the s ame in- ode block, this means that if we’re not careful, we’ll end up writin g these same blocks over and over. To remedy this problem, some ﬁle systems do not commit each update to disk one at a time (e.g., Linux ext3); rather, one can buffer a ll updates into a global transaction. In our example above, when the two ﬁles are created, the ﬁle system just marks the in-memory inode bitmap, inodes of the ﬁles, directory data, and directory inode as dirty, and add s them to the list of blocks that form the current transaction. When it is ﬁn ally time to write these blocks to disk (say, after a timeout of 5 seconds), t his single global transaction is committed containing all of the updates des cribed above. Thus, by buffering updates, a ﬁle system can avoid exces sive write trafﬁc to disk in many cases. Making The Log Finite We thus have arrived at a basic protocol for updating ﬁle-system on -disk structures. The ﬁle system buffers updates in memory for some ti me; when it is ﬁnally time to write to disk, the ﬁle system ﬁrst car efully writes out the details of the transaction to the journal (a.k.a. write-a head log); after the transaction is complete, the ﬁle system checkpoints t hose blocks to their ﬁnal locations on disk. However, the log is of a ﬁnite size. If we keep adding transactions to it (as in this ﬁgure), it will soon ﬁll. What do you think happens t hen?JournalTx1 Tx2 Tx3 Tx4 Tx5 ... Two problems arise when the log becomes full. The ﬁrst is simpler , but less critical: the larger the log, the longer recovery will t ake, as the recovery process must replay all the transactions within the log (in order) to recover. The second is more of an issue: when the log is full (or nea rly full), no further transactions can be committed to the disk, th us making the ﬁle system “less than useful” (i.e., useless). To address these problems, journaling ﬁle systems treat the log as a circular data structure, re-using it over and over; this is why the journal is sometimes referred to as a circular log . To do so, the ﬁle system must take action some time after a checkpoint. Speciﬁcally, once a tran saction has been checkpointed, the ﬁle system should free the space it w as occu- pying within the journal, allowing the log space to be reused. Th ere are many ways to achieve this end; for example, you could simply mark the OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 13 oldest and newest non-checkpointed transactions in the log in a journal superblock ; all other space is free.",3103
42. FSCK and Journaling,"Here is a graphical depiction:JournalJournal SuperTx1 Tx2 Tx3 Tx4 Tx5 ... In the journal superblock (not to be confused with the main ﬁle sys tem superblock), the journaling system records enough information to know which transactions have not yet been checkpointed, and thus red uces re- covery time as well as enables re-use of the log in a circular fash ion. And thus we add another step to our basic protocol: 1.Journal write: Write the contents of the transaction (containing TxB and the contents of the update) to the log; wait for these writes to complete. 2.Journal commit: Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction is now committed . 3.Checkpoint: Write the contents of the update to their ﬁnal locations within the ﬁle system. 4.Free: Some time later, mark the transaction free in the journal by updating the journal superblock. Thus we have our ﬁnal data journaling protocol. But there is still a problem: we are writing each data block to the disk twice , which is a heavy cost to pay, especially for something as rare as a system cr ash. Can you ﬁgure out a way to retain consistency without writing data twi ce? Metadata Journaling Although recovery is now fast (scanning the journal and replayin g a few transactions as opposed to scanning the entire disk), normal oper ation of the ﬁle system is slower than we might desire. In particular, for each write to disk, we are now also writing to the journal ﬁrst, thus d oubling write trafﬁc; this doubling is especially painful during seq uential write workloads, which now will proceed at half the peak write bandwidt h of the drive. Further, between writes to the journal and writes t o the main ﬁle system, there is a costly seek, which adds noticeable overhe ad for some workloads. Because of the high cost of writing every data block to disk twice, peo- ple have tried a few different things in order to speed up perfor mance. For example, the mode of journaling we described above is often call ed data journaling (as in Linux ext3), as it journals all user data (in addition to the metadata of the ﬁle system). A simpler (and more common) form of journaling is sometimes called ordered journaling (or just metadata c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 C RASH CONSISTENCY : FSCK AND JOURNALING journaling ), and it is nearly the same, except that user data is notwrit- ten to the journal. Thus, when performing the same update as abov e, the following information would be written to the journal: JournalTxB I[v2] B[v2] TxE The data block Db, previously written to the log, would instead be written to the ﬁle system proper, avoiding the extra write; giv en that most I/O trafﬁc to the disk is data, not writing data twice substant ially reduces the I/O load of journaling. The modiﬁcation does raise an interesti ng question, though: when should we write data blocks to disk? Let’s again consider our example append of a ﬁle to understand the problem better.",3020
42. FSCK and Journaling,"The update consists of three blocks: I[v2], B[v2 ], and Db. The ﬁrst two are both metadata and will be logged and then che ck- pointed; the latter will only be written once to the ﬁle system. W hen should we write Db to disk? Does it matter? As it turns out, the ordering of the data write does matter for metad ata- only journaling. For example, what if we write Db to disk after the trans- action (containing I[v2] and B[v2]) completes? Unfortunately, this ap- proach has a problem: the ﬁle system is consistent but I[v2] may e nd up pointing to garbage data. Speciﬁcally, consider the case where I[v2] and B[v2] are written but Db did not make it to disk. The ﬁle system w ill then try to recover. Because Db is notin the log, the ﬁle system will replay writes to I[v2] and B[v2], and produce a consistent ﬁle system ( from the perspective of ﬁle-system metadata). However, I[v2] will be p ointing to garbage data, i.e., at whatever was in the slot where Db was hea ded. To ensure this situation does not arise, some ﬁle systems (e.g., L inux ext3) write data blocks (of regular ﬁles) to the disk ﬁrst, before related metadata is written to disk. Speciﬁcally, the protocol is as follow s: 1.Data write: Write data to ﬁnal location; wait for completion (the wait is optional; see below for details). 2.Journal metadata write: Write the begin block and metadata to the log; wait for writes to complete. 3.Journal commit: Write the transaction commit block (containing TxE) to the log; wait for the write to complete; the transaction (i n- cluding data) is now committed . 4.Checkpoint metadata: Write the contents of the metadata update to their ﬁnal locations within the ﬁle system. 5.Free: Later, mark the transaction free in journal superblock. By forcing the data write ﬁrst, a ﬁle system can guarantee that a pointer will never point to garbage. Indeed, this rule of “write the poin ted-to object before the object that points to it” is at the core of crash cons is- tency, and is exploited even further by other crash consistency schemes [GP94] (see below for details). OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 15 In most systems, metadata journaling (akin to ordered journalin g of ext3) is more popular than full data journaling. For example, Win dows NTFS and SGI’s XFS both use some form of metadata journaling. Linux ext3 gives you the option of choosing either data, ordered, or unordere d modes (in unordered mode, data can be written at any time). All of t hese modes keep metadata consistent; they vary in their semantics f or data. Finally, note that forcing the data write to complete (Step 1) bef ore is- suing writes to the journal (Step 2) is not required for correctnes s, as indi- cated in the protocol above. Speciﬁcally, it would be ﬁne to concurre ntly issue writes to data, the transaction-begin block, and journal ed metadata; the only real requirement is that Steps 1 and 2 complete before the issuing of the journal commit block (Step 3). Tricky Case: Block Reuse There are some interesting corner cases that make journaling mor e tricky, and thus are worth discussing.",3146
42. FSCK and Journaling,"A number of them revolve around bloc k reuse; as Stephen Tweedie (one of the main forces behind ext3) sai d: “What’s the hideous part of the entire system? ... It’s deletin g ﬁles. Everything to do with delete is hairy. Everything to do with d elete... you have nightmares around what happens if blocks get deleted a nd then reallocated.” [T00] The particular example Tweedie gives is as follows. Suppose you ar e using some form of metadata journaling (and thus data blocks for ﬁle s arenotjournaled). Let’s say you have a directory called foo. The user adds an entry to foo (say by creating a ﬁle), and thus the contents of foo (because directories are considered metadata) are written to the log; assume the location of the foo directory data is block 1000. The log thus contains something like this: JournalTxB id=1I[foo] ptr:1000D[foo] [final addr:1000]TxE id=1 At this point, the user deletes everything in the directory and the di- rectory itself, freeing up block 1000 for reuse. Finally, the us er creates a new ﬁle (say foobar ), which ends up reusing the same block (1000) that used to belong to foo. The inode of foobar is committed to disk, as is its data; note, however, because metadata journaling is in use, only the inode offoobar is committed to the journal; the newly-written data in block 1000 in the ﬁle foobar isnotjournaled.JournalTxB id=1I[foo] ptr:1000D[foo] [final addr:1000]TxE id=1TxB id=2I[foobar] ptr:1000TxE id=2 c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 C RASH CONSISTENCY : FSCK AND JOURNALING Journal File System TxB Contents TxE Metadata Data (metadata) (data) issue issue issue complete complete complete issue complete issue issue complete complete Figure 42.1: Data Journaling Timeline Now assume a crash occurs and all of this information is still in the log. During replay, the recovery process simply replays everyt hing in the log, including the write of directory data in block 1000; the r eplay thus overwrites the user data of current ﬁle foobar with old directory contents. Clearly this is not a correct recovery action, and certa inly it will be a surprise to the user when reading the ﬁle foobar . There are a number of solutions to this problem. One could, for ex- ample, never reuse blocks until the delete of said blocks is chec kpointed out of the journal. What Linux ext3 does instead is to add a new type of record to the journal, known as a revoke record. In the case above, deleting the directory would cause a revoke record to be written t o the journal. When replaying the journal, the system ﬁrst scans for s uch re- voke records; any such revoked data is never replayed, thus avoid ing the problem mentioned above. Wrapping Up Journaling: A Timeline Before ending our discussion of journaling, we summarize the protoc ols we have discussed with timelines depicting each of them. Figu re 42.1 shows the protocol when journaling data and metadata, whereas Fig ure 42.2 shows the protocol when journaling only metadata. In each ﬁgure, time increases in the downward direction, and ea ch row in the ﬁgure shows the logical time that a write can be issued or mi ght complete. For example, in the data journaling protocol (Figure 42.",3205
42. FSCK and Journaling,"1), the writes of the transaction begin block (TxB) and the contents of the trans- action can logically be issued at the same time, and thus can be c ompleted in any order; however, the write to the transaction end block (TxE ) must not be issued until said previous writes complete. Similarly, th e check- pointing writes to data and metadata blocks cannot begin until t he trans- action end block has committed. Horizontal dashed lines show where write-ordering requirements must be obeyed. A similar timeline is shown for the metadata journaling protocol. N ote that the data write can logically be issued at the same time as t he writes OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 17 Journal File System TxB Contents TxE Metadata Data (metadata) issue issue issue complete complete complete issue complete issue complete Figure 42.2: Metadata Journaling Timeline to the transaction begin and the contents of the journal; however, it must be issued and complete before the transaction end has been issue d. Finally, note that the time of completion marked for each write in t he timelines is arbitrary. In a real system, completion time is de termined by the I/O subsystem, which may reorder writes to improve performa nce. The only guarantees about ordering that we have are those that mus t be enforced for protocol correctness (and are shown via the horizontal dashed lines in the ﬁgures). 42.4 Solution #3: Other Approaches We’ve thus far described two options in keeping ﬁle system metad ata consistent: a lazy approach based on fsck , and a more active approach known as journaling. However, these are not the only two approaches . One such approach, known as Soft Updates [GP94], was introduced by Ganger and Patt. This approach carefully orders all writes to t he ﬁle sys- tem to ensure that the on-disk structures are never left in an i nconsis- tent state. For example, by writing a pointed-to data block to di skbefore the inode that points to it, we can ensure that the inode never poin ts to garbage; similar rules can be derived for all the structures of the ﬁle sys- tem. Implementing Soft Updates can be a challenge, however; whe reas the journaling layer described above can be implemented with r elatively little knowledge of the exact ﬁle system structures, Soft Update s requires intricate knowledge of each ﬁle system data structure and thus adds a fair amount of complexity to the system. Another approach is known as copy-on-write (yes, COW ), and is used in a number of popular ﬁle systems, including Sun’s ZFS [B07]. Thi s tech- nique never overwrites ﬁles or directories in place; rather, it places new updates to previously unused locations on disk. After a number of u p- dates are completed, COW ﬁle systems ﬂip the root structure of the ﬁle system to include pointers to the newly updated structures. D oing so makes keeping the ﬁle system consistent straightforward. We’l l be learn- ing more about this technique when we discuss the log-structure d ﬁle system (LFS) in a future chapter; LFS is an early example of a COW .",3100
42. FSCK and Journaling,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 C RASH CONSISTENCY : FSCK AND JOURNALING Another approach is one we just developed here at Wisconsin. In thi s technique, entitled backpointer-based consistency (orBBC ), no ordering is enforced between writes. To achieve consistency, an additi onal back pointer is added to every block in the system; for example, each data block has a reference to the inode to which it belongs. When acces sing a ﬁle, the ﬁle system can determine if the ﬁle is consistent by c hecking if the forward pointer (e.g., the address in the inode or direct block ) points to a block that refers back to it. If so, everything must have saf ely reached disk and thus the ﬁle is consistent; if not, the ﬁle is inconsiste nt, and an error is returned. By adding back pointers to the ﬁle system, a n ew form of lazy crash consistency can be attained [C+12]. Finally, we also have explored techniques to reduce the numbe r of times a journal protocol has to wait for disk writes to complete. Ent itled optimistic crash consistency [C+13], this new approach issues as many writes to disk as possible by using a generalized form of the transaction checksum [P+05], and includes a few other techniques to detect incon- sistencies should they arise. For some workloads, these optimisti c tech- niques can improve performance by an order of magnitude. However, to truly function well, a slightly different disk interface is r equired [C+13]. 42.5 Summary We have introduced the problem of crash consistency, and discuss ed various approaches to attacking this problem. The older approach of building a ﬁle system checker works but is likely too slow to recov er on modern systems. Thus, many ﬁle systems now use journaling. Journ aling reduces recovery time from O(size-of-the-disk-volume) to O(si ze-of-the- log), thus speeding recovery substantially after a crash and r estart. For this reason, many modern ﬁle systems use journaling. We have als o seen that journaling can come in many different forms; the most commonly used is ordered metadata journaling, which reduces the amount of trafﬁc to the journal while still preserving reasonable consistency g uarantees for both ﬁle system metadata and user data. In the end, strong guara ntees on user data are probably one of the most important things to provide; oddly enough, as recent research has shown, this area remains a w ork in progress [P+14]. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 19 References [B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available online: http://www.ostep.org/Citations/zfs_last.pdf .ZFS uses copy-on-write and journal- ing, actually, as in some cases, logging writes to disk will perform better . [C+12] “Consistency Without Ordering” by Vijay Chidambaram, Tushar S harma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. FAST ’12, San Jose, Cal ifornia. A recent paper of ours about a new form of crash consistency based on back pointers.",3032
42. FSCK and Journaling,"Read it for the exciting details. [C+13] “Optimistic Crash Consistency” by Vijay Chidambaram, Thanu S. Pillai, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau . SOSP ’13, Nemacolin Woo dlands Resort, PA, November 2013. Our work on a more optimistic and higher performance journaling protocol. For workloads that call fsync() a lot, performance can be greatly improved. [GP94] “Metadata Update Performance in File Systems” by Gregory R. G anger and Yale N. Patt. OSDI ’94. A clever paper about using careful ordering of writes as the main way to achie ve consistency. Implemented later in BSD-based systems. [G+08] “SQCK: A Declarative File System Checker” by Haryadi S. Guna wi, Abhishek Ra- jimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. OSDI ’08, San Diego, Califor- nia. Our own paper on a new and better way to build a ﬁle system checker using SQL queries. We also show some problems with the existing checker, ﬁnding numerous bugs and odd behaviors, a direct result of the complexity of fsck . [H87] “Reimplementing the Cedar File System Using Logging and Group Commit” by Robert Hagmann. SOSP ’87, Austin, Texas, November 1987. The ﬁrst work (that we know of) that applied write-ahead logging (a.k.a. journaling) to a ﬁle system. [M+13] “ffsck: The Fast File System Checker” by Ao Ma, Chris Dragga, Andre a C. Arpaci- Dusseau, Remzi H. Arpaci-Dusseau. FAST ’13, San Jose, California , February 2013. A recent paper of ours detailing how to make fsck an order of magnitude faster. Some of the ide as have already been incorporated into the BSD ﬁle system checker [MK96] and are deployed tod ay. [MK96] “Fsck – The U NIX File System Check Program” by Marshall Kirk McKusick and T. J. Kowalski. Revised in 1996. Describes the ﬁrst comprehensive ﬁle-system checking tool, the epony- mousfsck . Written by some of the same people who brought you FFS. [MJLF84] “A Fast File System for UNIX” by Marshall K. McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry. ACM Transactions on Computing Systems, Volume 2:3, August 1984. You already know enough about FFS, right? But come on, it is OK to re-referenc e important papers. [P+14] “All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications” by Thanumalayan Sankaranarayana Pillai, Vijay Chidamb aram, Ramnatthan Alagappan, Samer Al-Kiswany, Andrea C. Arpaci-Dusseau, Remzi H. A rpaci-Dusseau. OSDI ’14, Broomﬁeld, Colorado, October 2014. A paper in which we study what ﬁle systems guarantee after crashes, and show that applications expect something different, leadi ng to all sorts of interesting problems. [P+05] “IRON File Systems” by Vijayan Prabhakaran, Lakshmi N. Baira vasundaram, Nitin Agrawal, Haryadi S. Gunawi, Andrea C. Arpaci-Dusseau, Remzi H. A rpaci-Dusseau. SOSP ’05, Brighton, England, October 2005. A paper mostly focused on studying how ﬁle systems react to disk failures. Towards the end, we introduce a transaction checksum to spe ed up logging, which was eventually adopted into Linux ext4. [PAA05] “Analysis and Evolution of Journaling File Systems” by Vij ayan Prabhakaran, Andrea C.",3117
42. FSCK and Journaling,"Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. USENIX ’05, Anaheim, California, April 2005. An early paper we wrote analyzing how journaling ﬁle systems work. [R+11] “Coerced Cache Eviction and Discreet-Mode Journaling” by Abhishek R ajimwale, Vijay Chidambaram, Deepak Ramamurthi, Andrea C. Arpaci-Dusseau, Remz i H. Arpaci-Dusseau. DSN ’11, Hong Kong, China, June 2011. Our own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to disk, even when explicitly told not to do that. Our solution to overcome this problem: if you want Ato be written to disk before B, ﬁrst write A, then send a lot of “dummy” writes to disk, hopefully causing Ato be forced to disk to make room for them in the cache. A neat if impractical solution. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 20 C RASH CONSISTENCY : FSCK AND JOURNALING [T98] “Journaling the Linux ext2fs File System” by Stephen C. Tweedie. The Fourth Annual Linux Expo, May 1998. Tweedie did much of the heavy lifting in adding journaling to the Linux e xt2 ﬁle system; the result, not surprisingly, is called ext3. Some nice d esign decisions include the strong focus on backwards compatibility, e.g., you can just add a journaling ﬁle to an e xisting ext2 ﬁle system and then mount it as an ext3 ﬁle system. [T00] “EXT3, Journaling Filesystem” by Stephen Tweedie. Talk at the Ot tawa Linux Sympo- sium, July 2000. olstrans.sourceforge.net/release/OLS2000-ext 3/OLS2000-ext3.html A tran- script of a talk given by Tweedie on ext3. [T01] “The Linux ext2 File System” by Theodore Ts’o, June, 2001.. Avail able online here: http://e2fsprogs.sourceforge.net/ext2.html .A simple Linux ﬁle system based on the ideas found in FFS. For a while it was quite heavily used; now it is re ally just in the kernel as an example of a simple ﬁle system. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG CRASH CONSISTENCY : FSCK AND JOURNALING 21 Homework (Simulation) This section introduces fsck.py , a simple simulator you can use to better understand how ﬁle system corruptions can be detected (a nd po- tentially repaired). Please see the associated README for det ails on how to run the simulator. Questions 1. First, run fsck.py -D ; this ﬂag turns off any corruption, and thus you can use it to generate a random ﬁle system, and see if you can determin e which ﬁles and directories are in there. So, go ahead and do that. Use the-pﬂag to see if you were right. Try this for a few different randomly- generated ﬁle systems by setting the seed ( -s) to different values, like 1, 2, and 3. 2. Now, let’s introduce a corruption. Run fsck.py -S 1 to start. Can you see what inconsistency is introduced? How would you ﬁx it in a re al ﬁle system repair tool? Use -cto check if you were right. 3. Change the seed to -S 3 or-S 19 ; which inconsistency do you see? Use -cto check your answer. What is different in these two cases? 4. Change the seed to -S 5 ; which inconsistency do you see? How hard would it be to ﬁx this problem in an automatic way? Use -cto check your answer. Then, introduce a similar inconsistency with -S 38 ; is this harder/possible to detect? Finally, use -S 642 ; is this inconsistency de- tectable? If so, how would you ﬁx the ﬁle system? 5. Change the seed to -S 6 or-S 13 ; which inconsistency do you see? Use -c to check your answer. What is the difference across these two c ases? What should the repair tool do when encountering such a situation? 6. Change the seed to -S 9 ; which inconsistency do you see? Use -cto check your answer. Which piece of information should a check-and-rep air tool trust in this case? 7. Change the seed to -S 15 ; which inconsistency do you see? Use -cto check your answer. What can a repair tool do in this case? If no repair is possible, how much data is lost? 8. Change the seed to -S 10 ; which inconsistency do you see? Use -cto check your answer. Is there redundancy in the ﬁle system structure here that can help a repair? 9. Change the seed to -S 16 and-S 20 ; which inconsistency do you see? Use -cto check your answer. How should the repair tool ﬁx the problem? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4171
43. Log-structured File System LFS,"43 Log-structured File Systems In the early 90’s, a group at Berkeley led by Professor John Ousterh out and graduate student Mendel Rosenblum developed a new ﬁle syst em known as the log-structured ﬁle system [RO91]. Their motivation to do so was based on the following observations: •System memories are growing : As memory gets bigger, more data can be cached in memory. As more data is cached, disk trafﬁc in- creasingly consists of writes, as reads are serviced by the cac he. Thus, ﬁle system performance is largely determined by its wri te performance. •There is a large gap between random I/O performance and se- quential I/O performance : Hard-drive transfer bandwidth has in- creased a great deal over the years [P98]; as more bits are packe d into the surface of a drive, the bandwidth when accessing said bits increases. Seek and rotational delay costs, however, have decrea sed slowly; it is challenging to make cheap and small motors spin the platters faster or move the disk arm more quickly. Thus, if you are able to use disks in a sequential manner, you gain a sizeable pe rfor- mance advantage over approaches that cause seeks and rotations. •Existing ﬁle systems perform poorly on many common workload s: For example, FFS [MJLF84] would perform a large number of writes to create a new ﬁle of size one block: one for a new inode, one to update the inode bitmap, one to the directory data block that the ﬁle is in, one to the directory inode to update it, one to the new dat a block that is a part of the new ﬁle, and one to the data bitmap to mark the data block as allocated. Thus, although FFS places all of these blocks within the same block group, FFS incurs many short seeks and subsequent rotational delays and thus performance fa lls far short of peak sequential bandwidth. •File systems are not RAID-aware : For example, both RAID-4 and RAID-5 have the small-write problem where a logical write to a single block causes 4 physical I/Os to take place. Existing ﬁl e sys- tems do not try to avoid this worst-case RAID writing behavior. 1 2 LOG-STRUCTURED FILESYSTEMS TIP: DETAILS MATTER All interesting systems are comprised of a few general ideas an d a num- ber of details. Sometimes, when you are learning about these syste ms, you think to yourself “Oh, I get the general idea; the rest is jus t details,” and you use this to only half-learn how things really work. Don’t do t his. Many times, the details are critical. As we’ll see with LFS, the general idea is easy to understand, but to really build a working system, you have to think through allof the tricky cases. An ideal ﬁle system would thus focus on write performance, and try to make use of the sequential bandwidth of the disk. Further, it would perform well on common workloads that not only write out data but also update on-disk metadata structures frequently. Finally, it would work well on RAIDs as well as single disks. The new type of ﬁle system Rosenblum and Ousterhout introduced was called LFS, short for the Log-structured File System . When writ- ing to disk, LFS ﬁrst buffers all updates (including metadat a.) in an in- memory segment ; when the segment is full, it is written to disk in one long, sequential transfer to an unused part of the disk. LFS nev er over- writes existing data, but rather always writes segments to free locations.",3345
43. Log-structured File System LFS,"Because segments are large, the disk (or RAID) is used efﬁcien tly, and performance of the ﬁle system approaches its zenith. THECRUX: HOWTOMAKE ALLWRITES SEQUENTIAL WRITES ? How can a ﬁle system transform all writes into sequential write s? For reads, this task is impossible, as the desired block to be read m ay be any- where on disk. For writes, however, the ﬁle system always has a ch oice, and it is exactly this choice we hope to exploit. 43.1 Writing To Disk Sequentially We thus have our ﬁrst challenge: how do we transform all updates t o ﬁle-system state into a series of sequential writes to disk? T o understand this better, let’s use a simple example. Imagine we are writin g a data block Dto a ﬁle. Writing the data block to disk might result in the follow ing on-disk layout, with Dwritten at disk address A0: D A0 OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 3 However, when a user writes a data block, it is not only data that ge ts written to disk; there is also other metadata that needs to be updated. In this case, let’s also write the inode (I) of the ﬁle to disk, and have it point to the data block D. When written to disk, the data block and inode would look something like this (note that the inode looks as big as the data block, which generally isn’t the case; in most systems, dat a blocks are 4 KB in size, whereas an inode is much smaller, around 128 byt es): D A0Iblk[0]:A0 This basic idea, of simply writing all updates (such as data bl ocks, inodes, etc.) to the disk sequentially, sits at the heart of LFS. If you un- derstand this, you get the basic idea. But as with all complicat ed systems, the devil is in the details. 43.2 Writing Sequentially And Effectively Unfortunately, writing to disk sequentially is not (alone) enou gh to guarantee efﬁcient writes. For example, imagine if we wrote a s ingle block to address A, at time T. We then wait a little while, and write to the disk at address A+ 1 (the next block address in sequential order), but at time T+δ. In-between the ﬁrst and second writes, unfortunately, the disk has rotated; when you issue the second write, it will thu s wait for most of a rotation before being committed (speciﬁcally, if the rot ation takes time Trotation , the disk will wait Trotation−δbefore it can commit the second write to the disk surface). And thus you can hopefully see that simply writing to disk in sequential order is not enough to a chieve peak performance; rather, you must issue a large number of contiguous writes (or one large write) to the drive in order to achieve good wri te performance. To achieve this end, LFS uses an ancient technique known as write buffering1. Before writing to the disk, LFS keeps track of updates in memory; when it has received a sufﬁcient number of updates, it w rites them to disk all at once, thus ensuring efﬁcient use of the disk. The large chunk of updates LFS writes at one time is referred to b y the name of a segment . Although this term is over-used in computer systems, here it just means a large-ish chunk which LFS uses t o group writes. Thus, when writing to disk, LFS buffers updates in an in-memory 1Indeed, it is hard to ﬁnd a good citation for this idea, since it was like ly invented by many and very early on in the history of computing. For a study of the beneﬁt s of write buffering, see Solworth and Orji [SO90]; to learn about its potential harms, see Mogul [M94].",3437
43. Log-structured File System LFS,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LOG-STRUCTURED FILESYSTEMS segment, and then writes the segment all at once to the disk. As long as the segment is large enough, these writes will be efﬁcient. Here is an example, in which LFS buffers two sets of updates int o a small segment; actual segments are larger (a few MB). The ﬁrs t update is of four block writes to ﬁle j; the second is one block being added to ﬁle k. LFS then commits the entire segment of seven blocks to disk at once . The resulting on-disk layout of these blocks is as follows: D[j,0] A0D[j,1] A1D[j,2] A2D[j,3] A3blk[0]:A0 blk[1]:A1 blk[2]:A2 blk[3]:A3 Inode[j]D[k,0] A5blk[0]:A5 Inode[k] 43.3 How Much To Buffer? This raises the following question: how many updates should LFS buffer before writing to disk? The answer, of course, depends on t he disk itself, speciﬁcally how high the positioning overhead is in compa rison to the transfer rate; see the FFS chapter for a similar analysis. For example, assume that positioning (i.e., rotation and seek over - heads) before each write takes roughly Tposition seconds. Assume further that the disk transfer rate is Rpeak MB/s. How much should LFS buffer before writing when running on such a disk? The way to think about this is that every time you write, you pay a ﬁxed overhead of the positioning cost. Thus, how much do you have to write in order to amortize that cost? The more you write, the better (obviously), and the closer you get to achieving peak bandwidth. To obtain a concrete answer, let’s assume we are writing out DMB. The time to write out this chunk of data ( Twrite ) is the positioning time Tposition plus the time to transfer D(D Rpeak), or: Twrite=Tposition+D Rpeak(43.1) And thus the effective rate of writing ( Reffective ), which is just the amount of data written divided by the total time to write it, is: Reffective =D Twrite=D Tposition+D Rpeak. (43.2) What we’re interested in is getting the effective rate ( Reffective ) close to the peak rate. Speciﬁcally, we want the effective rate to be some fraction Fof the peak rate, where 0< F <1(a typical Fmight be 0.9, or 90 percent of the peak rate). In mathematical form, this means we want Reffective = F×Rpeak. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 5 At this point, we can solve for D: Reffective =D Tposition+D Rpeak=F×Rpeak (43.3) D=F×Rpeak×(Tposition+D Rpeak) (43.4) D= (F×Rpeak×Tposition)+(F×Rpeak×D Rpeak) (43.5) D=F 1−F×Rpeak×Tposition (43.6) Let’s do an example, with a disk with a positioning time of 10 mil- liseconds and peak transfer rate of 100 MB/s; assume we want an e f- fective bandwidth of 90 percent of peak ( F= 0.9). In this case, D=0.9 0.1× 100MB/s×0.01seconds = 9MB . Try some different values to see how much we need to buffer in order to approach peak bandwidth. How much is needed to reach 95 percent of peak? 99 percent? 43.4 Problem: Finding Inodes To understand how we ﬁnd an inode in LFS, let us brieﬂy review how to ﬁnd an inode in a typical U NIXﬁle system. In a typical ﬁle system such as FFS, or even the old U NIX ﬁle system, ﬁnding inodes is easy, because they are organized in an array and placed on disk at ﬁxed locations . For example, the old U NIXﬁle system keeps all inodes at a ﬁxed por- tion of the disk.",3304
43. Log-structured File System LFS,"Thus, given an inode number and the start addres s, to ﬁnd a particular inode, you can calculate its exact disk addres s simply by multiplying the inode number by the size of an inode, and adding t hat to the start address of the on-disk array; array-based indexin g, given an inode number, is fast and straightforward. Finding an inode given an inode number in FFS is only slightly more complicated, because FFS splits up the inode table into chunks and places a group of inodes within each cylinder group. Thus, one must know how big each chunk of inodes is and the start addresses of each. After that, the calculations are similar and also easy. In LFS, life is more difﬁcult. Why? Well, we’ve managed to scatte r the inodes all throughout the disk. Worse, we never overwrite in place , and thus the latest version of an inode (i.e., the one we want) keeps mov ing. 43.5 Solution Through Indirection: The Inode Map To remedy this, the designers of LFS introduced a level of indirection between inode numbers and the inodes through a data structure ca lled theinode map (imap) . The imap is a structure that takes an inode number as input and produces the disk address of the most recent version of the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 LOG-STRUCTURED FILESYSTEMS TIP: USEA L EVEL OFINDIRECTION People often say that the solution to all problems in Computer Scienc e is simply a level of indirection . This is clearly not true; it is just the solution tomost problems (yes, this is still too strong of a comment, but you get the point). You certainly can think of every virtualization we have s tudied, e.g., virtual memory, or the notion of a ﬁle, as simply a level of indi rection. And certainly the inode map in LFS is a virtualization of inode num bers. Hopefully you can see the great power of indirection in these examp les, allowing us to freely move structures around (such as pages in th e VM example, or inodes in LFS) without having to change every referen ce to them. Of course, indirection can have a downside too: extra overhead. So next time you have a problem, try solving it with indirection, but make sure to think about the overheads of doing so ﬁrst. As Wheeler famou sly said, “All problems in computer science can be solved by another l evel of indirection, except of course for the problem of too many indirection s.” inode. Thus, you can imagine it would often be implemented as a sim ple array , with 4 bytes (a disk pointer) per entry. Any time an inode is wri tten to disk, the imap is updated with its new location. The imap, unfortunately, needs to be kept persistent (i.e., w ritten to disk); doing so allows LFS to keep track of the locations of inodes acr oss crashes, and thus operate as desired. Thus, a question: where s hould the imap reside on disk? It could live on a ﬁxed part of the disk, of course. Unfortunately, as it gets updated frequently, this would then require updates to ﬁ le structures to be followed by writes to the imap, and hence performance would s uffer (i.e., there would be more disk seeks, between each update and t he ﬁxed location of the imap).",3133
43. Log-structured File System LFS,"Instead, LFS places chunks of the inode map right next to where i t is writing all of the other new information. Thus, when appending a d ata block to a ﬁle k, LFS actually writes the new data block, its inode, and a piece of the inode map all together onto the disk, as follows: D A0I[k]blk[0]:A0 A1imapmap[k]:A1 In this picture, the piece of the imap array stored in the block ma rked imap tells LFS that the inode kis at disk address A1; this inode, in turn, tells LFS that its data block Dis at address A0. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 7 43.6 Completing The Solution: The Checkpoint Region The clever reader (that’s you, right?) might have noticed a probl em here. How do we ﬁnd the inode map, now that pieces of it are also now spread across the disk? In the end, there is no magic: the ﬁle sy stem must have some ﬁxed and known location on disk to begin a ﬁle lookup. LFS has just such a ﬁxed place on disk for this, known as the check- point region (CR) . The checkpoint region contains pointers to (i.e., ad- dresses of) the latest pieces of the inode map, and thus the inode m ap pieces can be found by reading the CR ﬁrst. Note the checkpoint re gion is only updated periodically (say every 30 seconds or so), and thus perfor- mance is not ill-affected. Thus, the overall structure of the on- disk layout contains a checkpoint region (which points to the latest pieces of the in- ode map); the inode map pieces each contain addresses of the inodes ; the inodes point to ﬁles (and directories) just like typical U NIXﬁle systems. Here is an example of the checkpoint region (note it is all the way a t the beginning of the disk, at address 0), and a single imap chun k, inode, and data block. A real ﬁle system would of course have a much bigger CR (indeed, it would have two, as we’ll come to understand later), many imap chunks, and of course many more inodes, data blocks, etc. imap [k...k+N]: A2 CR 0D A0I[k]blk[0]:A0 A1imapmap[k]:A1 A2 43.7 Reading A File From Disk: A Recap To make sure you understand how LFS works, let us now walk through what must happen to read a ﬁle from disk. Assume we have nothing i n memory to begin. The ﬁrst on-disk data structure we must read is the checkpoint region. The checkpoint region contains pointers (i.e. , disk ad- dresses) to the entire inode map, and thus LFS then reads in the entire in- ode map and caches it in memory. After this point, when given an in ode number of a ﬁle, LFS simply looks up the inode-number to inode-disk - address mapping in the imap, and reads in the most recent versi on of the inode. To read a block from the ﬁle, at this point, LFS proceeds exac tly as a typical U NIXﬁle system, by using direct pointers or indirect pointers or doubly-indirect pointers as need be. In the common case, LFS shou ld perform the same number of I/Os as a typical ﬁle system when read ing a ﬁle from disk; the entire imap is cached and thus the extra work L FS does during a read is to look up the inode’s address in the imap.",3037
43. Log-structured File System LFS,"c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 LOG-STRUCTURED FILESYSTEMS 43.8 What About Directories? Thus far, we’ve simpliﬁed our discussion a bit by only considering in- odes and data blocks. However, to access a ﬁle in a ﬁle system (suc h as /home/remzi/foo , one of our favorite fake ﬁle names), some directo- ries must be accessed too. So how does LFS store directory data? Fortunately, directory structure is basically identical to c lassic U NIX ﬁle systems, in that a directory is just a collection of (name, inod e number) mappings. For example, when creating a ﬁle on disk, LFS must both write a new inode, some data, as well as the directory data and its inode t hat refer to this ﬁle. Remember that LFS will do so sequentially on the disk (after buffering the updates for some time). Thus, creating a ﬁ lefoo in a directory would lead to the following new structures on disk: D[k] A0I[k]blk[0]:A0 A1(foo, k) D[dir] A2I[dir]blk[0]:A2 A3map[k]:A1 map[dir]:A3 imap The piece of the inode map contains the information for the location of both the directory ﬁle diras well as the newly-created ﬁle f. Thus, when accessing ﬁle foo (with inode number k), you would ﬁrst look in the inode map (usually cached in memory) to ﬁnd the location of the inode of directory dir(A3); you then read the directory inode, which gives you the location of the directory data ( A2); reading this data block gives you the name-to-inode-number mapping of ( foo,k). You then consult the inode map again to ﬁnd the location of inode number k(A1), and ﬁnally read the desired data block at address A0. There is one other serious problem in LFS that the inode map solves, known as the recursive update problem [Z+12]. The problem arises in any ﬁle system that never updates in place (such as LFS), but rather moves updates to new locations on the disk. Speciﬁcally, whenever an inode is updated, its location on disk ch anges. If we hadn’t been careful, this would have also entailed an upda te to the directory that points to this ﬁle, which then would have mand ated a change to the parent of that directory, and so on, all the way up t he ﬁle system tree. LFS cleverly avoids this problem with the inode map. Even though the location of an inode may change, the change is never reﬂected i n the directory itself; rather, the imap structure is updated whil e the directory holds the same name-to-inode-number mapping. Thus, through ind irec- tion, LFS avoids the recursive update problem. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 9 43.9 A New Problem: Garbage Collection You may have noticed another problem with LFS; it repeatedly write s the latest version of a ﬁle (including its inode and data) to new l ocations on disk. This process, while keeping writes efﬁcient, implies that LFS leaves old versions of ﬁle structures scattered throughout the di sk. We (rather unceremoniously) call these old versions garbage . For example, let’s imagine the case where we have an existing ﬁl e re- ferred to by inode number k, which points to a single data block D0. We now update that block, generating both a new inode and a new data block. The resulting on-disk layout of LFS would look something like t his (note we omit the imap and other structures for simplicity; a new c hunk of imap would also have to be written to disk to point to the new inod e): D0 A0I[k]blk[0]:A0 (both garbage)D0 A4I[k]blk[0]:A4 In the diagram, you can see that both the inode and data block have two versions on disk, one old (the one on the left) and one current and thus live (the one on the right). By the simple act of (logically) updating a data block, a number of new structures must be persisted by LFS, thus leaving old versions of said blocks on the disk.",3773
43. Log-structured File System LFS,"As another example, imagine we instead append a block to that ori g- inal ﬁlek. In this case, a new version of the inode is generated, but the old data block is still pointed to by the inode. Thus, it is still li ve and very much part of the current ﬁle system: D0 A0I[k]blk[0]:A0 (garbage)D1 A4I[k]blk[0]:A0 blk[1]:A4 So what should we do with these older versions of inodes, data blocks, and so forth? One could keep those older versions around and allow users to restore old ﬁle versions (for example, when they acciden tally overwrite or delete a ﬁle, it could be quite handy to do so); such a ﬁ le system is known as a versioning ﬁle system because it keeps track of the different versions of a ﬁle. However, LFS instead keeps only the latest live version of a ﬁle; t hus (in the background), LFS must periodically ﬁnd these old dead ve rsions of ﬁle data, inodes, and other structures, and clean them; cleaning should c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 LOG-STRUCTURED FILESYSTEMS thus make blocks on disk free again for use in subsequent writes. Note that the process of cleaning is a form of garbage collection , a technique that arises in programming languages that automatically free unused mem- ory for programs. Earlier we discussed segments as important as they are the mec hanism that enables large writes to disk in LFS. As it turns out, they ar e also quite integral to effective cleaning. Imagine what would happen if the LFS cleaner simply went through and freed single data blocks, inode s, etc., during cleaning. The result: a ﬁle system with some number of fr eeholes mixed between allocated space on disk. Write performance would d rop considerably, as LFS would not be able to ﬁnd a large contiguous reg ion to write to disk sequentially and with high performance. Instead, the LFS cleaner works on a segment-by-segment basis, thus clearing up large chunks of space for subsequent writing. The b asic clean- ing process works as follows. Periodically, the LFS cleaner reads in a number of old (partially-used) segments, determines which bl ocks are live within these segments, and then write out a new set of segme nts with just the live blocks within them, freeing up the old ones for w riting. Speciﬁcally, we expect the cleaner to read in Mexisting segments, com- pact their contents into Nnew segments (where N < M ), and then write theNsegments to disk in new locations. The old Msegments are then freed and can be used by the ﬁle system for subsequent writes. We are now left with two problems, however. The ﬁrst is mechanism : how can LFS tell which blocks within a segment are live, and whic h are dead? The second is policy: how often should the cleaner run, and wh ich segments should it pick to clean? 43.10 Determining Block Liveness We address the mechanism ﬁrst. Given a data block Dwithin an on- disk segment S, LFS must be able to determine whether Dis live. To do so, LFS adds a little extra information to each segment that desc ribes each block. Speciﬁcally, LFS includes, for each data block D, its inode number (which ﬁle it belongs to) and its offset (which block of the ﬁle this is). This information is recorded in a structure at the head of the segment k nown as the segment summary block .",3262
43. Log-structured File System LFS,"Given this information, it is straightforward to determine whe ther a block is live or dead. For a block Dlocated on disk at address A, look in the segment summary block and ﬁnd its inode number Nand offset T. Next, look in the imap to ﬁnd where Nlives and read Nfrom disk (perhaps it is already in memory, which is even better). Final ly, using the offset T, look in the inode (or some indirect block) to see where the inode thinks the Tth block of this ﬁle is on disk. If it points exactl y to disk addressA, LFS can conclude that the block Dis live. If it points anywhere else, LFS can conclude that Dis not in use (i.e., it is dead) and thus know that this version is no longer needed. Here is a pseudocode summar y: OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 11 (N, T) = SegmentSummary[A]; inode = Read(imap[N]); if (inode[T] == A) // block D is alive else // block D is garbage Here is a diagram depicting the mechanism, in which the segme nt summary block (marked SS) records that the data block at address A0 is actually a part of ﬁle kat offset 0. By checking the imap for k, you can ﬁnd the inode, and see that it does indeed point to that location. D A0I[k]blk[0]:A0 A1imapmap[k]:A1 ssA0: (k,0) There are some shortcuts LFS takes to make the process of determin ing liveness more efﬁcient. For example, when a ﬁle is truncated or d eleted, LFS increases its version number and records the new version number in the imap. By also recording the version number in the on-disk seg ment, LFS can short circuit the longer check described above simply by compar- ing the on-disk version number with a version number in the imap, thus avoiding extra reads. 43.11 A Policy Question: Which Blocks To Clean, And When? On top of the mechanism described above, LFS must include a set of policies to determine both when to clean and which blocks are wort h cleaning. Determining when to clean is easier; either period ically, dur- ing idle time, or when you have to because the disk is full. Determining which blocks to clean is more challenging, and has been the subject of many research papers. In the original LFS paper [ RO91], the authors describe an approach which tries to segregate hotand cold seg- ments. A hot segment is one in which the contents are being freque ntly over-written; thus, for such a segment, the best policy is to wai t a long time before cleaning it, as more and more blocks are getting over-w ritten (in new segments) and thus being freed for use. A cold segment, i n con- trast, may have a few dead blocks but the rest of its contents are r elatively stable. Thus, the authors conclude that one should clean cold segm ents sooner and hot segments later, and develop a heuristic that does ex actly that. However, as with most policies, this policy isn’t perfect; l ater ap- proaches show how to do better [MR+97]. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 LOG-STRUCTURED FILESYSTEMS 43.12 Crash Recovery And The Log One ﬁnal problem: what happens if the system crashes while LFS is writing to disk? As you may recall in the previous chapter about j our- naling, crashes during updates are tricky for ﬁle systems, an d thus some- thing LFS must consider as well.",3237
43. Log-structured File System LFS,"During normal operation, LFS buffers writes in a segment, and th en (when the segment is full, or when some amount of time has elapsed) , writes the segment to disk. LFS organizes these writes in a log, i.e., the checkpoint region points to a head and tail segment, and each seg ment points to the next segment to be written. LFS also periodically updates the checkpoint region. Crashes could clearly happen during either of these operations (write to a segment, write to the CR). So how does LFS han dle crashes during writes to these structures? Let’s cover the second case ﬁrst. To ensure that the CR update hap pens atomically, LFS actually keeps two CRs, one at either end of the d isk, and writes to them alternately. LFS also implements a careful pr otocol when updating the CR with the latest pointers to the inode map and othe r infor- mation; speciﬁcally, it ﬁrst writes out a header (with timesta mp), then the body of the CR, and then ﬁnally one last block (also with a timestam p). If the system crashes during a CR update, LFS can detect this by s eeing an inconsistent pair of timestamps. LFS will always choose to use th e most recent CR that has consistent timestamps, and thus consistent update of the CR is achieved. Let’s now address the ﬁrst case. Because LFS writes the CR every 30 seconds or so, the last consistent snapshot of the ﬁle system may be q uite old. Thus, upon reboot, LFS can easily recover by simply reading in the checkpoint region, the imap pieces it points to, and subsequent ﬁ les and directories; however, the last many seconds of updates would be los t. To improve upon this, LFS tries to rebuild many of those segments through a technique known as roll forward in the database community. The basic idea is to start with the last checkpoint region, ﬁnd t he end of the log (which is included in the CR), and then use that to read t hrough the next segments and see if there are any valid updates withi n it. If there are, LFS updates the ﬁle system accordingly and thus recovers m uch of the data and metadata written since the last checkpoint. See Ros enblum’s award-winning dissertation for details [R92]. 43.13 Summary LFS introduces a new approach to updating the disk. Instead of ove r- writing ﬁles in places, LFS always writes to an unused portion of the disk, and then later reclaims that old space through cleaning. This ap- proach, which in database systems is called shadow paging [L77] and in ﬁle-system-speak is sometimes called copy-on-write , enables highly efﬁ- cient writing, as LFS can gather all updates into an in-memory segment and then write them out together sequentially. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 13 TIP: TURN FLAWS INTO VIRTUES Whenever your system has a fundamental ﬂaw, see if you can turn i t around into a feature or something useful. NetApp’s WAFL does this with old ﬁle contents; by making old versions available, WAFL no l onger has to worry about cleaning quite so often (though it does delete old ver- sions, eventually, in the background), and thus provides a cool fe ature and removes much of the LFS cleaning problem all in one wonderful twist.",3174
43. Log-structured File System LFS,"Are there other examples of this in systems? Undoubtedly , but you’ll have to think of them yourself, because this chapter is over with a capital “O”. Over. Done. Kaput. We’re out. Peace. The large writes that LFS generates are excellent for performa nce on many different devices. On hard drives, large writes ensure that posi- tioning time is minimized; on parity-based RAIDs, such as RAID -4 and RAID-5, they avoid the small-write problem entirely. Recent r esearch has even shown that large I/Os are required for high performance on Flash-based SSDs [H+17]; thus, perhaps surprisingly, LFS-sty le ﬁle sys- tems may be an excellent choice even for these new mediums. The downside to this approach is that it generates garbage; old c opies of the data are scattered throughout the disk, and if one wants to r e- claim such space for subsequent usage, one must clean old segmen ts pe- riodically. Cleaning became the focus of much controversy in LFS, a nd concerns over cleaning costs [SS+95] perhaps limited LFS’s initial impact on the ﬁeld. However, some modern commercial ﬁle systems, includi ng NetApp’s WAFL [HLM94], Sun’s ZFS [B07], and Linux btrfs [R+13], and even modern ﬂash-based SSDs [AD14], adopt a similar copy-on-write approach to writing to disk, and thus the intellectual legacy of LFS lives on in these modern ﬁle systems. In particular, WAFL got around cle an- ing problems by turning them into a feature; by providing old ver sions of the ﬁle system via snapshots , users could access old ﬁles whenever they deleted current ones accidentally. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 LOG-STRUCTURED FILESYSTEMS References [AD14] “Operating Systems: Three Easy Pieces” (Chapter: Flash-based So lid State Drives) by Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. Arpaci-Dusseau Books, 2014. A bit gauche to refer you to another chapter in this very book, but who are we to judge? [B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Copy Available: http://www.ostep.org/Citations/zfs_last.pdf .Slides on ZFS; unfortunately, there is no great ZFS paper (yet). Maybe you will write one, so we can cite it here? [H+17] “The Unwritten Contract of of Solid State Drives” by Jun He, Su darsun Kannan, An- drea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, A pril 2017. Which unwritten rules one must follow to extract high performance from an SSD? Interestingl y, both request scale (large or parallel requests) and locality still matter, even on SSDs. The more things change ... [HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau, Michael Malcolm. USENIX Spring ’94. WAFL takes many ideas from LFS and RAID and puts it into a high-speed NFS appliance for the multi-billion dollar storage company N etApp. [L77] “Physical Integrity in a Large Segmented Database” by R. Lori e. ACM Transactions on Databases, Volume 2:1, 1977. The original idea of shadow paging is presented here. [MJLF84] “A Fast File System for UNIX” by Marshall K.",3035
43. Log-structured File System LFS,"McKusick, William N. Joy, Sam J. Lefﬂer, Robert S. Fabry. ACM TOCS, Volume 2:3, August 1984. The original FFS paper; see the chapter on FFS for more details. [MR+97] “Improving the Performance of Log-structured File Systems wit h Adaptive Meth- ods” by Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randol ph Y. Wang, Thomas E. Anderson. SOSP 1997, pages 238-251, October, Saint Malo, France. A more recent paper detail- ing better policies for cleaning in LFS. [M94] “A Better Update Policy” by Jeffrey C. Mogul. USENIX ATC ’94, June 1 994. In this paper, Mogul ﬁnds that read workloads can be harmed by buffering writes for too long and then sending them to the disk in a big burst. Thus, he recommends sending writes more fr equently and in smaller batches. [P98] “Hardware Technology Trends and Database Opportunities” by Dav id A. Patterson. ACM SIGMOD ’98 Keynote, 1998. Available online here: http://www.cs.berkeley.edu/ ˜pattrsn/talks/keynote.html .A great set of slides on technology trends in computer sys- tems. Hopefully, Patterson will create another of these sometime soon. [R+13] “BTRFS: The Linux B-Tree Filesystem” by Ohad Rodeh, Josef Bacik, C hris Mason. ACM Transactions on Storage, Volume 9 Issue 3, August 2013. Finally, a good paper on BTRFS, a modern take on copy-on-write ﬁle systems. [RO91] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosen- blum and John Ousterhout. SOSP ’91, Paciﬁc Grove, CA, October 1991. The original SOSP paper about LFS, which has been cited by hundreds of other papers and insp ired many real systems. [R92] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosenblum. http://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696 .pdf. The award-winning dis- sertation about LFS, with many of the details missing from the paper. [SS+95] “File system logging versus clustering: a performance compa rison” by Margo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, Venkata Padmanabhan. USENIX 1995 Technical Conference, New Orleans, Louisiana, 1995. A paper that showed the LFS performance sometimes has problems, particularly for workloads with many call s tofsync() (such as database workloads). The paper was controversial at the time. [SO90] “Write-Only Disk Caches” by Jon A. Solworth, Cyril U. Orji. SIGMOD ’ 90, Atlantic City, New Jersey, May 1990. An early study of write buffering and its beneﬁts. However, buffering for too long can be harmful: see Mogul [M94] for details. [Z+12] “De-indirection for Flash-based SSDs with Nameless Writes” by Yiying Zhang, Leo Prasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusse au. FAST ’13, San Jose, California, February 2013. Our paper on a new way to build ﬂash-based storage devices, to avoid redundant mappings in the ﬁle system and FTL. The idea is for the devic e to pick the physical location of a write, and return the address to the ﬁle system, which stores the mapping . OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG LOG-STRUCTURED FILESYSTEMS 15 Homework (Simulation) This section introduces lfs.py , a simple LFS simulator you can use to understand better how an LFS-based ﬁle system works.",3198
43. Log-structured File System LFS,"Read the README for details on how to run the simulator. Questions 1. Run./lfs.py -n 3 , perhaps varying the seed ( -s). Can you ﬁgure out which commands were run to generate the ﬁnal ﬁle system contents ? Can you tell which order those commands were issued? Finally, can yo u deter- mine the liveness of each block in the ﬁnal ﬁle system state? Use -oto show which commands were run, and -cto show the liveness of the ﬁnal ﬁle sys- tem state. How much harder does the task become for you as you incr ease the number of commands issued (i.e., change -n 3 to-n 5 )? 2. If you ﬁnd the above painful, you can help yourself a little bi t by show- ing the set of updates caused by each speciﬁc command. To do so, run ./lfs.py -n 3 -i . Now see if it is easier to understand what each com- mand must have been. Change the random seed to get different comman ds to interpret (e.g., -s 1 ,-s 2 ,-s 3 , etc.). 3. To further test your ability to ﬁgure out what updates are made to d isk by each command, run the following: ./lfs.py -o -F -s 100 (and per- haps a few other random seeds). This just shows a set of commands a nd does NOT show you the ﬁnal state of the ﬁle system. Can you reaso n about what the ﬁnal state of the ﬁle system must be? 4. Now see if you can determine which ﬁles and directories are l ive after a number of ﬁle and directory operations. Run tt ./lfs.py -n 20 -s 1and then examine the ﬁnal ﬁle system state. Can you ﬁgure out which pathnames are valid? Run tt ./lfs.py -n 20 -s 1 -c -v to see the results. Run with -oto see if your answers match up given the series of random commands. Use different random seeds to get more problems. 5. Now let’s issue some speciﬁc commands. First, let’s create a ﬁl e and write to it repeatedly. To do so, use the -Lﬂag, which lets you specify speciﬁc commands to execute. Let’s create the ﬁle ”/foo” and write to it fo ur times: -L c,/foo:w,/foo,0,1:w,/foo,1,1:w,/foo,2,1:w,/foo,3, 1 -o . See if you can determine the liveness of the ﬁnal ﬁle system sta te; use-cto check your answers. 6. Now, let’s do the same thing, but with a single write operatio n instead of four. Run ./lfs.py -o -L c,/foo:w,/foo,0,4 to create ﬁle ”/foo” and write 4 blocks with a single write operation. Compute the li veness again, and check if you are right with -c. What is the main difference be- tween writing a ﬁle all at once (as we do here) versus doing it on e block at a time (as above)? What does this tell you about the importance of b uffering updates in main memory as the real LFS does? 7. Let’s do another speciﬁc example. First, run the following: ./lfs.py -L c,/foo:w,/foo,0,1 . What does this set of commands do? Now, run ./lfs.py -L c,/foo:w,/foo,7,1 . What does this set of commands do? How are the two different? What can you tell about the size ﬁeld in the inode from these two sets of commands? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 LOG-STRUCTURED FILESYSTEMS 8. Now let’s look explicitly at ﬁle creation versus directory c reation. Run simu- lations./lfs.py -L c,/foo and./lfs.py -L d,/foo to create a ﬁle and then a directory. What is similar about these runs, and what i s different? 9. The LFS simulator supports hard links as well. Run the followin g to study how they work: ./lfs.py -L c,/foo:l,/foo,/bar:l,/foo,/goo -o -i . What blocks are written out when a hard link is created? How is t his similar to just creating a new ﬁle, and how is it different? How does the reference count ﬁeld change as links are created? 10. LFS makes many different policy decisions. We do not explore many of them here – perhaps something left for the future – but here is a simp le one we do explore: the choice of inode number. First, run ./lfs.py -p c100 -n 10 -o -a s to show the usual behavior with the ”sequential” alloca- tion policy, which tries to use free inode numbers nearest to zer o. Then, change to a ”random” policy by running ./lfs.py -p c100 -n 10 -o -a r (the-p c100 ﬂag ensures 100 percent of the random operations are ﬁle creations). What on-disk differences does a random poli cy versus a se- quential policy result in? What does this say about the importanc e of choos- ing inode numbers in a real LFS? 11. One last thing we’ve been assuming is that the LFS simulator al ways up- dates the checkpoint region after each update. In the real LFS , that isn’t the case: it is updated periodically to avoid long seeks. Run ./lfs.py -N -i -o -s 1000 to see some operations and the intermediate and ﬁnal states of the ﬁle system when the checkpoint region isn’t forced to dis k. What would happen if the checkpoint region is never updated? What if it is updated pe- riodically? Could you ﬁgure out how to recover the ﬁle system to t he latest state by rolling forward in the log? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",4806
44. Flash-based SSDs,"44 Flash-based SSDs After decades of hard-disk drive dominance, a new form of persist ent storage device has recently gained signiﬁcance in the world. G eneri- cally referred to as solid-state storage , such devices have no mechani- cal or moving parts like hard drives; rather, they are simply bu ilt out of transistors, much like memory and processors. However, unlike ty pical random-access memory (e.g., DRAM), such a solid-state storage device (a.k.a., an SSD ) retains information despite power loss, and thus is an ideal candidate for use in persistent storage of data. The technology we’ll focus on is known as ﬂash (more speciﬁcally, NAND-based ﬂash ), which was created by Fujio Masuoka in the 1980s [M+14]. Flash, as we’ll see, has some unique properties. For exam ple, to write to a given chunk of it (i.e., a ﬂash page ), you ﬁrst have to erase a big- ger chunk (i.e., a ﬂash block ), which can be quite expensive. In addition, writing too often to a page will cause it to wear out . These two properties make construction of a ﬂash-based SSD an interesting challenge: CRUX: HOWTOBUILD A F LASH -BASED SSD How can we build a ﬂash-based SSD? How can we handle the expen- sive nature of erasing? How can we build a device that lasts a long time, given that repeated overwrite will wear the device out? Will th e march of progress in technology ever cease? Or cease to amaze? 44.1 Storing a Single Bit Flash chips are designed to store one or more bits in a single trans is- tor; the level of charge trapped within the transistor is mapped to a binary value. In a single-level cell (SLC ) ﬂash, only a single bit is stored within a transistor (i.e., 1 or 0); with a multi-level cell (MLC ) ﬂash, two bits are encoded into different levels of charge, e.g., 00, 01, 10, and 1 1 are repre- sented by low, somewhat low, somewhat high, and high levels. Ther e is even triple-level cell (TLC ) ﬂash, which encodes 3 bits per cell. Overall, SLC chips achieve higher performance and are more expensive. 1 2 FLASH -BASED SSD S TIP: BECAREFUL WITHTERMINOLOGY You may have noticed that some terms we have used many times before (blocks, pages) are being used within the context of a ﬂash, but i n slightly different ways than before. New terms are not created to make you r life harder (although they may be doing just that), but arise becaus e there is no central authority where terminology decisions are made. What is a block to you may be a page to someone else, and vice versa, dependin g on the context. Your job is simple: to know the appropriate terms wit hin each domain, and use them such that people well-versed in the di scipline can understand what you are talking about. It’s one of those times wh ere the only solution is simple but sometimes painful: use your memory. Of course, there are many details as to exactly how such bit-lev el stor- age operates, down at the level of device physics. While beyond th e scope of this book, you can read more about it on your own [J10]. 44.2 From Bits to Banks/Planes As they say in ancient Greece, storing a single bit (or a few) does not a storage system make.",3117
44. Flash-based SSDs,"Hence, ﬂash chips are organized into banks or planes which consist of a large number of cells. A bank is accessed in two different sized units: blocks (sometimes called erase blocks ), which are typically of size 128 KB or 256 KB, and pages , which are a few KB in size (e.g., 4KB). Within each bank there are a large number of blocks; within each block, there are a large num ber of pages. When thinking about ﬂash, you must remember this new ter mi- nology, which is different than the blocks we refer to in disks an d RAIDs and the pages we refer to in virtual memory. Figure 44.1 shows an example of a ﬂash plane with blocks and pages ; there are three blocks, each containing four pages, in this simp le exam- ple. We’ll see below why we distinguish between blocks and pages ; it turns out this distinction is critical for ﬂash operations such as reading and writing, and even more so for the overall performance of the dev ice. The most important (and weird) thing you will learn is that to wri te to a page within a block, you ﬁrst have to erase the entire block; thi s tricky detail makes building a ﬂash-based SSD an interesting and worth while challenge, and the subject of the second-half of the chapter. 0 1 2 Block: Page: Content:000102030405060708091011 Figure 44.1: A Simple Flash Chip: Pages Within Blocks OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 3 44.3 Basic Flash Operations Given this ﬂash organization, there are three low-level operati ons that a ﬂash chip supports. The read command is used to read a page from the ﬂash; erase and program are used in tandem to write. The details: •Read (a page) : A client of the ﬂash chip can read any page (e.g., 2KB or 4KB), simply by specifying the read command and appro- priate page number to the device. This operation is typically q uite fast, 10s of microseconds or so, regardless of location on the device, and (more or less) regardless of the location of the previous request (quite unlike a disk). Being able to access any location uniform ly quickly means the device is a random access device. •Erase (a block): Before writing to a page within a ﬂash, the nature of the device requires that you ﬁrst erase the entire block the page lies within. Erase, importantly, destroys the contents of the bl ock (by setting each bit to the value 1); therefore, you must be sure that any data you care about in the block has been copied elsewhere (to memory, or perhaps to another ﬂash block) before executing the erase. The erase command is quite expensive, taking a few mill isec- onds to complete. Once ﬁnished, the entire block is reset and eac h page is ready to be programmed. •Program (a page): Once a block has been erased, the program com- mand can be used to change some of the 1’s within a page to 0’s, and write the desired contents of a page to the ﬂash. Program- ming a page is less expensive than erasing a block, but more costl y than reading a page, usually taking around 100s of microseconds on modern ﬂash chips. One way to think about ﬂash chips is that each page has a state as so- ciated with it. Pages start in an INVALID state.",3134
44. Flash-based SSDs,"By erasing the block that a page resides within, you set the state of the page (and all page s within that block) to ERASED , which resets the content of each page in the block but also (importantly) makes them programmable. When you progra m a page, its state changes to VALID , meaning its contents have been set and can be read. Reads do not affect these states (although you should only read from pages that have been programmed). Once a page has been pro- grammed, the only way to change its contents is to erase the enti re block within which the page resides. Here is an example of states tra nsition after various erase and program operations within a 4-page block: iiii Initial: pages in block are invalid (i) Erase() →EEEE State of pages in block set to erased (E) Program(0) →VEEE Program page 0; state set to valid (V) Program(0) → error Cannot re-program page after programming Program(1) →VVEE Program page 1 Erase() →EEEE Contents erased; all pages programmable c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 FLASH -BASED SSD S A Detailed Example Because the process of writing (i.e., erasing and programming) is so un- usual, let’s go through a detailed example to make sure it makes sense. In this example, imagine we have the following four 8-bit pages, within a 4-page block (both unrealistically small sizes, but useful w ithin this ex- ample); each page is VALID as each has been previously programmed. Page 0 Page 1 Page 2 Page 3 00011000 11001110 00000001 00111111 VALID VALID VALID VALID Now say we wish to write to page 0, ﬁlling it with new contents. To write any page, we must ﬁrst erase the entire block. Let’s assum e we do so, thus leaving the block in this state: Page 0 Page 1 Page 2 Page 3 11111111 11111111 11111111 11111111 ERASED ERASED ERASED ERASED Good news. We could now go ahead and program page 0, for exam- ple with the contents 00000011 , overwriting the old page 0 (contents 00011000 ) as desired. After doing so, our block looks like this: Page 0 Page 1 Page 2 Page 3 00000011 11111111 11111111 11111111 VALID ERASED ERASED ERASED And now the bad news: the previous contents of pages 1, 2, and 3 are all gone. Thus, before overwriting any page within a block, we must ﬁrst move any data we care about to another location (e.g., memory, or elsewhere on the ﬂash). The nature of erase will have a strong imp act on how we design ﬂash-based SSDs, as we’ll soon learn about. Summary To summarize, reading a page is easy: just read the page. Flas h chips do this quite well, and quickly; in terms of performance, they of fer the potential to greatly exceed the random read performance of modern disk drives, which are slow due to mechanical seek and rotation costs. Writing a page is trickier; the entire block must ﬁrst be erase d (taking care to ﬁrst move any data we care about to another location), and th en the desired page programmed. Not only is this expensive, but fre quent repetitions of this program/erase cycle can lead to the biggest reliability problem ﬂash chips have: wear out . When designing a storage system with ﬂash, the performance and reliability of writing is a cent ral focus.",3155
44. Flash-based SSDs,"We’ll soon learn more about how modern SSDs attack these issues, deliv- ering excellent performance and reliability despite these l imitations. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 5 Read Program Erase Device ( µs) ( µs) ( µs) SLC 25 200-300 1500-2000 MLC 50 600-900 ˜3000 TLC ˜75 ˜900-1350 ˜4500 Figure 44.2: Raw Flash Performance Characteristics 44.4 Flash Performance And Reliability Because we’re interested in building a storage device out of raw ﬂ ash chips, it is worthwhile to understand their basic performance character- istics. Figure 44.2 presents a rough summary of some numbers foun d in the popular press [V12]. Therein, the author presents the basi c operation latency of reads, programs, and erases across SLC, MLC, and TLC ﬂa sh, which store 1, 2, and 3 bits of information per cell, respectively . As we can see from the table, read latencies are quite good, takin g just 10s of microseconds to complete. Program latency is higher and more variable, as low as 200 microseconds for SLC, but higher as you pack more bits into each cell; to get good write performance, you will ha ve to make use of multiple ﬂash chips in parallel. Finally, erase s are quite expensive, taking a few milliseconds typically. Dealing wit h this cost is central to modern ﬂash storage design. Let’s now consider reliability of ﬂash chips. Unlike mechanical disks, which can fail for a wide variety of reasons (including the grues ome and quite physical head crash , where the drive head actually makes contact with the recording surface), ﬂash chips are pure silicon and in that sense have fewer reliability issues to worry about. The primary conce rn is wear out; when a ﬂash block is erased and programmed, it slowly accrues a little bit of extra charge. Over time, as that extra charge bui lds up, it becomes increasingly difﬁcult to differentiate between a 0 a nd a 1. At the point where it becomes impossible, the block becomes unusable. The typical lifetime of a block is currently not well known. Manuf ac- turers rate MLC-based blocks as having a 10,000 P/E (Program/E rase) cycle lifetime; that is, each block can be erased and programme d 10,000 times before failing. SLC-based chips, because they store only a single bit per transistor, are rated with a longer lifetime, usually 100, 000 P/E cycles. However, recent research has shown that lifetimes are much long er than expected [BD10]. One other reliability problem within ﬂash chips is known as distur- bance . When accessing a particular page within a ﬂash, it is possibl e that some bits get ﬂipped in neighboring pages; such bit ﬂips are know n as read disturbs orprogram disturbs , depending on whether the page is being read or programmed, respectively. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 FLASH -BASED SSD S TIP: THEIMPORTANCE OFBACKWARDS COMPATIBILITY Backwards compatibility is always a concern in layered system s. By deﬁning a stable interface between two systems, one enables i nnovation on each side of the interface while ensuring continued interoper ability. Such an approach has been quite successful in many domains: opera ting systems have relatively stable APIs for applications, disks p rovide the same block-based interface to ﬁle systems, and each layer in t he IP net- working stack provides a ﬁxed unchanging interface to the laye r above.",3376
44. Flash-based SSDs,"Not surprisingly, there can be a downside to such rigidity, as i nterfaces deﬁned in one generation may not be appropriate in the next. In some cases, it may be useful to think about redesigning the entire s ystem en- tirely. An excellent example is found in the Sun ZFS ﬁle system [ B07]; by reconsidering the interaction of ﬁle systems and RAID, the cr eators of ZFS envisioned (and then realized) a more effective integrate d whole. 44.5 From Raw Flash to Flash-Based SSDs Given our basic understanding of ﬂash chips, we now face our next task: how to turn a basic set of ﬂash chips into something that looks like a typical storage device. The standard storage interface is a s imple block- based one, where blocks (sectors) of size 512 bytes (or larger) can be read or written, given a block address. The task of the ﬂash-based SSD is to provide that standard block interface atop the raw ﬂash chips in side it. Internally, an SSD consists of some number of ﬂash chips (for persis- tent storage). An SSD also contains some amount of volatile (i.e., non- persistent) memory (e.g., SRAM); such memory is useful for cachi ng and buffering of data as well as for mapping tables, which we’ll lear n about below. Finally, an SSD contains control logic to orchestrate device op era- tion. See Agrawal et. al for details [A+08]; a simpliﬁed block dia gram is seen in Figure 44.3 (page 7). One of the essential functions of this control logic is to satisfy cl ient reads and writes, turning them into internal ﬂash operations a s need be. The ﬂash translation layer , or FTL, provides exactly this functionality. The FTL takes read and write requests on logical blocks (that comprise the device interface) and turns them into low-level read, erase, and program commands on the underlying physical blocks and physical pages (that com- prise the actual ﬂash device). The FTL should accomplish this t ask with the goal of delivering excellent performance and high reliabil ity. Excellent performance, as we’ll see, can be realized through a c om- bination of techniques. One key will be to utilize multiple ﬂas h chips inparallel ; although we won’t discuss this technique much further, suf- ﬁce it to say that all modern SSDs use multiple chips internally t o obtain higher performance. Another performance goal will be to reduce write ampliﬁcation , which is deﬁned as the total write trafﬁc (in bytes) issued to the ﬂash chips by the FTL divided by the total write trafﬁc (i n bytes) is- OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 7Host Interface LogicFlash ControllerMemoryFlash Flash Flash Flash Flash Flash Figure 44.3: A Flash-based SSD: Logical Diagram sued by the client to the SSD. As we’ll see below, naive approaches t o FTL construction will lead to high write ampliﬁcation and low perform ance. High reliability will be achieved through the combination of a fe w dif- ferent approaches. One main concern, as discussed above, is wear out . If a single block is erased and programmed too often, it will become un us- able; as a result, the FTL should try to spread writes across the blocks of the ﬂash as evenly as possible, ensuring that all of the blocks of t he device wear out at roughly the same time; doing so is called wear leveling and is an essential part of any modern FTL.",3311
44. Flash-based SSDs,"Another reliability concern is program disturbance. To minimi ze such disturbance, FTLs will commonly program pages within an erased block in order , from low page to high page. This sequential-programming ap- proach minimizes disturbance and is widely utilized. 44.6 FTL Organization: A Bad Approach The simplest organization of an FTL would be something we call di- rect mapped . In this approach, a read to logical page Nis mapped di- rectly to a read of physical page N. A write to logical page Nis more complicated; the FTL ﬁrst has to read in the entire block that pa geNis contained within; it then has to erase the block; ﬁnally, the FT L programs the old pages as well as the new one. As you can probably guess, the direct-mapped FTL has many prob- lems, both in terms of performance as well as reliability. The pe rformance problems come on each write: the device has to read in the entire b lock (costly), erase it (quite costly), and then program it (costly). The end re- sult is severe write ampliﬁcation (proportional to the number of p ages in a block) and as a result, terrible write performance, even sl ower than typical hard drives with their mechanical seeks and rotationa l delays. Even worse is the reliability of this approach. If ﬁle system met adata or user ﬁle data is repeatedly overwritten, the same block is era sed and programmed, over and over, rapidly wearing it out and potentially los- ing data. The direct mapped approach simply gives too much contr ol over wear out to the client workload; if the workload does not spread write load evenly across its logical blocks, the underlying phys ical blocks containing popular data will quickly wear out. For both reliabili ty and performance reasons, a direct-mapped FTL is a bad idea. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 FLASH -BASED SSD S 44.7 A Log-Structured FTL For these reasons, most FTLs today are log structured , an idea useful in both storage devices (as we’ll see now) and ﬁle systems above the m (as we’ll see in the chapter on log-structured ﬁle systems ). Upon a write to logical block N, the device appends the write to the next free spot in the currently-being-written-to block; we call this style of writ inglogging . To allow for subsequent reads of block N, the device keeps a mapping table (in its memory, and persistent, in some form, on the device); this table stores the physical address of each logical block in the system. Let’s go through an example to make sure we understand how the basic log-based approach works. To the client, the device looks li ke a typical disk, in which it can read and write 512-byte sectors ( or groups of sectors). For simplicity, assume that the client is reading or w riting 4-KB sized chunks. Let us further assume that the SSD contains some lar ge number of 16-KB sized blocks, each divided into four 4-KB pages; these parameters are unrealistic (ﬂash blocks usually consist of more pages) but will serve our didactic purposes quite well. Assume the client issues the following sequence of operations: •Write(100) with contents a1 •Write(101) with contents a2 •Write(2000) with contents b1 •Write(2001) with contents b2 These logical block addresses (e.g., 100) are used by the client of the SSD (e.g., a ﬁle system) to remember where information is located. Internally, the device must transform these block writes into the erase and program operations supported by the raw hardware, and somehow record, for each logical block address, which physical page of the SSD stores its data. Assume that all blocks of the SSD are currently not v alid, and must be erased before any page can be programmed. Here we show the initial state of our SSD, with all pages marked INVALID (i): 0 1 2 Block: Page: Content: State:00 i01 i02 i03 i04 i05 i06 i07 i08 i09 i10 i11 i When the ﬁrst write is received by the SSD (to logical block 100), t he FTL decides to write it to physical block 0, which contains four p hysical pages: 0, 1, 2, and 3.",3990
44. Flash-based SSDs,"Because the block is not erased, we cannot wr ite to it yet; the device must ﬁrst issue an erase command to block 0. Doi ng so leads to the following state: 0 1 2 Block: Page: Content: State:00 E01 E02 E03 E04 i05 i06 i07 i08 i09 i10 i11 i OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 9 Block 0 is now ready to be programmed. Most SSDs will write pages in order (i.e., low to high), reducing reliability problems rel ated to pro- gram disturbance . The SSD then directs the write of logical block 100 into physical page 0: 0 1 2 Block: Page: Content: State:00 a1 V01 E02 E03 E04 i05 i06 i07 i08 i09 i10 i11 i But what if the client wants to read logical block 100? How can it ﬁnd where it is? The SSD must transform a read issued to logical block 10 0 into a read of physical page 0. To accommodate such functionality , when the FTL writes logical block 100 to physical page 0, it records th is fact in anin-memory mapping table . We will track the state of this mapping table in the diagrams as well: Memory Flash ChipTable: 100 0 0 1 2 Block: Page: Content: State:00 a1 V01 E02 E03 E04 i05 i06 i07 i08 i09 i10 i11 i Now you can see what happens when the client writes to the SSD. The SSD ﬁnds a location for the write, usually just picking the next free page; it then programs that page with the block’s contents, and re cords the logical-to-physical mapping in its mapping table. Subsequ ent reads simply use the table to translate the logical block address presented by the client into the physical page number required to read the data. Let’s now examine the rest of the writes in our example write strea m: 101, 2000, and 2001. After writing these blocks, the state of th e device is: Memory Flash ChipTable: 100 0 101 1 2000 2 2001 3 0 1 2 Block: Page: Content: State:00 a1 V01 a2 V02 b1 V03 b2 V04 i05 i06 i07 i08 i09 i10 i11 i The log-based approach by its nature improves performance (eras es only being required once in a while, and the costly read-modify-w rite of the direct-mapped approach avoided altogether), and greatly e nhances reliability. The FTL can now spread writes across all pages, pe rforming what is called wear leveling and increasing the lifetime of the device; we’ll discuss wear leveling further below. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 FLASH -BASED SSD S ASIDE : FTL M APPING INFORMATION PERSISTENCE You might be wondering: what happens if the device loses power? Doe s the in-memory mapping table disappear? Clearly, such informa tion can- not truly be lost, because otherwise the device would not function a s a persistent storage device. An SSD must have some means of recoverin g mapping information. The simplest thing to do is to record some mapping information wit h each page, in what is called an out-of-band (OOB ) area. When the device loses power and is restarted, it must reconstruct its mapping ta ble by scanning the OOB areas and reconstructing the mapping table i n mem- ory. This basic approach has its problems; scanning a large SSD to ﬁ nd all necessary mapping information is slow.",3083
44. Flash-based SSDs,"To overcome this limit ation, some higher-end devices use more complex logging and checkpointing techniques to speed up recovery; learn more about logging by read ing chapters on crash consistency and log-structured ﬁle systems [ AD14]. Unfortunately, this basic approach to log structuring has some d own- sides. The ﬁrst is that overwrites of logical blocks lead to someth ing we call garbage , i.e., old versions of data around the drive and taking up space. The device has to periodically perform garbage collection (GC) to ﬁnd said blocks and free space for future writes; excessive gar bage collec- tion drives up write ampliﬁcation and lowers performance. The se cond is high cost of in-memory mapping tables; the larger the device, the more memory such tables need. We now discuss each in turn. 44.8 Garbage Collection The ﬁrst cost of any log-structured approach such as this one is tha t garbage is created, and therefore garbage collection (i.e., dead-block recla- mation) must be performed. Let’s use our continued example to make sense of this. Recall that logical blocks 100, 101, 2000, and 200 1 have been written to the device. Now, let’s assume that blocks 100 and 101 are written to again, wi th contentsc1andc2. The writes are written to the next free pages (in this case, physical pages 4 and 5), and the mapping table is update d accord- ingly. Note that the device must have ﬁrst erased block 1 to make such programming possible: Memory Flash ChipTable: 100 4 101 5 2000 2 2001 3 0 1 2 Block: Page: Content: State:00 a1 V01 a2 V02 b1 V03 b2 V04 c1 V05 c2 V06 E07 E08 i09 i10 i11 i OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 11 The problem we have now should be obvious: physical pages 0 and 1, although marked VALID , have garbage in them, i.e., the old versions of blocks 100 and 101. Because of the log-structured nature of the d e- vice, overwrites create garbage blocks, which the device must reclaim to provide free space for new writes to take place. The process of ﬁnding garbage blocks (also called dead blocks ) and reclaiming them for future use is called garbage collection , and it is an important component of any modern SSD. The basic process is simple: ﬁnd a block that contains one or more garbage pages, read in the live (non-garbage) pages from that block, write out those live pages to the log, and (ﬁnally) reclaim the entire block for use in writing. Let’s now illustrate with an example. The device decides it wan ts to reclaim any dead pages within block 0 above. Block 0 has two dead b locks (pages 0 and 1) and two lives blocks (pages 2 and 3, which contain blocks 2000 and 2001, respectively). To do so, the device will: •Read live data (pages 2 and 3) from block 0 •Write live data to end of the log •Erase block 0 (freeing it for later usage) For the garbage collector to function, there must be enough informa - tion within each block to enable the SSD to determine whether each page is live or dead. One natural way to achieve this end is to store, a t some location within each block, information about which logical blocks a re stored within each page. The device can then use the mapping ta ble to determine whether each page within the block holds live data or n ot.",3246
44. Flash-based SSDs,"From our example above (before the garbage collection has taken pla ce), block 0 held logical blocks 100, 101, 2000, 2001. By checking the mapping table (which, before garbage collection, contained 100->4, 101->5, 2000->2, 2001->3 ), the device can readily determine whether each of the pages within the SSD block holds live information. For example, 2 000 and 2001 clearly are still pointed to by the map; 100 and 101 are not and therefore are candidates for garbage collection. When this garbage collection process is complete in our example, t he state of the device is: Memory Flash ChipTable: 100 4 101 5 2000 6 2001 7 0 1 2 Block: Page: Content: State:00 E01 E02 E03 E04 c1 V05 c2 V06 b1 V07 b2 V08 i09 i10 i11 i As you can see, garbage collection can be expensive, requiring r eading and rewriting of live data. The ideal candidate for reclamation is a block that consists of only dead pages; in this case, the block can immed iately be erased and used for new data, without expensive data migrati on. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 FLASH -BASED SSD S ASIDE : A N EWSTORAGE API K NOWN ASTRIM When we think of hard drives, we usually just think of the most ba- sic interface to read and write them: read and write (there is also usu- ally some kind of cache ﬂush command, ensuring that writes have actu- ally been persisted, but sometimes we omit that for simplicity) . With log-structured SSDs, and indeed, any device that keeps a ﬂexibl e and changing mapping of logical-to-physical blocks, a new interfac e is use- ful, known as the trim operation. The trim operation takes an address (and possibly a length) and s imply informs the device that the block(s) speciﬁed by the address (a nd length) have been deleted; the device thus no longer has to track any in forma- tion about the given address range. For a standard hard drive, tr im isn’t particularly useful, because the drive has a static mapping of block ad- dresses to speciﬁc platter, track, and sector(s). For a log-str uctured SSD, however, it is highly useful to know that a block is no longer neede d, as the SSD can then remove this information from the FTL and later recla im the physical space during garbage collection. Although we sometimes think of interface and implementation as s epa- rate entities, in this case, we see that the implementation sh apes the inter- face. With complex mappings, knowledge of which blocks are no long er needed makes for a more effective implementation. To reduce GC costs, some SSDs overprovision the device [A+08]; by adding extra ﬂash capacity, cleaning can be delayed and push ed to the background , perhaps done at a time when the device is less busy. Adding more capacity also increases internal bandwidth, which can b e used for cleaning and thus not harm perceived bandwidth to the client. Many modern drives overprovision in this manner, one key to achieving e xcel- lent overall performance. 44.9 Mapping Table Size The second cost of log-structuring is the potential for extremely l arge mapping tables, with one entry for each 4-KB page of the device. W ith a large 1-TB SSD, for example, a single 4-byte entry per 4-KB page r esults in 1 GB of memory needed the device, just for these mappings.",3247
44. Flash-based SSDs,"Thus , this page-level FTL scheme is impractical. Block-Based Mapping One approach to reduce the costs of mapping is to only keep a pointer per block of the device, instead of per page, reducing the amount of mapping information by a factor ofSizeblock Sizepage. This block-level FTL is akin to having OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 13 bigger page sizes in a virtual memory system; in that case, you u se fewer bits for the VPN and have a larger offset in each virtual address . Unfortunately, using a block-based mapping inside a log-based FTL does not work very well for performance reasons. The biggest problem arises when a “small write” occurs (i.e., one that is less than t he size of a physical block). In this case, the FTL must read a large amount of live data from the old block and copy it into a new one (along with the data from the small write). This data copying increases write ampli ﬁcation greatly and thus decreases performance. To make this issue more clear, let’s look at an example. Assume the client previously wrote out logical blocks 2000, 2001, 2002, and 2 003 (with contents,a, b, c, d ), and that they are located within physical block 1 at physical pages 4, 5, 6, and 7. With per-page mappings, the transla- tion table would have to record four mappings for these logical block s: 2000→4, 2001→5, 2002→6, 2003→7. If, instead, we use block-level mapping, the FTL only needs to r ecord a single address translation for all of this data. The address ma pping, however, is slightly different than our previous examples. Spec iﬁcally, we think of the logical address space of the device as being choppe d into chunks that are the size of the physical blocks within the ﬂash. Thus, the logical block address consists of two portions: a chunk number a nd an offset. Because we are assuming four logical blocks ﬁt within e ach physical block, the offset portion of the logical addresses requir es 2 bits; the remaining (most signiﬁcant) bits form the chunk number. Logical blocks 2000, 2001, 2002, and 2003 all have the same chun k number (500), and have different offsets (0, 1, 2, and 3, respe ctively). Thus, with a block-level mapping, the FTL records that chunk 50 0 maps to block 1 (starting at physical page 4), as shown in this diagra m: Memory Flash ChipTable: 500 4 0 1 2 Block: Page: Content: State:00 i01 i02 i03 i04 a V05 b V06 c V07 d V08 i09 i10 i11 i In a block-based FTL, reading is easy. First, the FTL extracts the chunk number from the logical block address presented by the client, b y taking the topmost bits out of the address. Then, the FTL looks up the chunk- number to physical-page mapping in the table. Finally, the F TL computes the address of the desired ﬂash page by adding the offset from the logical address to the physical address of the block. For example, if the client issues a read to logical address 2002 , the de- vice extracts the logical chunk number (500), looks up the trans lation in the mapping table (ﬁnding 4), and adds the offset from the logica l ad- dress (2) to the translation (4).",3091
44. Flash-based SSDs,"The resulting physical-pag e address (6) is c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 FLASH -BASED SSD S where the data is located; the FTL can then issue the read to tha t physical address and obtain the desired data ( c). But what if the client writes to logical block 2002 (with content sc’)? In this case, the FTL must read in 2000, 2001, and 2003, and the n write out all four logical blocks in a new location, updating the mapping t able accordingly. Block 1 (where the data used to reside) can then be erased and reused, as shown here. Memory Flash ChipTable: 500 8 0 1 2 Block: Page: Content: State:00 i01 i02 i03 i04 E05 E06 E07 E08 a V09 b V10 c’ V11 d V As you can see from this example, while block level mappings grea tly reduce the amount of memory needed for translations, they cause si gnif- icant performance problems when writes are smaller than the ph ysical block size of the device; as real physical blocks can be 256KB or la rger, such writes are likely to happen quite often. Thus, a better sol ution is needed. Can you sense that this is the part of the chapter where w e tell you what that solution is? Better yet, can you ﬁgure it out yourself, before reading on? Hybrid Mapping To enable ﬂexible writing but also reduce mapping costs, many modern FTLs employ a hybrid mapping technique. With this approach, the FTL keeps a few blocks erased and directs all writes to them; these are called log blocks . Because the FTL wants to be able to write any page to any location within the log block without all the copying required by a p ure block-based mapping, it keeps per-page mappings for these log blocks. The FTL thus logically has two types of mapping table in its memor y: a small set of per-page mappings in what we’ll call the log table , and a larger set of per-block mappings in the data table . When looking for a particular logical block, the FTL will ﬁrst consult the log table; if the logic al block’s location is not found there, the FTL will then consult the data tabl e to ﬁnd its location and then access the requested data. The key to the hybrid mapping strategy is keeping the number of log blocks small. To keep the number of log blocks small, the FTL has to pe- riodically examine log blocks (which have a pointer per page) and switch them into blocks that can be pointed to by only a single block pointe r. This switch is accomplished by one of three main techniques, bas ed on the contents of the block [KK+02]. For example, let’s say the FTL had previously written out logical p ages 1000, 1001, 1002, and 1003, and placed them in physical block 2 (physical OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 15 pages 8, 9, 10, 11); assume the contents of the writes to 1000, 10 01, 1002, and 1003 are a,b,c, andd, respectively. Memory Flash ChipLog Table: Data Table: 250 8 0 1 2 Block: Page: Content: State:00 i01 i02 i03 i04 i05 i06 i07 i08 a V09 b V10 c V11 d V Now assume that the client overwrites each of these blocks (with d ata a’,b’,c’, andd’), in the exact same order, in one of the currently avail- able log blocks, say physical block 0 (physical pages 0, 1, 2, and 3). In this case, the FTL will have the following state: Memory Flash ChipLog Table: 1000 0 1001 1 1002 2 1003 3 Data Table: 250 8 0 1 2 Block: Page: Content: State:00 a’ V01 b’ V02 c’ V03 d’ V04 i05 i06 i07 i08 a V09 b V10 c V11 d V Because these blocks have been written exactly in the same man ner as before, the FTL can perform what is known as a switch merge .",3525
44. Flash-based SSDs,"In this case, the log block (0) now becomes the storage location for blocks 0, 1, 2, and 3, and is pointed to by a single block pointer; the old block (2) i s now erased and used as a log block. In this best case, all the per-pag e pointers required replaced by a single block pointer. Memory Flash ChipLog Table: Data Table: 250 0 0 1 2 Block: Page: Content: State:00 a’ V01 b’ V02 c’ V03 d’ V04 i05 i06 i07 i08 i09 i10 i11 i This switch merge is the best case for a hybrid FTL. Unfortunate ly, sometimes the FTL is not so lucky. Imagine the case where we have the same initial conditions (logical blocks 1000 ... 1003 stored i n physi- cal block 2) but then the client overwrites logical blocks 1000 an d 1001. What do you think happens in this case? Why is it more challengin g to handle? (think before looking at the result on the next page) c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 FLASH -BASED SSD S Memory Flash ChipLog Table: 1000 0 1001 1 Data Table: 250 8 0 1 2 Block: Page: Content: State:00 a’ V01 b’ V02 i03 i04 i05 i06 i07 i08 a V09 b V10 c V11 d V To reunite the other pages of this physical block, and thus be abl e to re- fer to them by only a single block pointer, the FTL performs what is called apartial merge . In this operation, logical blocks 1002 and 1003 are read from physical block 2, and then appended to the log. The resultin g state of the SSD is the same as the switch merge above; however, in this cas e, the FTL had to perform extra I/O to achieve its goals, thus incre asing write ampliﬁcation. The ﬁnal case encountered by the FTL known as a full merge , and re- quires even more work. In this case, the FTL must pull together pa ges from many other blocks to perform cleaning. For example, imagine t hat logical blocks 0, 4, 8, and 12 are written to log block A. To switch this log block into a block-mapped page, the FTL must ﬁrst create a data b lock containing logical blocks 0, 1, 2, and 3, and thus the FTL must rea d 1, 2, and 3 from elsewhere and then write out 0, 1, 2, and 3 together. Nex t, the merge must do the same for logical block 4, ﬁnding 5, 6, and 7 and re con- ciling them into a single physical block. The same must be done f or logi- cal blocks 8 and 12, and then (ﬁnally), the log block Acan be freed. Fre- quent full merges, as is not surprising, can seriously harm per formance and thus should be avoided when at all possible [GY+09]. Page Mapping Plus Caching Given the complexity of the hybrid approach above, others have sug - gested simpler ways to reduce the memory load of page-mapped FTL s. Probably the simplest is just to cache only the active parts of th e FTL in memory, thus reducing the amount of memory needed [GY+09]. This approach can work well. For example, if a given workload only accesses a small set of pages, the translations of those pages wil l be stored in the in-memory FTL, and performance will be excellent without high memory cost. Of course, the approach can also perform poorly. If mem- ory cannot contain the working set of necessary translations, each access will minimally require an extra ﬂash read to ﬁrst bring in the missing mapping before being able to access the data itself.",3194
44. Flash-based SSDs,"Even worse , to make room for the new mapping, the FTL might have to evict an old map- ping, and if that mapping is dirty (i.e., not yet written to the ﬂash per- sistently), an extra write will also be incurred. However, in many cases, the workload will display locality, and this caching approach wi ll both reduce memory overheads and keep performance high. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 17 44.10 Wear Leveling Finally, a related background activity that modern FTLs must i mple- ment is wear leveling , as introduced above. The basic idea is simple: because multiple erase/program cycles will wear out a ﬂash bloc k, the FTL should try its best to spread that work across all the blocks of t he de- vice evenly. In this manner, all blocks will wear out at roughly t he same time, instead of a few “popular” blocks quickly becoming unusabl e. The basic log-structuring approach does a good initial job of spread ing out write load, and garbage collection helps as well. However, some times a block will be ﬁlled with long-lived data that does not get over-wr itten; in this case, garbage collection will never reclaim the block, a nd thus it does not receive its fair share of the write load. To remedy this problem, the FTL must periodically read all the l ive data out of such blocks and re-write it elsewhere, thus making th e block available for writing again. This process of wear leveling incr eases the write ampliﬁcation of the SSD, and thus decreases performance as e xtra I/O is required to ensure that all blocks wear at roughly the sam e rate. Many different algorithms exist in the literature [A+08, M+1 4]; read more if you are interested. 44.11 SSD Performance And Cost Before closing, let’s examine the performance and cost of modern SSDs, to better understand how they will likely be used in persisten t storage systems. In both cases, we’ll compare to classic hard-disk driv es (HDDs), and highlight the biggest differences between the two. Performance Unlike hard disk drives, ﬂash-based SSDs have no mechanical com po- nents, and in fact are in many ways more similar to DRAM, in that they are “random access” devices. The biggest difference in perfor mance, as compared to disk drives, is realized when performing random rea ds and writes; while a typical disk drive can only perform a few hundre d ran- dom I/Os per second, SSDs can do much better. Here, we use some data from modern SSDs to see just how much better SSDs perform; we’re par- ticularly interested in how well the FTLs hide the performance issues of the raw chips. Table 44.4 shows some performance data for three different SSDs and one top-of-the-line hard drive; the data was taken from a few diff erent online sources [S13, T15]. The left two columns show random I/O per- formance, and the right two columns sequential; the ﬁrst three rows show data for three different SSDs (from Samsung, Seagate, and Intel), a nd the last row shows performance for a hard disk drive (orHDD ), in this case a Seagate high-end drive.",3046
44. Flash-based SSDs,"We can learn a few interesting facts from the table. First, and most dramatic, is the difference in random I/O performance between the SSDs c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 FLASH -BASED SSD S Random Sequential Reads Writes Reads Writes Device (MB/s) (MB/s) (MB/s) (MB/s) Samsung 840 Pro SSD 103 287 421 384 Seagate 600 SSD 84 252 424 374 Intel SSD 335 SSD 39 222 344 354 Seagate Savvio 15K.3 HDD 2 2 223 223 Figure 44.4: SSDs And Hard Drives: Performance Comparison and the lone hard drive. While the SSDs obtain tens or even hundreds of MB/s in random I/Os, this “high performance” hard drive has a pe ak of just a couple MB/s (in fact, we rounded up to get to 2 MB/s). Second, you can see that in terms of sequential performance, there is much l ess of a dif- ference; while the SSDs perform better, a hard drive is still a good choice if sequential performance is all you need. Third, you can see tha t SSD ran- dom read performance is not as good as SSD random write performance. The reason for such unexpectedly good random-write performance is due to the log-structured design of many SSDs, which transforms ra n- dom writes into sequential ones and improves performance. Final ly, be- cause SSDs exhibit some performance difference between sequent ial and random I/Os, many of the techniques we will learn in subsequent chap- ters about how to build ﬁle systems for hard drives are still appl icable to SSDs; although the magnitude of difference between sequential a nd ran- dom I/Os is smaller, there is enough of a gap to carefully consider how to design ﬁle systems to reduce random I/Os. Cost As we saw above, the performance of SSDs greatly outstrips modern har d drives, even when performing sequential I/O. So why haven’t SSDs c om- pletely replaced hard drives as the storage medium of choice? Th e an- swer is simple: cost, or more speciﬁcally, cost per unit of capacit y. Cur- rently [A15], an SSD costs something like $150 for a 250-GB drive; s uch an SSD costs 60 cents per GB. A typical hard drive costs roughly $50 f or 1-TB of storage, which means it costs 5 cents per GB. There is stil l more than a 10 ×difference in cost between these two storage media. These performance and cost differences dictate how large-scal e stor- age systems are built. If performance is the main concern, SSDs ar e a terriﬁc choice, particularly if random read performance is imp ortant. If, on the other hand, you are assembling a large data center and wish to store massive amounts of information, the large cost difference wi ll drive you towards hard drives. Of course, a hybrid approach can make sen se – some storage systems are being assembled with both SSDs and hard drives, using a smaller number of SSDs for more popular “hot” data and delivering high performance, while storing the rest of the “cold er” (less used) data on hard drives to save on cost. As long as the price gap ex ists, hard drives are here to stay. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 19 44.12 Summary Flash-based SSDs are becoming a common presence in laptops, desk- tops, and servers inside the datacenters that power the world’s e conomy.",3181
44. Flash-based SSDs,"Thus, you should probably know something about them, right? Here’s the bad news: this chapter (like many in this book) is just the ﬁrst step in understanding the state of the art. Some places to ge t some more information about the raw technology include research on actua l device performance (such as that by Chen et al. [CK+09] and Gru pp et al. [GC+09]), issues in FTL design (including works by Agrawa l et al. [A+08], Gupta et al. [GY+09], Huang et al. [H+14], Kim et al. [ KK+02], Lee et al. [L+07], and Zhang et al. [Z+12]), and even distribu ted systems comprised of ﬂash (including Gordon [CG+09] and CORFU [B+12]). A nd, if we may say so, a really good overview of all the things you need to do to extract high performance from an SSD can be found in a paper on the “unwritten contract” [HK+17]. Don’t just read academic papers; also read about recent advance s in the popular press (e.g., [V12]). Therein you’ll learn more pract ical (but still useful) information, such as Samsung’s use of both TLC and SLC c ells within the same SSD to maximize performance (SLC can buffer write s quickly) as well as capacity (TLC can store more bits per cell). And this is, as they say, just the tip of the iceberg. Dive in and learn mor e about this “iceberg” of research on your own, perhaps starting with Ma e t al.’s excellent (and recent) survey [M+14]. Be careful though; ice bergs can sink even the mightiest of ships [W15]. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 20 FLASH -BASED SSD S ASIDE : KEYSSD T ERMS •Aﬂash chip consists of many banks, each of which is organized into erase blocks (sometimes just called blocks ). Each block is further subdivided into some number of pages . •Blocks are large (128KB–2MB) and contain many pages, which are relatively small (1KB–8KB). •To read from ﬂash, issue a read command with an address and length; this allows a client to read one or more pages. •Writing ﬂash is more complex. First, the client must erase the en- tire block (which deletes all information within the block). The n, the client can program each page exactly once, thus completing the write. •A new trim operation is useful to tell the device when a particular block (or range of blocks) is no longer needed. •Flash reliability is mostly determined by wear out ; if a block is erased and programmed too often, it will become unusable. •A ﬂash-based solid-state storage device (SSD ) behaves as if it were a normal block-based read/write disk; by using a ﬂash translation layer (FTL), it transforms reads and writes from a client into reads, erases, and programs to underlying ﬂash chips. •Most FTLs are log-structured , which reduces the cost of writing by minimizing erase/program cycles. An in-memory translation layer tracks where logical writes were located within the phys ical medium. •One key problem with log-structured FTLs is the cost of garbage collection , which leads to write ampliﬁcation . •Another problem is the size of the mapping table, which can be- come quite large. Using a hybrid mapping or just caching hot pieces of the FTL are possible remedies. •One last problem is wear leveling ; the FTL must occasionally mi- grate data from blocks that are mostly read in order to ensure said blocks also receive their share of the erase/program load.",3299
44. Flash-based SSDs,"OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 21 References [A+08] “Design Tradeoffs for SSD Performance” by N. Agrawal, V . Prabha karan, T. Wobber, J. D. Davis, M. Manasse, R. Panigrahy. USENIX ’08, San Diego California, June 2008. An excellent overview of what goes into SSD design. [AD14] “Operating Systems: Three Easy Pieces” by Chapters: Crash Consistency: FSCK and Jour- naling and Log-Structured File Systems . Remzi Arpaci-Dusseau and Andrea Arpaci-Dusseau. A lot more detail here about how logging can be used in ﬁle systems; some of the s ame ideas can be applied inside devices too as need be. [A15] “Amazon Pricing Study” by Remzi Arpaci-Dusseau. February, 2 015. This is not an actual paper, but rather one of the authors going to Amazon and looking at current prices of h ard drives and SSDs. You too can repeat this study, and see what the costs are today. Do it. [B+12] “CORFU: A Shared Log Design for Flash Clusters” by M. Balakr ishnan, D. Malkhi, V . Prabhakaran, T. Wobber, M. Wei, J. D. Davis. NSDI ’12, San Jose, Cali fornia, April 2012. A new way to think about designing a high-performance replicated log for cluster s using Flash. [BD10] “Write Endurance in Flash Drives: Measurements and Analysis” by Simona Boboila, Peter Desnoyers. FAST ’10, San Jose, California, February 2010. A cool paper that reverse en- gineers ﬂash-device lifetimes. Endurance sometimes far exceeds man ufacturer predictions, by up to 100×. [B07] “ZFS: The Last Word in File Systems” by Jeff Bonwick and Bill Moor e. Available here: http://www.ostep.org/Citations/zfs_last.pdf .Was this the last word in ﬁle sys- tems? No, but maybe it’s close. [CG+09] “Gordon: Using Flash Memory to Build Fast, Power-efﬁcient Cl usters for Data-intensive Applications” by Adrian M. Caulﬁeld, Laura M. Grupp, Steven Swans on. ASPLOS ’09, Wash- ington, D.C., March 2009. Early research on assembling ﬂash into larger-scale clusters; deﬁnitel y worth a read. [CK+09] “Understanding Intrinsic Characteristics and System Implicat ions of Flash Memory based Solid State Drives” by Feng Chen, David A. Koufaty, and Xiaod ong Zhang. SIGMET- RICS/Performance ’09, Seattle, Washington, June 2009. An excellent overview of SSD performance problems circa 2009 (though now a little dated). [G14] “The SSD Endurance Experiment” by Geoff Gasior. The Tech Report, S eptember 19, 2014. Available: http://techreport.com/review/27062 .A nice set of simple experiments measuring performance of SSDs over time. There are many other similar stud ies; use google to ﬁnd more. [GC+09] “Characterizing Flash Memory: Anomalies, Observations, and Applications” by L. M. Grupp, A. M. Caulﬁeld, J. Coburn, S. Swanson, E. Yaakobi, P . H. Sie gel, J. K. Wolf. IEEE MICRO ’09, New York, New York, December 2009. Another excellent characterization of ﬂash performance. [GY+09] “DFTL: a Flash Translation Layer Employing Demand-Based S elective Caching of Page-Level Address Mappings” by Aayush Gupta, Youngjae Kim, Bhuva n Urgaonkar. ASP- LOS ’09, Washington, D.C., March 2009. This paper gives an excellent overview of different strategies for cleaning within hybrid SSDs as well as a new scheme which saves map ping table space and improves performance under many workloads.",3252
44. Flash-based SSDs,"[HK+17] “The Unwritten Contract of Solid State Drives” by Jun He, Sud arsun Kannan, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. EuroSys ’17, Belgr ade, Serbia, April 2017. Our own paper which lays out ﬁve rules clients should follow in order to get the best performance out of modern SSDs. The rules are request scale, locality, aligned sequenti ality, grouping by death time, and uniform lifetime. Read the paper for details. [H+14] “An Aggressive Worn-out Flash Block Management Scheme To Allev iate SSD Perfor- mance Degradation” by Ping Huang, Guanying Wu, Xubin He, Weijun Xiao. EuroSys ’14, 2014. Recent work showing how to really get the most out of worn-out ﬂash blocks; neat. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 22 FLASH -BASED SSD S [J10] “Failure Mechanisms and Models for Semiconductor Devices” by Unknown a uthor. Re- port JEP122F, November 2010. Available on the internet at this excit ing so-called web site: http://www.jedec.org/sites/default/files/docs/JEP12 2F.pdf .A highly detailed discussion of what is going on at the device level and how such devices fai l. Only for those not faint of heart. Or physicists. Or both. [KK+02] “A Space-Efﬁcient Flash Translation Layer For Compact Flas h Systems” by Jesung Kim, Jong Min Kim, Sam H. Noh, Sang Lyul Min, Yookun Cho. IEEE Transactions on Con- sumer Electronics, Volume 48, Number 2, May 2002. One of the earliest proposals to suggest hybrid mappings. [L+07] “A Log Buffer-Based Flash Translation Layer by Using Fully -Associative Sector Trans- lation. ” Sang-won Lee, Tae-Sun Chung, Dong-Ho Lee, Sangwon Park, Ha-Joo S ong. ACM Transactions on Embedded Computing Systems, Volume 6, Number 3, July 2007 A terriﬁc paper about how to build hybrid log/block mappings. [M+14] “A Survey of Address Translation Technologies for Flash Memor ies” by Dongzhe Ma, Jianhua Feng, Guoliang Li. ACM Computing Surveys, Volume 46, Numbe r 3, January 2014. Probably the best recent survey of ﬂash and related technologies. [S13] “The Seagate 600 and 600 Pro SSD Review” by Anand Lal Shimpi. AnandT ech, May 7, 2013. Available: http://www.anandtech.com/show/6935/seagate-600-ssd- review . One of many SSD performance measurements available on the internet. Haven’t heard of the internet? No problem. Just go to your web browser and type “internet” into the search tool . You’ll be amazed at what you can learn. [T15] “Performance Charts Hard Drives” by Tom’s Hardware. January 201 5. Available here: http://www.tomshardware.com/charts/enterprise-hdd-c harts .Yet another site with performance data, this time focusing on hard drives. [V12] “Understanding TLC Flash” by Kristian Vatto. AnandTech, Septemb er, 2012. Available: http://www.anandtech.com/show/5067/understanding-tl c-nand .A short descrip- tion about TLC ﬂash and its characteristics. [W15] “List of Ships Sunk by Icebergs” by Many authors. Available at t his location on the “web”:http://en.wikipedia.org/wiki/List ofshipssunkbyicebergs .Yes, there is a wikipedia page about ships sunk by icebergs. It is a really boring page and basically everyone knows the only ship the iceberg-sinking-maﬁa cares about is the Titanic.",3162
44. Flash-based SSDs,"[Z+12] “De-indirection for Flash-based SSDs with Nameless Writes” by Yiying Zhang, Leo Prasath Arulraj, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusse au. FAST ’13, San Jose, California, February 2013. Our research on a new idea to reduce mapping table space; the key is to re-use the pointers in the ﬁle system above to store locations of blocks, instead of add ing another level of indirection. OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG FLASH -BASED SSD S 23 Homework (Simulation) This section introduces ssd.py , a simple SSD simulator you can use to understand better how SSDs work. Read the README for details on how to run the simulator. It is a long README, so boil a cup of tea (caf - feinated likely necessary), put on your reading glasses, let t he cat curl up on your lap1, and get to work. Questions 1. The homework will mostly focus on the log-structured SSD, which is simulated with the “-T log” ﬂag. We’ll use the other types of SSDs for comparison. First, run with ﬂags -T log -s 1 -n 10 -q. Can you ﬁgure out which operations took place? Use -cto check your answers (or just use -Cinstead of -q -c ). Use different values of-sto generate different random workloads. 2. Now just show the commands and see if you can ﬁgure out the intermediate states of the Flash. Run with ﬂags -T log -s 2 -n 10 -C to show each command. Now, determine the state of the Flash between each command; use -Fto show the states and see if you were right. Use different random seeds to test your burgeonin g expertise. 3. Let’s make this problem ever so slightly more interesting by a dding the-r 20 ﬂag. What differences does this cause in the commands? Use-cagain to check your answers. 4. Performance is determined by the number of erases, programs, and reads (we assume here that trims are free). Run the same workloa d again as above, but without showing any intermediate states (e. g., -T log -s 1 -n 10 ). Can you estimate how long this workload will take to complete? (default erase time is 1000 microseconds , program time is 40, and read time is 10) Use the -Sﬂag to check your answer. You can also change the erase, program, and read times with the -E, -W, -R ﬂags. 5. Now, compare performance of the log-structured approach and the (very bad) direct approach ( -T direct instead of -T log ). First, estimate how you think the direct approach will perform, then che ck your answer with the -Sﬂag. In general, how much better will the log-structured approach perform than the direct one? 6. Let us next explore the behavior of the garbage collector. To do so, we have to set the high ( -G) and low ( -g) watermarks appro- priately. First, let’s observe what happens when you run a large r workload to the log-structured SSD but without any garbage col- lection. To do this, run with ﬂags -T log -n 1000 (the high wa- 1Now you might complain, “But I’m a dog person.” To this, we say, too b ad. Get a cat, put it on your lap, and do the homework. How else will you learn, if y ou can’t even follow the most basic of instructions? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 24 FLASH -BASED SSD S termark default is 10, so the GC won’t run in this conﬁguration). What do you think will happen? Use -Cand perhaps -Fto see. 7. To turn on the garbage collector, use lower values. The high wat er- mark (-G N ) tells the system to start collecting once Nblocks have been used; the low watermark ( -G M ) tells the system to stop col- lecting once there are only Mblocks in use. What watermark values do you think will make for a working system? Use -Cand-Fto show the commands and intermediate device states and see. 8. One other useful ﬂag is -J, which shows what the collector is doing when it runs. Run with ﬂags -T log -n 1000 -C -J to see both the commands and the GC behavior. What do you notice about the GC? The ﬁnal effect of GC, of course, is performance. Use -Sto look at ﬁnal statistics; how many extra reads and writes occur due to garbage collection? Compare this to the ideal SSD ( -T ideal ); how much extra reading, writing, and erasing is there due to th e nature of Flash? Compare it also to the direct approach; in what way (erases, reads, programs) is the log-structured approach s upe- rior? 9. One last aspect to explore is workload skew . Adding skew to the workload changes writes such that more writes occur to some smalle r fraction of the logical block space. For example, running with -K 80/20 makes 80 percent of the writes go to 20 percent of the blocks. Pick some different skews and perform many randomly-chosen operations (e. g., -n 1000 ), using ﬁrst -T direct to understand the skew, and then -T log to see the impact on a log-structured device. What do you expect will happen? One other small skew control to explore is -k 100; by adding this ﬂag to a skewed workload, the ﬁrst 100 writes are not skewed. The idea is to ﬁrst create a lot of data, but then onl y update some of it. What impact might that have upon a garbage collector? OPERATING SYSTEMS [VERSION 1.01]WWW .OSTEP .ORG",5035
45. Data Integrity and Protection,"45 Data Integrity and Protection Beyond the basic advances found in the ﬁle systems we have studi ed thus far, a number of features are worth studying. In this chapter, w e focus on reliability once again (having previously studied storage sys tem reliabil- ity in the RAID chapter). Speciﬁcally, how should a ﬁle system or s torage system ensure that data is safe, given the unreliable nature of modern storage devices? This general area is referred to as data integrity ordata protection . Thus, we will now investigate techniques used to ensure that t he data you put into your storage system is the same when the storage syste m returns it to you. CRUX: HOWTOENSURE DATA INTEGRITY How should systems ensure that the data written to storage is pro- tected? What techniques are required? How can such technique s be made efﬁcient, with both low space and time overheads? 45.1 Disk Failure Modes As you learned in the chapter about RAID, disks are not perfect, a nd can fail (on occasion). In early RAID systems, the model of failure was quite simple: either the entire disk is working, or it fails comp letely, and the detection of such a failure is straightforward. This fail-stop model of disk failure makes building RAID relatively simple [S90]. What you didn’t learn is about all of the other types of failure modes modern disks exhibit. Speciﬁcally, as Bairavasundaram et al. studied in great detail [B+07, B+08], modern disks will occasionally se em to be mostly working but have trouble successfully accessing one or more blocks. Speciﬁcally, two types of single-block failures are common and wor thy of consideration: latent-sector errors (LSEs ) and block corruption . We’ll now discuss each in more detail. 1 2 DATA INTEGRITY AND PROTECTION Cheap Costly LSEs 9.40 percent 1.40 percent Corruption 0.50 percent 0.05 percent Figure 45.1: Frequency Of LSEs And Block Corruption LSEs arise when a disk sector (or group of sectors) has been damaged in some way. For example, if the disk head touches the surface for s ome reason (a head crash , something which shouldn’t happen during nor- mal operation), it may damage the surface, making the bits unre adable. Cosmic rays can also ﬂip bits, leading to incorrect contents. For tunately, in-disk error correcting codes (ECC ) are used by the drive to determine whether the on-disk bits in a block are good, and in some cases, to ﬁx them; if they are not good, and the drive does not have enough informa- tion to ﬁx the error, the disk will return an error when a request i s issued to read them. There are also cases where a disk block becomes corrupt in a way not detectable by the disk itself. For example, buggy disk ﬁrmwar e may write a block to the wrong location; in such a case, the disk ECC indicate s the block contents are ﬁne, but from the client’s perspective the wron g block is returned when subsequently accessed. Similarly, a block ma y get cor- rupted when it is transferred from the host to the disk across a fa ulty bus; the resulting corrupt data is stored by the disk, but it is not wha t the client desires. These types of faults are particularly insidious bec ause they are silent faults ; the disk gives no indication of the problem when returning the faulty data.",3244
45. Data Integrity and Protection,"Prabhakaran et al. describes this more modern view of disk failu re as thefail-partial disk failure model [P+05]. In this view, disks can still fail in their entirety (as was the case in the traditional fail-stop model); how- ever, disks can also seemingly be working and have one or more block s become inaccessible (i.e., LSEs) or hold the wrong contents (i.e., corrup- tion). Thus, when accessing a seemingly-working disk, once in a while it may either return an error when trying to read or write a given block (a non-silent partial fault), and once in a while it may simply r eturn the wrong data (a silent partial fault). Both of these types of faults are somewhat rare, but just how rare? F ig- ure 45.1 summarizes some of the ﬁndings from the two Bairavasund aram studies [B+07,B+08]. The ﬁgure shows the percent of drives that exhibited at least one LSE or block corruption over the course of the study (about 3 years, over 1.5 million disk drives). The ﬁgure further sub-divides the r esults into “cheap” drives (usually SATA drives) and “costly” drives (usu ally SCSI or FibreChannel). As you can see, while buying better drives re duces the frequency of both types of problem (by about an order of magnitude ), they still happen often enough that you need to think carefully a bout how to handle them in your storage system. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 3 Some additional ﬁndings about LSEs are: •Costly drives with more than one LSE are as likely to develop ad- ditional errors as cheaper drives •For most drives, annual error rate increases in year two •The number of LSEs increase with disk size •Most disks with LSEs have less than 50 •Disks with LSEs are more likely to develop additional LSEs •There exists a signiﬁcant amount of spatial and temporal localit y •Disk scrubbing is useful (most LSEs were found this way) Some ﬁndings about corruption: •Chance of corruption varies greatly across different drive model s within the same drive class •Age effects are different across models •Workload and disk size have little impact on corruption •Most disks with corruption only have a few corruptions •Corruption is not independent within a disk or across disks in RAID •There exists spatial locality, and some temporal locality •There is a weak correlation with LSEs To learn more about these failures, you should likely read the orig inal papers [B+07,B+08]. But hopefully the main point should be clea r: if you really wish to build a reliable storage system, you must includ e machin- ery to detect and recover from both LSEs and block corruption. 45.2 Handling Latent Sector Errors Given these two new modes of partial disk failure, we should now tr y to see what we can do about them. Let’s ﬁrst tackle the easier of th e two, namely latent sector errors. CRUX: HOWTOHANDLE LATENT SECTOR ERRORS How should a storage system handle latent sector errors? How much extra machinery is needed to handle this form of partial failur e? As it turns out, latent sector errors are rather straightforward to han- dle, as they are (by deﬁnition) easily detected.",3122
45. Data Integrity and Protection,"When a storage system tries to access a block, and the disk returns an error, the storag e system should simply use whatever redundancy mechanism it has to ret urn the correct data. In a mirrored RAID, for example, the system should a ccess the alternate copy; in a RAID-4 or RAID-5 system based on parity, the system should reconstruct the block from the other blocks in the par ity group. Thus, easily detected problems such as LSEs are readily r ecovered through standard redundancy mechanisms. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 DATA INTEGRITY AND PROTECTION The growing prevalence of LSEs has inﬂuenced RAID designs over th e years. One particularly interesting problem arises in RAID- 4/5 systems when both full-disk faults and LSEs occur in tandem. Speciﬁcally , when an entire disk fails, the RAID tries to reconstruct the disk (say, onto a hot spare) by reading through all of the other disks in the parity g roup and recomputing the missing values. If, during reconstruction , an LSE is encountered on any one of the other disks, we have a problem: the reconstruction cannot successfully complete. To combat this issue, some systems add an extra degree of redunda ncy. For example, NetApp’s RAID-DP has the equivalent of two parity disks instead of one [C+04]. When an LSE is discovered during reconstruc tion, the extra parity helps to reconstruct the missing block. As alw ays, there is a cost, in that maintaining two parity blocks for each stripe is m ore costly; however, the log-structured nature of the NetApp WAFL ﬁle system mit- igates that cost in many cases [HLM94]. The remaining cost is sp ace, in the form of an extra disk for the second parity block. 45.3 Detecting Corruption: The Checksum Let’s now tackle the more challenging problem, that of silent fail ures via data corruption. How can we prevent users from getting bad dat a when corruption arises, and thus leads to disks returning bad d ata? CRUX: HOWTOPRESERVE DATA INTEGRITY DESPITE CORRUPTION Given the silent nature of such failures, what can a storage sys tem do to detect when corruption arises? What techniques are needed? How can one implement them efﬁciently? Unlike latent sector errors, detection of corruption is a key problem. How can a client tell that a block has gone bad? Once it is known that a particular block is bad, recovery is the same as before: you need to have some other copy of the block around (and hopefully, one that is not cor- rupt.). Thus, we focus here on detection techniques. The primary mechanism used by modern storage systems to preser ve data integrity is called the checksum . A checksum is simply the result of a function that takes a chunk of data (say a 4KB block) as input an d computes a function over said data, producing a small summary of th e contents of the data (say 4 or 8 bytes). This summary is referred t o as the checksum. The goal of such a computation is to enable a system to de tect if data has somehow been corrupted or altered by storing the checks um with the data and then conﬁrming upon later access that the data ’s cur- rent checksum matches the original storage value.",3146
45. Data Integrity and Protection,"OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 5 TIP: THERE ’SNOFREE LUNCH There’s No Such Thing As A Free Lunch, or TNSTAAFL for short, is an old American idiom that implies that when you are seemingly ge t- ting something for free, in actuality you are likely paying some c ost for it. It comes from the old days when diners would advertise a free lu nch for customers, hoping to draw them in; only when you went in, did you realize that to acquire the “free” lunch, you had to purchase on e or more alcoholic beverages. Of course, this may not actually be a problem , partic- ularly if you are an aspiring alcoholic (or typical undergraduat e student). Common Checksum Functions A number of different functions are used to compute checksums, a nd vary in strength (i.e., how good they are at protecting data integ rity) and speed (i.e., how quickly can they be computed). A trade-off that is com- mon in systems arises here: usually, the more protection you get, t he costlier it is. There is no such thing as a free lunch. One simple checksum function that some use is based on exclusive or (XOR). With XOR-based checksums, the checksum is computed b y XOR’ing each chunk of the data block being checksummed, thus prod uc- ing a single value that represents the XOR of the entire block. To make this more concrete, imagine we are computing a 4-byte che ck- sum over a block of 16 bytes (this block is of course too small to really be a disk sector or block, but it will serve for the example). The 16 dat a bytes, in hex, look like this: 365e c4cd ba14 8a92 ecef 2c3a 40be f666 If we view them in binary, we get the following: 0011 0110 0101 1110 1100 0100 1100 1101 1011 1010 0001 0100 1000 1010 1001 0010 1110 1100 1110 1111 0010 1100 0011 1010 0100 0000 1011 1110 1111 0110 0110 0110 Because we’ve lined up the data in groups of 4 bytes per row, it is ea sy to see what the resulting checksum will be: perform an XOR over e ach column to get the ﬁnal checksum value: 0010 0000 0001 1011 1001 0100 0000 0011 The result, in hex, is 0x201b9403. XOR is a reasonable checksum but has its limitations. If, for exa mple, two bits in the same position within each checksummed unit chan ge, the checksum will not detect the corruption. For this reason, people ha ve investigated other checksum functions. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 DATA INTEGRITY AND PROTECTION Another basic checksum function is addition. This approach has t he advantage of being fast; computing it just requires performing 2’s-complement addition over each chunk of the data, ignoring overﬂow. It can detec t many changes in data, but is not good if the data, for example, is sh ifted. A slightly more complex algorithm is known as the Fletcher check- sum , named (as you might guess) for the inventor, John G. Fletcher [F8 2]. It is quite simple to compute and involves the computation of two ch eck bytes,s1ands2. Speciﬁcally, assume a block Dconsists of bytes d1... dn;s1is deﬁned as follows: s1 = (s1 +di)mod255(computed over all di);s2in turn is: s2 = (s2 +s1)mod255(again over all di) [F04].",3129
45. Data Integrity and Protection,"The Fletcher checksum is almost as strong as the CRC (see below), det ecting all single-bit, double-bit errors, and many burst errors [F04] . One ﬁnal commonly-used checksum is known as a cyclic redundancy check (CRC ). Assume you wish to compute the checksum over a data blockD. All you do is treat Das if it is a large binary number (it is just a string of bits after all) and divide it by an agreed upon value ( k). The remainder of this division is the value of the CRC. As it turns out, one can implement this binary modulo operation rather efﬁciently, and hence the popularity of the CRC in networking as well. See elsewhere for m ore details [M13]. Whatever the method used, it should be obvious that there is no per - fect checksum: it is possible two data blocks with non-identica l contents will have identical checksums, something referred to as a collision . This fact should be intuitive: after all, computing a checksum is ta king some- thing large (e.g., 4KB) and producing a summary that is much sm aller (e.g., 4 or 8 bytes). In choosing a good checksum function, we are thu s trying to ﬁnd one that minimizes the chance of collisions while re main- ing easy to compute. Checksum Layout Now that you understand a bit about how to compute a checksum, let’s next analyze how to use checksums in a storage system. The ﬁrst q uestion we must address is the layout of the checksum, i.e., how should che ck- sums be stored on disk? The most basic approach simply stores a checksum with each disk s ec- tor (or block). Given a data block D, let us call the checksum over that dataC(D). Thus, without checksums, the disk layout looks like this: D0 D1 D2 D3 D4 D5 D6 With checksums, the layout adds a single checksum for every bloc k: OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 7C[D0]D0 C[D1]D1 C[D2]D2 C[D3]D3 C[D4]D4 Because checksums are usually small (e.g., 8 bytes), and dis ks only can write in sector-sized chunks (512 bytes) or multiples thereof, one problem that arises is how to achieve the above layout. One solution employe d by drive manufacturers is to format the drive with 520-byte sect ors; an extra 8 bytes per sector can be used to store the checksum. In disks that don’t have such functionality, the ﬁle system must ﬁgure out a way to store the checksums packed into 512-byte blocks. One such possibility is as follows: C[D0] C[D1] C[D2] C[D3] C[D4]D0 D1 D2 D3 D4 In this scheme, the nchecksums are stored together in a sector, fol- lowed by ndata blocks, followed by another checksum sector for the nextnblocks, and so forth. This approach has the beneﬁt of working on all disks, but can be less efﬁcient; if the ﬁle system, for exa mple, wants to overwrite block D1, it has to read in the checksum sector containing C(D1), update C(D1)in it, and then write out the checksum sector and new data block D1(thus, one read and two writes). The earlier approach (of one checksum per sector) just performs a single write. 45.4 Using Checksums With a checksum layout decided upon, we can now proceed to actu- ally understand how to usethe checksums.",3107
45. Data Integrity and Protection,"When reading a block D, the client (i.e., ﬁle system or storage controller) also reads its ch ecksum from diskCs(D), which we call the stored checksum (hence the subscript Cs). The client then computes the checksum over the retrieved block D, which we call the computed checksum Cc(D). At this point, the client com- pares the stored and computed checksums; if they are equal (i.e .,Cs(D) ==Cc(D), the data has likely not been corrupted, and thus can be safely returned to the user. If they do notmatch (i.e., Cs(D).=Cc(D)), this im- plies the data has changed since the time it was stored (since t he stored checksum reﬂects the value of the data at that time). In this ca se, we have a corruption, which our checksum has helped us to detect. Given a corruption, the natural question is what should we do about it? If the storage system has a redundant copy, the answer is eas y: try to use it instead. If the storage system has no such copy, the likel y answer is to return an error. In either case, realize that corruption dete ction is not a magic bullet; if there is no other way to get the non-corrupted da ta, you are simply out of luck. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 DATA INTEGRITY AND PROTECTION 45.5 A New Problem: Misdirected Writes The basic scheme described above works well in the general case of corrupted blocks. However, modern disks have a couple of unusual fa il- ure modes that require different solutions. The ﬁrst failure mode of interest is called a misdirected write . This arises in disk and RAID controllers which write the data to disk correctly, except in the wrong location. In a single-disk system, this means that the disk wrote block Dxnot to address x(as desired) but rather to address y(thus “corrupting” Dy); in addition, within a multi-disk system, the controller may also write Di,xnot to address xof diskibut rather to some other disk j. Thus our question: CRUX: HOWTOHANDLE MISDIRECTED WRITES How should a storage system or disk controller detect misdirected writes? What additional features are required from the checks um? The answer, not surprisingly, is simple: add a little more infor mation to each checksum. In this case, adding a physical identiﬁer (physical ID ) is quite helpful. For example, if the stored information now contai ns the checksum C(D)and both the disk and sector numbers of the block, it is easy for the client to determine whether the correct information resides within a particular locale. Speciﬁcally, if the client is read ing block 4 on disk 10 ( D10,4), the stored information should include that disk number and sector offset, as shown below. If the information does not match, a misdirected write has taken place, and a corruption is now detec ted. Here is an example of what this added information would look like on a two- disk system. Note that this ﬁgure, like the others before it, is n ot to scale, as the checksums are usually small (e.g., 8 bytes) whereas th e blocks are much larger (e.g., 4 KB or bigger): Disk 0Disk 1 C[D0] disk=0 block=0D0 C[D1] disk=0 block=1D1 C[D2] disk=0 block=2D2C[D0] disk=1 block=0D0 C[D1] disk=1 block=1D1 C[D2] disk=1 block=2D2 You can see from the on-disk format that there is now a fair amount of redundancy on disk: for each block, the disk number is repeated w ithin each block, and the offset of the block in question is also kept next to the block itself.",3401
45. Data Integrity and Protection,"The presence of redundant information should be no s ur- prise, though; redundancy is the key to error detection (in this case) and recovery (in others). A little extra information, while not stric tly needed with perfect disks, can go a long ways in helping detect problem atic situ- ations should they arise. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 9 45.6 One Last Problem: Lost Writes Unfortunately, misdirected writes are not the last problem we w ill address. Speciﬁcally, some modern storage devices also have an i ssue known as a lost write , which occurs when the device informs the up- per layer that a write has completed but in fact it never is pers isted; thus, what remains is the old contents of the block rather than the updat ed new contents. The obvious question here is: do any of our checksumming strategie s from above (e.g., basic checksums, or physical identity) help t o detect lost writes? Unfortunately, the answer is no: the old block likely has a matching checksum, and the physical ID used above (disk numbe r and block offset) will also be correct. Thus our ﬁnal problem: CRUX: HOWTOHANDLE LOST WRITES How should a storage system or disk controller detect lost writes? What additional features are required from the checksum? There are a number of possible solutions that can help [K+08]. One classic approach [BS04] is to perform a write verify orread-after-write ; by immediately reading back the data after a write, a system c an ensure that the data indeed reached the disk surface. This approach, however, is quite slow, doubling the number of I/Os needed to complete a write . Some systems add a checksum elsewhere in the system to detect los t writes. For example, Sun’s Zettabyte File System (ZFS) includes a check- sum in each ﬁle system inode and indirect block for every block inc luded within a ﬁle. Thus, even if the write to a data block itself is los t, the check- sum within the inode will not match the old data. Only if the write s to both the inode and the data are lost simultaneously will such a sch eme fail, an unlikely (but unfortunately, possible.) situation. 45.7 Scrubbing Given all of this discussion, you might be wondering: when do thes e checksums actually get checked? Of course, some amount of checki ng occurs when data is accessed by applications, but most data is ra rely accessed, and thus would remain unchecked. Unchecked data is prob- lematic for a reliable storage system, as bit rot could eventuall y affect all copies of a particular piece of data. To remedy this problem, many systems utilize disk scrubbing of var- ious forms [K+08]. By periodically reading through every block of the system, and checking whether checksums are still valid, the disk system can reduce the chances that all copies of a certain data item bec ome cor- rupted. Typical systems schedule scans on a nightly or weekly b asis. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 DATA INTEGRITY AND PROTECTION 45.8 Overheads Of Checksumming Before closing, we now discuss some of the overheads of using check- sums for data protection.",3130
45. Data Integrity and Protection,"There are two distinct kinds of overhead s, as is common in computer systems: space and time. Space overheads come in two forms. The ﬁrst is on the disk (or other storage medium) itself; each stored checksum takes up room on the d isk, which can no longer be used for user data. A typical ratio might b e an 8- byte checksum per 4 KB data block, for a 0.19 percent on-disk space overhe ad. The second type of space overhead comes in the memory of the sys- tem. When accessing data, there must now be room in memory for the checksums as well as the data itself. However, if the system si mply checks the checksum and then discards it once done, this overhead is shor t-lived and not much of a concern. Only if checksums are kept in memory (for an added level of protection against memory corruption [Z+13]) wil l this small overhead be observable. While space overheads are small, the time overheads induced by check- summing can be quite noticeable. Minimally, the CPU must compu te the checksum over each block, both when the data is stored (to determi ne the value of the stored checksum) and when it is accessed (to compute the checksum again and compare it against the stored checksum). On e ap- proach to reducing CPU overheads, employed by many systems that use checksums (including network stacks), is to combine data copyi ng and checksumming into one streamlined activity; because the copy is needed anyhow (e.g., to copy the data from the kernel page cache into a us er buffer), combined copying/checksumming can be quite effecti ve. Beyond CPU overheads, some checksumming schemes can induce ex- tra I/O overheads, particularly when checksums are stored dis tinctly from the data (thus requiring extra I/Os to access them), and for an y extra I/O needed for background scrubbing. The former can be reduced by de sign; the latter can be tuned and thus its impact limited, perhaps b y control- ling when such scrubbing activity takes place. The middle of t he night, when most (not all.) productive workers have gone to bed, may be a good time to perform such scrubbing activity and increase the rob ustness of the storage system. 45.9 Summary We have discussed data protection in modern storage systems, focu s- ing on checksum implementation and usage. Different checksum s protect against different types of faults; as storage devices evolve, n ew failure modes will undoubtedly arise. Perhaps such change will force th e re- search community and industry to revisit some of these basic app roaches, or invent entirely new approaches altogether. Time will tell. O r it won’t. Time is funny that way. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 11 References [B+07] “An Analysis of Latent Sector Errors in Disk Drives” by L. B airavasundaram, G. Good- son, S. Pasupathy, J. Schindler. SIGMETRICS ’07, San Diego, CA. The ﬁrst paper to study latent sector errors in detail. The paper also won the Kenneth C. Sevcik Outstandin g Student Paper award, named after a brilliant researcher and wonderful guy who passed away too soon .",3072
45. Data Integrity and Protection,"To show the OSTEP authors it was possible to move from the U.S. to Canada, Ken once sang us the Canadian national anthem, standing up in the middle of a restaurant to do so. We chose the U.S., b ut got this memory. [B+08] “An Analysis of Data Corruption in the Storage Stack” by La kshmi N. Bairavasun- daram, Garth R. Goodson, Bianca Schroeder, Andrea C. Arpaci-Dusseau , Remzi H. Arpaci- Dusseau. FAST ’08, San Jose, CA, February 2008. The ﬁrst paper to truly study disk corruption in great detail, focusing on how often such corruption occurs over three years for over 1.5 million drives. [BS04] “Commercial Fault Tolerance: A Tale of Two Systems” by Wend y Bartlett, Lisa Spainhower. IEEE Transactions on Dependable and Secure Computing, Vol. 1:1, Janua ry 2004. This classic in building fault tolerant systems is an excellent overview of the state of th e art from both IBM and Tandem. Another must read for those interested in the area. [C+04] “Row-Diagonal Parity for Double Disk Failure Correction” by P . Corbett, B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar. FAST ’04, San Jose, C A, February 2004. An early paper on how extra redundancy helps to solve the combined full-disk-f ailure/partial-disk-failure problem. Also a nice example of how to mix more theoretical work with practical . [F04] “Checksums and Error Control” by Peter M. Fenwick. Copy availabl e online here: http://www.ostep.org/Citations/checksums-03.pdf .A great simple tutorial on check- sums, available to you for the amazing cost of free. [F82] “An Arithmetic Checksum for Serial Transmissions” by John G. Fle tcher. IEEE Trans- actions on Communication, Vol. 30:1, January 1982. Fletcher’s original work on his eponymous checksum. He didn’t call it the Fletcher checksum, rather he just did n’t call it anything; later, others named it after him. So don’t blame old Fletch for this seeming act of braggadoc io. This anecdote might remind you of Rubik; Rubik never called it “ Rubik’s cube ”; rather, he just called it “my cube.” [HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau, Michael Malcolm. USENIX Spring ’94. The pioneering paper that describes the ideas and product at the heart of NetApp’s core. Based on this system, NetApp has grown into a multi -billion dollar storage company. To learn more about NetApp, read Hitz’s autobiography “How to Castrate a Bul l” (which is the actual title, no joking). And you thought you could avoid bull castration by going i nto CS. [K+08] “Parity Lost and Parity Regained” by Andrew Krioukov, Lakshm i N. Bairavasun- daram, Garth R. Goodson, Kiran Srinivasan, Randy Thelen, Andrea C. Ar paci-Dusseau, Remzi H. Arpaci-Dusseau. FAST ’08, San Jose, CA, February 2008. This work explores how different checksum schemes work (or don’t work) in protecting data. We reveal a number of interesting ﬂaws in current protection strategies. [M13] “Cyclic Redundancy Checks” by unknown. Available: http://www.mathpages.com/ home/kmath458.htm .A super clear and concise description of CRCs.",3060
45. Data Integrity and Protection,"The internet is full of i nfor- mation, as it turns out. [P+05] “IRON File Systems” by V . Prabhakaran, L. Bairavasundaram, N . Agrawal, H. Gunawi, A. Arpaci-Dusseau, R. Arpaci-Dusseau. SOSP ’05, Brighton, England. Our paper on how disks have partial failure modes, and a detailed study of how modern ﬁle systems reac t to such failures. As it turns out, rather poorly. We found numerous bugs, design ﬂaws, and other oddi ties in this work. Some of this has fed back into the Linux community, thus improving ﬁle system re liability. You’re welcome. [RO91] “Design and Implementation of the Log-structured File Syste m” by Mendel Rosen- blum and John Ousterhout. SOSP ’91, Paciﬁc Grove, CA, October 1991. So cool we cite it again. [S90] “Implementing Fault-Tolerant Services Using The State Machine Appr oach: A Tutorial” by Fred B. Schneider. ACM Surveys, Vol. 22, No. 4, December 1990. How to build fault tolerant services. A must read for those building distributed systems. [Z+13] “Zettabyte Reliability with Flexible End-to-end Data Inte grity” by Y. Zhang, D. Myers, A. Arpaci-Dusseau, R. Arpaci-Dusseau. MSST ’13, Long Beach, Californi a, May 2013. How to add data protection to the page cache of a system. Out of space, otherwise we would w rite something... c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 DATA INTEGRITY AND PROTECTION Homework (Simulation) In this homework, you’ll use checksum.py to investigate various as- pects of checksums. Questions 1. First just run checksum.py with no arguments. Compute the additive, XOR-based, and Fletcher checksums. Use -cto check your answers. 2. Now do the same, but vary the seed ( -s) to different values. 3. Sometimes the additive and XOR-based checksums produce the same c heck- sum (e.g., if the data value is all zeroes). Can you pass in a 4-by te data value (using the -Dﬂag, e.g.,-D a,b,c,d ) that does not contain only zeroes and leads the additive and XOR-based checksum having the same value? In general, when does this occur? Check that you are correct with the-cﬂag. 4. Now pass in a 4-byte value that you know will produce a differe nt checksum values for additive and XOR. In general, when does this occur? 5. Use the simulator to compute checksums twice (once each for a diffe rent set of numbers). The two number strings should be different (e.g., -D a1,b1,c1,d1 the ﬁrst time and -D a2,b2,c2,d2 the second) but should produce the same additive checksum. In general, when will the additive chec ksum be the same, even though the data values are different? Check your sp eciﬁc answer with the -cﬂag. 6. Now do the same for the XOR checksum. 7. Now let’s look at a speciﬁc set of data values. The ﬁrst is: -D 1,2,3,4 . What will the different checksums (additive, XOR, Fletcher) be for this data? Now compare it to computing these checksums over -D 4,3,2,1 . What do you notice about these three checksums? How does Fletcher comp are to the other two? How is Fletcher generally “better” than someth ing like the simple additive checksum? 8. No checksum is perfect. Given a particular input of your choosi ng, can you ﬁnd other data values that lead to the same Fletcher checksum? Whe n, in general, does this occur? Start with a simple data string (e.g. ,-D 0,1,2,3 ) and see if you can replace one of those numbers but end up with the sa me Fletcher checksum. As always, use -cto check your answers. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DATA INTEGRITY AND PROTECTION 13 Homework (Code) In this part of the homework, you’ll write some of your own code to implement various checksums. Questions 1. Write a short C program (called check-xor.c ) that computes an XOR- based checksum over an input ﬁle, and prints the checksum as output . Use a 8-bit unsigned char to store the (one byte) checksum. Make some t est ﬁles to see if it works as expected. 2. Now write a short C program (called check-fletcher.c ) that computes the Fletcher checksum over an input ﬁle. Once again, test your pr ogram to see if it works. 3. Now compare the performance of both: is one faster than the ot her? How does performance change as the size of the input ﬁle changes? Use internal calls togettimeofday to time the programs. Which should you use if you care about performance? About checking ability? 4. Read about the 16-bit CRC and then implement it. Test it on a numbe r of different inputs to ensure that it works. How is its performance as compared to the simple XOR and Fletcher? How about its checking ability? 5. Now build a tool ( create-csum.c ) that computes a single-byte checksum for every 4KB block of a ﬁle, and records the results in an output ﬁ le (speci- ﬁed on the command line). Build a related tool ( check-csum.c ) that reads a ﬁle, computes the checksums over each block, and compares the res ults to the stored checksums stored in another ﬁle. If there is a prob lem, the program should print that the ﬁle has been corrupted. Test the p rogram by manually corrupting the ﬁle. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",5031
46. Summary Dialogue on Persistence,"46 Summary Dialogue on Persistence Student: Wow, ﬁle systems seem interesting(.), and yet complicated. Professor: That’s why my spouse and I do our research in this space. Student: Hold on. Are you one of the professors who wrote this book? I thoug ht we were both just fake constructs, used to summarize some main poin ts, and perhaps add a little levity in the study of operating systems. Professor: Uh... er... maybe. And none of your business. And who did you think was writing these things? (sighs) Anyhow, let’s get on with it: w hat did you learn? Student: Well, I think I got one of the main points, which is that it is much harder to manage data for a long time (persistently) than it is to mana ge data that isn’t persistent (like the stuff in memory). After all, if your mach ines crashes, memory contents disappear. But the stuff in the ﬁle system needs to live forever. Professor: Well, as my friend Kevin Hultquist used to say, “Forever is a long time”; while he was talking about plastic golf tees, it’s especially true fo r the garbage that is found in most ﬁle systems. Student: Well, you know what I mean. For a long time at least. And even simple things, such as updating a persistent storage device, are complica ted, because you have to care what happens if you crash. Recovery, something I ha d never even thought of when we were virtualizing memory, is now a big deal. Professor: Too true. Updates to persistent storage have always been, and r e- main, a fun and challenging problem. Student: I also learned about cool things like disk scheduling, and about data protection techniques like RAID and even checksums. That stuff is c ool. Professor: I like those topics too. Though, if you really get into it, they can get a little mathematical. Check out some the latest on erasure codes if yo u want your brain to hurt. Student: I’ll get right on that. 1 2 SUMMARY DIALOGUE ON PERSISTENCE Professor: (frowns) I think you’re being sarcastic. Well, what else did you like? Student: And I also liked all the thought that has gone into building technology- aware systems, like FFS and LFS. Neat stuff. Being disk aware seems c ool. But will it matter anymore, with Flash and all the newest, latest technolo gies? Professor: Good question. And a reminder to get working on that Flash chap- ter... (scribbles note down to self) ... But yes, even with Flash, all of this stuff is still relevant, amazingly. For example, Flash Translation Layers (F TLs) use log-structuring internally, to improve performance and reliability of F lash-based SSDs. And thinking about locality is always useful. So while the technolog y may be changing, many of the ideas we have studied will continue to be useful, for a while at least. Student: That’s good. I just spent all this time learning it, and I didn’t want it to all be for no reason. Professor: Professors wouldn’t do that to you, would they? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2948
47. A Dialogue on Distribution,"47 A Dialogue on Distribution Professor: And thus we reach our ﬁnal little piece in the world of operating systems: distributed systems. Since we can’t cover much here, w e’ll sneak in a little intro here in the section on persistence, and focus mostly on dist ributed ﬁle systems. Hope that is OK. Student: Sounds OK. But what is a distributed system exactly, oh glorious and all-knowing professor? Professor: Well, I bet you know how this is going to go... Student: There’s a peach? Professor: Exactly. But this time, it’s far away from you, and may take some time to get the peach. And there are a lot of them. Even worse, som etimes a peach becomes rotten. But you want to make sure that when anyb ody bites into a peach, they will get a mouthful of deliciousness. Student: This peach analogy is working less and less for me. Professor: Come on. It’s the last one, just go with it. Student: Fine. Professor: So anyhow, forget about the peaches. Building distributed systems is hard, because things fail all the time. Messages get lost, machines go down, disks corrupt data. It’s like the whole world is working against you. Student: But I use distributed systems all the time, right? Professor: Yes. You do. And... ? Student: Well, it seems like they mostly work. After all, when I send a search request to Google, it usually comes back in a snap, with some great re sults. Same thing when I use Facebook, Amazon, and so forth. 1 2 A D IALOGUE ON DISTRIBUTION Professor: Yes, it is amazing. And that’s despite all of those failures taking place. Those companies build a huge amount of machinery into their sy stems so as to ensure that even though some machines have failed, the entire system stays up and running. They use a lot of techniques to do this: replication, r etry, and various other tricks people have developed over time to detect and recover from failures. Student: Sounds interesting. Time to learn something for real? Professor: It does seem so. Let’s get to work. But ﬁrst things ﬁrst ... (bites into peach he has been holding, which unfortunately is rotten) OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2128
48. Distributed Systems,"48 Distributed Systems Distributed systems have changed the face of the world. When you r web browser connects to a web server somewhere else on the planet, it i s par- ticipating in what seems to be a simple form of a client/server distributed system. When you contact a modern web service such as Google or Face- book, you are not just interacting with a single machine, however; be- hind the scenes, these complex services are built from a large c ollection (i.e., thousands) of machines, each of which cooperate to provide t he par- ticular service of the site. Thus, it should be clear what makes studying distributed systems interesting. Indeed, it is worthy of an en tire class; here, we just introduce a few of the major topics. A number of new challenges arise when building a distributed s ystem. The major one we focus on is failure ; machines, disks, networks, and software all fail from time to time, as we do not (and likely, will never) know how to build “perfect” components and systems. However, when we build a modern web service, we’d like it to appear to clients a s if it never fails; how can we accomplish this task? THECRUX: HOWTOBUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL How can we build a working system out of parts that don’t work correctly all the time? The basic question should remind you of some of the topic s we discussed in RAID storage arrays; however, the problems here tend to be more complex, as are the solutions. Interestingly, while failure is a central challenge in const ructing dis- tributed systems, it also represents an opportunity. Yes, mac hines fail; but the mere fact that a machine fails does not imply the entire s ystem must fail. By collecting together a set of machines, we can build a sys- tem that appears to rarely fail, despite the fact that its comp onents fail regularly. This reality is the central beauty and value of dis tributed sys- tems, and why they underly virtually every modern web service you use, including Google, Facebook, etc. 1 2 DISTRIBUTED SYSTEMS TIP: COMMUNICATION ISINHERENTLY UNRELIABLE In virtually all circumstances, it is good to view communication as a fundamentally unreliable activity. Bit corruption, down or non- working links and machines, and lack of buffer space for incoming packet s all lead to the same result: packets sometimes do not reach their destin ation. To build reliable services atop such unreliable networks, we mus t consider techniques that can cope with packet loss. Other important issues exist as well. System performance is often crit- ical; with a network connecting our distributed system together , system designers must often think carefully about how to accomplish the ir given tasks, trying to reduce the number of messages sent and furthe r make communication as efﬁcient (low latency, high bandwidth) as poss ible. Finally, security is also a necessary consideration. When connecting to a remote site, having some assurance that the remote party is w ho they say they are becomes a central problem. Further, ensuring that third parties cannot monitor or alter an on-going communication between tw o others is also a challenge.",3152
48. Distributed Systems,"In this introduction, we’ll cover the most basic aspect that is new in a distributed system: communication . Namely, how should machines within a distributed system communicate with one another? We’ll start with the most basic primitives available, messages, and buil d a few higher- level primitives on top of them. As we said above, failure will be a central focus: how should communication layers handle failures? 48.1 Communication Basics The central tenet of modern networking is that communication is fu n- damentally unreliable. Whether in the wide-area Internet, or a local-area high-speed network such as Inﬁniband, packets are regularly lost, cor- rupted, or otherwise do not reach their destination. There are a multitude of causes for packet loss or corruption. Some- times, during transmission, some bits get ﬂipped due to electr ical or other similar problems. Sometimes, an element in the system, such as a net- work link or packet router or even the remote host, are somehow dam- aged or otherwise not working correctly; network cables do acciden tally get severed, at least sometimes. More fundamental however is packet loss due to lack of buffering within a network switch, router, or endpoint. Speciﬁcally, even i f we could guarantee that all links worked correctly, and that all th e compo- nents in the system (switches, routers, end hosts) were up and r unning as expected, loss is still possible, for the following reason. Imagin e a packet arrives at a router; for the packet to be processed, it must be pla ced in memory somewhere within the router. If many such packets arrive at OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 3 // client code int main(int argc, char *argv[]) { int sd = UDP_Open(20000); struct sockaddr_in addrSnd, addrRcv; int rc = UDP_FillSockAddr(&addrSnd, \""machine.cs.wisc.ed u\"", 10000); char message[BUFFER_SIZE]; sprintf(message, \""hello world\""); rc = UDP_Write(sd, &addrSnd, message, BUFFER_SIZE); if (rc > 0) { int rc = UDP_Read(sd, &addrRcv, message, BUFFER_SIZE); } return 0; } // server code int main(int argc, char *argv[]) { int sd = UDP_Open(10000); assert(sd > -1); while (1) { struct sockaddr_in addr; char message[BUFFER_SIZE]; int rc = UDP_Read(sd, &addr, message, BUFFER_SIZE); if (rc > 0) { char reply[BUFFER_SIZE]; sprintf(reply, \""goodbye world\""); rc = UDP_Write(sd, &addr, reply, BUFFER_SIZE); } } return 0; } Figure 48.1: Example UDP/IP Client/Server Code once, it is possible that the memory within the router cannot accomm o- date all of the packets. The only choice the router has at that point is todrop one or more of the packets. This same behavior occurs at end hosts as well; when you send a large number of messages to a single ma- chine, the machine’s resources can easily become overwhelmed, a nd thus packet loss again arises. Thus, packet loss is fundamental in networking. The question th us becomes: how should we deal with it? 48.2 Unreliable Communication Layers One simple way is this: we don’t deal with it. Because some appli- cations know how to deal with packet loss, it is sometimes useful to let them communicate with a basic unreliable messaging layer, an example of the end-to-end argument one often hears about (see the Aside at end of chapter). One excellent example of such an unreliable layer is found in the UDP/IP networking stack available today on virtually all modern systems.",3406
48. Distributed Systems,"To use UDP , a process uses the sockets API in order to create a communication endpoint ; processes on other machines (or on the same machine) send UDP datagrams to the original process (a datagram is a ﬁxed-sized message up to some max size). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 DISTRIBUTED SYSTEMS int UDP_Open(int port) { int sd; if ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) { return -1; } struct sockaddr_in myaddr; bzero(&myaddr, sizeof(myaddr)); myaddr.sin_family = AF_INET; myaddr.sin_port = htons(port); myaddr.sin_addr.s_addr = INADDR_ANY; if (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) { close(sd); return -1; } return sd; } int UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) { bzero(addr, sizeof(struct sockaddr_in)); addr->sin_family = AF_INET; // host byte order addr->sin_port = htons(port); // short, network byte order struct in_addr *inAddr; struct hostent *hostEntry; if ((hostEntry = gethostbyname(hostName)) == NULL) { retur n -1; } inAddr = (struct in_addr *) hostEntry->h_addr; addr->sin_addr = *inAddr; return 0; } int UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) { int addrLen = sizeof(struct sockaddr_in); return sendto(sd, buffer, n, 0, (struct sockaddr *) addr, addrLen); } int UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) { int len = sizeof(struct sockaddr_in); return recvfrom(sd, buffer, n, 0, (struct sockaddr *) addr, (socklen_t *) &len); } Figure 48.2: A Simple UDP Library Figures 48.1 and 48.2 show a simple client and server built on top of UDP/IP . The client can send a message to the server, which the n responds with a reply. With this small amount of code, you have all you need to begin building distributed systems. UDP is a great example of an unreliable communication layer. If y ou use it, you will encounter situations where packets get lost (drop ped) and thus do not reach their destination; the sender is never thus in formed of the loss. However, that does not mean that UDP does not guard against any failures at all. For example, UDP includes a checksum to detect some forms of packet corruption. However, because many applications simply want to send data to a destination and not worry about packet loss, we need more. Speciﬁcal ly, we need reliable communication on top of an unreliable network. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 5 TIP: USECHECKSUMS FORINTEGRITY Checksums are a commonly-used method to detect corruption quickl y and effectively in modern systems. A simple checksum is addit ion: just sum up the bytes of a chunk of data; of course, many other more sophis- ticated checksums have been created, including basic cycli c redundancy codes (CRCs), the Fletcher checksum, and many others [MK09]. In networking, checksums are used as follows. Before sending a me ssage from one machine to another, compute a checksum over the bytes of the message. Then send both the message and the checksum to the des ti- nation. At the destination, the receiver computes a checksum ove r the incoming message as well; if this computed checksum matches th e sent checksum, the receiver can feel some assurance that the data l ikely did not get corrupted during transmission. Checksums can be evaluated along a number of different axes. Ef fective- ness is one primary consideration: does a change in the data lead t o a change in the checksum? The stronger the checksum, the harder it is for changes in the data to go unnoticed. Performance is the other imp ortant criterion: how costly is the checksum to compute? Unfortunately, effec- tiveness and performance are often at odds, meaning that checks ums of high quality are often expensive to compute.",3744
48. Distributed Systems,"Life, again, isn’t perfect. 48.3 Reliable Communication Layers To build a reliable communication layer, we need some new mech- anisms and techniques to handle packet loss. Let us consider a s imple example in which a client is sending a message to a server over a n unreli- able connection. The ﬁrst question we must answer: how does the sen der know that the receiver has actually received the message? The technique that we will use is known as an acknowledgment , or ackfor short. The idea is simple: the sender sends a message to the r e- ceiver; the receiver then sends a short message back to acknowledge its receipt. Figure 48.3 depicts the process. Sender [send message]Receiver [receive message] [send ack] [receive ack] Figure 48.3: Message Plus Acknowledgment When the sender receives an acknowledgment of the message, it c an then rest assured that the receiver did indeed receive the ori ginal mes- sage. However, what should the sender do if it does not receive an a c- knowledgment? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 DISTRIBUTED SYSTEMS Sender [send message;  keep copy;  set timer]Receiver ... (waiting for ack) ... [timer goes off;  set timer/retry] [receive message] [send ack] [receive ack;  delete copy/timer off] Figure 48.4: Message Plus Acknowledgment: Dropped Request To handle this case, we need an additional mechanism, known as a timeout . When the sender sends a message, the sender now sets a timer to go off after some period of time. If, in that time, no acknowledgm ent has been received, the sender concludes that the message has b een lost. The sender then simply performs a retry of the send, sending the same message again with hopes that this time, it will get through. For this approach to work, the sender must keep a copy of the message around, in case it needs to send it again. The combination of the timeout an d the retry have led some to call the approach timeout/retry ; pretty clever crowd, those networking types, no? Figure 48.4 shows an example. Unfortunately, timeout/retry in this form is not quite enough. Fi gure 48.5 shows an example of packet loss which could lead to trouble. In this example, it is not the original message that gets lost, but the ac knowledg- ment. From the perspective of the sender, the situation seems th e same: no ack was received, and thus a timeout and retry are in order. Bu t from the perspective of the receiver, it is quite different: now the same message has been received twice. While there may be cases where this i s OK, in general it is not; imagine what would happen when you are downloadi ng a ﬁle and extra packets are repeated inside the download. Thus, when we are aiming for a reliable message layer, we also usually want t o guarantee that each message is received exactly once by the receiver. To enable the receiver to detect duplicate message transmis sion, the sender has to identify each message in some unique way, and the receiver needs some way to track whether it has already seen each messag e be- fore. When the receiver sees a duplicate transmission, it simp ly acks the message, but (critically) does notpass the message to the application that receives the data. Thus, the sender receives the ack but the m essage is not received twice, preserving the exactly-once semantics ment ioned above.",3329
48. Distributed Systems,"There are myriad ways to detect duplicate messages. For examp le, the sender could generate a unique ID for each message; the receive r could track every ID it has ever seen. This approach could work, but it i s pro- hibitively costly, requiring unbounded memory to track all IDs . OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 7 Sender [send message;  keep copy;  set timer]Receiver [receive message] [send ack] ... (waiting for ack) ... [timer goes off;  set timer/retry] [receive message] [send ack] [receive ack;  delete copy/timer off] Figure 48.5: Message Plus Acknowledgment: Dropped Reply A simpler approach, requiring little memory, solves this proble m, and the mechanism is known as a sequence counter . With a sequence counter, the sender and receiver agree upon a start value (e.g., 1) for a c ounter that each side will maintain. Whenever a message is sent, the current value of the counter is sent along with the message; this counter v alue (N) serves as an ID for the message. After the message is sent, the sender then increments the value (to N+1). The receiver uses its counter value as the expected value for th e ID of the incoming message from that sender. If the ID of a received me s- sage (N) matches the receiver’s counter (also N), it acks the message and passes it up to the application; in this case, the receiver conc ludes this is the ﬁrst time this message has been received. The receiver then incre- ments its counter (to N+1), and waits for the next message. If the ack is lost, the sender will timeout and re-send message N. This time, the receiver’s counter is higher ( N+1), and thus the receiver knows it has already received this message. Thus it acks the messag e but does notpass it up to the application. In this simple manner, sequence counters can be used to avoid duplicates. The most commonly used reliable communication layer is known as TCP/IP , or just TCP for short. TCP has a great deal more sophistication than we describe above, including machinery to handle congest ion in the network [VJ88], multiple outstanding requests, and hundreds of other small tweaks and optimizations. Read more about it if you’re curious ; better yet, take a networking course and learn that material we ll. 48.4 Communication Abstractions Given a basic messaging layer, we now approach the next question in this chapter: what abstraction of communication should we use w hen building a distributed system? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 DISTRIBUTED SYSTEMS TIP: BECAREFUL SETTING THETIMEOUT VALUE As you can probably guess from the discussion, setting the timeout value correctly is an important aspect of using timeouts to retry messa ge sends. If the timeout is too small, the sender will re-send messages ne edlessly, thus wasting CPU time on the sender and network resources. If the time- out is too large, the sender waits too long to re-send and thus perc eived performance at the sender is reduced. The “right” value, from t he per- spective of a single client and server, is thus to wait just long enough to detect packet loss but no longer.",3133
48. Distributed Systems,"However, there are often more than just a single client and serve r in a distributed system, as we will see in future chapters. In a sc enario with many clients sending to a single server, packet loss at the ser ver may be an indicator that the server is overloaded. If true, clients mig ht retry in a different adaptive manner; for example, after the ﬁrst time out, a client might increase its timeout value to a higher amount, perhaps tw ice as high as the original value. Such an exponential back-off scheme, pio- neered in the early Aloha network and adopted in early Ethernet [ A70], avoids situations where resources are being overloaded by an exce ss of re-sends. Robust systems strive to avoid overload of this nature. The systems community developed a number of approaches over the years. One body of work took OS abstractions and extended them to operate in a distributed environment. For example, distributed shared memory (DSM ) systems enable processes on different machines to share a large, virtual address space [LH89]. This abstraction turn s a distributed computation into something that looks like a multi-threaded appl ication; the only difference is that these threads run on different mach ines instead of different processors within the same machine. The way most DSM systems work is through the virtual memory sys- tem of the OS. When a page is accessed on one machine, two things can happen. In the ﬁrst (best) case, the page is already local on the machine, and thus the data is fetched quickly. In the second case, the pa ge is cur- rently on some other machine. A page fault occurs, and the page fau lt handler sends a message to some other machine to fetch the page, install it in the page table of the requesting process, and continue exec ution. This approach is not widely in use today for a number of reasons. The largest problem for DSM is how it handles failure. Imagine, for exa mple, if a machine fails; what happens to the pages on that machine? W hat if the data structures of the distributed computation are spread a cross the entire address space? In this case, parts of these data struct ures would suddenly become unavailable. Dealing with failure when part s of your address space go missing is hard; imagine a linked list where a “next” pointer points into a portion of the address space that is gone. Yike s. A further problem is performance. One usually assumes, when wr it- ing code, that access to memory is cheap. In DSM systems, some acce sses OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 9 are inexpensive, but others cause page faults and expensive f etches from remote machines. Thus, programmers of such DSM systems had to be very careful to organize computations such that almost no communi ca- tion occurred at all, defeating much of the point of such an approach . Though much research was performed in this space, there was lit tle prac- tical impact; nobody builds reliable distributed systems usi ng DSM today. 48.5 Remote Procedure Call (RPC) While OS abstractions turned out to be a poor choice for building dis - tributed systems, programming language (PL) abstractions ma ke much more sense. The most dominant abstraction is based on the idea of a re- mote procedure call , orRPC for short [BN84]1.",3287
48. Distributed Systems,"Remote procedure call packages all have a simple goal: to make th e process of executing code on a remote machine as simple and straigh t- forward as calling a local function. Thus, to a client, a procedur e call is made, and some time later, the results are returned. The serve r simply deﬁnes some routines that it wishes to export. The rest of the magi c is handled by the RPC system, which in general has two pieces: a stub gen- erator (sometimes called a protocol compiler ), and the run-time library . We’ll now take a look at each of these pieces in more detail. Stub Generator The stub generator’s job is simple: to remove some of the pain of packi ng function arguments and results into messages by automating it . Numer- ous beneﬁts arise: one avoids, by design, the simple mistakes th at occur in writing such code by hand; further, a stub compiler can perha ps opti- mize such code and thus improve performance. The input to such a compiler is simply the set of calls a server wi shes to export to clients. Conceptually, it could be something as simp le as this: interface { int func1(int arg1); int func2(int arg1, int arg2); }; The stub generator takes an interface like this and generates a few dif- ferent pieces of code. For the client, a client stub is generated, which contains each of the functions speciﬁed in the interface; a clie nt program wishing to use this RPC service would link with this client stu b and call into it in order to make RPCs. Internally, each of these functions in the client stub do all of t he work needed to perform the remote procedure call. To the client, the c ode just 1In modern programming languages, we might instead say remote method invocation (RMI ), but who likes these languages anyhow, with all of their fancy objects? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 DISTRIBUTED SYSTEMS appears as a function call (e.g., the client calls func1(x) ); internally, the code in the client stub for func1() does this: •Create a message buffer. A message buffer is usually just a con- tiguous array of bytes of some size. •Pack the needed information into the message buffer. This infor- mation includes some kind of identiﬁer for the function to be calle d, as well as all of the arguments that the function needs (e.g., in our example above, one integer for func1 ). The process of putting all of this information into a single contiguous buffer is sometimes re - ferred to as the marshaling of arguments or the serialization of the message. •Send the message to the destination RPC server. The communi- cation with the RPC server, and all of the details required to ma ke it operate correctly, are handled by the RPC run-time library, de- scribed further below. •Wait for the reply. Because function calls are usually synchronous , the call will wait for its completion. •Unpack return code and other arguments. If the function just re- turns a single return code, this process is straightforward; how ever, more complex functions might return more complex results (e.g., a list), and thus the stub might need to unpack those as well.",3103
48. Distributed Systems,"Thi s step is also known as unmarshaling ordeserialization . •Return to the caller. Finally, just return from the client stub back into the client code. For the server, code is also generated. The steps taken on the ser ver are as follows: •Unpack the message. This step, called unmarshaling ordeserial- ization , takes the information out of the incoming message. The function identiﬁer and arguments are extracted. •Call into the actual function. Finally. We have reached the point where the remote function is actually executed. The RPC runtim e calls into the function speciﬁed by the ID and passes in the des ired arguments. •Package the results. The return argument(s) are marshaled back into a single reply buffer. •Send the reply. The reply is ﬁnally sent to the caller. There are a few other important issues to consider in a stub compil er. The ﬁrst is complex arguments, i.e., how does one package and send a complex data structure? For example, when one calls the write() system call, one passes in three arguments: an integer ﬁle des criptor, a pointer to a buffer, and a size indicating how many bytes (start ing at the pointer) are to be written. If an RPC package is passed a pointer , it needs to be able to ﬁgure out how to interpret that pointer, and perform t he OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 11 correct action. Usually this is accomplished through either wel l-known types (e.g., a buffertthat is used to pass chunks of data given a size, which the RPC compiler understands), or by annotating the data s truc- tures with more information, enabling the compiler to know which b ytes need to be serialized. Another important issue is the organization of the server with reg ards to concurrency. A simple server just waits for requests in a sim ple loop, and handles each request one at a time. However, as you might have guessed, this can be grossly inefﬁcient; if one RPC call blocks ( e.g., on I/O), server resources are wasted. Thus, most servers are const ructed in some sort of concurrent fashion. A common organization is a thread pool . In this organization, a ﬁnite set of threads are created when the server starts; when a message arrives, it is dispatched to one of these worker threads, which then does the work of the RPC call, eventually rep lying; during this time, a main thread keeps receiving other request s, and per- haps dispatching them to other workers. Such an organization enab les concurrent execution within the server, thus increasing its u tilization; the standard costs arise as well, mostly in programming complexity, as the RPC calls may now need to use locks and other synchronization primi - tives in order to ensure their correct operation. Run-Time Library The run-time library handles much of the heavy lifting in an RP C system; most performance and reliability issues are handled herein. W e’ll now discuss some of the major challenges in building such a run-time layer. One of the ﬁrst challenges we must overcome is how to locate a re- mote service.",3047
48. Distributed Systems,"This problem, of naming , is a common one in distributed systems, and in some sense goes beyond the scope of our current discu s- sion. The simplest of approaches build on existing naming system s, e.g., hostnames and port numbers provided by current internet protocols . In such a system, the client must know the hostname or IP address of th e machine running the desired RPC service, as well as the port nu mber it is using (a port number is just a way of identifying a particular com munica- tion activity taking place on a machine, allowing multiple commu nication channels at once). The protocol suite must then provide a mechanis m to route packets to a particular address from any other machine in t he sys- tem. For a good discussion of naming, you’ll have to look elsewhere, e.g ., read about DNS and name resolution on the Internet, or better yet ju st read the excellent chapter in Saltzer and Kaashoek’s book [SK09]. Once a client knows which server it should talk to for a particula r re- mote service, the next question is which transport-level protocol should RPC be built upon. Speciﬁcally, should the RPC system use a relia ble pro- tocol such as TCP/IP , or be built upon an unreliable communication l ayer such as UDP/IP? Naively the choice would seem easy: clearly we would like for a re- quest to be reliably delivered to the remote server, and clear ly we would c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 DISTRIBUTED SYSTEMS like to reliably receive a reply. Thus we should choose the relia ble trans- port protocol such as TCP , right? Unfortunately, building RPC on top of a reliable communication lay er can lead to a major inefﬁciency in performance. Recall from the d iscus- sion above how reliable communication layers work: with acknowledg - ments plus timeout/retry. Thus, when the client sends an RPC r equest to the server, the server responds with an acknowledgment so th at the caller knows the request was received. Similarly, when the ser ver sends the reply to the client, the client acks it so that the server k nows it was received. By building a request/response protocol (such as RPC) on top of a reliable communication layer, two “extra” messages are sen t. For this reason, many RPC packages are built on top of unreliable com - munication layers, such as UDP . Doing so enables a more efﬁcient RPC layer, but does add the responsibility of providing reliability to the RPC system. The RPC layer achieves the desired level of responsibi lity by us- ing timeout/retry and acknowledgments much like we described above. By using some form of sequence numbering, the communication layer can guarantee that each RPC takes place exactly once (in the ca se of no failure), or at most once (in the case where failure arises). Other Issues There are some other issues an RPC run-time must handle as well. For example, what happens when a remote call takes a long time to com- plete? Given our timeout machinery, a long-running remote call m ight appear as a failure to a client, thus triggering a retry, and t hus the need for some care here.",3094
48. Distributed Systems,"One solution is to use an explicit acknowledgme nt (from the receiver to sender) when the reply isn’t immediately generated; this lets the client know the server received the request. The n, after some time has passed, the client can periodically ask whether the s erver is still working on the request; if the server keeps saying “yes”, the cl ient should be happy and continue to wait (after all, sometimes a procedure c all can take a long time to ﬁnish executing). The run-time must also handle procedure calls with large argu ments, larger than what can ﬁt into a single packet. Some lower-level ne twork protocols provide such sender-side fragmentation (of larger packets into a set of smaller ones) and receiver-side reassembly (of smaller parts into one larger logical whole); if not, the RPC run-time may have to imp lement such functionality itself. See Birrell and Nelson’s excellent R PC paper for details [BN84]. One issue that many systems handle is that of byte ordering . As you may know, some machines store values in what is known as big endian ordering, whereas others use little endian ordering. Big endian stores bytes (say, of an integer) from most signiﬁcant to least signiﬁc ant bits, much like Arabic numerals; little endian does the opposite. Both are equally valid ways of storing numeric information; the question h ere is how to communicate between machines of different endianness. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 13 Aside: T HEEND-TO-ENDARGUMENT The end-to-end argument makes the case that the highest level in a system, i.e., usually the application at “the end”, is ultima tely the only locale within a layered system where certain functionality ca n truly be implemented. In their landmark paper [SRC84], Saltzer et al. a rgue this through an excellent example: reliable ﬁle transfer bet ween two ma- chines. If you want to transfer a ﬁle from machine Ato machine B, and make sure that the bytes that end up on Bare exactly the same as those that began on A, you must have an “end-to-end” check of this; lower- level reliable machinery, e.g., in the network or disk, provide s no such guarantee. The contrast is an approach which tries to solve the reliable-ﬁl e- transfer problem by adding reliability to lower layers of the sy stem. For example, say we build a reliable communication protocol and use it to build our reliable ﬁle transfer. The communication protocol guara ntees that every byte sent by a sender will be received in order by the receiver, say using timeout/retry, acknowledgments, and sequence numb ers. Un- fortunately, using such a protocol does not a reliable ﬁle transfer make; imagine the bytes getting corrupted in sender memory before the com- munication even takes place, or something bad happening when th e re- ceiver writes the data to disk. In those cases, even though the b ytes were delivered reliably across the network, our ﬁle transfer was ult imately not reliable. To build a reliable ﬁle transfer, one must includ e end-to- end checks of reliability, e.g., after the entire transfer is complete, read back the ﬁle on the receiver disk, compute a checksum, and compar e that checksum to that of the ﬁle on the sender.",3224
48. Distributed Systems,"The corollary to this maxim is that sometimes having lower layers pro- vide extra functionality can indeed improve system performanc e or oth- erwise optimize a system. Thus, you should not rule out having such machinery at a lower-level in a system; rather, you should caref ully con- sider the utility of such machinery, given its eventual usage in an overall system or application. RPC packages often handle this by providing a well-deﬁned endi - anness within their message formats. In Sun’s RPC package, the XDR (eXternal Data Representation ) layer provides this functionality. If the machine sending or receiving a message matches the endiannes s of XDR, messages are just sent and received as expected. If, however, the machine communicating has a different endianness, each piece of inform ation in the message must be converted. Thus, the difference in endian ness can have a small performance cost. A ﬁnal issue is whether to expose the asynchronous nature of com- munication to clients, thus enabling some performance optimiza tions. Speciﬁcally, typical RPCs are made synchronously , i.e., when a client issues the procedure call, it must wait for the procedure call to return c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 DISTRIBUTED SYSTEMS before continuing. Because this wait can be long, and because th e client may have other work it could be doing, some RPC packages enable you to invoke an RPC asynchronously . When an asynchronous RPC is is- sued, the RPC package sends the request and returns immediat ely; the client is then free to do other work, such as call other RPCs or other use- ful computation. The client at some point will want to see the resu lts of the asynchronous RPC; it thus calls back into the RPC layer, tel ling it to wait for outstanding RPCs to complete, at which point return argu ments can be accessed. 48.6 Summary We have seen the introduction of a new topic, distributed systems , and its major issue: how to handle failure which is now a commonplace ev ent. As they say inside of Google, when you have just your desktop machine , failure is rare; when you’re in a data center with thousands of mac hines, failure is happening all the time. The key to any distributed system is how you deal with that failure. We have also seen that communication forms the heart of any dis- tributed system. A common abstraction of that communication is foun d in remote procedure call (RPC), which enables clients to make r emote calls on servers; the RPC package handles all of the gory details , includ- ing timeout/retry and acknowledgment, in order to deliver a ser vice that closely mirrors a local procedure call. The best way to really understand an RPC package is of course to u se one yourself. Sun’s RPC system, using the stub compiler rpcgen , is an older one; Google’s gRPC and Apache Thrift are modern takes on the same. Try one out, and see what all the fuss is about. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG DISTRIBUTED SYSTEMS 15 References [A70] “The ALOHA System — Another Alternative for Computer Communicati ons” by Nor- man Abramson.",3112
48. Distributed Systems,"The 1970 Fall Joint Computer Conference. The ALOHA network pioneered some basic concepts in networking, including exponential back-off and retransmit, which formed the basis for communication in shared-bus Ethernet networks for years. [BN84] “Implementing Remote Procedure Calls” by Andrew D. Birrell , Bruce Jay Nelson. ACM TOCS, Volume 2:1, February 1984. The foundational RPC system upon which all others build. Yes, another pioneering effort from our friends at Xerox P ARC. [MK09] “The Effectiveness of Checksums for Embedded Control Networks” b y Theresa C. Maxino and Philip J. Koopman. IEEE Transactions on Dependable and Secure Co mputing, 6:1, January ’09. A nice overview of basic checksum machinery and some performance and robu stness comparisons between them. [LH89] “Memory Coherence in Shared Virtual Memory Systems” by Kai Li and Paul Hudak. ACM TOCS, 7:4, November 1989. The introduction of software-based shared memory via virtual memory. An intriguing idea for sure, but not a lasting or good one in the end . [SK09] “Principles of Computer System Design” by Jerome H. Saltzer and M. Frans Kaashoek. Morgan-Kaufmann, 2009. An excellent book on systems, and a must for every bookshelf. One of the few terriﬁc discussions on naming we’ve seen. [SRC84] “End-To-End Arguments in System Design” by Jerome H. Saltzer , David P . Reed, David D. Clark. ACM TOCS, 2:4, November 1984. A beautiful discussion of layering, abstraction, and where functionality must ultimately reside in computer systems. [VJ88] “Congestion Avoidance and Control” by Van Jacobson. SIGCOMM ’88 . A pioneering paper on how clients should adjust to perceived network congestion; deﬁni tely one of the key pieces of technology underlying the Internet, and a must read for anyone serious about s ystems, and for Van Jacobson’s relatives because well relatives should read all of your papers. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 DISTRIBUTED SYSTEMS Homework (Code) In this section, we’ll write some simple communication code to get you familiar with the task of doing so. Have fun. Questions 1. Using the code provided in the chapter, build a simple UDP-base d server and client. The server should receive messages from the client , and reply with an acknowledgment. In this ﬁrst attempt, do not add any ret ransmis- sion or robustness (assume that communication works perfectly). R un this on a single machine for testing; later, run it on two different ma chines. 2. Turn your code into a communication library . Speciﬁcally, make your own API, with send and receive calls, as well as other API calls as ne eded. Rewrite your client and server to use your library instead of raw socket c alls. 3. Add reliable communication to your burgeoning communication libra ry, in the form of timeout/retry . Speciﬁcally, your library should make a copy of any message that it is going to send. When sending it, it should s tart a timer, so it can track how long it has been since the message was sent. O n the re- ceiver, the library should acknowledge received messages. The client send should block when sending, i.e., it should wait until the message has been acknowledged before returning.",3193
48. Distributed Systems,"It should also be willing to re try sending indeﬁnitely. The maximum message size should be that of the largest single message you can send with UDP . Finally, be sure to perform timeout/ retry efﬁciently by putting the caller to sleep until either an ack ar rives or the transmission times out; do notspin and waste the CPU. 4. Make your library more efﬁcient and feature-ﬁlled. First, ad d very-large message transfer. Speciﬁcally, although the network limit maximum mes- sage size, your library should take a message of arbitrarily lar ge size and transfer it from client to server. The client should transmit t hese large mes- sages in pieces to the server; the server-side library code s hould assemble re- ceived fragments into the contiguous whole, and pass the single large buffer to the waiting server code. 5. Do the above again, but with high performance. Instead of sen ding each fragment one at a time, you should rapidly send many pieces, thus al lowing the network to be much more highly utilized. To do so, carefully mark each piece of the transfer so that the re-assembly on the receiver s ide does not scramble the message. 6. A ﬁnal implementation challenge: asynchronous message send w ith in- order delivery. That is, the client should be able to repeated ly call send to send one message after the other; the receiver should call re ceive and get each message in order, reliably; many messages from the sender s hould be able to be in ﬂight concurrently. Also add a sender-side call th at enables a client to wait for all outstanding messages to be acknowledged . 7. Now, one more pain point: measurement. Measure the bandwidth of each of your approaches; how much data can you transfer between two di fferent machines, at what rate? Also measure latency: for single packet s end and acknowledgment, how quickly does it ﬁnish? Finally, do your numbe rs look reasonable? What did you expect? How can you better set your expe ctations so as to know if there is a problem, or that your code is working we ll? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2076
49. Network File System NFS,"49 Sun’s Network File System (NFS) One of the ﬁrst uses of distributed client/server computing was in the realm of distributed ﬁle systems. In such an environment, ther e are a number of client machines and one server (or a few); the server st ores the data on its disks, and clients request data through well-formed protocol messages. Figure 49.1 depicts the basic setup. Client 0 Client 1 Client 2 Client 3Server RAIDNetwork Figure 49.1: A Generic Client/Server System As you can see from the picture, the server has the disks, and cli ents send messages across a network to access their directories and ﬁ les on those disks. Why do we bother with this arrangement? (i.e., why don’t we just let clients use their local disks?) Well, primarily th is setup allows for easy sharing of data across clients. Thus, if you access a ﬁle on one machine (Client 0) and then later use another (Client 2), you wi ll have the same view of the ﬁle system. Your data is naturally shared across these different machines. A secondary beneﬁt is centralized administration ; for example, backing up ﬁles can be done from the few server machi nes instead of from the multitude of clients. Another advantage could be security ; having all servers in a locked machine room prevents certain types of problems from arising. 1 2 SUN’SNETWORK FILESYSTEM (NFS) CRUX: HOWTOBUILD A D ISTRIBUTED FILESYSTEM How do you build a distributed ﬁle system? What are the key aspec ts to think about? What is easy to get wrong? What can we learn from existing systems? 49.1 A Basic Distributed File System We now will study the architecture of a simpliﬁed distributed ﬁ le sys- tem. A simple client/server distributed ﬁle system has more c omponents than the ﬁle systems we have studied so far. On the client side , there are client applications which access ﬁles and directories through theclient- side ﬁle system . A client application issues system calls to the client-side ﬁle system (such as open() ,read() ,write() ,close() ,mkdir() , etc.) in order to access ﬁles which are stored on the server. Thus , to client applications, the ﬁle system does not appear to be any different than a lo- cal (disk-based) ﬁle system, except perhaps for performance; in this way, distributed ﬁle systems provide transparent access to ﬁles, an obvious goal; after all, who would want to use a ﬁle system that required a differ- ent set of APIs or otherwise was a pain to use? The role of the client-side ﬁle system is to execute the actions n eeded to service those system calls. For example, if the client issue s aread() request, the client-side ﬁle system may send a message to the server-side ﬁle system (or, as it is commonly called, the ﬁle server ) to read a partic- ular block; the ﬁle server will then read the block from disk (or it s own in-memory cache), and send a message back to the client with th e re- quested data. The client-side ﬁle system will then copy the da ta into the user buffer supplied to the read() system call and thus the request will complete. Note that a subsequent read() of the same block on the client may be cached in client memory or on the client’s disk even; in the best such case, no network trafﬁc need be generated.",3211
49. Network File System NFS,"Client Application Client-side File System Networking LayerFile Server Networking LayerDisks Figure 49.2: Distributed File System Architecture From this simple overview, you should get a sense that there are tw o important pieces of software in a client/server distributed ﬁl e system: the client-side ﬁle system and the ﬁle server. Together their beh avior deter- mines the behavior of the distributed ﬁle system. Now it’s time to study one particular system: Sun’s Network File System (NFS). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 3 ASIDE : W HYSERVERS CRASH Before getting into the details of the NFSv2 protocol, you might be wondering: why do servers crash? Well, as you might guess, ther e are plenty of reasons. Servers may simply suffer from a power outage (tem- porarily); only when power is restored can the machines be restar ted. Servers are often comprised of hundreds of thousands or even millions of lines of code; thus, they have bugs (even good software has a few bugs per hundred or thousand lines of code), and thus they eventua lly will trigger a bug that will cause them to crash. They also hav e memory leaks; even a small memory leak will cause a system to run out of me m- ory and crash. And, ﬁnally, in distributed systems, there is a network between the client and the server; if the network acts strange ly (for ex- ample, if it becomes partitioned and clients and servers are working but cannot communicate), it may appear as if a remote machine has cra shed, but in reality it is just not currently reachable through the ne twork. 49.2 On To NFS One of the earliest and quite successful distributed systems was devel- oped by Sun Microsystems, and is known as the Sun Network File Sys- tem (or NFS) [S86]. In deﬁning NFS, Sun took an unusual approach: in- stead of building a proprietary and closed system, Sun instead de veloped anopen protocol which simply speciﬁed the exact message formats that clients and servers would use to communicate. Different groups could develop their own NFS servers and thus compete in an NFS marketpl ace while preserving interoperability. It worked: today there are many com- panies that sell NFS servers (including Oracle/Sun, NetApp [ HLM94], EMC, IBM, and others), and the widespread success of NFS is like ly at- tributed to this “open market” approach. 49.3 Focus: Simple And Fast Server Crash Recovery In this chapter, we will discuss the classic NFS protocol (versi on 2, a.k.a. NFSv2), which was the standard for many years; small cha nges were made in moving to NFSv3, and larger-scale protocol changes we re made in moving to NFSv4. However, NFSv2 is both wonderful and frus- trating and thus serves as our focus. In NFSv2, the main goal in the design of the protocol was simple and fast server crash recovery . In a multiple-client, single-server environment, this goal makes a great deal of sense; any minute that the server is down (or unavailable) makes allthe client machines (and their users) unhappy and unproductive.",3036
49. Network File System NFS,"Thus, as the server goes, so goes the entire sy stem. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 SUN’SNETWORK FILESYSTEM (NFS) 49.4 Key To Fast Crash Recovery: Statelessness This simple goal is realized in NFSv2 by designing what we refer to as a stateless protocol. The server, by design, does not keep track of any- thing about what is happening at each client. For example, the s erver does not know which clients are caching which blocks, or which ﬁles are currently open at each client, or the current ﬁle pointer position for a ﬁle, etc. Simply put, the server does not track anything about what cli ents are doing; rather, the protocol is designed to deliver in each protocol r equest all the information that is needed in order to complete the request. If it doesn’t now, this stateless approach will make more sense as we dis cuss the protocol in more detail below. For an example of a stateful (not stateless) protocol, consider the open() system call. Given a pathname, open() returns a ﬁle descriptor (an inte- ger). This descriptor is used on subsequent read() orwrite() requests to access various ﬁle blocks, as in this application code (note tha t proper error checking of the system calls is omitted for space reasons): char buffer[MAX]; int fd = open(\""foo\"", O_RDONLY); // get descriptor \""fd\"" read(fd, buffer, MAX); // read MAX bytes from foo (via fd) read(fd, buffer, MAX); // read MAX bytes from foo ... read(fd, buffer, MAX); // read MAX bytes from foo close(fd); // close file Figure 49.3: Client Code: Reading From A File Now imagine that the client-side ﬁle system opens the ﬁle by sen ding a protocol message to the server saying “open the ﬁle ’foo’ and give me back a descriptor”. The ﬁle server then opens the ﬁle locally on it s side and sends the descriptor back to the client. On subsequent rea ds, the client application uses that descriptor to call the read() system call; the client-side ﬁle system then passes the descriptor in a messag e to the ﬁle server, saying “read some bytes from the ﬁle that is referred to by the descriptor I am passing you here”. In this example, the ﬁle descriptor is a piece of shared state between the client and the server (Ousterhout calls this distributed state [O91]). Shared state, as we hinted above, complicates crash recovery. Im agine the server crashes after the ﬁrst read completes, but before th e client has issued the second one. After the server is up and running aga in, the client then issues the second read. Unfortunately, the ser ver has no idea to which ﬁle fdis referring; that information was ephemeral (i.e., in memory) and thus lost when the server crashed. To handle this situa- tion, the client and server would have to engage in some kind of recovery protocol , where the client would make sure to keep enough information around in its memory to be able to tell the server what it needs to know (in this case, that ﬁle descriptor fdrefers to ﬁle foo). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 5 It gets even worse when you consider the fact that a stateful serv er has to deal with client crashes. Imagine, for example, a client th at opens a ﬁle and then crashes. The open() uses up a ﬁle descriptor on the server; how can the server know it is OK to close a given ﬁle? In normal operation , a client would eventually call close() and thus inform the server that the ﬁle should be closed. However, when a client crashes, the server never receives aclose() , and thus has to notice the client has crashed in order to close the ﬁle.",3579
49. Network File System NFS,"For these reasons, the designers of NFS decided to pursue a state less approach: each client operation contains all the information need ed to complete the request. No fancy crash recovery is needed; the se rver just starts running again, and a client, at worst, might have to ret ry a request. 49.5 The NFSv2 Protocol We thus arrive at the NFSv2 protocol deﬁnition. Our problem state- ment is simple: THECRUX: HOWTODEFINE A S TATELESS FILEPROTOCOL How can we deﬁne the network protocol to enable stateless operation? Clearly, stateful calls like open() can’t be a part of the discussion (as it would require the server to track open ﬁles); however, the clien t appli- cation will want to call open() ,read() ,write() ,close() and other standard API calls to access ﬁles and directories. Thus, as a r eﬁned ques- tion, how do we deﬁne the protocol to both be stateless andsupport the POSIX ﬁle system API? One key to understanding the design of the NFS protocol is under- standing the ﬁle handle . File handles are used to uniquely describe the ﬁle or directory a particular operation is going to operate upon; thu s, many of the protocol requests include a ﬁle handle. You can think of a ﬁle handle as having three important components: a volume identiﬁer , aninode number , and a generation number ; together, these three items comprise a unique identiﬁer for a ﬁle or directory tha t a client wishes to access. The volume identiﬁer informs the server whic h ﬁle sys- tem the request refers to (an NFS server can export more than one ﬁ le system); the inode number tells the server which ﬁle within th at partition the request is accessing. Finally, the generation number is n eeded when reusing an inode number; by incrementing it whenever an inode n um- ber is reused, the server ensures that a client with an old ﬁle h andle can’t accidentally access the newly-allocated ﬁle. Here is a summary of some of the important pieces of the protocol; the full protocol is available elsewhere (see Callaghan’s book for an ex cellent and detailed overview of NFS [C00]). c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 SUN’SNETWORK FILESYSTEM (NFS) NFSPROC_GETATTR expects: file handle returns: attributes NFSPROC_SETATTR expects: file handle, attributes returns: nothing NFSPROC_LOOKUP expects: directory file handle, name of file/directory to l ook up returns: file handle NFSPROC_READ expects: file handle, offset, count returns: data, attributes NFSPROC_WRITE expects: file handle, offset, count, data returns: attributes NFSPROC_CREATE expects: directory file handle, name of file, attributes returns: nothing NFSPROC_REMOVE expects: directory file handle, name of file to be removed returns: nothing NFSPROC_MKDIR expects: directory file handle, name of directory, attribu tes returns: file handle NFSPROC_RMDIR expects: directory file handle, name of directory to be remo ved returns: nothing NFSPROC_READDIR expects: directory handle, count of bytes to read, cookie returns: directory entries, cookie (to get more entries) Figure 49.4: The NFS Protocol: Examples We brieﬂy highlight the important components of the protocol.",3138
49. Network File System NFS,"First , the LOOKUP protocol message is used to obtain a ﬁle handle, which i s then subsequently used to access ﬁle data. The client passes a directory ﬁle handle and name of a ﬁle to look up, and the handle to that ﬁle (or directory) plus its attributes are passed back to the client f rom the server. For example, assume the client already has a directory ﬁle hand le for the root directory of a ﬁle system ( /) (indeed, this would be obtained through the NFS mount protocol , which is how clients and servers ﬁrst are connected together; we do not discuss the mount protocol here for sake of brevity). If an application running on the client opens th e ﬁle /foo.txt , the client-side ﬁle system sends a lookup request to the serve r, passing it the root ﬁle handle and the name foo.txt ; if successful, the ﬁle handle (and attributes) for foo.txt will be returned. In case you are wondering, attributes are just the metadata tha t the ﬁle system tracks about each ﬁle, including ﬁelds such as ﬁle crea tion time, last modiﬁcation time, size, ownership and permissions informat ion, and so forth, i.e., the same type of information that you would get back i f you calledstat() on a ﬁle. Once a ﬁle handle is available, the client can issue READ and W RITE protocol messages on a ﬁle to read or write the ﬁle, respectively. T he READ protocol message requires the protocol to pass along the ﬁle han dle OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 7 of the ﬁle along with the offset within the ﬁle and number of bytes t o read. The server then will be able to issue the read (after all, the h andle tells the server which volume and which inode to read from, and the offset an d count tells it which bytes of the ﬁle to read) and return the data to the client (or an error if there was a failure). WRITE is handled sim ilarly, except the data is passed from the client to the server, and jus t a success code is returned. One last interesting protocol message is the GETATTR request; g iven a ﬁle handle, it simply fetches the attributes for that ﬁle, inc luding the last modiﬁed time of the ﬁle. We will see why this protocol request is imp or- tant in NFSv2 below when we discuss caching (can you guess why?). 49.6 From Protocol To Distributed File System Hopefully you are now getting some sense of how this protocol is turned into a ﬁle system across the client-side ﬁle system and the ﬁle server. The client-side ﬁle system tracks open ﬁles, and gene rally trans- lates application requests into the relevant set of protocol mess ages. The server simply responds to protocol messages, each of which contain s all information needed to complete request. For example, let us consider a simple application which reads a ﬁ le. In the diagram (Figure 49.5), we show what system calls the app lication makes, and what the client-side ﬁle system and ﬁle server do i n respond- ing to such calls. A few comments about the ﬁgure. First, notice how the client track s all relevant state for the ﬁle access, including the mapping of the integer ﬁle descriptor to an NFS ﬁle handle as well as the current ﬁle pointe r.",3145
49. Network File System NFS,"This enables the client to turn each read request (which you may hav e noticed donotspecify the offset to read from explicitly) into a properly-form atted read protocol message which tells the server exactly which byte s from the ﬁle to read. Upon a successful read, the client updates the current ﬁle position; subsequent reads are issued with the same ﬁle han dle but a different offset. Second, you may notice where server interactions occur. When the ﬁl e is opened for the ﬁrst time, the client-side ﬁle system sends a L OOKUP request message. Indeed, if a long pathname must be traversed (e.g., /home/remzi/foo.txt ), the client would send three LOOKUPs: one to look up home in the directory /, one to look up remzi inhome , and ﬁnally one to look up foo.txt inremzi . Third, you may notice how each server request has all the informat ion needed to complete the request in its entirety. This design poi nt is critical to be able to gracefully recover from server failure, as we will now discuss in more detail; it ensures that the server does not need state to b e able to respond to the request. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 SUN’SNETWORK FILESYSTEM (NFS) Client Server fd = open(”/foo”, ...); Send LOOKUP (rootdir FH, ”foo”) Receive LOOKUP request look for ”foo” in root dir return foo’s FH + attributes Receive LOOKUP reply allocate ﬁle desc in open ﬁle table store foo’s FH in table store current ﬁle position (0) return ﬁle descriptor to application read(fd, buffer, MAX); Index into open ﬁle table with fd get NFS ﬁle handle (FH) use current ﬁle position as offset Send READ (FH, offset=0, count=MAX) Receive READ request use FH to get volume/inode num read inode from disk (or cache) compute block location (using offset) read data from disk (or cache) return data to client Receive READ reply update ﬁle position (+bytes read) set current ﬁle position = MAX return data/error code to app read(fd, buffer, MAX); Same except offset=MAX and set current ﬁle position = 2*MAX read(fd, buffer, MAX); Same except offset=2*MAX and set current ﬁle position = 3*MAX close(fd); Just need to clean up local structures Free descriptor ”fd” in open ﬁle table (No need to talk to server) Figure 49.5: Reading A File: Client-side And File Server Actions OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 9 TIP: IDEMPOTENCY ISPOWERFUL Idempotency is a useful property when building reliable systems. When an operation can be issued more than once, it is much easier to hand le failure of the operation; you can just retry it. If an operation is notidem- potent, life becomes more difﬁcult. 49.7 Handling Server Failure With Idempotent Operations When a client sends a message to the server, it sometimes does not re- ceive a reply. There are many possible reasons for this failure t o respond. In some cases, the message may be dropped by the network; networks do lose messages, and thus either the request or the reply could be l ost and thus the client would never receive a response. It is also possible that the server has crashed, and thus is not c urrently responding to messages. After a bit, the server will be rebooted and start running again, but in the meanwhile all requests have been los t.",3259
49. Network File System NFS,"In all of these cases, clients are left with a question: what should they do when the server does not reply in a timely manner? In NFSv2, a client handles all of these failures in a single, uni form, and elegant way: it simply retries the request. Speciﬁcally, after sending the request, the client sets a timer to go off after a speciﬁed time period. If a reply is received before the timer goes off, the timer is cancele d and all is well. If, however, the timer goes off before any reply is received, the client assumes the request has not been processed and resends it. If th e server replies, all is well and the client has neatly handled the prob lem. The ability of the client to simply retry the request (regardl ess of what caused the failure) is due to an important property of most NFS req uests: they are idempotent . An operation is called idempotent when the effect of performing the operation multiple times is equivalent to the e ffect of performing the operation a single time. For example, if you store a v alue to a memory location three times, it is the same as doing so once; thu s “store value to memory” is an idempotent operation. If, however, you in- crement a counter three times, it results in a different amount than doing so just once; thus, “increment counter” is not idempotent. More ge ner- ally, any operation that just reads data is obviously idempotent; an oper- ation that updates data must be more carefully considered to det ermine if it has this property. The heart of the design of crash recovery in NFS is the idempotency of most common operations. LOOKUP and READ requests are trivially idempotent, as they only read information from the ﬁle server and d o not update it. More interestingly, WRITE requests are also idemp otent. If, for example, a WRITE fails, the client can simply retry it. The WRITE message contains the data, the count, and (importantly) the exa ct offset to write the data to. Thus, it can be repeated with the knowledge that the outcome of multiple writes is the same as the outcome of a single one. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 SUN’SNETWORK FILESYSTEM (NFS) Case 1: Request Lost Client [send request]Server (no mesg) Case 2: Server Down Client [send request]Server (down) Case 3: Reply lost on way back from Server Client [send request]Server [recv request] [handle request] [send reply] Figure 49.6: The Three Types Of Loss In this way, the client can handle all timeouts in a uniﬁed way. If a WRITE request was simply lost (Case 1 above), the client will re try it, the server will perform the write, and all will be well. The same wi ll happen if the server happened to be down while the request was sent, bu t back up and running when the second request is sent, and again all wor ks as desired (Case 2). Finally, the server may in fact receive t he WRITE request, issue the write to its disk, and send a reply. This re ply may get lost (Case 3), again causing the client to re-send the request . When the server receives the request again, it will simply do the exac t same thing: write the data to disk and reply that it has done so. If the client this time receives the reply, all is again well, and thus the client has handled both message loss and server failure in a uniform manner.",3289
49. Network File System NFS,"Neat. A small aside: some operations are hard to make idempotent. For example, when you try to make a directory that already exists, y ou are informed that the mkdir request has failed. Thus, in NFS, if the ﬁle server receives a MKDIR protocol message and executes it successfully but the reply is lost, the client may repeat it and encounter that failu re when in fact the operation at ﬁrst succeeded and then only failed on the re try. Thus, life is not perfect. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 11 TIP: PERFECT ISTHEENEMY OFTHEGOOD (VOLTAIRE ’SLAW) Even when you design a beautiful system, sometimes all the corne r cases don’t work out exactly as you might like. Take the mkdir example abov e; one could redesign mkdir to have different semantics, thus mak ing it idempotent (think about how you might do so); however, why bother? The NFS design philosophy covers most of the important cases, and ove r- all makes the system design clean and simple with regards to f ailure. Thus, accepting that life isn’t perfect and still building th e system is a sign of good engineering. Apparently, this wisdom is attributed to Vol taire, for saying “... a wise Italian says that the best is the enemy of t he good” [V72], and thus we call it Voltaire’s Law . 49.8 Improving Performance: Client-side Caching Distributed ﬁle systems are good for a number of reasons, but sendi ng all read and write requests across the network can lead to a big p erfor- mance problem: the network generally isn’t that fast, especial ly as com- pared to local memory or disk. Thus, another problem: how can we im- prove the performance of a distributed ﬁle system? The answer, as you might guess from reading the big bold words in the sub-heading above, is client-side caching . The NFS client-side ﬁle system caches ﬁle data (and metadata) that it has read from the server in client memory. Thus, while the ﬁrst access is expensive (i.e. , it requires network communication), subsequent accesses are serviced qui te quickly out of client memory. The cache also serves as a temporary buffer for writes. When a cl ient application ﬁrst writes to a ﬁle, the client buffers the data i n client mem- ory (in the same cache as the data it read from the ﬁle server) bef ore writ- ing the data out to the server. Such write buffering is useful because it de- couples application write() latency from actual write performance, i.e., the application’s call to write() succeeds immediately (and just puts the data in the client-side ﬁle system’s cache); only later does the data get written out to the ﬁle server. Thus, NFS clients cache data and performance is usually great and we are done, right? Unfortunately, not quite. Adding caching in to any sort of system with multiple client caches introduces a big and i nteresting challenge which we will refer to as the cache consistency problem . 49.9 The Cache Consistency Problem The cache consistency problem is best illustrated with two cli ents and a single server. Imagine client C1 reads a ﬁle F, and keeps a cop y of the ﬁle in its local cache. Now imagine a different client, C2, overw rites the ﬁle F, thus changing its contents; let’s call the new version of th e ﬁle F c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 SUN’SNETWORK FILESYSTEM (NFS) C1 cache: F[v1]C2 cache: F[v2]C3 cache: empty Server S disk: F[v1] at first       F[v2] eventually Figure 49.7: The Cache Consistency Problem (version 2), or F[v2] and the old version F[v1] so we can keep the tw o distinct (but of course the ﬁle has the same name, just differen t contents).",3623
49. Network File System NFS,"Finally, there is a third client, C3, which has not yet accesse d the ﬁle F. You can probably see the problem that is upcoming (Figure 49.7). I n fact, there are two subproblems. The ﬁrst subproblem is that th e client C2 may buffer its writes in its cache for a time before propagating t hem to the server; in this case, while F[v2] sits in C2’s memory, any acces s of F from another client (say C3) will fetch the old version of the ﬁle (F[v1 ]). Thus, by buffering writes at the client, other clients may get stale versions of the ﬁle, which may be undesirable; indeed, imagine the case wher e you log into machine C2, update F, and then log into C3 and try to read th e ﬁle, only to get the old copy. Certainly this could be frustrating. Thu s, let us call this aspect of the cache consistency problem update visibility ; when do updates from one client become visible at other clients? The second subproblem of cache consistency is a stale cache ; in this case, C2 has ﬁnally ﬂushed its writes to the ﬁle server, and th us the server has the latest version (F[v2]). However, C1 still has F[v1] in i ts cache; if a program running on C1 reads ﬁle F, it will get a stale version (F[v 1]) and not the most recent copy (F[v2]), which is (often) undesirable. NFSv2 implementations solve these cache consistency problems in two ways. First, to address update visibility, clients impleme nt what is some- times called ﬂush-on-close (a.k.a., close-to-open ) consistency semantics; speciﬁcally, when a ﬁle is written to and subsequently closed by a client application, the client ﬂushes all updates (i.e., dirty page s in the cache) to the server. With ﬂush-on-close consistency, NFS ensures tha t a subse- quent open from another node will see the latest ﬁle version. Second, to address the stale-cache problem, NFSv2 clients ﬁrst c heck to see whether a ﬁle has changed before using its cached content s. Specif- ically, before using a cached block, the client-side ﬁle syste m will issue a GETATTR request to the server to fetch the ﬁle’s attributes. T he attributes, importantly, include information as to when the ﬁle was last modi ﬁed on the server; if the time-of-modiﬁcation is more recent than the ti me that the ﬁle was fetched into the client cache, the client invalidates the ﬁle, thus removing it from the client cache and ensuring that subsequent reads will go to the server and retrieve the latest version of the ﬁle. If, on the other OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 13 hand, the client sees that it has the latest version of the ﬁle, i t will go ahead and use the cached contents, thus increasing performanc e. When the original team at Sun implemented this solution to the sta le- cache problem, they realized a new problem; suddenly, the NFS s erver was ﬂooded with GETATTR requests. A good engineering principle t o follow is to design for the common case , and to make it work well; here, although the common case was that a ﬁle was accessed only from a sin- gle client (perhaps repeatedly), the client always had to se nd GETATTR requests to the server to make sure no one else had changed the ﬁ le. A client thus bombards the server, constantly asking “has anyone changed this ﬁle?”, when most of the time no one had. To remedy this situation (somewhat), an attribute cache was added to each client. A client would still validate a ﬁle before acces sing it, but most often would just look in the attribute cache to fetch the attri butes. The attributes for a particular ﬁle were placed in the cache wh en the ﬁle was ﬁrst accessed, and then would timeout after a certain amount of time (say 3 seconds). Thus, during those three seconds, all ﬁle acces ses would determine that it was OK to use the cached ﬁle and thus do so wit h no network communication with the server.",3835
49. Network File System NFS,"49.10 Assessing NFS Cache Consistency A few ﬁnal words about NFS cache consistency. The ﬂush-on-close be - havior was added to “make sense”, but introduced a certain perf ormance problem. Speciﬁcally, if a temporary or short-lived ﬁle was creat ed on a client and then soon deleted, it would still be forced to the serve r. A more ideal implementation might keep such short-lived ﬁles in memor y until they are deleted and thus remove the server interaction entire ly, perhaps increasing performance. More importantly, the addition of an attribute cache into NFS mad e it very hard to understand or reason about exactly what version of a ﬁle one was getting. Sometimes you would get the latest version; sometim es you would get an old version simply because your attribute cache ha dn’t yet timed out and thus the client was happy to give you what was in client memory. Although this was ﬁne most of the time, it would (and still does.) occasionally lead to odd behavior. And thus we have described the oddity that is NFS client cachin g. It serves as an interesting example where details of an implem entation serve to deﬁne user-observable semantics, instead of the other way around. 49.11 Implications On Server-Side Write Buffering Our focus so far has been on client caching, and that is where most of the interesting issues arise. However, NFS servers tend to b e well- equipped machines with a lot of memory too, and thus they have cachi ng concerns as well. When data (and metadata) is read from disk, NF S c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 SUN’SNETWORK FILESYSTEM (NFS) servers will keep it in memory, and subsequent reads of said dat a (and metadata) will not go to disk, a potential (small) boost in perform ance. More intriguing is the case of write buffering. NFS servers abs olutely may notreturn success on a WRITE protocol request until the write has been forced to stable storage (e.g., to disk or some other persiste nt device). While they can place a copy of the data in server memory, returnin g suc- cess to the client on a WRITE protocol request could result in incorr ect behavior; can you ﬁgure out why? The answer lies in our assumptions about how clients handle serve r failure. Imagine the following sequence of writes as issued by a client: write(fd, a_buffer, size); // fill first block with a’s write(fd, b_buffer, size); // fill second block with b’s write(fd, c_buffer, size); // fill third block with c’s These writes overwrite the three blocks of a ﬁle with a block of a’s, then b’s, and then c’s. Thus, if the ﬁle initially looked like this : xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxx yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyy zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz zzzzzzzzz We might expect the ﬁnal result after these writes to be like t his, with the x’s, y’s, and z’s, would be overwritten with a’s, b’s, and c’s, respect ively. aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb bbbbbbbbb ccccccccccccccccccccccccccccccccccccccccccccccccccc ccccccccc Now let’s assume for the sake of the example that these three clien t writes were issued to the server as three distinct WRITE protoc ol mes- sages.",3281
49. Network File System NFS,"Assume the ﬁrst WRITE message is received by the serve r and issued to the disk, and the client informed of its success. Now as sume the second write is just buffered in memory, and the server also reports it success to the client before forcing it to disk; unfortunately, the server crashes before writing it to disk. The server quickly restart s and receives the third write request, which also succeeds. Thus, to the client, all the requests succeeded, but we are su rprised that the ﬁle contents look like this: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaa yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy yyyyyyyyy <--- oops ccccccccccccccccccccccccccccccccccccccccccccccccccc ccccccccc Yikes. Because the server told the client that the second write was successful before committing it to disk, an old chunk is left in t he ﬁle, which, depending on the application, might be catastrophic. To avoid this problem, NFS servers must commit each write to stable (persistent) storage before informing the client of success; doi ng so en- ables the client to detect server failure during a write, and thus retry until OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 15 ASIDE : INNOVATION BREEDS INNOVATION As with many pioneering technologies, bringing NFS into the worl d also required other fundamental innovations to enable its success. Probably the most lasting is the Virtual File System (VFS ) /Virtual Node (vnode ) interface, introduced by Sun to allow different ﬁle systems to b e readily plugged into the operating system [K86]. The VFS layer includes operations that are done to an entire ﬁle s ystem, such as mounting and unmounting, getting ﬁle-system wide stat istics, and forcing all dirty (not yet written) writes to disk. The vnode layer consists of all operations one can perform on a ﬁle, such as open, close, reads, writes, and so forth. To build a new ﬁle system, one simply has to deﬁne these “methods ”; the framework then handles the rest, connecting system calls to th e particular ﬁle system implementation, performing generic functions common to all ﬁle systems (e.g., caching) in a centralized manner, and thu s providing a way for multiple ﬁle system implementations to operate simulta neously within the same system. Although some of the details have changed, many modern systems ha ve some form of a VFS/vnode layer, including Linux, BSD variants, macO S, and even Windows (in the form of the Installable File System). Eve n if NFS becomes less relevant to the world, some of the necessary found a- tions beneath it will live on. it ﬁnally succeeds. Doing so ensures we will never end up with ﬁ le con- tents intermingled as in the above example. The problem that this requirement gives rise to in NFS server i m- plementation is that write performance, without great care, ca n be the major performance bottleneck. Indeed, some companies (e.g., Net work Appliance) came into existence with the simple objective of bu ilding an NFS server that can perform writes quickly; one trick they use i s to ﬁrst put writes in a battery-backed memory, thus enabling to quick ly reply to WRITE requests without fear of losing the data and without the c ost of having to write to disk right away; the second trick is to use a ﬁle sys- tem design speciﬁcally designed to write to disk quickly whe n one ﬁnally needs to do so [HLM94, RO91]. 49.12 Summary We have seen the introduction of the NFS distributed ﬁle system. NFS is centered around the idea of simple and fast recovery in the fac e of server failure, and achieves this end through careful protocol d esign.",3634
49. Network File System NFS,"Idem- potency of operations is essential; because a client can safely r eplay a failed operation, it is OK to do so whether or not the server has exe cuted the request. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 16 SUN’SNETWORK FILESYSTEM (NFS) ASIDE : KEYNFS T ERMS •The key to realizing the main goal of fast and simple crash recove ry in NFS is in the design of a stateless protocol. After a crash, the server can quickly restart and begin serving requests again ; clients justretry requests until they succeed. •Making requests idempotent is a central aspect of the NFS protocol. An operation is idempotent when the effect of performing it multi- ple times is equivalent to performing it once. In NFS, idempotenc y enables client retry without worry, and uniﬁes client lost-mes sage retransmission and how the client handles server crashes. •Performance concerns dictate the need for client-side caching and write buffering , but introduces a cache consistency problem . •NFS implementations provide an engineering solution to cache consistency through multiple means: a ﬂush-on-close (close-to- open ) approach ensures that when a ﬁle is closed, its contents are forced to the server, enabling other clients to observe the upda tes to it. An attribute cache reduces the frequency of checking wi th the server whether a ﬁle has changed (via GETATTR requests). •NFS servers must commit writes to persistent media before retu rn- ing success; otherwise, data loss can arise. •To support NFS integration into the operating system, Sun intro- duced the VFS/Vnode interface, enabling multiple ﬁle system im- plementations to coexist in the same operating system. We also have seen how the introduction of caching into a multiple- client, single-server system can complicate things. In part icular, the sys- tem must resolve the cache consistency problem in order to behave rea- sonably; however, NFS does so in a slightly ad hoc fashion which can occasionally result in observably weird behavior. Finally, we s aw how server caching can be tricky: writes to the server must be forc ed to stable storage before returning success (otherwise data can be lost). We haven’t talked about other issues which are certainly releva nt, no- tably security. Security in early NFS implementations was rem arkably lax; it was rather easy for any user on a client to masquerade as ot her users and thus gain access to virtually any ﬁle. Subsequent in tegration with more serious authentication services (e.g., Kerberos [NT9 4]) have addressed these obvious deﬁciencies. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG SUN’SNETWORK FILESYSTEM (NFS) 17 References [AKW88] “The AWK Programming Language” by Alfred V . Aho, Brian W. Kerni ghan, Peter J. Weinberger. Pearson, 1988 (1st edition). A concise, wonderful book about awk. We once had the pleasure of meeting Peter Weinberger; when he introduced himself, he said “I’m Peter Weinberger, you know, the ’W’ in awk?” As huge awk fans, this was a moment to savor. One of us (Remzi ) then said, “I love awk.",3055
49. Network File System NFS,"I particularly love the book, which makes everything so wonderful ly clear.” Weinberger replied (crestfallen), “Oh, Kernighan wrote the book.” [C00] “NFS Illustrated” by Brent Callaghan. Addison-Wesley Profess ional Computing Series, 2000. A great NFS reference; incredibly thorough and detailed per the protocol i tself. [ES03] “New NFS Tracing Tools and Techniques for System Analysis” by Da niel Ellard and Margo Seltzer. LISA ’03, San Diego, California. An intricate, careful analysis of NFS done via passive tracing. By simply monitoring network trafﬁc, the authors show how to de rive a vast amount of ﬁle system understanding. [HLM94] “File System Design for an NFS File Server Appliance” by Da ve Hitz, James Lau, Michael Malcolm. USENIX Winter 1994. San Francisco, California, 1994. Hitz et al. were greatly inﬂuenced by previous work on log-structured ﬁle systems. [K86] “Vnodes: An Architecture for Multiple File System Types in Sun UNIX” by Steve R. Kleiman. USENIX Summer ’86, Atlanta, Georgia. This paper shows how to build a ﬂexible ﬁle system architecture into an operating system, enabling multiple differe nt ﬁle system implementations to coexist. Now used in virtually every modern operating system in some form . [NT94] “Kerberos: An Authentication Service for Computer Networks ” by B. Clifford Neu- man, Theodore Ts’o. IEEE Communications, 32(9):33-38, September 199 4.Kerberos is an early and hugely inﬂuential authentication service. We probably should write a b ook chapter about it some- time... [O91] “The Role of Distributed State” by John K. Ousterhout. 1991. Ava ilable at this site: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.p s.A rarely referenced dis- cussion of distributed state; a broader perspective on the problems and chall enges. [P+94] “NFS Version 3: Design and Implementation” by Brian Pawlows ki, Chet Juszczak, Peter Staubach, Carl Smith, Diane Lebel, Dave Hitz. USENIX Summer 1994 , pages 137-152. The small modiﬁcations that underlie NFS version 3. [P+00] “The NFS version 4 protocol” by Brian Pawlowski, David Noveck , David Robinson, Robert Thurlow. 2nd International System Administration and Networ king Conference (SANE 2000). Undoubtedly the most literary paper on NFS ever written. [RO91] “The Design and Implementation of the Log-structured File Syst em” by Mendel Rosen- blum, John Ousterhout. Symposium on Operating Systems Principles (SOSP), 1991. LFS again. No, you can never get enough LFS. [S86] “The Sun Network File System: Design, Implementation and Exp erience” by Russel Sandberg. USENIX Summer 1986. The original NFS paper; though a bit of a challenging read, it is worthwhile to see the source of these wonderful ideas. [Sun89] “NFS: Network File System Protocol Speciﬁcation” by Sun Micro systems, Inc. Request for Comments: 1094, March 1989. Available: http://www.ietf.org/rfc/rfc1094.txt . The dreaded speciﬁcation; read it if you must, i.e., you are getting paid to r ead it. Hopefully, paid a lot. Cash money. [V72] “La Begueule” by Francois-Marie Arouet a.k.a. Voltaire.",3052
49. Network File System NFS,"Pub lished in 1772. Voltaire said a number of clever things, this being but one example. For example, Vol taire also said “If you have two religions in your land, the two will cut each others throats; but if you have thi rty religions, they will dwell in peace.” What do you say to that, Democrats and Republicans? c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 18 SUN’SNETWORK FILESYSTEM (NFS) Homework (Measurement) In this homework, you’ll do a little bit of NFS trace analysis using real traces. The source of these traces is Ellard and Seltzer’s effort [ ES03]. Make sure to read the related README and download the relevant t ar- ball from the OSTEP homework page (as usual) before starting. Questions 1. A ﬁrst question for your trace analysis: using the timestamps fo und in the ﬁrst column, determine the period of time the traces were taken fr om. How long is the period? What day/week/month/year was it? (does th is match the hint given in the ﬁle name?) Hint: Use the tools head -1 andtail -1 to extract the ﬁrst and last lines of the ﬁle, and do the calculat ion. 2. Now, let’s do some operation counts. How many of each type of op era- tion occur in the trace? Sort these by frequency; which operati on is most frequent? Does NFS live up to its reputation? 3. Now let’s look at some particular operations in more detail. F or example, the GETATTR request returns a lot of information about ﬁles, incl uding which user ID the request is being performed for, the size of the ﬁ le, and so forth. Make a distribution of ﬁle sizes accessed within the trace; what is the average ﬁle size? Also, how many different users access ﬁl es in the trace? Do a few users dominate trafﬁc, or is it more spread out? Wha t other interesting information is found within GETATTR replies? 4. You can also look at requests to a given ﬁle and determine how ﬁ les are be- ing accessed. For example, is a given ﬁle being read or written s equentially? Or randomly? Look at the details of READ and WRITE requests/repl ies to compute the answer. 5. Trafﬁc comes from many machines and goes to one server (in this trace). Compute a trafﬁc matrix, which shows how many different clients th ere are in the trace, and how many requests/replies go to each. Do a few ma chines dominate, or is it more evenly balanced? 6. The timing information, and the per-request/reply unique ID, s hould allow you to compute the latency for a given request. Compute the latenci es of all request/reply pairs, and plot them as a distribution. What is t he average? Maximum? Minimum? 7. Sometimes requests are retried, as the request or its reply coul d be lost or dropped. Can you ﬁnd any evidence of such retrying in the trace sample? 8. There are many other questions you could answer through more ana lysis. What questions do you think are important? Suggest them to us, and per- haps we’ll add them here. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2933
50. Andrew File System AFS,"50 The Andrew File System (AFS) The Andrew File System was introduced at Carnegie-Mellon Unive rsity (CMU)1in the 1980’s [H+88]. Led by the well-known Professor M. Satya- narayanan of Carnegie-Mellon University (“Satya” for short), th e main goal of this project was simple: scale . Speciﬁcally, how can one design a distributed ﬁle system such that a server can support as many c lients as possible? Interestingly, there are numerous aspects of design and imple menta- tion that affect scalability. Most important is the design of the protocol be- tween clients and servers. In NFS, for example, the protocol forces clients to check with the server periodically to determine if cached c ontents have changed; because each check uses server resources (includin g CPU and network bandwidth), frequent checks like this will limit the number of clients a server can respond to and thus limit scalability. AFS also differs from NFS in that from the beginning, reasonable user- visible behavior was a ﬁrst-class concern. In NFS, cache consist ency is hard to describe because it depends directly on low-level impl ementa- tion details, including client-side cache timeout intervals . In AFS, cache consistency is simple and readily understood: when the ﬁle is ope ned, a client will generally receive the latest consistent copy from t he server. 50.1 AFS Version 1 We will discuss two versions of AFS [H+88, S+85]. The ﬁrst version (which we will call AFSv1, but actually the original system was called the ITC distributed ﬁle system [S+85]) had some of the basic desi gn in place, but didn’t scale as desired, which led to a re-design an d the ﬁnal protocol (which we will call AFSv2, or just AFS) [H+88]. We now discus s the ﬁrst version. 1Though originally referred to as “Carnegie-Mellon University”, CMU l ater dropped the hyphen, and thus was born the modern form, “Carnegie Mellon Universit y.” As AFS derived from work in the early 80’s, we refer to CMU in its original fu lly-hyphenated form. See https://www.quora.com/When-did-Carnegie-Mellon-Univ ersity-remove-the- hyphen-in-the-university-name for more details, if you are into really boring minutiae. 1 2 THEANDREW FILESYSTEM (AFS) TestAuth Test whether a file has changed (used to validate cached entries) GetFileStat Get the stat info for a file Fetch Fetch the contents of file Store Store this file on the server SetFileStat Set the stat info for a file ListDir List the contents of a directory Figure 50.1: AFSv1 Protocol Highlights One of the basic tenets of all versions of AFS is whole-ﬁle caching on thelocal disk of the client machine that is accessing a ﬁle. When you open() a ﬁle, the entire ﬁle (if it exists) is fetched from the server a nd stored in a ﬁle on your local disk. Subsequent application read() and write() operations are redirected to the local ﬁle system where the ﬁle i s stored; thus, these operations require no network communication a nd are fast. Finally, upon close() , the ﬁle (if it has been modiﬁed) is ﬂushed back to the server.",3025
50. Andrew File System AFS,"Note the obvious contrasts with NFS, which cach es blocks (not whole ﬁles, although NFS could of course cache every block of an entire ﬁle) and does so in client memory (not local disk). Let’s get into the details a bit more. When a client application ﬁ rst calls open() , the AFS client-side code (which the AFS designers call Venus ) would send a Fetch protocol message to the server. The Fetch protocol message would pass the entire pathname of the desired ﬁle (for ex am- ple,/home/remzi/notes.txt ) to the ﬁle server (the group of which they called Vice ), which would then traverse the pathname, ﬁnd the de- sired ﬁle, and ship the entire ﬁle back to the client. The clie nt-side code would then cache the ﬁle on the local disk of the client (by writing it to local disk). As we said above, subsequent read() andwrite() system calls are strictly local in AFS (no communication with the server occurs); they are just redirected to the local copy of the ﬁle. Because the read() andwrite() calls act just like calls to a local ﬁle system, once a block is accessed, it also may be cached in client memory. Thus, AFS a lso uses client memory to cache copies of blocks that it has in its local disk . Fi- nally, when ﬁnished, the AFS client checks if the ﬁle has been modiﬁed (i.e., that it has been opened for writing); if so, it ﬂushes the n ew version back to the server with a Store protocol message, sending the entir e ﬁle and pathname to the server for permanent storage. The next time the ﬁle is accessed, AFSv1 does so much more efﬁ- ciently. Speciﬁcally, the client-side code ﬁrst contacts the s erver (using the TestAuth protocol message) in order to determine whether the ﬁle has changed. If not, the client would use the locally-cached copy , thus improving performance by avoiding a network transfer. The ﬁgure above shows some of the protocol messages in AFSv1. Note that this early ver- sion of the protocol only cached ﬁle contents; directories, for exampl e, were only kept at the server. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 3 TIP: M EASURE THEN BUILD (PATTERSON ’SLAW) One of our advisors, David Patterson (of RISC and RAID fame), used to always encourage us to measure a system and demonstrate a proble m before building a new system to ﬁx said problem. By using experimen- tal evidence, rather than gut instinct, you can turn the proces s of system building into a more scientiﬁc endeavor. Doing so also has the fr inge ben- eﬁt of making you think about how exactly to measure the system bef ore your improved version is developed. When you do ﬁnally get around to building the new system, two things are better as a result: ﬁr st, you have evidence that shows you are solving a real problem; second, you now have a way to measure your new system in place, to show that it act ually improves upon the state of the art. And thus we call this Patterson’s Law . 50.2 Problems with Version 1 A few key problems with this ﬁrst version of AFS motivated the de- signers to rethink their ﬁle system. To study the problems in d etail, the designers of AFS spent a great deal of time measuring their exis ting pro- totype to ﬁnd what was wrong.",3196
50. Andrew File System AFS,"Such experimentation is a good thing, because measurement is the key to understanding how systems work and how to improve them; obtaining concrete, good data is thus a nece s- sary part of systems construction. In their study, the authors fou nd two main problems with AFSv1: •Path-traversal costs are too high : When performing a Fetch or Store protocol request, the client passes the entire pathname (e.g., /home/ remzi/notes.txt ) to the server. The server, in order to access the ﬁle, must perform a full pathname traversal, ﬁrst looking in the root directory to ﬁnd home , then inhome to ﬁndremzi , and so forth, all the way down the path until ﬁnally the desired ﬁle is located . With many clients accessing the server at once, the designers of AFS found that the server was spending much of its CPU time simply walking down directory paths. •The client issues too many TestAuth protocol messages : Much like NFS and its overabundance of GETATTR protocol messages, AFSv1 generated a large amount of trafﬁc to check whether a lo- cal ﬁle (or its stat information) was valid with the TestAuth prot o- col message. Thus, servers spent much of their time telling cli ents whether it was OK to used their cached copies of a ﬁle. Most of the time, the answer was that the ﬁle had not changed. There were actually two other problems with AFSv1: load was not balanced across servers, and the server used a single distinc t process per client thus inducing context switching and other overheads. Th e load c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 THEANDREW FILESYSTEM (AFS) imbalance problem was solved by introducing volumes , which an ad- ministrator could move across servers to balance load; the context -switch problem was solved in AFSv2 by building the server with threads i nstead of processes. However, for the sake of space, we focus here on the main two protocol problems above that limited the scale of the system. 50.3 Improving the Protocol The two problems above limited the scalability of AFS; the server CPU became the bottleneck of the system, and each server could only se r- vice 20 clients without becoming overloaded. Servers were receiv ing too many TestAuth messages, and when they received Fetch or Store me s- sages, were spending too much time traversing the directory hi erarchy. Thus, the AFS designers were faced with a problem: THECRUX: HOWTODESIGN A S CALABLE FILEPROTOCOL How should one redesign the protocol to minimize the number of server interactions, i.e., how could they reduce the number of Te stAuth messages? Further, how could they design the protocol to make thes e server interactions efﬁcient? By attacking both of these issue s, a new pro- tocol would result in a much more scalable version AFS. 50.4 AFS Version 2 AFSv2 introduced the notion of a callback to reduce the number of client/server interactions. A callback is simply a promise fr om the server to the client that the server will inform the client when a ﬁle t hat the client is caching has been modiﬁed.",3028
50. Andrew File System AFS,"By adding this state to the system, the client no longer needs to contact the server to ﬁnd out if a cac hed ﬁle is still valid. Rather, it assumes that the ﬁle is valid until the server tells it otherwise; notice the analogy to polling versus interrupts . AFSv2 also introduced the notion of a ﬁle identiﬁer (FID) (similar to the NFS ﬁle handle ) instead of pathnames to specify which ﬁle a client was interested in. An FID in AFS consists of a volume identiﬁer, a ﬁle identiﬁer, and a “uniquiﬁer” (to enable reuse of the volume and ﬁle IDs when a ﬁle is deleted). Thus, instead of sending whole pathname s to the server and letting the server walk the pathname to ﬁnd the desired ﬁle, the client would walk the pathname, one piece at a time, cac hing the results and thus hopefully reducing the load on the server. For example, if a client accessed the ﬁle /home/remzi/notes.txt , andhome was the AFS directory mounted onto /(i.e.,/was the local root directory, but home and its children were in AFS), the client would ﬁrst Fetch the directory contents of home , put them in the local-disk cache, and set up a callback on home . Then, the client would Fetch the directory OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 5 Client (C 1) Server fd = open(“/home/remzi/notes.txt”, ...); Send Fetch (home FID, “remzi”) Receive Fetch request look for remzi in home dir establish callback(C 1) on remzi return remzi’s content and FID Receive Fetch reply write remzi to local disk cache record callback status of remzi Send Fetch (remzi FID, “notes.txt”) Receive Fetch request look for notes.txt in remzi dir establish callback(C 1) on notes.txt return notes.txt’s content and FID Receive Fetch reply write notes.txt to local disk cache record callback status of notes.txt localopen() of cached notes.txt return ﬁle descriptor to application read(fd, buffer, MAX); perform local read() on cached copy close(fd); do localclose() on cached copy if ﬁle has changed, ﬂush to server fd = open(“/home/remzi/notes.txt”, ...); Foreach dir (home, remzi) if (callback(dir) == VALID) use local copy for lookup(dir) else Fetch (as above) if (callback(notes.txt) == VALID) open local cached copy return ﬁle descriptor to it else Fetch (as above) then open and return fd Figure 50.2: Reading A File: Client-side And File Server Actions remzi , put it in the local-disk cache, and set up a callback on remzi . Finally, the client would Fetch notes.txt , cache this regular ﬁle in the local disk, set up a callback, and ﬁnally return a ﬁle descript or to the calling application. See Figure 50.2 for a summary. The key difference, however, from NFS, is that with each fetch of a directory or ﬁle, the AFS client would establish a callback with the server, thus ensuring that the server would notify the client of a change in its cached state. The beneﬁt is obvious: although the ﬁrst access to/home/ c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 THEANDREW FILESYSTEM (AFS) ASIDE : CACHE CONSISTENCY ISNOTA P ANACEA When discussing distributed ﬁle systems, much is made of the c ache con- sistency the ﬁle systems provide.",3140
50. Andrew File System AFS,"However, this baseline consi stency does not solve all problems with regards to ﬁle access from multiple cl ients. For example, if you are building a code repository, with multiple c lients performing check-ins and check-outs of code, you can’t simply rely on the underlying ﬁle system to do all of the work for you; rather, you h ave to use explicit ﬁle-level locking in order to ensure that the “right” thing happens when such concurrent accesses take place. Indeed, an y applica- tion that truly cares about concurrent updates will add extra ma chinery to handle conﬂicts. The baseline consistency described in thi s chapter and the previous one are useful primarily for casual usage, i.e., wh en a user logs into a different client, they expect some reasonable versi on of their ﬁles to show up there. Expecting more from these protocols is settin g yourself up for failure, disappointment, and tear-ﬁlled frust ration. remzi/notes.txt generates many client-server messages (as described above), it also establishes callbacks for all the directories a s well as the ﬁle notes.txt, and thus subsequent accesses are entirely loca l and require no server interaction at all. Thus, in the common case where a ﬁle is cached at the client, AFS behaves nearly identically to a loca l disk-based ﬁle system. If one accesses a ﬁle more than once, the second access should be just as fast as accessing a ﬁle locally. 50.5 Cache Consistency When we discussed NFS, there were two aspects of cache consisten cy we considered: update visibility and cache staleness . With update visi- bility, the question is: when will the server be updated with a new version of a ﬁle? With cache staleness, the question is: once the server h as a new version, how long before clients see the new version instead of an old er cached copy? Because of callbacks and whole-ﬁle caching, the cache consiste ncy pro- vided by AFS is easy to describe and understand. There are two im- portant cases to consider: consistency between processes on different ma- chines, and consistency between processes on the same machine. Between different machines, AFS makes updates visible at th e server and invalidates cached copies at the exact same time, which is when the updated ﬁle is closed. A client opens a ﬁle, and then writes to it (perhaps repeatedly). When it is ﬁnally closed, the new ﬁle is ﬂushed to the server (and thus visible). At this point, the server then “breaks” ca llbacks for any clients with cached copies; the break is accomplished by con tacting each client and informing it that the callback it has on the ﬁle i s no longer valid. This step ensures that clients will no longer read stal e copies of the ﬁle; subsequent opens on those clients will require a re-fet ch of the OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 7 Client 1 Client 2 Server Comments P1 P2 Cache P 3 Cache Disk open(F) - - - File created write(A) A - - close() A - A open(F) A - A read()→A A - A close() A - A open(F) A - A write(B) B - A open(F) B - A Local processes read()→B B - A see writes immediately close() B - A B open(F) A A Remote processes B read() →A A A do not see writes...",3175
50. Andrew File System AFS,"B close() A A close() B ✁A B ... until close() B open(F) B B has taken place B read() →B B B B close() B B B open(F) B B open(F) B B B write(D) D B B D write(C) C B D close() C C close() D ✁C D D open(F) D D Unfortunately for P 3 D read() →D D D the last writer wins D close() D D Figure 50.3: Cache Consistency Timeline new version of the ﬁle from the server (and will also serve to rees tablish a callback on the new version of the ﬁle). AFS makes an exception to this simple model between processes on the same machine. In this case, writes to a ﬁle are immediatel y visible to other local processes (i.e., a process does not have to wait until a ﬁ le is closed to see its latest updates). This makes using a single ma chine be- have exactly as you would expect, as this behavior is based upon ty pical UNIXsemantics. Only when switching to a different machine would y ou be able to detect the more general AFS consistency mechanism. There is one interesting cross-machine case that is worthy of fur ther discussion. Speciﬁcally, in the rare case that processes on diff erent ma- chines are modifying a ﬁle at the same time, AFS naturally empl oys what is known as a last writer wins approach (which perhaps should be called last closer wins ). Speciﬁcally, whichever client calls close() last will update the entire ﬁle on the server last and thus will be the “wi nning” ﬁle, i.e., the ﬁle that remains on the server for others to see. Th e result is a ﬁle that was generated in its entirety either by one client or t he other. Note the difference from a block-based protocol like NFS: in NFS, writ es of individual blocks may be ﬂushed out to the server as each clien t is up- dating the ﬁle, and thus the ﬁnal ﬁle on the server could end up as a mix of updates from both clients. In many cases, such a mixed ﬁle outpu t c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 THEANDREW FILESYSTEM (AFS) would not make much sense, i.e., imagine a JPEG image getting mod i- ﬁed by two clients in pieces; the resulting mix of writes would n ot likely constitute a valid JPEG. A timeline showing a few of these different scenarios can be seen in Figure 50.3. The columns show the behavior of two processes (P 1and P 2) on Client 1and its cache state, one process (P 3) on Client 2and its cache state, and the server (Server), all operating on a single ﬁle cal led, imag- inatively,F. For the server, the ﬁgure simply shows the contents of the ﬁle after the operation on the left has completed. Read through it a nd see if you can understand why each read returns the results that it does. A commentary ﬁeld on the right will help you if you get stuck. 50.6 Crash Recovery From the description above, you might sense that crash recovery is more involved than with NFS. You would be right. For example, imagin e there is a short period of time where a server (S) is not able to contac t a client (C1), for example, while the client C1 is rebooting. Whi le C1 is not available, S may have tried to send it one or more callback re call messages; for example, imagine C1 had ﬁle F cached on its local di sk, and then C2 (another client) updated F, thus causing S to send mess ages to all clients caching the ﬁle to remove it from their local caches. Bec ause C1 may miss those critical messages when it is rebooting, upon rejoin ing the system, C1 should treat all of its cache contents as suspect. Thu s, upon the next access to ﬁle F, C1 should ﬁrst ask the server (with a Te stAuth protocol message) whether its cached copy of ﬁle F is still valid; i f so, C1 can use it; if not, C1 should fetch the newer version from the serve r.",3634
50. Andrew File System AFS,"Server recovery after a crash is also more complicated. The proble m that arises is that callbacks are kept in memory; thus, when a s erver re- boots, it has no idea which client machine has which ﬁles. Thus, upon server restart, each client of the server must realize that th e server has crashed and treat all of their cache contents as suspect, and (a s above) reestablish the validity of a ﬁle before using it. Thus, a serve r crash is a big event, as one must ensure that each client is aware of the cra sh in a timely manner, or risk a client accessing a stale ﬁle. There ar e many ways to implement such recovery; for example, by having the server s end a message (saying “don’t trust your cache contents.”) to each clien t when it is up and running again, or by having clients check that the s erver is alive periodically (with a heartbeat message, as it is called). As you can see, there is a cost to building a more scalable and sensible cac hing model; with NFS, clients hardly noticed a server crash. 50.7 Scale And Performance Of AFSv2 With the new protocol in place, AFSv2 was measured and found to be much more scalable that the original version. Indeed, each serv er could OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 9 Workload NFS AFS AFS/NFS 1. Small ﬁle, sequential read Ns·Lnet Ns·Lnet 1 2. Small ﬁle, sequential re-read Ns·Lmem Ns·Lmem 1 3. Medium ﬁle, sequential read Nm·Lnet Nm·Lnet 1 4. Medium ﬁle, sequential re-read Nm·Lmem Nm·Lmem 1 5. Large ﬁle, sequential read NL·Lnet NL·Lnet 1 6. Large ﬁle, sequential re-read NL·Lnet NL·LdiskLdisk Lnet 7. Large ﬁle, single read Lnet NL·Lnet NL 8. Small ﬁle, sequential write Ns·Lnet Ns·Lnet 1 9. Large ﬁle, sequential write NL·Lnet NL·Lnet 1 10. Large ﬁle, sequential overwrite NL·Lnet2·NL·Lnet 2 11. Large ﬁle, single write Lnet 2·NL·Lnet2·NL Figure 50.4: Comparison: AFS vs. NFS support about 50 clients (instead of just 20). A further beneﬁt w as that client-side performance often came quite close to local performa nce, be- cause in the common case, all ﬁle accesses were local; ﬁle reads u sually went to the local disk cache (and potentially, local memory). Onl y when a client created a new ﬁle or wrote to an existing one was there need to send a Store message to the server and thus update the ﬁle with new cont ents. Let us also gain some perspective on AFS performance by compar- ing common ﬁle-system access scenarios with NFS. Figure 50.4 (pa ge 9) shows the results of our qualitative comparison. In the ﬁgure, we examine typical read and write patterns anal ytically, for ﬁles of different sizes. Small ﬁles have Nsblocks in them; medium ﬁles have Nmblocks; large ﬁles have NLblocks. We assume that small and medium ﬁles ﬁt into the memory of a client; large ﬁles ﬁt on a loc al disk but not in client memory. We also assume, for the sake of analysis, that an access across th e net- work to the remote server for a ﬁle block takes Lnettime units. Access to local memory takes Lmem , and access to local disk takes Ldisk. The general assumption is that Lnet> Ldisk> Lmem .",3086
50. Andrew File System AFS,"Finally, we assume that the ﬁrst access to a ﬁle does not hit in an y caches. Subsequent ﬁle accesses (i.e., “re-reads”) we assum e will hit in caches, if the relevant cache has enough capacity to hold the ﬁl e. The columns of the ﬁgure show the time a particular operation (e.g. , a small ﬁle sequential read) roughly takes on either NFS or AFS. The right- most column displays the ratio of AFS to NFS. We make the following observations. First, in many cases, the per - formance of each system is roughly equivalent. For example, when ﬁrst reading a ﬁle (e.g., Workloads 1, 3, 5), the time to fetch the ﬁle from the re- mote server dominates, and is similar on both systems. You might th ink AFS would be slower in this case, as it has to write the ﬁle to local disk; however, those writes are buffered by the local (client-side) ﬁ le system cache and thus said costs are likely hidden. Similarly, you migh t think that AFS reads from the local cached copy would be slower, again be- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 THEANDREW FILESYSTEM (AFS) cause AFS stores the cached copy on disk. However, AFS again beneﬁ ts here from local ﬁle system caching; reads on AFS would likely hit i n the client-side memory cache, and performance would be similar to N FS. Second, an interesting difference arises during a large-ﬁle s equential re-read (Workload 6). Because AFS has a large local disk cache, i t will access the ﬁle from there when the ﬁle is accessed again. NFS, in contrast, only can cache blocks in client memory; as a result, if a large ﬁle (i.e., a ﬁle bigger than local memory) is re-read, the NFS client will have t o re-fetch the entire ﬁle from the remote server. Thus, AFS is faster than N FS in this case by a factor ofLnet Ldisk, assuming that remote access is indeed slower than local disk. We also note that NFS in this case increases ser ver load, which has an impact on scale as well. Third, we note that sequential writes (of new ﬁles) should perfor m similarly on both systems (Workloads 8, 9). AFS, in this case, will write the ﬁle to the local cached copy; when the ﬁle is closed, the AFS cl ient will force the writes to the server, as per the protocol. NFS will b uffer writes in client memory, perhaps forcing some blocks to the serve r due to client-side memory pressure, but deﬁnitely writing them t o the server when the ﬁle is closed, to preserve NFS ﬂush-on-close consistenc y. You might think AFS would be slower here, because it writes all data to local disk. However, realize that it is writing to a local ﬁle system; those writes are ﬁrst committed to the page cache, and only later (in the back ground) to disk, and thus AFS reaps the beneﬁts of the client-side OS me mory caching infrastructure to improve performance. Fourth, we note that AFS performs worse on a sequential ﬁle over- write (Workload 10). Thus far, we have assumed that the workloads that write are also creating a new ﬁle; in this case, the ﬁle exists , and is then over-written. Overwrite can be a particularly bad case for AFS, because the client ﬁrst fetches the old ﬁle in its entirety, only to subs equently over- write it.",3159
50. Andrew File System AFS,"NFS, in contrast, will simply overwrite blocks and thus avoid the initial (useless) read2. Finally, workloads that access a small subset of data within lar ge ﬁles perform much better on NFS than AFS (Workloads 7, 11). In these cas es, the AFS protocol fetches the entire ﬁle when the ﬁle is opened; unf ortu- nately, only a small read or write is performed. Even worse, if the ﬁle is modiﬁed, the entire ﬁle is written back to the server, doubling the per- formance impact. NFS, as a block-based protocol, performs I/O that i s proportional to the size of the read or write. Overall, we see that NFS and AFS make different assumptions an d not surprisingly realize different performance outcomes as a resu lt. Whether these differences matter is, as always, a question of workload. 2We assume here that NFS writes are block-sized and block-aligned; if t hey were not, the NFS client would also have to read the block ﬁrst. We also assume the ﬁ le was notopened with the O TRUNC ﬂag; if it had been, the initial open in AFS would not fetch the soon to be truncated ﬁle’s contents. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 11 ASIDE : THEIMPORTANCE OFWORKLOAD One challenge of evaluating any system is the choice of workload . Be- cause computer systems are used in so many different ways, the re are a large variety of workloads to choose from. How should the storage sys- tem designer decide which workloads are important, in order to ma ke reasonable design decisions? The designers of AFS, given their experience in measuring how ﬁl e sys- tems were used, made certain workload assumptions; in particul ar, they assumed that most ﬁles were not frequently shared, and accesse d sequen- tially in their entirety. Given those assumptions, the AFS des ign makes perfect sense. However, these assumptions are not always correct. For example, i mag- ine an application that appends information, periodically, to a log. These little log writes, which add small amounts of data to an existing large ﬁle, are quite problematic for AFS. Many other difﬁcult workloads exist as well, e.g., random updates in a transaction database. One place to get some information about what types of workloads are common are through various research studies that have been perfor med. See any of these studies for good examples of workload analysis [B+91, H+11, R+00, V99], including the AFS retrospective [H+88]. 50.8 AFS: Other Improvements Like we saw with the introduction of Berkeley FFS (which added sy m- bolic links and a number of other features), the designers of AFS t ook the opportunity when building their system to add a number of featur es that made the system easier to use and manage. For example, AFS provi des a true global namespace to clients, thus ensuring that all ﬁles were named the same way on all client machines. NFS, in contrast, allows each client to mount NFS servers in any way that they please, and thus only by con- vention (and great administrative effort) would ﬁles be named s imilarly across clients.",3050
50. Andrew File System AFS,"AFS also takes security seriously, and incorporates mechanism s to au- thenticate users and ensure that a set of ﬁles could be kept priv ate if a user so desired. NFS, in contrast, had quite primitive support f or security for many years. AFS also includes facilities for ﬂexible user-managed acces s control. Thus, when using AFS, a user has a great deal of control over who exac tly can access which ﬁles. NFS, like most U NIX ﬁle systems, has much less support for this type of sharing. Finally, as mentioned before, AFS adds tools to enable simpler ma n- agement of servers for the administrators of the system. In think ing about system management, AFS was light years ahead of the ﬁeld. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 THEANDREW FILESYSTEM (AFS) 50.9 Summary AFS shows us how distributed ﬁle systems can be built quite diff er- ently than what we saw with NFS. The protocol design of AFS is partic - ularly important; by minimizing server interactions (through whole-ﬁle caching and callbacks), each server can support many clients and thus reduce the number of servers needed to manage a particular sit e. Many other features, including the single namespace, security, a nd access-control lists, make AFS quite nice to use. The consistency model provide d by AFS is simple to understand and reason about, and does not lead to the oc ca- sional weird behavior as one sometimes observes in NFS. Perhaps unfortunately, AFS is likely on the decline. Because N FS be- came an open standard, many different vendors supported it, and , along with CIFS (the Windows-based distributed ﬁle system protocol), NFS dominates the marketplace. Although one still sees AFS install ations from time to time (such as in various educational institutions, i ncluding Wisconsin), the only lasting inﬂuence will likely be from the id eas of AFS rather than the actual system itself. Indeed, NFSv4 now adds se rver state (e.g., an “open” protocol message), and thus bears an increasing similar- ity to the basic AFS protocol. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG THEANDREW FILESYSTEM (AFS) 13 References [B+91] “Measurements of a Distributed File System” by Mary Baker , John Hartman, Martin Kupfer, Ken Shirriff, John Ousterhout. SOSP ’91, Paciﬁc Grove, Californi a, October 1991. An early paper measuring how people use distributed ﬁle systems. Matches m uch of the intuition found in AFS. [H+11] “A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications” by Tyler Harter, Chris Dragga, Michael Vaughn, Andrea C. Arpaci-Dusse au, Remzi H. Arpaci- Dusseau. SOSP ’11, New York, New York, October 2011. Our own paper studying the behavior of Apple Desktop workloads; turns out they are a bit different than many of the serv er-based workloads the systems research community usually focuses upon. Also a good recent re ference which points to a lot of related work. [H+88] “Scale and Performance in a Distributed File System” by John H. H oward, Michael L. Kazar, Sherri G. Menees, David A.",3038
50. Andrew File System AFS,"Nichols, M. Satyanarayanan, Rober t N. Sidebotham, Michael J. West. ACM Transactions on Computing Systems (ACM TOCS), Volum e 6:1, Febru- ary 1988. The long journal version of the famous AFS system, still in use in a number of places throughout the world, and also probably the earliest clear thinking on how to bui ld distributed ﬁle systems. A wonderful combination of the science of measurement and princ ipled engineering. [R+00] “A Comparison of File System Workloads” by Drew Rosell i, Jacob R. Lorch, Thomas E. Anderson. USENIX ’00, San Diego, California, June 2000. A more recent set of traces as compared to the Baker paper [B+91], with some interesting twists. [S+85] “The ITC Distributed File System: Principles and Design” by M. Sa tyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West. SOSP ’ 85, Orcas Island, Wash- ington, December 1985. The older paper about a distributed ﬁle system. Much of the basic design of AFS is in place in this older system, but not the improvements for scale. T he name change to “Andrew” is an homage to two people both named Andrew, Andrew Carnegie and Andrew Me llon. These two rich dudes started the Carnegie Institute of Technology and the Mellon Institute of Industrial Research, respectively, which eventually merged to become what is now known as Carn egie Mellon University. [V99] “File system usage in Windows NT 4.0” by Werner Vogels. SOSP ’99, Kiawah Island Resort, South Carolina, December 1999. A cool study of Windows workloads, which are inherently different than many of the UNIX-based studies that had previously been done. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 14 THEANDREW FILESYSTEM (AFS) Homework (Simulation) This section introduces afs.py , a simple AFS simulator you can use to shore up your knowledge of how the Andrew File System works. Read the README ﬁle for more details. Questions 1. Run a few simple cases to make sure you can predict what values wil l be read by clients. Vary the random seed ﬂag ( -s) and see if you can trace through and predict both intermediate values as well as the ﬁnal values stored in the ﬁles. Also vary the number of ﬁles ( -f), the number of clients (-C), and the read ratio ( -r, from between 0 to 1) to make it a bit more challenging. You might also want to generate slightly longer traces to make for more interesting interactions, e.g., ( -n 2 or higher). 2. Now do the same thing and see if you can predict each callback that the AFS server initiates. Try different random seeds, and make sure to use a high level of detailed feedback (e.g., -d 3 ) to see when callbacks occur when you have the program compute the answers for you (with -c). Can you guess exactly when each callback occurs? What is the precise con dition for one to take place? 3. Similar to above, run with some different random seeds and see if you can predict the exact cache state at each step. Cache state can be o bserved by running with -cand-d 7 . 4. Now let’s construct some speciﬁc workloads. Run the simulation with-A oa1:w1:c1,oa1:r1:c1 ﬂag. What are different possible values observed by client 1 when it reads the ﬁle a, when running with the random sched- uler? (try different random seeds to see different outcomes)? O f all the possible schedule interleavings of the two clients’ operati ons, how many of them lead to client 1 reading the value 1, and how many reading th e value 0? 5. Now let’s construct some speciﬁc schedules. When running with the-A oa1:w1:c1,oa1:r1:c1 ﬂag, also run with the following schedules: -S 01,-S 100011 ,-S 011100 , and others of which you can think. What value will client 1 read? 6. Now run with this workload: -A oa1:w1:c1,oa1:w1:c1 , and vary the schedules as above. What happens when you run with -S 011100 ? What about when you run with -S 010011 ? What is important in determining the ﬁnal value of the ﬁle? OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",3932
51. Summary Dialogue on Distribution,"51 Summary Dialogue on Distribution Student: Well, that was quick. Too quick, in my opinion. Professor: Yes, distributed systems are complicated and cool and well worth your study; just not in this book (or course). Student: That’s too bad; I wanted to learn more. But I did learn a few things. Professor: Like what? Student: Well, everything can fail. Professor: Good start. Student: But by having lots of these things (whether disks, machines, or wha t- ever), you can hide much of the failure that arises. Professor: Keep going. Student: Some basic techniques like retrying are really useful. Professor: That’s true. Student: And you have to think carefully about protocols: the exact bits that are exchanged between machines. Protocols can affect everything , including how systems respond to failure and how scalable they are. Professor: You really are getting better at this learning stuff. Student: Thanks. And you’re not a bad teacher yourself. Professor: Well thank you very much too. Student: So is this the end of the book? Professor: I’m not sure. They don’t tell me anything. Student: Me neither. Let’s get out of here. Professor: OK. Student: Go ahead. Professor: No, after you. Student: Please, professors ﬁrst. 1 2 SUMMARY DIALOGUE ON DISTRIBUTION Professor: No, please, after you. Student: (exasperated) Fine. Professor: (waiting) ... so why haven’t you left? Student: I don’t know how. Turns out, the only thing I can do is participate in these dialogues. Professor: Me too. And now you’ve learned our ﬁnal lesson... OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",1583
Appendices. Appendix B Virtual Machine Monitors,"A A Dialogue on Virtual Machine Monitors Student: So now we’re stuck in the Appendix, huh? Professor: Yes, just when you thought things couldn’t get any worse. Student: Well, what are we going to talk about? Professor: An old topic that has been reborn: virtual machine monitors , also known as hypervisors . Student: Oh, like VMware? That’s cool; I’ve used that kind of software before. Professor: Cool indeed. We’ll learn how VMMs add yet another layer of virtu- alization into systems, this one beneath the OS itself. Crazy and amaz ing stuff, really. Student: Sounds neat. Why not include this in the earlier part of the book, the n, on virtualization? Shouldn’t it really go there? Professor: That’s above our pay grade, I’m afraid. But my guess is this: there is already a lot of material there. By moving this small aside on VMMs int o the appendix, a particular instructor can choose whether to includ e it or skip it. But I do think it should be included, because if you can understand ho w VMMs work, then you really understand virtualization quite well. Student: Alright then, let’s get to work. 1 B Virtual Machine Monitors B.1 Introduction Years ago, IBM sold expensive mainframes to large organizations , and a problem arose: what if the organization wanted to run different oper- ating systems on the machine at the same time? Some applications h ad been developed on one OS, and some on others, and thus the problem. As a solution, IBM introduced yet another level of indirection in th e form of a virtual machine monitor (VMM ) (also called a hypervisor ) [G74]. Speciﬁcally, the monitor sits between one or more operating systems and the hardware and gives the illusion to each running OS that it con- trols the machine. Behind the scenes, however, the monitor actua lly is in control of the hardware, and must multiplex running OSes across the physical resources of the machine. Indeed, the VMM serves as an operat- ing system for operating systems, but at a much lower level; the O S must still think it is interacting with the physical hardware. Th us,transparency is a major goal of VMMs. Thus, we ﬁnd ourselves in a funny position: the OS has thus far ser ved as the master illusionist, tricking unsuspecting applicati ons into thinking they have their own private CPU and a large virtual memory, whil e se- cretly switching between applications and sharing memory as w ell. Now, we have to do it again, but this time underneath the OS, who is us ed to being in charge. How can the VMM create this illusion for each OS r un- ning on top of it? THECRUX: HOW TO VIRTUALIZE THE MACHINE UNDERNEATH THE OS The virtual machine monitor must transparently virtualize th e ma- chine underneath the OS; what are the techniques required to d o so? 1 2 VIRTUAL MACHINE MONITORS B.2 Motivation: Why VMMs? Today, VMMs have become popular again for a multitude of reasons. Server consolidation is one such reason. In many settings, people ru n services on different machines which run different operating systems (or even OS versions), and yet each machine is lightly utilized. I n this case, virtualization enables an administrator to consolidate multiple OSes onto fewer hardware platforms, and thus lower costs and ease adminis tration. Virtualization has also become popular on desktops, as many users wish to run one operating system (say Linux or Mac OS X) but still h ave access to native applications on a different platform (say Wind ows).",3456
Appendices. Appendix B Virtual Machine Monitors,"This type of improvement in functionality is also a good reason. Another reason is testing and debugging. While developers writ e code on one main platform, they often want to debug and test it on the many different platforms that they deploy the software to in the ﬁeld . Thus, virtualization makes it easy to do so, by enabling a developer to run many operating system types and versions on just one machine. This resurgence in virtualization began in earnest the mid-t o-late 1990’s, and was led by a group of researchers at Stanford headed by Professor Mendel Rosenblum. His group’s work on Disco [B+97], a virtual mach ine monitor for the MIPS processor, was an early effort that revived VMM s and eventually led that group to the founding of VMware [V98], now a market leader in virtualization technology. In this chapter, w e will dis- cuss the primary technology underlying Disco and through that w indow try to understand how virtualization works. B.3 Virtualizing the CPU To run a virtual machine (e.g., an OS and its applications) on top of a virtual machine monitor, the basic technique that is used is limited direct execution , a technique we saw before when discussing how the OS vir- tualizes the CPU. Thus, when we wish to “boot” a new OS on top of the VMM, we simply jump to the address of the ﬁrst instruction and le t the OS begin running. It is as simple as that (well, almost). Assume we are running on a single processor, and that we wish to multiplex between two virtual machines, that is, between tw o OSes and their respective applications. In a manner quite similar to a n operating system switching between running processes (a context switch ), a virtual machine monitor must perform a machine switch between running vir- tual machines. Thus, when performing such a switch, the VMM mu st save the entire machine state of one OS (including registers, P C, and un- like in a context switch, any privileged hardware state), res tore the ma- chine state of the to-be-run VM, and then jump to the PC of the to-be -run VM and thus complete the switch. Note that the to-be-run VM’s PC ma y be within the OS itself (i.e., the system was executing a syst em call) or it may simply be within a process that is running on that OS (i.e., a user- mode application). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 3 We get into some slightly trickier issues when a running appli cation or OS tries to perform some kind of privileged operation . For example, on a system with a software-managed TLB, the OS will use special priv- ileged instructions to update the TLB with a translation before restarting an instruction that suffered a TLB miss. In a virtualized envi ronment, the OS cannot be allowed to perform privileged instructions, becaus e then it controls the machine rather than the VMM beneath it. Thus, the V MM must somehow intercept attempts to perform privileged operation s and thus retain control of the machine. A simple example of how a VMM must interpose on certain operations arises when a running process on a given OS tries to make a system call.",3109
Appendices. Appendix B Virtual Machine Monitors,"For example, the process may be trying to call open() on a ﬁle, or may be callingread() to get data from it, or may be calling fork() to create a new process. In a system without virtualization, a system call i s achieved with a special instruction; on MIPS, it is a trap instruction, and on x86, it is theint (an interrupt) instruction with the argument 0x80 . Here is the open library call on FreeBSD [B00] (recall that your C code ﬁrst mak es a library call into the C library, which then executes the prope r assembly sequence to actually issue the trap instruction and make a sys tem call): open: push dword mode push dword flags push dword path mov eax, 5 push eax int 80h On U NIX-based systems, open() takes just three arguments: int open(char *path, int flags, mode t mode) . You can see in the code above how the open() library call is implemented: ﬁrst, the ar- guments get pushed onto the stack ( mode, flags, path ), then a 5 gets pushed onto the stack, and then int 80h is called, which trans- fers control to the kernel. The 5, if you were wondering, is the pre -agreed upon convention between user-mode applications and the kernel for the open() system call in FreeBSD; different system calls would place dif fer- ent numbers onto the stack (in the same position) before calling t he trap instruction int and thus making the system call1. When a trap instruction is executed, as we’ve discussed before, it usu- ally does a number of interesting things. Most important in our exa mple here is that it ﬁrst transfers control (i.e., changes the PC) to a well-deﬁned trap handler within the operating system. The OS, when it is ﬁrst start- ing up, establishes the address of such a routine with the hardw are (also a privileged operation) and thus upon subsequent traps, the har dware 1Just to make things confusing, the Intel folks use the term “interrupt” for w hat almost any sane person would call a trap instruction. As Patterson said abou t the Intel instruction set: “It’s an ISA only a mother could love.” But actually, we kind of like it, and we’re not its mother. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 VIRTUAL MACHINE MONITORS Process Hardware Operating System 1.Execute instructions (add, load, etc.) 2.System call: Trap to OS 3.Switch to kernel mode; Jump to trap handler 4.In kernel mode; Handle system call; Return from trap 5.Switch to user mode; Return to user code 6.Resume execution (@PC after trap) Table B.1: Executing a System Call knows where to start running code to handle the trap. At the same time of the trap, the hardware also does one other crucial thing: it cha nges the mode of the processor from user mode tokernel mode . In user mode, op- erations are restricted, and attempts to perform privileged op erations will lead to a trap and likely the termination of the offending process ; in ker- nel mode, on the other hand, the full power of the machine is availab le, and thus all privileged operations can be executed. Thus, in a t raditional setting (again, without virtualization), the ﬂow of control would b e like what you see in Table B.1.",3110
Appendices. Appendix B Virtual Machine Monitors,"On a virtualized platform, things are a little more interestin g. When an application running on an OS wishes to perform a system call, it d oes the exact same thing: executes a trap instruction with the argume nts carefully placed on the stack (or in registers). However, it is the VMM that controls the machine, and thus the VMM who has installed a trap handler that will ﬁrst get executed in kernel mode. So what should the VMM do to handle this system call? The VMM doesn’t really know how to handle the call; after all, it does not know the details of each OS that is running and therefore does not know wh at each call should do. What the VMM does know, however, is where the OS’s trap handler is. It knows this because when the OS booted up, it tried to install its own trap handlers; when the OS did so, it was trying to do something privileged, and therefore trapped into the VMM ; at that time, the VMM recorded the necessary information (i.e., where t his OS’s trap handlers are in memory). Now, when the VMM receives a trap f rom a user process running on the given OS, it knows exactly what to do: i t jumps to the OS’s trap handler and lets the OS handle the system c all as it should. When the OS is ﬁnished, it executes some kind of privil eged instruction to return from the trap ( rett on MIPS, iret on x86), which again bounces into the VMM, which then realizes that the OS is t rying to return from the trap and thus performs a real return-from-trap a nd thus returns control to the user and puts the machine back in user mode . The entire process is depicted in Tables B.2 and B.3, both for the norm al case without virtualization and the case with virtualization (we le ave out the exact hardware operations from above to save space). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 5 Process Operating System 1.System call: Trap to OS 2.OS trap handler: Decode trap and execute appropriate syscall routine; When done: return from trap 3.Resume execution (@PC after trap) Table B.2: System Call Flow Without Virtualization Process Operating System VMM 1.System call: Trap to OS 2.Process trapped: Call OS trap handler (at reduced privilege) 3.OS trap handler: Decode trap and execute syscall; When done: issue return-from-trap 4.OS tried return from trap: Do real return from trap 5.Resume execution (@PC after trap) Table B.3: System Call Flow with Virtualization As you can see from the ﬁgures, a lot more has to take place when virtualization is going on. Certainly, because of the extra jump ing around, virtualization might indeed slow down system calls and thus coul d hurt performance. You might also notice that we have one remaining question: what mode should the OS run in? It can’t run in kernel mode, because then it would have unrestricted access to the hardware. Thus, it mu st run in some less privileged mode than before, be able to access its own da ta structures, and simultaneously prevent access to its data st ructures from user processes. In the Disco work, Rosenblum and colleagues handled this problem quite neatly by taking advantage of a special mode provided by th e MIPS hardware known as supervisor mode.",3181
Appendices. Appendix B Virtual Machine Monitors,"When running in this mode, one still doesn’t have access to privileged instructions, but one ca n access a little more memory than when in user mode; the OS can use this extr a memory for its data structures and all is well. On hardware that doesn’t have such a mode, one has to run the OS in user mode and use memory protection (page tables and TLBs) to protect OS data structures appro- priately. In other words, when switching into the OS, the monitor w ould have to make the memory of the OS data structures available to th e OS via page-table protections; when switching back to the running ap plication, the ability to read and write the kernel would have to be removed . c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 VIRTUAL MACHINE MONITORS Virtual Address Space \""Physical Memory\"" Machine Memory 0 1 2 3OS Page Table VPN 0 to PFN 10 VPN 2 to PFN 03 VPN 3 to PFN 08 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15VMM Page Table PFN 03 to MFN 06 PFN 08 to MFN 10 PFN 10 to MFN 05 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Figure B.1: VMM Memory Virtualization B.4 Virtualizing Memory You should now have a basic idea of how the processor is virtualized: the VMM acts like an OS and schedules different virtual machi nes to run, and some interesting interactions occur when privilege levels change. But we have left out a big part of the equation: how does the VMM virtuali ze memory? Each OS normally thinks of physical memory as a linear array of pag es, and assigns each page to itself or user processes. The OS itself , of course, already virtualizes memory for its running processes, such tha t each pro- cess has the illusion of its own private address space. Now we must add another layer of virtualization, so that multiple OSes can share the actual physical memory of the machine, and we must do so transparently . This extra layer of virtualization makes “physical” memory a vi rtual- ization on top of what the VMM refers to as machine memory , which is the real physical memory of the system. Thus, we now have an addit ional layer of indirection: each OS maps virtual-to-physical addres ses via its per-process page tables; the VMM maps the resulting physical mappings to underlying machine addresses via its per-OS page tables. Figure B.1 depicts this extra level of indirection. In the ﬁgure, there is just a single virtual address space wit h four pages, three of which are valid (0, 2, and 3). The OS uses its pag e ta- ble to map these pages to three underlying physical frames (1 0, 3, and 8, respectively). Underneath the OS, the VMM performs a furthe r level of indirection, mapping PFNs 3, 8, and 10 to machine frames 6, 10 , and 5 respectively. Of course, this picture simpliﬁes things qui te a bit; on a real system, there would be Voperating systems running (with Vlikely OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 7 Process Operating System 1.Load from memory: TLB miss: Trap 2.OS TLB miss handler: Extract VPN from VA; Do page table lookup; If present and valid: get PFN, update TLB; Return from trap 3.Resume execution (@PC of trapping instruction); Instruction is retried; Results in TLB hit Table B.4: TLB Miss Flow without Virtualization greater than one), and thus VVMM page tables; further, on top of each running operating system OS i, there would be a number of processes Pi running ( Pilikely in the tens or hundreds), and hence Pi(per-process) page tables within OS i.",3466
Appendices. Appendix B Virtual Machine Monitors,"To understand how this works a little better, let’s recall how address translation works in a modern paged system. Speciﬁcally, let’s discuss what happens on a system with a software-managed TLB during add ress translation. Assume a user process generates an address (for an instruc- tion fetch or an explicit load or store); by deﬁnition, the process ge nerates avirtual address , as its address space has been virtualized by the OS. As you know by now, it is the role of the OS, with help from the hardware, to turn this into a physical address and thus be able to fetch the desired contents from physical memory. Assume we have a 32-bit virtual address space and a 4-KB page s ize. Thus, our 32-bit address is chopped into two parts: a 20-bit vir tual page number (VPN), and a 12-bit offset. The role of the OS, with help from the hardware TLB, is to translate the VPN into a valid physical pa ge frame number (PFN) and thus produce a fully-formed physical address which can be sent to physical memory to fetch the proper data. In the com mon case, we expect the TLB to handle the translation in hardware, thus mak- ing the translation fast. When a TLB miss occurs (at least, on a sy stem with a software-managed TLB), the OS must get involved to servi ce the miss, as depicted here in Table B.4. As you can see, a TLB miss causes a trap into the OS, which handles the fault by looking up the VPN in the page table and installing t he trans- lation in the TLB. With a virtual machine monitor underneath the OS, however, thing s again get a little more interesting. Let’s examine the ﬂow of a TLB miss again (see Table B.5 for a summary). When a process makes a virtu al memory reference and misses in the TLB, it is not the OS TLB miss h an- dler that runs; rather, it is the VMM TLB miss handler, as the V MM is the true privileged owner of the machine. However, in the normal c ase, the VMM TLB handler doesn’t know how to handle the TLB miss, so it immediately jumps into the OS TLB miss handler; the VMM knows t he c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 VIRTUAL MACHINE MONITORS Process Operating System Virtual Machine Monitor 1.Load from memory TLB miss: Trap 2.VMM TLB miss handler: Call into OS TLB handler (reducing privilege) 3. OS TLB miss handler: Extract VPN from VA; Do page table lookup; If present and valid, get PFN, update TLB 4.Trap handler: Unprivileged code trying to update the TLB; OS is trying to install VPN-to-PFN mapping; Update TLB instead with VPN-to-MFN (privileged); Jump back to OS (reducing privilege) 5.Return from trap 6.Trap handler: Unprivileged code trying to return from a trap; Return from trap 7.Resume execution (@PC of instruction); Instruction is retried; Results in TLB hit Table B.5: TLB Miss Flow with Virtualization location of this handler because the OS, during “boot”, tried to ins tall its own trap handlers. The OS TLB miss handler then runs, does a page ta- ble lookup for the VPN in question, and tries to install the VPN-to- PFN mapping in the TLB.",3027
Appendices. Appendix B Virtual Machine Monitors,"However, doing so is a privileged operation, a nd thus causes another trap into the VMM (the VMM gets notiﬁed when any non-privileged code tries to do something that is privileged, of course). At this point, the VMM plays its trick: instead of installing th e OS’s VPN- to-PFN mapping, the VMM installs its desired VPN-to-MFN mappi ng. After doing so, the system eventually gets back to the user-lev el code, which retries the instruction, and results in a TLB hit, fetch ing the data from the machine frame where the data resides. This set of actions also hints at how a VMM must manage the virtu- alization of physical memory for each running OS; just like the OS h as a page table for each process, the VMM must track the physical-to- machine mappings for each virtual machine it is running. These per-ma chine page tables need to be consulted in the VMM TLB miss handler in order t o de- termine which machine page a particular “physical” page map s to, and even, for example, if it is present in machine memory at the curr ent time (i.e., the VMM could have swapped it to disk). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 9 ASIDE : HYPERVISORS ANDHARDWARE -MANAGED TLB S Our discussion has centered around software-managed TLBs and t he work that needs to be done when a miss occurs. But you might be wondering: how does the virtual machine monitor get involved with a hardware-managed TLB? In those systems, the hardware walks t he page table on each TLB miss and updates the TLB as need be, and thus th e VMM doesn’t have a chance to run on each TLB miss to sneak its trans - lation into the system. Instead, the VMM must closely monitor cha nges the OS makes to each page table (which, in a hardware-managed sys- tem, is pointed to by a page-table base register of some kind), an d keep a shadow page table that instead maps the virtual addresses of each pro- cess to the VMM’s desired machine pages [AA06]. The VMM instal ls a process’s shadow page table whenever the OS tries to install the process’s OS-level page table, and thus the hardware chugs along, transl ating vir- tual addresses to machine addresses using the shadow table, w ithout the OS even noticing. Finally, as you might notice from this sequence of operations, TLB misses on a virtualized system become quite a bit more expensive than in a non-virtualized system. To reduce this cost, the designer s of Disco added a VMM-level “software TLB”. The idea behind this data st ructure is simple. The VMM records every virtual-to-physical mapping that it sees the OS try to install; then, on a TLB miss, the VMM ﬁrst consu lts its software TLB to see if it has seen this virtual-to-physical mapping be- fore, and what the VMM’s desired virtual-to-machine mapping sh ould be. If the VMM ﬁnds the translation in its software TLB, it simpl y installs the virtual-to-machine mapping directly into the hardware T LB, and thus skips all the back and forth in the control ﬂow above [B+97]. B.5 The Information Gap Just like the OS doesn’t know too much about what application pro- grams really want, and thus must often make general policies th at hope- fully work for all programs, the VMM often doesn’t know too much about what the OS is doing or wanting; this lack of knowledge, sometimes called the information gap between the VMM and the OS, can lead to various inefﬁciencies [B+97].",3384
Appendices. Appendix B Virtual Machine Monitors,"For example, an OS, when it has not hing else to run, will sometimes go into an idle loop just spinning and waiting for the next interrupt to occur: while (1) ; // the idle loop It makes sense to spin like this if the OS in charge of the entire machine and thus knows there is nothing else that needs to run. However, w hen a c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 VIRTUAL MACHINE MONITORS ASIDE : PARA -VIRTUALIZATION In many situations, it is good to assume that the OS cannot be modiﬁe d in order to work better with virtual machine monitors (for example, b ecause you are running your VMM under an unfriendly competitor’s operatin g system). However, this is not always the case, and when the OS ca n be modiﬁed (as we saw in the example with demand-zeroing of pages), it may run more efﬁciently on top of a VMM. Running a modiﬁed OS to run on a VMM is generally called para-virtualization [WSG02], as the virtualization provided by the VMM isn’t a complete one, but rathe r a partial one requiring OS changes to operate effectively. Rese arch shows that a properly-designed para-virtualized system, with jus t the right OS changes, can be made to be nearly as efﬁcient a system without a VMM [BD+03]. VMM is running underneath two different OSes, one in the idle loop and one usefully running user processes, it would be useful for the VM M to know that one OS is idle so it can give more CPU time to the OS doing useful work. Another example arises with demand zeroing of pages. Most oper- ating systems zero a physical frame before mapping it into a pr ocess’s address space. The reason for doing so is simple: security. If th e OS gave one process a page that another had been using without zeroing it, an information leak across processes could occur, thus potentially leak- ing sensitive information. Unfortunately, the VMM must zero pa ges that it gives to each OS, for the same reason, and thus many times a page will be zeroed twice, once by the VMM when assigning it to an OS, and once by the OS when assigning it to a process. The authors of Disco had n o great solution to this problem: they simply changed the OS (IRIX ) to not zero pages that it knew had been zeroed by the underlying VMM [B +97]. There are many other similar problems to these described here. One solution is for the VMM to use inference (a form of implicit information ) to overcome the problem. For example, a VMM can detect the idle loop b y noticing that the OS switched to low-power mode. A different appr oach, seen in para-virtualized systems, requires the OS to be changed. This more explicit approach, while harder to deploy, can be quite eff ective. B.6 Summary Virtualization is in a renaissance. For a multitude of reasons, u sers and administrators want to run multiple OSes on the same machine at the same time. The key is that VMMs generally provide this serv icetrans- parently ; the OS above has little clue that it is not actually controlling t he hardware of the machine. The key method that VMMs use to do so is to extend the notion of limited direct execution; by setting up th e hard- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 11 TIP: USEIMPLICIT INFORMATION Implicit information can be a powerful tool in layered systems whe re it is hard to change the interfaces between systems, but more i nforma- tion about a different layer of the system is needed.",3415
Appendices. Appendix B Virtual Machine Monitors,"For example, a block-based disk device might like to know more about how a ﬁle sys- tem above it is using it; Similarly, an application might want to know what pages are currently in the ﬁle-system page cache, but th e OS pro- vides no API to access this information. In both these cases, res earchers have developed powerful inferencing techniques to gather the needed in- formation implicitly, without requiring an explicit interface between lay- ers [AD+01,S+03]. Such techniques are quite useful in a virtua l machine monitor, which would like to learn more about the OSes running above i t without requiring an explicit API between the two layers. ware to enable the VMM to interpose on key events (such as traps) , the VMM can completely control how machine resources are allocated whi le preserving the illusion that the OS requires. You might have noticed some similarities between what the OS does for processes and what the VMM does for OSes. They both virtualize the hardware after all, and hence do some of the same things. Howe ver, there is one key difference: with the OS virtualization, a numb er of new abstractions and nice interfaces are provided; with VMM-leve l virtual- ization, the abstraction is identical to the hardware (and thu s not very nice). While both the OS and VMM virtualize hardware, they do s o by providing completely different interfaces; VMMs, unlike the OS, are not particularly meant to make the hardware easier to use. There are many other topics to study if you wish to learn more about virtualization. For example, we didn’t even discuss what happe ns with I/O, a topic that has its own new and interesting issues when it c omes to virtualized platforms. We also didn’t discuss how virtualizat ion works when running “on the side” with your OS in what is sometimes calle d a “hosted” conﬁguration. Read more about both of these topics if you’re in - terested [SVL01]. We also didn’t discuss what happens when a col lection of operating systems running on a VMM uses too much memory. Finally, hardware support has changed how platforms support vir tu- alization. Companies like Intel and AMD now include direct supp ort for an extra level of virtualization, thus obviating many of the softw are tech- niques in this chapter. Perhaps, in a chapter yet-to-be-writ ten, we will discuss these mechanisms in more detail. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 VIRTUAL MACHINE MONITORS References [AA06] “A Comparison of Software and Hardware Techniques for x86 Virtualization” Keith Adams and Ole Agesen ASPLOS ’06, San Jose, California A terriﬁc paper from two VMware engineers about the surprisingly small beneﬁts of having hardware support for virtualization. Also an excellent general discussion about vi rtualization in VMware, includ- ing the crazy binary-translation tricks they have to play in order to virtualize the di fﬁcult-to-virtualize x86 platform. [AD+01] “Information and Control in Gray-box Systems” Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau SOSP ’01, Banff, Canada Our own work on how to infer information and even exert control over the OS from app lication level, without any change to the OS.",3185
Appendices. Appendix B Virtual Machine Monitors,"The best example therein: determining whi ch ﬁle blocks are cached in the OS using a probabilistic probe-based technique; doing so allows applicati ons to better utilize the cache, by ﬁrst scheduling work that will result in hits. [B00] “FreeBSD Developers’ Handbook: Chapter 11 x86 Assembly Language Programming” http://www.freebsd.org/doc/en/books/developers-handbook/ A nice tutorial on system calls and such in the BSD developers handbook. [BD+03] “Xen and the Art of Virtualization” Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harri s, Alex Ho, Rolf Neuge- bauer, Ian Pratt, Andrew Warﬁeld SOSP ’03, Bolton Landing, New York The paper that shows that with para-virtualized systems, the overheads of virtuali zed systems can be made to be incredibly low. So successful was this paper on the Xen virtual machine monitor that it launched a company. [B+97] “Disco: Running Commodity Operating Systems on Scalable Multiprocessors” Edouard Bugnion, Scott Devine, Kinshuk Govil, Mendel Rosenblum SOSP ’97 The paper that reintroduced the systems community to virtual machine resear ch; well, perhaps this is unfair as Bressoud and Schneider [BS95] also did, but here we began to u nderstand why virtualization was going to come back. What made it even clearer, however, is when this grou p of excellent researchers started VMware and made some billions of dollars. [BS95] “Hypervisor-based Fault-tolerance” Thomas C. Bressoud, Fred B. Schneider SOSP ’95 One the earliest papers to bring back the hypervisor , which is just another term for a virtual machine monitor. In this work, however, such hypervisors are used to improve system tolerance of hardware faults, which is perhaps less useful than some of the more practical scenari os discussed in this chapter; however, still quite an intriguing paper in its own right. [G74] “Survey of Virtual Machine Research” R.P . Goldberg IEEE Computer, Volume 7, Number 6 A terriﬁc survey of a lot of old virtual machine research. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG VIRTUAL MACHINE MONITORS 13 [SVL01] “Virtualizing I/O Devices on VMware Workstation’s Hosted Virtual Machine Monitor” Jeremy Sugerman, Ganesh Venkitachalam and Beng-Hong Lim USENIX ’01, Boston, Massachusetts Provides a good overview of how I/O works in VMware using a hosted architecture which exploits many native OS features to avoid reimplementing them within the VMM. [V98] VMware corporation. Available: http://www.vmware.com/ This may be the most useless reference in this book, as you can clearly look thi s up yourself. Anyhow, the company was founded in 1998 and is a leader in the ﬁeld of virtualization. [S+03] “Semantically-Smart Disk Systems” Muthian Sivathanu, Vijayan Prabhakaran, Florentina I. Popovici, Timot hy E. Denehy, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau FAST ’03, San Francisco, California, March 2003 Our work again, this time showing how a dumb block-based device can infer mu ch about what the ﬁle system above it is doing, such as deleting a ﬁle. The technology used ther ein enables interesting new functionality within a block device, such as secure delete, or more reli able storage. [WSG02] “Scale and Performance in the Denali Isolation Kernel” Andrew Whitaker, Marianne Shaw, and Steven D. Gribble OSDI ’02, Boston, Massachusetts The paper that introduces the term para-virtualization. Although one can argue that Bu gnion et al. [B+97] introduce the idea of para-virtualization in the Disco paper, Whitaker et al . take it further and show how the idea can be more general than what was thought before. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",3640
Appendix D Monitors Deprecated,"C A Dialogue on Monitors Professor: So it’s you again, huh? Student: I bet you are getting quite tired by now, being so, well you know, old? Not that 50 years old is that old, really. Professor: I’m not 50. I’ve just turned 40, actually. But goodness, I guess to you, being 20-something ... Student: ... 19, actually ... Professor: (ugh) ... yes, 19, whatever, I guess 40 and 50 seem kind of similar. But trust me, they’re not. At least, that’s what my 50-year old frie nds tell me. Student: Anyhow ... Professor: Ah yes. What are we talking about again? Student: Monitors. Not that I know what a monitor is, except for some kind of old-fashioned name for the computer display sitting in front of me. Professor: Yes, this is a whole different type of thing. It’s an old concurrency primitive, designed as a way to incorporate locking automatically into o bject- oriented programs. Student: Why not include it in the section on concurrency then? Professor: Well, most of the book is about C programming and the POSIX threads libraries, where there are no monitors, so there’s that. But there are some historical reasons to at least include the information on the topic, so here it is, I guess. Student: Ah, history. That’s for old people, like you, right? Professor: (glares) Student: Oh take it easy. I kid. Professor: I can’t wait until you take the ﬁnal exam... 1 D Monitors (Deprecated) Around the time concurrent programming was becoming a big deal, ob ject- oriented programming was also gaining ground. Not surprisingly , peo- ple started to think about ways to merge synchronization into a m ore structured programming environment. One such approach that emerged was the monitor . First described by Per Brinch Hansen [BH73] and later reﬁned by Tony Hoare [H74], t he idea behind a monitor is quite simple. Consider the following pret end monitor written in C++ notation: monitor class account { private: int balance = 0; public: void deposit(int amount) { balance = balance + amount; } void withdraw(int amount) { balance = balance - amount; } }; Figure D.1: A Pretend Monitor Class Note: this is a “pretend” class because C++ does not support moni- tors, and hence the monitor keyword does not exist. However, Java does support monitors, with what are called synchronized methods. Below, we will examine both how to make something quite like a monitor in C/C++, as well as how to use Java synchronized methods. In this example, you may notice we have our old friend the account and some routines to deposit and withdraw an amount from the balanc e. As you also may notice, these are critical sections ; if they are called by multiple threads concurrently, you have a race condition and the poten- tial for an incorrect outcome. In a monitor class, you don’t get into trouble, though, because the monitor guarantees that only one thread can be active within the mon- itor at a time . Thus, our above example is a perfectly safe and working 1 2 MONITORS (DEPRECATED ) piece of code; multiple threads can call deposit() or withdraw() and know that mutual exclusion is preserved.",3084
Appendix D Monitors Deprecated,"How does the monitor do this? Simple: with a lock. Whenever a thread tries to call a monitor routine, it implicitly tries to ac quire the mon- itor lock. If it succeeds, then it will be able to call into the rou tine and run the method’s code. If it does not, it will block until the thread that is in the monitor ﬁnishes what it is doing. Thus, if we wrote a C++ class t hat looked like the following, it would accomplish the exact same goal as the monitor class above: class account { private: int balance = 0; pthread_mutex_t monitor; public: void deposit(int amount) { pthread_mutex_lock(&monitor); balance = balance + amount; pthread_mutex_unlock(&monitor); } void withdraw(int amount) { pthread_mutex_lock(&monitor); balance = balance - amount; pthread_mutex_unlock(&monitor); } }; Figure D.2: A C++ Class that acts like a Monitor Thus, as you can see from this example, the monitor isn’t doing too much for you automatically. Basically, it is just acquiring a loc k and re- leasing it. By doing so, we achieve what the monitor requires: only one thread will be active within deposit() or withdraw(), as desir ed. D.1 Why Bother with Monitors? You might wonder why monitors were invented at all, instead of just using explicit locking. At the time, object-oriented programmi ng was just coming into fashion. Thus, the idea was to gracefully blen d some of the key concepts in concurrent programming with some of the basic approaches of object orientation. Nothing more than that. D.2 Do We Get More Than Automatic Locking? Back to business. As we know from our discussion of semaphores, just having locks is not quite enough; for example, to implement t he pro- ducer/consumer solution, we previously used semaphores to both pu t threads to sleep when waiting for a condition to change (e.g., a p roducer waiting for a buffer to be emptied), as well as to wake up a threa d when a particular condition has changed (e.g., a consumer signaling that it has indeed emptied a buffer). OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 3 monitor class BoundedBuffer { private: int buffer[MAX]; int fill, use; int fullEntries = 0; cond_t empty; cond_t full; public: void produce(int element) { if (fullEntries == MAX) // line P0 wait(&empty); // line P1 buffer[fill] = element; // line P2 fill = (fill + 1)  percent MAX; // line P3 fullEntries++; // line P4 signal(&full); // line P5 } int consume() { if (fullEntries == 0) // line C0 wait(&full); // line C1 int tmp = buffer[use]; // line C2 use = (use + 1)  percent MAX; // line C3 fullEntries--; // line C4 signal(&empty); // line C5 return tmp; // line C6 } } Figure D.3: Producer/Consumer with Monitors and Hoare Semantics Monitors support such functionality through an explicit construc t known as a condition variable . Let’s take a look at the producer/consumer so- lution, here written with monitors and condition variables. In this monitor class, we have two routines, produce() and consume (). A producer thread would repeatedly call produce() to put data in to the bounded buffer, while a consumer() would repeatedly call consum e().",3116
Appendix D Monitors Deprecated,"The example is a modern paraphrase of Hoare’s solution [H74]. You should notice some similarities between this code and the sema phore- based solution in the previous note. One major difference is how cond i- tion variables must be used in concert with an explicit state variable ; in this case, the integer fullEntries determines whether a producer or consumer must wait, depending on its state. Semaphores, in contra st, have an internal numeric value which serves this same purpose . Thus, condition variables must be paired with some kind of external sta te value in order to achieve the same end. The most important aspect of this code, however, is the use of the two condition variables, empty and full, and the respective wait() and signal() calls that employ them. These operations do exactly what you might think: wait() blocks the calling thread on a given condi tion; signal() wakes one waiting thread that is waiting on the condition. However, there are some subtleties in how these calls operate; un der- standing the semantics of these calls is critically important to understand- c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 MONITORS (DEPRECATED ) ing why this code works. In what researchers in operating system s call Hoare semantics (yes, a somewhat unfortunate name), the signal() immediately wakes one waiting thread and runs it; thus, the mon itor lock, which is implicitly held by the running thread, immedia tely is trans- ferred to the woken thread which then runs until it either block s or ex- its the monitor. Note that there may be more than one thread waiting ; signal() only wakes one waiting thread and runs it, while the others must wait for a subsequent signal. A simple example will help us understand this code better. Ima gine there are two threads, one a producer and the other a consumer. The con- sumer gets to run ﬁrst, and calls consume() , only to ﬁnd that fullEntries = 0(C0) , as there is nothing in the buffer yet. Thus, it calls wait(&full) (C1) , and waits for a buffer to be ﬁlled. The producer then runs, ﬁnds it doesn’t have to wait (P0) , puts an element into the buffer (P2) , in- crements the ﬁll index (P3) and the fullEntries count (P4) , and calls signal(&full)(P5) . In Hoare semantics, the producer does not con- tinue running after the signal; rather, the signal immediat ely transfers control to the waiting consumer, which returns from wait() (C1) and immediately consumes the element produced by the producer (C2) and so on. Only after the consumer returns will the producer get to ru n again and return from the produce() routine. D.3 Where Theory Meets Practice Tony Hoare, who wrote the solution above and came up with the ex- act semantics for signal() andwait() , was a theoretician. Clearly a smart guy, too; he came up with quicksort after all [H61]. However , the semantics of signaling and waiting, as it turns out, were not ide al for a real implementation. As the old saying goes, in theory, there is n o differ- ence between theory and practice, but in practice, there is.",3060
Appendix D Monitors Deprecated,"OLDSAYING : THEORY VS . PRACTICE The old saying is “in theory, there is no difference between the ory and practice, but in practice, there is.” Of course, only practiti oners tell you this; a theory person could undoubtedly prove that it is not true. A few years later, Butler Lampson and David Redell of Xerox PARC were building a concurrent language known as Mesa , and decided to use monitors as their basic concurrency primitive [LR80]. They wer e well- known systems researchers, and they soon found that Hoare semanti cs, while more amenable to proofs, were hard to realize in a real syst em (there are a lot of reasons for this, perhaps too many to go through he re). In particular, to build a working monitor implementation, Lamps on and Redell decided to change the meaning of signal() in a subtl e but crit- ical way. The signal() routine now was just considered a hint [L83]; it OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 5 would move a single waiting thread from the blocked state to a runn able state, but it would not run it immediately. Rather, the signali ng thread would retain control until it exited the monitor and was deschedul ed. D.4 Oh Oh, A Race Given these new Mesa semantics , let us again reexamine the code above. Imagine again a consumer (consumer 1) who enters the moni- tor and ﬁnds the buffer empty and thus waits (C1) . Now the producer comes along and ﬁlls the buffer and signals that a buffer has bee n ﬁlled, moving the waiting consumer from blocked on the full condition varia ble to ready. The producer keeps running for a while, and eventuall y gives up the CPU. But Houston, we have a problem. Can you see it? Imagine a differ- ent consumer (consumer 2) now calls into the consume() routine; it will ﬁnd a full buffer, consume it, and return, setting fullEntrie s to 0 in the meanwhile. Can you see the problem yet? Well, here it comes. Our ol d friend consumer 1 now ﬁnally gets to run, and returns from wait() , ex- pecting a buffer to be full (C1...) ; unfortunately, this is no longer true, as consumer 2 snuck in and consumed the buffer before consumer 1 ha d a chance to consume it. Thus, the code doesn’t work, because in the t ime between the signal() by the producer and the return from wait() by con- sumer 1, the condition has changed. This timeline illustrates the problem: Producer Consumer1 Consumer2 C0 (fullEntries=0) C1 (Consumer 1: blocked) P0 (fullEntries=0) P2 P3 P4 (fullEntries=1) P5 (Consumer1: ready) C0 (fullEntries=1) C2 C3 C4 (fullEntries=0) C5 C6 C2 (using a buffer, fullEntries=0.) Figure D.4: Why the Code doesn’t work with Hoare Semantics Fortunately, the switch from Hoare semantics to Mesa semantics re- quires only a small change by the programmer to realize a working so- lution. Speciﬁcally, when woken, a thread should recheck the condition it was waiting on; because signal() is only a hint, it is possible that the condition has changed (even multiple times) and thus may not be i n the desired state when the waiting thread runs.",3039
Appendix D Monitors Deprecated,"In our example, tw o lines of code must change, lines P0 and C0: c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 MONITORS (DEPRECATED ) public: void produce(int element) { while (fullEntries == MAX) // line P0 (CHANGED IF->WHILE) wait(&empty); // line P1 buffer[fill] = element; // line P2 fill = (fill + 1)  percent MAX; // line P3 fullEntries++; // line P4 signal(&full); // line P5 } int consume() { while (fullEntries == 0) // line C0 (CHANGED IF->WHILE) wait(&full); // line C1 int tmp = buffer[use]; // line C2 use = (use + 1)  percent MAX; // line C3 fullEntries--; // line C4 signal(&empty); // line C5 return tmp; // line C6 } Figure D.5: Producer/Consumer with Monitors and Mesa Semantics Not too hard after all. Because of the ease of this implementation, virtually any system today that uses condition variables with s ignaling and waiting uses Mesa semantics. Thus, if you remember nothing else at all from this class, you can just remember: always recheck the condition after being woken. Put in even simpler terms, use while loops and not if statements when checking conditions. Note that this is alway s correct, even if somehow you are running on a system with Hoare semantics; in that case, you would just needlessly retest the condition an extr a time. D.5 Peeking Under The Hood A Bit To understand a bit better why Mesa semantics are easier to im ple- ment, let’s understand a little more about the implementation of M esa monitors. In their work [LR80], Lampson and Redell describe thre e differ- ent types of queues that a thread can be a part of at a given time: t heready queue, a monitor lock queue, and a condition variable queue. Note that a program might have multiple monitor classes and multiple condi tion variable instances; there is a queue per instance of said item s. With a single bounded buffer monitor, we thus have four queues to consider: the ready queue, a single monitor queue, and two condit ion variable queues (one for the full condition and one for the empty). T o better understand how a thread library manages these queues, what we will do is show how a thread transitions through these queues in th e pro- ducer/consumer example. In this example, we walk through a case where a consumer might be woken up but ﬁnd that there is nothing to consume. Let us consider t he following timeline. On the left are two consumers (Con1 and Con2) a nd a producer (Prod) and which line of code they are executing; on the ri ght is the state of each of the four queues we are following for this example : OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 7 t | Con1 Con2 Prod | Mon | Empty | Full | FE | Comment --------------------------------------------------- --------------------- 0 C0 0 1 C1 Con1 0 Con1 waiting on full 2 <Context switch> Con1 0 switch: Con1 to Prod 3 P0 Con1 0 4 P2 Con1 0 Prod doesn’t wait (FE=0) 5 P3 Con1 0 6 P4 Con1 1 Prod updates fullEntries 7 P5 1 Prod signals: Con1 now ready 8 <Context switch> 1 switch: Prod to Con2 9 C0 1 switch to Con2 10 C2 1 Con2 doesn’t wait (FE=1) 11 C3 1 12 C4 0 Con2 changes fullEntries 13 C5 0 Con2 signals empty (no waiter) 14 C6 0 Con2 done 15 <Context switch> 0 switch: Con2 to Con1 16 C0 0 recheck fullEntries: 0. 17 C1 Con1 0 wait on full again Figure D.6: Tracing Queues during a Producer/Consumer Run the ready queue of runnable processes, the monitor lock queue call ed Monitor, and the empty and full condition variable queues. We als o track time (t), the thread that is running (square brackets around t he thread on the ready queue that is running), and the value of fullEntries (FE).",3623
Appendix D Monitors Deprecated,"As you can see from the timeline, consumer 2 (Con2) sneaks in and consumes the available data (t=9..14) before consumer 1 (Con1), who was waiting on the full condition to be signaled (since t=1), gets a c hance to do so. However, Con1 does get woken by the producer’s signal (t=7), and thus runs again even though the buffer is empty by the time i t does so. If Con1 didn’t recheck the state variable fullEntries (t=16 ), it would have erroneously tried to consume data when no data was present t o consume. Thus, this natural implementation is exactly what le ads us to Mesa semantics (and not Hoare). D.6 Other Uses Of Monitors In their paper on Mesa, Lampson and Redell also point out a few places where a different kind of signaling is needed. For examp le, con- sider the following memory allocator (Figure D.7). Many details are left out of this example, in order to allow us to foc us on the conditions for waking and signaling. It turns out the signal /wait code above does not quite work; can you see why? Imagine two threads call allocate. The ﬁrst calls allocate(20 ) and the second allocate(10). No memory is available, and thus both threa ds call wait() and block. Some time later, a different thread comes along a nd calls free(p, 15), and thus frees up 15 bytes of memory. It then signal s that it has done so. Unfortunately, it wakes the thread waiting for 20 byt es; that thread rechecks the condition, ﬁnds that only 15 bytes are avail able, and c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 MONITORS (DEPRECATED ) monitor class allocator { int available; // how much memory is available? cond_t c; void*allocate(int size) { while (size > available) wait(&c); available -= size; // and then do whatever the allocator should do // and return a chunk of memory } void free(void *pointer, int size) { // free up some memory available += size; signal(&c); } }; Figure D.7: A Simple Memory Allocator calls wait() again. The thread that could have beneﬁted from th e free of 15 bytes, i.e., the thread that called allocate(10), is not woke n. Lampson and Redell suggest a simple solution to this problem. In- stead of a signal() which wakes a single waiting thread, they e mploy a broadcast() which wakes allwaiting threads. Thus, all threads are woken up, and in the example above, the thread waiting for 10 bytes wil l ﬁnd 15 available and succeed in its allocation. In Mesa semantics, using a broadcast() is always correct, as all threads should recheck the condition of interest upon waking anyhow. Howeve r, it may be a performance problem, and thus should only be used when needed. In this example, a broadcast() might wake hundreds of w aiting threads, only to have one successfully continue while the rest i mmedi- ately block again; this problem, sometimes known as a thundering herd , is costly, due to all the extra context switches that occur. D.7 Using Monitors To Implement Semaphores You can probably see a lot of similarities between monitors and sema phores. Not surprisingly, you can use one to implement the other.",3058
Appendix D Monitors Deprecated,"Here, we show how you might implement a semaphore class using a monitor (Figure D.8). As you can see, wait() simply waits for the value of the semaphore t o be greater than 0, and then decrements its value, whereas post () incre- ments the value and wakes one waiting thread (if there is one). I t’s as simple as that. To use this class as a binary semaphore (i.e., a lock), you just in itialize the semaphore to 1, and then put wait()/post() pairs around crit ical sec- tions. And thus we have shown that monitors can be used to implemen t semaphores. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 9 monitor class Semaphore { int s; // value of the semaphore Semaphore(int value) { s = value; } void wait() { while (s <= 0) wait(); s--; } void post() { s++; signal(); } }; Figure D.8: Implementing a Semaphore with a Monitor D.8 Monitors in the Real World We already mentioned above that we were using “pretend” monitors ; C++ has no such concept. We now show how to make a monitor-like C++ class, and how Java uses synchronized methods to achieve a simil ar end. A C++ Monitor of Sorts Here is the producer/consumer code written in C++ with locks and c on- dition variables (Figure D.9). You can see in this code example t hat there is little difference between the pretend monitor code and the wor king C++ class we have above. Of course, one obvious difference is the ex plicit use of a lock ”monitor”. More subtle is the switch to the POSIX standa rd pthread condsignal() andpthread condwait() calls. In partic- ular, notice that when calling pthread condwait() , one also passes in the lock that is held at the time of waiting. The lock is needed i nside pthread condwait() because it must be released when this thread is put to sleep and re-acquired before it returns to the caller (t he same be- havior as within a monitor but again with explicit locks). A Java Monitor Interestingly, the designers of Java decided to use monitors as they thought they were a graceful way to add synchronization primitives int o a lan- guage. To use them, you just use add the keyword synchronized to the method or set of methods that you wish to use as a monitor (here is an example from Sun’s own documentation site [S12a,S12b]): This code does exactly what you think it should: provide a counter that is thread safe. Because only one thread is allowed into the m onitor at a time, only one thread can update the value of ”c”, and thus a ra ce condition is averted. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 MONITORS (DEPRECATED ) class BoundedBuffer { private: int buffer[MAX]; int fill, use; int fullEntries; pthread_mutex_t monitor; // monitor lock pthread_cond_t empty; pthread_cond_t full; public: BoundedBuffer() { use = fill = fullEntries = 0; } void produce(int element) { pthread_mutex_lock(&monitor); while (fullEntries == MAX) pthread_cond_wait(&empty, &monitor); buffer[fill] = element; fill = (fill + 1)  percent MAX; fullEntries++; pthread_cond_signal(&full); pthread_mutex_unlock(&monitor); } int consume() { pthread_mutex_lock(&monitor); while (fullEntries == 0) pthread_cond_wait(&full, &monitor); int tmp = buffer[use]; use = (use + 1)  percent MAX; fullEntries--; pthread_cond_signal(&empty); pthread_mutex_unlock(&monitor); return tmp; } } Figure D.9: C++ Producer/Consumer with a “Monitor” Java and the Single Condition Variable In the original version of Java, a condition variable was also supp lied with each synchronized class.",3489
Appendix D Monitors Deprecated,"To use it, you would call either wait() ornotify() (sometimes the term notify is used instead of signal, but they me an the same thing). Oddly enough, in this original implementation, th ere was no way to have two (or more) condition variables. You may have noticed i n the producer/consumer solution, we always use two: one for signalin g a buffer has been emptied, and another for signaling that a buffe r has been ﬁlled. To understand the limitations of only providing a single condition variable, let’s imagine the producer/consumer solution with only a sin- gle condition variable. Imagine two consumers run ﬁrst, and both get stuck waiting. Then, a producer runs, ﬁlls a single buffer, wa kes a single OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 11 public class SynchronizedCounter { private int c = 0; public synchronized void increment() { c++; } public synchronized void decrement() { c--; } public synchronized int value() { return c; } } Figure D.10: A Simple Java Class with Synchronized Methods consumer, and then tries to ﬁll again but ﬁnds the buffer full ( MAX=1). Thus, we have a producer waiting for an empty buffer, a consumer w ait- ing for a full buffer, and a consumer who had been waiting about to run because it has been woken. The consumer then runs and consumes the buffer. When it calls no- tify(), though, it wakes a single thread that is waiting on the c ondition. Because there is only a single condition variable, the consumer m ight wake the waiting consumer, instead of the waiting producer. Thus, the solution does not work. To remedy this problem, one can again use the broadcast solution. I n Java, one calls notifyAll() to wake all waiting threads. In this case, the consumer would wake a producer and a consumer, but the consumer would ﬁnd that fullEntries is equal to 0 and go back to sleep, wh ile the producer would continue. As usual, waking all waiters can lead t o the thundering herd problem. Because of this deﬁciency, Java later added an explicit Condit ion class, thus allowing for a more efﬁcient solution to this and other similar con- currency problems. D.9 Summary We have seen the introduction of monitors, a structuring concept de - veloped by Brinch Hansen and and subsequently Hoare in the earl y sev- enties. When running inside the monitor, a thread implicitly h olds a mon- itor lock, and thus prevents other threads from entering the monit or, al- lowing the ready construction of mutual exclusion. We also have seen the introduction of explicit condition variable s, which allow threads to signal() and wait() much like we saw with sema phores in the previous note. The semantics of signal() and wait() are cr itical; be- cause all modern systems implement Mesa semantics, a recheck of the condition that the thread went to sleep on is required for correct e xecu- tion. Thus, signal() is just a hint that something has changed; it is the responsibility of the woken thread to make sure the conditions are right c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 MONITORS (DEPRECATED ) for its continued execution.",3112
Appendix D Monitors Deprecated,"Finally, because C++ has no monitor support, we saw how to emulate monitors with explicit pthread locks and condition variables. We also saw how Java supports monitors with its synchronized routines, and some of the limitations of only providing a single condition variable in su ch an environment. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG MONITORS (DEPRECATED ) 13 References [BH73] “Operating System Principles” Per Brinch Hansen, Prentice-Hall, 1973 Available: http://portal.acm.org/citation.cfm?id=540365 One of the ﬁrst books on operating systems; certainly ahead of its time. Introduced m onitors as a concurrency primitive. [H74] “Monitors: An Operating System Structuring Concept” C.A.R. Hoare CACM, Volume 17:10, pages 549–557, October 1974 An early reference to monitors; however, Brinch Hansen probably was the tr ue inventor. [H61] “Quicksort: Algorithm 64” C.A.R. Hoare CACM, Volume 4:7, July 1961 The famous quicksort algorithm. [LR80] “Experience with Processes and Monitors in Mesa” B.W. Lampson and D.R. Redell CACM, Volume 23:2, pages 105–117, February 1980 An early and important paper highlighting the differences between theor y and practice. [L83] “Hints for Computer Systems Design” Butler Lampson ACM Operating Systems Review, 15:5, October 1983 Lampson, a famous systems researcher, loved using hints in the design of computer systems. A hint is something that is often correct but can be wrong; in this use, a signal() is te lling a waiting thread that it changed the condition that the waiter was waiting on, but not to trust that the condition will be in the desired state when the waiting thread wakes up. In this paper about hints for desi gning systems, one of Lampson’s general hints is that you should use hints. It is not as confusing as it sounds. [S12a] “Synchronized Methods” Sun documentation http://java.sun.com/docs/books/tutorial/essential/concurrency /syncmeth.html [S12b] “Condition Interface” Sun documentation http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurre nt/locks/Condition.html c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",2102
Appendix E A Dialogue on Labs,"E A Dialogue on Labs Student: Is this our ﬁnal dialogue? Professor: I hope so. You’ve been becoming quite a pain, you know. Student: Yes, I’ve enjoyed our conversations too. What’s up here? Professor: It’s about the projects you should be doing as you learn this materia l; you know, actual programming, where you do some real work inste ad of this incessant talking and reading. The real way to learn. Student: Sounds important. Why didn’t you tell me earlier? Professor: Well, hopefully those using this book actually do look at this part earlier, all throughout the course. If not, they’re really missing so mething. Student: Seems like it. So what are the projects like? Professor: Well, there are two types of projects. The ﬁrst set are what you migh t callsystems programming projects, done on machines running Linux and in the C programming environment. This type of programming is quite us eful to know, as when you go off into the real world, you very well might have to do some of this type of hacking yourself. Student: What’s the second type of project? Professor: The second type is based inside a real kernel, a cool little teaching kernel developed at MIT called xv6. It is a “port” of an old version of UNIX to Intel x86, and is quite neat. With these projects, instead of writ ing code that interacts with the kernel (as you do in systems programming), you actually get to re-write parts of the kernel itself. Student: Sounds fun. So what should we do in a semester? You know, there are only so many hours in the day, and as you professors seem to forget, we students take four or ﬁve courses, not just yours. Professor: Well, there is a lot of ﬂexibility here. Some classes just do all systems programming, because it is so practical. Some classes do all xv6 hack ing, because it really gets you to see how operating systems work. And some, as y ou may have guessed, do a mix, starting with some systems programming, and th en doing xv6 at the end. It’s really up to the professor of a particular class. 1 2 A D IALOGUE ON LABS Student: (sighing) Professors have all the control, it seems... Professor: Oh, hardly. But that little control they do get to exercise is one of the fun parts of the job. Deciding on assignments is important you kno w — and not something any professor takes lightly. Student: Well, that is good to hear. I guess we should see what these projects are all about... Professor: OK. And one more thing: if you’re interested in the systems pro- gramming part, there is also a little tutorial about the UNIXand C programming environment. Student: Sounds almost too useful to be true. Professor: Well, take a look. You know, classes are supposed to be about useful things, sometimes. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2770
Appendix F Laboratory - Tutorial,"F Laboratory: Tutorial This is a very brief document to familiarize you with the basics of the C programming environment on U NIXsystems. It is not comprehensive or particularly detailed, but should just give you enough to get you going. A couple of general points of advice about programming: if you want to become an expert programmer, you need to master more than just t he syntax of a language. Speciﬁcally, you should know your tools ,know your libraries , and know your documentation . The tools that are rel- evant to C compilation are gcc,gdb, and maybe ld. There are tons of library routines that are also available to you, but fortunatel y a lot of functionality is included in libc, which is linked with all C programs by default — all you need to do is include the right header ﬁles. Fi nally, knowing how to ﬁnd the library routines you need (e.g., learning t o ﬁnd and read man pages) is a skill worth acquiring. We’ll talk about e ach of these in more detail later on. Like (almost) everything worth doing in life, becoming an expert in these domains takes time. Spending the time up-front to learn mor e about the tools and environment is deﬁnitely well worth the effort. F.1 A Simple C Program We’ll start with a simple C program, perhaps saved in the ﬁle “hw .c”. Unlike Java, there is not necessarily a connection between the ﬁ le name and the contents of the ﬁle; thus, use your common sense in naming ﬁl es in a manner that is appropriate. The ﬁrst line speciﬁes a ﬁle to include, in this case stdio.h , which “prototypes” many of the commonly used input/output routines; the one we are interested in is printf() . When you use the #include di- rective, you are telling the C preprocessor ( cpp) to ﬁnd a particular ﬁle (e.g.,stdio.h ) and to insert it directly into your code at the spot of the #include . By default, cpp will look in the directory /usr/include/ to try to ﬁnd the ﬁle. The next part speciﬁes the signature of the main() routine, namely that it returns an integer ( int), and will be called with two arguments, 1 2 LABORATORY : TUTORIAL /*header files go up here */ /*note that C comments are enclosed within a slash and a star, an d may wrap over lines */ // if you use gcc, two slashes will work too (and may be preferr ed) #include <stdio.h> /*main returns an integer */ int main(int argc, char *argv[]) { /*printf is our output function; by default, writes to standard out */ /*printf returns an integer, but we ignore that */ printf(\""hello, world \""); /*return 0 to indicate all went well */ return(0); } an integer argc , which is a count of the number of arguments on the com- mand line, and an array of pointers to characters ( argv ), each of which contain a word from the command line, and the last of which is null. There will be more on pointers and arrays below. The program then simply prints the string “hello, world” and ad- vances the output stream to the next line, courtesy of the backsl ash fol- lowed by an “n” at the end of the call to printf() . Afterwards, the pro- gram completes by returning a value, which is passed back to th e shell that executed the program. A script or the user at the terminal c ould check this value (in csh and tcsh shells, it is stored in the status vari- able), to see whether the program exited cleanly or with an error .",3314
Appendix F Laboratory - Tutorial,"F.2 Compilation and Execution We’ll now learn how to compile the program. Note that we will use gcc as our example, though on some platforms you may be able to use a different (native) compiler, cc. At the shell prompt, you just type: prompt> gcc hw.c gcc is not really the compiler, but rather the program called a “com- piler driver”; thus it coordinates the many steps of the compilat ion. Usu- ally there are four to ﬁve steps. First, gcc will execute cpp, the C pre- processor, to process certain directives (such as #define and#include . The program cpp is just a source-to-source translator, so its end-product is still just source code (i.e., a C ﬁle). Then the real compilati on will begin, usually a command called cc1. This will transform source-level C code into low-level assembly code, speciﬁc to the host machine. The a ssem- bleraswill then be executed, generating object code (bits and things that machines can really understand), and ﬁnally the link-editor (or linker) ld will put it all together into a ﬁnal executable program. Fortuna tely(.), for most purposes, you can blithely be unaware of how gcc works, and just use it with the proper ﬂags. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : TUTORIAL 3 The result of your compilation above is an executable, named (by de - fault)a.out . To then run the program, we simply type: prompt> ./a.out When we run this program, the OS will set argc and argv properly so that the program can process the command-line arguments as ne ed be. Speciﬁcally, argc will be equal to 1,argv[0] will be the string “./a.out”, andargv[1] will be null, indicating the end of the array. F.3 Useful Flags Before moving on to the C language, we’ll ﬁrst point out some useful compilation ﬂags for gcc. prompt> gcc -o hw hw.c # -o: to specify the executable name prompt> gcc -Wall hw.c # -Wall: gives much better warnings prompt> gcc -g hw.c # -g: to enable debugging with gdb prompt> gcc -O hw.c # -O: to turn on optimization Of course, you may combine these ﬂags as you see ﬁt (e.g., gcc -o hw -g -Wall hw.c ). Of these ﬂags, you should always use -Wall , which gives you lots of extra warnings about possible mistakes. Don’t ignore the warnings. Instead, ﬁx them and thus make them blissfully disappear. F.4 Linking with Libraries Sometimes, you may want to use a library routine in your program. Because so many routines are available in the C library (which is auto- matically linked with every program), all you usually have to d o is ﬁnd the right#include ﬁle. The best way to do that is via the manual pages , usually just called the man pages . For example, let’s say you want to use the fork() system call1. By typingman fork at the shell prompt, you will get back a text description of howfork() works. At the very top will be a short code snippet, and that will tell you which ﬁles you need to #include in your program in order to get it to compile. In the case of fork() , you need to #include bothsys/types.h andunistd.h , which would be accomplished as follows: #include <sys/types.h> #include <unistd.h> 1Note thatfork() is a system call, and not just a library routine.",3146
Appendix F Laboratory - Tutorial,"However, the C libr ary provides C wrappers for all the system calls, each of which simply tr ap into the operating system. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LABORATORY : TUTORIAL However, some library routines do not reside in the C library, and therefore you will have to do a little more work. For example, the ma th library has many useful routines, such as sines, cosines, tang ents, and the like. If you want to include the routine tan() in our code, you should again ﬁrst check the man page. At the top of the Linux man page for t an, you will see the following two lines: #include <math.h> ... Link with -lm. The ﬁrst line you already should understand — you need to #include the math library, which is found in the standard location in the ﬁ le system (i.e.,/usr/include/math.h ). However, what the next line is telling you is how to “link” your program with the math library. A number of useful libraries exist and can be linked with; many of those re side in /usr/lib ; it is indeed where the math library is found. There are two types of libraries: statically-linked librari es (which end in.a), and dynamically-linked ones (which end in .so). Statically-linked libraries are combined directly into your executable; that is , the low-level code for the library is inserted into your executable by the link er, and re- sults in a much larger binary object. Dynamic linking improves on this by just including the reference to a library in your program exe cutable; when the program is run, the operating system loader dynamicall y links in the library. This method is preferred over the static approac h because it saves disk space (no unnecessarily large executables are made) and al- lows applications to share library code and static data in memory . In the case of the math library, both static and dynamic versions are av ailable, with the static version called /usr/lib/libm.a and the dynamic one /usr/lib/libm.so . In any case, to link with the math library, you need to specify t he li- brary to the link-editor; this can be achieved by invoking gcc with the right ﬂags. prompt> gcc -o hw hw.c -Wall -lm The-lXXX ﬂag tells the linker to look for libXXX.so orlibXXX.a , probably in that order. If for some reason you insist on the static lib rary over the dynamic one, there is another ﬂag you can use — see you if you can ﬁnd out what it is. People sometimes prefer the static vers ion of a library because of the slight performance cost associated wit h using dynamic libraries. One ﬁnal note: if you want the compiler to search for headers in a di f- ferent path than the usual places, or want it to link with libra ries that you specify, you can use the compiler ﬂag -I/foo/bar to look for headers in the directory /foo/bar , and the-L/foo/bar ﬂag to look for libraries in the/foo/bar directory. One common directory to specify in this manner is “.” (called “dot”), which is U NIX shorthand for the current directory. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : TUTORIAL 5 Note that the -Iﬂag should go on a compile line, and the -Lﬂag on the link line.",3117
Appendix F Laboratory - Tutorial,"F.5 Separate Compilation Once a program starts to get large enough, you may want to split it into separate ﬁles, compiling each separately, and then li nk them to- gether. For example, say you have two ﬁles, hw.c andhelper.c , and you wish to compile them individually, and then link them togeth er. # we are using -Wall for warnings, -O for optimization prompt> gcc -Wall -O -c hw.c prompt> gcc -Wall -O -c helper.c prompt> gcc -o hw hw.o helper.o -lm The-cﬂag tells the compiler just to produce an object ﬁle — in this case, ﬁles called hw.o andhelper.o . These ﬁles are not executables, but just machine-level representations of the code within each source ﬁle. To combine the object ﬁles into an executable, you have to “l ink” them together; this is accomplished with the third line gcc -o hw hw.o helper.o ). In this case, gcc sees that the input ﬁles speciﬁed are not source ﬁles ( .c), but instead are object ﬁles ( .o), and therefore skips right to the last step and invoked the link-editor ldto link them together into a single executable. Because of its function, this line is often c alled the “link line”, and would be where you specify link-speciﬁc commands suc h as -lm. Analogously, ﬂags such as -Wall and-Oare only needed in the compile phase, and therefore need not be included on the link line but rather only on compile lines. Of course, you could just specify all the C source ﬁles on a single li ne togcc (gcc -Wall -O -o hw hw.c helper.c ), but this requires the system to recompile every source-code ﬁle, which can be a time-c onsuming process. By compiling each individually, you can save time by onl y re- compiling those ﬁles that have changed during your editing, and thus increase your productivity. This process is best managed by anot her pro- gram,make , which we now describe. F.6 Makeﬁles The program make lets you automate much of your build process, and is thus a crucially important tool for any serious program (and p ro- grammer). Let’s take a look at a simple example, saved in a ﬁle cal led Makefile . To build your program, now all you have to do is type: prompt> make This will (by default) look for Makefile ormakefile , and use that as its input (you can specify a different makeﬁle with a ﬂag; re ad the c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 6 LABORATORY : TUTORIAL hw: hw.o helper.o gcc -o hw hw.o helper.o -lm hw.o: hw.c gcc -O -Wall -c hw.c helper.o: helper.c gcc -O -Wall -c helper.c clean: rm -f hw.o helper.o hw man pages to ﬁnd out which). The gnu version of make ,gmake , is more fully featured than traditional make, so we will focus upon it for the rest of this discussion (though we will use the two terms interchange ably). Most of these notes are based on the gmake info page; to see how to ﬁnd those pages, see the Documentation section below. Also note: on Linu x systems,gmake andmake are one and the same. Makeﬁles are based on rules, which are used to decide what need s to happen. The general form of a rule: target: prerequisite1 prerequisite2 ...",3032
Appendix F Laboratory - Tutorial,"command1 command2 ... Atarget is usually the name of a ﬁle that is generated by a program; examples of targets are executable or object ﬁles. A target can a lso be the name of an action to carry out, such as “clean” in our example. Aprerequisite is a ﬁle that is used as input to create the target. A target often depends on several ﬁles. For example, to build the e xecutable hw, we need two object ﬁles to be built ﬁrst: hw.o andhelper.o . Finally, a command is an action that make carries out. A rule may have more than one command, each on its own line. Important: You have to put a single tab character at the beginning of every command lin e. If you just put spaces, make will print out some obscure error message and exit. Usually a command is in a rule with prerequisites and serves to cre- ate a target ﬁle if any of the prerequisites change. However, th e rule that speciﬁes commands for the target need not have prerequisites. F or ex- ample, the rule containing the delete command associated with t he target “clean” does not have prerequisites. Going back to our example, when make is executed, it roughly works like this: First, it comes to the target hw, and it realizes that to build it, it must have two prerequisites, hw.o andhelper.o . Thus,hwdepends on those two object ﬁles. Make then will examine each of those target s. In examining hw.o , it will see that it depends on hw.c . Here is the key: if hw.c has been modiﬁed more recently than hw.o has been created, make will know that hw.o is out of date and should be generated anew; in that case, it will execute the command line, gcc -O -Wall -c hw.c , which generates hw.o . Thus, if you are compiling a large program, make will know which object ﬁles need to be re-generated based on their dep en- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : TUTORIAL 7 dencies, and will only do the necessary amount of work to recreate the executable. Also note that hw.o will be created in the case that it does not exist at all. Continuing along, helper.o may also be regenerated or created, based on the same criteria as deﬁned above. When both of the object ﬁles have been created, make is now ready to execute the command to cr e- ate the ﬁnal executable, and goes back and does so: gcc -o hw hw.o helper.o -lm . Up until now, we’ve been ignoring the clean target in the makeﬁle. To use it, you have to ask for it explicitly. Type prompt> make clean This will execute the command on the command line. Because there are no prerequisites for the clean target, typing make clean will al- ways result in the command(s) being executed. In this case, th eclean target is used to remove the object ﬁles and executable, quite h andy if you wish to rebuild the entire program from scratch. Now you might be thinking, “well, this seems OK, but these makeﬁ les sure are cumbersome.” And you’d be right — if they always had to be written like this. Fortunately, there are a lot of shortcuts that makemake even easier to use. For example, this makeﬁle has the same func tionality but is a little nicer to use: # specify all source files here SRCS = hw.c helper.c # specify target here (name of executable) TARG = hw # specify compiler, compile flags, and needed libs CC = gcc OPTS = -Wall -O LIBS = -lm # this translates .c files in src list to .o’s OBJS = $(SRCS:.c=.o) # all is not really needed, but is used to generate the target all: $(TARG) # this generates the target executable $(TARG): $(OBJS) $(CC) -o $(TARG) $(OBJS) $(LIBS) # this is a generic rule for .o files  percent.o:  percent.c $(CC) $(OPTS) -c $< -o $@ # and finally, a clean line clean: rm -f $(OBJS) $(TARG) Though we won’t go into the details of make syntax, as you can see, this makeﬁle can make your life somewhat easier. For example, it allows c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 8 LABORATORY : TUTORIAL you to easily add new source ﬁles into your build, simply by addin g them to theSRCS variable at the top of the makeﬁle.",3990
Appendix F Laboratory - Tutorial,"You can also easily change the name of the executable by changing the TARG line, and the compiler, ﬂags, and library speciﬁcations are all easily modiﬁed. One ﬁnal word about make : ﬁguring out a target’s prerequisites is not always trivial, especially in large and complex programs. Not s urpris- ingly, there is another tool that helps with this, called makedepend . Read about it on your own and see if you can incorporate it into a makeﬁle. F.7 Debugging Finally, after you have created a good build environment, and a cor - rectly compiled program, you may ﬁnd that your program is buggy. On e way to ﬁx the problem(s) is to think really hard — this method is s ome- times successful, but often not. The problem is a lack of information ; you just don’t know exactly what is going on within the program, and ther e- fore cannot ﬁgure out why it is not behaving as expected. Fortunate ly, there is some help: gdb, the GNU debugger. Let’s take the following buggy code, saved in the ﬁle buggy.c , and compiled into the executable buggy. #include <stdio.h> struct Data { int x; }; int main(int argc, char *argv[]) { struct Data *p = NULL; printf(\"" percentd \"", p->x); } In this example, the main program dereferences the variable pwhen it is NULL, which will lead to a segmentation fault. Of course, t his prob- lem should be easy to ﬁx by inspection, but in a more complex program , ﬁnding such a problem is not always easy. To prepare yourself for a debugging session, recompile your progra m and make sure to pass the -gﬂag to each compile line. This includes extra debugging information in your executable that will be useful du ring your debugging session. Also, don’t turn on optimization ( -O); though this may work, it may also lead to confusion during debugging. After re-compiling with -g, you are ready to use the debugger. Fire upgdb at the command prompt as follows: prompt> gdb buggy This puts you inside an interactive session with the debugger. Note that you can also use the debugger to examine “core” ﬁles that we re pro- OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : TUTORIAL 9 duced during bad runs, or to attach to an already-running progr am; read the documentation to learn more about this. Once inside, you may see something like this: prompt> gdb buggy GNU gdb ... Copyright 2008 Free Software Foundation, Inc. (gdb) The ﬁrst thing you might want to do is to go ahead and run the pro- gram. To do this, simply type run at gdb command prompt. In this case, this is what you might see: (gdb) run Starting program: buggy Program received signal SIGSEGV, Segmentation fault. 0x8048433 in main (argc=1, argv=0xbffff844) at buggy.cc:1 9 19 printf(\"" percentd \"", p->x); As you can see from the example, in this case, gdb immediately pin- points where the problem occurred; a “segmentation fault” was ge ner- ated at the line where we tried to dereference p. This just means that we accessed some memory that we weren’t supposed to access. At this p oint, the astute programmer can examine the code, and say “aha.",3050
Appendix F Laboratory - Tutorial,"it mus t be thatpdoes not point to anything valid, and thus should not be derefer- enced.”, and then go ahead and ﬁx the problem. However, if you didn’t know what was going on, you might want to examine some variable. gdb allows you to do this interactively during the debug session. (gdb) print p 1 = (Data *) 0x0 By using the print primitive, we can examine p, and see both that it is a pointer to a struct of type Data, and that it is currently set to NULL (or zero, or hex zero which is shown here as “0x0”). Finally, you can also set breakpoints within your program to have the debugger stop the program at a certain routine. After doing this, it is often useful to step through the execution (one line at a time), an d see what is happening. (gdb) break main Breakpoint 1 at 0x8048426: file buggy.cc, line 17. (gdb) run Starting program: /homes/hacker/buggy Breakpoint 1, main (argc=1, argv=0xbffff844) at buggy.cc: 17 17 struct Data *p = NULL; (gdb) next 19 printf(\"" percentd \"", p->x); (gdb) Program received signal SIGSEGV, Segmentation fault. 0x8048433 in main (argc=1, argv=0xbffff844) at buggy.cc:1 9 19 printf(\"" percentd \"", p->x); c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 10 LABORATORY : TUTORIAL In the example above, a breakpoint is set at the main() routine; thus, when we run the program, the debugger almost immediately stops e xe- cution at main. At that point in the example, a “next” command is i ssued, which executes the next source-level command. Both “next” and “ step” are useful ways to advance through a program — read about them in t he documentation for more details2. This discussion really does not do gdb justice; it is a rich and ﬂexi- ble debugging tool, with many more features than can be describe d in the limited space here. Read more about it on your own and become an expert in your copious spare time. F.8 Documentation To learn a lot more about all of these things, you have to do two thing s: the ﬁrst is to use these tools, and the second is to read more about th em on your own. One way to ﬁnd out more about gcc,gmake , andgdb is to read their man pages; type man gcc ,man gmake , orman gdb at your command prompt. You can also use man -k to search the man pages for keywords, though that doesn’t always work as well as it might; googli ng is probably a better approach here. One tricky thing about man pages: typing man XXX may not result in the thing you want, if there is more than one thing called XXX. For example, if you are looking for the kill() system call man page, and if you just type man kill at the prompt, you will get the wrong man page, because there is a command-line program called kill . Man pages are divided into sections , and by default, man will return the man page in the lowest section that it ﬁnds, which in this case is section 1 . Note that you can tell which man page you got by looking at the top of the page: if you see kill(2) , you know you are in the right man page in Section 2, where system calls live. Type man man to learn more about what is stored in each of the different sections of the man pages. Also note that man -a kill can be used to cycle through all of the different man pages named “kill”. Man pages are useful for ﬁnding out a number of things.",3263
Appendix F Laboratory - Tutorial,"In particu lar, you will often want to look up what arguments to pass to a library ca ll, or what header ﬁles need to be included to use a library call. Al l of this should be available in the man page. For example, if you look up the open() system call, you will see: SYNOPSIS #include <sys/types.h> #include <sys/stat.h> #include <fcntl.h> int open(const char *path, int oflag, / *mode_t mode */...); 2In particular, you can use the interactive “help” command while debugg ing withgdb OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : TUTORIAL 11 That tells you to include the headers sys/types.h ,sys/stat.h , andfcntl.h in order to use the open call. It also tells you about the parameters to pass to open, namely a string called path , and integer ﬂag oflag , and an optional argument to specify the mode of the ﬁle. If there were any libraries you needed to link with to use the call, it wou ld tell you that here too. Man pages require some effort to use effectively. They are often di- vided into a number of standard sections. The main body will desc ribe how you can pass different parameters in order to have the functi on be- have differently. One particularly useful section is called the RETURN VALUES part of the man page, and it tells you what the function will return unde r success or failure. From the open() man page again: RETURN VALUES Upon successful completion, the open() function opens the file and return a non-negative integer representing the lowest numbered unused file descriptor. Otherwise, -1 is returned, errno is set to indicate the error, and no files are created or modified. Thus, by checking what open returns, you can see if the open suc- ceeded or not. If it didn’t, open (and many standard library routin es) will set a global variable called errno to a value to tell you about the error. See theERRORS section of the man page for more details. Another thing you might want to do is to look for the deﬁnition of a structure that is not speciﬁed in the man page itself. For examp le, the man page for gettimeofday() has the following synopsis: SYNOPSIS #include <sys/time.h> int gettimeofday(struct timeval *restrict tp, void*restrict tzp); From this page, you can see that the time is put into a structure of typetimeval , but the man page may not tell you what ﬁelds that struct has. (in this case, it does, but you may not always be so lucky) Thu s, you may have to hunt for it. All include ﬁles are found under the dire ctory /usr/include , and thus you can use a tool like grep to look for it. For example, you might type: prompt> grep ’struct timeval’ /usr/include/sys/ *.h This lets you look for the deﬁnition of the structure in all ﬁles that end with.hin/usr/include/sys . Unfortunately, this may not always work, as that include ﬁle may include others which are found else where. A better way to do this is to use a tool at your disposal, the com- piler. Write a program that includes the header time.h , let’s say called main.c . Then, instead of compiling it, use the compiler to invoke the preprocessor. The preprocessor processes all the directives in y our ﬁle, such as#define commands and #include commands.",3176
Appendix F Laboratory - Tutorial,"To do this, type c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 12 LABORATORY : TUTORIAL gcc -E main.c . The result of this is a C ﬁle that has all of the needed structures and prototypes in it, including the deﬁnition of the t imeval struct. Probably an even better way to ﬁnd these things out: google. You should always google things you don’t know about — it’s amazing how much you can learn simply by looking it up. Info Pages Also quite useful in the hunt for documentation are the info pages , which provide much more detailed documentation on many GNU tools. You can access the info pages by running the program info , or viaemacs , the preferred editor of hackers, by executing Meta-x info . A program likegcc has hundreds of ﬂags, and some of them are surprisingly useful to know about. gmake has many more features that will improve your build environment. Finally, gdb is quite a sophisticated debugger. Read the man and info pages, try out features that you hadn’t tried bef ore, and become a power user of your programming tools. F.9 Suggested Readings Other than the man and info pages, there are a number of useful b ooks out there. Note that a lot of this information is available for free on- line; however, sometimes having something in book form seems to make it easier to learn. Also, always look for O’Reilly books on topics you are interested in; they are almost always of high quality. •“The C Programming Language”, by Brian Kernighan and Dennis Ritchie. This is thedeﬁnitive C book to have. •“Managing Projects with make”, by Andrew Oram and Steve Tal- bott. A reasonable and short book on make. •“Debugging with GDB: The GNU Source-Level Debugger”, by Richa rd M. Stallman, Roland H. Pesch. A little book on using GDB. •“Advanced Programming in the UNIX Environment”, by W. Richard Stevens and Steve Rago. Stevens wrote some excellent books, and this is a must for U NIXhackers. He also has an excellent set of books on TCP/IP and Sockets programming. •“Expert C Programming”, by Peter Van der Linden. A lot of the useful tips about compilers, etc., above are stolen directly from here. Read this. It is a great and eye-opening book, even though a little out of date. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",2252
Appendix G Laboratory - Systems Projects,"G Laboratory: Systems Projects This chapter presents some ideas for systems projects. We usual ly do about six or seven projects in a 15-week semester, meaning one eve ry two weeks or so. The ﬁrst few are usually done by a single student, and the last few in groups of size two. Each semester, the projects follow this same outline; however, we vary the details to keep it interesting and make “sharing” of code ac ross semesters more challenging (not that anyone would do that.). We also use the M oss tool [M94] to look for this kind of “sharing”. As for grading, we’ve tried a number of different approaches, eac h of which have their strengths and weaknesses. Demos are fun but time consuming. Automated test scripts are less time intensive but require a great deal of care to get them to carefully test interesting cor ner cases. Check the book web page for more details on these projects; if you’d lik e the automated test scripts, we’d be happy to share. G.1 Intro Project The ﬁrst project is an introduction to systems programming. Typi cal assignments have been to write some variant of the sort utility, with different constraints. For example, sorting text data, sorting binary data, and other similar projects all make sense. To complete the projec t, one must get familiar with some system calls (and their return err or codes), use a few simple data structures, and not much else. G.2 U NIXShell In this project, students build a variant of a U NIXshell. Students learn about process management as well as how mysterious things like pi pes and redirects actually work. Variants include unusual featu res, like a redirection symbol that also compresses the output via gzip. Anot her variant is a batch mode which allows the user to batch up a few req uests and then execute them, perhaps using different scheduling d isciplines. 1 2 LABORATORY : SYSTEMS PROJECTS G.3 Memory-allocation Library This project explores how a chunk of memory is managed, by building an alternative memory-allocation library (like malloc() andfree() but with different names). The project teaches students how to usemmap() to get a chunk of anonymous memory, and then about pointers in great detail in order to build a simple (or perhaps, more complex) free l ist to manage the space. Variants include: best/worst ﬁt, buddy, an d various other allocators. G.4 Intro to Concurrency This project introduces concurrent programming with POSIX threa ds. Build some simple thread-safe libraries: a list, hash table, and some more complicated data structures are good exercises in adding locks t o real- world code. Measure the performance of coarse-grained versus ﬁne -grained alternatives. Variants just focus on different (and perhaps m ore complex) data structures. G.5 Concurrent Web Server This project explores the use of concurrency in a real-world appli ca- tion. Students take a simple web server (or build one) and add a thr ead pool to it, in order to serve requests concurrently. The thread pool should be of a ﬁxed size, and use a producer/consumer bounded buffer to pa ss requests from a main thread to the ﬁxed pool of workers. Learn how threads, locks, and condition variables are used to build a real server. Variants include scheduling policies for the threads. G.6 File System Checker This project explores on-disk data structures and their consist ency. Students build a simple ﬁle system checker. The debugfs tool can be used on Linux to make real ﬁle-system images; crawl through the m and make sure all is well. To make it more difﬁcult, also ﬁx any probl ems that are found. Variants focus on different types of problems: pointers , link counts, use of indirect blocks, etc. G.7 File System Defragmenter This project explores on-disk data structures and their perform ance implications. The project should give some particular ﬁle-syst em images to students with known fragmentation problems; students should then crawl through the image, and look for ﬁles that are not laid out seque n- tially. Write out a new “defragmented” image that ﬁxes this pr oblem, perhaps reporting some statistics. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY : SYSTEMS PROJECTS 3 G.8 Concurrent File Server This project combines concurrency and ﬁle systems and even a lit tle bit of networking and distributed systems. Students build a sim ple con- current ﬁle server. The protocol should look something like NFS, with lookups, reads, writes, and stats. Store ﬁles within a single dis k image (designed as a ﬁle). Variants are manifold, with different su ggested on- disk formats and network protocols. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES 4 LABORATORY : SYSTEMS PROJECTS References [M94] “Moss: A System for Detecting Software Plagiarism” Alex Aiken Available: http://theory.stanford.edu/˜aiken/moss/ OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG",4872
Appendix H Laboratory - xv6 Projects,"H Laboratory: xv6 Projects This chapter presents some ideas for projects related to the xv6 kernel. The kernel is available from MIT and is quite fun to play with; d oing these projects also make the in-class material more directly r elevant to the projects. These projects (except perhaps the ﬁrst couple) a re usually done in pairs, making the hard task of staring at the kernel a lit tle easier. H.1 Intro Project The introduction adds a simple system call to xv6. Many variant s are possible, including a system call to count how many system calls have taken place (one counter per system call), or other information-ga thering calls. Students learn about how a system call actually takes pla ce. H.2 Processes and Scheduling Students build a more complicated scheduler than the default rou nd robin. Many variants are possible, including a Lottery schedul er or multi- level feedback queue. Students learn how schedulers actually work, as well as how a context switch takes place. A small addendum is to a lso require students to ﬁgure out how to make processes return a prope r error code when exiting, and to be able to access that error code through the wait() system call. H.3 Intro to Virtual Memory The basic idea is to add a new system call that, given a virtual address, returns the translated physical address (or reports that the a ddress is not valid). This lets students see how the virtual memory system se ts up page tables without doing too much hard work. Another variant explores h ow to transform xv6 so that a null-pointer dereference actually g enerates a fault. 1 2 LABORATORY :XV6 PROJECTS H.4 Copy-on-write Mappings This project adds the ability to perform a lightweight fork() , called vfork() , to xv6. This new call doesn’t simply copy the mappings but rather sets up copy-on-write mappings to shared pages. Upon ref erence to such a page, the kernel must then create a real copy and updat e page tables accordingly. H.5 Memory mappings An alternate virtual memory project is to add some form of memory- mapped ﬁles. Probably the easiest thing to do is to perform a laz y page-in of code pages from an executable; a more full-blown approach is to bu ild anmmap() system call and all of the requisite infrastructure needed to fault in pages from disk upon dereference. H.6 Kernel Threads This project explores how to add kernel threads to xv6. A clone() system call operates much like fork but uses the same address sp ace. Stu- dents have to ﬁgure out how to implement such a call, and thus how to create a real kernel thread. Students also should build a lit tle thread library on top of that, providing simple locks. H.7 Advanced Kernel Threads Students build a full-blown thread library on top of their kernel t hreads, adding different types of locks (spin locks, locks that sleep whe n the pro- cessor is not available) as well as condition variables. Requisi te kernel support is added as well. H.8 Extent-based File System This ﬁrst ﬁle system project adds some simple features to the ba sic ﬁle system. For ﬁles of type EXTENT, students change the inode to store extents (i.e., pointer, length pairs) instead of just pointers . Serves as a relatively light introduction to the ﬁle system. H.9 Fast File System Students transform the basic xv6 ﬁle system into the Berkeley F ast File System (FFS). Students build a new mkfs tool, introduce block groups and a new block-allocation policy, and build the large-ﬁle excep tion. The basics of how ﬁle systems work are understood at a deeper level. OPERATING SYSTEMS [VERSION 1.00] WWW .OSTEP .ORG LABORATORY :XV6 PROJECTS 3 H.10 Journaling File System Students add a rudimentary journaling layer to xv6. For each wri te to a ﬁle, the journaling FS batches up all dirtied blocks and write s a record of their pending update to an on-disk log; only then are the blocks mod iﬁed in place. Students demonstrate the correctness of their system b y intro- ducing crash points and showing that the ﬁle system always recov ers to a consistent state. H.11 File System Checker Students build a simple ﬁle system checker for the xv6 ﬁle syste m. Students learn about what makes a ﬁle system consistent and how ex actly to check for it. c/circlecopyrt2008–18, A RPACI -DUSSEAUTHREE EASY PIECES",4264
