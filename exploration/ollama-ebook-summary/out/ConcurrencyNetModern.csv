filename,title,text,len
01-Concurrency in .NET.pdf,01-Concurrency in .NET,"MANNINGRiccardo TerrellModern patterns of concurrent and\n parallel programming\n\nChapter dependency graph\nChapter 1\n• Why concurrency and definitions?• Pitfalls of concurrent programmingChapter 2\n• Solving problems by composing simple solutions• Simplifying programming with closures\nChapter 3\n• Immutable data structures• Lock-free concurrent codeChapter 6\n• Functional reactive programming• Querying real-time event streams\nChapter 4\n• Big data parallelism • The Fork/Join patternChapter 7\n• Composing parallel operations• Querying real-time event streams\nChapter 5\n• Isolating and controlling side effectsChapter 8\n• Parallel asynchronous computations• Composing asynchronous executions\nChapter 11\n• Agent (message-passing) modelChapter 10\n• Task combinators\n• Async combinators and conditional operators\nChapter 13\n• Reducing memory consumption• Parallelizing dependent tasksChapter 12\n• Composing asynchronous TPL Dataflow blocks• Concurrent Producer/Consumer pattern \nChapter 14\n• Scalable applications using CQRS patternChapter 9\n• Cooperating asynchronous computations• Extending asynchronous F# computational expressions\n \n\nConcurrency in .NET \n \n\n \n\nConcurrency in .NET \nModern patterns of concurrent and parallel programming\nRICCARDO TERRELL\nMANNING\nShelter  ISland\n \n\nFor online information and ordering of this and other Manning books, please visit www.manning.com. The publisher offers discounts on this book when ordered in quantity.For more information, please contactSpecial Sales DepartmentManning Publications Co.20 Baldwin RoadPO Box 761Shelter Island, NY 11964Email: orders@manning.com©2018 by Manning Publications Co. All rights reserved.No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid- free paper, and we exert our best efforts to that end. Recognizing also our \nresponsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.\n∞\n Manning Publications Co. 20 Baldwin RoadPO Box 761 Shelter Island, NY 11964 Development editor: Marina Michaels\n Technical development editor: Michael Lund\n Technical proofreader: Viorel Moisei\n Review editor: Aleksandar Dragosavljevic ´\n Project manager: Tiffany Taylor\n Copy editor: Katie Petito\n Proofreader: Elizabeth Martin\n Typesetter: Happenstance Type-O-Rama\n Cover designer: Marija Tudor\nISBN 9781617292996Printed in the United States of America1 2 3 4 5 6 7 8 9 10 – EBM – 23 22 21 20 19 18\n \nvI dedicate this book to my wonderful and supportive wife, Bryony. Your support, love, \ncare, and continued encouragement throughout my writing process are what allowed \nme to turn this project into a reality. I love you and admire you for your many efforts \nand patience while I was busy writing. Thank you for always believing in me.\nI also dedicate this book to my loyal pugs, Bugghina and Stellina, who were always \nunconditionally and enthusiastically beside me while writing this book. You’re \nman’s best friends and this author’s furriest fans.",3667
02-brief contents.pdf,02-brief contents,"viibrief contents\nPart 1  Benefits of functional programming applicable to concurrent programs   ............................................ 1\n1 ■ Functional concurrency foundations 3\n2 ■ Functional programming techniques for concurrency 30\n3 ■ Functional data structures and immutability 59\nPart 2  How to approach the different parts of a concurrent program .............................................. 95\n4 ■ The basics of processing big data: data parallelism, part 1 97\n5 ■ PLINQ and MapReduce: data parallelism, part 2  118\n6 ■ Real-time event streams: functional reactive programming 148\n7 ■ Task-based functional parallelism 182\n8 ■ Task asynchronicity for the win 213\n9 ■ Asynchronous functional programming in F# 247\n10 ■  Functional combinators for fluent concurrent programming 275\n11 ■ Applying reactive programming everywhere with agents 328\n12 ■  Parallel workflow and agent programming with TPL Dataflow 365\n \nviiiviii brief contents\nPart 3  Modern patterns of concurrent  programming applied ........................................... 395\n13 ■  Recipes and design patterns for successful concurrent programming 397\n14 ■  Building a scalable mobile app with concurrent functional programming 449",1234
03-contents.pdf,03-contents,"ixcontents\npreface xixacknowledgments xxiiiabout this book xxv\nabout the author xxix\nabout the cover illustrator xxxi\nPart 1  Benefits of functional programming applicable to concurrent programs   .... 1\n 1 Functional concurrency foundations 3\n 1.1 What you’ll learn from this book 5\n 1.2 Let’s start with terminology 6\nSequential programming performs one task at a time 6 \n■ Concurrent programming runs multiple tasks at the \nsame time 7 ■ Parallel programming executes multiples tasks \nsimultaneously   8 ■ Multitasking performs multiple tasks \nconcurrently over time 10 ■ Multithreading for performance \nimprovement 11\n 1.3 Why the need for concurrency? 12\nPresent and future of concurrent programming 14\n \nxx contents\n 1.4 The pitfalls of concurrent programming 15\nConcurrency hazards 16 ■ The sharing of state \nevolution 18 ■ A simple real-world example: parallel \nquicksort    18 ■ Benchmarking in F# 22\n 1.5 Why choose functional programming \nfor concurrency? 23\nBenefits of functional programming 25\n 1.6 Embracing the functional paradigm 26\n 1.7 Why use F# and C# for functional concurrent \nprogramming? 27\n 2 Functional programming techniques for concurrency 30\n 2.1 Using function composition to solve \ncomplex problems 31\nFunction composition in C# 31 ■ Function \ncomposition in F# 33\n 2.2 Closures to simplify functional thinking 34\nCaptured variables in closures with lambda expressions 36 \n■ Closures in a multithreading environment 37\n 2.3 Memoization-caching technique for \nprogram speedup 39\n 2.4 Memoize in action for a fast web crawler 43\n 2.5 Lazy memoization for better performance 46\nGotchas for function memoization 47\n 2.6 Effective concurrent speculation to amortize the cost of \nexpensive computations 48\nPrecomputation with natural functional support 50 ■ Let the best \ncomputation win 51\n 2.7 Being lazy is a good thing 52\nStrict languages for understanding concurrent behaviors 53 \nLazy caching technique and thread-safe Singleton pattern 54 ■ Lazy \nsupport in F# 56 ■ Lazy and Task, a powerful combination 56\n 3 Functional data structures and immutability 59\n 3.1 Real-world example: hunting the thread-unsafe object 60\n.NET immutable collections: a safe solution 63 ■ .NET concurrent \ncollections: a faster solution 68 ■ The agent message-passing \npattern: a faster, better solution 70\n \n xi xi contents\n 3.2 Safely sharing functional data structures \namong threads 72\n 3.3 Immutability for a change 73\nFunctional data structure for data parallelism 76 ■ Performance \nimplications of using immutability 76 ■ Immutability in \nC# 77 ■ Immutability in F# 79 ■ Functional lists: linking \ncells in a chain 80 ■ Building a persistent data structure: an \nimmutable binary tree 86\n 3.4 Recursive functions: a natural way to iterate 88\nThe tail of a correct recursive function: tail-call optimization 89 \nContinuation passing style to optimize recursive function 90\nPart 2  How to approach the different parts of a concurrent program .............................. 95\n 4 The basics of processing big data: data parallelism, part 1 97\n 4.1 What is data parallelism? 98\nData and task parallelism 99 ■ The “embarrassingly parallel” \nconcept 100 ■ Data parallelism support in .NET 101\n 4.2 The Fork/Join pattern: parallel Mandelbrot 102\nWhen the GC is the bottleneck: structs vs. class objects 107  \nThe downside of parallel loops 110\n 4.3 Measuring performance speed 110\nAmdahl’s Law defines the limit of performance improvement 111 \nGustafson’s Law: a step further to measure performance improvement 112 \n■ The limitations of parallel loops: the sum of \nprime numbers 112 ■ What can possibly go wrong with a simple \nloop? 114 ■ The declarative parallel programming model 115\n 5 PLINQ and MapReduce: data parallelism, part 2 118\n 5.1 A short introduction to PLINQ 119\nHow is PLINQ more functional? 120 ■ PLINQ and pure \nfunctions: the parallel word counter 121 ■ Avoiding side effects \nwith pure functions 123 ■ Isolate and control side effects: \nrefactoring the parallel word counter 124\n 5.2 Aggregating and reducing data in parallel 125\nDeforesting: one of many advantages to folding 127 ■ Fold in \nPLINQ: Aggregate functions 129 ■ Implementing a parallel \nReduce function for PLINQ 135 ■ Parallel list comprehension in \nF#: PSeq 137 ■ Parallel arrays in F# 137\n \nxiixii contents\n 5.3 Parallel MapReduce pattern 139\nThe Map and Reduce functions 140 ■ Using MapReduce with the \nNuGet package gallery 141\n 6 Real-time event streams: functional reactive programming 148\n 6.1 Reactive programming: big event processing 149\n 6.2 .NET tools for reactive programming 152\nEvent combinators—a better solution 153 ■ .NET interoperability \nwith F# combinators 154\n 6.3 Reactive programming in .NET: \nReactive Extensions (Rx) 156\nFrom LINQ/PLINQ to Rx 159 ■ IObservable: the dual \nIEnumerable 160 ■ Reactive Extensions in action 161 \nReal-time streaming with RX 162 ■ From events to F# \nobservables 163\n 6.4 Taming the event stream: Twitter emotion analysis using \nRx programming 164\nSelectMany: the monadic bind operator 171\n 6.5 An Rx publisher-subscriber 173\nUsing the Subject type for a powerful publisher-subscriber hub 173 \n■ Rx in relation to concurrency 174 ■ Implementing \na reusable Rx publisher-subscriber 175 ■ Analyzing tweet \nemotions using an Rx Pub-Sub class 177 ■ Observers in \naction 179 ■ The convenient F# object expression 180\n 7 Task-based functional parallelism 182\n 7.1 A short introduction to task parallelism 183\nWhy task parallelism and functional programming? 184 ■ Task \nparallelism support in .NET 185\n 7.2 The .NET Task Parallel Library 187\nRunning operations in parallel with TPL Parallel.Invoke 188\n 7.3 The problem of void in C# 191\nThe solution for void in C#: the unit type 191\n 7.4 Continuation-passing style: a functional control flow 193\nWhy exploit CPS? 194 ■ Waiting for a task to complete:  \nthe continuation model 195\n \n xiii xiii contents\n 7.5 Strategies for composing task operations 200\nUsing mathematical patterns for better composition 201 \n■ Guidelines for using tasks 207\n 7.6 The parallel functional Pipeline pattern 207\n 8 Task asynchronicity for the win 213\n 8.1 The Asynchronous Programming Model (APM) 214\nThe value of asynchronous programming 215 ■ Scalability and \nasynchronous programming 217 ■ CPU-bound and I/O-bound \noperations 218\n 8.2 Unbounded parallelism with asynchronous \nprogramming 219\n 8.3 Asynchronous support in .NET 220\nAsynchronous programming breaks the code structure 223 \nEvent-based Asynchronous Programming 223\n 8.4 C# Task-based Asynchronous Programming 223\nAnonymous asynchronous lambdas 226 ■ Task<T> is a monadic \ncontainer 227\n 8.5 Task-based Asynchronous Programming:  \na case study 230\nAsynchronous cancellation 234 ■ Task-based asynchronous \ncomposition with the monadic Bind operator 238 ■ Deferring \nasynchronous computation enables composition 239 ■ Retry if \nsomething goes wrong 240 ■ Handling errors in asynchronous \noperations 241 ■ Asynchronous parallel processing of the historical \nstock market 243 ■ Asynchronous stock market parallel processing \nas tasks complete 245\n 9 Asynchronous functional programming in F# 247\n 9.1 Asynchronous functional aspects 248\n 9.2 What’s the F# asynchronous workflow? 248\nThe continuation passing style in computation expressions 249 \n■ The asynchronous workflow in action:  \nAzure Blob storage parallel operations 251\n \nxivxiv contents\n 9.3 Asynchronous computation expressions 256\nDifference between computation expressions and monads 257 \n■ AsyncRetry: building your own \ncomputation expression 259 ■ Extending the asynchronous \nworkflow 261 ■ Mapping asynchronous operation: the Async \n.map functor 262 ■ Parallelize asynchronous workflows: \nAsync.Parallel 264 ■ Asynchronous workflow cancellation \nsupport 268 ■ Taming parallel asynchronous operations 271\n 10 Functional combinators for fluent concurrent programming 275\n 10.1 The execution flow isn’t always on the happy path: \nerror handling 276\nThe problem of error handling in imperative programming 277\n 10.2 Error combinators: Retry, Otherwise, and  \nTask.Catch in C# 279\nError handling in FP: exceptions for flow control 282 ■ Handling \nerrors with Task<Option<T>> in C# 284 ■ The F# AsyncOption \ntype: combining Async and Option 284 ■ Idiomatic F# functional \nasynchronous error handling 286 ■ Preserving the exception \nsemantic with the Result type 287\n 10.3 Taming exceptions in asynchronous operations 290\nModeling error handling in F# with Async and Result 295 \n■ Extending the F# AsyncResult type with monadic \nbind operators 296\n 10.4 Abstracting operations with functional combinators 300\n 10.5 Functional combinators in a nutshell 301\nThe TPL built-in asynchronous combinators 301 ■ Exploiting \nthe Task.WhenAny combinator for redundancy and interleaving 302 \n■ Exploiting the Task.WhenAll combinator for \nasynchronous for-each 304 ■ Mathematical pattern review: what \nyou’ve seen so far 305\n 10.6 The ultimate parallel composition \napplicative functor 308\nExtending the F# async workflow with applicative functor operators 315 \n■ Applicative functor semantics in F# with infix \noperators 317 ■ Exploiting heterogeneous parallel computation \nwith applicative functors 318 ■ Composing and executing \nheterogeneous parallel computations 319 ■ Controlling flow with \nconditional asynchronous combinators 321 ■ Asynchronous \ncombinators in action 325\n \n xv xv contents\n 11 Applying reactive programming everywhere with agents 328\n 11.1 What’s reactive programming, and how is it useful? 330\n 11.2 The asynchronous message-passing \nprogramming model 331\nRelation with message passing and immutability 334 \nNatural isolation 334\n 11.3 What is an agent? 334\nThe components of an agent 335 ■ What an agent can \ndo 336 ■ The share-nothing approach for lock-free concurrent \nprogramming 336 ■ How is agent-based programming \nfunctional? 337 ■ Agent is object-oriented 338\n 11.4 The F# agent: MailboxProcessor 338\nThe mailbox asynchronous recursive loop 340\n 11.5 Avoiding database bottlenecks with F# \nMailboxProcessor 341\nThe MailboxProcessor message type: discriminated unions 344 \n■ MailboxProcessor two-way \ncommunication 345 ■ Consuming the AgentSQL \nfrom C# 346 ■ Parallelizing the workflow with group \ncoordination of agents 347 ■ How to handle errors with \nF# MailboxProcessor 349 ■ Stopping MailboxProcessor \nagents—CancellationToken 350 ■ Distributing the \nwork with MailboxProcessor 351 ■ Caching operations \nwith an agent 352 ■ Reporting results from a \nMailboxProcessor 357 ■ Using the thread pool to report events from \nMailboxProcessor 359\n 11.6 F# MailboxProcessor: 10,000 agents for a game of life 359\n 12  Parallel workflow and agent programming with  TPL Dataflow 365\n 12.1 The power of TPL Dataflow 366\n 12.2 Designed to compose: TPL Dataflow blocks 367\nUsing BufferBlock<TInput> as a FIFO buffer 368 ■ Transforming \ndata with TransformBlock<TInput, TOutput> 369 ■ Completing \nthe work with ActionBlock<TInput > 370 ■ Linking dataflow \nblocks 372\n \nxvixvi contents\n 12.3 Implementing a sophisticated Producer/Consumer \nwith TDF 372\nA multiple Producer/single Consumer pattern: TPL Dataflow 372 \nA single Producer/multiple Consumer pattern 374\n 12.4 Enabling an agent model in C# using TPL Dataflow 374\nAgent fold-over state and messages: Aggregate 377 \nAgent interaction: a parallel word counter 378\n 12.5 A parallel workflow to compress and encrypt a \nlarge stream 382\nContext: the problem of processing a large stream of data 383 \n■ Ensuring the order integrity of a \nstream of messages 388 ■ Linking, propagating, \nand completing 389 ■ Rules for building a TDF \nworkflow 390 ■ Meshing Reactive Extensions (Rx)  \nand TDF 391\nPart 3  Modern patterns of concurrent programming applied ............................ 395\n 13  Recipes and design patterns for successful  concurrent programming 397\n 13.1 Recycling objects to reduce memory consumption 398\nSolution: asynchronously recycling a pool of objects 399\n 13.2 Custom parallel Fork/Join operator 401\nSolution: composing a pipeline of steps forming the Fork/Join pattern 402\n 13.3 Parallelizing tasks with dependencies: designing code to \noptimize performance 404\nSolution: implementing a dependencies graph of tasks 405\n 13.4 Gate for coordinating concurrent I/O operations sharing \nresources: one write, multiple reads 409\nSolution: applying multiple read/write operations to shared thread-safe resources 409\n 13.5 Thread-safe random number generator 414\nSolution: using the ThreadLocal object 415\n \n xvii xvii contents\n 13.6 Polymorphic event aggregator 416\nSolution: implementing a polymorphic publisher-subscriber pattern 416\n 13.7 Custom Rx scheduler to control the degree of \nparallelism 419\nSolution: implementing a scheduler with multiple concurrent agents 419\n 13.8 Concurrent reactive scalable client/server 422\nSolution: combining Rx and asynchronous programming 423\n 13.9 Reusable custom high-performing parallel  \nfilter -map operator 431\nSolution: combining filter and map parallel operations 431\n 13.10 Non-blocking synchronous message-passing model 435\nSolution: coordinating the payload between operations using the agent programming model 436\n 13.11 Coordinating concurrent jobs using the agent \nprogramming model 440\nSolution: implementing an agent that runs jobs with a configured degree of parallelism 441\n 13.12 Composing monadic functions 444\nSolution: combining asynchronous operations using the Kleisli composition operator 445\n 14  Building a scalable mobile app with concurrent  functional programming 449\n 14.1 Functional programming on the server in the \nreal world 450\n 14.2 How to design a successful performant application 451\nThe secret sauce: ACD 452 ■ A different asynchronous pattern: \nqueuing work for later execution 453\n 14.3 Choosing the right concurrent programming model 454\nReal-time communication with SignalR 457\n 14.4 Real-time trading: stock market high-level \narchitecture 457\n \nxviii contents xviii\n 14.5 Essential elements for the stock market application 461\n 14.6 Let’s code the stock market trading application 462\nBenchmark to measure the scalability of the stock ticker application 482\n appendix A Functional programming 484\n appendix B F# overview 498\n appendix C Interoperability between an F# asynchronous workflow and  \n.NET Task 513\n  index 516",14588
04-preface.pdf,04-preface,"xixpreface\nYou’re probably reading this book, Concurrency in .NET, because you want to build \nblazingly fast applications, or learn how to dramatically increase the performance of an existing one. You care about performance because you’re dedicated to producing faster programs, and because you feel excited when a few changes in your code make your application faster and more responsive. Parallel programming provides endless possibilities for passionate developers who desire to exploit new technologies. When considering performance, the benefits of utilizing parallelism in your programming can’t be overstated. But using imperative and object-oriented programming styles to write concurrent code can be convoluted and introduces complexities. For this reason, concurrent programming hasn’t been embraced as common practice writ large, lead-ing programmers to search for other options.\nWhen I was in college, I took a class in functional programming. At the time, I was \nstudying Haskell; and even though there was a steep learning curve, I enjoyed every lesson. I remember watching the first examples and being amazed by the elegance of the solutions as well as their simplicity. Fifteen years later, when I began searching for other options to enhance my programs utilizing concurrency, my thoughts returned to these lessons. This time, I was able to fully realize how powerful and useful functional programming would be in designing my daily programs. There are several benefits to using a functional approach in your programming style, and I discuss each of them in this book.\n \nxx preface xx\nMy academic adventures met my professional work when I was challenged to build \na software system for the health care industry. This project involved making an appli-cation to analyze radiological medical images. The image processing required several steps such as image noise reduction, Gaussian algorithm, image interpolation, and image filtering to apply color to the gray image. The application was developed using Java and initially ran as anticipated. Eventually the department increased the demand, as often happens, and problems started to appear. The software didn’t have any prob-lems or bugs, but with the increasing number of images to analyze, it became slower. \nNaturally, the first proposed solution to this problem was to buy a more powerful \nserver. Although this was a valid solution at the time, today if you buy a new machine with the intention of gaining more CPU computation speed, you’ll be disappointed. This is because the modern CPU has more than one core, but the speed of a single core isn’t any faster than a single core purchased in 2007. The better and more enduring alternative to buying a new server/computer was to introduce parallelism to take advan-tage of multicore hardware and all of its resources, ultimately speeding up the image processing.\nIn theory, this was a simple task, but in practice it wasn’t so trivial. I had to learn how \nto use threads and locking; and, unfortunately, I gained firsthand experience in what a deadlock is.\nThis deadlock spurred me to make massive changes to the code base of the applica-\ntion. There were so many changes that I introduced bugs not even related to the original purpose of my changes. I was frustrated, the code base was unmaintainable and fragile, and the overall process was prone to bugs. I had to step back from the original problem and look for a solution from a different perspective. There had to be a better way.\nThe tools we use have a profound (and devious!) influence on our thinking habits, and, therefore, on our thinking abilities.\n—Edsger Dijkstra\nAfter spending a few days looking for a possible solution to solve the multithreading \nmadness, I realized the answer. Everything I researched and read was pointing toward the functional paradigm. The principles I had learned in that college class so many years ago became my mechanism for moving forward. I rewrote the core of the image processing application to run in parallel using a functional language. Initially, transi-tioning from an imperative to a functional style was a challenge. I had forgotten almost all that I learned in college, and I’m not proud to say that during this experience, I wrote code that looked very object-oriented in functional language; but it was a success-ful decision overall. The new program compiled and ran with a dramatic performance improvement, and the hardware resources were completely utilized and bug free. Above all, an unanticipated and fantastic surprise was that functional programming resulted in an impressive reduction in the number of lines of code: almost 50% fewer than the original implementation using object-oriented language.\n \n xxi preface  xxi\nThis experience made me reconsider OOP as the answer for all my programming \nproblems. I realized that this programming model and approach to problem solv-ing had a limited perspective. My journey into functional programming began with a requirement for a good concurrent programming model. \nEver since, I’ve had a keen interest in functional programming applied to multi-\nthreading and concurrency. Where others saw a complex problem and a source of issues, I saw a solution in functional programming as a powerful tool that could use the available hardware to run faster. I came to appreciate how the discipline leads to a coherent, composable, beautiful way of writing concurrent programs.\nI first had the idea for this book in July 2010, after Microsoft introduced F# as part \nof Visual Studio 2010. It was already clear at that time that an increasing number of mainstream programming languages supported the functional paradigm, including C#, C++, Java, and Python. In 2007, C# 3.0 introduced first-class functions to the lan-guage, along with new constructs such as lambda expressions and type inference to allow programmers to introduce functional programming concepts. Soon to follow was Language Integrate Query (LINQ), permitting a declarative programming style. \nIn particular, the .NET platform has embraced the functional world. With the intro-\nduction of F#, Microsoft has full-featured languages that support both object-oriented and functional paradigms. Additionally, object-oriented languages like C# are becom-ing more hybrid and bridging the gap between different programming paradigms, allowing for both programming styles.\nFurthermore, we’re facing the multicore era, where the power of CPUs is measured \nby the number of cores available, instead of clock cycles per second. With this trend, sin-gle-threaded applications won’t achieve improved speed on multicore systems unless the applications integrate parallelism and employ algorithms to spread the work across multiple cores.\nIt has become clear to me that multithreading is in demand, and it has ignited my \npassion to bring this programming approach to you. This book combines the power of concurrent programming and functional paradigm to write readable, more mod-ular, maintainable code in both the C# and F# languages. Your code will benefit from these techniques to function at peak performance with fewer lines of code, resulting in increased productivity and resilient programs.\nIt’s an exciting time to start developing multithreaded code. More than ever, software \ncompanies are making tools and capabilities available to choose the right programming style without compromise. The initial challenges of learning parallel programming will diminish quickly, and the reward for your perseverance is infinite. No matter what your field of expertise is, whether you’re a backend developer or a frontend web developer, or if you build cloud-based applications or video games, the use of parallelism to obtain better performance and to build scalable applications is here to stay. \nThis book draws on my experience with functional programming for writing concur -\nrent programs in .NET using both C# and F#. I believe that functional programming is \n \nxxii preface xxii\nbecoming the de facto way to write concurrent code, to coordinate asynchronous and parallel programs in .NET, and that this book will give you everything you need to be ready and master this exciting world of multicore computers.",8278
05-acknowledgments.pdf,05-acknowledgments,"xxiiiacknowledgments\nWriting a book is a daunting feat for anyone. Doing so in your secondary language is infinitely more challenging and intimidating. For me, you don’t dare entertain dreams such as this without being surrounded by a village of support. I would like to thank all of those who have supported me and participated in making this book a reality.\nMy adventure with F# started in 2013, when I attended a FastTrack to F# in NYC. I \nmet Tomas Petricek, who inspired me to fall headfirst into the F# world. He welcomed me into the community and has been a mentor and confidant ever since.\nI owe a huge debt of gratitude to the fantastic staff at Manning Publications. The \nheavy lifting for this book began 15 months ago with my development editor, Dan Maharry, and continued with Marina Michaels, both of whom have been patient and sage guides in this awesome task.\nThank you to the many technical reviewers, especially technical development editor \nMichael Lund and technical proofer Viorel Moisei. Your critical analysis was essential to ensuring that I communicated on paper all that is in my head, much of which was at risk of being “lost in translation.” Thank you also to those who participated in Manning’s MEAP program and provided support as peer reviewers: Andy Kirsch, Anton Herzog, Chris Bolyard, Craig Fullerton, Gareth van der Berg, Jeremy Lange, Jim Velch, Joel Kotarski, Kevin Orr, Luke Bearl, Pawel Klimczyk, Ricardo Peres, Rohit Sharma, Stefano Driussi, and Subhasis Ghosh. \n \nxxiv acknowledgments xxiv\nI received endless support from the members of the F# community who have rallied \nbehind me along the way, especially Sergey Tihon, who spent countless hours as my sounding board.\nAnd thank you to my family and friends who have cheered me on and patiently \nwaited for me to join the world again for social weekends, dinner outings, and the rest.\nAbove all, I would like to acknowledge my wife, who supports my every endeavor and \nhas never allowed me to shy away from a challenge. \nI must also recognize my dedicated and loyal pugs, Bugghina and Stellina, who were \nalways at my side or on my lap while I was writing this book deep into the night. It was also during our long evening walks that I was able to clear my head and find the best ideas for this book.",2309
06-about this book.pdf,06-about this book,"xxvabout this book\nConcurrency in .NET provides insights into the best practices necessary to build con-current and scalable programs in .NET, illuminating the benefits of the functional paradigm to give you the right tools and principles for handling concurrency easily and correctly. Ultimately, armed with your newfound skills, you’ll have the knowledge needed to become an expert at delivering successful high-performance solutions.\nWho should read this book \nIf you’re writing multithreaded code in .NET, this book can help. If you’re interested in using the functional paradigm to ease the concurrent programming experience to maximize the performance of your applications, this book is an essential guide. This book will benefit any .NET developers who want to write concurrent, reactive, and asynchronous applications that scale and perform by self-adapting to the current hard-ware resources wherever the program runs.\nThis book is also suitable for developers who are curious about exploiting functional \nprogramming to implement concurrent techniques. Prior knowledge or experience with the functional paradigm isn’t required, and the basic concepts are covered in appendix A.\nThe code examples use both the C# and F# languages. Readers familiar with C# will \nfeel comfortable right away. Familiarity with the F# language isn’t strictly required, and \n \nxxvi about  this book xxvi\na basic overview is covered in appendix B. Functional programming experience and knowledge isn’t required; the necessary concepts are included in the book. \nA good working knowledge of .NET is assumed. You should have moderate experi-\nence in working with .NET collections and knowledge of the .NET Framework, with a minimum of .NET version 3.5 required (LINQ, \nAction<> , and Func<>  delegates). \nFinally, this book is suitable for any platform supported by .NET (including .NET Core).\nHow this book is organized: a roadmap\nThe book’s 14 chapters are divided into 3 parts. Part 1 introduces functional concur -\nrent programming concepts and the skills you need in order to understand the func-tional aspects of writing multithreaded programs:\n¡ Chapter 1 highlights the main foundations and purposes behind concurrent programming and the reasons for using functional programming to write multi-threaded applications. \n¡ Chapter 2 explores several functional programming techniques to improve the performance of a multithreaded application. The purpose of this chapter is to provide concepts used during the rest of the book, and to make you familiar with powerful ideas that have originated from the functional paradigm.\n¡ Chapter 3 provides an overview of the functional concept of immutability. It explains how immutability is used to write predictable and correct concurrent programs, and how it’s applied to implement and use functional data structures, which are intrinsically thread safe.\nPart 2 dives into the different concurrent programming models of the functional par -\nadigm. We’ll explore subjects such as the Task Parallel Library (TPL), and implement-ing parallel patterns such as Fork/Join, Divide and Conquer, and MapReduce. Also discussed are declarative composition, high-level abstraction in asynchronous opera-tions, the agent programming model, and the message-passing semantic:\n¡ Chapter 4 covers the basics of processing a large amount of data in parallel, including patterns such as Fork/Join.\n¡ Chapter 5 introduces more advanced techniques for parallel processing massive data, such as aggregating and reducing data in parallel and implementing a par -\nallel MapReduce pattern.\n¡ Chapter 6 provides details of the functional techniques to process real-time streams of events (data), using functional higher-order operators with .NET Reactive Extensions to compose asynchronous event combinators. The tech-niques learned are used to implement a concurrent friendly and reactive pub-lisher-subscriber pattern.\n¡ Chapter 7 explains the task-based programming model applied to functional programming to implement concurrent operations using the Monadic pattern based on a continuation-passing style. This technique is then used to build a con-current- and functional-based pipeline. \n \n xxvii about  this book  xxvii\n¡ Chapter 8 concentrates on the C# asynchronous programming model to imple-ment unbounded parallel computations. This chapter also examines error han-dling and compositional techniques for asynchronous operations.\n¡ Chapter 9 focuses on the F# asynchronous workflow, explaining how the deferred and explicit evaluation of this model permits a higher compositional semantic. Then, we explore how to implement custom computation expressions to raise the level of abstraction, resulting in a declarative programming style.\n¡ Chapter 10 wraps up the previous chapters and culminates in implementing combinators and patterns such as Functor, Monad, and Applicative to compose and run multiple asynchronous operations and handle errors, while avoiding side effects.\n¡ Chapter 11 delves into reactive programming using the message-passing pro-gramming model. It covers the concept of natural isolation as a complementary technique with immutability for building concurrent programs. This chapter focuses on the F# \nMailboxProcessor  for distributing parallel work using the \nagent model and the share-nothing approach.\n¡ Chapter 12 explains the agent programming model using the .NET TPL Data-flow, with examples in C#. You’ll implement both a stateless and stateful agent using C# and run multiple computations in parallel that communicate with each other using (passing) messages in a pipeline style\nPart 3 puts into practice all the functional concurrent programming techniques learned in the previous chapters:\n¡ Chapter 13 contains a set of reusable and useful recipes to solve complex con-current issues based on real-world experiences. The recipes use the functional patterns you’ve seen throughout the book.\n¡ Chapter 14 presents a full application designed and implemented using the func-tional concurrent patterns and techniques learned in the book. You’ll build a highly scalable, responsive server application, and a reactive client-side program. Two versions are presented: one using Xamarin Visual Studio for an iOS (iPad)-based program, and one using WPF. The server-side application uses a combina-tion of different programming models, such as asynchronous, agent-based, and reactive, to ensure maximum scalability.\nThe book also has three appendices:\n¡ Appendix A summarizes the concepts of functional programming. This appen-dix provides basic theory about functional techniques used in the book. \n¡ Appendix B covers the basic concepts of F#. It’s a basic overview of F# to make you feel comfortable and help you gain familiarity with this programming language.\n¡ Appendix C illustrates few techniques to ease the interoperability between the F# asynchronous workflow and the .NET task in C#.\n \nxxviii about  this book xxviii\nAbout the code \nThis book contains many examples of source code, both in numbered listings and inline with normal text. In both cases, source code is formatted in a \nfixed-width font \nlike this  to separate it from ordinary text. Sometimes code is also in bold  to high-\nlight the topic under discussion.\nIn many cases, the original source code has been reformatted; we’ve added line \nbreaks and reworked indentation to accommodate the available page space in the book. In some cases, even this was not enough, and listings include line-continuation markers (\n➥). Additionally, comments in the source code have often been removed \nfrom the listings when the code is described in the text. Code annotations accompany many of the listings, highlighting important concepts.\nThe source code for this book is available to download from the publisher’s web-\nsite (www.manning.com/books/concurrency-in-dotnet) and from GitHub (https:/ /github.com/rikace/fConcBook ). Most of the code is provided in both C# and F# ver -\nsions. Instructions for using this code are provided in the README file included in the repository root.\nBook forum\nPurchase of Concurrency in .NET includes free access to a private web forum run by Manning Publications where you can make comments about the book, ask techni-cal questions, and receive help from the author and from other users. To access the forum, go to https:/ /forums.manning.com/forums/concurrency-in-dotnet. You can also learn more about Manning’s forums and the rules of conduct at https:/ /forums.manning.com/forums/about.\nManning’s commitment to our readers is to provide a venue where a meaningful dia-\nlogue between individual readers and between readers and the author can take place. It is not a commitment to any specific amount of participation on the part of the author, whose contribution to the forum remains voluntary (and unpaid). We suggest you try asking the author some challenging questions, lest his interest stray! The forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.",9133
07-about the author.pdf,07-about the author,"xxixabout the author\nRiccardo Terrell is a seasoned software engineer and Microsoft MVP who is passionate about functional programming. He has over 20 years’ experience delivering cost-effective technology solu-tions in a competitive business environment.\nIn 1998, Riccardo started his own software business in Italy, \nwhere he specialized in providing customized medical software to his clients. In 2007, Riccardo moved to the United States and ever \nsince has been working as a .NET senior software developer and senior software archi-tect to deliver cost-effective technology solutions in the business environment. Riccardo is dedicated to integrating advanced technology tools to increase internal efficiency, enhance work productivity, and reduce operating costs. \nHe is well known and actively involved in the functional programming community, \nincluding .NET meetups and international conferences. Riccardo believes in multi- paradigm programming as a mechanism to maximize the power of code. You can keep up with Riccardo and his coding adventures on his blog, www.rickyterrell.com.",1096
08-about the cover illustration.pdf,08-about the cover illustration,"xxxiabout the cover illustration\nThe figure on the cover of Concurrency in .NET is a man from a village in Abyssinia, today called Ethiopia. The illustration is taken from a Spanish compendium of regional dress customs first published in Madrid in 1799, engraved by Manuel Albuerne (1764-1815). The book’s title page states \nColeccion general de los Trages que usan actualmente todas las Nacionas del Mundo desubierto, dibujados y grabados con la mayor exactitud por R.M.V.A.R. Obra muy util y en special para los que tienen la del viajero universal \nwhich we translate, as literally as possible, thus: \nGeneral collection of costumes currently used in the nations of the known world, designed and printed with great exactitude by R.M.V.A.R. This work is very useful especially for those who hold themselves to be universal travelers \nAlthough little is known of the engraver, designers, and workers who colored this illustration by hand, the exactitude of their execution is evident in this drawing. The Abyssinian is just one of many figures in this colorful collection. Their diversity speaks vividly of the uniqueness and individuality of the world’s towns and regions just 200 years ago. This was a time when the dress codes of two regions separated by a few dozen miles identified people uniquely as belonging to one or the other. The collection brings to life a sense of isolation and distance of that period—and of every other his-toric period except our own hyperkinetic present. \n \nxxxii about  the cover  illustration xxxii\nDress codes have changed since then, and the diversity by region, so rich at the time, \nhas faded away. It’s now often hard to tell the inhabitant of one continent from another. Perhaps, trying to view it optimistically, we have traded a cultural and visual diversity for a more varied personal life—or a more varied and interesting intellectual and technical life. \nWe at Manning celebrate the inventiveness, the initiative, and the fun of the com-\nputer business with book covers based on the rich diversity of regional life of two centu-ries ago‚ brought back to life by the pictures from this collection.",2154
09-Part 1 Benefits of functional programming applicable to concurrent programs.pdf,09-Part 1 Benefits of functional programming applicable to concurrent programs,"Part 1 \nBenefits of functional \nprogramming applicable to \nconcurrent programs  \nF unctional programming is a programming paradigm that focuses on abstrac-\ntion and composition. In these first three chapters you’ll learn how to treat computations as the evaluation of expressions to avoid the mutation of data. To enhance concurrent programming, the functional paradigm provides tools and techniques to write deterministic programs. Output only depends upon input and not on the state of the program at execution time. The functional paradigm also facilitates writing code with fewer bugs by emphasizing separation of con-cerns between purely functional aspects, isolating side effects, and controlling unwanted behaviors.  \nThis part of the book introduces the main concepts and benefits of functional \nprogramming applicable to concurrent programs. Concepts discussed include programming with pure functions, immutability, laziness, and composition.",958
10-1 Functional concurrency foundations.pdf,10-1 Functional concurrency foundations,"31Functional concurrency \nfoundations\nThis chapter covers\n¡ Why you need concurrency \n¡ Differences between concurrency, parallelism, and multithreading \n¡ Avoiding common pitfalls when writing concurrent applications \n¡ Sharing variables between threads\n¡ Using the functional paradigm to develop concurrent programs\nIn the past, software developers were confident that, over time, their programs would run faster than ever. This proved true over the years due to improved hard-ware that enabled programs to increase speed with each new generation. \nFor the past 50 years, the hardware industry has experienced uninterrupted \nimprovements. Prior to 2005, the processor evolution continuously delivered faster single-core CPUs, until finally reaching the limit of CPU speed predicted by Gordon Moore.  Moore, a computer scientist, predicted in 1965 that the density and speed of transistors would double every 18 months before reaching a maximum speed beyond which technology couldn’t advance. The original prediction for the increase of CPU \n \n4 chapter  1 Functional concurrency foundations\nspeed presumed a speed-doubling trend for 10 years. Moore’s prediction, known as Moore’s Law, was correct—except that progress continued for almost 50 years (decades past his estimate). \nToday, the single-processor CPU has nearly reached the speed of light, all the while \ngenerating an enormous amount of heat due to energy dissipation; this heat is the lim-iting factor to further improvements. \nCPU has nearly reached the speed of light \nThe speed of light is the absolute physical limit for electric transmission, which is also the limit for electric signals in the CPU. No data propagation can be transmitted faster than the light medium. Consequentially, signals cannot propagate across the surface of the chip fast enough to allow higher speeds. Modern chips have a base cycle frequency of roughly 3.5 GHz, meaning 1 cycle every 1/3,500,000,000 seconds, or 2.85 nanosec -\nonds. The speed of light is about 3e8 meters per second, which means that data can be propagated around 0.30 cm (about a foot) in a nanosecond. But the bigger the chip, the longer it takes for data to travel through it. A fundamental relationship exists between circuit length (CPU physical size) and pro-cessing speed: the time required to perform an operation is a cycle of circuit length and the speed of light. Because the speed of light is constant, the only variable is the size of the CPU; that is, you need a small CPU to increase the speed, because shorter circuits require smaller and fewer switches. The smaller the CPU, the faster the transmission. In fact, creating a smaller chip was the primary approach to building faster CPUs with higher clock rates. This was done so effectively that we’ve nearly reached the physical limit for improving CPU speed.For example, if the clock speed is increased to 100 GHz, a cycle will be 0.01 nanosec -\nonds, and the signals will only propagate 3 mm in this time. Therefore, a CPU core ideally needs to be about 0.3 mm in size. This route leads to a physical size limitation. In addi -\ntion, this high frequency rate in such a small CPU size introduces a thermal problem in the equation. Power in a switching transistor is roughly the frequency ^2, so in moving from 4 GHz to 6 GHz there is a 225% increase of energy (which translates to heat). The problem besides the size of the chip becomes its vulnerability to suffer thermal damage such as changes in crystal structure.\n \nMoore’s prediction about transistor speed has come to fruition (transistors cannot run any faster) but it isn’t dead (modern transistors are increasing in density, providing opportunities for parallelism within the confines of that top speed). The combination of multicore architecture and parallel programming models is keeping Moore’s Law alive! As CPU single-core performance improvement stagnates, developers adapt by segueing into multicore architecture and developing software that supports and inte-grates concurrency. \nThe processor revolution has begun. The new trend in multicore processor design \nhas brought parallel programming into the mainstream. Multicore processor architec-ture offers the possibility of more efficient computing, but all this power requires addi-tional work for developers. If programmers want more performance in their code, they",4389
11-1.2 Lets start with terminology.pdf,11-1.2 Lets start with terminology,"5 What you’ll learn from this book\nmust adapt to new design patterns to maximize hardware utilization, exploiting multi-ple cores through parallelism and concurrency. \nIn this chapter, we’ll cover general information about concurrency by examining \nseveral of its benefits and the challenges of writing traditional concurrent programs. Next, we’ll introduce functional paradigm concepts that make it possible to overcome traditional limitations by using simple and maintainable code. By the end of this chap-ter, you’ll understand why concurrency is a valued programming model, and why the functional paradigm is the right tool for writing correct concurrent programs.\n1.1 What you’ll learn from this book\nIn this book I’ll look at considerations and challenges for writing concurrent multi-threaded applications in a traditional programming paradigm. I’ll explore how to successfully address these challenges and avoid concurrency pitfalls using the func-tional paradigm. Next, I’ll introduce the benefits of using abstractions in functional programming to create declarative, simple-to-implement, and highly performant con-current programs. Over the course of this book, we’ll examine complex concurrent issues providing an insight into the best practices necessary to build concurrent and scalable programs in .NET using the functional paradigm. You’ll become familiar with how functional programming helps developers support concurrency by encouraging immutable data structures that can be passed between threads without having to worry about a shared state, all while avoiding side effects. By the end of the book you’ll master how to write more modular, readable, and maintainable code in both C# and F# lan-guages. You’ll be more productive and proficient while writing programs that function at peak performance with fewer lines of code. Ultimately, armed with your newfound skills, you’ll have the knowledge needed to become an expert at delivering successful high-performance solutions.\nHere’s what you’ll learn:\n¡ How to combine asynchronous operations with the Task Parallel Library\n¡ How to avoid common problems and troubleshoot your multithreaded and asyn-chronous applications\n¡ Knowledge of concurrent programming models that adopt the functional para-digm (functional, asynchronous, event-driven, and message passing with agents and actors)\n¡ How to build high-performance, concurrent systems using the functional paradigm\n¡ How to express and compose asynchronous computations in a declarative style\n¡ How to seamlessly accelerate sequential programs in a pure fashion by using data-parallel programming\n¡ How to implement reactive and event-based programs declaratively with Rx-style event streams\n¡ How to use functional concurrent collections for building lock-free multi-threaded programs",2825
12-1.2.3 Parallel programming executes multiples tasks simultaneously.pdf,12-1.2.3 Parallel programming executes multiples tasks simultaneously,"6 chapter  1 Functional concurrency foundations\n¡ How to write scalable, performant, and robust server-side applications\n¡ How to solve problems using concurrent programming patterns such as the Fork/Join, parallel aggregation, and the Divide and Conquer technique\n¡ How to process massive data sets with parallel streams and parallel Map/Reduce implementations\nThis book assumes you have knowledge of general programming, but not functional programming. To apply functional concurrency in your coding, you only need a subset of the concepts from functional programming, and I’ll explain what you need to know along the way. In this fashion, you’ll gain the many benefits of functional concurrency in a shorter learning curve, focused on what you can use right away in your day-to-day coding experiences.\n1.2 Let’s start with terminology \nThis section defines terms related to the topic of this book, so we start on common ground. In computer programming, some terms (such as concurrency, parallelism, and \nmultithreading) are used in the same context, but have different meanings. Due to their similarities, the tendency to treat these terms as the same thing is common, but it is not correct. When it becomes important to reason about the behavior of a program, it’s crucial to make a distinction between computer programming terms. For example, concurrency is, by definition, multithreading, but multithreading isn’t necessarily con-current. You can easily make a multicore CPU function like a single-core CPU, but not the other way around. \nThis section aims to establish a common ground about the definitions and terminol-\nogies related to the topic of this book. By the end of this section, you’ll learn the mean-ing of these terms:\n¡ Sequential programming \n¡ Concurrent programming\n¡ Parallel programming\n¡ Multitasking \n¡ Multithreading \n1.2.1 Sequential programming performs one task at a time\nSequential programming is the act of accomplishing things in steps. Let’s consider a sim-ple example, such as getting a cup of cappuccino at the local coffee shop. You first stand in line to place your order with the lone barista. The barista is responsible for taking the order and delivering the drink; moreover, they are able to make only one drink at a time so you must wait patiently—or not—in line before you order. Making a cappuccino involves grinding the coffee, brewing the coffee, steaming the milk, froth-ing the milk, and combining the coffee and milk, so more time passes before you get your cappuccino. Figure 1.1 shows this process.\n \n 7 Let’s start with terminology \nCombine cof fee\nand milkFroth milkSteam milkBrew cof feeGrind cof fee\nFigure 1.1  For each person in line, the barista is sequentially repeating the same set of instructions \n(grind coffee, brew coffee, steam milk, froth milk, and combine the coffee and the milk to make a cappuccino).\nFigure 1.1 is an example of sequential work, where one task must be completed before the next. It is a convenient approach, with a clear set of systematic (step-by-step) instructions of what to do and when to do it. In this example, the barista will likely not get confused and make any mistakes while preparing the cappuccino because the steps are clear and ordered. The disadvantage of preparing a cappuccino step-by-step is that the barista must wait during parts of the process. While waiting for the coffee to be ground or the milk to be frothed, the barista is effectively inactive (blocked). The same concept applies to sequential and concurrent programming models. As shown in figure 1.2, sequential programming involves a consecutive, progressively ordered execution of processes, one instruction at a time in a linear fashion. \n Process 1  Process 2  Process 3  Process 4  Action\nFigure 1.2  Typical sequential coding involving a consecutive, progressively ordered execution of \nprocesses\nIn imperative and object-oriented programming (OOP) we tend to write code that behaves sequentially, with all attention and resources focused on the task currently running. We model and execute the program by performing an ordered set of state-ments, one after another.\n1.2.2 Concurrent programming runs multiple tasks at the same time\nSuppose the barista prefers to initiate multiple steps and execute them concurrently? This moves the customer line along much faster (and, consequently, increases gar -\nnered tips). For example, once the coffee is ground, the barista can start brewing the espresso. During the brewing, the barista can take a new order or start the process of steaming and frothing the milk. In this instance, the barista gives the perception of \n \n8 chapter  1 Functional concurrency foundations\ndoing multiple operations at the same time (multitasking), but this is only an illusion. More details on multitasking are covered in section 1.2.4. In fact, because the barista has only one espresso machine, they must stop one task to start or continue another, which means the barista executes only one task at a time, as shown in figure 1.3. In modern multicore computers, this is a waste of valuable resources. \nCombine cof fee\nand milkSteam milk\nFroth milkBrew cof feeGrind cof fee\nFigure 1.3  The barista switches between the operations (multitasking) of preparing the coffee (grind \nand brew) and preparing the milk (steam and froth). As a result, the barista executes segments of multiple tasks in an interleaved manner, giving the illusion of multitasking. But only one operation is executed at a time due to the sharing of common resources. \nConcurrency describes the ability to run several programs or multiple parts of a program \nat the same time. In computer programming, using concurrency within an application provides actual multitasking, dividing the application into multiple and independent processes that run at the same time (concurrently) in different threads. This can hap-pen either in a single CPU core or in parallel, if multiple CPU cores are available. The throughput (the rate at which the CPU processes a computation) and responsiveness of the program can be improved through the asynchronous or parallel execution of a task. An application that streams video content is concurrent, for example, because it simultaneously reads the digital data from the network, decompresses it, and updates its presentation onscreen.     \nConcurrency gives the impression that these threads are running in parallel and \nthat different parts of the program can run simultaneously. But in a single-core envi-ronment, the execution of one thread is temporarily paused and switched to another thread, as is the case with the barista in figure 1.3. If the barista wishes to speed up pro-duction by simultaneously performing several tasks, then the available resources must be increased. In computer programming, this process is called parallelism.\n1.2.3 Parallel programming executes multiples tasks simultaneously   \nFrom the developer’s prospective, we think of parallelism when we consider the ques-tions, “How can my program execute many things at once?” or “How can my program solve one problem faster?” Parallelism is the concept of executing multiple tasks at once concurrently, literally at the same time on different cores, to improve the speed of \n \n 9 Let’s start with terminology \nthe application. Although all parallel programs are concurrent, we have seen that not all concurrency is parallel. That’s because parallelism depends on the actual runtime environment, and it requires hardware support (multiple cores). Parallelism is achiev-able only in multicore devices (figure 1.4) and is the means to increasing performance and throughput of a program.\nCore 1\nCore 2\nCore 3\nCore 4Processor\nTo return to the coffee shop example, imagine that you’re the manager and wish to reduce the waiting time for customers by speeding up drink production. An intuitive solution is to hire a second barista and set up a second coffee station. With two baristas working simultaneously, the queues of customers can be processed independently and in parallel, and the preparation of cappuccinos (figure 1.5) speeds up.\nCombine cof fee\nand milkFroth milkSteam milkBrew cof feeGrind cof fee\nFigure 1.5  The production of cappuccinos is faster because two baristas can work in parallel with two \ncoffee stations.\nNo break in production results in a benefit in performance. The goal of parallelism is to maximize the use of all available computational resources; in this case, the two baris-tas are working in parallel at separate stations (multicore processing). Figure 1.4  Only multicore machines allow parallelism for \nsimultaneously executing different tasks. In this figure, each core is performing an independent task.",8774
13-1.3 Why the need for concurrency.pdf,13-1.3 Why the need for concurrency,"10 chapter  1 Functional concurrency foundations\nParallelism can be achieved when a single task is split into multiple independent \nsubtasks, which are then run using all the available cores. In figure 1.5, a multicore machine (two coffee stations) allows parallelism for simultaneously executing different tasks (two busy baristas) without interruption.\nThe concept of timing is fundamental for simultaneously executing operations in \nparallel. In such a program, operations are concurrent if they can be executed in parallel, and these operations are parallel if the executions overlap in time (see figure 1.6).\nStart\nEndFor i = 0 to n\nEvaluate modelSequential approach\nStart\nEndEvaluate\nmodelEvaluate\nmodelEvaluate\nmodelEvaluate\nmodelParallel approach\nFigure 1.6  Parallel computing is a type of computation in which many calculations are carried out \nsimultaneously, operating on the principle that large problems can often be divided into smaller ones, which are then solved at the same time.\nParallelism and concurrency are related programming models. A parallel program is also concurrent, but a concurrent program isn’t always parallel, with parallel pro-gramming being a subset of concurrent programming. While concurrency refers to the design of the system, parallelism relates to the execution. Concurrent and paral-lel programming models are directly linked to the local hardware environment where they’re performed. \n1.2.4 Multitasking performs multiple tasks concurrently over time\nMultitasking is the concept of performing multiple tasks over a period of time by exe-cuting them concurrently. We’re familiar with this idea because we multitask all the time in our daily lives. For example, while waiting for the barista to prepare our cap-puccino, we use our smartphone to check our emails or scan a news story. We’re doing two things at one time: waiting and using a smartphone.\nComputer multitasking was designed in the days when computers had a single CPU \nto concurrently perform many tasks while sharing the same computing resources. Ini-tially, only one task could be executed at a time through time slicing of the CPU. (Time slice refers to a sophisticated scheduling logic that coordinates execution between mul-\ntiple threads.) The amount of time the schedule allows a thread to run before sched-uling a different thread is called thread quantum. The CPU is time sliced so that each thread gets to perform one operation before the execution context is switched to another thread. Context switching is a procedure handled by the operating system to \n \n 11 Let’s start with terminology \nmultitask for optimized performance (figure 1.7). But in a single-core computer, it’s possible that multitasking can slow down the performance of a program by introducing extra overhead for context switching between threads.\nContext switching on\na single-core machine\nFigure 1.7  Each task has a different shade, indicating that the context switch in a single-core machine \ngives the illusion that multiple tasks run in parallel, but only one task is processed at a time. \nThere are two kinds of multitasking operating systems:\n¡ Cooperative multitasking systems, where the scheduler lets each task run until it fin-\nishes or explicitly yields execution control back to the scheduler\n¡ Preemptive multitasking systems (such as Microsoft Windows), where the scheduler \nprioritizes the execution of tasks, and the underlying system, considering the pri-ority of the tasks, switches the execution sequence once the time allocation is completed by yielding control to other tasks\nMost operating systems designed in the last decade have provided preemptive mul-titasking. Multitasking is useful for UI responsiveness to help avoid freezing the UI during long operations. \n1.2.5 Multithreading for performance improvement \nMultithreading is an extension of the concept of multitasking, aiming to improve the performance of a program by maximizing and optimizing computer resources. Multithreading is a form of concurrency that uses multiple threads of execution. Multithreading implies concurrency, but concurrency doesn’t necessarily imply multi-threading. Multithreading enables an application to explicitly subdivide specific tasks into individual threads that run in parallel within the same process. \nNOTE   A process is an instance of a program running within a computer system. \nEach process has one or more threads of execution, and no thread can exist outside a process.\nA thread is a unit of computation (an independent set of programming instructions \ndesigned to achieve a particular result), which the operating system scheduler inde-pendently executes and manages. Multithreading differs from multitasking: unlike multitasking, with multithreading the threads share resources. But this “sharing resources” design presents more programming challenges than multitasking does. We discuss the problem of sharing variables between threads later in this chapter in sec-tion 1.4.1.\n \n12 chapter  1 Functional concurrency foundations\nThe concepts of parallel and multithreading programming are closely related. But \nin contrast to parallelism, multithreading is hardware-agnostic, which means that it can be performed regardless of the number of cores. Parallel programming is a superset of multithreading. You could use multithreading to parallelize a program by sharing resources in the same process, for example, but you could also parallelize a program by executing the computation in multiple processes or even in different computers. Fig-ure 1.8 shows the relationship between these terms.\nComputer with two or more CPUs\nMultitasking\nConcurrency ConcurrencyComputer with one CPU\nMultitasking\nConcurrency Concurrency\nMultithreading\nParallelis mMultithreading\nParallelis m\nFigure 1.8  Relationship between concurrency, parallelism, multithreading, and multitasking in a single \nand a multicore device \nTo summarize:\n¡ Sequential programming refers to a set of ordered instructions executed one at a time on one CPU.\n¡ Concurrent programming handles several operations at one time and doesn’t require hardware support (using either one or multiple cores).\n¡ Parallel programming executes multiple operations at the same time on multiple CPUs. All parallel programs are concurrent, running simultaneously, but not all concurrency is parallel. The reason is that parallelism is achievable only on multi-core devices.\n¡ Multitasking concurrently performs multiple threads from different processes. Multitasking doesn’t necessarily mean parallel execution, which is achieved only when using multiple CPUs.\n¡ Multithreading extends the idea of multitasking; it’s a form of concurrency that uses multiple, independent threads of execution from the same process. Each thread can run concurrently or in parallel, depending on the hardware support.\n1.3 Why the need for concurrency?\nConcurrency is a natural part of life—as humans we’re accustomed to multitasking. We can read an email while drinking a cup of coffee, or type while listening to our favorite song. The main reason to use concurrency in an application is to increase \n \n 13 Why the need for concurrency?\nperformance and responsiveness, and to achieve low latency. It’s common sense that if one person does two tasks one after another it would take longer than if two people did those same two tasks simultaneously. \nIt’s the same with applications. The problem is that most applications aren’t written \nto evenly split the tasks required among the available CPUs. Computers are used in many different fields, such as analytics, finance, science, and health care. The amount of data analyzed is increasing year by year. Two good illustrations are Google and Pixar. \nIn 2012, Google received more than 2 million search queries per minute; in 2014, that \nnumber more than doubled. In 1995, Pixar produced the first completely computer-  \ngenerated movie, Toy Story. In computer animation, myriad details and information must be rendered for each image, such as shading and lighting. All this information changes at the rate of 24 frames per second. In a 3D movie, an exponential increase in changing information is required. \nThe creators of Toy Story used 100 connected dual-processor machines to create their \nmovie, and the use of parallel computation was indispensable. Pixar’s tools evolved for Toy Story 2; the company used 1,400 computer processors for digital movie editing, \nthereby vastly improving digital quality and editing time. In the beginning of 2000, Pixar’s computer power increased even more, to 3,500 processors. Sixteen years later, the computer power used to process a fully animated movie reached an absurd 24,000 cores. The need for parallel computing continues to increase exponentially.\nLet’s consider a processor with N  (as any number) running cores. In a single-threaded \napplication, only one core runs. The same application executing multiple threads will be faster, and as the demand for performance grows, so too will the demand for N  to grow, \nmaking parallel programs the standard programming model choice for the future. \nIf you run an application in a multicore machine that wasn’t designed with con-\ncurrency in mind, you’re wasting computer productivity because the application as it sequences through the processes will only use a portion of the available computer power. In this case, if you open Task Manager, or any CPU performance counter, you’ll notice only one core running high, possibly at 100%, while all the other cores are underused or idle. In a machine with eight cores, running non-concurrent programs means the overall use of the resources could be as low as 15% (figure 1.9).\nFigure 1.9  Windows Task Manager \nshows a program poorly utilizing CPU resources.",9860
14-1.4.1 Concurrency hazards.pdf,14-1.4.1 Concurrency hazards,"14 chapter  1 Functional concurrency foundations\nSuch waste of computing power unequivocally illustrates that sequential code isn’t the correct programming model for multicore processers. To maximize the use of the available computational resources, Microsoft’s .NET platform provides parallel execu-tion of code through multithreading. By using parallelism, a program can take full advantage of the resources available, as illustrated by the CPU performance counter in figure 1.10, where you’ll notice that all the processor cores are running high, possibly at 100%. Current hardware trends predict more cores instead of faster clock speeds; therefore, developers have no choice but to embrace this evolution and become paral-lel programmers.\n1.3.1 Present and future of concurrent programming\nMastering concurrency to deliver scalable programs has become a required skill. Com-panies are interested in hiring and investing in engineers who have a deep knowledge of writing concurrent code. In fact, writing correct parallel computation can save time and money. It’s cheaper to build scalable programs that use the computational resources available with fewer servers, than to keep buying and adding expensive hard-ware that is underused to reach the same level of performance. In addition, more hardware requires more maintenance and electric power to operate.\nThis is an exciting time to learn to write multithreaded code, and it’s rewarding to \nimprove the performance of your program with the functional programming (FP) approach. Functional programming is a programming style that treats computation as the evaluation of expressions and avoids changing-state and mutable data. Because immutability is the default, and with the addition of a fantastic composition and declar -\native programming style, FP makes it effortless to write concurrent programs. More details follow in section1.5.\nWhile it’s a bit unnerving to think in a new paradigm, the initial challenge of learning \nparallel programming diminishes quickly, and the reward for perseverance is infinite. You’ll find something magical and spectacular about opening the Windows Task Man-ager and proudly noticing that the CPU usage spikes to 100% after your code changes. Once you become familiar and comfortable with writing highly scalable systems using the functional paradigm, it will be difficult to go back to the slow style of sequential code.\nConcurrency is the next innovation that will dominate the computer industry, and it \nwill transform how developers write software. The evolution of software requirements Figure 1.10  A program written with \nconcurrency in mind can maximize CPU resources, possibly up to 100%.\n \n 15 The pitfalls of concurrent programming \nin the industry and the demand for high-performance software that delivers great user experience through non-blocking UIs will continue to spur the need for concurrency. In lockstep with the direction of hardware, it’s evident that concurrency and parallel-ism are the future of programming. \n1.4 The pitfalls of concurrent programming \nConcurrent and parallel programming are without doubt beneficial for rapid respon-siveness and speedy execution of a given computation. But this gain of performance and reactive experience comes with a price. Using sequential programs, the execu-tion of the code takes the happy path of predictability and determinism. Conversely, multithreaded programming requires commitment and effort to achieve correctness. Furthermore, reasoning about multiple executions running simultaneously is difficult because we’re used to thinking sequentially. \nDeterminism \nDeterminism is a fundamental requirement in building software as computer programs \nare often expected to return identical results from one run to the next. But this prop-erty becomes hard to resolve in a parallel execution. External circumstances, such as the operating system scheduler or cache coherence (covered in chapter 4), could influ-ence the execution timing and, therefore, the order of access for two or more threads and modify the same memory location. This time variant could affect the outcome of the program.\n \nThe process of developing parallel programs involves more than creating and spawn-ing multiple threads. Writing programs that execute in parallel is demanding and requires thoughtful design. You should design with the following questions in mind:  \n¡ How is it possible to use concurrency and parallelism to reach incredible compu-tational performance and a highly responsive application?\n¡ How can such programs take full advantage of the power provided by a multicore computer?\n¡ How can communication with and access to the same memory location between threads be coordinated while ensuring thread safety? (A method is called thread-safe when the data and state don’t get corrupted if two or more threads attempt to access and modify the data or state at the same time.)  \n¡ How can a program ensure deterministic execution?\n¡ How can the execution of a program be parallelized without jeopardizing the quality of the final result?\nThese aren’t easy questions to answer. But certain patterns and techniques can help. For example, in the presence of side effects,\n1 the determinism of the computation \nis lost because the order in which concurrent tasks execute becomes variable. The \n1 A side effect arises when a method changes some state from outside its scope, or it communicates with the “outside world,” such as calling a database or writing to the file system.\n \n16 chapter  1 Functional concurrency foundations\nobvious solution is to avoid side effects in favor of pure functions. You’ll learn these techniques and practices during the course of the book.\n1.4.1 Concurrency hazards \nWriting concurrent programs isn’t easy, and many sophisticated elements must be con-sidered during program design. Creating new threads or queuing multiple jobs on the thread pool is relatively simple, but how do you ensure correctness in the program? When many threads continually access shared data, you must consider how to safeguard the data structure to guarantee its integrity. A thread should write and modify a memory location atomically,\n2 without interference by other threads. The reality is that programs \nwritten in imperative programming languages or in languages with variables whose val-ues can change (mutable variables) will always be vulnerable to data races, regardless of the level of memory synchronization or concurrent libraries used. \nNOTE  A data race occurs when two or more threads in a single process access \nthe same memory location concurrently, and at least one of the accesses updates the memory slot while other threads read the same value without using any exclusive locks to control their accesses to that memory.\nConsider the case of two threads (Thread 1 and Thread 2) running in parallel, both trying to access and modify the shared value x as shown in figure 1.11. For Thread 1 \nto modify a variable requires more than one CPU instruction: the value must be read from memory, then modified and ultimately written back to memory. If Thread 2 tries to read from the same memory location while Thread 1 is writing back an updated value, the value of x changed. More precisely, it’s possible that Thread 1 and Thread 2 read the value x simultaneously, then Thread 1 modifies the value x and writes it back to memory, while Thread 2 also modifies the value x. The result is data corruption. This phenomenon is called race condition.\nThread 1 x = 42\nMutable shared state x = 42 x = 43 x = 43x = x + 1Modify value\nThread 2 x = 42 x = x + 1Modify value\nTimeRead value Read valueWrite value Write value\nFigure 1.11  Two threads (Thread 1 and Thread 2) run in parallel, both trying to access and modify the \nshared value x. If Thread 2 tries to read from the same memory location while Thread 1 writes back an updated value, the value of x changes. This result is data corruption or race condition.\n2 An atomic operation accesses a shared memory and completes in a single step relative to other threads. \n \n 17 The pitfalls of concurrent programming \nThe combination of a mutable state and parallelism in a program is synonymous with problems. The solution from the imperative paradigm perspective is to protect the mutable state by locking access to more than one thread at a time. This technique is called mutual exclusion because the access of one thread to a given memory loca-tion prevents access of other threads at that time. The concept of timing is central as multiple threads must access the same data at the same time to benefit from this tech-nique. The introduction of locks to synchronize access by multiple threads to shared resources solves the problem of data corruption, but introduces more complications that can lead to deadlock. \nConsider the case in figure 1.12 where Thread 1 and Thread 2 are waiting for each \nother to complete work and are blocked indefinitely in that waiting. Thread 1 acquires Lock A, and, right after, Thread 2 acquires Lock B. At this point, both threads are wait-ing on a lock that will never be released. This is a case of deadlock. \nThread 1\nThread 2\nLock Lock\nLock attempt Lock attemptDeadlock\nTimeLock A\nLock B\nFigure 1.12. In this scenario, Thread 1 acquires Lock A, and Thread 2 acquires Lock B. Then, Thread 2 tries to acquire Lock A while Thread 1 tries to acquire Lock B that is already acquired by Thread 2, which is waiting to acquire Lock A before releasing Lock B. At this point, both threads are waiting at the lock that’ll never be released. This is a case of deadlock.\nHere is a list of concurrency hazards with a brief explanation. Later, you’ll get more details on each, with a specific focus on how to avoid them:\n¡ Race condition is a state that occurs when a shared mutable resource (a file, image, variable, or collection, for example) is accessed at the same time by multi-ple threads, leaving an inconsistent state. The consequent data corruption makes a program unreliable and unusable.\n¡ Performance decline is a common problem when multiple threads share state contention that requires synchronization techniques. Mutual exclusion locks (or mutexes), as the name suggests, prevent the code from running in parallel by forcing multiple threads to stop work to communicate and synchronize memory access. The acquisition and release of locks comes with a performance penalty, slowing down all processes. As the number of cores gets larger, the cost of lock",10571
15-1.4.2 The sharing of state evolution.pdf,15-1.4.2 The sharing of state evolution,,0
16-1.4.3 A simple real-world example parallel quicksort.pdf,16-1.4.3 A simple real-world example parallel quicksort,"18 chapter  1 Functional concurrency foundations\ncontention can potentially increase. As more tasks are introduced to share the same data, the overhead associated with locks can negatively impact the compu-tation. Section 1.4.3 demonstrates the consequences and overhead costs due to introducing lock synchronization.  \n¡ Deadlock is a concurrency problem that originates from using locks. It occurs when a cycle of tasks exists in which each task is blocked while waiting for another to proceed. Because all tasks are waiting for another task to do something, they’re blocked indefinitely. The more that resources are shared among threads, the more locks are needed to avoid race condition, and the higher the risk of deadlocks.\n¡ Lack of composition is a design problem originating from the introduction of locks in the code. Locks don’t compose. Composition encourages problem dis-mantling by breaking up a complex problem into smaller pieces that are easier to solve, then gluing them back together. Composition is a fundamental tenet in FP. \n1.4.2 The sharing of state evolution \nReal-world programs require interaction between tasks, such as exchanging infor -\nmation to coordinate work. This cannot be implemented without sharing data that’s accessible to all the tasks. Dealing with this shared state is the root of most problems related to parallel programming, unless the shared data is immutable or each task has its own copy. The solution is to safeguard all the code from those concurrency prob-lems. No compiler or tool can help you position these primitive synchronization locks in the correct location in your code. It all depends on your skill as a programmer.\nBecause of these potential problems, the programming community has cried out, \nand in response, libraries and frameworks have been written and introduced into mainstream object-oriented languages (such as C# and Java) to provide concurrency safeguards, which were not part of the original language design. This support is a design correction, illustrated with the presence of shared memory in imperative and object-oriented, general-purpose programming environments. Meanwhile, functional languages don’t need safeguards because the concept of FP maps well onto concurrent programming models.\n1.4.3 A simple real-world example: parallel quicksort    \nSorting algorithms are used generally in technical computing and can be a bottleneck. \nLet’s consider a Quicksort algorithm,3 a CPU-bound computation amenable to paralleliza-\ntion that orders the elements of an array. This example aims to demonstrate the pitfalls of converting a sequential algorithm into a parallel version and points out that introducing parallelism in your code requires extra thinking before making any decisions. Otherwise, performance could potentially have an opposite outcome to that expected. \nQuicksort is a Divide and Conquer algorithm; it first divides a large array into two \nsmaller sub-arrays of low elements and high elements. Quicksort can then recursively sort the sub-arrays, and is amenable to parallelization. It can operate in place on an array, requiring small additional amounts of memory to perform the sorting. The algo-rithm consists of three simple steps, as shown in figure 1.13:\n1 Select a pivot element.\n2 Partition the sequence into subsequences according to their order relative to the pivot.\n3 Quicksort the subsequences. \n125,000\nitems250,000\nitems125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems250,000\nitems\n250,000\nitems\n250,000\nitems500,000\nitems\n1,000,000\nitems\n500,000\nitems\nFigure 1.13. The recursive function divides and conquers. Each block is divided into equal halves, where the pivot element must be the median of the sequence, until each portion of code can be executed independently. When all the single blocks are completed, they send the result back to the previous caller to be aggregated. Quicksort is based on the idea of picking a pivot point and partitioning the sequence into sub-sequence elements smaller than the pivot and bigger than the pivot elements before recursively sorting the two smaller sequences.\nRecursive algorithms, especially ones based on a form of Divide and Conquer, are a great candidate for parallelization and CPU-bound computations.\nThe Microsoft Task Parallel Library (TPL), introduced after the release of .NET 4.0, \nmakes it easier to implement and exploit parallelism for this type of algorithm. Using the TPL, you can divide each step of the algorithm and perform each task in parallel, recursively. It’s a straight and easy implementation, but you must be careful of the level of depth to which the threads are created to avoid adding more tasks than necessary. \nTo implement the Quicksort algorithm, you’ll use the FP language F#. Due to its \nintrinsic recursive nature, however, the idea behind this implementation can also be \n3 Tony Hoare invented the Quicksort algorithm in 1960, and it remains one of the most acclaimed algo-rithms with great practical value.\n \n 19 The pitfalls of concurrent programming \nparallelism in your code requires extra thinking before making any decisions. Otherwise, performance could potentially have an opposite outcome to that expected. \nQuicksort is a Divide and Conquer algorithm; it first divides a large array into two \nsmaller sub-arrays of low elements and high elements. Quicksort can then recursively sort the sub-arrays, and is amenable to parallelization. It can operate in place on an array, requiring small additional amounts of memory to perform the sorting. The algo-rithm consists of three simple steps, as shown in figure 1.13:\n1 Select a pivot element.\n2 Partition the sequence into subsequences according to their order relative to the pivot.\n3 Quicksort the subsequences. \n125,000\nitems250,000\nitems125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems\n125,000\nitems250,000\nitems\n250,000\nitems\n250,000\nitems500,000\nitems\n1,000,000\nitems\n500,000\nitems\nFigure 1.13. The recursive function divides and conquers. Each block is divided into equal halves, where the pivot element must be the median of the sequence, until each portion of code can be executed independently. When all the single blocks are completed, they send the result back to the previous caller to be aggregated. Quicksort is based on the idea of picking a pivot point and partitioning the sequence into sub-sequence elements smaller than the pivot and bigger than the pivot elements before recursively sorting the two smaller sequences.\nRecursive algorithms, especially ones based on a form of Divide and Conquer, are a great candidate for parallelization and CPU-bound computations.\nThe Microsoft Task Parallel Library (TPL), introduced after the release of .NET 4.0, \nmakes it easier to implement and exploit parallelism for this type of algorithm. Using the TPL, you can divide each step of the algorithm and perform each task in parallel, recursively. It’s a straight and easy implementation, but you must be careful of the level of depth to which the threads are created to avoid adding more tasks than necessary. \nTo implement the Quicksort algorithm, you’ll use the FP language F#. Due to its \nintrinsic recursive nature, however, the idea behind this implementation can also be \n3 Tony Hoare invented the Quicksort algorithm in 1960, and it remains one of the most acclaimed algo-rithms with great practical value.\n \n20 chapter  1 Functional concurrency foundations\napplied to C#, which requires an imperative for loop approach with a mutable state. C# \ndoesn’t support optimized tail-recursive functions such as F#, so a hazard exists of rais-ing a stack overflow exception when the call-stack pointer exceeds the stack constraint. In chapter 3, we’ll go into detail on how to overcome this C# limitation. \nListing 1.1 shows a Quicksort function in F# that adopts the Divide and Conquer \nstrategy. For each recursive iteration, you select a pivot point and use that to partition the total array. You partition the elements around the pivot point using the \nList.par -\ntition  API, then recursively sort the lists on each side of the pivot. F# has great built-in \nsupport for data structure manipulation. In this case, you’re using the List.parti -\ntion  API, which returns a tuple containing two lists: one that satisfies the predicate and \nanother that doesn’t. \nListing 1.1  Simple Quicksort algorithm\nlet rec quicksortSequential aList =    match aList with    | [] -> []    | firstElement :: restOfList ->        let smaller, larger =            List.partition (fun number -> number < firstElement) restOfList        quicksortSequential smaller @ (firstElement :: \n➥ quicksortSequential larger)\nRunning this Quicksort algorithm against an array of 1 million random, unsorted inte-gers on my system (eight logical cores; 2.2 GHz clock speed) takes an average of 6.5 seconds. But when you analyze this algorithm design, the opportunity to parallelize is evident. At the end of \nquicksortSequential , you recursively call into quicksortSe -\nquential with each partition of the array identified by the (fun number -> number < \nfirstElement) restOfList . By spawning new tasks using the TPL, you can rewrite in \nparallel this portion of the code. \nListing 1.2  Parallel Quicksort algorithm using the TPL\nlet rec quicksortParallel aList =    match aList with    | [] -> []    | firstElement :: restOfList ->        let smaller, larger =            List.partition (fun number -> number < firstElement) restOfList        let left  = Task.Run(fun () -> quicksortParallel smaller)         let right = Task.Run(fun () -> quicksortParallel larger)          left.Result @ (firstElement :: right.Result)              \nTask.Run executes the recursive calls in tasks that can run in parallel; for each recursive call, tasks are dynamically created.Appends the result for each task into a sorted array\n \n 21 The pitfalls of concurrent programming \nThe algorithm in listing 1.2 is running in parallel, which now is using more CPU resources by spreading the work across all available cores. But even with improved resource utiliza-tion, the overall performance result isn’t meeting expectations.\nExecution time dramatically increases instead of decreases. The parallelized Quick-\nsort algorithm is passed from an average of 6.5 seconds per run to approximately 12 sec-onds. The overall processing time has slowed down. In this case, the problem is that the algorithm is over-parallelized. Each time the internal array is partitioned, two new tasks are spawned to parallelize this algorithm. This design is spawning too many tasks in relation to the cores available, which is inducing parallelization overhead. This is especially true in a Divide and Conquer algorithm that involves parallelizing a recursive function. It’s important that you don’t add more tasks than necessary. The disappointing result demon-strates an important characteristic of parallelism: inherent limitations exist on how much extra threading or extra processing will help a specific algorithmic implementation. \nTo achieve better optimization, you can refactor the previous \nquicksortParallel  \nfunction by stopping the recursive parallelization after a certain point. In this way, the algorithm’s first recursions will still be executed in parallel until the deepest recursion, which will revert to the serial approach. This design guarantees taking full advantage of cores. Plus, the overhead added by parallelizing is dramatically reduced.\nListing 1.3 shows this new design approach. It takes into account the level where the \nrecursive function is running; if the level is below a predefined threshold, it stops par -\nallelizing. The function \nquicksortParallelWithDepth  has an extra argument, depth , \nwhose purpose is to reduce and control the number of times a recursive function is parallelized. The \ndepth  argument is decremented on each recursive call, and new tasks \nare created until this argument value reaches zero. In this case, you’re passing the value resulting from \nMath.Log(float System.Enviroment.ProcessorCount, 2.) + 4.  for \nthe max depth . This ensures that every level of the recursion will spawn two child tasks \nuntil all the available cores are enlisted. \nListing 1.3  A better parallel Quicksort algorithm using the TPL\nlet rec quicksortParallelWithDepth depth aList =        match aList with    | [] -> []    | firstElement :: restOfList ->        let smaller, larger =            List.partition (fun number -> number < firstElement) restOfList        if depth < 0 then               let left  = quicksortParallelWithDepth depth smaller              let right = quicksortParallelWithDepth depth larger               left @ (firstElement :: right)        else            let left  = Task.Run(fun () -> \n➥ quicksortParallelWithDepth (depth - 1) smaller) \n            let right = Task.Run(fun () -> \n➥ quicksortParallelWithDepth (depth - 1) larger)  \n            left.Result @ (firstElement :: right.Result)Tracks the function recursion level with the depth parameter \nIf the value of depth \nis negative, skips the \nparallelization \nSequentially executes the Quicksort using the current threadIf the value of depth is positive, allows the function to be called recursively, spawning two new tasks",13468
17-1.5 Why choose functional programming for concurrency.pdf,17-1.5 Why choose functional programming for concurrency,"22 chapter  1 Functional concurrency foundations\nOne relevant factor in selecting the number of tasks is how similar the predicted run \ntime of the tasks will be. In the case of quicksortParallelWithDepth , the duration of \nthe tasks can vary substantially, because the pivot points depend on the unsorted data. They don’t necessarily result in segments of equal size. To compensate for the uneven sizes of the tasks, the formula in this example calculates the \ndepth  argument to pro-\nduce more tasks than cores. The formula limits the number of tasks to approximately 16 times the number of cores because the number of tasks can be no larger than \n2 ^ \ndepth.  Our objective is to have a Quicksort workload that is balanced, and that doesn’t \nstart more tasks than required.  Starting a Task  during each iteration (recursion), when \nthe depth level is reached, saturates the processors. \nIn most cases, the Quicksort generates an unbalanced workload because the frag-\nments produced are not of equal size. The conceptual formula log2(ProcessorCount) \n+ 4 calculates the depth  argument to limit and adapt the number of running tasks \nregardless of the cases.4 If you substitute depth = log2(ProcessorCount) + 4 and \nsimplify the expression, you see that the number of tasks is 16 times ProcessorCount . \nLimiting the number of subtasks by measuring the recursion depth is an extremely important technique.\n5 \nFor example, in the case of four-core machines, the depth is calculated as follows: \ndepth = log2(ProcessorCount) + 4depth = log2(2) + 4depth = 2 + 4\nThe result is a range between approximately 36 to 64 concurrent tasks, because during each iteration two tasks are started for each branch, which in turn double in each itera-tion. In this way, the overall work of partitioning among threads has a fair and suitable distribution for each core.\n1.4.4 Benchmarking in F# \nYou executed the Quicksort sample using the F# REPL (Read-Evaluate-Print-Loop), which is a handy tool to run a targeted portion of code because it skips the compilation step of the program. The REPL fits quite well in prototyping and data-analysis devel-opment because it facilitates the programming process. Another benefit is the built-in \n#time  functionality, which toggles the display of performance information. When it’s \nenabled, F# Interactive  measures real time, CPU time, and garbage collection infor -\nmation for each section of code that’s interpreted and executed. \nTable 1.1 sorts a 3 GB array, enabling the 64-bit environment flag to avoid size restric-\ntion. It’s run on a computer with eight logical cores (four physical cores with hyper-thread-ing). On an average of 10 runs, table 1.1 shows the execution times in seconds.\n4 The function log2 is an abbreviation for Log in base 2. For example, log2(x ) represents the logarithm of \nx to the base 2.\n5 Recall that for any value a , 2 ^ (a  + 4) is the same as 16 × 2^a ; and that if a  = log2(b ), 2^a  = b.\n \n 23 Why choose functional programming for concurrency?\nTable 1.1  Benchmark of sorting with Quicksort\nSerial Parallel Parallel 4 threads Parallel 8 threads\n6.52 12.41 4.76 3.50\nIt’s important to mention that for a small array, fewer than 100 items, the parallel sort algorithms are slower than the serial version due to the overhead of creating and/or spawning new threads. Even if you correctly write a parallel program, the overhead introduced with concurrency constructors could overwhelm the program runtime, delivering the opposite expectation by decreasing performance. For this reason, it’s important to benchmark the original sequential code as a baseline and then continue to measure each change to validate whether parallelism is beneficial. A complete strat-egy should consider this factor and approach parallelism only if the array size is greater than a threshold (recursive depth), which usually matches the number of cores, after which it defaults back to the serial behavior. \n1.5 Why choose functional programming for concurrency?\nThe trouble is that essentially all the interesting applications of concurrency involve the deliberate and controlled mutation of shared state, such as screen real estate, the file system, or the internal data structures of the program. The right solution, therefore, is to provide mechanisms which allow the safe mutation of shared state section.\n—Peyton Jones, Andrew Gordon, and Sigbjorn Finne (“Concurrent Haskell,” \nProceedings of the 23rd ACM Symposium on Principles of Programming \nLanguages, St. Petersburg Beach, FL, January 1996)\nFP is about minimizing and controlling side effects, commonly referred to as pure func-tional programming. FP uses the concept of transformation, where a function creates a copy of a value x and then modifies the copy, leaving the original value x unchanged and free to be used by other parts of the program. It encourages considering whether mutability and side effects are necessary when designing the program. FP allows muta-bility and side effects, but in a strategic and explicit manner, isolating this area from the rest of the code by utilizing methods to encapsulate them. \nThe main reason for adopting functional paradigms is to solve the problems that \nexist in the multicore era. Highly concurrent applications, such as web servers and data-analysis databases, suffer from several architectural issues. These systems must be scalable to respond to a large number of concurrent requests, which leads to design challenges for handling maximum resource contention and high-scheduling fre-quency. Moreover, race conditions and deadlocks are common, which makes trouble-shooting and debugging code difficult. \nIn this chapter, we discussed a number of common issues specific to developing con-\ncurrent applications in either imperative or OOP. In these programming paradigms, we’re dealing with objects as a base construct. Conversely, in terms of concurrency, deal-ing with objects has caveats to consider when passing from a single-thread program to a massively parallelizing work, which is a challenging and entirely different scenario. \n \n24 chapter  1 Functional concurrency foundations\nNOTE  A thread is an operating system construct that functions like a virtual \nCPU. At any given moment, a thread is allowed to run on the physical CPU for a slice of time. When the time for a thread to run expires, it’s swapped off the CPU for another thread. Therefore, if a single thread enters an infinite loop, it cannot monopolize all the CPU time on the system. At the end of its time slice, it will be switched out for another thread.\nThe traditional solution for these problems is to synchronize access to resources, avoiding contention between threads. But this same solution is a double-edged sword because using primitives for synchronization, such as \nlock  for mutual exclusion, leads \nto possible deadlock or race conditions. In fact, the state of a variable (as the name variable implies) can mutate. In OOP, a variable usually represents an object that’s lia-ble to change over time. Because of this, you can never rely on its state and, consequen-tially, you must check its current value to avoid unwanted behaviors (figure 1.14). \nCore 1\nCore 2\nCore 3\nCore 4Processor Mutable\nImmutableShared stateNondeterminism\nDeterminismFunctional\nprogrammingImperative\nand OO \nprogramming\nFigure 1.14  In the functional paradigm, due to immutability as a default construct, concurrent \nprogramming guarantees deterministic execution, even in the case of a shared state. Conversely, imperative and OOP use mutable states, which are hard to manage in a multithread environment, and this leads to nondeterministic programs.\nIt’s important to consider that components of systems that embrace the FP concept can no longer interfere with each other, and they can be used in a multithreaded envi-ronment without using any locking strategies. \nDevelopment of safe parallel programs using a share of mutable variables and side- \neffect functions takes substantial effort from the programmer, who must make critical decisions, often leading to synchronization in the form of locking. By removing those fundamental problems through functional programming, you can also remove those concurrency-specific issues. This is why FP makes an excellent concurrent program-ming model. It is an exceptional fit for concurrent programmers to achieve correct high performance in highly multithreaded environments using simple code. At the",8534
18-1.7 Why use F and C for functional concurrent programming.pdf,18-1.7 Why use F and C for functional concurrent programming,"25 Why choose functional programming for concurrency?\nheart of FP, neither variables nor state are mutable and cannot be shared, and functions may not have side effects. \nFP is the most practical way to write concurrent programs. Trying to write them in \nimperative languages isn’t only difficult, it also leads to bugs that are difficult to dis-cover, reproduce, and fix.\nHow are you going to take advantage of every computer core available to you? The \nanswer is simple: embrace the functional paradigm!  \n1.5.1 Benefits of functional programming\nThere are real advantages to learning FP, even if you have no plans to adopt this style in the immediate future. Still, it’s hard to convince someone to spend their time on something new without showing immediate benefits. The benefits come in the form of idiomatic language features that can initially seem overwhelming. FP, however, is a paradigm that will give you great coding power and positive impact in your programs after a short learning curve. Within a few weeks of using FP techniques, you’ll improve the readability and correctness of your applications. \nThe benefits of FP (with focus on concurrency) include the following:\n¡ Immutability —A property that prevents modification of an object state after cre-\nation. In FP, variable assignment is not a concept. Once a value has been asso-ciated with an identifier, it cannot change. Functional code is immutable by definition. Immutable objects can be safely transferred between threads, lead-ing to great optimization opportunities. Immutability removes the problems of memory corruption (race condition) and deadlocks because of the absence of mutual exclusion.\n¡ Pure function —This has no side effects, which means that functions don’t change \nany input or data of any type outside the function body. Functions are said to be pure if they’re transparent to the user, and their return value depends only on the input arguments. By passing the same arguments into a pure function, the result won’t change, and each process will return the same value, producing con-sistent and expected behavior.\n¡ Referential transparency —The idea of a function whose output depends on and \nmaps only to its input. In other words, each time a function receives the same arguments, the result is the same. This concept is valuable in concurrent pro-gramming because the definition of the expression can be replaced with its value and will have the same meaning. Referential transparency guarantees that a set of functions can be evaluated in any order and in parallel, without changing the application's behavior. \n¡ Lazy evaluation —Used in FP to retrieve the result of a function on demand or to \ndefer the analysis of a big data stream until needed. \n¡ Composability —Used to compose functions and create higher-level abstractions \nout of simple functions. Composability is the most powerful tool to defeat com-plexity, letting you define and build solutions for complex problems. \n \n26 chapter  1 Functional concurrency foundations\nLearning to program functionally allows you to write more modular, expression- oriented, and conceptually simple code. The combinations of these FP assets will let you understand what your code is doing, regardless of how many threads the code is executing.\nLater in this book, you’ll learn techniques to apply parallelism and bypass issues \nassociated with mutable states and side effects. The functional paradigm approach to these concepts aims to simplify and maximize efficiency in coding with a declarative programming style.\n1.6 Embracing the functional paradigm \nSometimes, change is difficult. Often, developers who are comfortable in their domain knowledge lack the motivation to look at programming problems from a different per -\nspective. Learning any new program paradigm is hard and requires time to transition to developing in a different style. Changing your programming perspective requires a switch in your thinking and approach, not solely learning new code syntax for a new programming language.\nGoing from a language such as Java to C# isn’t difficult; in terms of concepts, they’re \nthe same. Going from an imperative paradigm to a functional paradigm is a far more difficult challenge. Core concepts are replaced. You have no more state. You have no more variables. You have no more side effects. \nBut the effort you make to change paradigms will pay large dividends. Most develop-\ners will agree that learning a new language makes you a better developer, and liken that to a patient whose doctor prescribes 30 minutes of exercise per day to be healthy. The patient knows the real benefits in exercise, but is also aware that daily exercise implies commitment and sacrifice. \nSimilarly, learning a new paradigm isn’t hard, but does require dedication, engage-\nment, and time. I encourage everyone who wants to be a better programmer to con-sider learning the FP paradigm. Learning FP is like riding a roller coaster: during the process there will be times when you feel excited and levitated, followed by times when you believe that you understand a principle only to descend steeply—screaming—but the ride is worth it. Think of learning FP as a journey, an investment in your personal and professional career with guaranteed return. Keep in mind that part of the learning is to make mistakes and develop skills to avoid those in the future.\nThroughout this process, you should identify the concepts that are difficult to under -\nstand and try to overcome those difficulties. Think about how to use these abstractions in practice, solving simple problems to begin with. My experience shows that you can break through a mental roadblock by finding out what the intent of a concept is by using a real example. This book will walk you through the benefits of FP applied to concur -\nrency and a distributed system. It’s a narrow path, but on the other side, you’ll emerge with several great foundational concepts to use in your everyday programming. I am confident you’ll gain new insights into how to solve complex problems and become a superior software engineer using the immense power of FP.\n \n 27 Why use F# and C# for functional concurrent programming?\n1.7 Why use F# and C# for functional concurrent programming?\nThe focus of this book is to develop and design highly scalable and performant sys-tems, adopting the functional paradigm to write correct concurrent code. This doesn’t mean you must learn a new language; you can apply the functional paradigm by using tools that you’re already familiar with, such as the multipurpose languages C# and F#. Over the years several functional features have been added to those languages, making it easier for you to shift to incorporating this new paradigm. \nThe intrinsically different approach to solving problems is the reason these lan-\nguages were chosen. Both programming languages can be used to solve the same prob-lem in very different ways, which makes a case for choosing the best tool for the job. With a well-rounded toolset, you can design a better and easier solution. In fact, as soft-ware engineers, you should think of programming languages as tools. \nIdeally, a solution should be a combination of C# and F# projects that work together \ncohesively. Both languages cover a different programming model, but the option to choose which tool to use for the job provides an enormous benefit in terms of produc-tivity and efficiency. Another aspect to selecting these languages is their different con-current programming model support, which can be mixed. For instance:\n¡ F# offers a much simpler model than C# for asynchronous computation, called asynchronous workflows. \n¡ Both C# and F# are strongly typed, multipurpose programming languages with support for multiple paradigms that encompass functional, imperative, and OOP techniques. \n¡ Both languages are part of the .NET ecosystem and derive a rich set of libraries that can be used equally by both languages. \n¡ F# is a functional-first programming language that provides an enormous pro-ductivity boost. In fact, programs written in F# tend to be more succinct and lead to less code to maintain. \n¡ F# combines the benefits of a functional declarative programming style with sup-port from the imperative object-oriented style. This lets you develop applications using your existing object-oriented and imperative programming skills. \n¡ F# has a set of built-in lock-free data structures, due to default immutable con-structors. An example is the discriminated union and the record types. These types have structural equality and don’t allow \nnull s that lead to “trusting” the \nintegrity of the data and easier comparisons.\n¡ F#, different from C#, strongly discourages the use of null  values, also known as \nthe billion-dollar mistake, and, instead, encourages the use of immutable data structures. This lack of null reference helps to minimize the number of bugs in programming.\n \n28 chapter  1 Functional concurrency foundations\nThe null reference origin\nTony Hoare introduced the null reference in 1965, while he was designing the ALGOL object-oriented language. Some 44 years later, he apologized for inventing it by calling it the billion-dollar mistake. He also said this:“. . . I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes . . ..”  \n6\n \n¡ F# is naturally parallelizable because it uses immutably as a default type construc-tor, and because of its .NET foundation, it integrates with the C# language with state-of-the-art capability at the implementation level. \n¡ C# design tends toward an imperative language, first with full support for OOP. (I like to define this as imperative OO.) The functional paradigm, during the past years and since the release of .NET 3.5, has influenced the C# language with the addition of features like lambda expressions and LINQ for list comprehension.\n¡ C# also has great concurrency tools that let you easily write parallel programs and readily solve tough real-world problems. Indeed, exceptional multicore devel-opment support within the C# language is versatile, and capable of rapid devel-opment and prototyping of highly parallel symmetric multiprocessing (SMP) applications. These programming languages are great tools for writing concur -\nrent software, and the power and options for workable solutions aggregate when used in coexistence. SMP is the processing of programs by multiple processors that share a common operating system and memory.\n¡ F# and C# can interoperate. In fact, an F# function can call a method in a C# library, and vice versa.\nIn the coming chapters, we’ll discuss alternative concurrent approaches, such as data parallelism, asynchronous, and the message-passing programming model. We’ll build libraries using the best tools that each of these programming languages can offer and compare those with other languages. We’ll also examine tools and libraries like the TPL and Reactive Extensions (Rx) that have been successfully designed, inspired, and implemented by adopting the functional paradigm to obtain composable abstraction. \nIt’s obvious that the industry is looking for a reliable and simple concurrent pro-\ngramming model, shown by the fact that software companies are investing in libraries that remove the level of abstraction from the traditional and complex memory-synchro-nization models. Examples of these higher-level libraries are Intel’s Threading Building Blocks (TBB) and Microsoft’s TPL. \nThere are also interesting open source projects, such as OpenMP (which provides \npragmas\n [compiler-specific definitions that you can use to create new preprocessor \n6 From a speech at QCon London in 2009: http:/ /mng.bz/u74T .\n \n 29 Summary\nfunctionality or to send implementation-defined information to the compiler] that you can insert into a program to make parts of it parallel) and OpenCL (a low-level language to communicate with Graphic Processing Units [GPUs]). GPU programming has traction and has been sanctioned by Microsoft with C++ AMP extensions and Accelerator .NET. \nSummary\n¡ No silver bullet exists for the challenges and complexities of concurrent and parallel programming. As a professional engineer, you need different types of ammunition, and you need to know how and when to use them to hit the target. \n¡ Programs must be designed with concurrency in mind; programmers cannot continue writing sequential code, turning a blind eye to the benefits of parallel programming. \n¡ Moore’s Law isn’t incorrect. Instead, it has changed direction toward an increased number of cores per processor rather than increased speed for a single CPU. \n¡ While writing concurrent code, you must keep in mind the distinction between concurrency, multithreading, multitasking, and parallelism.\n¡ The share of mutable states and side effects are the primary concerns to avoid in a concurrent environment because they lead to unwanted program behaviors and bugs.\n¡ To avoid the pitfalls of writing concurrent applications, you should use program-ming models and tools that raise the level of abstraction.\n¡ The functional paradigm gives you the right tools and principles to handle con-currency easily and correctly in your code. \n¡ Functional programming excels in parallel computation because immutability is the default, making it simpler to reason about the share of data.",13552
19-2.1.1 Function composition in C.pdf,19-2.1.1 Function composition in C,"302Functional programming \ntechniques for concurrency\nThis chapter covers\n¡ Solving complex problems by composing simple solutions\n¡ Simplifying functional programming with closures\n¡ Improving program performance with functional techniques\n¡ Using lazy evaluation \nWriting code in functional programming can make you feel like the driver of fast car, speeding along without the need to know how the underlying mechanics work. In chapter 1, you learned that taking an FP approach to writing concurrent applica-tions better answers the challenges in writing those applications than, for example, an object-oriented approach does. Key concepts, such as immutable variables and purity, in any FP language mean that while writing concurrent applications remains far from easy, developers can be confident that they won’t face several of the tradi-tional pitfalls of parallel programming. The design of FP means issues such as race conditions and deadlocks can’t happen.",972
20-2.1 Using function composition to solve complex problems.pdf,20-2.1 Using function composition to solve complex problems,"31 Using function composition to solve complex problems \n2In this chapter we’ll look in more detail at the main FP principles that help in our \nquest to write high-quality concurrent applications. You’ll learn what the principles are, how they work in both C# (as far as possible) and in F#, and how they fit into the pat-terns for parallel programming. \nIn this chapter, I assume that you have a familiarity with the basic principles of FP. If \nyou don’t, see appendix A for the detailed information you need to continue. By the end of this chapter, you’ll know how to use functional techniques to compose simple functions to solve complex problems and to cache and precompute data safely in a multi threaded environment to speed up your program execution.\n2.1 Using function composition to solve complex problems \nFunction composition is the combining of functions in a manner where the output from one function becomes the input for the next function, leading to the creation of a new function. This process can continue endlessly, chaining functions together to create powerful new functions to solve complex problems. Through composition, you can achieve modularization to simplify the structure of your program. \nThe functional paradigm leads to simple program design. The main motivation \nbehind function composition is to provide a simple mechanism for building easy-to- understand, maintainable, reusable, and succinct code. In addition, the composition of functions with no side effects keeps the code pure, which preserves the logic of parallel-ism. Basically, concurrent programs that are based on function composition are easier to design and less convoluted than programs that aren’t. \nFunction composition makes it possible to construct and glue together a series of \nsimple functions into a single massive and more complex function. Why is it important to glue code together? Imagine solving a problem in a top-down way. You start with the big problem and then deconstruct it into smaller problems, until eventually it’s small enough that you can directly solve the problem. The outcome is a set of small solutions that you can then glue back together to solve the original larger problem. Composition is the glue to piece together big solutions.\nThink of function composition as pipelining in the sense that the resulting value \nof one function supplies the first parameter to the subsequent function. There are differences: \n¡ Pipelining executes a sequence of operations, where the input of each function is the output of the previous function.\n¡ Function composition returns a new function that’s the combination of two or more functions and isn’t immediately invoked (input -> function -> output).\n2.1.1 Function composition in C# \nThe C# language doesn’t support function composition natively, which creates seman-tic challenges. But it’s possible to introduce the functionality in a straightforward man-ner. Consider a simple case in C# (shown in listing 2.1) using a lambda expression to define two functions.\n \n32 chapter  2 Functional programming techniques for concurrency\nListing 2.1  HOFs grindCoffee  and brewCoffee  to Espresso  in C#\nFunc<CoffeeBeans, CoffeeGround> grindCoffee = coffeeBeans                       => new CoffeeGround(coffeeBeans); Func<CoffeeGround, Espresso> brewCoffee = coffeeGround                       => new Espresso(coffeeGround); \nThe first function, grindCoffee , accepts an object coffeeBeans  as a parameter and \nreturns an instance of a new CoffeeGround . The second function, brewCoffee , takes \nas a parameter a coffeeGround  object and returns an instance of a new Espresso . The \nintent of these functions is to make an Espresso  by combining the ingredients result-\ning from their evaluation. How can you combine these functions? In C#, you have the option of executing the functions consecutively, passing the result of the first function into the second one as a chain.\nListing 2.2  Composition function in C# (bad)\nCoffeeGround coffeeGround = grindCoffee(coffeeBeans); Espresso espresso = brewCoffee(coffeeGround); Espresso espresso = brewCoffee(grindCoffee(coffeeBeans)); \nFirst, execute the function grindCoffee , passing the parameter coffeeBeans , then \npass the result coffeeGround  into the function brewCoffee . A second and equivalent \noption is to concatenate the execution of both grindCoffee  and brewCoffee , which \nimplements the basic idea of function composition. But this is a bad pattern in terms of readability because it forces you to read the code from right to left, which isn’t the nat-ural way to read English. It would be nice to read the code logically from left to right. \nA better solution is to create a generic, specialized extension method that can be \nused to compose any two functions with one or more generic input arguments. The following listing defines a \nCompose  function and refactors the previous example. (The \ngeneric arguments are in bold.) \nListing 2.3  Compose  function in C#\nstatic Func<A, C> Compose<A, B, C>(this Func<A, B> f, Func<B, C> g)                               => (n) => g(f(n)); Func<CoffeeBeans, Espresso> makeEspresso =  \n➥ grindCoffee.Compose(brewCoffee); \nEspresso espresso = makeEspresso(coffeBeans);A higher-order function, grindCoffee, returns a Func delegate that \ntakes CoffeeBeans as an argument and then returns CoffeeGround.\nA higher-order function brewCoffee returns an Espresso \nand takes a coffeeGround object as a parameter.\nShows the bad function composition that reads inside out \nCreates a generic extension method for any generic delegate Func<A,B>, which takes as an input argument a generic delegate Func<B,C> and returns a combined function Func<A,C>\nThe F# compiler has deduced that the function must use the same type for both input and output.",5844
21-2.2 Closures to simplify functional thinking.pdf,21-2.2 Closures to simplify functional thinking,"33 Using function composition to solve complex problems \nAs shown in figure 2.1, the higher-order function Compose chains the functions grind-\nCoffee and brewCoffee , creating a new function makeEspresso  that accepts an argu-\nment coffeeBeans  and executes brewCoffee (grindCoffee(coffeeBeans) . \ninput A output CCompose<A, B, C>\nFunc<A, B> funcOne Func<B, C> funcTwo\ninput\nCoffeeBeansoutput\nEspressoCompose<CoffeeBeans, CoffeeGround, Espresso >\nFunc<CoffeeBeans,\nCoffeeGround> grindCoffeeFunc<CoffeeGround,\nEspresso> brewCoffee\nFigure 2.1  Function composition from function Func<CoffeeBeans, CoffeeGround> \ngrindCoffee  to function Func<CoffeeGround, Espresso> brewCoffee . Because the output of \nfunction grindCoffee  matches the input of function brewCoffee , the functions can be composed in \na new function that maps from input CoffeeBeans  to output Espresso .\nIn the function body, you can easily see the line that looks precisely like the lambda expression \nmakeEspresso . This extension method encapsulates the notion of compos-\ning functions. The idea is to create a function that returns the result of applying the inner function \ngrindCoffee  and then applying the outer function brewCoffee  to the \nresult. This is a common pattern in mathematics, and it would be represented by the notation \nbrewCoffee of grindCoffee , meaning grindCoffee applied to brewCoffee . \nIt’s easy to create higher-order functions (HOFs)1 using extension methods to raise the \nlevel of abstraction defining reusable and modular functions. \nHaving a compositional semantic built into the language, such as in F#, helps struc-\nture the code in a declarative nature. It’s unfortunate that there’s no similarly sophisti-cated solution in C#. In the source code for this book, you’ll find a library with several overloads of the \nCompose  extension methods that can provide similar useful and reus-\nable solutions. \n2.1.2 Function composition in F# \nFunction composition is natively supported in F#. In fact, the definition of the compose  \nfunction is built into the language with the >> infix operator. Using this operator in F#, \nyou can combine existing functions to build new ones. \nLet’s consider a simple scenario where you want to increase by 4 and multiply by 3 \neach element in a list. The following listing shows how to construct this function with and without the help of function composition so you can compare the two approaches. \nListing 2.4  F# support for function composition\nlet add4 x = x + 4            \n1  A higher-order function (HOF) takes one or more functions as input and returns a function as its result.The F# compiler can infer the argument types for each function without explicit notation.\n \n34 chapter  2 Functional programming techniques for concurrency\nlet multiplyBy3 x = x * 3          let list = [0..10]           let newList = List.map(fun x ->  \n➥ multiplyBy3(add4(x))) list \nlet newList = list |>  \n➥ List.map(add4 >> multiplyBy3)\nThe example code applies the function add4  and multiplyBy3  to each element of the \nlist using the map, part of the List  module in F#. List.map  is equivalent to the Select  \nstatic method in LINQ. The combination of the two functions is accomplished using a sequential semantic approach that forces the code to read unnaturally from inside out: \nmultiplyBy3(add4(x)) . The function composition style, which uses a more idiomatic \nF# with the >> infix operator, allows the code to read from left to right as in a textbook, \nand the result is much more refined, succinct, and easier to understand. \nAnother way to achieve function composition with simple and modular code seman-\ntics is by using a technique called closures.\n2.2 Closures to simplify functional thinking\nA closure aims to simplify functional thinking, and it allows the runtime to manage \nstate, releasing extra complexity for the developer. A closure is a first-class function with free variables that are bound in the lexical environment. Behind these buzzwords hides a simple concept: closures are a more convenient way to give functions access to local state and to pass data into background operations. They are special functions that carry an implicit binding to all the nonlocal variables (also called free variables or up- values) referenced. Moreover, a closure allows a function to access one or more non-local variables even when invoked outside its immediate lexical scope, and the body of this special function can transport these free variables as a single entity, defined in its enclosing scope. More importantly, a closure encapsulates behavior and passes it around like any other object, granting access to the context in which the closure was created, reading, and updating these values. \nFree variables and closures\nThe notion of a free variable  refers to a variable referenced in a function that has neither \nthe local variables nor the parameters of that function. The purpose of closures is to make these variables available when the function is executed, even if the original vari-ables have gone out of scope. \n \nIn FP or in any other programming language that supports higher-order functions, without the support of closures the scope of the data could create problems and The F# compiler has deduced that the function must use the same type for both input and output.\nDefines a range of numbers from 0 to 10. In F#, you can define a collection using a range indicated by integers separated by the range operator.\nIn F#, you can apply HOF operations using list comprehension. The HOF map applies the same function projection to each element of the given list. In F#, the collection modules, like List, Seq, Array, and Set, take the collection argument as the last position.\nApplies HOF operations combining the function add4 and multiplyBy3 using function composition\n \n 35 Closures to simplify functional thinking\ndisadvantages. In the case of C# and F#, however, the compiler uses closures to increase and expand the scope of variables. Consequently, the data is accessible and visible in the current context, as shown in figure 2.2.\nIncrement() Outer function\nLocal variableX = 42\nreturn X++;Inner functionFunc<int> incr = Increment();\nint a = incr();int b = incr();int c = incr();\nFigure 2.2. In this example using a closure, the local variable X, in the body of the outer function \nIncrement , is exposed in the form of a function ( Func<int> ) generated by the inner function. The \nimportant thing is the return type of the function Increment , which is a function capturing the enclosed \nvariable X, not the variable itself. Each time the function reference incr  runs, the value of the captured \nvariable X increases. \nIn C#, closures have been available since .NET 2.0; but the use and definition of closures is easier since the introduction of lambda expressions and the anonymous method in .NET, which make for a harmonious mixture. \nThis section uses C# for the code samples, though the same concepts and techniques \napply to F#. This listing defines a closure using an anonymous method. \nListing 2.5  Closure defined in C# using an anonymous method \nstring freeVariable = ""I am a free variable"";     Func<string, string> lambda = value => freeVariable + "" "" + value;     \nIn this example, the anonymous function lambda  references a free variable freeVariable  \nthat’s in its enclosing scope. The closure gives the function access to its surrounding state (in this case, \nfreeVariable) , providing clearer and more readable code. Replicating the \nsame functionality without a closure probably means creating a class that you want the function to use (and that knows about the local variable), and passing that class as an argument. Here, the closure helps the runtime to manage state, avoiding the extra and unnecessary boilerplate of creating fields to manage state. This is one of the benefits of a closure: it can be used as a portable execution mechanism for passing extra context into HOFs. Not surprisingly, closures are often used in combination with LINQ. You should consider closures as a positive side effect of lambda expressions and a great programming trick for your toolbox.Indicates the free variable\nShows the anonymous function referencing a free variable",8321
22-2.2.2 Closures in a multithreading environment.pdf,22-2.2.2 Closures in a multithreading environment,"36 chapter  2 Functional programming techniques for concurrency\n2.2.1 Captured variables in closures with lambda expressions \nThe power of closures emerges when the same variable can be used even when it would have gone out of scope. Because the variable has been captured, it isn’t garbage col-lected. The advantage of using closures is that you can have a method-level variable, which is generally used to implement techniques for memory caching to improve com-putational performance. These functional techniques of memoization and functional pre-\ncomputation are discussed later in this chapter.\nListing 2.6 uses an event programming model (EPM) to download an image that \nasynchronously illustrates how captured variables work with closures. When the down-load completes, the process continues updating a client application UI. The imple-mentation uses an asynchronous semantic API call. When the request completes, the registered event \nDownloadDataCompleted  fires and executes the remaining logic. \nListing 2.6  Event register with a lambda expression capturing a local variable \nvoid UpdateImage(string url){    System.Windows.Controls.Image image = img;     var client = new WebClient();    client.DownloadDataCompleted += (o, e) =>      {        if (image != null)            using (var ms = new MemoryStream(e.Result))            {                var imageConverter = new ImageSourceConverter();                image.Source = (ImageSource) \n➥ imageConverter.ConvertFrom(ms);\n            }    };    client.DownloadDataAsync(new Uri(url));    }\nFirst, you get a reference of the image control named img. Then you use a lambda \nexpression to register the handler callback for the event DownloadDataCompleted  to \nprocess when DownloadDataAsync  completes. Inside the lambda block, the code can \naccess the state from out of scope directly due to closures. This access allows you to check the state of the image pointer, and, if it isn’t \nnull , update the UI. \nThis is a fairly straightforward process, but the timeline flow adds interesting behav-\nior. The method is asynchronous, so by the time data has returned from the service and the callback updates the \nimage , the method is already complete. \nIf the method completes, should the local variable image  be out of scope? How is the \nimage updated then? The answer is called a captured variable. The lambda expression captures the local variable \nimage , which consequently stays in scope even though nor -\nmally it would be released. From this example, you should consider captured variables as a snapshot of the values of the variables at the time the closure was created. If you built the same process without this captured variable, you’d need a class-level variable to hold the image value. Captures an instance of the local image control into the variable image\nRegisters the event DownloadDataCompleted using an inline lambda expression\nStarts DownloadDataAsync asynchronously\n \n 37 Closures to simplify functional thinking\nNOTE  The variable captured by the lambda expression contains the value at \nthe time of evaluation, not the time of capture. Instance and static variables may be used and changed without restriction in the body of a lambda. \nTo prove this, let’s analyze what happens if we add a line of code at the end of listing 2.6, changing the image reference to a \nnull  pointer (in bold).\nListing 2.7  Proving the time of captured variable evaluation \nvoid UpdateImage(string url){    System.Windows.Controls.Image image = img;    var client = new WebClient();    client.DownloadDataCompleted += (o, e) =>    {        if (image != null) {            using (var ms = new MemoryStream(e.Result))            {                var imageConverter = new ImageSourceConverter();                image.Source = (ImageSource) \n➥ imageConverter.ConvertFrom(ms);\n            }        }    };    client.DownloadDataAsync(new Uri(url));    image = null; }\nBy running the program with the applied changes, the image in the UI won’t update because the pointer is set to \nnull  before executing the lambda expression body. Even \nthough the image had a value at the time it was captured, it’s null  at the time the code \nis executed. The lifetime of captured variables is extended until all the closures refer -\nencing the variables become eligible for garbage collection.\nIn F#, the concept of null  objects doesn’t exist, so it isn’t possible to run such unde-\nsirable scenarios. \n2.2.2 Closures in a multithreading environment\nLet’s analyze a use case scenario where you use closures to provide data to a task that \noften runs in a different thread than the main one. In FP, closures are commonly used to manage mutable state to limit and isolate the scope of mutable structures, allowing thread-safe access. This fits well in a multithreading environment.\nIn listing 2.8, a lambda expression invokes the method \nConsole.WriteLine  from \na new Task of the TPL ( System.Threading.Tasks.Task ). When this task starts, the \nlambda expression constructs a closure that encloses the local variable iteration , \nwhich is passed as an argument to the method that runs in another thread. In this case, the compiler is automatically generating an anonymous class with this variable as an exposed property. Variable image is nulled; consequently, it will be disposed when out of scope. This method completes before the callback executes because the asynchronous function DownloadDataAsync doesn’t block, provoking unwanted behavior.  \n \n38 chapter  2 Functional programming techniques for concurrency\nListing 2.8  Closure capturing variables in a multithreaded environment \nfor (int iteration = 1; iteration < 10; iteration++){     Task.Factory.StartNew(() => Console.WriteLine(""{0} - {1}"", \n➥ Thread.CurrentThread.ManagedThreadId, iteration));\n}\nClosures can lead to strange behaviors. In theory, this program should work: you expect the program to print the numbers 1 to 10. But in practice, this isn’t the case; the program will print the number 10 ten times, because you’re using the same variable in several lambda expressions, and these anonymous functions share the variable value. \nLet’s analyze another example. In this listing, you pass data into two different threads \nusing lambda expressions. \nListing 2.9  Strange behavior using closures in multithreaded code \nAction<int> displayNumber = n => Console.WriteLine(n);int i = 5;Task taskOne =  Task.Factory.StartNew(() => displayNumber(i));i = 7;Task taskTwo =  Task.Factory.StartNew(() => displayNumber(i));Task.WaitAll(taskOne, taskTwo);\nEven if the first lambda expression captures the variable i before its value changes, \nboth threads will print the number 7 because the variable i is changed before both \nthreads start. The reason for this subtle problem is the mutable nature of C#. When a closure captures a mutable variable by a lambda expression, the lambda captures the reference of the variable instead of the current value of that variable. Consequently, if a task runs after the referenced value of the variable is changed, the value will be the latest in memory rather than the one at the time the variable was captured. \nThis is a reason to adopt other solutions instead of manually coding the parallel \nloop. \nParallel.For  from the TPL solves this bug. One possible solution in C# is to \ncreate and capture a new temporary variable for each Task . That way, the declaration of \nthe new variable is allocated in a new heap location, conserving the original value. This same sophisticated and ingenious behavior doesn’t apply in functional languages. Let’s look at a similar scenario using F#.\nListing 2.10  Closures capturing variables in a multithreaded environment in F# \nlet tasks = Array.zeroCreate<Task> 10for index = 1 to 10 do    tasks.[index - 1] <- Task.Factory.StartNew(fun () -> \n➥ Console.WriteLine index)",7925
23-2.3 Memoization-caching technique for program speedup.pdf,23-2.3 Memoization-caching technique for program speedup,"39 Memoization-caching technique for program speedup\nRunning this version of the code, the result is as expected: the program prints the numbers 1 to 10. The explanation is that F# handles its procedural \nfor loop differ -\nently than does C#. Instead of using a mutable variable and updating its value during each iteration, the F# compiler creates a new immutable value for every iteration with a different location in memory. The outcome of this functional behavior of preferring immutable types is that the lambda captures a reference to an immutable value that never changes.\nMultithreading environments commonly use closures because of the simplicity of \ncapturing and passing variables in different contexts that require extra thinking. The following listing illustrates how the .NET TPL library can use closures to execute multi-ple threads using the \nParallel.Invoke  API.\nListing 2.11  Closures capturing variables in a multithreaded environment \npublic void ProcessImage(Bitmap image) {   byte[] array = image.ToByteArray(ImageFormat.Bmp);    Parallel.Invoke(     () => ProcessArray(array, 0, array.Length / 2),     () => ProcessArray(array, array.Length / 2, array.Length)); }\nIn the example, Parallel.Invoke spawns two independent tasks, each running the \nProcessArray  method against a portion of the array  whose variable is captured and \nenclosed by the lambda expressions. \nTIP  Keep in mind that the compiler handles closures by allocating an object \nunderneath that encapsulates the function and its environment. Therefore, closures are heavier in terms of memory allocations than regular functions, and invoking them is slower. \nIn the context of task parallelism, be aware of variables captured in closures: because closures capture the reference of a variable, not its actual value, you can end up shar -\ning what isn’t obvious. Closures are a powerful technique that you can use to imple-ment patterns to increase the performance of your program. \n2.3 Memoization-caching technique for program speedup\nMemoization, also known as tabling, is an FP technique that aims to increase the perfor -\nmance of an application. The program speedup is achieved by caching the results of a function, and avoiding unnecessary extra computational overhead that originates from repeating the same computations. This is possible because memoization bypasses the execution of expensive function calls by storing the result of prior computations with the identical arguments (as shown in figure 2.3) for retrieval when the arguments are presented again. A memoized function keeps in memory the result of a computa-tion so it can be returned immediately in future calls. Shows the functions that convert an image to byte array format\nShows the functions to process in \nparallel the byte array split in half\n \n40 chapter  2 Functional programming techniques for concurrency\nInput\nResultMemoize\nFunction\ninitializerFunction initializer\ninputTable\nFigure 2.3  Memoization is a technique to cache values for a function, ensuring a run of only one \nevaluation. When an input value is passed into a memoized function, the internal table storage verifies if an associated result exists for this input to return immediately. Otherwise, the function initializer runs the computation, and then it updates the internal table storage and returns the result. The next time the same input value is passed into the memoized function, the table storage contains the associated result and the computation is skipped.\nThis concept may sound complex at first, but it’s a simple technique once applied. Memoization uses closures to facilitate the conversion of a function into a data structure that facilitates access to a local variable. A closure is used as a wrapper for each call to a memoized function. The purpose of this local variable, usually a lookup table, is to store the results of the internal function as a value and to use the arguments passed into this function as a key reference.\nThe memoization technique fits well in a multithreaded environment, providing an \nenormous performance boost. The main benefit arises when a function is repeatedly applied to the same arguments; but, running the function is more expensive in terms of CPU computation than accessing the corresponding data structure. To apply a color filter to an image, for example, it’s a good idea to run multiple threads in parallel. Each thread accesses a portion of the image and modifies the pixels in context. But it’s pos-sible that the filter color is applied to a set of pixels having identical values. In this case, if the computation will get the same result, why should it be re-evaluated? Instead, the result can be cached using memoization, and the threads can skip unnecessary work and finish the image processing more quickly.\nCache \nA cache is a component that stores data so future requests for that data can be served \nfaster; the data stored in a cache might be the result of an earlier computation or a dupli-cate of data stored elsewhere. \n \nThe following listing shows a basic implementation of a memoized function in C#.\n \n 41 Memoization-caching technique for program speedup\nListing 2.12  Simple example that clarifies how memoization works \nstatic Func<T, R> Memoize<T, R>(Func<T, R> func)       where T : IComparable{    Dictionary<T, R> cache = new Dictionary<T, R>();       return arg => {                                              if (cache.ContainsKey(arg))                               return cache[arg];                                  return (cache[arg] = func(arg));                             };}\nFirst, you define the Memoize  function, which internally uses the generic collection \nDictionary  as a table variable for caching. A closure captures the local variable so it \ncan be accessed from both the delegate pointing to the closure and the outer function. When the HOF is called, it first tries to match the input to the function to validate whether the parameter has already been cached. If the parameter key exists, the cache table returns the result. If the parameter key doesn’t exist, the first step is to evaluate the function with the parameter, add the parameter and the relative result to the cache table, and ultimately return the result. It’s important to mention that memoization is an HOF because it takes a function as input and returns a function as output.\nTIP  Dictionary lookups occur in constant time, but the hash function used by \nthe dictionary can be slow to execute in certain circumstances. This is the case with strings, where the time it takes to hash a string is proportional to its length. Unmemoized functions perform better than the memoized ones in certain sce-narios. I recommend profiling the code to decide if the optimization is needed and whether memoization improves performance.\nThis is the equivalent \nmemoize  function implemented in F#.\nListing 2.13  memoize  function in F#\nlet memoize func =    let table = Dictionary<_,_>()    fun x ->   if table.ContainsKey(x) then table.[x]               else                    let result = func x                    table.[x] <- result                    result\nThis is a simple example using the previously defined memoize  function. In listing 2.14, \nthe Greeting  function returns a string with a welcoming message for the name passed Shows the generic Memoize function, which requires the generic type of T to be comparable because this value is used for lookupsLists an instance of a mutable collection dictionary to store and look up values\nUses a lambda expression that captures the local table with a closure\nVerifies that the value arg has been computed and stored in the cache\nIf the key passed as an argument exists in the table, then the value associated is returned as the result.\nIf the key doesn’t exist, then the value is computed, stored in the table, and returned as the result.\n \n42 chapter  2 Functional programming techniques for concurrency\nas an argument. The message also includes the time when the function is called, which is used to keep track of time when the function runs. The code applies a two-second delay between each call for demonstration purposes. \nListing 2.14  Greeting example in C# \npublic static string Greeting(string name) {      return $""Warm greetings {name}, the time is \n➥ {DateTime.Now.ToString(""hh:mm:ss"")}"";    \n}                                    Console.WriteLine(Greeting (""Richard"")); System.Threading.Thread.Sleep(2000);Console.WriteLine(Greeting (""Paul""));System.Threading.Thread.Sleep(2000);Console.WriteLine(Greeting (""Richard""));  // outputWarm greetings Richard, the time is 10:55:34Warm greetings Paul, the time is 10:55:36Warm greetings Richard, the time is 10:55:38 \nNext, the code re-executes the same messages but uses a memoized version of the func-tion \nGreeting .\nListing 2.15  Greeting example using a memoized function\nvar greetingMemoize = Memoize<string, string>(Greeting); Console.WriteLine(greetingMemoize (""Richard"")); System.Threading.Thread.Sleep(2000);Console.WriteLine(greetingMemoize (""Paul""));System.Threading.Thread.Sleep(2000);Console.WriteLine(greetingMemoize(""Richard""));     // outputWarm greetings Richard, the time is 10:57:21     Warm greetings Paul, the time is 10:57:23        Warm greetings Richard, the time is 10:57:21     \nThe output indicates that the first two calls happened at different times as anticipated. But what happens in a third call? Why does the third function call return the message with the exact same time as the first one? The answer is memoization. \nThe first and third function \ngreetingMemoize(""Richard"")  calls have the same \narguments, and their results have been cached only once during the initial call by the function \ngreetingMemoize . The result from the third function call isn’t the effect of its \nexecution, but is the stored result of the function with the same argument, and conse-quently the time matches.\nThis is how memoization works. The memoized function’s job is to look up the argu-\nment passed in an internal table. If it finds the input value, it returns the previously computed result. Otherwise, the function stores the result in the table. Memoize is an HOF , so you pass a function as an argument that constrains the signature of the former function. In this way, the memoized function can replace the original one, injecting caching functionality.\nThe time of the Greeting function is the same because the computation happens only once and then the result is memoized.",10585
24-2.4 Memoize in action for a fast web crawler.pdf,24-2.4 Memoize in action for a fast web crawler,"43 Memoize in action for a fast web crawler\n2.4 Memoize in action for a fast web crawler\nNow, you’ll implement a more interesting example using what you learned in the pre-vious section. For this example, you’ll build a web crawler that extracts and prints into the console the page title of each website visited. Listing 2.16 runs the code without memoization. Then you’ll re-execute the same program using the memoization tech-nique and compare the outcome. Ultimately, you’ll download multiple websites’ con-tents, combining parallel execution and memoization.\nListing 2.16  Web crawler in C# \npublic static IEnumerable<string> WebCrawler(string url) {         string content = GetWebContent(url);       yield return content;       foreach (string item in AnalyzeHtmlContent(content))       yield return GetWebContent(item);}static string GetWebContent(string url) {        using (var wc = new WebClient())           return wc.DownloadString(new Uri(url));}static readonly Regex regexLink =         new Regex(@""(?<=href=('|""""))https?://.*?(?=\1)"");static IEnumerable<string> AnalyzeHtmlContent(string text) {          foreach (var url in regexLink.Matches(text))               yield return url.ToString();}static readonly Regex regexTitle =         new Regex(""<title>(?<title>.*?)<\\/title>"", RegexOptions.Compiled);static string ExtractWebPageTitle(string textPage) {         if (regexTitle.IsMatch(textPage))               return regexTitle.Match(textPage).Groups[""title""].Value;        return ""No Page Title Found!"";}\nThe WebCrawler function downloads the content of the web page URL passed as an \nargument by calling the method GetWebContent . Next, it analyzes the content down-\nloaded and extracts the hyperlinks contained in the web page, which are sent back to the initial function to be processed, repeating the operations for each of the hyper -\nlinks. Here is the web crawler in action.\nListing 2.17  Web crawler execution \nList<string> urls = new List<string> {     @""http://www.google.com"",    @""http://www.microsoft.com"",Shows the function that recursively fetches and analyzes the content of a website and sub-websites\nDownloads the content of a website in string format\nExtracts sub-links from \nthe content of a website\nExtracts the page title of a website\nInitializes a website list to analyze\n \n44 chapter  2 Functional programming techniques for concurrency\n    @""http://www.bing.com"",    @""http://www.google.com""};var webPageTitles = from url in urls                     from pageContent in WebCrawler(url)                    select ExtractWebPageTitle(pageContent);foreach (var webPageTitle in webPageTitles)    Console.WriteLine(webPageTitle);                 // OUTPUT Starting Web Crawler for http://www.google.com...GoogleGoogle Images ...Web Crawler completed for http://www.google.com in 5759msStarting Web Crawler for http://www.microsoft.com...Microsoft CorporationMicrosoft - Official Home PageWeb Crawler completed for http://www.microsoft.com in 412msStarting Web Crawler for http://www.bing.com...BingMsn...Web Crawler completed for http://www.bing.com in 6203msStarting Web Crawler for http://www.google.com...GoogleGoogle Images...Web Crawler completed for http://www.google.com in 5814ms\nYou’re using LINQ (Language Integrated Query) to run the web crawler against a col-lection of given URLs. When the query expression is materialized during the \nforeach  \nloop, the function ExtractWebPageTitle  extracts the page title from each page’s con-\ntent and prints it to the console. Because of the cross-network nature of the operation, the function \nGetWebContent  requires time to complete the download. One problem \nwith the previous code implementation is the existence of duplicate hyperlinks. It’s com-mon that web pages have duplicate hyperlinks, which in this example cause redundant and unnecessary downloads. A better solution is to memoize the function \nWebCrawler . \nListing 2.18  Web crawler execution using memoization\nstatic Func<string, IEnumerable<string>> WebCrawlerMemoized = \n➥Memoize<string, IEnumerable<string>>(WebCrawler);    \nvar webPageTitles = from url in urls                             from pageContent in WebCrawlerMemoized(url)                    select ExtractWebPageTitle(pageContent);Uses a LINQ expression to analyze the websites from the URL collection \nMemoized version of the function WebCrawler\nUses a LINQ expression in combination with \nthe memoize function to analyze the websites\n \n 45 Memoize in action for a fast web crawler\nforeach (var webPageTitle in webPageTitles)             Console.WriteLine(webPageTitle);                // OUTPUT Starting Web Crawler for http://www.google.com...GoogleGoogle Images ...Web Crawler completed for http://www.google.com in 5801msStarting Web Crawler for http://www.microsoft.com...Microsoft CorporationMicrosoft - Official Home PageWeb Crawler completed for http://www.microsoft.com in 4398msStarting Web Crawler for http://www.bing.com...BingMsn...Web Crawler completed for http://www.bing.com in 6171msStarting Web Crawler for http://www.google.com...GoogleGoogle Images...Web Crawler completed for http://www.google.com in 02ms\nIn this example, you implemented the HOF WebCrawlerMemoized , which is the \nmemoized version of the function WebCrawler . The output confirms that the \nmemoized version of the code runs faster. In fact, to extract the content a second time from the web page www.google.com took only 2 ms, as opposed to more than 5 seconds without memoization.\nA further improvement should involve downloading the web pages in parallel. For -\ntunately, because you used LINQ to process the query, only a marginal code change is required to use multiple threads. Since the advent of the .NET 4.0 framework, LINQ has an extension method \nAsParallel()  to enable a parallel version of LINQ (or PLINQ). \nThe nature of PLINQ is to deal with data parallelism; both topics will be covered in chapter 4.\nLINQ and PLINQ are technologies designed and implemented using functional \nprogramming concepts, with special attention to emphasizing a declarative program-ming style. This is achievable because the functional paradigm tends to raise the level of abstraction in comparison with other program paradigms. Abstraction consents to write code without the need to know the implementation details of the underlying library, as shown here.\nListing 2.19  Web crawler query using PLINQ\n    var webPageTitles = from url in urls.AsParallel()                          from pageContent in WebCrawlerMemoized(url)                     select ExtractWebPageTitle(pageContent); Implements an extension method that enables LINQ to use multiple threads to process the query",6730
25-2.6 Effective concurrent speculation to amortize the cost of expensive computations.pdf,25-2.6 Effective concurrent speculation to amortize the cost of expensive computations,"46 chapter  2 Functional programming techniques for concurrency\nPLINQ is easy to use and can give you substantial performance benefits. Although we only showed one method, the \nAsParallel  extension method, there’s more to it than that. \nBefore running the program, you have one more refactoring to apply—caches. \nBecause they must be accessible by all threads, caches tend to be static. With the intro-duction of parallelism, it’s possible for multiple threads to simultaneously access the memoize function, causing a race-condition problem due to the underlying mutable data structure exposed. The race-condition problem is discussed in the previous chap-ter. Fortunately, this is an easy fix, as shown in this listing.\nListing 2.20  Thread-safe memoization function \npublic Func<T, R> MemoizeThreadSafe<T, R>(Func<T, R> func)                                                  where T : IComparable{  ConcurrentDictionary<T, R> cache = new ConcurrentDictionary<T, R>();   return arg => cache.GetOrAdd(arg, a => func(a));}public Func<string, IEnumerable<string>> WebCrawlerMemoizedThreadSafe =                   MemoizeThreadSafe<string, IEnumerable<string>>(WebCrawler);var webPageTitles =                  from url in urls.AsParallel()                 from pageContent in WebCrawlerMemoizedThreadSafe(url)                  select ExtractWebPageTitle(pageContent);  \nThe quick answer is to replace the current Dictionary  collection with the equivalent \nthread-safe version ConcurrentDictionary . This refactoring interestingly requires less \ncode. Next, you implement a thread-safe memoized version of the function GetWeb-\nContent , which is used for the LINQ expression. Now you can run the web crawler in \nparallel. To process the pages from the example, a dual-core machine can complete the analysis in less than 7 seconds, compared to the 18 seconds of the initial imple-mentation. The upgraded code, besides running faster, also reduces the network I/O operations.\n2.5 Lazy memoization for better performance\nIn the previous example, the web crawler allows multiple concurrent threads to access the memoized function with minimum overhead. But it doesn’t enforce the function initializer \nfunc(a)  from being executed multiple times for the same value, while eval-\nuating the expression. This might appear to be a small issue, but in highly concur -\nrent applications, the occurrences multiply (in particular, if the object initialization is expensive). The solution is to add an object to the cache that isn’t initialized, but rather a function that initializes the item on demand. You can wrap the result value from the function initializer into a \nLazy  type (as highlighted with bold in listing 2.21). \nThe listing shows the memoization solution, which represents a perfect design in terms of thread safety and performance, while avoiding duplicate cache item initialization. Shows thread-safe memoization using the \nConcurrent collection ConcurrentDictionary\nUses a PLINQ expression to analyze the websites in parallel \n \n 47 Lazy memoization for better performance\nListing 2.21  Thread-safe memoization function with safe lazy evaluation \nstatic Func<T, R> MemoizeLazyThreadSafe<T, R>(Func<T, R> func) where T : IComparable{    ConcurrentDictionary<T, Lazy<R>> cache = \n➥ new ConcurrentDictionary<T, Lazy<R>>(); \n    return arg => cache.GetOrAdd(arg, a => \n➥ new Lazy<R>(() => func(a))).Value;\n}\nAccording to the Microsoft documentation, the method GetOrAdd  doesn’t prevent the \nfunction func  from being called more than once for the same given argument, but it \ndoes guarantee that the result of only one “evaluation of the function” is added to the collection. There could be multiple threads checking the cache concurrently before the cached value is added, for example. Also, there’s no way to enforce the function \nfunc(a)  to be thread safe. Without this guarantee, it’s possible that in a multithreaded \nenvironment, multiple threads could access the same function simultaneously—mean-ing \nfunc(a)  should also be thread safe itself. The solution proposed, avoiding prim-\nitive locks, is to use the Lazy<T>  construct in .NET 4.0. This solution gives you the \nguarantee of full thread safety, regardless of the implementation of the function func , \nand ensures a single evaluation of the function. \n2.5.1 Gotchas for function memoization\nThe implementations of memoization introduced in the previous code examples are a somewhat naive approach. The solution of storing data in a simple dictionary works, but it isn’t a long-term solution. A dictionary is unbounded; consequently, the items are never removed from memory but only added, which can, at some point, lead to memory leak issues. Solutions exist to all these problems. One option is to implement a memoize function that uses a \nWeakReference  type to store the result values, which \npermits the results to be collected when the garbage collector (GC) runs. Since the introduction of the collection \nConditionalWeakDictionary  with the .NET 4.0 frame-\nwork, this implementation is simple: a dictionary takes as a key a type instance that’s held as a weak reference. The associated values are kept as long as the key lives. When the key is reclaimed by the GC to be collocated, the reference to the data is removed, mak-ing it available for collection. \nWeak references are a valuable mechanism for handling references to managed \nobjects. The typical object reference (also known as a strong reference) has a deter -\nministic behavior, where as long as you have a reference to the object, the GC won’t collect the object that consequently stays alive. But in certain scenarios, you want to keep an invisible string attached to an object without interfering with the GC’s ability to reclaim that object’s memory. If the GC reclaimed the memory, your string becomes unattached and you can detect this. If the GC hasn’t touched the object yet, you can pull the string and retrieve a strong reference to the object to use it again. This facility is useful for automatically managing a cache that can keep weak references to the least Uses thread-safe and lazy-evaluated memoization \n \n48 chapter  2 Functional programming techniques for concurrency\nrecently used objects without preventing them from being collected and inevitably opti-mizing the memory resources.\nAn alternative option is to use a cache-expiration policy by storing a timestamp \nwith each result, indicating the time when the item is persisted. In this case, you have to define a constant time to invalidate the items. When the time expires, the item is removed from the collection. The downloadable source code for this book holds both of these implementations.\nTIP  It’s good practice to consider memoization only when the cost to evaluate \na result is higher than the cost to store all the results computed during run-time. Before making the final decision, benchmark your code with and without memoization, using a varying range of values.\n2.6 Effective concurrent speculation to amortize the cost of expensive computations\nSpeculative Processing (precomputation) is a good reason to exploit concurrency. Specula-tive Processing is an FP pattern in which computations are performed before the actual algorithm runs, and as soon as all the inputs of the function are available. The idea behind concurrent speculation is to amortize the cost of expensive computation and improve the performance and responsiveness of a program. This technique is easily applicable in par -\nallel computing, where multicore hardware can be used to precompute multiple opera-tions spawning a concurrently running task and have the data ready to read without delay. \nLet’s say you’re given a long list of input words, and you want to compute a function \nthat finds the best fuzzy match\n2 of a word in the list. For the fuzzy-match algorithm, \nyou’re going to apply the Jaro-Winkler distance, which measures the similarity between two strings. We’re not going to cover the implementation of this algorithm here. You can find the complete implementation in the online source code.\nJaro-Winkler algorithm\nThe Jaro-Winkler distance is a measure of similarity between two strings. The higher the \nJaro-Winkler distance, the more similar the strings. The metric is best suited for short strings, such as proper names. The score is normalized so that 0 equates to no similarity and 1 is an exact match.\n \nThis listing shows the implementation of the fuzzy-match function using the Jaro-Winkler algorithm (highlighted in bold).\nListing 2.22  Implementing a fuzzy match in C#\npublic static string FuzzyMatch(List<string> words, string word){\n2 Fuzzy matching is a technique  of finding segments of text and corresponding matches that may be less than 100% perfect.\n \n 49 Effective concurrent speculation to amortize the cost of expensive computations\n    var wordSet = new HashSet<string>(words);       string bestMatch =        (from w in wordSet.AsParallel()                     select JaroWinklerModule.Match(w, word))            .OrderByDescending(w => w.Distance)            .Select(w => w.Word)            .FirstOrDefault();    return bestMatch;                           }\nThe function FuzzyMatch  uses PLINQ to compute in parallel the fuzzy match for the \nword passed as an argument against another array of strings. The result is a HashSet  \ncollection of matches, which is then ordered by best match to return the first value from the list. \nHashSet  is an efficient data structure for lookups. \nThe logic is similar to a lookup. Because List<string> words  could contain dupli-\ncates, the function first instantiates a data structure that’s more efficient. Then the function utilizes this data structure to run the actual fuzzy match. This implementation isn’t efficient, as the design issue is evident: \nFuzzyMatch  is applied each time it’s called \nto both of its arguments. The internal table structure is rebuilt every time FuzzyMatch  is \nexecuted, wasting any positive effect. \nHow can you improve this efficiency? By applying a combination of a partial function \napplication or a partial application and the memoization technique from FP, you can achieve precomputation. For more details about partial application, see appendix A. The concept of precomputation is closely related to memoization, which in this case uses a table containing pre-calculated values. The next listing shows the implementa-tion of a faster fuzzy-match function (as highlighted in bold). \nListing 2.23  Fast fuzzy match using precomputation \nstatic Func<string, string> PartialFuzzyMatch(List<string> words) {    var wordSet = new HashSet<string>(words);       return word =>        (from w in wordSet.AsParallel()            select JaroWinklerModule.Match(w, word))            .OrderByDescending(w => w.Distance)            .Select(w => w.Word)            .FirstOrDefault();                  }Func<string, string> fastFuzzyMatch =  \n➥ PartialFuzzyMatch(words); \nstring magicFuzzyMatch = fastFuzzyMatch(""magic"");string lightFuzzyMatch = fastFuzzyMatch(""light"");   Removes possible word duplicates by creating a HashSet collection from the word list. HashSet is an efficient data structure for lookups.Uses PLINQ to run in parallel the best match algorithm\nReturns the best match \nA partial applied function takes only one parameter and returns a new function that performs a clever lookup.\nEfficient lookup data structure is kept in a closure after instantiation and is used by the lambda expression.\nNew function that uses the same lookup data for each call, reducing repetitive computation\nShows the function that precomputes the List<string> words passed and returns a function that takes the word argument\nUses the fastFuzzyMatch function",11847
26-2.7.2 Lazy caching technique and thread-safe Singleton pattern.pdf,26-2.7.2 Lazy caching technique and thread-safe Singleton pattern,"50 chapter  2 Functional programming techniques for concurrency\nFirst, you create a partial applied version of the function PartialFuzzyMatch . This new \nfunction takes as an argument only List<string> words  and returns a new function \nthat handles the second argument. This is a clever strategy because it consumes the first argument as soon as it’s passed, by precomputing the efficient lookup structure. \nInterestingly, the compiler uses a closure to store the data structure, which is accessi-\nble through the lambda expression returned from the function. A lambda expression is an especially handy way to provide a function with a precomputed state. Then, you can define the \nfastFuzzyMatch  function by supplying the argument List<string> words , \nwhich is used to prepare an underlying lookup table, resulting in faster computation. After supplying \nList<string> words , fastFuzzyMatch  returns a function that takes \nthe string word  argument, but immediately computes the HashSet  for the lookup. \nNOTE  The function fuzzyMatch  in listing 2.22 is compiled as a static function \nthat constructs a set of strings on every call. Instead, fastFuzzyMatch  in list-\ning 2.23 is compiled as a static read-only property, where the value is initialized in a static constructor. This is a fine difference, but it has a massive effect on code performance. \nWith these changes, the processing time is reduced by half when performing the fuzzy match against the strings magic and light, compared to the one that calculates these values as needed.\n2.6.1 Precomputation with natural functional support\nNow let’s look at the same fuzzy-match implementation using the functional language F#. Listing 2.24 shows a slightly different implementation due to the intrinsic func-tional semantic of F# (the \nAsParallel  method is highlighted in bold). \nListing 2.24  Implementing a fast fuzzy match in F#\nlet fuzzyMatch (words:string list) =    let wordSet = new HashSet<string>(words)        let partialFuzzyMatch word =                        query { for w in wordSet.AsParallel() do                    select (JaroWinkler.getMatch w word) }        |> Seq.sortBy(fun x -> -x.Distance)        |> Seq.head    fun word -> partialFuzzyMatch word          let fastFuzzyMatch = fuzzyMatch words           let magicFuzzyMatch = fastFuzzyMatch ""magic""let lightFuzzyMatch = fastFuzzyMatch ""light”""   Creates an efficient lookup data structure HashSet that also removes duplicate words In F#, all the functions are curried as a default: the signature of the FuzzyMatch function is (string set -> string -> string), which implies that it can be directly partially applied. In this case, by supplying only the first argument, wordSet, you create the partial applied function partialFuzzyMatch.\nReturns a function that uses a lambda expression to “closure over” and exposes the internal HashSet Applies the precomputation technique by supplying the first argument to the function fuzzyMatch, which immediately computes the HashSet with the values passed\nUses the fastFuzzyMatch function\n \n 51 Effective concurrent speculation to amortize the cost of expensive computations\nThe implementation of fuzzyMatch  forces the F# runtime to generate the internal set \nof strings on each call. In opposition, the partial applied function fastFuzzyMatch  \ninitializes the internal set only once and reuses it for all the subsequent calls. Precom-putation is a caching technique that performs an initial computation to create, in this case, a \nHashSet<string>  ready to be accessed.\nThe F# implementation uses a query expression to query and transform the data. \nThis approach lets you use PLINQ as in the equivalent C# in listing 2.23. But in F# there’s a more functional style to parallelize operations on sequences—adopting the parallel sequence (\nPSeq ). Using this module, the function fuzzyMatch  can be rewritten \nin a compositional form:\nlet fuzzyMatch (words:string list) =    let wordSet = new HashSet<string>(words)    fun word ->        wordSet        |> PSeq.map(fun w -> JaroWinkler.getMatch w word)        |> PSeq.sortBy(fun x -> -x.Distance)        |> Seq.head\nThe code implementations of fuzzyMatch  in C# and F# are equivalent, but the former \nfunctional language is curried as a default. This makes it easier to refactor using partial application. The F# parallel sequence \nPSeq  used in the previous code snippet is cov-\nered in chapter 5.\nIt’s clearer by looking at the fuzzyMatch  signature type: \nstring set -> (string -> string)\nThe signature reads as a function that takes a string set as an argument, returns a func-tion that takes a string as an argument, and then returns a string as a return type. This chain of functions lets you utilize the partial application strategy without thinking about it.\n2.6.2 Let the best computation win \nAnother example of speculative evaluation is inspired by the unambiguous choice operator,\n3 created by Conal Elliott (http:/ /conal.net)  for his functional reactive pro-\ngramming (FRP) implementation (http:/ /conal.net/papers/push-pull-frp). The idea behind this operator is simple: it’s a function that takes two arguments and concur -\nrently evaluates them, returning the first result  available. \nThis concept can be extended to more than two parallel functions. Imagine that \nyou’re using multiple weather services to check the temperature in a city. You can simul-taneously spawn separate tasks to query each service, and after the fastest task returns, you don’t need to wait for the other to complete. The function waits for the fastest task to come back and cancels the remaining tasks. The following listing shows a simple implementation without support for error handling.\n3 Conal Elliott, “Functional Concurrency with Unambiguous Choice,” November 21, 2008, http:/ /\nmng.bz/4mKK. \n \n52 chapter  2 Functional programming techniques for concurrency\nListing 2.25  Implementing the fastest weather task\npublic Temperature SpeculativeTempCityQuery(string city, \n➥ params Uri[] weatherServices)\n{    var cts = new CancellationTokenSource();        var tasks =    (from uri in weatherServices        select Task.Factory.StartNew<Temperature>(() =>                queryService(uri, city), cts.Token)).ToArray();     int taskIndex = Task.WaitAny(tasks);            Temperature tempCity = tasks[taskIndex].Result;    cts.Cancel();                                   return tempCity;}\nPrecomputation is a crucial technique for implementing any kind of function and ser -\nvice, from simple to complex and more advanced computation engines. Speculative evaluation aims to consume CPU resources that would otherwise sit idle. This is a con-venient technique in any program, and it can be implemented in any language that supports closure to capture and expose these partial values. \n2.7 Being lazy is a good thing\nA common problem in concurrency is having the ability to correctly initialize a shared object in a thread-safe manner. To improve the startup time of an application when the object has an expensive and time-consuming construct, this need is even more accentuated. \nLazy evaluation is a programming technique used to defer the evaluation of an \nexpression until the last possible moment, when it’s accessed. Believe it or not, laziness can lead to success—and in this case, it’s an essential tool for your tool belt. Somewhat counterintuitive, the power of lazy evaluation makes a program run faster because it only provides what’s required for the query result, preventing excessive computations. Imagine writing a program that executes different long-running operations, possibly analyzing large amounts of data to produce various reports. If these operations are eval-uated simultaneously, the system can run into performance issues and hang. Plus, it’s possible that not all of these long-running operations are immediately necessary, which provokes a waste of resources and time if they begin right away. \nA better strategy is to perform long-running operations on demand and only as \nneeded, which also reduces memory pressure in the system. In effect, lazy evaluation also leads to efficient memory management, improving performance due to lower memory consumption. Being lazy in this case is more efficient. Reducing unnecessary and expensive garbage collection cleanups in managed programming languages—such as C#, Java, and F#—makes the programs run faster.Uses a cancellation token to cancel a task when the fastest completes\nUses LINQ to spawn in parallel one task for each weather service Waits for the fastest task to come back\nCancels remaining slower tasks\n \n 53 Being lazy is a good thing\n2.7.1 Strict languages for understanding concurrent behaviors\nThe opposite of lazy evaluation is eager evaluation, also known as strict evaluation, which means that the expression is evaluated immediately. C# and F#, as well as the majority of other mainstream programming languages, are strict languages. \nImperative programming languages don’t have an internal model for containing \nand controlling side effects, so it’s reasonable that they’re eagerly evaluated. To under -\nstand how a program executes, a language that’s strictly evaluated must know the order in which side effects (such as I/O) run, making it easy to understand how the program executes. In fact, a strict language can analyze the computation and have an idea of the work that must be done.\nBecause both C# and F# aren’t purely FP languages, there’s no guarantee that every \nvalue is referentially transparent; consequently, they cannot be lazily evaluated pro-gramming languages.\nIn general, lazy evaluation can be difficult to mix with imperative features, which \nsometimes introduce side effects, such as exception and I/O operation, because the \norder of operations becomes non-deterministic. For more information, I recommend “Why Functional Programming Matters,” by John Hughes (http://mng.bz/qp3B).\nIn FP, lazy evaluation and side effects cannot coexist. Despite the possibility of add-\ning the notion of lazy evaluation in an imperative language, the combination with side effects makes the program complex. In fact, lazy evaluation forces the developer to remove the order of execution constraints and dependencies according to which parts of the program are evaluated. Writing a program with side effects can become difficult because it requires the notion of function order of execution, which reduces the oppor -\ntunity for code modularity and compositionality. Functional programming aims to be explicit about side effects, to be aware of them, and to provide tools to isolate and con-trol them. For instance, Haskell uses the functional programming language convention of identifying a function with side effects with the \nIO type. This Haskell function defini-\ntion reads a file, causing side effects:\nreadFile :: IO ()     \nThis explicit definition notifies the compiler of the presence of side effects, and the compiler then applies optimization and validation as needed.\nLazy evaluation becomes an important technique with multicore and multithread-\ning programs. To support this technique, Microsoft introduced (with Framework 4.0) a generic type constructor called \nLazy<T> , which simplifies the initialization of objects with \ndeferred creation in a thread-safe fashion. Here’s the definition of a lazy object Person .\nListing 2.26  Lazy initialization of the Person  object\nclass Person     {        public readonly string FullName;         public Person(string firstName, string lastName)    {        FullName = firstName + "" "" + lastName;Defines the Person class\nShows the read-only field for the full name of the person assigned in the constructor \n \n54 chapter  2 Functional programming techniques for concurrency\n        Console.WriteLine(FullName);        }} Lazy<Person> fredFlintstone = new Lazy<Person>(() => \n➥ new Person(""Fred"", ""Flintstone""), true);          \nPerson[] freds = new Person[5]; for(int i = 0;i < freds.Length;i++)           freds[i] = fredFlintstone.Value; // outputFred Flintstone\nIn the example, you define a simple Person  class with a read-only field, which also \ncauses FullName  to print on the console. Then, you create a lazy initializer for this \nobject by supplying a factory delegate into Lazy<Person> , which is responsible for the \nobject instantiation. In this case, a lambda expression is convenient to use in place of the factory delegate. Figure 2.4 illustrates this.\nLazyValue\nInitialization Person\nValue\nValue\nValue\nValueLazy Person\nLazy Person\nLazy Person\nLazy Personperson\narray\nFigure 2.4. The value of the Person  object is initialized only once, when the Value  property is accessed \nthe first time. Successive calls return the same cached value. If you have an array of Lazy<Person>  \nobjects, when the items of the array are accessed, only the first one is initialized. The others will reuse the cache result.\nWhen the actual evaluation of the expression is required to use the underlying object \nPerson , you access the Value  property on the identifier, which forces the factory del-\negate of the Lazy object to be performed only one time if the value isn’t materialized \nyet. No matter how many consecutive calls or how many threads simultaneously access the lazy initializer, they all wait for the same instance. To prove it, the listing creates an array of five \nPerson s, which is initialized in the for loop. During each iteration, the \nPerson  object is retrieved by calling the identifier property Value , but even if it’s called \nfive times, the output ( Fred Flintstone)  is called only once. \n2.7.2 Lazy caching technique and thread-safe Singleton pattern\nLazy evaluation in .NET is considered a caching technique because it remembers the result of the operation that has been performed, and the program can run more effi-ciently by avoiding repetitive and duplicate operations. \nBecause the execution operations are done on demand and, more importantly, only \nonce, the \nLazy<T>  construct is the recommended mechanism to implement a Single-\nton pattern. The Singleton pattern creates a single instance of a given resource, which is Initializes the lazy object Person; the return value is Lazy<Person>, which isn’t evaluated until it’s forced\nShows the array of five people\nThe instance of the underlying lazy object is available via the Value property.\n \n 55 Being lazy is a good thing\nshared within the multiple parts of your code. This resource needs to be initialized only once, the first time it’s accessed, which is precisely the behavior of \nLazy<T> . \nYou have different ways of implementing the Singleton pattern in .NET, but certain \nof these techniques have limitations, such as unguaranteed thread safety or lost lazy instantiation.\n4 The Lazy<T>  construct provides a better and simpler singleton design, \nwhich ensures true laziness and thread safety, as shown next. \nListing 2.27  A Singleton pattern using Lazy<T>\npublic sealed class Singleton{    private static readonly Lazy<Singleton> lazy =        new Lazy<Singleton>(() => new Singleton(), true);        public static Singleton Instance => lazy.Value;    private Singleton()    { }}\nThe Lazy<T>  primitive also takes a Boolean flag, passed after the lambda expression, as \nan optional argument to enable thread-safe behavior. This implements a sophisticated and light version of the Double-Check Locking pattern. \nNOTE  In software engineering, Double-Checked Locking (also known as dou-\nble-checked locking optimization) is a software design pattern used to reduce the overhead of acquiring a lock by first testing the locking criterion (the “lock hint”) without acquiring the lock. \nThis property guarantees that the initialization of the object is thread safe. When the flag is enabled, which is the default mode, no matter how many threads call the Sin-gleton \nLazyInitializer , all the threads receive the same instance, which is cached \nafter the first call. This is a great advantage, without which you’d be forced to manually guard and ensure the thread safety for the shared field. \nIt’s important to emphasize that if the lazy-evaluated object implementation is thread-\nsafe, that doesn’t automatically mean that all its properties are thread safe as well.\nLazyInitializer\nIn .NET, LazyInitializer  is an alternative static class that works like Lazy<T> , but \nwith optimized initialization performance and more convenient access. In fact, there’s no need for the \nnew object initialization to create a Lazy  type due to the exposure of its func -\ntionality through a static method. Here’s a simple example showing how to lazily initialize a big image using \nLazyInitializer :\n private BigImage bigImage; public BigImage BigImage =>      LazyInitializer.EnsureInitialized(ref bigImage, () => new BigImage());\n \n4 See “Implementing Singleton in C#,” MSDN, http://mng.bz/pLf4. Calls the Singleton constructor delegate",17054
27-2.7.3 Lazy support in F.pdf,27-2.7.3 Lazy support in F,,0
28-2.7.4 Lazy and Task a powerful combination.pdf,28-2.7.4 Lazy and Task a powerful combination,"56 chapter  2 Functional programming techniques for concurrency\n2.7.3 Lazy support in F# \nF# supports the same Lazy<T>  type with the addition of lazy computation, which is of \ntype Lazy<T> , where the actual generic type that is used for T is determined from the \nresult of the expression. The F# standard library automatically enforces mutual exclu-sion, so that pure function code is thread safe when simultaneously forcing the same lazy value from separate threads. The F# use of the \nLazy  type is a little different from \nC#, where you wrap the function around a Lazy  data type. This code example shows \nthe F# Lazy  computation of a Person  object:\nlet barneyRubble = lazy( Person(""barney"", ""rubble"") ) printfn ""%s"" (barneyRubble.Force().FullName)          \nThe function barneyRubble  creates an instance of Lazy<Person> , for which the value \nisn’t yet materialized. Then, to force the computation, you call the method Force that \nretrieves the value on demand.\n2.7.4 Lazy and Task, a powerful combination \nFor performance and scalability reasons, in a concurrent application it’s useful to com-bine a lazy evaluation that can be executed on demand using an independent thread. The \nLazy  initializer Lazy<T>  can be utilized to implement a useful pattern to instanti-\nate objects that require asynchronous operations. Let’s consider the class Person  that \nwas used in the previous section. If the first and second name fields are loaded from a database, you can apply a type \nLazy<Task<Person>>  to defer the I/O computation. It’s \ninteresting that between Task<T>  and Lazy<T>  there’s a commonality: both evaluate a \ngiven expression exactly once. \nListing 2.29  Lazy asynchronous operation to initialize the Person  object\nLazy<Task<Person>> person =    new Lazy<Task<Person>>(async () =>          {        using (var cmd = new SqlCommand(cmdText, conn))        using (var reader = await cmd.ExecuteReaderAsync())        {            if (await reader.ReadAsync())            {                string firstName = reader[""first_name""].ToString();                string lastName = reader[""last_name""].ToString();                return new Person(firstName, lastName);            }        }        throw new Exception(""Failed to fetch Person"");    });async Task<Person> FetchPerson(){    return await person.Value;              }Shows the asynchronous lambda constructor for the Lazy type\nMaterialized asynchronously the Lazy type\n \n 57 Summary\nIn this example, the delegate returns a Task<Person> , which asynchronously deter -\nmines the value once and returns the value to all callers. These are the kind of designs that ultimately improve the scalability of your program. In the example, this feature implements asynchronous operations using the \nasync-await  keywords (introduced in \nC# 5.0). Chapter 8 covers in detail the topics of asynchronicity and scalability.\nThis is a useful design that can improve scalability and parallelism in your program. \nBut there’s a subtle risk. Because the lambda expression is asynchronous, it can be exe-cuted on any thread that calls \nValue , and the expression will run within the context. A \nbetter solution is to wrap the expression in an underlying Task , which will force the asyn-\nchronous execution on a thread-pool thread. This listing shows the preferred pattern.\nListing 2.30  Better pattern \nLazy<Task<Person>> person =    new Lazy<Task<Person>>(() => Task.Run(        async () =>        {            using (var cmd = new SqlCommand(cmdText, conn))            using (var reader = await cmd.ExecuteReaderAsync())            {                if(await reader.ReadAsync())                {                    string firstName = reader[""first_name""].ToString();                    string lastName = reader[""last_name""].ToString();                    return new Person(firstName, lastName);                } else throw new Exception(“No record available”);            }        }    ));\nSummary\n¡ Function composition applies the result of one function to the input of another, creating a new function. You can use it in FP to solve complex problems by decomposing them into smaller and simpler problems that are easier to solve and then ultimately piece together these sets of solutions.\n¡ Closure is an in-line delegate/anonymous method attached to its parent method, where the variables defined in the parent’s method body can be referenced from within the anonymous method. Closure provides a convenient way to give a func-tion access to local state (which is enclosed in the function), even if it’s out of scope. It’s the foundation to designing functional programming code segments that include memoization, lazy initialization, and precomputation to increase computation speed.\n¡ Memoization is a functional programming technique that maintains the results of intermediate computations instead of recomputing them. It’s considered a form of caching.\n \n58 chapter  2 Functional programming techniques for concurrency\n¡ Precomputation is a technique to perform an initial computation that gener -\nates a series of results, usually in the form of a lookup table. These precomputed values can be used directly from an algorithm to avoid needless, repetitive, and expensive computations each time your code is executed. Generally, precom-putation replaces memoization and is used in combination with partial applied functions. \n¡ Lazy initialization is another variation of caching. Specifically, this technique defers the computation of a factory function for the instantiation of an object until needed, creating the object only once. The main purpose of lazy initializa-tion is to improve performance by reducing memory consumption and avoiding unnecessary computation.",5773
29-3.1 Real-world example hunting the thread-unsafe object.pdf,29-3.1 Real-world example hunting the thread-unsafe object,"593Functional data structures \nand immutability\nThis chapter covers\n¡ Building parallel applications with functional data structures\n¡ Using immutability for high-performant, lock-free code\n¡ Implementing parallel patterns with functional recursion\n¡ Implementing immutable objects in C# and F#\n¡ Working with tree data structures\nData comes in a multitude of forms. Consequently, it’s not surprising that many computer programs are organized around two primary constraints: data and data manipulation. Functional programming fits well into this world because, to a large extent, this programming paradigm is about data transformation. Functional trans-formations allow you to alter a set of structured data from its original form into another form without having to worry about side effects or state. For example, you can transform a collection of countries into a collection of cities using a map function and keep the initial data unchanged. Side effects are a key challenge for \n \n60 chapter  3 Functional data structures and immutability\nconcurrent programming because the effects raised in one thread can influence the behavior of another thread. \nOver the past few years, mainstream programming languages have added new fea-\ntures to make multithreaded applications easier to develop. Microsoft, for example, has added the TPL and the \nasync/await  keywords to the .NET framework to reduce pro-\ngrammers’ apprehension when implementing concurrent code. But there are still chal-lenges with keeping a mutable state protected from corruption when multiple threads are involved. The good news is that FP lets you write code that transforms immutable data without side effects.\nIn this chapter, you’ll learn to write concurrent code using a functional data struc-\nture and using immutable states, adopting the right data structure in a concurrent environment to improve performance effortlessly. Functional data structures boost per -\nformance by sharing data structures between threads and running in parallel without synchronization. \nAs a first step in this chapter, you’ll develop a functional list in both C# and F#. These \nare great exercises for understanding how immutable functional data structures work. Next, we’ll cover immutable tree data structures, and you’ll learn how to use recursion in FP to build a binary tree structure in parallel. Parallel recursion is used in an example to simultaneously download multiple images from the web.\nBy the end of the chapter, you’ll exploit immutability and functional data structures \nto run a program faster in parallel, avoiding the pitfalls, such as race conditions, of shared mutable of state. In other words, if you want concurrency and a strong guaran-tee of correctness, you must give up mutation.\n3.1 Real-world example: hunting the thread-unsafe object\nBuilding software in a controlled environment usually doesn’t lead to unwelcome sur -\nprises. Unfortunately, if a program that you write on your local machine is deployed to a server that isn’t under your control, this might introduce different variables. In the production environment, programs can run into unanticipated problems and unpre-dictable heavy loads. I’m sure that more than once in your career, you’ve heard, “It works on my machine.”\nWhen software goes live, multiple factors can go wrong, causing the programs to \nbehave unreliably. A while ago, my boss called me to analyze a production issue. The application was a simple chat system used for customer support. The program was using web sockets to communicate from the frontend directly with the Windows server hub written in C#. The underlying technology to establish the bidirectional communication between client and server was Microsoft SignalR (http:/ /mng.bz/Fal1). See figure 3.1.\nSignalR from MSDN documentation\nASP.NET SignalR is a library for ASP.NET developers that simplifies the process of adding \nreal-time web functionality to applications. Real-time web functionality has server code \n \n 61 Real-world example: hunting the thread-unsafe object\npush content to connected clients instantly as it becomes available, rather than having the server wait for a client to request new data. SignalR can be used to add any sort of real-time web functionality to your ASP.NET application. While chat is often used as an example, you can do much more. Any time a user refreshes a web page to see new data, or the page implements long pooling to retrieve new data, it’s a candidate for using SignalR. Exam-ples include dashboards and monitoring applications, collaborative applications (such as simultaneous editing of documents), job progress updates, and real-time forms.\n \nBefore being deployed in production, the program had passed all the tests. Once deployed, however, the server’s resources were stressed. The CPU usage was contin-ually between 85% to 95% of capacity, negatively affecting overall performance by preventing the system from being responsive to incoming requests. The result was unacceptable, and the problem needed a quick resolution. \nIIS server\nweb application\nRequest\nResponse\nStatic shared\nlookup tableSignalR server hub\nClients\nconnected\nAs Sherlock Holmes said, “When you have eliminated the impossible, whatever remains, however improbable, must be the truth.” I put on my super-sleuth hat and then, using a valued lens, I began to look at the code. After debugging and investiga-tion, I detected the portion of code that caused the bottleneck. \nI used a profiling tool to analyze the application’s performance. Sampling and pro-\nfiling the application is a good place to start looking for bottlenecks in the application. The profiling tool samples the program when it runs, examining the execution times to inspect as conventional data. The data collected is a statistical profiling representa-tion of the individual methods that are doing the most work in the application. The final report shows these methods, which can be inspected by looking for the hot path (http:/ /mng.bz/agzj) where most of the work in the application is executed. \nThe high CPU-core utilization problem originated in the \nOnConnected  and OnDis-\nconnected  methods due to the contention of a shared state. In this case, the shared \nstate was a generic Dictionary  type, used to keep the connected users in memory. A \nThread contention is a condition where one thread is waiting for an object, being held Figure 3.1  Architecture of a \nweb server chat application using a SignalR hub. The clients connected are registered in a local static dictionary (lookup table) whose instance is shared. \n \n62 chapter  3 Functional data structures and immutability\nby another thread, to be released. The waiting thread cannot continue until the other thread releases the object (it’s locked). This listing shows the problematic server code. \nListing 3.1  SignalR hub in C# that registers connections in context\nstatic Dictionary<Guid, string> onlineUsers =    new Dictionary<Guid, string>(); public override Task OnConnected() {    Guid connectionId = new Guid (Context.ConnectionId);     System.Security.Principal.IPrincipal user = Context.User;    string userName;    if (!onlineUsers.TryGetValue(connectionId, out userName)){         RegisterUserConnection (connectionId, user.Identity.Name);        onlineUsers.Add(connectionId, user.Identity.Name);     }    return base.OnConnected();}public override Task OnDisconnected() {    Guid connectionId = new Guid (Context.ConnectionId);    string userName;    if (onlineUsers.TryGetValue(connectionId, out userName)){         DeregisterUserConnection(connectionId, userName);        onlineUsers.Remove(connectionId);     }    return base.OnDisconnected();}\nThe operations OnConnected  and OnDisconnected  rely on a shared global dictionary, \ncommunally used in these types of programs to maintain a local state. Notice that each time one of these methods is executed, the underlying collection is called twice. The program logic checks whether the \nUser Connection Id  exists and applies some behav-\nior accordingly:\nstring userName;if (!onlineUsers.TryGetValue(connectionId, out userName)){\nCan you see the issue? For each new client request, a new connection is established, and a new instance of the hub is created. The local state is maintained by a static vari-able, which keeps track of the current user connection and is shared by all instances of the hub. According to the Microsoft documentation, “A static constructor is only called one time, and a static class remains in memory for the lifetime of the application domain in which your program resides.”\n1 \nHere’s the collection used for user-connection tracking:\nstatic Dictionary<Guid, string> onlineUsers =  \nnew Dictionary<Guid, string>();Shares an instance of a static dictionary to handle the state of online users\nEach connection is associated with a unique identifier Guid. Checks if the current user is already connected and stored in the dictionary \nBoth operations of adding and removing a user are performed after checking the state of the dictionary. \n1 For more information on static classes and static class members, see http:/ /mng.bz/agzj.",9220
30-3.1.1 .NET immutable collections a safe solution.pdf,30-3.1.1 .NET immutable collections a safe solution,"63 Real-world example: hunting the thread-unsafe object\nGuid  is the unique connection identifier created by SignalR when the connection \nbetween client and server is established. The string represents the name of the user defined during login. In this case, the program clearly runs in a multithreaded envi-ronment. Every incoming request is a new thread; consequently, there will be several requests simultaneously accessing the shared state, which eventually leads to multi-threading problems. \nThe MSDN documentation is clear in this regard. It says that a \nDictionary  collection \ncan support multiple readers concurrently, as long as the collection isn’t modified.2 \nEnumerating through the collection is intrinsically not thread safe because a thread could update the dictionary while another thread is changing the state of the collection.\nSeveral possible solutions exist to avoid this limitation. The first approach is to \nmake the collection thread safe and accessible by multiple threads for both \nread  and \nwrite  operations using lock primitive . This solution is correct but downgrades \nperformance. \nThe preferred alternative is to achieve the same level of thread safety without syn-\nchronization; for example, using immutable collections.\n3.1.1 .NET immutable collections: a safe solution\nMicrosoft introduced immutable collections, found in the namespace System.Collec -\ntions.Immutable , with .NET Framework 4.5. This is part of the evolution of threading \ntools after TPL in .NET 4.0 and the async  and await  keywords after .NET 4.5. \nThe immutable collections follow the functional paradigm concepts covered in this \nchapter, and provide implicit thread safety in multithreaded applications to overcome the challenge to maintain and control mutable state. Similar to concurrent collections, they’re also thread safe, but the underlying implementation is different. Any operations that change the data structures don’t modify the original instance. Instead, they return a changed copy and leave the original instance unchanged. The immutable collections have been heavily tuned for maximum performance and use the Structural Sharing \n3 \npattern to minimize garbage collector (GC) demands. As an example, this code snip-pet creates an immutable collection from a generic mutable one (the immutable com-mand is in bold). Then, by updating the collections with a new item, a new collection is created, leaving the original unaffected:\nvar original = new Dictionary<int, int>().ToImmutableDictionary();var modifiedCollection = original.Add(key, value);\nAny changes to the collection in one thread aren’t visible to the other threads, because they still reference the original unmodified collection, which is the reason why immutable collections are inherently thread safe. \nTable  3.1 shows an implementation of an immutable collection for each of the \nrelated mutable generic collections.\n2 For more information on thread safety, see http:/ /mng.bz/k8Gg.\n3 “Persistent Data Structure,” https:/ /en.wikipedia.org/wiki/Persistent_data_structure. \n \n64 chapter  3 Functional data structures and immutability\nTable 3.1  Immutable collections for .NET Framework 4.5\nImmutable collection Mutable collection \nImmutableList<T> List<T>\nImmutableDictionary<TKey, TValue> Dictionary<TKey, TValue>\nImmutableHashSet<T> HashSet<T>\nImmutableStack<T> Stack<T>\nImmutableQueue<T> Queue<T>\nNOTE  The previous version of .NET attempted to provide collections with \nimmutable capabilities using the generic ReadOnlyCollection  and the exten-\nsion method AsReadOnly , which transforms a given mutable collection into \na read-only one. But this collection is a wrapper that prevents modifying the underlying collection. Therefore, in a multithreaded program, if a thread changes the wrapped collection, the read-only collection reflects those changes. Immutable collections resolve this issue. \nHere are two ways to create an immutable list.\nListing 3.2  Constructing .NET immutable collections\nvar list = ImmutableList.Create<int>(); list = list.Add(1); list = list.Add(2);list = list.Add(3);var builder = ImmutableList.CreateBuilder<int>(); builder.Add(1); builder.Add(2);builder.Add(3);list = builder.ToImmutable(); \nThe second approach simplifies the construction of the list by creating a temporary list builder, which is used to add an element to the list and then seals (freezes) the ele-ments into an immutable structure. \nIn reference to the data corruption (race condition) problem in the original chat \nprogram, immutable collections can be used in a Windows server hub to maintain the state of the open SignalR connections. This is safely accomplished with multithread access. Luckily, the \nSystem.Collections.Immutable namespace contains the equiva-\nlent version of Dictionary  for lookups: ImmutableDictionary. \nYou may ask, “But if the collection is immutable, how it can be updated while pre-\nserving thread safety?” You can use lock statements around operations that involve reading or writing the collection. Building a thread-safe collection using locks is straightforward; but it is a more expensive approach than required. A better option is to protect the writes with a single compare-and-swap (CAS) operation, which removes Creates an empty immutable list\nAdds a new item to the list and returns a new listCreates a list builder to construct  a list definition, with mutable semantics, then freezes the collection \nAdds a new item to the list builder, which mutates the collection in place \nCloses the list builder to create the immutable list \n \n 65 Real-world example: hunting the thread-unsafe object\nthe need for locks and leaves the read operation unguarded. This lock-free technique is more scalable and performs better than the counterpart (one that uses a synchroni-zation primitive). \ncas operations\nCAS is a special instruction used in multithreaded programming as a form of synchro-\nnization that atomically performs an operation on memory locations. An atomic oper -\nation either succeeds or fails as a unit. \nAtomicity refers to operations that alter a state in a single step in such a way that the \noutcome is autonomous, observed as either done or not done, with no in-between state. Other parallel threads can only see the old or the new state. When an atomic opera-tion is performed on a shared variable, threads cannot observe its modification until it completes. In fact, an atomic operation reads a value as it appears at a single moment in time. Primitive atomic operations are machine instructions and can be exposed by .NET in the \nSystem.Threading.Interlocked  class, such as the Interlocked.Compare -\nExchange  and the Interlocked.Increment  methods.\nThe CAS instruction modifies shared data without the need to acquire and release a \nlock and allows extreme levels of parallelism. This is where immutable data structures really shine because they minimize the chances of incurring ABA problems (https:/ /en.wikipedia.org/wiki/ABA_problem).\nThe ABA problem \nThe ABA problem occurs when executing an atomic CAS operation: one thread is sus -\npended before executing the CAS, and a second thread modifies the target of the CAS instruction from its initial value. When the first thread resumes, the CAS succeeds, despite the changes to the target value.\n \nThe idea is to keep the state that has to change contained into a single and, most importantly, isolated immutable object (in this case, the \nImmutableDictionary ). \nBecause the object is isolated, there’s no sharing of state; therefore, there’s nothing to synchronize. \nThe following listing shows the implementation of a helper object called \nAtom . The \nname is inspired by the Clojure atom (https:/ /clojure.org/reference/atoms), which internally uses the \nInterlocked.CompareExchange  operator to perform atomic CAS \noperations.\nListing 3.3  Atom  object to perform CAS instructions \npublic sealed class Atom<T> where T : class {    public Atom(T value)    {        this.value = value;    } Creates a helper object for atomic CAS instructions\n \n66 chapter  3 Functional data structures and immutability\n    private volatile T value;    public T Value => value;     public T Swap(Func<T, T> factory)     {        T original, temp;        do {            original = value;            temp = factory (original);        }        while (Interlocked.CompareExchange(ref value, temp, original) \n➥ != original); \n        return original;    }}\nThe Atom  class encapsulates a reference object of type T marked volatile ,4 which must \nbe immutable to achieve the correct behavior of value swapping. The property Value  \nis used to read the current state of a wrapped object. The purpose of the Swap  func-\ntion is to execute the CAS instruction to pass to the caller of this function a new value based on the previous value using the \nfactory  delegate. The CAS operation takes an \nold and a new value, and it atomically sets the Atom  to the new value only if the current \nvalue equals the passed-in old value. If the Swap  function can’t set the new value using \nInterlocked.CompareExchange , it continues to retry until it’s successful.\nListing 3.4 shows how to use the Atom  class with the ImmutableDictionary  object \nin the context of a SignalR server hub. The code implements only the OnConnected \nmethod. The same concept applies to the OnDisconnected  function.\nListing 3.4  Thread-safe ImmutableDictionary  using an Atom  object \nAtom<ImmutableDictionary<Guid, string>> onlineUsers =    new Atom<ImmutableDictionary<Guid, string>>        (ImmutableDictionary<Guid, string>.Empty); public override Task OnConnected() {    Grid connectionId = new Guid (Context.ConnectionId);    System.Security.Principal.IPrincipal user = Context.User;    var temp = onlineUsers.Value;     if(onlineUsers.Swap(d => {                        if (d.ContainsKey(connectionId)) return d;                    return d.Add(connectionId, user.Identity.Name);                    }) != temp) {         RegisterUserConnection (connectionId, user.Identity.Name);    }    return base.OnConnected();}Gets the current value of this instance\nComputes a new value based on the current value of the instance\nRepeats the CAS instruction until it succeeds \n4 For more information on the volatile  keyword, see https:/ /msdn.microsoft.com/en-us/library/\nx13ttww7.aspx. Passes an empty ImmutableDictionary \nas the argument to the Atom object to \ninitialize the first state\nCreates a temporary copy of the original ImmutableDictionary and calls the Value property\nUpdates atomically the underlying immutable collection with the swap operation if the key connectionId isn’t found Registers the new user connection, if the \noriginal ImmutableDictionary and the collection \nreturned from the Swap function are different \nand an update has been performed\n \n 67 Real-world example: hunting the thread-unsafe object\nThe Atom Swap  method wraps the call to update the underlying Immutable-\nDictionary . The Atom Value  property can be accessed at any time to check the \ncurrent open SignalR connections. This operation is thread safe because it’s read-only. The \nAtom  class is generic, and it can be used to update atomically any type. But \nimmutable collections have a specialized helper class (described next).\nthe immutable interlocked  class\nBecause you need to update the immutable collections in a thread-safe manner, Mic-\nrosoft introduced the ImmutableInterlocked class, which can be found in the System \n.Collections.Immutable namespace. This class provides a set of functions that \nhandles updating immutable collections using the CAS mechanism previously men-tioned. It exposes the same functionality of the \nAtom  object. In this listing, Immutable-\nDictionary  replaces Dictionary .\nListing 3.5  Hub maintaining open connections using ImmutableDictionary\nstatic ImmutableDictionary<Guid, string> onlineUsers =    ImmutableDictionary<Guid, string>.Empty; public override Task OnConnected() {    Grid connectionId = new Guid (Context.ConnectionId);    System.Security.Principal.IPrincipal user = Context.User;    if(ImmutableInterlocked.TryAdd (ref onlineUsers, \n➥ connectionId, user.Identity.Name)) { \n        RegisterUserConnection (connectionId, user.Identity.Name);    }    return base.OnConnected();}public override Task OnDisconnected() {    Grid connectionId = new Guid (Context.ConnectionId);    string userName;    if(ImmutableInterlocked.TryRemove (ref onlineUsers, \n➥ connectionId, out userName)) { \n        DeregisterUserConnection(connectionId, userName);    }    return base.OnDisconnected();}\nUpdating an ImmutableDictionary  is performed atomically, which means in this case \nthat a user connection is added only if it doesn’t exist. With this change, the SignalR hub works correctly and is lock free, and the server didn’t spike high percentages of CPU utilization. But there’s a cost to using immutable collections for frequent updates. For example, the time required to add 1 million users to the \nImmutableDictionary \nusing ImmutableInterlocked  is 2.518 seconds. This value is probably acceptable in \nmost cases, but if you’re aiming to produce a highly performant system, it’s important to do the research and employ the right tool for the job. \nIn general, the use of immutable collections fits perfectly for shared state among \ndifferent threads, when the number of updates is low. Their value (state) is guaranteed Shows an instance of an empty ImmutableDictionary\nImmutableInterlocked tries to add a new item to the immutable collections in a thread-safe manner.\nImmutableInterlocked removes an item. If the item exists, the function returns true.",13777
31-3.1.2 .NET concurrent collections a faster solution.pdf,31-3.1.2 .NET concurrent collections a faster solution,"68 chapter  3 Functional data structures and immutability\nto be thread safe; it can be safely passed among additional threads. If you need a collec-tion that has to handle many updates concurrently, a better solution is to exploit a .NET concurrent collection.\n3.1.2 .NET concurrent collections: a faster solution \nIn the .NET framework, the System.Collections.Concurrent  namespace provides a \nset of thread-safe collections designed to simplify thread-safe access to shared data. Concurrent collections are mutable collection instances that aim to increase the per -\nformance and scalability of multithreaded applications. Because they can be safely accessed and updated by multiple threads at the same time, they’re recommended for multithreaded programs instead of the analogous collections in \nSystem.Collec -\ntions.Generic . Table 3.2 shows the concurrent collections available in .NET.\nTable 3.2  Concurrent collection details \nConcurrent collection Implementation details Synchronization techniques \nConcurrentBag<T> Works like a generic list If multiple threads are detected, a primitive monitor coordinates their access; otherwise, the synchronization is avoided.\nConcurrentStack<T> Generic stack implemented using a singly linked listLock free using a CAS technique.\nConcurrentQueue<T> Generic queue implemented using a linked list of array segmentsLock free using CAS technique.\nConcurrentDictionary<K, V> Generic dictionary imple-mented using a hash table Lock free for read operations; lock synchronization for updates.\nBack to the SignalR hub example of “Hunt the thread-unsafe object,” Concurrent -\nDictionary is a better option than the not-thread-safe Dictionary , and due to the \nfrequent and wide number of updates, it’s also a better option than ImmutableDic -\ntionary . In fact, System.Collections.Concurrent  has been designed for high per -\nformance using a mix of fine-grained5 and lock-free patterns. These techniques ensure \nthat threads accessing the concurrent collection are blocked for a minimum amount of time or, in certain cases, completely avoid the blocking. \nConcurrentDictionary can ensure scalability while handling several requests per \nsecond. It’s possible to assign and retrieve values using square-bracket indexing like the conventional generic \nDictionary , but ConcurrentDictionary  also offers a num-\nber of methods that are concurrency friendly such as AddOrUpdate  or GetOrAdd . The \nAddOrUpdate  method takes a key and a value parameter, and another parameter that is a \n5 For more information on parallel computing, see https:/ /en.wikipedia.org/wiki/Parallel  \n_computing.\n \n 69 Real-world example: hunting the thread-unsafe object\ndelegate. If the key isn’t in the dictionary, it’s inserted using the value parameter. If the key is in the dictionary, the delegate is called and the dictionary is updated with the resulting value. Providing what you do in the delegate is also thread safe, this removes the danger of another thread coming in and changing the dictionary between you read-ing a value out of it and writing another back. \nNOTE  Be aware that regardless of whether the methods exposed by Concurrent-\nDictionary  are atomic and thread safe, the class has no control over the del-\negates called by AddOrUpdate  and GetOrAdd , which could be implemented \nwithout a thread-safety guard.\nIn the following listing, the ConcurrentDictionary keeps the state of open connec-\ntions in the SignalR hub.\nListing 3.6  Hub maintaining open connections using ConcurrentDictionary\nstatic ConcurrentDictionary<Guid, string> onlineUsers =        new ConcurrentDictionary<Guid, string>(); public override Task OnConnected() {    Grid connectionId = new Guid (Context.ConnectionId);    System.Security.Principal.IPrincipal user = Context.User;    if(onlineUsers.TryAdd(connectionId, user.Identity.Name)) {         RegisterUserConnection (connectionId, user.Identity.Name);    }    return base.OnConnected();}public override Task OnDisconnected() {    Grid connectionId = new Guid (Context.ConnectionId);    string userName;    if(onlineUsers.TryRemove (connectionId, out userName)) {         DeregisterUserConnection(connectionId, userName);    }    return base.OnDisconnected();}\nThe code looks similar to the code listing using the ImmutableDictionary , (listing \n3.5), but the performance of adding and removing many connections ( connection ) is \nfaster. For example, the time required to add 1 million users to the ConcurentDictio -\nnarry  is only 52 ms, in comparison to the 2.518 s of the ImmutableDictionary . This \nvalue is probably fine in many cases, but if you want to produce a highly performant system, it’s important to research and employ the right tool.\nYou need to understand how these collections work. Initially, it seems the collections \nare used without any FP style, due to their mutable characteristics. But the collections create an internal snapshot that mimics a temporary immutability to preserve thread safety during their iteration, allowing the snapshot to be enumerated safely. Shows the instance of an empty ConcurrentDictionary\nThe onlineUsers ConcurrentDictionary \ntries to add a new item; if the item doesn’t \nexist, it’s added and the user is registered.\nThe onlineUsers ConcurrentDictionary \nremoves the connectionId if it exists.",5362
32-3.1.3 The agent message-passing pattern a faster better solution.pdf,32-3.1.3 The agent message-passing pattern a faster better solution,"70 chapter  3 Functional data structures and immutability\nConcurrent collections work well with algorithms that consider the producer/con-\nsumer6 implementation. A Producer/Consumer pattern aims to partition and balance the \nworkload between one or more producers and one or more consumers. A producer gen-erates data in an independent thread and inserts it into a queue. A consumer runs a separate thread concurrently, which consumes the data from the queue. For example, a producer could download images and store them in a queue that’s accessed by a con-sumer that performs image processing. These two entities work independently, and if the workload from the producer increases, you can spawn a new consumer to balance the workload. The Producer/Consumer pattern is one of the most widely used parallel programming patterns, and it will be discussed and implemented in chapter 7.\n3.1.3 The agent message-passing pattern: a faster, better solution \nThe final solution for “Hunt the thread-unsafe object” was the introduction of a local agent into the SignalR hub, which provides asynchronous access to maintain high scal-ability during high-volume access. An agent is a unit of computation that handles one message at a time, and the message is sent asynchronously, which means the sender doesn’t have to wait for the answer, so there’s no blocking. In this case, the dictionary is isolated and can be accessed only by the agent, which updates the collection in a sin-gle-thread fashion, eliminating the hazard of data corruption and the need for locks. This fix is scalable because the agent’s asynchronous semantic operation can process 3 million messages per second, and the code runs faster because it removes the extra overhead from using synchronizations. \nProgramming with agents and message passing is discussed in chapter 11. Don’t \nworry if you don’t completely understand the code; it will become clear during this jour -\nney, and you can always reference appendix B. This approach requires fewer changes in the code compared to the previous solutions, but application performance isn’t jeopar -\ndized. This listing shows the implementation of the agent in F#.\nListing 3.7  F# agent that ensures thread-safe access to mutable states\ntype AgentMessage =     | AddIfNoExists of id:Guid * userName:string    | RemoveIfNoExists of id:Guidtype AgentOnlineUsers() =    let agent = MailboxProcessor<AgentMessage>.Start(fun inbox ->        let onlineUsers = Dictionary<Guid, string>()         let rec loop() = async {            let! msg = inbox.Receive()            match msg with            | AddIfNoExists(id, userName) ->                 let exists, _ = onlineUsers.TryGetValue(id)  \n6 For more information on the producer-consumer problem, also known as the bounded-buffer prob-lem, see https:/ /en.wikipedia.org/wiki/Producer–consumer_problem. Uses a discriminated union that represents the type of messages for the agent\nInside the body of the \nagent, even a mutable \ncollection is thread safe \nbecause it’s isolated.\nThe message received is pattern-matched to branch out to the corresponding functionality.\nThe lookup operation is thread safe because it’s executed by the single-thread agent.\n \n 71 Real-world example: hunting the thread-unsafe object\n                if not exists = true then                    onlineUsers.Add(id, userName)                    RegisterUserConnection (id, userName)            | RemoveIfNoExists(id) ->                 let exists, userName = onlineUsers.TryGetValue(id)                 if exists = true then                    onlineUsers.Remove(id) |> ignore                    DeregisterUserConnection(id, userName)            return! loop() }        loop() )\nIn the following listing, the refactored C# code uses the final solution. Because of the interoperability between .NET programming languages, it’s possible to develop a library using one language that’s accessed by the other. In this case, C# is accessing the F# library with the \nMailboxProcessor  (Agent ) code. \nListing 3.8  SignalR hub in C# using an F# agent\nstatic AgentOnlineUsers onlineUsers = new AgentOnlineUsers() public override Task OnConnected() {    Guid connectionId = new Guid (Context.ConnectionId);    System.Security.Principal.IPrincipal user = Context.User;    onlineUsers.AddIfNoExists(connectionId, user.Identity.Name);     return base.OnConnected();}public override Task OnDisconnected() {    Guid connectionId = new Guid (Context.ConnectionId);    onlineUsers.RemoveIfNoExists(connectionId);     return base.OnDisconnected();}\nIn summary, the final solution solved the problem by dramatically reducing the CPU consumption to almost zero (figure 3.2). \nThe takeaway from this experience is that sharing a mutable state in the multithread-\ning environment isn’t a good idea. Originally, the \nDictionary  collection had to main-\ntain the user connections currently online; mutability was almost a necessity. You could use a functional approach with an immutable structure, but instead create a new col-lection for each update, which is probably overkill. A better solution is to use an agent to isolate mutability and make the agent accessible from the caller methods. This is a functional approach that uses the natural thread safety of agents.\nThe result of this approach is an increase of scalability because access is asynchro-\nnous without blocking, and it allows you to easily add logic in the agent body, such as logging and error handling.The message received is \npattern-matched to branch \nout to the corresponding \nfunctionality.\nThe lookup operation is \nthread safe because it’s \nexecuted by the \nsingle-thread agent.\nUses a static instance of the F# \nagent from the referenced library\nMethods that \nasynchronously, with \nno blocking, send a \nmessage to the agent to \nperform thread-safe \nupdate operations",5896
33-3.3 Immutability for a change.pdf,33-3.3 Immutability for a change,"72 chapter  3 Functional data structures and immutability\nIIS server\nweb application\nRequest\nResponse\nStatic shared\nlookup tableSignalR server hub\nClients\nconnectedAgentIf multiple connections arrived\nsimultaneously , they’ re queued\nand guaranteed to be processe d\nin order , one at a time.\nThe connected clients areregistered in an internalagent state, which provides\nisolation and thread safety.\nFigure 3.2  Architecture of a web server for a chat application using a SignalR hub. This solution, \ncompared to figure 3.1, removes the mutable dictionary that was shared between multiple threads to handle the incoming requests. To replace the dictionary, there is a local agent that guarantees high scalability and thread safety in this multithreaded scenario.  \n3.2 Safely sharing functional data structures among threads\nA persistent data structure (also known as a functional data structure) is a data structure \nin which no operations result in permanent changes to the underlying structure. Per-sistent means that all versions of the structure that are modified endure over time. In other words, such a data structure is immutable because update operations don’t mod-ify the data structure but return a new one with the updated values.\nPersistent, in terms of data, is commonly misconstrued as storing data in a physical \nentity, such as a database or filesystem. In FP, a functional data structure is long lasting. Most traditional imperative data structures (such as those from \nSystem.Collections  \n.Generic:  Dictionary , List , Queue , Stack , and so forth) are ephemeral because their \nstate exists only for a short time between updates. Updates are destructive, as shown in figure 3.3.\nA functional data structure guarantees consistent behavior regardless of whether the \nstructure is accessed by different threads of execution or even by a different process, with no concern for potential changes to the data. Persistent data structures don’t sup-port destructive updates, but instead keep the old versions of a data structure. \nUnderstandably, when compared to traditional imperative data structures, purely \nfunctional data structures are notoriously memory allocation intensive, which leads to substantial performance degradation. Fortunately, persistent data structures are designed with efficiency in mind, by carefully reusing a common state between versions \n \n 73 Immutability for a change\nof a data structure. This is possible by using the immutable nature of the functional data structures: Because they can never be changed, reusing different versions is effort-less. You can compose a new data structure from parts of an old one by referring to the existing data rather than copying it. This technique is called structural sharing (see sec-tion 3.3.5). This implementation is more streamlined than creating a new copy of data every time an update is performed, leading to improved performance. \nA new list is created with the\nvalue 5 replacing the value 3.The original list isn ’t mutated.Updating the value 3 with\nnumber 5, mutating thelist in place\n4\n3\n2\n14\n5\n2\n14\n353\n2\n1\n3.3 Immutability for a change\nIn Working Effectively with Legacy Code, author Michael Feathers compares OOP and FP \nas follows: \nObject-oriented programming makes code understandable by encapsulating moving parts. Functional programming makes code understandable by minimizing moving parts.\n— Michael Feathers, Working Effectively with Legacy Code (Prentice Hall, 2004)\nWhat this means is that immutability minimizes the parts of code that change, making it easier to reason about how those parts behave. Immutability makes the functional code free of side effects. A shared variable, which is an example of a side effect, is a serious obstacle for creating parallel code and results in non-deterministic execution. By removing the side effect, you can have a good coding approach.\nIn .NET, for example, the framework designers decided to construct \nstring s as \nimmutable objects, using a functional approach, to make it easier to write better code. As you recall, an immutable object is one whose state cannot be modified after it’s cre-ated. The adoption of immutability in your coding style and the learning curve required by this necessitates extra attention; but the resulting cleaner code syntax and devolu-tion (reducing unnecessary boilerplate code) will be well worth the effort. Moreover, the outcome of adopting this transformation of data versus the mutation of data signifi-cantly reduces the likelihood of bugs in your code, and the interactions and dependen-cies between different parts of your code base become easier to manage. \nThe use of immutable objects as part of your programming model forces each thread \nto process against its own copy of data, which facilitates writing correct, concurrent code. In addition, it is safe to have multiple threads simultaneously accessing shared Figure 3.3  Destructive update vs. \na persistent update of a list. The list at right is updating in place to mutate the value 3 with the value 5, without preserving the original list. This process is also known as destructive. At left, the functional list doesn’t mutate its values, but creates a new list with the updated value. \n \n74 chapter  3 Functional data structures and immutability\ndata if that access is read-only. In fact, because you do not need locks or synchronization techniques, the hazards of possible deadlocks and race conditions will never occur (fig-ure 3.4). We discussed these techniques in chapter 1.\nMutability\nUnshared mutable\ndata doesn’t need\nsynchronization.\nImmutabilityShared UnsharedShared mutable\ndata needs\nsynchronization.\nUnshared immutable\ndata doesn’t need\nsynchronization.Shared immutable\ndata doesn’t need\nsynchronization.\nFunctional languages, such as F#, are immutable by default, which makes them perfect for concurrency. Immutability won’t instantaneously cause your code to run faster or make your program massively scalable, but it does prepare your code to be parallelized with small changes in the code base. \nIn object-oriented languages, such as C# and Java, writing concurrent applications \ncan be difficult because mutability is the default behavior, and there’s no tool to help prevent or offset it. In imperative programming languages, mutable data structures are considered perfectly normal and, although global state isn’t recommended, mutable state is commonly shared across areas of a program. This is a recipe for disaster in par -\nallel programming. Fortunately, as mentioned earlier, C# and F# when compiled share the same intermediate language, which makes it easy to share functionality. You can define the domain and objects of your program in F# to take advantage of its types and conciseness (most importantly, its types are immutable by default), for example. Then, develop your program in C# to consume the F# library, which guarantees immutable behavior without extra work.\nImmutability is an important tool for building concurrent applications, but using \nimmutable types doesn’t make the program run faster. But it does make the code ready for parallelism; immutability facilitates increased degrees of concurrency, which in a multicore computer translates into better performance and speed. Immutable objects can be shared safely among multiple threads, avoiding the need of lock synchroniza-tion, which can keep programs from running in parallel.Figure 3.4  A Cartesian representation of the implications of using a mutable or immutable state in combination with a shared or unshared state \n \n 75 Immutability for a change\nThe .NET framework provides several immutable types—some are functional, some \ncan be used in multithreaded programs, and some both. Table 3.3 lists the characteris-tics of these types, which will be covered later in this chapter.\nTable 3.3  Characteristics of .NET framework immutable types\nType .NET lang.Is it \nfunctional?Characteristics Thread safe? Utilization\nF# list F# Yes Immutable linked list with fast append insertionYes Used in combination with recursion to build and traverse n-ele -\nment lists \nArray C# and F# No Zero-indexed mutable array type stored in a continuous memory location Yes with partition\naEfficient data storage for fast access\nConcurrent collectionsC# and F# No Set of collections optimized for multi -\nthreaded read/ write access Yes Shared data in multi-threaded program; perfect fit for the Producer/Consumer pattern\nImmutable collectionsC# and F# Yes Set of collections that make it easier to work with a parallel com -\nputing environment; their value can be passed freely between different threads with-out generating data corruption Yes Keeping state under control when multiple threads are involved\nDiscrim-inated union (DU)F# Yes Represents a data type that stores one of sev -\neral possible options Yes Commonly used to model domains and to represent hierarchical structures like an abstract syntax tree \nTuple C# and F# Yes Type that groups two or more values of any (possibly different) typeNo Used to return mul -\ntiple values from functions\nF# tuple F# Yes Yes\nRecord typeF# Yes Represents aggregates of named value prop-erties; can be viewed as a tuple with named members that can be accessed using dot notation Yes Used in place of conventional classes providing immutable semantics; fits well in domain design like DU and can be used in C#\na Each thread works on a separate part of the array.",9539
34-3.3.1 Functional data structure for data parallelism.pdf,34-3.3.1 Functional data structure for data parallelism,,0
35-3.3.3 Immutability in C.pdf,35-3.3.3 Immutability in C,"76 chapter  3 Functional data structures and immutability\n3.3.1 Functional data structure for data parallelism \nImmutable data structures are a perfect fit for data parallelism because they facilitate \nsharing data among otherwise isolated tasks in an efficient zero-copy manner. In fact, when multiple threads access partitionable data in parallel, the role of immutability is fundamental to safely processing chunks of data that belong to the same structure but that appear isolated. It’s possible to achieve the same grade of correct data parallelism by adopting functional purity, which means instead of immutability using a function that avoids side effects. \nThe underlying functionality of PLINQ, for instance, promotes purity. A function is \npure when it has no side effects and its return value is only determined by its input values.\nPLINQ is a higher-level abstraction language that lies on top of multithreading com-\nponents, abstracting the lower-level details while still exposing a simplified LINQ seman-tic. PLINQ aims to reduce the time of execution and increase the overall performance of the query, using all available computer resources. (PLINQ is covered in chapter 5.)\n3.3.2 Performance implications of using immutability\nCertain coders assume that programming with immutable objects is inefficient and has severe performance implications. For example, the pure functional way to append something to a list is to return a new copy of the list with the new element added, leav-ing the original list unchanged. This can involve increased memory pressure for the GC. Because every modification returns a new value, the GC must deal with a large number of short-lived variables. But, because the compiler knows that existing data is immutable, and because the data will not change, the compiler can optimize memory allocation by reusing the collection partially or as a whole. Consequently, the perfor -\nmance impact of using immutable objects is minimum, almost irrelevant, because a typical copy of an object, in place of a traditional mutation, creates a shallow copy. In this way, the objects referenced by the original object are not copied; only the refer -\nence is copied, which is a small bitwise replica of the original.\nGC’s origin in functional programming\nIn 1959, in response to memory issues found in Lisp, John McCarthy invented the GC. The GC attempts to reclaim garbage (memory) occupied by objects that are no longer useful to the program. It’s a form of automatic memory management. Forty years later, mainstream languages such as Java and C# adopted the GC. The GC provides enhance -\nments in terms of shared data structures, which can be difficult to accomplish correctly in unmanaged programming languages like C and C++, because certain pieces of code must be responsible for deallocation. Because the elements are shared, it isn’t obvious which code should be responsible for deallocation. In memory-managed programming languages such as C# and F#, the garbage collector automates this process. \n \nWith the speed of CPUs today, this is almost an irrelevant price to pay in comparison to the benefits achieved as a thread-safety guarantee. A mitigating factor to consider \n \n 77 Immutability for a change\nis that, currently, performance translates into parallel programming, which requires more copying of objects and more memory pressure. \n3.3.3 Immutability in C#\nIn C#, immutability isn’t a supported construct. But it isn’t difficult to create immutable objects in C#; the problem is that the compiler doesn’t enforce this style and the pro-grammer must do so with code. Adopting immutability in C# requires additional effort and extra diligence. In C#, an immutable object can be created by using the keyword \nconst  or readonly . \nAny field can be decorated with the const  keyword; the only precondition is that the \nassignment and declaration are a single-line statement. Once declared and assigned, the \nconst  value cannot be changed, and it belongs at the class level, accessing it directly \nand not by an instance. \nThe other option, decorating a value with the readonly  keyword, can be done \ninline or through the constructor when the class is instantiated. After the initializa-tion of a field marked \nreadonly , the field value cannot be changed, and its value is \naccessible through the instance of the class. More important, to maintain the object as immutable when there are required changes to properties or state, you should create a new instance of the original object with the updated state. Keep in mind that \nreadonly  \nobjects in C# are first-level immutable and shallow immutable only. In C#, an object is shallow immutable when the immutability isn’t guaranteed to all its fields and proper -\nties, but only to the object itself. If an object \nPerson  has a read-only property Address , \nwhich is a complex object exposing properties such as street, city, and ZIP code, then these properties don’t inherit the immutability behavior if not marked as read-only. Conversely, an immutable object with all the fields and properties marked as read-only is deeply immutable. \nThis listing shows immutable class \nPerson  in C#.\nListing 3.9  Shallow immutable class Person  in C#\nclass Address{    public Address(string street, string city, string zipcode){        Street = street;        City = city;        ZipCode = zipcode;    }    public string Street;      public string City;        public string ZipCode; }class Person {    public Person(string firstName, string lastName, int age, \n➥ Address address){\n        FirstName = firstName;        LastName = lastName;        Age = age;        Address = address;    }Fields of the Address object that aren’t marked read-only\n \n78 chapter  3 Functional data structures and immutability\n    public readonly string FirstName;     public readonly string LastName;      public readonly int Age;              public readonly Address Address;  }\nIn this code, the Person  object is shallow immutable because, despite the field Address  \nbeing immune to modification (it’s marked read-only), its underlying fields can be changed. In fact, you can create an instance of the object \nPerson  and Address  as\nAddress address = new Address(""Brown st."", ""Springfield"", ""55555"");Person person = new Person(""John"", ""Doe"", 42, address);\nNow, if you try to modify the field Address,  the compiler throws an exception (in \nbold), but you can still change the fields of the object address.ZipCode :\nperson.Address = // Errorperson.Address.ZipCode = ""77777"";\nThis is an example of a shallow immutable object. Microsoft realized the impor -\ntance of programming with immutability in a modern context and introduced a feature to easily create an immutable class with C# 6.0. This feature, called getter-only auto-properties, lets you declare auto-properties without a setter method, which implicitly creates a \nreadonly  backing field. This, unfortunately, implements the \nshallow immutable behavior.\nListing 3.10  Immutable class in C# with getter-only auto-properties\nclass Person {    public Person(string firstName, string lastName, int age, \n➥ Address address){\n        FirstName = firstName;        LastName = lastName;        Age = age;        Address = address;    }    public string FirstName {get;}      public string LastName {get;}       public int Age {get;}               public Address Address {get;}       public Person ChangeFirstName(string firstName) {            return new Person(firstName, this.LastName, this.Age, this.Address);    }    public Person ChangeLstName(string lastName) {               return new Person(this.FirstName, lastName, this.Age, this.Address);    }    public Person ChangeAge(int age) {                           return new Person(this.FirstName, this.LastName, age, this.Address);    }    public Person ChangeAddress(Address address) {               return new Person(this.firstName, this.LastName, this.Age, address);    }}Fields of the Person object that are marked read-only\nThe getter-only property is assigned directly to the underlying field from the constructor.\nShows the functions to update the fields of a Person object by creating a new instance without changing the original",8282
36-3.3.5 Functional lists linking cells in a chain.pdf,36-3.3.5 Functional lists linking cells in a chain,"79 Immutability for a change\nIn this immutable version of the class Person , it’s important to notice that the methods \nresponsible for updating the FirstName , LastName , Age, and Address  don’t mutate any \nstate; instead, they create a new instance of Person . In OOP, objects are instantiated \nby invoking the constructor, then setting the state of the object by updating properties and calling methods. This approach results in an inconvenient and verbose construc-tion syntax. This is where the functions added to \nChange  the properties of the Person  \nobject come into play. Using these functions, it’s possible to adopt a chain pattern, which is known as fluent interface. Here’s an example of such a pattern by creating an \ninstance of a class \nPerson  and changing the age and address:\nAddress newAddress = new Address(""Red st."", ""Gotham"", ""123459"");Person john = new Person(""John"", ""Doe"", 42, address);Person olderJohn = john.ChangeAge(43).ChangeAddress(newAddress);\nIn summary, to make a class immutable in C#, you must: \n¡ Always design a class with a constructor that takes the argument that’s used to set the state of the object. \n¡ Define the fields as read-only and utilize properties without a public setter; the values will be assigned in the constructor. \n¡ Avoid any method designed to mutate the internal state of the class.\n3.3.4 Immutability in F#\nAs mentioned, the programming language F# is immutable by default. Therefore, the concept of a variable doesn’t exist because, by definition, if a variable is immutable, then it isn’t a variable. F# replaces a variable with an identifier, which associates (binds) with a value using the keyword \nlet. After this association, the value cannot change. \nBesides a full set of immutable collections, F# has a built-in series of helpful immutable constructs, designed for pure functional programming, as shown in listing 3.11. These built-in types are \ntuple  and record , and they have a number of advantages over the \nCLI types:\n¡ They are immutable.\n¡ They cannot be null .\n¡ They have built-in structural equality and comparison.\nThis listing shows use of an immutable type in F#.\nListing 3.11  F# immutable types \nlet point = (31, 57)        let (x,y) = point              type Person= { First : string; Last: string; Age:int}    let person = { First=""John""; Last=""Doe""; Age=42}        \nThe type tuple  is a set of unnamed ordered values, which can be of different het-\nerogeneous (https:/ /en.wikipedia.org/wiki/Homogeneity_and_heterogeneity) types. Uses a tuple that defines two values It’s possible to deconstruct the tuple and access its values. Uses a record type to define a person type\nShows an instance of a person using a record type\n \n80 chapter  3 Functional data structures and immutability\nTuple  has the advantage of being usable on the fly, and is perfect for defining tempo-\nrary and lightweight structures containing an arbitrary number of elements. For exam-ple, (true, “Hello”, 2, 3.14) is a four tuple.\nThe type \nrecord  is similar to tuple , where each element is labeled, giving a name \nto each of the values. The advantage of record  over tuple  is that the labels help to \ndistinguish and to document what each element is for. Moreover, the properties of a record are automatically created for the fields defined, which is convenient because it saves keystrokes. A record in F# can be considered as a C# class with all properties read-only. Most valuable is the ability to correctly and quickly implement immutable classes in C# by using this type. In fact, it’s possible to create an F# library in your solution by creating your domain model using the \nrecord  type and then reference this library into \nyour C# project. Here’s how C# code looks when it references the F# library with the  \nrecord  type:\nPerson person = new Person(""John"", ""Doe"", 42)\nThis is a simple and effective way to create an immutable object. Additionally, the F# implementation requires only one line of code, compared to the equivalent in C# (11 lines of code using read-only fields).\n3.3.5 Functional lists: linking cells in a chain\nThe most common and generally adopted functional data structure is the list, which \nis a series of homogeneous types used to store an arbitrary number of items. In FP, lists are recursive data structures composed by two linked elements: \nHead  or Cons  and  \nTail . The purpose of Cons  is to provide a mechanism to contain a value and a connec-\ntion to other Cons  elements via an object reference pointer. This pointer reference is \nknown as the Next  pointer. \nLists also have a special state called nil to represent a list with no items, which is the \nlast link connected to anything. The nil, or empty, case is convenient during a recur -\nsive traverse of a list to determine its end. Figure 3.5 shows a constructed list of four \nCons  cells and an empty list. Each cell ( Head ) contains a number and a reference to \nthe remaining list ( Tail ), until the last Cons  cell, which defines an empty list. This data \nstructure is similar to a singly linked list (https:/ /en.wikipedia.org/wiki/Linked_list), where each node in the chain has a single link to another node, representing a series of nodes linked together into a chain.\n1Head\n3 3 [ ] 2Tail\nFigure 3.5  Functional list of integers composed by four numbers and an empty list (the last box [ ] on \nthe right). Each item has a reference, the black arrow, linked to the rest of the list. The first item on the left is the head of the list, which is linked to the rest of the list, the tail. \n \n 81 Immutability for a change\nIn functional lists, the operations to add new elements or remove existing elements don’t modify the current structure but return a new one with the updated values. Under the hood, immutable collections can safely share common structures, which limits memory consumption. This technique is called structural sharing. Figure 3.6 shows how structural \nsharing minimizes memory consumption to generate and update functional lists.\nTail list CTail list A\nTail list B1\n6 List C5List A\nList B2\n43 [ ]\nFigure 3.6  The structural sharing technique to create new lists optimizing memory space. In summary, \nList A has three items plus an empty cell, List B has five, and List C six. Each item is linked to the rest of the list. For example, the head item of List B is the number 4, which is linked to the tail (the numbers 5,1, 2, 3, and [ ]).\nIn figure 3.6, List A is composed of three numbers and an empty list. By adding two new items to List A, the structural sharing technique gives the impression that a new List B is created, but in reality it links a pointer from the two items to the previous and unmodified List A. The same scenario repeats for List C. At this point, all three lists (A, B, and C) are accessible, each with its own elements. \nClearly, functional lists are designed to provide better performance by adding or remov-\ning items from the head. In fact, lists work well for linear traversal, and appending per -\nforms in constant time O(1) because the new item is added to the head of the previous list. But it isn’t efficient for random access because the list must be traversed from the left for each lookup, which has O(n ) time, where n  is the number of elements in the collection. \nBig O notation \nBig O notation, also known as asymptotic notation , is a way of summarizing the perfor -\nmance of algorithms based on problem size. Asymptotic describes the behavior of a func -\ntion as its input size approaches infinity. If you have a list of 100 elements, appending a new item to the front of the list has constant time of O(1) because the operation involves a single step, regardless of the size of the list. Conversely, if you’re searching for an item, then the cost of the operation is O(100) because the worst-case scenario requires 100 \n \n82 chapter  3 Functional data structures and immutability\niterations through the list to find the element. The problem size is usually designated as n, and the measure is generalized as O(n).What about the complexity of parallel programs? Big O notation measures the complexity of running an algorithm sequentially, but in the case of a parallel program, this measure doesn’t apply. It’s possible, though, to express complexity of a parallel algorithm by introducing a parameter P that represents the num -\nber of cores on a machine. For example, the cost complexity of a parallel search is O(n/P) because you can break the list into a segment for each core to simultaneously execute the search. This list represents the most common types of complexity, which are in order, starting with the least expensive:\n¡ O(1) constant  —The time is always 1, regardless of the size of the input.\n¡ O(log n) logarithmic  —Time increases as a fraction of the input size.\n¡ O(n) linear —Time increases linearly with input size.\n¡ O(n log n) log linear —Time increases by multiplying the input by a fraction of its \nsize.\n¡ O(n2) quadratic —Time increases with the square of the input size.\n \nA new list is created by prepending a new element to an existing list by taking an empty list as initial value, and then linking the new element to the existing list structure. This operation to \nCons  onto the head of the list is repeated for all items, and consequently, \nevery list terminates with an empty state. \nOne of the biggest attractions of functional lists is the ease with which they can be \nused to write thread-safe code. In fact, functional data structures can be passed by refer -\nence to a callee with no risk of it being corrupted, as shown in figure 3.7.\nOriginallist\nMultiple threads can access th e\nreference to the list withou t\ngenerating corr upt data.[ ]\n3\n2Thread 1\nThread 2\nThread 31Reference\nto the list[ ]Function caller\n3\n2\n1Pass by\nreference\nFigure 3.7  The list is passed by reference to the function caller (callee). Because the list is immutable, \nmultiple threads can access the reference without generating any data corruption.(continued)\n \n 83 Immutability for a change\nBy definition, to be thread safe, an object must preserve a consistent state every time it’s observed. You shouldn’t observe a data structure collection removing an item from it while in the middle of a resize, for example. In a multithreaded program, applying the execution against an isolated portion of a functional data structure is an excellent and safe way to avoid sharing data. \nfunctional  lists in f#\nF# has a built-in implementation of an immutable list structure, which is represented as a linked list (a linear data structure that consists of a set of items linked together in a chain). Every programmer has written a linked list at one point. In the case of func-tional lists, however, the implementation requires a little more effort to guarantee the immutable behavior that the list never changes once created. Fortunately, the repre-sentation of a list in F# is simple, taking advantage of the support for algebraic data types (ADT) (https:/ /en.wikipedia.org/wiki/Algebraic_data_type) that let you define a generic recursive \nList  type.\nAn ADT is a composite type, which means that its structure is the result of combin-\ning other types. In F#, ADTs are called discriminated unions (DU), and they’re a precise modeling tool to represent well-defined sets of data shapes under the same type. These different shapes are called cases of a DU.\nThink about a representation of the motor vehicle domain, where the types \nCar and \nTruck  belong to the same base type Vehicle . DUs fit well for building complicated data \nstructures (like linked lists and a wide range of trees) because they’re a simpler alter -\nnative to a small-object hierarchy. For example, this is a DU definition for the domain \nVehicle :\ntype Vehicle=    | Motorcycle of int    | Car of int    | Truck of int\nYou can think of DUs as a mechanism to provide additional semantic meaning over a type. For example, the previous DU can be read as “A Vehicle type that can be a Car, a Motorcycle, or a Truck.”\nThe same representation in C# should use a \nVehicle  base class with derived types for \nCar, Truck , and Motorcycle . The real power of a DU is when it’s combined with pattern \nmatching to branch to the appropriate computation, depending on the discriminated case passed. The following F# function prints the number of wheels for the vehicle passed: \nlet printWheels vehicle =    match vehicle with    | Car(n) -> Console.WriteLine(""Car has {0} wheels"", n)    | Motorcycle(n) -> Console.WriteLine(""Motorcycle has {0} wheels"", n)    | Truck(n) -> Console.WriteLine(""Truck has {0} wheels"", n)\nThis listing represents a recursive list, using the F# DU that satisfies the definition given in the previous section. A list can either be empty or is formed by an element and an existing list.\n \n84 chapter  3 Functional data structures and immutability\nListing 3.13  Representation of a list in F# using discriminated unions\ntype FList<'a> =    | Empty                                 | Cons of head:'a * tail:FList<'a>  let rec map f (list:FList<'a>) =            match list with    | Empty -> Empty    | Cons(hd,tl) -> Cons(f hd, map f tl)let rec filter p (list:FList<'a>) =    match list with    | Empty -> Empty    | Cons(hd,tl) when p hd = true -> Cons(hd, filter p tl)    | Cons(hd,tl) -> filter p tl\nYou can now create a new list of integers as follows:\nlet list = Cons (1, Cons (2, Cons(3, Empty))) \nF# already has a built-in generic List  type that lets you rewrite the previous imple-\nmented FList  using the following two (equivalent) options:\nlet list  = 1 :: 2 :: 3 :: []let list = [1; 2; 3]\nThe F# list is implemented as a singly linked list, which provides instant access to the head of the list O(1) and linear time O(n) for element access, where (n) is the index of the item.\nfunctional  lists in c#\nYou have several ways to represent a functional list in OOP. The solution adopted in C# is a generic class \nFList<T> , so it can store values of any type. This class exposes read-\nonly auto-getter properties for defining the head element of the list and the FList<T>  \ntail linked list. The IsEmpty  property indicates if the current instance contains at least \na value. The following listing shows the full implementation.\nListing 3.14  Functional list in C#\npublic sealed class FList<T>{    private FList(T head, FList<T> tail)     {        Head = head;        Tail = tail.IsEmpty                ? FList<T>.Empty : tail;        IsEmpty = false;    }    private FList()                         {        IsEmpty = true;    }    public T Head { get; }                  public FList<T> Tail { get; }       Empty caseCons case has the head element and the tail\nRecursive function that traverses a list using pattern matching to deconstruct and performs a transformation on each item\nCreates a list with a value and a reference to tail\nCreates a new list that’s empty\nThe Head property returns the first element from the list.\nThe Tail property returns the rest of the linked list.\n \n 85 Immutability for a change\n    public bool IsEmpty { get; }            public static FList<T> Cons(T head, FList<T> tail)          {        return tail.IsEmpty            ? new FList<T>(head, Empty)            : new FList<T>(head, tail);    }    public FList<T> Cons(T element)         {        return FList<T>.Cons(element, this);    }    public static readonly FList<T> Empty = new FList<T>(); }\nThe FList<T>  class has a private constructor to enforce its instantiation using either \nthe static helper method Cons  or the static field Empty . This last option returns an \nempty instance of the FList<T>  object, which can be used to append new elements \nwith the instance method Cons . Using the FList<T>  data structure, it’s possible to cre-\nate functional lists in C# as follows:\nFList<int> list1 = FList<int>.Empty;FList<int> list2 = list1.Cons(1).Cons(2).Cons(3);FList<int> list3 = FList<int>.Cons(1, FList<int>.Empty);FList<int> list4 = list2.Cons(2).Cons(3);\nThe code sample shows a few important properties for building an FList  of integers. \nThe first list1  is created from an initial state of empty list  using the field Empty \nFList<int>.Empty , which is a common pattern used in immutable data structures. \nThen, with this initial state, you can use the fluent semantic approach to chain a series of \nCons  to build the collection as shown with list2  in the code example. \nlaziness  values  in functional  lists \nIn chapter 2, you saw how lazy evaluation is an excellent solution to avoid excessive \nduplicate operations by remembering operation results. Moreover, lazily evaluated code benefits from a thread-safe implementation. This technique can be useful in the context of functional lists by deferring computations and consequently gaining in per -\nformance. In F#, lazy thunks (computations that have been deferred) are created using the \nlazy  keyword:\nlet thunkFunction = lazy(21 * 2)\nThis listing defines a generic lazy list implementation.\nListing 3.15  Lazy list implementation using F# \ntype LazyList<'a> =    | Cons of head:'a * tail:Lazy<'a LazyList>      | Emptylet empty = lazy(Empty)                         let rec append items list =                         match items withThis property indicates the state of the list.\nA static method provides a \nnicer syntax for creating lists.\nThis Cons function provides a fluent semantic to chain an item to a given list.\nThis static constructor \ninstantiates an empty list.\nUses a DU to define a list with a lazy evaluated tail\nUses a helper function to represent an empty list\nShows the function that appends an item at the top of a given list",17830
37-3.3.6 Building a persistent data structure an immutable binary tree.pdf,37-3.3.6 Building a persistent data structure an immutable binary tree,"86 chapter  3 Functional data structures and immutability\n    | Cons(head, Lazy(tail)) ->        Cons(head, lazy(append tail list))          | Empty -> listlet list1 = Cons(42, lazy(Cons(21, empty)))      // val list1: LazyList<int> = Cons (42,Value is not created.)let list = append (Cons(3, empty)) list1        // val list : LazyList<int> = Cons (3,Value is not created.)let rec iter action list =                          match list with    | Cons(head, Lazy(tail)) ->        action(head)        iter action tail    | Empty -> ()list |> iter (printf ""%d .. "")                  // 3 .. 42 .. 21 ..\nTo be more efficient in handling empty states, the lazy list implementation shifts the laziness into the tail of the \nCons  constructor, improving performance for the succes-\nsive data structures. For example, the append  operation is delayed until the head is \nretrieved from the list. \n3.3.6 Building a persistent data structure: an immutable binary tree \nIn this section, you’ll learn how to build a binary tree (B-tree) in F#, using recursion and \nmultithreaded processes. A tree structure is, in layman’s terms, a collection of nodes that are connected in such a way that no cycles are allowed. A tree tends to be used where performance matters. (It’s odd that the .NET Framework never shipped with a tree in its collection namespaces.) Trees are the most common and useful data structures in computer programming and are a core concept in functional programming languages. \nA tree is a polymorphic recursive data structure containing an arbitrary number of \ntrees—trees within a tree. This data structure is primarily used to organize data based on keys, which makes it an efficient tool for searches. Due to its recursive definition, trees are best used to represent hierarchical structures, such as a filesystem or a database. Moreover, trees are considered advanced data structures, which are generally used in subjects such as machine learning and compiler design. FP provides recursion as a pri-mary constructor to iterate data structures, making it complementary for this purpose.\nTree structures allow representing hierarchies and composing complex structures \nout of simple relationships and are used to design and implement a variety of efficient algorithms. Common uses of trees in XML/Markup are parsing, searching, compress-ing, sorting, image processing, social networking, machine learning, and decision trees. This last example is widely used in domains such as forecasting, finance, and gaming. \nThe ability to express a tree in which each node may have an arbitrary number of \nbranches, like n-ary and B-tree, turns out to be an impediment rather than a benefit. The append function reclusively adds an item to a list; it can be used to append two lists.Creates a list with two elements: 42 and 21\nAppends the value 3 to \nlist1 previously created\nUses a function to iterate recursively through a list\nPrints the values of the list using the iter function\n \n 87 Immutability for a change\nThis section covers the B-tree, which is a self-balancing tree where every node has between zero to two child nodes at most, and the difference in depth (called height) of the tree between any leaves is at most one. Depth of a node is defined as the number of edges from the node to the root node. In the B-tree, each node points to two other nodes, called the left and right child nodes. \nA better tree definition is provided by figure 3.8, which shows key properties of the \ndata structure. \n4\n6\n4 5 8\n5 7 68\nA tree has a special node call root, which has no parent (node 4 in figure 3.8), and may be either a leaf or a node with two or more children. A parent node has at least one child, and each child has one parent. Nodes with no children are treated as leaves (nodes 6, 5, 5, 7 in the figure), and children of the same parent are known as siblings.\nb-trees  in functional  f#\nWith F#, it’s easy to represent a tree structure because of the support of ADTs and dis-criminated unions. In this case, DU provides an idiomatic functional way to represent a tree. This listing shows a generic DU-based binary tree definition with a special case for empty branches. \nListing 3.16  Immutable B-tree representation in F# \ntype Tree<'a> =     | Empty         | Node of leaf:'a * left:Tree<'a> * right:Tree<'a> let tree =          Node (20,        Node (9, Node (4, Node (2, Empty, Empty), Empty),                 Node (10, Empty, Empty)),        Empty)\nThe elements in a B-tree are stored using the Node  type constructor, and the Empty  case \nidentifier represents an empty node that doesn’t specify any type information. The \nEmpty  case serves as a placeholder identifier. With this B-tree definition, you can create \nhelper functions to insert or to verify an item in the tree. These functions are imple-mented in idiomatic F#, using recursion and pattern matching. Figure 3.8  Binary tree representation where every \nnode has between zero and two child nodes. In this figure, node 4 is the root from which two branches start, nodes 8 and 6. The left branch is a link to the left subtree and the right branch is a link to the right subtree. The nodes without child nodes, 6, 5, 5, and 7, are called leaves.\nUses a DU that defines the generic tree\nShows an empty caseUses a node case that defines a generic value leaf, and recursively branches out to left and right sub-trees\nShows an instance of a tree of integers",5467
38-3.4.2 Continuation passing style to optimize recursive function.pdf,38-3.4.2 Continuation passing style to optimize recursive function,"88 chapter  3 Functional data structures and immutability\nListing 3.17  B-tree helper recursive functions \nlet rec contains item tree =     match tree with    | Empty -> false    | Node(leaf, left, right) ->        if leaf = item then true        elif item < leaf then contains item left        else contains item rightlet rec insert item tree =     match tree with    | Empty -> Node(item, Empty, Empty)    | Node(leaf, left, right) as node ->        if leaf = item then node        elif item < leaf then Node(leaf, insert item left, right)        else Node(leaf, left, insert item right)let ``exist 9`` = tree |> contains 9let ``tree 21`` = tree |> insert 21let ``exist 21`` = ``tree 21`` |> contains 21\nBecause the tree is immutable, the function insert returns a new tree, with the copy of only the nodes that are in the path of the node being inserted. Traversing a DU tree in functional programming to look at all the nodes involves a recursive function. Three main approaches exist to traversing a tree: in-order, post-order, and pre-order traversal (https:/ /en.wikipedia.org/wiki/Tree_traversal). For example, in the in-order tree nav-igation, the nodes on the left side of the root are processed first, then the root, and ultimately the nodes on its right as shown here.\nListing 3.18  In-order navigation function\nlet rec inorder action tree =     seq {        match tree with        | Node(leaf, left, right) ->            yield! inorder action left            yield action leaf            yield! inorder action right        | Empty -> ()    }tree |> inorder (printfn ""%d"") |> ignore \nThe function inorder  takes as an argument a function to apply to each value of the \ntree. In the example, this function is an anonymous lambda that prints the integer stored in the tree.\n3.4 Recursive functions: a natural way to iterate\nRecursion is calling a function on itself, a deceptively simple programming concept. \nHave you ever stood in between two mirrors? The reflections seem to carry on for -\never—this is recursion. Functional recursion is the natural way to iterate in FP because Uses recursion to define functions that walk through a tree structure\nUses a function that traverses the tree structure from the root to the left sub-nodes and then moving to the right sub-nodes\nUses the inorder function to print all node values in the tree\n \n 89 Recursive functions: a natural way to iterate\nit avoids mutation of state. During each iteration, a new value is passed into the loop constructor instead to be updated (mutated). In addition, a recursive function can be composed, making your program more modular, as well as introducing opportunities to exploit parallelization.\nRecursive functions are expressive and provide an effective strategy to solve complex \nproblems by breaking them into smaller, although identical, subtasks. (Think in terms of Russian nesting dolls, with each doll being identical to the one before, only smaller.) While the whole task may seem daunting to solve, the smaller tasks are easier to solve directly by applying the same function to each of them. The ability to split the task into smaller tasks that can be performed separately makes recursive algorithms candidates for parallelization. This pattern, also called Divide and Conquer,\n8 leads to dynamic task \nparallelism, in which tasks are added to the computation as the iteration advances. For more information, reference the example in section 1.4.3. Problems with recursive data structures naturally use the Divide and Conquer strategy due to its inherent potential for concurrency.\nWhen considering recursion, many developers fear performance penalties for the \nexecution time of a large number of iterations, as well as receiving a \nStackoverflow  \nexception. The correct way to write recursive functions is using the techniques of tail recursion and CPS. Both strategies are good ways to minimize stack consumption and increase speed, as you’ll see in the examples to come.\n3.4.1 The tail of a correct recursive function: tail-call optimization\nA tail call, also known as tail-call optimization (TCO), is a subroutine call performed as the final action of a procedure. If a tail call might lead to the same subroutine being called again in the call chain, then the subroutine is said to be tail recursive, a special case of recursion. Tail-call recursion is a technique that converts a regular recursive function into an optimized version that can handle large inputs without any risks and side effects. \nNOTE  The primary reason for a tail call as an optimization is to improve data \nlocality, memory usage, and cache usage. By doing a tail call, the callee uses the same stack space as the caller. This reduces memory pressure. It marginally improves the cache because the same memory is reused for subsequent callers and can stay in the cache, rather than evicting an older cache line to make room for a new cache line.\nWith tail-call recursion, there are no outstanding operations to perform within the function it returns, and the last call of the function is a call to itself. You’ll refactor the implementation of a factorial number function into a tail-call optimized function. The following listing shows the tail-call optimized recursive function implementation. \n8 The Divide and Conquer pattern solves a problem by recursively dividing it into subproblems, solving each one independently, and then recombining the sub-solutions into a solution to the original problem.\n \n90 chapter  3 Functional data structures and immutability\nListing 3.19  Tail-call recursive implementation of a factorial in F#\nlet rec factorialTCO (n:int) (acc:int) =    if n <= 1 then acc    else factorialTCO (n-1) (acc * n) let factorial n = factorialTCO n 1\nIn this implementation of the recursive function, the parameter acc acts as an accumu-\nlator. By using an accumulator and ensuring that the recursive call is the last operation in the function, the compiler optimizes the execution to reuse a single-stack frame, instead of storing each intermediate result of the recursion onto different stack frames as shown in figure 3.9.\nfactorialTCO(4)\nfactorialTCO(24)factorialTCO( 4, 3)\nfactorialTCO(12, 2)\nfactorialTCO(24, 1)\n24\nThe figure illustrates the tail-recursive definitions of factorials. Although F# supports tail-call recursive functions, unfortunately, the C# compiler isn’t designed to opti-mize tail-call recursive functions. \n3.4.2 Continuation passing style to optimize recursive function\nSometimes, optimized tail-call recursive functions aren’t the right solution or can be difficult to implement. In this case, one possible alternative approach is CPS, a tech-nique to pass the result of a function into a continuation. CPS is used to optimize recursive functions because it avoids stack allocation. Moreover, CPS is used in the Microsoft TPL, in \nasync/await  in C#, and in async-workflow  in F#.\nCPS plays an important role in concurrent programming. This following code exam-\nple shows how the CPS pattern is used in a function GetMaxCPS :\nstatic void GetMaxCPS(int x, int y, Action<int> action)                                         => action(x > y ? x : y);GetMaxCPS (5, 7, n => Console.WriteLine(n));\nThe argument for the continuation passing is defined as a delegate Action<int> , \nwhich can be used conveniently to pass a lambda expression. The interesting part is that the function with this design never returns a result directly; instead, it supplies The last operation of the function recursively calls itself without computing any other operations. \nFigure 3.9  Tail-recursive definition of \na factorial, which can reuse a single stack frame\n \n 91 Recursive functions: a natural way to iterate\nthe result to the continuation procedure. CPS can also be used to implement recursive functions using tail calls. \nrecursive  functions  with cps\nAt this point, with basic knowledge about CPS, you’ll refactor the factorial example \nfrom listing 3.19 to use the CPS approach in F#. (You can find the C# implementation in the downloadable source code for this book.)\nListing 3.20  Recursive implementation of factorials using CPS in F#\nlet rec factorialCPS x continuation =    if x <= 1 then continuation()     else factorialCPS (x - 1) (fun () -> x * continuation()) let result = factorialCPS 4 (fun () -> 1) \nThis function is similar to the previous implementation with the accumulator; the dif-ference is that the function is passed instead of the accumulator variable. In this case, the action of the function \nfactorialCPS  applies the continuation  function to its result. \nb-tree structure  walked  in parallel  recursively  \nListing 3.21 shows an example that iterates recursively through a tree structure to per -\nform an action against each element. The function WebCrawler , from chapter 2, builds \na hierarchy representation of web links from a given website. Then it scans the HTML content from each web page, looking for image links that download in parallel. The code examples from chapter 2 (listings 2.16, 2.17, 2.18, and 2.19) were intended to be an introduction to a parallel technique rather than a typical task-based parallelism procedure. Downloading any sort of data from the internet is an I/O operation; you’ll learn in chapter 8 that it’s best practice to perform I/O operations asynchronously. \nListing 3.21  Parallel recursive divide-and-conquer function \nlet maxDepth = int(Math.Log(float System.Environment.ProcessorCount,\n➥ 2.)+4.) \nlet webSites : Tree<string> =    WebCrawlerExample.WebCrawler(""http://www.foxnews.com"")    |> Seq.fold(fun tree site -> insert site tree ) Empty let downloadImage (url:string) =    use client = new System.Net.WebClient()    let fileName = Path.GetFileName(url)    client.DownloadFile(url, @""c:\Images\"" + fileName)    let rec parallelDownloadImages tree depth =                   match tree with    | _ when depth = maxDepth ->        tree |> inorder downloadImage |> ignore    | Node(leaf, left, right) ->        let taskLeft  = Task.Run(fun() ->            parallelDownloadImages left (depth + 1))        let taskRight = Task.Run(fun() ->            parallelDownloadImages right (depth + 1))The value of result is 24. \nUses a threshold to avoid the creation of too many tasks in comparison with the number of cores\nUses a fold constructor to create \nthe tree structure representing \nthe website hierarchy\nDownloads the image into a local file\nShows the recursive function that walks the tree structure in parallel to download  simultaneously multiple images \n \n92 chapter  3 Functional data structures and immutability\n        let taskLeaf  = Task.Run(fun() -> downloadImage leaf)        Task.WaitAll([|taskLeft;taskRight;taskLeaf|])         | Empty -> ()\nThe Task.Run  constructor is used to create and spawn the tasks. The parallel recursive \nfunction parallelDownloadImages takes the argument depth, which is used to limit \nthe number of tasks created to optimize resource consumption. \nIn every recursive call, the depth value increases by one, and when it exceeds the \nthreshold maxDepth , the rest of the tree is processed sequentially. If a separate task is \ncreated for every tree node, then the overhead of creating new tasks would exceed the benefit gained from running the computations in parallel. If you have a computer with eight processors, then spawning 50 tasks will impact the performance tremendously because of the contention generated from the tasks sharing the same processors. The TPL scheduler is designed to handle large numbers of concurrent tasks, but its behavior isn’t appropriate for every case of dynamic task parallelism (http:/ /mng.bz/ww1i),\n and \nin some circumstances, as in the previous parallel recursive function, a manual tune-up is preferred.\nUltimately, the \nTask.WaitAll  construct is used to wait for the tasks to complete. Fig-\nure 3.10 shows the hierarchy representation of the spawned tasks running in parallel. \nTask BTask C2 6\nTask A7 9 8 5Root\nFigure 3.10  From the root node, Task C is created to process the right side of the subtree. This process \nis repeated for the subtree running Task A. When it completes, the left side of the subtree is processed by Task B. This operation is repeated for all the subtrees, and for each iteration, a new task is created. \nThe execution time to complete the recursive parallel operation parallelDownload -\nImages  has been measured against a sequential version. The benchmark is the average \nof downloading 50 images three times (table 3.4). \nTable 3.4  Benchmark of downloading 50 images using parallel recursion\nSerial Parallel\n19.71 4.12Waits for the tasks to complete\n \n 93 Recursive functions: a natural way to iterate\nparallel  calculator  \nAnother interesting way to use a tree structure is building a parallel calculator. After what you’ve learned, the implementation of such a program isn’t trivial. You can use ADTs in the form of F# DUs to define the type of operations to perform:\ntype Operation = Add | Sub | Mul | Div | Pow\nThen, the calculator can be represented as a tree structure, where each operation is a node with the details to perform a calculation:\ntype Calculator =    | Value of double    | Expr of Operation * Calculator * Calculator\nClearly, from this code, you can see the resemblance to the tree structure previously used: \ntype Tree<'a> =        | Empty            | Node of leaf:'a * left:Tree<'a> * right:Tree<'a> \nThe only difference is that the Empty  case in the tree structure is replaced with the \nvalue  case in the calculator. To perform any mathematical operation you need a value. \nThe leaf of the tree becomes the Operation  type, and the left and right branches recur -\nsively reference the calculator type itself, exactly as the tree did. \nNext, you can implement a recursive function that iterates through the calculator \ntree and performs the operations in parallel. This listing shows the implementation of the \neval  function and its use. \nListing 3.23  Parallel calculator \nlet spawn (op:unit->double) = Task.Run(op)    let rec eval expr =    match expr with                               | Value(value) -> value                       | Expr(op, lExpr, rExpr) ->                       let op1 = spawn(fun () -> eval lExpr)         let op2 = spawn(fun () -> eval rExpr)         let apply = Task.WhenAll([op1;op2])           let lRes, rRes = apply.Result.[0], apply.Result.[1]        match op with                                 | Add -> lRes + rRes        | Sub -> lRes - rRes        | Mul -> lRes * rRes        | Div -> lRes / rRes        | Pow -> System.Math.Pow(lRes, rRes)\nThe function eval  recursively evaluates in parallel a set of operations defined as a tree \nstructure. During each iteration, the expression passed is pattern matched to extract the value if the case is a \nValue  type, or to compute the operation if the case is an Expr  Uses a helper function to spawn a Task to run an operation\nPattern-matches against the calculator DU to branch the evolution\nIf the expr case is a Value, extracts the value and returns it\nIf the expr case is an Expr, extracts the operation and recursively reevaluates the branches to extract the values\nSpawns a task for each reevaluation, which could be another operation to compute\nWaits for the operations to complete \nAfter having evaluated the values, which could be the result from other operations, performs the current operation\n \n94 chapter  3 Functional data structures and immutability\ntype. Interestingly, the recursive re-evolution for each branch of the node case Expr  is \nmade in parallel. Each branch Expr  returns a value  type, which is calculated in each \nchild (sub-nodes) operation. Then, these values are used for the last operation, which is the root of the operation tree passed as argument for the final result. Here is a sim-ple set of operations in the shape of a calculator tree, which compute the operations 2^10 / 2^9 + 2 * 2:\nlet operations =            Expr(Add,    Expr(Div,      Expr(Pow, Value(2.0), Value(10.0)),      Expr(Pow, Value(2.0), Value(9.0))),    Expr(Mul, Value(2.0), Value(2.0)))let value = eval operations    \nIn this section, the code for defining a tree data structure and performing a recursive task-based function is shown in F#; but the implementation is feasible in C# as well. Rather than showing all the code here, you can download the full code from the book’s website.\nSummary \n¡ Immutable data structures use intelligent approaches, such as structural sharing, to minimize the copy-shared elements to minimize GC pressure.\n¡ It’s important to dedicate some time to profiling application performance to avoid bottlenecks and bad surprises when the program runs in production and under heavy payloads.\n¡ Lazy evaluation can be used to guarantee thread safety during the instantiation of an object and to gain performance in functional data structures by deferring computation to the last moment.\n¡ Functional recursion is the natural way to iterate in functional programming because it avoids mutation of state. In addition, a recursive function can be com-posed, making your program more modular.\n¡ Tail-call recursion is a technique that converts a regular recursive function into an optimized version that can handle large inputs without any risks or side effects.\n¡ Continuation passing style (CPS) is a technique to pass the result of a function into a continuation. This technique is used to optimize recursive functions because it avoids stack allocation. Moreover, CPS is used in the Task Parallel Library in .NET 4.0, in \nasync/await  in C#, and in async-workflow  in F#. \n¡ Recursive functions are great candidates to implement a Divide and Conquer technique, which leads to dynamic task parallelism.",17883
39-Part 2 How to approach the different parts of a concurrent program.pdf,39-Part 2 How to approach the different parts of a concurrent program,"Part 2\nHow to approach the different \nparts of a concurrent program\nT his part of the book dives into functional programming concepts and \napplicability. We’ll explore various concurrent programming models, with an emphasis on the benefits and advantages of this paradigm. Topics will include the Task Parallel Library along with parallel patterns such as Fork/Join, Divide and Conquer, and MapReduce. We’ll also discuss declarative composition, high-level abstraction in asynchronous operations, the agent programming model, and the message-passing semantic. You’ll see firsthand how functional program-ming allows you to compose program elements without evaluating them. These techniques parallelize work and make programs easier to reason about and more efficient to run due to optimal memory consumption.",812
40-4.2 The ForkJoin pattern parallel Mandelbrot.pdf,40-4.2 The ForkJoin pattern parallel Mandelbrot,"974The basics of \nprocessing big data: \ndata parallelism, part 1\nThis chapter covers\n¡ The importance of data parallelism in a world of big data\n¡ Applying the Fork/Join pattern\n¡ Writing declarative parallel programs\n¡ Understanding the limitation of a parallel  \nfor loop\n¡ Increasing performance with data parallelism\nImagine you’re cooking a spaghetti for dinner for four, and let’s say it takes 10 min-utes to prepare and serve the pasta. You begin the preparation by filling a medi-um-sized pot with water to boil. Then, two more friends show up at your house for dinner. Clearly, you need to make more pasta. You can switch to a bigger pot of water with more spaghetti, which will take longer to cook, or you can use a second pot in tandem with the first, so that both pots of pasta will finish cooking at the same time. Data parallelism works in much the same way. Massive amounts of data can be processed if “cooked” in parallel. \nIn the last decade, the amount of data being generated has increased exponen-\ntially. In 2017 it was estimated that every minute there were 4,750,000 “likes” on \n \n98 chapter  4 The basics of processing big data: data parallelism, part 1\nFacebook, almost 400,000 tweets, more than 2.5 million posts on Instagram, and more than 4 million Google searches. These numbers continue to increase at the rate of 15% every year. This acceleration impacts businesses that now must quickly analyze multi-tudes of big data (https:/ /en.wikipedia.org/wiki/Big_data). How is it possible to ana-lyze this massive amount of data while maintaining quick responses? The answer comes from a new breed of technologies designed with data parallelism in mind, specifically, with a focus on the ability to maintain performance in the event of continually increas-ing data. \nIn this chapter, you’ll learn concepts, design patterns, and techniques to rapidly pro-\ncess tons of data. You’ll analyze the problems originating from parallel loop constructs and learn about solutions. You’ll also learn that by using functional programming in combination with data parallelism it’s possible to achieve impressive performance improvements in your algorithms with minimal code changes. \n4.1 What is data parallelism?\nData parallelism is a programming model that performs the same set of operations on a large quantity of data in parallel. This programming model is gaining traction because it quickly processes massive volumes of data in the face of a variety of big data prob-lems. Parallelism can compute an algorithm without requiring reorganization of its structure, thereby progressively increasing scalability. \nThe two models of data parallelism are single instruction single data and single \ninstruction multiple data:\n¡ Single instruction single data (SISD) is used to define a single-core architecture. A single-core processor system executes one task per any CPU clock cycle; there-fore, the execution is sequential and deterministic. It receives one instruction (single instruction), performs the work required for a single piece of data, and returns the results of the computation. This processor architecture will not be covered in this book. \n¡ Single instruction multiple data (SIMD) is a form of parallelism achieved by distribut-\ning the data among the available multiple cores and applies the same operations at any given CPU clock cycle. This type of parallel, multicore CPU architecture is commonly used to exploit data parallelism.  \nTo achieve data parallelism, the data is split into chunks, and each chunk is subject to intensive computations and processed independently, either to produce new data to aggregate or to reduce to a scalar value. If you aren’t familiar with these terms, they should be clear by the end of the chapter.\nThe ability to compute chunks of data independently is the key to achieving signif-\nicant performance increase, because removing dependencies between blocks of data eliminates the need for synchronization to access data and any concerns about race conditions, as shown in figure 4.1. \n \n 99 What is data parallelism?\nData\nset\nParallelismTask 1\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6Data\nsetData\nset\nParallelismTask 1\nN 1/6\nData\nsetTask 2\nN 1/6\nTask 3\nN 1/6\nTask 4\nN 1/6\nTask 5\nN 1/6\nTask 6\nN 1/6\nFigure 4.1  Data parallelism is achieved by splitting the data set into chunks and independently \nprocessing each partition in parallel, assigning each chunk to a separate task. When the tasks complete, the data set is reassembled. In this figure, the data set on the left is processed by multiple tasks using a lock to synchronize their access to the data as a whole. In this case, the synchronization is a source of contention between threads and creates performance overhead. The data set on the right is split into six parts, and each task performs against one-sixth of the total size N of the data set. This design removes the necessity of using locks to synchronize.\nData parallelism can be achieved in a distributed system, by dividing the work among multiple nodes; or in a single computer; or by partitioning the work into separated threads. This chapter focuses on implanting and using multicore hardware to perform data parallelism. \n4.1.1 Data and task parallelism\nThe goal of data parallelism is decomposing a given data set and generating a sufficient number of tasks to maximize the use of CPU resources. In addition, each task should be scheduled to compute enough operations to guarantee a faster execution time. This is in contrast to context switching, which could introduce negative overhead. \nData parallelism comes in two flavors:\n¡ Task parallelism targets the execution of computer programs across multiple pro-cessors, where each thread is responsible for performing a different operation at the same time. It is the simultaneous execution of many different functions across the same or different data sets on multiple cores. \n¡ Data parallelism targets the distribution of a given data set into smaller partitions across multiple tasks, where each task performs the same instruction in paral-lel. For example, data parallelism could refer to an image-processing algorithm, where each image or pixel is updated in parallel by independent tasks. Con-versely, task parallelism would compute in parallel a set of images, applying a different operation for each image.  See figure 4.2. \n \n100 chapter  4 The basics of processing big data: data parallelism, part 1\nData\nsetParallelismData\nsetTask 2\nTask 3\nTask 4\nTask 5\nTask 6Data\nsetParallelismData parallelism Task parallelis m\nTask 1\n1N 1/6\nData\nsetTask 2\n2N 1/6\nTask 3\n3N 1/6\nTask 4\n4N 1/6\nTask 5\n5N 1/6\nTask 6\n6N 1/6Task 1\nFigure 4.2  Data parallelism is the simultaneous execution of the same function across the elements \nof a data set. Task parallelism is the simultaneous execution of multiple different functions across the same or different data sets.\nIs summary, task parallelism focuses on executing multiple functions (tasks), and aims to reduce the overall time of computation by running these tasks simultaneously. Data parallelism reduces the time it takes to process a data set by splitting the same algo-rithm computation among multiple CPUs to be performed in parallel.\n4.1.2 The “embarrassingly parallel” concept\nIn data parallelism, the algorithms applied to process the data are sometimes referred to as “embarrassingly parallel” and have the special property of natural scalability.\n1 \nThis property influences the amount of parallelism in the algorithm as the number of available hardware threads increases. The algorithm will run faster in a more powerful computer. In data parallelism, the algorithms should be designed to run each opera-tion independently in a separate task associated with a hardware core. This structure has the advantage of automatically adapting the workload at runtime and adjusting the data partitioning based on the current computer. This behavior guarantees running the program on all available cores. \nConsider summing a large array of numbers. Any part of this array may be summed \nup independently from any other part. The partial sums can then be summed together themselves and achieve the same result as if the array had been summed in series. Whether or not the partial sums are computed on the same processor or at the same time doesn’t matter. Algorithms like this one with a high degree of independence are known as embarrassingly parallel problems: the more processors that you throw at them, the faster they will run. In chapter 3 you saw the Divide and Conquer pattern that provides natural parallelism. It distributes work to numerous tasks and then com-bines (reduces) the results again. Other embarrassingly parallel designs don’t require a complex coordination mechanism to provide natural auto-scalability. Examples of \n1 For more information on embarrassingly parallel, also known as pleasing parallel, see https://en .wikipedia.org/wiki/Embarrassingly_parallel.\n \n 101 What is data parallelism?\ndesign patterns that use this approach are Fork/Join, Parallel Aggregation (reduce), and MapReduce. We’ll discuss these designs later in this chapter. \n4.1.3 Data parallelism support in .NET\nIdentifying code in your programs that can be parallelized isn’t a trivial task, but com-mon rules and practices can help. The first thing to do is profiling the application. This analysis of the program identifies where the code spends its time, which is your clue for where you should start deeper investigations to improve performance and to detect opportunities for parallelism. As a guide, an opportunity for parallelism is when two or more portions of source code can be executed deterministically in paral-lel, without changing the output of the program. Alternatively, if the introduction of parallelism would change the output of the program, the program isn’t deterministic and could become unreliable; therefore, parallelism is unusable. \nTo ensure deterministic results in a parallel program, the blocks of source code that \nrun simultaneously should have no dependencies between them. In fact, a program can be parallelized easily when there are no dependencies or when existing dependen-cies can be eliminated. For example, in the Divide and Conquer pattern, there are no dependencies among the recursive executions of the functions so that parallelism can be accomplished. \nA prime candidate for parallelism is a large data set where a CPU-intensive opera-\ntion can be performed independently on each element. In general, loops in any form (\nfor loop, while  loop, and for-each loop) are great candidates to exploit parallel-\nism. Using Microsoft’s TPL, reshaping a sequential loop into a parallel one is an easy task. This library provides a layer of abstraction that simplifies the implementation over common parallelizable patterns that are involved in data parallelism. These patterns can be materialized using the parallel constructs \nParallel.For  and Parallel.ForEach  \noffered by the TPL Parallel  class. \nHere are a few patterns found in programs that provide an opportunity for \nparallelism:\n¡ Sequential loops, where there are no dependencies between the iteration steps.\n¡ Reduction and/or aggregation operations, where the results of the computa-tion between steps are partially merged. This model can be expressed using a MapReduce pattern.\n¡ Unit of computation, where explicit dependencies can be converted into a Fork/Join pattern to run each step in parallel. \n¡ Recursive type of algorithms using a Divide and Conquer approach, where each iteration can be executed independently in a different thread in parallel.\nIn the .NET framework, data parallelism is also supported through PLINQ, which I recommend. The query language offers a more declarative model for data parallelism, compared to the \nParallel  class, and is used for parallel evaluation of arbitrary queries \nagainst a data source. Declarative implies what you want to be done with data rather \n \n102 chapter  4 The basics of processing big data: data parallelism, part 1\nthan how you want that to be done. Internally, the TPL uses sophisticated schedul-ing algorithms to distribute parallelized computations efficiently between the available processing cores. Both C# and F# take advantage of these technologies in a similar way. In the next section, you’ll see these technologies in both programming languages, which can be mixed and complement each other nicely. \n4.2 The Fork/Join pattern: parallel Mandelbrot\nThe best way to understand how to convert a sequential program into a parallel one is with an example. In this section, we’ll transform a program using the Fork/Join pat-tern to exploit parallel computation and to achieve faster performance.\nIn the Fork/Join pattern, a single thread forks and coordinates with multiple inde-\npendent parallel workers and then merges the individual results when complete. Fork/Join parallelism manifests in two primary steps:\n1 Split a given task into a set of subtasks scheduled to run independently in parallel. \n2 Wait for the forked parallel operations to complete, and then successively join the subtask results into the original work.\nRegarding data parallelism, figure 4.3 shows a close resemblance to figure 4.1. The dif-ference is in the last step, where the Fork/Join pattern merges the results back into one.\nAggregate\ndataFork Data\nsetJoin ParallelismTask 1\n1N 1/6\nTask 2\n2N 1/6\nTask 3\n3N 1/6\nTask 4\n4N 1/6\nTask 5\n5N 1/6\nTask 6\n6N 1/6\nFigure 4.3  The Fork/Join pattern splits a task into subtasks that can be executed independently in \nparallel. When the operations complete, the subtasks are joined again. It isn’t a coincidence that this pattern is often used to achieve data parallelism. There are clearly similarities. \nAs you can see, this pattern fits well in data parallelism. The Fork/Join pattern speeds up the execution of a program by partitioning the work into chunks (fork) and run-ning each chunk individually in parallel. After each parallel operation completes, the chunks are merged back again (join). In general, Fork/Join is a great pattern for encoding structured parallelism because the fork and join happen at once (syn-chronously with respect to the caller), but in parallel (from the perspective of perfor -\nmance and speed). The Fork/Join abstraction can be accomplished easily using the \n \n 103 The Fork/Join pattern: parallel Mandelbrot\nParallel.For  loop from the .NET Parallel  class. This static method transparently \ndeals with the partition of data and execution of tasks.\nLet’s analyze the Parallel.For  loop construct with an example. First, you imple-\nment a sequential for loop to draw a Mandelbrot image (see figure 4.4), and then the \ncode will be refactored to run faster. We’ll evaluate the pros and cons of the approach.\nMandelbrot\nA Mandelbrot set of images is made by sampling complex numbers and, for each, deter -\nmining whether the result tends toward infinity when a particular mathematical operation is iterated on it. A complex number is a combination of a real number and an imaginary \nnumber, where any number you can think of is a real number, and an imaginary number is when a squared value gives a negative result. Treating the real and imaginary parts of each number as image coordinates, pixels are colored according to how rapidly the sequence diverges, if at all. Images of the Mandelbrot set display an elaborate boundary that reveals progressively ever-finer recursive detail at increasing magnifications. It’s one of the best-known examples of mathematical visualization.\n \nFor this example, the details of implementing the algorithm aren’t important. What’s important is that for each pixel in the picture (image), a computation runs for each assigned color. This computation is independent because each pixel color doesn’t depend on other pixel colors, and the assignment can be done in parallel. In fact, each pixel can have a different color assigned regardless of the color of the other pixels in the image. The absence of dependencies affects the execution strategy; each computa-tion can run in parallel. \nIn this context, the Mandelbrot algorithm is used to draw an image representing the \nmagnitude value of the complex number. The natural representation of this program uses a \nfor loop to iterate through each value of the Cartesian plane to assign the cor -\nresponding color for each point. The Mandelbrot algorithm decides the color. Before Figure 4.4  The Mandelbrot drawing resulting from \nrunning the code in this section\n \n104 chapter  4 The basics of processing big data: data parallelism, part 1\ndelving into the core implementation, you need an object for the complex number. The following listing shows a simple implementation of a complex number used to make operations over other imaginary complex numbers.\nListing 4.1  Complex number object\nclass Complex{    public Complex(float real, float imaginary)    {        Real = real;        Imaginary = imaginary;    }    public float Imaginary { get; }     public float Real { get; }          public float Magnitude      => (float)Math.Sqrt(Real * Real + Imaginary * Imaginary);       public static Complex operator +(Complex c1, Complex c2)      => new Complex(c1.Real + c2.Real, c1.Imaginary + c2.Imaginary);     public static Complex operator *(Complex c1, Complex c2)      => new Complex(c1.Real * c2.Real - c1.Imaginary * c2.Imaginary,                     c1.Real * c2.Imaginary + c1.Imaginary * c2.Real); }\nThe Complex  class contains a definition for the Magnitude  property. The interesting part \nof this code is the two overloaded operators for the Complex  object. These operators are \nused to add and multiply a complex number, which is used in the Mandelbrot algorithm. The following listing shows the two core functions of the Mandelbrot algorithm. The func-tion \nisMandelbrot  determines if the complex number belongs to the Mandelbrot set.\nListing 4.2  Sequential Mandelbrot            \nFunc<Complex, int, bool> isMandelbrot = (complex, iterations) => {    var z = new Complex(0.0f, 0.0f);    int acc = 0;    while (acc < iterations && z.Magnitude < 2.0)    {        z = z * z + complex;        acc += 1;    }    return acc == iterations;};for (int col = 0; col < Cols; col++) {          for (int row = 0; row < Rows; row++) {          var x = ComputeRow(row);                    var y = ComputeColumn(col);                 var c = new Complex(x, y);Uses the auto-getter property that enforces immutability\nThe Magnitude property \ndetermines the relative size \nof the complex number.\nOperator overloading performs addition \nand multiplication on complex types.\nUses the function to determine \nif a complex number is part of \nthe Mandelbrot set\nUses outer and inner loops over the columns and rows of the image\nShows the operations to convert the current pixel points into values to construct a complex number\n \n 105 The Fork/Join pattern: parallel Mandelbrot\n        var color = isMandelbrot(c, 100) ? Color.Black : Color.White;         var offset = (col * bitmapData.Stride) + (3 * row);        pixels[offset + 0] = color.B; // Blue component              pixels[offset + 1] = color.G; // Green component             pixels[offset + 2] = color.R; // Red component           }}\nThe code omits details regarding the bitmap generation, which isn’t relevant for the purpose of the example. You can find the full solution in the downloadable source code online.\nIn this example, there are two loops: the outer loop iterates over the columns of the \npicture box, and the inner loop iterates over its rows. Each iteration uses the functions \nComputeColumn  and ComputeRow , respectively, to convert the current pixel coordinates \ninto the real and imaginary parts of a complex number. Then, the function isMandel -\nbrot  evaluates if the complex number belongs to the Mandelbrot set. This function \ntakes as arguments a complex number and a number of iterations, and it returns a Bool-ean if—or not—the complex number is a member of the Mandelbrot set. The func-tion body contains a loop that accumulates a value and decrements a count. The value returned is a Boolean that’s true if the accumulator \nacc equals the iterations count.\nIn the code implementation, the program requires 3.666 seconds to evaluate the \nfunction isMandelbrot  1 million times, which is the number of pixels composing the \nMandelbrot image. A faster solution is to run the loop in the Mandelbrot algorithm in parallel. As mentioned earlier, the TPL provides constructs that can be used to blindly parallelize programs, which results in incredible performance improvements. In this example, the higher-order \nParallel.For  function is used as a drop-in replacement \nfor the sequential loop. This listing shows the parallel transformation with minimal changes, keeping the sequential structure of the code.\nListing 4.3  Parallel Mandelbrot            \nFunc<Complex, int, bool> isMandelbrot = (complex, iterations) =>{    var z = new Complex(0.0f, 0.0f);    int acc = 0;    while (acc < iterations && z.Magnitude < 2.0)    {        z = z * z + complex;        acc += 1;    }    return acc == iterations;};System.Threading.Tasks.Parallel.For(0, Cols - 1, col => {     for (int row = 0; row < Rows; row++) {        var x = ComputeRow(row);        var y = ComputeColumn(col);Uses a function to determine the color of a pixelUses this code to assign color attributes to the image pixel\nThe parallel for-loop construct \nis applied only to the outer loop \nto prevent oversaturation of \nCPU resources.\n \n106 chapter  4 The basics of processing big data: data parallelism, part 1\n        var c = new Complex(x, y);        var color = isMandelbrot(c, 100) ? Color.DarkBlue : Color.White;        var offset = (col * bitmapData.Stride) + (3 * row);        pixels[offset + 0] = color.B; // Blue component        pixels[offset + 1] = color.G; // Green component        pixels[offset + 2] = color.R; // Red component    }});\nNote that only the outer loop is paralleled to prevent oversaturation of the cores with work items. With a simple change, the execution time decreased to 0.699 seconds in a quad-core machine. \nOversaturation is a form of extra overhead, originating in parallel programming, \nwhen the number of threads created and managed by the scheduler to perform a com-putation grossly exceeds the available hardware cores. In this case, parallelism could make the application slower than the sequential implementation. \nAs a rule of thumb, I recommend that you parallelize expensive operations at the \nhighest level. For example, figure 4.5 shows nested \nfor loops; I suggest you apply paral-\nlelism only to the outer loop. \nFigure 4.5  Using a Parallel.For  construct, this benchmark compares the execution time of the \nsequential loop, which runs in 9.038 seconds, against the parallel, which runs in 3.443 seconds. The \nParallel.For  loop is about three times faster than the sequential code. Moreover, the last column on \nthe right is the execution time of the over-saturated parallel loop, where both outer and inner loops are using the \nParallel.For  construct. The over-saturated parallel loop runs in 5.788 seconds, which is \n50% slower than the non-saturated version.\nElapsed time vs. CPU time\nFrom the chart in figure 4.5, the CPU time is the time for which the CPU was executing a \ngiven task. The elapsed time is the total clock time that the operation took to complete",23656
41-4.2.1 When the GC is the bottleneck structs vs. class objects.pdf,41-4.2.1 When the GC is the bottleneck structs vs. class objects,"107 The Fork/Join pattern: parallel Mandelbrot\nregardless of resource delays or parallel execution. In general, the elapsed time is higher than CPU time; but this value changes in a multicore machine. When a concurrent program runs in a multicore machine, you achieve true parallelism. In this case, CPU time becomes the sum of all the execution times for each thread running in a different CPU at same given time. In a quad-core computer, for example, when you run a single-threaded (sequential) program, the elapsed time is almost equal to CPU time because there’s only one core working. When running the same program in parallel using all four cores, the elapsed time becomes lower, because the program runs faster, but the CPU time increases because it’s calculated by the sum of the execution time of all four parallel threads. When a program uses more than one CPU to complete the task, the CPU time may be more than the elapsed time. In summary, the elapsed time refers to how much time the program takes with all the parallelism going on, while the CPU time measures how much time all the threads are taking, ignoring the fact that the threads overlap when running in parallel. \n \nIn general, the optimal number of worker threads for a parallel task should be equal to the number of available hardware cores divided by the average fraction of core uti-lization per task. For example, in a quad-core computer with 50% average core uti-lization per task, the perfect number of worker threads for maximum throughput is eight: (4 cores × (100% max CPU utilization / 50% average core utilization per task)). Any number of worker threads above this value could introduce extra overhead due to additional context switching, which would downgrade the performance and processor utilization.\n4.2.1 When the GC is the bottleneck: structs vs. class objects\nThe goal of the Mandelbrot example is to transform a sequential algorithm into a faster one. No doubt you’ve achieved a speed improvement; 9.038 to 3.443 seconds is a little more than three times faster on a quad-core machine. Is it possible to further optimize performance? The TPL scheduler is partitioning the image and assigning the work to different tasks automatically, so how can you improve the speed? In this case, the optimization involves reducing memory consumption, specifically by minimizing memory allocation to optimize garbage collection. When the GC runs, the execution of the program stops until the garbage collection operation completes. \nIn the Mandelbrot example, a new \nComplex  object is created in each iteration to \ndecide if the pixel coordinate belongs to the Mandelbrot set. The Complex  object is a \nreference type, which means that new instances of this object are allocated on the heap. This piling of objects onto the heap results in memory overhead, which forces the GC to intervene to free space.\nA reference object, as compared to a value type, has extra memory overhead due to \nthe pointer size required to access the memory location of the object allocated in the heap. Instances of a class are always allocated on the heap and accessed via a pointer dereference. Therefore, passing reference objects around, because they’re a copy of the pointer, is cheap in terms of memory allocation: around 4 or 8 bytes, according the hardware architecture. Additionally, it’s important to keep in mind that an object also \n \n108 chapter  4 The basics of processing big data: data parallelism, part 1\nhas a fixed overhead of 8 bytes for 32-bit processes and 16 bytes for 64-bit processes. In comparison, a value type isn’t allocated in the heap but rather in the stack, which removes any overhead memory allocation and garbage collection. \nKeep in mind, if a value type (struct) is declared as a local variable in a method, it’s \nallocated on the stack. Instead, if the value type is declared as part of a reference type (class), then the struct allocation becomes part of that object memory layout and exists on the heap.\nThe Mandelbrot algorithm creates and destroys 1 million \nComplex  objects in the for \nloop; this high rate of allocation creates significant work for the GC. By replacing the \nComplex  object from reference to value type, the speed of execution should increase \nbecause allocating a struct to the stack will never cause the GC to perform cleanup operations and won’t result in a program pause. In fact, when passing a \nvalue  type to a \nmethod, it’s copied byte for byte, therefore allocating a struct that will never cause gar -\nbage collection because it isn’t on the heap. \nNOTE  In many cases, the use of a reference type versus a value type can result \nin huge differences in performance. As an example, let’s compare an array of objects with an array of struct types in a 32-bit machine. Given an array of 1 million items, with each item represented by an object that contains 24 bytes of data, with reference types, the total size of the array is 72 MB (8 bytes array overhead + (4 bytes for the pointer × 1,000,000) + ( 8 bytes object overhead + 24 bytes data) × 1,000,000) = 72 MB). For the same array using a struct type, the size is different; it’s only 24 MB (8 bytes overhead for the array) + (24 bytes data) × 1,000,000) = 24 MB). Interestingly, in a 64-bit machine, the size of the array using value types doesn’t change; but for the array using reference types, the size increases to more than 40 MB for the extra pointer byte overhead.\nThe optimization of converting the \nComplex  object from reference to value type is sim-\nple, requiring only that you change the keyword class  to struct  as shown next. (The \nfull implementation of the Complex  object is intentionally omitted.) The struct  key-\nword converts a reference type (class) to a value type:\nclass Complex  {    public Complex(float real,              float imaginary){        this.Real = real;        this.Imaginary =         imaginary;}struct Complex {     public Complex(float real,              float imaginary){        this.Real = real;        this.Imaginary =         imaginary;}\nAfter this simple code change, the execution time to draw the Mandelbrot algorithm has increased the speed approximately 20%, as shown in figure 4.6. \n \n 109 The Fork/Join pattern: parallel Mandelbrot\nFigure 4.6  The Parallel.For  construct benchmark comparison of the Mandelbrot algorithm computed \nin a quad-core machine with 8 GB of RAM. The sequential code runs in 9.009 seconds, compared to the parallel version, which runs in 3.131 seconds—almost three times faster. In the right column, the better performance is achieved by the parallel version of the code that uses the value type as a complex number in place of the reference type. This code runs in 2.548 seconds, 20% faster than the original parallel code, because there are no GC generations involved during its execution to slow the process.\nThe real improvement is the number of GC generations  to free memory, which is reduced to zero using the struct type instead of the class reference type.\n2 Table 4.1 \nshows GC generation comparison between a Parallel.For  loop using many reference \ntypes (class) and a Parallel.For  loop using many value types (struct).\nTable 4.1  GC generations comparison\nOperation GC gen0 GC gen1 GC gen2\nParallel.For 1390 1 1\nParallel.For with struct value type 0 0 0\nThe version of the code that runs using the Complex  object as a reference type makes \nmany short-lived allocations to the heap: more than 4 million.3 A short-lived object is \nstored in the first GC generation, and it’s scheduled to be removed from the memory sooner than generations 1 and 2. This high rate of allocation forces the GC to run, which involves stopping all the threads that are running, except the threads needed for the GC. The interrupted tasks resume only after the GC operation completes. Clearly, the smaller the number of the GC generations, the faster the application performs.\n2 “Fundamentals of Garbage Collection,” http:/ /mng.bz/v998.\n3 “Dynamic Memory Allocation,” http:/ /mng.bz/w8kA.",8093
42-4.2.2 The downside of parallel loops.pdf,42-4.2.2 The downside of parallel loops,,0
43-4.3.2 Gustafsons Law a step further to measure performance improvement.pdf,43-4.3.2 Gustafsons Law a step further to measure performance improvement,"110 chapter  4 The basics of processing big data: data parallelism, part 1\n4.2.2 The downside of parallel loops\nIn the previous section, you ran both the sequential and parallel versions of the Man-delbrot algorithm to compare performance. The parallel code was implemented using the TPL \nParallel  class and a Parallel.For  construct, which can provide significant \nperformance improvements over ordinary sequential loops. \nIn general, the parallel for loop pattern is useful to perform an operation that \ncan be executed independently for each element of a collection (where the ele-ments don’t rely on each other). For example, mutable arrays fit perfectly in parallel loops because every element is located in a different location in memory, and the update can be effected in place without race conditions. The work of parallelizing the loop introduces complexity that can lead to problems that aren’t common or even encountered in sequential code. For example, in sequential code, it’s common to have a variable that plays the role of accumulator to read from or write to. If you try to parallelize a loop that uses an accumulator, you have a high probability of encountering a race condition problem because multiple threads are concurrently accessing the variables. \nIn a parallel \nfor loop, by default, the degree of parallelism depends on the number \nof available cores. The degree of parallelism refers to the number of iterations that can be run at the same time in a computer. In general, the higher the number of available cores, the faster the parallel \nfor loop executes. This is true until the point of diminish-\ning return that Amdahl’s Law (the speed of a parallel loop depends on the kind of work it does) predicts is reached. \n4.3 Measuring performance speed\nAchieving an increase in performance is without a doubt the main reason for writing parallel code. Speedup refers to the performance gained from executing a program in parallel on a multicore computer as compared to a single-core computer. \nA few different aspects should be considered when evaluating speedup. The com-\nmon way to gain speedup is by dividing the work between the available cores. In this way, when running one task per processor with n cores, the expectation is to run the program n times faster than the original program. This result is called linear speedup, \nwhich in the real world is improbable to reach due to overhead introduced by thread creation and coordination. This overhead is amplified in the case of parallelism, which involves the creation and partition of multiple threads. To measure the speedup of an application, the single-core benchmark is considered the baseline.\nThe formula to calculate the linear speedup of a sequential program ported into a \nparallel version is speedup = sequential time / parallel time. For example, assuming the exe-cution time of an application running in a single-core machine is 60 minutes, when the application runs on a two-core computer, the time decreases to 40 minutes. In this case, the speedup is 1.5 (60 / 40). \n \n 111 Measuring performance speed\nWhy didn’t the execution time drop to 30 minutes? Because parallelizing the appli-\ncation involves the introduction of some overhead, which prevents the linear speedup according to the number of cores. This overhead is due to the creation of new threads, which implicate contention, context switching, and thread scheduling. \nMeasuring performance and anticipating speedup is fundamental for the bench-\nmarking, designing, and implementing of parallel programs. For that reason, parallel-ism execution is an expensive luxury—it isn’t free but instead requires an investment of time in planning. Inherent overhead costs are related to the creation and coordination of threads. Sometimes, if the amount of work is too small, the overhead brought in parallelism can exceed the benefit and, therefore, overshadow the performance gain. Frequently, the scope and volume of a problem affect the code design and the time required to execute it. Sometimes, better performance is achievable by approaching the same problem with a different, more scalable solution.\nAnother tool to calculate whether the investment is worth the return is Amdahl’s \nLaw, a popular formula for calculating the speedup of a parallel program.\n4.3.1 Amdahl’s Law defines the limit of performance improvement\nAt this point, it’s clear that to increase the performance of your program and reduce the overall execution time of your code, it’s necessary to take advantage of parallel programming and the multicore resources available. Almost every program has a por -\ntion of the code that must run sequentially to coordinate parallel execution. As in the Mandelbrot example, rendering the image is a sequential process. Another common example is the Fork/Join pattern, which starts the execution of multiple threads in parallel and then waits for them to complete before continuing the flow. \nIn 1965, Gene Amdahl concluded that the presence of sequential code in a program \njeopardizes overall performance improvement. This concept counters the idea of lin-ear speedup. A linear speedup means that the time T (units of time) it takes to execute \na problem with p processors is T/p (the time it takes to execute a problem with one processor). This can be explained by the fact that programs cannot run entirely in par -\nallel; therefore, the increase of performance expected isn’t linear and is limited by the sequential (serial) code constraint. \nAmdahl’s Law says that, given a fixed data-set size, the maximum performance \nincrease of a program implemented using parallelism is limited by the time needed for the sequential portion of the program. According to Amdahl’s Law, no matter how many cores are involved in the parallel computation, the maximum speedup the pro-gram can achieve depends on the percent of time spent in sequential processing.\nAmdahl’s Law determines the speedup of a parallel program by using three variables:\n¡ Baseline duration of the program executed in a single-core computer\n¡ The number of available cores\n¡ The percentage of parallel code",6176
44-4.3.3 The limitations of parallel loops the sum of prime numbers.pdf,44-4.3.3 The limitations of parallel loops the sum of prime numbers,"112 chapter  4 The basics of processing big data: data parallelism, part 1\nHere’s the formula to calculate the speedup according with Amdahl’s Law:\nSpeedup = 1 / (1 – P + (P / N))\nThe numerator of the equation is always 1 because it represents the base duration. In the denominator, the variable N is the number of available cores, and P represents the percentage of parallel code. \nFor example, if the parallelizable code is 70% in a quad-core machine, the maximum \nexpected speedup is 2.13:    \nSpeedup = 1 / (1 – .70 + (.70 / 4)) = 1 / (.30 + .17) = 1 / 0.47 = 2.13 times\nA few conditions may discredit the result of this formula. For the one related to data parallelism, with the onset of big data, the portion of the code that runs in parallel for processing data analysis has more effect on performance as a whole. A more precise formula to calculate performance improvement due to parallelism is Gustafson’s Law. \n4.3.2 Gustafson’s Law: a step further to measure performance improvement\nGustafson’s Law is considered the evolution of Amdahl’s Law and examines the speedup gain by a different and more contemporary perspective—considering the increased number of cores available and the increasing volume of data to process. \nGustafson’s Law considers the variables that are missing in Amdahl’s Law for the \nperformance improvement calculation, making this formula more realistic for modern scenarios, such as the increase of parallel processing due to multicore hardware.\nThe amount of data to process is growing exponentially each year, thereby influenc-\ning software development toward parallelism, distributed systems, and cloud comput-ing. Today, this is an important factor that invalidates Amdahl’s Law and legitimizes Gustafson’s Law. \nHere’s the formula for calculating the speedup according to Gustafson’s Law:Speedup = S + (N × P)\nS represents the sequential units of work, P defines the number of units of work that can be executed in parallel, and N is the number of available cores.\nA final explanation: Amdahl’s Law predicts the speedup achievable by parallelizing \nsequential code, but Gustafson’s Law calculates the speedup reachable from an existing parallel program.\n4.3.3 The limitations of parallel loops: the sum of prime numbers\nThis section covers some of the limitations resulting from the sequential seman-tics of the parallel loop and techniques to overcome these disadvantages. Let’s first consider a simple example that parallelizes the sum of the prime numbers in a col-lection. Listing 4.4 calculates the sum of the prime numbers of a collection with 1 million items. This calculation is a perfect candidate for parallelism because each iteration performs the same operation exactly. The implementation of the code skips the sequential version, whose execution time to perform the calculation runs \n \n 113 Measuring performance speed\nin 6.551 seconds. This value will be used as a baseline to compare the speed with the parallel version of the code.\nListing 4.4  Parallel sum of prime numbers using a Parallel.For  loop construct\nint len = 10000000;long total = 0;       Func<int, bool> isPrime = n => {              if (n == 1) return false;     if (n == 2) return true;     var boundary = (int)Math.Floor(Math.Sqrt(n));     for (int i = 2; i <= boundary; ++i)              if (n % i == 0) return false;     return true;};Parallel.For(0, len, i => {                     if (isPrime(i))                          total += i;    });\nThe function isPrime  is a simple implementation used to verify whether a given num-\nber is prime. The for loop uses the total  variable as the accumulator to sum all the \nprime numbers in the collection. The execution time to run the code is 1.049 seconds in a quad-core computer. The speed of the parallel code is six times faster as compared with the sequential code. Perfect! But, not so fast. \nIf you run the code again, you’ll get a different value for the \ntotal  accumulator. \nThe code isn’t deterministic, so every time the code runs, the output will be different, because the accumulator variable \ntotal  is shared among different threads. \nOne easy solution is to use a lock to synchronize the access of the threads to the total  \nvariable, but the cost of synchronization in this solution hurts performance. A better solution is to use a \nThreadLocal<T>  variable to store the thread’s local state during \nloop execution. Fortunately, Parallel.For  offers an overload that provides a built-in \nconstruct for instantiating a thread-local object. Each thread has its own instance of \nThreadLocal , removing any opportunity for negative sharing of state. The ThreadLo -\ncal<T>  type is part of the System.Threading  namespace as shown in bold here. \nListing 4.5  Using Parallel.For  with ThreadLocal  variables\nParallel.For(0, len,     () => 0,         (int i, ParallelLoopState loopState, long tlsValue) => {              return isPrime(i) ? tlsValue += i : tlsValue;     },     value => Interlocked.Add(ref total, value));    The total variable is used as the accumulator.\nThe function isPrime determines if a number is a prime.\nThe Parallel.For loop construct uses an anonymous lambda to access the current counter.\nIf the counter i is a prime number, it’s added to the accumulator total.\nThe total variable is used as an accumulator. The function isPrime determines \nif a number is a prime.\nSeed initialization functions to create a defensive copy of the tlsValue variable by \neach thread; each thread will access its own copy using a ThreadLocal variable.",5584
45-4.3.5 The declarative parallel programming model.pdf,45-4.3.5 The declarative parallel programming model,"114 chapter  4 The basics of processing big data: data parallelism, part 1\nThe code still uses a global mutual variable total , but in a different way. In this version \nof the code, the third parameter of the Parallel.For  loop initializes a local state whose \nlifetime corresponds to the first iteration on the current thread through the last one. In this way, each thread uses a thread-local variable to operate against an isolated copy of state, which can be stored and retrieved separately in a thread-safe manner. \nWhen a piece of data is stored in a managed thread-local storage (TLS), as in the exam-\nple, it’s unique to a thread. In this case, the thread is called the owner of the data. The \npurpose of using thread-local data storage is to avoid the overhead due to lock synchro-nizations accessing a shared state. In the example, a copy of the local variable \ntlsValue  \nis assigned and used by each thread to calculate the sum of a given range of the col-lection that has been partitioned by the parallel partitioner algorithm. The parallel partitioner uses a sophisticated algorithm to decide the best approach to divide and distribute the chunks of the collection between threads. \nAfter a thread completes all of the iterations, the last parameter in the \nParallel  \n.For  loop that defines the join  operation is called. Then, during the join  operation, \nthe results from each thread are aggregated. This step uses the Interlocked  class for \nhigh performance and thread-safe operation of addition operations. This class was introduced in chapter 3 to perform CAS operations to safely mutate (actually swap) the value of an object in multithreaded environments. The \nInterlock  class provides other \nuseful operations, such as increment, decrement, and exchange of variables.\nThis section has mentioned an important term in data parallelism: aggregate. The \naggregate concept will be covered in chapter 5.\nListing 4.5, the final version of the code, produces a deterministic result with a speed \nof execution of 1.178 seconds : almost equivalent to the previous one. You pay a little \nextra overhead in exchange for correctness. When using shared state in a parallel loop, scalability is often lost because of synchronization on the shared state access.\n4.3.4 What can possibly go wrong with a simple loop?\nNow we consider a simple code block that sums the integers from a given array. Using any OOP language, you could write something like this.\nListing 4.6  Common for loop \nint sum = 0;for (int i = 0; i < data.Length; i++){    sum += data[i]; }\nYou’ve written something similar in your career as a programmer; likely, a few years ago, when programs were executed single-threaded. Back then this code was fine, but these days, you’re dealing with different scenarios and with complex systems and pro-grams that simultaneously perform multiple tasks. With these challenges, the previous code can have a subtle bug, in the \nsum line of code:\nsum += data[i];The sum variable is used as an accumulator whose value updates each iteration.\n \n 115 Measuring performance speed\nWhat happens if the values of the array are mutated while it’s traversed? In a multi-threaded program, this code presents the issue of mutability, and it cannot guarantee consistency. \nNote that not all state mutation is equally evil, if the mutation of state that’s only visi-\nble within the scope of a function is inelegant, but harmless. For example, suppose the previous sum in a \nfor loop is isolated in a function as follows:\nint Sum(int[] data){       int sum = 0;    for (int i = 0; i < data.Length; i++)    {        sum += data[i];     }}\nDespite updating the sum value, its mutation isn’t visible from outside the scope of the \nfunction. As a result, this implementation of sum can be considered a pure function. \nTo reduce complexity and errors in your program, you must raise the level of abstrac-\ntion in the code. For example, to compute a sum of numeric values, express your inten-tion in “what you want,” without repeating “how to do.” Common functionality should be part of the language, so you can express your intentions as \nint sum = data.Sum();\nIndeed, the Sum extension method (http:/ /mng.bz/f3nF) is part of the System \n.Linq  namespace in .NET. In this namespace, many methods, such as List  and Array , \nextend the functionality for any IEnumerable  object (http:/ /mng.bz/2bBv). It’s not \na coincidence that the ideas behind LINQ originate from functional concepts. The LINQ namespace promotes immutability, and it operates on the concept of transfor -\nmation instead of mutation, where a LINQ query (and lambda) let you transform a set of structured data from its original form into another form, without worrying about side effects or state. \n4.3.5 The declarative parallel programming model\nIn the sum of prime numbers example in listing 4.5, the Parallel.For  loop construc-\ntor definitely fits the purpose of speedup compared to the sequential code and does it efficiently, although the implementation is a bit more difficult to understand and maintain compared to the sequential version. The final code isn’t immediately clear to a developer looking at it for the first time. Ultimately, the intent of the code is to sum the prime numbers of a collection. It would be nice to have the ability to express the intentions of the program, defining step by step how to implement the algorithm.\nThis is where PLINQ comes into play. The following listing is the equivalent of the \nparallel \nSum using PLINQ (in bold) in place of the Parallel.For  loop. \nListing 4.7  Parallel sum of a collection using declarative PLINQ\n  long total = 0;  Parallel.For(0, len,    \nParallel sum using the Parallel.For construct\n \n116 chapter  4 The basics of processing big data: data parallelism, part 1\n       () => 0,           (int i, ParallelLoopState loopState, long tlsValue) => {                return isPrime(i) ? tlsValue += i : tlsValue;},value => Interlocked.Add(ref total, value));    long total = Enumerable.Range(0, len).AsParallel()                            .Where(isPrime).Sum(x => (long)x); \nThe functional declarative approach is only one line of code. Clearly, when compared to the \nfor loop implementation, it’s simple to understand, succinct, maintainable, and \nwithout any mutation of state. The PLINQ construct represents the code as a chain of functions, each one providing a small piece of functionality to accomplish the task. The solution adopts the higher-order-function aggregate part of the LINQ/PLINQ API, which in this case is the function \nSum() . The aggregate applies a function to each \nsuccessive element of a collection, providing the aggregated result of all previous ele-ments. Other common aggregate functions are \nAverage() , Max() , Min() , and Count() . \nFigure 4.7 shows benchmarks comparing the execution time of the parallel Sum.\nFigure 4.7  Benchmarking comparison of the sum of prime numbers. The benchmark runs in an eight-\ncore machine with 8 GB of RAM. The sequential version runs in 8.072 seconds. This value is used as a base for the other versions of the code. The \nParallel.For  version took 1.814 seconds, which is \napproximately 4.5 times faster than the sequential code. The Parallel.For ThreadLocal  version \nis a little faster than the parallel Loop . Ultimately, the PLINQ program is slowest among the parallel \nversions; it took 1.824 seconds to run.Parallel sum using PLINQSum of the values, casting the results into a \nlong type to avoid overflow exception \n \n 117 Summary\nAggregating values to avoid an arithmetic overflow exception \nThe previous PLINQ query isn’t optimized. Shortly, you’ll learn techniques to make this code more performant. Moreover, the size of the sequence to sum was reduced to 10,000 instead of the 1 million used earlier, because the \nSum()  function in PLINQ \nis compiled to execute in a checked block, which throws an arithmetic overflow excep -\ntion. The solution is to convert the base number from an integer-32  to integer-64 \n(long) , or to use the Aggregate  function instead, in this form: \nEnumerable.Range(0, len).AsParallel()           .Aggregate((acc,i) => isPrime(i) ? acc += i : acc); \n \nThe function Aggregate  will be covered in detail in chapter 5. \nSummary\n¡ Data parallelism aims to process massive amounts of data by partitioning and performing each chunk separately, then regrouping the results when completed. This lets you analyze the chunks in parallel, gaining speed and performance.\n¡ Mental models used in this chapter, which apply to data parallelism, are Fork/Join, Parallel Data Reduction, and Parallel Aggregation. These design patterns share a common approach that separates the data and runs the same task in par -\nallel on each divided portion.\n¡ Utilizing functional programming constructs, it’s possible to write sophisticated code to process and analyze data in a declarative and simple manner. This para-digm lets you achieve parallelism with little change in your code.\n¡ Profiling the program is a way to understand and ensure that the changes you make to adopt parallelism in your code are beneficial. To do that, measure the speed of the program running sequentially, then use a benchmark as a baseline to compare the code changes.",9351
46-5.1.2 PLINQ and pure functions the parallel word counter.pdf,46-5.1.2 PLINQ and pure functions the parallel word counter,"1185PLINQ and MapReduce: \ndata parallelism, part 2 \nThis chapter covers\n¡ Using declarative programming semantics \n¡ Isolating and controlling side effects\n¡ Implementing and using a parallel Reduce  \nfunction\n¡ Maximizing hardware resource utilization\n¡ Implementing a reusable parallel MapReduce pattern\nThis chapter presents MapReduce, one of the most widely used functional program-ming patterns in software engineering. Before delving into MapReduce, we’ll ana-lyze the declarative programming style that the functional paradigm emphasizes and enforces, using PLINQ and the idiomatic F#, \nPSeq . Both technologies analyze a \nquery statement at runtime and make the best strategy decision concerning how to execute the query in accordance with available system resources. Consequently, the more CPU power added to the computer, the faster your code will run. Using these strategies, you can develop code ready for next-generation computers. Next, you’ll \n \n 119 A short introduction to PLINQ\n5learn how to implement a parallel Reduce  function in .NET, which you can reuse in \nyour daily work to increase the speed of execution of aggregates functions. \nUsing FP, you can engage data parallelism in your programs without introducing \ncomplexity, compared to conventional programming. FP prefers declarative over pro-cedural semantics to express the intent of a program instead of describing the steps to achieve the task. This declarative programming style simplifies the adoption of parallel-ism in your code. \n5.1 A short introduction to PLINQ\nBefore we delve into PLINQ, we’ll define its sequential double, LINQ, an extension to the .NET Framework that provides a declarative programming style by raising the level of abstraction and simplifying the application into a rich set of operations to transform any object that implements the \nIEnumerable interface. The most common operations \nare mapping, sorting, and filtering. LINQ operators accept behavior as the parameter that usually can be passed in the form of lambda expressions. In this case, the lambda expression provided will be applied to each single item of the sequence. With the introduction of LINQ and lambda expressions, FP becomes a reality in .NET. \nYou can make queries run in parallel using all the cores of the development system to \nconvert LINQ to PLINQ by adding the extension \n.AsParallel()  to the query. PLINQ \ncan be defined as a concurrency engine for executing LINQ queries. The objective of parallel programming is to maximize processor utilization with increased throughput in a multicore architecture. For a multicore computer, your application should recog-nize and scale performance to the number of available processor cores. \nThe best way to write parallel applications isn’t to think about parallelism, and \nPLINQ fits this abstraction perfectly because it takes care of all the underlying require-ments, such as partitioning the sequences into smaller chunks to run individually and applying the logic to each item of each subsequence. Does that sound familiar? That’s because PLINQ implements the Fork/Join model underneath, as shown in figure 5.1. \nA B 1Task 1\n2\nC D 3Task 2\n4 3 5 6 1 4 2 A C\n.AsParallel()D E F B\nE F 5\nTask 36\nFigure 5.1  A PLINQ execution model. Converting a LINQ query to PLINQ is as simple as applying the \nAsParallel()  extension method, which runs in parallel the execution using a Fork/Join pattern. In \nthis figure, the input characters are transformed in parallel into numbers. Notice that the order of the input elements isn’t preserved.\n \n120 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nAs a rule of thumb, every time there is a for or for-each  loop in your code that does \nsomething with a collection, without performing a side effect outside the loop, con-sider transforming the loop into a LINQ. Then benchmark the execution and evaluate whether the query could be a fit to run in parallel using PLINQ.\nNOTE  This is a book about concurrency, so from now on it will mention only \nPLINQ, but for the majority of the cases, the same constructs, principles, and higher-order functions used to define a query also apply to LINQ.\nThe advantage of using PLINQ, compared to a parallel \nfor loop, is its capability of \nhandling automatically aggregation of temporary processing results within each run-ning thread that executes the query.\n5.1.1 How is PLINQ more functional?\nPLINQ is considered an ideal functional library, but why? Why consider the PLINQ version of code more functional than the original \nParallel.For  loop? \nWith Parallel.For , you’re telling the computer what to do: \n¡ Loop through the collection.\n¡ Verify if the number is prime. \n¡ If the number is prime, add it to a local accumulator.\n¡ When all iterations are done, add the accumulator to a shared value.\nBy using LINQ/PLINQ, you can tell the computer what you want in the form of a sen-tence: “Given a range from 0 to 1,000,000, where the number is prime, sum them all.”\nFP emphasizes writing declarative code over imperative code. Declarative code \nfocuses on what you want to achieve rather than how to achieve it. PLINQ tends to emphasize the intent of the code rather than the mechanism and is, therefore, much more functional.\nNOTE  In section 13.9, you’ll use a Parallel.ForEach  loop to build a high- \nperformance and reusable operator that combines the functions filter  \nand map. In this case, because the implementation details of the function are \nabstracted away and hidden from the eyes of the developer, the function par -\nallel FilterMap  becomes a higher-order operator that satisfies the declarative \nprogramming concept. \nIn addition, FP favors the use of functions to raise the level of abstraction, which aims to hide complexity. In this regard, PLINQ raises the concurrency programming model abstraction by handling the query expression and analyzing the structure to decide how to run in parallel, which maximizes performance speed.\nFP also encourages combining small and simple functions to solve complex prob-\nlems. The PLINQ pipeline fully satisfies this tenet with the approach of chaining pieces of extension methods together.\nAnother functional aspect of PLINQ is the absence of mutation. The PLINQ opera-\ntors don’t mutate the original sequence, but instead return a new sequence as a result \n \n 121 A short introduction to PLINQ\nof the transformation. Consequently, the PLINQ functional implementation gives you predictable results, even when the tasks are executed in parallel. \n5.1.2 PLINQ and pure functions: the parallel word counter\nNow let’s consider an example where a program loads a series of text files from a given folder and then parses each document to provide the list of the 10 most frequently used words. The process flow is the following (shown in figure 5.2):\n1 Collect the files from a given folder path.\n2 Iterate the files.\n3 For each text file, read the content. \n4 For each line, break it down into words.\n5 Transform each word into uppercase, which is useful for comparison.\n6 Group the collection by word. \n7 Order by higher count. \n8 Take the first 10 results.\n9 Project the result into tabular format (a dictionary).\nRead files Read text\nSplit content\nTransform wordsGroup by word\nProject result:\n10 most-used wordsFiles\nFigure 5.2  Representation of the flow process to count the times each word has been mentioned. First, \nthe files are read from a given folder, then each text file is read, and the content is split in lines and single words to be grouped by. \nThe following listing defines this functionality in the WordsCounter  method, which \ntakes as input the path of a folder and then calculates how many times each word has been used in all files. This listing shows the \nAsParallel  command in bold.\nListing 5.1  Parallel word-counting program with side effects\npublic static Dictionary<string, int> WordsCounter(string source){    var wordsCount =            (from filePath in                Directory.GetFiles(source, ""*.txt"")                             .AsParallel()                 from line in File.ReadLines(filePath)                from word in line.Split(' ')                select word.ToUpper())          .GroupBy(w => w)\nThe side effect of reading from the filesystem\nParallelizes the sequence of files\n \n122 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \n        .OrderByDescending(v => v.Count()).Take(10);     return wordsCount.ToDictionary(k => k.Key, v => v.Count());}\nThe logic of the program follows the previously defined flow step by step. It’s declara-tive, readable, and runs in parallel, but there’s a hidden problem. It has a side effect. The method reads files from the filesystem, generating an I/O side effect. As men-tioned previously, a function or expression is said to have a side effect if it modifies a state outside its scope or if its output doesn’t depend solely on its input. In this case, passing the same input to a function with side effects doesn’t guarantee to always pro-duce the same output. These types of functions are problematic in concurrent code because a side effect implies a form of mutation. Examples of impure functions are getting a random number, getting the current system time, reading data from a file or a network, printing something to a console, and so forth. To understand better why reading data from a file is a side effect, consider that the content of the file could change any time, and whenever the content of the file changes, it can return some-thing different. Furthermore, reading a file could also yield an error if in the mean-time it was deleted. The point is to expect that the function can return something different every time it’s called.\nDue to the presence of side effects, there are complexities to consider:\n¡ Is it really safe to run this code in parallel? \n¡ Is the result deterministic?\n¡ How can you test this method? \nA function that takes a filesystem path may throw an error if the directory doesn’t exist or if the running program doesn’t have the required permissions to read from the directory. Another point to consider is that with a function run in parallel using PLINQ, the query execution is deferred until its materialization. Materialization is the \nterm used to specify when a query is executed and produces a result. For this reason, successive materialization of a PLINQ query that contains side effects might generate different results due to the underlying data that might have changed. The result isn’t deterministic. This could happen if a file is deleted from the directory between differ -\nent calls, and then throws an exception. \nMoreover, functions with side effects (also called impure) are hard to test. One possi-\nble solution is to create a testing directory with a few text files that cannot change. This approach requires that you know how many words are in these files, and how many times they have been used to verify the correctness of the function. Another solution is to mock the directory and the data contained, which can be even more complex than the previous solution. A better approach exists: remove the side effects and raise the level of abstraction, simplifying the code while decoupling it from external dependencies. \nBut what are side effects? What’s a pure function, and why should you care?Orders the words mentioned by count and takes the first 10 values",11498
47-5.2 Aggregating and reducing data in parallel.pdf,47-5.2 Aggregating and reducing data in parallel,"123 A short introduction to PLINQ\n5.1.3 Avoiding side effects with pure functions\nOne principle of functional programming is purity. Pure functions are those without side effects, where the result is independent of state, which can change with time. That is, pure functions always return the same value when given the same inputs. This listing shows pure pure functions in C#.\nListing 5.2  Pure functions in C#\npublic static string AreaCircle(int radius) =>                 Math.Pow(radius, 2) * Math.PI;      public static int Add(int x, int y) => x + y;       \nThe listing is an example of side effects that are functions that mutate state, setting values of global variables. Because variables live in the block where they’re declared, a variable that’s defined globally introduces possible collision and affects the readability and maintainability of the program. This requires extra checking of the current value of the variable at any point and each time it’s called. The main problem of dealing with side effects is that they make your program unpredictable and problematic in concur -\nrent code, because a side effect implies a form of mutation. \nImagine passing the same argument to a function and each time obtaining a differ -\nent outcome. A function is said to have side effects if it does any of the following:\n¡ Performs any I/O operation (this includes reading/writing to the filesystem, to a database, or to the console)\n¡ Mutates global state and any state outside of the function’s scope\n¡ Throws exceptions \nAt first, removing side effects from a program can seem extremely limiting, but there are numerous benefits to writing code in this style:\n¡ Easy to reason about the correctness of your program. \n¡ Easy to compose functions for creating new behavior. \n¡ Easy to isolate and, therefore, easy to test, and less bug-prone.\n¡ Easy to execute in parallel. Because pure functions don’t have external depen-dencies, their order of execution (evaluation) doesn’t matter.\nAs you can see, introducing pure functions as part of your toolset immediately benefits your code. Moreover, the result of pure functions depends precisely on their input, which introduces the property of referential transparency.\nReferential transparency \nReferential transparency has a fundamental implication for side-effect-free functions; \ntherefore, it’s a desirable property because it represents the capability to replace a function call for a defined set of parameters with the value it returns without chang-ing the meaning of the program. Using referential transparency, you can exchange the The output never changes because the functions have no side effects.\n \n124 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nexpression for its value, and nothing changes. The concept is about representing the result of any pure function directly with the result of the evaluation. The order of evalua -\ntion isn’t important, and evaluating a function multiple times always leads to the same result— execution can be easily parallelized.\nMathematics is always referentially transparent. Given a function and an input value, the function maps always have the same output with the same input. For example, any func -\ntion f(x) = y is a pure function if for the same value x you end up getting the same result y \nwithout any internal or external state change.\n \nA program inevitably needs side effects to do something useful, of course, and func-tional programming doesn’t prohibit side effects, but rather encourages minimizing and isolating them.\n5.1.4 Isolate and control side effects: refactoring the parallel word counter\nLet’s re-evaluate listing 5.1, the WordsCounter  example. How can you isolate and con-\ntrol side effects in this code? \nstatic Dictionary<string, int> WordsCounter(string source) {           var wordsCount = (from filePath in                   Directory.GetFiles(source, ""*.txt"")                              .AsParallel()               from linein File.ReadLines(filePath)                from word in line.Split(' ')                select word.ToUpper())            .GroupBy(w => w)            .OrderByDescending(v => v.Count()).Take(10);                        return wordsCount.ToDictionary(k => k.Key, v => v.Count());}\nThe function can be split into a pure function at the core and a pair of functions with side effects. The I/O side effect cannot be avoided, but it can be separated from the pure logic. In this listing, the logic to count each word mentioned per file is extracted, and the side effects are isolated. \nListing 5.3  Decoupling and isolating side effects\nstatic Dictionary<string, int> PureWordsPartitioner                             (IEnumerable<IEnumerable<string>> content) =>    (from lines in content.AsParallel()         from line in lines        from word in line.Split(' ')        select word.ToUpper())        .GroupBy(w => w)        The bold code shows the \nside effect of reading \nfrom the filesystem.\nThe pure function without side effects; in this \ncase, the I/O operation is removed.\nThe result from the side-effect-free function can be parallelized without issues. (continued)\n \n 125 Aggregating and reducing data in parallel \n        .OrderByDescending(v => v.Count()).Take(10)        .ToDictionary(k => k.Key, v => v.Count());    static Dictionary<string, int> WordsPartitioner(string source){    var contentFiles =        (from filePath in Directory.GetFiles(source, ""*.txt"")            let lines = File.ReadLines(filePath)            select lines);    return PureWordsPartitioner(contentFiles);  }\nThe new function PureWordsPartitioner  is pure, where the result depends only on the \ninput argument. This function is side effect free and easy to prove correct. Conversely, the method \nWordsPartitioner  is responsible for reading a text file from the filesystem, \nwhich is a side effect operation, and then aggregating the results from the analysis. \nAs you can see from the example, separating the pure from the impure parts of \nyour code not only facilitates testing and optimization of the pure parts, but will also make you more aware of the side effects of your program and help you avoid mak-ing the impure parts bigger than they need to be. Designing with pure functions and decoupling side effects from pure logic are the two basic tenets that functional thinking brings to the forefront.\n5.2 Aggregating and reducing data in parallel \nIn FP, a fold, also known as reduce and accumulate, is a higher-order function that reduces \na given data structure, typically a sequence of elements, into a single value. Reduction, for example, could return an average value for a series of numbers, calculating a sum-mation, maximum value, or minimum value. \nThe \nfold  function takes an initial value, commonly called the accumulator, which \nis used as a container for intermediate results. As a second argument it takes a binary expression that acts as a reduction function to apply against each element in the sequence to return the new value for the accumulator. In general, in reduction you take a binary operator—that is, a function with two arguments—and compute it over a vector or set of elements of size n , usually from left to right. Sometimes, a special seed \nvalue is used for the first operation with the first element, because there’s no previous value to use. During each step of the iteration, the binary expression takes the current element from the sequence and the accumulator value as inputs, and returns a value that overwrites the accumulator. The final result is the value of the last accumulator, as shown in figure 5.3.The code to merge the  results into one dictionary, avoiding duplicatesCalls the side-effect-free function from an impure function\n \n126 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nSequence { 5, 7, 9 }\nItem 5 function (*) Accumulator 1 f(accumulator 1, item 5) = 1 * 5\nItem 7 function (*) Accumulator 5Accumulator 1\nf(accumulator 5, item 7) = 5 * 7\nItem 9 function (*) f(accumulator 35, item 9) = 35 * 9 Accumulator 35\nAggregate\nresult\n315\nFigure 5.3  The fold  function reduces a sequence to a single value. The function (f), in this case, is \nmultiplication and takes an initial accumulator with a value of 1. For each iteration in the sequence (5, \n7, 9), the function applies the calculation to the current item and accumulator. The result is then used to update the accumulator with the new value.\nThe fold  function has two forms, right-fold and left-fold, depending on where the first \nitem of the sequence to process is located. The right-fold starts from the first item in the list and iterates forward; the left-fold starts from the last item in the list and iter -\nates backward. This section covers the right-fold because it’s most often used. For the remainder of the section, the term fold will be used in place of right-fold. \nNOTE  Consider several performance implications when choosing between the \nright-fold and left-fold functions. In the case of folding over a list, the right-fold occurs in O(1) because it adds an item to the front of a list that is constant time. The left-fold requires O(n ), because it has to run through the whole list to add an \nitem. The left-fold cannot be used to handle or generate infinite lists, because the size of the list should be known to start folding backward from the last item.\nThe \nfold  function is particularly useful and interesting: it’s possible to express a variety \nof operations in terms of aggregation, such as filter , map, and sum. The fold  function \nis probably the most difficult to learn among the other functions in list comprehen-sion, but one of the most powerful.\nIf you haven’t read it yet, I recommend “Why Functional Programming Matters,” by \nJohn Hughes (www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf).\n This paper \ngoes into detail about the high applicability and importance of the fold  function in FP. \nThis listing uses F# and fold  to demonstrate the implementation of a few useful functions.\nListing 5.4  Implementing max and map using the F# fold  function\nlet map (projection:'a -> 'b) (sequence:seq<'a>) =       sequence |> Seq.fold(fun acc item -> (projection item)::acc) []\nThe map function using fold in F#",10323
48-5.2.1 Deforesting one of many advantages to folding.pdf,48-5.2.1 Deforesting one of many advantages to folding,"127 Aggregating and reducing data in parallel \nlet max (sequence:seq<int>) =        sequence |> Seq.fold(fun acc item -> max item acc) 0let filter (predicate:'a -> bool) (sequence:seq<'a>) =       sequence |> Seq.fold(fun acc item ->            if predicate item = true then item::acc else acc) [] let length (sequence:seq<'a>) =       sequence |> Seq.fold(fun acc item -> acc + 1) 0 \nThe equivalent of fold  in LINQ in C# is Aggregate . This listing uses the C# Aggregate  \nfunction to implement other useful functions.\nListing 5.5  Implementing Filter  and Length  using LINQ Aggregate  in C#\nIEnumerable<T> Map<T, R>(IEnumerable<T> sequence, Func<T, R> projection){      return sequence.Aggregate(new List<R>(), (acc, item) => {                       acc.Add(projection(item));                     return acc;     });}int Max(IEnumerable<int> sequence) {      return sequence.Aggregate(0, (acc, item) => Math.Max(item, acc));}IEnumerable<T> Filter<T>(IEnumerable<T> sequence, Func<T, bool> predicate){       return sequence.Aggregate(new List<T>(), (acc, item) => {               if (predicate(item))                 acc.Add(item);                 return acc;      });}int Length<T>(IEnumerable<T> sequence) {         return sequence.Aggregate(0, (acc, _) => acc + 1);}\nBecause of the inclusion of .NET list-comprehension support for parallelism, includ-ing the LINQ \nAggregate  and Seq.fold  operators, the implementation of these func-\ntions in C# and F# can be easily converted to run concurrently. More details about this conversion are discussed in the next sections.\n5.2.1 Deforesting: one of many advantages to folding\nReusability and maintainability are a few advantages that the fold  function provides. \nBut one special feature that this function permits is worth special mention. The fold  \nfunction can be used to increase the performance of a list-comprehension query. List comprehension is a construct, similar to LINQ/PLINQ in C#, to facilitate list-based que-ries on existing lists (https:/ /en.wikipedia.org/wiki/List_comprehension).The max function using fold in F#  \nThe filter function using fold in F#  \nThe length function to calculate the \nlength of a collection using fold in F#  \nThe Map function using \nLINQ Aggregate in C# \nThe Max function using LINQ Aggregate in C#  \nThe Filter function using LINQ \nAggregate in C#  \nThe Length function using LINQ Aggregate in C#  \n \n128 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nHow can the fold  function increase the performance speed of a list query regard-\nless of parallelism?  To answer, let’s analyze a simple PLINQ query. You saw that the use of functional constructs, like LINQ/PLINQ in .NET, transforms the original sequence avoiding mutation, which in strict-evaluated programming languages such as F# and C# often leads to the generation of intermediate data structures that are unnecessary. This listing shows a PLINQ query that filters and then transforms a sequence of num-bers to calculate the sum of the even values times two (doubled). The parallel execu-tion is in bold.\nListing 5.6  PLINQ query to sum the double of even numbers in parallel \nvar data = new int[100000];for(int i = 0; i < data.Length; i++)    data[i]=i;long total =    data.AsParallel()          .Where(n => n % 2 == 0)        .Select(n => n + n)        .Sum(x => (long)x);  \nIn these few lines of code, for each Where  and Select  of the PLINQ query, there’s a \ngeneration of intermediate sequences that unnecessarily increase memory allocation. In the case of large sequences to transform, the penalty paid to the GC to free mem-ory becomes increasingly higher, with negative consequences for performance. The allocation of objects in memory is expensive; consequently, optimization that avoids extra allocation is valuable for making functional programs run faster. Fortunately, the creation of these unnecessary data structures can often be avoided. The elimination of intermediate data structures to reduce the size of temporary memory allocation is referred to as deforesting. This technique is easily exploitable with the higher-order function \nfold , which takes the name Aggregate  in LINQ. This function is capable of \neliminating intermediate data-structure allocations by combining multiple operations, such as \nfilter  and map,  in a single step, which would otherwise have an allocation for \neach operation. This code example shows a PLINQ query to sum the double of even numbers in parallel using the \nAggregate  operator:\nlong total = data.AsParallel().Aggregate(0L, (acc, n) =>                                n % 2 == 0 ? acc + (n + n) : acc);\nThe PLINQ function Aggregate  has several overloads; in this case, the first argument \n0 is the initial value of the accumulator acc, which is passed and updated each iter -\nation. The second argument is the function that performs an operation against the item from the sequence, and updates the value of the accumulator \nacc. The body of \nthis function merges the behaviors of the previously defined Where , Select , and Sum \nPLINQ extensions, producing the same result. The only difference is the execution time. The original code ran in 13 ms; the updated version of the code, deforesting the function, ran in 8 ms.Due to parallelism, the order in which the values are processed isn’t guaranteed; but the result is deterministic because the Sum operation is commutative.\nThe value is cast to long number Sum(x => (long)x) to avoid an overflow exception.",5529
49-5.2.2 Fold in PLINQ Aggregate functions.pdf,49-5.2.2 Fold in PLINQ Aggregate functions,"129 Aggregating and reducing data in parallel \nDeforesting is a productive optimization tool when used with eager data structures, \nsuch as lists and arrays; but lazy collections behave a little differently. Instead of gener -\nating intermediate data structures, lazy sequences store the function to be mapped and the original data structure. But you’ll still have better performance speed improvement compared to a function that isn’t deforested.\n5.2.2 Fold in PLINQ: Aggregate functions\nThe same concepts you learned about the fold  function can be applied to PLINQ in \nboth F# and C#. As mentioned earlier, PLINQ has the equivalent of the fold  function \ncalled Aggregate . The PLINQ Aggregate  is a right-fold. Here’s one of its overloaded \nsignatures:\npublic static TAccumulate Aggregate<TSource, TAccumulate>(      this IEnumerable<TSource> source,       TAccumulate seed,       Func<TAccumulate, TSource, TAccumulate> func);\nThe function takes three arguments that map to the sequence source: the sequence source to process, the initial accumulator seed, and the function \nfunc , which updates \nthe accumulator for each element.\nThe best way to understand how Aggregate  works is with an example. In the exam-\nple in the sidebar, you’ll parallelize the k-means clustering algorithm using PLINQ and the \nAggregate  function. The example shows how remarkably simple and performant a \nprogram becomes by using this construct. \nk-means clustering\nk-means, also called Lloyd’s algorithm, is an unsupervised machine-learning algorithm \nthat categorizes a set of data points into clusters, each centered on its own centroid. A centroid of a cluster is the sum of points in it divided by the number of total points. It rep -\nresents the center of the mass of a geometric shape having uniform density. The k-means clustering algorithm takes an input data and a value k  that indicates the \nnumber of clusters to set, and then places the centroids randomly in these clusters. This algorithm takes as a parameter the number of clusters to find and makes an initial guess at the center of each cluster. The idea is to generate a number of centroids that produce the centers of the clusters. Each point in the data is linked with its nearest centroid. The distance is calculated using a simple Euclidean distance function (https:/ /\nen.wikipedia.org/wiki/Euclidean_distance ). Each centroid is then moved to the average \nof the positions of the points that are associated with it. A centroid is computed by tak -\ning the sum of its points and then dividing the result by the size of the cluster. The itera-tion involves these steps:\n1 Sum, or reduction , computes the sum of the points in each cluster. \n2 Divide each cluster sum by the number of points in that cluster.\n3 Reassign, or map, the points of each cluster to the closest centroid. \n \n130 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \n4 Repeat the steps until the cluster locations stabilize. You cut off processing af-ter an arbitrarily chosen number of iterations, because sometimes the algorithm does not converge.\nThe process is iterative, meaning that it repeats until the algorithm reaches a final result or exceeds the maximum number of steps. When the algorithm runs, it corrects continu -\nously and updates the centroids in each iteration to better cluster the input data. \n \nFor the data source used as input in the k-means clustering algorithm, you’ll use the “white wine quality” public records (figure 5.4), available for download at http:/ /mng.bz/9mdt. \nFigure 5.4  The result of running the k-means algorithms using C# LINQ for the serial version of code \nand C# PLINQ for the parallelized version. The centroids are the large points in both clusters. Each image represents one iteration of the k-means algorithm, with 11 centroids in the cluster. Each iteration of the algorithm computes the centroid of each cluster and then assigns each point to the cluster with the closest centroid.\nThe full implementation of the k-means program is omitted because of the length of the code; only the relevant excerpt of the code is shown in listings 5.7 and 5.8. But the full code implementation, in both F# and C#, is available and downloadable in the source code for this book.\nLet’s review two core functions: \nGetNearestCentroid and UpdateCentroids . Get-\nNearestCentroid  is used to update the clusters, as shown in listing 5.7. For every data \ninput, this function finds the closest centroid assigned to the cluster to which the input belongs (in bold).(continued)\n \n 131 Aggregating and reducing data in parallel \nListing 5.7  Finding the closest centroid (updating the clusters)\ndouble[] GetNearestCentroid(double[][] centroids, double[] center){            return centroids.Aggregate((centroid1, centroid2) =>                      Dist(center, centroid2) < Dist(center, centroid1)                ? centroid2                : centroid1);        }\nThe GetNearestCentroid  implementation uses the Aggregate  function to compare \nthe distances between the centroids to find the nearest one. During this step, if the inputs in any of the clusters aren’t updated because a closer centroid is not found, then the algorithm is complete and returns the result. \nThe next step, shown in listing 5.8, after the clusters are updated, is to update the \ncentroid locations. \nUpdateCentroids  calculates the center for each cluster and shifts \nthe centroids to that point. Then, with the updated centroid values, the algorithm repeats the previous step, running \nGetNearestCentroid  until it finds the closest result. \nThese operations continue running until a convergence condition is met, and the posi-tions of the cluster centers become stable. The bold code highlights commands dis-cussed in more depth following the listing.\nThe following implementation of the k-means clustering algorithm uses FP, \nsequence expressions with PLINQ, and several of the many built-in functions for manipulating data. \nListing 5.8  Updating the location of the centroids\ndouble[][] UpdateCentroids(double[][] centroids){    var partitioner = Partitioner.Create(data, true);     var result = partitioner.AsParallel()         .WithExecutionMode(ParallelExecutionMode.ForceParallelism)         .GroupBy(u => GetNearestCentroid(centroids, u))        .Select(points =>            points            .Aggregate(               seed: new double[N],                func: (acc, item) =>                       acc.Zip(item, (a, b) => a + b).ToArray())             .Select(items => items / points.Count())            .ToArray());    return result.ToArray();}\nWith the UpdateCentroids  function, there’s a great deal of processing to compute, so \nthe use of PLINQ can effectively parallelize the code, thereby increasing the speed. \nNOTE  Even if centroids don’t move on the plane, they may change their \nindexes in the resulting array due to the nature of GroupBy  and AsParallel .Uses the Aggregate LINQ function \nto find the closest centroid\nUses a tailored partitioner for maximizing the performance\nRuns the query in parallel from the partitioner\nForces parallelism regardless of the shape of the query, bypassing the default PLINQ analysis that could decide to run part of the operation sequentiallyUses the Aggregate function to find the center of the centroids in the cluster; the seed initial value is a double array with size N (the dimensionality of data).\nUses the Zip function to thread \nthe centroid-locations and \naccumulator sequences\n \n132 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nThe PLINQ query in the body of UpdateCentroids  performs aggregation in two steps. \nThe first uses the GroupBy function, which takes as an argument a function that pro-\nvides the key used for the aggregation. In this case, the key is computed by the previ-ous function \nGetNearestCentroid . The second step, mapping, which runs the Select  \nfunction, calculates the centers of new clusters for each given point. This calculation is performed by the \nAggregate  function, which takes the list of points as inputs (the loca-\ntion coordinates of each centroid) and calculates their centers mapped to the same cluster using the local accumulator \nacc as shown in listing 5.8. \nThe accumulator is an array of doubles with size N, which is the dimensionality (the \nnumber of characteristics/measurements) of the data to process. The value N is defined as a constant in the parent class because it never changes and can be safely shared. The \nZip function threads together the nearest centroids (points) and the accumulator \nsequences. Then, the center of that cluster is recomputed by averaging the position of the points in the cluster. \nThe implementation details of the algorithm aren’t crucial; the key point is that the \ndescription of the algorithm is translated precisely and directly into PLINQ using \nAggre-\ngate . If you try to re-implement the same functionality without the Aggregate  function, \nthe program runs in ugly and hard-to-understand loops with mutable shared variables. \nThe following listing shows the equivalent of the UpdateCentroids  function without the \nhelp of the Aggregate  function. The bold code is discussed further following the listing.\nListing 5.9  UpdateCentroids  function implemented without Aggregate  \ndouble[][] UpdateCentroidsWithMutableState(double[][] centroids){    var result = data.AsParallel()        .GroupBy(u => GetNearestCentroid(centroids, u))        .Select(points => {            var res = new double[N];            foreach (var x in points)                 for (var i = 0; i < N; i++)                    res[i] += x[i];             var count = points.Count();            for (var i = 0; i < N; i++)                res[i] /= count;             return res;        });    return result.ToArray();}\nFigure 5.5 shows benchmark results of running the k-means clustering algorithm. The benchmark was executed in a quad-core machine with 8 GB of RAM. The algorithms tested are the sequential LINQ, the parallel PLINQ, and the parallel PLINQ using a custom partitioner. \nNOTE  When multiple threads are used on a multiprocessor, more than one \nCPU may be used to complete a task. In this case, the CPU time may be more than the elapsed time.Uses an imperative loop to calculate the center of the centroids in the cluster\nUses the mutable state\n \n 133 Aggregating and reducing data in parallel \nFigure 5.5  Benchmark running the k-means algorithm using a quad-core machine with 8 GB of RAM. \nThe algorithms tested are the sequential LINQ and the parallel PLINQ with a variant of a tailored partitioner. The parallel PLINQ runs in 0.481 seconds, which is three times faster than the sequential LINQ version, which runs in 1.316 seconds. A slight improvement is the PLINQ with tailored partitioner that runs in 0.436 sec, which is 11% faster than the original PLINQ version.\nThe benchmark results are impressive. The parallel version of the k-means algorithm using PLINQ runs three times faster than the sequential version in a quad-core machine. The PLINQ partitioner version, shown in listing 5.8, is 11% faster than the PLINQ ver -\nsion. An interesting PLINQ extension is used in the function \nUpdateCentroids . The \nWithExecutionMode(ParallelExecution Mode.ForceParallelism)  extension is used \nto notify the TPL scheduler that the query must be performed concurrently. \nThe two options to configure ParallelExecutionMode  are ForceParallelism  and \nDefault . The ForceParallelism  enumeration forces parallel execution. The Default  \nvalue defers to the PLINQ query for the appropriate decision on execution.\nIn general, a PLINQ query isn’t absolutely guaranteed to run in parallel. The TPL \nscheduler doesn’t automatically parallelize every query, but it can decide to run the entire query, or only a part, sequentially, based upon factors such as the size and com-plexity of the operations and the current state of the available computer resources. The overhead involved in enabling parallelizing execution is more expensive than the speedup that’s obtained. But cases exist when you want to force the parallelism because you may know more about the query execution than PLINQ can determine from its analysis. You may be aware that a delegate is expensive, and consequently the query will absolutely benefit from parallelization, for example. \n \n134 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nThe other interesting extension used in the UpdateCentroids  function is the cus-\ntom partitioner. When parallelizing k-means, you divided the input data into chunks to avoid creating parallelism with excessively fine granularity:\n  var partitioner = Partitioner.Create(data, true)\nThe Partitioner<T>  class is an abstract class that allows for static and dynamic parti-\ntioning. The default TPL Partitioner  has built-in strategies that automatically handle \nthe partitioning, offering good performance for a wide range of data sources. The goal of the TPL \nPartitioner  is to find the balance between having too many parti-\ntions (which introduces overhead) and having too few partitions (which underutilizes the available resources). But situations exist where the default partitioning may not be appropriate, and you can gain better performance from a PLINQ query by using a tailored partitioning strategy. \nIn the code snippet, the custom partitioner is created using an overloaded version \nof the \nPartitioner.Create  method, which takes as an argument the data source and a \nflag indicating which strategy to use, either dynamic or static. When the flag is true, the partitioner strategy is dynamic, and static otherwise. Static partitioning often provides speedup on a multicore computer with a small number of cores (two or four). Dynamic partitioning aims to load balance the work between tasks by assigning an arbitrary size of chunks and then incrementally expanding the length after each iteration. It’s possi-ble to build sophisticated partitioners (http:/ /mng.bz/48UP) with complex strategies. \nUnderstanding how partitioning works\nIn PLINQ, there are four kinds of partitioning algorithms:\n¡ Range partitioning works with a data source with a defined size. Arrays are part of this category:\nint[] data = Enumerable.Range(0, 1000).ToArray();data.AsParallel().Select(n => Compute(n));\n¡ Stripped partitioning  is the opposite of Range . The data source size isn’t pre -\ndefined, so the PLINQ query fetches one item at a time and assigns it to a task until the data source becomes empty. The main benefit of this strategy is that the load can be balanced between tasks:\nIEnumerable<int> data = Enumerable.Range(0, 1000); data.AsParallel().Select(n => Compute(n));\n¡ Hash partitioning uses the value’s hash code to assign elements with the same \nhash code to the same task (for example, when a PLINQ query performs a \nGroupBy) . \n¡ Chunk partitioning works with incremental chunk size, where each task fetches \nfrom the data source a chunk of items, whose length expands with the number of iterations. With each iteration, larger chunks keep the task busy as much as possible.",15234
50-5.2.3 Implementing a parallel Reduce function for PLINQ.pdf,50-5.2.3 Implementing a parallel Reduce function for PLINQ,"135 Aggregating and reducing data in parallel \n5.2.3 Implementing a parallel Reduce function for PLINQ\nNow you’ve learned about the power of aggregate operations, which are particularly suited to scalable parallelization on multicore hardware due to low memory con-sumption and deforesting optimization. The low memory bandwidth occurs because aggregate functions produce less data than they ingest. For example, other aggregate functions such as \nSum()  and Average()  reduce a collection of items to a single value. \nThat’s the concept of reduction: it takes a function to reduce a sequence of elements to a single value. The PLINQ list extensions don’t have a specific function \nReduce , as in \nF# list comprehension or other functional programming languages such as Scala and Elixir. But after having gained familiarity with the \nAggregate  function, the implemen-\ntation of a reusable Reduce  function is an easy job. This listing shows the implementa-\ntion of a Reduce  function in two variants. The bold highlights annotated code.\nListing 5.10  Parallel Reduce  function implementation using Aggregate\nstatic TSource Reduce<TSource>(this ParallelQuery<TSource> source,                                  Func<TSource, TSource, TSource> reduce) =>ParallelEnumerable.Aggregate(source,                             (item1, item2) => reduce(item1, item2)); static TValue Reduce<TValue>(this IEnumerable<TValue> source, TValue seed,    Func<TValue, TValue, TValue> reduce) =>    source.AsParallel()    .Aggregate(               seed: seed,       updateAccumulatorFunc: (local, value) => reduce(local, value),        combineAccumulatorsFunc: (overall, local) =>                                               reduce(overall, local),        resultSelector: overall => overall);    int[] source = Enumerable.Range(0, 100000).ToArray();int result = source.AsParallel()        .Reduce((value1, value2) => value1 + value2);\nThe first Reduce  function takes two arguments: the sequence to reduce and a delegate \n(function) to apply for the reduction. The delegate has two parameters: the partial result and the next element of the collection. The underlying implementation uses \nAggregate  to treat the first item from the source sequence as an accumulator. \nThe second variant of the Reduce  function takes an extra parameter seed , which \nis used as the initial value to start the reduction with the first value of the sequence to aggregate. This version of the function merges the results from multiple threads. This action creates a potential dependency on both the source collection and the result. For this reason, each thread uses thread-local storage, which is non-shared memory, to cache partial results. When each operation has completed, the separate partial results are combined into a final result.Uses a function to determinate if a number is primeFor each iteration, the function func is applied to the current \nitem, and the previous value is used as an accumulator.\nCombines intermediates results \nfrom each thread (partition result)\nReturns the final result; in this place you could have a transformation against the output. Uses the Reduce function, passing an anonymous lambda to apply as a reducing function\n \n136 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nupdateAccumulatorFunc calculates the partial result for a thread. The combine-\nAccumulatorsFunc function merges the partial results into a final result. The last param-\neter is resultSelector , which is used to perform a user-defined operation on the final \nresults. In this case, it returns the original value. The remainder of the code is an exam-ple to apply the \nReduce  function to calculate the sum of a given sequence in parallel. \nassociativity  and commutativity  for deterministic  aggregation\nThe order of computation of an aggregation that runs in parallel using PLINQ (or \nPSeq ) applies the Reduce  function differently than the sequential version. In listing \n5.8, the sequential result was computed in a different order than the parallel result, but the two outputs are guaranteed to be equal because the operator + (plus) used to update the centroid distances has the special properties of associativity and commuta-tivity. This is the line of code used to find the nearest centroid: \nDist(center, centroid2) < Dist(center, centroid1)\nThis is the line of code used to find updates to the centroids:\npoints    .Aggregate(           seed: new double[N],           func: (acc, item) => acc.Zip(item, (a, b) => a + b).ToArray())    .Select(items => items / points.Count())\nIn FP, the mathematical operators are functions. The + (plus) is a binary operator, so it performs on two values and manipulates them to return a result. \nA function is associative when the order in which it’s applied doesn’t change the \nresult. This property is important for reduction operations. The + (plus) operator and the * (multiply) operator are associative because: \n(a + b) + c = a + (b + c)(a * b) * c = a * (b * c)\nA function is commutative when the order of the operands doesn’t change its output, so long as each operand is accounted for. This property is important for combiner oper-ations. The + (plus) operator and the * (multiply) operator are commutative because: \na + b + c = b + c + aa * b * c = b * c * a\nwhy does this matter ? \nUsing these properties, it’s possible to partition the data and have multiple threads operating independently on their own chunks, achieving parallelism, and still return the correct result at the end. The combination of these properties permits the imple-mentation of a parallel pattern such as Divide and Conquer, Fork/Join, or MapReduce. \nFor a parallel aggregation in PLINQ \nPSeq  to work correctly, the applied operation \nmust be both associative and commutative. The good news is that many of the most pop-ular kinds of reduction functions are both.",5922
51-5.2.4 Parallel list comprehension in F PSeq.pdf,51-5.2.4 Parallel list comprehension in F PSeq,,0
52-5.2.5 Parallel arrays in F.pdf,52-5.2.5 Parallel arrays in F,"137 Aggregating and reducing data in parallel \n5.2.4 Parallel list comprehension in F#: PSeq \nAt this point, you understand that declarative programming lends itself to data paral-lelization, and PLINQ makes this particularly easy. PLINQ provides extension methods and higher-order functions that can be used from both C# and F#. But a wrapper mod-ule around the functionality provided in PLINQ for F# makes the code more idiom-atic than working with PLINQ directly. This module is called \nPSeq , and it provides the \nparallel equivalent of the functions part of the Seq computation expression module. \nIn F#, the Seq module is a thin wrapper over the .NET IEnumerable<T>  class to mimic \nsimilar functionality. In F#, all the built-in sequential containers, such as arrays, lists, and sets are subtypes of the \nSeq type.\nIn summary, if parallel LINQ is the right tool to use in your code, then the PSeq  mod-\nule is the best way to use it in F#. This listing shows the implementation of the update-\nCentroids  function using PSeq  in idiomatic F# (in bold). \nListing 5.11  Idiomatic F# using PSeq  to implement updateCentroids  \nlet updateCentroids centroids =        data        |> PSeq.groupBy (nearestCentroid centroids)        |> PSeq.map (fun (_,points) ->            Array.init N (fun i ->                points |> PSeq.averageBy (fun x -> x.[i])))        |> PSeq.sort        |> PSeq.toArray\nThe code uses the F# pipe operator |> for construct pipeline semantics to compute \na series of operations as a chain of expressions. The applied higher-order operations with the \nPSeq.groupBy  and PSeq.map  functions follow the same pattern as the original \nupdateCentroids  function. The map function is the equivalent of Select  in PLINQ. \nThe Aggregate  function PSeq.averageBy  is useful because it replaces boilerplate code \n(necessary in PLINQ) that doesn’t have such functionality built in. \n5.2.5 Parallel arrays in F# \nAlthough the PSeq  module provides many familiar and useful functional constructs, \nsuch as map and reduce , these functions are inherently limited by the fact that they \nmust act upon sequences and not divisible ranges. Consequently, the functions pro-vided by the \nArray.Parallel  module from the F# standard library typically scale much \nmore efficiently when you increase the number of cores in the machine.\nListing 5.12  Parallel sum of prime numbers using F# Array.Parallel  \nlet len = 10000000let isPrime n =     if n = 1 then false    elif n = 2 then true    else Uses a function to determinate if a number is prime\n \n138 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \n        let boundary = int (Math.Floor(Math.Sqrt(float(n))))        [2..boundary - 1]        |> Seq.forall(fun i -> n % i <> 0)let primeSum =    [|0.. len|]    |> Array.Parallel.filter (fun x-> isPrime x)     |> Array.sum\nThe Array.Parallel  module provides versions of many ordinary higher-order array \nfunctions that were parallelized using the TPL Parallel  class. These functions are gen-\nerally much more efficient than their PSeq  equivalents because they operate on contigu-\nous ranges of arrays that are divisible in chunks rather than linear sequences. The Array \n.Parallel module provided by the F# standard library includes parallelized versions \nof several useful aggregate operators, most notably map. The function filter is developed \nusing the Array.Parallel.choose  function. See the the book’s source code.\ndifferent  strategies  in data parallelism : vector  check\nWe’ve covered fundamental programming design patterns that originated with func-tional programming and are used to process data in parallel quickly. As a refresher, these patterns are shown in table 5.1. \nTable 5.1  Parallel data patterns analyzed so far\nPattern Definition Pros and cons\nDivide and ConquerRecursively breaks down a problem into smaller problems until these become small enough to be solved directly. For each recursive call, an independent task is created to perform a sub-problem in parallel. The most popular example of the Divide and Conquer algorithm is Quicksort.With many recursive calls, this pattern could create extra over -\nhead associated with parallel processing that saturates the processors.\nFork/Join This pattern aims to split, or fork, a given data set into chunks of work so that each individual chunk is executed in parallel. After each par -\nallel portion of work is completed, the parallel chunks are then merged, or joined, together. The parallel section forks could be implemented using recursion, similar to Divide and Conquer, until a certain task’s granularity is reached.This provides efficient load balancing.\nAggregate/Reduce This pattern aims to combine in parallel all the elements of a given data set into a single value, by evaluating tasks on independent processing elements.This is the first level of optimization to consider when parallelizing loops with shared state.The elements of a data set to be reduced in parallel should satisfy the associative property. Using an associative operator, any two elements of the data set can be combined into one.Uses a built-in parallel array module \nin F#; the function filter is developed \nusing the Array.Parallel.choose \nfunction. See the book’s source code.",5300
53-5.3.2 Using MapReduce with the NuGet package gallery.pdf,53-5.3.2 Using MapReduce with the NuGet package gallery,"139 Parallel MapReduce pattern\nThe parallel programming abstractions in table 5.1 can be quickly implemented using the multicore development features available in .NET. Other patterns will be analyzed in the rest of the book. In the next section, we’ll examine the parallel MapReduce pattern.\n5.3 Parallel MapReduce pattern\nMapReduce is a pattern introduced in 2004 in the paper “MapReduce: Simplified \nData Processing on Large Clusters,” by Jeffrey Dean and Sanjay Ghemawat (https:/ /research.google.com/archive/mapreduce-osdi04.pdf).\nMapReduce provides particularly interesting solutions for big data analysis and to \ncrunch massive amounts of data using parallel processing. It’s extremely scalable and is used in some of the largest distributed applications in the world. Additionally, it’s designed for processing and generating large data sets to be distributed across multiple machines. Google’s implementation runs on a large cluster of machines and can process terabytes of data at a time. The design and principles are applicable for both a single machine (single-core) on a smaller scale, and in powerful multicore machines. \nThis chapter focuses on applying data parallelism in a single multicore computer, \nbut the same concepts can be applied for partitioning the work among multiple com-puters in the network. In chapters 11 and 12, we’ll cover the agent (and actor) pro-gramming model, which can be used to achieve such network distribution of tasks. \nThe idea for the MapReduce model (as shown in figure 5.6) is derived from the func-\ntional paradigm, and its name originates from concepts known as \nmap and reduce  combi-\nnators. Programs written using this more functional style can be parallelized over a large cluster of machines without requiring knowledge of concurrent programming. The actual runtime can then partition the data, schedule, and handle any potential failure. \nBig data setAggregate\ndataJoin ForkMap\nMap\nMap\nMap\nMap\nMapMapReduce\nReduce\nReduce\nReduce\nReduce\nReduceReduce\nFigure 5.6  A schematic illustration of the phases of a MapReduce computation. The MapReduce \npattern is composed primarily of two steps: map and reduce. The Map function is applied to all items and \nproduces intermediate results, which are merged using the Reduce  function. This pattern is similar to \nthe Fork/Join pattern because after splitting the data in chunks, it applies in parallel the tasks map and reduce independently. In the image, a given data set is partitioned into chunks that can be performed independently because of the absence of dependencies. Then, each chunk is transformed into a different shape using the \nMap function. Each Map execution runs simultaneously. As each map chunk operation \ncompletes, the result is passed to the next step to be aggregated using the Reduce  function. (The \naggregation can be compared to the join  step in the Fork/Join pattern.)\n \n140 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nThe MapReduce model is useful in domains where there’s a need to execute a massive number of operations in parallel. Machine learning, image processing, data mining, and distributed sort are a few examples of domains where MapReduce is widely used.\nIn general, the programming model is based upon five simple concepts. The order \nisn’t a rule and can be changed based on your needs:\n1 Iteration over input\n2 Computation of key/value pairs from each input\n3 Grouping of all intermediate values by key\n4 Iteration over the resulting groups\n5 Reduction of each group\nThe overall idea of MapReduce is to use a combination of maps and reductions to query a stream of data. To do so, you can map the available data to a different format, producing a new data item in a different format for each original datum. During a \nMap \noperation you can also reorder the items, either before or after you map them. Oper -\nations that preserve the number of elements are Map operations. If you have many ele-\nments you may want to reduce the number of them to answer a question. You can filter the input stream by throwing away elements you don’t care about. \nMapReduce pattern and GroupBy\nThe Reduce  function reduces an input stream to a single value. Sometimes instead of \na single value you’ll need to reduce a large number of input elements by grouping them according to a condition. This doesn’t actually reduce the number of elements, it only groups them, but you can then reduce each group by aggregating the group to a single value. You could, for example, calculate the sum of values in each group if the group con -\ntains values that can be summed.\n \nYou can combine elements into a single aggregated element and return only those that provide the answer you seek. Mapping before reducing is one way to do it, but you can also \nReduce  before you Map or even Reduce , Map, and then Reduce  even more, and so \non. In summary, MapReduce maps (translates data from one format to the other and orders the data) and reduces (filters, groups, or aggregates) the data. \n5.3.1 The Map and Reduce functions\nMapReduce is composed of two main phases:\n¡ Map receives the input and performs a map function to produce an output of \nintermediate key/value pairs. The values with the same key are then joined and passed to the second phase. \n¡ Reduce  aggregates the results from Map by applying a function to the values asso-\nciated with the same intermediate key to produce a possibly smaller set of values. \n \n 141 Parallel MapReduce pattern\nThe important aspect of MapReduce is that the output of the Map phase is com-\npatible with the input of the Reduce  phase. This characteristic leads to functional \ncompositionality. \n5.3.2 Using MapReduce with the NuGet package gallery\nIn this section, you’ll learn how to implement and apply the MapReduce pattern using \na program to download and analyze NuGet packages from the online gallery. NuGet is a package manager for the Microsoft development platform including .NET, and the NuGet gallery is the central package repository used by all package developers. At the time of writing, there were over 800,000 NuGet packages. The purpose of the program is to rank and determine the five most important NuGet packages, calculat-ing the importance of each  by adding its score rate with the score values of all its dependencies. \nBecause of the intrinsic relation between MapReduce and FP, listing 5.13 will be \nimplemented using F# and \nPSeq  to support data parallelism. The C# version of the code \ncan be found in the downloadable source code. \nIt’s possible to use the same basic idea to find other information, such as the depen-\ndencies for a package that you are using, what the dependencies of dependencies are, and so on. \nNOTE  Downloading all the information about the versions of all NuGet \npackages takes some time. In the solution, there’s a zipped file (nuget-latest- versions.model) in the subfolder Models in the downloadable source code. If you want to update the most current values, delete this file, run the application, and be patient. The new updated file will be zipped and saved for the next time.\nListing 5.13 defines both the \nMap and Reduce  functions. The Map function transforms \na NuGet package input into a key/value pair data structure, where the key is the name of the package and the value is the rank value (\nfloat ). This data structure is defined as \na sequence of key/value types because each package could have dependencies, which will be evaluated as part of the total score. The \nReduce  function takes as an argument \nthe name of the package with the sequence of associated score/values. This input matches the output of the previous \nMap function. \nListing 5.13  PageRank  object encapsulating the Map and Reduce  functions \ntype PageRank (ranks:seq<string*float>) =    let mapCache = Map.ofSeq ranks            let getRank (package:string) =        match mapCache.TryFind package with            | Some(rank) -> rank        | None -> 1.0    member this.Map (package:NuGet.NuGetPackageCache) =Uses an internal table to keep in memory the NuGet collection of name and score value pair\nIf the NuGet package isn’t found, a default score of 1.0 is used as the default rank.\n \n142 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \n        let score =             (getRank package.PackageName)               /float(package.Dependencies.Length)         package.Dependencies                   |> Seq.map (fun (Domain.PackageName(name,_),_,_) -> (name, score))    member this.Reduce (name:string) (values:seq<float>) =        (name, Seq.sum values)     \nThe PageRank  object encapsulates the Map and Reduce  functions, providing easy access \nto the same underlying data structure ranks. Next, you need to build the core of the program, MapReduce. Using FP style, you can model a reusable MapReduce function, passing the functions as input for both \nMap and Reduce  phases. Here is the implemen-\ntation of mapF .\nListing 5.14  mapF  function for the first phase of the MapReduce pattern\nlet mapF M (map:'in_value -> seq<'out_key * 'out_value>)           (inputs:seq<'in_value>) =    inputs    |> PSeq.withExecutionMode ParallelExecutionMode.ForceParallelism     |> PSeq.withDegreeOfParallelism M     |> PSeq.collect (map)     |> PSeq.groupBy (fst)     |> PSeq.toList \nThe mapF  function takes as its first parameter an integer value M, which determines the \nlevel of parallelism to apply. This argument is intentionally positioned first because it makes it easier to partially apply the function to reuse with the same value. Inside the body of \nmapF  the degree of parallelism is set using PSeq.withDegreeOfParallelism  M. This \nextension method is also used in PLINQ. The purpose of the configuration is to restrict the number of threads that could run in parallel, and it isn’t a coincidence that the query is eagerly materialized exercising the last function \nPSeq.toList . If you omit PSeq.with -\nDegreeOfParallelism , then the degree of parallelism isn’t guaranteed to be enforced. \nIn the case of a multicore single machine, it’s sometimes useful to limit the num-\nber of running threads per function. In the parallel MapReduce pattern, because Map \nand Reduce  are executed simultaneously, you might find it beneficial to constrain the \nresources dedicated for each step. For example, the value maxThreads  defined as\nlet maxThreads = max (Environment.ProcessorCount / 2, 1) \ncould be used to restrict each of the two MapReduce phases to half of the system threads.Uses a function to calculate the average \nscore of the NuGet dependencies\nUses a function to reduce to a single value all the scores related to one package\nForces the degree of parallelism Sets the degree of parallelism to an arbitrary value M\nMaps the items in the input collection\nGroups the mapped items by the key generated by the map function\nForces the materialization of the sequence to ensure that the degree of parallelism is applied\n \n 143 Parallel MapReduce pattern\nThe second argument of mapF  is the core map function, which operates on each input \nvalue and returns the output sequence key/value pairs. The type of the output sequence can be different from the type of the inputs. The last argument is the sequence of input values to operate against.\nAfter the \nmap function, you implement the reduce  aggregation. This listing shows the \nimplementation of the aggregation function reduceF  to run the second and final result. \nListing 5.15  reduceF  function for the second phase of MapReduce \nlet reduceF R (reduce:'key -> seq<'value> -> 'reducedValues)              (inputs:('key * seq<'key * 'value>) seq) =    inputs    |> PSeq.withExecutionMode ParallelExecutionMode.ForceParallelism      |> PSeq.withDegreeOfParallelism R         |> PSeq.map (fun (key, items) ->              items        |> Seq.map (snd)                          |> reduce key)                        |> PSeq.toList\nThe first argument R of the reduceF  function has the same purpose of setting the \ndegree of parallelism as the argument M in the previous mapF  function. The second \nargument is the reduce  function that operates on each key/values pair of the input \nparameter. In the case of the NuGet package example, the key is a string for the name of the package, and the sequence of values is the list of ranks associated with the pack-age. Ultimately, the input argument is the sequence of key/value pairs, which matches the output of the \nmapF  function. The reduceF  function generates the final output.\nAfter having defined the functions map and reduce , the last step is the easy one: put-\nting everything together (in bold).\nListing 5.16  mapReduce  composed of the mapF  and reduceF  functions\nlet mapReduce        (inputs:seq<'in_value>)        (map:'in_value -> seq<'out_key * 'out_value>)        (reduce:'out_key -> seq<'out_value> -> 'reducedValues)        M R =    inputs |> (mapF M map >> reduceF R reduce) \nBecause the output of the map function matches the input of the reduce  function, \nyou can easily compose them together. The listing shows this functional approach in the implementation of the \nmapReduce  function. The mapReduce  function arguments \nfeed the underlying mapF  and reduceF  functions. The same explanation applies. The \nimportant part of this code is the last line. Using the F# built-in pipe operator ( |>) and \nforward composition operator ( >>), you can put everything together. Forces the degree \nof parallelism \nSets the degree of parallelism to an arbitrary value R\nMaps the items in the input collection in the form of a key/value pair\nExtracts the values from the input sequence to apply the reduce function\nThe functions map and \nreduce are composed \nusing the F# forward-\ncomposition >> \noperator.\n \n144 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nThis code shows how you can now utilize the function mapReduce   from listing 5.16  \nto calculate the NuGet package ranking:\n        let executeMapReduce (ranks:(string*float)seq) =        let M,R = 10,5                let data = Data.loadPackages()        let pg = MapReduce.Task.PageRank(ranks)                mapReduce data (pg.Map) (pg.Reduce) M R\nThe class pg (PageRank ) is defined in listing 5.13 to provide the implementation of \nboth the map and reduce  functions. The arbitrary values M and R set how many workers \nto create for each step of the MapReduce. After the implementation of the mapF  and \nreduceF  functions, you compose them to implement a mapReduce  function that can be \nconveniently utilized as a new function.\nFigure 5.7  Benchmark running the MapReduce algorithm using a quad-core machine with 8 GB of \nRAM. The algorithms tested are sequential LINQ, parallel F# PSeq , and PLINQ with a variant of tailored \npartitioner. The parallel version of MapReduce that uses PLINQ runs in 1.136 seconds, which is 38% faster than the sequential version using regular LINQ in C#. The F# \nPSeq  performance is almost \nequivalent to PLINQ, as expected, because they share the same technology underneath. The parallel C# PLINQ with tailored partitioner is the fastest solution, running in 0.952 sec, about 18% faster than ordinary PLINQ, and twice as fast as the baseline (the sequential version).   \nAs expected, the serial implementation in figure 5.7 is the slowest one. Because the parallel versions F# \nPSeq  and C# PLINQ use the same underlying library, the speed \nvalues are almost equivalent. The F# PSeq  version is a little slower with a higher CPU \ntime because of the extra overhead induced by the wrapper. The fastest MapReduce is \n \n 145 Parallel MapReduce pattern\nthe PLINQ parallel version with tailored partitioner, which can be found in the source code for this book. \nThis is the result of the five most important NuGet packages:\nMicrosoft.NETCore.Platforms :     6033.799540Microsoft.NETCore.Targets :       5887.339802System.Runtime :                  5661.039574Newtonsoft.Json :                 4009.295644NETStandard.Library :             1876.720832\nIn MapReduce, any form of reduction performed in parallel can offer different results than a serial one if the operation isn’t associative.\nmapreduce  and a little  math \nThe associative and commutative properties introduced earlier in this chapter prove \nthe correctness and deterministic behavior of aggregative functions. In parallel and functional programming, the adoption of mathematical patterns is common to guar -\nantee accuracy in the implementation of a program. But a deep knowledge of mathe-matics isn’t necessary. \nCan you determine the values of x in the following equations?\n9 + x = 122 < x < 4\nIf you answered 3 for both functions, good news, you already know all the math that it takes to write deterministic concurrent programs in functional style using techniques from linear algebra (https:/ /en.wikipedia.org/wiki/Linear_algebra).\nwhat math can do to simplify  parallelism : monoids\nThe property of association leads to a common technique known as a monoid (https://\nwiki.haskell.org/Monoid), which works with many different types of values in a sim-ple way. The term monoid (not to be confused with monad: https:/ /wiki.haskell.org/Monad) comes from mathematics, but the concept is applicable to computer program-ming without any math knowledge. Essentially, monoids are operations whose output type is the same as the input, and which must satisfy some rules: associativity, identity, and closure.  \nYou read about associativity in the previous section. The identity property says that a \ncomputation can be executed multiple times without affecting the result. For example, an aggregation that is associative and commutative can be applied to one or more reduc-tion steps of the final result without affecting the output type. The closure rule enforces \nthat the input(s) and output(s) types of a given function must be the same. For example, addition takes two numbers as parameters and returns a third number as a result. This rule can be expressed in .NET with a function signature \nFunc<T, T, T> that ensures \nthat all arguments belong to the same type, in opposition to a function signature such as \nFunc<T1, T2, R> .\nIn the k-means example, the function UpdateCentroids  satisfies these laws because \nthe operations used in the algorithm are monoidal—a scary word that hides a simple concept. This operation is addition (for reduce).\n \n146 chapter  5 PLINQ and MapReduce: data parallelism, part 2 \nThe addition function takes two numbers and produces output of the same type. In \nthis case, the identity element is 0 (zero) because a value 0 can be added to the result of the operation without changing it. Multiplication is also a monoid, with the identity element 1. The value of a number multiplied by 1 does not change. \nWhy is it important that an operation returns a result of the same type as the input(s)? \nBecause it lets you chain and compose multiple objects using the monoidal operation, making it simple to introduce parallelism for these operations. \nThe fact that an operation is associative, for example, means you can fold a data \nstructure to reduce a list sequentially. But if you have a monoid, you can reduce a list using a \nfold  (Aggregate ), which can be more efficient for certain operations and also \nallows for parallelism. \nTo calculate the factorial of the number 8, the multiplication operations running in \nparallel on a two-core CPU should look something like table 5.2.\nTable 5.2  Parallel calculation of the factorial product of the number 8\nCore 1 Core 2\nStep 1 M1 = 1 * 2 M2 = 3 * 4  \nStep 2 M3 = M2 * 5  M4 = 6 * M1\nStep 3 M5 = M4 * 7 M6= 8 * M3\nStep 4 idle M7= M6 * M5\nResult 40320\nThe same result can be achieved using parallel aggregation in either F# or C# to reduce the list of numbers 1 to 8 into a single value:\n[1..8] |> PSeq.reduce (*)Enumerable.Range(1,8).AsParallel().Reduce((a,b)=> a * b);\nBecause multiplication is a monoidal operation for the type integer , you can be sure \nthat the result of running the operation in parallel is deterministic.\nNOTE  Many factors are involved when exploiting parallelism, so it’s important \nto continually benchmark and measure the speedup of an algorithm using the sequential version as baseline. In fact, in certain cases, a parallel loop might run slower than its sequential equivalent. If the sequence is too small to run in parallel, then the extra overhead introduced for the task coordination can pro-duce negative effects. In this case, the sequential loop fits the scenario better.\nSummary\n¡ Parallel LINQ and F# PSeq  both originate from the functional paradigm and are \ndesigned for data parallelism, simple code, and high performance. By default, these technologies take the logical processor count as the degree of parallelism. \n \n 147 Summary\nThese technologies handle the underlying processes regarding the partitioning of sequences in smaller chunks, set the degree of parallelism counting the logical machine cores, and run individually to process each subsequence. \n¡ PLINQ and F# PSeq  are higher-level abstraction technologies that lie on top \nof multithreading components. These technologies aim to reduce the time of query execution, engaging the available computer resources. \n¡ The .NET Framework allows tailored techniques to maximize performance in data analysis. Consider value types over reference types to reduce memory prob-lems, which otherwise could provoke a bottleneck due to the generation of too many GCs. \n¡ Writing pure functions, or functions without side effects, makes it easier to rea-son about the correctness of your program. Furthermore, because pure func-tions are deterministic, when passing the same input, the output doesn’t change. The order of execution doesn’t matter, so functions without side effects can eas-ily be executed in parallel.\n¡ Designing with pure functions and decoupling side effects from pure logic are the two basic tenets that functional thinking brings to the forefront.\n¡ Deforestation is the technique to eliminate the generation of intermediate data structures to reduce the size of temporary memory allocation, which benefits the performance of the application. This technique is easily exploitable with the higher-order function \nAggregate  in LINQ. It combines multiple operations in a \nsingle step, such as filter  and map, which would have otherwise had an alloca-\ntion for each operation.\n¡ Writing functions that are associative and commutative permits the implemen-tation of a parallel pattern like Divide and Conquer, Fork/Join, or MapReduce. \n¡ The MapReduce pattern is composed primarily of two steps: map and reduce. The \nMap function is applied to all items and produces intermediate results, \nwhich are merged using the Reduce  function. This pattern is similar to Fork/Join \nbecause after splitting the data into chunks, it applies in parallel the tasks Map \nand Reduce  independently.",23120
54-6.1 Reactive programming big event processing.pdf,54-6.1 Reactive programming big event processing,"1486Real-time event streams: \nfunctional reactive \nprogramming\nThis chapter covers\n¡ Understanding queryable event streams\n¡ Working with Reactive Extensions (Rx)\n¡ Combining F# and C# to make events first-class values\n¡ Processing high-rate data streams\n¡ Implementing a Publisher-Subscriber pattern\nWe’re used to responding to events in our lives daily. If it starts to rain, we get an umbrella. If the daylight in a room begins to dim, we flip the switch to turn on the electric light. The same is true in our applications, where a program must react to (or handle) events caused by something else happening in the application or a user interacting with it. Almost every program must handle events, whether they’re the receipt of an HTTP request for a web page on a server, a notification from your favor -\nite social media platform, a change in your filesystem, or a simple click of a button. \nToday’s challenge for applications isn’t reacting to one event, but reacting to a \nconstant high volume of events in (near) real time. Consider the humble smart-phone. We depend on these devices to be constantly connected to the internet and continuously sending and receiving data. These multidevice interconnections can \n \n 149 Reactive programming: big event processing\n6be compared to billions of sensors that are acquiring and sharing information, with the need for real-time analysis. In addition, this unstoppable massive stream of notifications continues to flow from the internet fire hose, requiring that the system be designed to handle back-pressure (https:/ /en.wikipedia.org/wiki/Back_pressure) and notifica-tions in parallel. \nBack-pressure refers to a situation where the event-fetching producer is getting too far \nahead of the event-processing consumer. This could generate potential spikes in memory consumption and possibly reserve more system resources for the consumer until the con-sumer is caught up. More details regarding back-pressure are covered later in the chapter.\nIt’s predicted that by 2020 more than 50 billion devices will be connected to the \ninternet. Even more stunning is that this expansion of digital information shows no signs of slowing any time soon! For this reason, the ability to manipulate and analyze high-speed data streams in real time will continue to dominate the field of data (big data) analysis and digital information. \nNumerous challenges exist to using a traditional programming paradigm for the \nimplementation of these kinds of real-time processing systems. What kinds of technol-ogies and tools can you use to simplify the event programming model? How can you concurrently handle multiple events without thinking concurrently? The answers lie with reactive programming. \nIn computing, reactive programming is a programming paradigm that maintains a continuous interaction with their environment, but at a speed which is determined by the environment, not the program itself.\n—Gèrard Berry (“Real Time Programming: Special Purpose or General Purpose \nLanguages,” Inria (1989), http:/ /mng.bz/br08)\nReactive programming is programming with everlasting asynchronous streams of events made simple. On top of that, it combines the benefits of functional programming for concurrency, which you’ve seen in earlier chapters, with the reactive programming toolkit to make event-driven programming highly beneficial, approachable, and safe. Furthermore, by applying various high-order operators on streams, you can easily achieve different computational goals.\nBy the end of this chapter, you’ll know how reactive programming avoids the prob-\nlems that occur when using imperative techniques to build reactive systems. You’ll design and implement event-driven applications, coupled with support for asynchro-nicity, that are responsive, scalable, and loose.\n6.1 Reactive programming: big event processing\nReactive programming, not to be confused with functional reactive programming, refers to a programming paradigm that focuses on listening and processing events asynchro-nously as a data stream, where the availability of new information drives the logic for -\nward rather than having the control flow driven by a thread of execution.\nA common example of reactive programming is a spreadsheet, where cells contain \nliteral values or formulas such as C1 = A1 + B1 or, in Excel lingo, \nC1 = Sum(A1:B1) . In \n \n150 chapter  6 Real-time event streams: functional reactive programming\nthis case, the value in the cell C1 is evaluated based on the values in other cells. When the value of one of the other cells B1 or A1 changes, the value of the formula automati-cally recalculates to update the value of C1, as seen in figure 6.1.\nCell C1 is automatically updated\nwhen cell A1 or B1 changes.\nFigure 6.1  This Excel spreadsheet is reactive, meaning that cell C1 reacts to a change of value in either \ncell A1 or B1 through the formula Sum(A1:B1) .\nThe same principle is applicable for processing data to notify the system when a change of state occurs. Analyzing data collections is a common requirement in soft-ware development. In many circumstances, your code could benefit from using a reac-tive event handler. The reactive event handler allows a compositional reactive semantic to express operations, such as \nFilter  and Map, against events elegantly and succinctly, \nrather than a regular event handler, which is designed to handle simple scenarios with limited flexibility. \nThe reactive programming approach to event handling is different from the tra-\nditional approach because events are treated as streams. This provides the ability to manipulate effortless events with different features, such as the ability to filter, map, and merge, in a declarative and expressive way. For example, you might design a web service that filters the event stream to a subset of events based on specified rules. The resulting solution uses reactive programming to capture the intended behaviors by describing the operations in a declarative manner, which is one of the tenets of FP. This is one reason why it’s commonly called functional reactive programming; but this term requires further explanation.\nWhat is functional reactive programming (FRP)? Technically, FRP is a programming \nparadigm based on values that change over time, using a set of simple compositional Reac-tive operators (\nbehavior  and event ) that, in turn, are used to build more complex \noperators. This programming paradigm is commonly used for developing UIs, robot-ics, and games, and for solving distributed and networked system challenges. Due to the powerful and simplified compositional aspect of FRP, several modern technologies use FRP principles to develop sophisticated systems. For example, the programming languages Elm (http://elm-lang.org) and Yampa (https:/ /wiki.haskell.org/Yampa) \nare based on FRP.   \n \n 151 Reactive programming: big event processing\nFrom the industry standpoint, FRP is a set of different but related functional pro-\ngramming technologies combined under the umbrella of event handling. The con-fusion is derived from similarity and misrepresentation—using the same words in different combinations:\n¡ Functional programming is a paradigm that treats computation as the evaluation of an expression and avoids changing state and mutable data.\n¡ Reactive programming is a paradigm that implements any application where there’s a real-time component.\nReactive programming is becoming increasingly more important in the context of real-time stream processing for big data analytics. The benefits of reactive program-ming are increased use of computing resources on multicore and multi-CPU hardware by providing a straightforward and maintainable approach for dealing with asynchro-nous and no-blocking computation and IO. Similarly, FRP offers the right abstraction to make event-driven programming highly beneficial, approachable, safe, and compos-able. These aspects let you build real-time, reactive programs with clean and readable code that’s easy to maintain and expand, all without sacrificing performance. \nThe reactive programming concept is non-blocking asynchronous based, reverting \ncontrol from “asking” to “waiting” for events, as shown in figure 6.2. This principle is called inversion of control (http:/ /martinfowler.com/bliki/InversionOfControl.html), \nalso referred to as the Hollywood Principle (don’t call me, I’ll call you). \nMerge\nMergeFilterParallel processing of events\nFilterMap\nConsumerConsumerConsumerConsumerConsumerConsumer\nMapHigh-rate\nevents\nHigh-rate\neventsEvent\nsource\nEvent\nsource\nEvent\nsource\nFigure 6.2  Real-time reactive programming promotes non-blocking (asynchronous) operations that are \ndesigned to deal with high-volume, high-velocity event sequences over time by handling multiple events simultaneously, possibly in parallel. \nReactive programming aims to operate on a high-rate sequence of events over time, sim-plifying the concurrent aspect of handling multiple events simultaneously (in parallel). \nWriting applications that are capable of reacting to events at a high rate is becoming \nincreasingly important. Figure 6.3 shows a system that’s processing a massive number of tweets per minute. These messages are sent by literally millions of devices, representing the event sources, into the system that analyzes, transforms, and then dispatches the tweets to those registered to read them. It’s common to annotate a tweet message with a",9514
55-6.2.2 .NET interoperability with F combinators.pdf,55-6.2.2 .NET interoperability with F combinators,"152 chapter  6 Real-time event streams: functional reactive programming\nhashtag to create a dedicated channel and group of interests. The system uses a hashtag to filter and partition the notifications by topic. \nMerge FilterEvent transformer\nMap\nFigure 6.3  Millions of devices represent a rich source of events, capable of sending a massive number \nof tweets per minute. A real-time reactive system can handle the massive quantities of tweets as an event stream by applying non-blocking (asynchronous) operations (\nmerge , filter , and map) and then \ndispatching the tweets to the listeners (consumers).\nEvery day, millions of devices send and receive notifications that could overflow and potentially crash the system if it isn’t designed to handle such a large number of sus-tained events. How would you write such a system? \nA close relationship exists between FP and reactive programming. Reactive program-\nming uses functional constructors to achieve composable event abstraction. As previ-ously mentioned, it’s possible to exploit higher-order operations on events such as \nmap, \nfilter , and reduce . The term FRP is commonly used to refer to reactive programming, \nbut this isn’t completely correct. \nNOTE  FRP is a comprehensive topic; but only the basic principles are covered \nin this chapter. For a deeper explanation of FRP, I recommend Functional Reac-tive Programming by Stephen Blackheath and Anthony Jones (Manning Publi-cations, 2016, www.manning.com/books/functional-reactive-programming).\n6.2 .NET tools for reactive programming\nThe .NET Framework supports events based on a delegate model. An event handler for a subscriber registers a chain of events and triggers the events when called. Using an imperative programming paradigm, the event handlers need a mutable state to keep track of the subscriptions to register a callback, which wraps the behavior inside a function to limit composability. \nHere’s a typical example of a button-click event registration that uses an event han-\ndler and anonymous lambda:\npublic MainWindow(){     myButton.Click += new System.EventHandler(myButton_Click);     myButton.Click += (sender, args) => MessageBox.Show(“Bye!”);}\n \n 153 .NET tools for reactive programming\nvoid myButton_Click(object sender, RoutedEventArgs e){     MessageBox.Show(“Hello!”);}\nThis pattern is the primary reason that .NET events are difficult to compose, almost impossible to transform, and, ultimately, the reason for accidental memory leaks. In general, using the imperative programming model requires a shared mutable state for communication between events, which could potentially hide undesirable side effects. When implementing complex event combinations, the imperative programming approach tends to be convoluted. Additionally, providing an explicit callback function limits your options to express code functionality in a declarative style. The result is a program that’s hard to understand and, over time, becomes impossible to expand and to debug. Furthermore, .NET events don’t provide support for concurrent programs to raise an event on a separate thread, making them a poor fit for today’s reactive and scalable applications.\nNOTE  Event streams are unbounded flows of data processing, originating from \na multitude of sources, which are analyzed and transformed asynchronously through a pipeline of composed operations. \nEvents in .NET are the first step toward reactive programming. Events have been part of the .NET Framework since the beginning. In the early days of the .NET Framework, events were primarily used when working with graphical user interfaces (GUIs). Today, \ntheir potential is being explored more fully. With the .NET Framework, Microsoft introduced a way to reason and treat events as first-class values by using the F# \nEvent  \n(and Observable ) module and .NET Reactive Extensions (Rx). Rx lets you compose \nevents easily and declaratively in a powerful way. Additionally, you can handle events as a data stream capable of encapsulating logic and state, ensuring that your code is with-out side effects and mutable variables. Now your code can fully embrace the functional paradigm, which focuses on listening and processing events asynchronously.\n6.2.1 Event combinators—a better solution \nCurrently, most systems get a callback and process these events when and as they hap-pen. But if you consider events as a stream, similar to lists or other collections, then you can use techniques for working with collections or processing events, which eliminates the need for callbacks. The F# list comprehension, introduced in chapter 5, provides a set of higher-order functions, such as \nfilter  and map, for working with lists in a declar -\native style: \nlet squareOfDigits (chars:char list)     |> List.filter (fun c -> Char.IsDigit c && int c % 2 = 0)     |> List.map (fun n -> int n * int n)\nIn this code, the function squareOfDigits  takes a list of characters and returns the \nsquare of the digits in the list. The first function filter  returns a list with elements \nfor which a given predicate is true; in this case, the characters are even digits. The sec-ond function, \nmap, transforms each element n passed into an integer and calculates its \n \n154 chapter  6 Real-time event streams: functional reactive programming\nsquare value n * n. The pipeline operator ( |>) sequences the operations as a chain of \nevaluations. In other words, the result of the operation on the left side of the equation will be used as an argument for the next operation in the pipeline.\nThe same code can be translated into LINQ to be more C# friendly:\nList<int> SquareOfDigits(List<char> chars) =>      chars.Where(c => char.IsDigit(c) && char.GetNumericValue(c) % 2 == 0)          .Select(c => (int)c * (int)c).ToList();\nThis expressive programming style is a perfect fit for working with events. Different than C#, F# has the advantage of treating events intrinsically (natively) as first-class values, which means you can pass them around like data. Additionally, you can write a function that takes an event as an argument to generate a new event. Consequently, an event can be passed into functions with the pipe operator (\n|>) like any other value. \nThis design and method of using events in F# is based on combinators, which look like programming using list comprehension against sequences. The event combinators are exposed in the F# module \nEvent  that can be used to compose events:\ntextBox.KeyPress|> Event.filter (fun c -> Char.IsDigit c.KeyChar && int c.KeyChar % 2 = 0)|> Event.map (fun n -> int n.KeyChar * n.KeyChar)\nIn this code, the KeyPress  keyboard event is treated as a stream, which is filtered to \nignore events that aren’t interesting, so that the final computation occurs only when the keys pressed are digits. The biggest benefit of using higher-order functions is a cleaner separation of concerns.\n1 C# can reach the same level of expressiveness and com-\npositionality using .NET Rx, as briefly described later in this chapter. \n6.2.2 .NET interoperability with F# combinators\nUsing F# event combinators, you can write code using an algebra of events that aims to separate complex events from simple ones. Is it possible to take advantage of the F# event combinators module to write more declarative C# code? Yes.\nBoth .NET programming languages F# and C# use the same common language run-\ntime (CLR), and both are compiled into an intermediate language (IL) that conforms to the Common Language Infrastructure (CLI) specification. This makes it possible to share the same code. \nIn general, events are understood by all .NET languages, but F# events are used as \nfirst-class values and, consequently, require only a small amount of extra attention. To ensure that the F# events can be used by other .NET languages, the compiler must be notified by decorating the event with the \n[<CLIEvent>]  attribute. It’s convenient \nand efficient to use the intrinsic compositional aspect of F# event combinators to build sophisticated event handlers that can be consumed in C# code. \nLet’s see an example to better understand how F# event combinators work and how \nthey can easily be consumed by other .NET programming languages. Listing 6.1 shows how to implement a simple game to guess a secret word using F# event combinators. \n1 The design principle separates a computer program into sections so each addresses a particular con-cern.  The value is simplifying development and maintenance of computer programs (https:/ /en.wikipedia.org/wiki/Separation_of_concerns).\n \n 155 .NET tools for reactive programming\nThe code registers two events: the KeyPress  event from the WinForms control passed \ninto the construct of KeyPressedEventCombinators , and the Elapsed  time event from \nSystem.Timers.Timer . The user enters text—in this case, only letters are allowed (no \ndigits)—until either the secret word is guessed or the timer (the given time interval)  \nhas elapsed. When the user presses a key, the filter and event combinators transform the event source into a new event through a chain of expressions. If the time expires before the secret word is guessed, a notification triggers a “Game Over” message; otherwise, it trig-gers the “You Won!” message when the secret word matches the input.\nListing 6.1  F# Event combinator to manage key-down events\ntype KeyPressedEventCombinators(secretWord, interval,\n➥ control:#System.Windows.Forms.Control) =\n    let evt =        let timer = new System.Timers.Timer(float interval)         let timeElapsed = timer.Elapsed |> Event.map(fun _ -> 'X')         let keyPressed = control.KeyPress                       |> Event.filter(fun kd -> Char.IsLetter kd.KeyChar)                       |> Event.map(fun kd -> Char.ToLower kd.KeyChar)         timer.Start()          keyPressed        |> Event.merge timeElapsed         |> Event.scan(fun acc c ->            if c = 'X' then ""Game Over""            else                let word = sprintf ""%s%c"" acc c                if word = secretWord then ""You Won!""                else word            ) String.Empty     [<CLIEvent>]    member this.OnKeyDown = evt \nThe type KeyPressedEventCombinators  has a constrvuctor parameter control, which \nrefers to any object that derives from System.Windows.Forms.Control . The # annota-\ntion in F# is called a flexible type, which indicates that a parameter is compatible with a \nspecified base type (http:/ /mng.bz/FSp2).\nThe KeyPress  event is linked to the System.Windows.Forms.Control base control \npassed into the type constructor, and its event stream flows into the F# event-combi-nators pipeline for further manipulation. The \nOnKeyDown  event is decorated with the \nattribute [<CLIEvent>]  to be exposed (published) and visible to other .NET languages. \nIn this way, the event can be subscribed to and consumed from C# code, obtaining reac-tive programmability by referencing the F# library project. Figure 6.4 presents the F# event-combinators pipeline, where the \nKeyPress  event stream runs through the series \nof functions linked as a chain. Creates and starts \na System.Timers \n.Timer instance The map function from the Event module registers and transforms the timer event to notify char ‘X’ when it triggers.\nRegisters the \nKeyPress event \nusing the F# \nEvent module to \nfilter and publish \nonly lowercase \nlettersMerges the filters to handle as a whole. When either the event timer or keypress is triggered, the event fires a notification.\nThe scan function maintains an internal state of the keys pressed and pushes the result from every call to the accumulator function. \nExposes the F# event to other .NET programming languages through the special CLIEvent attribute",11786
56-6.3 Reactive programming in .NET Reactive Extensions Rx.pdf,56-6.3 Reactive programming in .NET Reactive Extensions Rx,"156 chapter  6 Real-time event streams: functional reactive programming\nToLower\nChar\nMergeMap\nScan\nAccumulatorHandlerChar\nIsLetter\nFilter Key pressWinforms\ncontrol\nMap Time elapsed Timer\nTo  X'\nchar'\nFigure 6.4  An event-combinator pipeline showing how two event flows manage their own set of events \nbefore being merged and passed into the accumulator. When a key is pressed on a WinForms control, the \nfilter  event checks whether the key pressed is a letter, and then map retrieves the lowercase version \nof that letter to scan. When the time elapses on the timer, the map operator passes an “X” as in “no \nvalue” to the scan  function.\nThe event-combinator chain in figure 6.4 is complex, but it demonstrates the simplic-ity of expressing such a convoluted code design using events as first-class values. The F# event combinators raise the level of abstraction to facilitate higher-order opera-tions for events, which makes the code more readable and easier to understand when compared to an equivalent program written in imperative style. Implementing the program using the typical imperative style requires creating two different events that communicate the state of the timer and maintain the state of the text with a shared mutable state. The functional approach with event combinators removes the need for a shared immutable state; and, moreover, events are composable. \nTo summarize, the main benefits of using F# event combinators are:\n¡ Composability—You can define events that capture complex logic from simpler events. \n¡ Declarative—The code written using F# event combinators is based on functional principles; therefore, event combinators express what to accomplish, rather than how to accomplish a task.\n¡ Interoperability—F# event combinators can be shared across .NET languages so the complexity can be hidden in a library. \n6.3 Reactive programming in .NET: Reactive Extensions (Rx)\nThe .NET Rx library facilitates the composition of asynchronous event-based programs using observable sequences. Rx combines the simplicity of LINQ-style semantics for manipulating collections and the power of asynchronous programming models to use the clean async/await patterns from .NET 4.5. This powerful combination enables a toolset that lets you treat event streams using the same simple, composable, and \n \n 157 Reactive programming in .NET: Reactive Extensions (Rx)\ndeclarative style used for data collections ( List  and Array , for example). Rx provides a \ndomain-specific language (DSL) that provides a significantly simpler and more fluent API for handling complex, asynchronous event-based logic. Rx can be used to either develop a responsive UI or increase scalability in a server-side application. \nIn the nutshell, Rx is a set of extensions built for the \nIObservable<T>  and IOb-\nserver<T>  interfaces that provide a generalized mechanism for push-based notifica-\ntions based on the Observer pattern from the Gang of Four (GoF) book. \nThe Observer design pattern is based on events, and it’s one of the most common \npatterns in OOP. This pattern publishes changes made to an object’s state (the observ-able) to other objects (the observers) that subscribe to notifications describing any changes to that object (shown in figure 6.5). \nFigure 6.5  The original Observer pattern from the GoF book\nUsing GoF terminology, the IObservable  interfaces are subjects, and the IObserver  \ninterfaces are observers. These interfaces, introduced in .NET 4.0 as part of the System  \nnamespace, are an important component in the reactive programming model. \nThe Gang of Four book\nThis software engineering book by Martin Fowler et al. describes software design pat-terns in OOP. The title, Design Patterns: Elements of Reusable Object-Oriented Soft-\nware, is far too long, especially in an email, so the nickname “book by the Gang of Four” became the common way to refer to it. The authors are often referred to as the Gang of \nFour (GoF).\n \nHere’s the definition for both IObserver  and IObservable  interface signatures in C#:\npublic interface IObserver<T>{    void OnCompleted();    void OnError(Exception exception);    void OnNext(T value);}\n \n158 chapter  6 Real-time event streams: functional reactive programming\npublic interface IObservable<T>{    IDisposable Subscribe(IObserver<T> observer);}\nThese interfaces implement the Observer pattern, which allows Rx to create an \nobservable  from existing .NET CLR events. Figure 6.6 attempts to clarify the original \nUnified Modeling Language (UML) for the Observer pattern from the GoF book.\nNotifySubject:\nSubscriber\nof observerObserver\nObserverObserver\nWhen a notification is sent tothe subject, the obser\nvers that\nare listening retrieve the datathat has changed.The subject registers and unregisters\nthe obser vers, and notifies the obser vers\nwhen the state changes.\n \nFigure 6.6  The Observer pattern is based on an object called Subject , which maintains a list of \ndependencies (called observers) and automatically notifies the observers of any change of state to \nSubject . This pattern defines a one-to-many relationship between the observer subscribers, so that \nwhen an object changes state, all its dependencies are notified and updated automatically. \nThe IObservable<T>  functional interface (www.lambdafaq.org/what-is-a-functional- \ninterface) only implements the method Subscribe . When this method is called \nby an observer, a notification is triggered to publish the new item through the IOb-\nserver<T>.OnNext  method. The IObservable  interface, as the name implies, can be \nconsidered a source of data that’s constantly observed, which automatically notifies all registered observers of any state changes. Similarly, notifications for errors and completion are published through the \nIObserver<T>.OnError  and IObserver<T>  \n.OnCompleted  methods, respectively. The Subscribe  method returns an IDisposable  \nobject, which acts as a handle for the subscribed observer. When the Dispose  method \nis called, the corresponding observer is detached from the Observable , and it stops \nreceiving notifications. In summary:\n¡ IObserver<T>.OnNext  supplies the observer with new data or state information.\n¡ IObserver<T>.OnError  indicates that the provider has experienced an error \ncondition. \n¡ IObserver<T>.OnCompleted  indicates that the observer finished sending notifi-\ncations to observers.",6455
57-6.4 Taming the event stream Twitter emotion analysis using Rx programming.pdf,57-6.4 Taming the event stream Twitter emotion analysis using Rx programming,"159 Reactive programming in .NET: Reactive Extensions (Rx)\nThe same interfaces are used as a base definition for the F# IEvent<'a>  type, which is \nthe interface used to implement the F# event combinators previously discussed. As you can see, the same principles are applied with a slightly different approach to achieve the same design. The ability to code multiple asynchronous event sources is the main advantage of Rx.\nNOTE  .NET Rx can be downloaded using the Install-Package System.Reac -\ntive  command and referenced in your project through the NuGet package \nmanager. \n6.3.1 From LINQ/PLINQ to Rx\nThe .NET LINQ/PLINQ query providers, as discussed in chapter 5, operate as a mech-anism against an in-memory sequence. Conceptually, this mechanism is based on a pull model, which means that the items of the collections are pulled from the query during its evaluation. This behavior is represented by the iterator pattern of \nIEnumer-\nable<T> - IEnumerator<T> , which can cause a block while it’s waiting for data to iter -\nate. In opposition, Rx treats events as a data stream by defining the query to react over time as events arrive. This is a push model, where the events arrive and autonomously travel through the query. Figure 6.7 shows both models. \nPulling\nIEnumerable<T>IEnumerator<T>MoveNext()\nConsumerData\nsourceInteractive\n1. The consumer asks\n   for new data.\n2. The IEnumerable\IEnumerator    pattern pulls data from the source,    which blocks the execution if no\n    data is available.IObservable<T>IObserver<T>\nOnNext()\nConsumerPushingData\nsourceReactive\n1. The source notifies\n   the consumer that   new data is available.2. The IObser vable\IObser ver\n    pattern receives a notification\n    from the source when new    data is available, which is    pushed to the consumer .\n \nFigure 6.7  Push vs. pull model. The IEnumerable/IEnumerator  pattern is based on the pull \nmodel, which asks for new data from the source. Alternatively, the IObservable/IObserver  pattern \nis based on the push model, which receives a notification when new data is available to send to the consumer.\nIn the reactive case, the application is passive and causes no blocking in the data- retrieval process.\n \n160 chapter  6 Real-time event streams: functional reactive programming\nF#: the inspiration for Rx \nDuring an interview on Microsoft’s Channel 9 (http:/ /bit.ly/2v8exjV), Erik Meijer, the mind behind Rx, mentioned that F# was an inspiration for the creation of Reactive Exten -\nsions. One of the inspiring ideas behind the Reactive framework, composable events, does in fact come from F#. \n \n6.3.2 IObservable: the dual IEnumerable\nThe Rx push-based event model is abstracted by the IObservable<T>  interface, which \nis the dual  of the IEnumerable<T>  interface.2 While the term duality can sound daunt-\ning, it’s a simple and powerful concept. You can compare duality to the two sides of a coin, where the opposite side can be inferred from the one exposed.\nIn the context of computer science, this concept has been exploited by De Morgan’s \nLaw,\n3 which achieves the duality between conjunction && (AND) and disjunction || \n(OR) to prove that negation distributes over both conjunction and disjunction: \n!(a || b) == !a && !b !(a && b) == !a || !b \nLike the inverse of LINQ, where LINQ exposes a set of extension methods for the \nIEnumerable  interface to implement a pull-based model over collections, Rx exposes \na set of extension methods for the IObservable  interface to implement a push-based \nmodel over events. Figure 6.8 shows the dual relationship between these interfaces. \ntypeIObserver<'a> = interface  abstractOnNext : 'a with se t\n  abstractOnCompleted : unit->uni t\n  abstractOnError : Exception ->uni t\nendtypeIObservable<'a> = interface  abstractSubscribe : IObserver<'a>typeIEnumerator<'a> = interface   interface IDisposable   interface IEnumerator   abstractCurrent : 'a with ge t\n   abstractMoveNext : unit->boo l\nendtypeIEnumerable<'a> = interface   interface IEnumerable   abstractGetEnumerator : IEnumerator<'a\n>\nend\nFigure 6.8  Dual relationship between the IObserver  and IEnumerator  interfaces, and the \nIObservable  and IEnumerable  interfaces. This dual relationship is obtained by reversing the arrow \nin the functions, which means swapping the input and output.\nAs figure 6.8  shows, the IObservable  and IObserver  interfaces are obtained by \nreversing the arrow of the corresponding IEnumerable  and IEnumerator  interfaces. \n2 The term comes from duality. For further explanation, see https://en.wikipedia.org/wiki/Dual_ \n(category_theory).\n3 For more information, see https:/ /en.wikipedia.org/wiki/De_Morgan%27s_laws.\n \n 161 Reactive programming in .NET: Reactive Extensions (Rx)\nReversing the arrow means swapping the input and output of a method. For example, the current property of the \nIEnumerator  interface has this signature: \nUnit (or void in C#) -> get ‘a\nReversing the arrow of this property, you can obtain its dual: Unit <- set ‘a . This sig-\nnature in the reciprocal IObserver  interface matches the OnNext  method, which has \nthe following signature: \nset ‘a -> Unit (or void in C#)\nThe GetEnumerator  function takes no arguments and returns IEnumerator<T> , which \nreturns the next item in the list through the MoveNext  and Current  functions. The \nreverse IEnumerable  method can be used to traverse the IObservable , which pushes \ndata into the subscribed IObserver  by invoking its methods. \n6.3.3 Reactive Extensions in action\nCombining existing events is an essential characteristic of Rx, which permits a level of abstraction and compositionality that’s otherwise impossible to achieve. In .NET, events are one form of an asynchronous data source that can be consumed by Rx. To convert existing events into observables, Rx takes an event and returns an \nEvent-\nPattern  object, which contains the sender and event arguments. For example, a key-\npressed  event is converted into a reactive observable (in bold): \nObservable.FromEventPattern<KeyPressedEventArgs>(this.textBox,                                             nameof(this.textBox.KeyPress));\nAs you can see, Rx lets you handle events in a rich and reusable form.\nLet’s put the Rx framework into action by implementing the C# equivalent of the \nsecret word game previously defined using the F# event combinators KeyPressed-\nEventCombinators.  This listing shows the implementation using this pattern and the \ncorresponding reactive framework. \nListing 6.2  Rx KeyPressedEventCombinators  in C#\nvar timer = new System.Timers.Timer(timerInterval);var timerElapsed = Observable.FromEventPattern<ElapsedEventArgs>                    (timer, ""Elapsed"").Select(_ => 'X'); var keyPressed = Observable.FromEventPattern<KeyPressEventArgs>                    (this.textBox, nameof(this.textBox.KeyPress));                    .Select(kd => Char.ToLower(kd.EventArgs.KeyChar))                    .Where(c => Char.IsLetter(c));       timer.Start();timerElapsed    .Merge(keyPressed)      .Scan(String.Empty, (acc, c) =>      {\nThe Rx method FromEventPattern converts a .NET event into an observable.The LINQ-like semantic functions Select and Where register, \ncompose, and transform events to notify subscribers. \nThe filters are merged to be handled as a whole. When either event is triggered, this event fires a notification.\nThe Scan function maintains the internal state of the keys pressed and pushes the result from every call to the accumulator function. \n \n162 chapter  6 Real-time event streams: functional reactive programming\n        if (c == 'X') return ""Game Over"";        else        {            var word = acc + c;            if (word == secretWord) return ""You Won!"";            else return word;        }       }).     .Subscribe(value =>        this.label.BeginInvoke(            (Action)(() => this.label.Text = value)));\nThe Observable.FromEventPattern  method creates a link between the .NET event \nand the Rx IObservable , which wraps both Sender  and EventArgs . In the listing, \nthe imperative C# events for handling the key pressed ( KeyPressEventArgs ) and the \nelapsed timer ( ElapsedEventArgs ) are transformed into observables and then merged \nto be treated as a whole stream of events. Now it’s possible to construct all of the event handling as a single and concise chain of expressions.\nReactive Extensions are functional \nThe Rx framework provides a functional approach to handling events asynchronously as a stream. The functional aspect refers to a declarative programming style that uses fewer variables to maintain state and to avoid mutations, so you can compose events as a chain of expressions. \n \n6.3.4 Real-time streaming with RX\nAn event stream is a channel on which a sequence of ongoing events, by order of time, \narrives as values. Streams of events come from diverse sources, such as social media, the stock market, smartphones, or a computer mouse. Real-time stream processing aims to consume a live data stream that can be shaped into other forms. Consuming this data, which in many cases is delivered at a high rate, can be overwhelming, like drinking directly from a fire hose. Take, for example, the analysis of stock prices that continually change and then dispatching the result to multiple consumers, as shown in figure 6.9. \nThe Rx framework fits well in this scenario because it handles multiple asynchronous \ndata sources while delivering high-performance operations to combine, transform, and filter any of those data streams. At its core, Rx uses the \nIObservable<T>  interface to \nmaintain a list of dependent IObserver<T>  interfaces that are notified automatically of \nany event or data change. \n \n 163 Reactive programming in .NET: Reactive Extensions (Rx)\nThrottle Merge\nAnalyzeEvent transformer\nMap Map\nHigh-rate\neventsFilter by\nprice change\nFilter by\nstock volume\nGroup by\nstock symbol\nMap\nDistinct\nFigure 6.9  Event streams from different sources push data to an event transformer, which applies \nhigher-order operations and then notifies the subscribed observers. \n6.3.5 From events to F# observables\nAs you may recall, F# uses events for configurable callback constructs. In addition, it supports an alternative and more advanced mechanism for configurable callbacks that are more compositional than events. The F# language treats .NET events as values of type \nIEvent<'T> , which inherits from the interface IObservable<'T> , the same type \nused by Rx. For this reason, the main F# assembly, FSharp.Core , already provides an \nObservable module that exposes a set of useful functions over the values of the IOb-\nservable interface. This is considered a subset of Rx.\nFor example, in the following code snippet, the F# observables (in bold) are used \nto handle keypress and timer events from the KeyPressedEventCombinators example \n(listing 6.2):\nlet timeElapsed = timer.Elapsed |> Observable.map(fun _ -> 'X')   let keyPressed = control.KeyPress                       |> Observable.filter(fun c -> Char.IsLetter c)                        |> Observable.map(fun kd -> Char.ToLower kd.KeyChar)  let disposable = keyPressed|> Observable.merge timeElapsed|> Observable.scan(fun acc c ->\n \n164 chapter  6 Real-time event streams: functional reactive programming\n         if c = 'X' then ""Game Over""         else             let word = sprintf ""%s%c"" acc c             if word = secretWord then ""You Won!""             else word) String.Empty|> Observable.subscribe(fun text -> printfn “%s” text)\nIt’s possible to choose (and use) either Observable  or Event  when using F# to build \nreactive systems; but to avoid memory leaks, the preferred choice is Observable . When \nusing the F# Event  module, composed events are attached to the original event, and \nthey don’t have an unsubscribe mechanism that can lead to memory leaks. Instead, the \nObservable  module provides the subscribe  operator to register a callback function. \nThis operator returns an IDisposable  object that can be used to stop event-stream \nprocessing and to de-register all subscribed observable (or event) handlers in the pipe-line with one call of the \nDispose  method.\n6.4 Taming the event stream: Twitter emotion analysis using Rx programming \nIn this age of digital information where billions of devices are connected to the inter -\nnet, programs must correlate, merge, filter, and run real-time analytics. The speed of processing data has moved into the realm of real-time analytics, reducing latency to vir -\ntually zero when accessing information. Reactive programming is a superb approach for handling high-performance requirements because it’s concurrency friendly and scalable, and it provides a composable asynchronous data-processing semantic.\nIt’s estimated that in the United States there is an average of 24 million tweets per \nhour, amounting to almost 7,000 messages per second. This is a massive quantity of data to evaluate, and presents a serious challenge for consuming such a high-traffic stream. Consequently, a system should be designed to tame the occurrence of backpressure. This backpressure, for example, in the case of Twitter could be generated by a con-sumer of the live stream of data that can’t cope with the rate at which the producers emit events. \nBackpressure \nBackpressure occurs when a computer system can’t process the incoming data fast \nenough, so it starts to buffer the arriving data until the space to buffer it is reduced to the point of deteriorating the responsiveness of the system or, worse, raising an “Out Of Memory” exception. In the case of iterating over the items in an \nIEnumerable , the con -\nsumer of the items is “pulling”; the items are processed at a controlled pace. With IOb-\nservable , the items are “pushed” to the consumer. In this case, IObservable  could \npotentially produce values more rapidly than the subscribed observers can handle. This scenario generates excessive backpressure, causing strain on the system. To ease back -\npressure, Rx provides operators such as \nThrottle  and Buffer .\n \n \n 165 Taming the event stream: Twitter emotion analysis using Rx programming \nThe F# example in figure 6.10 illustrates a real-time analysis stream for determining the current feeling (emotion) of tweets published in the United States.\nThrottle FilterGroup\nbyEvent transformer\nSentiment\nanalysis\nTwitter messages\nEvent streamObserver\nObserver\nObserver\nFigure 6.10  The Twitter messages push a high-rate event stream to the consumer, so it’s important to \nhave tools like Rx to tame the continuous burst of notifications. First, the stream is throttled, then the messages are filtered, analyzed, and grouped by emotions. The result is a data stream from the incoming tweets that represents the latest status of emotions, whose values constantly update a chart and notify the subscribed observers.\nThis example uses F# to demonstrate the existing built-in support for observables, which is missing in C#. But the same functionality can be reproduced in C#, either using.NET Rx or by referencing and consuming an F# library, where the code exposes the implemented observable.\nThe analysis of the stream of tweets is performed by consuming and extracting the \ninformation from each message. Emotional analysis is performed using the Stanford CoreNLP library.  The result of this analysis is sent to a live animated chart that takes \nIObservable  as input and automatically updates the graph as the data changes. \nStanford CoreNLP\nThe Stanford CoreNLP (http:/ /nlp.stanford.edu ) is a natural-language analysis library \nwritten in Java, but it can be integrated in .NET using the IKVM bridge (www.ikvm.net). This library has several tools, including emotion analysis tools that predict the emo -\ntion of a sentence. You can install the Stanford CoreNLP library using the NuGet pack -\nage Install-Package Stanford.NLP.CoreNLP  (www.nuget.org/packages/Stanford.NLP.\nCoreNLP ), which also configures the IKVM bridge. For more details regarding how the \nCoreNLP library works, I recommend the online material.\n \n \n166 chapter  6 Real-time event streams: functional reactive programming\nThe following listing shows the emotion analysis function and the settings to enable the Stanford CoreNLP library. \nListing 6.3  Evaluating a sentence’s emotion using the CoreNLP library\nlet properties = Properties()properties.setProperty(""annotators"", ""tokenize,ssplit,pos,parse,emotion"")\n➥ |> ignore\nIO.Directory.SetCurrentDirectory(jarDirectory)let stanfordNLP = StanfordCoreNLP(properties) type Emotion =    | Unhappy    | Indifferent    | Happy let getEmotionMeaning value =    match value with    | 0 | 1 -> Unhappy    | 2 -> Indifferent    | 3 | 4 -> Happy let evaluateEmotion (text:string) =    let annotation = Annotation(text)    stanfordNLP.annotate(annotation)    let emotions =        let emotionAnnotationClassName =                 SentimentCoreAnnotations.SentimentAnnotatedTree().getClass()        let sentences = annotation.get(CoreAnnotations.SentencesAnnotation().\ngetClass()) \n➥ :?> java.util.ArrayList\n        [ for s in sentences ->            let sentence = s :?> Annotation            let sentenceTree = sentence.get(emotionAnnotationClassName) \n➥ :?> Tree\n            let emotion = NNCoreAnnotations.getPredictedClass(sentenceTree)            getEmotionMeaning emotion]    (emotions.[0]) \nIn the code, the F# DU defines different emotion levels (case values): Unhappy , Indif-\nferent , and Happy . These case values compute the distribution percentage among the \ntweets. The function evaluateEmotion  combines the text analysis from the Stanford \nlibrary and returns the resulting case value (emotion). \nTo retrieve the stream of tweets, I used the Tweetinvi library (https:/ /github.com/\nlinvi/tweetinvi). It provides a well-documented API and, more importantly, it’s designed to run streams concurrently while managing multithreaded scenarios. You can down-load and install this library from the NuGet package \nTweetinviAPI .\nNOTE  Twitter provides great support to developers who build applications \nusing its API. All that’s required is a Twitter account and an Application Sets the properties and creates an instance of StanfordCoreNLP\nA discriminated union categorizes emotions for each text message.\nGives a value from 0 to 4 to determine the emotion\nAnalyzes a text message, providing \nthe associated emotion\n \n 167 Taming the event stream: Twitter emotion analysis using Rx programming \nManagement (https:/ /apps.twitter.com) account to obtain the key and secret access. With this information, it’s possible to send and receive tweets and inter -\nact with the Twitter API.\nThis listing shows how to create an instance for the Tweetinvi library and how to access the settings to enable interaction with Twitter. \nListing 6.4  Settings to enable the Twitterinvi library \nlet consumerKey = ""<your Key>""let consumerSecretKey = ""<your secret key>""let accessToken = ""<your access token>""let accessTokenSecret = ""<your secret access token>""let cred = new TwitterCredentials(consumerKey, consumerSecretKey,\n➥ accessToken, accessTokenSecret)\nlet stream = Stream.CreateSampleStream(cred)stream.FilterLevel <- StreamFilterLevel.Low\nThis straightforward code creates an instance of the Twitter stream. The core of the Rx programming is in the following listing (highlighted in bold), where Rx and the F# \nObservable  module are used in combination to handle and analyze the event stream.\nListing 6.5  Observable pipeline to analyze tweets \nlet emotionMap =    [(Unhappy, 0)     (Indifferent, 0)     (Happy, 0)] |> Map.ofSeqlet observableTweets =    stream.TweetReceived     |> Observable.throttle(TimeSpan.FromMilliseconds(100.))     |> Observable.filter(fun args ->        args.Tweet.Language = Language.English)     |> Observable.groupBy(fun args ->        evaluateEmotion args.Tweet.FullText)     |> Observable.selectMany(fun args ->        args |> Observable.map(fun i ->            (args.Key, (max 1 i.Tweet.FavoriteCount))))     |> Observable.scan(fun sm (key,count) ->        match sm |> Map.tryFind key with        | Some(v) -> sm |> Map.add key (v + count)        | None    -> sm ) emotionMap     |> Observable.map(fun sm ->        let total = sm |> Seq.sumBy(fun v -> v.Value)         sm |> Seq.map(fun k ->            let percentageEmotion = ((float k.Value) * 100.) \n➥ / (float total)\n            let labelText = sprintf ""%A - %.2f.%%"" (k.Key) \n➥ percentageEmotion\n            (labelText, percentageEmotion)        ))Generates the event stream from the Twitter API\nControls the rate of events to avoid \noverwhelming the consumer \nFilters the incoming messages to target only those in English \nPartitions the message by emotion analysis \nFlattens messages into one sequence of emotions with the count of favorites \nMaintains the state of the total partition of messages by emotion\nCalculates the total percentage of emotions and \nreturns an observable to live update a chart \n \n168 chapter  6 Real-time event streams: functional reactive programming\nThe result of the observableTweets  pipeline is an IDisposable , which is used to stop \nlistening to the tweets and remove the subscription from the subscribed observable. Tweetinvi exposes the event handler \nTweetReceived , which notifies the subscribers \nwhen a new tweet has arrived. The observables are combined as a chain to form the \nobservableTweets  pipeline. Each step returns a new observable that listens to the \noriginal observable and then triggers the resulting event from the given function.\nThe first step in the observable channel is managing the backpressure, which is a \nresult of the high rate of arriving events. When writing Rx code, be aware that it’s possi-ble for the process to be overwhelmed when the event stream comes in too quickly. \nIn figure 6.11, the system on the left has no problem processing the incoming event \nstreams because the frequency of notifications over time has a sustainable throughput (desired flow). The system on the right struggles to keep up with a huge number of noti-\nfications (backpressure) that it receives over time, which could potentially collapse the system. In this case, the system responds by throttling the event streams to avoid failure. The result is a different rate of notifications between the observable and an observer.\nFilter\nMapDesired flow\nTimeFilter\nMapThrottleBackpressure\nTime\nFigure 6.11  Backpressure could negatively affect the responsiveness of a system, but it’s possible to \nreduce the rate of the incoming events and keep the system healthy by using the throttle  function to \nmanage the different rates between an observable and an observer.\nTo avoid the problem of backpressure, the throttle  function provides a layer of pro-\ntection that controls the rate of messages, preventing them from flowing too quickly:\nstream.TweetReceived    |> Observable.throttle(TimeSpan.FromMilliseconds(50.))\nThe throttle  function reduces a rapid fire of data down to a subset, corresponding to \na specific cadence (rhythm) as shown in figures 6.9 and 6.10. Throttle  extracts the last \nvalue from a burst of data in an observable sequence by ignoring any value that’s fol-lowed by another value in less than a time period specified. In listing 6.5, the frequency of event propagation was throttled to no more than once every 50 ms. \n \n 169 Taming the event stream: Twitter emotion analysis using Rx programming \nThrottle and buffer Rx operators for taming large volumes of events\nBe aware that the throttle  function can have destructive effects, which means the \nsignals that arrive with a higher rate than the given frequency are lost, not buffered. This happens because the \nthrottle  function discharges the signals from an observable \nsequence that’s followed by another signal before the given time expires. The throttle  \noperator is also called debounce , which stops messages from flowing in at a higher rate \nby setting an interval between messages. The \nbuffer  function is useful in cases where it’s too expensive to process one signal at \na time, and consequently it’s preferred for processing the signals in batches, at the cost of accepting a delay. There’s an issue to consider when using \nbuffer  with large-volume \nevents. In large-volume events, the signals are stored in memory for a period of time and the system could run into memory overflow problems. The purpose of the \nbuffer  opera -\ntor is to stash away a specified series of signals and then republish them once either the given time has expired or the buffer is full. Here, for example, the code in C# gets all the events that happened either every second or every 50 signals depending on which rule is satisfied first.\nObservable.Buffer(TimeSpan.FromSeconds(1), 50)\nIn the example of the tweet emotion analysis, the Buffer  extension method can be \napplied as follows:\nstream.TweetReceived.Buffer(TimeSpan.FromSeconds(1), 50)\n \nThe next step in the pipeline is filtering events that aren’t relevant (the command is in bold):\n|> Observable.filter(fun args -> args.Tweet.Language = Language.English)\nThis filter  function ensures that only the tweets that originate using the English lan-\nguage are processed. The Tweet  object, from the tweet message, has a series of proper -\nties, including the sender of the message, the hashtag, and the coordinates (location) that can be accessed.\nNext, the Rx \ngroupBy  operator provides the ability to partition the sequence into a \nseries of observable groups related to a selector function. Each of these sub-observables corresponds to a unique key value, containing all the elements that share that same key value the way it does in LINQ and in SQL: \n    |> Observable.groupBy(fun args -> evaluateEmotion args.Tweet.FullText)    |> Observable.selectMany(fun args -> args |> Observable.map(fun i -> \n(args.Key, i.Tweet.FavoriteCount)))\nIn this case, the key-value emotion partitions the event stream. The function evaluate-\nEmotion , which behaves as a group selector, computes and classifies the emotion for \neach incoming message. Each nested observable can have its own unique operation; the \nselectMany  operator is used to further subscribe these groups of observables by \nflattening them into one. Then, using the map function, the sequence is transformed \n \n170 chapter  6 Real-time event streams: functional reactive programming\ninto a new sequence of pairs (tuple) consisting of the Tweet-Emotion  value and the \ncount of how many times the tweet has been liked (or favored).\nAfter having been partitioned and analyzed, the data must be aggregated into a \nmeaningful format. The observable scan  function does this by pushing the result of \neach call to the accumulator  function. The returned observable will trigger notifica-\ntions for each computed state value, as shown in figure 6.12. \nAggregate (0, (x,y) => x + y)1 2 3 4\n10Scan(0, (x,y) => x + y)1 2 3 4\n0 1 3 6\nFigure 6.12  The aggregate  function returns a single value that is the accumulation of each value \nfrom running the given function ( x,y) against the initial accumulator 0. The scan  function returns a \nvalue for each item in the collection, which is the result of performing the given function against the accumulator in the current iteration.\nThe scan  function is like fold , or the LINQ Aggregate , but instead of returning a \nsingle value, it returns the intermediate evaluations resulting from each iteration (as shown in bold in the following code snippet). Moreover, it satisfies the functional par -\nadigm, maintaining state in an immutable fashion. The aggregate functions (such as \nscan  and fold)  are described as the generic concept of catamorphism (https:/ /wiki.\nhaskell.org/Catamorphisms) in FP:\n< code here that passes an Observable of tweets with emotions analysis >    |> Observable.scan(fun sm (key,count) ->                                  match sm |> Map.tryFind key with            | Some(v) -> sm |> Map.add key (v + count)            | None -> sm) emotionMap \nThis function scan  takes three arguments: an observable that’s passed conceptually in \nthe form of stream tweets with emotion analysis, an anonymous function to apply the underlying values of the observable to the accumulator, and an accumulator \nemotion-\nMap. The result of the scan  function is an updated accumulator that’s injected into the \nfollowing iteration. The initial accumulator state in the previous code is used by the \nscan  function in an empty F# Map, which is equivalent to an immutable .NET generic \nDictionary (System.Collections.Generic.Dictionary<K,V> ), where the key is \none of the emotions and the value is the count of its related tweets. The accumulator function \nscan  updates the entries of the collection with the new evaluated types and \nreturns the updated collection as new accumulator.\nThe last operation in the pipeline is to run the map function used to transform the \nobservables of the source into the representation of the total percentage of tweets ana-lyzed by emotions:\n|> Observable.map(fun sm ->    let total = sm |> Seq.sumBy(fun v -> v.Value)    sm |> Seq.map(fun k ->",29398
58-6.4.1 SelectMany the monadic bind operator.pdf,58-6.4.1 SelectMany the monadic bind operator,"171 Taming the event stream: Twitter emotion analysis using Rx programming \n        let percentageEmotion = ((float k.Value) * 100.) / (float total)        let labelText = sprintf ""%A - %.2f.%%"" (k.Key) percentageEmotion        (labelText, percentageEmotion)    ))\nThe transformation function is executed once for each subscribed observer. The map \nfunction calculates the total number of tweets from the observable passed, which con-tains the value of the accumulator from the previous \nscan  function:\nsm |> Seq.sumBy(fun v -> v.Value)\nThe result is returned in a format that represents the percentage of each emotion from the map table received so far. The final observable is passed into a \nLiveChart , which \nrenders the real-time updates. Now that the code is developed, you can use the Start-\nStreamAsync()  function to start the process of listening and receiving the tweets and \nhave the observable notify subscribers:\nLiveChart.Column(observableTweets,Name= sprintf ""Tweet Emotions"").ShowChart()do stream.StartStreamAsync()  \nCold and hot observables \nObservables come in two flavors: hot and cold. A hot observable represents a stream \nof data that pushes notifications regardless of whether there are any subscribers. For example, the stream of tweets is a hot stream of data because the data will keep flowing regardless of the status of subscribers. A cold observable is a stream of events that will always push notifications from the beginning of the stream, regardless of whether the subscribers start listening after the event is pushed.\n \nMuch like the Event  module in F#, the Observable  module defines a set of combina-\ntors for using the IObservable<T>  interface. The F# Observable  module includes add, \nfilter , map, partition , merge , choose , and scan . For more details, see appendix B. \nIn the previous example, the observable functions groupBy  and selectMany  are part \nof the Rx framework. This illustrates the utility that F# provides, providing the devel-oper options to mix and match tools to customize the best fit for the task.\n6.4.1 SelectMany: the monadic bind operator\nSelectMany  is a powerful operator that corresponds to the bind  (or flatMap)  operator \nin other programming languages. This operator constructs one monadic value from another and has the generic monadic binding signature \nM a -> (a -> M b) -> M b  \nwhere M represents any elevated type that behaves as a container. In the case of observ-\nables, it has this signature: \nIObservable<'T> -> ('T -> IObservable<'R>) -> IObservable<'R>\n \n172 chapter  6 Real-time event streams: functional reactive programming\nIn .NET, there are several types that match this signature, such as IObservable , IEnu-\nmerable , and Task . Monads (http:/ /bit.ly/2vDusZa), despite their reputation for com-\nplexity, can be thought of in simple terms: they are containers that encapsulate and abstract a given functionality with the objective of promoting composition between elevated types and avoiding side effects. Basically, when working with monads, you can think of working with boxes (containers) that are unpacked at the last moment—when they’re needed.\nThe main purpose of monadic computation is to make composition possible where \nit couldn’t be achieved otherwise. For example, by using monads in C#, it’s possible to directly sum an integer and a \nTask  type from the System.Threading.Tasks  namespace \nof integer ( Task<int>) (highlighted in bold):\nTask<int> result = from task in Task.Run<int>(() => 40)                   select task + 2;\nThe bind , or SelectMany , operation takes an elevated type and applies a function to its \nunderlying value, returning another elevated type. An elevated type is a wrapper around another type, like \nIEnumerable<int> , Nullable<bool> , or IObservable<Tweets> . \nThe meaning of bind  depends on the monad type. For IObservable , each event in the \nobservables input is evaluated to create a new observable. The resulting observables are then flattened to produce the output observable, as shown in figure 6.13.\n40 40\nWrapper+ 2 = 4240 + 2Apply\nfunction\nUnwrap\nvalueRewrap\nvalue\nFigure 6.13  An elevated type can be considered a special container where it’s possible to apply a \nfunction directly to the underlying type (in this case, 40). The elevated type works like a wrapper that contains a value, which can be extracted to apply a given function, after which the result is put back into the container. \nThe SelectMany  binder not only flattens data values but, as an operator, it also trans-\nforms and then flattens the nested monadic values. The underlying theory of monads is used by LINQ, which is used by the .NET compiler to interpret the \nSelectMany  pat-\ntern to apply the monadic behavior. For example, by implementing the SelectMany  \nextension method over the Task  type (as highlighted in bold in the following code \nsnippet), the compiler recognizes the pattern and interprets it as the monadic bind-ing, allowing the special composition: \nTask<R> SelectMany<T, R>(this Task<T> source, Func<T, Task<R>> selector) =>                source.ContinueWith(t => selector(t.Result)).Unwrap();",5194
59-6.5.1 Using the Subject type for a powerful publisher-subscriber hub.pdf,59-6.5.1 Using the Subject type for a powerful publisher-subscriber hub,,0
60-6.5.3 Implementing a reusable Rx publisher-subscriber.pdf,60-6.5.3 Implementing a reusable Rx publisher-subscriber,"173 An Rx publisher-subscriber\nWith this method in place, the previous LINQ-based code will compile and evaluate to a \nTask<int> that returns 42. Monads play an import role in functional concurrency \nand are covered more thoroughly in chapter 7.\n6.5 An Rx publisher-subscriber\nThe Publish/Subscribe pattern allows any number of publishers to communicate with any number of subscribers asynchronously via an event channel. In general, to accom-plish this communication, an intermediary hub is employed to receive the notifications, which are then forwarded to subscribers. Using Rx, it becomes possible to effectively define a Publish/Subscribe pattern by using the built-in tools and concurrency model.\nThe \nSubject  type is a perfect candidate for this implementation. It implements the \nISubject  interface, which is the combination of IObservable  and IObserver . This \nmakes the Subject  behave as both an observer and an observable, which allows it to \noperate like a broker to intercept notifications as an observer and to broadcast these notifications to all its observers. Think of the \nIObserver  and the IObservable  as con-\nsumer and publisher interfaces, respectively, as shown in figure 6.14. \nSubscriber\nPublisherPublisher\n(ButtonClicked)\nPublisher\n(KeyPressed)Publisher\n(MouseMoved)\nSubscriber\n(ButtonHandler)\nSubscriber\n(KeyHandler)Subscriber\n(MouseHandler)\nFigure 6.14  The publisher-subscriber hub manages the communication between any number of \nsubscribers (observers) with any number of publishers (observables). The hub, also known as a broker, receives the notifications from the publishers, which are then forwarded to the subscribers.\nUsing the Subject  type from Rx to represent a Publish/Subscribe pattern has the \nadvantage of giving you the control to inject extra logic, such as merge  and filter , \ninto the notification before it’s published.\n6.5.1 Using the Subject type for a powerful publisher-subscriber hub\nSubject s are the components of Rx, and their intention is to synchronize the values \nproduced by an observable and the observers that consume them. Subject s don’t \ncompletely embrace the functional paradigm because they maintain or manage states that could potentially mutate. Despite this fact, however, they’re useful for creating an event-like observable as a field, which is a perfect fit for a Publish/Subscribe pattern implementation. \n \n174 chapter  6 Real-time event streams: functional reactive programming\nThe Subject  type implements the ISubject  interface (highlighted in bold in the \nfollowing code snippet), which resides in the System.Reactive.Subjects  namespace \ninterface ISubject<T, R> : IObserver<T>, IObservable<R> {  }\nor ISubject<T> , if the source and result are of the same type.\nBecause a Subject<T>  and, consequently, ISubject<T>  are observers, they expose \nthe OnNext , OnCompleted , and OnError  methods. Therefore, when they’re called, the \nsame methods are called on all the subscribed observers.\nRx out of the box has different implementations of the Subject  class, each with a \ndiverse behavior. In addition, if the existing Subject s don’t satisfy your needs, then \nyou can implement your own. The only requirement to implementing a custom subject class is satisfying the \nISubject  interface implementation.\nHere are the other Subject  variants: \n¡ ReplaySubject  behaves like a normal Subject , but it stores all the messages \nreceived, providing the ability to make the messages available for current and future subscribers.\n¡ BehaviorSubject  always saves the latest available value, which makes it available \nfor future subscribers. \n¡ AsyncSubject  represents an asynchronous operation that routes only the last \nnotification received while waiting for the OnComplete  message.\nNOTE  The Subject  type is hot, which makes it vulnerable to losing notifica-\ntion messages pushed from the source observable when there are no listening observers. To offset this, carefully consider the type of \nSubject  to use, specifi-\ncally if all the messages prior to the subscription are required. An example of a hot observable is a mouse movement, where movements still happen and notifications are emitted regardless of whether there are listening observers.\n6.5.2 Rx in relation to concurrency\nThe Rx framework is based on a push model with support for multithreading. But it’s important to remember that Rx is single-threaded by default, and the paral-lel constructs that let you combine asynchronous sources must be enabled using Rx schedulers. \nOne of the main reasons to introduce concurrency in Rx programming is to facilitate \nand manage offloading the payload for an event stream. This allows a set of concurrent tasks to be performed, such as maintaining a responsive UI, to free the current thread. \nNOTE  Reactive Extensions allow you to combine asynchronous sources using \nparallel computations. These asynchronous sources could be potentially gen-erated independently from parallel computations. Rx handles the complexity involved in compositing these sources and lets you focus on their composition aspect in a declarative style.\n \n 175 An Rx publisher-subscriber\nMoreover, Rx lets you control the flow of incoming messages as specific threads to achieve high-concurrency computation. Rx is a system for querying event streams asyn-chronously, which requires a level of concurrency control.\nWhen multithreading is enabled, Rx programming increases the use of computing \nresources on multicore hardware, which improves the performance of computations. In this case, it’s possible for different messages to arrive from different execution con-texts simultaneously. In fact, several asynchronous sources could be the output from separate and parallel computations, merging into the same \nObservable  pipeline. In \nother words, observables and observers deal with asynchronous operations against a sequence of values in a push model. Ultimately, Rx handles all the complexity involved in managing access to these notifications and avoiding common concurrency problems as if they were running in a single thread. \nUsing a \nSubject  type (or any other observables from Rx), the code isn’t converted \nautomatically to run faster or concurrently. As a default, the operation to push the mes-sages to multiple subscribers by a \nSubject  is executed in the same thread. Moreover, \nthe notification messages are sent to all subscribers sequentially following their sub-scription order and possibly blocking the operation until it completes. \nThe Rx framework solves this limitation by exposing the \nObserveOn  and SubscribeOn  \nmethods, which lets you register a Scheduler  to handle concurrency. Rx schedulers \nare designed to generate and process events concurrently, increasing responsiveness and scalability while reducing complexity. They provide an abstraction over the con-currency model, which lets you perform operations against a stream of data moving without the need to be exposed directly to the underlying concurrent implementation. Moreover, Rx schedulers integrate support for task cancellation, error handling, and passing of state. All Rx schedulers implement the \nIScheduler  interface, which can be \nfound in the System.Reactive.Concurrency  namespace.    \nNOTE  The recommended built-in schedulers for .NET Frameworks after .NET \n4.0 are either TaskPoolScheduler  or ThreadPoolScheduler .\nThe SubscribeOn  method determines which Scheduler  to enable to queue messages \nthat run on a different thread. The ObserveOn  method determines which thread the \ncallback function will be run in. This method targets the Scheduler  that handles \noutput messages and UI programming (for example, to update a WPF interface). \nObserveOn  is primarily used for UI programming and Synchronization-Context \n(http:/ /bit.ly/2wiVBxu )interaction. \nIn the case of UI programming, both the SubscribeOn  and ObserveOn  operators can \nbe combined to better control which thread will run in each step of your observable pipeline.\n6.5.3 Implementing a reusable Rx publisher-subscriber \nArmed with the knowledge of Rx and the Subject  classes, it’s much easier to define \na reusable generic Pub-Sub  object that combines publication and subscription into \n \n176 chapter  6 Real-time event streams: functional reactive programming\nthe same source. In this section, you’ll first build a concurrent publisher-subscriber hub using the \nSubject  type in Rx. Then you’ll refactor the previous Twitter emotion \nanalyzer code example to exploit the new and simpler functionality provided by the Rx-based publisher-subscriber hub. \nThe implementation of the reactive publisher-subscriber hub uses a \nSubject  to \nsubscribe and then route values to the observers, allowing multicasting notifications emitted by the sources to the observers. This listing shows the implementation of the \nRxPubSub  class, which uses Rx to build the generic Pub-Sub  object. \nListing 6.6  Reactive publisher-subscriber in C#\npublic class RxPubSub<T> : IDisposable{    private ISubject<T> subject;     private List<IObserver<T>> observers = new List<IObserver<T>>();     private List<IDisposable> observables = new List<IDisposable>();     public RxPubSub(ISubject<T> subject)    {        this.subject = subject;     }    public RxPubSub() : this(new Subject<T>()) { }     public IDisposable Subscribe(IObserver<T> observer)    {        observers.Add(observer);        subject.Subscribe(observer);        return new Subscription<T>(observer, observers);     }    public IDisposable AddPublisher(IObservable<T> observable) =>            observable.SubscribeOn(TaskPoolScheduler.Default).Subscribe(subject);     public IObservable<T> AsObservable() => subject.AsObservable();     public void Dispose()    {        observers.ForEach(x => x.OnCompleted());        observers.Clear();     }}class ObserverHandler<T> : IDisposable {    private IObserver<T> observer;    private List<IObserver<T>> observers;    public ObserverHandler(IObserver<T> observer, \n➥ List<IObserver<T>> observers)\n    {The private subject notifies all registered observers when a change of the observables’  state is published.Shows the internal state of observers \nShows the internal \nstate of observables \nThe constructor creates an instance of the internal subject.\nThe Subscribe method registers \nthe observer to be notified and \nreturns an IDisposable to \nremove the observer.\nAddPublisher subscribes the observable using the default \nTaskPoolScheduler to handle concurrent notifications.\nExposes IObservable<T> from the internal ISubject to apply \nhigher-order operations against event notifications\nRemoves all subscribers when the object is disposed\nThe internal class ObserverHandler wraps an IObserver to produce an IDisposable object used to stop the notification flow and to remove it from the observers collection.",10962
61-6.5.4 Analyzing tweet emotions using an Rx Pub-Sub class.pdf,61-6.5.4 Analyzing tweet emotions using an Rx Pub-Sub class,"177 An Rx publisher-subscriber\n        this.observer = observer;        this.observers = observers;    }    public void Dispose()     {        observer.OnCompleted();        observers.Remove(observer);    }}\nAn instance of the RxPubSub  class can be defined either by a constructor that speci-\nfies a Subject  version or by the primary constructor that instantiated and passed the \ndefault Subject  from the primary constructor. In addition to the private Subject  field, \nthere are two private collection fields: the observers  collection and the subscribed \nobservables . \nFirst, the observers  collection maintains the state of the observers subscribed to \nthe Subject  through a new instance of the class Subscription . This class provides the \nunsubscribe method Dispose  through the interface IDisposable , which then removes \nthe specific observer when called. \nThe second private collection is observables . Observables maintain a list of IDis-\nposable  interfaces, which originated from the registration of each observable by the \nAddPublisher  method. Each observable can then be unregistered using the exposed \nDispose  method.\nIn this implementation, the Subject  is subscribed to the TaskPoolScheduler  \nscheduler:\nobservable.SubscribeOn(TaskPoolScheduler.Default)\nTaskPoolScheduler  schedules the units of work for each observer to run in a different \nthread using the current provided TaskFactory ( http://bit.ly/2vaemTA) . You can \neasily modify the code to accept any arbitrary scheduler. \nThe subscribed observables from the internal Subject  are exposed through the \nIObservable  interface, obtained by calling the method AsObservable . This property is \nused to apply high-order operations against event notifications: \npublic IObservable<T> AsObservable() => subject.AsObservable(); \nThe reason to expose the IObservable  interface on the Subject  is to guarantee that \nno one can perform an upper cast back to an ISubject  and mess things up. Subject s \nare stateful components, so it’s good practice to isolate access to them through encap-sulation; otherwise, \nSubject s could be reinitialized or updated directly.\n6.5.4 Analyzing tweet emotions using an Rx Pub-Sub class\nIn listing 6.7, you’ll use the C# Reactive Pub-Sub  class (RxPubSub)  to handle a stream of \ntweet emotions. The listing is another example of how simple it is to make the two pro-gramming languages C# and F# interoperable and allow them to coexist in the same solution. From the F# library implemented in section 6.4, the observable that pushes \na stream of tweet emotions is exposed so it’s easily subscribed to by external observers. (The observable commands are in bold.)\n \n178 chapter  6 Real-time event streams: functional reactive programming\nListing 6.7  Implementing observable tweet emotions \nlet tweetEmotionObservable(throttle:TimeSpan) =        Observable.Create(fun (observer:IObserver<_>) ->         let cred = new TwitterCredentials(consumerKey, consumerSecretKey, \n➥ accessToken, accessTokenSecret)\n        let stream = Stream.CreateSampleStream(cred)        stream.FilterLevel <- StreamFilterLevel.Low        stream.StartStreamAsync() |> ignore        stream.TweetReceived        |> Observable.throttle(throttle)        |> Observable.filter(fun args ->            args.Tweet.Language = Language.English)        |> Observable.groupBy(fun args ->            evaluateEmotion args.Tweet.FullText)        |> Observable.selectMany(fun args ->            args |> Observable.map(fun tw ->                                TweetEmotion.Create tw.Tweet args.Key))        |> Observable.subscribe(observer.OnNext)     )\nThe listing shows the implementation of tweetEmotionObservable  using the observ-\nable Create  factory operator. This operator accepts a function with an observer as its \nparameter, where the function behaves as an observable by calling its methods. \nThe Observable.Create  operator registers the observer passed into the function and \nstarts to push notifications as they arrive. The observable is defined from the subscribe  \nmethod, which pushes the notifications to the observer  calling the method OnNext . \nThe following listing shows the equivalent C# implementation of tweetEmotion-\nObservable  (in bold).\nListing 6.8  Implementing tweetEmotionObservable  in C#\nvar tweetObservable = Observable.FromEventPattern<TweetEventArgs>(stream,\n➥ ""TweetReceived"");\nObservable.Create<TweetEmotion>(observer =>{    var cred = new TwitterCredentials(        consumerKey, consumerSecretKey, accessToken, accessTokenSecret );\n    var stream = Stream.CreateSampleStream(cred);    stream.FilterLevel = StreamFilterLevel.Low;    stream.StartStreamAsync();    return Observable.FromEventPattern<TweetReceivedEventArgs>(stream, \n➥ ""TweetReceived"")\n        .Throttle(throttle)        .Select(args => args.EventArgs)        .Where(args => args.Tweet.Language == Language.English)        .GroupBy(args =>                  evaluateEmotion(args.Tweet.FullText))Observable.Create creates \nan observable by a given \nfunction that takes an \nobserver as a parameter, \nwhich is subscribed to the \nreturned observable. \nThe observable subscribes to \nthe OnNext method in the \nobserver to push the changes.",5264
62-6.5.6 The convenient F object expression.pdf,62-6.5.6 The convenient F object expression,"179 An Rx publisher-subscriber\n        .SelectMany(args =>                  args.Select(tw => TweetEmotion.Create(tw.Tweet, args.Key)))        .Subscribe(o=>observer.OnNext(o));});\nThe FromEventPattern  method converts a .NET CLR event into an observable. In this \ncase, it transforms the TweetReceived  events into an IObservable . \nOne difference between the C# and F# implementation is that the F# code doesn’t \nrequire creating an Observable  tweetObservable  using FromEventPattern . In fact, \nthe event handler TweetReceived  automatically becomes an observable in F# when \npassed into the pipeline stream.TweetReceived |> Observable. TweetEmotion  is a \nvalue type (structure) that carries the information of the tweet emotion (in bold).\nListing 6.9  TweetEmotion  struct to maintain tweet details\n[<Struct>]type TweetEmotion(tweet:ITweet, emotion:Emotion) =       member this.Tweet with get() = tweet       member this.Emotion  with get() = emotion       static member Create tweet emotion =                          TweetEmotion(tweet, emotion)\nThis next listing shows the implementation of RxTweetEmotion , which inherits from \nthe RxPubSub  class and subscribes an IObservable  to manage the tweet emotion noti-\nfications (in bold). \nListing 6.10  Implementing RxPubSub  TweetEmotion\nclass RxTweetEmotion : RxPubSub<TweetEmotion>  {     public RxTweetEmotion(TimeSpan throttle)       {          var obs = TweetsAnalysis.tweetEmotionObservable(throttle)                       .SubscribeOn(TaskPoolScheduler.Default);                            base.AddPublisher(obs);      }}\nThe class RxTweetEmotion  creates and registers the tweetEmotionObservable  observ-\nable to the base class using the AddPublisher  method through the obs observable, \nwhich elevates the notification bubble from the internal TweetReceived.  The next \nstep, to accomplish something useful, is to register the observers.\n6.5.5 Observers in action\nThe implementation of the RxTweetEmotion  class is completed. But without subscrib-\ning any observers, there’s no way to notify or react to an event when it occurs. To create Inherits RxTweetEmotion from RxPubSub Passes the throttle value into the constructor, then into the tweetEmotionObservable definition\nConcurrently runs the Tweet-Emotions notifications using \nTaskPoolScheduler. This is useful when handling concurrent \nmessages and multiple observers. \n \n180 chapter  6 Real-time event streams: functional reactive programming\nan implementation of the IObserver  interface, you could create a class that inherits \nand implements each of its methods. Fortunately, Rx has a set of helper functions to make this job easier. The method \nObserver.Create()  can define new observers:\n    IObserver<T> Create<T>(Action<T> onNext,                            Action<Exception> onError,                           Action onCompleted)\nThis method has a series of overloads, passing an arbitrary implementation of the \nOnNext , OnError , and OnCompleted  methods and returning an IObserver<T>  object \nthat calls the provided functions.\nThese Rx helper functions minimize the number of types created in a program as \nwell as unnecessary proliferation of classes. Here’s an example of an IObserver  that \nprints only positive tweets to the console:\nvar tweetPositiveObserver = Observer.Create<TweetEmotion>(tweet => {      if (tweet.Emotion.IsHappy)          Console.WriteLine(tweet.Tweet.Text);});\nAfter creating the tweetPositiveObserver  observer, its instance is registered to an \ninstance of the previous implemented RxTweetEmotion  class, which notifies each sub-\nscribed observer if a tweet with positive emotion is received:\nvar rxTweetEmotion = new RxTweetEmotion(TimeSpan.FromMilliseconds(150));IDisposable posTweets = rxTweetEmotion.Subscribe(tweetPositiveObserver);\nAn instance of the IDisposable  interface is returned for each observer subscribed. \nThis interface can be used to stop the observer from receiving the notifications and to unregister (remove) the observer from the publisher by calling the \nDispose  method.\n6.5.6 The convenient F# object expression\nThe F# object expression is a convenient way to implement on the fly any instance of an anonymous object that’s based on a known existing interface (or interfaces). Object expressions in F# work similarly to the \nObserver.Create()  method but can be \napplied to any given interface. Additionally, the instance created by an object expres-sion in F# can also feed other .NET programming languages due to the supported interoperability.\nThe following code shows how to use an object expression in F# to create an instance \nof \nIObserver<TweetEmotion>  to display only unhappy emotions to the console:\nlet printUnhappyTweets() =    { new IObserver<TweetEmotion> with          member this.OnNext(tweet) =               if tweet.Emotion = Unhappy then                     Console.WriteLine(tweet.Tweet.text)          member this.OnCompleted() = ()          member this.OnError(exn) = () }\nThe aim of object expressions is to avoid the extra code required to define and cre-ate new named types. The instance resulting from the previous object expression can \n \n 181 Summary\nbe used in the C# project by referencing the F# library and importing the correlated namespace. Here’s how you can use the F# object expression in the C# code:\nIObserver<TweetEmotion> unhappyTweetObserver = printUnhappyTweets();IDisposable disposable = rxTweetEmotion.Subscribe(unhappyTweetObserver);\nAn instance of the unhappyTweetObserver  observer is defined using the F# object \nexpression and is then subscribed to by rxTweetEmotion , which is now ready to receive \nnotifications. \nSummary\n¡ The reactive programming paradigm employs non-blocking asynchronous oper -\nations with a high rate of event sequences over time. This programming para-digm focuses on listening and treating a series of events asynchronously as an event stream.\n¡ Rx treats an event stream as a sequence of events. Rx lets you exercise the same expressive programming semantic as LINQ and apply higher-order operations such as \nfilter , map, and reduce  against events.\n¡ Rx for .NET provides full support for multithreaded programming. In fact, Rx is capable of handling multiple events simultaneously, possibly in parallel. More-over, it integrates with client programming, allowing GUI updates directly.\n¡ The Rx schedulers are designed to generate and process events concurrently, increasing responsiveness and scalability, while also reducing complexity. The Rx schedulers provide an abstraction over the concurrency model, which let you perform operations against moving data streams without the need to be exposed directly to the underlying concurrent implementation.\n¡ The programming language F# treats events as first-class values, which means you can pass them around as data. This approach is the root that influences event combinators that let you program against events as a regular sequence.\n¡ The special event combinators in F# can be exposed and consumed by other .NET programming languages, using this powerful programming style to sim-plify the traditional event-based programming model. \n¡ Reactive programming excels at taking full advantage of asynchronous execu-tion in the creation of components and composition of workflows. Furthermore, the inclusion of Rx capabilities to tame backpressure is crucial to avoid overuse or unbounded consumption of resources.\n¡ Rx tames backpressure for continuous bursts of notifications, permitting you to control a high-rate stream of events that could potentially overwhelm consumers. \n¡ Rx provides a set of tools for implementing useful reactive patterns, such as Publish/Subscribe.",7769
63-7.1.2 Task parallelism support in .NET.pdf,63-7.1.2 Task parallelism support in .NET,"1827Task-based functional \nparallelism\nThis chapter covers\n¡ Task parallelism and declarative programming semantics\n¡ Composing parallel operations with functional combinators\n¡ Maximizing resource utilization with the Task Parallel Library\n¡ Implementing a parallel functional pipeline pattern\nThe task parallelism paradigm splits program execution and runs each part in par -\nallel by reducing the total runtime. This paradigm targets the distribution of tasks across different processors to maximize processor utilization and improve perfor -\nmance. Traditionally, to run a program in parallel, code is separated into distinct areas of functionality and then computed by different threads. In these scenarios, primitive locks are used to synchronize the access to shared resources in the pres-ence of multiple threads. The purpose of locks is to avoid race conditions and mem-ory corruption by ensuring concurrent mutual exclusion. The main reason locks are \n \n 183 A short introduction to task parallelism \n7used is due to the design legacy of waiting for the current thread to complete before a resource is available to continue running the thread.\nA newer and better mechanism is to pass the rest of the computation to a callback \nfunction (which runs after the thread completes execution) to continue the work. This technique in FP is called continuation-passing style (CPS). In this chapter, you’ll learn how to adopt this mechanism to run multiple tasks in parallel without blocking program execution. With this technique, you’ll also learn how to implement task-based parallel programs by isolating side effects and mastering function composition, which simplifies the achievement of task parallelism in your code. Because compositionality is one of the most important features in FP, it eases the adoption of a declarative programming style. Code that’s easy to understand is also simple to maintain. Using FP, you’ll engage task parallelism in your programs without introducing complexity, as compared to conven-tional programming. \n7.1 A short introduction to task parallelism \nTask parallelism refers to the process of running a set of independent tasks in parallel across several processors. This paradigm partitions a computation into a set of smaller tasks and executes those smaller tasks on multiple threads. The execution time is reduced by simultaneously processing multiple functions.\nIn general, parallel jobs begin from the same point, with the same data, and can \neither terminate in a fire-and-forget fashion or complete altogether in a task-group con-tinuation. Any time a computer program simultaneously evaluates different and auton-omous expressions using the same starting data, you have task parallelism. The core of this concept is based on small units of computations called futures. Figure 7.1 shows the \ncomparison between data parallelism and task parallelism.\nData\nsetParallelismData\nsetTask 2\nTask 3\nTask 4\nTask 5\nTask 6Data\nsetParallelismData parallelism Task parallelis m\nTask 1\n1N 1/6\nData\nsetTask 2\n2N 1/6\nTask 3\n3N 1/6\nTask 4\n4N 1/6\nTask 5\n5N 1/6\nTask 6\n6N 1/6Task 1\nFigure 7.1  Data parallelism is the simultaneous execution of the same function across the elements of \na data set. Task parallelism is the simultaneous execution of multiple and different functions across the same or different data sets. \n \n184 chapter  7 Task-based functional parallelism\nTask parallelism isn’t data parallelism\nChapter 4 explains the differences between task parallelism and data parallelism. To \nrefresh your memory, these paradigms are at two ends of the spectrum. Data parallelism  \noccurs when a single operation is applied to many inputs. Task parallelism  occurs when \nmultiple diverse operations perform against their own input. It is used to query and call multiple Web APIs at one time, or to store data against different database servers. In short, task parallelism parallelizes functions; data parallelism parallelizes data.\n \nTask parallelism achieves its best performance by adjusting the number of running tasks, depending on the amount of parallelism available on your system, which corre-sponds to the number of available cores and, possibly, their current loads. \n7.1.1 Why task parallelism and functional programming?\nIn the previous chapters, you’ve seen code examples that deal with data parallelism and task composition. Those data-parallel patterns, such as Divide and Conquer, Fork/Join, and MapReduce, aim to solve the computational problem of splitting and com-puting in parallel smaller, independent jobs. Ultimately, when the jobs are terminated, their outputs are combined into the final result. \nIn real-world parallel programming, however, you commonly deal with different \nand more complex structures that aren’t so easily split and reduced. For example, the computations of a task that processes input data could rely on the result of other tasks. In this case, the design and approach to coordinating the work among multiple tasks is different than for the data parallelism model and can sometimes be challeng-ing. This challenge is due to task dependencies, which can reach convoluted connec-tions where execution times can vary, making the job distribution tough to manage.\nThe purpose of task parallelism is to tackle these scenarios, providing you, the devel-\noper, with a toolkit of practices, patterns, and, in the case of programming, the .NET Framework, a rich library that simplifies task-based parallel programming. In addition, FP eases the compositional aspect of tasks by controlling side effects and managing their dependencies in a declarative programming style.\nFunctional paradigm tenets play an essential role in writing effective and deterministic \ntask-based parallel programs. These functional concepts were discussed in the early chap-ters of this book. To summarize, here’s a list of recommendations for writing parallel code:\n¡ Tasks should evaluate side-effect-free functions, which lead to referential trans-parency and deterministic code. Pure functions make the program more pre-dictable because the functions always behave in the same way, regardless of the external state.\n¡ Remember that pure functions can run in parallel because the order of execu-tion is irrelevant.\n \n 185 A short introduction to task parallelism \n¡ If side effects are required, control them locally by performing the computation in a function with run-in isolation.\n¡ Avoid sharing data between tasks by applying a defensive copy approach. \n¡ Use immutable structures when data sharing between tasks cannot be avoided.\nNOTE  A defensive copy is a mechanism that reduces (or eliminates) the negative \nside effects of modifying a shared mutable object. The idea is to create a copy of the original object that can be safely shared; its modification won’t affect the original object.\n7.1.2 Task parallelism support in .NET\nSince its first release, the .NET Framework has supported the parallel execution of code through multithreading. Multithreaded programs are based on an indepen-dent execution unit called a thread, which is a lightweight process responsible for multitasking within a single application. (The \nThread  class can be found in the Base \nClass Library (BCL) System.Threading  namespace.) Threads are handled by the \nCLR. The creation of new threads is expensive in terms of overhead and memory. For example, the memory stack size associated with the creation of a thread is about 1 MB in an x86 architecture-based processor because it involves the stack, thread local storage, and context switches. \nFortunately, the .NET Framework provides a class \nThreadPool  that helps to over -\ncome these performance problems. In fact, it’s capable of optimizing the costs asso-ciated with complex operations, such as creating, starting, and destroying threads. Furthermore, the .NET \nThreadPool  is designed to reuse existing threads as much as \npossible to minimize the costs associated with the instantiation of new ones.  Figure 7.2 compares the two processes.\nThe ThreadPool class\nThe .NET Framework provides a ThreadPool  static class that loads a set of threads during \nthe initialization of a multithreaded application and then reuses those threads, instead of creating new threads, to run new tasks as required. In this way, the \nThreadPool  class lim-\nits the number of threads that are running at any given point, avoiding the overhead of creating and destroying application threads. In the case of parallel computation, \nThread-\nPool  optimizes the performance and improves the application’s responsiveness by avoid-\ning context switches.\n \nThe ThreadPool  class exposes the static method QueueUserWorkItem , which accepts a \nfunction (delegate) that represents an asynchronous operation. \n \n186 chapter  7 Task-based functional parallelism\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6Task 1 Thread 1Conventional threads\nAn instance of a new thread is created\nfor each operation or task. This can lead\nto memory-consumption issues.\nThread 2\nThread 3\nThread 4\nThread 5\nThread 6Task 2\nTask 3\nTask 4\nTask 5\nTask 6Task 1 Work item 1Thread pool\nTask are queued in a pool of work items, which are lightweight\ncompared to threads. The thread pool schedules tasks, reusing\neach thread for the next work item and returning it to the pool\nwhen the job is completed.\nWork item 2\nWork item 3\nWork item 4Worker 1\nWorker 2\nWorker 3\nFigure 7.2  If using conventional threads, you must create an instance of a new thread for each operation or \ntask. This can create memory consumption issues. By contrast, if using a thread pool, you queue a task in a pool of work items, which are lightweight compared to threads. The thread pool then schedules these tasks, reusing the thread for the next work item and returning it back to the pool when the job is completed.\nThe following listing compares starting a thread in a traditional way versus starting a thread using the \nThreadPool .QueueUserWorkItem  static method.\nListing 7.1  Spawning threads and ThreadPool.QueueUserWorkItem\nAction<string> downloadSite = url => {     var content = new WebClient().DownloadString(url);    Console.WriteLine($""The size of the web site {url} is \n➥ {content.Length}"");\n};      var threadA = new Thread(() => downloadSite(""http://www.nasdaq.com""));var threadB = new Thread(() => downloadSite(""http://www.bbc.com""));threadA.Start();threadB.Start(); threadA.Join();threadB.Join();  ThreadPool.QueueUserWorkItem(o => downloadSite(""http://www.nasdaq.com""));ThreadPool.QueueUserWorkItem(o => downloadSite(""http://www.bbc.com"")); \nA thread starts explicitly, but the Thread  class provides an option using the instance \nmethod Join  to wait for the thread. Each thread then creates an additional mem-\nory load, which is harmful to the runtime environment. Initiating an asynchronous Uses a function to download \na given website\nThreads must start explicitly, providing the option to wait (join) for completion.\nThe ThreadPool.QueueUserWorkItem immediately \nstarts an operation considered “fire and forget,” \nwhich means the work item needs to produce side \neffects in order for the calculation to be visible.",11274
64-7.2.1 Running operations in parallel with TPL Parallel.Invoke.pdf,64-7.2.1 Running operations in parallel with TPL Parallel.Invoke,"187 The .NET Task Parallel Library \ncomputation using ThreadPool ’s QueueUserWorkItem  is simple, but there are a few \nrestraints when using this technique that introduce serious complications in develop-ing a task-based parallel system: \n¡ No built-in notification mechanism when an asynchronous operation completes\n¡ No easy way to get back a result from a ThreadPool  worker\n¡ No built-in mechanism to propagate exceptions to the original thread\n¡ No easy way to coordinate dependent asynchronous operations\nTo overcome these limitations, Microsoft introduced the notion of tasks with the TPL, accessible through the \nSystem.Threading.Tasks  namespace. The tasks concept is the \nrecommended approach for building task-based parallel systems in .NET.\n7.2 The .NET Task Parallel Library \nThe .NET TPL implements a number of extra optimizations on top of ThreadPool , \nincluding a sophisticated TaskScheduler  work-stealing algorithm (http:/ /mng.bz/j4K1) \nto scale dynamically the degree of concurrency, as shown in figure 7.3. This algorithm guarantees an effective use of the available system processor resources to maximize the overall performance of the concurrent code. \nWorker 1Work-stealing algorithm\nTask 2\nTask 3\nTask 4\nTask 5\nTask 6Task 1 Work item 6 Work item 1\nWork item 41. The work item is sent\n    to the main queue.\n2. The work item    is dispatched to    a worker thread,    which has a private,    dedicated queue of    work items to    process.3. If the main queue is    empty , workers look\n    in the private queues\n    of other workers to\n    “steal” work.Work item 3\nWorker 2\nWork item 2\nWork item 5\nFigure 7.3  The TPL uses the work-stealing algorithm to optimize the scheduler. Initially, the TPL sends \njobs to the main queue (step 1). Then it dispatches the work items to one of the worker threads, which has a private and dedicated queue of work items to process (step 2). If the main queue is empty, the workers look in the private queues of the other workers to “steal” work (step 3). \n \n188 chapter  7 Task-based functional parallelism\nWith the introduction of the task concept in place of the traditional and limited thread model, the Microsoft TPL eases the process of adding concurrency and parallelism to a program with a set of new types. Furthermore, the TPL provides support through the \nTask  object to cancel and manage state, to handle and propagate exceptions, and to \ncontrol the execution of working threads. The TPL abstracts away the implementation details from the developer, offering control over executing the code in parallel.\nWhen using a task-based programming model, it becomes almost effortless to intro-\nduce parallelism in a program and concurrently execute parts of the code by convert-ing those parts into tasks. \nNOTE  The TPL provides the necessary infrastructure to achieve optimal utili-\nzation of CPU resources, regardless of whether you’re running a parallel pro-gram on a single or multicore computer.\nYou have several ways to invoke parallel tasks. This chapter reviews the relevant tech-niques to implement task parallelism.\n7.2.1 Running operations in parallel with TPL Parallel.Invoke\nUsing the .NET TPL, you can schedule a task in several ways, the Parallel.Invoke  \nmethod being the simplest. This method accepts an arbitrary number of actions (del-egates) as an argument in the form \nParamArray  and creates a task for each of the del-\negates passed. Unfortunately, the action-delegate signature has no input arguments, and it returns \nvoid , which is contrary to functional principles. In imperative program-\nming languages, functions returning void  are used for side effects.\nWhen all the tasks terminate, the Parallel.Invoke  method returns control to the \nmain thread to continue the execution flow. One important distinction of the Paral-\nlel.Invoke method is that exception handling, synchronous invocation, and schedul-\ning are handled transparently to the developer.\nLet’s imagine a scenario where you need to execute a set of independent, hetero-\ngeneous tasks in parallel as a whole, then continue the work after all tasks complete. Unfortunately, PLINQ and parallel loops (discussed in the previous chapters) cannot be used because they don’t support heterogeneous operations. This is the typical case for using the \nParallel.Invoke  method.\nNOTE  Heterogeneous tasks are a set of operations that compute as a whole regard-\nless of having different result types or diverse outcomes. \nListing 7.2 runs functions in parallel against three given images and then saves the result in the filesystem. Each function creates a locally defensive copy of the original image to avoid unwanted mutation. The code example is in F#; the same concept applies to all .NET programming languages. \n \n 189 The .NET Task Parallel Library \nListing 7.2  Parallel.Invoke  executing multiple heterogeneous tasks\nlet convertImageTo3D (sourceImage:string) (destinationImage:string) =     let bitmap = Bitmap.FromFile(sourceImage) :?> Bitmap           let w,h = bitmap.Width, bitmap.Height    for x in 20 .. (w-1) do        for y in 0 .. (h-1) do                      let c1 = bitmap.GetPixel(x,y)            let c2 = bitmap.GetPixel(x - 20,y)            let color3D = Color.FromArgb(int c1.R, int c2.G, int c2.B)            bitmap.SetPixel(x - 20 ,y,color3D)    bitmap.Save(destinationImage, ImageFormat.Jpeg)      let setGrayscale (sourceImage:string) (destinationImage:string) =        let bitmap = Bitmap.FromFile(sourceImage) :?> Bitmap          let w,h = bitmap.Width, bitmap.Height    for x = 0 to (w-1) do        for y = 0 to  (h-1) do                      let c = bitmap.GetPixel(x,y)            let gray = int(0.299 * float c.R + 0.587 * float c.G + 0.114 * \n➥ float c.B)\n            bitmap.SetPixel(x,y, Color.FromArgb(gray, gray, gray))    bitmap.Save(destinationImage, ImageFormat.Jpeg)     let setRedscale (sourceImage:string) (destinationImage:string) =       let bitmap = Bitmap.FromFile(sourceImage) :?> Bitmap         let w,h = bitmap.Width, bitmap.Height    for x = 0 to (w-1) do        for y = 0 to  (h-1) do                     let c = bitmap.GetPixel(x,y)            bitmap.SetPixel(x,y, Color.FromArgb(int c.R, \n➥ abs(int c.G – 255), abs(int c.B – 255)))\n    bitmap.Save(destinationImage, ImageFormat.Jpeg)      System.Threading.Tasks.Parallel.Invoke(    Action(fun ()-> convertImageTo3D ""MonaLisa.jpg"" ""MonaLisa3D.jpg""),    Action(fun ()-> setGrayscale ""LadyErmine.jpg"" ""LadyErmineRed.jpg""),    Action(fun ()-> setRedscale ""GinevraBenci.jpg"" ""GinevraBenciGray.jpg"")) \nIn the code, Parallel.Invoke  creates and starts the three tasks independently, one for \neach function, and blocks the execution flow of the main thread until all tasks com-plete. Due to the parallelism achieved, the total execution time coincides with the time to compute the slower method. \nNOTE  The source code intentionally uses the methods GetPixel  and SetPixel  \nto modify a Bitmap . These methods (especially GetPixel ) are notoriously slow; \nbut for the sake of the example we want to test the parallelism creating little Creates an image with a 3D effect from a given image\nCreates a copy of \nan image from a \ngiven file path \nA nested for loop accesses  the pixels of the image.\nSaves the newly created image in the filesystem\nCreates an image, transforming the colors to shades of gray\nA nested for loop accesses  the pixels of the image.\nCreates an image by applying the Red color filter\nA nested for loop accesses the pixels  of the image.\nSaves the newly created image in the filesystem\n \n190 chapter  7 Task-based functional parallelism\nextra overhead to induce extra CPU stress. In production code, if you need to iterate throughout an entire image, you’re better off marshaling the entire image into a byte array and iterating through that.\nIt’s interesting to notice that the \nParallel.Invoke  method could be used to implement a \nFork/Join pattern, where multiple operations run in parallel and then join when they’re all completed. Figure 7.4 shows the images before and after the image processing.\nGinevra de’  Benci Ginevra de’  Benci, red filter\nMona Lisa Mona Lisa , 3D\nLady with an Ermine Lady with an Ermine , shades of gray\nFigure 7.4  The resulting images from running the code in listing 7.2. You can find the full implementation in the downloadable source code.",8433
65-7.3.1 The solution for void in C the unit type.pdf,65-7.3.1 The solution for void in C the unit type,,0
66-7.3 The problem of void in C.pdf,66-7.3 The problem of void in C,"191 The problem of void in C#\nDespite the convenience of executing multiple tasks in parallel, Parallel.Invoke  \nlimits the control of the parallel operation because of the void  signature type. This \nmethod doesn’t expose any resources to provide details regarding the status and out-come, either succeed or fail, of each individual task. \nParallel.Invoke  can either \ncomplete successfully or throw an exception in the form of an AggregateException  \ninstance. In the latter case, any exception that occurs during the execution is post-poned and rethrown when all tasks have completed. In FP, exceptions are side effects that should be avoided. Therefore, FP provides a better mechanism to handle errors, a subject which will be covered in chapter 11. \nUltimately, there are two important limitations to consider when using the \nParallel \n.Invoke  method:\n¡ The signature of the method returns void , which prevents compositionality.  \n¡ The order of task execution isn’t guaranteed, which constrains the design of computations that have dependencies. \n7.3 The problem of void in C#\nIt’s common, in imperative programming languages such as C#, to define methods and delegates that don’t return values (\nvoid ), such as the Parallel.Invoke  method. \nThis method’s signature prevents compositionality. Two functions can compose when the output of a function matches the input of the other function.\nIn function-first programming languages such as F#, every function has a return \nvalue, including the case of the \nunit  type, which is comparable to a void  but is treated as \na value, conceptually not much different from a Boolean or integer. \nunit  is the type of any expression that lacks any other specific value. Think of func-\ntions used for printing to the screen. There’s nothing specific that needs to be returned, and therefore functions may return \nunit  so that the code is still valid. This is the F# \nequivalent of C#’s void . The reason F# doesn’t use void  is that every valid piece of code \nhas a return type, whereas void  is the absence of a return. Rather than the concept of \nvoid , a functional programmer thinks of unit . In F#, the unit  type is written as (). This \ndesign enables function composition. \nIn principle, it isn’t required for a programming language to support methods with \nreturn values. But a method without a defined output ( void ) suggests that the function \nperforms some side effect, which makes it difficult to run tasks in parallel.  \n7.3.1 The solution for void in C#: the unit type\nIn functional programming, a function defines a relationship between its input and out-put values. This is similar to the way mathematical theorems are written. For example, in the case of a pure function, the return value is only determined by its input values. \nIn mathematics, every function returns a value. In FP, a function is a mapping, and a \nmapping has to have a value to map. This concept is missing in mainstream imperative programming languages such as C#, C++, and Java, which treat \nvoid s as methods that \ndon’t return anything, instead of as functions that can return something meaningful. \n \n192 chapter  7 Task-based functional parallelism\nIn C#, you can implement a Unit  type as a struct  with a single value that can be \nused as a return type in place of a void -returning method. Alternatively, the Rx, dis-\ncussed in chapter 6, provides a unit  type as part of its library. This listing shows the \nimplementation of the Unit  type in C#, which was borrowed from the Microsoft Rx \n(http:/ /bit.ly/2vEzMeM).\nListing 7.3  Unit  type implementation in C#\npublic struct Unit : IEquatable<Unit>  {    public static readonly Unit Default = new Unit();      public override int GetHashCode() => 0;            public override bool Equals(object obj) => obj is Unit;       public override string ToString() => ""()"";    public bool Equals(Unit other) => true;           public static bool operator ==(Unit lhs, Unit rhs) => true;       public static bool operator !=(Unit lhs, Unit rhs) => false;  }\nThe Unit  struct implements the IEquatable  interface in such a way that forces all val-\nues of the Unit  type to be equal. But what’s the real benefit of having the Unit  type as a \nvalue in a language type system? What is its practical use?     \nHere are two main answers:\n¡ The type Unit  can be used to publish an acknowledgment that a function is \ncompleted.\n¡ Having a Unit  type is useful for writing generic code, including where a generic \nfirst-class function is required, which reduces code duplication.\nUsing the Unit  type, for example, you could avoid repeating code to implement \nAction<T>  or Func<T, R> , or functions that return a Task  or a Task<T> . Let’s consider \na function that runs a Task<TInput>  and transforms the result of the computation into \na TResult  type:\nTResult Compute<TInput, TResult>(Task<TInput> task,              Func<TInput, TResult> projection) => projection(task.Result);Task<int> task = Task.Run<int>(() => 42);bool isTheAnswerOfLife = Compute(task, n => n == 42);\nThis function has two arguments. The first is a Task<TInput>  that evaluates to an \nexpression. The result is passed into the second argument, a Func<TInput, TResult>  \ndelegate, to apply a transformation and then return the final value.The unit struct that implements the IEquatable interface to force the definition of a type-specific method for determining equality\nUses a helper static method to retrieve the instance of the Unit type Overrides the base methods to \nforce equality between Unit types\nEquality between unit types is always true.",5667
67-7.4.2 Waiting for a task to complete the continuation model.pdf,67-7.4.2 Waiting for a task to complete the continuation model,"193 Continuation-passing style: a functional control flow\nNOTE  This code implementation is for demo purposes only. It isn’t recom-\nmended to block the evaluation of the task to retrieve the result as the Compute  \nfunction does in the previous code snippet. Section 7.4 covers the right approach.\nHow would you convert the Compute  function into a function that prints the result? \nYou’re forced to write a new function to replace the Func<T>  delegate projection into \nan Action  delegate type. The new method has this signature: \nvoid Compute<TInput>(Task<TInput> task, Action<TInput> action) =>                     action(task.Result);Task<int> task = Task.Run<int>(() => 42);Compute(task, n => Console.WriteLine($""Is {n} the answer of life? \n➥ {n == 42}""));\nIt’s also important to point out that the Action  delegate type is performing a side \neffect: in this case, printing the result on the console, which is a function conceptually similar to the previous one.\nIt would be ideal to reuse the same function instead of having to duplicate code for \nthe function with the \nAction  delegate type as an argument. To do so, you’ll need to pass \na void  into the Func  delegate, which isn’t possible in C#. This is the case where the Unit  \ntype removes code repetition. By using the struct Unit  type definition, you can use \nthe same function that takes a Func  delegate to also produce the same behavior as the \nfunction with the Action  delegate type:\nTask<int> task = Task.Run<int>(() => 42);Unit unit = Compute(task, n => {     Console.WriteLine($""Is {n} the answer of life? {n == 42}"");     return Unit.Default;});\nIn that way, introducing the Unit  type in the C# language, you can write one Compute  \nfunction to handle both cases of returning a value or computing a side effect. Ulti-mately, a function returning a \nUnit  type indicates the presence of side effects, which \nis meaningful information for writing concurrent code. Moreover, there are FP lan-guages, such as Haskell, where the \nUnit  type notifies the compiler, which then distin-\nguishes between pure and impure functions to apply more granular optimization.\n7.4 Continuation-passing style: a functional control flow\nTask continuation is based on the functional idea of the CPS paradigm, discussed in \nchapter 3. This approach gives you execution control, in the form of continuation, by passing the result of the current function to the next one. Essentially, function contin-uation is a delegate that represents “what happens next.” CPS is an alternative for the conventional control flow in imperative programming style, where each command is executed one after another. Instead, using CPS, a function is passed as an argument into a method, explicitly defining the next operation to execute after its own computa-tion is completed. This lets you design your own flow-of-control commands. \n \n194 chapter  7 Task-based functional parallelism\n7.4.1 Why exploit CPS?\nThe main benefit of applying CPS in a concurrent environment is avoiding inconve-\nnient thread blocking that negatively impacts the performance of the program. For example, it’s inefficient for a method to wait for one or more tasks to complete, block-ing the main execution thread until its child tasks complete. Often the parent task, which in this case is the main thread, can continue, but cannot proceed immediately because its thread is still executing one of the other tasks. The solution: CPS, which allows the thread to return to the caller immediately, without waiting on its children. This ensures that the continuation will be invoked when it completes.\nOne downside of using explicit CPS is that code complexity can escalate quickly \nbecause CPS makes programs longer and less readable. You’ll see later in this chapter how to combat this issue by combining TPL and functional paradigms to abstract the complexity behind the code, making it flexible and simple to use. CPS enables several helpful task advantages:\n¡ Function continuations can be composed as a chain of operations. \n¡ A continuation can specify the conditions under which the function is called.\n¡ A continuation function can invoke a set of other continuations.\n¡ A continuation function can be canceled easily at any time during computation or even before it starts.\nIn the .NET Framework, a task is an abstraction of the classic (traditional) .NET thread (http:/ /mng.bz/DK6K), representing an independent asynchronous unit of work. The \nTask  object is part of the System.Threading.Tasks  namespace. The higher level of \nabstraction provided by the Task  type aims to simplify the implementation of concur -\nrent code and facilitate the control of the life cycle for each task operation. It’s possible, for example, to verify the status of the computation and confirm whether the operation is terminated, failed, or canceled. Moreover, tasks are composable in a chain of opera-tions by using continuations, which permit a declarative and fluent programming style. \nThe following listing shows how to create and run operations using the \nTask  type. \nThe code uses the functions from listing 7.2. \nListing 7.4  Creating and starting tasks\nTask monaLisaTask = Task.Factory.StartNew(() =>     convertImageTo3D(""MonaLisa.jpg"", ""MonaLisa3D.jpg""));       Task ladyErmineTask = new Task(() =>     setGrayscale(""LadyErmine.jpg"", ""LadyErmine3D.jpg""));ladyErmineTask.Start();         Task ginevraBenciTask = Task.Run(() =>     setRedscale(""GinevraBenci.jpg"", ""GinevraBenci3D.jpg""));     Runs the method convertImageTo3D using the StartNew Task static helper method\nRuns the method setGrayscale by creating a new Task instance, and then calls the Start() Task instance method Runs the method setRedscale using the \nsimplified static method Run(), which runs \na task with the default common properties \n \n 195 Continuation-passing style: a functional control flow\nThe code shows three different ways to create and execute a task:\n¡ The first technique creates and immediately starts a new task using the built-in \nTask.Factory.StartNew  method constructor.\n¡ The second technique creates a new instance of a task, which needs a function as a constructor parameter to serve as the body of the task. Then, calling the \nStart  \ninstance method, the Task  begins the computation. This technique provides the \nflexibility to delay task execution until the Start  function is called; in this way, \nthe Task  object can be passed into another method that decides when to sched-\nule the task for execution. \n¡ The third approach creates the Task  object and then immediately calls the Run \nmethod to schedule the task. This is a convenient way to create and work with tasks using the default constructor that applies the standard option values. \nThe first two options are a better choice if you need a particular option to instantiate a task, such as setting the \nLongRunning  option. In general, tasks promote a natural way \nto isolate data that depends on functions to communicate with their related input and output values, as shown in the conceptual example in figure 7.5.\nResult is\ncoffeeTask: grind coff ee beans\nbeans       powderInput is\ncoffee beansOutput is\ncoffee powder\nInput is\ncoffee powderTask: brew coff ee\npowder       cof fee\nFigure 7.5  When two tasks are composed together, the output of the first task becomes the input for \nthe second. This is the same as function composition. \nNOTE  The Task  object can be instantiated with different options to control \nand customize its behavior. The TaskCreationOptions.LongRunning  option \nnotifies the underlying scheduler that the task will be a long-running one, for example. In this case, the task scheduler might be bypassed to create an addi-tional and dedicated thread whose work won’t be impacted by thread-pool scheduling. For more information regarding \nTaskCreationOptions , see the \nMicrosoft MSDN documentation online (http:/ /bit.ly/2uxg1R6).\n7.4.2 Waiting for a task to complete: the continuation model\nYou’ve seen how to use tasks to parallelize independent units of work. But in common cases the structure of the code is more complex than launching operations in a fire-and-forget manner. The majority of task-based parallel computations require a more sophisticated level of coordination between concurrent operations, where order of execution can be influenced by the underlying algorithms and control flow of the pro-gram. Fortunately, the .NET TPL library provides mechanisms for coordinating tasks.\nLet’s start with an example of multiple operations running sequentially, and incre-\nmentally redesign and refactor the program to improve the code compositionality and \n \n196 chapter  7 Task-based functional parallelism\nperformance. You’ll start with the sequential implementation, and then you’ll apply dif-ferent techniques incrementally to improve and maximize the overall computational performance.\nListing 7.5 implements a face-detection program that can detect specific faces in a \ngiven image. For this example, you’ll use the images of the presidents of the United States on $20, $50, and $100 bills, using the side on which the president’s image is printed. The program will detect the face of the president in each image and return a new image with a square box surrounding the detected face. In this example, focus on the important code without being distracted by the details of the UI implementation. The full source code is downloadable from the book’s website. \nListing 7.5  Face-detection function in C#\nBitmap DetectFaces(string fileName) {    var imageFrame = new Image<Bgr, byte>(fileName);      var cascadeClassifier = new CascadeClassifier();      var grayframe = imageFrame.Convert<Gray, byte>();     var faces = cascadeClassifier.DetectMultiScale(        grayframe, 1.1, 3, System.Drawing.Size.Empty);     foreach (var face in faces)        imageFrame.Draw(face,                   new Bgr(System.Drawing.Color.BurlyWood), 3);     return imageFrame.ToBitmap();}void StartFaceDetection(string imagesFolder) {    var filePaths = Directory.GetFiles(imagesFolder);        foreach (string filePath in filePaths) {            var bitmap = DetectFaces(filePath);            var bitmapImage = bitmap.ToBitmapImage();            Images.Add(bitmapImage);          }}\nThe function DetectFaces  loads an image from the filesystem using the given filename \npath and then detects the presence of any faces. The library Emgu.CV  is responsible \nfor performing the face detection. The Emgu.CV  library is a .NET wrapper that per -\nmits interoperability with programming languages such as C# and F#, both of which can interact and call the functions of the underlying Intel OpenCV image-processing library.\n1 The function StartFaceDetection  initiates the execution, getting the filesys-\ntem path of the images to evaluate, and then sequentially processes the face detection in a \nfor-each  loop by calling the function DetectFaces . The result is a new BitmapIm -\nage type, which is added to the observable collection Images  to update the UI. Fig-\nure 7.6 shows the expected result—the detected faces are highlighted in a box.Instance of an Emgu.CV image to interop with the OpenCV library\nUses a \nclassifier to \ndetect face \nfeatures in \nan imageFace-detection process\nThe detected face(s) is \nhighlighted here, using a box \nthat’s drawn around it.\nThe processed image is added to the Images observable collection to update the UI.\n1 OpenCV (Open Source Computer Vision Library) is a high-performance image processing library by Intel (https:/ /opencv.org). \n \n 197 Continuation-passing style: a functional control flow\nThe first step in improving the performance of the program is to run the face- detection \nfunction in parallel, creating a new task for each image to evaluate.\nListing 7.6  Parallel-task implementation of the face-detection program\nvoid StartFaceDetection(string imagesFolder) {    var filePaths = Directory.GetFiles(imagesFolder);    var bitmaps = from filePath in filePaths                select Task.Run<Bitmap>(() => DetectFaces(filePath));     foreach (var bitmap in bitmaps) {         var bitmapImage = bitmap.Result;             Images.Add(bitmapImage.ToBitmapImage());    }}\nIn this code, a LINQ expression creates an IEnumerable  of Task<Bitmap> , which is \nconstructed with the convenient Task.Run  method. With a collection of tasks in place, \nthe code starts an independent computation in the for-each  loop; but the perfor -\nmance of the program isn’t improved. The problem is that the tasks still run sequen-tially, one by one. The loop processes one task at a time, awaiting its completion before continuing to the next task. The code isn’t running in parallel. \nYou could argue that choosing a different approach, such as using \nParallel.ForEach  \nor Parallel.Invoke  to compute the DetectFaces  function, could avoid the problem \nand guarantee parallelism. But you’ll see why this isn’t a good idea.\nLet’s adjust the design to fix the problem by analyzing what the foundational issue \nis. The IEnumerable  of Task<Bitmap>  generated by the LINQ expression is material-\nized during the execution of the for-each  loop. During each iteration, a Task<Bitmap>  \nis retrieved, but at this point, the task isn’t competed; in fact, it’s not even started. The reason lies in the fact that the \nIEnumerable  collection is lazily evaluated, so the under -\nlying task starts the computation at the last possible moment during its materialization. Consequently, when the result of the task bitmap inside the loop is accessed through the \nTask<Bitmap>.Result  property, the task will block the joining thread until the task is Figure 7.6  Result of the face-\ndetection process. The right side has the images with the detected face surrounded by a box frame.\nStarts a task sequentially \nfrom the TPL for each \nimage to process\n \n198 chapter  7 Task-based functional parallelism\ndone. The execution will resume after the task terminates the computation and returns the result.\nTo write scalable software, you can’t have any blocked threads. In the previous code, \nwhen the task’s \nResult  property is accessed because the task hasn’t yet finished running, \nthe thread pool will most likely create a new thread. This increases resource consumption and hurts performance. \nAfter this analysis, it appears that there are two issues to be corrected to ensure \nparallelism (figure 7.7):\n¡ Ensure that the tasks run in parallel.\n¡ Avoid blocking the main working thread and waiting for each task to complete.\nWorker 1\nImage 2Image 1\nImage 3Work item 2 Work item 3\nWork item 41. Images are sent to the task\n   scheduler and become work   items to be processed.2. Work items 1 and 3 are stolen\n    by workers 2 and 1, respectively .\n3. Worker 1 completes the\n    work and notifies the scheduler ,\n    which schedules the continuation    of work item 3 in the form of    work item 4.4. When work item 4 is processed,    the result updates the UI.Worker 2\nWork item 1\n \nFigure 7.7  The images are sent to the task scheduler, becoming work items to be processed (step 1). \nWork item 3 and work item 1 are then “stolen” by worker 1 and worker 2, respectively (step 2). Worker 1 completes the work and notifies the task scheduler, which schedules the rest of the work for continuation in the form of the new work item 4, which is the continuation of work item 3 (step 3). When work item 4 is processed, the result updates the UI (step 4).\nHere is how to fix issues to ensure the code runs in parallel and reduces memory consumption.\nListing 7.7  Correct parallel-task implementation of the DetectFaces  function\nThreadLocal<CascadeClassifier> CascadeClassifierThreadLocal =   new ThreadLocal<CascadeClassifier>(() => new CascadeClassifier()); Bitmap DetectFaces(string fileName) {\nUses a ThreadLocal instance to ensure \na defensive copy of CascadeClassifier \nfor each working task\n \n 199 Continuation-passing style: a functional control flow\n     var imageFrame = new Image<Bgr, byte>(fileName);     var cascadeClassifier = CascadeClassifierThreadLocal.Value;     var grayframe = imageFrame.Convert<Gray, byte>();     var faces = cascadeClassifier.DetectMultiScale(grayframe, 1.1, 3, \n➥ System.Drawing.Size.Empty); \n     foreach (var face in faces)        imageFrame.Draw(face, new Bgr(System.Drawing.Color.BurlyWood), 3);          return imageFrame.ToBitmap();}void StartFaceDetection(string imagesFolder) {     var filePaths = Directory.GetFiles(imagesFolder);     var bitmapTasks =       (from filePath in filePaths      select Task.Run<Bitmap>(() => DetectFaces(filePath))).ToList();      foreach (var bitmapTask in bitmapTasks)            bitmapTask.ContinueWith(bitmap => {                       var bitmapImage = bitmap.Result;                  Images.Add(bitmapImage.ToBitmapImage());            }, TaskScheduler.FromCurrentSynchronizationContext()); }\nIn the example, to keep the code structure simple, there’s the assumption that each computation completes successfully. A few code changes exist, but the good news is that true parallel computation is achieved without blocking any threads (by continuing the task operation when it completes). The main function \nStartFaceDetection  guarantees \nexecuting the tasks in parallel by materializing the LINQ expression immediately with a call to \nToList()  on the IEnumerable  of Task<Bitmap> . \nNOTE  When you write a computation that creates a load of tasks, fire a LINQ \nquery and make sure to materialize the query first. Otherwise, there’s no benefit, because parallelism will be lost and the task will be computed sequentially in the \nfor-each  loop. \nNext, a ThreadLocal  object is used to create a defensive copy of CascadeClassifier  \nfor each thread accessing the function DetectFaces . CascadeClassifier  loads into \nmemory a local resource, which isn’t thread safe. To solve this problem of thread unsafety, a local variable \nCascadeClassifier is instantiated for each thread that runs \nthe function. This is the purpose of the ThreadLocal  object (discussed in detail in \nchapter 4).\nThen, in the function StartFaceDetection , the for-each  loop iterates through the \nlist of Task<Bitmap> , creating a continuation for each task instead of blocking the exe-\ncution if the task is not completed. Because bitmapTask  is an asynchronous operation, \nthere’s no guarantee that the task has completed executing before the Result  property \nis accessed. It’s good practice to use task continuation with the function ContinueWith  \nto access the result as part of a continuation. Defining a task continuation is similar to Uses a LINQ expression on \nthe file paths that starts \nimage processing in parallel \nTask continuation ensures no blocking; \nthe operation passes the continuation \nof the work when it completes.TaskScheduler FromCurrentSynchronizationContext chooses the appropriate context to schedule work on the relevant UI.",19094
68-7.5.1 Using mathematical patterns for better composition.pdf,68-7.5.1 Using mathematical patterns for better composition,"200 chapter  7 Task-based functional parallelism\ncreating a regular task, but the function passed with the ContinueWith  method takes as \nan argument a type of Task<Bitmap> . This argument represents the antecedent task, \nwhich can be used to inspect the status of the computation and branch accordingly.\nWhen the antecedent task completes, the function ContinueWith  starts execution \nas a new task. Task continuation runs in the captured current synchronization context, \nTaskScheduler.FromCurrentSynchronizationContext , which automatically chooses \nthe appropriate context to schedule work on the relevant UI thread. \nNOTE  When the ContinueWith  function is called, it’s possible to initiate starting \nthe new task only if the first task terminates with certain conditions, such as if the task is canceled, by specifying the \nTaskContinuationOptions.OnlyOn Canceled  \nflag, or if an exception is thrown, by using the TaskContinuationOptions  \n.OnlyOnFaulted  flag.\nAs previously mentioned, you could have used Parallel.ForEach , but the problem is \nthat this approach waits until all the operations have finished before continuing, block-ing the main thread. Moreover, it makes it more complex to update the UI directly because the operations run in different threads.\n7.5 Strategies for composing task operations \nContinuations are the real power of the TPL. It’s possible, for example, to execute multiple continuations for a single task and to create a chain of task continuations that maintains dependencies with each other. Moreover, using task continuation, the underlying scheduler can take full advantage of the work-stealing mechanism and opti-mize the scheduling mechanisms based on the available resources at runtime.\nLet’s use task continuation in the face-detection example. The final code runs in paral-\nlel, providing a boost in performance. But the program can be further optimized in terms of scalability. The function \nDetectFaces  sequentially performs the series of operations \nas a chain of computations. To improve resource use and overall performance, a better design is to split the tasks and subsequent task continuations for each \nDetectFaces  oper -\nation run in a different thread. \nUsing task continuation, this change is simple. The following listing shows a new \nDetectFaces  function, with each step of the face-detection algorithm running in a \ndedicated and independent task. \nListing 7.8  DetectFaces  function using task continuation\nTask<Bitmap> DetectFaces(string fileName){    var imageTask = Task.Run<Image<Bgr, byte>>(        () => new Image<Bgr, byte>(fileName)   );    var imageFrameTask = imageTask.ContinueWith(        image => image.Result.Convert<Gray, byte>()   );    \nUses task continuation to pass the result of the \nwork into the attached function without blocking \n \n 201 Strategies for composing task operations \n    var grayframeTask = imageFrameTask.ContinueWith(        imageFrame => imageFrame.Result.Convert<Gray, byte>()   );      var facesTask = grayframeTask.ContinueWith(grayFrame =>      {         var cascadeClassifier = CascadeClassifierThreadLocal.Value;         return cascadeClassifier.DetectMultiScale(            grayFrame.Result, 1.1, 3, System.Drawing.Size.Empty);       }   );            var bitmapTask = facesTask.ContinueWith(faces =>       {          foreach (var face in faces.Result)               imageTask.Result.Draw(               face, new Bgr(System.Drawing.Color.BurlyWood), 3);            return imageTask.Result.ToBitmap();         }      );           return bitmapTask;}\nThe code works as expected; the execution time isn’t enhanced, although the program can potentially handle a larger number of images to process while still maintaining lower resource consumption. This is due to the smart \nTaskScheduler  optimization. \nBecause of this, the code has become cumbersome and hard to change. For example, if you add error handling or cancellation support, the code becomes a pile of spaghetti code—hard to understand and to maintain. It can be better. Composition is the key to controlling complexity in software.\nThe objective is to be able to apply a LINQ-style semantic to compose the functions \nthat run the face-detection program, as shown here (the command and module names to note are in bold):\nfrom image in Task.Run<Emgu.CV.Image<Bgr, byte>()from imageFrame in Task.Run<Emgu.CV.Image<Gray, byte>>()from faces in Task.Run<System.Drawing.Rectangle[]>()select faces;\nThis is an example of how mathematical patterns can help to exploit declarative compositional semantics. \n7.5.1 Using mathematical patterns for better composition\nTask continuation provides support to enable task composition. How do you combine tasks? In general, function composition takes two functions and injects the result from the first function into the input of the second function, thereby forming one function. In chapter 2, you implemented this \nCompose  function in C# (in bold): \n Func<A, C> Compose<A, B, C>(this Func<A, B> f, Func<B, C> g) =>                                                  (n) => g(f(n)); Uses task continuation to pass the result of the \nwork into the attached function without blocking \n \n202 chapter  7 Task-based functional parallelism\nCan you use this function to combine two tasks? Not directly, no. First, the return type of the compositional function should be exposing the task’s elevated type as follows (noted in bold):\nFunc<A, Task<C>> Compose<A, B, C>(this Func<A, Task<B>> f,                                        Func<B, Task<C>> g) => (n) => g(f(n));\nBut there’s a problem: the code doesn’t compile. The return type from the function f \ndoesn’t match the input of the function g: the function f(n)  returns a type Task<B> , \nwhich isn’t compatible with the type B in function g. \nThe solution is to implement a function that accesses the underlying value of the \nelevated type (in this case, the task) and then passes the value into the next function. This is a common pattern, called Monad, in FP; the Monad pattern is another design pattern, like the Decorator and Adapter patterns. This concept was introduced in sec-tion 6.4.1, but let’s analyze this idea further so you can apply the concept to improve the face-detection code.\nMonads are mathematical patterns that control the execution of side effects by \nencapsulating program logic, maintaining functional purity, and providing a powerful compositional tool to combine computations that work with elevated types. According to the monad definition, to define a monadic constructor, there are two functions, \nBind  \nand Return , to implement.\nthe monadic  operators , bind and return\nBind  takes an instance of an elevated type, unwraps the underlying value, and then \ninvokes the function over the extracted value, returning a new elevated type. This func-tion is performed in the future when it’s needed. Here the \nBind  signature uses the \nTask  object as an elevated type: \nTask<R> Bind<T, R>(this Task<T> m, Func<T, Task<R>> k)\nThe Return  value is an operator that wraps any type T into an instance of the elevated \ntype. Following the example of the Task  type, here’s the signature: \nTask<T> Return(T value)\nNOTE  The same applies to other elevated types: for example, replacing the \nTask  type with another elevated type such as the Lazy  and Observable  types.\nthe monad  laws\nUltimately, to define a correct monad, the Bind  and Return  operations need to satisfy \nthe monad laws:\n1 Left identity—Applying the Bind  operation to a value wrapped by the Return oper -\nation and then passed into a function is the same as passing the value straight into the function: \nBind(Return value, function) = function(value)\n \n 203 Strategies for composing task operations \n2 Right identity—Returning a bind-wrapped value is equal to the wrapped value directly:   \nBind(elevated-value, Return) = elevated-value\n3 Associative—Passing a value into a function f whose result is passed into a second \nfunction g is the same as composing the two functions f and g and then passing \nthe initial value:\nBind(elevated-value, f(Bind(g(elevated-value)) =             Bind(elevated-value, Bind(f.Compose(g), elevated-value))\nNow, using these monadic operations, you can fix the error in the previous Compose  \nfunction to combine the Task  elevated types as shown here:\nFunc<A, Task<C>> Compose<A, B, C>(this Func<A, Task<B>> f,                               Func<B, Task<C>> g) => (n) => Bind(f(n), g);\nMonads are powerful because they can represent any arbitrary operations against ele-vated types. In the case of the \nTask  elevated type, monads let you implement function \ncombinators to compose asynchronous operations in many ways, as shown in figure 7.8. \n41 + 1Apply\nfunction\nUnwrap\nvalueRewrap\nvalue\nMonadic\nbind\nTask<int>(41) int       Ta sk<int>M a (a -> M b) M b\nx     T ask<int>(x => x + 1) x     T ask<int>(x => x + 1) Bind(T ask<int>(41),Task<int>(42)\nFigure 7.8  The monadic Bind  operator takes the elevated value Task  that acts as a container \n(wrapper) for the value 42, and then it applies the function x ➔ Task<int>(x => x + 1) , where x is \nthe number 41 unwrapped. Basically, the Bind  operator unwraps an elevated value ( Task<int>(41) ) \nand then applies a function ( x + 1 ) to return a new elevated value ( Task<int>(42 ).\nSurprisingly, these monadic operators are already built into the .NET Framework in the form of LINQ operators. The LINQ \nSelectMany definition corresponds directly \nto the monadic Bind function. Listing 7.9 shows both the Bind  and Return  opera-\ntors applied to the Task  type. The functions are then used to implement a LINQ-style \nsemantic to compose asynchronous operations in a monadic fashion. The code is in F# and then consumed in C# to keep proving the easy interoperability between these programming languages (the code to note is in bold).\nListing 7.9  Task extension in F# to enable LINQ-style operators for tasks\n[<Sealed; Extension; CompiledName(""Task"")>]type TaskExtensions =  // 'T -> M<'T>  static member Return value : Task<'T> = Task.FromResult<'T> (value) The Return monadic operator takes \nany type T and returns a Task<T>.\n \n204 chapter  7 Task-based functional parallelism\n  // M<'T> * ('T -> M<'U>) -> M<'U>  static member Bind (input : Task<'T>, binder : 'T -> Task<'U>) =         let tcs = new TaskCompletionSource<'U>()            input.ContinueWith(fun (task:Task<'T>) ->           if (task.IsFaulted) then                 tcs.SetException(task.Exception.InnerExceptions)           elif (task.IsCanceled) then                 tcs.SetCanceled()           else                try                   (binder(task.Result)).ContinueWith(fun \n➥ (nextTask:Task<'U>) -> tcs.SetResult(nextTask.Result)) |> ignore \n                with                | ex -> tcs.SetException(ex)) |> ignore        tcs.Task  static member Select (task : Task<'T>, selector : 'T -> 'U) : Task<'U> =        task.ContinueWith(fun (t:Task<'T>) -> selector(t.Result))  static member SelectMany(input:Task<'T>, binder:'T -> Task<'I>, \nprojection:'T -> 'I -> 'R): Task<'R> =\n        TaskExtensions.Bind(input,            fun outer -> TaskExtensions.Bind(binder(outer), fun inner ->                TaskExtensions.Return(projection outer inner)))   static member SelectMany(input:Task<'T>, binder:'T -> Task<'R>) : Task<'R> \n=\n        TaskExtensions.Bind(input,            fun outer -> TaskExtensions.Bind(binder(outer), fun inner ->                TaskExtensions.Return(inner))) \nThe implementation of the Return  operation is straightforward, but the Bind  opera-\ntion is a little more complex. The Bind  definition can be reused to create other LINQ-\nstyle combinators for tasks, such as the Select  and two variants of the SelectMany  \noperators. In the body of the function Bind , the function ContinueWith , from the \nunderlying task instance, is used to extract the result from the computation of the input task. Then to continue the work, it applies the binder function to the result of the input \ntask.  Ultimately, the output of the nextTask  continuation is set as the result \nof the tcs TaskCompletionSource . The returning task is an instance of the underly-\ning TaskCompletionSource , which is introduced to initialize a task from any opera-\ntion that starts and finishes in the future. The idea of the TaskCompletionSource  is to \ncreate a task that can be governed and updated manually to indicate when and how a given operation completes. The power of the \nTaskCompletionSource  type is in the \ncapability of creating tasks that don’t tie up threads. The Bind operator takes a Task object as an \nelevated type, applies a function to the underlying \ntype, and returns a new elevated type Task<U>\nTaskCompletionSource initializes a behavior in the form of Task, so it can be treated like one.The Bind operator unwraps the result from \nthe Task elevated type and passes the result \ninto the continuation that executes the \nmonadic function binder.\nThe LINQ SelectMany operator acts as \nthe Bind monadic operator.\n \n 205 Strategies for composing task operations \nTaskCompletionSource\nThe purpose of the TaskCompletionSource<T>  object is to provide control and refer to \nan arbitrary asynchronous operation as a Task<T>.  When a TaskCompletionSource \n(http:/ /bit.ly/2vDOmSN ) is created, the underlying task properties are accessible \nthrough a set of methods to manage the lifetime and completion of the task. This includes \nSetResult , SetException , and SetCanceled . \n \napplying  the monad  pattern  to task operations  \nWith the LINQ operations SelectMany  on tasks in place, you can rewrite the DetectFaces  \nfunction using an expressive and comprehension query (the code to note is in bold). \nListing 7.10  DetectFaces  using task continuation based on a LINQ expression\nTask<Bitmap> DetectFaces(string fileName)  {     Func<System.Drawing.Rectangle[],Image<Bgr, byte>, Bitmap> \n➥ drawBoundries = \n        (faces, image) => {              faces.ForAll(face => image.Draw(face, new  \n➥ Bgr(System.Drawing.Color.BurlyWood), 3));  \n             return image.ToBitmap();      };         return from image in Task.Run(() => new Image<Bgr, byte>(fileName))                from imageFrame in Task.Run(() => image.Convert<Gray, \nbyte>())\n                from bitmap in Task.Run(() =>           CascadeClassifierThreadLocal.Value.DetectMultiScale(imageFrame,\n➥ 1.1, 3, System.Drawing.Size.Empty)).Select(faces => \n                                             drawBoundries(faces, image))                select bitmap;    }\nThis code shows the power of the monadic pattern, providing composition semantics over elevated types such as tasks. Moreover, the code of the monadic operations is con-centrated into the two operators \nBind  and Return , making the code maintainable and \neasy to debug. To add logging functionality or special error handling, for example, you only need to change one place in code, which is convenient. \nIn listing 7.10, the \nReturn  and Bind  operators were exposed in F# and consumed in \nC#, as a demonstration of the simple interoperability between the two programming languages. The source code for this book contains the implementation in C#. A beau-tiful composition of elevated types requires monads; the continuation monad shows how monads can readily express complex computations.\nusing the hidden  fmap functor  pattern  to apply  transformation  \nOne important function in FP is Map, which transforms one input type into a different \none. The signature of the Map function is\nMap :  (T -> R) -> [T] -> [R]The detected face(s) are highlighted \nusing a box that’s drawn around them.\nTask composition using the LINQ-like Task operators defined with the Task monadic operators\n \n206 chapter  7 Task-based functional parallelism\nAn example in C# is the LINQ Select  operator, which is a map function for IEnumer-\nable  types:\nIEnumerable<R>  Select<T,R>(IEnumerable<T> en, Func<T, R> projection)\nIn FP, this similar concept is called a functor, and the map function is defined as fmap . \nFunctors are basically types that can be mapped over. In F#, there are many:\nSeq.map : ('a -> 'b) -> 'a seq -> 'b seq List.map : ('a -> 'b) -> 'a list -> 'b list Array.map : ('a -> 'b) -> 'a [] -> 'b []Option.map : ('a -> 'b) -> 'a Option -> 'b Option \nThis mapping idea seems simple, but the complexity starts when you have to map ele-vated types. This is when the functor pattern becomes useful.\nThink about a functor as a container that wraps an elevated type and offers a way to \ntransform a normal function into one that operates on the contained values. In the case of the \nTask  type, this is the signature:\nfmap : ('T -> 'R) -> Task<'T> -> Task<'R>\nThis function has been previously implemented for the Task  type in the form of the \nSelect  operator as part of the LINQ-style operators set for tasks built in F#. In the last \nLINQ expression computation of the function DetectFaces , the Select  operator proj-\nects (map) the input Task<Rectangle[]>  into a Task<Bitmap> :\nfrom image in Task.Run(() => new Image<Bgr, byte>(fileName))from imageFrame in Task.Run(() => image.Convert<Gray, byte>())from bitmap in Task.Run(() =>             CascadeClassifierThreadLocal.Value.DetectMultiScale                        (imageFrame, 1.1, 3, System.Drawing.Size.Empty))     .select(faces => drawBoundries(faces, image))select bitmap;\nThe concept of functors becomes useful when working with another functional pat-tern, applicative functors, which will be covered in chapter 10.\nNOTE  The concepts of functors and monads come from the branch of math-\nematics called category theory,2 but it isn’t necessary to have any category theory \nbackground to follow and use these patterns.\nthe abilities  behind  monads  \nMonads provide an elegant solution to composing elevated types. Monads aim to con-\ntrol functions with side effects, such as those that perform I/O operations, providing a mechanism to perform operations directly on the result of the I/O without having a value from impure functions floating around the rest of your pure program. For this reason, monads are useful in designing and implementing concurrent applications. \n2 For more information, see https:/ /wiki.haskell.org/Category_theory .",18436
69-7.5.2 Guidelines for using tasks.pdf,69-7.5.2 Guidelines for using tasks,,0
70-7.6 The parallel functional Pipeline pattern.pdf,70-7.6 The parallel functional Pipeline pattern,"207 The parallel functional Pipeline pattern \n7.5.2 Guidelines for using tasks \nHere are several guidelines for using tasks: \n¡ It’s good practice to use immutable types for return values. This makes it easier to ensure that your code is correct. \n¡ It’s good practice to avoid tasks that produce side effects; instead, tasks should communicate with the rest of the program only with their returned values.\n¡ It’s recommended that you use the task continuation model to continue with the computation, which avoids unnecessary blocking.\n7.6 The parallel functional Pipeline pattern \nIn this section, you’re going to implement one of the most common coordination techniques—the Pipeline pattern. In general, a pipeline is composed of a series of computational steps, composed as a chain of stages, where each stage depends on the output of its predecessor and usually performs a transformation on the input data. You can think of the Pipeline pattern as an assembly line in a factory, where each item is constructed in stages. The evolution of an entire chain is expressed as a function, and it uses a message queue to execute the function each time new input is received. The message queue is non-blocking because it runs in a separate thread, so even if the stages of the pipeline take a while to execute, it won’t block the sender of the input from pushing more data to the chain.\nThis pattern is similar to the Producer/Consumer pattern, where a producer man-\nages one or more worker threads to generate data. There can be one or more consum-ers that consume the data being created by the producer. Pipelines allow these series to run in parallel. The implementation of the pipeline in this section follows a slightly different design as compared to the traditional one seen in figure 7.9. \nThe traditional Pipeline pattern with serial stages has a speedup, measured in \nthroughput, which is limited to the throughput of the slowest stage. Every item pushed into the pipeline must pass through that stage. The traditional Pipeline pattern cannot scale automatically with the number of cores, but is limited to the number of stages. Only a linear pipeline, where the number of stages matches the number of available logical cores, can take full advantage of the computer power. In a computer with eight cores, a pipeline composed of four stages can use only half of the resources, leaving 50% of the cores idle. \nFP promotes composition, which is the concept the Pipeline pattern is based on. In \nlisting 7.11, the pipeline embraces this tenet by composing each step into a single func-tion and then distributing the work in parallel, fully using the available resources. In an abstract way, each function acts as the continuation of the previous one, behaving as a continuation-passing style. The code listing implementing the pipeline is in F#, then consumed in C#. But in the downloadable source code, you can find the full imple-mentation in both programming languages. Here  the \nIPipeline  interface defines the \nfunctionality of the pipeline.\n \n208 chapter  7 Task-based functional parallelism\nTask 2\nTask 3Task 1 Stage 1 Buffer\nResult 2\nResult 3Result 1\nResult 2\nResult 3Result 1Stage 3\nWorker 1\nWorkItem 1\nWorkItem 2WorkItem 1Stage 1\nStage 2\nStage 3WorkItem 2\nWorker 2Traditional parallel pipeline\nThe pipeline creates a buf fer between each stage that works as a parallel Producer/Consumer pattern. There\nare almost as many buf fers as there are stages. Each work item is sent to stage 1; the result is passe d\ninto the first buf fer, which coordinates the work in parallel to push it into stage 2. This process\ncontinues until the end of the pipeline, when all the stages are computed.\nStage 2 Buffer\nFunctional parallel pipeline\nThe pipeline combines all the stages into one, as if composing multiple functions .\nEach work item is pushed into the combined steps to be processed\nin parallel, using the TPL and the optimized scheduler .\nWorkItem 1\nWorkItem 2Task 2\nTask 3Task 1\nFigure 7.9  The traditional pipeline creates a buffer between each stage that works as a parallel Producer/\nConsumer pattern. There are almost as many buffers as there are number of stages. With this design, each work item to process is sent to the initial stage, then the result is passed into the first buffer, which coordinates the work in parallel to push it into the second stage. This process continues until the end of the pipeline when all the stages are computed. By contrast, the functional parallel pipeline combines all the stages into one, as if composing multiple functions. Then, using a \nTask  object, each work item is pushed \ninto the combined steps to be processed in parallel and uses the TPL and the optimized scheduler.\nListing 7.11  IPipeline  interface\n[<Interface>]type IPipeline<'a,'b> =        abstract member Then : Func<'b, 'c> -> IPipeline<'a,'c>      abstract member Enqueue : 'a * Func<('a * 'b), unit)> -> unit Interface that defines the pipeline contract\nUses a function to expose a fluent API approach Uses a function to push new input to \nprocess into the pipeline\n \n 209 The parallel functional Pipeline pattern \n    abstract member Execute : (int * CancellationToken) -> IDisposable     abstract member Stop : unit -> unit     \nThe function Then  is the core of the pipeline, where the input function is composed of \nthe previous one, applying a transformation. This function returns a new instance of the pipeline, providing a convenient and fluent API to build the process.\nThe \nEnqueue  function is responsible for pushing work items into the pipeline for \nprocessing. It takes a Callback  as an argument, which is applied at the end of the pipe-\nline to further process the final result. This design gives flexibility to apply any arbitrary function for each item pushed. \nThe \nExecute  function starts the computation. Its input arguments set the size of the \ninternal buffer and a cancellation token to stop the pipeline on demand. This function returns an \nIDisposable  type, which can be used to trigger the cancellation token to \nstop the pipeline. Here is the full implementation of the pipeline (the code to note is in bold).\nListing 7.12  Parallel functional pipeline pattern  \n[<Struct>]type Continuation<'a, 'b>(input:'a, callback:Func<('a * 'b), unit) =    member this.Input with get() = input    member this.Callback with get() = callback    \ntype Pipeline<'a, 'b> private (func:Func<'a, 'b>) as this =    let continuations = Array.init 3 (fun _ -> new                    BlockingCollection<Continuation<'a,'b>>(100))        let then' (nextFunction:Func<'b,'c>) =        Pipeline(func.Compose(nextFunction)) :> IPipeline<_,_>    \n    let enqueue (input:'a) (callback:Func<('a * 'b), unit>) =        BlockingCollection<Continuation<_,_>>.AddToAny(continuations,\n➥ Continuation(input, callback))    \n    let stop() = for continuation in continuations do continuation.\nCompleteAdding() Starts the pipeline \nexecution The pipeline can be stopped at any time; this \nfunction triggers the underlying cancellation token. \nThe Continuation struct encapsulates the \ninput value for each task with the callback \nto run when the computation completes.Initializes the \nBlockingCollection \nthat buffers the work\nUses function composition to combine the \ncurrent function of the pipeline with the new \none passed and returns a new pipeline. The \ncompose function was introduced in chapter 2.\nThe Enqueue function pushes the work into the buffer. \nThe BlockingCollection is notified to complete, which stops the pipeline.\n \n210 chapter  7 Task-based functional parallelism\n    let execute blockingCollectionPoolSize \n(cancellationToken:CancellationToken) =\n        cancellationToken.Register(Action(stop)) |> ignore          for i = 0 to blockingCollectionPoolSize - 1 do            Task.Factory.StartNew(fun ( )->                 while (not <| continuations.All(fun bc -> bc.IsCompleted))                   && (not <| cancellationToken.IsCancellationRequested) do                    let continuation = ref \n➥ Unchecked.defaultof<Continuation<_,_>>\n                    BlockingCollection.TakeFromAny(continuations, \n➥ continuation)\n                    let continuation = continuation.Value                    continuation.Callback.Invoke(continuation.Input, \n➥ func.Invoke(continuation.Input)),\n            cancellationToken, TaskCreationOptions.LongRunning,\n➥ TaskScheduler.Default) |> ignore\n    static member Create(func:Func<'a, 'b>) =        Pipeline(func) :> IPipeline<_,_>     interface IPipeline<'a, 'b> with       member this.Then(nextFunction) = then' nextFunction       member this.Enqueue(input, callback) = enqueue input callback                 member this.Stop() = stop()       member this.Execute (blockingCollectionPoolSize,cancellationToken) =             execute blockingCollectionPoolSize cancellationToken            { new IDisposable with member self.Dispose() = stop() } \nThe Continuation  structure is used internally to pass through the pipeline functions \nto compute the items. The implementation of the pipeline uses an internal buffer com-posed by an array of the concurrent collection \nBlockingCollection<Collection> , \nwhich ensures thread safety during parallel computation of the items. The argument to this collection constructor specifies the maximum number of items to buffer at any given time. In this case, the value is \n100 for each buffer. \nEach item pushed into the pipeline is added to the collection, which in the future \nwill be processed in parallel. The Then  function is composing the function argument \nnextFunction with the function func , which is passed into the pipeline constructor. \nNote that you use the Compose  function defined in chapter 2 in listing 2.3 to combine \nthe functions func  and nextFunction :\nFunc<A, C> Compose<A, B, C>(this Func<A, B> f, Func<B, C> g) => (n) => g(f(n));\nWhen the pipeline starts the process, it applies the final composed function to each input value. The parallelism in the pipeline is achieved in the \nExecute  function, which \nspawns one task for each BlockingCollection  instantiated. This guarantees a buffer \nfor running the thread. The tasks are created with the LongRunning  option to schedule \na dedicated thread. The BlockingCollection  concurrent collection allows thread-safe \naccess to the items stored using the static methods TakeFromAny and AddToAny , which Registers the cancellation token to run \nthe stop function when it’s triggered \nStarts the tasks to compute in parallel \nThe static method creates a new instance of the pipeline.\n \n 211 The parallel functional Pipeline pattern \ninternally distribute the items and balance the workload among the running threads. This collation is used to manage the connection between the input and output of the pipeline, which behave as producer/consumer threads. \nNOTE  Using BlockingCollection , remember to call GetConsumingEnumer -\nable  because the BlockingCollection  class implements IEnumerable<T>.  \nEnumerating over the blocking collection instance won’t consume values. \nThe pipeline constructor is set as private  to avoid direct instantiation. Instead, the \nstatic method Create  initializes a new instance of the pipeline. This facilitates a fluent \nAPI approach to manipulate the pipeline.\nThis pipeline design ultimately resembles a parallel Produce/Consumer pattern \ncapable of managing the concurrent communication between many-producers to many-consumers.\nThe following listing uses the implemented pipeline to refactor the \nDetectFaces  \nprogram from the previous section. In C#, a fluent API approach is a convenient way to express and compose the steps of the pipeline.\nListing 7.13  Refactored DetectFaces  code using the parallel pipeline\nvar files = Directory.GetFiles(ImagesFolder);var imagePipe = Pipeline<string, Image<Bgr, byte>>    .Create(filePath => new Image<Bgr, byte>(filePath))    .Then(image => Tuple.Create(image, image.Convert<Gray, byte>()))    .Then(frames => Tuple.Create(frames.Item1,        CascadeClassifierThreadLocal.Value.DetectMultiScale(frames.Item2, 1.1,          3, System.Drawing.Size.Empty)))     .Then(faces =>{           foreach (var face in faces.Item2)               faces.Item1.Draw(face, \n➥ new Bgr(System.Drawing.Color.BurlyWood), 3);\n           return faces.Item1.ToBitmap();         }); imagePipe.Execute(cancellationToken);  foreach (string fileName in files)   imagePipe.Enqueue(file, (_, bitmapImage)                                     => Images.Add(bitmapImage)); \nBy exploiting the pipeline you developed, the code structure is changed considerably.\nNOTE  The F# pipeline implementation, in the previous section, uses the Func  \ndelegate to be consumed effortlessly by C# code. In the source code of the book you can find the implementation of the same pipeline that uses F# func-tions in place of the .NET \nFunc  delegate, which makes it a better fit for proj-\nects completely built in F#. In the case of consuming native F# functions from C#, the helper extension method \nToFunc  provides support for interoperability. \nThe ToFunc  extension method can be found in the source code.Constructs the pipeline using fluent API. \nStarts the execution of the pipeline. The cancellation token stops the pipeline at any given time.\nThe iteration pushes (enqueues) the \nfile paths into the pipeline queue, \nwhose operation is non-blocking.\n \n212 chapter  7 Task-based functional parallelism\nThe pipeline definition is elegant, and it can be used to construct the process to detect the faces in the images using a nice, fluent API. Each function is composed step by step, and then the \nExecute  function is called to start the pipeline. Because the underlying \npipeline processing is already running in parallel, the loop to push the file path of the images is sequential. The \nEnqueue  function of the pipeline is non-blocking, so there are \nno performance penalties involved. Later, when an image is returned from the computa-tion, the \nCallback  passed into the Enqueue  function will update the result to update the \nUI. Table 7.1 shows the benchmark to compare the different approaches implemented. \nTable 7.1  Benchmark processing of 100 images using four logical core computers with 16 GB RAM. The \nresults, expressed in seconds, represent the average from running each design three times.\nSerial loop Parallel Parallel \ncontinuationParallel LINQ \ncombinationParallel pipeline\n68.57 22.89 19.73 20.43 17.59\nThe benchmark shows that, over the average of downloading 100 images for three times, the pipeline parallel design is the fastest. It’s also the most expressive and concise pattern.\nSummary\n¡ Task-based parallel programs are designed with the functional paradigm in mind to guarantee more reliable and less vulnerable (or corrupt) code from functional properties such as immutability, isolation of side effects, and defensive copy. This makes it easier to ensure that your code is correct. \n¡ The Microsoft TPL embraces functional paradigms in the form of using a con-tinuation-passing style. This allows for a convenient way to chain a series of non-blocking operations. \n¡ A method that returns void  in C# code is a string signal that can produce side \neffects. A method with void  as output doesn’t permit composition in tasks using \ncontinuation. \n¡ FP unmasks mathematical patterns to ease parallel task composition in a declar -\native and fluent programming style. (The monad and functor patterns are hid-den in LINQ.) The same patterns can be used to reveal monadic operations with tasks, exposing a LINQ-semantic style.\n¡ A functional parallel pipeline is a pattern designed to compose a series of oper -\nations into one function, which is then applied concurrently to a sequence of input values queued to be processed. Pipelines are often useful when the data elements are received from a real-time event stream.\n¡ Task dependency is the Achilles heel of parallelism. Parallelism is restricted when two or more operations cannot run until other operations have completed. It’s essential to use tools and patterns to maximize parallelism as much as possible. A functional pipeline, CPS, and mathematical patterns like monad are the keys.",16368
71-8.1.1 The value of asynchronous programming.pdf,71-8.1.1 The value of asynchronous programming,"2138Task asynchronicity \nfor the win\nThis chapter covers\n¡ Understanding the Task-based Asynchronous Programming model (TAP) \n¡ Performing numerous asynchronous operations in parallel\n¡ Customizing asynchronous execution flow \nAsynchronous programming has become a major topic of interest over the last sev-eral years. In the beginning, asynchronous programming was used primarily on the client side to deliver a responsive GUI and to convey a high-quality user experience for customers. To maintain a responsive GUI, asynchronous programming must have consistent communication with the backend, and vice versa, or delays may be introduced into the response time. An example of this communication issue is when an application window appears to hang for a few seconds while background process-ing catches up with your commands. \nCompanies must address increasing client demands and requests while analyzing \ndata quickly. Using asynchronous programming on the application’s server side is the solution to allowing the system to remain responsive, regardless of the number of requests. Moreover, from a business point of view, an Asynchronous Programming \n \n214 chapter  8 Task asynchronicity for the win\nModel (APM) is beneficial. Companies have begun to realize that it’s less expensive to develop software designed with this model because the number of servers required to satisfy requests is considerably reduced by using a non-blocking (asynchronous) I/O system compared to a system with blocking (synchronous) I/O operations. Keep in \nmind that scalability and asynchronicity are terms unrelated to speed or velocity. Don’t worry if these terms are unfamiliar; they’re covered in the following sections. \nAsynchronous programming is an essential addition to your skill set as a developer \nbecause programming robust, responsive, and scalable programs is, and will continue to be, in high demand. This chapter will help you understand the performance seman-tics related to APM and how to write scalable applications. By the end of this chapter, you’ll know how to use asynchronicity to process multiple I/O operations in parallel regardless of the hardware resources available.\n8.1 The Asynchronous Programming Model (APM)\nThe word asynchronous derives from the combination of the Greek words asyn (meaning “not with”) and chronos (meaning “time”), which describe actions that aren’t occurring at the same time. In the context of running a program asynchronously, asynchronous  \nrefers to an operation that begins with a specific request, which may or may not succeed, and that completes at a point in the future. In general, asynchronous operations are executed independently from other processes without waiting for the result, whereas synchronous operations wait to finish before moving on to another task. \nImagine yourself in a restaurant with only one server. The server comes to your table \nto take the order, goes to the kitchen to place the order, and then stays in the kitchen, waiting for the meal to be cooked and ready to serve! If the restaurant had only one table, this process would be fine, but what if there are numerous tables? In this case, the process would be slow, and you wouldn’t receive good service. A solution is to hire more waiters, maybe one per table, which would increase the restaurant’s overhead due to increased salaries and would be wildly inefficient. A more efficient and effective solution is to have the server deliver the order to the chef in the kitchen and then con-tinue to serve other tables. When the chef has finished preparing the meal, the waiter receives a notification from the kitchen to pick up the food and deliver it to your table. In this way, the waiter can serve multiple tables in a timely fashion. \nIn computer programming, the same concept applies. Several operations are per -\nformed asynchronously, from starting execution of an operation to continuing to pro-cess other work while waiting for that operation to complete, then resuming execution once the data has been received. \nNOTE  The term continuing refers to continuation-passing style (CPS), which is \na form of programming where a function determines what to do next and can decide to continue the operation or to do something completely different. As you’ll see shortly, the APM is based on CPS (discussed in chapter 3).\nAsynchronous programs don’t sit idly waiting for any one operation, such as request-ing data from a web service or querying a database, to complete.\n \n 215 The Asynchronous Programming Model (APM)\n8.1.1 The value of asynchronous programming\nAsynchronous programming is an excellent model to exploit every time you build a \nprogram that involves blocking I/O operations. In synchronous programming, when a method is called, the caller is blocked until the method completes its current execu-tion. With I/O operations, the time that the caller must wait before a return of control to continue with the rest of code varies depending on the current operation in process. \nOften applications use a large number of external services, which perform opera-\ntions that take a user noticeable time to execute. For this reason, it’s vital to program in an asynchronous way. In general, developers feel comfortable when thinking sequen-tially: send a request or execute a method, wait for the response, and then process it. But a performant and scalable application cannot afford to wait synchronously for an action to complete. Furthermore, if an application joins the results of multiple operations, it’s necessary to perform all of these operations simultaneously for good performance.\nWhat happens if control never comes back to the caller because something went \nwrong during the I/O operation? If the caller never receives a return of control, then the program could hang. \nLet’s consider a server-side, multiuser application. For example, a regular e-com-\nmerce website application exists where, for each incoming request, the program has to make a database call. If the program is designed to run synchronously (figure 8.1), then only one dedicated thread is committed for each incoming request. In this case, each additional database call blocks the current thread that owns the incoming request, while waiting for the database to respond with the result. During this time, the thread pool must create a new thread to satisfy each incoming request, which will also block program execution while waiting for the database response. \nIf the application receives a high volume of requests (hundreds or perhaps thou-\nsands) simultaneously, the system will become unresponsive while trying to create the many threads needed to handle the requests. It will continue in this way until reaching the thread-pool limit, now with the risk of running out of resources. These circum-stances can lead to large memory consumption or worse, failure of the system. \nWhen the thread-pool resources are exhausted, successive incoming requests are \nqueued and waiting to be processed, which results in an unreactive system. More impor -\ntantly, when the database responses come back, the blocked threads are freed to con-tinue to process the requests, which can provoke a high frequency of context switches, negatively impacting performance. Consequently, the client requests to the website slow down, the UI turns unresponsive, and ultimately your company loses potential cus-tomers and revenue.\n \n216 chapter  8 Task asynchronicity for the win\nA\nB\nC\nB\nA\nD\nE\nCSynchronous I/O 1. Each new request begins\n   processing while the caller   waits for a response.2. A new thread is\n    created to r un each\n    database quer y.\n3. The process is synchronous.    Threads must wait for a database    response, and the system needs    more threads to handle new    incoming requests. 4. When multiple threads are created,\n    system per formance is negatively\n    affected due to increased context    switching and a high volume of    memor y consumption.\nIncoming\nrequestOS\nscheduler\nDatabaseOS memory\nFigure 8.1  Servers that handle incoming requests synchronously aren’t scalable. \nClearly, efficiency is a major reason to asynchronously model operations so that threads don’t need to wait for I/O operations to complete, allowing them to be reused by the scheduler to serve other incoming requests. When a thread that has been deployed for an asynchronous I/O operation is idle, perhaps waiting for a database response as in fig-ure 8.1, the scheduler can send the thread back to the thread pool to engage in further work. When the database completes, the scheduler notifies the thread pool to wake an available thread and send it on to continue the operation with the database result. \nIn server-side programs, asynchronous programming lets you deal effectively with \nmassive concurrent I/O operations by intelligently recycling resources during their idle time and by avoiding the creation of new resources (figure 8.2). This optimizes memory consumption and enhances performance.\nUsers ask much from the modern applications they must interact with. Modern \napplications must communicate with external resources, such as databases and web services, and work with disks or REST-based APIs to meet user demands. Also, today’s applications must retrieve and transform massive amounts of data, cooperate in cloud computations, and respond to notifications from parallel processes. To accommodate these complex interactions, the APM provides the ability to express computations with-out blocking executing threads, which improves availability (a reliable system) and throughput. The result is notably improved performance and scalability. \nThis is particularly relevant on servers where there can be a large amount of concur -\nrent I/O-bound activity. In this case the APM can handle many concurrent operations with low memory consumption due to the small number of threads involved. Even in",9976
72-8.3 Asynchronous support in .NET.pdf,72-8.3 Asynchronous support in .NET,"217 The Asynchronous Programming Model (APM)\nthe case where there aren’t many (thousands) concurrent operations, the synchronous approach is advantageous because it keeps the I/O-bound operations performing out of the .NET thread pool. \nA\nC\nE\nA\nC\nE\nDAsynchronous I/O1. Each new request begins\n   processing. The caller is free   to per form other work without\n   waiting (synchronously)   for a response.2. Database queries ru n\n    asynchronously , so the\n    OS scheduler can optimize    resource utilization.4. The OS scheduler optimizes thread\n    utilization and recycling, which    minimizes memor y consumption\n    and keeps the system responsive.\nIncoming\nrequestOS\nscheduler\nDatabaseOS memory\nB\nD\nB 3. The OS scheduler is notified when \nthe asynchronous work completes. Then, a thread is scheduled to continue the \noriginal process. \nFigure 8.2  Asynchronous I/O operations can start several operations in parallel without constraints \nthat will return to the caller when complete, which keeps the system scalable.\nBy enabling asynchronous programming in your application, your code derives several benefits:\n¡ Decoupled operations do a minimum amount of work in performance-critical paths.\n¡ Increased thread resource availability allows the system to reuse the same resources without the need to create new ones. \n¡ Better employment of the thread-pool scheduler enables scalability in server- based programs.\n8.1.2 Scalability and asynchronous programming\nScalability refers to a system with the ability to respond to an increased number of requests through the addition of resources, which affects a commensurate boost in parallel speedup. A system designed with this ability aims to continue performing well under the circumstance of a sustained, large number of incoming requests that can strain the application’s resources. Incremental scalability is achieved by different \n \n218 chapter  8 Task asynchronicity for the win\ncomponents—memory and CPU bandwidth, workload distribution, and quality of code, for example. If you design your application with the APM, it’s most likely scalable. \nKeep in mind that scalability isn’t about speed. In general, a scalable system doesn’t \nnecessarily run faster than a non-scalable system. In fact, an asynchronous operation doesn’t perform any faster than the equivalent synchronous operation. The true ben-efit is in minimizing performance bottlenecks in the application and optimizing the consumption of resources that allow other asynchronous operations to run in parallel, ultimately performing faster.\nScalability is vital in satisfying today’s increasing demands for instantaneous respon-\nsiveness. For example, in high-volume web applications, such as stock trading or social media, it’s essential that applications be both responsive and capable of concurrently managing a massive number of requests. Humans naturally think sequentially, evaluat-ing one action at a time in consecutive order. For the sake of simplicity, programs have been written in this manner, one step following the other, which is clumsy and time- consuming. The need exists for a new model, the APM, that lets you write non-blocking applications that can run out of sequence, as required, with unbounded power.   \n8.1.3 CPU-bound and I/O-bound operations\nIn CPU-bound computations, methods require CPU cycles where there’s one thread running on each CPU to do the work. In contrast, asynchronous I/O-bound computa-tions are unrelated to the number of CPU cores. Figure 8.3 shows the comparison. As previously mentioned, when an asynchronous method is called, the execution thread returns immediately to the caller and continues execution of the current method, while the previously called function runs in the background, thereby preventing blocking. The terms non-blocking and asynchronous are commonly used interchangeably because both define similar concepts.\nCPU-bound\nCPUCPU-bound computations receive input\nfrom the keyboard to do some work, andthen print the result to the screen. In asingle-core machine, each computationmust be completed before proceedingto the next one. I/O-bound computations are executed independentl y\nfrom the CPU, and the operation is done elsewhere.In this case several asynchronous database calls areexecuting simultaneously . Later , a notification will\ninform the caller when the operation iscomplete (a callback).\nI/O-bound\nCPU\nDatabase\nFigure 8.3  Comparison between CPU-bound and I/O-bound operations\n \n 219 Unbounded parallelism with asynchronous programming\nCPU-bound computations are operations that spend time performing CPU- intensive \nwork, using hardware resources to run all the operations. Therefore, as a ratio, it’s appropriate to have one thread for each CPU, where execution time is determined by the speed of each CPU. Conversely, with I/O-bound computations, the number of threads running is unrelated to the number of CPUs available, and execution time depends on the period spent waiting for the I/O operations to complete, bound only by the I/O drivers. \n8.2 Unbounded parallelism with asynchronous programming\nAsynchronous programming provides an effortless way to execute multiple tasks inde-\npendently and, therefore, in parallel. You may be thinking about CPU-bound compu-tations that can be parallelized using a task-based programming model (chapter 7). But what makes an APM special, as compared to CPU-bound computation, is its I/O-bound computation nature, which overcomes the hardware constraint of one working thread for each CPU core. \nAsynchronous, non-CPU-bound computations can benefit from having a larger \nnumber of threads running on one CPU. It’s possible to perform hundreds or even thousands of I/O operations on a single-core machine because it’s the nature of asyn-chronous programming to take advantage of parallelism to run I/O operations that can outnumber the available cores in a computer by an order of magnitude. You can do this because the asynchronous I/O operations push the work to a different location without impacting local CPU resources, which are kept free, providing the opportunity to execute additional work on local threads. To demonstrate this unbounded power, listing 8.1 is an example of running 20 asynchronous operations (in bold). These oper -\nations can run in parallel, regardless of the number of available cores.\nListing 8.1  Parallel asynchronous computations\nlet httpAsync (url : string) = async {        let req = WebRequest.Create(url)    let! resp = req.AsyncGetResponse()    use stream = resp.GetResponseStream()    use reader = new StreamReader(stream)    let! text = reader.ReadToEndAsync()    return text }let sites =            [ ""http://www.live.com"";""      ""http://www.fsharp.org"";      ""http://news.live.com"";      ""http://www.digg.com"";      ""http://www.yahoo.com"";      ""http://www.amazon.com""      ""http://news.yahoo.com"";     ""http://www.microsoft.com"";      ""http://www.google.com"";     ""http://www.netflix.com"";      ""http://news.google.com"";    ""http://www.maps.google.com"";      ""http://www.bing.com"";       ""http://www.microsoft.com"";      ""http://www.facebook.com"";   ""http://www.docs.google.com"";      ""http://www.youtube.com"";    ""http://www.gmail.com"";      ""http://www.reddit.com"";     ""http://www.twitter.com""; ]Reads asynchronously content of a given website\nLists arbitrary websites to download \n \n220 chapter  8 Task asynchronicity for the win\nsites|> Seq.map httpAsync   |> Async.Parallel      |> Async.RunSynchronously  \nIn this full asynchronous implementation example, the execution time is 1.546 sec-onds on a four-core machine. The same synchronous implementation runs in 11.230 seconds (the synchronous code is omitted, but you can find it in the source code com-panion of this book). Although the time varies according to network speed and band-width, the asynchronous code is about 7× faster than the synchronous code. \nIn a CPU-bound operation running on a single-core device, there’s no performance \nimprovement in simultaneously running two or more threads. This can reduce or decrease performance due to the extra overhead. This also applies to multicore processors, where the number of threads running far exceeds the number of cores. Asynchronicity doesn’t increase CPU parallelism, but it does increment performance and reduce the number of threads needed. Despite many attempts to make operating system threads cheap (low memory consumption and overhead for their instantiation), their allocation produces a large memory stack, becoming an unrealistic solution for problems that require numerous outstanding asynchronous operations. This was discussed in section 7.1.2.\nAsynchrony vs. parallelism \nParallelism is primarily about application performance, and it also facilitates CPU- intensive work on multiple threads, taking advantage of modern multicore computer architectures. Asynchrony is a superset of concurrency, focusing on I/O-bound rather than CPU-bound operations. Asynchronous programming addresses the issue of latency (anything that takes a long time to run).\n8.3 Asynchronous support in .NET\nThe APM has been a part of the Microsoft .NET Framework since the beginning (v1.1). It offloads the work from the main execution thread to other working threads with the purpose of delivering better responsiveness and of gaining scalability. \nThe original asynchronous programming pattern consists of splitting a long-running \nfunction into two parts. One part is responsible for starting the asynchronous operation (\nBegin ), and the other part is invoked when the operation completes ( End).\nThis code shows a synchronous (blocking) operation that reads from a file stream \nand then processes the generated byte array (the code to note is in bold):\nvoid ReadFileBlocking(string filePath, Action<byte[]> process) {    using (var fileStream = new FileStream(filePath, FileMode.Open,                                           FileAccess.Read, FileShare.Read))Creates a sequence of asynchronous operations to execute \nStarts the execution of multiple asynchronous computations in parallel \nRuns the program and waits for the result, which is okay for console or testing purpose. The recommended approach is to avoid blocking, as we’ll demonstrate shortly.  \n \n 221 Asynchronous support in .NET\n    {          byte[] buffer = new byte[fileStream.Length];          int bytesRead = fileStream.Read(buffer, 0, buffer.Length);          process(buffer);    }}\nTransforming this code into an equivalent asynchronous (non-blocking) operation requires a notification in the form of a callback to continue the original call-site (where the function is called) upon completion of the asynchronous I/O operation. In this case, the callback keeps the opportune state from the \nBegin  function, as shown in the \nfollowing listing (the code to note is highlighted in bold). The state is then rehydrated (restored to its original representation) when the callback resumes. \nListing 8.2  Reading from the filesystem asynchronously \nIAsyncResult ReadFileNoBlocking(string filePath, Action<byte[]> process){     var fileStream = new FileStream(filePath, FileMode.Open,                           FileAccess.Read, FileShare.Read, 0x1000,                          FileOptions.Asynchronous)      byte[] buffer = new byte[fileStream.Length];     var state = Tuple.Create(buffer, fileStream, process);       return fileStream.BeginRead(buffer, 0, buffer.Length,                                          EndReadCallback, state); }void EndReadCallback(IAsyncResult ar){     var state = ar.AsyncState;                  as (Tuple<byte[], FileStream, Action<byte[]>>)       using (state.Item2) state.Item2.EndRead(ar);         state.Item3(state.Item1);          }\nWhy is the asynchronous version of the operation that’s using the Begin/End pattern not blocking? Because when the I/O operation starts, the thread in context is sent back to the thread pool to perform other useful work if needed. In .NET, the thread-pool scheduler is responsible for scheduling work to be executed on a pool of threads, managed by the CLR.\nTIP  The flag FileOptions.Asynchronous  is passed as an argument in the con-\nstructor FileStream , which guarantees a true asynchronous I/O operation \nat the operating system level. It notifies the thread pool to avoid blocking. In the previous example, the \nFileStream  isn’t disposed, in the BeginRead  call, to \navoid the error of accessing a disposed object later when the Async computa-tion completes.Creates a FileStream instance using the option Asynchronous. Note the stream isn’t disposed here to avoid the error of accessing a disposed object later when the Async computation completes. Passes the state into the callback \npayload. The function process is \npassed as part of the tuple.\nThe BeginRead function starts, and \nEndReadCallback is passed as a callback \nto notify when the operations completes.\nThe callback rehydrates the state in the original form to access the underlying values.The FileStream is disposed and the data is processed.\n \n222 chapter  8 Task asynchronicity for the win\nWriting APM programs is considered more difficult than writing the sequential ver -\nsion. An APM program requires more code, which is more complex and harder to read and write. The code can be even more convoluted if a series of asynchronous opera-tions is chained together. In the next example, a series of asynchronous operations require a notification to proceed with the work assigned. The notification is achieved through a callback. \nCallbacks\nA callback is a function used to speed up the program. Asynchronous programming, \nusing a callback, creates new threads to run methods independently. While running asynchronously, a program notifies the calling thread of any update, which includes fail-ure, cancellation, progress, and completion, by a reentrant function used to register the continuation of another function. This process takes some time to produce a result. \n \nThis chain of asynchronous operations in the code produces a series of nested call-backs, also known as “callback hell”(http:/ /callbackhell.com). Callback-based code is problematic because it forces the programmer to cede control, restricting expressive-ness and, more importantly, eliminating the compositionality semantic aspect. \nThis is an example of code (conceptual) to read from a file stream, then compress \nand send the data to the network (the code to note is in bold): \nIAsyncResult ReadFileNoBlocking(string filePath){    // keep context and BeginRead}void EndReadCallback(IAsyncResult ar){    // get Read and rehydrate state, then BeginWrite (compress)}void EndCompressCallback(IAsyncResult ar){    // get Write and rehydrate state, then BeginWrite (send to the network)}void EndWriteCallback(IAsyncResult ar){    // get Write and rehydrate state, completed process}\nHow would you introduce more functionality to this process? The code isn’t easy to maintain! How can you compose this series of asynchronous operations to avoid the callback hell? And where and how would you manage error handling and release resources? The solutions are complex!\nIn general, the asynchronous Begin/End pattern is somewhat workable for a single \ncall, but it fails miserably when composing a series of asynchronous operations. Later in this chapter I’ll show how to conquer exceptions and cancellations such as these.",15505
73-8.3.1 Asynchronous programming breaks the code structure.pdf,73-8.3.1 Asynchronous programming breaks the code structure,,0
74-8.3.2 Event-based Asynchronous Programming.pdf,74-8.3.2 Event-based Asynchronous Programming,,0
75-8.4 C Task-based Asynchronous Programming.pdf,75-8.4 C Task-based Asynchronous Programming,"223 C# Task-based Asynchronous Programming \n8.3.1 Asynchronous programming breaks the code structure \nAs you can see from the previous code, an issue originating from traditional APM is the decoupled execution time between the start (\nBegin ) of the operation and its \ncallback notification ( End). This broken-code design divides the operation in two, vio-\nlating the imperative sequential structure of the program. Consequently, the opera-tion continues and completes in a different scope and possibly in a different thread, making it hard to debug, difficult to handle exceptions, and impossible to manage transaction scopes.\nIn general, with the APM pattern, it’s a challenge to maintain state between each \nasynchronous call. You’re forced to pass a state into each continuation through the call-back to continue the work. This requires a tailored state machine to handle the passing of state between each stage of the asynchronous pipeline. \nIn the previous example, to maintain the state between the \nfileStream.BeginRead  \nand its callback EndReadCallback , a tailored state  object was created to access the \nstream, the byte array buffer, and the function process:\n var state = Tuple.Create(buffer, fileStream, process);\nThis state  object was rehydrated when the operation completed to access the underly-\ning objects to continue further work. \n8.3.2 Event-based Asynchronous Programming\nMicrosoft recognized the intrinsic problems of APM and consequently introduced \n(with .NET 2.0) an alternate pattern called Event-based Asynchronous Programming (EAP).\n1 The EAP model was the first attempt to address issues with APM. The idea \nbehind EAP is to set up an event handler for an event to notify the asynchronous oper -\nation when a task completes. This event replaces the callback notification semantic. Because the event is raised on the correct thread and provides direct support access to UI elements, EAP has several advantages. Additionally, it’s built with support for progress reporting, canceling, and error handling—all occurring transparently for the developer. \nEAP provides a simpler model for asynchronous programming than APM, and it’s \nbased on the standard event mechanism in .NET, rather than on requiring a custom class and callbacks. But it’s still not ideal because it continues to separate your code into method calls and event handlers, increasing the complexity of your program’s logic. \n8.4 C# Task-based Asynchronous Programming \nCompared to its predecessor, .NET APM, Task-based Asynchronous Programming (TAP) aims to simplify the implementation of asynchronous programs and ease com-position of concurrent operation sequences. The TAP model deprecates both APM and EAP, so if you’re writing asynchronous code in C#, TAP is the recommended model. TAP presents a clean and declarative style for writing asynchronous code that \n1 For more information, see http:/ /mng.bz/2287. \n \n224 chapter  8 Task asynchronicity for the win\nlooks similar to the F# asynchronous workflow by which it was inspired. The F# asyn-chronous workflow will be covered in detail in the next chapter.\nIn C# (since version 5.0), the objects \nTask  and Task<T> , with the support of the key-\nwords async  and await , have become the main components to model asynchronous \noperations. The TAP model solves the callback problem by focusing purely on the syn-tactic aspect, while bypassing the difficulties that arise in reasoning about the sequence of events expressed in the code. Asynchronous functions in C# 5.0 address the issue of latency, which refers to anything that takes time to run.\nThe idea is to compute an asynchronous method, returning a task (also called a \nfuture) that isolates and encapsulates a long-running operation that will complete at a point in the future, as shown in figure 8.4. \nExecution thread Execution threadTask-based Asynchronous Programming model\nTask\nresult\nTask\nchannel\nTask\nchannel\nFigure 8.4  The task acts as a channel for the execution thread, which can continue working while \nthe caller of the operation receives the handle to the task. When the operation completes, the task is notified and the underlying result can be accessed.\nHere’s the task flow from figure 8.4:\n1 The I/O operation starts asynchronously in a separate execution thread. A new task instance is created to handle the operation.\n2 The task created is returned to the caller. The task contains a callback, which acts as a channel between the caller and the asynchronous operation. This channel communicates when the operation completes.\n3 The execution thread continues the operation while the main thread from the operation caller is available to process other work.\n4 The operation completes asynchronously.\n5 The task is notified, and the result is accessible by the caller of the operation.\nThe Task  object returned from the async/await expression provides the details of the \nencapsulated computation and a reference to its result that will become available when the operation itself is completed. These details include the status of the task, the result, if completed, and exception information, if any. \n \n 225 C# Task-based Asynchronous Programming \nThe .NET Task  and Task<T>  constructs were introduced in the previous chapter, \nspecifically for CPU-bound computations. The same model in combination with the \nasync/await  keywords can be used for I/O-bound operations.\nNOTE  The thread pool has two groups of threads: worker and I/O threads. A \nworker thread targets a job that’s CPU-bound. An I/O thread is more efficient for I/O-bound operations. The thread pool keeps a cache of worker threads because threads are expensive to create. The CLR thread pool keeps sepa-rate pools of each to avoid a situation where high demand on worker threads exhausts all the threads available to dispatch native I/O callbacks, potentially leading to deadlock. Imagine an application using a large number of worker threads, where each one is waiting for an I/O to complete.\nIn a nutshell, TAP consists of the following:\n¡ The Task  and Task<T>  constructs to represent asynchronous operations\n¡ The await  keyword to wait for the task operation to complete asynchronously, \nwhile the current thread isn’t blocked from performing other work \nFor example, given an operation to execute in a separate thread, you must wrap it into a \nTask : \nTask<int[]> processDataTask = Task.Run(() => ProcessMyData(data));// do other workvar result = processDataTask.Result;\nThe output of the computation is accessed through the Result  property, which blocks \nthe caller method until the task completes. For tasks that don’t return a result, you could call the \nWait  method instead. But this isn’t recommended. To avoid blocking the \ncaller thread, you can use the Task  async/await  keywords: \nTask<int[]> processDataTask = Task.Run(async () => ProcessMyData(data));// do other workvar result = await processDataTask;\nThe async  keyword notifies the compiler that the method runs asynchronously with-\nout blocking. By doing so, the calling thread will be released to process other work. Once the task completes, an available worker thread will resume processing the work.\nNOTE  Methods marked as async can return either void , Task , or Task<T> , but \nit’s recommended that you limit the use of the voided signature. This should only be supplied at the top-level entry point of the program and in UI event handlers. A better approach is to use \nTask<Unit >, introduced in the previous chapter. \nHere’s the previous code example converted to read a file stream asynchronously using the TAP way (the code to note is in bold):\nasync void ReadFileNoBlocking(string filePath, Action<byte[]> process){    using (var fileStream = new FileStream(filePath, FileMode.Open,                                FileAccess.Read, FileShare.Read, 0x1000,                                FileOptions.Asynchronous))",7973
76-8.4.2 TaskT is a monadic container.pdf,76-8.4.2 TaskT is a monadic container,"226 chapter  8 Task asynchronicity for the win\n   {      byte[] buffer = new byte[fileStream.Length];      int bytesRead = await fileStream.ReadAsync(buffer, 0, buffer.Length);      await Task.Run(async () => process(buffer));    }}\nThe method ReadFileNoBlocking  is marked async , the contextual keyword used to \ndefine an asynchronous function and to enable the use of the await  keyword within a \nmethod. The purpose of the await  construct is to inform the C# compiler to translate \nthe code into a continuation of a task that won’t block the current context thread, free-ing the thread to do other work.\nNOTE  The async/await  functionality in C# is based on registering a callback \nin the form of a continuation, which will be triggered when the task in con-text completes. It’s easy to implement code in a fluent and declarative style. The \nasync/await  is syntactic sugar for a continuation monad, which is imple-\nmented as a monadic bind operator using the function ContinuesWith . This \napproach plays well with method chaining because each method returns a task that exposes the \nContinuesWith  method. But it requires working with the tasks \ndirectly to get the result and hand it off to the next method. Furthermore, if you have a large number of tasks to chain together, you’re forced to drill through the results to get to the value you care about. Instead, what you need is a more generalized approach that can be used across methods and at an arbitrary level within the chain, which is what the \nasync/await  programming model offers.\nUnder the hood, the continuation is implemented using the ContinuesWith  function \nfrom the Task  object, which is triggered when the asynchronous operation has com-\npleted. The advantage of having the compiler build the continuation is to preserve the program structure and the asynchronous method calls, which then are executed without the need for callbacks or nested lambda expressions. \nThis asynchronous code has clear semantics organized in a sequential flow. In gen-\neral, when a method marked as \nasync  is invoked, the execution flow runs synchro-\nnously until it reaches an await -able task, denoted with the await  keyword, that hasn’t \nyet completed. When the execution flow reaches the await  keyword, it suspends the \ncalling method and yields control back to its caller until the awaited task is complete; in this way, the execution thread isn’t blocked. When the operation completes, its result is unwrapped and bound into the content variable, and then the flow continues with the remaining work.\nAn interesting aspect of TAP is that the execution thread captures the synchroniza-\ntion context and serves back to the thread that continues the flow, allowing direct UI updates without extra work. \n8.4.1 Anonymous asynchronous lambdas\nYou may have noticed a curious occurrence in the previous code—an anonymous func-tion was marked \nasync : \nawait Task.Run(async () => process(buffer));\n \n 227 C# Task-based Asynchronous Programming \nAs you can see, in addition to ordinary named methods, anonymous methods can also be marked \nasync . Here’s an alternative syntax to make an asynchronous anonymous \nlambda: \n Func<string, Task<byte[]>> downloadSiteIcone = async domain =>{    var response = await new         HttpClient().GetAsync($""http://{domain}/favicon.ico"");    return await response.Content.ReadAsByteArrayAsync();}\nThis is also called an asynchronous lambda,2 which is like any other lambda expression, \nonly with the async  modifier at the beginning to allow the use of the await  keyword \nin its body. Asynchronous lambdas are useful when you want to pass a potentially long-running delegate into a method. If the method accepts a \nFunc<Task> , you can \nfeed it an async lambda and get the benefits of asynchrony. Like any other lambda expression, it supports closure to capture variables and the asynchronous operation start, only when the delegate is invoked.\nThis feature provides an easy means for expressing asynchronous operations on the \nfly. Inside these asynchronous functions, the \nawait  expressions can wait for running \ntasks. This causes the rest of the asynchronous execution to be transparently enlisted as a continuation of the awaited task. In anonymous asynchronous lambdas, the same rules apply as in ordinary asynchronous methods. You can use them to keep code con-cise and to capture closures.\n8.4.2 Task<T> is a monadic container \nIn the previous chapter, you saw that the Task<T>  type can be thought of as a special \nwrapper, eventually delivering a value of type T if it succeeds. The Task<T>  type is a \nmonadic data structure, which means, among other things, that it can easily be com-posed with others. It’s not a surprise that the same concept also applies to the \nTask<T>  \ntype used in TAP. \nThe monadic container \nHere’s a refresh of the concept of monads introduced earlier in the book. In the context \nof Task , you can imagine a monad acting as a container. A monadic container is a pow -\nerful compositional tool used in functional programming to specify a way to chain oper -\nations together and to avoid dangerous and unwanted behaviors. Monads essentially mean you’re working with boxed, or closed over, values, like the \nTask  and Lazy  types, \nwhich are unpacked only at moment they’re needed. For example, monads let you take a value and apply a series of transformations to it in an independent manner that encap -\nsulates side effects. The type signature of a monadic function calls out potential side effects, providing a representation of both the result of the computation and the actual side effects that occurred as a result. \n \n2 Methods or lambdas with the async  modifier are called asynchronous functions.\n \n228 chapter  8 Task asynchronicity for the win\nWith this in mind, you can easily define the monadic operators Bind  and Return . In \nparticular, the Bind  operator uses the continuation-passing approach of the underlying \nasynchronous operation to generate a flowing and compositional semantic program-ming style. Here’s their definition, including the functor map (or fmap) operator:\nstatic Task<T> Return<T>(T task)=> Task.FromResult(task);static async Task<R> Bind<T, R>(this Task<T> task, Func<T, Task<R>> cont)     => await cont(await task.ConfigureAwait(false)).ConfigureAwait(false);static async Task<R> Map<T, R>(this Task<T> task, Func<T, R> map)      => map(await task.ConfigureAwait(false));\nThe definitions of the functions Map and Bind  are simple due to the use of the await  \nkeyword, as compared to the implementation of Task<T>  for CPU-bound computa-\ntions in the previous chapter. The Return  function lifts a T into a Task<T>  container. \nConfigureAwait3 in a Task  extension method removes the current UI context. This \nis recommended to obtain better performance in cases where the code doesn’t need to be updated or doesn’t need to interact with the UI. Now these operators can be exploited to compose a series of asynchronous computations as a chain of operations. The following listing downloads and writes asynchronously into the filesystem an icon image from a given domain. The operators \nBind  and Map are applied to chain the asyn-\nchronous computations (in bold).\nListing 8.3  Downloading an image (icon) from the network asynchronously\nasync Task DownloadIconAsync(string domain, string fileDestination){    using (FileStream stream = new FileStream(fileDestination,                         FileMode.Create, FileAccess.Write,                        FileShare.Write, 0x1000, FileOptions.Asynchronous))    await new HttpClient()         .GetAsync($""http://{domain}/favicon.ico"")         .Bind(async content => await                              content.Content.ReadAsByteArrayAsync())          .Map(bytes => Image.FromStream(new MemoryStream(bytes)))           .Tap(async image =>                            await SaveImageAsync(fileDestination, \n➥ ImageFormat.Jpeg, image));\nIn this code, the method DownloadIconAsync  uses an instance of the HttpClient  \nobject to obtain asynchronously the HttpResponseMessage  by calling the GetAsync  \nmethod. The purpose of the response message is to read the HTTP content (in this case, the image) as a byte array. The data is read by the \nTask.Bind  operator, and then \nconverted into an image using the Task.Map  operator. The function Task.Tap  (also \n3 For more information, see http:/ /mng.bz/T8US. Binds the asynchronous operation, unwrapping the result Task; otherwise it would be Task of Task\nMaps the result of the previous \noperation asynchronously The Tap function performs the side effect.\n \n 229 C# Task-based Asynchronous Programming \nknown as k-combinator) is used to facilitate a pipeline construct to cause a side effect with a given input and return the original value. Here’s the implementation of the \nTask \n.Tap function:\nstatic async Task<T> Tap<T>(this Task<T> task, Func<T, Task> action){     await action(await task);     return await task;}\nThe Tap operator is extremely useful to bridge void functions (such as logging or writ-\ning a file or an HTML page) in your composition without having to create additional code. It does this by passing itself into a function and returning itself. \nTap unwraps the \nunderlying elevated type, applies an action to produce a side effect, and then wraps the original value up again and returns it. Here, the side effect is to persist the image into the filesystem. The function \nTap can be used for other side effects as well. \nAt this point, these monadic operators can be used to define the LINQ pattern \nimplementing Select  and SelectMany , similar to the Task  type in the previous chapter, \nand to enable LINQ compositional semantics: \nstatic async Task<R> SelectMany<T, R>(this Task<T> task,                          Func<T, Task<R>> then) => await Bind(await task);\nstatic async Task<R> SelectMany<T1, T2, R>(this Task<T1> task,                          Func<T1, Task<T2>> bind, Func<T1, T2, R> project)       {            T taskResult = await task;            return project(taskResult, await bind(taskResult));       }static async Task<R> Select<T, R>(this Task<T> task, Func<T, R> project)             => await Map(task, project);static async Task<R> Return<R>(R value) => Task.FromResult(value);\nThe SelectMany operator is one of the many functions capable of extending the asyn-\nchronous LINQ-style semantic. The job of the Return  function is to lift the value R \ninto a Task<R> . The async/await  programming model in C# is based on tasks, and as \nmentioned in the previous chapter, it’s close in nature to the monadic concept of the operators \nBind  and Return . Consequently, it’s possible to define many of the LINQ \nquery operators, which rely on the SelectMany  operator. The important point is that \nusing patterns such as monads provides the opportunity to create a series of reusable combinators and eases the application of techniques allowing for improved compos-ability and readability of the code using LINQ-style semantic. \nNOTE   Section 7.15.3 of the C# specification has a list of operators that can be \nimplemented to support all the LINQ comprehension syntax.\nHere’s the previous DownloadIconAsync  example refactored using the LINQ expres-\nsion semantic:\nasync Task DownloadIconAsync(string domain, string fileDestination){     using (FileStream stream = new FileStream(fileDestination,",11505
77-8.5 TAP a case study.pdf,77-8.5 TAP a case study,"230 Chapter  8 Task asynchronicity for the win\n                       FileMode.Create, FileAccess.Write, FileShare.Write,                                          0x1000, FileOptions.Asynchronous))     await (from response in new HttpClient()                                 .GetAsync($""http://{domain}/favicon.ico"")            from bytes in response.Content.ReadAsByteArrayAsync()            select Bitmap.FromStream(new MemoryStream(bytes)))            .Tap(async image => (await image).Save(fileDestination));}\nUsing the LINQ comprehension version, the from  clause extracts the inner value of \nthe Task  from the async operation and binds it to the related value. In this way, the key-\nwords async/await  can be omitted because of the underlying implementation. \nTAP can be used to parallelize computations in C#, but as you saw, parallelization \nis only one aspect of TAP. An even more enticing proposition is writing asynchronous code that composes easily with the least amount of noise.\n8.5 Task-based Asynchronous Programming: a case study\nPrograms that compute numerous I/O operations that consume a lot of time are good candidates for demonstrating how asynchronous programming works and the powerful toolset that TAP provides to a developer. As an example, in this section TAP is examined in action by implementing a program that downloads from an HTTP server and analyzes the stock market history of a few companies. The results are rendered in a chart that’s hosted in a Windows Presentation Foundation (WPF) UI application. Next, the symbols \nare processed in parallel and program execution is optimized, timing the improvements. \nIn this scenario, it’s logical to perform the operations in parallel asynchronously. Every \ntime you want to read data from the network using any client application, you should call non-blocking methods that have the advantage of keeping the UI responsive (figure 8.5). \nDownloading multiple stocks’\nhistorical prices in parallel. Whenall the stocks have been analyzed,a chart is shown with the result.\nOS\nschedulerProcess stocksStock market\nservice\nFigure 8.5  Downloading historical stock prices asynchronously in parallel. The number of requests can \nexceed the number of available cores, and yet you can maximize parallelism.\n \n 231 Task-based Asynchronous Programming: a case study\nListing 8.4 shows the main part of the program. For the charting control, you’ll use the Microsoft \nWindows.Forms.DataVisualization  control.4 Let’s examine the asyn-\nchronous programming model on .NET in action. First, let’s define the data structure \nStockData  to hold the daily stock history:\nstruct StockData{public StockData(DateTime date, double open,                  double high, double low, double close)        {            Date = date;            Open = open;            High = high;            Low = low;            Close = close;        }public DateTime Date { get; }public Double Open   { get; }public Double High   { get; }public Double Low    { get; }public Double Close  { get; }}\nSeveral historical data points exist for each stock, so StockData  in the shape of value- \ntype struct can increase performance due to memory optimization. The following list-ing downloads and analyzes the historical stock data asynchronously (the code to note is in bold).\nListing 8.4  Analyzing the history of stock prices \nasync Task<StockData[]> ConvertStockHistory(string stockHistory)  {      return await Task.Run(() => {                 string[] stockHistoryRows =                    stockHistory.Split(Environment.NewLine.ToCharArray(),                                StringSplitOptions.RemoveEmptyEntries);             return (from row in stockHistoryRows.Skip(1)                     let cells = row.Split(',')                     let date = DateTime.Parse(cells[0])                     let open = double.Parse(cells[1])                     let high = double.Parse(cells[2])                     let low = double.Parse(cells[3])                     let close = double.Parse(cells[4])                     select new StockData(date, open, high, low, close))                                                              .ToArray();      });}    async Task<string> DownloadStockHistory(string symbol){    string url =\n4 For more information, see http://mng.bz/Jvo1.Method that parses the string of stock history data \nand returns an array of StockData\nUses an asynchronous parser of the CSV stock history \n \n232 Chapter  8 Task asynchronicity for the win\n        $""http://www.google.com/finance/historical?q={symbol}&output=csv"";    var request = WebRequest.Create(url);          using (var response = await request.GetResponseAsync()                                 .ConfigureAwait(false))       using (var reader = new StreamReader(response.GetResponseStream()))        return await reader.ReadToEndAsync().ConfigureAwait(false);  }async Task<Tuple<string, StockData[]>> ProcessStockHistory(string symbol){    string stockHistory = await DownloadStockHistoryAsync(symbol);        StockData[] stockData = await ConvertStockHistory(stockHistory);      return Tuple.Create(symbol, stockData);       }async Task AnalyzeStockHistory(string[] stockSymbols){    var sw = Stopwatch.StartNew();    IEnumerable<Task<Tuple<string, StockData[]>>> stockHistoryTasks =        stockSymbols.Select(stock => ProcessStockHistory(stock));       var stockHistories = new List<Tuple<string, StockData[]>>();    foreach (var stockTask in stockHistoryTasks)         stockHistories.Add(await stockTask);           ShowChart(stockHistories, sw.ElapsedMilliseconds);       }\nThe code starts creating a web request to obtain an HTTP response from the server so you can retrieve the underlying \nResponseStream  to download the data. The code uses \nthe instance methods GetReponseAsync()  and ReadToEndAsync()  to perform the I/O \noperations, which can take a long time. Therefore, they’re running asynchronously using the TAP pattern. Next, the code instantiates a \nStreamReader  to read the data in \na comma-separated values (CSV) format. The CSV data is then parsed in an understand-\nable structure, the object StockData , using a LINQ expression and the function Con­\nvertStockHistory . This function performs the data transformation using Task.Run ,5 \nwhich runs the supplied lambda on the ThreadPool . \nThe function ProcessStockHistory  downloads and converts the stock history \nasynchronously, then returns a Tuple object. Specifically, this return type is Task­\n<Tuple<string, StockData[]>>.  Interestingly, in this method, when the tuple is \ninstantiated at the end of the method, there’s no presence of any Task . This behavior is Web request for a given endpoint to retrieve the stock history; in this case, the financial Google API Web request to asynchronously \nget the HTTP response\nCreates a stream reader using the HTTP response to read \nthe content asynchronously; all the CSV text is read in once\nThe method ProcessStockHistory \nasynchronously executes the \noperation to download and to \nprocess the stock history.A new tuple instance carries information for \neach stock history analyzed to the chart.\nA lazy collection of \nasynchronous operations \nprocesses the historical data.\nAsynchronously processes the operations, one at a timeShows the chart\n5 Microsoft’s recommendation is to use a Task  method that runs in a GUI with a computational time of \nmore than 50 ms.\n \n 233 Task-based Asynchronous Programming: a case study\npossible because the method is marked with the async  keyword, and the compiler \nwraps the result automatically into a Task  type to match the signature. In TAP, by \ndenoting a method as async , all wrapping and unwrapping required to turn the result \ninto a task (and vice versa) are handled by the compiler. The resulting data is sent to the method \nShowChart  to display the stock history and the elapsed time. (The imple-\nmentation of ShowChart  is online in the source code companion to this book.)\nThe rest of the code is self-explanatory. The time to execute this program—download-\ning, processing, and rendering the stock historical data for seven companies—is 4.272 sec-onds. Figure 8.6 shows the results of the stock price variations for Microsoft (MSFT), EMC, Yahoo (YHOO), eBay (EBAY), Intel (INTC), and Oracle (ORCL).\nAs you can see, TAP returns tasks, allowing a natural compositional semantic for other methods with the same return type of \nTask . Let’s review what’s happening throughout \nthe process. You used the Google service in this example to download and analyze the stock market history (listing 8.4). This is a high-level architecture of a scalable service with similar behavior, as shown in figure 8.7.\nHere’s the flow of how the Stock Market service processes the requests: \n1 The user sends several requests asynchronously in parallel to download stock his-tory prices. The UI remains responsive.\n2 The thread pool schedules the work. Because the operations are I/O-bound, the number of asynchronous requests that can run in parallel could exceed the avail-able number of local cores. \n3 The Stock Market service receives the HTTP requests, and the work is dispatched to the internal program, which notifies the thread-pool scheduler to asynchro-nously handle the incoming requests to query the database.\n4 Because the code is asynchronous, the thread-pool scheduler can schedule the work by optimizing local hardware resources. In this way, the number of threads required to run the program is kept to a minimum, the system remains respon-sive, memory consumption is low, and the server is scalable.Figure 8.6  Chart of the stock price \nvariations over time",9721
78-8.5.1 Asynchronous cancellation.pdf,78-8.5.1 Asynchronous cancellation,"234 Chapter  8 Task asynchronicity for the win\n5 The database queries are processed asynchronously without keeping threads blocked.\n6 When the database completes the work, the result is sent back to the caller. At this point, the thread-pool scheduler is notified, and a thread is assigned to con-tinue the rest of the work.\n7 The responses are sent back to the Stock Market service caller as they complete.\n8 The user starts receiving the responses back from the Stock Market service.\n9 The UI is notified, and a thread is assigned to continue the rest of the work with-out blocking.\n10 The data received is parsed, and the chart is rendered. \nOS\nschedulerProcess stocksStock market\nserviceA\nC\nE\nA\nC\nE\nDOS\nscheduler\nDatabaseOS memory\nB\nD\nB\nFigure 8.7  Asynchronous programming model for downloading data in parallel from the network\nUsing the asynchronous approach means all the operations run in parallel, but the overall response time is still correlated to the time of the slowest worker. Conversely, the response time for the synchronous approach increases with each added worker. \n8.5.1 Asynchronous cancellation \nWhen executing an asynchronous operation, it’s useful to terminate execution prema-turely before it completes on demand. This works well for long-running, non-blocking operations, where making them cancellable is the appropriate practice to avoid tasks that could hang. You’ll want to cancel the operation of downloading the historical stock prices, for example, if the download exceeds a certain period of time. \nStarting with version 4.0, the .NET Framework introduced an extensive and conve-\nnient approach to cooperative support for canceling operations running in a different \n \n 235 Task-based Asynchronous Programming: a case study\nthread. This mechanism is an easy and useful tool for controlling task execution flow. The concept of cooperative cancellation allows the request to stop a submitted oper -\nation without enforcing the code (figure 8.8). Aborting execution requires code that supports cancellation. It’s recommended that you design a program that supports can-cellation as much as possible. \nThese are the .NET types for canceling a \nTask  or async operation:\n¡ CancellationTokenSource  is responsible for creating a cancellation token and \nsending cancellation requests to all copies of that token.\n¡ CancellationToken  is a structure utilized to monitor the state of the current token.\nCancellation is tracked and triggered using the cancellation model in the .NET Frame-work \nSystem.Threading.CancellationToken .\nSend request to initiate process\nRequest cancellation\nCancel\noperation Raise OperationCanceledException\nOperation completed\nNOTE  The cancellation is treated as a special exception of type Operation­\nCanceledException , which is convention for the calling code to be notified \nthat the cancellation was observed.\nCanCellation  support  in the tap model\nTAP supports cancellation natively; in fact, every method that returns a task provides at \nleast one overload with a cancellation token as a parameter. In this case, you can pass a cancellation token when creating the task, then the asynchronous operation checks the status of the token, and it cancels the computation if the request is triggered. \nTo cancel the download of the historical stock prices, you should pass an instance \nof \nCancellationToken  as an argument in the Task  method and then call the Cancel  \nmethod. The following listing shows this technique (in bold).\nListing 8.5  Canceling an asynchronous task \nCancellationTokenSource cts = new CancellationTokenSource();   async Task<string> DownloadStockHistory(string symbol,                                         CancellationToken token )   Figure 8.8  After a request to \nstart a process, a cancellation request is submitted that stops the rest of the execution, which returns to the caller in the form of \nOperationCanceledException .\nCreates a CancellationTokenSource instancePasses the CancellationToken in \nthe method to cancel\n \n236 Chapter  8 Task asynchronicity for the win\n{    string stockUrl =         $""http://www.google.com/finance/historical?q={symbol}}&output=csv"";    var request = await new HttpClient().GetAsync(stockUrl, token);  \n    return await request.Content.ReadAsStringAsync();}cts.Cancel();  \nCertain programming methods don’t have intrinsic support for cancellation. In those cases, it’s important to apply manual checking. This listing shows how to integrate can-cellation support to the previous stock market example where no asynchronous meth-ods exist to terminate operations prematurely. \nListing 8.6  Canceling manual checks in an asynchronous operation\nList<Task<Tuple<string, StockData[]>>> stockHistoryTasks =    stockSymbols.Select(async symbol => {        var url =         $""http://www.google.com/finance/historical?q={symbol}&output=csv"";        var request = HttpWebRequest.Create(url);        using (var response = await request.GetResponseAsync())        using (var reader = new StreamReader(response.GetResponseStream()))        {            token.ThrowIfCancellationRequested();            var csvData = await reader.ReadToEndAsync();            var prices = await ConvertStockHistory(csvData);            token.ThrowIfCancellationRequested();            return Tuple.Create(symbol, prices.ToArray());         }    }).ToList();\nIn cases like this, where the Task  method doesn’t provide built-in support for cancella-\ntion, the recommended pattern is to add more CancellationToken s as parameters of \nthe asynchronous method and to check for cancellation regularly. The option to throw an error with the method \nThrowIfCancellationRequested is the most convenient to \nuse because the operation would terminate without returning a result.\nInterestingly, the CancellationToken  (in bold) in the following listing supports the \nregistration of a callback, which will be executed right after cancellation is requested. In this listing, a \nTask  downloads the content of the Manning website, and it’s canceled \nimmediately afterward using a cancellation token. \nListing 8.7  Cancellation token callback\nCancellationTokenSource tokenSource = new CancellationTokenSource();CancellationToken token = tokenSource.Token;Task.Run(async () =>{Passes the CancellationToken in \nthe method to cancelTriggers the cancellation token\n \n 237 Task-based Asynchronous Programming: a case study\n    var webClient = new WebClient();    token.Register(() => webClient.CancelAsync());      var data = await webClient                     .DownloadDataTaskAsync(http://www.manning.com);}, token);tokenSource.Cancel();\nIn the code, a callback is registered to stop the underlying asynchronous operation in case the \nCancellationToken is triggered. \nThis pattern is useful and opens the possibility of logging the cancellation and firing \nan event to notify a listener that the operation has been canceled.\nCooperative  CanCellation  support\nUse of the CancellationTokenSource  makes it simple to create a composite token that \nconsists of several other tokens. This pattern is useful if there are multiple reasons to cancel an operation. Reasons could include a click of a button, a notification from the system, or a cancellation propagating from another operation. The \nCancellation ­\nSource.CreateLinkedTokenSource method generates a cancellation source that will \nbe canceled when any of the specified tokens is canceled (the code to note is in bold).\nListing 8.8  Cooperative cancellation token \nCancellationTokenSource ctsOne = new CancellationTokenSource(); CancellationTokenSource ctsTwo = new CancellationTokenSource();CancellationTokenSource ctsComposite = CancellationTokenSource.\nCreateLinkedTokenSource(ctsOne.Token, ctsTwo.Token);      \nCancellationToken ctsCompositeToken = ctsComposite.Token;Task.Factory.StartNew(async () =>  {    var webClient = new WebClient();    ctsCompositeToken.Register(() => webClient.CancelAsync());    var data = await webClient                     .DownloadDataTaskAsync(http://www.manning.com);}, ctsComposite.Token);        \nIn this listing, a linked cancellation source is created based on the two cancellation tokens. Then, the new composite token is employed. It will be canceled if any of the original \nCancellationToken s are canceled. A cancellation token is basically a thread-\nsafe flag (Boolean value) that notifies its parent that the CancellationTokenSource  \nhas been canceled.Registers a callback that cancels the download of the WebClient instance.\nInstances of CancellationToken to combine\nComposes the Cancellation­\nTokens into one composite\nPasses the composite cancellation token as a regular one; the task is then canceled by calling the Cancel() method of any of the tokens in the composite one.",8862
79-8.5.5 Handling errors in asynchronous operations.pdf,79-8.5.5 Handling errors in asynchronous operations,"238 Chapter  8 Task asynchronicity for the win\n8.5.2 Task-based asynchronous composition with the monadic Bind operator\nAs mentioned previously, async Task<T>  is a monadic type, which means that it’s a \ncontainer where you can apply the monadic operators Bind  and Return . Let’s analyze \nhow these functions are useful in the context of writing a program. Listing 8.9 takes advantage of the \nBind  operator to combine a sequence of asynchronous operations as \na chain of computations. The Return  operator lifts a value into the monad (container \nor elevated type). \nNOTE  As a reminder, the Bind  operator applies to the asynchronous Task<T>  \ntype, which allows it to pipeline two asynchronous operations, passing the result of the first operation to the second operation when it becomes available. \nIn general, a \nTask  asynchronous function takes an arbitrary argument type of 'T and \nreturns a computation of type Task<'R>  (with signature 'T ­> Task<'R> ), and it can \nbe composed using the Bind  operator. This operator says: “When the value 'R from the \nfunction ( g:'T ­> Task<'R>)  is evaluated, it passes the result into the function (f:'R \n­> Task<'U>) .” \nThe function Bind  is shown in figure 8.9 for demonstration purposes because it’s \nalready built into the system.\nTrue Bind\n42function (int -> bool) The\nmeaning\nof lifetruefunction (bool-> string)The Boolean output type\nof function A matches theinput of function B.\nFigure 8.9  The Bind  operator composes two functions that have the result wrapped into a Task  type, \nand where the value returned from the computation of the first Task  matches the input of the second \nfunction.\nWith this Bind  function (in bold in the listing), the structure of the stock analysis code \ncan be simplified. The idea is to glue together a series of functions.\nListing 8.9  Bind  operator in action\n    async Task<Tuple<string, StockData[]>> ProcessStockHistory(string symbol)    {        return await DownloadStockHistory(symbol)               .Bind(stockHistory => ConvertStockHistory(stockHistory))                 .Bind(stockData => Task.FromResult(Tuple.Create(symbol,                                                       stockData)));      }\nComposes an asynchronous operation using continuation­passing style\n \n 239 Task-based Asynchronous Programming: a case study\nThe asynchronous Task  computations are composed by invoking the Bind  operator \non the first async operation and then passing the result to the second async operation, and so forth. The result is an asynchronous function that has as an argument the value returned by the first \nTask  when it completes. It returns a second Task  that uses the \nresult of the first as input for its computation. \nThe code is both declarative and expressive because it fully embraces the functional \nparadigm. You’ve now used a monadic operator: specifically, one based on the contin-uation monad. \n8.5.3 Deferring asynchronous computation enables composition\nIn C# TAP, a function that returns a task begins execution immediately. This behavior \nof eagerly evaluating an asynchronous expression is called a hot task, which unfortu-nately has negative impact in its compositional form. The functional way of handling asynchronous operations is to defer execution until it’s needed, which has the benefit of enabling compositionality and provides finer control over the execution aspect. \nYou have three options for implementing APM:\n¡ Hot tasks —The asynchronous method returns a task that represents an already \nrunning job that will eventually produce a value. This is the model used in C#. \n¡ Cold tasks —The asynchronous method returns a task that requires an explicit \nstart from the caller. This model is often used in the traditional thread-based approach.\n¡ Task generators —The asynchronous method returns a task that will eventually \ngenerate a value, and that will start when a continuation is provided. This is the preferred way in functional paradigms because it avoids side effects and muta-tion. (This is the model used in F# to run asynchronous computations.)\nHow can you evaluate an asynchronous operation on demand using the C# TAP model? You could use a \nLazy<T>  type as the wrapper for a Task<T>  computation (see \nchapter 2), but a simpler solution is to wrap the asynchronous computation into a \nFunc<T>  delegate, which will run the underlying operation only when executed explic-\nitly. In the following code snippet this concept is applied to the stock history exam-ple, which defines the \nonDemand  function to lazily evaluate the DownloadStockHistory  \nTask  expression: \nFunc<Task<string>> onDemand = async () => await DownloadStockHistory(""MSFT"");string stockHistory = await onDemand();\nFrom the point of view of the code, to consume the underlying Task  of the Down­\nloadStockHistory  asynchronous expression, you need to treat and run the onDemand  \nexplicitly as a regular Func  with the ().\nNotice, there’s a small glitch in this code. The function onDemand runs the asynchro-\nnous expression, which must have a pre-fixed argument (in this case, ""MSFT"") . \n \n240 Chapter  8 Task asynchronicity for the win\nHow can you pass a different stock symbol to the function? The solution is currying \nand partial application, FP techniques that allow easier reuse of more abstract functions because you get to specialize. (They are explained in appendix A.)\nCurrying and partial application \nIn FP languages, a function is curried when it seems to take several parameters but takes only one and returns a function that takes the next parameter, and so on. For example, a function type signature \nA ­> B ­> C takes one argument A and returns a \nfunction B ­> C . Translated into C# code using delegates, this function is defined as \nFunc<A, Func<B, C>> .\nThis mechanism lets you partially apply a function by calling it with few parameters and creates a new function that applies only to the arguments passed. The same function can have different interpretations according to the number of parameters passed. \n \nHere’s the curried version of the onDemand function, which takes a string (symbol) as \nan argument that is then passed to the inner Task  expression and returns a function of \ntype Func<Task<string>> : \nFunc<string, Func<Task<string>>> onDemandDownload = symbol =>                  async () => await DownloadStockHistoryAsync(symbol);\nNow, this curried function can be partially applied to create specialized functions over a given string (in this case, a stock symbol), which will be passed and consumed by the wrapped \nTask  when the onDemand function is executed. Here’s the partially applied \nfunction to create the specialized onDemandDownloadMSFT : \nFunc<Task<string>> onDemandDownloadMSFT = onDemandDownload(""MSFT"");string stockHistoryMSFT = await onDemandDownloadMSFT();\nThe technique of differing asynchronous operations shows that you can build arbi-trarily complex logic without executing anything until you decide to fire things off. \n8.5.4 Retry if something goes wrong \nA common concern when working with asynchronous I/O operations, and, in particu-lar, with network requests, is the occurrence of an unexpected factor that jeopardizes the success of the operations. In these situations, you may want to retry an operation if a previous attempt fails. During the HTTP request made by the method \nDownload ­\nStockHistory , for example, there could be issues such as bad internet connections or \nunavailable remote servers. But these problems could be only a temporary state, and the same operation that fails an attempt once, might succeed if retried a few moments later. \nThe pattern of having multiple retries is a common practice to recover from tempo-\nrary problems. In the context of asynchronous operations, this model is achieved by creating a wrapper function, implemented with TAP and returning tasks. This changes \n \n 241 Task-based Asynchronous Programming: a case study\nthe evaluation of an asynchronous expression, as shown in the previous section. Then, if there are a few problems, this function applies the retry logic for a specified number of times with a specified delay between attempts. This listing shows the implementation of the asynchronous \nRetry  function as an extension method.\nListing 8.10  Retry  async operation \nasync Task<T> Retry<T>(Func<Task<T>> task, int retries, TimeSpan delay,              CancellationToken cts = default(CancellationToken)) =>     await task().ContinueWith(async innerTask =>     {        cts.ThrowIfCancellationRequested();            if (innerTask.Status != TaskStatus.Faulted)            return innerTask.Result;           if (retries == 0)            throw innerTask.Exception ?? throw new Exception();          await Task.Delay(delay, cts);               return await Retry(task, retries ­ 1, delay, cts);      }).Unwrap(); \nThe first argument is the async operation that will be re-executed. This function is specified lazily, wrapping the execution into a \nFunc<> , because invoking the operation \nstarts the task immediately. In case of exceptions, the operation Task<T>  captures error \nhandling via the Status  and Exception  properties. It’s possible to ascertain if the async \noperation failed by inspecting these properties. If the operation fails, the Retry  helper \nfunction waits for the specified interval, then retries the same operation, decreasing the number of retries until zero. With this \nRetry<T>  helper function in place, the func-\ntion DownloadStockHistory  can be refactored to perform the web request operation \nwith the retries logic: \nasync Task<Tuple<string, StockData[]>> ProcessStockHistory(string symbol){    string stockHistory =            await Retry(() => DownloadStockHistory(symbol), 5,                                                   TimeSpan.FromSeconds(2)) ;\n    StockData[] stockData = await ConvertStockHistory(stockHistory);      return Tuple.Create(symbol, stockData);        }\nIn this case, the retry logic should run for at most five times with a delay of two seconds between attempts. The \nRetry<T>  helper function should be typically attached to the \nend of a workflow.\n8.5.5 Handling errors in asynchronous operations\nAs you recall, the majority of asynchronous operations are I/O-bound; there’s a high \nprobability that something will go wrong during their execution. The previous section covered the solution to handle failure by applying retry logic. Another approach is If a token isn’t passed, then the default (CancellationToken) sets its value to CancellationToken.None. Uses a CancellationToken cts to \nstop the current execution \nReturns the result if the async operation is successful\nIf the function runs over the retries limit, it throws an exception. Delays the async operation if a failure occurs \nRetries the async operation, \ndecrementing the retry counter\n \n242 Chapter  8 Task asynchronicity for the win\ndeclaring a function combinator that links an async operation to a fallback one. If the first operation fails, then the fallback kicks in. It’s important to declare the fallback as a differed evaluated task. The following listing shows the code that defines the \nOther­\nwise  combinator, which takes two tasks and falls back the execution to the second task \nif the first one completes unsuccessfully.\nListing 8.11  Fallback Task  combinator \n  static Task<T> Otherwise<T>(this Task<T> task,                                            Func<Task<T>> otherTask) => task.ContinueWith(async innerTask => {   if (innerTask.Status == TaskStatus.Faulted) return await orTask(); \n   return innerTask.Result;}).Unwrap();\nWhen the task completes, the Task  type has a concept of whether it finished success-\nfully or failed. This is exposed by the Status  property, which is equal to TaskStatus  \n.Faulted  when an exception is thrown during the execution of the Task . The stock \nhistory analysis example requires FP refactoring to apply the Otherwise  combinator.\nNext is the code that combines the retry behavior, the Otherwise  combinator, and \nthe monadic operators for composing the asynchronous operations.\nListing 8.12  Otherwise  combinator applied to fallback behavior\nFunc<string, string> googleSourceUrl = (symbol) =>     $""http://www.google.com/finance/historical?q={symbol}&output=csv"";Func<string, string> yahooSourceUrl = (symbol) =>                 $""http://ichart.finance.yahoo.com/table.csv?s={symbol}"";async Task<string> DownloadStockHistory(Func<string, string> sourceStock,                                                              string symbol){    string stockUrl = sourceStock(symbol);            var request = WebRequest.Create(stockUrl);    using (var respone = await request.GetResponseAsync())    using (var reader = new StreamReader(respone.GetResponseStream()))        return await reader.ReadToEndAsync;}async Task<Tuple<string, StockData[]>> ProcessStockHistory(string symbol){    Func<Func<string, string>, Func<string, Task<string>>> downloadStock =        service => stock => DownloadStockHistory(service, stock);   otherTask is wrapped into a Func<> to \nbe evaluated only on demand.If innerTask fails, then orTask is computed.\nService function that generates endpoints to retrieve the stock history for the given symbol\nGenerates a stockUrl endpoint using the function sourceStock passed \nCurries the function DownloadStockHistory to partially \napply the endpoint service function and the stock symbol",13566
80-8.5.6 Asynchronous parallel processing of the historical stock market.pdf,80-8.5.6 Asynchronous parallel processing of the historical stock market,"243 Task-based Asynchronous Programming: a case study\n    Func<string, Task<string>> googleService =                                downloadStock(googleSourceUrl);        Func<string, Task<string>> yahooService =                                downloadStock(yahooSourceUrl);         return await Otherwise(() => googleService(symbol)                 .Retry(()=> yahooService(symbol)), 5, TimeSpan.FromSeconds(2))         .Bind(data => ConvertStockHistory(data))                 .Map(prices => Tuple.Create(symbol, prices));    }\nNote that the ConfigureAwait  Task  extension method has been omitted from the \ncode. The application of the Otherwise  combinator runs the function Download ­\nStockHistory  for both the primary and the fallback asynchronous operations. The \nfallback strategy uses the same functionality to download the stock prices, with the web request pointing to a different service endpoint (URL). If the first service isn’t avail-able, then the second one is used. \nThe two endpoints are provided by the functions \ngoogleSourceUrl  and yahoo­\nSourceUrl , which build the URL for the HTTP request. This approach requires a \nmodification of the DownloadStockHistory  function signature, which now takes the \nhigher-order function Func<string, string> sourceStock . This function is partially \napplied against both the functions googleSourceUrl  and yahooSourceUrl . The result \nis two new functions, googleService  and yahooService , that are passed as arguments \nto the Otherwise  combinator, which ultimately is wrapped into the Retry  logic. The \nBind  and Map operators are then used to compose the operations as a workflow without \nleaving the async Task  elevated world. All the operations are guaranteed to be fully \nasynchronous.\n8.5.6 Asynchronous parallel processing of the historical stock market\nBecause the function Task  represents operations that take time, it’s logical that you’ll \nwant to execute them in parallel when possible. One interesting aspect exists in the stock history code example. When the LINQ expression materializes, the asynchro-nous method \nProcessStockHistory  runs inside the for­each  loop by calling one task \nat a time and awaiting the result. These calls are non-blocking, but the execution flow is sequential; each task waits for the previous one to complete before starting. This isn’t efficient. \nThe following snippet shows the faulty behavior of running asynchronous operations \nsequentially using a \nfor­each  loop:\nasync Task ProcessStockHistory (string[] stockSymbols)\n{    var sw = Stopwatch.StartNew();Partial application of the DownloadStockHistory function. downloadStock generates the stock history service.\nThe Retry function applies the Otherwise combinator.\nThe Otherwise operator runs the \ngoogleService operation first; if it \nfails, then the yahooService \noperation executes.\nMonadic Bind operator composes the two async Task operations, Retry and ConvertStockHistory Uses a functor Map operator \nto convert the result \n \n244 Chapter  8 Task asynchronicity for the win\n    IEnumerable<Task<Tuple<string, StockData[]>>> stockHistoryTasks =         stockSymbols.Select(stock => ProcessStockHistory(stock));    var stockHistories = new List<Tuple<string, StockData[]>>();    foreach (var stockTask in stockHistoryTasks)        stockHistories.Add(await stockTask);    ShowChart(stockHistories, sw.ElapsedMilliseconds);}\nSuppose this time you want to launch these computations in parallel and then render the chart once all is complete. This design is similar to the Fork/Join pattern. Here, multiple asynchronous executions will be spawned in parallel and wait for all to com-plete. Then the results will aggregate and continue with further processing. The fol-lowing listing processes the stocks in parallel correctly.\nListing 8.13  Running the stock history analysis in parallel\nasync Task ProcessStockHistory( )\n{    var sw = Stopwatch.StartNew();    string[] stocks = new[] { ""MSFT"", ""FB"", ""AAPL"", ""YHOO"",                               ""EBAY"", ""INTC"", ""GOOG"", ""ORCL"" };    List<Task<Tuple<string, StockData[]>>> stockHistoryTasks =        stocks.Select(async stock => await                             ProcessStockHistory(stock)). ToList(); \n    Tuple<string, StockData[]>[] stockHistories =                               await Task.WhenAll(stockHistoryTasks);     ShowChart(stockHistories, sw.ElapsedMilliseconds);}\nIn the listing, stock collection is transformed into a list of tasks using an asynchronous lambda in the \nSelect  method of LINQ. It’s important to materialize the LINQ expres-\nsion by calling ToList() , which dispatches the tasks to run in parallel only once. This \nis possible due to the hot-task property, which means that a task runs immediately after its definition.\nTIP  By default, .NET limits open request connections to two at one time; to \nspeed up the process, you must change the value of the connection limit Ser­\nvicePointManager.DefaultConnectionLimit = stocks.Length.  \nThe method Task.WhenAll  (similar to Async.Parallel  in F#) is part of the TPL, and \nits purpose is to combine the results of a set of tasks into a single task array, then wait asynchronously for all to complete:\nTuple<string, StockData[]>[] result = await Task.WhenAll(stockHistoryTasks);\nIn this instance, the execution time drops to 0.534 sec from the previous 4.272 sec.The List operator guarantees the materialization \nof the LINQ query, which consequently runs the \nunderlying operations in parallel.Waits asynchronously, without blocking, for all tasks to complete",5606
81-8.5.7 Asynchronous stock market parallel processing as tasks complete.pdf,81-8.5.7 Asynchronous stock market parallel processing as tasks complete,"245 Task-based Asynchronous Programming: a case study\n8.5.7 Asynchronous stock market parallel processing as tasks complete\nAn alternative (and better) solution processes each stock history result as it arrives, instead of waiting for the download of all stocks to complete. This is a good pattern for performance improvement. In this case, it also reduces the payload for the UI thread by rendering the data in chunks. Consider the stock market analysis code, where mul-tiple pieces of historical data are downloaded from the web and then used to process an image to render to a UI control. If you wait for all the data to be analyzed before updating the UI, the program is forced to process sequentially on the UI thread. A more performant solution, shown next, is to process and update the chart as concur -\nrently as possible. Technically, this pattern is called interleaving. The important code to note is in bold.\nListing 8.14  Stock history analysis processing as each Task  completes\nasync Task ProcessStockHistory(){    var sw = Stopwatch.StartNew();    string[] stocks = new[] { ""MSFT"", ""FB"", ""AAPL"", ""YHOO"",                               ""EBAY"", ""INTC"", ""GOOG"", ""ORCL"" };    List<Task<Tuple<string, StockData[]>>> stockHistoryTasks =                stocks.Select(ProcessStockHistory).ToList();      while (stockHistoryTasks.Count > 0)        {        Task<Tuple<string, StockData[]>> stockHistoryTask =                  await Task.WhenAny(stockHistoryTasks);            stockHistoryTasks.Remove(stockHistoryTask);          Tuple<string, StockData[]> stockHistory = await stockHistoryTask;\n        ShowChartProgressive(stockHistory);       }}\nThe code made two changes from the previous version:\n¡ A while  loop removes the tasks as they arrive, until the last one.\n¡ Task.WhenAll  is replaced with Task.WhenAny . This method waits asynchronously \nfor the first task that reaches a terminal state and returns its instance.\nThis implementation doesn’t consider either exceptions or cancellations. Alterna-tively, you could check the status of the task \nstockHistoryTask  before further process-\ning to apply conditional logic. ToList() materializes the LINQ expression, \nensuring the underlying tasks run in parallel.\nRuns the evaluation in a while loop until there are async Tasks to process\nThe Task.WhenAny operator waits asynchronously \nfor the first operation to complete.Removes the completed operation from \nthe list, which is used in the predicate of \nthe while loop\nSends the result from the asynchronous operation to be rendered in the chart\n \n246 Chapter  8 Task asynchronicity for the win\nSummary\n¡ You can write asynchronous programs in .NET with Task-based Asynchronous Programming (TAP) in C#, which is the preferred model to use.\n¡ The asynchronous programming model lets you deal effectively with massive concurrent I/O operations by intelligently recycling resources during their idle time and by avoiding the creation of new resources, thereby optimizing memory consumption and enhancing performance.\n¡ The Task<T>  type is a monadic data structure, which means, among other things, \nthat it can easily be composed with other tasks in a declarative and effortless way.\n¡ Asynchronous tasks can be performed and composed using monadic operators, which leads to LINQ-style semantics. This has the advantage of providing a clear and fluid declarative programming style.\n¡ Executing relatively long-lasting operations using asynchronous tasks can increase the performance and responsiveness of your application, especially if it relies on one or more remote services.\n¡ The number of asynchronous computations that can run in parallel simultane-ously is unrelated to the number of CPUs available, and execution time depends on the period spent waiting for the I/O operations to complete, bound only by the I/O drivers. \n¡ TAP is based on the task type, enriched with the async  and await  keywords. This \nasynchronous programming model embraces the functional paradigm in the form of using continuation-passing style (CPS).\n¡ With TAP, you can easily implement efficient patterns, such as downloading par -\nallel multiple resources and processes as soon as they are available, instead of waiting for all resources to download.",4284
82-9.1 Asynchronous functional aspects.pdf,82-9.1 Asynchronous functional aspects,"2479Asynchronous functional \nprogramming in F#\nThis chapter covers\n¡ Making asynchronous computations cooperate\n¡ Implementing asynchronous operations in a functional style\n¡ Extending asynchronous workflow computational expressions\n¡ Taming parallelism with asynchronous operations\n¡ Coordinating cancellation of parallel asynchronous computations \nIn chapter 8, I introduced asynchronous programming as \nTask s executing inde-\npendently from the main application thread, possibly in a separated environment or across the network on different CPUs. This method leads to parallelism, where applications can perform an inordinately high number of I/O operations on a sin-gle-core machine. This is a powerful idea in terms of program execution and data throughput speed, casting away the traditional step-by-step programming approach. Both the F# and C# programming languages provide a slightly different, yet elegant, abstraction for expressing asynchronous computations, making them ideal tools,",1005
83-9.2.1 The continuation passing style in computation expressions.pdf,83-9.2.1 The continuation passing style in computation expressions,"248 chapter  9 Asynchronous functional programming in F#\nwell suited for modeling real-world problems. In chapter 8, you saw how to use the asynchronous programming model in C#. In this chapter, we look at how to do the same in F#. This chapter helps you understand the performance semantics of the F# asynchronous workflow so you can write efficient and performant programs for pro-cessing I/O-bound operations.\nI’ll discuss the F# approach and analyze it for its unique traits and how they impact \ncode design and explain how to easily implement and compose effective asynchronous operations in a functional style. I’ll also teach you how to write non-blocking I/O oper -\nations to increase the overall execution, efficiency, and throughput of your applications when running multiple asynchronous operations concurrently, all without worrying about hardware constraints. \nYou’ll see firsthand how to apply functional concepts for writing asynchronous \ncomputations. Then you’ll evaluate how to use these concepts to handle side effects and interact with the real world without compromising the benefits of the composi-tional semantics—keeping your code concise, clear, and maintainable. By the end of this chapter, you’ll come away with an appreciation of how modern applications must exploit parallelism and harness the power of multicore CPUs to run efficiently and to handle a large number of operations in a functional way.\n9.1 Asynchronous functional aspects\nAn asynchronous function is a design idiom where a normal F# function or method \nreturns an asynchronous computation. Modern asynchronous programming models such as the F# asynchronous workflow and C# \nasync/await  are functional because \napplying functional programming enables the experienced programmer to write sim-ple and declarative procedural code that runs asynchronously and in parallel.\nFrom the start, F# introduced support for the initiation of an asynchronous pro-\ngramming semantic definition that resembled synchronous code. It’s not a coincidence that C#, which has introduced several functional futures in its language, has been inspired by the functional approach of the F# asynchronous workflow to implement the \nasync /await  asynchronous model, replacing the conventional imperative APM. More-\nover, both the C# asynchronous task and the F# asynchronous workflow are monadic containers, which eases factoring out common functionality into generic, reusable components. \n9.2 What’s the F# asynchronous workflow? \nThe FP language F# provides full support for asynchronous programming:\n¡ It integrates with the asynchronous programming model provided by .NET.\n¡ It offers an idiomatic functional implementation of APM. \n¡ It supports interoperability with the task-based programming model in C#.\n \n 249 What’s the F# asynchronous workflow? \nThe asynchronous workflow in F# is designed to satisfy the functional paradigm pro-moting compositionality, simplicity, and expressing non-blocking computations by keeping the sequential structure of code. By definition, the asynchronous workflow is built on computation expressions, a generic component of the F# core language that provides monadic semantics to express a sequence of operations in continuation-pass-ing style (CPS). \nA key feature of the asynchronous workflow is combining non-blocking computa-\ntions with lightweight asynchronous semantics, which resembles a linear control flow.\n9.2.1 The continuation passing style in computation expressions \nMultithreaded code is notoriously resistant to the imperative style of writing. But using CPS, you can embrace the functional paradigm to make your code remarkably con-cise and easy to write. Let’s imagine that you’re programming using an old version of .NET Framework that doesn’t have the \nasync /await  programming model available \n(see chapter 8). In this case you need to compute a series of Task  operations, where \nthe input of each operation depends on the output of the previous one; the code can become complex and convoluted. In the following code example, the code downloads an image from Azure Blob storage and saves the bytes into a file. \nFor the sake of simplicity, the code that isn’t relevant for the example is omitted \nintentionally; the code to note is in bold. You can find the full implementation in the downloadable source code:\nlet downloadCloudMediaBad destinationPath (imageReference : string) =    log ""Creating connecton...""    let taskContainer = Task.Run<CloudBlobContainer>(fun () -> \n➥ getCloudBlobContainer())\n    log ""Get blob reference..."";    let container = taskContainer.Result    let taskBlockBlob = Task.Run<CloudBlob>(fun () -> \n➥ container.GetBlobReference(imageReference))\n    log ""Download data...""    let blockBlob = taskBlockBlob.Result    let bytes = Array.zeroCreate<byte> (int blockBlob.Properties.Length)    let taskData = Task.Run<byte[]>(fun () -> blockBlob.\nDownloadToByteArray(bytes, 0)|>ignore; bytes)\n    log ""Saving data...""    let data = taskData.Result    let taskComplete = Task.Run(fun () -> \n➥ File.WriteAllBytes(Path.Combine(destinationPath,imageReference), data))\n    taskComplete.Wait()    log ""Complete""\nGranted, the code is an extreme example that aims to validate the point that using traditional tools (with the same obsolete mindset) to write concurrent code produces verbose and impractical programs. The inexperienced developer can write code in this way more easily, because it’s easier to reason sequentially. The result, however, is a program that doesn’t scale, and each \nTask  computation calls the instance method \n \n250 chapter  9 Asynchronous functional programming in F#\nResult , which is a bad practice. In this situation and with a little study, CPS can solve \nthe problem of scalability. First, you define a function used to combine the operations in a pipeline shape: \nlet bind(operation:unit -> 'a, continuation:'a -> unit) =         Task.Run(fun () -> continuation(operation())) |> ignore\nThe bind  function accepts the continuation ( 'a -> unit ) function, which is called \nwhen the result of the operation ( unit -> 'a ) is ready. The main key is that you’re not \nblocking the calling thread, which may then continue executing useful code. When the result is ready, the continuation is called, allowing the computation to continue. You can now use this \nbind  function to rewrite the previous code in a fluent manner:\nlet downloadCloudMediaAsync destinationPath (imageReference : string) =        bind( (fun () -> log ""Creating connecton...""; getCloudBlobContainer()),         fun connection ->            bind( (fun () -> log ""Get blob reference..."";                 connection.GetBlobReference(imageReference)),                 fun blockBlob ->            bind( (fun () -> log ""Download data...""                let bytes = Array.zeroCreate<byte> (int blockBlob.Properties.\n➥ Length)\n                blockBlob.DownloadToByteArray(bytes, 0) |> ignore                bytes), fun bytes ->             bind( (fun () -> log ""Saving data..."";                   File.WriteAllBytes(Path.Combine(destinationPath,imageReference), \n➥ bytes)), fun () -> log ""Complete""))))\n[""Bugghina01.jpg""; ""Bugghina02.jpg""; ""Bugghina003.jpg""] |> Seq.iter \n(downloadCloudMediaAsync ""Images"")\nRunning the code, you’ll notice the bind  function executes the underlying anonymous \nlambda in its own thread. Every time the bind  function is called, a thread is pulled out \nfrom the thread pool, then, when the function completes, the thread is released back to the thread pool.\nThe F# asynchronous workflow is based on this same concept of CPS, which is useful \nfor modeling calculations that are difficult to capture sequentially. \nNOTE   It’s possible for async functions to hop between any number of threads \nthroughout their lifetime.\nFigure 9.1 shows the comparison between incoming requests handled in a synchro-nous and asynchronous way.",7959
84-9.2.2 The asynchronous workflow in action Azure Blob storage paralleloperations.pdf,84-9.2.2 The asynchronous workflow in action Azure Blob storage paralleloperations,"251 What’s the F# asynchronous workflow? \nA\nA\nB\nBSynchronous I/O The synchronous version\ncan send only onerequest at a time.The request is processed,\nand the result is sentback to the caller .The asynchronous version\ncan send many concurrentrequests simultaneously .The requests are\nprocessed concurrentlyon the ser ver side.\nThe results are sent backto the caller in the order inwhich they are completed.Incoming\nrequestDatabaseA\nB\nC\nD\nB\nD\nA\nCAsynchronous I/O \nIncoming\nrequestDatabase\nFigure 9.1  Comparison between synchronous (blocking) I/O and asynchronous (non-blocking) I/O \noperation systems. The synchronous version can send only one request at a time; after the request is processed, the result is sent back to the caller. The asynchronous version can send many concurrent requests simultaneously; after these requests are processed concurrently on the server side, they’re sent back to the caller in the order that they complete.\nThe F# asynchronous workflow also includes cancellation and exception continua-tions. Before we dig into the asynchronous workflow details, let’s look at an example.\n9.2.2 The asynchronous workflow in action: Azure Blob storage parallel operations \nLet’s imagine that your boss has decided that the company’s digital media assets should be stored in the cloud as well as locally. He asks you to create a simple uploader/downloader tool for that purpose and to synchronize and verify what’s new in the cloud. To handle media files as binary data for this scenario, you design a program to download a set of images from the network Azure Blob storage and render these images in a client-side application that’s based on WPF. Azure Blob storage (http:/ /mng.bz/X1FB) is a Micro-soft cloud service that stores unstructured data in the form of blobs (binary large objects). This service stores any type of data, which makes it a great fit to handle your company’s media files as binary data (figure 9.2). \nNOTE   The code examples in this chapter are in F#, but the same concepts are \napplicable to C#. The translated versions of these code examples can be found in the downloadable code for this book, available online. \n \n252 chapter  9 Asynchronous functional programming in F#\nOS\nschedulerSynchronous program\nExecutes operations sequentially ,\none at a time.Asynchronous program\nCan run multiple parallel requests, increasing overall executio n\nspeed. Thus, the asynchronous version of the program ca n\ndownload more images than the synchronous version\nin the same amount of time.\nFigure 9.2  The synchronous versus asynchronous programming model. The synchronous program \nexecutes each operation sequentially one at a time. The asynchronous version can run multiple requests in parallel, increasing the overall execution speed of the program. As a result, the asynchronous version of the program can download more images in the same period of time as compared to the synchronous version.\nAs mentioned earlier, to provide visual feedback, the program runs as a client WPF application. This application benefits from a \nFileSystemWatcher  (http:/ /mng.bz/\nDcRT) that’s listening for file-created events to pick up file changes in the local folder. When the images are downloaded and saved in this local folder, \nFileSystemWatcher  \ntriggers an event and synchronizes the updates of a local file collection with the path of the image, which is successively displayed in a WPF UI controller. (The code imple-mentation of the client WPF UI application isn’t reviewed here because it’s irrelevant to the main topic of this chapter.) \nLet’s compare the synchronous and asynchronous programs from figure 9.2. The \nsynchronous version of the program executes each step sequentially and iterates, with a conventional \nfor loop, the collection of images to download from the Azure Blob stor -\nage. This design is straightforward but doesn’t scale. Alternatively, the asynchronous version of the program is capable of processing multiple requests in parallel, which increases the number of images downloaded in the same period of time.\nLet’s analyze the asynchronous version of the program in more depth. In figure 9.3, \nthe program starts by sending a request to the Azure Blob storage to open the cloud blob container connection. When the connection is opened, the handle of the blob media stream is retrieved to begin downloading the image. The data is read from the stream, and, ultimately, persisted to a local filesystem. Then it repeats this operation for the next image through to the last. \n \n 253 What’s the F# asynchronous workflow? \nOS\nscheduler\nDownload\nprocess\nBegin\ndownloading\nimageSend\nrequest\nResponse End\ndownloading\nimageExecution\nthread\nExecution\nthreadThe execution thread starts\nto download an imageasynchronously .\nWhen the download is completed, thethread pool is notified and a thread is\nassigned to complete the work.The thread pool coordinatesresources to avoid blockingthe thread.\nFigure 9.3  Downloading an image asynchronously from the network (Azure Blob storage)\nEach download operation takes an average of 0.89 seconds over five runs, for a total time of 89.28 seconds to download 100 images. These values can vary according the network bandwidth. Obviously, the time to perform multiple synchronous I/O oper -\nations sequentially is equal to the sum of the time elapsed for each individual opera-tion, in comparison to the asynchronous approach, which by running in parallel has an overall response time equal to the slowest operation.\nNOTE  The Azure Blob storage has an API to download the blob directly into a \nlocal file, DownloadToFile ; but the code intentionally creates a large number \nof I/O operations to accentuate the problem of running I/O blocking opera-tions synchronously.\nThe following listing is the asynchronous workflow implementation of the program to download the images asynchronously from Azure Blob storage (the code to note is in bold).\nListing 9.1  Asynchronous workflow implementation to download images \nlet getCloudBlobContainerAsync() : Async<CloudBlobContainer> = async {    let storageAccount = CloudStorageAccount.Parse(azureConnection)     let blobClient = storageAccount.CreateCloudBlobClient()     let container = blobClient.GetContainerReference(""media"")     let! _ = container.CreateIfNotExistsAsync()     return container }Parses and creates the Azure storage connection\nCreates the blob clientRetrieves a reference of the media container\nCreates the container asynchronously if it doesn’t already exist\n \n254 chapter  9 Asynchronous functional programming in F#\nlet downloadMediaAsync(blobNameSource:string) (fileNameDestination:string)=  async {             let! container = getCloudBlobContainerAsync()            let blockBlob = container.GetBlockBlobReference(blobNameSource)    let! (blobStream : Stream) = blockBlob.OpenReadAsync()          use fileStream = new FileStream(fileNameDestination, FileMode.Create, \n➥ FileAccess.Write, FileShare.None, 0x1000, FileOptions.Asynchronous)\n    let buffer = Array.zeroCreate<byte> (int blockBlob.Properties.Length)    let rec copyStream bytesRead = async {        match bytesRead with        | 0 -> fileStream.Close(); blobStream.Close()        | n -> do! fileStream.AsyncWrite(buffer, 0, n)                  let! bytesRead = blobStream.AsyncRead(buffer, 0, buffer.\n➥ Length)\n                return! copyStream bytesRead }    let! bytesRead = blobStream.AsyncRead(buffer, 0, buffer.Length)     do! copyStream bytesRead  }\nNote that this code looks almost exactly like sequential code. The parts in bold are the only changes necessary to switch the code from synchronous to asynchronous.\nThe intentions of this code are direct and simple to interpret because of the sequen-\ntial structure of the code. This code simplification is the result of the pattern-based approach that the F# compiler uses to detect a computation expression, and in the case of an asynchronous workflow, it gives the illusion to the developer that callbacks have disappeared. Without callbacks, the program isn’t subject to inversion of control as in APM, which makes F# deliver a clean asynchronous code implementation with a focus on compositionality. \nBoth the \ngetCloudBlobContainerAsync  and downloadMediaAsync  functions are \nwrapped inside an async expression (workflow declaration), which turns the code \ninto a block that can be run asynchronously. The function getCloudBlobContainer-\nAsync  creates a reference to the container media . The return type of this asynchronous \noperation to identify the container is type Task<CloudBlobContainer> , which with \nthe Async<CloudBlobContainer>  is handled by the underlying asynchronous work-\nflow expression (explained later in the chapter). The key feature of an asynchronous workflow is to combine non-blocking computations with lightweight asynchronous semantics, which resembles a linear control flow. It simplifies the program structure of traditional callback-based asynchronous programming through syntactic sugar. \nThe methods that run asynchronously are bound to a different construct that uses \nthe \n! (pronounced bang) operator, which is the essence of an asynchronous workflow \nbecause it notifies the F# compiler to interpret the function in an exclusive way. The body of a \nlet!  binding registers the expression as a callback, in context for future eval-\nuation to the rest of the asynchronous workflow, and it extracts the underlying result from \nAsync<'T> .Converts a function to asyncAdds computational expression semantics with the ! \noperator to register continuation of the workflow\n \n 255 What’s the F# asynchronous workflow? \nIn the expression \nlet! bytesRead = blobStream.AsyncRead(buffer, 0, buffer.Length)\nthe return type of blobStream.AsyncRead  is Async<int> , indicating the number of \nbytes read from the asynchronous operation, which is extracted into the value bytes-\nRead . The rec copyStream function recursively and asynchronously copies the blob-\nStream  into the fileStream . Note the copyStream function is defined inside another \nasync workflow to capture (close over) the stream values that can be accessed to be copied. This code could be rewritten in an imperative style with identical behavior as follows:\nlet! bytesRead = blobStream.AsyncRead(buffer, 0, buffer.Length)let mutable bytesRead = bytesReadwhile bytesRead > 0 do     do! fileStream.AsyncWrite(buffer, 0, bytesRead)      let! bytesReadTemp = blobStream.AsyncRead(buffer, 0, buffer.Length)     bytesRead <- bytesReadTempfileStream.Close(); blobStream.Close()\nThe mutation of the variable bytesRead  is encapsulated and isolated inside the main \nfunction downloadMediaAsync  and is thread safe. \nBesides let! the other asynchronous workflow constructors are as follows:\n¡ use! —Works like let!  for disposable resources that are cleaned up when out \nof scope\n¡ do!—Binds an asynchronous workflow when the type is Async<unit>\n¡ return —Returns a result from the expression\n¡ return! —Executes the bound asynchronous workflow, returning the value of \nthe expression\nThe F# asynchronous workflow is based on the polymorphic data type Async<'a>  that \ndenotes an arbitrary asynchronous computation, which will materialize in the future, returning a value of type \n'a. This concept is similar to the C# TAP model. The main dif-\nference is that the F# Async< 'a> type isn’t hot, which means that it requires an explicit \ncommand to start the operation. \nWhen the asynchronous workflow reaches the start primitive, a callback is scheduled \nin the system, and the execution thread is released. Then, when the asynchronous oper -\nation completes the evaluation, the underlying mechanisms will notify the workflow, passing the result to the next step in the code flow. \nThe real magic is that the asynchronous workflow will complete at a later time, but \nyou don’t have to worry about waiting for the result because it will be passed as an argu-ment in the continuation function when completed. The compiler takes care of all of this, organically converting the \nBind  member calls into the continuation constructs. \nThis mechanism uses CPS for writing, implicitly, a structured callback-based program inside its body expression, which allows a linear style of coding over a sequence of operations.",12405
85-9.3.1 Difference between computation expressions and monads.pdf,85-9.3.1 Difference between computation expressions and monads,"256 chapter  9 Asynchronous functional programming in F#\nThe asynchronous execution model is all about continuations, where the evaluation \nof the asynchronous expression preserves the capability of having a function registered as a callback (figure 9.4).\nbind(fun() ->log ""Creating connection..."" ;\n             getCloudBlobContainer()), fun connection ->     bind(fun() ->log ""Get blob reference..."" ;\n                  connection.GetBlobReference(imageReference)), fun blockBlob ->\nasync {   log ""Creating connection..."" ;\n   let! connection = getCloudBlobContainerAsync()   log ""Get blob reference..."";   let blockBlob = connection.GetBlobReference(imageReference )\n   ...\nFigure 9.4  A comparison of the Bind  function with the computation expression version.\nThe benefits of using an asynchronous workflow are as follows:\n¡ Code that looks sequential but behaves asynchronously \n¡ Simple code that’s easy to reason about (because it looks like sequential code), which simplifies updates and modification \n¡ Asynchronous compositional semantics\n¡ Built-in cancellation support \n¡ Simple error handling\n¡ Easy to parallelize \n9.3 Asynchronous computation expressions\nComputation expressions are an F# feature that define a polymorphic construct used to customize the specification and behavior of the code, and lead you toward a compo-sitional programming style. The MSDN online documentation provides an excellent definition: \nComputation expressions in F# provide a convenient syntax for writing computations that can be sequenced and combined using control flow constructs and bindings. They can be used to provide a convenient syntax for monads, a functional programming feature that can be used to manage data, control, and side effects in functional programs.\n1\nComputation expressions are a helpful mechanism for writing computations that exe-cute a controlled series of expressions as an evaluation of feed-into steps. The first step serves as input to the second step, and that output serves as input for the third step, and so forth through the execution chain—unless an exception occurs, in which case the evaluation terminates prematurely, skipping the remaining steps.\n1 For more information, see http:/ /mng.bz/n1uZ.\n \n 257 Asynchronous computation expressions\nThink of a computation expression as an extension of the programming language \nbecause it lets you customize a specialized computation to reduce redundant code and apply heavy lifting behind the scenes to reduce complexity. You can use a computation expression to inject extra code during each step of the computation to perform opera-tions such as automatic logging, validation, control of state, and so on. \nThe F# asynchronous programming model, asynchronous workflow, relies on com-\nputation expressions, which are also used to define other implementations, such as sequence and query expressions. The F# asynchronous workflow pattern is syntactic sugar, interpreted by the compiler in a computation expression. In an asynchronous workflow, the compiler must be instructed to interpret the workflow expression as an asynchronous computation. The notification is semantically passed by wrapping the expression in an asynchronous block, which is written using curly braces and the \nasync  \nidentifier right at the beginning of the block, like so:  async { expression }  \nWhen the F# compiler interprets a computation as an asynchronous workflow, it \ndivides the whole expression into separate parts between the asynchronous calls. This transformation, referred to as desugaring, is based on the constituent primitives by the computation builder in context (in this case, the asynchronous workflow).\nF# supports computation expressions through a special type called \nbuilder , associ-\nated with the conventional monadic syntax. As you remember, the two primary monadic operators to define a computation builder are \nBind  and Return . \nIn the case of an asynchronous workflow, the generic monadic type is replaced and \ndefined with the specialized type Async :\nasync.Bind: Async<'T> → ('T → Async<'R>) → Async<'R>    async.Return: 'T → Async<'T>        \nThe asynchronous workflow hides nonstandard operations in the form of computa-tion builder primitives and reconstructs the rest of the computation in continuation. Nonstandard operations are bound in the body expression of the builder constructs with the \n! operator. It’s not a coincidence that the computation expression definition, \nthrough the Bind  and Return  operators, is identical to the monadic definition, which \nshares the same monadic operators. You can think of a computation expression as a continuation monad pattern.\n9.3.1 Difference between computation expressions and monads \nYou can also think of a computation expression as a general monadic syntax for \nF#, which is closely related to monads. The main difference between computation expressions and monads is found in their origin. Monads strictly represent mathe-matical abstractions, whereas the F# computation expression is a language feature that provides a toolset to a program with computation that can—or not—have a monadic structure.The asynchronous operation \nAsync<'T> is passed as the first \nargument, and a continuation ('T ➔ \nAsync<'R>) is passed as the second.Wraps a generic type 'T into an elevated type Async<'T>\n \n258 chapter  9 Asynchronous functional programming in F#\nF# doesn’t support type classes, so it isn’t possible to write a computation expres-\nsion that’s polymorphic over the type of computation. In F# you can select a com-putation expression with the most specialized behavior and convenient syntax (an example is coming).\nType classes \nA type class is a construct that provides specific polymorphism, which is achieved by \napplying constraint definitions to type variables. Type classes are akin to interfaces that define a behavior, but they’re more powerful. The compiler provides specialized behavior and syntax over the type inferred through these constraints. In .NET, you can think of a type class as an interface that defines a behavior, which the compiler can detect, and then provides an ad hoc implementation based on its type definition. Ultimately, a type can be made an instance of a typeclass if it supports that behavior.\n \nThe code written using the computation expression pattern is ultimately translated into an expression that uses the underlying primitives implemented by the computa-tion builder in context. This concept will be clearer with an example. \nListing 9.2 shows the desugared version of the function \ndownloadMediaAsync , where \nthe compiler translates the computation expression into a chain of method calls. This unwrapped code shows how the behavior of each single asynchronous part is encapsu-lated in the related primitive member of the computation builder. The keyword \nasync  \ntells the F# compiler to instantiate the AsyncBuilder , which implements the essential \nasynchronous workflow members Bind , Return , Using , Combine , and so on. The listing \nshows how the compiler translates the computation expression into a chain of method calls of the code from listing 9.1. (The code to note is in bold.)\nListing 9.2  Desugared DownloadMediaAsync  computation expression \nlet downloadMediaAsync(blobName:string) (fileNameDestination:string) =  async.Delay(fun() ->           async.Bind(getCloudBlobContainerAsync(), fun container ->          let blockBlob = container.GetBlockBlobReference(blobName)       async.Using(blockBlob.OpenReadAsync(), fun (blobStream:Stream) ->            let sizeBlob = int blockBlob.Properties.Length           async.Bind(blobStream.AsyncRead(sizeBlob), fun bytes ->                use fileStream = new FileStream(fileNameDestination, \n➥ FileMode.Create, FileAccess.Write, FileShare.None, bufferSize, \n➥ FileOptions.Asynchronous)\n       async.Bind(fileStream.AsyncWrite(bytes, 0, bytes.Length), fun () ->                            fileStream.Close()                            blobStream.Close()                             async.Return())))))     Delays execution of the function until an explicit request\nThe Bind operator is the desugared version of the let! operator.\nThe Using operator translates \nto the use! operator.\nReturns the operator that completes \nthe computation expression",8371
86-9.3.2 AsyncRetry building your own computation expression.pdf,86-9.3.2 AsyncRetry building your own computation expression,"259 Asynchronous computation expressions\nIn the code, the compiler transforms the let!  binding construct into a call to the Bind  \noperation, which unwraps the value from the computation type and executes the rest of the computation converted to a continuation. The \nUsing  operation handles com-\nputation where the resulting value type represents a resource that can be disposed. The first member in the chain, \nDelay , wraps the expression as a whole to manage the \nexecution, which can run later on demand.\nEach step of the computation follows the same pattern: the computation builder \nmember, like Bind  or Using , starts the operation and provides the continuation that \nruns when the operation completes, so you don’t wait for the result.\n9.3.2 AsyncRetry: building your own computation expression \nAs mentioned, a computation expression is a pattern-based interpretation (like LINQ/PLINQ), which means that the compiler can infer from the implementation of the mem-bers \nBind  and Return  that the type construct is a monadic expression. By following a few \nsimple specifications, you can build your own computation expression, or even extend an existing one, to deliver to an expression the special connotation and behavior you want.\nComputation expressions can contain numerous standard language constructs, as \nlisted in table 9.1; but the majority of these member definitions are optional and can be used according to your implementation needs. The mandatory and basic members to represent a valid computation expression for the compiler are \nBind  and Return .\nTable 9.1.  Computation expression operators\nMember Description\nBind : M<'a> * ('a ➔ M<'b>) ➔ M<'b> Transformed let! and do! within computation \nexpressions.\nReturn : 'a ➔ M<'a> Transformed return within computation expressions.\nDelay : (unit ➔ M<'a>) ➔ M<'a> Used to ensure side effects within a computation expression are performed when expected.\nYield : 'a \n➔ M<'a> Transformed yield within computation expressions.\nFor : seq<'a> * ('a ➔ M<'b>) ➔ M<'b> Transformed for ... do ... within computation \nexpressions. M<'b> can optionally be M<unit>.\nWhile : (unit ➔ bool) * M<'a> ➔ M<'a> Transformed while-do block within computation \nexpressions. M<'b> can optionally be M<unit>.\nUsing : 'a * ('a ➔ M<'b>) ➔ M<'b> \nwhen 'a :> IDisposableTransformed use bindings within computation \nexpressions.\nCombine : M<'a> ➔ M<'a> ➔ M<'a> Transformed sequencing within computation expres -\nsions. The first M<'a> can optionally be M<unit>.\nZero : unit ➔ M<'a> Transformed empty else branches of if/then \nwithin computation expressions.\nTryWith : M<'a> ➔ M<'a> ➔ M<'a> Transformed empty try/with bindings within  \ncomputation expressions.\nTryFinally : M<'a> ➔ M<'a> ➔ M<'a> Transformed try/finally bindings within  \ncomputation expressions.\n \n260 chapter  9 Asynchronous functional programming in F#\nLet’s build a computation expression that can be used with the example in listing 9.2. The first step of the function, \ndownloadMediaCompAsync , connects asynchronously to \nthe Azure Blob service, but what happens if the connection drops? An error is thrown and the computation stops. You could check whether the client is online before trying to connect; but it’s a general rule of thumb when working with network operations that you retry the connection a few times before aborting. \nIn the following listing, you’re building a computation expression that runs an asyn-\nchronous operation successfully a few times, with a delay in milliseconds between each retry before the operation stops (the code to note is in bold). \nListing 9.3  AsyncRetryBuilder  computation expression \ntype AsyncRetryBuilder(max, sleepMilliseconds : int) =    let rec retry n (task:Async<'a>) (continuation:'a -> Async<'b>) =       async {         try             let! result = task             let! conResult = continuation result               return conResult        with error ->                if n = 0 then return raise error                  else                do! Async.Sleep sleepMilliseconds                    return! retry (n - 1) task continuation }    member x.ReturnFrom(f) = f       member x.Return(v) = async { return v }     member x.Delay(f) = async { return! f() }     member x.Bind(task:Async<'a>, continuation:'a -> Async<'b>) =                                   retry max task continuation       member x.Bind(t : Task, f : unit -> Async<'R>) : Async<'R> =                                      async.Bind(Async.AwaitTask t, f) \nThe AsyncRetryBuilder  is a computation builder used to identify the value to con-\nstruct the computation. The following code shows how to use the computation builder (the code to note is highlighted in bold).\nListing 9.4  Using AsyncRetryBuilder to identify construct value\nlet retry = AsyncRetryBuilder(3, 250) Runs the task \nworkflow in a \ntry-catch blockThe operation succeeds and runs the continuation for the rest of the work.\nThe operation reached the limit of allowed reruns, and an error is thrown, stopping this computation.\nComputation can rerun but with a delay. Returns the computation itself\nLifts the \nvalue inside \nan asyncWraps the function inside async, so you can nest the computation in an asynchronous workflow\nBinds the async function and its continuation, starting the retry function. If the function succeeds, then the result will feed the continuation function.\nShows compatibility provided for Task-based operations\nDefines the value to identify the computation expression, which in case of an exception, reattempts to run the code three ties with a delay of 250 ms between retries",5661
87-9.3.4 Mapping asynchronous operation the Async.map functor.pdf,87-9.3.4 Mapping asynchronous operation the Async.map functor,"261 Asynchronous computation expressions\nlet downloadMediaCompAsync(blobNameSource:string)                          (fileNameDestination:string) = async {        let! container = retry {                 return! getCloudBlobContainerAsync() }      ... Rest of the code as before \nThe AsyncRetryBuilder instance retry  re-attempts to run the code in case of an \nexception three times, with a delay of 250 ms between retries. Now, the AsyncRetry -\nBuilder  computation expression can be used in combination with the asynchronous \nworkflow, to run and retry asynchronously (in case of failure), the downloadMedia -\nCompAsync  operation. It’s common to create a global value identifier for a computa-\ntion expression that can be reused in different parts of your program. For example, the asynchronous workflow and sequence expression can be accessed anywhere in the code without creating a new value.\n9.3.3 Extending the asynchronous workflow \nBesides creating custom computation expressions, the F# compiler lets you extend existing ones. The asynchronous workflow is a perfect example of a computation \nexpression that can be enhanced. In listing 9.4, the connection to the Azure Blob con-tainer is established through the asynchronous operation \ngetCloudBlobContainer-\nAsync , the implementation of which is shown here:\nlet getCloudBlobContainerAsync() : Async<CloudBlobContainer> = async {    let storageAccount = CloudStorageAccount.Parse(azureConnection)     let blobClient = storageAccount.CreateCloudBlobClient()     let container = blobClient.GetContainerReference(""media"")    let! _ = container.CreateIfNotExistsAsync()     return container }\nInside the body of the getCloudBlobContainerAsync  function, the CreateIfNotEx -\nistsAsync  operation returns a Task  type, which isn’t friendly to use in the context \nof asynchronous workflow. Fortunately, the F# async provides the Async.AwaitTask2 \noperator, which allows a Task  operation to be awaited and treated as an F# async com-\nputation. A vast number of asynchronous operations in .NET have return types of Task  \nor the generic version Task<'T> . These operations, designed to work primarily with C#, \naren’t compatible with the F# out-of-the-box asynchronous computation expressions.\nWhat’s the solution? Extend the computation expression. Listing 9.5 generalizes the \nF# asynchronous workflow model so that it can be used not only in async operations, but also with the \nTask  and Observable  types. The async computation expression needs \ntype-constructs that can create observables and tasks, as opposed to only asynchronous workflows. It’s possible to await all kinds of events produced by \nEvent  or IObservable  \nstreams and tasks from Task  operations. These extensions for the computation expres-\nsion, as you can see, abstract the use of the Async.AwaitTask  operator (the related \ncommands are in bold). The retry computation expression can be nested inside an async workflow.\n2 The Async.AwaitTask  creates computations that wait on the provided task and returns its result.\n \n262 chapter  9 Asynchronous functional programming in F#\nListing 9.5  Extending the asynchronous workflow to support Task<'a>\ntype Microsoft.FSharp.Control.AsyncBuilder with    member x.Bind(t:Task<'T>, f:'T -> Async<'R>) : Async<'R> = \n➥ async.Bind(Async.AwaitTask t, f)           \n    member x.Bind(t:Task, f:unit -> Async<'R>) : Async<'R> = \n➥ async.Bind(Async.AwaitTask t, f)             \n    member x.Bind (m:'a IObservable, f:'a -> 'b Async) = \n➥ async.Bind(Async.AwaitObservable m, f)            \n    member x.ReturnFrom(computation:Task<'T>) = \n➥ x.ReturnFrom(Async.AwaitTask computation)\nThe AsyncBuilder  lets you inject functions to extend the manipulation on other wrap-\nper types, such as Task  and Observable , whereas the Bind  function in the extension \nlets you fetch the inner value contained in the Observable  (or IEvent ) using the let!  \nand do! operators. This technique removes the need for adjunctive functions like \nAsync.AwaitEvent  and Async.AwaitTask . \nIn the first line of code, the compiler is notified to target the AsyncBuilder , which \nmanages the asynchronous computation expression transformation. The compiler, after this extension, can determine which \nBind  operation to use, according to the \nexpression signature registered through the let!  binding. Now you can use the asyn-\nchronous operation of type Task  and Observable  in an asynchronous workflow.\n9.3.4 Mapping asynchronous operation: the Async.map functor\nLet’s continue extending the capabilities of the F# asynchronous workflow. The F# asyn-\nchronous workflow provides a rich set of operators; but currently, there’s no built-in support for an \nAsync.map  function(also known as a functor) having type signature\n('a ➔ 'b) ➔ Async<'a> ➔ Async<'b>\nA functor is a pattern of mapping over structure, which is achieved by providing imple-mentation support for a two-parameter function called \nmap (better known as fmap) . \nFor example, the Select  operator in LINQ/PLINQ is a functor for the IEnumerable  \nelevated type. Mainly, functors are used in C# to implement LINQ-style fluent APIs that are used also for types (or contexts) other than collections.\nWe discussed the functor type in chapter 7, where you learned how to implement a \nfunctor (in bold) for the \nTask  elevated type:\nTask<T> fmap<T, R>(this Task<T> input, Func<T, R> map) =>                                 input.ContinueWith(t => f(t.Result));\nThis function has a signature ('T ➔ 'R) ➔ Task<T> ➔ Task<R> , so it takes a map func-\ntion 'T ➔ 'R as a first input (which means it goes from a value type T to a value type \nR, in C# code Func<T, R> ), and then upgrades type Task<'T>  as a second input and Extends the Async Bind operator to perform against other elevated types\n \n 263 Asynchronous computation expressions\nreturns a Task<'R> . Applying this pattern to the F# asynchronous workflow, the signa-\nture of the Async.map  function is \n('a -> 'b) -> Async<'a> -> Async<'b>\nThe first argument is a function 'a -> 'b , the second is an Async<'a> , and the output \nis an Async<'b> . Here’s the implementation of Async.map :\nmodule Async =    let inline map (func:'a -> 'b) (operation:Async<'a>) = async {        let! result = operation        return func result }\nlet! result = operation runs the asynchronous operation and unwraps the Asyn-\nc<'a>  type, returning the 'a type. Then, we can pass the value 'a to the function \nfunc:'a -> 'b  that converts 'a to 'b. Ultimately, once the value 'b is computed, the \nreturn  operator wraps the result 'b into the Async<>  type.\nThe inline keyword \nThe inline  keyword in F# is used to define a function that’s integrated into the calling code \nby amending the function body directly to the caller code. The most valuable application of the \ninline  keyword is inlining higher-order functions to the call site where their function \narguments are also inlined to produce a single fully optimized piece of code. F# can also \ninline  between compiled assemblies because inline  is conveyed via .NET metadata.\n \nThe map function applies an operation to the objects inside the Async  container,3 \nreturning a container of the same shape. The Async.map  function is interpreted as a \ntwo-argument function where a value is wrapped in the F# Async  context, and a func-\ntion is applied to it. The F# Async  type is added to both its input and output.\nThe main purpose of the Async.map  function is to operate (project) the result of an \nAsync  computation without leaving the context. Back to the Azure Blob storage example, \nyou can use the Async.map  function to download and transform an image as follows (the \ncode to note is in bold):\nlet downloadBitmapAsync(blobNameSource:string) = async {        let! token = Async.CancellationToken    let! container = getCloudBlobContainerAsync()        let blockBlob = container.GetBlockBlobReference(blobNameSource)    use! (blobStream : Stream) = blockBlob.OpenReadAsync()          return Bitmap.FromStream(blobStream) }let transformImage (blobNameSource:string) =     downloadBitmapAsync(blobNameSource)    |> Async.map ImageHelpers.setGrayscale    |> Async.map ImageHelpers.createThumbnail\nThe Async.map  function composes the async operations of downloading the image \nblobNameSource  from the Azure Table storage with the transformation functions \nsetGrayscale  and createThumbnail .\n3 Polymorphic types can be thought of as containers for values of another type.",8568
88-9.3.5 Parallelize asynchronous workflows Async.Parallel.pdf,88-9.3.5 Parallelize asynchronous workflows Async.Parallel,"264 chapter  9 Asynchronous functional programming in F#\nNOTE  I defined these ImageHelpers  functions in chapter 7, so they’re omit-\nted here intentionally. Please refer to the online source code for the full implementation.\nIn the snippet, the advantages of using the \nAsync.map  function are composability and \ncontinued encapsulation.\n9.3.5 Parallelize asynchronous workflows: Async.Parallel\nLet’s return to the example of downloading 100 images from Azure Blob storage using the F# asynchronous workflow. In section 9.2  you built the function \ndownload-\nMediaAsync  that downloads one cloud blob image using the asynchronous workflow. \nIt’s time to connect the dots and run the code. But instead of iterating through the list of images one operation at a time, the F# asynchronous workflow provides an elegant alternative: \nAsync.Parallel . \nThe idea is to compose all the asynchronous computations and execute them all at \nonce. Parallel composition of asynchronous computations is efficient because of the scalability properties of the .NET thread pool and the controlled, overlapped execu-tion of operations such as web requests by modern operating systems. \nUsing the F# \nAsync.Parallel  function, it’s possible to download hundreds of images \nin parallel (the code to note is in bold).\nListing 9.10  Async.Parallel  downloading all images in parallel\nlet retry = RetryAsyncBuilder(3, 250) let downloadMediaCompAsync (container:CloudBlobContainer)     (blobMedia:IListBlobItem) = retry {     let blobName = blobMedia.Uri.Segments.[blobMedia.Uri.Segments.Length-1]    let blockBlob = container.GetBlockBlobReference(blobName)    let! (blobStream : Stream) = blockBlob.OpenReadAsync()    return Bitmap.FromStream(blobStream)    }let transformAndSaveImage (container:CloudBlobContainer)                          (blobMedia:IListBlobItem) =      downloadMediaCompAsync container blobMedia         |> Async.map ImageHelpers.setGrayscale         |> Async.map ImageHelpers.createThumbnail         |> Async.tap (fun image ->                         let mediaName =                 blobMedia.Uri.Segments.[blobMedia.Uri.Segments.Length - 1]            image.Save(mediaName))let downloadMediaCompAsyncParallel() = retry {                let! container = getCloudBlobContainerAsync()       let computations =          container.ListBlobs() Defines a Retry computation expression\nRuns the asynchronous operations using a run and retry approach \nReturns an image from the operation\nThe map functions are extracted in an independent function and applied in the Async.Parallel pipeline.\nThe tap function applies side effects to its inputs and the result is ignored.\nRuns the asynchronous operations using a run and retry approach \nShares CloudBlobContainer argument among parallel non-blocking computations without contention because it’s read-only \nGets the list of images to download\n \n 265 Asynchronous computation expressions\n         |> Seq.map(transformAndSaveImage container)      return! Async.Parallel computations } let cancelOperation() =     downloadMediaCompAsyncParallel()      |> Async.StartCancelable \nThe Async.Parallel  function takes an arbitrary collection of asynchronous opera-\ntions and returns a single asynchronous workflow that will run all the computations in parallel, waiting for all of them to complete. The \nAsync.Parallel  function coor -\ndinates the work with the thread pool scheduler to maximize resource employment using a Fork/Join pattern, resulting in a performance boost.\nThe library function \nAsync.Parallel  takes a list of asynchronous computations and \ncreates a single asynchronous computation that starts the individual computations in parallel and waits for their completion to be processed as a whole. When all operations complete, the function returns the results aggregated in a single array. Now you can iterate over the array to retrieve the results for further processing.\nNotice the minimal code change and syntax required to convert a computation that \nexecutes one operation at a time into one that runs in parallel. Additionally, this con-version is achieved without the need to coordinate synchronization and memory locks. \nThe \nAsync.tap  operator applies a function asynchronously to a value passed as \ninput, ignores the result and then returns the original value. The Tap operator is intro-\nduced in listing 8.3  Here is its implementation using the F# Async  workflow (in bold):\nlet inline tap (fn:'a -> 'b) (x:Async<'a>) =        (Async.map fn x) |> Async.Ignore |> Async.Start; x\nYou can find this and other useful Async  functions in the source code of the book in \nthe FunctionalConcurrencyLib library.\nThe execution time to download the images in parallel using F# asynchronous work-\nflow in combination with Async.Parallel  is 10.958 seconds. The result is ~5 seconds \nfaster than APM, which makes it ~8× faster than the original synchronous implemen-tation. The major gains here include code structure, readability, maintainability, and compositionality.\nUsing an asynchronous workflow, you gained a simple asynchronous semantic to \nrun a non-blocking computation, which provides clear code to understand, maintain, and update. Moreover, thanks to the \nAsync.Parallel  function, multiple asynchronous \ncomputations can easily be spawned in parallel with minimum code changes to dramat-ically improve performance.Creates a sequence of non-blocking download computations, which aren’t running because they require explicit requests\nAggregates the sequence of asynchronous computations into a single asynchronous workflow that runs all operations in parallel\nThe StartCancelable  function executes the \nasynchronous computation with explicit requests \nwithout blocking the current thread and providing \na token that can be used to stop the computation.\n \n266 chapter  9 Asynchronous functional programming in F#\nThe Async type is not hot\nThe distinct functional aspect of the asynchronous workflow is its execution time. In F#, when an asynchronous function is called, the \nAsync<'a>  return type represents a com -\nputation that will materialize only with an explicit request. This feature lets you model \nand compose multiple asynchronous functions that can be executed conditionally on demand. This is the opposite behavior of the C# TAP asynchronous operations (\nasync / \nawait ), which start the execution immediately. \n \nUltimately, the implementation of the Async.StartCancelable  type extension starts an \nasynchronous workflow, without blocking the thread caller, using a new Cancellation-\nToken , and returns IDisposable  that cancels the workflow when disposed. You haven’t \nused Async.Start  because it doesn’t provide a continuation-passing semantic, which \nis useful in many cases to apply the operation to the result of the computation. In the example, you print a message when the computation completes; but the result type is accessible for further processing.\nHere’s the implementation of the more sophisticated \nAsync.StartCancelable  \noperator compared to Async.Start  (in bold):\ntype Microsoft.FSharp.Control.Async with   static member StartCancelable(op:Async<'a>) (tap:'a -> unit)(?onCancel)=        let ct = new System.Threading.CancellationTokenSource()        let onCancel = defaultArg onCancel ignore        Async.StartWithContinuations(op, tap, ignore, onCancel, ct.Token)        { new IDisposable with             member x.Dispose() = ct.Cancel() }\nThe underlying implementation of the Async.StartCancelable function uses the \nAsync.StartWithContinuations  operator, which provides built-in support for can-\ncellation behavior. When the asynchronous operation op:Async<'a>  is passed (as the \nfirst argument completes), the result is passed as a continuation into the second argu-ment function \ntap:'a -> unit . The optional parameter onCancel  represents the \nfunction that’s triggered; in this case, the main operation op:Async<'a>  is canceled. \nThe result of Async.StartCancelable is an anonymous object created dynamically \nbased on the IDisposable interface, which will cancel the operation if the Dispose  \nmethod is called.\nF# Async API\nTo create or use the async workflows to program, there’s a list of functions that the Async  \nmodule in F# exposes. These are used to trigger other functions providing a variety of ways to create the async workflow. This can be either a background thread or a .NET Framework \nTask  object, or running the computation in the current thread itself.\n \nThe previously utilized F# Async operators Async.StartWithContinuations , Async \n.Ignore , and Async.Start may require a bit more explanation.  \n \n 267 Asynchronous computation expressions\nasync.startwithcontinuations  \nAsync.StartWithContinuations  executes an asynchronous workflow starting immedi-\nately on the current OS thread, and after its completion passes respectively the result, exception, and cancel (\nOperationCancelledException)  to one of specified functions. \nIf the thread that initiates the execution has its own SynchronizationContext  associ-\nated with it, then final continuations will use this SynchronizationContext  for posting \nresults. This function is a good candidate for updating GUIs. It accepts as arguments three functions to invoke when the asynchronous computation completes successfully, or raises an exception, or is canceled. \nIts signature is \nAsync<'T> ->('T -> unit)*(exn -> unit)*(OperationCanceled-\nException -> unit) -> unit . Async.StartWithContinuations  doesn’t support a \nreturn value because the result of the computation is handled internally by the function targeting the successful output. \nListing 9.7  Async.StartWithContinuations  \nlet computation() = async {       use client = new  WebClient()    let! manningSite =              client.AsyncDownloadString(Uri(""http://www.manning.com""))     return manningSite    }Async.StartWithContinuations(computation(),                  (fun site-> printfn ""Size %d"" site.Length),              (fun exn->printfn""exception-%s""<|exn.ToString()),        (fun exn->printfn""cancell-%s""<|exn.ToString()))      \nasync.ignore  \nThe Async.Ignore  operator takes a computation and returns a workflow that executes \nsource computation, ignores its result, and returns unit . Its signature is Async.Ignore:  \nAsync< 'T> -> Async<unit>.\nThese are two possible approaches that use Async.Ignore : \nAsync.Start(Async.Ignore computationWithResult())let asyncIgnore = Async.Ignore >> Async.Start\nThe second option creates a function asyncIgnore , using function composition to \ncombine the Async.Ignore  and Async.Start  operators. The next listing shows the \ncomplete example, where the result of the asynchronous operation is ignored using the \nasyncIgnore  function (in bold).The asynchronous computation \nreturns a long string.\nStarts the asynchronous computation immediately using the current OS thread\nThe computation completes successfully and the continuation is invoked, printing the size  of the downloaded website.\nThe operation throws an exception; the exception continuation will execute, printing the exception details. The operation is canceled, and the cancellation \ncontinuation is invoked, printing information \nregarding the cancellation.",11340
89-9.3.6 Asynchronous workflow cancellation support.pdf,89-9.3.6 Asynchronous workflow cancellation support,"268 chapter  9 Asynchronous functional programming in F#\nListing 9.8  Async.Ignore\nlet computation() = async {       use client = new  WebClient()    let! manningSite =              client.AsyncDownloadString(Uri(""http://www.manning.com""))    printfn ""Size %d"" manningSite.Length         return manningSite    }Async.Ignore (computation())  \nIf you need to evaluate the result of an asynchronous operations without blocking, in a pure CPS style, the operator \nAsync.StartWithContinuations  offers a better \napproach.\nasync.start\nThe Async.Start  function in listing 9.9 doesn’t support a return value; in fact, its \nasynchronous computation is type Async<unit> . The operator Async.Start  executes \ncomputations asynchronously so the computation process should itself define ways for communication and returning the final result. This function queues an asynchronous workflow for execution in the thread pool and returns control immediately to the caller without waiting to complete. Because of this, the operation can be completed on another thread.\nIts signature is \nAsync.Start:  Async<unit> -> unit . As optional arguments, this \nfunction takes a cancellationToken.\nListing 9.9  Async.Start  \nlet computationUnit() = async {     do! Async.Sleep 1000    use client = new WebClient()    let! manningSite =              client.AsyncDownloadString(Uri(""http://www.manning.com""))    printfn ""Size %d"" manningSite.Length        } Async.Start(computationUnit())  \nBecause Async.Start  doesn’t support a return value, the size of the website is printed \ninside the expression, where the value is accessible. What if the computation does return a value, and you cannot modify the asynchronous workflow? It’s possible to dis-charge the result from an asynchronous computation using the \nAsync.Ignore  func-\ntion before starting the operation.\n9.3.6 Asynchronous workflow cancellation support \nWhen executing an asynchronous operation, it’s useful to terminate execution prema-\nturely, before it completes, on demand. This works well for long-running, non-blocking This asynchronous computation returns a long string.\nThe computation runs asynchronously and the result is discharged (ignored). \nCreates an asynchronous computation to download a website, with a one-second delay to simulate heavy computation\nPrints the website’s size from inside the body of the expression\nRuns the computation without blocking the caller thread\n \n 269 Asynchronous computation expressions\noperations, where making them cancelable is the appropriate practice to avoid tasks that can hang. For example, you may want to cancel the operation of downloading 100 images from Azure Blob storage if the download exceeds a certain period of time. The F# asynchronous workflow supports cancellation natively as an automatic mechanism, and when a workflow is canceled, it also cancels all the child computations.\nMost of the time you’ll want to coordinate cancellation tokens and maintain control \nover them. In these cases, you can supply your own tokens, but in many other cases, you can achieve similar results with less code by using the built-in F# asynchronous module default token. When the asynchronous operation begins, this underlying system passes a provided \nCancellationToken , or assigns an arbitrary one if not provided, to the work-\nflow, and it keeps track of whether a cancellation request is received. The computation builder, \nAsyncBuilder , checks the status of the cancellation token during each binding \nconstruct ( let! , do!, return! , use! ). If the token is marked “canceled” the workflow \nterminates. \nThis is a sophisticated mechanism that eases your work when you don’t need to do \nanything complex to support cancellation. Moreover, the F# asynchronous workflow supports an implicit generation and propagation of cancellation tokens through its execution, and any nested asynchronous operations are included automatically in the cancellation hierarchy during asynchronous computations. \nF# supports cancellation in different forms. The first is through the function \nAsync \n.StartWithContinuations , which observes the default token and cancels the workflow \nwhen the token is set as canceled. When the cancellation token triggers, the function to handle the cancellation token is called in place of the success one. The other options include passing a cancellation token manually or relying on the default \nAsync.Default -\nCancellationToken  to trigger Async.CancellationToken  (in bold in listing 9.10). \nListing 9.10 shows how to introduce support for cancellation in the previous Async \n.Parallel  image download (listing 9.6). In this example, the cancellation token is \npassed manually, because in the automatic version using the Async.DefaultCancella -\ntionToken,  there’s no code change, only the function to cancel the last asynchronous \noperation.\nListing 9.10  Canceling an asynchronous computation \nlet tokenSource = new CancellationTokenSource()    let container = getCloudBlobContainer()let parallelComp() =      container.ListBlobs()     |> Seq.map(fun blob -> downloadMediaCompAsync container blob)     |> Async.Parallel Async.Start(parallelComp() |> Async.Ignore, tokenSource.Token)    tokenSource.Cancel()           Instance of \nCancellationTokenSource used to \ngenerate a CancellationToken \nA cancellation token is generated and passed into the asynchronous computation to stop the execution on demand. \n \n270 chapter  9 Asynchronous functional programming in F#\nYou created an instance of a CancellationTokenSource  that passes a cancellation \ntoken to the asynchronous computation, starting the operation with the Async.Start  \nfunction and passing CancellationToken  as the second argument. Then you cancel \nthe operation, which terminates all nested operations.\nIn listing 9.11, Async.TryCancelled  appends a function to an asynchronous work-\nflow.  It’s this function that will be invoked when the cancellation token is marked. This is \nan alternative way to inject extra code to run in case of cancellation. The following listing shows how to use the \nAsync.TryCancelled  function, which also has the advantage of \nreturning a value, providing compositionality. (The code to note is in bold.)\nListing 9.11  Canceling an asynchronous computation with notification\nlet onCancelled = fun (cnl:OperationCanceledException) ->              printfn ""Operation cancelled!""let tokenSource = new CancellationTokenSource()let tryCancel = Async.TryCancelled(parallelComp(), onCancelled)  Async.Start(tryCancel, tokenSource.Token)\nTryCancelled  is an asynchronous workflow that can be combined with other computa-\ntions. Its execution begins on demand with an explicit request, using a starting function such as \nAsync.Start  or Async.RunSynchronously.\nasync.runsynchronously  \nThe Async.RunSynchronously  function blocks the current thread during the work-\nflow execution and continues with the current thread when the workflow completes. This approach is ideal to use in an F# interactive session for testing and in console applications, because it waits for the asynchronous computation to complete. It’s not the recommended way to run an asynchronous computation in a GUI program, how-ever, because it will block the UI.\nIts signature is \nAsync< 'T> -> 'T. As optional arguments, this function takes a timeout \nvalue and a cancellationToken. The following listing shows the simplest way to exe-\ncute an asynchronous workflow (in bold).\nListing 9.12  Async.RunSynchronously  \nlet computation() = async {      do! Async.Sleep 1000       use client = new  WebClient()    return! client.AsyncDownloadString(Uri(""www.manning.com""))        }\nlet manningSite = Async.RunSynchronously(computation())  printfn ""Size %d"" manningSite.Length Function that’s triggered to handle the \nOperationCanceledException exception \nin case an operation is canceled\nThe parallelComp function is wrapped \ninto the Async.TryCancelled operator to \nhandle the custom behaviors triggered \nif the operation is canceled.\nCreates an asynchronous computation to download a website\nAdds a one-second delay to simulate heavy computation\nDownloads a website \nasynchronously \nRuns the computation Prints the size of the downloaded website",8304
90-9.3.7 Taming parallel asynchronous operations.pdf,90-9.3.7 Taming parallel asynchronous operations,"271 Asynchronous computation expressions\n9.3.7 Taming parallel asynchronous operations\nThe Async.Parallel  programing model is a great feature for enabling I/O parallel-\nism based on the Fork/Join pattern. Fork/Join allows you to execute a series of compu-tation, such that execution branches off in parallel at designated points in the code, to merge at a subsequent point resuming the execution.\nBut because \nAsync.Parallel  relies on the thread pool, the maximum degree of par -\nallelism is guaranteed, and, consequently, performance increases. Also, cases exist where starting a large number of asynchronous workflows can negatively impact performance. Specifically, an asynchronous workflow is executed in a semi-preemptive manner, where after many operations (more than 10,000 in a 4 GB RAM computer) begin execution, asynchronous workflows are enqueued, and even if they aren’t blocking or waiting for a long-running operation, another workflow is dequeued for execution. This is an edge case that can damage the parallel performance, because the memory consumption of the pro-gram is proportional to the number of ready-to-run workflows, which can be much larger than the number of CPU cores. \nAnother case to pay attention to is when asynchronous operations that can run in \nparallel are constraints by external factors. For example, running a console application that performs web requests, the default maximum number of concurrent HTTP con-nections allowed by a \nServicePoint4 object is two. In the particular example of Azure \nBlob storage, you link the Async.Parallel  to execute multiple long-running opera-\ntions in parallel, but ultimately, without changing the base configuration, there will be only a limited two parallel web requests. For maximizing the performance of your code, it’s recommended you tame the parallelism of the program by throttling the number of concurrent computations.\nThe following code listing shows the implementation of two functions \nParallel -\nWithThrottle  and ParallelWithCatchThrottle , which can be used to refine the num-\nber of running concurrent asynchronous operations.\nListing 9.13  ParallelWithThrottle  and ParallelWithCatchThrottle  \ntype Result<'a> = Result<'a, exn>      module Result =    let ofChoice value =                   match value with        | Choice1Of2 value -> Ok value        | Choice2Of2 e -> Error emodule Async =  let parallelWithCatchThrottle (selector:Result<'a> -> 'b)                       (throttle:int)                               (computations:seq<Async<'a>>) = async {   \n4 Used to get or set the maximum number of concurrent connections allowed. Defines the Result<'a> alias\nHelper function to map between the Choice and Result DU types\nThe selector function applies a projection \nto the result of the async computation.\nThe max number of concurrent async operations\nLists async computations to execute in parallel\n \n272 chapter  9 Asynchronous functional programming in F#\n     use semaphore = new SemaphoreSlim(throttle)               let throttleAsync (operation:Async<'a>) = async {                try                    do! semaphore.WaitAsync()                     let! result = Async.Catch operation                                 return selector (result |> Result.ofChoice)              finally                    semaphore.Release() |> ignore }                return! computations                    |> Seq.map throttleAsync                    |> Async.Parallel  }  let parallelWithThrottle throttle computations =      parallelWithCatchThrottle id throttle computations   \nThe function parallelWithCatchThrottle  creates an asynchronous computation that \nexecutes all the given asynchronous operations, initially queuing each as work items and using a Fork/Join pattern. The parallelism is throttled, so that the most throttle computations run at one time. \nIn listing 9.13, the function \nAsync.Catch  is exploited to protect a parallel asynchro-\nnous computation from failure. The function parallelWithCatchThrottle  doesn’t \nthrow exceptions, but instead returns an array of F# Result types.\nThe second function, parallelWithThrottle , is a variant of the former function \nthat uses id in place of the selector  argument. The id function in F# is called an iden-\ntity function, which is a shortcut for an operation that returns itself: (fun x -> x) . In \nthe example, id  is used to bypass the selector  and return the result of the operation \nwithout applying any transformation.\nThe release of F# 4.1 introduced the Result<'TSuccess, 'TError>  type, a conve-\nnient DU that supports consuming code that could generate an error without having to implement exception handling. The \nResult  DU is typically used to represent and \npreserve an error that can occur during execution. \nThe first line of code in the previous listing defined a Result<'a>  type alias over the \nResult<'a, exn> , which assumes that the second case is always an exception ( exn). \nThis Result<'a>  type alias aims to simplify the pattern matching over the result :\nlet! result = Async.Catch operation\nYou can handle exceptions in F# asynchronous operations in different ways. The most idiomatic is to use \nAsync.Catch  as a wrapper that safeguards a computation by inter -\ncepting all the exceptions within the source computation. Async.Catch  takes a more \nfunctional approach because, instead of having a function as an argument to handle an error, it returns a discriminated union of \nChoice<'a, exn> , where 'a is the result \ntype of the asynchronous workflow, and exn is the exception thrown. The underlying \nvalues of the result Choice<'a, exn>  can be extracted with pattern matching. I cover \nerror handling in functional programming in chapter 10.The lock primitive used to throttle async computations\nThe function used to run each computation and limit parallelism by the lock primitive accessRuns the computation, guarding the result in case of exception\nMaps the result with \nthe Result DU type \nand then passes it to \nthe selector functionCompletes the computation and releases the lock \n \n 273 Asynchronous computation expressions\nNOTE  The nondeterministic behavior of asynchronous parallel computations \nmeans you don’t know which asynchronous computation will fail first. But the asynchronous combinator \nAsync.Parallel  reports the first failure between all \nthe computations, and it cancels the other jobs by invoking the cancellation token for the group of tasks.\nChoice<'T, exn>  is a DU5 with two union cases: \n¡ Choice1Of2 of 'T  contains the result for successful workflow completion.\n¡ Choice2Of2 of exn  represents the workflow failure and contains the thrown \nexception.\nHandling exceptions with this functional design lets you construct the asynchronous code in a compositional and natural pipeline structure.\nNOTE  The Async.Catch function preserves information about the error, mak-\ning it easier to diagnose the problem. Using Choice<_,_>  lets you use the type \nsystem to enforce the processing paths for both results and errors.\nChoice<'T, 'U>  is a DU built into the F# core, which is helpful; but in this case, you can \ncreate a better representation of the asynchronous computation result by replacing the DU \nChoice  with the meaningful DU Result<'a> .6 (The code to note is in bold.)\nListing 9.14  ParallelWithThrottle  with Azure Table Storage downloads \nlet maxConcurrentOperations = 100                ServicePointManager.DefaultConnectionLimit <- maxConcurrentOperations let downloadMediaCompAsyncParallelThrottle() = async {    let! container = getCloudBlobContainerAsync()      let computations =       container.ListBlobs()       |> Seq.map(fun blobMedia -> transformAndSaveImage container blobMedia)    return! Async.parallelWithThrottle                    maxConcurrentOperations computations }\nThe code sets the limit of the concurrent request maxConcurrentOperations  to 100 \nusing ServicePointManager.DefaultConnectionLimit . The same value is passed \nas an argument to parallelWithThrottle  to throttle the concurrent requests. max-\nConcurrentOperations  is an arbitrary number that can be large, but I recommend \nthat you test and measure the execution time and memory consumption of your pro-gram to detect which value has the best performance impact. \n5 For more information, see http:/ /mng.bz/03fl.\n6 Introduced in chapter 4.Sets the limit of max concurrent operations\nSets the DefaultConnectionLimit, \nwhich is two by default \nCreates a list of async operations\nExecutes the async operations, taming the parallelism\n \n274 chapter  9 Asynchronous functional programming in F#\nSummary\n¡ With asynchronous programming, you can download multiple images in parallel, removing hardware dependencies and releasing unbounded computational power.\n¡ The FP language F# provides full support for asynchronous programming inte-grating within the asynchronous programming model provided by .NET. It also offers an idiomatic functional implementation of the APM called asynchronous workflow, which can interop the task-based programming model in C#.\n¡ The F# asynchronous workflow is based on the Async<'a>  type, which defines \na computation that will complete sometime in the future. This provides great compositionality properties because it doesn’t start immediately. Asynchronous computation requires an explicit request to start. \n¡ The time to perform multiple synchronous I/O operations sequentially is equal to the sum of the time elapsed for each individual operation, in comparison to the asynchronous approach, which runs in parallel, so the overall response time is equal to the slowest operation.\n¡ Using a continuation passing style, which embraces the functional paradigm, your code becomes remarkably concise and easy to write as multithreaded code.\n¡ The F# computation expression, specifically in the form of an asynchronous workflow, performs and chains a series of computations asynchronously without blocking the execution of other work.\n¡ Computation expressions can be extended to operate with different elevated types without the need to leave the current context, or you can create your own to extend the compiler’s capabilities.\n¡ It’s possible to build tailored asynchronous combinators to handle special cases.",10358
91-10.1.1 The problem of error handling in imperative programming.pdf,91-10.1.1 The problem of error handling in imperative programming,"27510Functional combinators \nfor fluent concurrent \nprogramming\nThis chapter covers\n¡ Handling exceptions in a functional style\n¡ Using built-in Task  combinators\n¡ Implementing custom asynchronous combinators and conditional operators\n¡ Running parallel asynchronous heterogeneous computations\nIn the previous two chapters, you learned how to apply asynchronous programming to develop scalable and performant systems. You applied functional techniques to compose, control, and optimize the execution of multiple tasks in parallel. This chapter further raises the level of abstraction for expressing asynchronous computa-tions in a functional style. \nWe’ll start by looking at how to manage exceptions in a functional style, with a \nfocus on asynchronous operations. Next, we’ll explore functional combinators, a useful programming tool for building a set of utility functions that allow you to create com-plex functions by composing smaller and more concise operators. These combina-tors and techniques make your code more maintainable and performant, improving your ability to write concurrent computations and handle side effects. Toward the \n \n276 chapter  10 Functional combinators for fluent concurrent programming\nend of this chapter, we’ll go through how to interop between C# and F# by calling and passing asynchronous functions from one to the other. \nOf all the chapters in this book, this one is the most complex, because it covers FP \ntheory where the lexicon might appear as jargon initially. With great effort, comes great reward . . . .\nThe concepts explained in this chapter will provide exceptional tools for building \nsophisticated concurrent programs simply and easily. It’s not necessary for the average programmer to know exactly how the .NET garbage collector (GC) works, because it operates in the background. But the developer who understands the operational details of the GC can maximize a program’s memory use and performance.\nThroughout this chapter, we revisit the examples from chapter 9, with slightly more \ncomplex variations. The code examples are in C# or F#, using the programming lan-guage that best resonates with the idea in context. But all the concepts apply to both programming languages, and in most cases you’ll find the alternate code example in the source code.\nThis chapter can help you to understand the compositional semantics of functional \nerror handling and functional combinators so you can write efficient programs for pro-cessing concurrent (and parallel) asynchronous operations safely, with minimum effort and high-yield performance. \nBy the end of this chapter, you’ll see how to use built-in asynchronous combinators \nand how to design and implement efficient custom combinators that perfectly meet your applications’ requirements. You can raise the level of abstraction in complex and slow-running parts of the code to effortlessly simplify the design, control flow, and reduce the execution time.\n10.1 The execution flow isn’t always on the happy path: error handling \nMany unexpected issues can arise in software development. Enterprise applications, in general, are distributed and depend on a number of external systems, which can lead to a multitude of problems. Examples of these problems are:\n¡ Losing network connectivity during a web request\n¡ Applications that fail to communicate with the server\n¡ Data that becomes inadvertently null  while processing\n¡ Thrown exceptions\nAs developers, our goal is to write robust code that accounts for these issues. But address-ing potential issues can itself create complexity. In real-world applications, the execution flow isn’t always on the “happy path” where the default behavior is error-free (figure 10.1). To prevent exceptions and to ease the debugging process, you must deal with val-idation logic, value checking, logging, and convoluted code. In general, computer pro-grammers tend to overuse and even abuse exceptions. For example, in code it’s common for an exception to be thrown; and, absent the handler in that context, the caller of this piece of code is forced to handle that exception several levels up the call stack. \n \n 277 The execution flow isn’t always on the happy path: error handling \nValidate input\nRecord not found,\ndatabase errorInput value is null,\nempty , or not valid\nAuthorization error ,\ntimeout error .Update existing record input\nSend notification\nReturn resultReceive inputUser sends an\nupdate request\nIn asynchronous programming, error handling is important to guarantee the safe exe-cution of your application. It’s assumed that an asynchronous operation will complete, but what if something goes wrong and the operation never terminates? Functional and imperative paradigms approach error handling with different styles:\n¡ The imperative programming approach to handling errors is based on side effects. Imperative languages use the introduction of \ntry-catch  blocks and \nthrow  statements to generate side effects. These side effects disrupt the normal \nprogram flow, which can be hard to reason about. When using the traditional imperative programming style, the most common approach to handling an error is to guard the method from raising an error and return a \nnull  value if the \npayload is empty. This concept of error processing is widely used, but handling errors this way within the imperative languages isn’t a good fit because it intro-duces more opportunities for bugs. \n¡ The FP approach focuses on minimizing and controlling side effects, so error handling is generally done while avoiding mutation of state and without throw-ing exceptions. If an operation fails, for example, it should return a structural representation of the output that includes the notification of success or failure.\n10.1.1 The problem of error handling in imperative programming \nIn the .NET Framework, it’s easy to capture and react to errors in an asynchronous operation. One way is to wrap all the code that belongs to the same asynchronous com-putation into a \ntry-catch  block. \nTo illustrate the error-handling problem and how it can be addressed in a functional \nstyle, let’s revisit the example of downloading images from Azure Blob storage (covered in chapter 9). Listing 10.1 shows how it makes the method \nDownloadImageAsync  safe \nfrom exceptions that could be raised during its execution (in bold).Figure 10.1  The user sends an update \nrequest, which can easily stray from the happy path. In general, you write code thinking that nothing can go wrong. But producing quality code must account for exceptions or possible issues such as validation, failure, or errors that prevent the code from running correctly. \n \n278 chapter  10 Functional combinators for fluent concurrent programming\nListing 10.1  DownloadImageAsync  with traditional imperative error handling\nstatic async Task<Image> DownloadImageAsync(string blobReference){    try    {       var container = await Helpers.GetCloudBlobContainerAsync().  \n➥ ConfigureAwait(false); \n       CloudBlockBlob blockBlob = container. \n➥ GetBlockBlobReference(blobReference);   \n       using (var memStream = new MemoryStream())       {          await blockBlob.DownloadToStreamAsync(memStream).\nConfigureAwait(false);  \n          return Bitmap.FromStream(memStream);       }     }     catch (StorageException ex)     {        Log.Error(""Azure Storage error"", ex);          throw;     }     catch (Exception ex)     {        Log.Error(""Some general error"", ex);          throw;     }}async RunDownloadImageAsync()    {    try     {        var image = await DownloadImageAsync(""Bugghina0001.jpg"");        ProcessImage(image);     }     catch (Exception ex)     {            HanldlingError(ex);              throw;     }}\nIt seems easy and straightforward: first DownloadImageAsync is called by the caller \nRunDownloadImageAsync , and the image returned is processed. This code example \nalready assumes that something could go wrong and wraps the core execution into a \ntry-catch  block. Banking on the happy path—that’s the path in which everything goes \nright—is a luxury that a programmer cannot afford for building robust applications.\nAs you can see, when you start accounting for potential failures, input errors, and log-\nging routine, the method starts turning into lengthy boilerplate code. If you remove the error-handling lines of code, there are only 9 lines of meaningful core functionality, com-pared with 21 of boilerplate orchestration dedicated to error and log handling alone.\nA nonlinear program flow like this can quickly become messy because it’s hard to \ntrace all existing connections between \nthrow  and catch  statements. Furthermore, with Observes operations that could raise an exception\nSomewhere in the upper the call stack\nHandles and re-throws the error to bubble up the exception to the call stack",8910
92-10.2 Error combinators Retry Otherwise and Task.Catch in C.pdf,92-10.2 Error combinators Retry Otherwise and Task.Catch in C,"279 Error combinators: Retry, Otherwise, and Task.Catch in C#\nexceptions it’s unclear exactly where the errors are being caught. It’s possible to wrap up the validation routine with a \ntry-catch  statement right when it’s called, or the try-\ncatch  block can be inserted a couple levels higher. It becomes difficult to know if the \nerror is thrown intentionally.\nIn listing 10.1, the body of the method DownloadImageAsync is wrapped inside a \ntry-catch  block to safeguard the program in case an exception occurs. But in this case, \nthere’s no error handling applied; the exception is rethrown and a log with the error details is applied. The purpose of the \ntry-catch  block is to prevent an exception by \nsurrounding a piece of code that could be unsafe; but if an exception is thrown, the runtime creates a stack trace of all function calls leading up to the instruction that gen-erated the error.\nDownloadImageAsync is executed, but what kind of precaution should be used to \nensure that potential errors are handled? Should the caller be wrapped up into a try-\ncatch  block, too, as a precaution?\nImage image = await DownloadImageAsync(""Bugghina001.jpg"");\nIn general, the function caller is responsible for protecting the code by checking state of the objects for validity before use. What would happen if the check of state is miss-ing? Easy answer: more problems and bugs appear. \nIn addition, the complexity of the program increases when the same \nDownload-\nImageAsync appears in multiple places throughout the code, because each caller could \nrequire different error handling, leading to leaks and domain models with unnecessary complexity. \n10.2 Error combinators: Retry, Otherwise, and Task.Catch in C#\nIn chapter 8, we defined two extension methods for the Task  type, Retry  and Other-\nwise  (fallback), for asynchronous Task  operations that apply logic in case of an excep-\ntion. Fortunately, because asynchronous operations have external factors that make them vulnerable to exceptions, the .NET \nTask  type has built-in error handling via the \nStatus  and Exception  properties, as shown here ( Retry  and Otherwise  are in bold \nfor reference). \nListing 10.2  Refreshing the Otherwise  and Retry  functions\nstatic async Task<T> Otherwise<T>(this Task<T> task, \n➥ Func<Task<T>> orTask) =>    \n     task.ContinueWith(async innerTask => {          if (innerTask.Status == TaskStatus.Faulted) \n➥ return await orTask(); \n          return await Task.FromResult<T>(innerTask.Result);    }).Unwrap();static async Task<T> Retry<T>(Func<Task<T>> task, int retries, TimeSpan \n➥ delay, CancellationToken cts = default(CancellationToken))  \n    => await task().ContinueWith(async innerTask =>    {        cts.ThrowIfCancellationRequested();Provides a fallback function if something goes wrong\nRetries the given function a certain \nnumber of times, with a given delay \nbetween attempts\n \n280 chapter  10 Functional combinators for fluent concurrent programming\n        if (innerTask.Status != TaskStatus.Faulted)            return innerTask.Result;        if (retries == 0)            throw innerTask.Exception ?? throw new Exception();        await Task.Delay(delay, cts);        return await Retry(task, retries - 1, delay, cts);    }).Unwrap(); \nIt’s good practice to use the functions Retry  and Otherwise  to manage errors in your \ncode. For example, you can rewrite the call of the method DownloadImageAsync  using \nthe helper functions:\nImage image = await AsyncEx.Retry(async () =>    await DownloadImageAsync(""Bugghina001.jpg"")      .Otherwise(async () =>    await DownloadImageAsync(""Bugghina002.jpg"")),                        5, TimeSpan.FromSeconds(2));\nBy applying the functions Retry  and Otherwise  in the previous code, the function \nDownloadImageAsync changes behavior and becomes safer to run. If something goes \nwrong when DownloadImageAsync  is retrieving the image Bugghina001 , its operation \nfallback is to download an alternative image. The Retry  logic, which includes the Oth-\nerwise  (fallback) behavior, is repeated up to five times with a delay of two seconds \nbetween each operation, until it’s successful (figure 10.2). \nClient Server\nTime T0\nTime T0 + 2XFailingDownloadImageAsync(""Bugghina001.jpg"")\nXFailingOtherwiseDownloadImageAsync(""Bugghina002.jpg"")\nXFailingRetryDownloadImageAsync(""Bugghina001.jpg"")\nXFailingOtherwiseDownloadImageAsync(""Bugghina002.jpg"")\nImage image = Async.Retry( async () => await DownloadImageAsync(""Bugghina001.jpg"" )\n   .Otherwise( async () => await DownloadImageAsync(""Bugghina002.jpg""),                   5, TimeSpan.FromSeconds(2) )\nFigure 10.2  The client sends two requests to the server to apply the strategies of Otherwise  \n(fallback) and Retry  in case of failure. These requests ( DownloadImageAsync ) are safe to run \nbecause they both apply the Retry  and Otherwise  strategies to handle problems that may occur.\n \n 281 Error combinators: Retry, Otherwise, and Task.Catch in C#\nAdditionally, you can define a further extension method such as the Task.Catch  \nfunction, tailored specifically to handle exceptions generated during asynchronous operations.\nListing 10.3  Task.Catch  function\nstatic Task<T> Catch<T, TError>(this Task<T> task, \n➥ Func<TError, T> onError) where TError : Exception\n{    var tcs = new TaskCompletionSource<T>();        task.ContinueWith(innerTask =>    {        if (innerTask.IsFaulted && innerTask?.Exception?.InnerException \n➥ is TError)\n            tcs.SetResult(onError((TError)innerTask.Exception.\n➥ InnerException)); \n        else if (innerTask.IsCanceled)            tcs.SetCanceled();              else if (innerTask.IsFaulted)            tcs.SetException(innerTask?.Exception?.InnerException ?? \n➥ throw new InvalidOperationException()); \n        else            tcs.SetResult(innerTask.Result);      });    return tcs.Task;} \nThe function Task.Catch  has the advantage of expressing specific exception cases \nas type constructors. The following snippet shows an example of handling Storage-\nException  in the Azure Blob storage context (in bold):\nstatic Task<Image> CatchStorageException(this Task<Image> task) =>      task.Catch<Image, StorageException>(ex => Log($""Azure Blob \n➥ Storage Error {ex.Message}""));\nThe CatchStorageException  extension method can be applied as shown in this code \nsnippet:\nImage image = await DownloadImageAsync(""Bugghina001.jpg"")  \n.CatchStorageException();\nYes, this design could violate the principle of nonlocality, because the code used to recover from the exception is different from the originating function call. In addition, there’s no support from the compiler to notify the developer that the caller of the \nDownloadImageAsync  method is enforcing error handling, because its return type is a \nregular Task  primitive type, which doesn’t require and convey validation. In this last \ncase, when the error handling is omitted or forgotten, an exception could potentially arise, causing unanticipated side effects that might impact the entire system (beyond the function call), leading to disastrous consequences, such as crashing the applica-tion. As you can see, exceptions ruin the ability to reason about the code. Further -\nmore, the structured mechanism of throwing and catching exceptions in imperative programming has drawbacks that are against functional design principles. As one An instance of TaskCompletionSource returns a Task type to keep consistency in the async model.\nSets the Result or Exception of \nthe TaskCompletionSource \nbased on the output of the Task",7617
93-10.2.1 Error handling in FP exceptions for flow control.pdf,93-10.2.1 Error handling in FP exceptions for flow control,"282 chapter  10 Functional combinators for fluent concurrent programming\nexample, functions that throw exceptions can’t be composed or chained the way other functional artifacts can. \nGenerally, code is read more often than written, so it makes sense that best practices \nare aimed at simplifying understanding and reasoning about the code. The simpler the code, the fewer bugs it contains, and the easier it is to maintain the software overall. The use of exceptions for program flow control hides the programmer’s intention, which is why it’s considered a bad practice. Thankfully, you can avoid complex and cluttered code relatively easily.\nThe solution is to explicitly return values indicating success or failure of an opera-\ntion instead of throwing exceptions. This brings clarity to potentially error-prone code parts. In the following sections, I show two possible approaches that embrace the func-tional paradigm to ease the error-handling semantic structure.\n10.2.1 Error handling in FP: exceptions for flow control \nLet’s revisit the DownloadImageAsync  method, but this time handling the error in a \nfunctional style. First, look at the code example, followed by the details in the following listing. The new method \nDownloadOptionImage  catches the exception in a try-catch  \nblock as in the previous version of the code, but here the result is the Option  type (in \nbold).\nListing 10.4  Option  type for error handling in a functional style \nasync Task<Option<Image>> DownloadOptionImage(string blobReference)  {    try    {        var container = await Helpers.GetCloudBlobContainerAsync().\n➥ConfigureAwait(false);\n        CloudBlockBlob blockBlob = container.\n➥GetBlockBlobReference(blobReference);\n        using (var memStream = new MemoryStream())        {           await \n➥ blockBlob.DownloadToStreamAsync(memStream).ConfigureAwait(false);\n           return Option.Some(Bitmap.FromStream(memStream));            }    }    catch (Exception)    {        return Option.None;            }}\nThe Option  type notifies the function caller that the operation DownloadOptionIm -\nage has a particular output, which must be specifically managed. In fact, the Option  \ntype can have as a result either Some  or None . Consequently, the caller of the func-\ntion Download OptionImage  is forced to check the result for a value. If it contains the \nvalue, then this is a success, but if it doesn’t, then it’s a failure. This validation requires The output of the function is a composite \nTask wrapping an Option type.\nThe result Option type is either a Some \nvalue for a successful operation or None \n(nothing) in case of error. \n \n 283 Error combinators: Retry, Otherwise, and Task.Catch in C#\nthe programmer to write code to handle both possible outcomes. Use of this design makes the code predictable, avoids side effects, and permits \nDownloadOptionImage  to \nbe composable. \ncontrolling  side effects  with the option  type\nIn FP, the notion of null  values doesn’t exist. Functional languages such as Haskell, \nScala, and F# resolve this problem by wrapping the null able values in an Option  type. \nIn F#, the Option  type is the solution to the null-pointer exception; it’s a two-state dis-\ncriminated union (DU), which is used to wrap a value ( Some ), or no value ( None ). Con-\nsider it a box that might contain something or could be empty. Conceptually, you can think the \nOption  type as something that’s either present or absent. The symbolic defi-\nnition for Option  type is\ntype Option<'T> =| Some of value:T| None\nThe Some  case means that data is stored in the associated inner value T. The None case \nmeans there’s no data. Option<Image> , for example, may or may not contain an image. \nFigure 10.3 shows the comparison between a nullable primitive type and the equiva-lent \nOption  type.\nObject with a value\nRegular\nprimitiveEmpty object\nstring x = ""value"" string x = null\nOption\ntypeOptional<string> x =            Some(""value"" )Optional<string> x = Non e\nFigure 10.3  This illustrates the comparison between regular null able primitives (first row) and \nOption  types (second row). The main difference is that the regular primitive type can be either a valid \nor invalid (null ) value without informing the caller, whereas the Option  type wraps a primitive type \nsuggesting to the caller to check if the underlying value is valid.\nAn instance of the Option  type is created by calling either Some(value) , which rep-\nresents a positive response, or None , which is the equivalent of returning an empty \nvalue. With F#, you don’t need to define the Option  type yourself. It’s part of the stan-\ndard F# library, and there is a rich set of helper functions that go with it. \nC# has the Nullable<T>  type, which is limited to value types. The initial solution is to \ncreate a generic struct  that wraps a value. Using a value type ( struct ) is important for \nreducing the memory allocation and is ideal for avoiding null  reference exceptions by \nassigning a null  value to an Option  type itself.\nTo make the Option  type reusable, we use a generic C# struct Option<T>,  which \nwraps any arbitrary type that may or may not contain a value. The basic structure of \nOption<T>  has a property value  of type T and a flag HasValue  that indicates whether \nthe value is set.",5365
94-10.2.2 Handling errors with TaskOptionT in C.pdf,94-10.2.2 Handling errors with TaskOptionT in C,,0
95-10.2.3 The F AsyncOption type combining Async and Option.pdf,95-10.2.3 The F AsyncOption type combining Async and Option,"284 chapter  10 Functional combinators for fluent concurrent programming\nThe implementation of the Option  type in C# is straightforward, and isn’t illustrated \nhere. You can check the source code of this book if you’re interested in understanding the C# \nOption  type implementation. The higher level of abstraction achieved using the \nOption<T>  type allows the implementation of higher-order functions (HOFs), such as \nMatch  and Map, which simplifies the compositional structure of the code, and in this \ncase, with the function Match , allows pattern matching and a deconstructive semantic: \nR Match<R>(Func<R> none, Func<T, R> some) => hasValue ? some(value) : none();\nThe Match  function belongs to the Option  type instance, which offers a convenient \nconstruct by eliminating unnecessary casts and improving code readability.\n10.2.2 Handling errors with Task<Option<T>> in C#\nIn listing 10.4, I illustrated how the Option  type protects the code from bugs, mak-\ning the program safer from null -pointer exceptions, and suggested that the compiler \nhelps to avoid accidental mistakes. Unlike null  values, an Option  type forces the devel-\noper to write logic to check if a value is present, thereby mitigating many of the prob-lems of \nnull  and error  values. \nBack to the Azure Blob storage example, with the Option  type and the Match  HOF \nin place, you can execute the DownloadOptionImage  function, whose return type is a \nTask<Option<Image>> : \nOption<Image> imageOpt = await DownloadOptionImage (""Bugghina001.jpg"");\nBy using the compositional nature of the Task  and Option  types and their extended \nHOFs, the FP style (in bold) looks like this code snippet:\nDownloadOptionImage (""Bugghina001.jpg"")     .Map(opt => opt.Match(                     some: image => image.Save(""ImageFolder\Bugghina.jpg""),                     none: () => Log(""There was a problem downloading \n➥ the image"")));\nThis final code is fluent and expressive, and, more importantly, it reduces bugs because the compiler forces the caller to cover both possible outcomes: success and failure.\n10.2.3 The F# AsyncOption type: combining Async and Option \nThe same approach of handling exceptions using the Task<Option<T>>  type is applica-\nble to F#. The same technique can be exploited in the F# asynchronous workflow for a more idiomatic approach.\nThe improvement that F# achieves, as compared to C#, is support for type aliases, also \ncalled type abbreviations. A type alias is used to avoid writing a signature repeatedly, sim-\nplifying the code experience. Here’s the type alias for \nAsync<Option<’T>> :\ntype AsyncOption<'T> = Async<Option<'T>>\nYou can use this AsyncOption<'T>  definition directly in the code in place of Async-\n<Option<'T>>  for the same behavior. Another purpose of the type alias is to provide \n \n 285 Error combinators: Retry, Otherwise, and Task.Catch in C#\na degree of decoupling between the use of a type and the implementation of a type. This listing shows the equivalent F# implementation of the \nDownloadOptionImage  pre-\nviously implemented in C#.\nListing 10.5  F# implementation of the AsyncOption  type alias in action\nlet downloadOptionImage(blobReference:string) : AsyncOption<Image> =   async {      try                   let! container = Helpers.getCloudBlobContainerAsync()        let blockBlob = container.GetBlockBlobReference(blobReference)        use memStream = new MemoryStream()        do! blockBlob.DownloadToStreamAsync(memStream)        return Some(Bitmap.FromStream(memStream))            with                  | _ -> return None             }downloadOptionImage ""Bugghina001.jpg""|> Async.map(fun imageOpt ->             match imageOpt with                  | Some(image) -> do! image.SaveAsync(""ImageFolder\Bugghina.jpg"")     | None -> log ""There was a problem downloading the image"")\nThe function downloadOptionImage  asynchronously downloads an image from Azure \nBlob storage. The Async.map  function, with signature ('a -> 'b) -> Async<'a> -> \nAsync<'b> , wraps the output of the function and allows access to the underlying value. \nIn this case, the generic type 'a is an Option<Image> . \nNOTE  One important point shown in listing 10.5 is that the linear implemen-\ntation of asynchronous code allows exception handling in the same way as synchronous code. This mechanism, which is based on the \ntry-with  block, \nensures that the occurring exceptions bubble up through the non-blocking calls and are ultimately handled by the exception handler code. This mecha-nism is guaranteed to run correctly despite the presence of multiple threads that run in parallel during its execution.\nConveniently, the functions that belong to the F# \nAsync  module can be applied to \nthe alias AsyncOption , because it’s an Async  type that wraps an Option . The function \ninside the Async.map  operator extracts the Option  value, which is pattern matched to \nselect the behavior to run according to whether it has the value Some  or None .The return type is explicitly set to \nAsyncOption<Image>; this can be omitted. The try-with block safely manages potential errors.\nConstructs the Option type with a Some value (Image)\nApplies the HOF Async.map that accesses and projects against underlying Option value\nPattern matches the Option type to deconstruct \nand access the wrapped Image valueThe image SaveAsync extension method implementation can be found in the downloadable source code.",5472
96-10.2.5 Preserving the exception semantic with the Result type.pdf,96-10.2.5 Preserving the exception semantic with the Result type,"286 chapter  10 Functional combinators for fluent concurrent programming\n10.2.4 Idiomatic F# functional asynchronous error handling \nAt this point, the F# downloadOptionImage  function is safely downloading an image, \nensuring that it will catch the exception if a problem occurs without jeopardizing the application’s stability. But the presence of the \ntry-with  block, equivalent to try-catch  \nin C#, should be avoided when possible, because it encourages an impure (with side effects) programming style. In the context of asynchronous computation, the F# \nAsync  \nmodule provides an idiomatic and functional approach by using the Async.Catch  \nfunction as a wrapper that protects a computation. \nNOTE  I introduced Async.Catch  in chapter 9. It takes a more functional \napproach because, instead of having a function as an argument to handle an error, it returns a discriminated union of \nChoice<'a, exn> , where 'a is the \nresult type of the asynchronous workflow, and exn is the exception thrown. \nYou can use Async.Catch  to safely run and map asynchronous operations into a \nChoice<'a, exn>  type. To reduce the amount of boilerplate required, and generally \nsimplify your code, you can create a helper function that wraps an Async<'T>  and \nreturns an AsyncOption<'T>  by using the Async.Catch  operator. The following code \nsnippet shows the implementation. The helper function ofChoice  is supplementary to \nthe F# Option  module, whose purpose it is to map and convert a Choice  type into an \nOption  type:\nmodule Option =    let ofChoice choice =        match choice with        | Choice1Of2 value -> Some value        | Choice2Of2 _ -> Nonemodule AsyncOption =        let handler (operation:Async<'a>) : AsyncOption<'a> = async {            let! result = Async.Catch operation            return (Option.ofChoice result)        }\nAsync.Catch  is used for exception handling to convert Async<'T>  to Async<Choice  \n<'T, exn>> . This Choice  is then converted to an Option<'T>  using a simple conver -\nsion ofChoice  function. The AsyncOption  handler function can safely run and map \nasynchronous Async<'T>  operations into an AsyncOption  type. \nListing 10.6 shows the downloadOptionImage  implementation without the need to \nprotect the code with the try-with  block. The function AsyncOption.handler  is man-\naging the output, regardless of whether it succeeds or fails. In this case, if an error arises, \nAsync.Catch  will capture and transform it into an Option  type through the Option.\nofChoice  function (in bold).\nListing 10.6  AsyncOption  type alias in action\nlet downloadAsyncImage(blobReference:string) : Async<Image> = async {        let! container = Helpers.getCloudBlobContainerAsync()        let blockBlob = container.GetBlockBlobReference(blobReference)        use memStream = new MemoryStream()\n \n 287 Error combinators: Retry, Otherwise, and Task.Catch in C#\n        do! blockBlob.DownloadToStreamAsync(memStream)        return Bitmap.FromStream(memStream)    } downloadAsyncImage ""Bugghina001.jpg"" |> AsyncOption.handler             |> Async.map(fun imageOpt ->           match imageOpt with                | Some(image) -> image.Save(""ImageFolder\Bugghina.jpg"")    | None -> log ""There was a problem downloading the image"")|> Async.Start\nThe function AsyncOption.handler  is a reusable and composable operator that can \nbe applied to any asynchronous operation.\n10.2.5 Preserving the exception semantic with the Result type\nIn section 10.2.2, you saw how the functional paradigm uses the Option  type to han-\ndle errors and control side effects. In the context of error handling, Option  acts as a \ncontainer, a box where side effects fade and dissolve without creating unwanted behav-iors in your program. In FP, the notion of boxing dangerous code, which could throw errors, isn’t limited to the \nOption  type. \nIn this section, you’ll preserve the error semantic to use the Result  type, which \nallows different behaviors to dispatch and branch in your program based upon the type of error. Let’s say that as part of the implementation of an application, you want to ease the debugging experience or to communicate to the caller of a function the exception details if something goes wrong. In this case, the \nOption  type approach doesn’t fit the \ngoal, because it delivers None  (nothing) as far as information about what went wrong. \nWhile it’s unambiguous what a Some  result means, None doesn’t convey any information \nother than the obvious. By discarding the exception, it’s impossible to diagnose what could have gone wrong.\nGoing back to our example of downloading an image from Azure Blob storage, if \nsomething goes wrong during the retrieval of the data, there are diverse errors gener -\nated from different cases, such as the loss of network connectivity and file/image not found. In any event, you need to know the error details to correctly apply a strategy to recover from an exception.\nIn this listing, the \nDownloadOptionImage  method from the previous example \nretrieves an image from the Azure Blob storage. The Option  type (in bold) is exploited \nto handle the output in a safer manner, managing the event of errors.\nListing 10.7  Option  type, which doesn’t preserve error details \nasync Task<Option<Image>> DownloadOptionImage(string blobReference)        {            try            {                CloudStorageAccount storageAccount = \n➥ CloudStorageAccount.Parse(""<Azure Connection>"");Executes an asynchronous operation, capturing the exception automatically \nMaps the result of the computation to access the underlying imageOpt value\nDeconstructs the Option type with pattern matching to handle the different cases \n \n288 chapter  10 Functional combinators for fluent concurrent programming\n                CloudBlobClient blobClient = \n➥ storageAccount.CreateCloudBlobClient();\n                CloudBlobContainer container = \n➥ blobClient.GetContainerReference(""Media"");\n                await container.CreateIfNotExistsAsync();                CloudBlockBlob blockBlob = container.\n➥ GetBlockBlobReference(blobReference);\n                using (var memStream = new MemoryStream())                {                    await blockBlob.DownloadToStreamAsync(memStream).\n➥ ConfigureAwait(false);\n                    return Some(Bitmap.FromStream(memStream));                }            }            catch (StorageException)            {                return None;             }            catch (Exception)            {                return None;              }        }\nRegardless of the exception type raised, either a StorageException  or a generic \nException , the limitation with the code implementation is that the caller of the \nmethod DownloadOptionImage  doesn’t have any information regarding the exception, \nso a tailored recover strategy cannot be chosen.\nIs there a better way? How can the method provide details of a potential error and \navoid side effects? The solution is to use the polymorphic Result<'TSuccess, 'TError>  \ntype in place of the Option<'T>  type. \nResult<'TSuccess, 'TError>  can be used to handle errors in a functional style plus \ncarry the cause of the potential failure. Figure 10.4 compares a null able primitive, the \nequivalent Option  type, and the Result  type.\n Object with a value\nRegular\nprimitiveEmpty object\nstring x = ""value"" string x = null\nOption\ntypeOptional<string> x =            Some(""value"" )Optional<string> x = Non e\nResult\ntypeResult<string> x =             Success(""value"")Result<string> x = Failure(error)\nThe failure represents an errorin case something goes wrong.\nFigure 10.4  Comparing a regular null able primitive (top row), the Option  type (second row), and \nthe Result  type (bottom row). The Result Failure  is generally used to wrap an error if something \ngoes wrong.Regardless of the exception type raised, the Option type returns None in both cases.\n \n 289 Error combinators: Retry, Otherwise, and Task.Catch in C#\nIn certain programming languages, such as Haskell, the Result  structure is called \nEither , which represents a logical separation between two values that would never \noccur at the same time. For example, Result<int, string>  models two cases and can \nhave either value int or string . \nThe Result<'TSuccess, 'TError>  structure can also be used to guard your code \nagainst unpredictable errors, which makes the code more type safe and side effect free by eliminating the exception early on instead of propagating it. \nThe F# Result type\nThe Result  type, introduced in chapter 9, is an F# convenient DU that supports consum -\ning code that could generate an error without having to implement exception handling. Starting with F# 4.1, the \nResult  type is defined as part of the standard F# library. If \nyou’re using earlier versions of F#, you can easily define it and its helper functions in a few lines:\nType Result<'TSuccess,'TFailure> =    | Success of 'TSuccess    | Failure of 'TFailure\nIt’s possible, with minimum effort, to interop between the F# core library and C# to share the same F# \nResult  type structure to avoid code repetition. You can find the F# extension \nmethods that facilitate the interoperability of the Result  type in C# in the source code.\n \nIn listing 10.8, the C# example of the Result  type implementation is tailored to be \npolymorphic in only one type constructor, while forcing the Exception  type as an alter -\nnative value to handle errors. Consequently, the type system is forced to acknowledge error cases and makes the error-handling logic more explicit and predictable. Certain implementation details are omitted in this listing for brevity but provided in full as part of the downloadable source code.\nListing 10.8  Generic Result<T>  type in C#\nstruct Result<T>{    public T Ok { get; }                  public Exception Error { get; }      public bool IsFailed { get => Error != null; }    public bool IsOk => !IsFailed;    public Result(T ok)               {                 Ok = ok;        Error = default(Exception);    }    public Result(Exception error)      {         Error = error;        Ok = default(T);    }\nProperties to expose the values for either the successful or failure operations\nConstructors pass the value of the successful operation or an exception, in case of failure.",10404
97-10.3 Taming exceptions in asynchronous operations.pdf,97-10.3 Taming exceptions in asynchronous operations,"290 chapter  10 Functional combinators for fluent concurrent programming\n   public R Match<R>(Func<T, R> okMap, Func<Exception, R> failureMap)         => IsOk ? okMap(Ok) : failureMap(Error);                 public void Match(Action<T> okAction, Action<Exception> errorAction)         { if (IsOk) okAction(Ok); else errorAction(Error);}             public static implicit operator Result<T>(T ok) =>   \n➥ new Result<T>(ok);  \n   public static implicit operator Result<T>(Exception error) =>   \n➥ new Result<T>(error); \n   public static implicit operator Result<T>(Result.Ok<T> ok) =>   \n➥ new Result<T>(ok.Value); \n   public static implicit operator Result<T>(Result.Failure error) => \n➥ new Result<T>(error.Error);            \n}\nThe interesting part of this code is in the final lines where the implicit operators sim-plify the conversion to \nResult  during the assignment of primitives. This auto-construct \nto Result  type should be used by any function that potentially returns an error.\nHere, for example, is a simple synchronous function that loads the bytes of a given \nfile. If the file doesn’t exist, then a FileNotFoundException  exception is returned:\nstatic Result<byte[]> ReadFile(string path){    if (File.Exists(path))        return File.ReadAllBytes(path);    else        return new FileNotFoundException(path);}\nAs you can see, the output of the function ReadFile  is a Result<byte[]> , which wraps \neither the successful outcome of the function that returns a byte array, or the failure case that returns a \nFileNotFoundException  exception. Both return types, Ok and \nFailure , are implicitly converted without a type definition. \nNOTE  The purpose of the Result  class is similar to the Option  type discussed \nearlier. The Result  type allows you to reason about the code without looking \ninto the implementation details. This is achieved by providing a choice type with two cases, an \nOk case to return the value when the function succeeds, and a \nFailure Error  case to return the value when the function failed.\n10.3 Taming exceptions in asynchronous operations \nThe polymorphic Result  class in C# is a reusable component that’s recommended for \ntaming side effects in the case of functions that could generate exceptions. To indicate that a function can fail, the output is wrapped with a \nResult  type. The following listing This convenient Match function \ndeconstructs the Result type and applies \ndispatching behavioral logic.Implicit operators automatically convert any primitive type into a Result. \n \n 291 Taming exceptions in asynchronous operations \nshows the previous DownloadOptionImage  function refactored to follow the Result  \ntype model (in bold). The new function is named DownloadResultImage .\nListing 10.9  DownloadResultImage : handling errors and preserving semantics\nasync Task<Result<Image>> DownloadResultImage(string blobReference){    try    {        CloudStorageAccount storageAccount = \n➥ CloudStorageAccount.Parse(""<Azure Connection>"");\n        CloudBlobClient blobClient = \n➥ storageAccount.CreateCloudBlobClient();\n        CloudBlobContainer container = \n➥ blobClient.GetContainerReference(""Media"");\n        await container.CreateIfNotExistsAsync();        CloudBlockBlob blockBlob = container.\n➥GetBlockBlobReference(blobReference);\n        using (var memStream = new MemoryStream())        {            await blockBlob.DownloadToStreamAsync(memStream).\n➥ConfigureAwait(false);\n            return Image.FromStream(memStream);          }   }   catch (StorageException exn)   {        return exn;      }   catch (Exception exn)   {       return exn;     }   }\nIt’s important that the Result  type provides the caller of the DownloadResultImage  \nfunction the information necessary to handle each possible outcome in a tailored man-ner, including different error cases. In this example, because \nDownloadResultImage  is \ncalling a remote service, it also has the Task  effect (for asynchronous operations) as \nwell as the Result  effect. In the Azure storage example (from listing 10.9), when the \ncurrent state of an image is retrieved, that operation will hit the online media stor -\nage. It’s recommended to make it asynchronous, as I’ve mentioned, so the Result  type \nshould be wrapped in a Task . The Task  and Result  effects are generally combined in \nFP to implement asynchronous operations with error handling.\nBefore diving into how to use the Result  and Task  types in combination, let’s define \na few helper functions to simplify the code. The static class ResultExtensions  defines \na series of useful HOFs for the Result  type, such as bind  and map, which are applicable \nfor a convenient fluent semantic to encode common error-handling flows. For brevity purpose, in the following listing only the helper functions that treat the \nTask  and Result  The Result type implicit operators allow automatic wrapping of the primitive types into a Result, which is also wrapped into a Task.\n \n292 chapter  10 Functional combinators for fluent concurrent programming\ntypes are shown (in bold). The other overloads are omitted, with the full implementa-tion available in the code samples.\nListing 10.10  Task<Result<T>>  helper functions for compositional semantics\nstatic class ResultExtensions{     public static async Task<Result<T>> TryCatch<T>(Func<Task<T>> func)     {         try         {             return await func();         }         catch (Exception ex)         {              return ex;         }     }static async Task<Result<R>> SelectMany<T, R>(this Task<Result<T>> \n➥ resultTask, Func<T, Task<Result<R>>> func)\n{     Result<T> result = await resultTask.ConfigureAwait(false);     if (result.IsFailed)         return result.Error;       return await func(result.Ok);}static async Task<Result<R>> Select<T, R>(this Task<Result<T>> resultTask,\n➥ Func<T, Task<R>> func)\n{     Result<T> result = await resultTask.ConfigureAwait(false);     if (result.IsFailed)        return result.Error;     return await func(result.Ok).ConfigureAwait(false);}static async Task<Result<R>> Match<T, R>(this Task<Result<T>> resultTask,\n➥ Func<T, Task<R>> actionOk, Func<Exception, Task<R>> actionError)\n{     Result<T> result = await resultTask.ConfigureAwait(false);     if (result.IsFailed)         return await actionError(result.Error);     return await actionOk(result.Ok);}}\nThe TryCatch  function wraps a given operation into a try-catch  block to safeguard \nthe code from any exceptions if a problem arises. This function is useful for lifting and combining any \nTask  computation into a Result  type. In the following code snip-\npet, the function ToByteArrayAsync  asynchronously converts a given image into a byte \narray: \nTask<Result<byte[]>> ToByteArrayAsync(Image image){     return TryCatch(async () =>\n \n 293 Taming exceptions in asynchronous operations \n     {        using (var memStream = new MemoryStream())        {            await image.SaveImageAsync(memStream, image.RawFormat);            return memStream.ToArray();        }      });}\nThe underlying TryCatch  function ensures that regardless of the behavior in the oper -\nation, a Result  type is returned which wraps either a successful ( Ok byte array) or a \nfailure (Error  exception).\nThe extension methods Select  and SelectMany , part of the ResultExtensions  \nclass, are generally known in functional programming as, respectively, Bind  (or flat-\nMap) and Map. But in the context of .NET and specifically in C#, the names Select  and \nSelectMany  are the recommended terms because they follow the LINQ convention, \nwhich notifies the compiler that treats these functions as LINQ expressions to ease their composition semantic structure. Now, with the higher-order operators from the \nResultExtensions  class, it’s easy to fluently chain a series of actions that operate on the \nunderlying Result  value without leaving the context.\nThe following listing shows how the caller of DownloadResultImage  can handle the \nexecution flow in the case of success or failure as well as chaining the sequence of oper -\nations (the code to note is in bold). \nListing 10.11  Composing Task<Result<T>>  operations in functional style \nasync Task<Result<byte[]>> ProcessImage(string nameImage, string \ndestinationImage){\n    return await DownloadResultImages(nameImage)                .Map(async image => await ToThumbnail(image))                     .Bind(async image => await ToByteArrayAsync(image))                  .Tap(async bytes =>                                 await File.WriteAllBytesAsync(destinationImage, \n➥ bytes));    \nAs you can see from the ProcessImage  function signature, providing documentation \nthat a function might have error effects is one of the advantages of using the Result  \ntype. ProcessImage  first downloads a given image from the Azure Blob storage, then \nconverts it into a thumbnail format using the Bind  operator, which checks the previous \nResult  instance, and if it’s successful, executes the delegate passed in. Otherwise, the \nBind  operator returns the previous result. The Map operator also verifies the previous \nResult  value and acts accordingly by extracting the byte array from the image. \nThe chain continues until one of the operations fails. If failure occurs, then the other \noperations are skipped.Uses HOFs to effortlessly combine the functions, \nreturning the composite type Task<Result<T>>\nThe WriteAllBytesAsync extension method implementation can be found in the downloadable source code.\n \n294 chapter  10 Functional combinators for fluent concurrent programming\nNOTE  The Bind  function operates over lifted values, in this case a Task <Result  \n<Image>> . In contrast, the Map function performs against an unwrapped type.\nUltimately, the result byte array is saved in the destination path ( destinationImage ) \nspecified, or a log is executed if an error occurred. Rather than handling failure indi-vidually on each call, you should add the failure handling at the end of the computa-tion chain. This way, the failure-handling logic is at a predictable place in the code, making it easier to read and maintain. \nYou should understand that if any of these operations fails, the rest of the tasks are \nbypassed and none executed until the first function that handles the error (figure 10.5). In this example, the error is handled by the function \nMatch  (with the lambda action-\nError ). It’s important to perform compensation logic in case the call to a function isn’t \nsuccessful. \n \nValidate input InputSuccess:\noutput\nFailure:\nerrorValidate input Validate inputAfter validation, the input value\nis wrapped into a result type thatrepresents either success or failure.\nIf the result type is failure, the rest of thevalidation follows the failure path, aggregating\nthe remainder of the invalid information.\nFigure 10.5  The Result  type handles the operations in a way that, if during each step there is a failure, \nthe rest of the tasks are bypassed and not executed until the first function that handles the error. In this figure, if any of the validations throws an error, the rest of computation is skipped until the \nFailure  \nhandler (the Error  circle). \nBecause it’s both hard and inconvenient to extract the inner value of a Result  type, \nuse the composition mechanisms of functional error handling. These mechanisms force the caller to always handle both the success and the failure cases. Using this design of the \nResult  type, the program flow is declarative and easy to follow. Exposing \nyour intent is crucial if you want to increase readability of your code. Introducing the \nResult  class (and the composite type Task<Result<T>> ) helps to show, without side \neffects, if the method can fail or isn’t signaling something is wrong with your system. Furthermore, the type system becomes a helpful assistant for building software by spec-ifying how you should handle both successful and failure outcomes.\nThe \nResult  type provides a conditional flow in a high-level functional style, where \nyou pick a strategy for dealing with the error and register that strategy as a handler. When the lower-level code hits the error, it can then pick a handler without unwinding the call stack. This gives you more options. You can choose to cope with the problem and continue.",12412
98-10.3.2 Extending the F AsyncResult type with monadic bind operators.pdf,98-10.3.2 Extending the F AsyncResult type with monadic bind operators,"295 Taming exceptions in asynchronous operations \n10.3.1 Modeling error handling in F# with Async and Result \nThe previous section discussed the concept of Task  and Result  types combined for \nproviding safe and declarative error handling in a functional style. In addition to the TPL, the asynchronous workflow computation expression in F# offers a more idiom-atic functional approach. This section covers the recipe for taming exceptions by show-ing how to combine the F# \nAsync  type with the Result  structure.\nBefore looking in depth at the F# error-handling model for asynchronous opera-\ntions, we should define the type structure necessary. First, to fit into the context of error handling (specifically), as explained in chapter 9, you should define a \nResult<'a>  type \nalias over Result<'a, exn> , which assumes that the second case is always an exception \n(exn). This alias Result<'a>  simplifies pattern matching and deconstruction over the \nResult<'a, exn>  type:\nResult<'TSuccess> = Result<'TSuccess, exn>\nSecond, the type construct Async  has to wrap this Result<'a>  structure to define a new \ntype that’s used in concurrent operations to signal when an operation is completed. You need to treat \nAsync<'a> and Result<'a> as a single type, which can be done eas-\nily using an alias types that acts as a combinatorial structure:\ntype AsyncResult<'a> = Async<Result<'a>>\nThe AsyncResult<'a>  type carries the value of an asynchronous computation, with \neither a success or failure outcome. In the case of an exception, the error information is preserved. Conceptually, \nAsyncResult  is a separate type.\nNow, taking inspiration from the AsyncOption  type in section 10.2.2, define a \nhelper function AsyncResult.handler  to run a computation lifting the output into \na Result  type. For this purpose, the F# Async.Catch  function denotes a perfect fit. \nThe following listing shows a custom alternative representation of Async.Catch , called \nAsyncResult.handler . \nListing 10.12  AsyncResult  handler to catch and wrap asynchronous computations\nmodule Result =        let ofChoice value =                         match value with            | Choice1Of2 value -> Ok value            | Choice2Of2 e -> Error emodule AsyncResult =       let handler (operation:Async<'a>) : AsyncResult<'a> = async {            let! result = Async.Catch operation                 return (Result.ofChoice result) }       \nThe F# AsyncResult.handler  is a powerful operator that dispatches the execution \nflow in case of error. In a nutshell, the AsyncResult.handler  runs the Async.Catch  \nfunction in the background for error handling and uses the ofChoice  function to map Maps the function from the Choice DU, which is the returned type of the Async.Catch operator, to the Result type DU cases. This map function was defined in chapter 9.\nRuns the asynchronous operation \nusing the Async.Catch operator to \nsafeguard against possible error The output of the function is mapped in favor of the Result type.\n \n296 chapter  10 Functional combinators for fluent concurrent programming\nthe product of the computation ( Choice<Choice1Of2, Choice2Of2> Discriminated \nUnion ) to the Result<'a>  DU cases, which then branch the result of the computation \nrespectively to the OK or Error  union. (ofChoice  was introduced in chapter 9.)\n10.3.2 Extending the F# AsyncResult type with monadic bind operators \nBefore we go further, let’s define the monadic helper functions to deal with the Async-\nResult  type.\nListing 10.13  HOF extending the AsyncResult  type \nmodule AsyncResult =    let retn (value:'a) : AsyncResult<'a> =  \n➥ value |> Ok |> async.Return   \n    let map (selector : 'a -> Async<'b>) (asyncResult : AsyncResult<'a>) \n➥ : AsyncResult<'b> = \n      async {        let! result = asyncResult        match result with                | Ok x -> return! selector x |> handler          | Error err -> return (Error err)   }       let bind (selector : 'a -> AsyncResult<'b>) (asyncResult   \n➥ : AsyncResult<'a>) = async {      \n        let! result = asyncResult        match result with                 | Ok x -> return! selector x          | Error err -> return Error err    }    let bimap success failure operation = async {             let! result = operation        match result with        | Ok v -> return! success v |> handler           | Error x -> return! failure x |> handler }        \nThe map and bind  higher-order operators are the general functions used for composition. \nThese implementations are straightforward:\n¡ The retn  function lifts an arbitrary value 'a into an AsyncResult<'a>  elevated \ntype.\n¡ The let!  syntax in the map operator extracts the content from the Async  (runs it \nand awaits the result), which is the Result< 'a> type. Then, the selector  function \nis applied on a Result  value contained in the Ok case using the AsyncResult.\nhandler  function, because the outcome of the computation can be success or \nfailure. Ultimately, the result is returned wrapped in the AsyncResult  type.  \n¡ The function bind  uses continuation passing style (CPS) to pass a function that \nwill run a successful computation to further process the result. The continuation function \nselector  crosses the two types Async  and Result  and has the signature \n'a -> AsyncResult<'b> .Lifts an arbitrary given value into the AsyncResult type\nUses the AsyncResult.handler function to handle either the success or failure of the async operationMaps an AsyncResult type running the underlying \nasynchronous operation asyncResult and applies \nthe given selector function to the result\nBinds a monadic operator that performs a given function over the AsyncResult elevated type\nExecutes either the success or failure function against the AsyncResult type operation by branching the result of the computation respectively to the OK or Error union\n \n 297 Taming exceptions in asynchronous operations \n¡ If the inner Result is successful, then the continuation function selector  is \nevaluated with the result. The return!  syntax means that the return value is \nalready lifted.\n¡ If the inner Result is a failure, then the failure of the async operation is lifted.\n¡ The return syntax in map, retn , and bind  lifts the Result  value to an Async  type.\n¡ The return!  syntax in bind  means that the value is already lifted and not to call \nreturn  on it.\n¡ The bimap  function aims to execute the asynchronous operation AsyncResult  \nand then branches the execution flow to one of the continuation functions, either \nsuccess  or failure , according to the result.\nAlternatively, to make the code more succinct, you can use the built-in function \nResult.map  to turn a value into a function that works on a Result type. Then, if you \npass the output to Async.map , the resulting function works on an asynchronous value. \nUsing this compositional programming style, for example, the AsyncResult  map func-\ntion can be rewritten as follows:\nmodule AsyncResult =    let map (selector : 'a -> 'b) (asyncResult : AsyncResult<'a>) =         asyncResult |> Async.map (Result.map selector)\nThis programming style is a personal choice, so you should consider the tradeoff between succinct code and its readability. \nthe f# asyncresult higher -order  functions  in action\nLet’s see how to perform the AsyncResult  type and its HOFs bind , map, and return . Let’s \nconvert the C# code in listing 10.7 that downloads an image from Azure Blob storage into an idiomatic F# way to handle errors in an asynchronous operation context. \nWe stay with the Azure Blob storage example to simplify the understanding of the \ntwo approaches with a direct comparison by converting a function that you’re already familiar with (figure 10.6).\n \nBind Bind Bimap Bind\nValidate input InputSuccess:\noutput\nFailure:\nerrorValidate input Validate inputThe bind operator letsyou chain the result type.The bimap function pattern-\nmatches the Result type todispatch the continuation logic\nto the success or failure branch.\nFigure 10.6  The validation logic can be composed fluently with minimum effort, by applying the higher-\norder operators bind  and bimap . Furthermore, at the end of the pipeline, the bimap  function pattern-\nmatches the Result  type to dispatch the continuation logic to either the success  or failure  branch \nin a convenient and declarative style.\n \n298 chapter  10 Functional combinators for fluent concurrent programming\nThis listing shows the processImage  function implemented using the F# AsyncResult  \ntype with its higher-order compositional operators (in bold).\nListing 10.14  Using AsyncResult  HOFs for fluent composition \nlet processImage(blobReference:string) (destinationImage:string) \n➥ : AsyncResult<unit> = \n        async {            let storageAccount = CloudStorageAccount.Parse(""< Azure \nConnection >"") \n            let blobClient = storageAccount.CreateCloudBlobClient()              let container = blobClient.GetContainerReference(""Media"")            let! _ = container.CreateIfNotExistsAsync()             let blockBlob = container.GetBlockBlobReference(blobReference)            use memStream = new MemoryStream()            do! blockBlob.DownloadToStreamAsync(memStream)            return Bitmap.FromStream(memStream) }             |> AsyncResult.handler                        |> AsyncResult.bind(fun image -> toThumbnail(image))               |> AsyncResult.map(fun image -> toByteArrayAsync(image))              |> AsyncResult.bimap                                           (fun bytes -> FileEx.\nWriteAllBytesAsync(destinationImage, bytes)) \n                     (fun ex -> logger.Error(ex) |> AsyncResult.retn) \nThe behavior of processImage is similar to the related C# method processImage from \nlisting 10.7; the only difference is the result definition AsyncResult type. Semantically, \ndue to the intrinsic F# ( |>) pipe operator, the AsyncResult functions handler , bind , \nmap, and bimap  are chained in a fluent style, which is the nearest equivalent to the con-\ncept of fluent interfaces (or method chaining) used in the C# version of the code.\nraising  the abstraction  of the f# asyncresult with computation  expression\nImagine that you want to further abstract the syntax from code listing 10.12 so you can write \nAsyncResult  computations in a way that can be sequenced and combined \nusing control flow constructs. In chapter 9, you built custom F# computational expres-sions (CEs) to retry asynchronous operations in case of errors. CEs in F# are a safe way \nof managing the complexity and mutation of state. They provide a convenient syntax to manage data, control, and side effects in functional programs.\nIn the context of asynchronous operations wrapped into an \nAsyncResult  type, you \ncan use CEs to handle errors elegantly to focus on the happy path. With the Async-\nResult  monadic operators bind  and return  in place, implementing the related \ncomputation expression requires minimal effort to achieve a convenient and fluid pro-gramming semantic.\nHere, the code listing defines the monadic operators (in bold) for the computation \nbuilder that combines the \nResult  and Async  types: \nType AsyncResultBuilder () =    Member x.Return m = AsyncResult.retn mThe AsyncResult higher-order operator \ncan be composed in a fluent style.AsyncResult.retn lifts the logger function into the AsyncResult elevated type to match the output signature.\n \n 299 Taming exceptions in asynchronous operations \n    member x.Bind (m, f:'a -> AsyncResult<'b>) = AsyncResult.bind f m    member x.Bind (m:Task<'a>, f:'a -> AsyncResult<'b>) =            AsyncResult.bind f (m |> Async.AwaitTask |> AsyncResult.handler)    Ember x.ReturnFrom m = mLet asyncResult = AsyncResultBuilder() \nYou can add more members to the AsyncResultBuilder CE if you need support for \nmore advanced syntax; this is the minimal implementation required for the example. The only line of code that requires a clarification is the \nBind  with Task<'a>  type: \nmember x.Bind (m:Task<'a>, f) =               AsyncResult.bind f (m |> Async.AwaitTask \n➥ |> AsyncResult.handler)\nIn this case, as explained in section 9.3.3, the F# CE lets you inject functions to extend the manipulation to other wrapper types, in this case \nTask , whereas the Bind  function \nin the extension lets you fetch the inner value contained in the elevated type using the \nlet!  and do! operators. This technique removes the need for adjunctive func-\ntions such as Async.AwaitTask . The downloadable source code of this book contains \na more complete implementation of the AsyncResultBuilder  CE, but the extra CE \nimplementation details aren’t relevant or part of this book’s scope. \nA simple CE deals with asynchronous calls that return a Result  type and can be use-\nful for performing computations that may fail and then chain the results together. Let’s transform, once again, the \nprocessImage  function, but this time the computation is \nrunning inside the AsyncResultBuilder  CEs, as shown in bold in this listing.\nListing 10.15  Using AsyncResultBuilder  \nlet processImage (blobReference:string) (destinationImage:string) \n➥ : AsyncResult<unit> =\n    asyncResult  {          let storageAccount = CloudStorageAccount.Parse(""<Azure Connection>"")       let blobClient = storageAccount.CreateCloudBlobClient()       let container = blobClient.GetContainerReference(""Media"")       let! _ = container.CreateIfNotExistsAsync()       let blockBlob = container.GetBlockBlobReference(blobReference)       use memStream = new MemoryStream()       do! blockBlob.DownloadToStreamAsync(memStream)       let image = Bitmap.FromStream(memStream)       let! thumbnail = toThumbnail(image)       return! toByteArrayAsyncResult thumbnail      }      |> AsyncResult.bimap (fun bytes -> \n➥ FileEx.WriteAllBytesAsync(destinationImage, bytes)) \n                          (fun ex -> logger.Error(ex) |> async.Return.retn)    \nNow, all you need do is wrap the operations inside an asyncResult  CE block. The com-\npiler can recognize the monadic (CE) pattern and treats the computations in a special way. When the \nlet!  bind operator is detected, the compiler automatically translates \nthe AsyncResult.Return  and AsyncResult.Bind  operations of a CE in context.Wraps the code block into an \nasyncResult to make the bind \noperator run in the context of \nthe AsyncResultBuilder CEs",14489
99-10.5.1 The TPL built-in asynchronous combinators.pdf,99-10.5.1 The TPL built-in asynchronous combinators,"300 chapter  10 Functional combinators for fluent concurrent programming\n10.4 Abstracting operations with functional combinators\nLet’s say you need to download and analyze the history of a stock ticker symbol, or you decide you need to analyze the history of more than one stock to compare and contrast the best ones to buy. It’s a given that downloading data from the internet is an I/O-bound operation that should be executed asynchronously. But suppose you want to build a more sophisticated program, where downloading the stock data depends on other asynchronous operations (figure 10.7). Here are several examples:\n¡ If either the NASDAQ or the NYSE index is positive\n¡ If the last six months of the stock has a positive trend\n¡ If the volume of the stock compiles any number of positive criteria to buy\nLook for\na positive\nindex.\nIs the\ntrend over the\nlast six months\npositive?No\nNoYesNASDAQ positive\nBuyNYSE positive\nAre\nthere any\npositive factors\nto buy?\nYes\nWhat about running the flow in figure 10.7 for each stock symbol that you’re interested in? How would you combine the conditional logic of these operations while keeping the asynchronous semantic to parallelize the execution? How would you design the program? \nThe solution is functional asynchronous combinators. The following sections cover the \ncharacteristics of functional combinators, with the focus on asynchronous combina-tors. We’ll  cover how to use the built-in support in the .NET Framework and how to build and tailor your own asynchronous combinators to maximize the performance of your program using a fluid and declarative functional programming style. Figure 10.7  This diagram represents a sequential decision tree for buying stock. Each step likely involves an I/O operation to asynchronously interrogate an external service. You must be thoughtful in your approach to maintain this sequential flow, while performing the whole decision tree asynchronously.",1964
100-10.5.2 Exploiting the Task.WhenAny combinator for redundancy andinterleaving.pdf,100-10.5.2 Exploiting the Task.WhenAny combinator for redundancy andinterleaving,"301 Functional combinators in a nutshell \n10.5 Functional combinators in a nutshell \nThe imperative paradigm uses procedural control mechanisms such as if-else  state-\nments and for/while  loops to drive a program’s flow. This is contrary to the FP style. \nAs you leave the imperative world behind, you’ll learn to find alternatives to fill in that gap. A good solution is to use function combinators that orchestrate the flow of the program. FP mechanisms make it easy to combine two or more solutions from smaller problems into a single abstraction that solves a larger problem.\nAbstraction is a pillar of FP, which allows you to develop an application without wor -\nrying about the implementation details, allowing you to focus on the more important high-level semantics of the program. Essentially, abstraction captures the core of what a function or a whole program does, making it easier to get things done.\nIn FP, a combinator refers to either a function with no free variables (https://wiki \n.haskell.org/Pointfree) or a pattern for composing and combining any types. This sec-ond definition is the central topic of this section. \nFrom a practical viewpoint, functional combinators are programming constructs that \nallow you to merge and link primitive artifacts, such as other functions (or other com-binators), and behave as pieces of control logic together, working to generate more- advanced behaviors. In addition, functional combinators encourage modularity, which supports the objective to abstract functions into components that can be understood and reused independently, with codified meaning derived from rules governing their composition. You were introduced to this concept previously with the definition of asynchronous functions (combinators) such as \nOtherwise  and Retry  for the C# Task-\nbased Asynchronous Programming (TAP) model and the F# AsyncResult.handler .\nIn the context of concurrent programming, the main reason to use combinators is to \nimplement a program that can handle side effects without compromising a declarative and compositional semantic. This is possible because combinators abstract away from the developer implementation details that might handle side effects underneath, with the purpose of offering functions that compose effortlessly. Specifically, this section covers combinators that compose asynchronous operations. \nIf the side effects are limited to the scope of a single function, then the behavior call-\ning that function is idempotent. Idempotent means the operation can be applied multi-\nple times without changing the result beyond the initial application—the effect doesn’t change. It’s possible to chain these idempotent functions to produce complex behav-iors where the side effects are isolated and controlled. \n10.5.1 The TPL built-in asynchronous combinators\nThe F# asynchronous workflow and the .NET TPL provide a set of built-in combinators, such as \nTask.Run , Async.StartWithContinuation , Task.WhenAll , and Task.WhenAny . \nThese can be easily extended for implementing useful combinators to compose and build more sophisticated task-based patterns. For example, both the \nTask.WhenAll  \nand the F# Async.Parallel  operators are used to asynchronously wait on multiple \nasynchronous operations; the underlying results of those operations are grouped to \n \n302 chapter  10 Functional combinators for fluent concurrent programming\ncontinue. This continuation is the key that provides opportunities for composing the flow of a program in more complex structures, such as implementing the Fork/Join and Divide and Conquer patterns. \nLet’s start with a simple case in C# to understand the benefits of combinators. Imag-\nine you must run three asynchronous operations and calculate the sum of their output, awaiting each in turn. Note that each operation takes one second to compute:\nasync Task<int> A() { await Task.Delay(1000); return 1; }async Task<int> B() { await Task.Delay(1000); return 3; }async Task<int> C() { await Task.Delay(1000); return 5; }int a = await A();int b = await B();int c = await C(); int result = a + b + c;\nThe result (9) is computed in three seconds, one second for each operation. But what if you want to run those three methods in parallel? To run more than one background task, there are methods available to help you coordinate them. The simplest solution to run multiple tasks concurrently is to start them consecutively and collect references to them. The TPL \nTask.WhenAll  operator accepts a params  array of tasks, and returns \na task that is signaled when all the others are complete. You can eliminate the interme-diate variables from that last example to make the code less verbose:\nvar results = (await Task.WhenAll(A(), B(), C())).Sum();\nThe results come back in an array, and then the Sum()  LINQ operator is applied. With \nthis change, the result is computed in only one second. Now the task can completely represent an asynchronous operation and provide synchronous and asynchronous capabilities for joining with the operation, retrieving its results, and so on. This lets you build useful libraries of combinators that compose tasks to build larger patterns.\n10.5.2 Exploiting the Task.WhenAny combinator for redundancy and interleaving\nA benefit of using tasks is that they enable powerful composition. Once you have a sin-\ngle type capable of representing any arbitrary asynchronous operation, you can write combinators over the type that allow you to combine/compose asynchronous opera-tions in myriad ways. \nFor example, the TPL \nTask.WhenAny  operator allows you to develop parallel pro-\ngrams where one task of multiple asynchronous operations must be completed before the main thread can continue processing. This behavior of asynchronously waiting for the first operation to complete over a given set of tasks, before notifying the main thread for further processing, facilitates the design of sophisticated combinators. Redundancy, interleaving, and throttling are examples of properties that are derived from these combinators.\n \n 303 Functional combinators in a nutshell \nREDUNDANCY  Executes an asynchronous operation multiple times and selects \nthe one that completes first. \nINTERLEAVING  Launches multiple operations but processes them in the order \nthey complete. This was discussed in section 8.5.7.\nConsider the case where you want to buy an airplane ticket as soon as possible. You have a few airline web services to contact, but depending on web traffic, each service can have a different response time. In this case, you can use the \nTask.WhenAny  opera-\ntor to contact multiple web services to produce a single result, selected from the one that completes the fastest.\nListing 10.16  Redundancy with Task.WhenAny\nvar cts = new CancellationTokenSource(); Func<string, string, string, CancellationToken, Task<string>> \n➥ GetBestFlightAsync = async (from, to, carrier, token) => {\n        string url = $""flight provider{carrier}"";        using(var client = new HttpClient()) {        HttpResponseMessage response = await client.GetAsync(url, token);        return await response.Content.ReadAsStringAsync();    }};           var recommendationFlights = new List<Task<string>>(){    GetBestFlightAsync(""WAS"", ""SF"", ""United"", cts.Token),    GetBestFlightAsync(""WAS"", ""SF"", ""Delta"", cts.Token),    GetBestFlightAsync(""WAS"", ""SF"", ""AirFrance"", cts.Token),};     Task<string> recommendationFlight = await Task.\nWhenAny(recommendationFlights);  \nwhile (recommendationFlights.Count > 0){       try    {        var recommendedFlight = await recommendationFlight;               cts.Cancel();                  BuyFlightTicket(""WAS"", ""SF"", recommendedFlight);        break;    }    catch (WebException)            {        recommendationFlights.Remove(recommendationFlight);            }}Uses CancellationToken to cancel the operations \nstill running after the first one completesThe conceptual asynchronous \nfunction that fetches the price \nof a flight from a given carrier\nLists asynchronous operations to execute in parallel\nWaits for the first operation to complete with the Task.WhenAny operator\nRetrieves the result in a try-catch block \nto accommodate potential exceptions. \nEven if a first task completes \nsuccessfully, subsequent tasks may fail.\nIf the operation is successful, the other computations still running are canceled.",8426
101-10.5.4 Mathematical pattern review what youve seen so far.pdf,101-10.5.4 Mathematical pattern review what youve seen so far,"304 chapter  10 Functional combinators for fluent concurrent programming\nIn the code, Task.WhenAny  returns the task that completed first. It’s important to \nknow if the operation completes successfully, because if there’s an error you want to discharge the result and wait for the next computation to complete. The code must handle exceptions using a \ntry-catch , where the computation that failed is removed \nfrom the list of asynchronous recommended operations. When a first task completes successfully, you want to be sure to cancel the others still running. \n10.5.3 Exploiting the Task.WhenAll combinator  for asynchronous for-each\nThe Task.WhenAll  operator waits asynchronously on multiple asynchronous compu-\ntations that are represented as tasks. Consider that you want to send an email message to all your contacts. To speed up the process, you want to send the email to all recipi-ents in parallel without waiting for each separate message to complete before sending the next. In such a scenario, it would be convenient to process the list of emails in a \nfor-each  loop. How would you maintain the asynchronous semantic of the operation, \nwhile sending the emails in parallel? The solution is to implement a ForEachAsync  \noperator based on the Task.WhenAll  method. \nListing 10.17  Asynchronous for-each  loop with Task.WhenAll  \nstatic Task ForEachAsync<T>(this IEnumerable<T> source, \n➥ int maxDegreeOfParallelism, Func<T, Task> body)\n{    return Task.WhenAll(        from partition in Partitioner.Create(source).\nGetPartitions(maxDegreeOfParallelism)\n        select Task.Run(async () =>              {                     using (partition)                     while (partition.MoveNext())                     await body(partition.Current);        }));}\nFor each partition of the enumerable, the operator ForEachAsync  runs a function that \nreturns a Task  to represent the completion of processing that group of elements. Once \nthe work starts asynchronously, you can achieve concurrency and parallelism, invoking the body for each element and waiting on them all at the end, rather than waiting for each in turn. \nThe \nPartitioner  created limits the number of operations that can run in parallel \nto avoid making more tasks than necessary. This maximum degree of parallelism value is managed by partitioning the input data set into \nmaxDegreeOfParallelism  number \nof chunks and scheduling a separate task to begin execution for each partition. The \nForEachAsync  batches work to create fewer tasks than total work items. This can pro-\nvide significantly better overall performance, especially if the loop body has a small amount of work per item.\n \n 305 Functional combinators in a nutshell \nNOTE  This last example is similar in nature to Parallel.ForEach , the primary \ndifference being that Parallel.ForEach  is a synchronous method and uses \nsynchronous delegates.\nNow you can use the ForEachAsync  operator to send multiple emails asynchronously. \nListing 10.18  Using the asynchronous for-each  loop \nasync Task SendEmailsAsync(List<string> emails){    SmtpClient client = new SmtpClient();    Func<string, Task> sendEmailAsync = async emailTo =>    {        MailMessage message = new MailMessage(""me@me.com"", emailTo);        await client.SendMailAsync(message);    };    await emails.ForEachAsync(Environment.ProcessorCount, sendEmailAsync);}\nThese are a few simple examples that show how to use the built-in TPL combinators \nTask.WhenAll  and Task.WhenAny . In section 10.6, you’ll focus on constructing custom \ncombinators and composing existing ones in which both F# and C# principles apply. You’ll see that there’s an infinite number of combinators. We’ll look at several of the most common ones that are used to implement an asynchronous logical flow in a pro-gram: \nifAsync , AND (async), and OR (async).\nBefore jumping into building asynchronous combinators, let’s review the functional \npatterns that have been discussed so far. This refresher will lead to a new functional pat-tern, which is used to compose heterogeneous concurrent functions. Don’t worry if you aren’t familiar with this term; you will be shortly.\n10.5.4 Mathematical pattern review: what you’ve seen so far\nIn the previous chapters, I introduced the concepts of monoids, monads, and func-tors, which come from a branch of mathematics called category theory. Additionally, I discussed their important relationship to functional programming and functional concurrency. \nCategory theory lexicon\nCategory theory is a branch of mathematics that defines any collection of objects that can relate to each other via morphisms in sensible ways, such as composition and asso -\nciativity. Morphisms  is a buzzword that defines something that can mutate; think of \napplying a \nmap (or select ) function from one mathematical structure to another. Essen -\ntially, category theory consists of objects and arrows that are connected to each other, providing the basis of the composition. Category theory is a powerful idea generated from a need to organize mathematical concepts based on shared structure. Many useful concepts fall under the category theory umbrella, but you don’t need to have a mathe -\nmatical background to understand them and use their powerful properties, which for the majority are all about creating opportunities for composition.\n \n \n306 chapter  10 Functional combinators for fluent concurrent programming\nIn programming, these mathematical patterns are adopted to control the execution of side effects and to maintain functional purity. These patterns are interesting because of their properties of abstraction and compositionality. Abstraction favors composabil-ity, and together they’re the pillars of functional and concurrent programming. The following sections rehash the definition of these mathematical concepts.\nmonoids  for data parallelism  \nA monoid, as explained earlier, is a binary associative operation with an identity; it pro-\nvides a way to mash values of the same type together. The associative property allows you to run a computation in parallel effortlessly by providing the ability to divide a problem into chunks so it can be computed independently. Then, when each block of computation completes, the result is recomposed. A variety of interesting parallel operations turn out to be both associative and commutative, expressed using monoids: \nMap-Reduce  and Aggregation  in various forms such as sum, variance , average ,  \nconcatenation , and more. The .NET PLINQ, for example, uses monoidal operations \nthat are both associative and commutative to parallelize work correctly.\nThe following code example, based on content from chapter 4, shows how to use \nPLINQ for parallelizing the sum of the power of an array segment. The data set is par -\ntitioned in subarrays that are accumulated separately on their own threads using the accumulator initialized to the seed. Ultimately, all accumulators will be combined using the final reduce function (the \nAsParallel  function is in bold): \nvar random = new Random();var size = 1024 * Environment.ProcessorCount;int[] array = Enumerable.Range(0, size).Select(_ => \n➥ random.Next(0, size)).ToArray();\nlong parallelSumOfSquares = array.AsParallel()     .Aggregate(     seed: 0,       updateAccumulatorFunc: (partition, value) => \n➥ partition + (int)Math.Pow(value, 2),\n     combineAccumulatorsFunc: (partitions, partition) => \n➥ partitions + partition,\n     resultSelector: result => result);\nDespite the unpredictable order of the computation compared to the sequential ver -\nsion of the code, the result is deterministic because of the associativity and commuta-tivity properties of the \n+ operator.\nfunctors  to map elevated  types  \nThe functor is a pattern of mapping over elevated structures, which is archived and provides support for a two-parameter function called \nMap (also known as fmap ). The \ntype signature of the Map function takes as a first argument the function (T -> R),  \nwhich in C# is translated into Func<T, R> . When given an input type T, it applies a \ntransformation and returns a type R. A functor elevates functions with only one input.\nThe LINQ/PLINQ Select  operator can be considered a functor for the IEnumerable  \nelevated type. Mainly, functors are used in C# to implement LINQ-style fluent APIs that Seed for each partition\n \n 307 Functional combinators in a nutshell \nare used for types other than collections. In chapter 7, you implemented a functor for the \nTask  elevated type (the Map function is in bold): \nstatic Task<R> Map<T, R>(this Task<T> input, Func<T, R> map) =>                                    input.ContinueWith(t => map(t.Result));\nThe function Map takes a function map ( T -> R ) and a functor (wrapped context) \nTask<T>  and returns a new functor Task<R>  containing the result of applying the func-\ntion to the value and closing it once more. \nThe following code, from chapter 8, downloads an icon image from a given website \nand converts it into a bitmap. The operator Map is applied to chain the asynchronous \ncomputations (the code to note is in bold).\nBitmap icon = await new HttpClient()                       .GetAsync($""http://{domain}/favicon.ico"")                       .Bind(async content => await                             content.Content.ReadAsByteArrayAsync())                       .Map(bytes =>                               Bitmap.FromStream(new MemoryStream(bytes)));\nThis function has a signature (T -> R) -> Task<T> -> Task<R> , which means that it \ntakes a map function T -> R  as the first input that goes from a value type T to a value \ntype R, and then upgrades the type Task<T>  as a second input and returns the Task<R> .\nA functor is nothing more than a data structure that you can use to map functions \nwith the purpose of lifting values into a wrapper (elevated type), modifying them, and then putting them back into a wrapper. The reason for having \nfmap  return the same \nelevated type is to continue chaining operations. Essentially, functors create a context or an abstraction that allows you to securely manipulate and apply operations to values without changing any original values.\nmonads  to compose  without  side effects  \nMonads are a powerful compositional tool used in functional programming to avoid dangerous and unwanted behaviors (side effects). They allow you to take a value and apply a series of transformations in an independent manner encapsulating side effects. The type signature of monadic function calls out potential side effects, providing a repre-sentation of both the result of the computation and the actual side effects that occurred as a result. A monadic computation is represented by generic type \nM<'a>  where the type \nparameter specifies the type of value (or values) produced as the result of monadic com-putation (internally, the type may be a \nTask  or List , for example). When writing code \nusing monadic computations, you don’t use the underlying type directly. Instead you use two operations that every monadic computation must provide: \nBind  and Return . \nThese operations define the behavior of the monad and have the following type sig-\nnatures (for certain monads of type M<'a>  that could be replaced with Task<'a> ): \nBind: ('a -> M<'b>) -> M<'a> -> M<'b>Return: 'a -> M<'a>\nThe Bind  operator takes an instance of an elevated type, extracts the underlying value \nfrom it, and runs the function over that value, returning a new elevated value: \nTask<R> Bind<R, T>(this Task<T> task, Func<T, Task<R>> continuation)",11652
102-10.6 The ultimate parallel composition applicative functor.pdf,102-10.6 The ultimate parallel composition applicative functor,"308 chapter  10 Functional combinators for fluent concurrent programming\nYou can see in this implementation that the SelectMany  operator is built into the \nLINQ/PLINQ library.\nReturn  is an operator that lifts (wraps) any type into a different elevated context \n(monad type, like Task ), usually converting a non-monadic value into a monadic value. \nFor example, Task.FromResult  produces a Task<T>  from any given type T (in bold):\nTask<T> Return<T>(T value) => Task.FromResult(value);\nThese monadic operators are essential to LINQ/PLINQ and generate the opportu-nity for many other operators. For example, the previous code that downloads and converts an icon from a given website into a bitmap format can be rewritten using the monadic operators (in bold) in the following manner:\nBitmap icon = await (from content in new HttpClient().GetAsync($""http://\n{domain}/favicon.ico"")\n                     from bytes in content.Content.ReadAsByteArrayAsync())                     select Bitmap.FromStream(new MemoryStream(bytes));\nThe monad pattern is an amazingly versatile pattern for doing function composition with amplifying types while maintaining the ability to apply functions to instances of the underlying types. Monads also provide techniques for removing repetitive and awk-ward code and can allow you to significantly simplify many programming problems.\nWhat is the importance of laws? \nAs you’ve seen, each of the mathematical patterns mentioned must satisfy specific laws to expose their property, but why? The reason is, laws help you to reason about your pro -\ngram, providing information for the expected behavior of the type in context. Specifically, a concurrent program must be deterministic; therefore, a deterministic and predictable way to reason about the code helps to prove its correctness. If an operation is applied to combine two monoids, then you can assume, due to the monoid laws, that the compu-tation is associative, and the result type is also a monoid. To write concurrent combina-tors, it’s important to trust the laws that are derived from the abstract interface, such as monads and functors.\n10.6 The ultimate parallel composition applicative functor\nAt this point, I’ve discussed how a functor ( fmap ) can be used to upgrade functions \nwith one argument to work with elevated types. You’ve also learned how the monadic \nBind  and Return  operators are used to compose elevated types in a controlled and \nfluent manner. But there’s more! Let’s assume that you have a function from the nor-mal world: for example, a method that processes an image to create a \nThumbnail  over a \ngiven Bitmap  object. How would you apply such functionality to values from the elevated \nworld Task<Bitmap> ? \nNOTE  Normal world, in this case, refers to a function that performs over a nor -\nmal primitive, such as a bitmap. In contrast, a function from the elevated world would operate against an elevated type: for example, a bitmap elevated to \nTask<Bitmap> . \n \n 309 The ultimate parallel composition applicative functor\nHere’s the function ToThumbnail  to process a given image (the code to note is in bold):\nImage ToThumbnail (Image bitmap, int maxPixels){    var scaling = (bitmap.Width > bitmap.Height)                   ? maxPixels / Convert.ToDouble(bitmap.Width)                   : maxPixels / Convert.ToDouble(bitmap.Height);    var width = Convert.ToInt32(Convert.ToDouble(bitmap.Width) * scaling);    var heiht = Convert.ToInt32(Convert.ToDouble(bitmap.Height) * scaling);    return new Bitmap(bitmap.GetThumbnailImage(width, height, null, \n➥ IntPtr.Zero));\n}\nAlthough you can obtain a substantial number of different compositional shapes using core functions such as \nmap and bind , there’s the limitation that these functions take \nonly a single argument as an input. How can you integrate multiple-argument func-tions in your workflows, given that \nmap and bind  both take as input a unary function? \nThe solution is applicative functors.\nLet’s start with a problem to understand the reasons why you should apply the Appli-\ncative Functor pattern (technique). The functor has the map operator to upgrade func-\ntions with one and only one argument.\nIt’s common that functions that map to elevated types usually take more than one \nargument, such as the previous ToThumbnail  method that takes an image as the first \nargument and the maximum size in pixels for the image transformation as the second argument. The problem with such functions is that they aren’t easy to elevate in other contexts. If you load an image, for simplicity using the Azure Blob storage function \nDownloadImageAsync  as earlier, and later you want to apply the ToThumbnail  func-\ntion transformation, then the functor map cannot be used because the type signature doesn’t match. \nToThumbnail  (in bold in the following listing) takes two arguments, \nwhile the map function takes a single argument function as input.\nListing 10.19  Compositional limitation of the Task  functor map \nTask<R> map<T, R>(this Task<T> task, Func<T, R> map) =>                                     task.ContinueWith(t => map(t.Result));static async Task<Image> DownloadImageAsync(string blobReference){     var container = await Helpers.GetCloudBlobContainerAsync().\nConfigureAwait(false);\n     CloudBlockBlob blockBlob = container.\nGetBlockBlobReference(blobReference);\n     using (var memStream = new MemoryStream())     {          await blockBlob.DownloadToStreamAsync(memStream).\nConfigureAwait(false);\n          return Bitmap.FromStream(memStream);     } } static async Bitmap CreateThumbnail(string blobReference, int maxPixels)\n \n310 chapter  10 Functional combinators for fluent concurrent programming\n {     Image thumbnail =           await DownloadImageAsync(""Bugghina001.jpg"")                 .map(ToThumbnail);       return thumbnail; }\nThe problem with this code is that it doesn’t compile when you’re trying to apply \nToThumbnail  to the Task  map extension method map(ToThumbnail).  The compiler \nthrows an exception due to the signature mismatch.\nHow can you apply a function to several contexts at once? How can a function that takes \nmore than one argument be upgraded? This is where applicative functors come into play to apply a multi-parameter function over an elevated type. The following listing exploits applicative functors to compose the \nToThumbnail  and DownloadImageAsync  functions, \nmatching the type signature and maintaining the asynchronous semantic (in bold).\nListing 10.20  Better composition of the asynchronous operation\nStatic Func<T1, Func<T2, TR>> Curry<T1, T2, TR>(this Func<T1, T2, TR> func) => \n➥ p1 => p2 => func(p1, p2);\nstatic async Task<Image> CreateThumbnail(string blobReference, int maxPixels){   Func<Image, Func<int, Image>> ToThumbnailCurried = \n➥ Curry<Image, int, Image>(ToThumbnail); \n   Image thumbnail = await TaskEx.Pure(ToThumbnailCurried)                                .Apply(DownloadImageAsync(blobReference))                              .Apply(TaskEx.Pure(maxPixels));             return thumbnail;}\nLet’s explore this listing for clarity. The Curry  function is part of a helper static class, \nwhich is used to facilitate FP in C#. In this case, the curried version of the method \nToThumbnail  is a function that takes an image as input, and returns a function that \ntakes an integer ( int) as input for the maximum size in pixels allowed, and as output \nan Image  type: Func<Image, Func<int, Image>> ToThumbnailCurried . Then, this \nunary function is wrapped in the container Task  type, and overloads so greater arities \ncan be defined by currying that function. \nIn practice, the function that takes more than one argument, in this case ToThumb-\nnail , is curried and lifted into the Task  type using the Task Pure  extension method. \nThen, the resulting Task<Func<Image, Func<int, Image>>> is passed over the appli-\ncative functor Apply , which injects its output, Task<Image> , into the next function \napplied over DownloadImageAsync. Compilation error\nCurries the function\nLifts the function ToThumbnailCurried to the Task elevated typeUses an applicative function to chain the \ncomputations without exiting the Task context\n \n 311 The ultimate parallel composition applicative functor\nUltimately, the last applicative functor operator Apply  handles the transient param-\neter maxPixels  elevated using the Pure  extension method. From the perspective of \nthe functor map operator, the curried function ToThumbnailCurried  is partially applied \nand is exercised against an image  argument and then wrapped into the task. Therefore, \nconceptually, the signature is\nTask<ToThumbnailCurried(Image)>  \nThe function ToThumbnailCurried  takes an image  as input and then returns the par -\ntially applied function in the form of a Func<int, Image>  delegate, whose signature \ndefinition correctly matches the input of the applicative functor: Task<Func<int, \nImage>>.\nThe Apply  function can be viewed as a partial application for elevated functions, \nwhose next value is provided for every call in the form of an elevated (boxed) value. In this way, you can turn every argument of a function into a boxed value.\nCurrying and partial application\nCurrying is the technique of transforming a function with multiple arguments into a series \nof functions with only one argument, which always returns a value. The F# programming language performs currying automatically. In C# you need to enable it manually or rather by using a defined helper function, as in the example. This is the reason why function sig -\nnatures in FP are represented with multiple -> symbols, which is basically the symbol for a function. Look at a signature such as: \nstring -> int -> Image.\nThe function has two arguments, string and int, and it returns Image . But the correct \nway to read the function signature is a function that has only one argument: string . It \nwill return a new function with signature int -> Image .\nIn C#, the lack of support for the currying technique and partial application may make the applicative functor seem inconvenient. But after practice you’ll break through the initial barrier and see the real power and flexibility of the applicative functor technique.Partial application  refers to a function of many arguments that doesn’t have to be \napplied to all its inputs at once. You can imagine that applying it to the first argument doesn’t yield a value but rather a function on n – 1 arguments. In this spirit, you could \nbind a multi-parameter function to a single \nTask  and get an asynchronous operation of \nn – 1 arguments. Then you are left with the problem of applying a task of a function to a task of an argument, and that’s exactly what the applicative pattern solves. More details about currying can be found in appendix A.\n \nThe Applicative Functor pattern aims to lift and apply a function over an elevated context, and then apply a computation (transformation) to a specific elevated type. Because both the value and the function are applied in the same elevated context, they can be smashed together. \n \n312 chapter  10 Functional combinators for fluent concurrent programming\nLet’s analyze the functions Pure  and Apply . An applicative functor is a pattern \nimplemented by two operations, defined here, where AF represents any elevated type \n(in bold):\nPure : T -> AF<R> Apply : AF<T -> R> -> AF<T> -> F<R>\nIntuitively, the Pure  operator lifts a value into an elevated domain, and it’s equivalent \nto the Return  monadic operator. The name Pure  is a convention for an applicative \nfunctor definition. But in the case of applicative functions, this operator elevates a function. The \nApply  operator is a two-parameter function, both part of the same ele-\nvated domain. \nFrom the code example in the section “Functors to map elevated types,” you can see \nthat an applicative functor is any container (elevated type) that offers a way to trans-form a normal function into one that operates on contained values. \nNOTE  An applicative functor is a construct to provide the midpoint between \nfunctor’s map and monad’s bind . Applicative functors can work with elevated \nfunctions, because the values are wrapped in a context, the same as functors, but in this case the wrapped value is a function. This is useful if you want to apply a function that’s inside a functor to a value inside a functor. \nApplicative functors are useful when sequencing a set of actions in parallel without the need for any intermediate results. In fact, if the tasks are independent, then their exe-cution can be composed and parallelized using an applicative. An example is running a set of concurrent actions that read and transform parts of a data structure in order, then combine their results, shown in figure 10.8. \n( + 40)\n22 + 40Unwrap both types and apply\nthe function to the value. Apply\n2Rewrap\nthe value.Value in\na contextFunction wrappe d\nin an elevated type\nFigure 10.8  The Apply  operator implements the function wrapped inside an elevated type to a value \nin the context. The process triggers the unwrapping of both values; then, because the first value is a function, it’s applied automatically to the second value. Finally, the output is wrapped back inside the context of the elevated type.\nIn the context of the Task  elevated type, it takes a value Task<T>  and a wrapped \nfunction Task<(T -> R)>  (translated in C# as Task<Func<T, R>> ) and then returns \n \n 313 The ultimate parallel composition applicative functor\na new value Task<R>  generated by applying the underlying function to the value of \nTask<T> :\nstatic Task<R> Apply<T, R>(this Task<Func<T, R>> liftedFn, Task<T> task) {     var tcs = new TaskCompletionSource<R>();     liftedFn.ContinueWith(innerLiftTask =>                task.ContinueWith(innerTask =>                    tcs.SetResult(innerLiftTask.Result(innerTask.Result))            ));            return tcs.Task;}\nHere’s a variant of the Apply  operator defined for async Task  in the  TAP world, which \ncan be implemented rather than in terms of async/await :\nstatic async Task<R> Apply<T, R> (this Task<Func<T, R>> f, Task<T> arg)                        => (await f. ConfigureAwait(false))                           (await arg.ConfigureAwait(false));\nBoth Apply  functions have the same behavior despite their different implementations. \nThe first input value of Apply is a function wrapped into a Task : Task<Func<T, R>> . \nThis signature could look strange initially, but remember that in FP, functions are treated as values and can be passed around in the same way as strings or integers.\nNow, extending the \nApply  operator to a signature that accepts more inputs becomes \neffortless. This function is an example: \nstatic Task<Func<b, c>> Apply<a, b, c>(this Task<Func<a, b, c>> liftedFn, \n➥ Task<a> input) =>  \n                                                         Apply(liftedFn.map(Curry), input);\nNotice that this implementation is clever, because it applies the Curry  function to \nTask<Func<a, b, c>>  liftedFn  using the functor map, and then applies it over the \nelevated input value using the Apply  operator with smaller arity as previously defined. \nWith this technique, you continue to expand the Apply  operator to take as an input a \nfunction lifted with any number of parameters. \nNOTE  The Apply  operator has the argument order inverted compared to the \nstandard signature because it’s implemented as a static method to ease its use. \nIt turns out that functor and applicative functor work well together to facilitate compo-sition, including the composition of expressions running in parallel. When passing a function with more than one argument to the functor \nmap, the result type matches the \ninput of the Apply  function. \nYou can use an alternative way to implement an applicative functor in terms of using \nthe monadic operators bind  and return . But this approach prevents the code from \nrunning in parallel, because the execution of an operation would depend on the out-come of the previous one.\n \n314 chapter  10 Functional combinators for fluent concurrent programming\nWith the applicative functor in place, it’s effortless to compose a series of computa-\ntions with no limit on the number of arguments each expression takes. Let’s imagine that you need to blend two images to create a third new image, which is the overlap of the given images into a frame having a specific size. This listing shows you how (the \nApply  function is in bold).\nListing 10.21  Parallelizing the chain of computation with applicative functors\nstatic Image BlendImages(Image imageOne, Image imageTwo, Size size){     var bitmap = new Bitmap(size.Width, size.Height);     using (var graphic = Graphics.FromImage(bitmap)) {        graphic.InterpolationMode = InterpolationMode.HighQualityBicubic;        graphic.DrawImage(imageOne,              new Rectangle(0, 0, size.Width, size.Height),              new Rectangle(0, 0, imageOne.Width, imageTwo.Height),              GraphicsUnit.Pixel);        graphic.DrawImage(imageTwo,              new Rectangle(0, 0, size.Width, size.Height),              new Rectangle(0, 0, imageTwo.Width, imageTwo.Height),              GraphicsUnit.Pixel);        graphic.Save();      }      return bitmap;}async Task<Image> BlendImagesFromBlobStorageAsync(string blobReferenceOne, \n➥ string blobReferenceTwo, Size size)\n{       Func<Image, Func<Image, Func<Size, Image>>> BlendImagesCurried =                                Curry<Image, Image, Size, Image>(BlendImages);        Task<Image> imageBlended =              TaskEx.Pure(BlendImagesCurried)                    .Apply(DownloadImageAsync(blobReferenceOne))                    .Apply(DownloadImageAsync(blobReferenceTwo))                    .Apply(TaskEx.Pure(size));            return await imageBlended;}\nWhen you call Apply  the first time, with the DownloadImageAsync(blobReferenceOne)  \ntask, it immediately returns a new Task  without waiting for the DownloadImageAsync  \ntask to complete; consequently, the program immediately goes on to create the second \nDownloadImageAsync(blobReferenceTwo) . As a result, both tasks run in parallel.\nThe code assumes that all the functions have the same input and output; but this is \nnot a constraint. As long as the output type of an expression matches the input of the next expression, then the computation is still enforced and valid. Notice that in listing 10.21, each call starts independently, so they run in parallel and the total execution time for \nBlendImagesFromBlobStorageAsync  to complete is determined by the longest \ntime required of the Apply  calls to complete.",18809
103-10.6.1 Extending the F async workflow with applicativefunctoroperators.pdf,103-10.6.1 Extending the F async workflow with applicativefunctoroperators,"315 The ultimate parallel composition applicative functor\nApply vs. bind\nThe differential behavior between the bind  and Apply  operators is denoted by their \nfunction signature. In the context of an async workflow, for example, the first Async  type \nin the bind  operator is executed and awaiting completion to start the second async \noperation. The bind  operator should be used when the execution of an async operation \ndepends on the returned value of another async operation. The \nApply  operator has provided both async operations as an argument in the signa-\nture. It should be used when the async operations can be started independently. These concepts are valid for other elevated types such as the \nTask  type.\n \nThis example enforces the compositional aspect of concurrent functions. Alternatively, you could implement custom methods that blend the images directly, but in the larger scheme this approach enables the flexibility to combine more sophisticated behaviors.\n10.6.1 Extending the F# async workflow with applicative functor operators\nContinuing your introduction to applicative functors, in this section you’ll practice the same task concepts to extend the F# asynchronous workflow. Note that F# supports the TPL because it is part of the .NET ecosystem, and the applicative functors are based on the \nTask  type applied.\nThe following listing implements the two applicative functor operators pure  and \napply , which are purposely defined inside the Async  module to extend this type. Note \nthat because pure  is a future reserved keyword in F#, the compiler will give a warning.\nListing 10.22  F# async applicative functor\nmodule Async =      let pure value = async.Return value                   let apply funAsync opAsync = async {        let! funAsyncChild = Async.StartChild funAsync            let! opAsyncChild = Async.StartChild opAsync        let! funAsyncRes = funAsyncChild        let! opAsyncRes = opAsyncChild             return funAsyncRes opAsyncRes      }\nThe apply  function executes the two parameters, funAsync  and opAsync , in parallel \nusing the Fork/Join pattern, and then it returns the result of applying the output of the first function against the other.\nNotice that the implementation of the \napply  operator runs in parallel because each \nasynchronous function starts the evaluation using the Async.StartChild  operator. Lifts value to an Async\nStarts the two asyncs in parallel\nWaits for the results\n \n316 chapter  10 Functional combinators for fluent concurrent programming\nThe Async.StartChild operator\nThe Async.StartChild  operator takes a computation that starts within an asynchro -\nnous workflow, and returns a token (of type Async<'T> ) that can be used to wait until the \noperation completes. Its signature is as follows:\nAsync.StartChild : Async<'T> * ?int -> Async<Async<'T>>\nThis mechanism allows multiple asynchronous computations to be executed simultane -\nously. If a parent computation requests the result and the child computation isn’t fin-ished, then the parent computation is suspended until the child completes.\n \nLet’s see the capabilities that these functions provide in place. The same applicative functor concepts introduced in C# apply here; but the compositional semantic style provided in F# is nicer. Using the F# pipe (\n|>) operator to pass the intermediate result \nof a function on to the next one produces a more readable code.\nThe following listing implements the same chain of functions using the applicative \nfunctor in F# for blending asynchronously two images, as shown in C# in listing 10.21. In this case, the function \nblendImagesFromBlobStorage  in F# returns an Async  type \nrather than a Task  (in bold).\nListing 10.23  Parallel chain of operations with an F# async applicative functor \n    let blendImages (imageOne:Image) (imageTwo:Image) (size:Size) : Image =         let bitmap = new Bitmap(size.Width, size.Height)        use graphic = Graphics.FromImage(bitmap)        graphic.InterpolationMode <- InterpolationMode.HighQualityBicubic        graphic.DrawImage(imageOne,                            new Rectangle(0, 0, size.Width, size.Height),                            new Rectangle(0, 0, imageOne.Width, imageTwo.\nHeight),\n                            GraphicsUnit.Pixel)        graphic.DrawImage(imageTwo,                        new Rectangle(0, 0, size.Width, size.Height),                            new Rectangle(0, 0, imageTwo.Width, imageTwo.\nHeight),\n                            GraphicsUnit.Pixel)        graphic.Save() |> ignore        bitmap :> Image    let blendImagesFromBlobStorage (blobReferenceOne:string)     \n➥ (blobReferenceTwo:string) (size:Size) =        \n        Async.apply(            Async.apply(                Async.apply(                     Async.``pure`` blendImages)                     (downloadOptionImage(blobReferenceOne)))                     (downloadOptionImage(blobReferenceTwo)))                     (Async.``pure`` size)",5014
104-10.6.4 Composing and executing heterogeneous parallel computations.pdf,104-10.6.4 Composing and executing heterogeneous parallel computations,"317 The ultimate parallel composition applicative functor\nThe function blendImages  is lifted to the Task  world (elevated type) using the Async \n.pure  function. The resulting function, which has the signature Async<Image -> \nImage -> Size -> Image> , is applied over the output of the functions download-\nOptionImage(blobReferenceOne)  and downloadOptionImage(blobReferenceTwo) . \nThe lifted value size  runs in parallel.\nAs mentioned earlier, functions in F# are curried by default; the extra boilerplate \nrequired in C# isn’t necessary. Even if F# doesn’t support applicative functors as a built-in feature, it’s easy to implement the \napply  operator and exercise its composi-\ntional benefits. But this code isn’t particularly elegant, because the apply  function oper -\nators are nested rather than chained. A better way is to create a custom infix operator.\n10.6.2 Applicative functor semantics in F# with infix operators \nA more declarative and convenient approach in F# to write functional composition is \nto use custom infix operators. Unfortunately, this feature isn’t supported in C#. The support for custom infix operators means that you can define operators to achieve the desired level of precedence when operating over the arguments passed. An infix operator in F# is an operator that’s expressed using a mathematical notation called infix notation. For example, the multiply operator takes two numbers that are then mul-tiplied by each other. In this case, using infix notation, the multiplication operator is written between the two numbers it operates on. Operators are basically two-argument functions, but in this case, instead of writing a function \nmultiply  x y, an infix operator \nis positioned between the two arguments: x Multiply y . \nYou’re already familiar with a few infix operators in F#: the |> pipe operator and >> \ncomposition operators. But according to section 3.7 of the F# language specification, you can define your own operators. Here, an infix operator (in bold) is defined for both asynchronous functions \napply  and map:  \nlet (<*>) = Async.applylet (<!>) = Async.map\nNOTE   The <!> operator is the infix version of Async.map , while the <*> opera-\ntor is the infix version of Async.apply . These infix operators are generally used \nin other programming languages such as Haskell, so they’ve become the stan-dard. The \n<!> operator is defined as <$> in other programming languages; but \nin F# the <$> operator is reserved for future use, so <!> is used.\nUsing these operators, you can rewrite the previous code in a more concise manner:\nlet blendImagesFromBlobStorage (blobReferenceOne:string) \n➥ (blobReferenceTwo:string) (size:Size) =        \n     blendImages     <!> downloadOptionImage(blobReferenceOne)     <*> downloadOptionImage(blobReferenceOne)     <*> Async.``pure`` size\nIn general, I recommend that you not overuse or abuse the utilization of infix opera-tors, but instead find the right balance. You can see how, in the case of functors and applicative functors, the infix operator is a welcome feature.  \n \n318 chapter  10 Functional combinators for fluent concurrent programming\n10.6.3 Exploiting heterogeneous parallel computation with applicative functors\nApplicative functors lead to a powerful technique that allows you to write heteroge-neous parallel computations. Heterogeneous means an object is composed of a series of parts of different kinds (versus homogenous, of a similar kind). In the context of parallel programming, it means executing multiple operations together, even if the result \ntype  \nbetween each operation is different. \nFor example, with the current implementation, both the F# Async.Parallel  and \nthe TPL Task.WhenAll  take as an argument a sequence of asynchronous computations \nhaving the same result type. This technique is based on the combination of applicative functors and the concept of lifting, which aims to elevate any type into a different con-text. This idea is applicable to values and functions; in this specific case, the target is functions with an arbitrary cardinality of different argument types. To enable this feature to run heterogeneous parallel computation, the applicative functor \napply  operator is \ncombined with the technique of lifting a function. This combination is then used to con-struct a series of helpful functions generally called \nLift2 , Lift3 , and so forth. The Lift  \nand Lift1  operators aren’t defined because they’re functor map functions.\nThe following listing shows the implementation of the Lift2  and Lift3  functions \nin C#, which represents a transparent solution to performing parallel Async  returning \nheterogeneous types. Those functions will be used next.\nListing 10.24  C# asynchronous lift functions\nstatic Task<R> Lift2<T1, T2, R>(Func<T1, T2, R > selector, Task<T1> item1, \n➥ Task<T2> item2) \n{                                             Func<T1, Func<T2, R>> curry = x => y => selector(x, y);            var lifted1 = Pure(curry);             var lifted2 = Apply(lifted1, item1);            return Apply(lifted2, item2);        }static Task<R> Lift3<T1, T2, T3, R>(Func<T1, T2, T3, R> selector, \n➥ Task<T1> item1, Task<T2> item2, Task<T3> item3)  \n{            Func<T1, Func<T2, Func<T3, R>>> curry = x => y => z =>                                                      selector(x, y, z);         var lifted1 = Pure(curry);            var lifted2 = Apply(lifted1, item1);            var lifted3 = Apply(lifted2, item2);            return Apply(lifted3, item3);        }The lift functions apply a given function to the outputs of a set of Tasks. \nCurries the function to partially apply \nElevates the partially applied function The Apply operator exercises the lifting. The lift functions apply a \ngiven function to the \noutputs of a set of Tasks. \nElevates the partially applied function \n \n 319 The ultimate parallel composition applicative functor\nThe implementation of the Lift2  and Lift3  functions is based on applicative functors \nthat curry and elevate the function selector, enabling its applicability to the elevated argument types.\nThe same concepts to implement the \nLift2  and Lift3  functions affect the F# design. \nBut due to the intrinsic functional feature of the programming language, and the con-ciseness provided by infix operators, the implementation of the lift functions (in bold) in F# is concise:\nlet lift2 (func:'a -> 'b -> 'c) (asyncA:Async<'a>) (asyncB:Async<'b>) =     func <!> asyncA <*> asyncBlet lift3 (func:'a -> 'b -> 'c -> 'd) (asyncA:Async<'a>) \n➥ (asyncB:Async<'b>) (asyncC:Async<'c>) = \n    func <!> asyncA <*> asyncB <*> asyncC\nDue to the F# type inference system, the input values are wrapped into an Async  type, \nand the compiler can interpret that the infix operators <*> and <!> are the functor \nand applicative functor in the context of the Async  elevated type. Also, note that it’s \nconvention in F# to start the module-level functions with a lowercase initial letter.\n10.6.4 Composing and executing heterogeneous parallel computations\nWhat can you do with these functions in place? Let’s analyze an example that exploits these operators.\nImagine you’re tasked to write a simple program to validate the decision to buy stock \noptions based on a condition set by analyzing market trends and the history of the stocks. The program should be divided into three operations: \n1 Check the total amount available for purchase based on the bank account avail-able balance and the current price of the stock:\na Fetch the bank account balance.\nb Fetch the stock price from the stock market.\n2 Validate if a given stock symbol is recommended to buy: \na Analyze the market indexes. \nb Analyze the historical trend of the given stock. \n3 Given a stock ticker symbol, decide to buy or not buy a certain number of stock options based upon the money available calculated in step 1.\nThe next listing shows the asynchronous functions to implement the program, which ideally should be combined (in bold). Certain code implementation details are omit-ted because they’re irrelevant for this example.\nListing 10.25  Asynchronous operations to compose and run in parallel\nlet calcTransactionAmount amount (price:float) =    let readyToInvest = amount * 0.75    let cnt = Math.Floor(readyToInvest / price)    if (cnt < 1e-5) && (price < amount)    then 1 else int(cnt)               let rnd = Random() Calculates the transaction  amount including arbitrary fees\n \n320 chapter  10 Functional combinators for fluent concurrent programming\nlet mutable bankAccount = 500.0 + float(rnd.Next(1000))let getAmountOfMoney() = async {    return bankAccount}    let getCurrentPrice symbol = async {        let! (_,data) = processStockHistory symbol         return data.[0].open'  }  let getStockIndex index =  async {        let url = sprintf ""http://download.finance.yahoo.com/d/quotes.\n➥ csv?s=%s&f=snl1"" index\n        let req = WebRequest.Create(url)        let! resp = req.AsyncGetResponse()        use reader = new StreamReader(resp.GetResponseStream())        return! reader.ReadToEndAsync()    }       |> Async.map (fun (row:string) ->        let items = row.Split(',')        Double.Parse(items.[items.Length-1]))    |> AsyncResult.handler   let analyzeHistoricalTrend symbol = asyncResult {        let! data = getStockHistory symbol (365/2)        let trend = data.[data.Length-1] - data.[0]        return trend    }   let withdraw amount = async {    return        if amount > bankAccount        then Error(InvalidOperationException(""Not enough money""))        else            bankAccount <- bankAccount - amount            Ok(true)    }     \nEach operation runs asynchronously to evaluate the result of a different type. Respec-tively, the function \ncalcTransactionAmount returns a hypothetical cost for the \ntrade(buy) transaction, the function analyzeHistoricalTrend  returns the value of \nthe stock historical analysis that’s used to evaluate if the stock option is a recommended buy, the function \ngetStockIndex returns the current value of the stock price, and the \nfunction getCurrentPrice  returns the last stock price.\nHow would you compose and run these computations in parallel using a Fork/Join \npattern, for example, when the result type isn’t the same? A simple solution should be spawning an independent task for each function, then waiting for all tasks to complete to pass the results into a final function that aggregates the results and continues the work. It would be much nicer to glue all these functions together using a more generic Simulates an asynchronous web service request to the bank account that returns a random value\nRetrieves the last price of the stockprocessStockHistory (see chapter 8) downloads and parses \nthe historical trend of a given stock ticker symbol.\nDownloads and retrieves asynchronously the price of a \ngiven stock ticker symbol from the stock market\nMaps the data of the stock ticker by retrieving the closing price. The output is an AsyncResult type because the operation could throw an exception.\nAnalyzes the historical trend of a given stock ticker symbol. The operation runs asynchronously in an asyncResult computation expression to handle potential errors.\nRetrieves the current withdraw available asynchronously. Otherwise, an Error is returned if the bank account does not contain sufficient funds to proceed with the trade operation.",11473
105-10.6.5 Controlling flow with conditional asynchronous combinators.pdf,105-10.6.5 Controlling flow with conditional asynchronous combinators,"321 The ultimate parallel composition applicative functor\ncombinator, which promotes reusability and, of course, better compositionality with a set of polymorphic tools.\nThe following listing applies the technique to run heterogeneous computations in \nparallel using the \nlift2  function in F# to evaluate how many stock options are recom-\nmended to buy after running a few simple diagnostics asynchronously (in bold).\nListing 10.26  Running heterogeneous asynchronous operations\nlet howMuchToBuy stockId : AsyncResult<int> =    Async.lift2 (calcTransactionAmount)             (getAmountOfMoney())          (getCurrentPrice stockId)    |> AsyncResult.handler         let analyze stockId =          howMuchToBuy stockId    |> Async.StartCancelable(function            | Ok (total) -> printfn ""I recommend to buy %d unit"" total        | Error (e) -> printfn ""I do not recommend to buy now"")\nhowMuchToBuy  is a two-parameter function with an AsyncResult<float>  type as \noutput. The result type definition is from the output of the underlying function \ncalcTransactionAmount , in which the AsyncResult<float>  indicates either the suc-\ncess of the operation with the amount of stock to buy, or not to buy. The first argu-ment of \nstockId is an arbitrary stock ticker symbol to analyze. The howMuchToBuy  \nfunction uses the lift2  operator and waits without blocking the two underlying async \nexpressions (getAmount OfMoney and getCurrentPrice ) to complete each compu-\ntation. The analyze  function executes howMuchToBuy  to collect and output the rec-\nommended result. In this case, the execution is performed asynchronously using the \nAsync.StartCancelable function defined in section 9.3.5.\nOne of the many benefits of using applicative, functor, monads, and combinator is \ntheir reproducibility and common patterns (regardless of the technology used). This makes it easy to understand and create a vocabulary that can be used to communicate to the developers and express the intention of the code. \n10.6.5 Controlling flow with conditional asynchronous combinators \nIn general, it’s common to implement combinators by gluing other combinators together. Once you have a set of operators that can represent any arbitrary asynchronous operation, you can easily design new combinators over the type that allow you to com-bine and compose asynchronous operations in myriad different and sophisticated ways. \nThere are limitless possibilities and opportunities to customize asynchronous combi-\nnators to respond to your needs. You could implement an asynchronous combinator that emulates an \nif-else  statement equivalent to the imperative conditional logic, but how?Lifts the heterogeneous function to apply the \ncalcTransactionAmount operation against the \noutput of the getAmountOfMoney and \ngetCurrentPrice functions \nThe output is run through the AsyncResult.handler validator.\nRuns the analysis of a given stock ticker symbol, returning the recommendation to proceed with the buy \nStarts the computation using the Async.StartCancelable operator. The continuation function pattern matches the input Result to dispatch the rest of the computation according to whether it’s successful (Ok) or a failure (Error). \n \n322 chapter  10 Functional combinators for fluent concurrent programming\nThe solution is found in the functional patterns: \n¡ Monoids can be used to create the Or combinator. \n¡ Applicative functors can be used to create the And combinator.\n¡ Monads chain asynchronous operations and glue combinatory. \nIn this section, you’re going to define a few conditional asynchronous combinators and consume them to understand what capabilities are offered and the limited effort required. In fact, by using the combinators introduced so far, it’s a matter of compos-ing them to achieve different behaviors. Furthermore, in the case of F# infix operators, it’s easy to use the feature to elevate and operate functions inline, avoiding the need for intermediate functions. For example, you’ve defined functions such as \nlift2  and \nlift3  by which it’s possible to apply heterogeneous parallel computation. \nYou can abstract away the combination notion into conditional operators such as IF, \nAND, and OR. The following listing shows a few combinators that apply to the F# asyn-\nchronous workflow. Semantically, they’re concise and easy to compose due to the func-tional property of this programming language. But the same concepts can be ported into C# effortlessly, or perhaps by using the interoperability option (the code to note is in bold).\nListing 10.27  Async-workflow conditional combinators\nmodule AsyncCombinators =   let inline ifAsync (predicate:Async<bool>) (funcA:Async<'a>) \n➥ (funcB:Async<’a>) = \n           async.Bind(predicate, fun p -> if p then funcA else funcB)   let inline iffAsync (predicate:Async<'a -> bool>) (context:Async<'a>) =        async {           let! p = predicate <*> context           return if p then Some context else None }    let inline notAsync (predicate:Async<bool>) =                                 async.Bind(predicate, not >> async.Return)     let inline AND (funcA:Async<bool>) (funcB:Async<bool>) =         ifAsync funcA funcB (async.Return false)    let inline OR (funcA:Async<bool>) (funcB:Async<bool>) =        ifAsync funcA (async.Return true) funcB    let (<&&>)(funcA:Async<bool>) (funcB:Async<bool>) = AND funcA funcB    let (<||>)(funcA:Async<bool>) (funcB:Async<bool>) = OR funcA funcB\nThe ifAsync  combinator will take an asynchronous predicate and two arbitrary asyn-\nchronous operations as arguments, where only one of those computations will run according to the outcome of the predicate. This is a useful pattern to branch the logic of your asynchronous program without leaving the asynchronous context. \nThe \niffAsync  combinator takes a HOF condition that verifies the given context. \nIf the condition holds true, then it asynchronously returns the context; otherwise it returns \nNone  asynchronously. The combinators from the previous code may be applied \n \n 323 The ultimate parallel composition applicative functor\nin any combination before execution starts, and they act as syntactic sugar, by which the code looks the same as in the sequential case.\nInline functions\nThe inline  keyword inlines the body of a function at its call sites. In this way, functions \nmarked inline  will be inserted verbatim whenever the function is called at compile \ntime, improving the performance of the code execution. Note that inlining is a compiler process that trades code size for speed, whereby method calls to small or simple meth-ods are replaced by the method’s body.\n \nLet’s analyze in more detail these logical asynchronous combinators for a better understanding of how they work. This knowledge is key to building your own custom combinators.\nthe and logical  asynchronous  combinator\nThe asynchronous AND combinator returns the result after both functions funcA  and \nfuncB  complete. This behavior is similar to Task.WhenAll , but it runs the first expres-\nsion and waits for the result, then calls the second one and combines the results. If the evaluation is canceled, or fails, or returns the wrong result, then the other function will not run, applying a short-circuit logic.\nConceptually, the \nTask.WhenAll  operator previously described is a good fit to per -\nform logical AND over multiple asynchronous operations. This operator takes a pair of \niterators to a container of tasks or a variable number of tasks, and returns a single Task  \nthat fires when all the arguments are ready.\nThe AND operator can be combined into chains as long as they all return the same \ntype. Of course, it can be generalized and extended using applicative functors. Unless the functions have side effects, the result is deterministic and independent of order, so they can run parallel. \nthe or logical  asynchronous  combinator\nThe asynchronous OR combinator works like the addition operator with monoid struc-\nture, which means that the operations must be associative. The OR combinator starts \ntwo asynchronous operations in parallel, waiting for the first one to complete. The same properties of the \nAND combinators apply here. The OR combinator can be com-\nbined into chains; but the result cannot be deterministic unless both function evalua-tions return the same type, and both are canceled.\nThe combinator that acts like a logical \nOR of two asynchronous operations can be \nimplemented using the Task.WhenAny  operator, which starts the computations in paral-\nlel and picks the one that finishes first. This is also the basis of speculative computation, where you pitch several algorithms against each other.\nThe same approach for building \nAsync  combinators can be applied to the Async-\nResult  type, which provides a more powerful way to define generic operations where \nthe output depends on the success of the underlying operations. In other words, \n \n324 chapter  10 Functional combinators for fluent concurrent programming\nAsync Result  acts as two state flags, which can represent either a failure or a successful \noperation, where the latter provides the final value. Here are a few examples of Async-\nResult  combinators (in bold).\nListing 10.28  AsyncResult  conditional combinators\nmodule AsyncResultCombinators =    let inline AND (funcA:AsyncResult<'a>) (funcB:AsyncResult<'a>) \n➥ : AsyncResult<_> =\n        asyncResult {                let! a = funcA                let! b = funcB                return (a, b)        }    let inline OR (funcA:AsyncResult<'a>) (funcB:AsyncResult<'a>) \n➥ : AsyncResult<'a> =\n        asyncResult {            return! funcA            return! funcB        }    let (<&&>) (funcA:AsyncResult<'a>) (funcB:AsyncResult<'a>) =         AND funcA funcB    let (<||>) (funcA:AsyncResult<'a>) (funcB:AsyncResult<'a>) =         OR funcA funcB    let (<|||>) (funcA:AsyncResult<bool>) (funcB:AsyncResult<bool>) =         asyncResult {            let! rA = funcA            match rA with            | true -> return! funcB             | false -> return false        }    let (<&&&>) (funcA:AsyncResult<bool>) (funcB:AsyncResult<bool>) =         asyncResult {            let! (rA, rB) = funcA <&&> funcB            return rA && rB        }\nThe AsyncResult  combinators, compared to the Async  combinators, expose the logi-\ncal asynchronous operators AND and OR that perform conditional dispatch over generic \ntypes instead of bool  types. Here’s the comparison between the AND operators imple-\nmented for Async  and AsyncResult :\n    let inline AND (funcA:Async<bool>) (funcB:Async<bool>) =         ifAsync funcA funcB (async.Return false)    let inline AND (funcA:AsyncResult<'a>) (funcB:AsyncResult<'a>) \n➥ : AsyncResult<_> =\n        asyncResult {                let! a = funcA                let! b = funcB                return (a, b)        }",10949
106-10.6.6 Asynchronous combinators in action.pdf,106-10.6.6 Asynchronous combinators in action,"325 The ultimate parallel composition applicative functor\nThe AsyncResult  AND uses the Result  discriminated union to treat the Success  case as \nthe true value, which is carried over to the output of the underlying function. \nTips to implement custom asynchronous combinators\nUse this general strategy to create custom combinators:\n1 Describe the problem purely in terms of concurrency.\n2 Simplify the description until it’s reduced to a name.\n3 Consider alternative paths for simplification.\n4 Write and test (or import) the concurrency construct.\n \n10.6.6 Asynchronous combinators in action\nIn listing 10.26, the stock ticker symbol was analyzed and a recommendation decided asynchronously to buy a given stock. Now you need to add the conditional check \nif-else , which behaves asynchronously using the ifAsync  combinator: if the stock \noption is recommended to buy, then proceed with the transaction; otherwise it returns an error message. The code to note is in bold.\nListing 10.29  AsyncResult  conditional combinators\nlet gt (value:'a) (ar:AsyncResult<'a>) = asyncResult {        let! result = ar        return result > value    }    let doInvest stockId =    let shouldIBuy =           ((getStockIndex ""^IXIC"" |> gt 6200.0)            <|||>                       (getStockIndex ""^NYA"" |> gt 11700.0 ))        <&&&>  ((analyzeHistoricalTrend stockId) |> gt 10.0)          |> AsyncResult.defaultValue false      let buy amount = async {          let! price = getCurrentPrice stockId        let! result = withdraw (price*float(amount))        return result |> Result.bimap (fun x -> if x then amount else 0)                                       (fun _ -> 0)         }Checks if a given value is greater than the result of an \nasynchronous operation returning an AsyncResult \ntype. The generic type 'a must be comparable. Applies the logical async OR operator to evaluate the given functions. This function represents a predicate that says whether or not you should buy based on the current market. \nExploits the async infix OR operator Applies the logical \nasync infix AND \noperator to evaluate \nthe given functions\nHelper function that returns the default value of a given type, lifting the output into the AsyncResult type. This function is used in case of error during the calculation process.Checks the current bank balance returning the amount of stocks that can be purchased \nVerifies if the transaction is successful and then either returns \nAsync<int>, wrapping the amount value if the transaction is \nsuccessful, or returns Async<int> zero\n \n326 chapter  10 Functional combinators for fluent concurrent programming\n    AsyncComb.ifAsync shouldIBuy           (buy <!> (howMuchToBuy stockId))           (Async.retn <| Error(Exception(""Do not do it now"")))      |> AsyncResult.handler         \nIn this code example, the doInvest  function analyzes a given stock symbol, its histor -\nical trend, and the current stock market to recommend a trading transaction. This function \ndoInvest  combines asynchronous functions that operate as a whole to deter -\nmine the recommendation. The function shouldIBuy  applies the asynchronous OR \nlogical operator to check if either the ^IXIC or ^NYA index is greater than a given threshold. The result is used as base value to evaluate if the current stock market is good for buying operations. \nIf the result of the \nshouldIBuy  function is successful (true), the asynchronous AND \nlogical operator proceeds, executing the analyzeHistoricalTrend  function, which \nreturns the historical trend analysis of the given stock. Next, the buy function verifies \nthat the bank account balance is sufficient to buy the desired stock options; otherwise it returns an alternative value or zero if the balance is too low.\nUltimately, these functions are combined. The \nifAsync  combinator runs should-\nIBuy  asynchronously. According to its output, the code branches to either proceed with \na buy transaction or return an error message. The purpose of the  map  infix operator \n(<!>)  is to lift the function buy into the AsyncResult  elevated type, which is then exe-\ncuted against the number of stocks recommended to purchase calculated by the func-tion \nhowMuchToBuy . \nNOTE  The functions in listing 10.29 run as a unit of work, but each step is exe-\ncuted asynchronously, on demand.\nSummary\n¡ Exposing your intent is crucial if you want to increase the readability of your code. Introducing the \nResult  class helps to show if the method is a failure or suc-\ncess, removes unnecessary boilerplate code, and results in a clean design.\n¡ The Result  type gives you an explicit, functional way to handle errors without \nintroducing side effects (unlike throwing/catching exceptions), which leads to expressive and readable code implementations.\n¡ When you consider the execution semantics of your code, Result  and Option  \nfill a similar goal, accounting for anything other than the happy path when code executes. \nResult  is the best type to use when you want to represent and preserve Runs a conditional If statement to decide to buy or not buy the given stock If the shouldIBuy operation is positive, the buy \nfunction is lifted (AsyncResult) and executed \nagainst the amount of the number of stocks \nrecommended to buy. This amount is the output \nof the function howMuchToBuy. \nIf the shouldIBuy operation is negative, \nan error message is displayed. Wraps the overall function combinators in an async error catch \n \n 327 Summary\nan error that can occur during execution. Option  is better for when you wish to \nrepresent the existence or absence of a value, or when you want consumers to account for an error, but you don’t care about preserving that error.\n¡ FP unmasks patterns to ease composition of asynchronous operations through the support of mathematical patterns. For example, applicative functors, which are amplified functors, can combine functions with multiple arguments directly over elevated types.\n¡ Asynchronous combinators can be used to control the asynchronous execution flow of a program. This control of execution includes conditional logic. It’s effortless to compose a few asynchronous combinators to construct more sophis-ticated ones, such as the asynchronous versions of the \nAND and OR operators.\n¡ F# has support for infix operators, which can be customized to produce a conve-nient set of operators. These operators simplify the programming style to easily construct a very sophisticated chain of operations in a non-standard manner. \n¡ Applicatives and functors can be combined to lift conventional functions, whose execution against elevated types can be performed without leaving the context. This technique allows you to run in parallel a set of heterogeneous functions, whose outputs can be evaluated as a whole. \n¡ Using core functional functions, such as Bind , Return , Map, and Apply , makes it \nstraightforward to define rich code behavior that composes, run in parallel, and performs applications in an elevated world that mimics conditional logic, such as \nif-else .",7128
107-11 Applying reactive programming everywhere with agents.pdf,107-11 Applying reactive programming everywhere with agents,"32811Applying reactive programming \neverywhere with agents\nThis chapter covers\n¡ Using the message-passing concurrent model\n¡ Handling millions of messages per second\n¡ Using the agent programming model \n¡ Parallelizing a workflow and coordinating agents\nWeb applications play an important role in our lives, from large social networks and media streaming to online banking systems and collaborative online gaming. Cer -\ntain websites now handle as much traffic as the entire internet did less than a decade ago. Facebook and Twitter, two of the most popular websites, have billions of users each. To ensure that these applications thrive, concurrent connections, scalability, and distributed systems are essential. Traditional architectures from years past can-not operate under this high volume of requests.\nHigh-performance computing is becoming a necessity. The message-passing \nconcurrent programming model is the answer to this demand, as evidenced by the increasing support for the message-passing model in mainstream languages such as Java, C#, and C++.\n \n 329  \n11The number of concurrent online connections will certainly continue to grow. The \ntrend is shifting to physical devices that are interconnected, generating sophisticated and massive networks constantly operating and exchanging messages. It’s predicted that the Internet of Things (IoT) will expand to an installed base of 75 billion units by 2025 (http:/ /mng.bz/wiwP).\nWhat is the Internet of Things?\nThe Internet of Things (IoT), as its name implies, is a giant network of things (refrigera -\ntors, washing machines, and more) connected to the internet. Basically, anything with an on/off switch that can be connected to the internet can be part of the IoT. One analyst firm estimates that by 2020 there will be 26 billion connected devices (www.forbes.com/companies/gartner/ ). Other estimates put this number at more than 100 billion. That’s \na lot of data transfer. One challenge of the IoT is to convey the data in real time, with no slowing or bottlenecks, and to continually improve response time. Another challenge is security: all those things connected to the internet are open to hacking. \n \nThe continual evolution of devices connected online is inspiring a revolution in how developers design the next generation of applications. The new applications will have to be non-blocking, fast, and capable of reacting to high volumes of system notifica-tions. Events will control the execution of reactive applications. You’ll need a highly available and resource-efficient application able to adapt to this rapid evolution and respond to an infinitely increasing volume of internet requests. The event-driven and asynchronous paradigms are the primary architectural requirements for developing such applications. In this context, you’ll need asynchronous programming processed in parallel. \nThis chapter is about developing responsive and reactive systems, starting with the \nexceptional message-passing programming model, a general-purpose concurrent one with particularly wide applicability. The message-passing programming model has sev-eral commonalities with the microservices architecture (http:/ /microservices.io/).\nYou’ll use the agent-based concurrent programming style, which relies on mes-\nsage passing as a vehicle to communicate between small units of computations called agents. Each agent may own an internal state, with single-threaded access to guarantee thread safety without the need of any lock (or other any other synchronization prim-itive). Because agents are easy to understand, programming with them is an effective tool for building scalable and responsive applications that ease the implementation of advanced asynchronous logic.\nBy the end of this chapter, you’ll know how to use asynchronous message-passing \nsemantics in your applications to simplify and improve responsiveness and performance in your application. (If you are shaky on asynchronicity, review chapters 8 and 9.)\nBefore we plunge into the technical aspects of the message-passing architecture and \nthe agent model, let’s look at the reactive system, with an emphasis on the properties that make an application valuable in the reactive paradigm.",4249
108-11.2 The asynchronous message-passing programmingmodel.pdf,108-11.2 The asynchronous message-passing programmingmodel,"330 chapter  11 Applying reactive programming everywhere with agents\n11.1 What’s reactive programming, and how is it useful?\nReactive programming is a set of design principles used in asynchronous programming to create cohesive systems that respond to commands and requests in a timely manner. It is a way of thinking about systems’ architecture and design in a distributed environ-ment where implementation techniques, tooling, and design patterns are components of a larger whole—a system. Here, an application is divided into multiple distinct steps, each of which can be executed in an asynchronous and non-blocking fashion. Execu-tion threads that compete for shared resources are free to perform other useful work while the resource is occupied, instead of idling and wasting computational power.\nIn 2013, reactive programming became an established paradigm with a formalized \nset of rules under the umbrella of the Reactive Manifesto (www.reactivemanifesto.org/), which describes the number of constituent parts that determine a reactive system. The Reactive Manifesto outlines patterns for implementing robust, resilient, and responsive systems. The reason behind the Reactive Manifesto is the recent changes to application requirements (table 11.1). \nTable 11.1  Comparison between the requirements for past and present applications\nPast requirements for applications Present requirements for applications\nSingle processors Multicore processors\nExpensive RAM Cheap RAM\nExpensive disk memory Cheap disk memory\nSlow networks Fast networks\nLow volume of concurrent requests High volume of concurrent requests\nSmall data Big data\nLatency measured in seconds Latency measured in milliseconds \nIn the past, you might have had only a few services running on your applications, with ample response time, and time available for systems to be offline for maintenance. Today, applications are deployed over thousands of services, and each can run on mul-tiple cores. Additionally, users expect response times in milliseconds, as opposed to seconds, and anything less than 100% uptime is unacceptable. The Reactive Manifesto seeks to solve these problems by asking developers to create systems that have four properties. They must be responsive (react to users), resilient (react to failure), mes-sage-driven (react to events), and scalable (react to load). Figure 11.1 illustrates these properties and how they relate to each other.\n \n 331 The asynchronous message-passing programming model\nResponsive\nMessage drivenElastic Resilient• Consistent response times\n• Responds in a timely fashion\n• Varying workloads\n• Reacts to workload changes• Scalable• Controlled failures\n• Isolated components\n• Able to recover\n• Asynchronous message passing• Loose coupling\n• Location transparency\nFigure 11.1  According to the Reactive Manifesto, for a system to be called reactive, it must have four \nproperties: it must be responsive (must react to users), resilient (react to failure), message-driven (react to events), and scalable (react to load).\nA system built using the manifesto’s requirements will:\n¡ Have a consistent response time regardless of the workload undertaken.\n¡ Respond in a timely fashion, regardless of the volume of requests coming in. This ensures that the user isn’t spending significant amounts of time idly waiting for operations to complete, thereby providing a positive user experience.\nThis responsiveness is possible because reactive programming optimizes the use of the computing resources on multicore hardware, leading to better performance. Asyn-chronicity is one of the key elements of reactive programming. Chapters 8 and 9 cover the APM and how it plays an important role in building scalable systems. In chapter 14, you’ll build a complete server-side application that fully embraces this paradigm.\nA message-driven architecture is the foundation of reactive applications. Message- \ndriven means that reactive systems are built on the premise of asynchronous message passing; furthermore, with a message-driven architecture, components can be loosely coupled. The primary benefit of reactive programming is that it removes the need for explicit coordination between active components in a system, simplifying the approach to asynchronous computation.\n11.2 The asynchronous message-passing programming model\nIn a typical synchronous application, you sequentially perform an operation with a request/response model of communication, using a procedure call to retrieve data or modify a state. This pattern is limited due to a blocking programming style and design that cannot be scaled or performed out of sequence. \n \n332 chapter  11 Applying reactive programming everywhere with agents\nA message-passing-based architecture is a form of asynchronous communication where \ndata is queued, to be processed at a later stage, if necessary. In the context of reactive programming, the message-passing architecture uses an asynchronous semantic to com-municate between the individual parts of the system. As a result, it can handle millions of messages per second, producing an incredible boost to performance (figure 11.2).\n \nTask 1Task 2\n{ ... }Task 3\nTask 4\nTask 5\nThread\nusage{ ... }\nThread\nusageSynchronous (blocking)\nResource inef ficient and easily bottleneckedAsynchronous messaging-passing (reactive)\nReduces risk, conserves valuable resources,\nand requires less hardware/infrastructure\nTask 1\nXX\nXX\nResource lifetime\nFigure 11.2  The synchronous (blocking) communication is resource inefficient and easily bottlenecked. \nThe asynchronous message-passing (reactive) approach reduces blocking risks, conserves valuable resources, and requires less hardware/infrastructure. \nNOTE  The message-passing model has become increasingly popular and has \nbeen implemented into many new programming languages, often as a first-class language concept. In many other programming languages, it’s available using third-party libraries that build on top of conventional multithreading.\nThe idea of message-passing concurrency is based on lightweight units of compu-tation (or processes) that have exclusive ownership of state. The state, by design, is protected and unshared, which means it can be either mutable or immutable with-out running into any pitfalls due to a multithreaded environment (see chapter 1). In a message-passing architecture, two entities run in separate threads: the sender of a message and a receiver of the message. The benefit of this programming model is that \nall issues of memory sharing and concurrent access are hidden inside the commu-nication channel. Neither entity involved in the communication needs to apply any low-level synchronization strategies, such as locking. The message-passing architecture \n \n 333 The asynchronous message-passing programming model\n(message-passing concurrent model) doesn’t communicate by sharing memory, but instead communicates by sending messages. \nAsynchronous message passing decouples communication between entities and \nallows senders to send messages without waiting for their receivers. No synchronization is necessary between senders and receivers for message exchange, and both entities can run independently. Keep in mind that the sender cannot know when a message is received and handled by the recipient.\nThe message-passing concurrent model can at first appear more complicated than \nsequential or even parallel systems, as you’ll see in the comparison in figure 11.3 (the squares represent objects, and arrows represent a method call or a message). \nSequential programming\nInput\nOutputTask-based programming\nInput\nOutputMessage-passing programming\nInput\nOutput\nFigure 11.3  Comparison between task-based, sequential, and agent-based programming. Each block \nrepresents a unit of computation.\nIn figure 11.3, each block represents a unit of work:\n¡ Sequential programming is the simplest with a single input and produces a single output using a single control flow, where the blocks are connected directly in a linear fashion, each task dependent on the completion of the previous task. \n¡ Task-based programming is similar to the sequential programming model, but it may MapReduce or Fork/Join the control flow.\n¡ Message-passing programming may control the execution flow because the blocks are interconnected with other blocks in a continuous and direct manner. Ultimately, each block sends messages directly to other blocks, non-linearly. This design can seem complex and difficult to understand at first. But because blocks are encapsulated into active objects, each message is passed independent of other messages, with no blocking or lag time. With the message-passing concurrent model, you can have multiple building blocks, each with an independent input and output, which can be connected. Each block runs in isolation, and once isola-tion is achieved, it’s possible to deploy the computation into different tasks. \nWe’ll spend the rest of chapter on agents as the main tool for building message-passing concurrent models.",9104
109-11.2.1 Relation with message passing and immutability.pdf,109-11.2.1 Relation with message passing and immutability,,0
110-11.2.2 Natural isolation.pdf,110-11.2.2 Natural isolation,,0
111-11.3.2 What an agent can do.pdf,111-11.3.2 What an agent can do,"334 chapter  11 Applying reactive programming everywhere with agents\n11.2.1 Relation with message passing and immutability\nBy this point, it should be clear that immutability ensures increased degrees of concur -\nrency. (Remember, an immutable object is an object whose state cannot be modified after it’s created.) Immutability is a foundational tool for building concurrent, reliable, and predictable programs. But it isn’t the only tool that matters. Natural isolation is also critically important, perhaps more so, because it’s easier to achieve in program-ming languages that don’t support immutability intrinsically. It turns out that agents enforce coarse-grained isolation through message passing. \n11.2.2 Natural isolation\nNatural isolation is a critically important concept for writing lockless concurrent code. \nIn a multithreaded program, isolation solves the problem of shared state by giving each thread a copied portion of data to perform local computation. With isolation, there’s no race condition, because each task processes an independent copy of its own data. \nIsolation for building resilient systems \nIsolation is an aspect of building resilient systems. For example, in the event that a single component fails, the rest of the system is seemingly immune to this failure. Message passing is a tremendous help in simplifying the building process for correct concurrent systems enabled due to the isolation approach, also called the share-nothing approach.\n \nThe natural isolation or share-nothing approach is less complex to achieve than immu-tability, but both options represent orthogonal approaches and should be used in con-junction for reducing runtime overheads and avoiding race condition and deadlocks.\n11.3 What is an agent?\nAn agent is a single-thread unit of computation used to design concurrent applications \nbased on message passing in isolation (share-nothing approach). These agents are lightweight constructs that contain a queue and can receive and process messages. In this case, lightweight means that agents have a small memory footprint as compared to spawning new threads, so you can easily spin up 100,000 agents in a computer without a hitch. \nThink of an agent as a process that has exclusive ownership of some mutable state, \nwhich can never be accessed from outside of the agent. Although agents run concur -\nrently with each other, within a single agent everything is sequential. The isolation of the agent’s internal state is a key concept of this model, because it is completely inacces-sible from the outside world, making it thread safe. Indeed, if state is isolated, mutation can happen freely. \nAn agent’s basic functionality is to do the following:\n¡ Maintain a private state that can be accessed safely in a multithreaded environment \n¡ React to messages differently in different states\n \n 335 What is an agent?\n¡ Notify other agents\n¡ Expose events to subscribers \n¡ Send a reply to the sender of a message\nOne of the most important features of agent programming is that messages are sent asynchronously, and the sender doesn’t initiate a block. When a message is sent to an agent, it is placed in a mailbox.  The agent processes one message at a time sequentially in the order in which it was added to the mailbox, moving on to the next message only when it has finished processing the current message. While an agent processes a message, the other incoming messages aren’t lost, but are buffered into the internal isolated mailbox. Consequently, multiple agents can run in parallel effortlessly, which means that the performance of a well-written agent-based application scales with the number of cores or processors. \nAn agent isn’t an actor\nOn the surface, there are similarities between agents and actors, which sometimes cause people to use these terms interchangeably. But the main difference is that agents are in a process, while actors may be running on  another process. In fact, the reference \nto an agent is a pointer to a specific instance, whereas an actor reference is determined through location transparency. Location transparency is the use of names to identify net-\nwork resources, rather than their actual location, which means that the actor may be running in the same process, or on another process, or possibly on a remote machine. Agent-based concurrency is inspired by the actor model, but its construction is much simpler. Actor systems have built-in sophisticated tools for distribution support, which include supervision to manage exceptions and potentially self-heal the system, routing to customize the work distribution, and more. Several libraries and tool kits, such as Akka.net (http:/ /getakka.net/), Proto.Actor (http:/ /proto.actor/ ), and Microsoft Orleans (https:/ /dotnet.github.io/orleans/ ), implement the \nactor model for the .NET ecosystem . It’s no surprise that Microsoft Azure Service-Fabric (https:/ /azure.microsoft.com/en-us/services/service-fabric), used to build distributed, scalable, and fault-tolerant microservices in the cloud, is based on the actor model. For more information about the actor model in .NET, I recommend Anthony Brown’s Reactive \nApplications with Akka.Net (Manning, 2017).The tools and features provided by the actor libraries can be implemented and replicated easily for agents. You can find several libraries that overcome missing functionalities such as supervision and routing (http:/ /mbrace.io/  and http:/ /akka.net).\n \n11.3.1 The components of an agent\nFigure 11.4 shows the fundamental component parts of an agent:\n¡ Mailbox—An internal queue to buffer incoming messages implemented as asyn-chronous, race-free, and non-blocking.",5709
112-11.3.5 Agent is object-oriented.pdf,112-11.3.5 Agent is object-oriented,"336 chapter  11 Applying reactive programming everywhere with agents\n¡ Behavior—The internal function applied sequentially to each incoming message. The behavior is single-threaded.\n¡ State—Agents can have an internal state that’s isolated and never shared, so they never need to compete for locks to be accessed.\n¡ Message—Agents can communicate only through messages, which are sent asyn-chronously and are buffered in a mailbox.\nBehavior\n(message processor)A message is sent to themailbox to communicatewith the agent.\nMessages are dequeuedand processed sequentially .Single-threaded,\nencapsulated stateImmutable state\nMailbox\nqueueIncoming message Output messageStateAgent\nFigure 11.4  An agent consists of a mailbox that queues the income messages, a state, and a behavior \nthat runs in a loop, which processes one message at a time. The behavior is the functionality applied to the messages. \n11.3.2 What an agent can do\nThe agent programming model provides great support for concurrency and has an extensive range of applicability. Agents are used in data collection and mining, reduc-ing application bottlenecks by buffering requests, real-time analysis with bounded and unbounded reactive streaming, general-purpose number crunching, machine learn-ing, simulation, Master/Worker pattern, Compute Grid, MapReduce, gaming, and audio and video processing, to mention a few.\n11.3.3 The share-nothing approach for lock-free concurrent programming\nThe share-nothing architecture refers to message-passing programming, where each \nagent is independent and there’s no single point of contention across the system. This architecture model is great for building concurrent and safe systems. If you don’t share anything, then there’s no opportunity for race conditions. Isolated message-  \npassing blocks (agents) are a powerful and efficient technique to implement scalable programming algorithms, including scalable request servers and scalable distribut-ed-programming algorithms. The simplicity and intuitive behavior of the agent as a building block allows for designing and implementing elegant, highly efficient asyn-chronous and parallel applications that don’t share state. In general, agents perform \n \n 337 What is an agent?\ncalculations in reaction to the messages they receive, and they can send messages to other agents in a fire-and-forget manner or collect the responses, called replies  \n(figure 11.5). \nBehavior\n(message processor)Mailbox\nqueueStateMessages\nAgent\nBehavior\n(message processor)Mailbox\nqueueStateAgent\nBehavior\n(message processor)Mailbox\nqueueStateAgent\nBehavior\n(message processor)Mailbox\nqueueStateMessages\nAgentMessages\nMessagesMain thread\nFigure 11.5  Agents communicate with each other through a message-passing semantic, creating an \ninterconnected system of units of computation that run concurrently. Each agent has an isolated state and independent behavior.\n11.3.4 How is agent-based programming functional?\nCertain aspects of agent-based programming aren’t functional. Although agents (and \nactors) were developed in the context of functional languages, their purpose is to gener -\nate side effects, which is against the tenets of FP. An agent often performs a side effect, or sends a message to another agent, which will, in turn, perform a new side effect. \nLess important, but worth mentioning, is that FP in general separates logic from data. \nBut agents contain data and the logic for the processing function. Additionally, sending",3521
113-11.4 The F agent MailboxProcessor.pdf,113-11.4 The F agent MailboxProcessor,"338 chapter  11 Applying reactive programming everywhere with agents\na message to an agent doesn’t force any constraint on the return type. An agent behav-ior, which is the operation applied to each message, can either return a result or not return any result. In the latter scenario, the design of a message sent in a fire-and-forget fashion encourages program agents in a unidirectional flow pattern, which means that the messages flow forward from one agent to the next. This unidirectional message flow between agents can preserve their compositional semantic aspect, achieved by linking a given set of agents. The result is a pipeline of agents that represents the steps of opera-tions to process the messages, each executed independently and potentially in parallel. \nThe primary reason that the agent model is functional is that agents can send behavior \nto the state instead of sending state to the behavior. In the agent model, the sender, besides sending messages, can provide the function which implements an action to process the incoming messages. Agents are an in-memory slot where you can put in data structure, such as a bucket (container). In addition to providing data storage, agents allow you to send messages in the shape of a function, which is then applied atomically to the inter -\nnal bucket. \nNOTE  Atomically refers to a set of operations (atomic operations) that, when \nthey start,  must complete before any interrupt in a single step, such that other parallel threads can only ever see either the old or new state. \nThe function can be composed from other functions and then sent to the agent as a message. The advantage is the ability to update and change behavior at runtime using functions and function-composition fitting with the functional paradigm.\n11.3.5 Agent is object-oriented \nIt’s interesting to note that Alan Kay’s (https:/ /en.wikipedia.org/wiki/Alan_Kay) orig-\ninal vision for objects in Smalltalk is much closer to the agent model than it is to the objects found in most programming languages (the basic concept of “messaging,” for example). Kay believed that state changes should be encapsulated and not done in an unconstrained way. His idea of passing messages between objects is intuitive and helps to clarify the boundaries between objects.\nClearly, message passing resembles OOP, and you can lean on the OOP-style message \npassing, which is only calling a method. Here, an agent is like an object in an object- oriented program, because it encapsulates state and communicates with other agents by exchanging messages.\n11.4 The F# agent: MailboxProcessor\nThe support for the APM in F# doesn’t stop with asynchronous workflows (introduced \nin chapter 9). Additional support is provided inherently by the F# programming lan-guage, including \nMailboxProcessor , a primitive type that behaves as a lightweight \nin-memory message-passing agent (see figure 11.6).\nMailboxProcessor  works completely asynchronously, and provides a simple con-\ncurrent programming model that can deliver fast and reliable concurrent programs. \n \n 339 The F# agent: MailboxProcessor\nI could write an entire book about MailboxProcessor , its multipurpose uses, and the \nflexibility that it provides for building a wide range of diverse applications. The benefits of using it include having a dedicated and isolated message queue combined with an asynchronous handler, which is used to throttle the message processing to automati-cally and transparently optimize the usage of the computer’s resources.\n \nMailboxProcessor\n(agent)\nBehaviorMailbox\nreceives URLMessages\nWebsite\nBehavior:use client = new W ebClient()\nlet uri = Uri message\nlet! site = client.AsyncDownloadString(uri)While loop waiting fo r\nincoming messagesMailbox receives messages:\nlet! message = inbox.Receive()\nFigure 11.6  MailboxProcessor  (agent) waits asynchronously for incoming messages in the while  \nloop. The messages are strings representing the URL, which are applied to the internal behavior to download the related website.\nThe following listing shows a simple code example using a MailboxProcessor , which \nreceives an arbitrary URL to print the length of the website. \nListing 11.1  Simple MailboxProcessor  with a while  loop \ntype Agent<'T> = MailboxProcessor<'T>let webClientAgent =  Agent<string>.Start(fun inbox -> async {        while true do      let! message = inbox.Receive()        use client = new WebClient()      let uri = Uri message      let! site = client.AsyncDownloadString(uri)        printfn ""Size of %s is %d"" uri.Host site.Length    })agent.Post ""http://www.google.com""   agent.Post ""http://www.microsoft.com”    The method MailboxProcessor.Start returns a running agent.\nWaits asynchronously to receive a message\nUses the asynchronous workflow to download the data\nSends a message to the MailboxProcessor in a fire-and-forget fashion",4904
114-11.5 Avoiding database bottlenecks with F MailboxProcessor.pdf,114-11.5 Avoiding database bottlenecks with F MailboxProcessor,"340 chapter  11 Applying reactive programming everywhere with agents\nLet’s look at how to construct an agent in F#. First, there must be a name of the instance. In this case \nwebClientAgent  is the address of the mailbox processor. This \nis how you’ll post a message to be processed. The MailboxProcessor  is generally ini-\ntialized with the MailboxProcessor.Start  shortcut method, though you can create \nan instance by invoking the constructor directly, and then run the agent using the instance method \nStart . To simplify the name and use of the MailboxProcessor , you \nestablish it as the alias agent and then start the agent with Agent.Start .\nNext, there’s a lambda function with an inbox containing an asynchronous work-\nflow. Each message sent to the mailbox processor is sent asynchronously. The body of the agent functions as a message handler that accepts a mailbox (\ninbox :Mailbox-\nProcessor ) as an argument. This mailbox has a running logical thread that controls a \ndedicated and encapsulated message queue, which is thread safe, to use and coordinate the communication with other threads, or agents. The mailbox runs asynchronously, using the F# asynchronous workflow. It can contain long-running operations that don’t block a thread.\nIn general, messages need to be processed in order, so there must be a loop. This \nexample uses a non-functional \nwhile-true  style loop. It’s perfectly fine to use this or to \nuse a functional, recursive loop. The agent in listing 11.1 starts getting and processing messages by calling the asynchronous function \nagent.Receive()  using the let!  con-\nstruct inside an imperative while  loop. \nInside the loop is the heart of the mailbox processor. The call of the mailbox \nReceive  function waits for the incoming message without blocking the actual thread, \nand resumes once a message is received. The use of the let!  operator ensures that the \ncomputation is started immediately. \nThen the first message available is removed from the mailbox queue and is bound to \nthe message identifier. At this point, the agent reacts by processing the message, which in this example downloads and prints the size of a given website address. If the mailbox queue is empty and there are no messages to process, then the agent frees the thread back to the thread pool scheduler. That means no threads are idle while \nReceive waits \nfor incoming messages, which are sent to the MailboxProcessor  in a fire-and-forget \nfashion using the agent.Post  method.\n11.4.1 The mailbox asynchronous recursive loop\nIn the previous example, the agent mailbox waits for messages asynchronously using \nan imperative while  loop. Let’s modify the imperative loop so it uses a functional \nrecursion to avoid mutation and possibly so it holds local state. \nThe following listing is the same version of the agent that counts its messages (shown in \nlisting 11.1), but this time it uses a recursive asynchronous function that maintains a state.\nListing 11.2  Simple MailboxProcessor  with a recursive function\n  let agent = Agent<string>.Start(fun inbox ->       let rec loop count = async {              let! message = inbox.Receive() Uses an asynchronous recursive function that maintains a state in an immutable manner\n \n 341 Avoiding database bottlenecks with F# MailboxProcessor\n          use client = new WebClient()          let uri = Uri message          let! site = client.AsyncDownloadString(uri)          printfn ""Size of %s is %d - total messages %d"" uri.Host\n➥ site.Length (count + 1)\n          return! loop (count + 1) }              loop 0)        agent.Post ""http://www.google.com""    agent.Post ""http://www.microsoft.com""\nThis functional approach is a little more advanced, but it greatly reduces the amount of explicit mutation in your code and is often more general. In fact, as you’ll see shortly, you can use the same strategy to maintain and safely reuse the state for caching. \nPay close attention to the line of code for the \nreturn! loop (n + 1) , where the func-\ntion uses asynchronous workflows recursively to execute the loop, passing the increased value of the count. The call using \nreturn!  is tail-recursive, which means that the com-\npiler translates the recursion more efficiently to avoid stack overflow exceptions. See chapter 3 for more details about recursive function support (also in C#).\nA MailboxProcessor’s most important functions\nA MailboxProcessor ’s most important functions are as follows:\n¡ Start —This function defines the async callback that forms the message looping.\n¡ Receive —This is the async  function to receive messages from the internal queue.\n¡ Post —This function sends a message to the MailboxProcessor  in a fire-and-\nforget manner.\n \n11.5 Avoiding database bottlenecks with F# MailboxProcessor\nThe core feature of most applications is database access, which is frequently the real source of bottlenecks in code. A simple database performance tuning can speed up applications significantly and keep the server responsive. \nHow do you guarantee consistently high-throughput database access? To better facil-\nitate database access, the operation should be asynchronous, because of the I/O nature of database access. Asynchronicity ensures that the server can handle multiple requests in parallel. You may wonder about the number of parallel requests that a database server can handle before performance degrades (figure 11.7 shows performance deg-radation at a high level). No exact answer exists. It depends on many different factors: for example, the size of the database connection pool.\nA critical element of the bottleneck problem is controlling and throttling the incom-\ning requests to maximize the application’s performance. \nMailboxProcessor  provides a \nsolution by buffering the incoming messages and taming possible overflow of requests (see figure 11.8). Using \nMailboxProcessor  as a mechanism to throttle the database \noperations provides a granular control for optimizing the database connection-pool use. For example, the program could add or remove agents to execute the database operations in a precise grade of parallelism.The recursive function is a tail call, asynchronously passing an updated state.  \n \n342 chapter  11 Applying reactive programming everywhere with agents\n The databasereceives multipleapplication requests.Contention occurs due to a\nlimited number of availableconnections, causing waitsand queuing of requests.The number\nof concurrentrequests isreduced.\nBottleneckRequest\nRequest\nRequestRequest\nRequest\nRequest\nRequest\nFigure 11.7  A large number of concurrent requests to access the database are reduced due to the \nlimited size of the connection pool. \n The databasereceives multipleapplication requests.The agent (MailboxP rocessor) buffers\nand enqueues incoming requests, ensuringgranular controller over the databaseconnection pool.The number of\nconcurrent requests\nis tamed.\nRequest\nRequest\nRequestRequest\nRequest\nRequest\nRequestAgentThrottling request s\nBehavior\nStateMailbox\nFigure 11.8  The agent ( MailboxProcessor ) controls the incoming requests to optimize the \ndatabase connection-pool use.\nListing 11.3 shows a fully asynchronous function in F#. This function queries a given database and encapsulates the query in a \nMailboxProcessor  body. Encapsulating \nan operation as behavior of an agent assures only one database request at a time is processed.\nTIP  One obvious solution to handling a higher number of requests is to set the \ndatabase’s connection pool size to the maximum, but this isn’t a good practice. Often, your application isn’t the only client connected to the database, and if your application takes up all the connections, then the database server can’t perform as expected.\n \n 343 Avoiding database bottlenecks with F# MailboxProcessor\nTo access the database, use the traditional .NET Access-Data-Object (ADO). Alterna-tively, you could use Microsoft Entity Framework or any other data access you choose. I don’t cover how to access the Entity Framework data access component in this book. For more detail, refer to the MSDN online documentation at http:/ /mng.bz/4sdU.\nListing 11.3  Using MailboxProcessor  to manage database calls\ntype Person  =     { id:int; firstName:string; lastName:string; age:int } type SqlMessage =     | Command of id:int * AsyncReplyChannel<Person option>   let agentSql connectionString =    fun (inbox: MailboxProcessor<SqlMessage>) ->        let rec loop() = async {                            let! Command(id, reply) = inbox.Receive()                 use conn = new SqlConnection(connectionString)                use cmd = new SqlCommand(""Select FirstName, LastName, Age \n➥ from db.People where id = @id"")\n                cmd.Connection <- conn                cmd.CommandType <- CommandType.Text                cmd.Parameters.Add(""@id"", SqlDbType.Int).Value <- id                    if conn.State <> ConnectionState.Open then                     do! conn.OpenAsync()                    use! reader = cmd.ExecuteReaderAsync(  CommandBehavior.SingleResult ||| CommandBehavior.CloseConnection)                 let! canRead = (reader:SqlDataReader).ReadAsync()                  if canRead then                    let person =                         {   id = reader.GetInt32(0)                             firstName = reader.GetString(1)                            lastName = reader.GetString(2)                            age = reader.GetInt32(3)  }                    reply.Reply(Some person)                    else reply.Reply(None)                    return! loop() }            loop()                    type AgentSql(connectionString:string) =        let agentSql = new MailboxProcessor<SqlMessage>                                              (agentSql connectionString)        member this.ExecuteAsync (id:int) =             agentSql.PostAndAsyncReply(fun ch -> Command(id, ch))         member this.ExecuteTask (id:int) =             agentSql.PostAndAsyncReply(fun ch -> Command(id, ch))             |> Async.StartAsTask             Uses a Person record type Uses a single-case DU for defining the \nMailboxProcessor message\nDeconstructs pattern-matching \nagainst the message received to \naccess the underlying values\nOpens the SQL connection \nasynchronously using the \ndo! asynchronous \nworkflow operator\nAsynchronously creates an \nSQL reader instance\nIf the SQL command can run, it \nreplies to the caller with the \nSome result of the operation. \nIf the SQL command can’t run, it replies to the caller with a None result.\nExposes an API to interact with \nencapsulated MailboxProcessor",10695
115-11.5.4 Parallelizing the workflow with group coordination of agents.pdf,115-11.5.4 Parallelizing the workflow with group coordination of agents,"344 chapter  11 Applying reactive programming everywhere with agents\nInitially, the Person  data structure is defined as a record type, which can be con-\nsumed easily as an immutable class by any .NET programming language. The function \nagentSql  defines the body of a MailboxProcessor , whose behavior receives messages \nand performs database queries asynchronously. You make your application more robust by using an \nOption  type for the Person value, which would otherwise be null . \nDoing so helps prevent null  reference exceptions. \nThe type AgentSql encapsulates the MailboxProcessor , which originated from run-\nning the function agentSql . The access of the underlying agent is exposed through the \nmethods ExecuteAsync and ExecuteTask . \nThe purpose of the ExecuteTask  method is to encourage interoperability with C#. \nYou can compile the AgentSql  type into an F# library and distribute it as a reusable \ncomponent. If you want to use the component from C#, then you should also provide methods that return a type \nTask or Task<T> for the F# functions that run an asynchro-\nnous workflow object ( Async<'T> ). How to interop between F# Async  and .NET Task \ntypes is covered in appendix C.\n11.5.1 The MailboxProcessor message type: discriminated unions\nThe type SqlMessage Command  is a single-case DU used to send a message to the Mailbox-\nProcessor  with a well-defined type, which can be pattern-matched:\n    type SqlMessage =        | Command of id:int * AsyncReplyChannel<Person option>\nA common F# practice is to use a DU to define the different types of messages that a \nMailboxProcessor  can receive and pattern match them to deconstruct and obtain \nthe underlying data structure (for more on F#, see appendix B). Pattern matching over DUs gives a succinct way to process messages. A common pattern is to call \ninbox \n.Receive()  or inbox.TryReceive()  and follow that call with a match on the message \ncontents.\nPerformance tip for an F# single-case DU\nUsing single-case DU types (as in listing 11.3) to wrap primitive values is an effective design. But because union cases are compiled into classes, expect a performance drop. This performance drop is the trade-off for allocating, and later for collecting, the class by the GC. A better solution (available since F# 4.1) is to decorate the DUs with the \nStruct  \nattribute, allowing the compiler to treat these types as values, avoiding the extra heap allocation and GC pressure.\n \nUsing strongly typed messages makes it possible for the MailboxProcessor  behavior to \ndistinguish between different types of messages and to supply different handling codes associated with each type of message. \n \n 345 Avoiding database bottlenecks with F# MailboxProcessor\n11.5.2 MailboxProcessor two-way communication\nIn listing 11.3, the underlying MailboxProcessor  returns (replies) to the caller the \nresult of the database query in the shape of a Person  option type. This communication \nuses the AsyncReplyChannel<'T>  type, which defines the mechanism used to reply to \nthe channel parameter established during message initialization (figure 11.9). \nBehavior\n(message processor)Mailbox\nqueueStateAgentMessages\nPostAndAsyncReply\nAsyncReplyChannelMain thread\nFigure 11.9  The agent two-way communication generates an AsyncReplyChannel , which is used \nby the agent as a callback to notify the caller when the computation is completed, generally supplying a result.\nThe code that can wait asynchronously for a response uses the AsyncReplyChannel . \nOnce the computation is complete, use the Reply  function to return the results from \nthe mailbox:\n    type SqlMessage =        | Command of id:int * AsyncReplyChannel<Person option>        member this.ExecuteAsync (id:int) =             agentSql.PostAndAsyncReply(fun ch -> Command(id, ch)) \nThe PostAndAsyncReply  method initializes the channel for the Reply  logic, which \nhands off the reply channel to the agent as part of the message using an anonymous lambda (function). At this point, the workflow is suspended (without blocking) until the operation completes and a \nReply , carrying the result, is sent back to the caller by \nthe agent through the channel:\nreply.Reply(Some person)\nAs good practice, you should embed the AsyncReplyChannel  handler inside the \nmessage itself, as shown in the DU SqlMessage.Command of id:int *  AsyncReply-\nChannel<Person option> , because the reply of the sent message can be easily enforced \nby the compiler.\n \n346 chapter  11 Applying reactive programming everywhere with agents\nYou might be thinking: Why would you use a MailboxProcessor  to handle multiple \nrequests if only one message at a time can be processed? Are the incoming messages lost if the \nMailboxProcessor  is busy?\nSending messages to a MailboxProcessor  is always non-blocking; but from the \nagent’s perspective, receiving them is a blocking operation. Even if you’re posting mul-tiple messages to the agent, none of the messages will get lost, because they’re buffered and inserted into the mailbox queue. \nIt’s also possible to implement selective receive semantics to target and scan (http:/ /\nmng.bz/1lJr) for exact message types, and, depending on the agent behavior, the han-dler can wait for a specific message in the mailbox and temporarily defer others. This is a technique used to implement a finite-state machine with pause-and-resume capabilities.\n11.5.3 Consuming the AgentSQL from C#\nAt this point, you want to employ the AgentSql  so it can be consumed by other lan-\nguages. The exposed APIs are both C# Task  and F# asynchronous workflow friendly.\nUsing C#, it’s simple to employ AgentSql . After referencing the F# library contain-\ning the AgentSql , you can create an instance of the object and then call the Execute-\nTask  method: \nAgentSql agentSql = new AgentSql(""<< ConnectionString Here >>"");Person person = await agentSql.ExecuteTask(42);Console.WriteLine($""Fullname {person.FirstName} {person.LastName}"");\nExecuteTask  reruns a Task<Person> , so you can use the C# async/await model to \nextract the underlying value when the operation completes as a continuation.\nYou can use a similar approach in F#, an approach that supports the task-based pro-\ngramming model, although due to the intrinsic and superior support for the async workflow, I recommend that you use the \nExecuteAsync  method. In this case, you can \neither call the method inside an async computation expression, or call it by using the \nAsync.StartWithContinuations  function. With this function, a continuation handler \ncan continue the work when the AgentSql  replies with the result (see chapter 9). The \nfollowing listing is an example using both F# approaches (the code to note is in bold).\nListing 11.4  Interacting asynchronously with AgentSql\nlet token = CancellationToken()    let agentSql = AgentSql(""< Connection String Here >"")let printPersonName id = async {     let! (Some person) = agentSql.ExecuteAsync id              printfn ""Fullname %s %s"" person.firstName person.lastName}Async.Start(printPersonName 42, token)    Stops the MailboxProcessor with a cancellation token\nSends the message and waits \nasynchronously for the response \nfrom the MailboxProcessorStarts the computation asynchronously\n \n 347 Avoiding database bottlenecks with F# MailboxProcessor\nAsync.StartWithContinuations(agentSql.ExecuteAsync 42,       (fun (Some person) ->        printfn ""Fullname %s %s"" person.firstName person.lastName),      (fun exn -> printfn ""Error: %s"" exn.Message),      (fun cnl -> printfn ""Operation cancelled""), token)    \nThe Async.StartWithContinuations  function specifies the code to run when the job \ncompletes as a continuation. Async.StartWithContinuations  accepts three different \ncontinuation functions that are triggered with the output of the operation: \n¡ The code to run when the operation completes successfully, and a result is available.\n¡ The code to run when an exception occurs.\n¡ The code to run when an operation is canceled. The cancellation token is passed as an optional argument when you start the job. \nSee chapter 9 or the MSDN documentation online for more information (http:/ /mng .bz/teA8). \nAsync.StartWithContinuations  isn’t complicated, and it provides a conve-\nnient control over dispatching behaviors in the case of success, error, or cancellation. These functions passed are referred to as continuation functions. Continuation func-tions can be specified as a lambda expression in the arguments to \nAsync.StartWith-\nContinuations . Specifying code to run as a simple lambda expression is extremely \npowerful.\n11.5.4 Parallelizing the workflow with group coordination of agents\nThe main reason to have an agent process the messages to access a database is to con-trol the throughput and to properly optimize the use of the connection pool. How can you achieve this fine control of parallelism? How can a system perform multi-ple requests in parallel without encountering a decrease in performance? \nMailbox-\nProcessor  is a primitive type that’s flexible for building reusable components by \nencapsulating behavior and then exposing general or tailored interfaces that fit your program needs.\nListing 11.5 shows a reusable component, \nparallelWorker  (in bold), that spawns \na set of agents from a given count ( workers ). Here, each agent implements the same \nbehavior and processes the incoming requests in a round-robin fashion. Round-robin is an algorithm that, in this case, is employed by the agent mailbox queue to process the incoming messages as first-come first-served, in circular order, handling all processes without particular priority.\nListing 11.5  Parallel MailboxProcessor  workers\ntype MailboxProcessor<'a> with      static member public parallelWorker (workers:int)                 (behavior:MailboxProcessor<'a> -> Async<unit>)    Starts the computation, asynchronously managing how the operation completes Functions are triggered respectively if the operation completes successfully, completes with an error, or is canceled \nThe workers value defines the \nagents that run in parallel.Behavior to construct the underlying agent children\n \n348 chapter  11 Applying reactive programming everywhere with agents\n             (?errorHandler:exn -> unit) (?cts:CancellationToken) =         let cts = defaultArg cts (CancellationToken())                    let errorHandler = defaultArg errorHandler ignore              let agent = new MailboxProcessor<'a>((fun inbox ->            let agents = Array.init workers (fun _ ->                        let child = MailboxProcessor.Start(behavior, cts)                    child.Error.Subscribe(errorHandler)                        child)            cts.Register(fun () -> agents |> Array.iter(                                fun a -> (a :> IDisposable).Dispose()))             let rec loop i = async {                let! msg = inbox.Receive()                agents.[i].Post(msg)                    return! loop((i+1) % workers)            }            loop 0), cts)        agent.Start()\nThe main agent ( agentCoordinator ) initializes a collection of sub-agents to coordinate \nthe work and to provide access to the agent’s children through itself. When the parent agent receives a message sent to the \nparallelWorker  MailboxProcessor , the parent \nagent dispatches the message to the next available agent child (figure 11.10). \n \nAgent\nBehaviorAgent workerAgent workerAgent worker\nAgent workerAgent workerAgent worker\nStateMailbox MessagesPush messages to\nthe agent workers ina round-robin fashion.\nFigure 11.10  The parallel worker agent receives the messages that are sent to the children’s agents in \na round-robin fashion to compute the work in parallel.\nThe parallelWorker  function uses a feature called type extensions ( http:/ /mng.bz/Z5q9) \nto attach a behavior to the MailboxProcessor  type. The type extension is similar to an \nextension method. With this type extension, you can call the parallelWorker  function \nusing dot notation; as a result, the parallelWorker  function can be used and called by \nany other .NET programming language, keeping its implementation hidden.If the cancellation token or error handler \nisn’t passed, a default is created.\nInitializes the \nagent children \nThe error handler is \nsubscribed for each agent.Registers the cancellation token function, which stops and disposes of all the agentsSends the message to the agents in a round-robin fashion using a loop",12567
116-11.5.8 Caching operations with an agent.pdf,116-11.5.8 Caching operations with an agent,"349 Avoiding database bottlenecks with F# MailboxProcessor\nThe arguments of this function are as follows:\n¡ workers —The number of parallel agents to initialize.\n¡ behavior —The function to identically implement the underlying agents.\n¡ errorHandler —The function that each child agent subscribes to, to handle \neventual errors. This is an optional argument and can be omitted. In this case, an ignore function is passed.\n¡ cts—A cancellation token used to stop and dispose of all the children’s agents. \nIf a cancellation token isn’t passed as an argument, a default is initialized and passed into the agent constructor.\n11.5.5 How to handle errors with F# MailboxProcessor\nInternally, the parallelWorker  function creates an instance of the Mailbox Processor  \nagent, which is the parent coordinator of the agent’s array (children), equaling in number the value of the \nworkers  argument:\nlet agents = Array.init workers (fun _ ->                  let child = MailboxProcessor.Start(behavior, cts)                 child.Error.Subscribe(errorHandler)                     child)\nDuring the initialization phase, each agent child subscribes to its error event using the function \nerrorHandler . In the case of an exception thrown from the body of a \nMailbox Processor , the error event triggers and applies the function subscribed. \nDetecting and notifying the system in case of errors is essential in agent-based pro-\ngramming because it applies logic to react accordingly. The MailboxProcessor  has \nbuilt-in functionality for detecting and forwarding errors. \nWhen an uncaught error occurs in a MailboxProcessor  agent, the  agent raises the \nerror event: \nlet child = MailboxProcessor.Start(behavior, cts)child.Error.Subscribe(errorHandler)\nTo manage the error, you can register a callback function to the event handler. It’s common practice to forward the errors to a supervising agent. For example, here a simple supervisor agent displays the error received:\nlet supervisor = Agent<System.Exception>.Start(fun inbox ->    async { while true do                let! err = inbox.Receive()                printfn ""an error occurred in an agent: %A"" err })\nYou can define the error handler function that’s passed as an argument to initialize all the agent children:\nlet handler = fun error -> supervisor.Post errorlet agents = Array.init workers (fun _ ->                    let child = MailboxProcessor.Start(behavior, cts)                   child.Error.Subscribe(errorHandler)                   child)\n \n350 chapter  11 Applying reactive programming everywhere with agents\nIn critical application components, such as server-side requests represented as agents, you should plan to use the \nMailboxProcessor  to handle errors gracefully and restart \nthe application appropriately. \nTo facilitate error handling by notifying a supervisor agent, it’s convenient to define \na helper function:\nmodule Agent =    let withSupervisor (supervisor: Agent<exn>) (agent: Agent<_>) =       agent.Error.Subscribe(fun error -> supervisor.Post error); agent\nwithSupervisor  abstracts the registration for error handling in a reusable component. \nUsing this helper function, you can rewrite the previous portion of code that registers error handling for the \nparallelWorker , as shown here: \nlet supervisor = Agent<System.Exception>.Start(fun inbox -> async {                        while true do                            let! error = inbox.Receive()                           errorHandler error })let agent = new MailboxProcessor<'a>((fun inbox ->let agents = Array.init workers (fun _ ->                         MailboxProcessor.Start(behavior)                       |> withSupervisor supervisor)\nThe parallelWorker  encapsulates the agent supervisor, which uses the errorHandler  \nfunction as constructor behavior to handle the error messages from the children’s agent.\n11.5.6 Stopping MailboxProcessor agents—CancellationToken\nTo instantiate the children’s agent, use the MailboxProcessor  constructor that takes \na function parameter as a behavior of the agent, and takes as a second argument a \nCancellationToken  object. CancellationToken  registers a function to dispose and \nstop all the agents running. This function is executed when CancellationToken  is \ncanceled: \ncts.Register(fun () ->        agents |> Array.iter(fun a -> (a :> IDisposable).Dispose()))\nEach child in the MailboxProcessor  part of the parallelWorker  agent, when running, \nis represented by an asynchronous operation associated with a given Cancellation-\nToken . Cancellation tokens are convenient when there are multiple agents that depend \non each other, and you want to cancel all of them at once, similar to our example. \nA further implementation is to encapsulate the MailboxProcessor  agent into a \ndisposable: \ntype AgentDisposable<'T>(f:MailboxProcessor<'T> -> Async<unit>,                          ?cancelToken:CancellationTokenSource) =   let cancelToken = defaultArg cancelToken (new CancellationTokenSource())           let agent = MailboxProcessor.Start(f, cancelToken.Token)      member x.Agent = agent   interface IDisposable with       member x.Dispose() = (agent :> IDisposable).Dispose()                            cancelToken.Cancel())\n \n 351 Avoiding database bottlenecks with F# MailboxProcessor\nIn this way, the AgentDisposable  facilitates the cancellation and the memory deallo-\ncation (Dispose ) of the underlying MailboxProcessor  by calling the Dispose  method \nfrom the IDisposable  interface.\nUsing the AgentDisposable , you can rewrite the previous portion of code that regis-\nters the cancellation of the children’s agent for parallelWorker :\nlet agents = Array.init workers (fun _ ->                  new AgentDisposable<'a>(behavior, cancelToken)                |> withSupervisor supervisor)thisletCancelToken.Register(fun () ->            agents |> Array.iter(fun agent -> agent. Dispose())\nWhen the cancellation token thisletCancelToken  is triggered, the Dispose  method \nof all the children’s agents is called, causing them to stop. You can find the full imple-mentation of the refactored \nparallelWorker  in this book’s source code.\n11.5.7 Distributing the work with MailboxProcessor\nThe rest of the code is self-explanatory. When a message is posted to the parallel -\nWorker , the parent agent picks it up and forwards it to the first agent in line. The par -\nent agent uses a recursive loop to maintain the state of the last agent served by index. During each iteration, the index is increased to deliver the following available message to the next agent:\n let rec loop i = async {        let! msg = inbox.Receive()        agents.[i].Post(msg)        return! loop((i+1) % workers) }\nYou can use the parallelWorker  component in a wide range of cases. For the previous \nAgentSql  code example, you applied the parallelWorker  extension to reach the orig-\ninal goal of having control (management) over the number of parallel requests that can access the database server to optimize connection-pool consumption.\nListing 11.6  Using parallelWorker  to parallelize database reads \nlet connectionString =       ConfigurationManager.ConnectionStrings.[""DbConnection""].ConnectionStringlet maxOpenConnection = 10    let agentParallelRequests =      MailboxProcessor<SqlMessage>.parallelWorker(maxOpenConnection,                                                  agentSql connectionString)let fetchPeopleAsync (ids:int list) =     let asyncOperation =    Retrieves the connection string from the configuration Sets an arbitrary value for the maximum database connections opened concurrently \nCreates an instance of parallelWorker with \nan agent for connection \nUses a bulk operation to retrieve a range of IDs from the database\n \n352 chapter  11 Applying reactive programming everywhere with agents\n            ids            |> Seq.map (fun id -> agentParallelRequests.PostAndAsyncReply(                                                fun ch -> Command(id, ch)))            |> Async.Parallel        Async.StartWithContinuations(asyncOperation,              (fun people -> people |> Array.choose id                              |> Array.iter(fun person ->               printfn ""Fullname %s %s"" person.firstName person.lastName)),               (fun exn -> printfn ""Error: %s"" exn.Message),               (fun cnl -> printfn ""Operation cancelled""))\nIn this example the maximum number of open connections is arbitrary, but in a real case, this value varies. In this code, you first create the \nMailboxProcessor  agent-\nParallelRequests , which runs in parallel with the maxOpenConnection  number of \nagents. The function fetchPeopleAsync  is the final piece to glue together all the parts. \nThe argument passed into this function is a list of people IDs to fetch from the data-base. Internally, the function applies the \nagentParallelRequests  agent for each of \nthe IDs to generate a collection of asynchronous operations that will run in parallel using the \nAsync.Parallel  function. \nNOTE  To access a database in an asynchronous and parallel fashion, it’s best to \ncontrol and prioritize the read/write operation. Databases work best with a sin-gle writer at a time. In the previous example, all the operations are read, so the problem doesn’t exist. In chapter 13, as part of the real-world recipes, there’s a version of \nMailboxProcessor  parallelWorker  that prioritizes one write and \nmultiple reads in parallel.\nIn the example, the people IDs are retrieved in parallel; a more efficient way is to cre-ate an \nSqlCommand  that fetches the data in one database round trip. But the purpose of \nthe example still stands. The level of parallelism is controlled by the number of agents. This is an effective technique. In this book’s source code, you can find a complete and enhanced production-ready \nparallelWorker  component that you can reuse in your \ndaily work.\n11.5.8 Caching operations with an agent\nIn the previous section, you used the F# MailboxProcessor  to implement a perfor -\nmant and asynchronous database access agent, which could control the throughput of parallel operations. To take this a step further to improve the response time (speed) for the incoming requests, you can reduce the actual number of database queries. This is possible with the introduction of a database cache in your program. There’s no rea-son why a single query should be executed more than once per request if the result won’t change. By applying smart caching strategies in database access, you can unlock a significant increase in performance. Let’s implement an agent-based reusable cache component, which then can be linked to the \nagentParallelRequests  agent. \n \n 353 Avoiding database bottlenecks with F# MailboxProcessor\nThe cache agent’s objective is to isolate and store the state of the application while \nhandling the messages to read or update this state. This listing shows the implementa-tion of the \nMailboxProcessor  CacheAgent .\nListing 11.7  Cache agent using the MailboxProcessor\ntype CacheMessage<'Key> =    | GetOrSet of 'Key * AsyncReplyChannel<obj>    | UpdateFactory of Func<'Key,obj>    | Clear    type Cache<'Key when 'Key : comparison>     (factory : Func<'Key, obj>,  ?timeToLive : int) =        let timeToLive = defaultArg timeToLive 1000    let expiry = TimeSpan.FromMilliseconds (float timeToLive)        let cacheAgent = Agent.Start(fun inbox ->        let cache = Dictionary<'Key, (obj * DateTime)>( \n➥ HashIdentity.Structural)    \n            let rec loop (factory:Func<'Key, obj>) = async {                let! msg = inbox.TryReceive timeToLive                    match msg with                | Some (GetOrSet (key, channel)) ->                    match cache.TryGetValue(key) with                        | true, (v,dt) when DateTime.Now - dt < expiry ->                         channel.Reply v                        return! loop factory                    | _ ->                        let value = factory.Invoke(key)                            channel.Reply value                        cache.Add(key, (value, DateTime.Now))                        return! loop factory                | Some(UpdateFactory newFactory) ->                        return! loop (newFactory)                | Some(Clear) ->                    cache.Clear()                    return! loop factory                | None ->                    cache                     |> Seq.filter(function KeyValue(k,(_, dt)) ->                                                 DateTime.Now - dt > expiry)                    |> Seq.iter(function KeyValue(k, _) ->                                                 cache.Remove(k)|> ignore)                    return! loop factory }            loop factory )    member this.TryGet<'a>(key : 'Key) = async {        let! item = cacheAgent.PostAndAsyncReply(                                   fun channel -> GetOrSet(key, channel))Uses a DU to define the message type that the MailboxProcessor handles\nThe constructor takes a factory function for changing the agent’s behavior at runtime. \nSets the time-to-live \ntimeout for the \ncache invalidation\nUses an internal lookup state for caching\nWaits asynchronously for a message \nuntil the timeout expires. If the timeout \nexpired, the cache runs a cleanup.Tries to get a value from the \ncache; if it can’t get the value, \nit creates a new one using the \nfactory function. Then it sends \nthe value to the caller.\nUpdates the factory function \nWhen the value is retrieved from the cache agent, it validates the types and returns Some if successful; otherwise it returns None. \n \n354 chapter  11 Applying reactive programming everywhere with agents\n        match item with        | :? 'a as v -> return Some v        | _ -> return None  }   member this.GetOrSetTask (key : 'Key) =        cacheAgent.PostAndAsyncReply(fun channel -> GetOrSet(key, channel))        |> Async.StartAsTask  \n    member this.UpdateFactory(factory:Func<'Key, obj>) =        cacheAgent.Post(UpdateFactory(factory))    \nIn this example, the first type, CacheMessage , is the definition of the message that is \nsent to the MailboxProcessor  in the form DUs. This DU determines the valid mes-\nsages to send to the cache agent. \nNOTE  At this point in the book, DUs are not a new topic, but it’s worth mentioning \nthat they are a mighty tool in combination with the MailboxProcessor , because \nthey allow each defined type to contain a different signature. Consequently, they provide the ability to specify related groups of types and message contracts that are used to select and to branch into different reactions of the agent. \nThe core of the \nCacheAgent  implementation is to initialize and immediately start a \nMailboxProcessor  that constantly watches for incoming messages.\nThe constructs of F# make it easy to use lexical scoping to achieve isolation within \nasynchronous agents. This agent code uses the standard and mutable .NET dictionary collection to maintain the state originated from the different messages sent to an agent: \nlet cache = Dictionary<'Key, (obj * DateTime)>()\nThe internal dictionary is lexically private to the asynchronous agent, and no ability to read/write to the dictionary is made available other than to the agent. The mutable state in the dictionary is isolated. The agent function is defined as a recursive function loop that takes a single parameter factory, as shown here:\nAgent.Start(fun inbox ->        let rec loop (factory:Func<'Key, obj>) = async { ... }\nThe factory function represents the initialization policy to create and add an item when it isn’t found by the \ncacheAgent  in the local state cache. This factory function \nis continuously passed into the recursive function loop for state management, which allows you to swap the initialization procedure at runtime. In the case of caching the \nAgentSql  requests, if the database or the system goes offline, then the response strat-\negy can change. This is easily achieved by sending a message to the agent.\nThe agent receives the message semantic of the MailboxProcessor , which has a time-\nout to specify the expiration time. This is particularly useful for caching components to provoke a data invalidation, and then a data refresh:\nlet! msg = inbox.TryReceive timeToLiveExposes member for friendly \ncompatibility with C#\nUpdates the factory function \n \n 355 Avoiding database bottlenecks with F# MailboxProcessor\nThe TryReceive  of the inbox function returns a message option type, which can be \neither Some , when a message is received before the time timeToLive elapses, or None  \nwhen no message is received during the timeToLive  time: \n| None ->   cache   |> Seq.filter(function KeyValue(k,(_, dt)) -> DateTime.Now - dt > expiry)  |> Seq.iter(function KeyValue(k, _) -> cache.Remove(k) |> ignore)\nIn this case, when the timeout expires, the agent auto-refreshes the cached data by automatically invalidating (removing) all the cache items that expired. But if a mes-sage is received, the agent uses pattern matching to determine the message type so that the appropriate processing can be done. Here’s the range of capabilities for incoming messages:\n¡ GetOrSet —In this case, the agent searches the cache dictionary for an entry that \ncontains the specified key. If the agent finds the key and the invalidation time isn’t expired, then it returns the associated value. Otherwise, if the agent doesn’t find the key or the invalidation time is expired, then it applies the factory function to generate a new value, which is stored into the local cache in combination with the timestamp of its creation. The timestamp is used by the agent to verify the expira-tion time. Finally, the agent returns the result to the sender of the message:\n| Some (GetOrSet (key, channel)) ->                match cache.TryGetValue(key) with                | true, (v,dt) when DateTime.Now - dt < expiry ->                    channel.Reply v                    return! loop factory                | _ ->                    let value = factory.Invoke(key)                    channel.Reply value                    cache.Add(key, (value, DateTime.Now))                    return! loop factory\n¡ UpdateFactory —This message type, as already explained, allows the handler to \nswap the runtime initialization policy for the cache item: \n| Some(UpdateFactory newFactory) ->                 return! loop (newFactory)\n¡ Clear —This message type clears the cache to reload all items.\nUltimately, here’s the code that links the previous parallel AgentSql  agentParallel -\nRequests  to the CacheAgent :\nlet connectionString =      ConfigurationManager.ConnectionStrings.[""DbConnection""].ConnectionStringlet agentParallelRequests =      MailboxProcessor<SqlMessage>.parallelWorker(8, agentSql connectionString)let cacheAgentSql =    let ttl = 60000  \n \n356 chapter  11 Applying reactive programming everywhere with agents\n    CacheAgent<int>(fun id ->       agentParallelRequests.PostAndAsyncReply(fun ch -> Command(id, ch)), ttl)let person = cacheAgentSql.TryGet<Person> 42\nWhen the cacheAgentSql  agent receives the request, it checks whether the value 42 \nexists in the cache and if it’s expired. Otherwise, it interrogates the underlying parallel-\nWorker  to return the expected item and save it into the cache to speed up future requests \n(see figure 11.11).\n \nAgentparallelW orker\nBehaviorAgent workerAgent workerAgent worker\nAgent workerAgent workerAgent worker\nRequestsStateMailbox\nThe underlying agent workers\nare of type agentSql, which isused to access the database.\nBehavior\n(message processor)Mailbox\nqueueDatabase\nStateCacheAgentMessagesEach incoming request is processed asynchronouslyin the CacheAgent loop. If a value associated with the request (key) exists in the internal cache, it’s sent back to the caller\n. This ensures that the operation to compute the \nvalue isn ’t repeated. If the value isn ’t in the cache, the \noperation computes the value, adds it to the cache, and then sends the value back to the caller\n.\nFigure 11.11  The CacheAgent  maintains a local cache composed of key/value pairs, which associate \nan input (from a request) to a value. When a request arrives, the CacheAgent  verifies the existence \nof the input/key and then either returns the value (if the input/key already exists in the local cache) without running any computation, or it calculates the value to send to the caller. In the latter case, the value is also persisted in the local cache to avoid repeated computation for the same inputs.",20670
117-11.5.9 Reporting results from a MailboxProcessor.pdf,117-11.5.9 Reporting results from a MailboxProcessor,"357 Avoiding database bottlenecks with F# MailboxProcessor\n11.5.9 Reporting results from a MailboxProcessor\nSometimes, the MailboxProcessor  needs to report a state change to the system, where \na subscribed component is to handle the state change. For example, for the Cache-\nAgent  example to be more complete, you could extend it to include such features as \nnotification when data changes or when there’s a cache removal. \nBut how does a MailboxProcessor  report notifications to the outside system? This \nis accomplished by using events (listing 11.8). You’ve already seen how the Mailbox-\nProcessor  reports when an internal error occurs by triggering a notification to all of its \nsubscribers. You can apply the same design to report any other arbitrary events from the agent. Using the previous \nCacheAgent , let’s implement an event reporting that can be \nused to notify when data invalidation occurs. For the example, you’ll modify the agent for an auto-refresh that can be used to notify when data has changed (the code to note is in bold). \nNOTE  This notification pattern isn’t recommended in situations where the \nCacheAgent  handles many items, because, depending on the factory func-\ntion and the data to reload, the auto-refresh process could take more time to complete. \nListing 11.8  Cache with event notification for refreshed items\ntype Cache<'Key when 'Key : comparison>       (factory : Func<'Key, obj>,  ?timeToLive : int,          ?synchContext:SynchronizationContext) =    let timeToLive = defaultArg timeToLive 1000    let expiry = TimeSpan.FromMilliseconds (float timeToLive)    let cacheItemRefreshed = Event<('Key * 'obj)[]>()      let reportBatch items =            match synchContext with         | None -> cacheItemRefreshed.Trigger(items)          | Some ctx ->           ctx.Post((fun _ -> cacheItemRefreshed.Trigger(items)),null)     let cacheAgent = Agent.Start(fun inbox ->        let cache = Dictionary<'Key, (obj * \n➥ DateTime)>(HashIdentity.Structural)\n        let rec loop (factory:Func<'Key, obj>) = async {            let! msg = inbox.TryReceive timeToLive            match msg with            | Some (GetOrSet (key, channel)) ->                match cache.TryGetValue(key) with                | true, (v,dt) when DateTime.Now - dt < expiry ->                    channel.Reply v                    return! loop factory                | _ ->Uses an event to report a cache item refreshed, which indicates a change of state Triggers the event using the specified synchronization \ncontext, or directly if no synchronization context is specified\nNo synchronization context exists so it triggers as in the first case.\nUses the Post method of the \ncontext to trigger the event\n \n358 chapter  11 Applying reactive programming everywhere with agents\n                    let value = factory.Invoke(key)                    channel.Reply value                    reportBatch ([| (key, value) |])                        cache.Add(key, (value, DateTime.Now))                    return! loop factory            | Some(UpdateFactory newFactory) ->                return! loop (newFactory)            | Some(Clear) ->                cache.Clear()                return! loop factory            | None ->                cache                 |> Seq.choose(function KeyValue(k,(_, dt)) ->                         if DateTime.Now - dt > expiry then                             let value, dt = factory.Invoke(k), DateTime.Now                            cache.[k] <- (value,dt)                            Some (k, value)                        else None)                |> Seq.toArray                |> reportBatch                }        loop factory )    member this.TryGet<'a>(key : 'Key) = async {        let! item = cacheAgent.PostAndAsyncReply(                    fun channel -> GetOrSet(key, channel))        match item with        | :? 'a as v -> return Some v        | _ -> return None  }    member this.DataRefreshed = cacheItemRefreshed.Publish      member this.Clear() = cacheAgent.Post(Clear)\nIn this code, the event cacheItemRefreshed  channel dispatches the changes of state. \nBy default, F# events execute the handlers on the same thread on which they’re trig-gered. In this case, it uses the agent’s current thread. But depending on which thread originated the \nMailboxProcessor , the current thread can be either from the thread-\nPool  or coming from the UI thread, specifically from SynchronizationContext , a \nclass from System.Threading  that captures the current synchronization context. The \nlatter might be useful when the notification is triggered in response to an event that targets to update the UI. This is the reason the agent constructor, in the example, has the new parameter \nsynchContext , which is an option type that provides a convenient \nmechanism to control where the event is triggered.\nNOTE  The optional parameters in F# are written using the question mark ( ?) \nprefix syntax, ? synchContext , which passes the types as option values. \nThe Some ctx command means that the SynchronizationContext  isn’t null , and ctx \nis an arbitrary name given to access its value. When the synchronization context is Some \nctx, the reporting mechanism uses the Post  method to notify the state changes on the \nthread selected by the synchronization context. The method signature of the synchro-nization context \nctx.Post  takes a delegate and an argument used by the delegate. Triggers the event for \nthe refreshed items\nUses an event to report a cache item \nrefreshed, which indicates a change of state",5596
118-11.5.10 Using the thread pool to report events from MailboxProcessor.pdf,118-11.5.10 Using the thread pool to report events from MailboxProcessor,,0
119-11.6 F MailboxProcessor 10000 agents for a game of life.pdf,119-11.6 F MailboxProcessor 10000 agents for a game of life,"359 F# MailboxProcessor: 10,000 agents for a game of life\nAlthough the second argument isn’t required, null  is used as replacement. The func-\ntion reportBatch  triggers the event cacheItemRefreshed : \nthis.DataRefreshed.Add(printAgent.Post)\nIn the example, the change-of-state notification handler posts a message to a Mailbox-\nProcessor  to print a report in a thread-safe manner. But you could use the same idea \nin more complex scenarios, such as for updating a web page automatically with the most recent data using \nSignalR .\n11.5.10 Using the thread pool to report events from MailboxProcessor\nIn most cases, to avoid unnecessary overhead, it is preferable to trigger an event using the current thread. Still, there may be circumstances where a different thread-ing model could be better: for example, if triggering an event could block for a time or throw an exception that could kill the current process. A valid option is to trigger the event operating the thread pool to run the notification in a separate thread. The \nreportBatch  function can be refactored using the F# asynchronous workflow and the \nAsync.Start operator:\nlet reportBatch batch =  async { batchEvent.Trigger(batch) } |> Async.Start\nBe aware with this implementation, the code running on a thread pool cannot access UI elements.\n11.6 F# MailboxProcessor: 10,000 agents for a game of life\nMailboxProcessor , combined with asynchronous workflows, is a lightweight unit of \ncomputation (primitives), compared to threads. Agents can be spawned and destroyed with minimal overhead. You can distribute the work to various \nMailboxProcessor s, \nsimilar to how you might use threads, without having to incur the overhead associated with spinning up a new thread. For this reason, it’s completely feasible to create appli-cations that consist of hundreds of thousands of agents running in parallel with mini-mum impact to the computer resources.\nNOTE  In a 32-bit OS machine, you can create a little more than 1,300 threads \nbefore an out-of-memory exception is thrown. This limitation doesn’t apply to \nMailboxProcessor , which is backed up by the thread pool and isn’t directly \nmapped to a thread.\nIn this section, we use MailboxProcessor  from multiple instances by implementing \nthe Game of Life (https:/ /en.wikipedia.org/wiki/Game_of_Life). As described on Wikipedia, Life, as it is simply known, is a cellular automaton. It is a zero-player game, which means that once the game starts with a random initial configuration, it runs without any other input. This game consists of a collection of cells that run on a grid, each cell following a few mathematical rules. Cells can live, die, or multiply. Every cell interacts with its eight neighbors (the adjacent cells). A new state of the grid needs to be continually calculated to move the cells around to respect these rules.\n \n360 chapter  11 Applying reactive programming everywhere with agents\nThese are the Game of Life rules:\n¡ Each cell with one or no neighbors dies, as if by solitude.\n¡ Each cell with four or more neighbors dies, as if by overpopulation.\n¡ Each cell with two or three neighbors survives.\n¡ Each cell with three neighbors becomes populated.\nDepending on the initial conditions, the cells form patterns throughout the course of the game. The rules are applied repeatedly to create further generations until the cells reach a stable state (figure 11.12).\nAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nState\nAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nState\nAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nState\nAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nStateAgent\nBehavior\nState\nAgent\nBehavior\nStateAgent\nBehavior\nStateAgentCell connects and communicates with the\nsurrounding neighbor cells to verif y their state\nas dead or alive. The sur vival of the AgentCell\nis affected by these surrounding cells.\nFigure 11.12  When the Game of Life is set up, each cell (in the code example there are 100,000 cells) \nis constructed using an AgentCell MailboxProcessor . Each agent can be dead, a black circle, or \nalive depending the state of its neighbors.\nListing 11.9 is the implementation of the Game of Life cell, AgentCell , which is based \non the F# MailboxProcessor . Each agent cell communicates with the adjacent cells \nthrough asynchronous message passing, producing a fully parallelized Game of Life. For conciseness, and because they’re irrelevant for the main point of the example, I omitted a few parts of the code. You can find the full implementation in this book’s source code.\nListing 11.9  Game of Life with MailboxProcessor  as cells \ntype CellMessage =    | NeighborState of cell:AgentCell * isalive:bool    | State of cellstate:AgentCell    | Neighbors of cells:AgentCell list    | ResetCell    and State =    {   neighbors:AgentCell list        wasAlive:bool\nUses a DU that defines the message for the agent cell\n \n 361 F# MailboxProcessor: 10,000 agents for a game of life\n        isAlive:bool }        static member createDefault isAlive =        { neighbors=[]; isAlive=isAlive; wasAlive=false; }and AgentCell(location, alive, updateAgent:Agent<_>) as this =    let neighborStates = Dictionary<AgentCell, bool>()        let AgentCell =        Agent<CellMessage>.Start(fun inbox ->            let rec loop state = async {                let! msg = inbox.Receive()                match msg with                | ResetCell ->                   state.neighbors                    |> Seq.iter(fun cell -> cell.Send(State(this)))                     neighborStates.Clear()                   return! loop { state with wasAlive=state.isAlive }                 | Neighbors(neighbors) ->                   return! loop { state with neighbors=neighbors }                 | State(c) ->                    c.Send(NeighborState(this, state.wasAlive))                    return! loop state                | NeighborState(cell, alive) ->                    neighborStates.[cell] <- alive                    if neighborStates.Count = 8 then                           let aliveState =                           let numberOfneighborAlive =                               neighborStates                               |> Seq.filter(fun (KeyValue(_,v)) -> v)                               |> Seq.length                             match numberOfneighborAlive with                                | a when a > 3  || a < 2 -> false                            | 3 -> true                            | _ -> state.isAlive                       updateAgent.Post(Update(aliveState, location))                        return! loop { state with isAlive = aliveState }                    else return! loop state }            loop (State.createDefault alive ))    member this.Send(msg) = AgentCell.Post msg\nAgentCell  represents a cell in the grid of the Game of Life. The main concept is that \neach agent communicates with the neighbor cells about its (the agent’s) current state using asynchronous message passing. This pattern creates a chain of interconnected parallel communications that involves all the cells, which send their updated state to the \nupdateAgent  MailboxProcessor . At this point, the updateAgent  refreshes the \ngraphic in the UI. \nListing 11.10  updateAgent  that refreshes the WPF UI in real time \nlet updateAgent grid (ctx: SynchronizationContext) =   let gridProduct = grid.Width * grid.Height  let pixels = Array.zeroCreate<byte> (gridProduct)  Record type used to keep track of the state for each cell agent\nInternal state of each agent, to \nkeep track of the state of each \ncell agent’s neighbors\nNotifies all the cell’s \nneighbors of its current state\nRecursively maintains \na local stateUses an algorithm that updates \nthe current cell state according \nto the state of the neighbors\nRuns the rules of the \nGame of Life\nUpdates the agent that \nrefreshes the UI\nThe updateAgent constructor takes the captures that the SynchronizationContext used to update the WPF controller using the correct thread.\nArray of pixels used to render the state of the Game \nof Life. Each pixel represents a cell state. \n \n362 chapter  11 Applying reactive programming everywhere with agents\n  Agent<UpdateView>.Start(fun inbox ->    let gridState = Dictionary<Location, bool>(HashIdentity.Structural)    let rec loop () = async {         let! msg = inbox.Receive()         match msg with         | Update(alive, location, agent) ->                   agentStates.[location] <- alive                agent.Send(ResetCell)                          if agentStates.Count = gridProduct then                     agentStates.AsParallel().ForAll(fun s ->                     pixels.[s.Key.x+s.Key.y*grid.Width]                             <- if s.Value then 128uy else 0uy                  )                do! Async.SwitchToContext ctx                         image.Source <- createImage pixels                    do! Async.SwitchToThreadPool()                        agentStates.Clear()            return! loop()     }     loop())\nupdateAgent , as its name suggests, updates the state of each pixel with the correlated \ncell value received in the Update  message. The agent maintains the status of the pixels \nand uses that status to create a new image when all the cells have sent their new state. Next, \nupdateAgent  refreshes the graphical WPF UI with this new image, which rep-\nresents the current grid of the Game of Life:\ndo! Async.SwitchToContext ctx  image.Source <- createImage pixels    do! Async.SwitchToThreadPool()    \nIt’s important to note that updateAgent  agent uses the current synchronization con-\ntext to update the WPF controller correctly. The current thread is switched to the UI thread using the \nAsync .SwitchToContext  function (discussed in chapter 9).\nThe final piece of code to run the Game of Life generates a grid that acts as the play-\nground for the cells, and then a timer notifies the cells to update themselves (listing 11.11). In this example, the grid is a square of 100 cells per side, for a total of 10,000 cells (\nMailboxProcessor s) that run in parallel with a refresh timer of 50 ms, as shown \nin figure 11.13. There are 10,000 MailboxProcessor s communicating with each other \nand updating the UI 20 times every second (the code to note is in bold).\nListing 11.11  Creating the Game of Life grid and starting the timer to refresh \nlet run(ctx:SynchronizationContext) =     let size = 100         let grid = { Width= size; Height=size}         let updateAgent = updateAgent grid ctxShared grid state that represents the state \nof current generation of cells\nLists the Update message that updates the \nstate of the given cell and resets cell state\nWhen all cells notify that they’re updated, a new image representing the updated grid is generated to refresh the WPF UI application.The pixel of a related \ncell is updated with \nthe state alive (color) \nor dead (white).\nUpdates the UI using the correct \nthread passed from the constructor \nIndicates the size of each side of the grid Defines the grid using a record type with the accessible properties Width and Height\n \n 363 F# MailboxProcessor: 10,000 agents for a game of life\n     let cells = seq { for x = 0 to grid.Width - 1 do                           for y = 0 to grid.Height - 1 do                                   let agent = AgentCell({x=x;y=y},                                   alive=getRandomBool(),                                    updateAgent=updateAgent)                    yield (x,y), agent  } |> dict     let neighbours (x', y') =        seq {          for x = x' - 1 to x' + 1 do            for y = y' - 1 to y' + 1 do              if x <> x' || y <> y' then                 yield cells.[(x + grid.Width) % grid.Width,                              (y + grid.Height) % grid.Height]        } |> Seq.toList     cells.AsParallel().ForAll(fun pair ->         let cell = pair.Value        let neighbours = neighbours pair.Key        cell.Send(Neighbors(neighbours))         cell.Send(ResetCell)                  )\nThe notifications to all the cells  (agents) are sent in parallel using PLINQ. The cells  \nare an F# sequence that’s treated as a .NET IEnumerable , which allows an effortless \nintegration of LINQ/PLINQ.\nWhen the code runs, the program generates 10,000 F# MailboxProcessor s in less than \n1 ms with a memory consumption, specific for the agents, of less than 25 MB. Impressive!Generates a 100 x 100 grid, \ncreating one MailboaxProcessor \nper cell (for a total of 10,000 \nagents)\nNotifies all the cells in parallel about their neighbors and resets their state\nFigure 11.13  Game of Life. The GUI is a WPF application. \n \n364 chapter  11 Applying reactive programming everywhere with agents\nSummary\n¡ The agent programming model intrinsically promotes immutability and isola-tion for writing concurrent systems, so even complex systems are easier to reason about because the agents are encapsulated into active objects.\n¡ The Reactive Manifesto defines the properties to implement a reactive system, which is flexible, loosely coupled, and scalable.\n¡ Natural isolation is important for writing lockless concurrent code. In a multi-threaded program, isolation solves the problem of shared state by giving each thread a copied portion of data to perform local computation. When using isola-tion, there’s no race condition.\n¡ By being asynchronous, agents are lightweight, because they don’t block threads while waiting for a message. As a result, you can use hundreds of thousands of agents in a single application without any impact on the memory footprint.\n¡ The F# MailboxProcessor  allows two-way communication: the agent can use an \nasynchronous channel to return (reply) to the caller the result of a computation.\n¡ The agent programming model F# MailboxProcessor  is a great tool for solving \napplication bottleneck issues, such as multiple concurrent database accesses. In fact, you can use agents to speed up applications significantly and keep the server responsive.\n¡ Other .NET programming languages can consume the F# MailboxProcessor  by \nexposing the methods using the friendly TPL task-based programming model.",14412
120-12.2.3 Completing the work with ActionBlockTInput.pdf,120-12.2.3 Completing the work with ActionBlockTInput,"36512Parallel workflow and \nagent programming \nwith TPL Dataflow\nThis chapter covers\n¡ Using TPL Dataflow blocks \n¡ Constructing a highly concurrent workflow \n¡ Implementing a sophisticated Producer/Consumer pattern \n¡ Integrating Reactive Extensions with TPL Dataflow\nToday’s global market requires that businesses and industries be agile enough to respond to a constant flow of changing data. These workflows are frequently large, and sometimes infinite or unknown in size. Often, the data requires complex pro-cessing, leading to high throughput demands and potentially immense computa-tional loads. To cope with these requirements, the key is to use parallelism to exploit system resources and multiple cores. \nBut today’s .NET Framework’s concurrent programming models weren’t designed \nwith dataflow in mind. When designing a reactive application, it’s fundamental to build and treat the system components as units of work. These units react to mes-sages, which are propagated by other components in the chain of processing. These reactive models emphasize a push-based model for applications to work, rather than \n \n366 chapter  12 Parallel workflow and agent programming with TPL Dataflow\na pull-based model (see chapter 6). This push-based strategy ensures that the individual components are easy to test and link, and, most importantly, easy to understand. \nThis new focus on push-based constructions is changing how programmers design \napplications. A single task can quickly grow complex, and even simple-looking require-ments can lead to complicated code. \nIn this chapter, you’ll learn how the .NET Task Parallel Library Dataflow (TPL Data-\nflow, or TDF) helps you to tackle the complexity of developing modern systems with an API that builds on TAP. TDF fully supports asynchronous processing, in combina-tion with a powerful compositionality semantic and a better configuration mechanism than the TPL. TDF eases concurrent processing and implements tailored asynchronous parallel workflow and batch queuing. Furthermore, it facilitates the implementation of sophisticated patterns based on combining multiple components that talk to each other by passing messages.\n12.1 The power of TPL Dataflow\nLet’s say you’re building a sophisticated Producer/Consumer pattern that must sup-port multiple producers and/or multiple consumers in parallel, or perhaps it has to support workflows that can scale the different steps of the process independently. One solution is to exploit Microsoft TPL Dataflow. With the release of .NET 4.5, Microsoft introduced TPL Dataflow as part of the tool set for writing concurrent applications. TDF is designed with the higher-level constructs necessary to tackle easy parallel prob-lems while providing a simple-to-use, powerful framework for building asynchronous data-processing pipelines. TDF isn’t distributed as part of the .NET 4.5 Framework, so to access its API and classes, you need to import the official Microsoft NuGet Package (\ninstall-Package Microsoft.Tpl.DataFlow ).\nTDF offers a rich array of components (also called blocks) for composing dataflow \nand pipeline infrastructures based on the in-process message-passing semantic (see figure 12.1). This dataflow model promotes actor-based programming by providing in-process message passing for coarse-grained dataflow and pipelining tasks. \nTDF uses the task scheduler (\nTaskScheduler , http:/ /mng.bz/4N8F) of the TPL to \nefficiently manage the underlying threads and to support the TAP model (async/await) for optimized resource utilization. TDF increases the robustness of highly concurrent applications and obtains better performance for parallelizing CPU and I/O intensive operations, which have high throughput and low latency.\nNOTE  TPL Dataflow enables effective techniques for running embarrassingly \nparallel problems, explained in chapter 3, meaning there are many independent \ncomputations that can be executed in parallel in an evident way.\n \n 367 Designed to compose: TPL Dataflow blocks \nStep 1 Step 4Componentized workflow\nStep 3Step 2\nStep 5Step 8Step 6\nStep 7Notify\ncomplete\nFigure 12.1  Workflow composed by multiple steps. Each operation can be treated as an independent \ncomputation.\nThe concept behind the TPL Dataflow library is to ease the creation of multiple patterns, such as with batch-processing pipelines, parallel stream processing, data buffering, or joining and processing batch data from one or more sources. Each of these patterns can be used as a standalone, or may be composed with other patterns, enabling developers to easily express complex dataflow.  \n12.2 Designed to compose: TPL Dataflow blocks \nImagine you’re implementing a complex workflow process composed of many differ -\nent steps, such as a stock analysis pipeline. It’s ideal to split the computation in blocks, developing each block independently and then gluing them together. Making these blocks reusable and interchangeable enhances their convenience. This composable design would simplify the application of complex and convoluted systems. \nCompositionality is the main strength of TPL Dataflow, because its set of indepen-\ndent containers, known as blocks, is designed to be combined. These blocks can be a chain of different tasks to construct a parallel workflow, and are easily swapped, reordered, reused, or even removed. TDF emphasizes a component’s architectural approach to ease the restructure of the design. These dataflow components are useful when you have multiple operations that must communicate with one another asynchro-nously or when you want to process data as it becomes available, as shown in figure 12.2.\nTPL Dataflow workflow\nTransform AggregateProcess\ntask\nProcess\ntaskProcess\ntask\nBuffer\ninput dataTransformOutput\ndata\nFigure 12.2  TDF embraces the concepts of reusable components. In this figure, each step of the \nworkflow acts as reusable components. TDF brings a few core primitives that allow you to express computations based on Dataflow graphs.\n \n368 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nHere’s a high-level view of how TDF blocks operate: \n1 Each block receives and buffers data from one or more sources, including other blocks, in the form of messages. When a message is received, the block reacts by applying its behavior to the input, which then can be transformed and/or used to perform side effects. \n2 The output from the component (block) is then passed to the next linked block, and to the next one, if any, and so on, creating a pipeline structure.\nNOTE  The term reactive programming has been used for a long time to describe \ndataflow because the reaction is generated from receiving a piece of data.\nTDF excels at providing a set of configurable properties by which it’s possible, with small changes, to control the level of parallelism, manage the buffer size of the mail-box, and process data and dispatch the outputs.\nThere are three main types of dataflow blocks:\n¡ Source —Operates as producer of data.  It can also be read from. \n¡ Target —Acts as a consumer, which receives the data and can be written to. \n¡ Propagator —Acts as both a Source and a Target block.\nFor each of these dataflow blocks, TDF provides a set of subblocks, each with a differ -\nent purpose. It’s impossible to cover all the blocks in one chapter.  In the following sections we focus on the most common and versatile ones to adopt in general pipeline composition applications. \nTIP  TPL Dataflow’s most commonly used blocks are the standard Buffer-\nBlock , ActionBlock , and TransformBlock . Each is based on a delegate, which \ncan be in the form of anonymous function that defines the work to compute.  I recommend that you keep these anonymous methods short, simple to follow, and easier to maintain.\nFor more information about the Dataflow library, see the online MSDN documenta-tion (http:/ /mng.bz/GDbF).\n12.2.1 Using BufferBlock<TInput> as a FIFO buffer\nTDF BufferBlock<T>  acts as an unbounded buffer for data that’s stored in a first in, \nfirst out (FIFO) order (figure 12.3). In general, BufferBlock  is a great tool for enabling \nand implementing asynchronous Producer/Consumer patterns, where the internal message queue can be written to by multiple sources, or read from multiple targets. \n \n 369 Designed to compose: TPL Dataflow blocks \nInput OutputInternal buf fer TaskBufferBlock<T>\nFigure 12.3  The TDF BufferBlock  has an internal buffer where messages are queued, waiting to \nbe processed by the task. The input and output are the same types, and this block doesn’t apply any transformation on the data.\nHere is a simple Producer/Consumer using the TDF BufferBlock .\nListing 12.1  Producer/Consumer based on the TDF BufferBlock\nBufferBlock<int> buffer = new BufferBlock<int>(); async Task Producer(IEnumerable<int> values) {       foreach (var value in values)           buffer.Post(value);           buffer.Complete();         }async Task Consumer(Action<int> process){       while (await buffer.OutputAvailableAsync())                process(await buffer.ReceiveAsync());       }async Task Run(){        IEnumerable<int> range = Enumerable.Range(0,100);        await Task.WhenAll(Producer(range), Consumer(n =>                  Console.WriteLine($""value {n}"")));}\nThe items of the IEnumerable  values are sent through the buffer.Post  method \nto the BufferBlock  buffer, which retrieves them asynchronously using the buffer \n.ReceiveAsync  method. The OutputAvailableAsync  method knows when the next item \nis ready to be retrieved and makes the notification. This is important to protect the code from an exception; if the buffer tries to call the \nReceive  method after the block com-\npletes processing, an error is thrown. This BufferBlock  block essentially receives and \nstores data so that it can be dispatched to one or more other target blocks for processing. \n12.2.2 Transforming data with TransformBlock<TInput, TOutput>\nThe TDF TransformBlock<TInput,TOutput>  acts like a mapping function, which applies \na projection function to an input value and provides a correlated output (figure 12.4). The transformation function is passed as an argument in the form of a delegate \nFunc \n<TInput,TOutput> , which is generally expressed as a lambda expression. This block’s \ndefault behavior is to process one message at a time, maintaining strict FIFO ordering. Hands off through a bounded BufferBlock<T>\nSends a message to the BufferBlock\nNotifies the BufferBlock that there are no more items to process, and completes\nSignals when a new item is available to be retrieved \nReceives a message \nasynchronously\n \n370 chapter  12 Parallel workflow and agent programming with TPL Dataflow\n InputInput buf fer Output buf fer TaskTransformBlock<TInput, TOutput>\nOutput\nFigure 12.4  The TDF TransformBlock  has an internal buffer for both the input and output values; this \ntype of block has the same buffer capabilities as BufferBlock . The purpose of this block is to apply a \ntransformation function on the data; the Input  and Output  are likely different types.\nNote that TransformBlock<TInput,TOutput>  performs as the BufferBlock <TOutput> , \nwhich buffers both the input and output values. The underlying delegate can run synchronously or asynchronously. The asynchronous version has a type signature \nFunc <TInput,Task<TOutput>>  whose purpose it is to run the underlying function \nasynchronously. The block treats the process of that element as completed when the returned \nTask  appears terminated. This listing shows how to use the TransformBlock  \ntype (the code to note is in bold).\nListing 12.2  Downloading images using the TDF TransformBlock\nvar fetchImageFlag = new TransformBlock<string, (string, byte[])>(   async urlImage => {         using (var webClient = new WebClient()) {       byte[] data = await webClient.DownloadDataTaskAsync(urlImage);        return (urlImage, data);       }});List<string> urlFlags = new List<string>{     ""Italy#/media/File:Flag_of_Italy.svg"",     ""Spain#/media/File:Flag_of_Spain.svg"",     ""United_States#/media/File:Flag_of_the_United_States.svg""     };foreach (var urlFlag in urlFlags)    fetchImageFlag.Post($""https://en.wikipedia.org/wiki/{urlFlag}"");\nIn this example, the TransformBlock<string,(string, byte[])>  fetchImageFlag  \nblock fetches the flag image in a tuple string and byte array format. In this case, the output isn’t consumed anywhere, so the code isn’t too useful. You need another block to process the outcome in a meaningful way.\n12.2.3 Completing the work with ActionBlock<TInput >\nThe TDF ActionBlock  executes a given callback for any item sent to it. You can think \nof this  block logically as a buffer for data combined with a task for processing that data. Uses a lambda expression to process the urlImage asynchronously\nDownloads the flag image and \nreturns the relative byte arrayOutput consists of a tuple with the \nimage URL and the related byte array.\n \n 371 Designed to compose: TPL Dataflow blocks \nActionBlock<TInput>  is a target block that calls a delegate when it receives data, simi-\nlar to a for-each  loop (figure 12.5).\nInputInternal buf fer TaskActionBlock<TInput>\nFigure 12.5  The TDF ActionBlock  has an internal buffer for input messages that are queued if \nthe task is busy processing another message. This type of block has the same buffer capabilities as \nBufferBlock . The purpose of this block is to apply an action that completes the workflow without \noutput that likely produces side effects. In general, because ActionBlock  doesn’t have an output, it \ncannot compose to a following block, so it’s used to terminate the workflow.\nActionBlock<TInput>  is usually the last step in a TDF pipeline; in fact, it doesn’t pro-\nduce any output. This design prevents ActionBlock  from being combined with further \nblocks, unless it posts or sends the data to another block, making it the perfect candi-date to terminate the workflow process. For this reason, \nActionBlock  is likely to pro-\nduce side effects as a final step to complete the pipeline processing.\nThe following code shows the TransformBlock  from the previous listing pushing its \noutputs to the ActionBlock  to persist the flag images in the local filesystem (in bold).\nListing 12.3  Persisting data using the TDF ActionBlock\nvar saveData = new ActionBlock<(string, byte[])>(async data => {      (string urlImage, byte[] image) = data;            string filePath = urlImage.Substring(urlImage.IndexOf(""File:"") + 5);    await File.WriteAllBytesAsync(filePath, image);       });fetchImageFlag.LinkTo(saveData);       \nThe argument passed into the constructor during the instantiation of the Action-\nBlock  block can be either a delegate Action<TInput>  or Func<TInput,Task> . The \nlatter performs the internal action (behavior) asynchronously for each message input (received). Note that the \nActionBlock  has an internal buffer for the incoming data to \nbe processed, which works exactly like the BufferBlock . \nIt’s important to remember that the ActionBlock saveData  is linked to the previous \nTransformBlock fetchImageFlag  using the LinkTo  extension method. In this way, the \noutput produced by the TransformBlock  is pushed to the ActionBlock  as soon as available.Uses a lambda expression to process data asynchronously Deconstructs the tuple to \naccess underlying items\nWrites the data to the local \nfilesystem asynchronously\nLinks the output from the TransformBlock fetchImageFlag to the saveData ActionBlock",15626
121-12.2.4 Linking dataflow blocks.pdf,121-12.2.4 Linking dataflow blocks,,0
122-12.3.1 A multiple Producersingle Consumer pattern TPL Dataflow.pdf,122-12.3.1 A multiple Producersingle Consumer pattern TPL Dataflow,,0
123-12.3 Implementing a sophisticated ProducerConsumer with TDF.pdf,123-12.3 Implementing a sophisticated ProducerConsumer with TDF,"372 chapter  12 Parallel workflow and agent programming with TPL Dataflow\n12.2.4 Linking dataflow blocks\nTDF blocks can be linked with the help of the LinkTo  extension method. Linking data-\nflow blocks is a powerful technique for automatically transmitting the result of each computation between the connected blocks in a message-passing manner. The key component for building sophisticated pipelines in a declarative manner is to use con-necting blocks. If we look at the signature of the \nLinkTo  extension method from the \nconceptual point of view, it looks like a function composition:\nLinkTo: (a -> b) -> (b -> c) \n12.3 Implementing a sophisticated Producer/Consumer with TDF \nThe TDF programming model can be seen as a sophisticated Producer/Consumer pattern, because the blocks encourage a pipeline model of programming, with pro-ducers sending messages to decoupled consumers. These messages are passed asyn-chronously, maximizing throughput. This design provides the benefits of not blocking the producers, because the TDF blocks (queue) act as a buffer, eliminating waiting time. The synchronization access between producer and consumers may sound like an abstract problem, but it’s a common task in concurrent programming. You can view it as a design pattern for synchronizing two components.\n12.3.1 A multiple Producer/single Consumer pattern: TPL Dataflow\nThe Producer/Consumer pattern is one of the most widely used patterns in parallel pro-gramming. Developers use it to isolate work to be executed from the processing of that work. In a typical Producer/Consumer pattern, at least two separated threads run concur -\nrently: one produces and pushes the data to process into a queue, and the other verifies the presence of the new incoming piece of data and processes it. The queue that holds the tasks is shared among these threads, which requires care for accessing tasks safely. TDF is a great tool for implementing this pattern, because it has intrinsic support for multiple readers and multiple writers concurrently, and it encourages a pipeline pattern of programming with producers sending messages to decoupled consumers (figure 12.6).\nInput OutputInternal buf fer TaskBufferBlock<T>\nProducer B ConsumerProducer A\nProducer CDataData\nDataData\nFigure 12.6  Multiple-producers/one-consumer pattern using the TDF BufferBlock , which can \nmanage and throttle the pressure of multiple producers\n \n 373 Implementing a sophisticated Producer/Consumer with TDF \nIn the case of a multiple-Producer/single-Consumer pattern, it’s important to enforce a restriction between the number of items generated and the number of items con-sumed. This constraint aims to balance the work between the producers when the con-sumer cannot handle the load. This technique is called throttling. Throttling protects the program from running out of memory if the producers are faster than the con-sumer. Fortunately, TDF has built-in support for throttling, which is achieved by setting the maximum size of the buffer through the property \nBoundedCapacity , part of the \nDataFlowBlockOptions . In listing 12.4, this property ensures that there will never be \nmore than 10 items in the BufferBlock  queue. Also, in combination with enforcing \nthe limit of the buffer size, it’s important to use the function SendAsync , which waits \nwithout blocking for the buffer to have available space to place a new item.\nListing 12.4  Asynchronous Producer/Consumer using TDF\nBufferBlock<int> buffer = new BufferBlock<int>(       new DataFlowBlockOptions { BoundedCapacity = 10 }); async Task Produce(IEnumerable<int> values){      foreach (var value in values)          await buffer.SendAsync(value);;    }async Task MultipleProducers(params IEnumerable<int>[] producers){    await Task.WhenAll(          from values in producers select Produce(values).ToArray())                .ContinueWith(_ => buffer.Complete());  }async Task Consumer(Action<int> process){     while (await buffer.OutputAvailableAsync())           process(await buffer.ReceiveAsync());        }async Task Run() {       IEnumerable<int> range = Enumerable.Range(0, 100);       await Task.WhenAll(MultipleProducers(range, range, range),            Consumer(n => Console.WriteLine($""value {n} - ThreadId            {Thread.CurrentThread.ManagedThreadId}"")));}\nBy default, TDF blocks have the value DataFlowBlockOptions.Unbounded  set to -1, \nwhich means that the queue is unbounded (unlimited) to the number of messages. But you can reset this value to a specific capacity that limits the number of messages the block may be queuing. When the queue reaches maximum capacity, any additional incoming messages will be postponed for later processing, making the producer wait Sets the BoundedCapacity to manage and \nthrottle the pressure from multiple producersSends the message to the buffer block asynchronously. The SendAsync method helps throttle the messages sent. \nRuns multiple producers in parallel, \nwaiting for all to terminate before \nnotifying the buffer block to complete\nWhen all producers terminate, the buffer block is notified as complete.\nSafeguards the buffer block from receiving a message only if there are any items available in the queue",5245
124-12.3.2 A single Producermultiple Consumer pattern.pdf,124-12.3.2 A single Producermultiple Consumer pattern,,0
125-12.4 Enabling an agent model in C using TPL Dataflow.pdf,125-12.4 Enabling an agent model in C using TPL Dataflow,"374 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nbefore further work. Likely, making the producer slow down (or wait) isn’t a problem because the messages are sent asynchronously.\n12.3.2 A single Producer/multiple Consumer pattern\nThe TDF BufferBlock  intrinsically supports a single Producer/multiple Consumer \npattern. This is handy if the producer performs faster than the multiple consumers, such as when they’re running intensive operations. \nFortunately, this pattern is running on a multicore machine, so it can use multiple \ncores to spin up multiple processing blocks (consumers), each of which can handle the producers concurrently. \nAchieving the multiple-consumer behavior is a matter of configuration. To do so, set \nthe \nMaxDegreeOfParallelism  property to the number of parallel consumers to run. \nHere’s listing 12.4 modified to apply a max-degree-of-parallelism set to the number of available logical processors:\nBufferBlock<int> buffer = new BufferBlock<int>(new DataFlowBlockOptions {               BoundedCapacity = 10,               MaxDegreeOfParallelism = Environment.ProcessorCount });\nNOTE  Logical cores are the number of physical cores times the number of \nthreads that can run on each core. An 8-core processor that runs two threads per core has 16 logical processors.\nBy default, the TDF block setting processes only one message at a time, while buffering the other incoming messages until the previous one completes. Each block is inde-pendent of others, so one block can process one item while another block processes a different item. But when constructing the block, you can change this behavior by set-ting the \nMaxDegreeOfParallelism  property in the DataFlowBlockOptions to a value \ngreater than 1. You can use TDF to speed up the computations by specifying the num-ber of messages that can be processed in parallel. The internals of the class handle the rest, including the ordering of the data sequence.\n12.4 Enabling an agent model in C# using TPL Dataflow \nTDF blocks are designed to be stateless by default, which is perfectly fine for most scenarios. But there are situations in an application when it’s important to maintain a state: for example, a global counter, a centralized in-memory cache, or a shared data-base context for transactional operations.\nIn such situations, there’s a high probability that the shared state is also the subject of \nmutation, because of continually tracking certain values. The problem has always been the difficulty of handling asynchronous computations combined with mutable state. As pre-viously mentioned, the mutation of shared state becomes dangerous in a multithreaded environment by leading you into a tar pit of concurrent issues (http:/ /curtclifton  \n.net/papers/MoseleyMarks06a.pdf). Luckily, TDF encapsulates the state inside the blocks, while the channels between blocks are the only dependencies. By design, this permits isolated mutation in a safe manner.\n \n 375 Enabling an agent model in C# using TPL Dataflow \nAs demonstrated in chapter 11, the F# MailboxProcessor  can solve these problems \nbecause it embraces the agent model philosophy, which can maintain an internal state by safeguarding its access to be concurrent safe (only one thread at a time can access the agent). Ultimately, the F# \nMailboxProcessor  can expose a set of APIs to the C# \ncode that can consume it effortlessly. Alternatively, you can reach the same perfor -\nmance using TDF to implement an agent object in C#, and then that agent object can act as the F# \nMailboxProcessor . \nStateful vs. stateless \nStateful  means that the program keeps track of the state of interaction. This is usually \naccomplished by setting values in a storage field designated for that purpose. Stateless  means there’s no record of previous interactions, and each interaction request \nmust be handled based entirely on the new information that comes with it.\n \nThe implementation of StatefulDataFlowAgent  relies on the instance of actionBlock  \nto receive, buffer, and process incoming messages with an unbounded limit (figure 12.7). Note that the max degree of parallelism is set to the default value 1 as designed, embrac-ing the single-threaded nature of the agent model. The state of the agent is initialized in the constructor and is maintained through a polymorphic and mutable value \nTState , \nwhich is reassigned as each message is processed. (Remember that the agent model only allows access by one thread at a time, ensuring that the messages are processed sequen-tially to eliminate any concurrent problems.) It’s good practice to use an immutable state, regardless of the safety provided by the agent implementation. \nBuffer TaskActionBlock<T>Stateless agent\nMessages\nBuffer TaskActionBlock<T>Stateful agent\nMessagesState\nFigure 12.7  The stateful and stateless agents implemented using the TDF ActionBlock . The stateful \nagent has an internal isolated arbitrary value to maintain in memory a state that can change.\n \n376 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nThe next listing shows the implementation of the StatefulDataFlowAgent  class, which \ndefines a stateful and generic agent that encapsulates the TDF AgentBlock  to process \nand store type values (in bold). \nListing 12.5  Stateful agent in C# using TDF\nclass StatefulDataFlowAgent<TState, TMessage> : IAgent<TMessage>{     private TState state;     private readonly ActionBlock<TMessage> actionBlock;     public StatefulDataFlowAgent(            TState initialState,             Func<TState, TMessage, Task<TState>> action,             CancellationTokenSource cts = null)         {            state = initialState;            var options = new ExecutionDataFlowBlockOptions {                CancellationToken = cts != null ?                 cts.Token : CancellationToken.None               };            actionBlock = new ActionBlock<TMessage>(                          async msg => state = await action(state, msg), options);        }     public Task Send(TMessage message) => actionBlock.SendAsync(message);     public void Post(TMessage message) => actionBlock.Post(message);}\nThe CancellationToken  can stop the agent at any time, and it’s the only optional \nparameter passed into the constructor. The function Func<TState,TMessage, Task-\n<TState>>  is applied to each message, in combination with the current state. When \nthe operation completes, the current state is updated, and the agent moves to pro-cess the next available message. This function is expecting an asynchronous operation, which is recognizable from the return type of \nTask<TState>.  \nNOTE   In the source code for this book, you can find several useful helper func-\ntions and implementation of agents using TDF with constructors that support either asynchronous or synchronous operations, which are omitted in listing 12.5 for brevity.\nThe agent implements the inheritances from the interface \nIAgent<TMessage> , which \ndefines the two members Post  and Send , used to pass messages to the agent synchro-\nnously or asynchronously, respectively: \n    public interface IAgent<TMessage>    {        Task Send(TMessage message);        void Post(TMessage message);    }Uses an asynchronous function to define the behavior of the agent\nIf a cancellation token isn’t provided in the constructor, a new token is provided.Constructs the internal ActionBlock \nthat acts as an encapsulated agent",7504
126-12.4.2 Agent interaction a parallel word counter.pdf,126-12.4.2 Agent interaction a parallel word counter,"377 Enabling an agent model in C# using TPL Dataflow \nUse the helper factory function Start , as in the F# MailboxProcessor , to initialize a \nnew agent, represented by the implemented interface IAgent<TMessage>  :\nIAgent<TMessage> Start<TState, TMessage>(TState initialState, \n➥ Func<TState, TMessage, Task<TState>> action, \n➥ CancellationTokenSource cts = null) => \n    new StatefulDataFlowAgent<TState, TMessage>(initialState, action, cts);\nBecause the interaction with the agent is only through sending ( Post  or Send ) a mes-\nsage, the primary purpose of the IAgent<TMessage>  interface is to avoid exposing the \ntype parameter for the state, which is an implementation detail of the agent. \nIn listing 12.6, agentStateful  is an instance of the StatefulDataFlowAgent  agent, \nwhich receives a message containing the web address where it should download its content asynchronously. Then, the result of the operation is cached into the local state, \nImmutable -\nDictionary<string,string> , to avoid repeating identical operations. For example, the \nGoogle website is mentioned twice in the urls  collections, but it’s downloaded only once. \nUltimately, the content of each website is persisted in the local file system for the sake of the \nexample. Notice that, apart from any side effects that occur when downloading and persist-ing the data, the implementation is side effect free. The changes in state are captured by always passing the state as an argument to the action function (or \nLoop function).\nListing 12.6  Agent based on TDF in action\nList<string> urls = new List<string> {                  @""http://www.google.com"",                @""http://www.microsoft.com"",                @""http://www.bing.com"",                @""http://www.google.com""            };var agentStateful = Agent.Start(ImmutableDictionary<string,string>.Empty,    async (ImmutableDictionary<string,string> state, string url) => {        if (!state.TryGetValue(url, out string content))       using (var webClient = new WebClient()){         content = await webClient.DownloadStringTaskAsync(url);           await File.WriteAllTextAsync(createFileNameFromUrl(url), content);         return state.Add(url, content);         }   return state;                });urls.ForEach(url => agentStateful.Post(url));\n12.4.1 Agent fold-over state and messages: Aggregate\nThe current state of an agent is the result of reducing all the messages it has received so far using the initial state as an accumulator value, and then processing the function as a reducer. You can imagine this agent as a fold (aggregator) in time over the stream of messages received. Interestingly, the \nStatefulDataFlowAgent  constructor shares a signa-\nture and behavior similar to the LINQ extension method Enumerable.Aggregate . For \ndemonstration purposes, the following code swaps the agent construct from the previous implementation with its counterpart, the LINQ \nAggregate  operator:\nurls.Aggregate(ImmutableDictionary<string,string>.Empty,                async (state, url) => {Uses an asynchronous anonymous \nfunction to construct the agent. This \nfunction performs the current state \nand input message received.\nThe function, which acts as the behavior of the agent, returns the updated state to keep track of any changes available to the next message processing. \n \n378 chapter  12 Parallel workflow and agent programming with TPL Dataflow\n     if (!state.TryGetValue(url, out string content))         using (var webClient = new WebClient())         {             content = await webClient.DownloadStringTaskAsync(url);               await File.WriteAllTextAsync(createFileNamFromUrl(url), \ncontent);\n             return state.Add(url, content);          }     return state;});\nAs you can see, the core logic hasn’t changed. Using the StatefulDataFlowAgent  con-\nstructor, which operates over message passing instead of in a collection, you imple-mented an asynchronous reducer similar to the LINQ \nAggregate  operator. \n12.4.2 Agent interaction: a parallel word counter \nAccording to the actor definition from Carl Hewitt,1 one of the minds behind the actor \nmodel: “One actor is no actor. They come in systems.” This means that actors come in systems and communicate with each other. The same rule applies to agents. Let’s look at an example of using agents that interact with each other to group-count the number of times a word is present in a set of text files (figure 12.8).\nLet’s start with a simple stateless agent that takes a string message and prints it. You can \nuse this agent to log the state of an application that maintains the order of the messages:\nIAgent<string> printer = Agent.Start((string msg) =>         WriteLine($""{msg} on thread {Thread.CurrentThread.\nManagedThreadId}"")); \nThe output also includes the current thread ID to verify the multiple threads used. This listing shows the implementation of the agent system for the group-count of words.\nListing 12.7  Word counter pipeline using agents \nIAgent<string> reader = Agent.Start(async (string filePath) =>  {    await printer.Send(""reader received message"");              var lines = await File.ReadAllLinesAsync(filePath);         lines.ForEach(async line => await parser.Send(line));   });char[] punctuation = Enumerable.Range(0, 256).Select(c => (char)c)       .Where(c => Char.IsWhiteSpace(c) || Char.IsPunctuation(c)).ToArray();IAgent<string> parser = Agent.Start(async (string line) => {    await printer.Send(""parser received message"");              foreach (var word in line.Split(punctuation))                  await counter.Send(word.ToUpper());});\n1  For more information on Carl Eddie Hewett, see https:/ /en.wikipedia.org/wiki/Carl_Hewitt.The agent posts a log to the printer agent.The reader agent asynchronously reads \nall the text lines from a given file.\nSends all the lines of a given text file to the parser agent. \nForEach is an extension method in the source code.\nThe parser agent splits the text into single words and sends them to the counter agent.\n \n 379 Enabling an agent model in C# using TPL Dataflow \nIReplyAgent<string, (string, int)> counter =     Agent.Start(ImmutableDictionary<string, int>.Empty,           (state, word) => {               printer.Post(""counter received message"");                     int count;                     if (state.TryGetValue(word, out count))                   return state.Add(word, count++);                    else return state.Add(word, 1);        }, (state, word) => (state, (word, state[word])));  foreach (var filePath in Directory.EnumerateFiles(@""myFolder"", ""*.txt""))     reader.Post(filePath);var wordCount_This = await counter.Ask(""this"");       var wordCount_Wind = await counter.Ask(""wind"");       \nProducer\nCollects files from the directory\nand sends them to the agent\nBufferAsk\nPost and wait for\nreply; asynchronous\nResponse\nAsynchronous reply channelTaskActionBlock<T>Stateful agentCounter agent\nTwo-way communicationReader agent Parser agent\nStateFilesystem\nBuffer TaskActionBlock<T>Stateless agent\nMessages\nMessagesBuffer TaskActionBlock<T>Stateless agentMessages\nFigure 12.8. Simple interaction between agents by exchanging messages. The agent programming model promotes the single responsibility principle to write code. Note the counter agent provides a two-way communication, so the user can ask (interrogate) the agent, sending a message at any given time and receiving a reply in the form of a channel, which acts as asynchronous callback. When the operation completes, the callback provides the result.The agent posts a log to the printer agent.\nThe counter agent checks if the word exists \nin the local state, and increments a counter \nor creates a new entry accordingly. The counter agent allows two-way communication, so you can send an ask message to receive a result back (reply) asynchronously\n \n380 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nThe system is composed of three agents that communicate with each other to form a chain of operations:\n¡ The reader  agent\n¡ The parser  agent\n¡ The counter  agent\nThe word-counting process starts with a for-each  loop to send the file paths of a given \nfolder to the first reader  agent. This agent reads the text from a file, and then sends \neach line of the text to the parser  agent:\nvar lines = await File.ReadAllLinesAsync(filePath); lines.ForEach(async line => await parser.Send(line));\nThe parser  agent splits the text message into single words, and then passes each of \nthose words to the last counter  agent:\nlines.Split(punctuation).ForEach(async word =>                       await counter.Send(word.ToUpper()));\nThe counter  agent is a stateful agent that does the work of maintaining the count of \nthe words as they’re updated.\nAn ImmutableDictionary  collection defines the state of the counter  agent that stores \nthe words along with the count for the number of times each word has been found. For each message received, the \ncounter  agent checks whether the word exists in an internal \nstate ImmutableDictionary<string, int>  to either increment the existing counter or \nstart a new one. \nNOTE  The advantage of using the agent programming model to implement \nword counting is that the agent is thread safe, and it can be shared between threads working on related texts freely. Moreover, the use of an immutable \nImmutableDictionary  to store the state can be passed outside the agent and \ncarry on processing without having to worry about the internal state becoming inconsistent and corrupted.\nThe interesting factor of the \ncounter agent is the ability to respond to the caller asyn-\nchronously using the Ask method. You can interrogate the agent for the count of a \nparticular word at any time. \nThe interface IReplyAgent  is the result of expanding the functionality of the previ-\nous interface IAgent  with the Ask method:\ninterface IReplyAgent<TMessage, TReply> : IAgent<TMessage>{        Task<TReply> Ask(TMessage message);}\nListing  12.8 shows the implementation of the two-way communication Stateful-\nReplyDataFlowAgent  agent, in which the internal state is represented by a single poly-\nmorphic mutable variable.\n \n 381 Enabling an agent model in C# using TPL Dataflow \nThis agent has two different behaviors:\n¡ One to handle the Send  a message method.\n¡ One to handle the Ask method. The Ask method sends a message and then waits \nasynchronously for a response.\nThese behaviors are passed in the form of generic Func  delegates into the agent’s con-\nstructor. The first function ( Func<TState, TMessage, Task<TState>>)  processes \neach message in combination with the current state and updates it accordingly. This logic is identical to the agent \nStatefulDataFlowAgent . \nConversely, the second function ( Func<TState , TMessage , Task<(TState , TReply)>>)  \nhandles the incoming messages, computes the agent’s new state, and ultimately replies to the sender. The output type of this function is a tuple, which contains the state of the agent, including a handle (callback) that acts as response (reply). The tuple is wrapped into a \nTask  type to be awaited without blocking, as with any asynchronous function. \nWhen creating the message Ask to interrogate the agent, the sender passes an \ninstance of TaskCompletionSource<TReply>  into the payload of the message, and a \nreference is returned by the Ask function to the caller. This object, TaskCompletion -\nSource,  is fundamental for providing a channel to communicate asynchronously \nback to the sender through a callback, and the callback is notified from the agent when the result of the computation is ready. This model effectively generates two-way communication. \nListing 12.8  Stateless agent in C# using TDF\nclass StatefulReplyDataFlowAgent<TState, TMessage, TReply> :                                          IReplyAgent<TMessage, TReply>{     private TState state;     private readonly ActionBlock<(TMessage,                          Option<TaskCompletionSource<TReply>>)> actionBlock;     public StatefulReplyDataFlowAgent(TState initialState,            Func<TState, TMessage, Task<TState>> projection,            Func<TState, TMessage, Task<(TState, TReply)>> ask,            CancellationTokenSource cts = null)        {         state = initialState;         var options = new ExecutionDataFlowBlockOptions {          CancellationToken = cts?.Token ?? CancellationToken.None };            actionBlock = new ActionBlock<(TMessage,                           Option<TaskCompletionSource<TReply>>)>(                async message => {The IReplyAgent interface defines the Ask method to ensure that the agent enables two-way communication.The ActionBlock message type is a tuple, where a TaskCompletionSource option is passed into the payload to supply a channel for communicating back to the caller asynchronously. \nThe agent construct takes two functions to respectively define the fire-and-forget and two-way communications.",13055
127-12.5.1 Context the problem of processing a large stream of data.pdf,127-12.5.1 Context the problem of processing a large stream of data,"382 chapter  12 Parallel workflow and agent programming with TPL Dataflow\n   (TMessage msg, Option<TaskCompletionSource<TReply>> replyOpt) = message;    await replyOpt.Match(                  None: async () => state = await projection(state, msg),            Some: async reply => {                        (TState newState, TReply replyresult) = await ask(state, msg);                 state = newState;             reply.SetResult(replyresult);             });          }, options);        }        public Task<TReply> Ask(TMessage message)        {            var tcs = new TaskCompletionSource<TReply>();              actionBlock.Post((message, Option.Some(tcs)));            return tcs.Task;                   }        public Task Send(TMessage message) =>                actionBlock.SendAsync((message, Option.None));}\nNOTE  TDF makes no guarantee of built-in isolation, and consequently, an \nimmutable state could be shared across the process function and be mutated outside the scope of the agent, resulting in unwanted behavior. Diligence in restricting and controlling access to the shared mutable state is highly recommended.\nTo enable \nStatefulReplyDataFlowAgent  to handle both types of communications, one-\nway Send  and two-way Ask, the message is constructed by including a Task Completion-\nSource  option type. In this way, the agent infers if a message is either from the Post  \nmethod, with None  TaskCompletionSource , or from the Ask method, with Some  Task-\nCompletionSource . The Match  extension method of the Option  type, Match<T, R>(None \n: Action<T>, Some(item) : Func<T,R>(item)) , is used to branch out to the corre-\nsponding behavior of the agent.\n12.5 A parallel workflow to compress and encrypt a large stream\nIn this section, you’ll build a complete asynchronous and parallelized workflow com-bined with the agent programming model to demonstrate the power of the TDF library.This example uses a combination of TDF blocks and the \nStatefulDataFlowAgent  \nagent linked to work as a parallel pipeline. The purpose of this example is to ana-lyze and architect a real case application. It then evaluates the challenges encountered The Match extension method of the Option type is used to branch behavior over the TaskCompletionSource option.If the TaskCompletionSource is None, \nthen the projection function is applied. \nIf the TaskCompletionSource is \nSome, then the Ask function is \napplied to reply to the caller. \nThe Ask member creates a TaskCompletionSource \nused as channel to communicate back to the caller \nwhen the operation run by the agent is completed.\n \n 383 A parallel workflow to compress and encrypt a large stream\nduring the development of the program, and examines how TDF can be introduced in the design to solve these challenges. \nTDF processes the blocks that compose a workflow at different rates and in parallel. \nMore importantly, it efficiently spreads the work out across multiple CPU cores to max-imize the speed of computation and overall scalability. This is particularly useful when you need to process a large stream of bytes that could generate hundreds, or even thou-sands, of chunks of data.\n12.5.1 Context: the problem of processing a large stream of data\nLet’s say that you need to compress a large file to make it easier to persist or transmit over the network, or that a file’s content must be encrypted to protect that informa-tion. Often, both compression and encryption must be applied. These operations can take a long time to complete if the full file is processed all at once. Furthermore, it’s challenging to move a file, or stream data, across the network, and the complexity increases with the size of the file, due to external factors, such as latency and unpredict-able bandwidth. In addition, if the file is transferred in one transaction, and something goes wrong, then the operation tries to resend the entire file, which can be time- and resource-consuming. In the following sections, you’ll tackle this problem step by step.\nIn .NET, it isn’t easy to compress a file larger than 4 GB, due to the framework limita-\ntion on the size of data to compress. Due to the maximum addressable size for a 32-bit pointer, if you create an array over 4 GB, an \nOutOfMemoryArray  exception is thrown. \nStarting with .NET 4.5 and for 64-bit platforms, the option gcAllowVeryLargeObjects  \n(http:/ /mng.bz/x0c4) is available to enable arrays greater than 4 GB. This option allows 64-bit applications to have a multidimensional array with size \nUInt32.MaxValue \n(4,294,967,295) elements. Technically, you can apply the standard GZip compression that’s used to compress streams of bytes to data larger than 4 GB; but the GZip distribu-tion doesn’t support this by default. The related .NET \nGZipStream class inheritably has \na 4 GB limitation. \nHow can you compress and encrypt a large file without being constrained by the 4 GB \nlimit imposed by the framework classes? A practical solution involves using a chunking routine to chop the stream of data. Chopping the stream of data makes it easier to com-press and/or encrypt each block individually and ultimately write the block content to an output stream. The chunking technique splits the data, generally into chunks of the same size,  applies the appropriate transformation to each chunk (compression before encryption), glues the chunks together in the correct order, and compresses the data. It’s vital to guarantee the correct order of the chunks upon reassembly at the end of the workflow. Due to the intensive I/O asynchronous operations, the packages might not arrive in the correct sequence, especially if the data is transferred across the network. You must verify the order during reassembly (figure 12.9). \n \n384 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nand decompress the data, as well as use asynchronous helper functions for compress-ing and encrypting bytes array.\nThe function \nCompressAndEncrypt  takes as an argument the source and destination \nstreams to process, the chunkSize  argument defines the size in which the data is split \n(the default is 1 MB if no value is provided), and CancellationTokenSource  stops the \ndataflow execution at any point. If no CancellationTokenSource  is provided, a new \ntoken is defined and propagated through the dataflow operations. \nThe core of the function consists of three TDF building blocks, in combination with \na stateful agent that completes the workflow. The inputBuffer  is a BufferBlock  type \nthat, as the name implies, buffers the incoming chunks of bytes read from the source stream, and holds these items to pass them to the next blocks in the flow, which is the linked \nTransformBlock  compressor  (the code to note is in bold).\nListing 12.9  Parallel stream compression and encryption using TDF\nasync Task CompressAndEncrypt(    Stream streamSource, Stream streamDestination,     long chunkSize = 1048576, CancellationTokenSource cts = null){    cts = cts ?? new CancellationTokenSource();      var compressorOptions = new ExecutionDataflowBlockOptions {    MaxDegreeOfParallelism = Environment.ProcessorCount,    BoundedCapacity = 20,    CancellationToken = cts.Token    };          var inputBuffer = new BufferBlock<CompressingDetails>(        new DataflowBlockOptions {                CancellationToken = cts.Token, BoundedCapacity = 20 }); var compressor = new TransformBlock<CompressingDetails,      CompressedDetails>(async details => {        var compressedData = await IOUtils.Compress(details.Bytes);         return details.ToCompressedDetails(compressedData);         }, compressorOptions);\nvar encryptor = new TransformBlock<CompressedDetails, EncryptDetails>(     async details => {        byte[] data = IOUtils.CombineByteArrays(details.CompressedDataSize, \n➥ details.ChunkSize, details.Bytes);         \n        var encryptedData = await IOUtils.Encrypt(data);            return details.ToEncryptDetails(encryptedData);     A new cancellation token is provided if none is supplied in the constructor.Sets the BoundedCapacity value to throttle the messages \nand reduce memory consumption by limiting the \nnumber of MemoryStreams created at the same time\nAsynchronously compresses the data (the method \nis provided in the source code of the book)\n Converts the current data structure into the \nmessage shape to send to the next blockCombines the data and metadata into a byte array pattern that will be deconstructed and parsed during the revert operation decrypt–decompress \nAsynchronously encrypts the data (the method is provided in the source code of the book)\n#D Converts the current data structure into the message shape to send to the next blockTransform\n(compress)AggregateBuffer\ninput dataThe transform block\nruns multiple subtask s\nin parallel. The aggregate agen t\nmaintains the integrity\nof the message order.\nProcess\ntask\nTransform\n(encrypt)Process\ntask\nBuffer TaskActionBlock<T>Stateful agent\nMessagesState\nFigure 12.9  The transform blocks process the messages in parallel. The result is sent to the next block \nwhen the operation completes. The aggregate agent’s purpose is to maintain the integrity of the order of the messages, similar to the \nAsOrdered  PLINQ extension method.\nThe opportunity for parallelism fits naturally in this design, because the chunks of the data can be processed independently. \nEncryption and compression: order matters\nIt might seem that because the compression and encryption operations are independent of one another, it makes no difference in which order they’re applied to a file. This isn’t true. The order in which the operations of compression and encryption are applied is vital. \nEncryption has the effect of turning input data into high-entropy data,\n2 which is a mea -\nsure of the unpredictability of information content. Therefore, the encrypted data appears like a random array of bytes, which makes finding common patterns less prob-able. Conversely, compression algorithms work best when there are several similar pat-terns in the data, which can be expressed with fewer bytes. When data must be both compressed and encrypted, you achieve the best results by first compressing and then encrypting the data. In this way, the compression algorithm can find similar patterns to shrink, and consequently the encryption algorithm produces the chunks of data having almost the same size. Furthermore, if the order of the opera -\ntions is compression then encryption, not only should the output be a smaller file, but the encryption will most likely take less time because it’ll operate on less data.\n \nListing  12.9 shows the full implementation of the parallel compression–encryption workflow. Note that in the source code, you can find the reverse workflow to decrypt \n2  Information entropy is defined as the average amount of information produced by a stochastic source of data. See https:/ /en.wikipedia.org/wiki/Entropy_(information_theory).\n \n 385 A parallel workflow to compress and encrypt a large stream\nand decompress the data, as well as use asynchronous helper functions for compress-ing and encrypting bytes array.\nThe function \nCompressAndEncrypt  takes as an argument the source and destination \nstreams to process, the chunkSize  argument defines the size in which the data is split \n(the default is 1 MB if no value is provided), and CancellationTokenSource  stops the \ndataflow execution at any point. If no CancellationTokenSource  is provided, a new \ntoken is defined and propagated through the dataflow operations. \nThe core of the function consists of three TDF building blocks, in combination with \na stateful agent that completes the workflow. The inputBuffer  is a BufferBlock  type \nthat, as the name implies, buffers the incoming chunks of bytes read from the source stream, and holds these items to pass them to the next blocks in the flow, which is the linked \nTransformBlock  compressor  (the code to note is in bold).\nListing 12.9  Parallel stream compression and encryption using TDF\nasync Task CompressAndEncrypt(    Stream streamSource, Stream streamDestination,     long chunkSize = 1048576, CancellationTokenSource cts = null){    cts = cts ?? new CancellationTokenSource();      var compressorOptions = new ExecutionDataflowBlockOptions {    MaxDegreeOfParallelism = Environment.ProcessorCount,    BoundedCapacity = 20,    CancellationToken = cts.Token    };          var inputBuffer = new BufferBlock<CompressingDetails>(        new DataflowBlockOptions {                CancellationToken = cts.Token, BoundedCapacity = 20 }); var compressor = new TransformBlock<CompressingDetails,      CompressedDetails>(async details => {        var compressedData = await IOUtils.Compress(details.Bytes);         return details.ToCompressedDetails(compressedData);         }, compressorOptions);\nvar encryptor = new TransformBlock<CompressedDetails, EncryptDetails>(     async details => {        byte[] data = IOUtils.CombineByteArrays(details.CompressedDataSize, \n➥ details.ChunkSize, details.Bytes);         \n        var encryptedData = await IOUtils.Encrypt(data);            return details.ToEncryptDetails(encryptedData);     A new cancellation token is provided if none is supplied in the constructor. Sets the BoundedCapacity value to throttle the messages \nand reduce memory consumption by limiting the \nnumber of MemoryStreams created at the same time\nAsynchronously compresses the data (the method \nis provided in the source code of the book)\n Converts the current data structure into the \nmessage shape to send to the next blockCombines the data and metadata into a byte array pattern that will be deconstructed and parsed during the revert operation decrypt–decompress \nAsynchronously encrypts the data (the method is provided in the source code of the book)\n#D Converts the current data structure into the message shape to send to the next block \n386 chapter  12 Parallel workflow and agent programming with TPL Dataflow\n    }, compressorOptions);var asOrderedAgent = Agent.Start((new Dictionary<int, EncryptDetails>(),0), async((Dictionary<int,EncryptDetails>,int)state,EncryptDetails msg)=>{        Dictionary<int, EncryptDetails> details, int lastIndexProc) = state;      details.Add(msg.Sequence, msg);      while (details.ContainsKey(lastIndexProc+1)) {          msg = details[lastIndexProc + 1];          await streamDestination.WriteAsync(msg.EncryptedDataSize, 0,                                          msg.EncryptedDataSize.Length);          await streamDestination.WriteAsync(msg.Bytes, 0,                                         msg.Bytes.Length);           lastIndexProc = msg.Sequence;          details.Remove(lastIndexProc);          }      return (details, lastIndexProc); }, cts);var writer = new ActionBlock<EncryptDetails>(async details => await                     asOrderedAgent.Send(details), compressorOptions); var linkOptions = new DataflowLinkOptions { PropagateCompletion = true };inputBuffer.LinkTo(compressor, linkOptions);      compressor.LinkTo(encryptor, linkOptions);            encryptor.LinkTo(writer, linkOptions);            long sourceLength = streamSource.Length;byte[] size = BitConverter.GetBytes(sourceLength);await streamDestination.WriteAsync(size, 0, size.Length);            chunkSize = Math.Min(chunkSize, sourceLength);       int indexSequence = 0; while (sourceLength > 0) {   byte[] data = new byte[chunkSize];   int readCount = await streamSource.ReadAsync(data, 0, data.Length);    byte[] bytes = new byte[readCount];   Buffer.BlockCopy(data, 0, bytes, 0, readCount);   var compressingDetails = new CompressingDetails {            Bytes = bytes,            ChunkSize = BitConverter.GetBytes(readCount),            Sequence = ++indexSequence        };await inputBuffer.SendAsync(compressingDetails); sourceLength -= readCount;                if (sourceLength < chunkSize)    chunkSize = sourceLength;            if (sourceLength == 0)    inputBuffer.Complete();              }The behavior of the asOrderedAgent Agent keeps track of the order of the messages received to maintain the order (persist the data).\nPersists the data asynchronously; the file \nstream could be replaced with a network \nstream to send the data across the wire.\nThe chunk of data that is processed is removed from the local state, keeping track of the items to perform.The ActionBlock reader sends the chunk \nof data wrapped into a details data \nstructure to the asOrdered agent. \nLinks the dataflow blocks to compose the workflow\nThe total size of the file stream is persisted as the first chunk of data; in this way, the decompression algorithm knows how to retrieve the information and how long to run.\nDetermines the chunk size to partition data\nReads the source stream into chunks \nuntil the end of the stream\nSends the chunk of data read from the source stream to the inputBuffer\nChecks the current source stream position after each read operation to decide when to complete the operation \nNotifies the input buffer when the source stream reaches the end\n \n 387 A parallel workflow to compress and encrypt a large stream\nawait inputBuffer.Completion.ContinueWith(task => compressor.Complete());await compressor.Completion.ContinueWith(task => encryptor.Complete());await encryptor.Completion.ContinueWith(task => writer.Complete());await writer.Completion;await streamDestination.FlushAsync();   }\nThe bytes read from the stream are sent to the buffer block by using the SendAsync  \nmethod:\nvar compressingDetails = new CompressingDetails {       Bytes = bytes,       ChunkSize = BitConverter.GetBytes(chunkSize),       Sequence = ++indexSequence    };await buffer.SendAsync(compressingDetails);\nEach chunk of bytes read from the stream source is wrapped into the data structure’s \nCompressingDetails , which contains the additional information of byte-array size. \nThe monotonic value is later used in the sequence of chunks generated to preserve the order. A monotonic value is a function between ordered sets that preserves or reverses \nthe given value, and the value always either decreases or increases. The order of the block is important both for a correct compression–encryption operation and for cor -\nrect decryption and decompression into the original shape. \nIn general, if the purpose of the block is purely to forward item operations from one \nblock to several others, then you don’t need the \nBufferBlock . But in the case of read-\ning a large or continuous stream of data, this block is useful for taming the backpres-sure generated from the massive amount of data partitioned to the process by setting an appropriate \nBoundedCapacity . In this example, the BoundedCapacity  is restricted \nto a capacity of 20 items. When there are 20 items in this block, it will stop accepting new items until one of the existing items passes to the next block. Because the dataflow source of data originated from asynchronous I/O operations, there’s a risk of poten-tially large amounts of data to process. It’s recommended that you limit internal buffer -\ning to throttle the data by setting the \nBoundedCapacity  property in the options defined \nwhen constructing the BufferBlock .\nThe next two block types are compression transformation and encryption trans-\nformation. During the first phase (compression), the TransformBlock  applies the \ncompression to the chunk of bytes and enriches the message received Compressing -\nDetails  with the relative data information, which includes the compressed byte array \nand its size. This information persists as part of the output stream accessible during the decompression.\nThe second phase (encryption) enciphers the chunk of compressed byte array and \ncreates a sequence of bytes resulting from the composition of three arrays: \nCompressed -\nDataSize , ChunkSize , and data array. This structure instructs the decompression and \ndecryption algorithms to target the right portion of bytes to revert from the stream.",20086
128-12.5.5 Meshing Reactive Extensions Rx and TDF.pdf,128-12.5.5 Meshing Reactive Extensions Rx and TDF,"388 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nNOTE  Keep in mind that when there are multiple TDF blocks, certain TDF \ntasks may be idle while the others are executing, so you have to tune the block’s execution option to avoid potential starvation. Details regarding this optimiza-tion are explained in the coming section.\n12.5.2 Ensuring the order integrity of a stream of messages\nThe TDF documentation guarantees that TransformBlock  will propagate the messages \nin the same order in which they arrived. Internally, TransformBlock  uses a reordering \nbuffer to fix any out-of-order issues that might arise from processing multiple messages concurrently. Unfortunately, due to the high number of asynchronous and intensive I/O operations running in parallel, keeping the integrity of the message order doesn’t apply to this case. This is why you implemented the additional sequential ordering preservation using monotonically values.\nIf you decide to send or stream the data over the network, then the guarantee of \ndelivering the packages in the correct sequence is lost, due to variables such as the unpredictable bandwidth and unreliable network connection. To safeguard the order integrity when processing chunks of data, your final step in the workflow is the stateful \nasOrderedAgent  agent. This agent behaves as a multiplexer by reassembling the items \nand persists them in the local filesystem, maintaining the correct sequence. The order value of the sequence is kept in a property of the \nEncryptDetails  data structure, which \nis received by the agent as a message. \nThe multiplexer pattern\nThe multiplexer is a pattern generally used in combination with a Producer/Consumer \ndesign. It allows its consumer, which in the previous example is the last stage of the pipeline, to receive the chunks of data in the correct sequential order. The chunks of data don’t need to be sorted or reordered. Instead, the fact that each producer (TDF block) queue is locally ordered allows the multiplexer to look for the next value (message) in the sequence. The multiplexer waits for a message from a producer dataflow block. When a chunk of data arrives, the multiplexer looks to see if the chunk’s sequence number is the next in the expected sequence. If it is, the multiplexer persists the data to the local filesystem. If the chunk of data isn’t the one expected next in the sequence, the multi-plexer holds the value in an internal buffer and repeats the analysis operation for the next message received. This algorithm allows the multiplexer to put together the inputs from the incoming producer message in a way that ensures sequential order without sorting the values.\n \nThe accuracy for the whole computation requires preservation of the order of the source sequence and the partitions to ensure that the order is consistent at merge time.\nNOTE  In the case of sending the chunks of data over the network, the same \nstrategy of persisting the data in the local filesystem is applicable by having the agent working as a receiver on the other side of the wire.\n \n 389 A parallel workflow to compress and encrypt a large stream\nThe state of this agent is preserved using a tuple. The first item of the tuple is a col-lection \nDictionary<int, EncryptDetails> , where the key represents the sequence \nvalue of the original order by which the data was sent. The second item, lastIndex -\nProc , is the index of the last item processed, which prevents reprocessing the same \nchunks of data more than once. The body of asOrderedAgent  runs the while  loop \nthat uses this value lastIndexProc  and makes sure that the processing of the chunks \nof data starts from the last item unprocessed. The loop continues to iterate until the order of the items is continued; otherwise it breaks out from the loop and waits for the next message, which might complete the missing gap in the sequence.\nThe \nasOrderedAgent  agent is plugged into the workflow through the TDF Action-\nBlock  writer, which sends it to the EncryptDetails  data structure for the final work. \nGZipStream vs. DeflateStream: how to choose\nThe .NET Framework provides a few option classes for compressing a stream of bytes. Listing 12.9 used \nSystem.IO.Compression.GZipStream  for the compression mod -\nule; but the alternative System.IO.Compression.DeflateStream  presents a valid \noption. Starting with .NET Framework 4.5, the DeflateStream  compression stream \nuses the zlib library, which results in a better compression algorithm and, in most cases, smaller compressed data as compared to earlier versions. The \nDeflateStream  com -\npression algorithm optimization maintains back-compatibility with data compressed with the earlier version. One reason to choose the \nGZipStream  class is that it adds a cyclic \nredundancy check (CRC) to the compressed data to determine if it has been corrupted. Please refer to the MSDN online documentation (http:/ /mng.bz/h082 ) for further details \nabout these streams.\n \n12.5.3 Linking, propagating, and completing \nThe TDF blocks in the compress-encrypt workflow are linked using the LinkTo  exten-\nsion method, which by default propagates only data (messages). But if the workflow is linear, as in this example, it’s good practice to share information among the blocks through an automatic notification, such as when the work is terminated or eventual errors accrue. This behavior is achieved by constructing the \nLinkTo  method with the \nDataFlowLinkOptions  optional argument and the PropagateCompletion  property set \nto true. Here’s the code from the previous example with this option built in: \nvar linkOptions = new DataFlowLinkOptions { PropagateCompletion = true };inputBuffer.LinkTo(compressor, linkOptions);compressor.LinkTo(encryptor, linkOptions);encryptor.LinkTo(writer, linkOptions);\nThe PropagateCompletion  optional property informs the dataflow block to automat-\nically propagate its results and exceptions to the next stage when it completes. This is \n \n390 chapter  12 Parallel workflow and agent programming with TPL Dataflow\naccomplished by calling the Complete  method when the buffer block triggers the com-\nplete notification upon reaching the end of the stream: \n       if (sourceLength < chunkSize)           chunkSize = sourceLength;       if (sourceLength == 0)           buffer.Complete();\nThen all the dataflow blocks are announced in a cascade as a chain that the process has completed:\nawait inputBuffer.Completion.ContinueWith(task => compressor. Complete());\nawait compressor.Completion.ContinueWith(task => encryptor. Complete());\nawait encryptor.Completion.ContinueWith(task => writer. Complete());\nawait writer.Completion;\nUltimately, you can run the code as follows:\nusing (var streamSource = new FileStream(sourceFile, FileMode.OpenOrCreate,                           FileAccess.Read, FileShare.None, useAsync: true))using (var streamDestination = new FileStream(destinationFile,         FileMode.Create, FileAccess.Write, FileShare.None, useAsync: true))       await CompressAndEncrypt(streamSource, streamDestination)\nTable 12.1 shows the benchmarks for compressing and encrypting different file sizes, including the inverted operation of decrypting and decompressing. The benchmark result is the average of each operation run three times.\nTable 12.1  Benchmarks for compressing and encrypting different file sizes\nFile size in GB Degree of parallelismCompress-encrypt time \nin secondsDecrypt-decompress time \nin seconds\n3 1 524.56 398.52\n3 4 123.64 88.25\n3 8 69.20 45.93\n12 1 2249.12 1417.07\n12 4 524.60 341.94\n12 8 287.81 163.72\n12.5.4 Rules for building a TDF workflow\nHere are few good rules and practices for successfully implementing TDF in your \nworkflow:\n¡ Do one thing, and do it well. This is a principal of modern OOP, the single responsi-\nbility principle (https:/ /en.wikipedia.org/wiki/Single_responsibility_principle). The idea is that your block should perform only one action and should have only one reason to change.\n \n 391 A parallel workflow to compress and encrypt a large stream\n¡ Design for composition. In the OOP world, this is known as the open closed principle (https:/ /en.wikipedia.org/wiki/Open/closed_principle), where the dataflow building blocks are designed to be open for extension but closed to modification.\n¡ DRY. This principle (don’t repeat yourself) encourages you to write reusable code and reusable dataflow building block components.\nPerformance tip: recycling MemoryStreams\nThe .NET programming languages rely on a mark-and-sweep GC that can negatively impact the performance of a program that generates a large number of memory allo -\ncations due to GC pressure. This is a performance penalty that the code (such as in list-ing 12.9) pays when creating a \nSystem.IO.MemoryStream  instance for each compress \nand encrypt operation, including its underlying byte array. The quantity of \nMemoryStream  instances increases with the number of chunks of data to \nprocess, which can be hundreds in a large stream/file. As that byte array grows, the Mem-\noryStream  resizes it by allocating a new and larger array, and then copying the original \nbytes into it. This is inefficient, not only because it creates new objects and throws the old ones away, but also because it has to do the legwork of copying the content each time it resizes. One way to alleviate the memory pressure that can be caused by the frequent creation and destruction of large objects is to tell the .NET GC to compact the large object heap (LOH) using this setting:\nGCSettings.LargeObjectHeapCompactionMode = GCLargeObjectHeapCompactionMode.CompactOnce\nThis solution, while it may reduce the memory footprint of your application, does nothing to solve the initial problem of allocating all that memory in the first place. A better solu -\ntion is to create an object pool, also known as pooled buffers, to pre-allocate an arbitrary number of \nMemoryStream s that can be reused (a generic and reusable object pool is \navailable in chapter 13). Microsoft has released a new object, called \nRecyclableMemoryStream , which \nabstracts away the implementation of an object pool optimized for MemoryStream , and \nminimizes the number of large object heap allocations and memory fragmentation. The discussion of \nRecycableMemoryStream  is out of the scope of this book. For more infor -\nmation, refer to the MSDN online documentation.\n \n12.5.5 Meshing Reactive Extensions (Rx) and TDF\nTDF and Rx (discussed in chapter 6) have important similarities, despite having inde-pendent characteristics and strengths, and these libraries complement each other, making them easy to integrate. TDF is closer to an agent-based programming model, focused on providing building blocks for message passing, which simplifies the imple-mentation of parallel CPU- and I/O-intensive applications with high throughput and low latency, while also providing developers explicit control over how data is buffered. \n \n392 chapter  12 Parallel workflow and agent programming with TPL Dataflow\nRx is keener to the functional paradigm, providing a vast set of operators that predom-inantly focused on coordination and composition of event streams with a LINQ-based API.\nTDF has built-in support for integrating with Rx, which allows it to expose the source \ndataflow blocks as both observables and observers. The \nAsObservable  extension \nmethod transforms TDF blocks into an observable sequence, which allows the output of the dataflow chain to flow efficiently into an arbitrary set of Reactive fluent extension methods for further processing. Specifically, the \nAsObservable  extension method con-\nstructs an IObservable<T>  for an ISourceBlock<T>.  \nNOTE  TDF can also act as an observer. The AsObserver  extension method cre-\nates an IObserver<T>  for an ITargetBlock<T> , where the OnNext  calls for the \nobserver result in the data being sent to the target. The OnError  calls result in \nthe exception faulting the target, and the OnCompleted  calls will result in Com-\nplete  being called on the target. \nLet’s see the integration of Rx and TDF in action. In listing 12.9, the last block of the parallel compress-encrypt stream dataflow is the stateful \nasOrderedAgent . The partic-\nularity of this component is the presence of an internal state that keeps track of the messages received and their order. As mentioned, the construct signature of a stateful agent is similar to the LINQ \nAggregate  operator, which in terms of Rx can be replaced \nwith the RX Observable.Scan  operator. This operator is covered in chapter 6.\nThe following listing shows the integration between Rx and TDF by replacing the \nasOrderedAgent  agent from the last block of the parallel compress-encrypt stream \nworkflow.\nListing 12.10  Integrating Reactive Extensions with TDF\ninputBuffer.LinkTo(compressor, linkOptions);compressor.LinkTo(encryptor, linkOptions);encryptor.AsObservable()                  .Scan((new Dictionary<int, EncryptDetails>(), 0),  (state, msg) => Observable.FromAsync(async() => {          (Dictionary<int,EncryptDetails> details, int lastIndexProc) = state; details.Add(msg.Sequence, msg); while (details.ContainsKey(lastIndexProc + 1)) {    msg = details[lastIndexProc + 1];    await streamDestination.WriteAsync(msg.EncryptedDataSize, 0,                                        msg.EncryptedDataSize.Length);    await streamDestination.WriteAsync(msg.Bytes, 0, msg.Bytes.Length);    lastIndexProc = msg.Sequence;    details.Remove(lastIndexProc);    }  return (details, lastIndexProc);}) .SingleAsync().Wait()).SubscribeOn(TaskPoolScheduler.Default).Subscribe();      Enables Rx integration with TDF\nRuns the Rx Scan \noperation asynchronously \nRx subscribes to TaskPoolScheduler\n \n 393 Summary\nAs you can see, you swapped the asOrderedAgent  with the Rx Observable.Scan  opera-\ntor without changing the internal functionality. TDF blocks and Rx observable streams can be completed successfully or with errors, and the \nAsObservable  method will trans-\nlate the block completion (or fault) into the completion of the observable stream. But if the block faults with an exception, that exception will be wrapped in an \nAggregate-\nException  when it is passed to the observable stream. This is similar to how linked \nblocks propagate their faults.\nSummary\n¡ A system written using TPL Dataflow benefits from a multicore system because all the blocks that compose a workflow can run in parallel.\n¡ TDF enables effective techniques for running embarrassingly parallel problems, where many independent computations can be executed in parallel in an evi-dent way.\n¡ TDF has built-in support for throttling and asynchrony, improving both I/O-bound and CPU-bound operations. In particular, it provides the ability to build responsive client applications while still getting the benefits of massively parallel processing.\n¡ TDF can be used to parallelize the workflow to compress and encrypt a large stream of data by processing blocks at different rates. \n¡ The combination and integration of Rx and TDF simplifies the implementation of parallel CPU- and I/O-intensive applications, while also providing developers explicit control over how data is buffered.",15358
129-Part 3 Modern patterns of concurrent programming applied.pdf,129-Part 3 Modern patterns of concurrent programming applied,"Part 3\nModern patterns of concurrent \nprogramming applied\nT his third and final part of the book allows you to put into practice all the \nfunctional concurrent programming techniques you’ve learned thus far. These chapters will become your go-to reference for questions and answers about concurrency. \nChapter 13 covers recipes to solve both common and complex problems you \nmay encounter in concurrent applications using the functional paradigm. Chap-ter 14 walks you through the full implementation of a scalable and highly perfor -\nmant stock market server application, which includes iOS and WPF versions for the client side. \nFunctional paradigm principles learned in the book will be applied in the \ndesign and architecture decisions, as well as to code development, to achieve a highly performant and scalable solution. You’ll see in this section the positive side effects that come from applying functional principles to reduce bugs and increase maintainability.",979
130-13.1.1 Solution asynchronously recycling a pool of objects.pdf,130-13.1.1 Solution asynchronously recycling a pool of objects,"39713Recipes and design \npatterns for successful \nconcurrent programming\nThis chapter covers\n¡ Twelve code recipes that answer common problems in parallel programming\nThe 12 recipes presented in this chapter have broad applications. You can use the core ideas as a reference when you’re facing a similar problem and require a quick answer. The material demonstrates how the functional concurrent abstractions cov-ered throughout this book make it possible to solve complex problems by develop-ing sophisticated and rich functions with relatively few lines of code. I’ve kept the implementations of the recipes as simple as possible, so you’ll need to deal from time to time with cancellations and exception handling. \nThis chapter shows you how to put together everything you’ve learned so far to \ncombine concurrent programming models using the functional programming abstraction as a glue to write efficient and performant programs. By the end of this chapter, you’ll have at your disposal a set of useful and reusable tools for solving com-mon concurrent coding problems. \nEach recipe is built in either C# or F#; for the majority of the code implementa-\ntion, you can find both versions in the downloadable code online. Also, keep in mind that F# and C# are .NET programming languages with interoperability support to interact with each other. You can easily use a C# program in F# and vice versa.\n \n398 chapter  13 Recipes and design patterns for successful concurrent programming\n13.1 Recycling objects to reduce memory consumption \nIn this section you’ll implement a reusable asynchronous object pool. This should be used in cases where the recycling of objects benefits the reduction of memory con-sumption. Minimizing the number of GC generations allows your program to enjoy better performance speed. Figure 13.1, repeated from chapter 12, shows how to apply concurrent Producer/Consumer patterns, from listing 12.9, to compress and encrypt a large file in parallel. \n \nTransform\n(compress)AggregateBuffer\ninput dataThe transform block\nruns multiple subtask s\nin parallel. The aggregate agen t\nmaintains the integrity\nof the message order.\nProcess\ntask\nTransform\n(encrypt)Process\ntask\nBuffer TaskActionBlock<T>Stateful agent\nMessagesState\nFigure 13.1. The Transform  blocks process the messages in parallel. The result is sent to the next \nblock when the operation completes. The Aggregate agent’s purpose is to maintain the integrity of order of the messages, similar to the \nAsOrdered  PLINQ extension method.\nThe function CompressAndEncrypt , from listing 12.9, partitions a large file in a set of \nbyte-array chunks, which has the negative effect of producing a large volume of GC generations due to high memory consumption. Each memory chunk is created, pro-cessed, and collected by the GC when the memory pressure reaches the trigger point for demanding for more resources. \nThis high volume operation of creating and destroying byte array causes many GC \ngenerations, which negatively impact the overall performance of the application. In fact, the program allocates a considerable number of memory buffers (byte arrays) for its full execution in a multithreaded fashion, meaning that multiple threads can simul-taneously allocate the same amount of memory. Consider that each buffer is 4,096 bytes of memory, and 25 threads are running simultaneously; in this case, about 102,400 bytes are being simultaneously allocated in the heap. Additionally, when each thread completes its execution, many buffers are out of scope, pressuring the GC to start a gen-eration. This is bad for performance, because the application is under heavy memory management.\n \n 399 Recycling objects to reduce memory consumption \n13.1.1 Solution: asynchronously recycling a pool of objects \nTo optimize the performance of a concurrent application with intense memory con-sumption, recycle the objects that otherwise are subject to be garbage collected by the system. In the parallel compress and encrypt stream example, you want to reuse the same byte buffers (byte arrays) generated instead of creating new ones. This is possible using \nObjectPool , a class designed to provide a cached pool of objects that recycles \nthe items that aren’t being used. This reuse of objects avoids expensive acquisition and release of resources, minimizing the potential memory allocation. Specifically, in the highly concurrent example, you need a thread-safe and non-blocking (task-based) concurrent object pool (figure 13.2).\nConsumer B\nusing object TConsumer A\nusing object T\nRequest object\nfrom the pool.Send object back when\nutilization is complete.\nSend object back when\nutilization is complete.GetAsync\nPutAsyncObject T\nObject T\nObject T\nObject T\nObject T\nFigure 13.2  An object pool can asynchronously handle multiple concurrent requests for reusable \nobjects from multiple consumers. The consumer then sends the object back to the object pool when it’s done with the work. Internally, object pool generates a queue of objects using a given factory delegate. These objects are then recycled to reduce memory consumption and the cost of new instantiation.\nIn listing 13.1 the implementation of ObjectPoolAsync  is based on a TDF that uses the \nBufferBlock  as a building block. The ObjectPoolAsync  pre-initializes a set of objects \nfor an application to use and reuse when needed. Furthermore, TDF is intrinsically thread safe while providing an asynchronous, non-blocking semantic.\nListing 13.1  Asynchronous object pool implementation using TDF\npublic class ObjectPoolAsync<T> :IDisposable{    private readonly BufferBlock<T> buffer;      private readonly Func<T> factory;      private readonly int msecTimeout;\n \n400 chapter  13 Recipes and design patterns for successful concurrent programming\n    public ObjectPoolAsync(int initialCount, Func<T> factory, \n➥ CancellationToken cts, int msecTimeout = 0)\n    {        this.msecTimeout = msecTimeout;        buffer = new BufferBlock<T>(              new DataflowBlockOptions { CancellationToken = cts });        this.factory = () => factory();         for (int i = 0; i < initialCount; i++)            buffer.Post(this.factory());      }    public Task<bool> PutAsync(T item) => buffer.SendAsync(item);         public Task<T> GetAsync(int timeout = 0)      {        var tcs = new TaskCompletionSource<T>();        buffer.ReceiveAsync(TimeSpan.FromMilliseconds(msecTimeout))            .ContinueWith(task =>            {                if (task.IsFaulted)                    if (task.Exception.InnerException is TimeoutException)                        tcs.SetResult(factory());                    else                        tcs.SetException(task.Exception);                else if (task.IsCanceled)                    tcs.SetCanceled();                else                    tcs.SetResult(task.Result);                            });        return tcs.Task;    }    public void Dispose() => buffer.Complete();}\nObjectPoolAsync  accepts as arguments an initial number of objects to create and a \nfactory delegate constructor. ObjectPoolAsync  exposes two functions to orchestrate \nthe object’s recycle: \n¡ PutAsync —An item can be Put into the pool asynchronously.\n¡ GetAsync —An item can be taken from the pool asynchronously.\nIn the downloadable source code, you can find the full solution of the CompressAnd -\nEncrypt  program updated to use ObjectPoolAsync . Figure 13.3 is a graphical compar -\nison of the GC generations for different file sizes between the original version of the program and the new one that exploits \nObjectPoolAsync .Uses BufferBlock to asynchronously coordinate the underlying set of types T\nUses a factory delegate to generate a new instance of type TDuring the initialization of the object pool, the buffer is filled with instances of type T to have objects available from the start. When the consumer is done, the object type T \nis sent back to the object pool to be recycled. \nThe object pool sends an object type T when a consumer makes the request.",8097
131-13.2.1 Solution composing a pipeline of steps  forming the ForkJoin pattern.pdf,131-13.2.1 Solution composing a pipeline of steps  forming the ForkJoin pattern,"401 Custom parallel Fork/Join operator \n200\n200 Without ObjectPool with 1 GbWithout ObjectPool with 2 GbWithout ObjectPool with 3 GbWith ObjectPool with 1 GbWith ObjectPool with 2 GbWith ObjectPool with 3 GbKey\n150\n100\n50\n0\nGcDelta[0] GcDelta[1]GC usage\nGcDelta[2]131143\n1524356266119\n545203289\n411\nFigure 13.3  Comparison of chapter 12’s CompressAndEncrypt  program, which processes \ndifferent large files (1 GB, 2 GB, and 3 GB), implemented with and without AsyncObjectPool . The \nimplementation using the object pool has a low number of GC generations compared to the original one. Minimizing GC generation results in better performance.\nThe results displayed in the chart demonstrate how the CompressAndEncrypt  program \nimplemented using ObjectPoolAsync  dramatically reduces the GC generations, speed-\ning up overall application performance. In an eight-core machine, the new version of \nCompressAndEncrypt  is about 8% faster.\n13.2 Custom parallel Fork/Join operator \nIn this section you implement a reusable extension method to parallelize Fork/Join operations. Let’s say you detected a piece of code in your program that would benefit from being executed in parallel using a Divide and Conquer pattern to speed up perfor -\nmance. You decide to refactor the code to use a concurrent Fork/Join pattern (figure 13.4). And the more you check the program, the more similar patterns arise. \nNOTE  As you may recall from section 4.2, Fork/Join, like Divide and Conquer, \nbreaks the work into small tasks until each small task is simple enough that it can be solved without further breakups, and then it coordinates the parallel workers.\n \n402 chapter  13 Recipes and design patterns for successful concurrent programming\nAggregate\ndataFork Data\nsetJoin ParallelismTask 1\n1N 1/6\nTask 2\n2N 1/6\nTask 3\n3N 1/6\nTask 4\n4N 1/6\nTask 5\n5N 1/6\nTask 6\n6N 1/6\nFigure 13.4  The Fork/Join pattern splits a task into subtasks that can be executed independently in \nparallel. When the operations complete, the subtasks are joined again. It isn’t a coincidence that this pattern is often used to achieve data parallelism. In fact, there are clearly similarities. \nUnfortunately, in .NET, there’s no built-in support for parallel Fork/Join extension methods to be reused on demand. But you can create this and more to have a reusable and flexible operator that does the following:\n¡ Splits the data \n¡ Applies the Fork/Join pattern in parallel\n¡ Optionally allows you to configure the degree of parallelism\n¡ Merges the results using a reducer function\nThe .NET operator Task.WhenAll  and the F# Async.Parallel  can compose a set of \ngiven tasks in parallel; but these operators don’t provide an aggregate (or reduce) functionality to join the results. Moreover, they lack configurability when you want to control the degree of parallelism. To get your desired operator, you need a tailored solution.\n13.2.1 Solution: composing a pipeline of steps  forming the Fork/Join pattern\nWith TDF, you can compose different building blocks together as a pipeline. You can use the pipeline to define the steps of a Fork/Join pattern (figure 13.5), where the Fork step runs a set of tasks in parallel, then the following step joins the results, and the final step applies a reducer block for the ultimate output. For the later step of the workflow that aggregates the results, you need an object that maintains the state of the previous steps. In this case, you use the agent-based block built in chap-ter 12 using TDF.\nThe Fork/Join pattern is implemented as an extension method over a generic \nIEnu-\nmerable  to be accessed conveniently in a fluent style from the code, as shown in listing \n13.2 (the code to note is in bold).\n \n 403 Custom parallel Fork/Join operator \nBuffer\nblockTransformManyBlock\nTransformManyBlockFork\nMergeJoin\nAs\nobservable\nRxTransformManyBlock\nBehavior\n(message\nprocessor)Mailbox\nqueueStateAgent block\nFigure 13.5  Fork/Join pattern implemented using TDF, where each step of the computation is defined \nusing a different dataflow block\nListing 13.2  Parallel ForkJoin  using TDF\n    public static async Task<R> ForkJoin<T1, T2, R>(        this IEnumerable<T1> source,              Func<T1, Task<IEnumerable<T2>>> map,          Func<R, T2, Task<R>> aggregate,            R initialState, CancellationTokenSource cts = null,        int partitionLevel = 8, int boundCapacity = 20)       {       cts = cts ?? new CancellationTokenSource();       var blockOptions = new ExecutionDataflowBlockOptions {            MaxDegreeOfParallelism = partitionLevel,            BoundedCapacity = boundCapacity,            CancellationToken = cts.Token        };                   var inputBuffer = new BufferBlock<T1>(               new DataflowBlockOptions {                   CancellationToken = cts.Token,                   BoundedCapacity = boundCapacity                });              var mapperBlock = new TransformManyBlock<T1, T2>\n➥ (map, blockOptions); \n       var reducerAgent = Agent.Start(initialState, aggregate, cts);         var linkOptions = new DataflowLinkOptions{PropagateCompletion=true};       inputBuffer.LinkTo(mapperBlock, linkOptions);           IDisposable disposable = mapperBlock.AsObservable()            .Subscribe(async item => await reducerAgent.Send(item));          foreach (var item in source)            await inputBuffer.SendAsync(item);           inputBuffer.Complete();The functions map and aggregate return a Task type to ensure concurrent behavior.The partitionLevel is set to a default value of 8 and the \nboundCapacity to 20; these are arbitrary values that \ncan, and should, be updated according to your needs.\nInstances of the building blocks that are \nshaping the Fork/Join pipeline \nPackages up the properties of \nthe execution details to configure \nthe inputBuffer BufferBlock\nConnects the building blocks to form and link the steps to run the Fork/Join pattern\nThe TransformManyBlock is \ntransformed into an Observable, \nwhich is used to push the outputs as a \nmessage to the reducerAgent. Starts the Fork/Join process by pushing the items of the input collection into the first step of the pipeline",6215
132-13.3.1 Solution implementing a dependencies graph of tasks.pdf,132-13.3.1 Solution implementing a dependencies graph of tasks,"404 chapter  13 Recipes and design patterns for successful concurrent programming\n        var tcs = new TaskCompletionSource<R>();        await inputBuffer.Completion.ContinueWith(task =>                                          mapperBlock.Complete());        await mapperBlock.Completion.ContinueWith(task => {             var agent = reducerAgent as StatefulDataflowAgent<R, T2>;            disposable.Dispose();            tcs.SetResult(agent.State);        });                return await tcs.Task;    }\nThe ForkJoin  extension method accepts as an argument the IEnumerable  source to \nprocess a mapping function, to transform its items, as well as an aggregate (reducer) function to merge all the results coming from the mapping computation. The argu-ment \ninitialState  is the seed required by the aggregate function for the initial state \nvalue. But if the result type T2 can be combined (because the monoidal laws are satis-\nfied), you could modify the method to use a reducer function with zero initial state, as explained in listing 5.10. \nThe underlying dataflow blocks are linked to form a pipeline. Interestingly, \nmap-\nperBlock  is converted into an Observable  using the AsObservable  extension method, \nwhich is then subscribed to send messages to the reducerAgent  when an output is mate-\nrialized. The values partitionLevel  and boundCapacity  are used respectively to set \nthe degree of parallelism and the bound capacity.\nHere is a simple example of how to exploit the ForkJoin  operator: \nTask<long> sum = Enumerable.Range(1, 100000)          .ForkJoin<int, long, long>(                        async x => new[] { (long)x * x },                        async (state, x) => state + x, 0L);\nThe previous code sums the squares of all number from 1 to 100,000 using the Fork/Join pattern.\n13.3 Parallelizing tasks with dependencies: designing code to optimize performance\nLet’s imagine you need to write a tool that can execute a series of asynchronous tasks—each with a different set of dependencies that influence the order of the operations. You can address this with sequential and imperative execution; but if you want to max-imize performance, sequential operations won’t do. Instead, you must build the tasks to run in parallel. Many concurrent problems can be considered a static collection of atomic operations with dependencies between their inputs and outputs. On comple-tion of the operation, the output is used as input to other dependent operations. To optimize performance, these tasks need to be scheduled based on the dependency, and the algorithm must be optimized to run the dependent tasks in serial as necessary and in parallel as much as possible.\nYou want a reusable component that runs a series of tasks in parallel, ensuring that \nall the dependencies that could influence the order of the operations are respected. How do you create a programming model that exposes the underlying parallelism of When the mapperBlock is completed, \nthe continuation Task sets the \nreducerAgent as a result for the tcs \nTask passed to the caller as output. \n \n 405 Parallelizing tasks with dependencies: designing code to optimize performance\na collection of operations that are executed efficiently, either in parallel or serially depending on the dependencies with other operations?\n13.3.1 Solution: implementing a dependencies graph of tasks\nThe solution is called a directed acyclic graph (DAG), which aims to form a graph by breaking down operations into a series of atomic tasks with defined dependencies. The acyclic nature of the graph is important because it removes the possibility of dead-locks between tasks, provided the tasks are truly atomic. When specifying the graph, it’s important to understand all dependencies between tasks, especially hidden dependen-cies that may result in deadlocks or race conditions. Figure 13.6 is a typical example of a data structure in the shape of a graph, which can be used to represent scheduling constraints between the operations of the graph. A graph is an extremely powerful data structure in computer science that gives rise to strong algorithms.\n1\n2\n34\n6\n57\n8\nYou can apply the DAG structure as a strategy to run tasks in parallel while respecting the order of the dependencies for increasing performance. You can define this graph structure using the F# \nMailboxProcessor , which keeps an internal state for the tasks \nregistered to be performed in the shape of edge dependencies.\nValidating a directed acyclic graph\nWhen working with any graph data structure, like a DAG, you need to deal with the prob-lem of registering the edges correctly. In figure 13.6, what if node 2 with dependen -\ncies to nodes 7 and 8 is registered, but node 8 doesn’t exist? It could also happen that some edges depend on each other, which would lead to a directed cycle. In the case of a directed cycle, it’s critical to run tasks in parallel; otherwise certain tasks could wait for -\never on another to complete, in a deadlock.The solution is called topological sort, which means that you can order all the vertices of the graph in such a way that all its directed edges target from a vertex earlier in the order to a vertex later in that order. For example, if task A must complete before task B, and task B must complete before task C which must complete before task A, then there’s a cycle reference, and the system will notify you of the mistake by throwing an exception. If a precedence constraint has a direct cycle, then there’s no solution. This kind of checking is called directed cycle detection. If a directed graph has satisfied these rules, it’s consid-ered a DAG, which is primed to run several tasks that have dependencies in parallel.You can find the complete version of listing 13.4, which includes the DAG validation, in the source code online. \n Figure 13.6  A graph is a collection \nof vertices connected by edges. In this representation of a DAG, node 1 has dependencies on nodes 4 and 5, node 2 depends on node 5, node 3 has dependencies on nodes 5 and 6, and so on.\n \n406 chapter  13 Recipes and design patterns for successful concurrent programming\nThe following example uses the F# MailboxProcessor  as a perfect candidate to imple-\nment a DAG to run in parallel operations with dependencies. First, let’s define the discriminated union used to manage the tasks and run their dependencies.\nListing 13.3  Message type and data structure to coordinate task execution\ntype TaskMessage =             | AddTask of int * TaskInfo    | QueueTask of TaskInfo    | ExecuteTasks     and TaskInfo =         { Context : System.Threading.ExecutionContext        Edges : int array; Id : int; Task : Func<Task>      EdgesLeft : int option; Start : DateTimeOffset option      End : DateTimeOffset option }\nThe TaskMessage type represents the message cases sent to the underlying agent \nof the ParallelTasksDAG , implemented in listing 13.4. These messages are used for \ntask coordination and dependency synchronization. The TaskInfo type contains and \ntracks the details of the registered tasks during the execution of the DAG, including the dependency edges. The execution context (http:/ /mng.bz/2F9o) is captured to access information during the delayed execution, such as the current user, any state associated with the logical thread of execution, code-access security information, and so forth. The start and end for the execution time are published when the event fires.\nListing 13.4  DAG F# agent to parallelize the execution of operations\ntype ParallelTasksDAG() =    let onTaskCompleted = new Event<TaskInfo>()    let dagAgent = new MailboxProcessor<TaskMessage>(fun inbox ->    let rec loop (tasks : Dictionary<int, TaskInfo>)                    (edges : Dictionary<int, int list>) = async {      let! msg = inbox.Receive()      match msg with    | ExecuteTasks ->               let fromTo = new Dictionary<int, int list>()        let ops = new Dictionary<int, TaskInfo>()           for KeyValue(key, value) in tasks do              let operation =                { value with EdgesLeft = Some(value.Edges.Length) }            for from in operation.Edges do              let exists, lstDependencies = fromTo.TryGetValue(from)              if not <| exists then                 fromTo.Add(from, [ operation.Id ])              else fromTo.[from] <- (operation.Id :: lstDependencies)              ops.Add(key, operation)        ops |> Seq.iter (fun kv ->       Commands are sent to the ParallelTasksDAG underlying dagAgent agent, which is responsible for the task’s execution coordination.\nWraps the details of each task to run\nEvent that shows an instance of onTaskCompletedEvent, utilized to notify when a task completesAgent internal state to track the tasks and their \ndependencies. The collections are mutable because the \nstate changes during the execution of the \nParallelTasksDAG, and because they inherited thread \nsafety from being inside an agent.\nWaits \nasynchronously \nfor a message\nMessage case that starts execution of ParallelTasksDAG\nCollection that maps a monotonically \nincreased index with a task to run The process iterates through the task list, analyzing the dependencies among the other tasks to create a topological structure representing the order of the task execution. \n \n 407 Parallelizing tasks with dependencies: designing code to optimize performance\n            match kv.Value.EdgesLeft with                  | Some(n) when n = 0 -> inbox.Post(QueueTask(kv.Value))            | _ -> ())        return! loop ops fromTo    | QueueTask(op) ->           Async.Start <| async {             let start = DateTimeOffset.Now            match op.Context with             | null -> op.Task.Invoke() |> Async.AwaitATsk            | ctx -> ExecutionContext.Run(ctx.CreateCopy(),                       (fun op -> let opCtx = (op :?> TaskInfo)                                opCtx.Task.Invoke().ConfigureAwait(false)), \n➥ taskInfo)\n            let end' = DateTimeOffset.Now            onTaskCompleted.Trigger  { op with Start = Some(start)                                               End = Some(end') }             let exists, deps = edges.TryGetValue(op.Id)            if exists && deps.Length > 0 then               let depOps = getDependentOperation deps tasks []               edges.Remove(op.Id) |> ignore               depOps |> Seq.iter (fun nestedOp ->                               inbox.Post(QueueTask(nestedOp))) }        return! loop tasks edges     | AddTask(id, op) -> tasks.Add(id, op)                            return! loop tasks edges }    loop (new Dictionary<int, TaskInfo>(HashIdentity.Structural))          (new Dictionary<int, int list>(HashIdentity.Structural)))     [<CLIEventAttribute>]  member this.OnTaskCompleted = onTaskCompleted.Publish      member this.ExecuteTasks() = dagAgent.Post ExecuteTasks    member this.AddTask(id, task, [<ParamArray>] edges : int array) =    let data = { Context = ExecutionContext.Capture()                 Edges = edges; Id = id; Task = task                 NumRemainingEdges = None; Start = None; End = None }    dagAgent.Post(AddTask(id, data))   \nThe purpose of the function AddTask  is to register a task including arbitrary depen -\ndency edges. This function accepts a unique ID, a function task that must be executed, and a set of edges that represent the IDs of other registered tasks, all of which must be completed before the current task can be executed. If the array is empty, it means there are no dependencies. The \nMailboxProcessor  named dagAgent  keeps the registered \ntasks in a current state tasks , which is a map ( tasks : Dictionary<int, TaskInfo> ) \nbetween the ID of each task and its details. The agent also keeps the state of the edge dependencies for each task ID (\nedges : Dictionary<int, int list> ). The Dic-\ntionary  collections are mutable because the state changes during the execution of \nthe ParallelTasksDAG , and because they inherited thread safety from being inside an \nagent. When the agent receives the notification to start the execution, part of the pro -\ncess involves verifying that all the edge dependencies are registered and that there are Message case to queue up a task, run it, and ultimately, remove it from the agent state as an active dependency when it completes \nIf the ExecutionContext captured is null, then runs the task function in the current context.\nRuns the task using the ExecutionContext captured\nTriggers and publishes the onTaskCompleted event to notify that a task is completed. The event contains the task information.\nMessage case that adds a task to be executed according to its dependencies, if any\nStarts the execution of \nthe tasks registered Adds a task, its dependencies, and the current ExecutionContext for the DAG execution. \n \n408 chapter  13 Recipes and design patterns for successful concurrent programming\nno cycles within the graph. This verification step is available in the full implementation of the \nParallelTasksDAG in the downloadable source code. The following code is an \nexample in C# that references and consumes the F# library to run the ParallelTasks -\nDAG. The tasks registered mirror the dependencies from figure 13.6: \nFunc<int, int, Func<Task>> action = (id, delay) => async () => {    Console.WriteLine($""Starting operation{id} in Thread Id {Thread.CurrentThread.ManagedThreadId} . . . "");    await Task.Delay(delay);};var dagAsync = new DAG.ParallelTasksDAG();dagAsync.OnTaskCompleted.Subscribe(op =>     Console.WriteLine($""Operation {op.Id} completed in Thread Id      { \nThread.CurrentThread.ManagedThreadId}""));\ndagAsync.AddTask(1, action(1, 600), 4, 5);dagAsync.AddTask(2, action(2, 200), 5);dagAsync.AddTask(3, action(3, 800), 6, 5);dagAsync.AddTask(4, action(4, 500), 6);dagAsync.AddTask(5, action(5, 450), 7, 8);dagAsync.AddTask(6, action(6, 100), 7);dagAsync.AddTask(7, action(7, 900));dagAsync.AddTask(8, action(8, 700));dagAsync.ExecuteTasks();\nThe helper function’s action  purpose is to print when a task starts, indicating the \ncurrent thread Id as a reference to prove the multithreaded functionality. The event \nOnTaskCompleted  is registered to notify when each task completes printing in the \nconsole the task ID and the current thread Id. Here’s the output when the method \nExecute Tasks  is called:\nStarting operation 8 in Thread Id 23...Starting operation 7 in Thread Id 24...Operation 8 Completed in Thread Id 23Operation 7 Completed in Thread Id 24Starting operation 5 in Thread Id 23...Starting operation 6 in Thread Id 25...Operation 6 Completed in Thread Id 25Starting operation 4 in Thread Id 24...Operation 5 Completed in Thread Id 23Starting operation 2 in Thread Id 27...Starting operation 3 in Thread Id 30...Operation 4 Completed in Thread Id 24Starting operation 1 in Thread Id 28...Operation 2 Completed in Thread Id 27Operation 1 Completed in Thread Id 28Operation 3 Completed in Thread Id 30\nAs you can see, the tasks run in parallel with a different thread of execution (different thread ID), and the dependency order is preserved.",15133
133-13.4.1 Solution applying multiple readwrite operations to shared thread-safe resources.pdf,133-13.4.1 Solution applying multiple readwrite operations to shared thread-safe resources,,0
134-13.4 Gate for coordinating concurrent IO operations sharing resources one write multiple reads.pdf,134-13.4 Gate for coordinating concurrent IO operations sharing resources one write multiple reads,"409 Gate for coordinating concurrent I/O operations sharing resources: one write, multiple reads \n13.4 Gate for coordinating concurrent I/O operations sharing resources: one write, multiple reads \nImagine you’re implementing a server application where there are many concurrent \nclient requests coming in. These concurrent requests came into the server application because of the need to access shared data. Occasionally, a request that needs to modify the shared data would come in, requiring the data to be synchronized.  \nWhen a new client request arrives, the thread pool dispatches a thread to handle the \nrequest and to start the processing. Imagine if at this point the request wants to update data in the server in a thread-safe manner. You must face the problem of how to coordi-nate the read and write operations so that they access the resources concurrently with-out blocking. In this case, blocking means to coordinate the access of a shared resource. In doing so, the write operation locks the other operations to take ownership of the resource until its operation is complete.\nA possible solution is to use primitive lock, such as \nReaderWriterLockSlim ( http:/ /\nmng.bz/FY0J), which also manages access to a resource, allowing multiple threads. \nBut in this book you learned that you should avoid using primitive locks when possi-\nble. Locks prevent the code from running in parallel, and in many cases, overwhelm the thread pool by forcing it to create a new thread for each request. The other threads are blocked from acquiring access to the same resources. Another downside is that locks could be held for an extremely long time, causing the threads that have been awakened from the thread pool to process the read requests, to be immediately put to sleep wait-ing for the writer thread to complete its task. Additionally, this design doesn’t scale.\nFinally, the read and write operations should be handled differently to allow multi-\nple reads to happen simultaneously, because these operations don’t change the data. This should be balanced by ensuring write operations are only processed one at a time, while blocking the reads from retrieving stale data.\nYou need a custom coordinator that can synchronize the read and write operations \nasynchronously without blocking. This coordinator should execute the writes one at a time in sequential order without blocking any threads and leave the reads to run in parallel.\n13.4.1 Solution: applying multiple read/write operations to shared thread-safe resources\nReaderWriterAgent  offers reader-writer asynchronous semantics without blocking \nany threads and maintains a FIFO order of operations. It reduces resource consump-tion and improves the performance of the application. In fact, \nReaderWriterAgent  \ncan perform an extraordinary amount of work using only a few threads. Regardless of the number of operations being made against the \nReaderWriterAgent , only a few \nresources are required.\nIn the examples that follow, you want to send multiple read and write operations \nto a shared database. These operations are processed giving higher priority to reader \n \n410 chapter  13 Recipes and design patterns for successful concurrent programming\nthreads than writers, as shown in figure 13.7. The same concepts can be applied to any other resources, such as a filesystem.\nNOTE  In general, ReaderWriterAgent  is a better fit for programs that concur -\nrently access resources asynchronously using I/O operations. \n Gate agent,\nsynchronizingone write andmultiple reads\nOnly one write operation is executed at a time.\nWhen multiple operations arrive to access thedatabase, the read operations are queued towait asynchronously for the write operationto complete before proceeding.GateBehavior\nState\nRead\nWrite\nRead Write\nRead\nRead\nBehavior\nStateDatabase\nWhen multiple read operations arrive,they access the database and are processedasynchronously in parallel according to the\ndegree of parallelism configured.GateRead\nRead\nRead\nRead\nReadReadReadRead\nReadReadDatabase\nFigure 13.7  ReaderWriterAgent  acts as a gate agent to asynchronously synchronize the access \nto share resources. In the top image, only one write operation at a time is executed, while the read operations are queued up to wait asynchronously for the write operation to complete before proceeding. In the bottom image, multiple read operations are processed asynchronously in parallel according to the degree of parallelism configured.\n \n 411 Gate for coordinating concurrent I/O operations sharing resources: one write, multiple reads \nListing 13.5 is the implementation of ReaderWriterAgent  using the F# MailboxPro -\ncessor . The reason for choosing the F# MailboxProcessor  is for simplicity in defin-\ning state machines, which are convenient to implement a reader-writer asynchronous coordinator. First, you need to define the message types to represent the operations by which the \nReaderWriterAgent  coordinates and synchronizes the read and write \noperations.\nListing 13.5  Message types used by the ReaderWriterAgent  coordinator  \ntype ReaderWriterMsg <'r,'w> =             | Command of ReadWriteMessages<'r,'w>    | CommandCompleted            and ReaderWriterGateState =               | SendWrite    | SendRead of count:int    | Idleand ReadWriteMessages<'r,'w> =       | Read of r:'r    | Write of w:'w\nThe ReaderWriterMsg message type denotes the command to either read or write to \nthe database or to notify that the operation is completed. ReaderWriterGateState is \na DU used to queue up the read/write operations to the ReaderWriterAgent . Ulti-\nmately, the ReadWriteMessages  DU identifies the cases for the read/write operations \nqueued in the internal ReaderWriterAgent .\nThis listing shows the implementation of the ReaderWriterAgent type.\nListing 13.6  ReaderWriterAgent  coordinates asynchronous operations \ntype ReaderWriterAgent<'r,'w>(workers:int, behavior: MailboxProcessor<ReadWriteMessages<'r,'w>> -> \n➥ Async<unit>,?errorHandler, ?cts:CancellationTokenSource) =  \n    let cts = defaultArg cts (new CancellationTokenSource())      let errorHandler = defaultArg errorHandler ignore             let supervisor = MailboxProcessor<Exception>.Start(fun inbox -> async {            while true do                       let! error = inbox.Receive(); errorHandler error })    let agent = MailboxProcessor<ReaderWriterMsg<'r,'w>>.Start(fun inbox ->        let agents = Array.init workers (fun _ ->                      (new AgentDisposable<ReadWriteMsg<'r,'w>>(behavior, cts))                .withSupervisor supervisor)      Uses a DU about the command cases to send to queue up the read/write operations\nUses message types to change the state and coordinate the operations in the internal queue of the ReaderWriterAgent\nThe constructor takes the number of workers to configure the degree of parallelism, the behavior of the agent that accesses the database for the read/write operations, and the optional arguments to handle errors and to cancel the underlying agent to stop the still-active operations. If the optional arguments aren’t passed into the \nconstructor, they’re initialized with the default value.\nThe supervisor agent handles exceptions. A while-true loop is used to wait asynchronously for incoming messages.\nCreates a collection of agents with given behavior \npassed to parallelize the read/write operations to \nthe database. Access is synchronized.Each newly created agent registers the error handler to notify the supervisor agent.\n \n412 chapter  13 Recipes and design patterns for successful concurrent programming\n        cts.Token.Register(fun () ->           agents |> Array.iter(fun agent -> (agent:>IDisposable).Dispose()))        let writeQueue = Queue<_>()                  let readQueue = Queue<_>()                   let rec loop i state = async {            let! msg = inbox.Receive()            let next = (i+1) % workers              match msg with            | Command(Read(req)) ->                match state with                       | Idle -> agents.[i].Agent.Post(Read(req))                          return! loop next (SendRead 1)                | SendRead(n) when writeQueue.Count = 0 ->                    agents.[i].Agent.Post(Read(req))                    return! loop next (SendRead(n+1))                | _ -> readQueue.Enqueue(req)                       return! loop i state            | Command(Write(req)) ->                         match state with                | Idle -> agents.[i].Agent.Post(Write(req))                          return! loop next SendWrite                | SendRead(_) | SendWrite -> writeQueue.Enqueue(req)                                             return! loop i state            | CommandCompleted ->                       match state with                | Idle -> failwith ""Operation no possible""                | SendRead(n) when n > 1 -> return! loop i (SendRead(n-1))                | SendWrite | SendRead(_) ->                    if writeQueue.Count > 0 then                        let req = writeQueue.Dequeue()                        agents.[i].Agent.Post(Write(req))                        return! loop next SendWrite                    elif readQueue.Count > 0 then                        readQueue |> Seq.iteri (fun j req ->                            agents.[(i+j)%workers].Agent.Post(Read(req)))                        let count = readQueue.Count                        readQueue.Clear()                        return! loop ((i+ count)%workers) (SendRead count)                    else return! loop i Idle }        loop 0 Idle), cts.Token)    let postAndAsyncReply cmd createRequest =        agent.PostAndAsyncReply(fun ch ->                          createRequest(AsyncReplyChannelWithAck(ch, fun () -> \n➥ agent.Post(CommandCompleted))) |> cmd |> ReaderWriterMsg.Command \n    member this.Read(readRequest) = postAndAsyncReply Read  readRequest    member thisWrite(writeRequest) = postAndAsyncReply Write writeRequest\nThe implementation of the underlying F# MailboxProcessor , in the ReaderWriter-\nAgent  type, is a multi-state machine that coordinates exclusive writes and reads access \nto shared resources. The ReaderWriterAgent  creates sub-agents that access the Registers the cancellation strategy to \nstop the underlying agent workers \nUses internal queues to manage the access and execution of the read/write operations \nThe Command Read case, \nbased on the current agent \nstate, can queue up a new read \noperation, start an  Read \noperation when the writeQueue \nis empty, or stay idle. The Command Write case, based on the current agent state, can either stay idle or queue up a Write operation.  \nCommandCompleted notifies when an operation completes to update the current state of the read/write queues. \nThis function establishes asynchronous bidirectional communication between the agent and the caller to send the command and wait, without blocking, for the response.  \n \n 413 Gate for coordinating concurrent I/O operations sharing resources: one write, multiple reads \nresources based on the ReadWriteMsg  message type received. When the agent coordi-\nnator receives a Read  command, its current state is checked using pattern matching to \napply exclusive access logic:\n¡ If the state is Idle , the Read  command is sent to the agent children to be processed. \nIf there are no active writes, then the state of the main agent is changed to SendRead.\n¡ If the state is SendRead , the Read  operation is sent to the agent’s children to be \nperformed only if there are no active writes.\n¡ In all other cases, the Read  operation is placed in the local Read  queue for later \nprocessing.\nIn the case of a Write  command sent to the agent coordinator, the message is pattern \nmatched and processed according to its current state:\n¡ If the state is Idle , the Write  command is sent to the sub-agent inboxes to be pro-\ncessed. The state of the main agent is then changed to SendWrite.\n¡ In all other cases, the Write  operation is placed in the local Write  queue for later \nprocessing.\nFigure 13.8 shows the ReaderWriterAgent  multi-state machine.\n If the state is Idle, the readcommand is sent to the agent’schildren to be processed. Ifthere are no active writes,the state of the main agentis changed to SendRead.If the state is Idle, the write command\nis sent to the subagents’ inboxes to beprocessed. The state of the main agentis changed to SendW rite.\nIf the state is SendW rite,\nthe write operation isqueued in the local write\nqueue for later processing.If the state is SendRead,\na read operation is queued\nin the local read queue for\nlater processing.If the state is SendRead,the read operation is sentto the agent’s children, tobe per\nformed only if there\nare no active writes.Send readSend writeWrite messageIdle Behavior\nState\nReadRead\nRead Database\nWrite\nWrite messageRead messageRead message\nFigure 13.8  The ReaderWriterAgent  works as a state machine, where each state aims to \nasynchronously synchronize the access to share resources (in this case, a database).",13155
135-13.6.1 Solution implementing a polymorphic  publisher-subscriber pattern.pdf,135-13.6.1 Solution implementing a polymorphic  publisher-subscriber pattern,"414 chapter  13 Recipes and design patterns for successful concurrent programming\nThe following code snippet is a simple example that uses ReaderWriterAgent . For sim-\nplicity, instead of concurrently accessing a database, you’re accessing a local mutable dictionary in a thread-safe and non-blocking manner:\ntype Person  = { id:int; firstName:string; lastName:string; age:int }let myDB = Dictionary<int, Person>()let agentSql connectionString =    fun (inbox: MailboxProcessor<_>) ->        let rec loop() = async {            let! msg = inbox.Receive()            match msg with            | Read(Get(id, reply)) ->                match myDB.TryGetValue(id) with                | true, res -> reply.Reply(Some res)                | _ -> reply.Reply(None)            | Write(Add(person, reply)) ->                let id = myDB.Count                myDB.Add(id, {person with id = id})                reply.Reply(Some id)            return! loop() }        loop()let agent = ReaderWriterAgent(maxOpenConnection, agentSql connectionString)let write person = async {    let! id = agent.Write(fun ch -> Add(person, ch))    do! Async.Sleep(100)}let read personId = async {    let! resp = agent.Read(fun ch -> Get(personId, ch))    do! Async.Sleep(100)}[ for person in people do     yield write  person    yield read person.Id    yield write  person    yield read person.Id    yield read person.Id ]    |> Async.Parallel\nThe code example creates the agentSql  object, whose purpose it is to emulate a data-\nbase accessing the local resource myDB . The instance agent of the ReaderWriterAgent  \ntype coordinates the parallel operations reads and writes, which accesses concurrently and in a thread-safe manner the \nmyDB  dictionary without blocking. In a real-world sce-\nnario, the mutable collection myDB  represents a database, a file, or any sort of shared \nresource.\n13.5 Thread-safe random number generator\nOften, when dealing with multithreaded code, you need to generate random num-bers for an operation in the program. For example, suppose you’re writing a web \n \n 415 Thread-safe random number generator\nserver application that needs to randomly send back an audio clip when a user sends a request. For performance reasons, the set of audio clips is loaded in the memory of the server, which is concurrently receiving a large number of requests. For each request, an audio clip must be randomly selected and sent back to the user to be played. \nIn most cases, the \nSystem.Random  class is a fast-enough solution for producing ran-\ndom number values. But an effective application of a Random  instance that is accessed in \nparallel becomes a challenging problem to solve in a high-performance style. When an instance of the \nRandom  class is used by multiple threads, its internal state can be compro-\nmised, and it will potentially always return zero. \nNOTE  The System.Random  class may not be random in the crypto-graphical \nsense. If you care about the quality of the random numbers, you should be using \nRNGCryptoServiceProvider , which generates cryptographically strong \nrandom numbers.\n13.5.1 Solution: using the ThreadLocal object\nThreadLocal<T>  ensures that each thread receives its own instance of a Random  class, \nguaranteeing completely thread-safe access even in a multithreaded program. The fol-lowing listing shows the implementation of the thread-safe random number generator using the \nThreadLocal<T>  class, which provides a strongly typed and locally scoped \ntype to create object instances that are kept separate for each thread.\nListing 13.7  Thread-safe random number generator \npublic class ThreadSafeRandom : Random{    private ThreadLocal<Random> random =        new ThreadLocal<Random>(() => new Random(MakeRandomSeed()));      public override int Next() => random.Value.Next();            public override int Next(int maxValue) =>                                   random.Value.Next(maxValue);     public override int Next(int minValue, int maxValue) =>                                       random.Value.Next(minValue, maxValue);    public override double NextDouble() => random.Value.NextDouble();     public override void NextBytes(byte[] buffer) =>                                                   random.Value.NextBytes(buffer);    static int MakeRandomSeed() =>                      Guid.NewGuid().ToString().GetHashCode(); } \nThreadSafeRandom  represents a thread-safe pseudo random number generator. \nThis class is a subclass of Random  and overrides the methods  Next , NextDouble , and Creates a thread-safe random number \ngenerator using the ThreadLocal<T> class\nExposes the Random class methodsCreates a seed that doesn’t depend on the system clock. \nA unique value is created with each invocation.",4778
136-13.6 Polymorphic event aggregator.pdf,136-13.6 Polymorphic event aggregator,"416 chapter  13 Recipes and design patterns for successful concurrent programming\nNextBytes . The MakeRandomSeed  method provides a unique value for each instance of \nthe underlying Random  class, which does not depend on the system clock.\nThe constructor for ThreadLocal<T>  accepts a Func<T>  delegate to create a \nthread-local instance of the Random  class. The ThreadLocal<T>.Value  is used to access \nthe underlying value. Here you access the ThreadSafeRandom  instance from a parallel \nloop to simulate a concurrent environment. \nIn this example, the parallel loop calls ThreadSafeRandom  concurrently to obtain a \nrandom number for accessing the clips  array: \nvar safeRandom = new ThreadSafeRandom();string[] clips = new string[] { ""1.mp3"", ""2.mp3"", ""3.mp3"", ""4.mp3""};Parallel.For(0, 1000, (i) =>{     var clipIndex = safeRandom.Next(4);     var clip = clips[clipIndex];     Console.WriteLine($""clip to play {clip} - Thread Id                           {Thread.CurrentThread.ManagedThreadId}"");});\nHere's the result, in print or on the console: \nclip to play 2.mp3 - Thread Id 11clip to play 2.mp3 - Thread Id 8clip to play 1.mp3 - Thread Id 20clip to play 2.mp3 - Thread Id 20clip to play 4.mp3 - Thread Id 13clip to play 1.mp3 - Thread Id 8clip to play 4.mp3 - Thread Id 11clip to play 3.mp3 - Thread Id 11clip to play 2.mp3 - Thread Id 20clip to play 3.mp3 - Thread Id 13\nNOTE  A single instance of ThreadLocal<T>  allocates a few hundred bytes, so it’s \nimportant to consider how many of these active instances are necessary at any time. If the program requires many parallel operations, it’s recommended to work on a local copy to avoid accessing thread-local storage as much as possible.\n13.6 Polymorphic event aggregator\nIn this section, assume that you need a tool to work in a program that requires raising several events of different types in the system, and then has a publish and subscribe system that can access these events.\n13.6.1 Solution: implementing a polymorphic  publisher-subscriber pattern\nFigure 13.9 illustrates how to manage events of different types. Listing 13.8 shows the \nEventAggregator  implementation using Rx (in bold).\n \n 417 Polymorphic event aggregator\n Publishers thatemit events ofdifferent typesPublisher\nevent type CIEventAggregatorGetEvent<'Event> : unit -> IObservable<'Event>Publish<'Event> : 'Event -> unit\nEventAggregato r\nsubject\n.SubscribeOn(TaskPoolScheduler\nWhen events are pushedas a stream, each differenttype of event notifies the\nsubscriber of the same type.EventAggregator manages eventsof different types. The events aretreated as obser vable using\nreactive extensions.Publisher\nevent type BPublisher\nevent type A\nSubscriber\nevent type ASubscriber\nevent type B\nSubscriber\nevent type CSubscriber\nevent type B\nGetEvent\nPublish\nFigure 13.9  The EventAggregator  manages events of different types. When the events are \npublished, the EventAggregator  matches and notifies the subscriber and events of the same type.\nListing 13.8  EventAggregator  using Rx\ntype IEventAggregator =             inherit IDisposable             abstract GetEvent<'Event> : unit -> IObservable<'Event>    abstract Publish<'Event> : eventToPublish:'Event -> unittype internal EventAggregator() =    let disposedErrorMessage = ""The EventAggregator is already disposed.""    let subject = new Subject<obj>()           interface IEventAggregator with          member this.GetEvent<'Event>(): IObservable<'Event> =              if (subject.IsDisposed) then failwith disposedErrorMessage            subject.OfType<'Event>().AsObservable<'Event>()                     .SubscribeOn(TaskPoolScheduler.Default)               member this.Publish(eventToPublish: 'Event): unit =               if (subject.IsDisposed) then failwith disposedErrorMessage            subject.OnNext(eventToPublish)                member this.Dispose(): unit = subject.Dispose()           static member Create() = new EventAggregator():>IEventAggregator Interfaces to define the contract for the EventAggregator, which also implements the IDisposable interface to ensure the cleanup of the resource subject\nInstance of the Subject type (Rx) that \ncoordinates event registration and \nnotification\nRetrieves the event as IObservable based on the type \nSubscribes Observable in the TaskPool scheduler to enforce concurrent behavior\nPublishes event notifications to \nall subscribers of the event type \n \n418 chapter  13 Recipes and design patterns for successful concurrent programming\nThe interface IEventAggregator helps to loosely couple the EventAggregator imple-\nmentation. This means that the consuming code won’t need to change (as long as the interface doesn’t change), even if the inner workings of the class change. Notice that \nIEventAggregator  inherits from IDisposable  to clean up any resources that were allo-\ncated when an instance of EventAggregator was created.\nThe methods GetEvent  and Publish  encapsulate an instance of the Rx Subject  type, \nwhich behaves as a hub for events. GetEvent  exposes IObservable  from the subject \ninstance to allow a simple way to handle event subscriptions. By default, the Rx Subject  \ntype is single threaded, so you use the SubscribeOn  extension method to ensure that \nEventAggregator  runs concurrently and exploits TaskPoolScheduler . The method \nPublish  notifies all the subscribers to the EventAggregator  concurrently.\nThe static member Create  generates an instance of EventAggregator  and exposes \nonly the single interface IEventAggregator . The following code example shows how to \nsubscribe to and publish events using the EventAggregator , and the output of running \nthe program:\nlet evtAggregator = EventAggregator.Create()type IncrementEvent = { Value: int }type ResetEvent = { ResetTime: DateTime }evtAggregator    .GetEvent<ResetEvent>()    .ObserveOn(Scheduler.CurrentThread)    .Subscribe(fun evt -> printfn ""Counter Reset at: %A - Thread Id %d"" \n➥ evt.ResetTime Thread.CurrentThread.ManagedThreadId)\nevtAggregator    .GetEvent<IncrementEvent>()    .ObserveOn(Scheduler.CurrentThread)    .Subscribe(fun evt ->  printfn ""Counter Incremented. Value: %d - Thread \n➥ Id %d"" evt.Value Thread.CurrentThread.ManagedThreadId)\nfor i in [0..10] do     evtAggregator.Publish({ Value = i })evtAggregator.Publish({ ResetTime = DateTime(2015, 10, 21) })\nHere’s the output:\nCounter Incremented. Value: 0 - Thread Id 1Counter Incremented. Value: 1 - Thread Id 1Counter Incremented. Value: 2 - Thread Id 1Counter Incremented. Value: 3 - Thread Id 1Counter Incremented. Value: 4 - Thread Id 1Counter Incremented. Value: 5 - Thread Id 1Counter Incremented. Value: 6 - Thread Id 1Counter Incremented. Value: 7 - Thread Id 1Counter Incremented. Value: 8 - Thread Id 1Counter Incremented. Value: 9 - Thread Id 1Counter Incremented. Value: 10 - Thread Id 1Counter Reset at: 10/21/2015 00:00:00 AM - Thread Id 1",6934
137-13.7.1 Solution implementing a scheduler withmultipleconcurrentagents.pdf,137-13.7.1 Solution implementing a scheduler withmultipleconcurrentagents,,0
138-13.7 Custom Rx scheduler to control thedegreeofparallelism.pdf,138-13.7 Custom Rx scheduler to control thedegreeofparallelism,"419 Custom Rx scheduler to control the degree of parallelism \nThe interesting idea of the EventAggregator  is how it handles events of different \ntypes. In the example, the EventAggregator  instance registers two different event \ntypes (IncrementEvent  and ResetEvent ), and the Subscribe  function sends the noti-\nfication by targeting only the subscribers for a specific event type.\n13.7 Custom Rx scheduler to control the degree of parallelism \nLet’s imagine you need to implement a system for querying large volumes of event streams asynchronously, and it requires a level of concurrency control. A valid solution for composing asynchronous and event-based programs is Rx, which is based on observ-ables to generate sequence data concurrently. But as discussed in chapter 6, Rx isn’t mul-tithreaded by default. To enable a concurrency model, it’s necessary to configure Rx to use a scheduler that supports multithreading by invoking the \nSubscribeOn  extension. For \nexample, Rx provides a few scheduler options including the TaskPool  and ThreadPool  \ntypes, which schedule all the actions to take place potentially using a different thread. \n But there’s a problem, because both schedulers start with one thread by default and \nthen have a time delay of about 500 ms before they’ll increase the number of threads required on demand. This behavior can have performance-critical consequences. \nFor example, consider a computer with four cores where there are eight actions \nscheduled. The Rx thread pool, by default, starts with one thread. If each action takes 2.000 ms, then three actions are queued up waiting for 500 ms before the Rx scheduler thread pool’s size is increased. Consequently, instead of executing four actions in par -\nallel right away, which would take 4 seconds in total for all eight actions, the work isn’t completed for 5.5 sec, because three of the tasks are idle in the queue for 500 ms. Fortu-nately, the cost of expanding the thread pool is only a one-time penalty. In this case, you need a custom Rx scheduler that supports concurrency with fine control over the level of parallelism. It should initialize the internal thread pool at startup time rather than when needed to avoid the cost during critical time computation.\nIf you enable concurrency in Rx using one of the available schedulers, there’s no \noption to configure the max degree of parallelism. This is a limitation, because in certain circumstances you only want few threads to be concurrently processing the event stream.\n13.7.1 Solution: implementing a scheduler with multiple concurrent agents \nThe Rx SubscribeOn  extension method requires passing as an argument an object that \nimplements the IScheduler  interface. The interface defines the methods responsible \nfor scheduling the action to be performed, either as soon as possible or at a point in the future. You can build a custom scheduler for Rx that supports the concurrency model with the option of configuring a degree of parallelism, shown in figure 13.10.\n \n420 chapter  13 Recipes and design patterns for successful concurrent programming\nPublishers that firemultiple events in\nparallelThe agent schedulerimplements the ISchedulerinterface to customize\nthe concurrent behaviorof the Rx scheduler .The Rx scheduler uses an agent tocoordinate and manage parallelism.\nThis is achieved by a pool of agentworkers that push notificationsto subscribers.\nPublisher CRx ISchedule r\nRx paralle l\nagent schedule r\nPublisher BPublisher A Subscriber\nSubscriber\nSubscriber\nSubscriberSubscriber\nPublishe rBehavior\nState\nBehavio rAgent\nStateBehavio rAgent\nState\nBehaviorAgent\nStateBehaviorAgent\nState\nFigure 13.10  ParallelAgentScheduler  is a custom scheduler that aims to tailor the concurrent \nbehavior of the Rx. The Rx scheduler uses an agent to coordinate and manage the parallelism. This is achieved by a pool of agent workers that push notifications to subscribers.\nThe following listing shows the implementation of the ParallelAgentScheduler  \nscheduler for Rx, which uses the agent parallelWorker  (shown in listing 11.5) to man-\nage the degree of parallelism (the code to note is in bold).\nListing 13.9  Rx custom scheduler for managing the degree of parallelism\ntype ScheduleMsg = ScheduleRequest * AsyncReplyChannel<IDisposable> let schedulerAgent (inbox:MailboxProcessor<ScheduleMsg>) =      let rec execute (queue:IPriorityQueue<ScheduleRequest>) =  async {        match queue |> PriorityQueue.tryPop with            | None -> return! idle queue -1        | Some(req, tail) ->            let timeout =                  int <| (req.Due - DateTimeOffset.Now).TotalMilliseconds            if timeout > 0 && (not req.IsCanceled)            then return! idle queue timeout            else                if not req.IsCanceled then req.Action.Invoke()Uses a message type to schedule a job. The message response is an IDisposable wrapped in a reply channel. The IDisposable object is used to cancel/unsubscribe the notification. The schedulerAgent function creates an instance of a \nMailboxProcessor that prioritizes and coordinates the \nrequest of jobs to run.\nWhen the agent is in execution and it receives a job request, the agent tries to pop a job to run from the internal priority-queue. If there are no jobs to execute, the agent switches to an idle state.\n \n 421 Custom Rx scheduler to control the degree of parallelism \n                return! execute tail  }    and idle (queue:IPriorityQueue<_>) timeout = async {           let! msg = inbox.TryReceive(timeout)        let queue =            match msg with            | None -> queue            | Some(request, replyChannel)->                   replyChannel.Reply(Disposable.Create(fun () ->                                             request.IsCanceled <- true))                   queue |> PriorityQueue.insert request        return! execute queue }    idle (PriorityQueue.empty(false)) -1type ParallelAgentScheduler(workers:int) =    let agent = MailboxProcessor<ScheduleMsg>                            .parallelWorker(workers, schedulerAgent)    interface IScheduler with                  member this.Schedule(state:'a, due:DateTimeOffset, \n➥ action:ScheduledAction<'a>) =\n            agent.PostAndReply(fun repl ->                         let action () = action.Invoke(this :> IScheduler, state)                let req = ScheduleRequest(due, Func<_>(action))                req, repl)        member this.Now = DateTimeOffset.Now            member this.Schedule(state:'a, action) =                    let scheduler = this :> IScheduler            let due = scheduler.Now            scheduler.Schedule(state, due, action)        member this.Schedule(state:'a, due:TimeSpan,                                       action:ScheduledAction<'a>) =            let scheduler = this :> IScheduler            let due = scheduler.Now.Add(due)            scheduler.Schedule(state, due, action)\nParallelAgentScheduler  introduces a level of concurrency to schedule and perform \nthe tasks pushed in a distributed pool of running agents (F# MailboxProcessor ). Note \nthat all actions sent to ParallelAgentScheduler  can potentially run out of order. \nParallel AgentScheduler  can be used as an Rx scheduler by injecting a new instance \ninto the SubscribeOn  extension method. The following code snippet is a simple exam-\nple to use this custom scheduler:\n    let scheduler = ParallelAgentScheduler(4)    Observable.Interval(TimeSpan.FromSeconds(0.4))        .SubscribeOn(scheduler)        .Subscribe(fun _ ->           printfn ""ThreadId: %A "" Thread.CurrentThread.ManagedThreadId) \nThe instance scheduler of the ParallelAgentScheduler  object is set to have four con-\ncurrent agents running and ready to react when a new notification is pushed. In the example, the observable operator \nInterval  sends a notification every 0.4 seconds, which \nis handled concurrently by the underlying agents of the parallelWorker . The benefits When the agent is in an idle state, and a job \nrequest arrives, the job is pushed to the local \nqueue  and scheduled for execution. Returns a disposable \nobject used to cancel \nthe scheduled action\nCreates an instance of the agent parallelWorker \n(from chapter 11), creating a collection of \nsub-agent workers passing the schedulerAgent \nbehavior \nImplements the IScheduler that defines a Reactive Extensions scheduler \nPosts and schedules a job request to the \ninstance of the parallelWorker, which \ndispatches the jobs to run in parallel \nthrough its internal agent workers",8587
139-13.8.1 Solution combining Rx and asynchronous programming.pdf,139-13.8.1 Solution combining Rx and asynchronous programming,"422 chapter  13 Recipes and design patterns for successful concurrent programming\nof using this custom ParallelAgentScheduler  scheduler is that there’s no downtime \nand delay in creating new threads, and it provides fine control over the degree of paral-lelism. There are times, for example, when you’ll want to limit the level of parallelism for analyzing an event stream, such as when events waiting to be processed are buffered in the internal queue of the underlying agents and consequently not lost.\n13.8 Concurrent reactive scalable client/server \nThe challenge: You need to create a server that listens asynchronously on a given port for incoming requests from multiple TCP clients. Additionally, you want the server to be\n¡ Reactive\n¡ Able to manage a large number of concurrent connections\n¡ Scalable\n¡ Responsive\n¡ Event driven\nThese requirements ensure that you can use functional high-order operations to com-pose the event stream operations over the TCP socket connections in a declarative and non-blocking way.\nNext, the client requests need to be processed concurrently by the server, with result-\ning responses sent back to the client. The Transmission Control Protocol (TCP) server connection can be either secured or unsecured. TCP is the most-used protocol on the internet today, used to provide accurate delivery that preserves the order of data pack-ets from one endpoint to another. TCP can detect when packets are wrong or missing, and it manages the action necessary for resending them. Connectivity is ultra-import-ant in applications, and the .NET Framework provides a variety of different ways to help you support that need. \nYou also need a long-running client program that uses TCP sockets to connect to \nthe server. After the connection is established, both the client and server endpoints can send and receive bytes asynchronously and sometimes close the connection properly and reopen it at a later time.\nThe client program that attempts to connect to the TCP server is asynchronous, \nnon-blocking, and capable of maintaining the application’s responsiveness, even under pressure (from sustaining a large number of data transfers). For this example, the cli-ent/server socket-based application continually transfers volumes of packets at a high rate of speed as soon as the connection is established. The data is transmitted from the server to the client streaming in chunks, where each chunk represents the historical stocks prices on a particular date. This stream of data is generated by reading and pars-ing the comma-separated values (CSV) files in the solution. When the client receives the data, it begins to update a chart in real time.\nThis scenario is applicable to any operations that use reactive programming based \non streams. Examples you may encounter are remote binary listeners, socket program-ming, and any other unpredictable event-oriented application, such as when a video needs to be streamed across the network.\n \n 423 Concurrent reactive scalable client/server \n13.8.1 Solution: combining Rx and asynchronous programming \nTo build the client/server program shown in listing 13.10, the CLR TcpListener  and \nTcpClient  classes provide a convenient model for creating a socket server with a few \ncode lines. Used in combination with TAP and Rx, they increase the level of scalabil-ity and reliability of the program. But to work in the reactive style, the traditional application design must change. \nSpecifically, to achieve the requirements of a high-performing TCP client/server \nprogram, you need to implement the TCP sockets in an asynchronous style. For this reason, consider using a combination of Rx and TAP. Reactive programming, in particular, fits this scenario because it can deal with source events from any stream regardless of its type (network, file, memory, and so on). Here’s the Rx definition from Microsoft:\nThe Reactive Extensions (Rx) is a library for composing asynchronous and event-based programs using observable sequences and LINQ-style query operators, and parameterize the concurrency in the asynchronous data streams using Schedulers.\nTo implement the server in a scalable way, the instance of the TcpListener  class listens \nfor incoming connections. When a connection is established, it’s routed, as a TcpCli-\nent, from the listener handler to manage the NetworkStream . This stream is then used \nfor reading and writing bytes for data-sharing between client and server. Figure 13.11 shows the connection logic of the server program.\n• Read StockData• Serializ e\n• Write to client\n   streamEvent connection\nSubscribeNetworkStrea mMultiple\nconnectionrequests\nThe Subscribe operator createsa NetworkStream from the clientconnection. The stream is used toinitiate communication between the\nclient and ser ver in a different thread.The task scheduler spawnsa task for each connection\nestablished.The T cpListener ser ver accepts client\nconnections asynchronously in a loop usingthe ToAcceptT cpClientObser vable operator .\nThe connection with the client isestablished. The event that carriesthe client stream is pushed throughthe obser vable pipeline.The communicationhandler serializes and\nsends StockData to the\nclient.\nFigure 13.11  The TcpListener  server accepts client connections asynchronously in a loop. When a \nconnection is established, the event that carries the client stream is pushed through the Observable  \npipeline to be processed. Next, the connection handlers start reading the stock ticker symbol histories, serialize, and write the data to the client \nNetworkStream . \n \n424 chapter  13 Recipes and design patterns for successful concurrent programming\nListing 13.10  Reactive TcpListener  server program \nstatic void ConnectServer(int port, string sslName = null){    var cts = new CancellationTokenSource();    string[] stockFiles = new string[] { ""aapl.csv"", ""amzn.csv"", ""fb.csv"", \n➥ ""goog.csv"", ""msft.csv"" };  \n    var formatter = new BinaryFormatter();       TcpListener.Create(port)          .ToAcceptTcpClientObservable()        .ObserveOn(TaskPoolScheduler.Default)          .Subscribe(client =>  {     using (var stream = GetServerStream(client, sslName))      {          stockFiles         .ObservableStreams(StockData.Parse)            .Subscribe(async stock => {           var data = Serialize(formatter, stock);            await stream.WriteAsync(data, 0, data.Length, cts.Token);           });     }    },    error => Console.WriteLine(""Error: "" + error.Message),      () => Console.WriteLine(""OnCompleted""),           cts.Token);}\nIn the example, the server shows the implementation of a reactive TCP listener that acts as an observable of the stock ticker. The natural approach for a listener is to sub-scribe to an endpoint and receive clients as they connect. This is achieved by the exten-sion method \nToAcceptTcpClientObservable , which produces an observable of the \nIObservable <TcpClient >. The ConnectServer  method uses the TcpListener .Create  \nconstruct to generate a TcpListener  using a given port number on which the server is \nlistening asynchronously, and an optional name of the Secure Sockets Layer (SSL) to establish a secure or regular connection. \nThe custom observable extension method \nToAcceptTcpClientObservable  uses the \ngiven TcpListener  instance to provide mid-level network services across an underlying \nsocket object. When a remote client becomes available and a connection is established, a \nTcpClient  object is created to handle the new communication, which is then sent \ninto a different long-running thread with the use of a Task  object. \nNext, to guarantee the concurrent behavior of the socket handler, the scheduler is \nconfigured using the ObserveOn  operator to subscribe and move the work to another \nscheduler, TaskPoolScheduler . In this way, the ToAcceptTcpClientObservable  opera-\ntor can orchestrate a large number of TcpClient s concurrently as a sequence. Collects \nstock ticker \nfiles (csv)\n.NET Binary Formatter used for convenience. The formatter type can be replaced with any other serializer. \nConverts a TcpListener into an observable sequence on a given portSubscribes the event flow from the ToAcceptTcpClientObservable to run on the current TaskPoll scheduler to ensure concurrent behavior\nCreates the Network stream to start the communication and \ndata transfer. If the sslName value is provided, the Network \nstream returned uses a secure SSL base socket\nReturns an observable that pushes and parses the stock histories from each file into a collection of StockData type Subscribes the event notification from ObservableStreams \nto serialize the  StockData received into a byte array and \nthen to write the data  into the Network stream\nImplements the Observer methods \nOnError and OnCompleted\n \n 425 Concurrent reactive scalable client/server \nThen, the internals of the observable ToAcceptTcpClientObservable  fetch the Tcp-\nClient  reference from the task, and create the network stream used as a channel to \nsend the packets of data generated by the ObservableStreams  custom observable oper -\nator. The GetServerStream  method retrieves either a secure or regular stream accord-\ning to the value that nameSsl  passed. This method determines whether the nameSsl  \nvalue for an SSL connection has been set and, if so, creates an SslStream  using TcpCli-\nent.GetStream  and the configured server name to get the server certificate. \nAlternatively, if SSL isn’t used, GetServerStream  gets the NetworkStream  from the \nclient using the TcpClient .GetStream  method. You can find the GetServerStream  \nmethod in the source code. When the ObservableStreams  materialize, the event \nstream that’s generated flows into the Subscribe  operator. The operator then asyn-\nchronously serializes the incoming data into chunks of byte arrays that are sent across the network through the client stream. For simplicity, the serializer is the .NET binary formatter, but you can replace it with one that better fits your needs.\nThe data is sent across the network in the form of byte arrays, because it’s the only \nreusable data message type that can contain any shape of object. This listing shows an implementation of the core observable operator \nToAcceptTcpClientObservable  used \nby the underlying TcpListener  to listen for remote connections and react accordingly.\nListing 13.11  Asynchronous and reactive ToAcceptTcpClientObservable  \nstatic IObservable<TcpClient> ToAcceptTcpClientObservable(this TcpListener \n➥ listener, int backlog = 5)\n{        listener.Start(backlog);      return Observable.Create<TcpClient>(async (observer, token) =>     {        try        {            while (!token.IsCancellationRequested)             {                 var client = await listener.AcceptTcpClientAsync();                  Task.Factory.StartNew(_ => observer.OnNext(client), token,                            TaskCreationOptions.LongRunning);             }            observer.OnCompleted();        }        catch (OperationCanceledException)        {            observer.OnCompleted();          }        catch (Exception error)        {            observer.OnError(error);          }        finally        {Starts listening with a given client’s buffer backlogCreates the Observable operator that captures \na cancellation token from the context \nThe while loop continues to iterate until the \ncancellation token requests a cancellation. Accepts new clients from the \nlistener, asynchronously \nRoutes the client connections to the \nobserver into an asynchronous task to \nlet multiple clients connect together\nImplements the Observer methods OnCompleted and OnError to respectively handle the cases of cancellation and exception\n \n426 chapter  13 Recipes and design patterns for successful concurrent programming\n            listener.Stop();        }        return Disposable.Create(() =>         {            listener.Stop();            listener.Server.Dispose();        });    });}\nToAcceptTcpClientObservable  takes an instance of TcpListener , which starts listen-\ning asynchronously for new incoming connection requests in a while  loop, until the \noperation is canceled using a cancellation token. When a client successfully connects, a \nTcpClient  reference flows out as a message within a sequence. This message executes \ninto an asynchronous Task  to service the client/server interaction, letting multiple cli-\nents connect concurrently to the same listener. Once a connection is accepted, another \nTask  starts repeating the procedure of listening for new connection request.\nUltimately, when the observable is disposed, or the cancellation token requests a \ncancellation, the function passed into the Disposable .Create  operator is triggered to \nstop and close the underlying server listener.\nNOTE  In general, use the Disposable .Create  method to write an action to \nclean up resources and to stop useless messages flowing to an observer that has been disposed.\nThe data transferred is generated through the \nObservableStreams  extension method, \nwhich reads and parses a set of CSV files to extract the historical stocks prices. This data is then pushed to the clients, connected through the \nNetworkStream .\nThis shows the implementation of ObservableStreams .\nListing 13.12  Custom Observable  stream reader and parser \nstatic IObservable<StockData> ObservableStreams    (this IEnumerable<string> filePaths,    \n➥ Func<string, string, StockData> map, int delay = 50) \n{    return filePaths           .Select(key =>                 new FileLinesStream<StockData>(key, row => map(key, row)))       .Select(fsStock =>  {         var startData = new DateTime(2001, 1, 1);         return Observable.Interval(TimeSpan.FromMilliseconds(delay))                  .Zip(fsStock.ObserveLines(), (tick, stock) => {                          stock.Date = startData + TimeSpan.FromDays(tick);                       return stock;                 });            }Creates a cleanup function that runs when the observable is disposed\nThe ObservableStreams custom observable extension method takes as an argument a list of file paths to process, and a lambda function for the file content transformation. An instance of FileLinesStream is created for each file and used \nto generate an observable. This observable reads each line of \nthe file and applies the map function for the transformation. \nThe Interval operator is used to apply a delay between notifications. The value can be zero to disable the delays.The Zip operator combines an element from each sequence in turn. \nIn this case, one sequence is generated from the Interval operator, \nwhich ensures that a delay is applied for each notification.\n \n 427 Concurrent reactive scalable client/server \n        )        .Aggregate((o1, o2) => o1.Merge(o2));    }\nObservableStreams  generates a series of observables of StockData  type, one for each \nof the filePaths  passed. The class FileLinesStream , whose implementation is omitted \nfor simplicity, opens the FileStream  of a given file path. It then reads the content text \nfrom the stream as an observable and applies a projection to transform each line of text read into a \nStockData  type. Ultimately it pushes the results out as an observable. \nThe most interesting part of the code is the application of the two Observable  oper -\nators Interval  and Zip, which are used together to apply an arbitrary delay, if speci-\nfied, between messages. The Zip operator combines an element from each sequence \nin turn, which means that each StockData  entry is paired with an element, produced \nevery interval time. In this case, the combination of a StockData  with the interval time \nensures a delay for each notification. \nUltimately, the combination of the Aggregate  and Merge  operators is used to merge \nthe observables generated from each file:\n.Aggregate((o1, o2) => o1.Merge(o2));\nNext, to complete the client/server program, you implement the reactive client class, shown in figure 13.12. Listing 13.13 shows the implementation of the client side.\nEvent connection SubscribeClient\nNetworkStrea mDeserializ e\nSubscrib eGroupBy symbol\nSelectMany\nThrottle\nThe connection with the ser ver\nis established, and an event istriggered. The event is pushedthrough the obser vable pipeline\nto start reading data fromthe network stream.Subscribe sendsnotificationsto update thelive chart.TcpClient requests\na connection toTcpListener .The network stream\nasynchronously readsthe incoming data ina loop.The data that’s read\nis deserialized andpushed through theobser vable pipeline.\nServer\nFigure 13.12  TcpClient  requests a connection to the TcpListener  server. When the connection \nis established, it triggers an event that carries the client stream, which is pushed through the observable pipeline. Next, a \nNetworkStream  is created to start reading the data asynchronously in a loop from \nthe server. The data read is next deserialized and analyzed through the observable pipeline to ultimately update the live chart.The Aggregate operator merges (reduces) all the observables into one.\n \n428 chapter  13 Recipes and design patterns for successful concurrent programming\nListing 13.13  Reactive TcpClient  program \nvar endpoint = new IPEndPoint(IPAddress.Parse(""127.0.0.1""), 8080);var cts = new CancellationTokenSource();var formatter = new BinaryFormatter();endpoint.ToConnectClientObservable()      .Subscribe(client => {        GetClientStream(client, sslName)           .ReadObservable(0x1000, cts.Token)           .Select(rawData => Deserialize<StockData>(formatter, rawData))        .GroupBy(item => item.Symbol)           .SelectMany(group =>                     group.Throttle(TimeSpan.FromMilliseconds(20))                      .ObserveOn(TaskPoolScheduler.Default))          .ObserveOn(ctx)        .Subscribe(stock =>                UpdateChart(chart, stock, sw.ElapsedMilliseconds) );        },        error => Console.WriteLine(""Error: "" + error.Message),        () => Console.WriteLine(""OnCompleted""),        cts.Token);\nThe code starts with an IPEndPoint  instance, which targets the remote server endpoint \nto connect. The observable operator ToConnectClientObservable  creates an instance \nof a TcpClient  object to initiate the connection. Now, you can use the Observable  \noperators to subscribe to the remote client connection. When the connection with the server is established, the \nTcpClient  instance is passed as an observable to begin receiv-\ning the stream of data to process. In this implementation, the remote NetworkStream  \nis accessed calling the GetClientStream  method. The stream of data flows into the \nobservable pipeline though the ReadObservable  operator, which routes the incoming \nmessages from the underlying TcpClient  sequence into another observable sequence \nof type ArraySegment  bytes.\nAs part of the stream-processing code, after the chunks of rawData  received from the \nserver are converted into StockData , the GroupBy  operator filters the stock tickers by \nsymbol into multiple observables. At this point, each observable can have its own unique operations. Grouping allows throttling to act independently on each stock symbol, and only stocks with identical symbols will be filtered within the given throttle time span. \nA common problem with writing reactive code is when the events come in too \nquickly. A fast-moving stream of events can overwhelm your program’s processing. In Creates an observable over an instance of the TcpClient object to initiate and notify when the connection to the server is established\nCreates a stream from the network streams used for the communication between the server and clientDelivers continuous chunks of bytes from the underlying stream \nuntil it can be read or a cancellation is requested. The byte array \nflows through the observable pipeline in an asynchronous way.\nGroups the incoming data by the stock symbol, creating an observable for each stock ticker symbol\nThrottles the incoming notification to avoid overwhelming the consumer. Throttling can be done based on the data stream itself (rather than just a timespan). This partition of the stream by the stock symbol \nstarts a new thread for each partition.The last step of the observable pipeline subscribes the notification to update a live chart.\n \n 429 Concurrent reactive scalable client/server \nlisting 13.13, because you have a bunch of UI updates, using the throttling operator can help deal with a massive flood of stream data without overwhelming the live updates. The operator after the throttling, \nObserveOn (TaskPoolScheduler .Default ), starts a \nnew thread for each partition originated by the GroupBy . The Subscribe  method ulti-\nmately updates the live chart with the stock values. Here’s the implementation of the \nToConnectClientObservable  operator.\nListing 13.14  Custom Observable ToConnectClientObservable  operator\nstatic IObservable<TcpClient> ToConnectClientObservable(this IPEndPoint \n➥ endpoint)\n{    return Observable.Create<TcpClient>(async (observer, token) =>  {       var client = new TcpClient();         try      {          await client.ConnectAsync(endpoint.Address, endpoint.Port);           token.ThrowIfCancellationRequested();            observer.OnNext(client);         }      catch (Exception error)      {          observer.OnError(error);      }        return Disposable.Create(() => client.Dispose());      });}\nToConnectClientObservable  creates an instance of TcpClient  from the given IPEnd-\nPoint  endpoint, and then it tries to connect asynchronously to the remote server. \nWhen the connection is established successfully, the TcpClient  client reference is \npushed out through the observer. \nThe last phase of the code to program is the ReadObservable  observable operator, \nwhich is built to asynchronously and continuously read chunks of data from a stream. In this program, the stream is the \nNetworkStream  produced as result of the connection \nbetween the server and client. \nListing 13.15  Observable stream reader\npublic static IObservable<ArraySegment<byte>> ReadObservable(this Stream \n➥ stream, int bufferSize, CancellationToken token = \n➥ default(CancellationToken))\n{      var buffer = new byte[bufferSize];      var asyncRead = Observable.FromAsync<int>(async ct => {           await stream.ReadAsync(buffer, 0, sizeof(int), ct); Creates an observable, passing the cancellation token from the current context Starts waiting asynchronously for a \nconnection to the server to be established \nChecks if a cancellation has been sent to stop observing the connection \nThe connection is established, and the notification is pushed to the observers.\nWhen the observable is disposed, \nTcpClient and its connection are closed.\nConverts an asynchronous \noperation to an observable\nReads the size of the chunk of data (buffer) from the stream to \nconfigure the read length, and reads the buffer with the given size \n \n430 chapter  13 Recipes and design patterns for successful concurrent programming\n          var size = BitConverter.ToInt32(buffer, 0);            await stream.ReadAsync(buffer, 0, size, ct);             return size});      return Observable.While(             () => !token.IsCancellationRequested && stream.CanRead,            Observable.Defer(() =>                      !token.IsCancellationRequested && stream.CanRead                        ? asyncRead                        : Observable.Empty<int>())                .Catch((Func<Exception, IObservable<int>>)(ex => \n➥ Observable.Empty<int>())) \n                .TakeWhile(returnBuffer => returnBuffer > 0)                .Select(readBytes => \n➥ new ArraySegment<byte>(buffer, 0, readBytes)))  \n        .Finally(stream.Dispose);}\nOne important note to consider when implementing this ReadObservable  is that the \nstream must be read in chunks to be reactive. That’s why the ReadObservable  operator \ntakes a buffer size as an argument to define the size of the chunks.\nThe purpose of the ReadObservable  operator is to read a stream in chunks to facilitate \nworking with data that’s larger than the memory available, or that could be infinite with an unknown size, like streaming from the network. In addition, it promotes the compo-sitional nature of Rx for applying multiple transformations to the stream itself, because reading chunks at a time allows data transformations while the stream is still in motion. At this point, you have an extension method that iterates on the bytes from a stream. \nIn the code, the \nFromAsync  extension method allows you to convert a Task<T> , in \nthis case stream .ReadAsync , into an IObservable<T>  to treat the data as a flow of events \nand to enable programming with Rx. Underneath, Observable.FromAsync  creates an \nobservable that only starts the operation independently every time it’s subscribed to.\nThen, the underlying stream is read as an Observable  while  loop until data is \navailable or the operation is canceled. The Observable  Defer  operator waits until an \nobserver subscribes to it, and then starts pushing the data as a stream. Next, during each iteration, a chunk of data is read from the stream. This data is then pushed into a buffer that takes the form of an \nArraySegment <byte >, which slices the payload in the \nright length. ReadObservable  returns an IObservable  of ArraySegment <byte >, which \nis an efficient way to manage the byte arrays in a pool. The buffer size may be larger than the payload of bytes received, for example, so the use of \nArraySegment <byte > holds the \nbyte array and payload length. \nIn conclusion, when receiving and processing data, the .NET Rx allows shorter and \ncleaner code than traditional solutions. Furthermore, the complexity of building a Reads the size of the chunk of data (buffer) from the stream to \nconfigure the read length, and reads the buffer with the given size Continues reading in a while loop until there is data to be read\nIteratively invokes the observable factory, which starts from the current stream position. The Defer observable operator starts the process only when a subscriber exists.Handles the case of an error silently, passing an empty result\nWhen a chunk of data is read, an instance of \nArraySegment is created to wrap the buffer, \nwhich is then pushed to the observers.",26525
140-13.9.1 Solution combining filter and map parallel operations.pdf,140-13.9.1 Solution combining filter and map parallel operations,,0
141-13.9 Reusable custom high-performing parallelfiltermapoperator.pdf,141-13.9 Reusable custom high-performing parallelfiltermapoperator,"431 Reusable custom high-performing parallel filter -map operator \nTCP-based reactive client/server program is heavily reduced in comparison to a tradi-tional model. In fact, you don’t have to deal with low-level \nTcpClient  and TcpListener  \nobjects, and the flow of bytes is handled through a high-level abstraction offered by observable operators. \n13.9 Reusable custom high-performing parallel filter-map operator \nYou have a collection of data, and you need to perform the same operation on each element of the data to satisfy a given condition. This operation is CPU-bound and may take time. You decide to create a custom and reusable high-performant operator to filter and map the elements of a given collection. The combination of filtering and transforming the elements of a collection is a common operation for analyzing data structures. It’s possible to achieve a solution using LINQ or PLINQ in parallel with the \nWhere  and Select  operators; but a more optimal performance solution is available. As \nyou saw in section 5.2.1, for each call and repeated use of high-order operators such as map (\nSelect ), filter (Where ), and other similar functions of the PLINQ query (and \nLINQ), as shown in figure 13.13, intermediate sequences are generated that unnec-essarily increase memory allocation. This is due to the intrinsic functional nature of LINQ and PLINQ, where collections are transformed instead of mutated. In the case of transforming large sequences, the penalty paid to the GC to free up memory becomes increasingly higher, with negative consequences to the performance of the program. \nnumber 1[numbers].Where(IsPrime).Select(ToPow)\nIsPrime 1 -\nnumber 2 IsPrime 2 ToPow 2\nnumber 3 IsPrime 3 ToPow 3\nnumber n IsPrime n ToPow n\nIn this example, you want to derive the sum of all the prime numbers in 100 million digits.\n13.9.1 Solution: combining filter and map parallel operations\nThe implementation of a custom and parallel filter and map operator with top perfor -\nmance requires attention to minimize (or eliminate) unnecessary temporary data allo-cation, as shown in figure 13.14. This technique of reducing data allocation during data manipulation to increase the performance of the program is known as deforestation. Figure 13.13  In this diagram, each number \n(first column) is first filtered by IsPrime  \n(second column) to verify if it’s a prime number. Then, the prime numbers are passed into the \nToPow  function (third column). For example, \nthe first value, number 1, is not a prime number, so the \nToPow  function isn’t running. \n \n432 chapter  13 Recipes and design patterns for successful concurrent programming\nA1[source].Where(filter).Select(map)\nfilter A1 map(filter A1)\nA2 filter A2 map(filter A2)\nA3 filter A3 map(filter A3)\nAn filter An map(filter An)A1[source].FilterMap(filter, map)\nmap(filter A1)\nA2 map(filter A2)\nA3 map(filter A3)\nAn map(filter An)\nFigure 13.14  The left graph shows the operations Where  and Select  over a given source, done in \nseparate steps, which introduces extra memory allocation and consequentially more GC generations. The right graph shows that applying the \nWhere  and Select  (filter and map) operations together in a \nsingle step avoids extra allocation and reduces GC generations, increasing the speed of the program. \nThe next listing shows the code of the ParallelFilterMap  function, which uses the \nParallel.ForEach  loop to eliminate intermediate data allocations by processing only \none array, instead of creating one temporary collection for each operator.\nListing 13.16  ParallelFilterMap  operator \nstatic TOutput[] ParallelFilterMap<TInput, TOutput>(this IList<TInput> \n➥ input, Func<TInput, Boolean> predicate,   \n                     Func<TInput, TOutput> transform,                      ParallelOptions parallelOptions = null){    parallelOptions = parallelOptions ?? new ParallelOptions();    var atomResult = new Atom<ImmutableList<List<TOutput>>>                                  (ImmutableList<List<TOutput>>.Empty);    Parallel.ForEach(Partitioner.Create(0, input.Count),        parallelOptions,        () => new List<TOutput>(),             delegate (Tuple<int, int> range, ParallelLoopState state,                 List<TOutput> localList)        {            for (int j = range.Item1; j < range.Item2; j++)              {                var item = input[j];                if (predicate(item))                               localList.Add(transform(item));              }            return localList;The extension method takes the lambda functions to filter (predicate) and map (transform) the input values from the source. Creates an instance of the Atom object (defined in chapter 3) \nto apply compare-and-swap update operations over the \nunderlying ImmutableList in a thread-safe manner\nEach thread uses a Local-Thread instance of List<TOutput> for isolated and thread-safe operations. Each iteration runs an independent thread (task) from \nthe thread pool that performs the filter and map \noperations over a partitioned set from the input source.Applies the filter and map functions for each item of the current portioned set of data\n \n 433 Reusable custom high-performing parallel filter -map operator \n        }, localList => atomResult.Swap(r => r.Add(localList)));      return atomResult.Value.SelectMany(id => id).ToArray();     }\nThe parallel ForEach  loop applies the predicate  and map functions for each element \nof the input collection. In general, if the body of the parallel loop performs only a small amount of work, better performance results come from partitioning the itera-tions into larger units of work. The reason for this is the overhead when processing a loop, which involves the cost of managing worker threads and the cost of invoking a delegate method. Consequently, it’s good practice to partition the parallel iteration space by a certain constant using the \nPartitioner .Create  constructor. Then each \nbody invokes the filter and map functions for a certain range of elements, amortizing invocations of the loop body delegate.\nNOTE  Due to parallelism, the order in which the values will be processed \ndoesn’t guarantee the result will be in the same order.\nFor each iteration of the ForEach  loop, there’s an anonymous delegate invocation that \ncauses a penalty in terms of memory allocation and, consequently, performance. One invocation occurs for the filter function, a second invocation occurs for the map func-tion, and ultimately an invocation happens for the delegate passed into the parallel loop. The solution is to tailor the parallel loop specific to the filter and map operations to avoid extra invocations of the body delegate. \nThe parallel \nForEach operator forks off a set of threads, each of which calculates an \nintermediate result by performing the filter and map functions over its own partition of data and placing the value into its dedicated slot in the intermediate array.\nEach thread (task) governed by the parallel loop captures an isolated instance of a \nlocal \nList<TOutput>  through the concept of local values. Local values are variables that \nexist locally within a parallel loop. The body of the loop can access the value directly, without having to worry about synchronization. \nNOTE  The reason to use local and isolated instance of List<TOutput>  is to \navoid excessive contention, which happens when too many threads try to access a \nsingle shared resource simultaneously, leading to bad performance.\nEach partition will compute its own intermediate value that will then combine into a single final value. \nWhen the loop completes, and it’s ready to aggregate each of its local results, it does \nso with the \nlocalFinally  delegate. But the delegate requires synchronization access to \nthe variable that holds the final result. An instance of the ImmutableList  collection is \nused to overcome this limitation to merge the final results in a thread-safe manner. When each iteration completes, the shared atomResult Atom object updates the underlying ImmutableList. Ultimately, the result is \nflattened into an array. \n \n434 chapter  13 Recipes and design patterns for successful concurrent programming\nNOTE  Write  operations (such as adding an item) in immutable collections \nreturn a new immutable instance instead of changing the existing instance. This isn’t as wasteful as it first sounds because immutable collections share their memory. \nNote the \nImmutableList  is encapsulated in an Atom  object, from chapter 3. The Atom  \nobject uses a compare-and-swap (CAS) strategy to apply thread-safe writes and updates of objects without the need of locks and other forms of primitive synchronization. In this example, the \nAtom  class holds a reference to the immutable list and updates it \nautomatically.\nThe following code snippet tests the parallel sum of only the prime numbers from \n100 million digits:\nbool IsPrime(int n){    if (n == 1) return false;    if (n == 2) return true;    var boundary = (int) Math.Floor(Math.Sqrt(n));    for (int i = 2; i <= boundary; ++i)        if (n % i == 0) return false;    return true;}BigInteger ToPow(int n) => (BigInteger) Math.BigMul(n, n);var nums = Enumerable.Range(0, 100000000).ToList();BigInteger SeqOperation() =>                nums.Where(IsPrime).Select(ToPow).Aggregate(BigInteger.Add);BigInteger ParallelLinqOperation() =>   nums.AsParallel().Where(IsPrime).Select(ToPow).Aggregate(BigInteger.Add);BigInteger ParallelFilterMapInline() =>           nums.ParallelFilterMap(IsPrime, ToPow).Aggregate(BigInteger.Add);\nFigure 13.15 compares the sequential code (as baseline), the PLINQ version, and the custom \nParallelFilterMap  operator. The figure shows the result of the benchmark \ncode running the sum of the prime numbers for the 100 million digits. The bench-mark was performed in a quad-core machine with 6 GB of RAM. The sequential code takes an average of 196.482 seconds to run and is used as baseline. The PLINQ version of the code is faster and runs in 74.926 seconds, almost three times faster, which is expected in a quad-core computer. The custom \nParallelFilterMap  operator is the \nfastest, at approximately 52.566 seconds.",10258
142-13.10.1 Solution coordinating the payload between operations using the agent programming model.pdf,142-13.10.1 Solution coordinating the payload between operations using the agent programming model,"435 Non-blocking synchronous message-passing model \nFigure 13.15  Benchmark chart comparing the Sequential and Parallel LINQ versions of the \ncode with the custom ParallelFilterMap  operator. In a quad-core machine, the custom \nParallelFilterMap  operator is approximately 80% faster than the sequential version of the code, \nand 30% faster than the PLINQ version.\n13.10 Non-blocking synchronous message-passing model \nLet’s imagine you need to build a scalable program capable of handling a large num-ber of operations without blocking any threads. You need a program that loads, pro-cesses, and saves a large number of images, for example. These operations are handled with few threads in a collaborative way, which optimizes the resources without blocking any threads and without jeopardizing the performance of the program. \nSimilar to the Producer/Consumer pattern, there are two flows of data. One flow is the \ninput, where the processing starts, followed by intermediate steps to transform the data, followed by the output with the final result of the operations. These processes, the pro-ducer and the consumer, share a common fixed-size buffer used as a queue. The queue is buffered to increase the overall speed and increase throughput to allow for multiple consumers and producers. In fact, when the queue is safe to use by multiple consumers and producers, then it’s easy to change the level of concurrency for different parts of the pipeline at runtime. The producer, however, could write into the queue when it isn’t full, or conversely, it can block when the queue is full. On the other side, the consumer could read from the queue when it is not empty, but it will block in other cases when the queue is empty. You want to implement a producer and consumer pattern based on message pass-ing to avoid thread blocking and maximize the application’s scalability.\n \n436 chapter  13 Recipes and design patterns for successful concurrent programming\n13.10.1 Solution: coordinating the payload between operations using the agent programming model\nThere are two flavors of message passing models for concurrent systems: synchronous \nand asynchronous. You’re already familiar with asynchronous models such as the agent (and actor) model, explained in chapters 11 and 12, and based on asynchronous mes-sage passing. In this recipe, you’ll use the synchronous version of message passing, which is also known as communicating sequential processes (CSP).\nCSP has much in common with the actor model, both being based on message pass-\ning. But CSP emphasizes the channels used for communication, rather than the entities between which communication takes place.\nThis CSP synchronous message passing for concurrent programming models is used \nfor data exchange between channels, which can be scheduled to multiple threads and might run in parallel. Channels are similar to thread workers that communicate directly with each other by publishing messages, and where other channels can then listen for these messages without the sender knowing who’s listening. \nYou can imagine the channel as a thread-safe queue, where any task with a refer -\nence to a channel can add messages to one end, and any task with a reference to it can remove messages from the other end. Figure 13.16 illustrates the channel model.\n1\n2Recv\nSendApplies\nsubscribed\nbehavior\n3\n4\n51\n2Recv\nSubscribe\nbehavior\nRecv\nSubscribe\nbehavior\nRecv\nSubscribe\nbehavior3\n4\n5\n1\n2\n3\n4\n5SendSend\nRecv1\n2\n3\n4\n5\n1\n2\n3\n4\n5Subscribe\nbehavior\nFigure 13.16  The channel receives ( Recv ) a message, and applies the subscribed behavior. The \nchannels communicate by sending ( Send ) messages, often creating an interconnected system that’s \nsimilar to the actor model. Each channel contains a local queue of messages used to synchronize the communication with other channels without blocking.\n \n 437 Non-blocking synchronous message-passing model \nA channel doesn’t need to know about what channel will process the message later in the pipeline. It only has to know what channel to forward the messages to. On the other side, listeners on channels can subscribe and unsubscribe without affecting any chan-nels sending the messages. This design promotes loose coupling between channels.\nThe primary strength of CSP is its flexibility, where channels are first-class and can \nbe independently created, written to, read from, and passed between tasks. The fol-lowing listing shows the implementation of the channel in F#, which uses \nMailbox-\nProcessor  for the underlying message synchronization due to the close similarity with \nthe agent-programming model. The same concepts apply to C#. You can find the full implementation in C# using TDF in the downloadable source code.\nListing 13.17  ChannelAgent  for CSP implementation using MailboxProcessor  \ntype internal ChannelMsg<'a> =       | Recv of ('a -> unit) * AsyncReplyChannel<unit>    | Send of 'a * (unit -> unit) * AsyncReplyChannel<unit>type [<Sealed>] ChannelAgent<'a>() =    let agent = MailboxProcessor<ChannelMsg<'a>>.Start(fun inbox ->        let readers = Queue<'a -> unit>()           let writers = Queue<'a * (unit -> unit)>()           let rec loop() = async {            let! msg = inbox.Receive()            match msg with            | Recv(ok , reply) ->                  if writers.Count = 0 then                    readers.Enqueue ok                    reply.Reply( () )                else                    let (value, cont) = writers.Dequeue()                    TaskPool.Spawn cont                       reply.Reply( (ok value) )                return! loop()            | Send(x, ok, reply) ->                    if readers.Count = 0 then                    writers.Enqueue(x, ok)                    reply.Reply( () )                else                    let cont = readers.Dequeue()                    TaskPool.Spawn ok                      reply.Reply( (cont x) )                return! loop() }        loop())    member this.Recv(ok: 'a -> unit)  =          agent.PostAndAsyncReply(fun ch -> Recv(ok, ch)) |> Async.Ignore    member this.Send(value: 'a, ok:unit -> unit)  =          agent.PostAndAsyncReply(fun ch -> Send(value, ok, ch)) |> Async.\nIgnoreUses a DU to define the message type to send to the ChannelAgent in order to coordinate the channel operations Uses internal queues to keep track of the read and write operations of the channel\nWhen a Recv message is received, if the current writer queue is empty, then the read function is queued up, waiting for a writer function to balance the work. \nWhen a Recv message is received and \nthere is at least one writer function \navailable in the queue, a task is spawned \nto run the read function.\nWhen a Send message is received, if the current reader queue is empty, then the write function is queued up, waiting for a reader function to balance the work. \nWhen a Send message is received, if the \ncurrent reader queue is empty, then the \nwrite function is queued up, waiting for a \nreader function to balance the work. \n \n438 chapter  13 Recipes and design patterns for successful concurrent programming\n    member this.Recv() =             Async.FromContinuations(fun (ok, _,_) ->            agent.PostAndAsyncReply(fun ch -> Recv(ok, ch))             |> Async.RunSynchronously)    member this.Send (value:'a) =               Async.FromContinuations(fun (ok, _,_) ->            agent.PostAndAsyncReply(fun ch -> Send(value, ok, ch))             |> Async.RunSynchronously )let run (action:Async<_>) = action |> Async.Ignore |> Async.Start  let rec subscribe (chan:ChannelAgent<_>) (handler:'a -> unit) =       chan.Recv(fun value -> handler value                           subscribe chan handler) |> run\nThe ChannelMsg  DU represents the message type that ChannelAgent  handles. When \na message arrives, the Recv  case is used to execute a behavior applied to the payload \npassed. The Send  case is used to communicate a message to the channel.\nThe underlying MailboxProcessor  contains two generic queues, one for each opera-\ntion, Recv  or Send . As you can see, when a message is either received or sent, the behavior \nof the agent, in the function loop() , checks the count of available messages to load bal-\nance and synchronize the communication without blocking any threads. ChannelAgent  \naccepts continuation functions with its Recv  and Send  operations. If a match is available, \nthe continuation is invoked immediately; otherwise, it’s queued for later. Keep in mind that a synchronous channel eventually gives a result, so the call is logically blocking. But when using F# async workflows, no actual threads are blocked while waiting.\nThe last two functions in the code help run a channel operation (usually \nSend ), while \nthe subscribe  function is used to register and apply a handler to the messages received. \nThis function runs recursively and asynchronously waiting for messages from the channel. \nThe TaskPool.Spawn  function assumes a function with signature (unit -> unit) -> \nunit  that forks the computation on a current thread scheduler. This listing shows the \nimplementation of TaskPool , which uses the concepts covered in chapter 7.\nListing 13.18  Dedicated  TaskPool  agent (MailboxProcessor )\ntype private Context = {cont:unit -> unit; context:ExecutionContext} type TaskPool private (numWorkers) =    When a Send message is received and there’s at least one reader function available in the queue, a task is spawned to run the write function.\nRuns an asynchronous operation in a \nseparate thread, discharging the resultThis helper function registers a handler applied to the next available message in the channel. The function recursively and asynchronously (without blocking) waits for messages from the channel (without blocking).\nUses a record type to wrap the current \nExecutionContext captured when the \noperation cont is added to the TaskPoolThe constructor of TaskPool takes the number of workers to set the degree of parallelism. \n \n 439 Non-blocking synchronous message-passing model \n    let worker (inbox: MailboxProcessor<Context>) =           let rec loop() = async {            let! ctx = inbox.Receive()            let ec = ctx.context.CreateCopy()                   ExecutionContext.Run(ec, (fun _ -> ctx.cont()), null)            return! loop() }        loop()    let agent = MailboxProcessor<Context>.parallelWorker(numWorkers,     \n➥ worker)  \n    static let self = TaskPool(2)      member private this.Add (continutaion:unit -> unit) =          let ctx = { cont = continutaion;                     context = ExecutionContext.Capture() }         agent.Post(ctx)            static member Spawn (continuation:unit -> unit) =         self.Add continuation \nThe Context  record type is used to capture the ExecutionContext  at the moment \nwhen the continuation function cont  was passed to the pool. TaskPool  initializes the \nMailboxProcessor  parallelWorker  type to handle multiple concurrent consumers \nand producers (refer to chapter 11 for the implementation and details of the paral-\nlelWorker  agent).\nThe purpose of TaskPool  is to control how many tasks to schedule and to dedicate to \nrun the continuation function in a tight loop. In this example, it runs only one task, but you can have any number.\nAdd enqueues the given continuation function, which will be executed when a \nthread on a channel offers communication and another thread offers matching com-munication. Until such compensation between channels is achieved, the thread will wait asynchronously.\nIn this code snippet, the \nChannelAgent  implements a CSP pipeline, which loads an \nimage, transforms it, and then saves the newly created image into the local MyPicture  \nfolder:\nlet rec subscribe (chan:ChannelAgent<_>) (handler:'a -> unit) =    chan.Recv(fun value -> handler value                           subscribe chan handler) |> runlet chanLoadImage = ChannelAgent<string>()let chanApply3DEffect = ChannelAgent<ImageInfo>()let chanSaveImage = ChannelAgent<ImageInfo>()subscribe chanLoadImage (fun image ->    let bitmap = new Bitmap(image)Sets the behavior of each worker agentA Context type is processed using the captured ExecutionContext when received by one of the worker agents.\nCreates an instance of the F# MailboxProcessor parallelWorker to concurrently run multiple operations limited by the degree of parallelism \nAdds a continuation action to the TaskPool. The current ExecutionContext is captured and sent to the parallelWorker agent in the form of a Context record type. When a continuation task is sent to the underlying \nagent, the current Execution context is captured and \npassed as part of the message payload.",12772
143-13.11.1 Solution implementing an agent that runs jobs with a configured degree of parallelism.pdf,143-13.11.1 Solution implementing an agent that runs jobs with a configured degree of parallelism,"440 chapter  13 Recipes and design patterns for successful concurrent programming\n    let imageInfo = { Path = Environment.GetFolderPath(Environment.\nSpecialFolder.MyPictures)\n                      Name = Path.GetFileName(image)                      Image = bitmap }    chanApply3DEffect.Send imageInfo |> run)subscribe chanApply3DEffect (fun imageInfo ->    let bitmap = convertImageTo3D imageInfo.Image    let imageInfo = { imageInfo with Image = bitmap }    chanSaveImage.Send imageInfo |> run)subscribe chanSaveImage (fun imageInfo ->    printfn ""Saving image %s"" imageInfo.Name    let destination = Path.Combine(imageInfo.Path, imageInfo.Name)    imageInfo.Image.Save(destination))let loadImages() =    let images = Directory.GetFiles(@"".\Images"")    for image in images do        chanLoadImage.Send image |> runloadImages()\nAs you can see, implementing a CSP-based pipeline is simple. After you define the channels \nchanLoadImage , chanApply3DEffect , and chanSaveImage , you have to reg-\nister the behaviors using the subscribe  function. When a message is available to be \nprocessed, the behavior is applied.\n13.11 Coordinating concurrent jobs using the agent programming model \nThe concepts of parallelism and asynchronicity were covered extensively earlier in this \nbook. Chapter 9 shows how powerful and convenient the Async.Parallel  operator is \nfor running a large number of asynchronous operations in parallel. Often, however, you may need to map across a sequence of asynchronous operations and run functions on the elements in parallel. In this case, a feasible solution can be implemented:\n    let inline asyncFor(operations: #seq<'a> Async, map:'a -> 'b) =        Async.map (Seq.map map) operations\nNow, how would you limit and tame the degree of parallelism to process the elements to balance resource consumption? This issue comes up surprisingly often when a program is doing CPU-heavy operations, and there’s no reason to run more threads than the number of processors on the machine. When there are too many concurrent threads running, contention and context-switching make the program enormously inefficient, even for a few hundred tasks. This is a problem of throttling. How can you throttle asynchronous and CPU-bound computations awaiting results without block-ing? The challenge becomes even more difficult because these asynchronous opera-tions are spawned at runtime, which makes the total number of asynchronous jobs to run unknown. \n \n 441 Coordinating concurrent jobs using the agent programming model \n13.11.1 Solution: implementing an agent that runs jobs with a configured degree of parallelism \nThe solution is using an agent model to implement a job coordinator that lets you throttle the degree of parallelism by limiting the number of tasks that are processed in parallel, as shown in figure 13.17. In this case, the agent’s only mission is to gate the number of concurrent tasks and send back the result of each operation without block-ing. In addition, the agent should conveniently expose an observable channel where you can register to receive notifications when a new result is computed.\nMultiple jobs are sent\nto the taming agentto be executed.\nThe Subscribe handleris notified when each\njob completes.The agent r uns the jobs in\nparallel, limited by the degreeof parallelism configured.\nHandle r1\n2\nBehaviorAgent\nStateJobJobJobJobJobJob\nJobJob\nMailbox\nSubscribe\nFigure 13.17  The TamingAgent  runs the jobs in parallel, limited by the degree of parallelism \nconfigured. When an operation completes, the Subscribe operator notifies the registered handlers \nwith the output of the job.  \nLet’s define the agent that can tame the concurrent operations. The agent must receive a message, but must also send back to the caller, or subscriber, a response for the result computed.\nIn the following listing, the implementation of the \nTamingAgent  runs asynchronous \noperations, efficiently throttling the degree of parallelism. When the number of con-current operations exceeds this degree, they’re queued and processed later.\nListing 13.19  TamingAgent  \ntype JobRequest<'T, 'R> =       | Ask of 'T * AsyncReplyChannel<'R>    | Completed    | Quittype TamingAgent<'T, 'R>(limit, operation:'T -> Async<'R>) = Uses a DU representing the message to send to the TamingAgent to start a new job and notify when it completes\n \n442 chapter  13 Recipes and design patterns for successful concurrent programming\n    let jobCompleted = new Event<'R>()        let tamingAgent = Agent<JobRequest<'T, 'R>>.Start(fun agent ->        let dispose() = (agent :> IDisposable).Dispose()           let rec running jobCount = async {             let! msg = agent.Receive()          match msg with          | Quit -> dispose()          | Completed -> return! running (jobCount - 1)           | Ask(job, reply) ->                do!                 async { try                             let! result = operation job                               jobCompleted.Trigger result                               reply.Reply(result)                           finally agent.Post(Completed) }                 |> Async.StartChild |> Async.Ignore                   if jobCount <= limit - 1 then return! running (jobCount + 1)               else return! idle () }            and idle () =                 agent.Scan(function                 | Completed -> Some(running (limit - 1))              | _ -> None)        running 0)      member this.Ask(value) = tamingAgent                                     .PostAndAsyncReply(fun ch -> Ask(value, ch))     member this.Stop() = tamingAgent.Post(Quit)    member x.Subscribe(action) = jobCompleted.Publish |>     \n➥Observable.subscribe(action) \nThe JobRequest  DU represents the message type for the agent tamingAgent . This mes-\nsage has a Job case, which handles the value to send to compute and a reply channel \nwith the result. The Completed  case is used by an agent to notify when a computation \nis terminated and the next job available can be processed. Ultimately, the Quit  case \n(message) is sent to stop the agent when needed.\nThe TamingAgent  constructor takes two arguments: the concurrent execution limit \nand the asynchronous operation for each job. The body of the TamingAgent  type relies \non two mutually recursive functions to track the number of concurrently running operations. When the agent starts with zero operations, or the number of running jobs doesn’t pass the limit of the degree of parallelism imposed, the function running will An event object is used to notify the subscriber when a job completes.\nHelper functions dispose of \nand stop the TamingAgent.\nRepresents a state when the agent is working\nDecrements the count of work items when a job completes\nStarts the job item and continues in a running state\nRuns the job asynchronously \nto obtain the result \nWhen the job completes, the \nevent jobCompleted is triggered \nto notify the subscribers. \nSends the result of the job \ncompleted back to the caller When the job completes, the \ninbox sends itself a notification \nto decrease the job count \nQueues the specified asynchronous workflow for processing \nin a separate thread to guarantee concurrent behaviorRepresents the idle state when the agent is blocked because the limit of concurrent jobs is reached\nUses the Scan function to wait for completion \nof work and change the agent state\nStarts in a running state with zero job items\nQueues an operation and waits asynchronously for a response Provides support to subscribe the jobCompleted event as Observable \n \n 443 Coordinating concurrent jobs using the agent programming model \nwait for a new incoming message to process. Conversely, when the jobs running reach the enforced limit, the execution flow of the agent switches the function to idle. It uses the \nScan  operator to wait for a type of message to discharge the others. \nThe Scan  operator is used in the F# MailboxProcessor  (agent) to process only \na subset and targeted type of messages. The Scan  operator takes a lambda function \nthat returns an Option  type. The messages you want to be found during the scanning \nprocess should return Some , and the messages you want to ignore for now should \nreturn None . \nThe operation signature passed into the constructor is 'T -> Async<'R> , which \nresembles the Async.map  function. This function is applied to each job that’s sent to \nthe agent through the method member Ask, which takes a value type that is passed to \nthe agent to initiate, or queue, a new job. When the computation completes, the sub-scribers of the underlying event \njobCompleted  are notified with the new result, which is \nalso replied back asynchronously to the caller that sent the message across the channel \nAsyncReplyChannel . \nAs mentioned, the purpose of the event jobCompleted  is to notify the subscribers \nthat have registered a callback function through the method member Subscribe , \nwhich uses the Observable  module for convenience and flexibility.\nHere’s how the TamingAgent  is used to transform a set of images. This example \nis similar to the CPS Channel  one, allowing you to compare code between different \napproaches.\nListing 13.20  TamingAgent  in action for image transformation\nlet loadImage = (fun (imagePath:string) -> async {    let bitmap = new Bitmap(imagePath)    return { Path = Environment.GetFolderPath(Environment.SpecialFolder.\nMyPictures)\n                Name = Path.GetFileName(imagePath)                Image = bitmap } })  let apply3D = (fun (imageInfo:ImageInfo) -> async {    let bitmap = convertImageTo3D imageInfo.Image    return { imageInfo with Image = bitmap } })   let saveImage = (fun (imageInfo:ImageInfo) -> async {    printfn ""Saving image %s"" imageInfo.Name    let destination = Path.Combine(imageInfo.Path, imageInfo.Name)    imageInfo.Image.Save(destination)    return imageInfo.Name})   let loadandApply3dImage (imagePath:string) =         Async.retn imagePath >>= loadImage >>= apply3D >>= saveImage let loadandApply3dImageAgent = TamingAgent<string, string>(2, \nloadandApply3dImage)     Loads an image from the given file path, returning a record type with the image and its information loaded \nFunctions to apply a 3D effect to the image\nSaves the image to the \nlocal MyPicture folder\nComposes the previously defined asynchronous functions \nusing the monadic return and bind async operators \nCreates an instance of the TamingAgent capable of running two jobs concurrently, applying the composed function loadandApply3dImage for each job",10617
144-13.12.1 Solution combining asynchronous operations usingtheKleislicomposition operator.pdf,144-13.12.1 Solution combining asynchronous operations usingtheKleislicomposition operator,"444 chapter  13 Recipes and design patterns for successful concurrent programming\nloadandApply3dImageAgent.Subscribe(fun imageName -> printfn ""Saved image %s  \n➥ from subscriber"" imageName)  \nlet transformImages() =      let images = Directory.GetFiles(@"".\Images"")    for image in images do        loadandApply3dImageAgent.Ask(image)         |> run (fun imageName ->                     printfn ""Saved image %s - from reply back"" imageName) \nThe three asynchronous functions, loadImage , apply3D , and saveImage , are com-\nposed together, forming the function loadandApply3dImage  using the F# async bind  \ninfix operator >>= defined in chapter 9. As a refresher, here’s the implementation: \nlet bind (operation:'a -> Async<'b>) (xAsync:Async<'a>) = async {    let! x = xAsync    return! operation x }let (>>=) (item:Async<'a>) (operation:'a -> Async<'b>) =                                                bind operation item\nThen, the loadandApply3dImageAgent  instance of the TamingAgent  is defined by pass-\ning the argument limit into the constructor. This sets the degree of parallelism of the agent and the argument function \nloadandApply3dImage , which represents the behav-\nior for the job computations. The Subscribe  function registers a callback that runs \nwhen each job completes. In this example, it displays the name of the image of the completed job. \nNOTE  The image paths are sent sequentially. The TamingAgent  is thread safe, \nso multiple threads can send messages simultaneously without any problem.\nThe loadImages()  function reads the image paths from the directory Images, and in a \nfor-each  loop sends the values to the loadandApply3dImageAgent  TamingAgent . The \nrun function uses CPS to execute a callback when the result is computed and replied \nback.\n13.12 Composing monadic functions \nYou have functions that take a simple type and return an elevated type like Task  or \nAsync , and you need to compose those functions. You might think you need to get \nthe first result and next apply it to the second function, and then repeat for all the functions. This process can be rather cumbersome. This is a case for employing the concept of function composition. As a reminder, you can create a new function from two smaller ones. It usually works, as long as the functions have matching output and input types. \nThis rule doesn’t apply for monadic functions, because they don’t have matching \ninput/output types. For example, monadic \nAsync  and Task  functions cannot be com-\nposed because Task<T>  isn’t the same as T. Subscribes a handler to run when a job-completed notification arrives\nStarts the process by reading the image files and pushing a new job to the loadandApply3dImageAgent TamingAgent instance\n \n 445 Composing monadic functions \nHere’s the signature for the monadic Bind  operator:\nBind : (T -> Async<R>) -> Async<T> -> Async<R>Bind : (T -> Task<R>) -> Task <T> -> Task <R>\nThe Bind  operator can pass elevated values into functions that handle the wrapped \nunderlying value. How can you compose monadic functions effortlessly?\n13.12.1 Solution: combining asynchronous operations using the Kleisli composition operator\nThe composition between monadic functions is named Kleisli composition, and in FP it’s usually represented with the infix operator \n>=> that can be constructed using the \nmonadic Bind  operator. The Kleisli  operator essentially provides a composition con-\nstruct over monadic functions, which instead of composing regular functions like a -> \nb and b -> c , is used to compose over a -> M b  and b -> M c , where M is an elevated \ntype.\nThe signature of the Kleisli  composition operator for elevated types, such as the \nAsync  and Task  types, is\nKleisli (>=>) : ('T -> Async<TR>) -> (TR -> Async<R>) -> T -> Async<R>Kleisli (>=>) : ('T -> Task<TR>) -> (TR -> Task <R>) -> T -> Task <R>\nWith this operator, two monadic functions can compose directly as follows:\n(T -> Task<TR>) >=> (TR -> Task<R>)(T -> Async<TR>) >=> (TR -> Async<R>)\nThe result is a new monadic function:\nT -> Task<R>T -> Async<R>\nThe next code snippet shows the implementation of the Kleisli  operator in C#, which \nuses the monadic Bind  operator underneath. The operator Bind  (or SelectMany ) for \nthe Task  type was introduced in chapter 7:\n static Func<T, Task<U>> Kleisli<T, R, U>(Func<T, Task<R>> task1,   Func<R, Task<U>> task2) => async value => await task1(value).Bind(task2);\nThe equivalent function in F# can also be defined using the conventional kleisli  \ninfix operator >=>, in this case applied to the Async  type:\nlet kleisli (f:'a -> Async<'b>) (g:'b -> Async<'c>) (x:'a) = (f x) >>= glet (>=>) (f:'a -> Async<'b>) (g:'b -> Async<'c>) (x:'a) = (f x) >>= g\nThe Async  bind  and infix  operator >>= was introduced in chapter 9. Here’s the imple-\nmentation as a reminder:\nlet bind (operation:'a -> Async<'b>) (xAsync:Async<'a>) = async {        let! x = xAsync        return! operation x }let (>>=)(item:Async<'a>) (operation:'a -> Async<'b>) = bind operation item\n \n446 chapter  13 Recipes and design patterns for successful concurrent programming\nLet’s see where and how the Kleisli  operator can help. Consider the case of multiple \nasynchronous operations that you want to compose effortlessly. These functions have the following signature:\noperationOne   : ('a -> Async<'b>) operationTwo   : ('b -> Async<'c>) operationThree : ('c -> Async<'d>) \nConceptually, the composed function would look like:\n(‘a -> Async<’b>) -> (‘b -> Async<’c>) -> (‘c -> Async<’d>)\nAt a high level, you can think of this composition over monadic functions as a pipeline, where the result of the first function is piped into the next one and so on until the last step. In general, when you think of piping, you can think of two approaches: applica-tive (\n<*>) and monadic ( >>=). Because you need the result of the previous call in your \nnext call, the monadic style ( >>=) is the better choice. \nFor this example, you use the TamingAgent  from the previous recipe. The Taming-\nAgent  has the method member Ask, whose signature matches the scenario, where it \ntakes a generic argument  'T and returns an Async<'R>  type. At this point, you use the \nKleisli  operator to compose a set of TamingAgent  types to form a pipeline of agents, as \nshown in figure 13.18. The result of each agent is computed independently and passed as input, in the form of a message, to the next agent until the last node of the chain per -\nforms the final side effect. The technique of linking and composing agents can lead to robust designs and concurrent systems. When an agent returns (replies back) a result to the caller, it can be composed into a pipeline of agents.\nTaming\nagen tTaming\nagen t>=>Taming\nagen t>=>Taming\nagen t>=>\nFigure 13.18  The pipeline processing pattern is useful when you want to process data in multiple steps. \nThe idea behind the pattern is that inputs are sent to the first agent in the pipeline. The main benefit of the pipeline processing pattern is that it provides a simple way to balance the tradeoff between overly sequential processing (which may reduce performance) and overly parallel processing (which may have a large overhead).\nThis listing shows the TamingAgent  composition in action. The example is a rework of \nlisting 13.20, which reuses the same function for loading, transforming, and saving an image.\nListing 13.21  TamingAgent  with the Kleisli  operator\nlet pipe limit operation job : Async<_> =       let agent = TamingAgent(limit, operation)    agent.Ask(job)let loadImageAgent = pipe 2 loadImage    let apply3DEffectAgent = pipe 2 apply3D  let saveImageAgent = pipe 2 saveImage    Creates an instance of the TamingAgent type and exposes its asynchronous Ask method, which ensures a reply back to the caller through the AsyncReplyChannel when the job completes \nCreates an instance of the TamingAgent agent pipe for each function about image processing\n \n 447 Summary\nlet pipeline =           loadImageAgent >=> apply3DEffectAgent >=> saveImageAgent let transformImages() =       let images = Directory.GetFiles(@"".\Images"")    for image in images do        pipeline image         |> run (fun imageName -> printfn ""Saved image %s"" imageName)\nIn this example, the program uses the TamingAgent  to transform an image, different \nthan listing 13.20. In earlier recipes, the three functions that load, transform, and save, in that order, an image to the local filesystem are composed together to form a new function. This function is handled and applied to all the incoming messages by a sin-gle instance of \nTamingAgent  type. In this application (listing 13.21), an instance of \nTamingAgent  is created for each function to run, and then the agents are composed \nthrough the underlying method Ask to form a pipeline. The Ask asynchronous func-\ntion ensures a reply to the caller through the AsyncReplyChannel  when the job com-\npletes. The composition of the agents is eased by the Kleisli  operator.\nThe purpose of the pipe  function is to help create an instance of the TamingAgent  \nand expose the function Ask, whose signature 'a -> Async<'b>  resembles the monadic \nBind  operator used for the composition with other agents.\nAfter the definition of the three agents, loadImageAgent , apply3DEffectAgent , and \nsaveImageAgent , using the pipe  helper function, it becomes simple to create a pipeline \nby composing these agents using the Kleisli  operator. \nSummary\n¡ You should use a concurrent object pool to recycle instances of the same objects without blocking to optimize the performance of a program. The number of GC generations can be dramatically reduced by using a pool of objects, which improves the speed of a program’s execution.\n¡ You can parallelize a set of dependent tasks with a constrained order of execu-tion. This process is useful because it maximizes parallelism as much as possible among the execution of multiple tasks, regardless of dependency. \n¡ Multiple threads can coordinate the access of shared resources for reader-writer types of operations without blocking, maintaining a FIFO ordering. This coordi-nation allows the read operations to run simultaneously, while asynchronously (non-blocking) waiting for eventual write operations. This pattern increases the performance of an application due to introduction of parallelism and the reduced consumption of resources.\n¡ An event aggregator acts similar to the mediator design pattern, where all events go through a central aggregator and can be consumed from anywhere in the application. Rx allows you to implement an event aggregator that supports multi-threading to handle multiple events concurrently.Combines the asynchronous operations generated \nfrom the pipe function using the Kleisli operator\nStarts the process by reading the image files and pushing a new job to the pipeline \n \n448 chapter  13 Recipes and design patterns for successful concurrent programming\n¡ You can implement a custom Rx scheduler using the IScheduler  interface, to \nallow the taming of incoming events with a fine control over the degree of par -\nallelism. Furthermore, by explicitly setting the level of parallelism, the Rx sched-uler internal thread pool isn’t penalized with downtime for expanding the size of threads when required.\n¡ Even without built-in support for the CSP programming model in .NET, you can use either the F# \nMailboxProcessor  or TDF to coordinate and balance the pay-\nload between asynchronous operations in a non-blocking synchronous message- passing style.",11632
145-14.3 Choosing the right concurrent programming model.pdf,145-14.3 Choosing the right concurrent programming model,"44914Building a scalable mobile \napp with concurrent \nfunctional programming\nThis chapter covers\n¡ Designing scalable, performant applications\n¡ Using the CQRS pattern with WebSocket notifications\n¡ Decoupling an ASP.NET Web API controller using Rx\n¡ Implementing a message bus\nLeading up to this chapter, you learned about and mastered concurrent functional techniques and patterns for building highly performant and scalable applications. This chapter is the culmination and practical application of those techniques, where you use your knowledge of TPL tasks, asynchronous workflow, message-passing pro-gramming, and reactive programming with reactive extensions to develop a fully concurrent application. \nThe application you’re building in this chapter is based on a mobile interface that \ncommunicates with a Web API endpoint for real-time monitoring of the stock market. It includes the ability to send commands to buy and sell stocks and to maintain those orders using a long-running asynchronous operation on the server side. This opera-tion reactively applies the trade actions when the stocks reach the desired price point. \n \n450 chapter  14 Building a scalable mobile app with concurrent functional programming\nDiscussion points include architecture choice and explanation of how the functional \nparadigm fits well in both the server and client sides of a system when designing a scal-able and responsive application. By the end of this chapter, you’ll know how to design optimal concurrent functional patterns and how to choose the most effective concur -\nrent programming model. \n14.1 Functional programming on the server in the real world \nA server-side application must be designed to handle multiple requests concurrently. \nIn general, conventional web applications can be thought of as embarrassingly parallel, because requests are entirely isolated and easy to execute independently. The more pow-erful the server running the application, the higher the number of requests it can handle. \nThe program logic of modern, large-scale web applications is inherently concur -\nrent. Additionally, highly interactive modern web and real-time applications, such as multiplayer browser games, collaborative platforms, and mobile services are a huge challenge in terms of concurrency programming. These applications use instant noti-fications and asynchronous messaging as building blocks to coordinate the different operations and communicate between different concurrent requests that likely run in parallel. In these cases, it’s no longer possible to write a simple application with a single sequential control flow; instead, you must plan for the synchronization of independent components in a holistic manner. You might ask, why should you use FP when building a server-side application?\nIn September 2013, Twitter published the paper “Your Server as a Function” (Marius \nEriksen, https:/ /monkey.org/~marius/funsrv.pdf). Its purpose was to validate the archi-\ntecture and programming model that Twitter adopted for building server-side software on a large scale, where systems exhibit a high degree of concurrency and environmental variability. The following is a quote from the paper: \nWe present three abstractions around which we structure our server software at Twitter. They adhere to the style of functional programming—emphasizing immutability, the composition of first-class functions, and the isolation of side effects—and combine to present a large gain in flexibility, simplicity, ease of reasoning, and robustness.\nThe support provided for concurrent FP in .NET is key to making it a great tool for server-side programming. Support exists for running operations asynchronously in a declarative and compositional semantic style; additionally, you can use agents to develop thread-safe components. You can combine these core technologies for declar -\native processing of events and for efficient parallelism with the TPL.\nFunctional programming facilitates the implementation of a stateless server (fig-\nure 14.1), which is an important asset for building scalability when architecting large web applications required to handle a huge number of request concurrently, such as social networks or e-commerce sites. A program is stateless when the operations (such as functions, methods, and procedures) aren’t sensitive to the state of the computation. Consequently, all the data used in an operation is passed as inputs to the operation, \nand all the data used by the operations invoked is passed back as outputs. A stateless \n \n 451 How to design a successful performant application \ndesign never stores application or user data for later computational needs. The stateless design eases concurrency, because it’s easy for each stage of the application to run on a different thread. The stateless design is the key that makes the design able to scale out perfectly according to Amdahl’s Law.\nIn practice, a stateless program can be effortlessly parallelized and distributed \namong computers and processes to scale out performance. You don’t need to know where the computation runs, because no part of the program will modify any data struc-tures, which avoids data races. Also, the computation can run in different processes or different computers without being constrained to perform in a specific environment.\nClient 1\nLoad\nbalancerData 1Stateful server\nClient 2 Data 2\nClient 3 Data 3Client 1Data\nData\nData\nData\nData\nDataStateless server\nClient 2\nClient 3\nFigure 14.1  Server with state (stateful) compared to a server without state (stateless). The stateful \nserver must keep the state between requests, which limits the scalability of the system, requiring more resources to run. The stateless server can auto-scale because there’s no sharing of state. Before stateless servers, there can be a load balancer that distributes the incoming requests, which can be routed to any machine without worrying about hitting a particular server.\nUsing FP techniques, you can build sophisticated, fully asynchronous and adaptive systems that auto-scale using the same level of abstractions, with the same semantics, across all dimensions of scale, from CPU cores to data centers.\n14.2 How to design a successful performant application \nWhen processing hundreds of thousands of requests simultaneously per second in a large-scale setting, you need a high degree of concurrency and efficiency in handling I/O and synchronization to ensure maximum throughput and CPU use in server soft-ware. Efficiency, safety, and robustness are paramount goals that have traditionally conflicted with code modularity, reusability, and flexibility. The functional paradigm emphasizes a declarative programming style, which forces asynchronous programs to be structured as a set of components whose data dependencies are witnessed by the various asynchronous combinators. \n \n452 chapter  14 Building a scalable mobile app with concurrent functional programming\nNOTE  As discussed in chapter 8, asynchronous I/O operations should run in \nparallel, because their scalability can outnumber the processors available by an order of magnitude. Furthermore, to correctly achieve such unbounded resource capability, the asynchronous operations have to be written in a functional style, to not manipulate state in memory, and instead deal with immutable values.\nWhen implementing a program, you should bake performance goals into the design up front. Performance is an aspect of software design that cannot be an afterthought; it must be included as an explicit goal from the start. It’s not impossible to redesign an \nexisting application from the ground up, but it’s far more expensive than designing it correctly in the first place. \n14.2.1 The secret sauce: ACD\nYou want a system capable of flexing to an increase (or decrease) of requests with a commensurate boost in parallel speedup with the addition of resources. The secret ingredients for designing and implementing such a system are asynchronicity, caching, and distribution (ACD): \n¡ Asynchronicity refers to an operation that completes in the future rather than in real time. You can interpret asynchronicity as an architectural design—queuing work that can be completed later to smooth out the processing load, for example. It’s important to decouple operations so you do the minimal amount of work in performance-critical paths. Similarly, you can use asynchronous programming to schedule requests for nightly processes.\n¡ Caching aims to avoid repeating work. For example, caching saves the results of earlier work that can be used again later, without repeating the work performed to get those results. Usually, caching is applied in front of time-consuming opera-tions that are frequently repeated and whose output doesn’t change often.\n¡ Distribution aims to partition requests across multiple systems to scale out process-ing. It’s easier to implement distribution in a stateless system: the less state the server holds, the easier it is to distribute work. \nNOTE  When designing a performant, scalable, and resilient web application, \nit’s important to consider the points made in “Fallacies of Distributed Com-puting” (www.rgoarchitects.com/Files/fallacies.pdf). Arnon Rotem-Gal-Oz, the author, refers to the assumption that distributed systems work in a secure, reliable, homogeneous network that has zero latency, infinite bandwidth, and zero transport cost, and in which the topology doesn’t change.\nACD is a main ingredient for writing scalable and responsive applications that can maintain high throughput under a heavy workload. That’s a task that’s becoming increasingly vital. \n \n 453 How to design a successful performant application \n14.2.2 A different asynchronous pattern: queuing work for later execution\nAt this point, you should have a clear idea of what asynchronous programming means. Asynchronicity, as you recall, means you dispatch a job that will complete in the future. \nThis can be achieved using two patterns. The first is based on continuation passing style (CPS), or callbacks, discussed in chapters 8 and 9. The second is based on asynchro-nous message passing, covered in chapters 11 and 12. As mentioned in the previous section, asynchronicity can also be the result (behavior) of a design in an application. \nThe pattern in figure 14.2 implements asynchronous systems at a design level, aim-\ning to smooth the workload of the program by sending the operations, or requests to do work, to a service that queues tasks to be completed in the future. The service can be in a remote hardware device, a remote server in a cloud service, or a different process in a local machine. In the latter case, the execution thread sends the request in a fire-and-forget fashion, which releases it for further work at a later time. An example of a task that uses this design is scheduling a message to be sent to a mailing list. \nQueueSends workrequest toqueue.\nThread 1Server\nprocessExecution\nthread A\nProcessing serverExecution\nthread AExecution thread A\nThread 2\nThread 3\nFigure 14.2  The work is passed to a queue, and the remote worker picks up the message and performs \nthe requested action later in the future.\nWhen the operation completes, it can send a notification to the origin (sender) of the request with details of the outcome. Figure 14.2 shows six steps:\n1 The execution thread sends the job or request to the service, which queues it. The task is picked up and stored to be performed in the future.\n2 At some point, the service grabs the task from the queue and dispatches the work to be processed. The processing server is responsible for scheduling a thread to run the operation.\n3 The scheduled thread runs the operation, likely using a different thread per task.\n4 Optimally, when the work is completed, the service notifies the origin (sender) that the work is completed. \n \n454 chapter  14 Building a scalable mobile app with concurrent functional programming\n5 While the request is processed in the background, the execution thread is free to perform other work.\n6 If something goes wrong, the task is rescheduled (re-queued) for later execution.\nInitially, online companies invested in more powerful hardware to accommodate the increased volume of requests. This approach proved to be a pricey option, considering the associated costs. In recent years, Twitter, Facebook, StackOverflow.com, and other companies have proven that it’s possible to have a quick, responsive system with fewer machines through the use of good software design and patterns such as ACD. \n14.3 Choosing the right concurrent programming model\nIncreasing the performance of a program using concurrency and parallelism has been \nat the center of discussion and research for many years. The result of this research has been the emergence of several concurrency programming models, each with its own strengths and weaknesses. The common theme is a shared ambition to perform and offer characteristics to enable faster code. In addition to these concurrency program-ming models, companies have developed tools to assist such programming: Microsoft created the TPL and Intel incorporated Threading Building Blocks (TBB) to produce high-quality and efficient libraries to help professional developers build parallel pro-grams. There are many concurrency programming models that vary in their task inter -\naction mechanisms, task granularities, flexibility, scalability, and modularity. \nAfter years of experience in building high-scalable systems, I’m convinced that the \nright programing model is a combination of programming models tailored to each part of your system. You might consider using the actor model for message-passing systems, and PLINQ for data parallelism computation in each of your nodes, while downloading data for pre-computation analysis by using non-blocking I/O asynchronous processing. The key is finding the right tool or combination of tools for the job. \nThe following list represents my choice for concurrent technology based on  \ncommon cases: \n¡ In the presence of pure functions and operations with well-defined control dependencies, where the data can be partitioned or operate in a recursive style, consider using TPL to establish a dynamic task parallel computation in the form of either the Fork/Join or Divide and Conquer pattern. \n¡ If a parallel computation requires preserving the order of the operations, or the algorithm depends on logical flow, then consider using a DAG with either the TPL task primitive or the agent model (see chapter 13).\n¡ In the case of a sequential loop, where each iteration is independent and there are no dependencies among the steps, the TPL Parallel Loop can speed up perfor -\nmance by computing the data in simultaneous operations running in separate tasks.\n¡ In the case of processing data in the form of a combination operator, for exam-ple by filtering and aggregating the input elements, Parallel LINQ (PLINQ) is likely a good solution to speed up computation. Consider a parallel reducer (also called a fold or aggregate), such as the parallel \nAggregator  function, for merg-\ning the results and using the Map-Reduce pattern.\n \n 455 Choosing the right concurrent programming model\n¡ If the application is designed to perform a sequence of operations as a workflow, and if the order of execution for a set of tasks is relevant and must be respected, then use either the Pipeline or Producer/Consumer pattern; these are great solutions for parallelizing the operations effortlessly. You can easily implement these patterns using either the TPL Dataflow or F# \nMailboxProcessor .\nKeep in mind when building deterministic parallel programs that you can build them from the bottom up by composing deterministic parallel patterns of computation and data access. It’s recommended that parallel patterns should provide control over the granularity of their execution, expanding and contracting the parallelism based on the resources available. \nIn this section, you’ll build an application that simulates an online stock market ser -\nvice (figure 14.3). This service periodically updates stock prices and pushes the updates to all connected clients in real time. This high-performance application can handle huge numbers of simultaneous connections inside a web server.\nFigure 14.3  UI of the mobile (Apple iPad) stock market example. The panel on the left side provides \nstock price updates in real time. The panel on the right is used to manage the portfolio and set trade orders for buying and selling stocks.\nThe client is a mobile application, an iOS app for iPhone built using Xamarin and Xamarin.Forms. In the mobile client, the values change in real time in response to notifications from the server. Users of the application can manage their own portfolio by setting orders to buy and/or sell a specific stock when it reaches a predetermined \n \n456 chapter  14 Building a scalable mobile app with concurrent functional programming\nprice. In addition to the mobile application, a WPF version of the client program is provided in the downloadable source code. \nNOTE  To run the mobile project, install Xamarin (www.xamarin.com). Refer -\nence the online documentation for further instructions.\nXamarin and Xamarin.Forms\nXamarin is a framework that developers can use to rapidly create cross-platform user interfaces. It provides an abstraction for the user interface that will be rendered using native controls on iOS, Android, Windows, or Windows Phone. This means that applica-tions can share a large portion of their user interface code and still retain the native look and feel of the target platform. Xamarin.Forms is a cross-platform, natively backed UI toolkit abstraction that developers can use to easily create user interfaces that can be shared across Android, iOS, Win -\ndows, and Windows Phone. The user interfaces are rendered using the native controls of the target platform, making it possible for Xamarin.Forms applications to retain the appropriate look and feel for each platform. Both Xamarin and Xamarin.Forms are huge topics that aren’t relevant in the context of this book. For more information, see www.xamarin.com/forms .\n \nAs you build your application, you’ll take a closer look at how to apply functional concurrency to such an application. You'll combine this knowledge with concurrent functional techniques and patterns presented in previous chapters. You’ll use the Command and Query Responsibility Segregation (CQRS) pattern, Rx, and asyn-chronous programming to handle parallel requests. You’ll include event sourcing based on functional persistence (that is, an event store using the agent-program-ming model), and more. I explain these patterns later with the pertinent part of the application. \nThe web server application is an ASP.NET Web API that uses Rx to push the messages \noriginated by the incoming requests from the controller to other components of the application. These components are implemented using agents (F# \nMailboxProcessor ) \nthat spawn a new agent for each established and active user connection. In this way, the application can be maintained in an isolated state per user, and provide an easy oppor -\ntunity for scalability. \nThe mobile application is built in C#, which is, in general, a good choice for cli-\nent-side development in combination with the TAP model and Rx. Instead of C#, for the web-server code you’ll use F#; but you can find the C# version of the program in the source code of this book. The primary reason for choosing F# for the server-side code is immutability as a default construct, which fits perfectly in the stateless architecture used in the stock market example. Also, the built-in support for the agent programming",19879
146-14.3.1 Real-time communication with SignalR.pdf,146-14.3.1 Real-time communication with SignalR,,0
147-14.4 Real-time trading stock market high-level architecture.pdf,147-14.4 Real-time trading stock market high-level architecture,"457 Real-time trading: stock market high-level architecture \nmodel with the F# MailboxProcessor  can encapsulate and maintain state effortlessly \nin a thread-safe manner. Furthermore, as you’ll see shortly, F# represents a less-verbose solution compared to C# for implementing the CQRS pattern, making the code explicit and capturing what happens in a function without hidden side effects.\nThe application uses ASP.NET SignalR to provide server broadcast functionality for \nreal-time updates. Server broadcast refers to a communication initiated by the server and \nthen sent to clients. \n14.3.1 Real-time communication with SignalR\nMicrosoft’s SignalR library provides an abstraction over some of the transports that \nare required to push server-side content to the connected clients as it happens in real time. This means that servers and their clients can push data back and forth in real time, establishing a bidirectional communication channel. SignalR takes advantage of several transports, automatically selecting the best available transport given the client and server.\nThe connection starts as HTTP and is then promoted to a WebSocket connection if \navailable. WebSocket is the ideal transport for SignalR since it makes the most efficient use of server memory, has the lowest latency, and has the greatest number of underlying features. If these requirements aren’t met, SignalR falls back, attempting to use other transports to make its connections, such as Ajax long polling. SignalR will always try to use the most efficient transport and will keep falling back until it selects the best one that’s compatible with the context. This decision is made automatically during an initial stage in the communication between the client and the server, known as negotiation. \n14.4 Real-time trading: stock market high-level architecture \nBefore diving into the code implementation of the stock market application, let’s review the high-level architecture of the application so you have a good handle on what you’re developing. The architecture is based on the CQRS pattern, which enforces the separation between domain layers and the use of models for reading and writing. \nNOTE  The code used to implement the server side of the application in this \nchapter is in F#, but you can find the full C# version in the online download-able source code. The same principles explained in the following sections apply to both C# and F#.\nThe key tenet of CQRS is to separate commands, which are operations that cause state change (side effects in the system), from query requests that provide data for read-only activities without changing the state of any object, as shown in figure 14.4. The CQRS patterns are also based on the separation of concerns, which is important in all aspects of software development and for solutions built on message-based architectures.\n \n458 chapter  14 Building a scalable mobile app with concurrent functional programming\nQuery modelQuery service\nClientRead\nstorage\nWrite\nstorage Command modelCommand servic e\nFigure 14.4  The CQRS pattern enforces the separation between domain layers and the use of models \nfor reading and writing. To maximize the performance of the read operations, the application can benefit from a separate data storage optimized specifically for queries. Often, such storage might be a NoSQL database. The synchronization between the read/write storage instances is performed asynchronously in the background mode, and can take some time. Such data storages are considered to be eventually consistent.\nThe benefits of using the CQRS pattern include adding the ability to manage more business complexity while making your system easier to scale down, the ability to write optimized queries, and simplifying the introduction of the caching mechanism by wrapping the read portion of the API. Employing CQRS in the case of systems with a massive disparity between the workload of writes and reads allows you to drastically scale the read portion. Figure 14.5 shows the diagram of the stock market web-server application based on the CQRS pattern.\nYou can think about this functional architecture as a dataflow architecture. Inside \nthe application, data flows through various stages. In each step, the data is filtered, enriched, transformed, buffered, broadcast, persisted, or processed any number of ways. The steps of the flow shown in figure 14.5 are as follows:\n1 The user sends a request to the server. The request is shaped as a command to set a trading order to buy or sell a given stock. The ASP.NET Web API controller implements the \nIObservable  interface to expose the Subscribe  method, which \nregisters observers that listen to the incoming request. This design transforms the controller in a message publisher, which sends the command to the subscrib-ers. In this example, there’s only one subscriber, an agent (\nMailboxProcessor ) \nthat acts as a message bus. But there could be any number of subscribers, for example for logging and performance metrics.\n2 The incoming requests into the Web API actions are validated and transformed into a system command, which is wrapped into an envelope that enriches it with metadata such as a timestamp and a unique ID. This unique ID, which usually is represented by the SignalR connection ID, is used later to store the events aggre-gated by a unique identifier that’s user-specific, which simplifies the targeting and execution of potential queries and replaying event histories. \n \n 459 Real-time trading: stock market high-level architecture \nEvent\nstoreCommand\nhandle rCommand StockT icker\nTrading\nsupervisorSubscriber-publisher\nUpdates clients throughout Signal RValidatio nWeb API\n[Post]\ncommand\nSignalR\nconnection s\nTrading\nagentTrading\nagentTrading\nagentStockMarket\nupdates stock prices\nSignalR HUB\n• Rx• Agent• Rx• Agent• Rx• Agent• Rx• Agent• RxPubSub• Agent• Async\n• Rx• Agent\n• Async\n• Rx• Agent\n• Async\n• Agent• Async\n• Rx• Agent\nFigure 14.5  A representative model of the stock market web-server application, which is based on \nthe CQRS pattern. The commands ( Write s) are pushed across the application pipeline to perform the \ntrading operations in a different channel than the queries ( Read s). In this design, the queries ( Read s) \nare performed automatically by the system in the form of server notifications, which are broadcast to the clients through SignalR connections. Imagine SignalR as the channel that allows the client to receive the notifications generated from the server. In the callouts it’s specified as the technologies used to implement the specified component.\n3 The command is passed into a command handler, which pushes the message to subscribers through a message bus. The subscriber of the command handler is the \nStockTicker , an object implemented using an agent to maintain the state, as \nthe name implies, of the stock market tickers.\n4 The StockTicker  and StockMarket  types have established a bidirectional com-\nmunication, which is used to notify about stock price updates. In this case, Rx is used to randomly and constantly update the stock prices that are sent to the \nStockMarket  and then flow to the StockTicker . The SignalR hub then broad-\ncasts the updates to all the active client connections. \n5 The StockTicker  sends the notification to the TradingCoordinator  object, \nwhich is an agent that maintains a list of active users. When a user registers with the application, the \nTradingCoordinator  receives a notification and spawns a \nnew agent if the user is new. The application server creates a new agent instance for each incoming request that represents a new client connection. The \nTrad-\ningCoordinator  object implements the IObservable  interface, which is used to \nestablish a reactive publisher-subscriber with Rx to send the messages to the regis-tered observers, the \nTradingAgent s. \n \n460 chapter  14 Building a scalable mobile app with concurrent functional programming\n6 The TradingCoordinator  receives the commands for trading operations, and dis-\npatches them to the associated agent (user), verifying the unique client connec-tion identifier. The \nTradingAgent  type is an agent that implements the IObserver  \ninterface, which is registered to receive the notification from the IObservable  \nTradingCoordinator . There’s a TradingAgent  for each user, and the main pur -\npose is to maintain the state of a portfolio and the trading orders to buy and sell the stocks. This object is continuously receiving stock market updates to verify whether any of the orders in its state satisfy the criteria to trigger the trading operation. \n7 The application implements event sourcing to store the trading events. The events are per group—by user and ordered by timestamp. Potentially, the history of each user can be replayed. \n8 When a trade is triggered, the TradingAgent  notifies the client’s mobile appli-\ncation through SignalR. The application’s objective is to have the client sending the trade orders and waiting asynchronously for a notification when each opera-tion is completed.\nThe application diagram in figure 14.5 is based on the CQRS pattern, with a clear sep-aration between the reads and the writes. It’s interesting to note that real-time notifica-tions are enabled for the query side (reads), so the user doesn’t need to send a request to retrieve updates.\nThe envelope \nIn general, it’s good practice to wrap messages in envelopes, because they can carry extra information about the message, which is convenient for implementing a message-passing system. Usually, the most important pieces of extra information are a unique ID and a timestamp when the message was created. The message IDs are important because they enable detection of replays and can make your system idempotent. \n \nGoing back to the CQRS pattern diagram from figure 14.4, which is repeated in fig-ure 4.6, you can see that there are two separate storages: one for the read and one for the write. Designing storage separation in this way, using the CQRS pattern, is recom-mended to maximize performance of the read operations. In the case of two detached storages, the write side must update the read side. This synchronization is performed asynchronously in the background mode and can take time, so the read data storage is considered to be eventually consistent. \nQuery modelQuery service\nClientRead\nstorage\nWrite\nstorage Command modelCommand servic e\nFigure 14.6  The CQRS pattern",10553
148-14.6 Lets code the stock market trading application.pdf,148-14.6 Lets code the stock market trading application,"461 Essential elements for the stock market application\nEventual consistency and the consistency, availability,  and partition (CAP) theorem\nThe CAP theorem argues that persisting state in distributed systems is difficult to imple-ment correctly. This theorem says that there are three distinct and desirable properties for distributed systems with an inherent correlation, but any real system can have at most two of these properties for any shared data:\n¡ Consistency is a property that describes a consistent view of data on all nodes of the distributed system, where the system assures that write operations have an atomic characteristic and the updates are disseminated simultaneously to all nodes, yielding the same results.\n¡ Availability is the demand property, where the system will eventually answer every \nrequest in reasonable time, even in the case of failures.\n¡ Partition tolerance describes the fact that the system is resilient to message loss -\nes between nodes. A partition is an arbitrary split between nodes of a system, resulting in complete message loss between nodes. \n \nEventual consistency is a consistency model used in distributed computing to achieve high availability, guaranteeing that eventually all accesses to that item will return the last updated value. In the stock market application, however, the eventual consistency is automatically handled by the system. The users will receive the updates and latest values through real-time notifications when the data changes. This is possible due to the SignalR bidirectional communication between the server and the clients, which is a convenient mechanism because users don’t have to ask for updates, the server will pro-vide updates automatically.\n14.5 Essential elements for the stock market application\nYou haven’t yet learned several essential elements for the stock market application because it’s assumed you’ve already encountered the topics. I’ll briefly review these items and include where you can continue your study as needed.\nThe first essential element is F#. If you have a shallow background in F#, see appen-\ndix B for information and summaries that you might find useful.\nThe server-side application is based on the ASP.NET Web API, which requires knowl-\nedge of that technology. For the client side, the mobile application uses Xamarin and Xamarin.Forms with the Model-View-ViewModel (MVVM) pattern for data binding; but you don’t need to have any particular knowledge of these frameworks.\n \n462 chapter  14 Building a scalable mobile app with concurrent functional programming\nThe MVVM pattern\nThe MVVM pattern can be used on all XAML platforms. Its intent is to provide a clean sep-aration of concerns between the UI controls and their logic. There are three core compo-nents in the MVVM pattern: the Model (business rule, data access, model classes), the \nView (UI Extensible Application Markup Language, or XAML), and the ViewModel (agent \nor middle man between View and Model). Each serves a distinct and separate role. The ViewModel acts as an interface between Model and View. It provides data binding between View and Model data as well as handling all UI actions by using commands. The View binds its control value to properties on a ViewModel, which, in turn, exposes data contained in Model objects.\n \nThroughout the rest of this chapter, you’re going to use the following:\n¡ Reactive Extensions for .NET\n¡ Task Parallel Library\n¡ F# MailboxProcessor\n¡ Asynchronous workflows\nThe same concepts applied in the following code examples are relevant for all the .NET programming languages. \n14.6 Let’s code the stock market trading application\nThis section covers the code examples to implement the real-time mobile stock market application with trading capabilities, as shown in figure 14.7. The parts of the program that aren’t relevant or strictly important with the objective of the chapter are intention-ally omitted. But you can find the full functional implementation in the downloadable source code.\nQuery modelQuery service\nClientRead\nstorage\nWrite\nstorage Command modelCommand servic e\nFigure 14.7  The architecture diagram of the stock market web-server application. This is a high-level \ndiagram compared to figure 14.5, which aims to clarify the components of the application. Note that each component, other than \nValidation  and Command , is implemented using a combination of Rx, the \nIObservable  and IObserver  interfaces, and the agent-programming model.\n \n 463 Let’s code the stock market trading application\nLet’s start with the server Web API controller, where the client mobile application sends the requests to perform the trading operations. \nNOTE  The source code companion for the book has also a WPF implementa-\ntion of the client-side application.\nNote that the controller represents the write domain of the CQRS pattern; in fact, the actions are HTTP POST only, as shown here (the code to note is in bold).\nListing 14.1  Web API trading controller \n[<RoutePrefix(""api/trading"")>]type TradingController() =    inherit ApiController()    let subject = new Subject<CommandWrapper>()      let publish connectionId cmd =          match cmd with        | Result.Ok(cmd) ->                     CommandWrapper.Create connectionId cmd              subject.OnNext                   | Result.Error(e) -> subject.OnError(exn (e))          cmd    let toResponse (request : HttpRequestMessage) result =            match result with            | Ok(_) -> request.CreateResponse(HttpStatusCode.OK)            | _ -> request.CreateResponse(HttpStatusCode.BadRequest)      [<Route(""sell""); HttpPost>]    member this.PostSell([<FromBody>] tr : TradingRequest) = async {                 let connectionId = tr.ConnectionID               return                 {   Symbol = tr.Symbol.ToUpper()                    Quantity = tr.Quantity                    Price = tr.Price                    Trading = TradingType.Sell }                    |> tradingdValidation                           |> publish connectionId                         |> toResponse this.Request              } |> Async.StartAsTask                          interface IObservable<CommandWrapper> with        member this.Subscribe observer = subject.Subscribe observer      override this.Dispose disposing =        if disposing then subject.Dispose()        base.Dispose disposing    The controller uses a Subject instance \nto behave as an observable to publish \ncommands to the observer registered.\nPublishes the command using RxValidates the command using the Result type \nCreates a wrapper around a given command to enrich the type with metadata \nUses a helper \nfunction for the \ncontroller actions \nto deliver an  \nHTTP response \nShows the current connection ID from SignalR context\nValidates using a function composition\nPublishes the command using Rx\nStarts the actions as a Task to make the action run asynchronously\nThe controller uses a Subject instance \nto behave as an observable to publish \ncommands to the registered observer.\nDisposes the Subject; important  to free up the resources \n \n464 chapter  14 Building a scalable mobile app with concurrent functional programming\nThe Web API controller TradingController  exposes the sell ( PostSell ) and the buy \n(PostBuy ) actions. Both these actions have an identical code implementation with dif-\nferent purposes. Only one is presented in the listing, to avoid repetition. \nEach action control is built around two core functions, validate and publish. \ntradingdValidation  is responsible for validating messages per-connection because \nthey’re received from the client. publish  is responsible for publishing the messages to \nthe control subscribers for core processing. \nThe PostSell  action validates the incoming request through the tradingVal -\nidation  function, which returns either Result .Ok or Result .Error  according to the \nvalidity of its input. Then, the output from the validation function is wrapped into a command object using the \nCommandWrapper.Create  function and published to the sub-\nscribed observers subject.OnNext .\nThe TradingController  uses an instance of the Subject  type, from the Rx library, to \nact as an observable by implementing the IObservable  interface. In this way, this con-\ntroller is loosely coupled and behaves as a Publisher/Subscriber pattern, sending the commands to the observers that are registered. The registration of this controller as an \nObservable  is plugged into the Web API framework using a class that implements the \nIHttpControllerActivator , as shown here (the code to note is in bold).\nListing 14.2  Registering a Web API controller as Observable  \ntype ControlActivatorPublisher(requestObserver:IObserver<CommandWrapper>) =   interface IHttpControllerActivator with         member this.Create(request, controllerDescriptor, controllerType) =        if controllerType = typeof<TradingController> then              let obsController =                let tradingCtrl = new TradingController()               tradingCtrl               |> Observable.subscribeObserver requestObserver                |> request.RegisterForDispose                                   tradingCtrl                         obsController :> IHttpController              else raise (ArgumentException(""Unknown controller type requested""))\nThe ControlActivatorPublisher  type implements the interface IHttpController-\nActivator , which injects a custom controller activator into the Web API framework. \nIn this case, when a request matches the type of the TradingController , the Control-\nActivatorPublisher  transforms the controller in an Observable  publisher, and then \nit registers the controller to the command dispatcher. The tradingRequestObserver  \nobserver, passed into the CompositionRoot  constructor, is used as subscription for the \nTradingController  controller, which can now dispatch messages from the actions to \nthe subscribers in a reactive and decoupled manner.Interfaces to plug a new controller constructor or activator into the Web API framework \nIf the controller type  requested matches the TradingController, then a new instance is created and registered as an Observable.\n \n 465 Let’s code the stock market trading application\nUltimately, the sub-value of the subscribed observer requestObserver  represents the \nsubscription and must be registered for disposal together with the TradingController  \ninstance tradingCtrl , using the request .RegisterForDispose  method.\nThis listing shows the next step, the subscriber of the TradingController  observable \ncontroller.\nListing 14.3  Configuring the SignalR hub and agent message bus \ntype Startup() =    let agent = new Agent<CommandWrapper>(fun inbox ->           let rec loop () = async {            let! (cmd:CommandWrapper) = inbox.Receive()             do! cmd |> AsyncHandle               return! loop() }        loop())    do agent.Start()         \n    member this.Configuration(builder : IAppBuilder) =       let config =          let config = new HttpConfiguration()          config.MapHttpAttributeRoutes()          config.Services.Replace(typeof<IHttpControllerActivator>,               ControlActivatorPublisher(Observer.Create(fun x ->                                                      agent.Post(x)))) \n           let configSignalR =                   new HubConfiguration(EnableDetailedErrors = true)     Owin.CorsExtensions.UseCors(builder, Cors.CorsOptions.AllowAll)  builder.MapSignalR(configSignalR) |> ignore  builder.UseWebApi(config) |> ignore\nThe Startup  function is executed when the web application begins to apply the con-\nfiguration settings. This is where the CompositionRoot  class (defined in listing 14.2) \nbelongs, to replace the default IHttpControllerActivator  with its new instance. \nThe subscriber type passed into the ControlActivatorPublisher  constructor is an \nobserver, which posts the messages that arrive from the TradingController  actions to \nthe MailboxProcessor  agent instance. The TradingController  publisher sends the \nmessages through the OnNext  method of the observer interface to all the subscribers, \nin this case the agent, which only depends on the IObserver  implementation, and \ntherefore reduces the dependencies.\nThe MailboxProcessor Post  method, agent.Post , publishes the wrapped message \ninto a Command  type using Rx. Note that the controller itself implements the IObservable  \ninterface, so it can be imagined as a message endpoint, command wrapper, and publisher.Instance of an agent that acts as message bus to send commands\nThe agent asynchronously handles the commands received, publishing them through the AsyncHandle command handler.\nReplaces the default \nIHttpControllerActivator \nbuilt in the Web API \nframework with the custom \nControlActivatorPublisher\nThe root subscriber passed into the ControlActivatorPublisher constructor is an observer that sends messages asynchronously to the agent instance. \nEnables the SignalR \nhubs in the application\n \n466 chapter  14 Building a scalable mobile app with concurrent functional programming\nThe subscriber MailboxProcessor  agent asynchronously handles the incoming \nmessages like a message bus, but at a smaller and more focused level (figure 14.8). A message bus provides a number of advantages, ranging from scalability to a naturally decoupled system to multiplatform interoperability. Message-based architectures that use a message bus focus on common message contracts and message passing. The rest of the configuration method enables the SignalR hubs in the application throughout \nthe \nIAppBuilder  provided.\nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.8  The command and command-handler are implemented in listing 14.4.\nThis listing shows the implementation of the AsyncHandle  function, which handles the \nagent messages in the form of CQRS commands.\nListing 14.4  Command handler with async retry logic \nmodule CommandHandler =    let retryPublish = RetryAsyncBuilder(10, 250)     let tradingCoordinator = TradingCoordinator.Instance()       let Storage = new EventStorage()         let AsyncHandle (commandWrapper:CommandWrapper) =          let connectionId = commandWrapper.ConnectionId       retryPublish {                     tradingCoordinator.PublishCommand(Instance of the custom RetryAsyncBuilder computation expression defined in listing 9.4 A single instance of \nTradingCoordinator to publish \nmessages to the TradingAgent \nrepresenting the active clients\nInstance of the EventStorage for saving events to implement EventSourcing \nCommand handler that executes the domain behavior. In this case it asynchronously publishes  a command with a retry semantic.\n \n 467 Let’s code the stock market trading application\n                      PublishCommand(connectionId, commandWrapper))               let event =                let cmd = commandWrapper.Command                match cmd with                     | BuyStockCommand(connId,trading) ->                        StocksBuyedEvent(commandWrapper.Id, trading)                | SellStockCommand(connId, trading) ->                        StocksSoldEvent(commandWrapper.Id, trading)               let eventDescriptor = Event.Create (commandWrapper.Id, event)            Storage.SaveEvent (Guid(connectionId)) eventDescriptor          }\nThe retryPublish  is an instance of the custom RetryAsyncBuilder  computation \nexpression defined in listing 9.4. This computation expression aims to run operations asynchronously, and it retries the computation, with an applied delay, in case some-thing goes wrong. \nAsyncHandle  is a command handler responsible for executing the \nCommand behaviors on the domain. The commands are represented as trading oper -\nations to either buy or sell stocks. In general, commands are directives to perform an action to the domain (behaviors).\nThe purpose of \nAsyncHandle  is to publish the commands received from the Trad-\ningCoordinator  instance, the next step of the application pipeline, in a message-pass-\ning style. The command is the message received by the MailboxProcessor  agent, \ndefined during the application Startup  (listing 14.3). \nThis message-driven programming model leads to an event-driven type of architec-\nture, where the message-driven system recipients await the arrival of messages and react to them, otherwise lying dormant. In an event-driven system notification, the listeners are attached to the sources of events and are invoked when the event is emitted. \nEvent-driven architecture\nEvent-driven architecture (EDA) is an application design style that builds on the funda -\nmental aspects of event notifications to facilitate immediate information dissemination and reactive business process execution. In an application based on EDA, information is propagated in real time throughout a highly distributed environment, enabling the dif-ferent components of the application that receive a notification to proactively respond to business activities. EDA promotes low latency and a highly reactive system. The dif-ference between event-driven and message-driven systems is that event-driven systems focus on addressable event sources, whereas a message-driven system concentrates on addressable recipients.\n \nThe AsyncHandle  handler is also responsible for transforming each command received \ninto an Event  type, which is then persisted in the event storage (figure 14.9). The event Publishes the command and the user-specified ID to the stock market. The ID is defined by the SignalR connection unique identifier.\nUses a pattern match to transform \nthe command into an event type\nPersists the event into the event storage \n \n468 chapter  14 Building a scalable mobile app with concurrent functional programming\nstorage is part of the event sourcing strategy implementation to store the current state of the application in listing 14.5. \nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.9  The event storage is implemented in listing 14.5. \nListing 14.5  EventBus  implementation using an agent \nmodule EventBus =    let public EventPublisher = new Event<Event>()       let public Subscribe (eventHandle: Events.Event -> unit) =        EventPublisher.Publish |> Observable.subscribe(eventHandle)     let public Notify (event:Event) = EventPublisher.Trigger event  module EventStorage =    type EventStorageMessage =         | SaveEvent of id:Guid * event:EventDescriptor    | GetEventsHistory of Guid * AsyncReplyChannel<Event list option>    type EventStorage() =              let eventstorage = MailboxProcessor.Start(fun inbox ->           let rec loop (history:Dictionary<Guid, EventDescription list>) =              async {                 let! msg = inbox.Receive()                match msg with                | SaveEvent(id, event) ->    Event broker for event-based communication based on the Publisher/Subscriber pattern\nUses event storage message types to save events or get the history\nIn-memory implementation of the event storage using a MailboxProcessor for thread safety \nState of the MailboxProcessor to implement an in-memory event storage Saves the event using \nthe user connection \nSignalR unique ID as a \nkey. if an entry with the \nsame key already exists, \nthe event is appended. \n \n 469 Let’s code the stock market trading application\n                    EventBus.Notify event.EventData                     match history.TryGetValue(id) with                    | true, events -> history.[id] <- (event :: events)                    | false, _ -> history.Add(id, [event])                | GetEventsHistory(id, reply) ->                         match history.TryGetValue(id) with                    | true, events ->                        events |> List.map (fun i -> i.EventData) |> Some                        |> reply.Reply                    | false, _ -> reply.Reply(None)                return! loop history   }          loop (Dictionary<Guid, EventDescriptor list>())) \n        member this.SaveEvent(id:Guid) (event:EventDescriptor) =                   eventstorage.Post(SaveEvent(id, event))        member this.GetEventsHistory(id:Guid) =                 eventstorage.PostAndReply(fun rep -> GetEventsHistory(id,rep))            |> Option.map(List.iter) \nThe EventBus  type is a simple implementation of a Publisher/Subscriber pattern over \nevents. Internally, the Subscribe  function uses Rx to register any given event, which \nis notified when the EventPublisher  is triggered through the Notify  function. The \nEventBus  type is a convenient way to signal different parts of the application when a \nnotification is emitted by a component upon reaching a given state. \nEvents are the result of an action that has already happened, which is likely the \noutput of executing a command. The EventStorage  type is an in-memory storage for \nsupporting the concept of event sourcing, which is basically the idea of persisting a sequence of state-changing events of the application, rather than storing the current state of an entity. In this way, the application is capable of reconstructing, at any given time, an entity’s current state by replaying the events. Because saving an event is a single operation, it’s inherently atomic.\nThe \nEventStorage  implementation is based on the F# agent MailboxProcessor , \nwhich guarantees thread safety for accessing the underlying event data-structure history \nDictionary<Guid, EventDescriptor list> . The EventStorageMessage  DU defines \ntwo operations to run against the event storage:\n¡ SaveEvent  adds an EventDescriptor  to the internal state of the event storage \nagent by the given unique ID. If the ID exists, then the event is appended. \n¡ GetEventsHistory  retrieves the event history in ordered sequence by time \nwithin the given unique ID. In general, the event history is replayed using a given function action, as in listing 14.5.Event broker for event-based communication based on Publisher/Subscriber pattern\nRetrieves the event history \nState of the MailboxProcessor to implement an in-memory event storage \nSaves the event using the user connection SignalR unique ID as key; if an entry with the same key already exists, the event is appended \nRetrieves the event history \nReorders the event history \n \n470 chapter  14 Building a scalable mobile app with concurrent functional programming\nThe implementation uses an agent because it’s a convenient way to abstract away the basics of an event store. With that in place, you can easily create different types of event stores by changing only the two \nSaveEvent  and GetEventsHistory  functions.\nLet’s look at the StockMarket  object shown in figure 14.10. Listing 14.6 shows the \ncore implementation of the application, the StockMarket  object.\nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.10  The StockMarket  object is implemented in listing 14.6. \nListing 14.6  StockMarket  type to coordinate the user connections\ntype StockMarket (initStocks : Stock array) =       let subject = new Subject<Trading>()       static let instanceStockMarket =            Lazy.Create(fun () -> StockMarket(Stock.InitialStocks()))    let stockMarketAgent =        Agent<StockTickerMessage>.Start(fun inbox ->            let rec marketIsOpen (stocks : Stock array)                      (stockTicker : IDisposable) = async {                  let! msg = inbox.Receive()                match msg with                  | GetMarketState(c, reply) ->                     reply.Reply(MarketState.Open)                    return! marketIsOpen stocks stockTicker                   | GetAllStocks(c, reply) ->                     reply.Reply(stocks |> Seq.toList)This instance simulates the stock market, updating the stocks.\nInstance of the Rx Subject that implements a Publisher/Subscriber pattern\nUses two states of the agent to change the \nMarket between the Open or Close state \nState of the MailboxProcessor to keep the updated stock values Uses pattern matching to dispatch the messages to the relative behavior\n \n 471 Let’s code the stock market trading application\n                    return! marketIsOpen stocks stockTicker                   | UpdateStockPrices ->                     stocks                     |> PSeq.iter(fun stock ->                                let isStockChanged = updateStocks stock stocks                        isStockChanged                        |> Option.iter(fun _ ->                            subject.OnNext(Trading.UpdateStock(stock))))                    return! marketIsOpen stocks stockTicker                   | CloseMarket(c) ->                    stockTicker.Dispose()                    return! marketIsClosed stocks                        | _ -> return! marketIsOpen stocks stockTicker }            and marketIsClosed (stocks : Stock array) = async {                    let! msg = inbox.Receive()                match msg with                  | GetMarketState(c, reply) ->                      reply.Reply(MarketState.Closed)                      return! marketIsClosed stocks                | GetAllStocks(c,reply) ->                     reply.Reply((stocks |> Seq.toList))                     return! marketIsClosed stocks                | OpenMarket(c) ->                     return! marketIsOpen stocks (startStockTicker inbox)                | _ -> return! marketIsClosed stocks }            marketIsClosed (initStocks))                member this.GetAllStocks(connId) =        stockMarketAgent.PostAndReply(fun ch -> GetAllStocks(connId, ch))    member this.GetMarketState(connId) =        stockMarketAgent.PostAndReply(fun ch -> GetMarketState(connId, ch))    member this.OpenMarket(connId) =        stockMarketAgent.Post(OpenMarket(connId))    member this.CloseMarket(connId) =        stockMarketAgent.Post(CloseMarket(connId))    member this.AsObservable() = subject.AsObservable().\nSubscribeOn(TaskPoolScheduler.Default)  \n        static member Instance() = instanceStockMarket.Value \nThe StockMarket  type is responsible for simulating the stock market in the applica-\ntion. It uses operations such as OpenMarket  and CloseMarket  to either start or stop \nbroadcasting notifications of stock updates, and GetAllStocks  retrieves stock tickers \nto monitor and manage for users. The StockMarket  type implementation is based on \nthe agent model using the MailboxProcessor  to take advantage of the intrinsic thread \nsafety and convenient concurrent asynchronous message-passing semantic that’s at the core of building highly performant and reactive (event-driven) systems.Updates the stock values, sending notifications to the subscribers \nUses parallel iteration using PSeq to dispatch stock updates as fast as possible\nUses two states of the agent to change the market between the open and closed states \nUses pattern matching to dispatch the messages to the relative behavior\nExposes the StockMarket type as an observable to subscribe the underlying changes\n \n472 chapter  14 Building a scalable mobile app with concurrent functional programming\nThe StockTicker  price updates are simulated by sending high-rate random requests \nto the stockMarketAgent  MailboxProcessor  using UpdateStockPrices , which then \nnotifies all active client subscribers. \nThe AsObservable  member exposes the StockMarket  type as a stream of events \nthroughout the IObservable  interface. In this way, the type StockMarket  can notify \nthe IObserver  subscribed to the IObservable  interface of the stock updates, which are \ngenerated when the message UpdateStock  is received.\nThe function that updates the stock uses a Rx timer to push random values for each \nof the stock tickers registered, increasing or decreasing the prices with a small percent-age, as shown here. \nListing 14.7  Function to update the stock ticker prices every given interval\nlet startStockTicker (stockAgent : Agent<StockTickerMessage>) =     Observable.Interval(TimeSpan.FromMilliseconds 50.0)     |> Observable.subscribe(fun _ -> stockAgent.Post UpdateStockPrices)\nstartStockTicker  is a fake service provider that tells StockTicker  every 50 ms that it’s \ntime to update the prices. \nNOTE  Sending messages to an F# MailboxProcessor  agent (or TPL Data-\nflow block) is unlikely to be a bottleneck in your system, because the Mail-\nboxProcessor  can handle 30 million messages per second on a machine \nwith a 3.3 GHz core.\nThe TradingCoordinator  (figure 14.11) type’s purpose is to manage the underly-\ning SignalR active connections and TradingAgent  subscribers, which act as observ-\ners, through the MailboxProcessor  coordinatorAgent . Listing  14.8 shows the \nimplementation.\nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.11  The trading coordinator is implemented in listing 14.8. \n \n 473 Let’s code the stock market trading application\nListing 14.8  TradingCoordinator  agent to handle active trading children agent\ntype CoordinatorMessage =      | Subscribe of  id : string * initialAmount : float *  \n➥     caller:IHubCallerConnectionContext<IStockTickerHubClient>\n    | Unsubscribe of   id : string    | PublishCommand of connId : string * CommandWrappertype TradingCoordinator() =       //Listing 6.6 Reactive Publisher Subscriber in C#    let subject = new RxPubSub<Trading>()        static let tradingCoordinator =                Lazy.Create(fun () -> new TradingCoordinator())      let coordinatorAgent =        Agent<CoordinatorMessage>.Start(fun inbox ->            let rec loop (agents : Map<string, \n➥ (IObserver<Trading> * IDisposable)>) = async {\n                let! msg = inbox.Receive()                match msg with                   | Subscribe(id, amount, caller) ->                        let observer = TradingAgent(id, amount, caller)                      let dispObsrever = subject.Subscribe(observer)                    observer.Agent                     |> reportErrorsTo id supervisor |> startAgent                       caller.Client(id).SetInitialAsset(amount)                       return! loop (Map.add id (observer :> \n➥ IObserver<Trading>, dispObsrever) agents) \n                | Unsubscribe(id) ->                       match Map.tryFind id agents with                     | Some(_, disposable) ->                          disposable.Dispose()                        return! loop (Map.remove id agents)                   | None -> return! loop agents                | PublishCommand(id, command) ->                      match command.Command with                   | TradingCommand.BuyStockCommand(id, trading) ->                       match Map.tryFind id agents with                      | Some(a, _) ->                        let tradingInfo = { Quantity=trading.Quantity;                                             Price=trading.Price;                                             TradingType = TradingType.Buy}                         a.OnNext(Trading.Buy(trading.Symbol, tradingInfo))                         return! loop agents                     | None -> return! loop agents                   | TradingCommand.SellStockCommand(id, trading) ->                      match Map.tryFind id agents with                     | Some(a, _) ->Uses a discriminated union to define the message type for the TradingCoordinator\nUses an agent-based type, which is the core for the subscription of the sub-registered observer agents and for the coordination of their execution operation \nReactive Publisher/Subscriber defined in listing 6.6\nUses a singleton instance of \nthe type TradingCoordinator\nSubscribes a new TradingAgent that will be notified when a stock price is updated\nUses an instance of the TradingAgent that acts as an observer to receive notifications in a reactive style\nApplies supervision logic to the newly created TradingAgents defined in chapter 11Notifies the client of its successful registration using SignalR\nUnsubscribes an existing TradingAgent from a given unique ID and closes the channel to receive notifications. The unsubscription is performed by disposing the observer.Publishes the commands using reactive Publisher/Subscriber to set the orders to Buy or Sell a stock\n \n474 chapter  14 Building a scalable mobile app with concurrent functional programming\n                        let tradingInfo = { Quantity=trading.Quantity;                                             Price=trading.Price;                                             TradingType = TradingType.Sell}                        a.OnNext(Trading.Sell(trading.Symbol, tradingInfo))                       return! loop agents                    | None -> return! loop agents }            loop (Map.empty))    member this.Subscribe(id : string, initialAmount : float,     \n➥ caller:IHubCallerConnectionContext<IStockTickerHubClient>) = \n        coordinatorAgent.Post(Subscribe(id, initialAmount, caller))     member this.Unsubscribe(id : string) =                      coordinatorAgent.Post(Unsubscribe(id))    member this.PublishCommand(command) =                      coordinatorAgent.Post(command)      member this.AddPublisher(observable : IObservable<Trading>) =                    subject.AddPublisher(observable)      static member Instance() = tradingCoordinator.Value      interface IDisposable with           member x.Dispose() =  subject.Dispose()\nThe CoordinatorMessage  discriminated union defines the messages for the coor-\ndinatorAgent . These message types are used for coordinating the operations for the \nunderlying TradingAgent s subscribed for update notifications. \nYou can think of the coordinatorAgent  as an agent that’s responsible for maintain-\ning the active clients. It either subscribes or unsubscribes them according to whether they’re connecting to the application or disconnecting from it, and then it dispatches operational commands to the active ones. In this case, the SignalR hub notifies the \nTradingCoordinator  when a new connection is established or an existing one is \ndropped so it can register or unregister the client accordingly. \nThe application uses the agent model to generate a new agent for each incoming \nrequest. For parallelizing request operations, the TradingCoordinator  agent spawns \nnew agents and assigns work via messages. This enables parallel I/O-bound operations as well as parallel computations. The \nTradingCoordinator  exposes the IObservable  \ninterface through an instance of the RxPubSub  type, which is defined in listing 6.6.  \nRxPubSub is used here to implement a high-performant reactive Publisher/Subscriber, \nwhere the TradingAgent  observers can register to receive potential notifications when a \nstock ticker price is updated. In other words, the TradingCoordinator  is an Observable  \nthat the TradingAgent  observer can subscribe to, implementing a reactive Publisher/\nSubscriber pattern to receive notifications. Subscribes a new \nTradingAgent that will \nbe notified when a stock \nprice is updated\nThe TradingCoordinator is exposed as an observable through an instance of the Reactive Publisher/Subscriber RxPubSub type.\nUses a member that is allowed to add publishers to trigger the notification for the RxPubSub Uses a singleton instance of \nthe type TradingCoordinator\nDisposes the underlying RxPubSub subject type (important)\n \n 475 Let’s code the stock market trading application\nThe method member AddPublisher  registers any type that implements the  \nIObservable interface, which is responsible for updating all the TradingAgent s sub-\nscribed. In this implementation, the IObservable  type registered as Publisher  in the \nTradingCoordinator is the StockMarket  type.\nThe StockMarket  member methods Subscribe  and Unsubscribe  are used to regis-\nter or unregister client connections received from the StockTicker  SignalR hub. The \nrequests to subscribe or unsubscribe are passed directly to the underlying coordina -\ntorAgent observable type. \nThe subscription operation triggered by the Subscribe  message checks if a  \nTradingAgent  (figure 14.12) type exists in the local observer state, verifying the con-\nnection unique ID. If the TradingAgent  doesn’t exist, then a new instance is created, \nand it’s subscribed to the subject instance by implementing the IObserver  inter -\nface. Then, the supervision strategy reportErrorsTo  (to report and handle errors) \nis applied to the newly created TradingAgent  observer. This supervision strategy was \ndiscussed in section 11.5.5. \nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.12  The TradingAgent  represents an agent-based portfolio for each user connected to \nthe system. This agent keeps the user portfolio up to date and coordinates the operations of buying and selling a stock. The \nTradingAgent  is implemented in listing 14.9.\nNote that the TradingAgent  construct takes a reference to the underlying SignalR \nchannel, which is used to enable direct communication to the client, in this case a mobile device for real-time notifications. The trading operations \nBuy and Sell  are dis-\npatched to the related TradingAgent , which is identified using the unique ID from the \nlocal observer’s state. The dispatch operation is performed using the OnNext  seman-\ntic of the Observer  type. As mentioned, the TradingCoordinator 's responsibility is to \ncoordinate the operations of the TradingAgent , whose implementation is shown in \nlisting 14.9.\n \n476 chapter  14 Building a scalable mobile app with concurrent functional programming\nListing 14.9  TradingAgent  that represents an active user \n   type TradingAgent(connId : string, initialAmount : float, caller : \n➥ IHubCallerConnectionContext<IStockTickerHubClient>) = \n     let agent = new Agent<Trading>(fun inbox ->              let rec loop cash (portfolio : Portfolio)            (buyOrders : Treads) (sellOrders : Treads) = async {           let! msg = inbox.Receive()          match msg with\n          | Kill(reply) -> reply.Reply()              | Error(exn) -> raise exn             | Trading.Buy(symbol, trading) ->                 let items = setOrder buyOrders symbol trading               let buyOrders =                   createOrder symbol trading TradingType.Buy              caller.Client(connId).UpdateOrderBuy(buyOrders)              return! loop cash portfolio items sellOrders          | Trading.Sell(symbol, trading) ->                   let items = setOrder sellOrders symbol trading              let sellOrder =                   createOrder symbol trading TradingType.Sell              caller.Client(connId).UpdateOrderSell(sellOrder)              return! loop cash portfolio buyOrders items          | Trading.UpdateStock(stock) ->                  caller.Client(connId).UpdateStockPrice stock              let cash, portfolio, sellOrders = updatePortfolio cash \n➥ stock portfolio sellOrders TradingType.Sell\n              let cash, portfolio, buyOrders = updatePortfolio cash \n➥ stock portfolio buyOrders TradingType.Buy\n              let asset = getUpdatedAsset portfolio sellOrders \n➥ buyOrders cash   \n              caller.Client(connId).UpdateAsset(asset)                  return! loop cash portfolio buyOrders sellOrders  }     loop initialAmount (Portfolio(HashIdentity.Structural)) \n(Treads(HashIdentity.Structural)) (Treads(HashIdentity.Structural)))\n  member this.Agent = agent  interface IObserver<Trading> with         member this.OnNext(msg) = agent.Post(msg:Trading)       member this.OnError(exn) = agent.Post(Error exn)         member this.OnCompleted() = agent.PostAndReply(Kill)   The constructor accepts a reference of the SignalR \nconnection to enable real-time notifications.\nThe TradingAgent maintains local \nin-memory state of the client portfolio \nand the trading orders in process. Uses a special message to implement the observer method to complete (terminate) the notifications and handle errors\nShows the trade order messages\nUpdates the value of the stock, which notifies the client and updates the portfolio if the new value satisfies any of the trading orders in progress\nChecks the current portfolio for potential \nupdates according to the new stock valueThe client receives notifications through the underlying SignalR channel. \nThe TradingAgent implements the IObserver interface  to act as subscribers for  the observable TradingCoordinator type \nUses a special message to implement the observer method to complete (terminate) the notifications and handle errors\n \n 477 Let’s code the stock market trading application\nThe TradingAgent  type is an agent-based object that implements the IObserver  inter -\nface to allow sending messages to the underlying agent using a reactive semantic. Fur -\nthermore, because the TradingAgent  type is an Observer , it can be subscribed to the \nTradingCoordinator , and consequently receives notifications automatically in the \nform of message passing. This is a convenient design to decouple parts of the applica-tion that can communicate by flowing messages in a reactive and independent man-ner. The \nTradingAgent  represents a single active client, which means that there’s an \ninstance of this agent for each user connected. As mentioned in chapter 11, having thousands of running agents (\nMailboxProcessor s) doesn’t penalize the system. \nThe local state of the TradingAgent  maintains and manages the current client \nportfolio, including the trading orders for buying and selling stocks. When either a \nTradingMessage .Buy or TradingMessage .Sell  message is received, the TradingAgent  \nvalidates the trade request, adds the operation to the local state, and then sends a notifi-cation to the client, which updates the local state of the transaction and the related UI.\nThe \nTradingMessage .UpdateStock  message is the most critical. The TradingAgent  \ncould potentially receive a high volume of messages, whose purpose it is to update the \nPortfolio s with a new stock price. More importantly, because the price of a stock could be \nchanged in the update, the functionality triggered with the UpdateStock  message checks \nif any of the existing (in-progress) trading operations, buyOrders  and sellOrders , are \nsatisfied with the new value. If any of the trades in progress are performed, the portfolio is updated accordingly, and the client receives a notification for each update.\nAs mentioned, the \nTradingAgent entity keeps the channel reference of the connec-\ntion to the client for communicating eventual updates, which is established during the \nOnConnected event in the SignalR hub (figure 14.13 and listing 14.10).\nValidatio nStockMarket\nTrading agent Trading agent Trading agentCommandWeb API\nIObservable Command\nhandle rStockT icker\nSignalR hub\nTrading\ncoordinato rEvent\nstorage\nFigure 14.13  The StockTicker  SignalR hub is implemented in listing 14.10.\n \n478 chapter  14 Building a scalable mobile app with concurrent functional programming\nListing 14.10  StockTicker  SignalR hub \n[<HubName(""stockTicker"")>]   type StockTickerHub() as this =    inherit Hub<IStockTickerHubClient>()  \n    let stockMarket : StockMarket = StockMarket.Instance()    let tradingCoordinator : TradingCoordinator = TradingCoordinator.\nInstance()   \n        override x.OnConnected() =           let connId = x.Context.ConnectionId        stockMarket.Subscribe(connId, 1000., this.Clients)         base.OnConnected()    override x.OnDisconnected(stopCalled) =           let connId = x.Context.ConnectionId        stockMarket.Unsubscribe(connId)           base.OnDisconnected(stopCalled)    member x.GetAllStocks() =            let connId = x.Context.ConnectionId        let stocks = stockMarket.GetAllStocks(connId)        for stock in stocks do            this.Clients.Caller.SetStock stock    member x.OpenMarket() =          let connId = x.Context.ConnectionId        stockMarket.OpenMarket(connId)        this.Clients.All.SetMarketState(MarketState.Open.ToString())    member x.CloseMarket() =          let connId = x.Context.ConnectionId        stockMarket.CloseMarket(connId)        this.Clients.All.SetMarketState(MarketState.Closed.ToString())    member x.GetMarketState() =          let connId = x.Context.ConnectionId        stockMarket.GetMarketState(connId).ToString()\nThe StockTickerHub  class derives from the SignalR Hub class, which is designed to \nhandle the connections, bidirectional interaction, and calls from clients. A SignalR \nHub class instance is created for each operation on the hub, such as connections and \ncalls from the client to the server. If you instead put state in the SignalR Hub class, then \nyou’d lose it because the hub instances are transient. This is the reason you’re using the \nTradingAgent s to manage the mechanism that keeps stock data, updates prices, \nand broadcasts price updates. Uses the SignalR attribute to define the hub name that is referenced from the client to be accessed\nThe StockTickerHub implements the strongly \ntyped Hub< IStockTickerHubClient> class to \nenable SignalR communications.\nUses a single instance of the StockMarket and TradingCoordinator types, which is agent-based and can be used as a singleton instance in a thread-safe manner\nUses SignalR base events to manage new and dropped connections For each connection event raised, an agent is either subscribed or unsubscribed accordingly. \nMethods that manage  \nthe stock market events \n \n 479 Let’s code the stock market trading application\nThe Singleton pattern is a common option to keep alive an instance object inside a \nSignalR hub. In this case, you’re creating a singleton instance of the StockMarket  type; \nand because its implementation is agent-based, there are no thread-race issues and per -\nformance penalties, as explained in section 3.1. \nThe SignalR base methods OnConnected  and OnDisconnected  are raised each time \na new connection is established or dropped, and a TradingAgent  instance is either cre-\nated and registered or unregistered and destroyed accordingly.\nThe other methods handle the stock market operations, such as opening and closing \nthe market. For each of those operations, the underlying SignalR channel notifies the active clients immediately, as shown in the following listing.\nListing 14.11  Client StockTicker  interface to receive notifications using SignalR\n    interface IStockTickerHub    {        Task Init(string serverUrl, IStockTickerHubClient client);        string ConnectionId { get; }        Task GetAllStocks();        Task<string> GetMarketState();        Task OpenMarket();        Task CloseMarket();    }\nThe IStockTickerHub  interface is used in the client side to define the methods in the \nSignalR Hub class that clients can call. To expose a method on the hub that you want to \nbe callable from the client, declare a public method. Note that the methods defined in the interface can be long running, so they return a \nTask  (or Task<T> ) type designed to \nrun asynchronously to avoid blocking the connection when the WebSocket  transport is \nused. When a method returns a Task  object, SignalR waits for the task to complete, and \nthen it sends the unwrapped result back to the client.\nYou’re using a Portable Class Library (PCL) to share the same functionality among \ndifferent platforms. The purpose of the IStockTickerHub  interface is to establish an \nad hoc platform-specific contract for the SignalR hub implementation. In this way, each platform has to satisfy a precise definition of this interface, injected at runtime using the \nDependencyService  class provider (http:/ /mng.bz/vFc3):\nIStockTickerHub stockTickerHub = DependencyService.Get<IStockTickerHub>();\nAfter having defined the IStockTickerHub  contract to establish the way that the client \nand server communicate, listing 14.12 shows the implementation of the mobile appli-cation, in particular of the \nViewModel  class, which represents the core functionality. \nSeveral of the properties have been removed from the original source code, because repetitive logic could distract from the main objective of the example. \n \n480 chapter  14 Building a scalable mobile app with concurrent functional programming\nListing 14.12  Client-side mobile application using Xamarine.Forms\npublic class MainPageViewModel : ModelObject, IStockTickerHubClient{    public MainPageViewModel(Page page)    {        Stocks = new ObservableCollection<StockModelObject>();        Portfolio = new ObservableCollection<Models.OrderRecord>();        BuyOrders = new ObservableCollection<Models.OrderRecord>();                  SellOrders = new ObservableCollection<Models.OrderRecord>();         SendBuyRequestCommand =                  new Command(async () => await SendBuyRequest());        SendSellRequestCommand =                  new Command(async () => await SendSellRequest());   \n        stockTickerHub = DependencyService.Get<IStockTickerHub>();         hostPage = page;        var hostBase = ""http://localhost:8735/"";        stockTickerHub                         .Init(hostBase, this)            .ContinueWith(async x =>            {                var state = await stockTickerHub.GetMarketState();                isMarketOpen = state == ""Open"";                OnPropertyChanged(nameof(IsMarketOpen));                OnPropertyChanged(nameof(MarketStatusMessage));                await stockTickerHub.GetAllStocks();            }, TaskScheduler.FromCurrentSynchronizationContext());          client = new HttpClient();        client.BaseAddress = new Uri(hostBase);        client.DefaultRequestHeaders.Accept.Clear();        client.DefaultRequestHeaders.Accept.Add(            new MediaTypeWithQualityHeaderValue(""application/json""));     }    private IStockTickerHub stockTickerHub;      private HttpClient client;    private Page hostPage;    public Command SendBuyRequestCommand { get; }    public Command SendSellRequestCommand { get; }     private double price;    public double Price      {        get => price; set        {            if (price == value)Uses observable collections to notify the \nauto-updates for the ViewModel properties\nUses asynchronous commands to send the trading orders to buy or sell Initializes the stockTickerHub to establish a connection \nto the server. During the initialization and client-server \nconnection, the UI is updated accordingly.\nThe stockTickerHub initialization is performed in the UI synchronization context to freely update the UI controls. \nInitializes the HttpClient \nused to send requests \nto the Web Server API\nProperty in the ViewModel used for data binding with the UI. Only one property  is shown for demonstration purposes; other properties follow the same structure. \n \n 481 Let’s code the stock market trading application\n                return;            price = value;            OnPropertyChanged();        }    }    private async Task SendTradingRequest(string url)      {        if (await Validate()) {        var request = new \n➥ TradingRequest(stockTickerHub.ConnectionId, Symbol, Price, Amount);\n        var response = await client.PostAsJsonAsync(url, request);        response.EnsureSuccessStatusCode();                }    }    private async Task SendBuyRequest() =>             await SendTradingRequest(""/api/trading/buy"");       private async Task SendSellRequest() =>           await SendTradingRequest(""/api/trading/sell"");      public ObservableCollection<Models.OrderRecord> Portfolio { get; }    public ObservableCollection<Models.OrderRecord> BuyOrders { get; }    public ObservableCollection<Models.OrderRecord> SellOrders { get; }    public ObservableCollection<StockModelObject> Stocks { get; } \n    public void UpdateOrderBuy(Models.OrderRecord value) =>                                               BuyOrders.Add(value);      public void UpdateOrderSell(Models.OrderRecord value) =>                                               SellOrders.Add(value);  }\nThe class MainPageViewModel  is the ViewModel  component of the mobile client appli-\ncation, which is based on the MVVM pattern (http://mng.bz/qfbR) to enable commu-nication and data binding between the UI (View) and the ViewModel. In this way, the UI and the presentation logic have separate responsibilities, providing a clear separa-tion of concerns in the application.\nNote that the class \nMainPageViewModel  implements the interface IStockTickerHub -\nClient , which permits the notifications from the SignalR channel after the connection \nis established. The interface IStockTickerHubClient  is defined in the StockTicker.\nCore project, and it represents the contract for the client that the server relies on. This code snippet shows the implementation of this interface:\ntype IStockTickerHubClient =    abstract SetMarketState : string -> unit    abstract UpdateStockPrice : Stock -> unit    abstract SetStock : Stock -> unit    abstract UpdateOrderBuy : OrderRecord -> unit    abstract UpdateOrderSell : OrderRecord -> unit    abstract UpdateAsset : Asset -> unit    abstract SetInitialAsset : float -> unitUses a function to send \nrequests to the Web Server API\nUses observable collections to notify the auto-updates for the ViewModel properties Uses functions triggered by the SignalR channel when \nsending notifications from the web server application \nto the client. These functions update the UI.",54106
149-14.6.1 Benchmark to measure the scalability of the  stock ticker application.pdf,149-14.6.1 Benchmark to measure the scalability of the  stock ticker application,"482 chapter  14 Building a scalable mobile app with concurrent functional programming\nThese notifications will flow in the application automatically from the server side into the mobile application, updating the UI control in real time. In listing 14.12, the observable collections defined at the top of the class are used to communicate in a bidi-rectional manner with the UI. When one of these collections is updated, the changes are propagated to the bind UI controllers to reflect the state (http://mng.bz/nvma).\nThe \nCommand  of the ViewModel is used to define a user operation, which is data \nbound to a button, to send the request asynchronously to the web server to per -\nform a trade for the stock defined in the UI.1 The request is executed, launching the \nSendTradingRequest  method that’s used to buy or sell a stock according to the API \nendpoint targeted.\nThe SignalR connection is established through the initialization of the stock-\nTickerHub  interface, and an instance is created by calling the DependencyService.\nGet<IStockTickerHub>  method. After the creation of the stockTickerHub  instance, \nthe application initialization is performed by calling the Init  method, which calls \nthe remote server for locally loading the stocks with the method stockTickerHub.\nGetAllStocks  and the current state of the market with the method stockTickerHub.\nGetMarketState  to update the UI. \nThe application initialization is performed asynchronously using the FromCurrent -\nSynchronizationContext  TaskScheduler , which provides functionality for propagat-\ning updates to the UI controllers from the main UI thread without the need to apply any thread-marshal operation.\nUltimately, the application receives the notifications from the SignalR channel, \nwhich is connected to the stock market server, through the invocation of the methods defined in the \nIStockTickerHubClient  interface. These methods are UpdateOrder -\nBuy, UpdatePortofolio , and UpdateOrderSell , which are responsible updating the UI \ncontrollers by changing the relative observable collections.\n14.6.1 Benchmark to measure the scalability of the  stock ticker application\nThe stock ticker application was deployed on Microsoft Azure Cloud with a medium configuration (two cores and 3.5 GB of RAM), and stress-tested using an online tool to simulate 5,000 concurrent connections, each generating hundreds of HTTP requests. This test aimed to verify the web server performance under excessive loads to ensure that critical information and services are available at speeds that end users expect. The result was green, validating that web server application can sustain many concurrent active users and cope with excessive loads of HTTP requests.\n1 “Stephen Cleary, Async Programming: Patterns for Asynchronous MVVM Applications: Data Bind-ing,” https://msdn.microsoft.com/magazine/dn605875. \n \n 483 Summary\nSummary\n¡ Conventional web applications can be thought of as embarrassingly parallel because requests are entirely isolated and easy to execute independently. The more powerful the server running the application, the more requests it can handle. \n¡ You can effortlessly parallelize and distribute a stateless program among com-puters and processes to scale out performance. There’s no need to maintain any state where the computation runs, because no part of the program will modify any data structures, avoiding data races.\n¡ Asynchronicity, caching, and distribution (ACD) are the secret ingredients when designing and implementing a system capable of flexing to an increase (or decrease) of requests with a commensurate parallel speedup with the addition of resources.\n¡ You can use Rx to decouple an ASP.NET Web API and push the messages orig-inated by the incoming requests from the controller to other components of the subscriber application. These components could be implemented using the agent programming model, which spawns a new agent for each established and active user connection. In this way, the application can be maintained in an iso-lated state per user, and provide an easy opportunity for scalability. \n¡ The support provided for concurrent FP in .NET is key to making it a great tool for server-side programming. Support exists for running operations asynchro-nously in a declarative and compositional semantic style; additionally, agents can be used to develop thread-safe components. These core technologies can be com-bined for declarative processing of events and for efficient parallelism with TPL.\n¡ Event-driven architecture (EDA) is an application design style that builds on the fundamental aspects of event notifications to facilitate immediate information dissemination and reactive business process execution. In an EDA, information is propagated in real time throughout a highly distributed environment, enabling the different components of the application that receive a notification to proac-tively respond to business activities. EDA promotes low latency and a highly reac-tive system. The difference between event-driven and message-driven systems is that event-driven systems focus on addressable event sources; message-driven sys-tems concentrate on addressable recipients.",5233
150-appendix a Functionalprogramming.pdf,150-appendix a Functionalprogramming,"484appendix A\nFunctional programming\nIt’s anecdotal to say that learning FP makes you a better programmer. It’s true that \nFP provides an alternative, often simpler, way of thinking about problems. Moreover, many techniques from FP can be successfully applied to other languages. No matter what language you work in, programming in a functional style provides benefits. \nFP is more a mindset than a particular set of tools or languages. Getting famil-\niar with different programming paradigms is what makes you a better programmer, and a multiparadigm programmer is more powerful than a polyglot programmer.  Therefore . . . \nWith the technical background having been sorted out in the chapters of this \nbook, this appendix doesn’t cover the aspects of FP applied to concurrency, such as immutability, referential transparency, side-effect-free functions, and lazy evalu-ations. Rather, it covers general information about what FP means and the reasons why you should care about it. \nWhat is functional programming? \nFP means different things to different people. It’s a program paradigm that treats a computation as an evaluation of an expression. A paradigm in science describes dis-tinct concepts or thought patterns. \nFP involves using state and mutable data to solve domain problems, and it’s based \non lambda calculus. Consequently, functions are first-class values. \n \n 485 What is functional programming? \nFirst-class values\nA first-class value in a programming language is an entity that supports all the operations \navailable to other entities. These operations typically include being passed as a parame -\nter, returned from a function, or assigned to a variable.\n \nFP is a programming style that reasons in terms of evaluation of expressions versus the execution of a statement. The term expression comes from mathematics; an expression is always returning a result (value) without mutating the program state. A statement doesn’t return anything and can change the program state: \n¡ Execution of statements refers to a program expressed as a sequence of commands or statements. Commands specify how to achieve an end result by creating objects and manipulating them.\n¡ Evaluation of expressions refers to how a program specifies object properties that you want to get as result. You don’t specify the steps necessary to construct the object, and you can’t accidentally use the object before it’s created.\nThe benefits of functional programming \nHere’s a list of the benefits of FP:\n¡ Composability and modularity—With the introduction of pure functions, you can compose functions and create higher-level abstractions from simple functions. Using modules, the program can be organized in a better way. Composability is the most powerful tool to defeat complexity; it lets you define and build solutions for complex problems.\n¡ Expressiveness—You can express complex ideas in a succinct and declarative for -\nmat, improving the clarity of the intention and ability to reason about your pro-gram and reducing code complexity.\n¡ Reliability and testing—Functions exist without side effects; a function only evalu-ates and returns a value that depends on its arguments. Therefore, you can exam-ine a function by focusing solely on its arguments, which allows for better testing to easily validate the correctness of your code.\n¡ Easier concurrency—Concurrency encourages referential transparency and immu-tability, which are the primary keys for writing correct, lock-free concurrent applications to run effectively on multiple cores. \n¡ Lazy evaluation—You can retrieve the result of the function on demand. Suppose you have a big data stream to analyze. Thanks to LINQ, you can use deferred exe-cution and lazy evaluation to process your data analysis on demand (only when needed). \n¡ Productivity—This is an enormous benefit: you can write fewer lines of code while achieving the same implementation as other paradigms. Productivity reduces the time it takes to develop programs, which can translate to a larger profit margin. \n \n486 appendix  a Functional programming\n¡ Correctness—You can write less code, naturally reducing the possible number of bugs. \n¡ Maintainability—This benefit results from the other benefits, such as the code being composable, modular, expressive, and correct.\nLearning to program functionally leads to more modular, expression-oriented, con-ceptually simple code. The combinations of these FP assets let you understand what your code is doing, regardless of how many threads it’s executing.\nThe tenets of functional programming\nThere are four main tenets to FP that lead to a composable and declarative program-ming style:\n¡ Higher-order functions (HOFs) as first-class values\n¡ Immutability\n¡ Pure functions, also known as side-effect-free functions \n¡ Declarative programming style\nThe clash of program paradigms: from imperative to object-oriented to functional programming\nObject-oriented programming makes code understandable by encapsulating moving parts. Functional programming makes code understandable by minimizing moving parts.\n—Michael Feathers, author of Working with Legacy Code, via Twitter\nThis section describes three programming paradigms: \n¡ Imperative programming describes computations in terms of statements that change the program’s state and define the sequence of commands to perform. Therefore, an imperative paradigm is a style that computes a series of statements to mutate a state. \n¡ Functional programming builds the structures and elements of a program by treating computations as the evaluation of expressions; therefore, FP promotes immutability and avoids state. \n¡ Object-oriented programming (OOP) organizes objects rather than actions, and its data structures contain data rather than logic. The main programming paradigms can be distinguished between imperative and functional. OOP is orthogonal to imperative and functional programming, in the sense that it can be combined with both. You don’t have to prefer one paradigm over another, but you can write software with an OOP style using functional or imperative concepts.\nOOP has been around for almost two decades, and its design principles were used by languages such as Java, C#, and VB.Net. OOP has had great success because of its ability to represent and model the user domain, raising the level of abstraction. The primary idea behind the introduction of OOP languages was code reusability, but this \n \n 487 The clash of program paradigms: from imperative to object-oriented to functional programming\nidea is often corrupted by modifications and customizations required for specific sce-narios and ad hoc objects. OOP programs developed with low coupling and good code reusability felt like a complex maze, with many secret and convoluted passages reduc-ing code readability.\nTo mitigate this hard-to-achieve code reusability, developers started to create design \npatterns to address OOP’s cumbersome nature. Design patterns encouraged develop-ers to tailor software around the patterns, making the code base more complex, diffi-cult to understand, and, in certain cases, maintainable but still far from reusable. In OOP, design patterns are useful when defining solutions to recurring design problems, but they can be considered a defect of abstraction in the language itself. \nIn FP, design patterns have a different meaning; in fact, most of the OOP-specific \ndesign patterns are unnecessary in functional languages because of the higher level of abstraction and HOFs used as building blocks. The higher level of abstraction and reduced workload around the low-level details in FP style has the advantage of produc-ing shorter programs. When the program is small, it’s easier to understand, improve, and verify. FP has fantastic support for code reuse and for reducing repetitive code, which is the most effective way to write code that’s less prone to error.\nHigher-order functions for increasing abstraction\nThe principle of an HOF means that functions can be passed as arguments to other functions, and functions can return different functions within their return values. .NET has the concept of generic delegates, such as \nAction<T>  and Func<T, TResult> , \nwhich can be used as HOFs to pass functions as parameters with lambda support. Here’s an example of using the generic delegate \nFunc<T,R>  in C#:\nFunc<int, double> fCos = n => Math.Cos( (double)n );double x = fCos(5); IEnumerable<double> values = Enumerable.Range(1, 10).Select(fCos);\nThe equivalent code can be represented in F# with function semantics, without the need to use the \nFunc<T, TResult> delegate explicitly: \nlet fCos = fun n -> Math.Cos( double n )let x = fCos 5let values = [1..10] |> List.map fCos\nHOFs are at the core of harnessing the power of FP. HOFs have the following benefits: \n¡ Composition and modularity\n¡ Code reusability\n¡ Ability to create highly dynamic and adaptable systems\nFunctions in FP are considered first-class values, meaning that functions can be named by variables, can be assigned to variables, and can appear anywhere that any other language constructs can appear. If you’re coming from a straight OOP experience, this concept allows you to use functions in a non-canonical way, such as applying rela-tively generic operations to standard data structures. HOFs let you focus on results, not \n \n488 appendix  a Functional programming\nsteps. This is a fundamental and powerful shift in approaching functional languages. Different functional techniques allow you to achieve functional composition: \n¡ Composition\n¡ Currying \n¡ Partially applied functions or partial applications\nThe power of using delegates leads to express functionality that targets not only meth-ods that do one thing, but also behavioral engines that you can enhance, reuse, and extend. This kind of programming style, which is at the root of the functional par -\nadigm, has the benefit of reducing the amount of code refactoring: instead of hav-ing several specialized and rigid methods, the program can be expressed by fewer but much more general and reusable methods that can be amplified to handle multiple and different scenarios. \nHOFs and lambda expressions for code reusability\nOne of the many useful reasons for using lambda expressions is to refactor the code, \nreducing redundancy. It’s good practice in memory-managed languages such as C# to dispose of resources deterministically when possible. Consider the following example:\nstring text;using (var stream = new StreamReader(path)) {    text = stream.ReadToEnd();}\nIn this code, the StreamReader  resource is disposed with the using  keyword. This is a \nwell-known pattern, but limitations do exist. The pattern isn’t reusable because the dis-posable variable is declared inside the using scope, making it impossible to reuse after it’s disposed, and it generates exceptions if it calls the disposed objects. Refactoring the code in a classic OOP style is no trivial task. It’s possible to use a template method pattern, but this solution also introduces more complexity with the need for a new base class and implementation for each derived class. A better and more elegant solution is to use a lambda expression (anonymous delegate). Here’s the code to implement the static helper method and its use:\nR Using<T,R>(this T item, Func<T, R> func) where T : IDisposable {\n    using (item)           return func(item);}string text = new StreamReader(path).Using(stream => stream.ReadToEnd());\nThis code implements a flexible and reusable pattern for cleaning up disposable resources. Here the only constraint is that the generic type \nT must be a type that imple-\nments IDisposable . \n \n 489 The clash of program paradigms: from imperative to object-oriented to functional programming\nLambda expressions and anonymous functions \nThe term lambda or lambda expression most often refers to anonymous functions. The intention behind a lambda expression is to express computations based on a function, using variable binding and substitution. In simpler terms, a lambda expression is an unnamed method written in place of a delegate instance that introduces the notion of anonymous functions. \nLambda expressions raise the level of abstraction to simplify the programming expe-\nrience. Functional languages such as F# are based on lambda calculus, which is used to express computations on function abstractions; therefore, a lambda expression is part of the FP language. In C#, however, the main motivation for introducing lambdas is to facilitate streaming abstractions that enable stream-based declarative APIs. Such abstraction presents an accessible and natural path to multicore parallelism, making lambda expressions a valuable tool in the domain of current computing.\nLambda calculus vs. lambda expressions\nLambda calculus  (also known as λ-calculus) is a formal system in mathematical logic and \ncomputer science for expressing computations using variable binding and substitution using functions as its only data structure. Lambda calculus behaves as a small program -\nming language that expresses and evaluates any computable function. For example, .NET LINQ is based on lambda calculus.A lambda expression defines a special anonymous method. Anonymous methods are \ndelegate instances with no actual method declaration name. The terms lambda and \nlambda expression most often refer to anonymous functions. A lambda method is syntactic sugar and a more compact syntax for embedding an anon -\nymous method in code:\nFunc<int, int> f1 = delegate(int i) { return i + 1; }; Func<int, int> f2 = i => i+1;     \n \nTo create a lambda expression, you specify input parameters (if any) on the left side of the lambda operator \n=> (pronounced “goes to”), and you put the expression or state-\nment block on the right side. For example, the lambda expression (x, y) => x + y  \nspecifies two parameters x and y and returns the sum of these values. \nEach lambda expression has three parts:\n¡ (x, y)  —A set of parameters.\n¡ => —The goes to operator ( =>) that separates an argument list from the result \nexpression.\n¡ x + y  —A set of statements that perform an action or return a value. In this exam-\nple, the lambda expression returns the sum of x and y.\nHere’s how you implement three lambda expressions with the same behavior:\nFunc<int, int, int> add = delegate(int x, int x){ return x + y; };Func<int, int, int> add = (int x, int y) => { return x + y; };Func<int, int, int> add = (x, y) => x + yAnonymous method\nLambda expression\n \n490 appendix  a Functional programming\nThe part Func<int, int, int>  defines a function that takes two integers and returns \na new integer. \nIn F#, the strong type system can bind a name or label to a function without an \nexplicit declaration. F# functions are primitive values, similar to integers and strings. It’s possible to translate the previous function into the equivalent F# syntax as follows:\nlet add = (fun x y -> x + y)let add = (+) \nIn F#, the plus (+) operator is a function that has the same signature as add, which takes \ntwo numbers and returns the sum as a result. \nLambda expressions are a simple and effective solution to assign and execute a block \nof inline code, especially in an instance when the block of code serves one specific pur -\npose and you don’t need to define it as a method. There are numerous advantages for introducing lambda expressions into your code. Here is a short list:\n¡ You don’t need explicit parameterization of types; the compiler can figure out the parameter types. \n¡ Succinct inline coding (the functions exist within the line) avoids disruptions caused when developers must look elsewhere in the code to find functionality. \n¡ Captured variables limit the exposure of class-level variables.\n¡ Lambda expressions make the code flow readable and understandable. \nCurrying\nThe term currying originates from Haskell Curry, a mathematician who was an import-ant influence on the development of FP. Currying is a technique that lets you mod-ularize functions and reuse code. The basic idea is to transform the evaluation of a function that takes multiple parameters into the evaluation of a sequence of functions, each with a single parameter. Functional languages are closely related to mathemat-ical concepts, where functions can have only one parameter. F# follows this concept because functions with multiple parameters are declared as a series of new functions, each with only one parameter. \nIn practice, the other .NET languages have functions with more than one argu-\nment; and from the OOP perspective, if you don’t pass into a function all the argu-ments expected, the compiler throws an exception. Conversely, in FP it’s extremely easy to write a curried function that returns any function you give it. But as previously mentioned, lambda expressions provide a great syntax for creating anonymous dele-gates, thereby making it easy to implement a curried function. Moreover, it’s possible to implement currying in any programming language that supports closure—an inter -\nesting concept because this technique simplifies lambda expressions, including only single-parameter functions. \nThe currying technique makes it possible to treat all functions with one or any num-\nber of arguments as if they take only one argument, independent of the number of \n \n 491 The clash of program paradigms: from imperative to object-oriented to functional programming\narguments needed to execute. This creates a chain of functions where each consumes a single parameter. \nAt the end of this chain of functions, all parameters are available at once, which \nallows the original function to execute. Moreover, currying allows you to create special-ized groups of functions generated from fixing the arguments of a base function. For instance, when you curry a function of two arguments and apply it to the first argument, then the functionality is limited by one dimension. This isn’t a limitation but a powerful technique, because then you can apply the new function to the second argument to compute a particular value.\nIn mathematic notation, an important difference exists between these two functions:\nAdd(x, y, z)Add x y z\nThe difference is that the first function takes a single argument of type tuple  (com-\nposed by the three items x, y, and z), and the second function takes the input item \nx and returns a function that takes the input item y, which returns a function that \ntakes item z and then returns the result of the final computation. In simpler words, the \nequivalent function can be rewritten as \n(((Add x) y) z)\nIt’s important to mention that function applications are left associative, taking one argument at a time. The previous function \nAdd is an application against x, and the \nresult is then applied to y. The result of this application ((Add x) y) is then applied \nto z. Because each of these transitional steps yields a function, it’s perfectly acceptable \nto define a function as\nPlus2 = Add 2\nThis function is equivalent to Add x . In this case, you can expect the function Plus2  \nto take two input arguments, and it always passes 2 as a fixed parameter. For clarity, it’s possible to rewrite the previous function as follows:\nPlus2 x = Add 2 x\nThe process of yielding intermediate functions (each taking one input argument) is called currying. Let’s see currying in action. Consider the following simple C# function \nthat uses a lambda expression:\nFunc<int,int,int> add = (x,y) => x + y;         Func<int,Func<int,int>> curriedAdd = x => y => x + y;     \nThis code defines the function Func<int, int, int> add , which takes two integers as \narguments and returns an integer as a result. When this function is called, the compiler requires both arguments \nx and y. But the curried version of the function add, curried­\nAdd, results in a delegate with the special signature Func<int,Fun< int, int>>.  \nIn general, any delegate of type Func<A,B,R>  can be transformed into a delegate of \ntype Func<A, Func<B,R>> . This curried function takes only one argument and returns \na function that takes the original function as an argument and then returns a value of \n \n492 appendix  a Functional programming\ntype A. The curried function curriedAdd  can be used to create powerful specialized \nfunctions. For example, you can define an increment  function by adding the value 1:\nFunc<int,int> increment = curriedAdd(1)\nNow, you can use this function to define other functions that perform several forms of addition:\nint a = curriedAdd(30)int b = increment(41)Func<int, int> add30 = curriedAdd(30)int c = add30(12)\nOne benefit of currying a function is that the creation of specialized functions is easier to reuse; but the real power is that curried functions introduce a useful concept called partially applied functions, which is covered in the next section. Additional benefits of the currying technique are function parameter reduction and easy-to-reuse abstract functions. \nautomatic  currying  in c#\nIt’s possible to automate and raise the level of abstraction of the currying technique in C# with the help of extension methods. In this example, the purpose of the curry extension method is to introduce syntactic sugar to hide the currying implementation:\nstatic Func<A, Func<B, R>> Curry<A, B, R>(this Func<A, B, R> function){    return a => b => function(a, b);}\nThis is the previous code refactored using the helper extension method:\nFunc<int,int,int> add = (x,y) => x + y;Func<int,Func<int,int>> curriedAdd = add.Curry();\nThis syntax looks more succinct. It’s important to notice that the compiler can infer the types used in all the functions and, for this, it’s most helpful. In fact, even though \nCurry  is a generic function, it’s not required to pass generic parameters explicitly. \nUsing this currying technique lets you use a different syntax that’s more conducive to building a library of complex composite functions from simple functions. The source code, which you can download as part of the resources for this book, has a library that contains a full implementation of helper methods, including an extension method for automatic currying.\nun-currying\nAs easily as applying the curry technique to a function, you can un-curry a function by using higher-order functions to revert the curried function. Un-currying is, obviously, \nthe opposite transformation to currying. Think of un-currying as a technique to undo \ncurrying by applying a generic un-curry function.\n \n 493 The clash of program paradigms: from imperative to object-oriented to functional programming\nIn the following example, the curried function with signature Func<A, Func<B, R>>  \nwill be converted back to a multi-argument function: \npublic static Func<A, B, R> Uncurry<A, B, R>(Func<A, Func<B, R>> function)                                       => (x, y) => function(x)(y); \nThe primary purpose of un-currying a function is to bring the signature of a curried function back into a more OOP style. \ncurrying  in f#\nIn F#, function declarations are curried by default. But even though this is done auto-matically by the compiler for you, it’s helpful to understand how F# handles curried functions. \nThe following example shows two F# functions that multiply two values. If you’re not \nfamiliar with F#, these functions may seem equivalent or at least similar, but they aren’t:\nlet multiplyOne (x,y) = x * ylet multiplyTwo x y = x * ylet resultOne = multiplyOne(7, 8)let resultTwo = multiplyTwo 7 8let values = (7,8)let resultThree = multiplyOne values\nBesides the syntax, no apparent difference exists between these functions, but they behave differently. The first function has only one parameter, which is a tuple with the required values, but the second function has two distinct parameters \nx and y.\nThe difference becomes clear when you look into the signatures of these functions’ \ndeclarations:\nval multiplyOne : (int * int) -> intval multiplyTwo : int -> int -> int\nNow it’s obvious that these functions are different. The first function takes a tuple as an input argument and returns an integer. The second function takes an integer as its first input and returns a function that takes an integer as input and then returns an integer. This second function, which takes two arguments, is transformed automatically by the compiler into a chain of functions, each with one input argument.\nThis example shows the equivalent curried functions, which is how the compiler \ninterprets it for you: \nlet multiplyOne x y = x * ylet multiplyTwo = fn x -> fun y -> x * ylet resultOne = multiplyOne 7 8let resultTwo = multiplyTwo 7 8let resultThree =     let tempMultiplyBy7 = multiplyOne 7    tempMultiplyBy7 8\n \n494 appendix  a Functional programming\nIn F#, the implementation of these functions is equivalent because, as previously men-tioned, they’re curried by default. The main purpose of currying is to optimize a func-tion for easily applying partial application. \nPartially applied functions\nThe partially applied function (or partial function application) is a technique of fixing \nmultiple arguments to a function and producing another function of smaller arity (the arity of a function is the number of its arguments). In this way, a partial function pro-vides a function with fewer arguments than expected, which produces a specialized function for the given values. Partially applied functions, in addition to function com-position, make functional modularization possible.\nMore simply, a partial function application is a process of binding values to parame-\nters, which means that partially applied functions are functions that reduce the number of function arguments by using fixed (default) values. If you have a function with \nN \narguments, it’s possible to create a function with N-1 arguments that calls the original \nfunction with a fixed argument. Because partial application depends on currying, the two techniques occur together. The difference between partial application and curry-ing is that partial application binds more than one parameter to a value, so to evaluate the rest of the function you need to apply the remaining arguments.\nIn general, partial application transforms a generic function into a new and special-\nized function. Let’s take the C# curried function: \nFunc<int,int,int> add = (x,y) => x + y;\nHow can you create a new function with a single argument? \nThis is the case where partial function application becomes useful, because you can \npartially apply a function against an HOF with a default value for the first argument to the original function. Here’s the extension method that can be used to partially apply a function:\nstatic Func<B, R> Partial<A, B, R>(this Func<A, B, R> function, A argument)                              => argument2 => function(argument, argument2);\nAnd here’s an example to exercise this technique: \nFunc<int, int, int> max = Math.Max;Func<int, int> max5 = max.Partial(5);int a = max5(8);int b = max5(2);int c = max5(12);\nMath.Max(int,int) is an example of a function that can be extended with partially \napplied functions. Introducing a partially applied function in this case, the default argument \n5 is fixed, and it creates a new specialized function max5  that evaluates the \nmaximum value between two numbers with a default of 5. Thanks to partial applica-\ntion, you created a new and more specific function out of an existing one. \nFrom an OOP perspective, think of partial function applications as a way to override \nfunctions. It’s also possible to use this technique to extend on-the-fly functionality of a third-party library that isn’t extensible.  \n \n 495 The clash of program paradigms: from imperative to object-oriented to functional programming\nAs mentioned, in F# functions are curried by default, which leads to an easier way to \ncreate partial functions than in C#. Partial function applications have many benefits, including the following: \n¡ They allow functions to be composed without hesitation.\n¡ They alleviate the need to pass a separate set of parameters, by avoiding building unnecessary classes that contain override versions of same method with a differ -\nent number of inputs. \n¡ They enable the developer to write highly general functions by parameterizing their behavior.\nThe practical benefit of using partial function applications is that functions constructed by supplying only a portion of the argument are good for code reusability, functional extensibility, and composition. Moreover, partially applied functions simplify the use of HOFs in your programming style. Partial function application can also be deferred for performance improvement, which was introduced in section 2.6.\nPower of partial function application and currying in C#\nLet’s consider a more complete example of partial function application and currying that can cover a real-use scenario. \nRetry  in listing A.1 is an extension method for a del-\negate Func<T>  for any function that takes no parameters and returns a value of type T. \nThe purpose of this method is to execute the incoming function in a try-catch block, \nand if an exception is thrown while executing, the function will retry the operation up to a maximum of three times. \nListing A.1  Retry  extension method in C# \npublic static T Retry<T>(this Func<T> function)    {    int retry = 0;                T result = default(T);            bool success = false;     do{            try {                    result = function();                        success = true;                }            catch {                    retry++;                    }    } while (!success && retry < 3);         return result;}\nLet’s say that this method tries to read text from a file. In the following code, the method \nReadText  accepts a file path as input and returns the text from a file. To exe-\ncute the functionality with the attached Retry  behavior to fall back on and recover in \ncase of issues, you can use a closure, as shown here:Applies a static method to a general Func<T> delegate\nSets the counter \nSets the result to a default value of T\nExecutes the function. If successful, then the while loop stops and the result returns; otherwise, a new iteration computes.Increases count if an error occurs\nIterates three times or until success\n \n496 Appendix  A Functional programming\nstatic string ReadText(string filePath) => File.ReadAllText(filePath);string filePath = ""TextFile.txt"";Func<string> readText = () => ReadText(filePath);string text = readText.Retry();\nYou can use a lambda expression to capture the local variable filePath  and pass it into \nthe method ReadText . This process lets you create a Func<string>  that matches the \nsignature of the Retry  extension method, which can be attached. If the file is blocked \nor owned by another process, an error is thrown, and the Retry  functionality kicks in \nas expected. If the first call fails, the method will retry a second time and a third time. Finally, it returns the default value of \nT.\nThis works, but you might wonder what happens if you want to retry a function that \nneeds a string parameter. The solution is to partially apply the function. The following code implements a function that takes a string as a parameter, which is the file path to read the text from, and then it passes that parameter to the \nReadText  method. The \nRetry  behavior only works with functions that take no parameters, so the code doesn’t \ncompile:\nFunc<string, string> readText = (path) => ReadText(path);string text = readText.Retry();          string text = readText(filePath).Retry();     \nThe behavior of Retry doesn’t work with this version of readText . One possible solu-\ntion is to write another version of the Retry  method that takes an additional generic- \ntype parameter that specifies the type of the parameter you need to pass once invoked. This isn’t ideal, because you have to figure out how to share this new \nRetry logic across \nall the methods using it, each with different arguments or implementations. \nA better option is to use and combine currying and partial function application. In \nthe following listing, the helper methods Curry  and Partial  are defined as extension \nmethods.\nListing A.2   Retry  helper extensions in C#\nstatic class RetryExtensions{    public static Func<R> Partial<T, R>(this Func<T, R> function, T arg){            return () => function(arg);     }        public static Func<T, Func<R>> Curry<T, R>(this Func<T, R> function){        return arg => () => function(arg);    }}Func<string, string> readText = (path) => ReadText(path);string text = readText.Partial(""TextFile.txt"").Retry();\n \n 497 The clash of program paradigms: from imperative to object-oriented to functional programming\nFunc<string, Func<string>> curriedReadText = readText.Curry();string text = curriedReadText(""TextFile.txt"").Retry();\nThis approach lets you inject the file path and use the Retry  function smoothly. This is \npossible because both helper functions, Partial  and Curry , adapt the function read-\nText  into a function that doesn’t need a parameter, ultimately matching the signature \nof Retry .",33370
151-appendix b F overview.pdf,151-appendix b F overview,"498appendix B\nF# overview\nThis appendix explores the basic syntax of F#, which is an established general- \npurpose functional first language with object-oriented programming (OOP) sup-port. In fact, F# embraces the .NET common language infrastructure (CLI) object model, which allows the declaration of interfaces, classes, and abstract classes. Fur -\nthermore, F# is a statically and strongly typed language, which means that the com-piler can detect the data type of variables and functions at compile time. F#’s syntax is different from C-style languages, such as C#, because curly braces aren’t used to delimit blocks of code. Moreover, whitespace rather than commas and indentation is important to separate arguments and delimit the scope of a function body. In addition, F# is a cross-platform programming language that can run inside and out-side the .NET ecosystem. \nThe let binding \nIn F#, let is one of the most important keywords that binds an identifier to a value, \nwhich means giving a name to value (or, bind a value to a name). It’s defined as let \n<identifier> = <value>.  \nThe let bindings are immutable by default. Here are a few code examples:\nlet myInt = 42let myFloat = 3.14let myString = ""hello functional programming"" let myFunction = fun number -> number * number\nAs you can see from the last line, you can name a function by binding the identifier \nmyFunction  to the lambda expression fun number -> number * number . \n \n 499 Creating mutable types: mutable and ref\nThe fun keyword is used to define a lambda expression (anonymous function) in \nthe syntax as fun args -> body . Interestingly, you don’t need to define types in code \nbecause, due to its strong built-in type-inference system, the F# compiler can understand them natively. For example, in the previous code, the compiler inferred that the argu-ment of the \nmyFunction function is a number due to the multiplication ( *) operator.\nUnderstanding function signatures in F#\nIn F#, as in most of the functional languages, function signatures are defined with an arrow notation that reads from left to right. Functions are expressions that always have an output, so the last right arrow will always point to the return type. For exam-ple, when you see \ntypeA -> typeB , you can interpret it as a function that takes an \ninput value of typeA and produces a value of typeB . The same principle is applicable \nto functions that take more than two arguments. When the signature of a function is \ntypeA -> typeB -> typeC , you read the arrows from left to right, which creates two \nfunctions. The first function is typeA -> (typeB -> typeC) , which takes an input of \ntypeA and produces the function typeB -> typeC . \nHere’s the signature for the add function: \nval add : x:int -> y:int -> int     \nThis takes one argument x:int  and returns as a result a function that takes y:int  as \ninput and returns an int as a result. The arrow notation is intrinsically connected to \ncurrying and anonymous functions.\nCreating mutable types: mutable and ref\nOne of the main concepts in FP is immutability. F# is a functional-first programming language; but explicitly using the \nimmutable  keywords lets you create mutable types \nthat behave like variables, as in this example: \nlet mutable myNumber = 42 \nNow it’s possible to change the value of myNumber  with the goes-to ( <-) operator:\nmyNumber  <-  51\nAnother option when defining a mutable type is to use a reference cell that defines a storage location that lets you create mutable values with reference semantics. The \nref \noperator declares a new reference cell that encapsulates a value, which can then be changed using the \n:= operator and accessed using the ! (bang) operator:\nlet myRefVar = ref 42myRefVar := 53printfn ""%d"" !myRefVar\nThe first line declares the reference cell myRefVar  with the value 42, and the second \nline changes its value to 53. In the last line of code, the underlying value is accessed and printed. \nMutable variables and reference cells can be used in almost the same situations; but \nthe mutable types are preferred, unless the compiler doesn’t allow it and a reference cell can be used instead. In expressions that generate a closure where a mutable state is \n \n500 appendix  b F# overview\nrequired, for example, the compiler will report that a mutable variable cannot be used. In this case, a reference cell overcomes the problem.\nFunctions as first-class types\nIn F#, functions are first-order data types; they can be declared using the let keyword \nand used in exactly the same way as any other variable:\nlet square x = x * x let plusOne x = x + 1let isEven x = x % 2 = 0\nFunctions always return a value, despite not having an explicit return  keyword. The \nvalue of the last statement executed in the function is the return value.\nComposition: pipe and composition operators\nThe pipe (|>) and the composition ( >>) operators are used to chain functions and \narguments to improve code readability. These operators let you establish pipelines of \nfunctions in a flexible manner. The definition of these operators is simple:\nlet inline (|>) x f = f xlet inline (>>) f g x = g(f x)\nThe following example shows how to take advantage of these operators to build a func-tional pipeline:\nlet squarePlusOne x =  x |> square |> plusOnelet plusOneIsEven = plusOne >> isEven\nIn the last line of code, the composition ( >>) operator lets you eliminate the explicit \nneed for an input parameter definition. The F# compiler understands that the func-tion \nplusOneIsEven  is expecting an integer as input. The kind of function that doesn’t \nneed parameter definitions is called a point-free function.\nThe main differences between the pipe ( |>) and composition ( >>) operators are \ntheir signature and use. The pipeline operator takes functions and arguments, while composition combines functions.\nDelegates\nIn .NET, a delegate is a pointer to a function; it’s a variable that holds the reference to a method that shares the same common signature. In F#, function values are used in place of delegates; but F# provides support for delegates to interop with the .NET APIs. This is the syntax in F# to define a delegate: \ntype delegate-typename = delegate of typeA -> typeB\nThe following code shows the syntax for creating a delegate with a signature that rep-resents an addition operation: \ntype MyDelegate = delegate of (int * int) -> intlet add (a, b) = a + blet addDelegate = MyDelegate(add)let result = addDelegate.Invoke(33, 9)\n \n 501 Basic data types \nIn the example, the F# function add is passed directly as arguments to the delegate \nconstructor MyDelegate . Delegates can be attached to F# function values and to static \nor instance methods. The Invoke  method on the delegate type addDelegate  calls the \nunderlying function add. \nComments \nThree kinds of comments are used in F#: block comments are placed between the symbols \n(* and *) , line comments start with the symbols // and continue until the end of the line, \nand XML doc comments come after the symbols / / / that let you use XML tags to generate code documentation based on the compiler-generated file. Here’s how these look:\n(* This is block comment *)// Single line comments use a double forward slash/// This comment can be used to generate documentation.\nOpen statements \nYou use the open  keyword to open a namespace or module, similar to statements in C#. \nThis code opens the System  namespace: open System . \nBasic data types \nTable B.1 shows the list of F# primitive types.\nTable B.1 Basic data types\nF# type .NET type Size in bytes Range Example \nsbyte System.SByte 1 -128 to 127 42y\nbyte System.Byte 1 0 to 255 42uy\nint16 System.Int16 2 -32,768 to 32,767 42s\nuint16 System.UInt16 2 0 to 65,535 42us\nint / int32 System.Int32 4 -2,147,483,648 to 2,147,483,64742\nuint32 System.UInt32 4 0 to 4,294,967,295 42u\nint64 System.Int64 8 -9,223,372,036,854,775,808 to 9,223,372,036,854,775,80742L\nuint64 System.UInt64 8 0 to 18,446,744,073,709,551,61542UL\nfloat32 System.Single 4 ±1.5e-45 to ±3.4e38 42.0F\nfloat System.Double 8 ±5.0e-324 to ±1.7e308 42.0\ndecimal System.Decimal16 ±1.0e-28 to ±7.9e28 42.0M\nchar System.Char 2 U+0000 to U+ffff 'x' \nstring System.String 20 + (2 * size of string)0 to about 2 billion characters ""Hello World""\nbool System.Boolean1 Only two possible values:  \ntrue or falsetrue\n \n502 appendix  b F# overview\nSpecial string definition\nIn F#, the string type is an alias for the System.String type; but in addition to the con-\nventional .NET semantic, you have a special triple-quoted way to declare strings. This special string definition lets you declare a string without the need of escaping special characters as in the verbatim case. The following example defines the same string the standard way and the F# triple-quoted way to escape special characters:\nlet verbatimHtml = @""<input type=""submit"" value=""Submit"">""let tripleHTML = """"""<input type=""submit"" value=""Submit"">""""""\nTuple\nA tuple is a group of unnamed and ordered values, which can be of different types. \nTuples are useful for creating ad hoc data structures and are a convenient way for a function to return multiple values. A tuple is defined as a comma-separated collection of values. Here’s how to construct tuples:\nlet tuple = (1, ""Hello"")let tripleTuple = (""one"", ""two"", ""three"") \nA tuple can also be deconstructed. Here the tuple values 1 and ""Hello""  are bound, \nrespectively, to the identifiers a and b, and the function swap  switches the order of two \nvalues in a given tuple (a, b) :\nlet (a, b) = tuplelet swap (a, b) = (b, a)\nTuples are normally objects, but they can also be defined as value type structs, as shown here:\nlet tupleStruct = struct (1, ""Hello"")\nNote that the F# type inference can automatically generalize the function to have a generic type, which means that tuples work with any type. It’s possible to access and obtain the first and second elements of the tuple using the \nfst and snd functions: \nlet one = fst tuplelet hello = snd tuple    \nRecord types\nA record type is similar to a tuple, except the fields are named and defined as a semi-\ncolon-separated list. While tuples provide one method of storing potentially hetero-geneous data in a single container, it can become difficult to interpret the purpose of the elements when more than a few exist. In this case, a record type helps to interpret the purpose of data by labeling their definition with a name. A record type is explicitly defined using the \ntype  keyword, and it’s compiled down to an immutable, public, and \nsealed .NET class. Furthermore, the compiler automatically generates the structural equality and comparison functionality, as well as providing a default constructor that populates all the fields contained in the record.\nNOTE  If the record is marked with the CLIMutable  attribute, it will include a \ndefault, no-argument constructor for use in other .NET languages. \n \n 503 Discriminated unions \nThis example shows how to define and instantiate a new record type:\ntype Person = { FirstName : string; LastName : string; Age : int }let fred = { FirstName = ""Fred""; LastName = ""Flintstone""; Age = 42 }\nRecords can be extended with properties and methods:\ntype Person with    member this.FullName = sprintf ""%s %s"" this.FirstName this.LastName\nRecords are immutable types, which means that instances of records cannot be modi-fied. But you can conveniently clone records by using the \nwith  clone semantic:\nlet olderFred = { fred with Age = fred.Age + 1 }\nA record type can also be represented as a structure using the [<Struct>] attribute. \nThis is helpful in situations where performance is critical and overrides the flexibility of reference types:\n[<Struct>]type Person = { FirstName : string; LastName : string; Age : int }\nDiscriminated unions \nDiscriminated unions (DUs) are a type that represents a set of values that can be one of \nseveral well-defined cases, each possibly with different values and types. DUs can be thought of in the object-oriented paradigm as a set of classes that are inherited from the same base class. In general, DUs are the tool used for building complicated data structures, to model domains, and to represent recursive structures like a \nTree  data \ntype. \nThe following code shows the suit and the rank of a playing card:\ntype Suit = Hearts | Clubs | Diamonds | Spadestype Rank =         | Value of int        | Ace        | King        | Queen        | Jack       static member GetAllRanks() =             [ yield Ace              for i in 2 .. 10 do yield Value i              yield Jack              yield Queen              yield King ]\nAs you can see, DUs can be extended with properties and methods. The list represent-ing all the cards in the deck can be computed as follows:\n     let fullDeck =         [ for suit in [ Hearts; Diamonds; Clubs; Spades] do              for rank in Rank.GetAllRanks() do                   yield { Suit=suit; Rank=rank } ]\nAdditionally, DUs can also be represented as structures with the [<Struct>]  attribute.\n \n504 appendix  b F# overview\nPattern matching\nPattern matching is a language construct that empowers the compiler to interpret the \ndefinition of a data type and apply a series of conditions against it. In this way, the com-piler forces you to write pattern-matching constructs by covering all possible cases to match the given value. This is known as exhaustive pattern matching. Pattern-matching \nconstructs are used for control flow. They’re conceptually similar to a series of \nif/then  \nor case/switch  statements but are much more powerful. They let you decompose data \nstructures into their underlying components during each match and then perform certain computations on these values. In all programming languages, control flow refers to the decisions made in code that affect the order in which statements are executed in \nan application. \nIn general, most common patterns involve algebraic data types, such as discrimi-\nnated unions, record types, and collections. The following code example has two imple-mentations of the Fizz-Buzz (https:/ /en.wikipedia.org/wiki/Fizz_buzz) game. The first pattern-matching construct has a set of conditions to test the evaluation of the function \ndivisibleBy.  If either condition is true or false, the second implementation uses the \nwhen  clause, called guard, to specify and integrate additional tests that must succeed to \nmatch the pattern: \nlet fizzBuzz n =     let divisibleBy m = n % m = 0    match divisibleBy 3,divisibleBy 5 with    | true, false -> ""Fizz""    | false, true -> ""Buzz""    | true, true -> ""FizzBuzz""    | false, false -> sprintf ""%d"" nlet fizzBuzz n =    match n with    | _ when (n % 15) = 0 -> ""FizzBuzz""    | _ when (n % 3) = 0 -> ""Fizz""    | _ when (n % 5) = 0 -> ""Buzz""    | _ -> sprintf ""%d"" n [1..20] |> List.iter fizzBuzz\nWhen a pattern-matching construct is evaluated, the expression is passed into the \nmatch <expression> , which is tested against each pattern until the first positive match. \nThen the corresponding body is evaluated. The _ (underscore) character is known as a \nwildcard, which is one way to always have a positive match. Often, this pattern is used as final clause for a general catch to apply to a common behavior.\nActive patterns \nActive patterns are constructs that extend the capabilities of pattern matching, allowing \nfor partition and deconstruction of a given data structure, thus guaranteeing the flex-ibility to transform and extract underlying values by making the code more readable and making the results of the decomposition available for further pattern matching. \n \n 505 Active patterns \nAdditionally, active patterns let you wrap arbitrary values in a DU data structure for \neasy pattern matching. It’s possible to wrap objects with an active pattern, so that you can use those objects in pattern matching as easily as any other union type. \nSometimes active patterns do not generate a value; in this case, they’re called partial \nactive patterns and result in a type that is an option type. To define a partial active pat-tern, you use the underscore wildcard character (_) at the end of the list of patterns inside the banana clips \n(| |)  created with the combination of parentheses and pipe \ncharacters. Here’s how a typical partial active pattern looks:\nlet (|DivisibleBy|_|) divideBy n =     if n % divideBy = 0 then Some DivisibleBy else None\nIn this partial active pattern, if the value n is divisible by the value divideBy , then the \nreturn type is Some() , which indicates that the active pattern succeeds. Otherwise, the \nNone  return type indicates that the pattern failed and moved to the next match expres-\nsion. Partial active patterns are used to partition and match only part of the input space. \nThe following code illustrates how to pattern match against a partial active pattern:\nlet fizzBuzz n =     match n with     | DivisibleBy 3 & DivisibleBy 5 -> ""FizzBuzz""     | DivisibleBy 3 -> ""Fizz""     | DivisibleBy 5 -> ""Buzz""     | _ -> sprintf ""%d"" n[1..20] |> List.iter fizzBuzz\nThis function uses the partial active pattern (|DivisibleBy|_|)  to test the input value \nn. If it’s divisible by a value 3 and 5, the first case succeeds. If it’s divisible by only 3, then \nthe second cause succeeds, and so forth. Note that the & operator lets you run more \nthan one pattern on the same argument.\nAnother type of active pattern is the parameterized active pattern, which is similar to the \npartial active pattern, but takes one or more additional arguments as input.\nMore interesting is the multicase active pattern, which partitions the entire input space \ninto different data structures in the shape of a DU. Here’s the FizzBuzz  example, imple-\nmented using multicase active patterns:\nlet (|Fizz|Buzz|FizzBuzz|Val|) n =     match n % 3, n % 5 with    | 0, 0 -> FizzBuzz    | 0, _ -> Fizz    | _, 0 -> Buzz    | _ -> Val n\nBecause active patterns convert data from one type to another, they’re great for data transformation and validation. Active patterns come in four related varieties: single case, partial case, multicase, and partial parameterized. For more details about active patterns, see the MSDN documentation (http:/ /mng.bz/Itmw) and Isaac Abraham’s Get Programming with F# (Manning,  2018).\n \n506 appendix  b F# overview\nCollections \nF# supports the standard .NET collections like arrays and sequences ( IEnumerable ). \nIn addition, it offers a set of immutable functional collections: lists, sets, and maps. \nArrays \nArrays are zero-based, mutable collections with a fixed-size number of elements of the same type. They support fast, random access of elements because they are compiled as a contiguous block of memory. Here are the different ways to create, filter, and project an array:\nlet emptyArray= Array.emptylet emptyArray = [| |]  let arrayOfFiveElements = [| 1; 2; 3; 4; 5 |]let arrayFromTwoToTen= [| 2..10 |]let appendTwoArrays = emptyArray |> Array.append arrayFromTwoToTenlet evenNumbers = arrayFromTwoToTen |> Array.filter(fun n -> n % 2 = 0)let squareNumbers = evenNumbers |> Array.map(fun n -> n * n)\nThe elements of an array can be accessed and updated by using the dot operator ( .) \nand brackets [ ]:\nlet arr = Array.init 10 (fun i -> i * i)arr.[1] <- 42arr.[7] <- 91\nArrays can also be created in various other syntaxes, using the functions from the Array  \nmodule: \nlet arrOfBytes = Array.create 42 0uylet arrOfSquare = Array.init 42 (fun i -> i * i)let arrOfIntegers = Array.zeroCreate<int> 42\nSequences (seq) \nSequences are a series of elements of the same type. Different from the List  type, \nsequences are lazily evaluated, which means that elements can be computed on demand (only when they are needed). This provides better performance than a list in cases where not all the elements are needed. Here’s a different way to create, filter, and project a sequence:\nlet emptySeq = Seq.emptylet seqFromTwoToFive = seq { yield 2; yield 3; yield 4; yield 5 }let seqOfFiveElements = seq { 1 .. 5 }let concatenateTwoSeqs = emptySeq |> Seq.append seqOfFiveElementslet oddNumbers = seqFromTwoToFive |> Seq.filter(fun n -> n % 2 <> 0)let doubleNumbers = oddNumbers |> Seq.map(fun n -> n + n)\nSequences can use the yield  keyword to lazily return a value that becomes part of \nthe sequence.\n \n 507 Loops\nLists \nIn F#, the List  collection is an immutable, singly linked list of elements of the same \ntype. In general, lists are a good choice for enumeration, but aren’t recommended for random access and concatenation when performance is critical. Lists are defined using the \n[ ... ]  syntax. Here are a few examples to create, filter, and map a list:\nlet emptyList = List.emptylet emptyList = [ ]  let listOfFiveElements = [ 1; 2; 3; 4; 5 ]let listFromTwoToTen = [ 2..10 ]let appendOneToEmptyList = 1::emptyListlet concatenateTwoLists = listOfFiveElements @ listFromTwoToTenlet evenNumbers = listOfFiveElements |> List.filter(fun n -> n % 2 = 0)let squareNumbers = evenNumbers |> List.map(fun n -> n * n)\nLists use brackets ( [ ]) and the semicolon ( ;) delimiters to append multiple items to \nthe list, the symbol :: to append one item, and the at-sign operator ( @) to concatenate \ntwo given lists.\nSets\nA set is a collection based on binary trees, where the elements are of the same type. \nWith sets, the order of insertion is not preserved, and duplicates aren’t allowed. A set is immutable, and every operation to update its elements creates a new set. Here are a few different ways to create a set:\nlet emptySet = Set.empty<int>let setWithOneItem = emptySet.Add 8let setFromList = [ 1..10 ] |> Set.ofList\nMaps\nA map is an immutable, key-value pair of a collection of elements with the same type. \nThis collection associates values with a key, and it behaves like the Set type, which \ndoesn’t allow duplicates or respect the order of insertion. The following example shows how to instantiate a map in different ways:\nlet emptyMap = Map.empty<int, string>let mapWithOneItem = emptyMap.Add(42, ""the answer to the meaning of life"")let mapFromList = [ (1, ""Hello""), (2, ""World"") ] |> Map.ofSeq\nLoops\nF# supports loop constructs to iterate over enumerable collections like lists, arrays, sequences, maps, and so forth. The \nwhile...do  expression performs iterative execu-\ntion while a specified condition is true:\nlet mutable a = 10while (a < 20) do   printfn ""value of a: %d"" a   a <- a + 1\n \n508 appendix  b F# overview\nThe for...to  expression iterates in a loop over a set of values of a loop variable:\n  for i = 1 to 10 do    printf ""%d "" i\nThe for...in  expression iterates in a loop over each element in a collection of values:\nfor i in [1..10] do   printfn ""%d"" i\nClasses and inheritance\nAs previously mentioned, F# supports OOP constructs like other .NET programming \nlanguages. In fact, it’s possible to define class objects to model real-world domains. The \ntype  keyword used in F# to declare a class can expose properties, methods, and fields. \nThe following code shows the definition of the subclass Student  that’s inherited from \nthe class Person :\ntype Person(firstName, lastName, age) =    member this.FirstName = firstName    member this.LastName = lastName    member this.Age = age     member this.UpdateAge(n:int) =        Person(firstName, lastName, age + n)      override this.ToString() =         sprintf ""%s %s"" firstName lastNametype Student(firstName, lastName, age, grade) =    inherit Person(firstName, lastName, age)    member this.Grade = grade\nThe properties FirstName , LastName , and Age are exposed as fields; the method \nUpdateAge  returns a new Person  object with the modified Age. It’s possible to change \nthe default behavior of methods inherited from the base class using the override  key-\nword. In the example, the ToString  base method is overridden to return the full name. \nThe object Student  is a subclass defined using the inherit  keyword, and inherits its \nmembers from the base class Person , in addition to adding its own member Grade . \nAbstract classes and inheritance\nAn abstract class is an object that provides a template to define classes. Usually it exposes \none or more incomplete implementations of methods or properties and requires you to create subclasses to fill in these implementations. But it’s possible to define a default behavior, which can be overridden. In the following example, the abstract class \nShape  \ndefines the Rectangle  and Circle  classes: \n[<AbstractClass>]type Shape(weight :float, height :float) =    member this.Weight = weight    member this.Height = height\n \n 509 Object expressions\n    abstract member Area : unit -> float    default this.Area() = weight * heighttype Rectangle(weight :float, height :float) =    inherit Shape(weight, height)type Circle(radius :float) =    inherit Shape(radius, radius)    override this.Area() = radius * radius * Math.PI\nThe AbstractClass attribute notifies the compiler that the class has abstract mem-\nbers. The Rectangle  class uses the default implementation of the method Area , and \nthe Circle  class overrides it with a custom behavior.\nInterfaces\nAn interface represents a contract for defining the implementation details of a class. But \nin an interface declaration, the members aren’t implemented. An interface provides an abstract way to refer to the public members and functions that it exposes. In F#, to define an interface, the members are declared using the \nabstract  keyword, followed \nby their type signature:\ntype IPerson =   abstract FirstName : string   abstract LastName : string   abstract FullName : unit -> string\nThe interface methods implemented by a class are accessed through the interface defi-nition, rather than through the instance of the class. Thus, to call an interface method, a cast operation is applied against the class using the \n:> (upcast) operator:\ntype Person(firstName : string, lastName : string) =    interface IPerson with        member this.FirstName = firstName        member this.LastName = lastName        member this.FullName() = sprintf ""%s %s"" firstName lastNamelet fred = Person(""Fred"", ""Flintstone"")(fred :> IPerson).FullName()\nObject expressions\nInterfaces represent a useful implementation of code that can be shared between \nother parts of a program. But it might require cumbersome work to define an ad hoc interface implemented through the creation of new classes. A solution is to use an object expression, which lets you implement interfaces on the fly by using anonymous classes. Here’s an example to create a new object that implements the \nIDisposable  \ninterface to apply a color to the console and then revert to the original:\nlet print color =    let current = Console.ForegroundColor    Console.ForegroundColor <- color\n \n510 appendix  b F# overview\n    {   new IDisposable with            member x.Dispose() =                                Console.ForegroundColor <- current    }    using(print ConsoleColor.Red) (fun _ -> printf ""Hello in red!!"")using(print ConsoleColor.Blue) (fun _ -> printf ""Hello in blue!!"")\nCasting\nThe conversion of a primitive value into an object type is called boxing, which is applied using the function \nbox. This function upcasts any type to the .NET System.Object  \ntype, which in F# is abbreviated by the name obj. \nThe upcast function applies an “up” conversion for classes and interface hierarchies, \nwhich goes from a class up to the inherited one. The syntax is expr :> type . The suc-\ncess of the conversion is checked at compile time. \nThe downcast function is used to apply a conversion that goes “down” a class or inter -\nface hierarchy: for example, from an interface to an implemented class. The syntax is \nexpr :?> type , where the question mark inside the operator suggests that the oper -\nation may fail with an InvalidCastException . It’s safe to compare and test the type \nbefore applying the downcast. This is possible using the type test operator :?, which \nis equivalent to the is operator in C#. The match expression returns true if the value \nmatches a given type; otherwise, it returns false: \nlet testPersonType (o:obj) =    match o with    | o :? IPerson -> printfn ""this object is an IPerson""    | _ -> printfn ""this is not an IPerson""\nUnits of measure\nUnits of measure (UoM) are a unique feature of F#’s type system and provide the ability to define a context and to annotate statically typed unit metadata to numeric literals. This is a convenient way to manipulate numbers that represent a specific unit of mea-sure, such as meters, seconds, pounds, and so forth. The F# type system checks that a UoM is used correctly in the first place, eliminating runtime errors. For example, the F# compiler will throw an error if a \nfloat<m/sec>  is used where it expects a float<mil> . \nFurthermore, it’s possible to associate specific functions to a defined UoM that performs work on units rather than on numeric literals. Here, the code shows how to define the meter (m) and second (sec) UoM and then executes an operation to calculate speed: \n[<Measure>]type m [<Measure>]type sec let distance = 25.0<m>let time = 10.0<sec>let speed = distance / time\n \n 511 Event module API reference\nEvent module API reference\nThe event module provides functions for managing event streams. Table B.1 lists the API \nreferences from the online MSDN documentation (http:/ /mng.bz/a0hG).\nTable B.2 API references\nFunction Description \nadd :\n('T -> unit) -> \nEvent<'Del,'T> -> unit Runs the function each time the event is triggered. \nchoose :\n('T -> 'U option) -> \nIEvent<'Del,'T> -> IEvent<'U> Returns a new event that fires on a selection of messages from the original event. The selection function takes an original message to an optional new message. \nfilter :\n('T -> bool) -> \nIEvent<'Del,'T> -> IEvent<'T> Returns a new event that listens to the original event and triggers the resulting event only when the argument to the event passes the given function. \nmap :\n('T -> 'U) -> \nIEvent<'Del, 'T> -> IEvent<'U> Returns a new event that passes values transformed by the given function. \nmerge :\nIEvent<'Del1,'T> -> \nIEvent<'Del2,'T> -> IEvent<'T> Fires the output event when either of the input events fire. \npairwise :\nIEvent<'Del,'T> -> \nIEvent<'T * 'T> Returns a new event that triggers on the second and sub -\nsequent triggerings of the input event. The Nth triggering \nof the input event passes the arguments from the N − 1th \nand Nth triggerings as a pair. The argument passed to the \nN − 1th triggering is held in hidden internal state until the Nth triggering occurs. \npartition :\n('T -> bool) -> \nIEvent<'Del,'T> -> IEvent<'T> * IEvent<'T> Returns a pair of events that listen to the original event. When the original event triggers, either the first or second event of the pair is triggered accordingly with the result of the predicate. \n \n512 appendix  b F# overview\nFunction Description \nscan :\n('U -> 'T -> 'U) -> 'U -> \nIEvent<'Del,'T> -> IEvent<'U> Returns a new event consisting of the results of applying the given accumulating function to successive values trig-gered on the input event. An item of internal state records the current value of the state parameter. The internal state is not locked during the execution of the accumulation func -\ntion, so care should be taken that the input IEvent isn’t \ntriggered by multiple threads simultaneously. \nsplit :\n('T -> Choice<'U1,'U2>) \n-> IEvent<'Del,'T> -> IEvent<'U1> * IEvent<'U2> Returns a new event that listens to the original event and triggers the first resulting event if the application of the function to the event arguments returned a Choice1Of2, and the second event if it returns a Choice2Of2. \nLearn more\nFor more information about learning F#, I recommend Isaac Abraham’s Get Program-\nming with F#: A Guide for .NET Developers (Manning, 2018, www.manning.com/books/get-programming-with-f-sharp). Table B.2 API references (continued)",32196
152-appendix c Interoperability between an F asynchronous workflow and .NET Task.pdf,152-appendix c Interoperability between an F asynchronous workflow and .NET Task,"513appendix C\n Interoperability between \nan F# asynchronous \nworkflow and .NET Task\nDespite the similarities between the asynchronous programming models exposed by C# and F# programming languages, their interoperability isn’t a trivial accom-plishment. F# programs tend to use more asynchronous computation expressions than .NET Task. Both types are similar, but they do have semantic differences, as shown in chapters 7 and 8. For example, tasks start immediately after their creation, while F# \nAsync  must be explicitly started. \nHow can you interop between F# asynchronous computation expressions and \n.NET Task? It’s possible to use F# functions such as Async.StartAsTask<T>  and \nAsync.AwaitTask<T>  to interop with a C# library that returns or awaits a Task  type.\nConversely, there are no equivalent methods for converting an F# Async  to a Task  \ntype. It would be helpful to use the built-in F# Async.Parallel  computation in C#. \nIn this listing, repeated from chapter 9, the F# downloadMediaAsyncParallel  func-\ntion downloads asynchronously in parallel images from Azure Blob storage. \nListing C.1   Async  parallel function to download images from Azure Blob storage \nlet downloadMediaAsyncParallel containerName = async {    let storageAccount = CloudStorageAccount.Parse(azureConnection)    let blobClient = storageAccount.CreateCloudBlobClient()    let container = blobClient.GetContainerReference(containerName)    let computations =        container.ListBlobs()        |> Seq.map(fun blobMedia -> async {    let blobName = blobMedia.Uri.Segments.                         [blobMedia.Uri.Segments.Length - 1]    let blockBlob = container.GetBlockBlobReference(blobName)    use stream = new MemoryStream()\n \n514 appendix  c  Interoperability between an F# asynchronous workflow and .NET Task\n    do! blockBlob.DownloadToStreamAsync(stream)    let image = System.Drawing.Bitmap.FromStream(stream)    return image })    return! Async.Parallel computations } \nThe return type from downloadMediaAsyncParallel  is an Async<Image[]> . As men-\ntioned, an F# Async  type is in general difficult to interop with and acts as a task ( async/\nawait ) from the C# code. In the following code snippet, the C# code runs the F# \ndownload MediaAsyncParallel  function as a Task  using the Async.Parallel  operator:\nvar cts = new CancellationToken();var images = await downloadMediaAsyncParallel(""MyMedia""). AsTask(cts);\nThe code interoperability becomes effortless with the help of the AsTask  extensions \nmethod. The interoperability solution is to implement a utilities F# module that exposes a set of extension methods that can be consumed by other .NET languages. \nListing C.2 Helper extension methods to interop Task  and an async workflow \nmodule private AsyncInterop =    let asTask(async: Async<'T>, token: CancellationToken option) =        let tcs = TaskCompletionSource<'T>()            let token = defaultArg token Async.CancellationToken            Async.StartWithContinuations(async,                       tcs.SetResult, tcs.SetException,           tcs.SetException, token)           tcs.Task                 let asAsync(task: Task, token: CancellationToken option) =        Async.FromContinuations(                fun (completed, caught, canceled) ->                let token = defaultArg token Async.CancellationToken                 task.ContinueWith(new Action<Task>(fun _ ->                      if task.IsFaulted then caught(task.Exception)                  else if task.IsCanceled then                      canceled(new OperationCanceledException(token)|>raise)                  else completed()), token)                     |> ignore)    let asAsyncT(task: Task<'T>, token: CancellationToken option) =        Async.FromContinuations(                fun (completed, caught, canceled) ->                let token = defaultArg token Async.CancellationToken                  task.ContinueWith(new Action<Task<'T>>(fun _ ->                       if task.IsFaulted then caught(task.Exception)                   else if task.IsCanceled then                         canceled(OperationCanceledException(token) |> raise)                   else completed(task.Result)), token)                      |> ignore)Runs the sequence of F# async computations in parallel\nInstance that allows control of the execution in the form of a Task If a cancellation token isn’t passed as an argument, a \ndefault is assigned using the contextual one, which is \nautomatically propagated through the async workflow. \nStarts the execution with continuations to \npass the termination context into the specific \ncontinuation function based on whether the \nevaluation is successful, faulted, or canceledReturns the \nTaskCompletionSource \nto expose task-based \nbehavior\nStarts the execution from continuations to capture the current evaluation result (success, exception, and cancellation) to continue with one of the given continuation functions\nContinues the \nevaluation using \nTask contention \npassing styleNotifies of successful computation completion\nContinues the \nevaluation using \nTask contention \npassing styleNotifies of successful computation completion\n \n 515  \n[<Extension>]  type AsyncInteropExtensions =  [<Extension>]  static member AsAsync (task: Task<'T>) = AsyncInterop.asAsyncT \n➥ (task, None)   \n  [<Extension>]  static member AsAsync (task: Task<'T>, token: CancellationToken) =      AsyncInterop.asAsyncT (task, Some token)      [<Extension>]  static member AsTask (async: Async<'T>) = AsyncInterop.asTask \n➥ (async, None)    \n  [<Extension>]  static member AsTask (async: Async<'T>, token: CancellationToken) =      AsyncInterop.asTask (async, Some token)    \nThe AsyncInterop  module is private, but the core functions that allow interoperability \nbetween the F# Async  and the C# Task  are exposed through the AsyncInteropExten -\nsions  type. The attribute Extension  upgrades the methods as an extension, making it \naccessible by other .NET programming languages. \nThe asTask  method converts an F# Async  type to a Task , starting the asynchronous \noperation with the Async.StartWithContinuations  function. Internally, this function \nuses the TaskCompletionSource  to return an instance of a Task , which maintains the \nstate of the operation. When the operation completes, the returned state can be a can-cellation, an exception, or the actual result if successful. \nNOTE  These extension methods are built into F# to allow access to the async \nworkflow, but the module is compiled into a library that can be referenced and consumed in C#. Even if this code is in F#, it targets the C# language. \nThe function \nasAsync  aims to convert a Task  into an F# Async  type. This function uses \nAsync.FromContinuations  to create an asynchronous computation, which provides \nthe callback that will execute one of the given continuations for success, exception, or cancellation. \nAll these functions take as a second argument an optional \nCancellationToken , \nwhich can be used to stop the current operation. If no token is provided, then the \nDefaultCancellationToken  in the context will be assigned by default. \nThese functions provide interoperability between the Task-based Asynchronous Pat-\ntern (TAP) of .NET TPL and the F# asynchronous programming model.Exposes the helper functions through the \nextension method to be consumed by \nother .NET programming languages",7453
153-index.pdf,153-index,"516Symbols\n# annotation 155\n| | character 505\n> infix operator 33\n:= operator 499\n! operator 257, 499\n@ operator 507\n* operator 136\n+ operator 136, 490\n<*> operator 446\n>> operator 143, 500\n>>= operator 446\n|> operator 143, 500\n_ (underscore) character 504\nA\nABA problem 65\nAbstractClass attribute 509\nabstract classes, in F# 508–509\nabstractions, raising with higher-order \nfunctions 487–488\nAccess-Data-Object (ADO) 343\nacc parameter 90\naccumulator 125accumulator function 170\nACD (asynchronicity, caching, and distribution) 452\nActionBlock<TInput>completing work  \nwith 370–371\nAction delegate type 193\nactive patterns, in F# 504–505\nAddOrUpdate method 68AddPublisher method 179\nAddTask function 407\nAddToAny method 210\nADO (Access-Data-Object) 343\nADT (algebraic data types) 83\nAgent message-passing pattern 70–71\nagent.Post method 340\nagent programming models\ncoordinating concurrent jobs 440–444\ncoordinating payload between  \noperations 436–440\nenabling in C# using TPL Dataflow 374–382\nagent interaction 378–382\nagent state and messages 377–378\nagents\nadvantages of 336\ncaching operations with 352–356\ncomponents of 335–336\nconcurrent, implementing scheduler  \nwith 419–422\nfunctional programming 337–338\ninteraction 378–382object-oriented 338overview of 334–338\nparallelizing workflows with group coordination \nof 347–349\nreactive programming with 328–364\nasynchronous message-passing programming \nmodels 331–334\nF# MailboxProcessor 341, 338–359\noverview of reactive programming 330–331index\n \n 517\nindex\nshare-nothing approach for lock-free concurrent \nprogramming 336–337\nstate and messages 377–378\nAgentSQL, consuming from C# 346–347\nagentSql function 344\nAggregateException 191Aggregate functions 117, 132, 129–134, 134\naggregating\ndata in parallel 125–139\nAggregate functions 129–134\ndeforesting 127–129parallel arrays in F# 137–139\nPSeq 137\ndeterministic\nassociativity for 136\ncommutativity for 136\nAggregator function 454\nalgebraic data types (ADT) 83\nAmdahl's Law 111, 111–112\namortizing costs, of expensive computations 48–52\nprecomputation 50–51speculative evaluation 51–52\nanalyzeHistoricalTrend function 320\nAND logical asynchronous combinators 323\nanonymous asynchronous lambdas 226–227\nanonymous functions, lambda expressions \nand 489–490\nAPM (Asynchronous Programming  \nModel) 213–219\napplicative functors 308–327\nasynchronous combinators in action 325–327\ncontrolling flow with conditional asynchronous \ncombinators 321–325\nAND logical asynchronous combinators 323\nOR logical asynchronous combinators 323–325\nextending F# async workflows with \noperators 315–317\nheterogeneous parallel computations\ncomposing 319–321executing 319–321exploiting 318–319\nsemantics in F# with infix operators 317\nApply operator 312\nArray.Parallel module 138\narrays, in F# 506\nAsk method 380\nAsObservable method 177, 393AsParallel( ) method 45, 50, 119, 306\nAsReadOnly method 64\nassociative function 136\nassociativity, for deterministic aggregation 136\nasymptotic notation 81\nasync-await keyword 57\nAsync.AwaitTask operator 261\nAsync.CancellationToken 269Async.Catch function 273\nAsync.DefaultCancellationToken 269asynchronous cancellations 234–237\ncancellation support in TAP models 235–237\ncooperative cancellation support 237\nasynchronous combinators\noverview of 325–327\nTPL build in 301–302\nasynchronous computation 256\nasynchronous workflow cancellation \nsupport 268–270\nAsync.map functor 262–264\nAsync.Parallel 264–268\nAsync.Ignore 267–268Async.Start 268Async.StartWithContinuations 267\nAsyncRetry 259–261deferring 239–240extending asynchronous workflows 261–262\nmonads vs. 257–259\nparallel asynchronous operations 271–274\nasynchronous ForEach, exploiting Task.WhenAll \ncombinators for 304–305\nasynchronous functional programming,  \nin F# 247–274\nasynchronous computation expressions 256\nasynchronous functional aspects of 248\nF# asynchronous workflow 248–256\nasynchronous I/O systems 214\nasynchronous lambda 227\nasynchronous message-passing  \nprogramming 331–334\nnatural isolation 334\nrelation with message passing and \nimmutability 334\nasynchronous operations 271–274\ncombining with Kleisli composition \noperator 445–448\nexceptions in 290–299\n \n518\nindex\nhandling errors as 241–243\nparallel 271–274\nasynchronous programming 215–217\nAsynchronous Programming Model 214–219\nbreaking code structure 223\nCPU-bound operations 218–219\nin .NET, support for\nEvent-based Asynchronous Programming 223\noverview of 220–223\nI/O-bound operations 218–219\nscalability and 217–218\nTask-based Asynchronous  \nProgramming 223–246, 230\nanonymous asynchronous lambdas 226–227\nasynchronous cancellations 234–237\ndeferring asynchronous computation 239–240\nhandling errors in asynchronous \noperations 241–243\nretry 240–241Task<T> type 227–230\nwith monadic Bind operators 238–239\nunbounded parallelism with 219–220\nwith Rx 423–431\nAsynchronous Programming Model  \n(APM) 213–219\nasynchronous recursive loops 340–341\nasynchronous workflows\ncancellation support 268–270\nextending 261–262in F# 248–256\nAzure Blob storage parallel  \noperations 251–256\nCPS in computation expressions 249–251\nextending with applicative functor \noperators 315–317\ninteroperability between .NET TPL  \nand 513–515\noverview of 27\nasynchrony\noverview of 452\nparallelism vs. 220\nAsync.Ignore operators 267–268\nasync keyword 225\nAsync.map function 263, 285\nAsync.map functor 262–264\nAsync, modeling error handling in F# with 295–296async operation 241\nAsyncOption function 286\nAsyncOption type, in F# 284–285\nAsync.Parallel function 264–268\nAsync.Ignore operators 267–268\nAsync.Start operators 268\nAsync.StartWithContinuations operators 267\nAsyncReplyChannel 345AsyncResultBuilder 299AsyncResult types, in F#\nextending with monadic Bind operators 296–299\nhigher-order functions in action 297–298\nraising abstractions with computation \nexpressions 298–299\nAsyncRetry 259–261AsyncRetryBuilder 260Async.RunSynchronously function 270\nAsync.StartCancelable function 266\nAsync.StartChild operator 316\nAsync.StartContinuation function 321\nAsync.Start operators 268\nAsync.StartWithContinuations  \noperators 267, 346, 515\nAsyncSubject 174Async.SwitchToContext function 362\nAsync.tap operator 265\nAsync.TryCancelled function 270\nAtom class 434\natomicity 65atomic operations 338\nAtom Swap method 67\nAtom Value property 67\nat-sign operator 507\nawait keyword 225\nAzure Blob service, parallel operations 251–256\nB\nbackground operations 34\nbackpressure 149, 164\nbanana clips 505\nbang operator 499\nBegin function 221\nBehaviorSubject 174benchmarking in F# 22–23\nBig O notation 81\nbimap function 297asynchronous operations (continued)\n \n 519\nindex\nbind function 250\nBind operators\nextending F# AsyncResult types with 296–299\nF# AsyncResult higher-order functions in \naction 297–298\nraising abstraction of F# AsyncResult with \ncomputation expressions 298–299\nmonadic, TAP with 238–239\noverview of 205\nBlendImagesFromBlobStorageAsync function 314\nblendImages function 317\nBlockingCollection class 210\nblocking (synchronous) I/O systems 214\nBoundedCapacity property 373\nB-tree (binary tree) structures 86, 87–91, 88–92\nBufferBlock 368BufferBlock<TInput>using as FIFO buffer 368–369\nBuffer function 164, 169\nbytesRead function 255\nC\ncache, defined 40\ncaching\nlazy 54–55memoization 39–42operations with agents 352–356\noverview of 452\ncalcTransactionAmount function 320\ncallback function 164, 221\ncancellations\nasynchronous 234–237cooperative cancellation support 237\nsupport in TAP models 235–237\nCancellationSource.CreateLinkedTokenSource \nmethod 237\ncancellation token 349\nCancellationToken objects 235, 268, 350–351\nCancellationTokenSource 235CAP (consistency, availability, and partition) \ntheorem 461\ncaptured variables, in closures 36–37\nCascadeClassifier variable 199\nCAS (compare-and-swap) operations 65–67, 433\ncasting in F# 510\ncatamorphism 170CatchStorageException method 281\ncategory theory 206, 305chain of expressions 155\nchunk partitioning 134\nclasses, abstract, in F# 508–509\nclass keyword 108\nclass objects, struct vs. 107–109\nCLI (Common Language Infrastructure) 154, 498\nCLIEvent attribute 154\nCLIMutable attribute 502\nclosure rule 35, 145\nclosures 34–39\ncaptured variables in, with lambda \nexpressions 36–37\nin multithreading environments 37–39\nCLR (common language runtime) 154\ncold observable 171\ncold tasks 239\ncollections, in F# 506\ncombinators, in F#, .NET interoperability  \nwith 154–156\ncombineAccumulatorsFunc function 136\ncombiner operations 136\nCommand and Query Responsibility Segregation \npattern (CQRS) 456\ncomma-separated values format (CSV) 232, 422\ncomments, in F# 501\nCommon Language Infrastructure (CLI) 154, 498\ncommon language runtime (CLR) 154\ncommunicating sequential processes (CSP) 436\ncommutative function 136\ncommutativity, for deterministic aggregation 136\ncompare-and-swap operations (CAS) 433\ncomposability 25Compose function 32\ncomposition operators, in F# 500\nCompressAndEncrypt function 385, 398\ncomputation expressions\nCPS in 249–251\nraising abstraction of F# AsyncResult  \nwith 298–299\ncomputations, amortizing costs of 48–52\nprecomputation 50–51speculative evaluation 51–52\nComputeColumn function 105\nComputeRow function 105\nconcurrency\nadvantages of 12–15\nfunctional programming for 23–26\n \n520\nindex\nhazards of 16–18\noverview of 6, 8\nConcurrentDictionary 46concurrent I/O operations 409–414\nconcurrent programming 7–8\nchoosing models 454–457\nlock-free, share-nothing approach for 336–337\npitfalls of 15–23\nbenchmarking in F# 22–23\nconcurrency hazards 16–18\nparallel quicksort 18–22\nsharing state evolution 18\npresent and future of 14–15\nconditional asynchronous combinators, controlling \nflow with 321–325\nAND logical asynchronous combinators 323\nOR logical asynchronous combinators 323–325\nConfigureAwait Task method 243\nconjunction 160ConnectServer method 424\nconsistency, availability, and partition theorem \n(CAP) 461\nConsole.WriteLine method 37\nconst keyword 77\nconstruct 54continuation function 91, 205\nContinuesWith method 226\nContinueWith method 200\ncontrol flow 504\ncooperative cancellation, support for 237\ncooperative multitasking systems 11\ncopyStream function 255\ncosts, of computations 48–52\nprecomputation 50–51speculative evaluation 51–52\nC# programming language\nconsuming AgentSQL from 346–347\ncurrying in 492–495, 492–497\nenabling agent models using TPL  \nDataflow 374–382\nagent interaction 378–382\nagent state and messages 377–378\nfunctional concurrent programming in 27–29\nfunctional lists in 84–85\nfunction composition in 31–33\nhandling errors with Task<Option<T>> in 284immutability in 77–79\npartial function applications in 495–497\nTask-based Asynchronous Programming for 223–\n230\nanonymous asynchronous lambdas 226–227\nTask<T> type 227–230\nvoids in 191–193\nCPS (continuation-passing style) 193–200\nadvantages of 194–195\nin computation expressions 249–251\noptimizing recursive functions with 91\nB-tree structures 91–92\nparallel calculators 93–94\noverview of 183\nwaiting for tasks to complete 195–200\nCPU-bound operations 218–219\nCPU time 106\nCQRS (Command and Query Responsibility \nSegregation) pattern 456\nCreateIfNotExistsAsync operation 261\nCSP (communicating sequential processes) 436\nCSV (comma-separated values) format 232, 422\nctx command 358\nCurrent function 161\ncurriedAdd function 492\ncurried functions 240, 311\ncurrying 490–494\nin C# 492–495, 492–497\nin F# 493–494\nun-currying 492–493\nD\nDAG (directed acyclic graph) 404–408\ndata\naggregating 125–139\nAggregate functions 129–134\ndeforesting 127–129parallel arrays in F# 137–139\nPSeq 137\nbasic types 501\ntransforming with TransformBlock<TInput, \nTOutput> 369–370\ndatabases, avoiding bottlenecks with F# \nMailboxProcessor 341–359\ncaching operations with agents 352–356\nCancellationToken object 350–351\nconsuming AgentSQL from C# 346–347concurrency (continued)\n \n 521\nindex\ndistributing work with MailboxProcessor 351–352\nhandling errors with 349–350\nMailboxProcessor message types 344\nMailboxProcessor two-way  \ncommunication 345–346\nparallelizing workflows 347–349\nreporting events from MailboxProcessor 359\nreporting results from MailboxProcessor 357–359\nDataFlowBlockOptions.Unbounded 373DataFlowLinkOptions argument 389\ndata parallelism 97–117, 118–147\naggregating data 125–139\nAggregate functions 129–134\ndeforesting 127–129parallel arrays in F# 137–139\nPSeq 137\nFork/Join pattern 102–110\nparallel loops, pitfalls of 110\nstruct vs. class objects 107–109\nfunctional data structures for 76\nin .NET, support for 101–102\nMandelbrot algorithm 102–110\nparallel loops, pitfalls of 110\nstruct vs. class objects 107–109\nmeasuring performance speed 110–117\nAmdahl's Law 111–112\ndeclarative parallel programming  \nmodel 115–117\nGustafson's Law 112\nparallel loops, limitations of 112–114\nsimple loops, pitfalls of 114–115\nmonoids for 306\noverview 98–101parallel MapReduce pattern 139–147\nMap function 140–141\nNuGet package gallery with 141–147\nReduce function 140–141\nPLINQ 125–147\npure functions and 121–122\nside effects, isolating and controlling 124–125\nside effects with pure functions 123–124\nreducing data 125–139\ndeforesting 127–129implementing parallel Reduce function for \nPLINQ 135–136\nparallel arrays in F# 137–139\nPSeq 137strategies 138–139task parallelism and 99–100\ndata race 16\ndata structures 60\ndata values, flattening 172\ndeadlock 17declarative parallel programming model 115–117\ndecoupled operations 217\ndefensive copy 185\nDeflateStream 389deforesting 127–129degree of parallelism 110\ndelegates, in F# 500–501\ndependencies, parallelizing tasks with 404–408\nDetectFaces function 196\ndeterminism 15deterministic aggregation\nassociativity for 136\ncommutativity for 136\nDictionary variable 41\ndimensionality 132directed acyclic graph (DAG) 404–408\ndiscriminated unions (DU) 83\ndisjunction 160Disposable.Create method 426\ndistribution 452Divide and Conquer pattern 89, 101\ndivisibleBy function 504\ndomain-specific language (DSL) 157\ndouble-checked locking 55\ndowncast function 510\nDownloadDataAsync 36DownloadDataCompleted 36DownloadIconAsync method 228\nDownloadImageAsync method 278\nDownloadImage method 277\ndownloadMediaAsync function 254, 258, 264\ndownloadMediaAsyncParallel function 513\ndownloadMediaCompAsync function 260\nDownloadOptionImage method 281\nDSL (domain-specific language) 157\nDU (discriminated unions) 83, 503\nE\nEAP (Event-based Asynchronous \nProgramming) 223\n \n522\nindex\nEDA (event-driven architecture) 467\nElapsedEventArgs 162Elapsed Time 106\nelevated types, functors to map 306–307\nelevated world 308\nElm language 150\nembarrassingly parallel 100–101\nEmgu.CV library 196\nemotion analysis 173–177, 164–179\nencryption 384End function 223\nEndReadCallback 223Enqueue function 209\nenvelopes 460EPM (event programming model) 36\nerror combinators 279–284\nerror handling in functional  \nprogramming 282–284\nexceptions in asynchronous operations 290–299\nextending F# AsyncResult types with monadic \nBind operators 296–299\nF# AsyncResult higher-order functions in \naction 297–298\nraising abstraction of F# AsyncResult with \ncomputation expressions 298–299\nF# AsyncOption type 284–285\nhandling errors with Task<Option<T>> in C# 284\nidiomatic F# functional asynchronous error \nhandling 286–287\nmodeling error handling in F# with Async and \nResult 295–296\npreserving exception semantics with Result \ntype 287–290\nerrorHandler function 349\nerror handling\nas asynchronous operations 241–243\nidiomatic F# functional asynchronous error \nhandling 286–287\nin functional programming 282–284\nin imperative programming 277–279\nmodeling handling in F# with Async 295–296\nmodeling handling in F# with Result 295–296\nwith F# MailboxProcessor 349–350\nwith Task<Option<T>> in C# 284\nEuclidean function 129\neval function 93\nevent aggregators, polymorphic 416–419Event-based Asynchronous Programming 223\nevent combinators 153–154\nevent-driven architecture (EDA) 467\nevent modules, in F# 511\nevent programming model (EPM) 36\nevents\nfrom MailboxProcessor, reporting using thread \npools 359\nto F# observables 163–164\nevent streams 153\nexception semantics, preserving with Result \ntype 287–290\nExecute function 209\nExecuteTask method 344\nExpr type 94\nExtensible Application Markup Language \n(XAML) 462\nExtractWebPageTitle function 44\nF\nfactorialCPS function 91\nfAsyncResult operation 297\nFIFO (first in, first out) buffer, using \nBufferBlock<TInput> as 368–369\nFileNotFoundException 290FileOptions.Asynchronous flag 221\nFileSystemWatcher 252FilterMap function 120\nFilter operator, combining with Map  \noperators 431–434\nfirst-class values 485\nfirst in, first out buffer, using BufferBlock<TInput> \nas 368–369\nFizz-Buzz 504flatMap operator 171\nflow, controlling with conditional asynchronous \ncombinators 321–325\nAND logical asynchronous combinators 323\nOR logical asynchronous combinators 323–325\nfluent concurrent programming, functional \ncombinators for 301–308, 275–327\nabstracting operations with 300\napplicative functors 308–327\nerror combinators 279–284\nerror handling in imperative  \nprogramming 277–279\nexploiting Task.WhenAll combinators for \nasynchronous ForEach 304–305\n \n 523\nindex\nexploiting Task.WhenAny combinators for \nredundancy and interleaving 302–304\nfunctors to map elevated types 306–307\nmonads 307–308monoids for data parallelism 306\nOtherwise in C# 279–284\npathematical patterns 305–306\nRetry in C# 279–284\nTask.Catch in C# 279–284\nTPL builds in asynchronous  \ncombinators 301–302\nfluent interface 79\nfmap functor patterns, to apply \ntransformations 205–206\nfmap operator 228\nfold function 125, 128\nForceParallelism 133ForEach, asynchronous 304–305\nForEachAsync operator 304\nfor-each loop 101, 120, 199\nForEach operator 433\nFork/Join operators 401–404\nFork/Join patterns 102–110\ncomposing pipeline of steps forming 402–404\noverview of 244, 404\nparallel loops, pitfalls of 110\nstruct vs. class objects 107–109\nfor loop 20, 101\nforward composition operator 143\nFP (functional programming) 30–58, 484–497\nadvantages of 25–26, 485–486\nagent-based 337–338amortizing cost of expensive computations 48–52\nprecomputation 50–51speculative evaluation 51–52\nclosures 34–39\ncaptured variables in 36–37\nin multithreading environments 37–39\ncurrying 490–494\nin C# 492–495, 492–497\nin F# 493–494\nun-currying 492–493\nerror handling in 282–284\nfor concurrency 23–26\nfrom imperative to object-oriented to 486–487\nfunction composition 31–34\nin C# 31–33\nin F# 33–34higher-order functions\nlambda expressions and 488\nraising abstractions with 487–488\nlambda expressions, anonymous functions \nand 489–490\nlazy evaluation 52–58\ncaching techniques 54–55\nin F#, support for 56\nSingleton patterns 54–55\nstrict languages 53–54\nwith task 56–58\nmemoization\nfor improved performance 46–48\nfor web crawlers 43–46\nmemoization-caching techniques 39–42\non servers 450–451\npartial function applications 494–495\nadvantages of 495\nin C# 495–497\ntenets of 486\nF# programming language 498–512\nabstract classes and inheritance 508–509\nactive patterns 504–505\napplicative functor semantics in, with infix \noperators 317\narrays 506asynchronous functional programming  \nin 247–274\nasynchronous computation expressions 256\nasynchronous functional aspects of 248\nasynchronous workflows in 248–256\nAzure Blob storage parallel  \noperations 251–256\nCPS in computation expressions 249–251\nextending with applicative functor \noperators 315–317\ninteroperability between .NET TPL  \nand 513–515\nAsyncOption type in 284–285\nAsyncResult higher-order functions in \naction 297–298\nbasic data types in 501\nbenchmarking in 22–23\nB-tree in 87–88\ncasting in 510\nclass and inheritance in 508\ncollections in 506\n \n524\nindex\ncombinators in, .NET interoperability with 154–\n156\ncomments in 501\ncreating mutable types 499–500\ncurrying in 493–494\ndelegates in 500–501\ndiscriminated unions in 503\nevent module API reference in 511\nextending AsyncResult types with monadic Bind \noperators 296–299\nF# AsyncResult higher-order functions in \naction 297–298\nraising abstraction of F# AsyncResult with \ncomputation expressions 298–299\nfor functional concurrent programming 27–29\nfunctional lists in 83–84\nfunction composition in 33–34\nfunctions as first-class types 500\nfunction signatures in 499\nidiomatic functional asynchronous error \nhandling 286–287\nimmutability in 79–80\ninterfaces 509let binding in 498–499\nlists in 507\nloops in 507–508\nMailboxProcessor in 341, 338–359\nasynchronous recursive loops 340–341\ncaching operations with agents 352–356\nCancellationToken object 350–351\nconsuming AgentSQL from C# 346–347\ndistributing work with 351–352\nhandling errors with 349–350\nmessage types 344\nparallelizing workflows with group \ncoordination of agents 347–349\nreporting results from 357–359\ntwo-way communication 345–346\nusing thread pools to report events from 359\nwith Game of Life 359–364\nmaps in 507\nmodeling error handling in\nwith Async 295–296\nwith Result 295–296\nobject expressions 180–181, 509\nobservables in, from events to 163–164open statements in 501\nparallel arrays in 137–139\npattern matching in 504\npipe and composition operators in 500\nraising abstraction of AsyncResult with \ncomputation expressions 298–299\nrecord types in 502–503\nresources in 512\nsequences in 506\nsets in 507\nspecial string definition 502\nsupport for lazy evaluation in 56\ntuples in 502\nunits of measure in 510\nfree variables 34\nfrom clause 230\nFromEventPattern method 179\nFRP (functional reactive programming) 148–181\n.NET tools for 152–156\nevent combinators 153–154\n.NET interoperability with F# \ncombinators 154–156\noverview of 149–152\nReactive Extensions 156–164\nfrom events to F# observables 163–164\nfrom LINQ/PLINQ to 159\nin action 161–162\nIObservable interface 160–161\nreal-time streaming with 162\nTwitter emotion analysis 164–173\nRx Publisher-Subscriber 173–181\nanalyzing tweet emotions with 177–179\nF# object expressions 180–181\nin relation to concurrency 174–175\nobservers in action 179–180\nreusable 175–177Subject type for hubs 173–174\nfst function 502\nfunAsync parameter 315\nfunctional asynchronous combinators 300\nfunctional combinators 301–308\nabstracting operations with 300\nexploiting Task.WhenAll combinators for \nasynchronous ForEach 304–305\nexploiting Task.WhenAny combinators 302–304\nfor fluent concurrent programming 275–327\napplicative functors 308–327F# programming language (continued)\n \n 525\nindex\nerror combinators 279–284\nerror handling in imperative \nprogramming 277–279\nOtherwise in C# 279–284\nRetry in C# 279–284\nTask.Catch in C# 279–284\nfunctors to map elevated types 306–307\nmonads 306–307, 306–308\npathematical patterns 305–306\nTPL build in asynchronous combinators 301–302\nFunctionalConcurrencyLib library 265\nfunctional concurrent programming\nusing C# for 27–29\nusing F# for 27–29\nfunctional data structures 59–94\nfor data parallelism 76\nrecursive functions 88–94\ntail-call optimization 89–90\nwith CPS 91\nsharing among threads 72–73\nfunctional lists 80–86\nin C# 84–85\nin F# 83–84\nlaziness values in 85–86\nfunctional precomputation 36\nfunction composition 31–34\nin C# 31–33\nin F# 33–34\nfunctions, as first-class types 500\nfunction signatures, in F# 499\nfunctor patterns, fmap 205–206\nfunctors, to map elevated types 306–307\nfun keyword 499\nfutures 183FuzzyMatch function 49\nG\nGame of Life, MailboxProcessor with 359–364\nGang of Four (Fowler) 157\ngcAllowVeryLargeObjects option 383\nGC (Garbage Collector) 47\nGetAsync function 228, 400\ngetCloudBlobContainerAsync function 254\nGetEnumerator function 161\nGetEventsHistory function 470\nGetNearestCentroid function 130GetOrAdd method 47, 69\nGetOrSet function 355\nGetPixel method 189\nGetReponseAsync( ) method 232\nGetServerStream method 425\ngetter-only auto-properties feature 78\nGetWebContent method 43\nGoF (Gang of Four) book 157\ngoogleSourceUrl function 243\nGPUs (Graphic Processing Units) 29\nGroupBy function 132, 140, 169\nGUIs (graphical user interfaces) 153\nGustafson's Law 112\nGZipStream class 383, 389\nH\nhash partitioning 134\nHaskell-Yampa 150HasValue flag 283\nheterogeneous parallel computations\ncomposing 319–321executing 319–321exploiting with applicative functors 318–319\nheterogeneous tasks 188\nHOF (higher-order functions)\nlambda expressions and 488\noverview of 33, 284\nraising abstractions with 487–488\nHollywood Principle 151\nhot observable 171\nhot tasks 239\nHttpResponseMessage 228\nI\nidempotent functions 301\nidentity property 145\nIDisposable interface 180\nifAsync combinator 322\nif-else statement 321\nIL (intermediate language) 154\nImageHelpers function 264\nimmutability 25, 59–94\nB-tree structures 86–88\nfunctional data structure for data parallelism 76\nfunctional lists 80–86\nin C# 84–85\n \n526\nindex\nin F# 83–84\nlaziness values in 85–86\nin C# 77–79\nin F# 79–80\nmessage passing and 334\nperformance implications of 76–77\nImmutableInterlocked class 67–68\nimperative programming 279, 486\nIncrement function 35\ninfix operators 33, 317\ninheritance, in F# 508, 508–509\ninherit keyword 508\ninline functions 323\ninline keyword 263\ninorder function 88\ninputs 450input space 505\ninterfaces, in F# 509\nInterlock class 114\nInterlocked.CompareExchange method 65\nInterlocked.Increment method 65\nintermediate language (IL) 154\nInternet of Things (IoT) 329\nInvalidCastException 510inversion of control 151\nI/O-bound operations 218–219\nIObservable interface 160–161, 458\nI/O operations, concurrent 409–414\nI/O systems 53, 214\nIoT (Internet of Things) 329\nIPEndPoint 429IPipeline interface 207\nIReplyAgent interface 380\nIScheduler interface 175, 419\nIsEmpty property 84\nisMandelbrot function 104\nisolation, natural 334\nisPrime function 113\nJ\nJaro-Winkler algorithm 48\nK\nKeyPressedEventCombinators 155, 161, 163\nKeyPressEventArgs 162KeyPress keyboard event 154\nKleisli composition operator, combining \nasynchronous operations with 445–448\nk-means clustering 129\nL\nLambda calculus 489\nlambda expressions\nanonymous asynchronous lambdas 226–227\nanonymous functions and 489–490\nhigher-order functions and 488\nwith captured variables in closures 36–37\nLanguage Integrated Query (LINQ) 44\nlarge streams of data\nprocessing 383–387with parallel workflows 382–393\ncompleting dataflow blocks 389–390\nensuring order integrity of streams of \nmessages 388–389\nintegrating Rx with TPL Dataflow 391–393\nlinking dataflow blocks 389–390\npropagating dataflow blocks 389–390\nrules for building dataflow workflows 390–391\nlaziness values, in functional lists 85–86\nlazy evaluation 52–58\ncaching techniques 54–55\nin F#, support for 56\nSingleton patterns 54–55\nstrict languages 53–54\nwith task 56–58\nLazyInitializer class 55\nLazy type 46\nleafs 87left-fold form 126\nlet binding, in F# 498–499\nlet keyword 79\nlet! syntax 296\nlift2 function 321\nlinear speedup 110\nlinking TPL Dataflow blocks 372–389, 372–390\nLinkTo method 372\nLINQ (Language Integrated Query) 44\nLINQ/PLINQ, from Rx to 159\nlist-comprehension query 127\nList.partition API 20\nlists, in F# 507immutability (continued)\n \n 527\nindex\nLloyd's algorithm 129\nloadandApply3dImage function 444\nloadImage function 444\nlocation transparency 335\nlock-free concurrent programming 336–337\nlogical cores 374\nLongRunning option 195\nloops\nin F# 507–508\nparallel\nlimitations of 112–114\npitfalls of 110\nrecursive, asynchronous 340–341\nsimple, pitfalls of 114–115\nM\nMagnitude property 104\nMailboxProcessor\nCancellationToken object 350–351\ndistributing work with 351–352\nin F# 341, 338–359\nasynchronous recursive loops 340–341\ncaching operations with agents 352–356\nconsuming AgentSQL from C# 346–347\nhandling errors with 349–350\nparallelizing workflows with group \ncoordination of agents 347–349\nmessage types 344\nreporting results from 357–359\ntwo-way communication 345–346\nusing thread pools to report events from 359\nwith Game of Life 359–364\nMakeRandomSeed method 416\nMandelbrot algorithm 102–110\nparallel loops, pitfalls of 110\nstruct vs. class objects 107–109\nmapF function 143\nMap function 140–141\nMap operators, combining with Filter \noperators 431–434\nMapReduce pattern\nmath and 145\nNuGet package gallery with 141–147\noverview of 454\nmaps, in F# 507\nMatch function 294materialization 122mathematical patterns 201–206\nabilities behind monads 206\napplying monad patterns to task operations 205\nfmap functor patterns to apply \ntransformations 205–206\nmonadic laws 202–204\nmonadic operators 202\nmaxConcurrentOperations 273MaxDegreeOfParallelism property 304, 374\nmaxPixels parameter 311\nmemoization 36\ncaching techniques 39–42\nfor improved performance 46–48\nfor web crawlers 43–46\nmemory, recycling objects to reduce \nconsumption 398–401\nmessage passing, immutability and 334\nmessage-passing model 332\nmessages, ensuring order integrity of streams \nof 388–389\nmessage types, in F# MailboxProcessor 344\nmobile apps 449–483\nchoosing concurrent programming  \nmodel 454–457\ndesigning performant applications 451–454\nasynchronicity, caching, and distribution 452\nqueuing work for later execution 453–454\nfunctional programming on servers 450–451\nModel-View-ViewModel (MVVM) 461\nmonadic container 227\nmonads 307–308\nabilities behind 206\napplying patterns to task operations 205\ncomposing functions 444–448\ncomputation expressions vs. 257–259\nlaws 202, 308\noperators 202\nmonoids\nfor data parallelism 306\noverview of 145–147\nmonotonic value 387\nmorphisms 305MoveNext function 161\nmulticase active pattern 505\nmultiple Producer/single Consumer  \npattern 372–374\n \n528\nindex\nmultiplexer pattern 388\nmultitasking systems 10–11\nmultithreading\nenvironments, closures in 37–39\noverview of 6, 11–12\nmutable types, creating in F# 499–500\nmutual exclusion 17\nMVVM (Model-View-ViewModel) 461\nN\n.NET platform\nconcurrent collections 68–70\ndata parallelism in, support for 101–102\nimmutable collections 63–68\nCAS operations 65–67\nImmutableInterlocked class 67–68\ninteroperability with F# combinators 154–156\nsupport for asynchronous programming  \nin 220–223\nsupport for task parallelism in 185–187\ntools for functional reactive  \nprogramming 152–154\nTPL 187–191\nbuild in asynchronous combinators 301–302\nrunning operations in parallel with Parallel.\nInvoke method 188–191\nNetworkStream method 425\nnon-blocking (asynchronous) I/O systems 214\nnormal world 308\nNuGet package gallery, MapReduce pattern \nwith 141–147\nnull values 27, 277\nO\nobject expressions, in F# 180–181, 509\nobject-oriented programming (OOP) 7, 486–487\nObjectPoolAsync 399objects\nasynchronously recycling pools of 399–401\nrecycling to reduce memory consumption 398–\n401\nObservable Defer operator 430\nObservable.FromEventPattern method 162\nObservable pipeline 175\nobservables\nin F#, from events to 163–164\noverview of 171Observable.Scan operator 392\nObservableStreams method 426\nobservableTweets pipeline 168\nObserveOn method 175\nObserver.Create( ) method 180\nobservers, in action 179–180\nofChoice function 286\n.OnCompleted method 158\nOnConnected method 61, 479\nonDemand function 239\nOnDisconnected method 61, 66, 479\n.OnError method 158\nOnKeyDown event 155\n.OnNext method 158\nOnNext method 161\nOOP (object-oriented programming) 7\nopAsync parameter 315\nopen statements, in F# 501\nOperationCanceledException 235operations\nabstracting with functional combinators 300\ncaching with agents 352–356\ncoordinating payload between, using agent \nprogramming models 436–440\nrunning in parallel with Parallel.Invoke \nmethod 188–191\noperator 154, 317\nOption type, controlling side effects with 283–284\norder integrity, of streams of messages 388–389\nOR logical asynchronous combinators 323–325\nOtherwise combinator 242\nOutOfMemoryArray exception 383\nOutputAvailableAsync method 369\nover-parallelized algorithms 21\nP\nPageRank (pg) class 144\nparadigm 484parallel arrays, in F# 137–139\nparallel calculators 93–94\nParallel class 101\nparallelDownloadImages function 92\nParallelExecutionMode 133ParallelFilterMap function 432, 434\nparallel Filter-Map operators 431–434\nParallel.ForEach loop 432\n \n 529\nindex\nParallel.For function 103, 105\nparallel functional pipeline patterns 207–212\nParallel.Invoke method\noverview of 39\nrunning operations in parallel with 188–191\nparallelism\nasynchrony vs. 220\ncontrolling degree of, with Rx Scheduler 419–422\noverview of 6, 184\nparallel loops\nlimitations of 112–114\noverview of 38\npitfalls of 110\nparallel MapReduce pattern 139–147\nMap function 140–141\nNuGet package gallery with 141–147\nMapReduce and math 145\nmonoids 145–147\nReduce function 140–141\nparallel operations, for Azure Blob storage 251–256\nparallel programming\ndeclarative models 115–117\noverview of 8–10\nparallel quicksort 18–22\nparallel Reduce function, implementing for \nPLINQ 135–136\nparallel sequence (PSeq) 51\nParallelWithCatchThrottle function 271\nParallelWithThrottle function 271\nparallelWorker function 348\nparallel workflows, large streams of data  \nwith 382–393\ncompleting dataflow blocks 389–390\nensuring order integrity of streams of \nmessages 388–389\nintegrating Rx with TPL Dataflow 391–393\nlinking dataflow blocks 389–390\nprocessing large streams of data 383–387\npropagating dataflow blocks 389–390\nrules for building dataflow workflows 390–391\nparameterized active pattern 505\npartial function applications 494–495\nadvantages of 495\nin C# 495–497\nPartialFuzzyMatch function 50\npartially applied functions 492Partitioner.Create method 134, 433\npartitioning, overview of 134\npathematical patterns 305–306\npatterns, matching in F# 504\npayloads, coordinating between operations 436–440\nperformance\nimplications of immutability for 76–77\nimproving with memoization 46–48\nmeasuring speed 110–117\nAmdahl's Law 111–112\ndeclarative parallel programming  \nmodel 115–117\nGustafson's Law 112\nparallel loops, limitations of 112–114\nsimple loops, pitfalls of 114–115\nperformant applications, designing 451–454\nasynchronicity, caching, and distribution 452\nqueuing work for later execution 453–454\npg (PageRank) class 144\npipeline patterns 207–212\npipe operators, in F# 500\nPLINQ (parallel LINQ) 125–147\nimplementing parallel Reduce function  \nfor 135–136\nisolating and controlling side effects 124–125\npure functions and 121–124\npolymorphic event aggregators 416–419\npolymorphic Publisher-Subscriber patterns 416–419\nPostAndAsyncReply method 345\nprecomputation 50–51preemptive multitasking systems 11\nProcessArray method 39\nprocessImage function 298\nProcessImage function 293\nProcessorCount 22ProcessStockHistory function 232\nProducer/Consumer patterns\nimplementing with TPL Dataflow 372–374\nmultiple Producer/single Consumer \npattern 372–374\nsingle Producer/multiple Consumer \npattern 374\noverview of 70\nPropagateCompletion property 389\npropagating TPL Dataflow blocks 389–390\nPSeq (parallel sequence) 51, 137\n \n530\nindex\nPSeq.withDegreeOfParallelism 142Publisher-Subscriber patterns 173–181\nanalyzing tweet emotions with 177–179\nF# object expressions 180–181\nin relation to concurrency 174–175\nobservers in action 179–180\npolymorphic 416–419reusable 175–177Subject type for hubs 173–174\npure functions\navoiding side effects with 123–124\nPLINQ and 121–122\nPureWordsPartitioner function 125\nPutAsync function 400\nQ\nQueueUserWorkItem class 185\nQuicksort algorithm 18–22\nquicksortParallel function 21\nquicksortParallelWithDepth function 21\nquicksortSequential 20\nR\nrace condition 16\nrandom number generators, thread-safe 414–416\nrange partitioning 134\nReactive Manifesto 330\nreactive programming\noverview of 330–331\nwith agents 328–364\nasynchronous message-passing programming \nmodels 331–334\navoiding database bottlenecks 341–359\nF# MailboxProcessor 338–341\noverview of agents 334–338\nReaderWriterLockSlim 409Read-Evaluate-Print-Loop (REPL) 22\nReadFileNoBlocking method 226\nReadObservable operator 428\nReadOnlyCollection 64readonly keyword 77\nReadText method 496\nReadToEndAsync( ) method 232\nread/write operations, applying multiple 409–414\nreal-time streaming, with Rx 162\nrecord types, in F# 502–503recursive functions 88–94\noptimizing with CPS 91\nB-tree structures 91–92\nparallel calculators 93–94\ntail-call optimization 89–90\nrecursive loops, asynchronous 340–341\nRecyclableMemoryStream object 391\nreduceF function 143\nReduce function 135, 140\nreducing data in parallel 125–139\ndeforesting 127–129implementing parallel Reduce function for \nPLINQ 135–136\nparallel arrays in F# 137–139\nPSeq 137\nreduction function 125\nreduction operations 136\nreference cell 499\nreferential transparency 25, 123\nReplaySubject 174REPL (Read-Evaluate-Print-Loop) 22\nreportBatch function 359\nreporting\nevents from MailboxProcessor, using thread \npools 359\nresults from F# MailboxProcessor 357–359\nresources\nin F# 512\nthread-safe, shared, applying multiple read/write \noperations to 409–414\nResultExtensions class 291, 293\nResult property 225\nResult type\nmodeling error handling in F# with 295–296\npreserving exception semantics with 287–290\nretn function 296\nReturn operator 205, 238\nreturn! syntax 297\nright-fold form 126\nRNGCryptoServiceProvider 415Round-robin algorithm 347\nRunDownloadImage method 278\nRxPubSub class 176\nRx (Reactive Extensions) 156–164\nfrom events to F# observables 163–164\nfrom LINQ/PLINQ to 159\nin action 161–162\n \n 531\nindex\nintegrating with TPL Dataflow 391–393\nIObservable interface 160–161\nPublisher-Subscriber patterns 173–181\nanalyzing tweet emotions with 177–179\nF# object expressions 180–181\nin relation to concurrency 174–175\nobservers in action 179–180\nreusable 175–177Subject type for hubs 173–174\nreal-time streaming with 162\nTwitter emotion analysis 164–173\nwith asynchronous programming 423–431\nRx Scheduler, controlling degree of \nparallelism 419–422\nRxTweetEmotion class 179\nS\nSaveEvent function 470\nsaveImage function 444\nscalability, asynchronous programming  \nand 217–218\nschedulers, implementing with multiple concurrent \nagents 419–422\nSelectMany operators 169, 171–173, 229, 308\nSelect method 132, 244\nSendAsync function 373\nseparation of concerns 154, 457\nsequences, in F# 506\nsequential loops 101\nsequential programming 6–7\nservers, functional programming on 450–451\nServicePointManager.DefaultConnectionLimit 273ServicePoint object 271\nSetPixel method 189\nsets, in F# 507\nshare-nothing approach\nfor lock-free concurrent programming 336–337\noverview of 334\nsiblings 87side effects\ncontrolling with Option type 283–284\nisolating and controlling 124–125\nwith pure functions 123–124\nSignalR Hub class 60, 466, 478\nSignalR, real-time communication with 457\nSIMD (Single Instruction Multiple Data) 98simple loops, pitfalls of 114–115\nSingle Instruction Multiple Data (SIMD) 98\nSingle Instruction Single Data (SISD) 98\nsingle Producer/multiple Consumer pattern 374\nSingleton patterns 54–55\nSISD (Single Instruction Single Data) 98\nSMP (symmetric multiprocessing processing) 28\nsnd function 502\nspeculative evaluation 51–52\nspeed, of performance 110–117\nAmdahl's Law 111–112\ndeclarative parallel programming model 115–117\nGustafson's Law 112\nparallel loops, limitations of 112–114\nsimple loops, pitfalls of 114–115\nSqlCommand 352squareOfDigits function 153\nStackoverflow exception 89\nStanford CoreNLP library 165\nStartStreamAsync( ) function 171\nstate evolution, sharing 18\nstateful 375StatefulDataFlowAgent class 376\nStatefulReplyDataFlowAgent 380stateless 375Status property 242\nStockData 231strict evaluation 53\nstrict languages 53–54\nstrings, special definition 502\nstripped partitioning 134\nStruct attribute 344\nstruct, class objects vs. 107–109\nstruct keyword 108\nStructural Sharing pattern 63, 73, 81\nSubject type, for Publisher-Subscriber hubs 173–174\nSubscribe method 158\nSubscribeOn method 175, 418\nSum( ) function 116, 302\nsymmetric multiprocessing processing (SMP) 28\nsynchronous I/O systems 214\nsynchronous message-passing models 435–440\nSystem.Collections.Concurrent namespace 68\nSystem.Collections.Immutable namespace 63, 67\nSystem.IO.MemoryStream 391System.Random class 415\n \n532\nindex\nSystem.Reactive.Concurrency namespace 175\nSystem.Threading.Interlocked class 65\nSystem.Threading namespace 113\nSystem.Threading.Tasks namespace 172, 187, 194\nT\ntail-call optimization 89–90\nTakeFromAny method 210\nTap operator 229\nTAP (Task-based Asynchronous \nProgramming) 230–246\nasynchronous cancellations 234–237\ncancellation support in 235–237\ndeferring asynchronous computation 239–240\nfor C# 223–230\nanonymous asynchronous lambdas 226–227\nTask<T> type 227–230\nhandling errors in asynchronous  \noperations 241–243\noverview of 223, 515\nretry 240–241with monadic Bind operators 238–239\ntask-based functional parallelism 182–212\ncontinuation-passing style 193–200\nadvantages of 194–195\nwaiting for tasks to complete 195–200\n.NET TPL 187–191\nparallel functional pipeline patterns 207–212\nstrategies for composing task operations 200–207\nguidelines for using tasks 207\nmathematical patterns for better \ncomposition 201–206\ntask parallelism 183–212\nadvantages of 184–185\nin .NET, support for 185–187\nvoids in C# 191–193\nTask.Bind operator 228\nTask.Catch function 281\nTaskCompletionSource 204TaskContinuationOptions.OnlyOnCanceled \nflag 200\nTaskContinuationOptions.OnlyOnFaulted flag 200\nTaskCreationOptions 195TaskCreationOptions.LongRunning option 195\nTask.Factory.StartNew method 195\ntask generators 239\nTask.Map operator 228task operations\napplying monad patterns to 205\nstrategies for composing 200–207\nguidelines for using tasks 207\nmathematical patterns for better \ncomposition 201–206\nTask<Option<T>>in C# 284\ntask parallelism 183–212\nadvantages of 184–185\ndata parallelism and 99–100\nin .NET, support for 185–187\nTask Parallel Library (TPL) 19\nTaskPoolScheduler 175, 424\nTaskPool.Spawn function 438\nTask.Run method 197\ntasks\nguidelines for using 207\nlazy evaluation with 56–58\nparallelizing with dependencies 404–408\nwaiting to complete 195–200\ntask scheduler 366\nTask<T> type 227–230\nTask.WhenAll combinators, exploiting for \nasynchronous ForEach 304–305\nTask.WhenAll method 245, 302, 323\nTask.WhenAny combinators, exploiting 302–304\nTask.WhenAny method 245, 303\nTBB (Threading Building Blocks) 28, 454\nTCO (tail-call optimization) 89–90\nTcpClient class 423\nTcpClient.GetStream method 425\nTcpListener class 423\nTCP (Transmission Control Protocol) 422\nThen function 210\nThread class 185\nThreading Building Blocks (TBB) 28, 454\nThreadLocal objects, thread-safe 415–416\nthread-local storage (TLS) 114\nThreadPool class 185, 234\nThreadPool.QueueUserWorkItem method 186\nthreadpools 215ThreadPoolScheduler 175thread pools, to report events from \nMailboxProcessor 359\nthread quantum 10\nthread-safe method 15\nthread-safe random number generators 414–416\n \n 533\nindex\nthread-safe resources, shared 409–414\nthreads, sharing functional data structures \namong 72–73\nThrottle function 164, 168\nthrottling 373ThrowIfCancellationRequested method 236\nthrow statements 277\nthunks 85time slice 10\nTLS (thread-local storage) 114\ntlsValue variable 114\nToAcceptTcpClientObservable method 424\nToConnectClientObservable operator 429\nToFunc method 211\nToList( ) method 244\nToThumbnailCurried function 310\nToThumbnail function 309\nTPL Dataflow 365–393\nadvantages of 366–367\nblocks 367–372\ncompleting 389–390completing work with ActionBlock  \n<TInput > 370–371\nlinking 372–389, 372–390\npropagating 389–390transforming data with TransformBlock  \n<TInput, TOutput> 369–370\nusing BufferBlock<TInput> as FIFO \nbuffer 368–369\nenabling agent models in C# using 374–382\nagent interaction 378–382\nagent state and messages 377–378\nimplementing Producer/Consumer patterns \nwith 372–374\nmultiple Producer/single Consumer \npattern 372–374\nsingle Producer/multiple Consumer \npattern 374\nintegrating Rx with 391–393\nrules for building workflows 390–391\nTPL (Task Parallel Library) 19\nTradingCoordinator object 459\ntransformation function 171\ntransformations, fmap functor patterns to \napply 205–206\nTransformBlock<TInput, TOutput>transforming \ndata with 369–370Transmission Control Protocol (TCP) 422\ntree structure 86\ntry-catch blocks 277\nTryCatch function 292\nTryReceive function 355\ntuples, in F# 502\ntweetEmotionObservable 178Tweetinvi library 166\nTwitter, emotion analysis 164–179\ntype classes 258\nU\nunbounded parallelism, asynchronous \nprogramming with 219–220\nun-currying 492–493underscore character 504\nunit types 191–193\nUoM (units of measure), in F# 510\nupcast function 509\n(upcast) operator 509\nupdateAccumulatorFunc function 136\nUpdateCentroids function 130, 137, 145\nUpdateFactory function 355\nusing keyword 488\nUsing operation 259\nV\nvariable 24void-returning method 192\nvoids, in C# 191–193\nvolatile keyword 66\nW\nweak reference 47\nWeakReference type 47\nWebCrawler function 43, 45, 91\nweb crawlers, memoization for 43–46\nwhile...do expression 507\nwhile loop 101\nwildcard 504Windows.Forms.DataVisualization control 231\nWindows Presentation Foundation (WPF) 230\nWordsCounter method 121\nworkflows\nasynchronous, extending 261–262\n \n534\nindex\nparallelizing with group coordination of \nagents 347–349\nTPL Dataflow, rules for building 390–391\nWPF (Windows Presentation Foundation) 230\nX\nXAML (Extensible Application Markup \nLanguage) 462Y\nyahooSourceUrl function 243\nZ\nZip operator 427workflows (continued)\n \nGlossary\nAsynchronicity —When a program performs requests that don’t complete immediately \nbut that are fulfilled later, and where the program issuing the request must do meaning-ful work in the meantime.Concurrency —The notion of multiple things happening at the same time. Usually, con-\ncurrent programs have multiple threads of execution, each typically executing differ -\nent code.Parallelism —The state of a program when more than one thread runs simultaneously to \nspeed up the program’s execution.Process —A standard operating system process. Each instance of the .NET CLR runs in \nits own process. Processes are typically independent.Thread —The smallest sequence of programmed instructions that the OS can manage \nindependently. Each .NET process has many threads running within the one process and sharing the same heap.\nSelecting the right concurrent pattern\nApplication characteristic Concurrent pattern\nYou have a sequential loop where each iteration runs an independent operation.Use the Parallel Loop pattern to run autonomous opera -\ntions simultaneously (chapter 3).\nYou write an algorithm that divides the problem domain dynamically at runtime.Use dynamic task parallelism, which uses a Divide and Conquer technique to spawn new tasks on demand (chapter 4).\nYou have to parallelize the execution of a distinct set of operations without dependencies and aggregate the result.Use the Fork/Join pattern to run in parallel a set of tasks that permit you to reduce the results of all the operations when completed (chapter 4).\nYou need to parallelize the execution of a dis-tinct set of operations where order of execution depends on dataflow constraints.Use the Task Graph pattern to make the dataflow dependencies between tasks clear (chapter 13).\nYou have to analyze and accumulate a result for a large data set by performing operations such as filtering, grouping, and aggregating.Use the MapReduce pattern to parallelize the process -\ning in a different and independent step of a massive volume of data in a timely manner (chapter 5).\nYou need to aggregate a large data set by apply -\ning a common operation.Use the Parallel Aggregation, or Reducer, pattern to merge partial results (chapter 5).\nYou implement a program that repetitively per -\nforms a series of independent operations con -\nnected as a chain.Use the Pipeline pattern to run in parallel a set of oper -\nations that are connected by queues, preserving the order of inputs (chapters 7 and 12).\nYou have multiple processes running inde -\npendently for which work must be synchronized.Use the Producer/Consumer pattern to safely share a common buffer. This buffer is used by the producer to queue the generated data in a thread-safe manner; the data is then picked up by the consumer to perform some operation (chapters 8 and 13).\n \nRiccardo Terrell \nUnlock the incredible performance built into your multi-processor machines. Concurrent applications run faster because they spread work across processor cores, \nperforming several tasks at the same time. Modern tools and techniques on the .\nNET  platform, including parallel LINQ , \nfunctional programming, asynchronous programming, and the Task Parallel Library, offer powerful alternatives to traditional thread-based concurrency.\nConcurrency in .NET  teaches you to write code that delivers the \nspeed you need for performance-sensitive applications. Featur-ing examples in both \nC# and F#, this book guides you through \nconcurrent and parallel designs that emphasize functional programming in theory and practice. You’ll start with the foundations of concurrency and master essential techniques and design practices to optimize code running on modern multiprocessor systems. \nWhat’s Inside\n● The most important concurrency abstractions\n● Employing the agent programming model\n● Implementing real-time event-stream processing\n● Executing unbounded asynchronous operations\n● Best concurrent practices and patterns that apply \n   to all platforms\nFor readers skilled with C# or F#.\nRiccardo Terrell  is a seasoned . NET  software engineer, senior \nsoftware architect, and Microsoft MVP  who is passionate \nabout functional programming.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners of this book should visit manning.com/books/concurrency-in-dot-net\n$59.99 / Can $79.99  \n[INCLUDING eBOOK]\nConcurrency in .NET.NET PROGRAMMING\nMANNING“A complementary source \nof knowledge about modern \nconcurrent functional \nprogramming on the . NET\nplatform—an absolute \nmust-read.” \n—Pawel Klimczyk, Microsoft MVP\n“Not just for those cutting \ncode on Windows. You can \nuse the gold dust in this \n book on any platform!” \n—Kevin Orr, Sumus Solutions\n“Presents real-world problems \nand offers different kinds of \nconcurrency to solve them.”—Andy Kirsch, Rally Health \n“Easiest entry into \nconcurrency I’ve \n  come across so far!” \n—Anton Herzog\nAFMG TechnologiesSee first page",51076
