filename,title,text,len
01-Cover.pdf,01-Cover,"MACHINE LEARNING“A must read for \npractitioners aspiring \nto build real-world \nsystems, not just  \ntrain models.”\n—Jacopo Tagliabue\nCo-founder of Bauplan,  \nco-creator of RecList and evalRS\n“This book will sit on \nmy bookshelf as the \nstandard reference on \nthis topic from now on.”\n—Will Kurt\nAI engineer and author of  \nBayesian Statistics the Fun Way and  \nGet Programming with HaskellBuilding Recommendation  \nSystems in Python and JAX\nTwitter: @oreillymedia\nlinkedin.com/company/oreilly-media\nyoutube.com/oreillymedia Implementing and designing systems that make suggestions \nto users are among the most popular and essential machine \nlearning applications available. Whether you want customers \nto find the most appealing items at your online store, videos \nto enrich and entertain them, or news they need to know, \nrecommendation systems (RecSys) provide the way. \nIn this practical book, authors Bryan Bischof and Hector Yee  \nillustrate core concepts and provide examples to help you  \ncreate a RecSys for any industry or scale. You’ll learn the  \nmath, ideas, and implementation details you need to succeed.  \nThis book includes the RecSys platform components,  \nrelevant MLOps tools in your stack, plus code examples and \nhelpful suggestions in PySpark, SparkSQL, FastAPI,  \nand Weights & Biases. \nYou’ll learn:\n• The data essential for building a RecSys\n• How to frame your data and business as a RecSys problem\n• Ways to evaluate models appropriate for your system\n• Methods to implement, train, test, and deploy the model  \nyou choose\n• Metrics you need to track to ensure your system is  \nworking as planned\n• How to improve your system as you learn more about  \nyour users, products, and business caseBryan Bischof  leads AI at Hex,  \nand is an adjunct professor in the \nRutgers Masters of Business and \nAnalytics program where he teaches  \nData Science. Previously, he was  \nthe Head of Data Science at Weights \nand Biases, where he built the DS, \nML, and Data Engineering teams. \nHector Yee  is a staff software \nengineer at Google who’s worked  \non projects that include the  \nfirst content-based ranker on  \nimage search and self-driving  \ncar perception. \n9781492 09799057999US $79.99  CAN $99.99\nISBN: 978-1-492-09799-0\nPraise for Building Recommendation Systems in\nPython and JAX\nBryan and Hector have created something special here, introducing concepts that\ntake most people years to learn within the RecSys domain and then providing\nclear code examples that put them into practice. I wish I had this book when I\nstarted out on my RecSys journey.\n—Even Oldridge, Director of Engineering,\nRecommender Systems, NVIDIA\nThis is a book I’ve been waiting for, making recommendation systems accessible using\nJAX. The only other thing you need is your laptop.\n—Shaked Zychlinski, former Head of\nRecommendations at Lightricks\nBryan and Hector have distilled decades of recommendation system advancements into a\nconcise, yet practical guide. Bridging the gap between theory and application, this book\nis packed with easy-to-understand Python and JAX examples. This is an indispensable\nguide for RecSys practitioners at all levels, from novices to experts.\n—Eugene Yan, Applied Scientist, Amazon\nThis book takes a holistic approach to building recommender systems, synthesizing\nmath, code, systems design, and business application. It covers all the nuances\nthat practitioners need to consider to implement real-world solutions. The intuitive\nexamples using publicly available datasets enables the reader to turn abstract\nconcepts into concrete learnings.\n—Eric Colson, AI Advisor, Former Chief Algorithms Officer  at\nStitch Fix, Former VP of Data Science and Engineering at Netflix\nRecommender systems are among the most impactful ML systems ever deployed.\nThis book brilliantly navigates the balance between principled modeling, clear code\nexamples, and architectural best practices. A must read for practitioners aspiring to\nbuild real-world systems, not just train models.\n—Jacopo Tagliabue, Co-founder of Bauplan, Adjunct Professor\nof ML Systems at NYU, Co-creator of RecList and evalRS\nFor years I’ve found there is a tremendous gap between recommendation systems as\ndescribed in texts and as practiced in the field. Y ee and Bischof ’s excellent Building\nRecommendation Systems in Python and JAX  closes this gap and will make readers finally\nfeel initiated into this vital area of data science.\n—Will Kurt, AI Engineer and author of Bayesian Statistics the\nFun Way  and Get Programming with Haskell\nThis book is an essential resource for anyone interested in the information retrieval (IR)\nspace. The authors take special care to do the incredibly important and nuanced work of\npreparing the reader to solve problems in this space. With this book as a reference, you\nwill be able to think through how to set up the IR problem, think through the practical\nsteps to take, and then get building.\n—Eric Schles, Research Scientist, Johns Hopkins University\nBryan Bischof and Hector YeeBuilding Recommendation\nSystems in Python and JAX\nHands-on Production Systems at Scale\nBoston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing",5271
02-Table of Contents.pdf,02-Table of Contents,"978-1-492-09799-0\n[LSI]Building Recommendation Systems in Python and JAX\nby Bryan Bischof and Hector Y ee\nCopyright © 2024 Bryan Bischof and Resonant Intelligence LLC. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Nicole Butterfield\nDevelopment Editor:  Jill Leonard\nProduction Editor:  Aleeya Rahman\nCopyeditor:  Sharon Wilkey\nProofreader:  Piper Editorial Consulting, LLCIndexer:  Judith McConville\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nDecember 2023:  First Edition\nRevision History for the First Edition\n2023-12-04: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492097990  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Building Recommendation Systems in\nPython and JAX , the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\nPart I. Warming Up\n1.Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nKey Components of a Recommendation System                                                        4\nCollector                                                                                                                        4\nRanker                                                                                                                            4\nServer                                                                                                                             4\nSimplest Possible Recommenders                                                                                 5\nThe Trivial Recommender                                                                                          5\nMost-Popular-Item Recommender                                                                           6\nA Gentle Introduction to JAX                                                                                        7\nBasic Types, Initialization, and Immutability                                                           7\nIndexing and Slicing                                                                                                    9\nBroadcasting                                                                                                                  9\nRandom Numbers                                                                                                      10\nJust-in-Time Compilation                                                                                         11\nSummary                                                                                                                         12\n2.User-Item Ratings and Framing the Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13\nThe User-Item Matrix                                                                                                    13\nUser-User Versus Item-Item Collaborative Filtering                                                16\nThe Netflix Challenge                                                                                                   17\nSoft Ratings                                                                                                                     19\nData Collection and User Logging                                                                              19\nWhat to Log                                                                                                                20\nv\nCollection and Instrumentation                                                                               23\nFunnels                                                                                                                         24\nBusiness Insight and What People Like                                                                      26\nSummary                                                                                                                         27\n3.Mathematical Considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  29\nZipf ’s Laws in RecSys and the Matthew Effect                                                          29\nSparsity                                                                                                                            32\nUser Similarity for Collaborative Filtering                                                                 34\nPearson Correlation                                                                                                   35\nRatings via Similarity                                                                                                 36\nExplore-Exploit as a Recommendation System                                                        37\nϵ-greedy                                                                                                                       38\nWhat Should ϵ Be?                                                                                                     39\nThe NLP-RecSys Relationship                                                                                     40\nVector Search                                                                                                              40\nNearest-Neighbors Search                                                                                        42\nSummary                                                                                                                         42\n4.System Design for Recommending. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\nOnline Versus Offline                                                                                                    43\nCollector                                                                                                                          44\nOffline Collector                                                                                                         44\nOnline Collector                                                                                                         45\nRanker                                                                                                                             45\nOffline Ranker                                                                                                            46\nOnline Ranker                                                                                                            46\nServer                                                                                                                               46\nOffline Server                                                                                                              47\nOnline Server                                                                                                              47\nSummary                                                                                                                         47\n5.Putting It All Together: Content-Based Recommender. . . . . . . . . . . . . . . . . . . . . . . . . .  49\nRevision Control Software                                                                                            50\nPython Build Systems                                                                                                    51\nRandom-Item Recommender                                                                                      52\nObtaining the STL Dataset Images                                                                              54\nConvolutional Neural Network Definition                                                                55\nModel Training in JAX, Flax, and Optax                                                                    56\nInput Pipeline                                                                                                                 58\nSummary                                                                                                                         70\nvi | Table of Contents\nPart II. Retrieval\n6.Data Processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  73\nHydrating Y our System                                                                                                 73\nPySpark                                                                                                                        73\nExample: User Similarity in PySpark                                                                       77\nDataLoaders                                                                                                                82\nDatabase Snapshots                                                                                                    84\nData Structures for Learning and Inference                                                              85\nVector Search                                                                                                              86\nApproximate Nearest Neighbors                                                                             87\nBloom Filters                                                                                                               88\nFun Aside: Bloom Filters as the Recommendation System                                 89\nFeature Stores                                                                                                              90\nSummary                                                                                                                         94\n7.Serving Models and Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95\nArchitectures by Recommendation Structure                                                           95\nItem-to-User Recommendations                                                                             96\nQuery-Based Recommendations                                                                             96\nContext-Based Recommendations                                                                          98\nSequence-Based Recommendations                                                                        99\nWhy Bother with Extra Features?                                                                            99\nEncoder Architectures and Cold Starting                                                                100\nDeployment                                                                                                                  103\nModels as APIs                                                                                                         103\nSpinning Up a Model Service                                                                                 104\nWorkflow Orchestration                                                                                         105\nAlerting and Monitoring                                                                                            108\nSchemas and Priors                                                                                                  108\nIntegration Tests                                                                                                       109\nObservability                                                                                                             110\nEvaluation in Production                                                                                            111\nSlow Feedback                                                                                                          111\nModel Metrics                                                                                                           112\nContinuous Training and Deployment                                                                    113\nModel Drift                                                                                                               113\nDeployment Topologies                                                                                          114\nThe Evaluation Flywheel                                                                                             117\nDaily Warm Starts                                                                                                    117\nLambda Architecture and Orchestration                                                             118\nLogging                                                                                                                      119\nTable of Contents | vii\nActive Learning                                                                                                        122\nSummary                                                                                                                       125\n8.Putting It All Together: Data Processing and Counting Recommender. . . . . . . . . . . .  127\nTech Stack                                                                                                                     128\nData Representation                                                                                                    129\nBig Data Frameworks                                                                                                  130\nCluster Frameworks                                                                                                132\nPySpark Example                                                                                                     132\nGloVE Model Definition                                                                                            142\nGloVE Model Specification in JAX and Flax                                                       143\nGloVE Model Training with Optax                                                                       145\nSummary                                                                                                                   146\nPart III. Ranking\n9.Feature-Based and Counting-Based Recommendations. . . . . . . . . . . . . . . . . . . . . . . .  149\nBilinear Factor Models (Metric Learning)                                                               149\nFeature-Based Warm Starting                                                                                    153\nSegmentation Models and Hybrids                                                                           155\nTag-Based Recommenders                                                                                      155\nHybridization                                                                                                            157\nLimitations of Bilinear Models                                                                                  157\nCounting Recommenders                                                                                           159\nReturn to the Most-Popular-Item Recommender                                               159\nCorrelation Mining                                                                                                  160\nPointwise Mutual Information via Co-occurrences                                            162\nSimilarity from Co-occurrence                                                                              163\nSimilarity-Based Recommendations                                                                     164\nSummary                                                                                                                       165\n10. Low-Rank Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\nLatent Spaces                                                                                                                167\nDot Product Similarity                                                                                                169\nCo-occurrence Models                                                                                                171\nReducing the Rank of a Recommender Problem                                                    172\nOptimizing for MF with ALS                                                                                 174\nRegularization for MF                                                                                             176\nRegularized MF Implementation                                                                           177\nWSABIE                                                                                                                     199\nDimension Reduction                                                                                                 199\nviii | Table of Contents\nIsometric Embeddings                                                                                             203\nNonlinear Locally Metrizable Embeddings                                                          204\nCentered Kernel Alignment                                                                                    206\nAffinity and p-sale                                                                                                       206\nPropensity Weighting for Recommendation System Evaluation                          208\nPropensity                                                                                                                 209\nSimpson’s and Mitigating Confounding                                                               210\nSummary                                                                                                                       212\n11. Personalized Recommendation Metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  213\nEnvironments                                                                                                               214\nOnline and Offline                                                                                                   214\nUser Versus Item Metrics                                                                                        215\nA/B Testing                                                                                                                215\nRecall and Precision                                                                                                    216\n@ k                                                                                                                              218\nPrecision at k                                                                                                             219\nRecall at k                                                                                                                  219\nR-precision                                                                                                                220\nmAP , MMR, NDCG                                                                                                    220\nmAP                                                                                                                           220\nMRR                                                                                                                           221\nNDCG                                                                                                                        222\nmAP Versus NDCG?                                                                                               223\nCorrelation Coefficients                                                                                          223\nRMSE from Affinity                                                                                                    224\nIntegral Forms: AUC and cAUC                                                                               224\nRecommendation Probabilities to AUC-ROC                                                    225\nComparison to Other Metrics                                                                                225\nBPR                                                                                                                                226\nSummary                                                                                                                       227\n12. Training for Ranking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  229\nWhere Does Ranking Fit in Recommender Systems?                                            229\nLearning to Rank                                                                                                         230\nTraining an LTR Model                                                                                               231\nClassification for Ranking                                                                                      231\nRegression for Ranking                                                                                           231\nClassification and Regression for Ranking                                                           232\nW ARP                                                                                                                            233\nk-order Statistic                                                                                                            234\nBM25                                                                                                                             236\nTable of Contents | ix\nMultimodal Retrieval                                                                                                  238\nSummary                                                                                                                       238\n13. Putting It All Together: Experimenting and Ranking. . . . . . . . . . . . . . . . . . . . . . . . . . .  241\nExperimentation Tips                                                                                                  241\nKeep It Simple                                                                                                           242\nDebug Print Statements                                                                                          242\nDefer Optimization                                                                                                  243\nKeep Track of Changes                                                                                            244\nUse Feature Engineering                                                                                         244\nUnderstand Metrics Versus Business Metrics                                                      245\nPerform Rapid Iteration                                                                                          245\nSpotify Million Playlist Dataset                                                                                 246\nBuilding URI Dictionaries                                                                                      248\nBuilding the Training Data                                                                                     249\nReading the Input                                                                                                     252\nModeling the Problem                                                                                             254\nFraming the Loss Function                                                                                     257\nExercises                                                                                                                        261\nSummary                                                                                                                       262\nPart IV. Serving\n14. Business Logic. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  265\nHard Ranking                                                                                                               266\nLearned Avoids                                                                                                            266\nHand-Tuned Weights                                                                                                  267\nInventory Health                                                                                                          268\nImplementing Avoids                                                                                                  269\nModel-Based Avoids                                                                                                    271\nSummary                                                                                                                       272\n15. Bias in Recommendation Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  273\nDiversification of Recommendations                                                                       274\nImproving Diversity                                                                                                274\nApplying Portfolio Optimization                                                                           276\nMultiobjective Functions                                                                                            277\nPredicate Pushdown                                                                                                    278\nFairness                                                                                                                          279\nSummary                                                                                                                       280\nx | Table of Contents\n16. Acceleration Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  281\nSharding                                                                                                                        282\nLocality Sensitive Hashing                                                                                          282\nk-d Trees                                                                                                                        284\nHierarchical k-means                                                                                                  288\nCheaper Retrieval Methods                                                                                        290\nSummary                                                                                                                       290\nPart V. The Future of Recs\n17. Sequential Recommenders. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  293\nMarkov Chains                                                                                                             294\nOrder-Two Markov Chain                                                                                      295\nOther Markov Models                                                                                             295\nRNN and CNN Architectures                                                                                    297\nAttention Architectures                                                                                              299\nSelf-Attentive Sequential Recommendation                                                        300\nBERT4Rec                                                                                                                 300\nRecency Sampling                                                                                                    301\nMerging Static and Sequential                                                                                301\nSummary                                                                                                                       303\n18. What’s Next for Recs?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nMultimodal Recommendations                                                                                 306\nGraph-Based Recommenders                                                                                    307\nNeural Message Passing                                                                                          308\nApplications                                                                                                              310\nRandom Walks                                                                                                         311\nMetapath and Heterogeneity                                                                                  313\nLLM Applications                                                                                                        313\nLLM Recommenders                                                                                               314\nLLM Training                                                                                                            314\nInstruct Tuning for Recommendations                                                                317\nLLM Rankers                                                                                                            317\nRecommendations for AI                                                                                       318\nSummary                                                                                                                       319\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  321\nTable of Contents | xi",31835
03-Preface.pdf,03-Preface,"Preface\nHow did you come to find this book? Did you see an ad for it on a website? Maybe\na friend or mentor suggested it; or perhaps you saw a post on social media that\nreferenced it. Could it be that you found it sitting on a shelf in a bookstore—a\nbookstore that your trusty maps app led you to? However you came to find it, you’ve\nalmost certainly come to this book via a recommendation system.\nImplementing  and designing systems that provide suggestions to users is among\nthe most popular and most essential applications of machine learning (ML) to any\nbusiness. Whether you want to help your users find the best clothing to match their\ntastes, the most appealing items to buy from an online store, videos to enrich and\nentertain them, maximally engaging content to surface from their networks, or the\nnews highlights they need to know on that day, recommendation systems provide\nthe way.\nModern recommendation system designs are as diverse as the domains they serve.\nThese systems consist of the computer software architectures to implement and\nexecute product goals, in addition to the algorithmic components of ranking. Meth‐\nods for ranking recommendations can come from traditional statistical learning\nalgorithms, linear-algebraic inspirations, geometric considerations, and, of course,\ngradient-based methods. Just as the algorithmic methods are diverse, so too are the\nmodeling and evaluation considerations for recommending: personalized ranking,\nsearch recommendations, sequence modeling, and the scoring for all of these are now\nneed-to-know for the ML engineer working with recommendation systems.\nThe abbreviation RecSys is often used by practitioners to describe\nthe field of recommendation systems. Therefore, in this book, we\nuse RecSys when referring to the field, and recommendation sys‐\ntem when referring to what we build.\nxiii\n1Some may quibble that Apple also has core recommendation systems at the heart of its company. While it’s\ncertainly true that the App Store forms a crucial strategic product for the company, we remain conservative\nin our four-out-of-five assessment and say that recommendation systems are not Apple’s primary revenue-\ngenerating capability.If you’re an ML practitioner, you are probably aware of recommendation systems,\nand you may know one or two of the simplest modeling approaches and be able\nto speak intelligently about the relevant data structures and model architectures;\nhowever, RecSys frequently falls outside the core curriculum of data science and ML.\nMany senior data scientists with years of experience in the industry know little about\nactually building a recommendation system and may feel intimidated when the topic\ncomes up. Despite drawing on similar foundations and skills as other ML problems,\nRecSys has a vibrant community with a fast-moving focus that can make it easy to\nrelegate building recommendation systems to other  data scientists who have already\ninvested the time, or are willing to stay on top of the latest information.\nThe reason this book exists, is to break through those perceived barriers. Understand‐\ning recommendation systems at a practical level is not only useful for business cases\nrequiring content to be served to users, but the underlying ideas of RecSys often\nbridge gaps between an incredibly diverse set of other types of ML. Take, for exam‐\nple, an article recommendation system that may utilize natural language processing\n(NLP) to find representations of the articles, sequential modeling to promote longer\nengagement, and contextual components to allow user queries to guide results. If\nyou’re approaching the field from a purely academic interest, no matter what aspects\nof mathematics you’re interested in, sooner or later, there appears a link or applica‐\ntion in RecSys!\nFinally, if connections to other fields, applications of nearly all of mathematics,\nor the obvious business utility aren’t  enough to get you interested in RecSys, the\nstunning cutting-edge technology might: RecSys is at and beyond the forefront of\nML at all times. One benefit of having obvious revenue impact is that companies\nand practitioners need to always be pushing the boundaries of what is possible and\nhow they go about it. The most advanced deep learning architectures and best code\ninfrastructures are brought to bear on this field. That’s hardly a surprise when you\nconsider that at the heart of four of the five letters in FAANG—which stands for\nMeta (formerly Facebook), Apple, Amazon, Netflix, and Google—lies one or many\nrecommendation systems.1\nAs a practitioner, you’ll need to understand how to do the following:\n•Take your data and business problem and frame it as a RecSys problem•\n•Identify the essential data to get started building a RecSys•\nxiv | Preface",4828
04-OReilly Online Learning.pdf,04-OReilly Online Learning,"•Determine the appropriate models for your RecSys problem and how should you•\nevaluate them.\n•Implement, train, test, and deploy the aforementioned models•\n•Track metrics to ensure that your system is working as planned•\n•Incrementally improve your system as you learn more about your users, prod‐•\nucts, and business case\nThis book illustrates the core concepts and examples necessary to complete these\nsteps, whatever the industry or scale. We’ll guide you through the math, ideas, and\nimplementation details for building recommendation systems—whether it’s your first\nor your fiftieth. We’ll show you how to build these systems with Python and JAX.\nIf you’re not yet familiar, JAX is a Python framework from Google that seeks to\nmake autodifferentiation and functional programming paradigms first-class objects.\nAdditionally, it uses a NumPy API style especially convenient for ML practitioners\nfrom a variety of backgrounds.\nWe will show code examples and architecture models that capture the essential\nconcepts necessary and provide the way to scale these systems to production applica‐\ntions.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nPreface | xv\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nThe included code snippets reference notebooks that will run on moderate-size and,\nin most cases, free resources. To facilitate easy experimentation and exploration we\nprovide the code via Google Colab notebooks.\nSupplemental material (code examples, exercises, etc.) is available for download at\nESRecsys on GitHub .\nIf you have a technical question or a problem using the code examples, please send\nemail to bookquestions@oreilly.com .\nThis book is here to help you get your job done. In general, if example code is\noffered with this book, you may use it in your programs and documentation. Y ou\ndo not need to contact us for permission unless you’re reproducing a significant\nportion of the code. For example, writing a program that uses several chunks of code\nfrom this book does not require permission. Selling or distributing examples from\nO’Reilly books does require permission. Answering a question by citing this book\nand quoting example code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation does\nrequire permission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “ Building Recommenda‐\ntion Systems in Python and JAX  by Bryan Bischof and Hector Y ee. Copyright 2024\nBryan Bischof and Resonant Intelligence LLC, 978-1-492-09799-0. ”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com .\nxvi | Preface",3492
05-How to Contact Us.pdf,05-How to Contact Us,,0
06-Acknowledgments.pdf,06-Acknowledgments,"O’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit http://oreilly.com .\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-889-8969 (in the United States or Canada)\n707-829-7019 (international or local)\n707-829-0104 (fax)\nsupport@oreilly.com\nhttps://www.oreilly.com/about/contact.html\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/build_rec_sys_python_jax .\nFor news and information about our books and courses, visit https://oreilly.com .\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\nFollow us on Twitter: https://twitter.com/oreillymedia\nWatch us on Y ouTube: https://youtube.com/oreillymedia\nAcknowledgments\nHector would like to thank his husband, Donald, for his loving support during the\nwriting of this book and for the snacks his sister Serena sends all the time. He would\nalso like to dedicate this book to his relatives who have passed. A big thank you\ngoes to the Google reviewers Ed Chi, Courtney Hohne, Sally Goldman, Richa Nigam,\nMingliang Jiang, and Anselm Levskaya. Thanks to Bryan Hughes for reviewing the\nWikipedia code.\nPreface | xvii\nBryan would like to thank his colleagues from Stitch Fix, where he learned many\nof the key ideas in this book—in particular, Ian Horn’s patient guidance on transfer\nlearning, Dr. Molly Davies’s mentorship on experimentation and effect estimates,\nMark Weiss’s deep partnership on understanding the relationship between availability\nand recommendations, Dr. Reza Sohrabi’s introduction to transformers, Dr. Xi Chen’s\nencouragement on GNNs for recs, and Dr. Leland McInnes for his careful advice\non dimension reduction and approximate nearest neighbors. Bryan benefitted a lot\nfrom conversations with Dr. Natalia Gardiol, Dr. Daniel Fleischman, Dr. Andrew Ho,\nJason Liu, Dr. Dan Marthaler, Dr. Chris Moody, Oz Raza, Dr. Anna Schneider, Ujjwal\nSarin, Agnieszka Szefer, Dr. Daniel Tasse, Diyang Tang, Zach Winston, and others\nhe has almost certainly forgotten. Outside of his incredible Stitch Fix colleagues,\nhe especially wants to thank Dr. Eric Bunch, Dr. Lee Goerl, Dr. Will Chernoff, Leo\nRosenberg, and Janu Verma for collaboration over the years. Dr. Brian Amadio as\nan excellent colleague and originally suggested that he write this book. Dr. Even\nOldridge for encouraging him to actually try it. Eugene Y an and Karl Higley—neither\nof whom he’s met but has been significantly inspired by. He’ d like to thank Dr.\nZhongzhu Lin and Dr. Alexander Rosenberg, who both had formative impacts on\nhis career. Cianna Salvatora, who assisted in early literature review, and Valentina\nBesprozvannykh, who greatly assisted in reading early draft notes and providing\nguidance.\nBoth authors thank Tobias Zwingmann, Ted Dunning, Vicki Boykis, Eric Schles,\nShaked Zychlinski, and Will Kurt, who spend much time giving careful technical\nfeedback on book manuscripts—without which this book would have been incom‐\nprehensible. Rebecca Novack, who harangued us into signing up for this project. And\nJill Leonard, who removed nearly 100 erroneous instances of the word utilize  from\nthe manuscript, and who offered an incredible amount of patient partnership on the\nbook text.\nxviii | Preface",3935
07-Part I. Warming Up.pdf,07-Part I. Warming Up,"PART I\nWarming Up\nHow do we get all the data in the right place to train a recommendation system, and for\nreal-time inference?\nSo, you’ve decided to dive into the world of recommendation systems! Are you\nhoping to suggest just the right thing based on users’ quirky preferences across a vast\nsea of choices? If so, you’ve set quite the challenge for yourself! On the surface, these\nsystems might seem straightforward: if User A and User B have similar tastes, then\nmaybe what A likes, B will too. But, as with all things that seem simple, there’s a depth\nthat’s waiting to be explored.\nHow do we capture the essence of a user’s history and feed it into a model? Where do\nwe stash this model so it’s ready to serve up suggestions on the fly? And how do we\nmake sure it doesn’t suggest something that steps out of bounds or goes against the\nbusiness rulebook? Collaborative filtering is our starting point, a guiding light. But\nthere’s an entire universe beyond it that makes these systems tick, and together, we’re\ngoing to navigate it.",1050
08-Key Components of a Recommendation System.pdf,08-Key Components of a Recommendation System,"CHAPTER 1\nIntroduction\nRecommendation systems are integral to the development of the internet that we\nknow today and are a central function of emerging technology companies. Beyond\nthe search ranking that opened the web’s breadth to everyone, the new and exciting\nmovies all your friends are watching, or the most relevant ads that companies pay top\ndollar to show you lie more applications of recommendation systems every year. The\naddictive For Y ou page from TikTok, the Discover Weekly playlist by Spotify, board\nsuggestions on Pinterest, and Apple’s App Store are all hot technologies enabled by\nthe recommendation systems. These days, sequential transformer models, multimo‐\ndal representations, and graph neural nets are among the brightest areas of R&D in\nmachine learning (ML)—all being put to use in recommendation systems.\nUbiquity of any technology often prompts questions of how the technology works,\nwhy it has become so common, and if we can get in on the action. For recommenda‐\ntion systems, the how is quite complicated. We’ll need to understand the geometry of\ntaste, and how only a little bit of interaction from a user can provide us a GPS signal\nin that abstract space. Y ou’ll see how to quickly gather a great set of candidates and\nhow to refine them to a cohesive set of recommendations. Finally, you’ll learn how to\nevaluate your recommender, build the endpoint that serves inference, and log about\nits behavior.\nWe will formulate variants of the core problem to be solved by recommendation\nsystems but, ultimately, the motivating problem framing is as follows:\nGiven a collection of things that may be recommended, choose an ordered few for the\ncurrent context and user that best match according to a certain objective.\n3",1772
09-Collector.pdf,09-Collector,,0
10-Ranker.pdf,10-Ranker,,0
11-Simplest Possible Recommenders.pdf,11-Simplest Possible Recommenders,"Key Components of a Recommendation System\nAs we increase complexity and sophistication, let’s keep in mind the components of\nour system. We will use string diagrams  to keep track of our components, but in the\nliterature these diagrams are presented in a variety of ways.\nWe will identify and build on three core components of recommendation systems: the\ncollector, ranker, and server.\nCollector\nThe collector’s role is to know what is in the collection of things that may be rec‐\nommended, and the necessary features or attributes of those things. Note that this\ncollection is often a subset based on context or state.\nRanker\nThe ranker’s role is to take the collection provided by the collector and order some or\nall of its elements, according to a model for the context and user.\nServer\nThe server’s role is to take the ordered subset provided by the ranker, ensure that the\nnecessary data schema is satisfied—including essential business logic—and return the\nrequested number of recommendations.\nTake, for example, a hospitality scenario with a waiter:\nWhen you sit down at your table, you look at the menu, unsure of what you should\norder. Y ou ask the waiter, “What do you think I should order for dessert?”\nThe waiter checks their notes and says, “We’re out of the key lime pie, but people really\nlike our banana cream pie. If you like pomegranate, we make pom ice cream from\nscratch; and it’s hard to go wrong with the donut a la mode—it’s our most popular\ndessert. ”\nIn this short exchange, the waiter first serves as a collector: identifying the desserts on\nthe menu, accommodating current inventory conditions, and preparing to talk about\nthe characteristics of the desserts by checking their notes.\nNext, the waiter serves as a ranker; they mention items high scoring in popularity\n(banana cream pie and donut a la mode) as well as a contextually high match item\nbased on the patron’s features (if they like pomegranate).\nFinally, the waiter serves the recommendations verbally, including both explanatory\nfeatures of their algorithm and multiple choices.\n4 | Chapter 1: Introduction",2127
12-A Gentle Introduction to JAX.pdf,12-A Gentle Introduction to JAX,"While this seems a bit cartoonish, remember to ground discussions of recommenda‐\ntion systems in real-world applications. One of the advantages of working in RecSys\nis that inspiration is always nearby.\nSimplest Possible Recommenders\nWe’ve established the components of a recommender, but to really make this practi‐\ncal, we need to see this in action. While much of the book is dedicated to practical\nrecommendation systems, first we’ll start with a toy and scaffold from there.\nThe Trivial Recommender\nThe absolute simplest recommender is not very interesting but can still be demon‐\nstrated in the framework. It’s called the trivial recommender  (TR) because it contains\nvirtually no logic:\ndef get_trivial_recs () -> Optional [List[str]]:\n   item_id = random.randint(0, MAX_ITEM_INDEX )\n   if get_availability (item_id):\n       return [item_id]\n   return None\nNotice that this recommender may return either a specific item_id  or None . Also\nobserve that this recommender takes no arguments, and MAX_ITEM_INDEX  is referenc‐\ning a variable out of scope. Software principles ignored, let’s think about the three\ncomponents:\nCollector\nA random item_id  is generated. The TR collects by checking the availability\nof item_id . We could argue that having access to item_id  is also part of the\ncollector’s responsibility. Conditional upon the availability, the collection of rec‐\nommendable things is either [item_id]  or None  (recall that None  is a collection in\nthe set-theoretic sense ).\nRanker\nThe TR ranks with a no-op; i.e., the ranking of 1 or 0 objects in a collection is the\nidentity function on that collection, so we merely do nothing and move on to the\nnext step.\nServer\nThe TR serves recommendations by its return  statements. The only schema\nthat’s been specified in this example is that the return type is Optional\n[List[str]] .\nSimplest Possible Recommenders | 5\nThis recommender, which is not interesting or useful, provides a skeleton that we will\nadd to as we develop further.\nMost-Popular-Item Recommender\nThe most-popular-item recommender  (MPIR) is the simplest recommender that con‐\ntains any utility. Y ou probably won’t want to build applications around it, but it’s\nuseful in tandem with other components in addition to providing a basis for further\ndevelopment.\nAn MPIR works just as it says; it returns the most popular items:\ndef get_item_popularities () -> Optional [Dict[str, int]]:\n    ...\n        # Dict of pairs: (item-identifier, count times item chosen)\n        return item_choice_counts\n    return None\ndef get_most_popular_recs (max_num_recs : int) -> Optional [List[str]]:\n    items_popularity_dict  = get_item_popularities ()\n    if items_popularity_dict :\n        sorted_items  = sorted(\n            items_popularity_dict .items(),\n            key=lambda item: item[1]),\n            reverse=True,\n        )\n        return [i[0] for i in sorted_items ][:max_num_recs ]\n    return None\nHere we assume that get_item_popularities  has knowledge of all available items\nand the number of times they’ve been chosen.\nThis recommender attempts to return the k most popular items available. While\nsimple, this is a useful recommender that serves as a great place to start when\nbuilding a recommendation system. Additionally, we will see this example return\nover and over, because other recommenders use this core and iteratively improve the\ninternal components.\nLet’s look at the three components of our system again:\nCollector\nThe MPIR first makes a call to get_item_popularities  that—via database or\nmemory access—knows which items are available and how many times they’ve\nbeen selected. For convenience, we assume that the items are returned as a\ndictionary, with keys given by the string that identifies the item, and values\nindicating the number of times that item has been chosen. We implicitly assume\nhere that items not appearing in this list are not available.\n6 | Chapter 1: Introduction",4003
13-Basic Types Initialization and Immutability.pdf,13-Basic Types Initialization and Immutability,"Ranker\nHere we see our first simple ranker: ranking by sorting on values. Because the\ncollector has organized our data such that the values of the dictionary are the\ncounts, we use the Python built-in sorting function sorted . Note that we use key\nto indicate that we wish to sort by the second element of the tuples—in this case,\nequivalent to sorting by values—and we send the reverse  flag to make our sort\ndescending.\nServer\nFinally, we need to satisfy our API schema, which is again provided via the\nreturn type hint: Optional[List[str]] . This wants the return type to be the\nnullable list of item-identifier strings that we’re recommending, so we use a list\ncomprehension to grab the first element of the tuples. But wait! Our function\nhas this max_num_recs  field—what might that be doing there? Of course, this is\nsuggesting that our API schema is looking for no greater than max_num_recs  in\nthe response. We handle this via the slice operator, but note that our return is\nbetween 0 and max_num_recs  results.\nConsider the possibilities at your fingertips equipped with the MPIR; recommending\ncustomers’ favorite item in each top-level category could make for a simple but useful\nfirst stab at recommendations for ecommerce. The most popular video of the day\nmay make for a good home-page experience on your video site.\nA Gentle Introduction to JAX\nSince this book has JAX in the title, we will provide a gentle introduction to JAX here.\nIts official documentation can be found on the JAX website .\nJAX is a framework for writing mathematical code in Python that is just-in-time\n(JIT) compiled. JIT compilation allows the same code to run on CPUs, GPUs, and\nTPUs. This makes it easy to write performant code that takes advantage of the\nparallel-processing power of vector processors.\nAdditionally, one of the design philosophies of JAX is to support tensors and gradi‐\nents as core concepts, making it an ideal tool for ML systems that utilize gradient-\nbased learning on tensor-shaped data. The easiest way to play with JAX is probably\nvia Google Colab , which is a hosted Python notebook on the web.\nBasic Types, Initialization, and Immutability\nLet’s start by learning about JAX types. We’ll construct a small, three-dimensional\nvector in JAX and point out some differences between JAX and NumPy:\nimport jax.numpy  as jnp\nimport numpy as np\nA Gentle Introduction to JAX | 7\nx = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)\nprint(x)\n[1. 2. 3.]\nprint(x.shape)\n(3,)\nprint(x[0])\n1.0\nx[0] = 4.0\nTypeError : '<class ' jaxlib.xla_extension .ArrayImpl '>'\nobject does not support item assignment . JAX arrays are immutable .\nJAX’s interface is mostly similar to that of NumPy. We import JAX’s version of\nNumPy as jnp to distinguish it from NumPy ( np) by convention so that we know\nwhich version of a mathematical function we want to use. This is because sometimes\nwe might want to run code on a vector processor like a GPU or TPU that we can use\nJAX for, or we might prefer to run some code on a CPU in NumPy.\nThe first point to notice is that JAX arrays have types. The typical float type is\nfloat32 , which uses 32 bits to represent a floating-point number. Other types exist,\nsuch as float64 , which has greater precision, and float16 , which is a half-precision\ntype that usually only runs on some GPUs.\nThe other point to note is that JAX tensors have shape. This is usually a tuple, so (3,)\nmeans a three-dimensional vector along the first axis. A matrix has two axes, and a\ntensor has three or more axes.\nNow we come to places where JAX differs from NumPy. It is really important to pay\nattention to “JAX—The Sharp Bits”  to understand these differences. JAX’s philosophy\nis about speed and purity. By making functions pure (without side effects) and by\nmaking  data immutable, JAX is able to make some guarantees to the underlying\naccelerated linear algebra (XLA) library that it uses to talk to GPUs. JAX guarantees\nthat these functions applied to data can be run in parallel and have deterministic\nresults without side effects, and thus XLA is able to compile these functions and make\nthem run much faster than if they were run just on NumPy.\nY ou can see that modifying one element in x results in an error. JAX would prefer\nthat the array x is replaced rather than modified. One way to modify elements in an\narray is to do it in NumPy rather than JAX and convert NumPy arrays to JAX—for\nexample, using jnp.array(np_array) —when the subsequent code needs to run fast\non immutable data.\n8 | Chapter 1: Introduction",4603
14-Indexing and Slicing.pdf,14-Indexing and Slicing,,0
15-Chapter 2. User-Item Ratings and Framing the Problem.pdf,15-Chapter 2. User-Item Ratings and Framing the Problem,"Indexing and Slicing\nAnother important skill to learn is that of indexing and slicing arrays:\nx = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=jnp.int32)\n# Print the whole matrix.\nprint(x)\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n# Print the first row.\nprint(x[0])\n[1 2 3]\n# Print the last row.\nprint(x[-1])\n[7 8 9]\n# Print the second column.\nprint(x[:, 1])\n[2 5 8]\n# Print every other element\nprint(x[::2, ::2])\n[[1 3]\n [7 9]]\nNumPy introduced indexing and slicing operations that allow us to access different\nparts of an array. In general, the notation follows a start:end:stride  convention.\nThe first element indicates where to start, the second indicates where to end (but not\ninclusive), and the stride indicates the number of elements to skip over. The syntax is\nsimilar to that of the Python range  function.\nSlicing allows us to access views of a tensor elegantly. Slicing and indexing are\nimportant skills to master, especially when we start to manipulate tensors in batches,\nwhich we typically do to make the most use of acceleration hardware.\nBroadcasting\nBroadcasting  is another feature of NumPy and JAX to be aware of. When a binary\noperation such as addition or multiplication is applied to two tensors of different\nsizes, the tensor with axes of size 1 is lifted up in rank to match that of the larger-sized\ntensor. For example, if a tensor of shape (3,3)  is multiplied by a tensor of shape\n(3,1) , the rows of the second tensor are duplicated before the operation so that it\nlooks like a tensor of shape (3,3) :\nA Gentle Introduction to JAX | 9\nx = jnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=jnp.int32)\n# Scalar broadcasting.\ny = 2 * x\nprint(y)\n[[ 2  4  6]\n [ 8 10 12]\n [14 16 18]]\n# Vector broadcasting. Axes with shape 1 are duplicated.\nvec = jnp.reshape(jnp.array([0.5, 1.0, 2.0]), [3, 1])\ny = vec * x\nprint(y)\n[[ 0.5  1.   1.5]\n [ 4.   5.   6. ]\n [14.  16.  18. ]]\nvec = jnp.reshape(vec, [1, 3])\ny = vec * x\nprint(y)\n[[ 0.5  2.   6. ]\n [ 2.   5.  12. ]\n [ 3.5  8.  18. ]]\nThe first case is the simplest, that of scalar multiplication. The scalar is multiplied\nthroughout the matrix. In the second case, we have a vector of shape (3,1)  multiply‐\ning the matrix. The first row is multiplied by 0.5, the second row is multiplied by 1.0,\nand the third row is multiplied by 2.0. However, if the vector has been reshaped to\n(1,3) , the columns are multiplied by the successive entries of the vector instead.\nRandom Numbers\nAlong  with JAX’s philosophy of pure functions comes its particular way of han‐\ndling random numbers. Because pure functions do not cause side effects, a\nrandom-number generator cannot modify the random number seed, unlike other\nrandom-number generators. Instead, JAX deals with random-number keys whose\nstate is updated explicitly:\nimport jax.random  as random\nkey = random.PRNGKey(0)\nx = random.uniform(key, shape=[3, 3])\nprint(x)\n[[0.35490513  0.60419905  0.4275843  ]\n [0.23061597  0.6735498   0.43953657 ]\n [0.25099766  0.27730572  0.7678207  ]]\nkey, subkey = random.split(key)\nx = random.uniform(key, shape=[3, 3])\nprint(x)\n10 | Chapter 1: Introduction\n[[0.0045197   0.5135027   0.8613342  ]\n [0.06939673  0.93825936  0.85599923 ]\n [0.706004    0.50679076  0.6072922  ]]\ny = random.uniform(subkey, shape=[3, 3])\nprint(y)\n[[0.34896135  0.48210478  0.02053976 ]\n [0.53161216  0.48158717  0.78698325 ]\n [0.07476437  0.04522789  0.3543167  ]]\nJAX first requires you to create a random-number key from a seed. This key is then\npassed into random-number generation functions like uniform  to create random\nnumbers in the 0 to 1 range.\nTo create more random numbers, however, JAX requires that you split the key into\ntwo parts: a new key to generate other keys, and a subkey to generate new random\nnumbers. This allows JAX to deterministically and reliably reproduce random num‐\nbers even when many parallel operations are calling the random-number generator.\nWe just split a key into as many parallel operations as needed, and the random\nnumbers resulting are now randomly distributed but also reproducible. This is a nice\nproperty when you want to reproduce experiments reliably.\nJust-in-Time Compilation\nJAX starts to diverge from NumPy in terms of execution speed when we start using\nJIT compilation. JITing code—transforming the code to be compiled just in time—\nallows the same code to run on CPUs, GPUs, or TPUs:\nimport jax\nx = random.uniform(key, shape=[2048, 2048]) - 0.5\ndef my_function (x):\n  x = x @ x\n  return jnp.maximum(0.0, x)\n%timeit my_function (x).block_until_ready ()\n302 ms ± 9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nmy_function_jitted  = jax.jit(my_function )\n%timeit my_function_jitted (x).block_until_ready ()\n294 ms ± 5.45 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nA Gentle Introduction to JAX | 11\nThe JITed code is not that much faster on a CPU but will be dramatically faster\non a GPU or TPU backend. Compilation also carries some overhead when the\nfunction is called the first time, which can skew the timing of the first call. Functions\nthat can be JITed have restrictions, such as mostly calling JAX operations inside\nand having restrictions on loop operations. Variable-length loops trigger frequent\nrecompilations. The “Just-in-Time Compilation with JAX” documentation  covers a\nlot of the nuances of getting functions to JIT compile.\nSummary\nWhile we haven’t done much math yet, we have gotten to the point where we can\nbegin providing recommendations and implementing deeper logic into these compo‐\nnents. We’ll start doing things that look like ML soon enough.\nSo far, we have defined what a recommendation problem is, set up the core architec‐\nture of our recommendation system—the collector, the ranker, and the server—and\nshown a couple of trivial recommenders to illustrate how the pieces come together.\nNext we’ll explain the core relationship that recommendation systems seek to exploit:\nthe user-item matrix. This matrix lets us build a model of personalization that will\nlead to ranking.\n12 | Chapter 1: Introduction",6157
16-The User-Item Matrix.pdf,16-The User-Item Matrix,"CHAPTER 2\nUser-Item Ratings and\nFraming the Problem\nIf you were asked to curate the selection for a cheese plate at a local café, you might\nstart with your favorites. Y ou might also spend a bit of time asking for your friends’\nfavorites. Before you order a large stock in these cheeses for the café, you would\nprobably want to run a small experiment—maybe asking a group of friends to taste\nyour selections and tell you their preferences.\nIn addition to receiving your friends’ feedback, you’ d also learn about your friends\nand the cheeses. Y ou’ d learn which kinds of cheeses your friends like and which\nfriends have similar tastes. Y ou can also learn which cheeses are the most popular and\nwhich cheeses are liked by the same people.\nThis data would start to give you hints about your first cheese recommender. In this\nchapter, we’ll talk about how to turn this idea into the right stuff for a recommenda‐\ntion system. By way of this example, we’ll discuss one of the underlying notions of a\nrecommender: how to predict a user’s affinity for things they’ve never seen.\nThe User-Item Matrix\nIt’s extremely common to hear those who work on recommendation systems\ntalk about matrices, and in particular the user-item matrix. While linear algebra\nis deep, both mathematically and as it applies to RecSys, we will begin with simple\nrelationships.\n13\nBefore we get to the matrix forms, let’s write down some binary relationships between\na set of users and a set of items. For the sake of this example, think of a group of\nfive friends (mysteriously named A, B, C, D, E) and a blind cheese tasting where\nof four cheeses (gouda , chèvre , emmentaler , brie). The friends are asked to rate the\ncheeses, 1–4:\n1.A starts, “OK, I really enjoy gouda , so give that a 5; chèvre  and emmentaler  are 1.\nyummy too, 4; and brie is awful, 1. ”\n2.B replies, “What?! brie is my favorite! 4.5! chèvre  and emmentaler  are fine, 3; and 2.\ngouda  is just OK, 2. ”\n3.C gives ratings of 3, 2, 3, and 4, respectively. 3.\n4.D gives 4, 4, 5, but we run out of brie before D can try it. 4.\n5.E starts to not feel well, and tries only gouda , giving it a 3. 5.\nThe first thing you may notice is that such expository writing is a bit tedious to read\nand parse. Let’s summarize these results in a convenient table ( Table 2-1 ):\nTable 2-1. Cheeses and ratings\nCheese taster Gouda Chèvre Emmentaler Brie\nA 5 4 4 1\nB 2 3 3 4.5\nC 3 2 3 4\nD 4 4 5 -\nE 3 - - -\nY our first instinct may be to write this in a form more appropriate for computers. Y ou\nmight create a collection of lists:\nA:5, 4, 4, 1\nB:2, 3, 3, 4 . 5\nC:3, 2, 3, 4\nD:4, 4, 5, −\nE:3, − , − , −\nThis may work in some scenarios, but you might want to more clearly indicate the\npositional meaning in each list. Y ou could simply visualize this data with a heatmap\n(Figure 2-1 ):\n14 | Chapter 2: User-Item Ratings and Framing the Problem\nimport seaborn as sns\n_ = np.nan\nscores = np.array([[5,4,4,1],\n    [2,3,3,4.5],\n    [3,2,3,4],\n    [4,4,5,_],\n    [3,_,_,_]])\nsns.heatmap(\n    scores,\n    annot=True,\n    fmt="".1f"",\n    xticklabels =['Gouda', 'Chevre' , 'Emmentaler' , 'Brie',],\n    yticklabels =['A','B','C','D','E',]\n)\nFigure 2-1. Cheese ratings matrix\nAs we observe datasets with huge numbers of users or items, and with more and\nmore sparsity, we will need to employ a data structure more well suited to represent‐\ning only the necessary data. A variety of so-called dense representations  exists, but\nfor now we will use the simplest form: tuples of user_id , item_id , and rating . In\npractice, the structure is often a dictionary with indices provided by the IDs.\nDense and Sparse Representations\nTwo types of structures for these kinds of data are dense and sparse\nrepresentations. Loosely, a sparse representation  is one such that a\ndatum exists for each nontrivial observation. A dense representation\nalways contains a datum for each possibility even when trivial\n(null  or zero).\nThe User-Item Matrix | 15",4028
17-The Netflix Challenge.pdf,17-The Netflix Challenge,"Let’s see what this data looks like as a dictionary:\n'indices' : [\n  (0,0),(0,1),(0,2),(0,3),\n  (1,0),(1,1),(1,2),(1,3),\n  (2,0),(2,1),(2,2),(2,3),\n  (3,0),(3,1),(3,2),\n  (4,0)\n]\n'values' : [\n  5,4,4,1,\n  2,3,3,4.5,\n  3,2,3,4,\n  4,4,5,\n  3\n]\nA few natural questions emerge:\n1.What’s the most popular cheese? From the observations so far, it’s looking like1.\nemmentaler  is potentially the favorite, but E didn’t try emmentaler .\n2.Would D like brie? It seems to be a contentious cheese. 2.\n3.If you were asked to buy only two cheeses, which should you buy to best satisfy3.\neveryone?\nThis example and associated questions are intentionally simple, but the point is clear\nthat this matrix representation is at least convenient for capturing these ratings.\nWhat may not be obvious is that beyond the convenience of this data visualization\nis the mathematical utility of this representation. Question 2 suggests an inherent\nRecSys problem: “predict how much a user will like an item they haven’t seen. ” This\nquestion may also be recognizable as a problem from a linear algebra class: “How\ncan we fill in unknown elements of a matrix from the ones we know?” This is called\nmatrix completion.\nThe back-and-forth between creating user experiences that capture their needs and\nthe mathematical formulations to model this data and needs is at the heart of recom‐\nmendation systems.\nUser-User Versus Item-Item Collaborative Filtering\nBefore  we dive into the linear algebra, let’s consider the purely data science perspec‐\ntive called collaborative filtering  (CF), a term originally used by David Goldberg\net al. in their 1992 paper “Using Collaborative Filtering to Weave an Information\nTapestry” .\n16 | Chapter 2: User-Item Ratings and Framing the Problem\nThe underlying idea of CF is that those with similar tastes help others to know\nwhat they like without having to try it themselves. The collaboration  terminology was\noriginally intended to mean among similar-taste users, and filtering  was originally\nintended to mean filtering out choices people will not like.\nY ou can think of this CF strategy in two ways:\n•Two users with similar tastes will continue to have similar tastes.•\n•Two items with similar user fans will continue to be popular with other users•\nwho are similar to those fans.\nThese may sound identical, but they appear differently in the mathematical interpre‐\ntations. At a high level, the difference is in deciding which kind of similarity your\nrecommender should prioritize: user similarity or item similarity.\nIf you prioritize user similarity , then to provide a recommendation for a user A, you\nfind a similar user B and then choose a recommendation from B’s list of liked content\nthat A hasn’t seen yet.\nIf you prioritize item similarity , then to provide a recommendation for a user A, you\nfind an item that A liked, chèvre , and then you find an item similar to chèvre  that A\nhasn’t seen, emmentaler , and recommend it for A.\nLater we will dive deeper into similarity, but let’s quickly link these ideas to our\npreceding discussion. Similar users  are rows of the user-item matrix that are similar as\nvectors; similar items  are columns of the user-item matrix that are similar as vectors.\nVector Similarity\nDot product similarity  is more precisely defined in Chapter 10 .\nFor now, consider similarity to be computed by normalizing the\nvectors and then taking their cosine similarity. Given entities of\nany kind that you’ve associated to vectors (lists of numbers), vector\nsimilarity  compares which entities are most alike with respect to\nthe characteristics captured by those lists of numbers (called the\nlatent space ).\nThe Netflix  Challenge\nIn 2006, Netflix kicked off an online competition called the Netflix Prize. This com‐\npetition challenged teams to improve on the performance of Netflix CF algorithms\non a dataset released as open source by the company. While such a competition\nis common today via websites like Kaggle or conference, at that time, it was very\nexciting and novel for those interested in RecSys.\nThe Netflix  Challenge | 17\nThe competition consisted of several intermediate rounds awarding a Progress Prize\nand the final Netflix Prize awarded in 2009. The data provided was a collection\nof 2,817,131 triples consisting of ( user, movie, date_rated ). And half of these\nadditionally included the rating itself. Notice that as in our preceding example, the\nuser-item information is nearly enough to specify the problem. In this particular\ndataset, the date was provided. Later, we will dig into how time might be a factor, and\nin particular, for sequential recommendation systems.\nThe stakes were quite high in this competition. Requirements for beating the internal\nperformance were a 10% increase in root mean square error (RMSE); we will discuss\nthis loss function later. And the spoils added up to over $1.1 million. The final\nwinners were BellKor’s Pragmatic Chaos (which incidentally won the two previous\nProgress Prizes) with a test RMSE of 0.8567. In the end, only a 20-minute earlier\nsubmission time kept BellKor ahead of the competitors The Ensemble.\nTo read in detail about the winning submissions, check out “The BigChaos Solution\nto the Netflix Grand Prize”  by Andreas Töscher and Michael Jahrer and “The Big‐\nChaos Solution to the Netflix Prize 2008”  by the same authors. Meanwhile, let’s\nreview a few important lessons from this competition:\nFirst, we see that the user-item matrix we’ve discussed appears in these solutions as\nthe critical mathematical data structure. The model selection and training is impor‐\ntant, but parameter tuning provided a huge improvement in several algorithms. We\nwill return to parameter tuning in later chapters. The authors state that several model\ninnovations came from reflecting on the business use case and human behavior and\ntrying to capture those patterns in the model architectures. Next, linear-algebraic\napproaches resulted in the first reasonably performant solutions, and building on top\nof them led to the winning model. Finally, eking out the performance that Netflix\noriginally demanded to win the competition took so long that business circumstances\nchanged and the solution was no longer useful .\nThat last point might be the most  important thing an ML developer needs to learn\nabout recommendation systems; see the following tip.\nStart with Simplicity\nBuild  a working usable model quickly and iterate while the model\nis still relevant to the needs of the business.\n18 | Chapter 2: User-Item Ratings and Framing the Problem",6651
18-Soft Ratings.pdf,18-Soft Ratings,,0
19-What to Log.pdf,19-What to Log,"Soft Ratings\nIn our cheese-tasting example, each cheese either received a numerical rating or\nwas not tried by a guest. These are hard ratings : regardless of whether the cheese\nis a brie or a chèvre, the ratings are explicit, and their absence indicates a lack of\ninteraction between the user and item. In some contexts, we’ll want to accommodate\ndata indicating a user does interact with an item and yet provides no rating.\nA common example is a movies app; a user may have watched a movie with the app\nbut not provided a star rating. This indicates that the item (in this case, a movie)\nhas been observed, but we don’t have the rating for our algorithms to learn from.\nHowever, we can still use this implicit data to do the following:\n•Exclude this item from future recommendations•\n•Use this data as a separate term in our learner•\n•Assign a default rating value to indicate “interesting enough to watch, not signifi‐•\ncant enough to rate”\nIt turns out that implicit ratings are critical for training effective recommendation\nsystems, not only because users often don’t give hard ratings, but also because implicit\nratings provide a different level of signal. Later, when we wish to train multilevel\nmodels to predict both click likelihood and buy likelihood, these two levels will prove\nextremely important.\nTo sum up:\n•A hard rating occurs when the user directly responds to a prompt for feedback•\non an item.\n•A soft rating occurs when the user’s behavior implicitly communicates feedback•\non an item without responding to a direct prompt.\nData Collection and User Logging\nWe’ve  established that we learn from both explicit ratings and implicit ratings, so\nhow and where do we get this data? To dive into this, we’ll need to start worrying\nabout application code. In many businesses, the data scientists and ML engineers\nare separate from the software engineers, but working with recommendation systems\nrequires alignment between the two functions.\nData Collection and User Logging | 19\nWhat to Log\nThe simplest and most obvious data collection is user ratings. If users are given the\noption to provide ratings, or even a thumbs-up or thumbs-down, that component will\nneed to be built and that data will need to be stored. These ratings must be stored\nnot only for the opportunity to build recommendations, but also to prevent the bad\nuser experience of rating something and then shortly thereafter not having the rating\nappear when revisiting the page.\nSimilarly, it’s useful to understand a few other key interactions that can improve and\nexpand your recommendation system: page loads, page views, clicks, and add-to-bag.\nFor these types of data, let’s use a slightly more complicated example: the ecommerce\nwebsite Bookshop.org . This one site has multiple applications of recommendation\nsystems, almost all of which we will return to in time. For now, let’s focus on some\ninteractions ( Figure 2-2 ).\nFigure 2-2. Bookshop.org landing page\n20 | Chapter 2: User-Item Ratings and Framing the Problem\nPage loads\nWhen  you first load up Bookshop.org, it starts with items on the page. The Best\nSellers of the Week are all clickable images to those book listings. Despite the user\nhaving no choice in loading this initial page, it’s actually quite important to log the\ncontents of this initial page load.\nThese options represent the population of books that the user has seen. If a user has\nseen an option, they have the opportunity to click it, which will ultimately be an\nimportant implicit signal.\nPropensity Scores\nThe consideration of the population of all items a user has seen\nis deeply tied to propensity score matching. In mathematics, pro‐\npensity scores  are the probability that an observational unit will be\nassigned to the treatment group versus the control group.\nCompare this setup to the simple 50-50 A/B test: every unit has a 50% chance\nof being exposed to your treatment. In a feature-stratified A/B test, you purposely\nchange the probability of exposure dependent on a certain feature or collection of\nfeatures (often called covariates  in this context). Those probabilities of exposure are\nthe propensity scores.\nWhy bring up A/B testing here? Later, we’ll be interested in mining our soft ratings\nfor signal on user preference, but we must consider the possibility that the lack of a\nsoft rating is not an implicit bad rating. Thinking back to the cheeses: taster D never\nhad a chance to rate brie, so there’s no reason to think D has a preference for aversion\non brie. This is because D was not exposed to  brie.\nNow thinking back to Bookshop.org: the landing page does not show The Hitchhiker’s\nGuide to the Galaxy , so the user has no way to click it and implicitly communicate\ninterest in that book. The user could use the search option, but that’s a different kind\nof signal—which we’ll talk about later and is, in fact, a much stronger signal.\nWhen understanding implicit ratings like “did the user look at something, ” we need\nto properly account for the entire population of choices they were exposed to, and\nuse the inverse of that population size to weigh the importance of clicking. For this\nreason, understanding all page loads is important.\nPage views and hover\nWebsites  have gotten much more complicated, and now users must contend with a\nvariety of interactions. Figure 2-3  demonstrates what happens if the user clicks the\nright arrow in the Best Sellers of the Week carousel and then moves their mouse over\nthe Cooking at Home option.\nData Collection and User Logging | 21\nFigure 2-3. Bookshop.org top sellers\nThe user has unveiled a new option, and by mousing over it, has made it larger\nand given it a visual effect. These are ways to communicate more information to the\nuser, and remind the user that these options are clickable. To the recommender, these\nclicks can be used as more implicit feedback.\nFirst, the user clicked the carousel scroll—so some of what they saw in the carousel\nwas interesting enough to dig further. Second, they moused over Cooking at Home ,\nwhich they might click or might just want to see if additional information becomes\navailable when hovering. Many websites use a hover interaction to provide a pop-up\ndetail. While Bookshop.org doesn’t implement something like this, internet users\nhave been trained to expect this behavior by all the websites that do, and so the signal\nis still meaningful. Third, the user has now uncovered a new potential item in their\ncarousel scroll—which we should add to our page loads but with a higher rating\nbecause it required interaction to uncover.\nAll this and more can be encoded into the website’s logging. Rich and verbose logging\nis one of the most important ways to improve a recommendation system. Having\nmore logging data than you need is almost always better than having the opposite.\nClicks\nIf you thought hovering meant interest, wait until you consider clicking! Not in all\ncases, but in the large majority, clicking is a strong indicator of product interest. For\necommerce, clicking often is computed as part of the recommendation team’s core\nkey performance indicators (KPIs).\n22 | Chapter 2: User-Item Ratings and Framing the Problem",7267
20-Funnels.pdf,20-Funnels,"This is for two reasons:\n•Clicking is almost always required to purchase, so it’s an upstream filter for most•\nbusiness transactions.\n•Clicking requires explicit user action, so it’s a good measure of intent.•\nNoise will always exist of course, but clicks are the go-to indicator of a client’s interest.\nMany production recommendation systems are trained on click data—not ratings\ndata—because of the much higher data volume and the strong correlation between\nclick behavior and purchase behavior.\nClick-Stream Data\nSometimes in recommendation systems you hear people talk about\nclick-stream  data. This important view into click data also considers\nthe order of a user’s clicks in a single session . Modern recommen‐\ndation systems put a lot of effort into utilizing the order of items\na user clicks, calling this sequential recommendations , and have\nshown dramatic improvements via this additional dimension. We\nwill discuss sequence-based recommendations in Chapter 7 .\nAdd-to-bag\nWe’ve  finally arrived; the user has added an item to their bag or cart or queue.\nThis is an extremely strong indicator of interest and is often quite correlated with\npurchasing. There are even reasons to argue that add-to-bag is a better signal than\npurchase/order/watch. Add-to-bag is essentially the end of the line for soft ratings,\nand usually beyond this you’ d want to start collecting ratings and reviews.\nImpressions\nWe might also wish to log impressions  of an item that wasn’t clicked. This supplies the\nrecommendation system with negative feedback on items that the user isn’t interested\nin. For example, if the cheeses gouda , chèvre  and emmentaler  are offered to the user\nbut the user tastes only chèvre , perhaps the user doesn’t like gouda . They might not\nhave gotten around to tasting emmentaler , on the other hand, so these impressions\nmay carry only noisy signal.\nCollection and Instrumentation\nWeb applications frequently instrument all the interactions we’ve discussed via events.\nIf you don’t yet know what events are, maybe ask a buddy in your engineering\norg—but we’ll also give you the skinny. Like logging, events  are specially formatted\nmessages that the application sends out when a certain block of code is executed.\nData Collection and User Logging | 23\nAs in the example of a click, the application needs to make a call to get the next con‐\ntent to show the user, it’s common to also “fire an event” at this moment, indicating\ninformation about the user, what they clicked, the session-ID for later reference, the\ntime, and various other useful details. This event can be handled downstream in any\nnumber of ways, but there’s an increasingly prevalent pattern of path bifurcation to\nthe following:\n•A log database, like a mySQL application database tied to the service•\n•An event stream for real-time handling•\nThe latter will be interesting: event streams are often connected to listeners via\ntechnologies like Apache Kafka. This kind of infrastructure can get complicated fast\n(consult your local data engineer or MLOps person), but a simple model for what\nhappens is that all of a particular kind of log are sent to several destinations that you\nthink can make use of these events.\nIn the recommender case, an event stream can be connected up to a sequence of\ntransformations to process the data for downstream learning tasks. This will be\nenormously useful if you want to build a recommendation system that uses those\nlogs. Other important uses are real-time metrics logging for what is going on at any\ngiven time on the website.\nFunnels\nWe’ve just worked through our first example of a funnel, which no good data scientist\ncan avoid thinking about. Like them or hate them, funnel analyses are crucial for\ncritical evaluation of your website, and by extension your recommendation system.\nClick-Streams\nA funnel  is a collection of steps a user must take to get from\none state to another; it’s called a funnel because at each of the\ndiscrete steps, a user may stop proceeding through, or  drop off, thus\nreducing the population size at each step.\nIn our discussion of events and user logging, each step is relevant for a subset\nof the previous. This means that the process is a funnel, as shown in Figure 2-4 .\nUnderstanding the drop-off rate at each step reveals important characteristics of your\nwebsite and your recommendations.\n24 | Chapter 2: User-Item Ratings and Framing the Problem\nFigure 2-4. An onboarding funnel\nThree important funnel analyses can be considered in Figure 2-4 :\n1.Page view to add-to-bag user flow1.\n2.Page view to add-to-bag per recommendation2.\n3.Add-to-bag to complete purchase3.\nThe first funnel is merely identifying, at a high level, the percentage of users who take\neach step in the flow. This is a high-level measure of your website optimization, the\ngeneral interestingness of your product offering, and the quality of your user leads.\nThe second funnel, which is more fine-grained, takes into consideration the recom‐\nmendations themselves. As mentioned previously in terms of propensity scoring,\nusers can proceed through the funnel for a particular item only if they’re shown the\nitem. This concept intersects with the use of funnels because you want to understand\nat a high level how certain recommendations correlate with funnel drop-off, but also,\nwhen using a recommendation system, the confidence in your recommendations\nshould correlate well with the funnel metrics. We will return to this in more detail\nin Part III , but for now you should remember to think about different categories\nof recommendation-user pairs and how their funnels may look compared to the\naverage.\nFinally, we can consider add-to-bag to completion. This actually isn’t part of the\nRecSys problem but should be on your mind as a data scientist or ML engineer trying\nto improve the product. No matter how good your recommendations are, this funnel\nmay destroy any of your hard work.  Before working on a recommender problem,\nyou should almost always investigate the funnel performance in getting a user from\nadd-to-bag to check-out-completed. If there’s something cumbersome or difficult\nData Collection and User Logging | 25",6257
21-Summary.pdf,21-Summary,"about this flow, it will almost certainly provide a bigger bang for your buck to fix\nthis than to improve recommendations. Investigate the drop-offs, do user studies\nto understand what might be confusing, and work with product and engineering\nteams to ensure that everyone is aligned on this flow before you start building a\nrecommender for ecommerce.\nBusiness Insight and What People Like\nIn the previous example from Bookshop.org, Top Sellers of the Week is the primary\ncarousel on the page. Recall our earlier work on get_most_popular_recs ; what pow‐\ners the carousel is simply that recommender but applied to a specific collector—one\nthat looks only in the last week.\nThis carousel is an example of a recommender providing  business insight in addition\nto driving recommendations. A common mission of a growth team is to understand\nweekly trends and KPIs, often metrics like weekly active users and new sign-ups. For\nmany digital-first companies, growth teams are additionally interested in understand‐\ning the primary drivers of engagement.\nLet’s take an example: as of this writing, the Netflix show Squid Game  became the\ncompany’s most popular series of all time, breaking a huge number of records in the\nprocess. Squid Game  reached 111 million viewers in the first month. Most obviously,\nSquid Game  needs to be featured in the Top Shows of the Week or Hottest Titles\ncarousels, but where else should a breakout hit like this matter?\nThe first important insight companies almost always ask for is attribution : if the\nnumbers go up in a week, what led to that? Is there something important or special\nabout launches that drove additional growth? How can we learn from those signals\nto do better in the future? In the case of Squid Game —a foreign-language show that\nsaw massive interest from an English-speaking audience—executives might take away\nthe inclination to invest more in shows from South Korea or in subtitled shows with\nhigh drama. The flip side of this coin is also important: when growth metrics lag,\nexecutives nearly always ask why. Being able to point to what was the most popular,\nand how it may have deviated from expectation, helps a lot.\nThe other important insight can feed back into recommendations; during exciting\ndebuts like Squid Game , it’s easy to get caught up in the excitement as you see all your\nmetrics go up and to the right, but might this negatively affect metrics also? If you\nhave a show debuting the same week or two as Squid Game , you’ll be less enthusiastic\nabout all this success. Overall, successes like this usually drive incremental  growth,\nwhich is great for business, and in total, metrics will all probably look up. Other\nitems, however, may have less successful launches due to a zero-sum game among the\ncore user base. This can have a negative effect on longer-term metrics and can even\nmake later recommendations less effective.\n26 | Chapter 2: User-Item Ratings and Framing the Problem\nLater, you will learn about diversity of recommendations; there are many reasons to\ncare about diversifying your recommendations, but here we observe one: diversifying\ncan increase the overall ability to match your users with items. As you keep a broad\nbase of users highly engaged, you increase your future opportunity for growth.\nIncremental Gains\nIncremental gains  is an economics term now used in growth marketing and growth\nanalytics. Incremental gains refer to a margin of increase in addition to the gains\nexpected from an expended effort.\nA simple example is a business that usually adds a user for every $100 in marketing\nspending, gets some positive press, and the next week gets a user for every $80 in\nmarketing spending. By keeping the marketing budget fixed that week at $1,600, the\nbusiness would get 20 new users instead of 16—an incremental gain of 4 users. This\nframework is especially common when testing new treatments or programs.\nFinally, beyond surfacing the trending hits, another benefit of knowing what’s really\nhot on your platform or service is advertising. When a phenomenon starts, a huge\nadvantage can result from priming the pump—making noise and driving publicity\nof the success. This sometimes leads to a network effect, and in these days of viral\ncontent and easy distribution, this can have multiplicative impacts on your platform’s\ngrowth.\nSummary\nThis constitutes the most basic aspects of formulating your recommendation prob‐\nlems and preparing yourself to solve them.\nThe user-item matrix gave us a tool to summarize the relationship between users\nand items in the simplest case of numerical ratings and will generalize to more\ncomplicated models later. We saw our first notion of vector similarity, which will be\nexpanded to a deep geometric notion of relevance. Next, we learned about the kinds\nof signals that users can provide via explicit and implicit actions. Finally, we learned\nhow to capture these actions for training models.\nNow that we’ve finished our problem framing, we’ve got a bit of a math review for\nyou. Don’t worry, you can keep your ruler and compass packed away, and you won’t\nbe required to prove anything or compute any integrals. Y ou will, however, see some\nimportant mathematical notions that will help you think clearly about expectations\nfor your recommendation systems and ensure you’re asking the right questions.\nSummary | 27",5423
22-Chapter 3. Mathematical Considerations.pdf,22-Chapter 3. Mathematical Considerations,,0
23-Zipfs Laws in RecSys and the Matthew Effect.pdf,23-Zipfs Laws in RecSys and the Matthew Effect,"CHAPTER 3\nMathematical Considerations\nMost of this book is focused on implementation and on practical considerations\nnecessary to get recommendation systems working. In this chapter, you’ll find the\nmost abstract and theoretical concepts of the book. The purpose of this chapter is to\ncover a few of the essential ideas that undergird the field. It’s important to understand\nthese ideas as they lead to pathological behavior in recommendation systems and\nmotivate many architectural decisions.\nWe’ll start by discussing the shape of data you often see in recommendation systems,\nand why that shape can require careful thought. Next we’ll talk about the underlying\nmathematical idea, similarity, that drives most modern recommendation systems.\nWe’ll briefly cover a different way of thinking about what a recommender does,\nfor those with a more statistical inclination. Finally, we’ll use analogies to NLP to\nformulate the popular approach.\nZipf’s Laws in RecSys and the Matthew Effect\nIn a great many applications of ML, a caveat is given early: the distribution of\nobservations of unique items from a large corpus is modeled by Zipf’s law —the fre‐\nquency of occurrence drops exponentially. In recommendation systems, the Matthew\neffect  appears in the popular item’s click rates or the popular user’s feedback rates.\nFor example, popular items have dramatically larger click counts than average, and\nmore-engaged users give far more ratings than average.\nThe Matthew Effect\nThe Matthew effect—or popularity bias —states that the most popu‐\nlar items continue to attract the most attention and widen the gap\nwith other items.\n29\nTake, for example, the MovieLens dataset , an extremely popular dataset for bench‐\nmarking recommendation systems. Jenny Sheng  observes the behavior shown in\nFigure 3-1  for a number of movie ratings:\nFigure 3-1. Zipfian  distribution of movie-rank ratings\nAt first glance, the rapid decline in ratings is obvious and stark, but is it a problem?\nLet’s assume our recommender will be built as a user-based collaborative filtering\n(CF) model—as alluded to in Chapter 2 . Then how might these distributions affect\nthe recommender?\nWe will consider the distributional ramifications of this phenomenon. Let the proba‐\nbility mass function be described by the simple Zipf ’s law:\nfk,M=1/k\n∑n= 1M1/n\nFor M number of tokens in the corpus (in our example, the number of movies), k is\nthe rank of a token when sorted by number of occurrences.\nLet’s consider users A and B, with NA=ℐA and NB=ℐB ratings, respectively.\nObserve that the probability of Vi, the ith most popular video, appearing in ℐX for a\nuser X is given by the following:\nPi=fi,M\n∑j= 1Mfj,M=1/i\n∑j= 1M1/j\n30 | Chapter 3: Mathematical Considerations\nThus the joint probability of an item appearing in two user’s ratings is shown here:\nPi2=1/i\n∑j= 1M1/j2\nIn words, the probability of two users sharing an item in their rating sets drops off\nwith the square of its popularity rank.\nThis becomes important when we also consider that our, yet unstated, definition\nof user-based CF is based on similarity in users’ ratings sets. This similarity is the\nnumber of jointly rated items by two users, divided by the total number of items rated by\neither.\nTaking this definition, we can, for example, compute the similarity score for one\nshared item among A and B:\n∑\ni= 1MPi2\n∥ℐA∪ℐB∥\nThe average similarity score of two users is then generalized as follows via repeated\napplication of the preceding equation:\n∑\nt= 1minNA,NB∏\nik=ik− 1+ 1t− 1\n∑\ni= 1MPik2\n∥ℐA∪ℐB∥\nt\nvia repeated application of the preceding observation.\nThese combinatorial formulas not only indicate the relevance of the Zipfian in our\nalgorithms, but we also see an almost direct effect on the output of scores. Consider\nthe experiment in “Quantitative Analysis of Matthew Effect and Sparsity Problem\nof Recommender Systems”  by Hao Wang et al. on the Last.fm dataset . Last.fm is a\nmusic-listening tracker enabling users to keep track of all the songs they listen to;\nfor Last.fm users, the authors demonstrate average similarity scores for pairs of users,\nand they find that this Matthew effect persists into the similarity matrix ( Figure 3-2 ).\nZipf’s Laws in RecSys and the Matthew Effect  | 31",4317
24-Sparsity.pdf,24-Sparsity,"Figure 3-2. Matthew effect  as seen on the Last.fm dataset\nObserve the radical difference between “hot” cells and all the others. The bright cells\nare few among the mostly dark, suggesting a difficult combination of some extremely\npopular items among the far more common frequency close to zero. While these\nresults might seem scary, later we’ll consider diversity-aware loss functions that can\nmitigate the Matthew effect. A simpler way is to use downstream sampling methods,\nwhich we will discuss as part of our explore-exploit algorithms. Finally, the Matthew\neffect is only the first of two major impacts of this Zipfian; let’s turn our attention to\nthe second.\nSparsity\nWe must now reckon with sparsity. As the ratings skew more and more toward the\nmost popular items, the least popular items are starved for data and recommenda‐\ntions, which is called data sparsity.  This connects to the linear-algebraic definition:\nmostly zeros or not populated elements in a vector. When you consider again our\nuser-item matrix, less popular items constitute columns with few entries; these are\nsparse vectors. Similarly, at scale we see that the Matthew effect pushes more and\nmore of the total ratings into certain columns, and the matrix becomes sparse in the\ntraditional mathematical sense. For this reason, sparsity is an extremely well-known\nchallenge for recommendation systems.\n32 | Chapter 3: Mathematical Considerations\nAs before, let’s consider the implication on our CF algorithms from these sparse rat‐\nings. Again observe that the probability of Vi, the ith most popular item, appearing\nin ℐX for a user X is given by the following:\nPi=fi,M\n∑j= 1Mfj,M=1/i\n∑j= 1M1/j\nThen\nM− 1 *Pi\nis the expected number of other users who click the ith most popular item, so\nsumming over all, i yields the total number of other users who will share a rating\nwith X:\n∑\ni= 1M\nM− 1 *Pi\nAgain, as we pull back to the overall trends, we observe this sparsity sneaking into the\nactual computations for our CF algorithms, consider the trend of users of different\nranks, and see how much their rankings are used to collaborate  in other users’\nrankings ( Figure 3-3 ).\nFigure 3-3. User similarity counts for the Last.fm dataset\nSparsity | 33",2262
25-The NLP-RecSys Relationship.pdf,25-The NLP-RecSys Relationship,"We see that this is an important result to always be aware of: sparsity pushes emphasis\nonto the most popular users and has the risk of making your recommender myopic.\nItem-Based Collaborative Filtering\nWhile  the equations are different, in this section, they apply simi‐\nlarly to item-based CF. Similarity in items exhibits the same inheri‐\ntance of the Zipfian in their scores, and items consulted in the CF\nprocess drop off by rank.\nUser Similarity for Collaborative Filtering\nIn mathematics, it’s common to hear discussion of distances . Even back to the Pytha‐\ngorean theorem, we are taught to think of relationships between points as distances\nor dissimilarity. Indeed, this fundamental idea is canonized in mathematics as part of\nthe definition of a metric:\nda,c≤da,b+db,c\nIn ML, we often instead concern ourselves with the notion of similarity—an\nextremely related topic. In many cases, we can compute similarity or dissimilarity,\nas they are complements of each other; when d:X×X 0, 1⊂ℝ is a dissimilarity\nfunction , then we often define the following:\nSima,b: = 1 −da,b\nThis may seem like a needlessly precise statement, but in fact you’ll see that a\nvariety of options are available for framing similarity . Furthermore, sometimes we\neven formulate similarity measures where the associated distance measure does not\nestablish a metric on the set of objects. These so-called pseudospaces can still be\nincredibly important, and we’ll show where they come up in Chapter 10 .\nIn the literature, you’ll find that papers commonly start by introducing a new similar‐\nity measure and then training a model you’ve seen before on that new measure. As\nyou’ll see, the way you choose to relate objects (users, items, features, etc.) can have a\nlarge effect on what your algorithms learn.\nFor now, let’s laser in on some specific similarity measures. Consider a classic ML\nproblem of clustering: we have a space (usually ℝn) in which our data is represented\nand are asked to partition our data into subcollections of the population and assign\nthese collections names. Frequently, these collections are intended to capture a cer‐\ntain meaning, or at the very least be useful for summarizing the collection elements’\nfeatures.\n34 | Chapter 3: Mathematical Considerations\nWhen you do that clustering, you frequently are considering points near to one\nanother in that space. Further, if you’re given a new observation and asked to assign\nit to a collection as an inference task, you normally compute the new observation’s\nnearest neighbors . This could be the k-nearest neighbors or simply the nearest neigh‐\nbor among cluster centers; either way, your task is to use the notion of similarity to\nassociate—and thus classify. In CF, this same notion is used to relate a user for whom\nyou wish to make recommendations to those you already have data from.\nNearest Neighbors\nNearest neighbors  is a catchall term that arises from the simple\ngeometric idea that, given some space (points defined by feature\nvectors) and a point in that space, you can find the other points\nclosest to it. This has applications in all of ML, including classi‐\nfication, ranking/recommendation, and clustering. “ Approximate\nNearest Neighbors” on page 87  provides more details.\nSo how can we define similarity for our users in CF? They’re not obviously in the\nsame space, so our usual tools seem to be lacking.\nPearson Correlation\nOur original CF formulation indicated that users with similar tastes collaborate to\nrecommend items for one another. Let two users A and B have a set of co-rated\nitems—simply the set of items with ratings from each—written as ℛA,B, and a rating\nof item x by user A written as rA,x. Then the following is the sum of deviation from\nA’s average rating over all of its co-rated items with B:\n∑\nx ∈ℛA,BrA,x−rA\nIf we think of these ratings as a random variable and consider the analog for B, the\ncorrelation between the jointly distributed variables (the population covariance) is\nour Pearson correlation:\nUSimA,B=∑x ∈ℛA,BrA,x−rArB,x−rB\n∑x ∈ℛA,BrA,x−rA2∑x ∈ℛA,BrB,x−rB2\nUser Similarity for Collaborative Filtering | 35\nKeeping in mind a few details here is extremely important:\n•This is the similarity of the jointly distributed variables describing the users’•\nratings.\n•We compute this via all co-rated items, so user similarity is defined via item-•\nratings.\n•This is a pairwise similarity measure taking values in [–1,1] ∈ℝ. •\nCorrelation and Similarity\nIn Part III , you will learn about additional definitions of correlation\nand similarity  that are more well suited for handling ranking data\nand that accommodate implicit rankings in particular.\nRatings via Similarity\nNow that we’ve introduced user similarity, let’s use it! For a user A and item x, we can\nestimate the rating via similar users’ ratings:\nAffA,i=rA+∑U ∈ NAUSimA,U*rU,i−rA\n∑U ∈ NAUSimA,U\nThis is the prediction for user A’s rating of item x, which takes A’s average adjusted\nrating of the similarity-weighted average ratings of all of A’s neighbors. In other\nwords: A’s rating will probably be the average of people who have ratings like A’s\nrating, adjusted to how generous A is with ratings in general. We call this estimate the\nuser-item affinity  score .\nBut wait! What’s NA? It’s the neighborhood of A, via our USim definition from\nthe preceding section. The idea here is that we are aggregating ratings over the local\nregion of users identified as similar to our target user by the previous USim metric.\nHow many neighbors? How do you pick those neighbors? These will be the subject\nof later chapters; for now, assume they’re k-nearest neighbors and assume that some\nhyperparameter tuning is used to determine a good value for k.\nCorrelation Metric Spaces\nY ou might wonder, “Does this Pearson correlation yield a metric space under a\ntransformation?” The answer is yes, but clearly defining the metric space is a bit\nmore complicated than our simple definition. While the preceding equation can get\nus a distance, it’s not good enough to get us a metric space without a more novel\ntransformation.\n36 | Chapter 3: Mathematical Considerations\nIn particular, for PA,B, the previously defined correlation, 1 −PA,B yields a\ndistance that satisfies all metric properties except  the triangle inequality. There are\nseveral known ways to adjust this, though: 1 −PA,B2 is the most common. For\na survey, see “Metric Distances Derived from Cosine Similarity and Pearson and\nSpearman Correlations”  by Stijn van Dongen and Anton J. Enright.\nExplore-Exploit as a Recommendation System\nSo far we’ve presented two ideas, slightly in tension with each other:\n•The MPIR, a simple, easy-to-understand recommender•\n•The Matthew effect in recommendation systems and its runaway behavior in•\ndistributions of ratings\nBy now, you likely realize that the MPIR will amplify the Matthew effect and that the\nMatthew effect will drive the MPIR to the trivial recommender in the limit. This is\nthe classic difficulty of maximizing a loss function with no randomization: it quickly\nsettles into a modal state.\nThis problem—and many others like it—encourages some modification to the algo‐\nrithm to prevent this failure mode and continues to expose the algorithm and users\nto other options. The basic strategy for explore-exploit schemes,  or multiarmed bandits\nas they’re called, is to take not only the outcome-maximizing recommendation but\nalso a collection of alternative variants,  and randomly determine which to use as a\nresponse.\nTaking a step back: given a collection of variant recommendations, or arms , A, for\nwhich the outcome of each recommendation is yt, we have a prior reward function\nRyt. The bandit (called an  agent  in this literature) would like to maximize Ryt but\ndoesn’t know the distribution of the outcomes Ya ∈ A . The agent thus assumes some\nprior distributions for Ya ∈ A  and then collects data to update those distributions;\nafter sufficient observations, the agent can estimate the expected values of each\ndistribution, μa ∈ A =EℛYa.\nIf the agent was able to confidently estimate these reward values, the recommenda‐\ntion problem would be solved: at inference, the agent would simply estimate the\nreward values for all variants for the user and select the reward-optimizing arm.\nThis is, of course, ridiculous in totality, but the basic idea is useful nonetheless:\nhold prior assumptions about what will be the greatest expected reward, and explore\nalternatives with some frequency to continue to update the distributions and refine\nyour estimators.\nExplore-Exploit as a Recommendation System | 37\nEven when not explicitly using a multiarmed bandit, this insight is a powerful and\nuseful framework for understanding the goal of a recommendation system. Utilizing\nthe ideas of prior estimates for good recommendations and exploring other options\nto gain signal is a core idea that’s recurring. Let’s see one practicality of this approach.\nϵ-greedy\nHow often should you explore versus use your reward-optimizing arm? The first best\nalgorithm is ϵ-greedy: for ϵ ∈ 0, 1, at each request the agent has the probability ϵ\nof choosing a random arm and the probability 1 −ϵ of selecting the currently highest\nestimated reward arm.\nLet’s take the MPIR and slightly modify it to include some exploration:\nfrom jax import random\nkey = random.PRNGKey(0)\ndef get_item_popularities () -> Optional [Dict[str, int]]:\n    ...\n        # Dict of pairs: (item-identifier, count item chosen)\n        return item_choice_counts\n    return None\ndef get_most_popular_recs_ep_greedy (\n    max_num_recs : int,\n    epsilon: float\n) -> Optional [List[str]]:\n    assert epsilon<1.0\n    assert epsilon>0\n    items_popularity_dict  = get_item_popularities ()\n    if items_popularity_dict :\n        sorted_items  = sorted(\n            items_popularity_dict .items(),\n            key=lambda item: item[1]),\n            reverse=True,\n        )\n        top_items  = [i[0] for i in sorted_items ]\n        recommendations  = []\n        for i in range(max_num_recs ): # we wish to return max_num_recs\n            if random.uniform(key)>epsilon: # if greater than epsilon, exploit\n                recommendations .append(top_items .pop(0))\n            else: # otherwise, explore\n                explore_choice  = random.randint(1,len(top_items ))\n                recommendations .append(top_items .pop(explore_choice ))\n        return recommendations\n    return None\n38 | Chapter 3: Mathematical Considerations\nThe only modification to our MPIR is that now we have two cases for each potential\nrecommendation from our max_num_recs . If a random probability is less than our\nϵ, we proceed as before and select the most popular; otherwise, we select a random\nrecommendation.\nMaximizing Reward\nWe’re interpreting maximization of reward as selecting the most-\npopular items. This is an important assumption, and as we\nmove into more complicated recommenders, this will be the cru‐\ncial assumption that we modify to get different algorithms and\nschemes.\nNow let’s summarize our recommender components again:\nCollector\nThe collector here need not change; we still want to get the item popularities first.\nRanker\nThe ranker also does not change! We begin by ranking the possible recommen‐\ndations by popularity.\nServer\nIf the collector and ranker remain the same, clearly the server is what must be\nadapted for this new recommender. This is the case; instead of taking the top\nitems to fill max_num_recs , we now utilize our ϵ to determine at each step if the\nnext recommendation added to our list should be next in line from the ranker\nor a random selection. Otherwise, we adhere to the same API schema and return\nthe same shape of data.\nWhat Should ϵ Be?\nIn the preceding discussion, ϵ is a fixed number for the entire call, but what should\nthe value be? This is actually an area of great study, and the general wisdom is to start\nwith large ϵ (to encourage more exploration) and then reduce over time. Determin‐\ning the rate at which you decrease it, the starting value, and so on, requires serious\nthought and research. Additionally, this value can be tied into your prediction loop\nand be part of the training process. See “The Exploration-Exploitation Trade-Off:\nIntuitions and Strategies”  by Joseph Rocca for a deeper dive.\nOther—often better—sampling techniques exist for optimization. Importance sam‐\npling  can utilize the ranking functions we build later to integrate the explore-exploit\nwith what our data has to teach.\nExplore-Exploit as a Recommendation System | 39",12719
26-Vector Search.pdf,26-Vector Search,"The NLP-RecSys Relationship\nLet’s  utilize some intuition from a different area of ML, natural language processing.\nOne  of the fundamental models in NLP is word2vec : a sequence-based model for\nlanguage understanding that uses the words that occur in sentences together.\nFor skipgram-word2vec , the model takes sentences and attempts to learn the implicit\nmeaning of their words via their co-occurrence relationships with other words in\nthose sentences. Each pair of co-occurring words constitutes a sample that is one-hot\nencoded and sent into a vocabulary-sized layer of neurons, with a bottleneck layer\nand a vocabulary-sized output layer for probabilities that words will occur.\nVia this network, we reduce the size of our representation to the bottleneck dimen‐\nsion and thus find a smaller dimensional representation of all our words than the\noriginal corpus-sized one-hot embedding. The thinking is that similarity of words\ncan now be computed via vector similarity in this new representation space.\nWhy is this related to recommendation systems? Well, because if we take the ordered\nsequence of user-item interactions (e.g., the sequence of movies a user has rated),\nwe can utilize the same idea from word2vec to find item similarity instead of word\nsimilarity. In this analogy, the user history is the sentence .\nPreviously, using our CF similarity, we decided that similar users can help inform\nwhat a good recommendation for a user should be. In this model, we are finding\nitem-item similarity, so instead we assume that a user will like the items similar to\nthose previously liked.\nItems as Words\nY ou may have noticed that natural language models treat words\nas sequences, and in fact, our user history is a sequence too!\nFor now, hold onto this knowledge. Later, this will guide us to\nsequence-based methods for RecSys.\nVector Search\nWe have built a collection of vector representations of our items, and we claim that\nsimilarity in this space (often called a latent space , representation space , or ambient\nspace ) means similarity in likability  to users.\nTo convert this similarity to a recommendation, consider a user A with a collection\nof previously liked items ℛA, and consider A=vxx ∈ℛA the set of vectors\nassociated to those items in this latent space. We are looking for a new item y that we\nthink is good for A.\n40 | Chapter 3: Mathematical Considerations\nThe Old Curse\nThese latent spaces tend to be of high dimension, which Euclidean\ndistance famously performs poorly in. As regions become sparse,\nthe distance function performance decreases; local distances are\nmeaningful, but global distances are not to be trusted. Instead,\ncosine distance shows better performance, but this is a topic of\ndeep exploration. Additionally, instead of minimizing the distance,\nin practice it’s better to maximize the similarity.\nOne simple way to use similarity to produce a recommendation is to take the closest\nitem to the average of those that A likes:\nargmax yUSimvy,avgA∣ y ∈ Items\nHere, d− , −  is a distance function in the latent space (usually cosine distance).\nThe argmax essentially treats all of A’s ratings equally and suggests something near\nthose. In practice, this process is often fraught. First, you could weight the terms by\nrating:\nargmax yUSimvy,∑vx∈ Arx\nℛA∣ y ∈ Items\nThis can potentially improve the representativeness of the user feedback in the\nrecommendations. Alternatively, you might find that a user rates movies across a\nvariety of genres and themes. Averaging here will definitely lead to worse results, so\nmaybe you want to simply find recommendations similar to one movie the user liked,\nweighted by that rating:\nargmax yUSimvy,vx\nrx∣ y ∈ Items,vx∈ A\nFinally, you may even want to do this process several times for different items a user\nliked to get k recommendations:\nmin −kargmax yUSimvy,vx\nrx∣ y ∈ Items∣ vx∈ A\nThe NLP-RecSys Relationship | 41",3956
27-Nearest-Neighbors Search.pdf,27-Nearest-Neighbors Search,,0
28-Chapter 4. System Design for Recommending.pdf,28-Chapter 4. System Design for Recommending,"Now we have k recommendations; each is similar to something that the user has liked\nand is weighted by how much they liked it. This approach utilized only an implicit\ngeometry of the items formed by their co-occurrences.\nLatent spaces and the geometric power that comes with them for recommendations\nwill be a through line for the rest of the book. We will often formulate our loss func‐\ntions via these geometries, and we’ll exploit the geometric intuition to brainstorm\nwhere to expand our technique next.\nNearest-Neighbors Search\nA reasonable question to ask is “How do I get these vectors that minimize this\ndistance?” In all the preceding schemes, we are computing many distances and then\nfinding minimums. In general, the problem of nearest neighbors is an extremely\nimportant and well-studied question.\nWhile finding the exact nearest neighbors can sometimes be slow, a lot of great\nprogress has been made on approximate nearest neighbors (ANN) searches. These\nalgorithms not only return very close to the actual nearest neighbors, but they also\nperform orders of complexity faster. In general, when you see us (or other publica‐\ntions) computing an argmin  (the argument that minimized the function) over some\ndistances, there’s a good chance ANN is what’s used in practice.\nSummary\nRecommendation systems in the preceding chapter discussed data distribution prin‐\nciples such as Zipf ’s law and the Matthew Effect. These principles lead to challenges,\nsuch as skewed user similarity scores and data sparsity. In the world of ML, while\nthe traditional math focuses on distance, the emphasis is on the concept of similarity.\nDifferent measures of similarity can drastically alter algorithm learning outcomes,\nwith clustering being a primary application.\nIn the realm of recommendations, items are often represented in high-dimensional\nlatent spaces. Similarity in these spaces hints at user preferences. Methods include\nrecommending items close to a user’s average liked items, and this may be improved\nby adding a weighting by user-rating. However, individual preferences necessitate\ndiverse recommendations. Latent spaces continue to be influential, driving recom‐\nmendation techniques.\nLocating these vectors effectively requires the nearest-neighbors search. Though exact\nmethods are resource-intensive, approximate nearest-neighbors offer a fast, precise\nsolution, providing the foundation for the recommendation systems discussed in the\ncurrent chapter.\n42 | Chapter 3: Mathematical Considerations",2545
29-Collector.pdf,29-Collector,"CHAPTER 4\nSystem Design for Recommending\nNow  that you have a foundational understanding of how recommendation systems\nwork, let’s take a closer look at the elements needed and at designing a system that\nis capable of serving recommendations at industrial scale. Industrial scale  in our\ncontext will primarily refer to reasonable scale  (a term introduced by Ciro Greco,\nAndrea Polonioli, and Jacopo Tagliabue in “ML and MLOps at a Reasonable Scale” )—\nproduction applications for companies with tens to hundreds of engineers working\non the product, not thousands.\nIn theory, a recommendation system is a collection of math formulas that can take\nhistorical data about user-item interactions and return probability estimates for a\nuser-item-pair’s affinity. In practice, a recommendation system is 5, 10, or maybe 20\nsoftware systems, communicating in real time and working with limited information,\nrestricted item availability, and perpetually out-of-sample behavior, all to ensure that\nthe user sees something.\nThis chapter is heavily influenced by “System Design for Recommendations and\nSearch”  by Eugene Y an and “Recommender Systems, Not Just Recommender Models”\nby Even Oldridge and Karl Byleen-Higley.\nOnline Versus Offline\nML systems consist of the stuff that you do in advance and the stuff that you do\non the fly. This division, between online and offline, is a practical consideration\nabout the information necessary to perform tasks of various types. To observe and\nlearn large-scale patterns, a system needs access to lots of data; this is the offline\ncomponent. Performing inference, however, requires only the trained model and\nrelevant input data. This is why many ML system architectures are structured in this\n43",1758
30-Online Collector.pdf,30-Online Collector,"way. Y ou’ll frequently encounter the terms batch  and real-time  to describe the two\nsides of the online-offline paradigm ( Figure 4-1 ).\nFigure 4-1. Real-time versus batch\nA batch process  does not require user input, often has longer expected time peri‐\nods for completion, and is able to have all the necessary data available simultane‐\nously. Batch processes often include tasks like training a model on historical data,\naugmenting one dataset with an additional collection of features, or transforming\ncomputationally expensive data. Another characteristic you see more frequently in\nbatch processes is that they work with the full relevant dataset involved, not only an\ninstance of the data sliced by time or otherwise.\nA real-time  process  is carried out at the time of the request; said differently, it is\nevaluated during the inference process. Examples include providing a recommenda‐\ntion upon page load, updating the next episode after the user finishes the last, and\nre-ranking recommendations after one has been marked not interesting . Real-time\nprocesses are often resource constrained because of the need for rapidity, but like\nmany things in this domain, as the world’s computational resources expand, we\nchange the definition of resource constrained.\nLet’s return to the components introduced in Chapter 1 —the collector, ranker, and\nserver—and consider their roles in offline and online systems.\nCollector\nThe collector’s role is to know what is in the collection of items that may be recom‐\nmended and the necessary features or attributes of those items.\nOffline  Collector\nThe offline  collector  has access to and is responsible for the largest datasets. Under‐\nstanding all user-item interactions, user similarities, item similarities, feature stores\nfor users and items, and indices for nearest-neighbor lookup are all under the pur‐\nview of the offline collector. The offline collector needs to be able to access the\n44 | Chapter 4: System Design for Recommending",2018
31-Offline Ranker.pdf,31-Offline Ranker,"relevant data extremely fast, and sometimes in large batches. For this purpose, offline\ncollectors often implement sublinear search functions or specifically tuned indexing\nstructures. They may also leverage distributed compute for these transformations.\nIt’s important to remember that the offline collector not only needs access and\nknowledge of these datasets but will also be responsible for writing the necessary\ndownstream datasets to be used in real time.\nOnline Collector\nThe online collector  uses the information indexed and prepared by the offline collec‐\ntor to provide real-time access to the parts of this data necessary for inference. This\nincludes techniques like searching for nearest neighbors, augmenting an observation\nwith features from a feature store, and knowing the full inventory catalog. The online\ncollector will also need to handle recent user behavior; this will become especially\nimportant when we see sequential recommenders in Chapter 17 .\nOne additional role the online collector may take on is encoding a request. In the\ncontext of a search recommender, we want to take the query and encode it into the\nsearch space  via an embedding model. For contextual recommenders, we need to\nencode the context into the latent space  via an embedding model also.\nEmbedding Models\nOne  popular subcomponent in the collector’s work will involve an\nembedding step; see Machine Learning Design Patterns  by Valliappa\nLakshmanan et al. (O’Reilly). The embedding step on the offline\nside involves both training the embedding model and constructing\nthe latent space for later use. On the online side, the embedding\ntransformation will need to embed a query into the right space. In\nthis way, the embedding model serves as a transformation that you\ninclude as part of your model architecture.\nRanker\nThe ranker’s role is to take the collection provided by the collector, and order some or\nall of its elements according to a model for the context and user. The ranker actually\ngets two components itself, the filtering and the scoring.\nFiltering  can be thought of as the coarse inclusion and exclusion of items appropriate\nfor recommendation. This process is usually characterized by rapidly cutting away\na lot of potential recommendations that we definitely don’t wish to show. A trivial\nexample is not recommending items we know the user has already chosen in the past.\nScoring  is the more traditional understanding of ranking: creating an ordering of\npotential recommendations with respect to the chosen objective function.\nRanker | 45",2591
32-Online Ranker.pdf,32-Online Ranker,,0
33-Offline Server.pdf,33-Offline Server,"Offline  Ranker\nThe goal of the offline  ranker  is to facilitate filtering and scoring. What differentiates\nit from the online ranker is how it runs validation and how the output can be used\nto build fast data structures that the online ranker can utilize. Additionally, the offline\nranker can integrate with a human review process for human-in-the loop ML .\nAn important technology that will be discussed later is the  bloom filter . A bloom\nfilter allows the offline ranker to do work in batches, so that filtering in real time\ncan happen much faster. An oversimplification of this process would be to use a\nfew features of the request to quickly select subsets of all possible candidates. If\nthis step can be completed quickly—in terms of computational complexity, striving\nfor something less than quadratic in the number of candidates—then downstream\ncomplex algorithms can be made much more performant.\nSecond to the filtering step is the ranking step. In the offline component, ranking is\ntraining the model that learns how to rank items. As you will see later, learning to\nrank items to perform best with respect to the objective function is at the heart of the\nrecommendation models. Training these models, and preparing the aspects of their\noutput, is part of the batch responsibility of the ranker.\nOnline Ranker\nThe online ranker  gets a lot of praise but really utilizes the hard work of other compo‐\nnents. The online ranker first does filtering, utilizing the filtering infrastructure built\noffline—for example, an index lookup or a bloom filter application. After filtering,\nthe number of candidate recommendations has been tamed, and thus we can actually\ncome to the most infamous of the tasks: rank recommendations.\nIn the online ranking phase, usually a feature store is accessed to take the candidates\nand embellish them with the necessary details, and then a scoring and ranking model\nis applied. Scoring or ranking may happen in several independent dimensions and\nthen be collated into one final ranking. In the multiobjective paradigm, you may have\nseveral of these ranks associated with the list of candidates returned by a ranker.\nServer\nThe server’s role is to take the ordered subset provided by the ranker, ensure that the\nnecessary data schema is satisfied (including essential business logic), and return the\nrequested number of recommendations.\n46 | Chapter 4: System Design for Recommending",2454
34-Online Server.pdf,34-Online Server,,0
35-Summary.pdf,35-Summary,"Offline  Server\nThe offline  server  is responsible for high-level alignment of the hard requirements of\nrecommendations returned from the system. In addition to establishing and enforc‐\ning schema, these rules can be more nuanced things like “never return this pair of\npants when also recommending this top. ” Often waved off as “business logic, ” the\noffline server is responsible for creating efficient ways to impose top-level priorities\non the returned recommendations.\nAn additional responsibility for the offline server is handling tasks like experimenta‐\ntion. At some point, you’ll likely want to run online experiments to test out all the\namazing recommendation systems you build with this book. The offline server is the\nplace where you’ll implement the logic necessary to make experimentation decisions\nand provide the implications in a way the online server can use them in real time.\nOnline Server\nThe online server  takes the rules, requirements, and configurations established and\nmakes their final application to the ranked recommendations. A simple example is\ndiversification rules; as you will see later, diversification of recommendations can\nhave a significant impact on the quality of a user’s experience. The online server can\nread the diversification requirements from the offline server and apply them to the\nranked list to return the expected number of diverse recommendations.\nSummary\nIt’s important to remember that the online server is the endpoint from which other\nsystems will be getting a response. While it’s usually where the message is coming\nfrom, many of the most complicated components in the system are upstream. Be\ncareful to instrument this system in a way that when responses are slow, each system\nis observable enough that you can identify where those performance degradations are\ncoming from.\nNow that we’ve established the framework and you understand the functions of the\ncore components, we will discuss the aspects of ML systems next and the kinds of\ntechnologies associated with them.\nIn this next chapter, we’ll get our hands dirty with the aforementioned components\nand see how we might implement the key aspects. We’ll wrap it up by putting it all\ntogether into a production-scale recommender using only the content of each item.\nLet’s go!\nSummary | 47",2337
36-Random-Item Recommender.pdf,36-Random-Item Recommender,"CHAPTER 5\nPutting It All Together:\nContent-Based Recommender\nThroughout this part of the book, we’ve introduced some of the most basic compo‐\nnents in a recommendation system. In this chapter, we’ll get hands-on. We’re going\nto design and implement a recommendation system for images from Pinterest. This\nchapter, along with the book’s other “Putting It All Together” chapters, will show you\nhow to work with datasets by using open source tools. The material for this kind of\nchapter refers to code hosted on GitHub that you will need to download and play\nwith in order to properly experience the content.\nSince this is the first practical hands-on chapter, here are some extra setup instruc‐\ntions for the development environment. We developed this code on Windows run‐\nning in a Windows Subsystem for Linux (WSL) Ubuntu virtual machine. The code\nshould run fine on Linux machines, with more technical adaptation for macOS and a\nlot more for Windows, in which case it would be better to run it on a WSL2 Ubuntu\nvirtual machine. Y ou can look at the setup for WSL in the Microsoft documentation\nfor Windows . We picked Ubuntu for the image. Y ou will also need NVIDIA CUDA\nand cuDNN  if you have an NVIDIA GPU and want to use it.\nWe will be using the Shop the Look (STL) dataset  from “Complete the Look: Scene-\nBased Complementary Product Recommendation”  by Wang-Cheng Kang et al.\nIn this chapter, we will show you how to build a content-based recommender. Recall\nthat a content-based recommender uses indirect, generalizable representations of the\nitems you wish to represent. Imagine, for instance, that you want to recommend a\ncake but cannot use the name of a cake. Instead, you might use descriptions of the\ncake or its ingredients as the content features.\n49\nWith the STL dataset, we will try to match scenes, which are pictures of a person in\na particular setting, with products that might go well with the scene. The training\nset contains pairs of scenes with single products, and we want to use the content\nrecommender to extend recommendations to the entire catalog of products and sort\nthem in some kind of ranking order. The content recommender, because it uses\nindirect content features to make recommendations, can be used to recommend\nnew products that haven’t been in the recommendation system or to warm-start a\nrecommendation system with manually curated data before users start using it and a\nfeedback loop is established. In the case of the STL dataset, we’ll focus on the visual\nappearance of the scene and the products.\nWe will generate content embeddings via a convolutional neural network (CNN)\narchitecture, and then train the embedding via a triplet loss and show how to create a\ncontent recommendation system.\nThis chapter covers the following topics:\n•Revision control software•\n•Python build systems•\n•Random-item recommender•\n•Obtaining the STL dataset images•\n•Definition of CNN•\n•Model training in JAX, Flax, and Optax•\n•Input pipeline•\nRevision Control Software\nRevision control software  is a software system that keeps track of code changes. Think\nof it as a database that tracks versions of code you have written, while providing\nadded functionality like showing the differences between each version of code and\nallowing you to revert to a previous version.\nThere are many kinds of revision control systems. We host the code for this book on\nGitHub .\nThe revision control software we use is called Git. Code changes are done in batches\ncalled a patch , and each patch is uploaded to a source control repository like GitHub\nso that it can be cloned and worked on by many people at the same time.\n50 | Chapter 5: Putting It All Together: Content-Based Recommender\nY ou can use this command to clone the book code sample repository:\ngit clone git@github.com:BBischof/ESRecsys.git\nFor this chapter, look in the directory ESRecsys/pinterest  for instructions on how to\nrun the code in detail. This chapter will mostly focus on descriptions and pointers to\nthe repository so that you’ll able to get a feel for these systems in practice.\nPython Build Systems\nPython  packages  are libraries that provide functionality beyond the standard Python\nlibraries. These include ML packages such as TensorFlow and JAX but also more util‐\nitarian packages like the absl flags library or machine learning operations (MLOps)\nlibraries like Weights & Biases .\nThese packages are usually hosted on the Python Package Index .\nTake a look at the file requirements.txt :\nabsl-py==1.1.0\ntensorflow==2.9.1\ntyped-ast==1.5.4\ntyping_extensions==4.2.0\njax==0.3.25\nflax==0.5.2\noptax==0.1.2\nwandb==0.13.4\nY ou can see that we have picked a small set of Python packages to install for our\ndependencies. The format is package name, two equal signs, and then the version of\nthe package.\nOther build systems that work with Python include the following:\n•pip•\n•Bazel•\n•Anaconda•\nFor this chapter, we will use pip.\nBefore  installing the packages, however, you might want to read up on Python\nvirtual environments . Python virtual environments are a way to keep track of Python\npackage dependencies per project so that if different projects use different versions of\nthe same package, they won’t interfere with one another because each project has its\nown Python virtual environment to run in.\nPython Build Systems | 51\nY ou can create and activate a Python virtual environment by typing the following into\na Unix shell:\npython -m venv pinterest_venv\nsource pinterest_venv/bin/activate\nThe first command creates a Python virtual environment, and the second one acti‐\nvates it. Y ou will have to activate a virtual environment every time you open a new\nshell so that Python knows what environment to work in.\nAfter the virtual environment is created, you can then use pip to install packages\ninto the virtual environment, and those newly installed packages will not affect the\nsystem-level packages.\nY ou can do this by running this command in the ESRecsys/pinterest  directory:\npip install -r requirements.txt\nThis will install the specified packages and any subpackages that they might depend\non into the virtual environment.\nRandom-Item Recommender\nThe first program we will look at is a random-item recommender ( Example 5-1 ).\nExample 5-1. Setting up flags\nFLAGS = flags.FLAGS\n_INPUT_FILE  = flags.DEFINE_string (\n  ""input_file"" , None, ""Input cat json file."" )\n_OUTPUT_HTML  = flags.DEFINE_string (\n  ""output_html"" , None, ""The output html file."" )\n_NUM_ITEMS  = flags.DEFINE_integer (\n  ""num_items"" , 10, ""Number of items to recommend."" )\n# Required flag.\nflags.mark_flag_as_required (""input_file"" )\nflags.mark_flag_as_required (""output_html"" )\ndef read_catalog (catalog: str) -> Dict[str, str]:\n    """"""\n      Reads in the product to category catalog.\n    """"""\n    with open(catalog, ""r"") as f:\n        data = f.read()\n    result = json.loads(data)\n    return result\ndef dump_html (subset, output_html :str) -> None:\n    """"""\n      Dumps a subset of items.\n52 | Chapter 5: Putting It All Together: Content-Based Recommender\n    """"""\n    with open(output_html , ""w"") as f:\n        f.write(""<HTML>\n"")\n        f.write(""""""\n        <TABLE><tr>\n        <th>Key</th>\n        <th>Category</th>\n        <th>Image</th>\n        </tr>"""""" )\n        for item in subset:\n            key, category  = item\n            url = pin_util .key_to_url (key)\n            img_url = ""<img src= \""%s\"">"" % url\n            out = ""<tr><td> %s</td><td> %s</td><td> %s</td></tr> \n"" %\n            (key, category , img_url)\n            f.write(out)\n        f.write(""</TABLE></HTML>"" )\ndef main(argv):\n    """"""\n      Main function.\n    """"""\n    del argv  # Unused.\n    catalog = read_catalog (_INPUT_FILE .value)\n    catalog = list(catalog.items())\n    random.shuffle(catalog)\n    dump_html (catalog[:_NUM_ITEMS .value], _OUTPUT_HTML .value)\nHere we use the absl flags library to pass in arguments to the program such as the\npath to the JSON catalog file that contains the STL scene, and product pairs.\nFlags can have different types like string and integer, and you can mark them as\nrequired. If a required flag is not passed to the program, it will complain and stop\nrunning. Flags can be accessed via their value method.\nWe load and parse the STL dataset by using the JSON Python library, and then we\nrandomly shuffle the catalog and dump the top few results in HTML.\nY ou can run the random-item recommender via the following command:\npython3 random_item_recommender.py\n--input_file =STL-Dataset/fashion-cat.json  --output_html =output.html\nAfter completion, you can open the output.html  file with your web browser and see\nsome random items from the catalog. Figure 5-1  shows a sample.\nRandom-Item Recommender | 53",8858
37-Model Training in JAX Flax and Optax.pdf,37-Model Training in JAX Flax and Optax,"Figure 5-1. Random-item recommender\nThe fashion-catalog.json  file contains descriptions of products and their Pinterest ID,\nwhile fashion.json  contains pairings of a scene with a recommended product.\nNext, we’ll look at how to recommend multiple new items for a single scene by\ntraining an ML model on scene-product pairings.\nIt is generally a good idea to create a random-item recommender the first time you\nencounter a corpus just so you have an idea of the kind of items in the corpus and\nyou have a baseline to compare to.\nObtaining the STL Dataset Images\nThe first step in the process of creating a content-based recommender is fetching the\ncontent. In this case, the STL dataset’s content is mostly images, with some metadata\nabout the image (like the type of product). We will be using just the image content for\nthis chapter.\nY ou can look at the code in fetch_images.py  to see how this is done, by using the\nPython standard library urllib to fetch the images. Be aware that doing too much\nfetching on someone else’s website might trigger their bot defenses and cause them to\nblacklist your IP address, so it might be a wise idea to rate-limit fetches or find some\nother way to get the data.\nWe have downloaded thousands of image files and put them together into an archive\nas a Weights & Biases artifact. Since the archive is already in this artifact, you\ndon’t need to scrape the images yourself, but the code we’ve supplied will allow\nyou to do so.\nY ou can read up on artifacts in the Weights & Biases documentation . Artifacts are\nan MLOps concept that version and package together archives of data and track\nproducers and consumers of the data.\n54 | Chapter 5: Putting It All Together: Content-Based Recommender\nY ou can download the image artifact by running the following:\nwandb artifact  get building-recsys/recsys-pinterest/shop_the_look:latest\nThe images will then be in the local directory artifacts/shop_the_look:v1 .\nConvolutional Neural Network Definition\nNow  that we have the images, the next step is figuring out how to represent the data.\nImages come in different sizes and are a complex type of content to analyze. We can\nuse the raw pixels as the representation of our content, but the drawback is that tiny\nchanges in pixel values can cause large differences in the distance between images.\nWe do not want that. Rather, we want to somehow learn what is important in the\nimages and ignore parts of the image, such as the background color, that might not be\nas important.\nFor this task, we will use a convolutional neural network (CNN)  to compute an\nembedding vector for the image. An embedding vector  is a kind of feature vector for\nthe image that is learned from data and is of fixed size. We use embedding vectors\nfor our representation because we want our database to be small and compact, easy\nto score over large numbers of images in the corpus, and relevant to the task at hand,\nwhich is to match products to a given scene image.\nThe neural network architecture we use is a variant of residual networks, or Resnet.\nRefer to “Deep Residual Learning for Image Recognition”  by Kaiming He et al. for\ndetails about the architecture and for references on CNNs. Briefly, a convolution layer\nrepeatedly applies a small filter of typically 3 × 3 size over an image. This results\nin a feature map of the same resolution as the input if the stride is (1, 1) (which\nmeans apply the filter with a 1-pixel step in the x direction and a 1-pixel step in the\ny direction), or quarter size if the stride is (2, 2). The residual skip connection is just\na shortcut from the previous input layer to the next, so in effect, the nonlinear part\nof the networks learns the residual from the linear skip part, hence the name residual\nnetwork.\nAdditionally, we use the BatchNorm layer, details of which can be found at “Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal Covariate\nShift”  by Sergey Ioffe and Christian Szegedy, and the “Searching for Activation Func‐\ntions”  by Prajit Ramachandran, Barret Zoph, and Quoc V . Le.\nOnce we specify the model, we also need to optimize it for the task.\nConvolutional Neural Network Definition  | 55\nModel Training in JAX, Flax, and Optax\nOptimizing  our model should be pretty straightforward in any ML framework. Here\nwe show how to do it easily with JAX, Flax, and Optax . JAX is a lower-level NumPy-\nlike ML library, and Flax  is a higher-level neural network library that provides\nfunctionality such as neural network modules and embedding layers. Optax  is a\nlibrary that does optimization that we will use to minimize our loss function.\nIf you are familiar with NumPy, JAX is quite easy to pick up. JAX shares the same API\nas NumPy but has the capability of running the resulting code on vector processors\nsuch as GPUs or TPUs by doing JIT compilation. JAX device arrays and NumPy\narrays can be easily converted back and forth, which makes it easy to develop for the\nGPU and yet easy to debug on the CPU.\nIn addition to learning how to represent the images, we also need to specify how they\nare related to one another.\nSince the embedding vectors are of fixed dimensions, the easiest similarity score is\nsimply the dot product of the two vectors. See “Similarity from Co-occurrence” on\npage 163 for other kinds of similarity measures. So, given an image for a scene, we\ncompute the scene embedding and do the same for the product to obtain a product\nembedding, and take the dot product of the two to obtain a score for the closeness of\nfit of a scene s to a product p:\nscores,p=s*p\nWe use CNNs to obtain the embedding of an image.\nWe use separate CNNs for the scene and product, however, because they come from\ndifferent kinds of images. Scenes tend to show the context we’re matching products\nto and contain people and the setting, whereas products tend to be catalog images\nof shoes and bags with a blank background, so we need different neural networks to\ndetermine what is important in the image.\nOnce we have the score, that alone is not sufficient, though. We need to make\nsure that a good match of a scene and product, which we call the positive product ,\nis higher scoring than a negative product. The positive product is a good match\nfor the scene, and the negative product is a not-so-good match for the scene. The\npositive product comes from the training data, and the negative product comes from\nrandomly sampling the catalog. A loss that can capture the relationship between a\npositive scene-product pair (A, B) and negative scene-product pair (A, C) is called\ntriplet loss . Let’s go into some detail for defining the triplet loss .\n56 | Chapter 5: Putting It All Together: Content-Based Recommender\nSuppose we want the score for the positive scene-product pair to be one more than a\nnegative scene-product pair. We then have the following inequality:\nscorescene ,posproduct >scorescene ,negproduct + 1\nThe 1 is just an arbitrary constant we use, called a margin , to make sure that the\npositive scene-product score is larger than the negative scene-product score.\nSince the process of gradient descent minimizes a function, we then convert the\npreceding inequality into a loss function by moving all terms to one side:\n0 > 1 +scorescene ,negproduct −scorescene ,posproduct\nAs long as the quantity on the right side is larger than 0, we want to minimize it; but\nif it is already less than 0, we do not. Therefore, we encode the quantity in a rectified\nlinear unit, which is represented by the function max(0, x). We can thus write out\nour loss function as follows:\nlossscene ,posproduct ,negproduct =\nmax 0, 1 +scorescene ,negproduct −scorescene ,posproduct\n \nSince we usually minimize loss functions, this ensures that as long as the\nscore(scene, neg_product)  is 1 more than score(scene, pos_product) , the opti‐\nmization procedure will try to minimize the score of the negative pair while increas‐\ning the score of the positive pair.\nThe next example covers the following modules in order so that they make sense as\nthey follow the flow of data from reading to training to making recommendations:\ninput__pipeline.py\nHow the data is read\nmodels.py\nHow the neural networks are specified\ntrain_shop_the_look.py\nHow the neural network is fit using Optax\nmake_embeddings.py\nHow to make a compact database of scene and products\nModel Training in JAX, Flax, and Optax | 57",8497
38-Input Pipeline.pdf,38-Input Pipeline,"make_recommendations.py\nHow to use the compact database of embeddings to create a list of product\nrecommendations per scene\nInput Pipeline\nExample 5-2  shows the code for input_pipeline.py . We use the ML library TensorFlow\nfor its data pipeline.\nExample 5-2. TensorFlow data pipeline\nimport tensorflow  as tf\ndef normalize_image (img):\n  img = tf.cast(img, dtype=tf.float32)\n  img = (img / 255.0) - 0.5\n  return img\ndef process_image (x):\n  x = tf.io.read_file (x)\n  x = tf.io.decode_jpeg (x, channels =3)\n  x = tf.image.resize_with_crop_or_pad (x, 512, 512)\n  x = normalize_image (x)\n  return x\ndef process_image_with_id (id):\n  image = process_image (id)\n  return id, image\ndef process_triplet (x):\n  x = (process_image (x[0]), process_image (x[1]), process_image (x[2]))\n  return x\ndef create_dataset (\n    triplet: Sequence [Tuple[str, str, str]]):\n    """"""Creates a triplet dataset.\n    Args:\n      triplet: filenames of scene, positive product, negative product.\n    """"""\n    ds = tf.data.Dataset.from_tensor_slices (triplet)\n    ds = ds.map(process_triplet )\n    return ds\nY ou can see that create_dataset  takes in three filenames: that of a scene, then a\npositive match and a negative match. For this example, the negative match is simply\nselected at random from the catalog. We cover more sophisticated ways of picking\nthe negative in Chapter 12 . The image filenames are processed by reading the file,\ndecoding the image, cropping it to a fixed size, and then rescaling the data so that it\n58 | Chapter 5: Putting It All Together: Content-Based Recommender\nbecomes a floating-point image centered around 0 and with small values between –1\nand 1. We do this because most neural networks are initialized with the assumption\nthat the data they get is roughly normally distributed, and so if you pass in too large a\nvalue, it would be far out of the norm of the expected input range.\nExample 5-3  shows how to specify our CNN and STL model with Flax.\nExample 5-3. Defining  the CNN model\nfrom flax import linen as nn\nimport jax.numpy  as jnp\nclass CNN(nn.Module):\n    """"""Simple CNN.""""""\n    filters : Sequence [int]\n    output_size  : int\n    @nn.compact\n    def __call__ (self, x, train: bool = True):\n        for filter in self.filters:\n            # Stride 2 downsamples 2x.\n            residual  = nn.Conv(filter, (3, 3), (2, 2))(x)\n            x = nn.Conv(filter, (3, 3), (2, 2))(x)\n            x = nn.BatchNorm (\n              use_running_average =not train, use_bias =False)(x)\n            x = nn.swish(x)\n            x = nn.Conv(filter, (1, 1), (1, 1))(x)\n            x = nn.BatchNorm (\n              use_running_average =not train, use_bias =False)(x)\n            x = nn.swish(x)\n            x = nn.Conv(filter, (1, 1), (1, 1))(x)\n            x = nn.BatchNorm (\n              use_running_average =not train, use_bias =False)(x)\n            x = x + residual\n            # Average pool downsamples 2x.\n            x = nn.avg_pool (x, (3, 3), strides=(2, 2), padding=""SAME"")\n        x = jnp.mean(x, axis=(1, 2))\n        x = nn.Dense(self.output_size , dtype=jnp.float32)(x)\n        return x\nclass STLModel (nn.Module):\n    """"""Shop the look model that takes in a scene\n        and item and computes a score for them.\n    """"""\n    output_size  : int\n    def setup(self):\n        default_filter  = [16, 32, 64, 128]\n        self.scene_cnn  = CNN(\n          filters=default_filter , output_size =self.output_size )\n        self.product_cnn  = CNN(\nInput Pipeline | 59\n          filters=default_filter , output_size =self.output_size )\n    def get_scene_embed (self, scene):\n        return self.scene_cnn (scene, False)\n    def get_product_embed (self, product):\n        return self.product_cnn (product, False)\n    def __call__ (self, scene, pos_product , neg_product ,\n                 train: bool = True):\n        scene_embed  = self.scene_cnn (scene, train)\n        pos_product_embed  = self.product_cnn (pos_product , train)\n        pos_score  = scene_embed  * pos_product_embed\n        pos_score  = jnp.sum(pos_score , axis=-1)\n        neg_product_embed  = self.product_cnn (neg_product , train)\n        neg_score  = scene_embed  * neg_product_embed\n        neg_score  = jnp.sum(neg_score , axis=-1)\n        return pos_score , neg_score , scene_embed ,\n          pos_product_embed , neg_product_embed\nHere  we use Flax’s neural network class Module . The annotation nn.compact  is there\nso we do not have to specify a setup function for simple neural network architectures\nlike this one and can simply specify the layers in the call  function. The call  function\naccepts two parameters, an image x and a Boolean train  that tells the module\nwhether we are calling it in training mode. The reason we need the Boolean training\nis that the BatchNorm layers are updated only during training and are not updated\nwhen the network is fully learned.\nIf you look at the CNN specification code, you can see how we set up the residual\nnetwork. We can freely mix neural network functions like swish  with JAX functions\nlike mean . The swish  function is a nonlinear activation for the neural network that\ntransforms the input in such a way as to weight some values of activation more than\nothers.\nThe STL model, on the other hand, has a more complicated setup, so we have to\nspecify the setup code to create two CNN towers: one for the scene and another for\nthe product. A CNN tower  is just a copy of the same architecture but has different\nweights for different image types. As mentioned earlier, we have a different tower for\neach type of image because each represents different things; one tower is for the scene\n(which provides the context to which we are matching products), and a separate\ntower is for the products. As a result, we add in two different methods for converting\nscene and product images into scene and product embeddings.\n60 | Chapter 5: Putting It All Together: Content-Based Recommender\nThe call is also different. It doesn’t have the annotation compact because we have a\nmore complicated setup. In the call function for the STL model, we first compute the\nscene embedding, then the positive product embedding, and then the positive score.\nAfter that, we do the same for the negative score. We then return the positive score,\nnegative score, and all three embedding vectors. We return the embedding vectors\nas well as the scores because we want to ensure that the model generalizes to new,\nunseen data as in a held-out validation set, so we want to make sure the embedding\nvectors are not too large. The concept of capping their size is called regularization .\nNow let’s take a look at train_shop_the_look.py  (Example 5-4 ). We’ll break it into\nseparate function calls and discuss them one by one.\nExample 5-4. Generating triplets for training\ndef generate_triplets (\n    scene_product : Sequence [Tuple[str, str]],\n    num_neg: int) -> Sequence [Tuple[str, str, str]]:\n    """"""Generate positive and negative triplets.""""""\n    count = len(scene_product )\n    train = []\n    test = []\n    key = jax.random.PRNGKey(0)\n    for i in range(count):\n        scene, pos = scene_product [i]\n        is_test = i % 10 == 0\n        key, subkey = jax.random.split(key)\n        neg_indices  = jax.random.randint(subkey, [num_neg], 0, count - 1)\n        for neg_idx in neg_indices :\n            _, neg = scene_product [neg_idx]\n            if is_test:\n                test.append((scene, pos, neg))\n            else:\n                train.append((scene, pos, neg))\n    return train, test\n def shuffle_array (key, x):\n    """"""Deterministic string shuffle.""""""\n    num = len(x)\n    to_swap = jax.random.randint(key, [num], 0, num - 1)\n    return [x[t] for t in to_swap]\nThe code fragment reads in the scene-product JSON database and generates triplets\nof scene, positive product, and negative products for the input pipeline. The inter‐\nesting part to note here is how JAX handles random numbers. JAX’s philosophy\nis functional in nature, meaning that functions are pure and have no side effects.\nRandom-number generators carry state, so in order to make JAX random-number\ngenerators function, you have to pass in the state to the random-number genera‐\ntor. The mechanism for this is to have a pseudo random number generator key,\nInput Pipeline | 61\nPNRGKey, as the object-carrying state. We initialize one arbitrarily from the number\n0. Whenever we wish to use the key, though, we have to split it into two by using\njax.random.split , then use one to generate the next random number and a subkey\nto perform the random action. In this case, we use the subkey to select a random\nnegative from the entire corpus of products for our negative. We cover more complex\nways to sample the negative in Chapter 12 , but randomly selecting a negative is the\nsimplest way to construct the triplet for triplet loss.\nSimilar to the way the negatives are selected, we again use JAX’s random functionality\nto generate a list of indices to swap, in order to shuffle the array for the training step.\nRandom shuffling is important in stochastic gradient descent to break up any kind of\nstructure in the training data to ensure that the gradients are stochastic. We use JAX’s\nrandom shuffling mechanism for better reproducibility so that experiments are more\nlikely to be the same, given the same initial data and settings.\nThe next pair of functions we will look at are listed in Example 5-5  and show how\nthe train and eval steps are written. The train step takes the state of the model,\nwhich contains the model parameters as well as the gradient information, which\nvaries depending on the optimizer being used. This step also takes in batches of\nscenes, positive products, and negative products in order to construct the triplet loss.\nIn addition to optimizing for the triplet loss, we want to minimize the size of the\nembeddings whenever they go outside the unit sphere. The process of minimizing the\nsize of the embeddings is called regularization , so we add it to the triplet loss to obtain\nthe final loss.\nExample 5-5. Training and evaluation steps\ndef train_step (state, scene, pos_product ,\n               neg_product , regularization , batch_size ):\n    def loss_fn(params):\n        result, new_model_state  = state.apply_fn (\n            params,\n            scene, pos_product , neg_product , True,\n            mutable=['batch_stats' ])\n        triplet_loss  = jnp.sum(nn.relu(1.0 + result[1] - result[0]))\n        def reg_fn(embed):\n            return nn.relu(\n              jnp.sqrt(jnp.sum(jnp.square(embed), axis=-1)) - 1.0)\n        reg_loss  = reg_fn(result[2]) +\n                   reg_fn(result[3]) + reg_fn(result[4])\n        reg_loss  = jnp.sum(reg_loss )\n        return (triplet_loss  + regularization  * reg_loss ) / batch_size\n    grad_fn = jax.value_and_grad (loss_fn)\n    loss, grads = grad_fn(state.params)\n    new_state  = state.apply_gradients (grads=grads)\n    return new_state , loss\n62 | Chapter 5: Putting It All Together: Content-Based Recommender\ndef eval_step (state, scene, pos_product , neg_product ):\n    def loss_fn(params):\n        result, new_model_state  = state.apply_fn (\n            state.params,\n            scene, pos_product , neg_product , True,\n            mutable=['batch_stats' ])\n        # Use a fixed margin for the eval.\n        triplet_loss  = jnp.sum(nn.relu(1.0 + result[1] - result[0]))\n        return triplet_loss\nFlax, being written on top of JAX, is also functional in philosophy, so the existing\nstate is used to compute the gradient of the loss function, which when applied returns\na new state variable. This ensures that the functions remain pure and the state\nvariables are mutable.\nThis functional philosophy is what allows JAX to JIT compile or use JIT functions so\nthey run fast on CPU, GPU, or TPU.\nThe eval step, in comparison, is rather simple. It just computes the triplet loss without\nthe regularization loss as our evaluation metric. Again, we cover more sophisticated\nevaluation metrics in Chapter 11 .\nFinally, let’s take a look at the body of the training program, shown in Example 5-6 .\nWe store our hyperparameters such as learning rate, regularization, and output size in\na config dictionary. We do this so we can pass the config dictionary on to the Weights\n& Biases MLOps service for safekeeping and also so we can do hyperparameter\nsweeps.\nExample 5-6. Main body of code for training the model\ndef main(argv):\n    """"""Main function.""""""\n    del argv  # Unused.\n    config = {\n        ""learning_rate""  : _LEARNING_RATE .value,\n        ""regularization""  : _REGULARIZATION .value,\n        ""output_size""  : _OUTPUT_SIZE .value\n    }\n    run = wandb.init(\n        config=config,\n        project=""recsys-pinterest""\n    )\n    tf.config.set_visible_devices ([], 'GPU')\n    tf.compat.v1.enable_eager_execution ()\n    logging.info(""Image dir %s, input file %s"",\n      _IMAGE_DIRECTORY .value, _INPUT_FILE .value)\n    scene_product  = pin_util .get_valid_scene_product (\nInput Pipeline | 63\n      _IMAGE_DIRECTORY .value, _INPUT_FILE .value)\n    logging.info(""Found %d valid scene product pairs.""  % len(scene_product ))\n    train, test = generate_triplets (scene_product , _NUM_NEG .value)\n    num_train  = len(train)\n    num_test  = len(test)\n    logging.info(""Train triplets %d"", num_train )\n    logging.info(""Test triplets %d"", num_test )\n     # Random shuffle the train.\n    key = jax.random.PRNGKey(0)\n    train = shuffle_array (key, train)\n    test = shuffle_array (key, test)\n    train = np.array(train)\n    test = np.array(test)\n    train_ds  = input_pipeline .create_dataset (train).repeat()\n    train_ds  = train_ds .batch(_BATCH_SIZE .value).prefetch (\n      tf.data.AUTOTUNE )\n    test_ds = input_pipeline .create_dataset (test).repeat()\n    test_ds = test_ds.batch(_BATCH_SIZE .value)\n    stl = models.STLModel (output_size =wandb.config.output_size )\n    train_it  = train_ds .as_numpy_iterator ()\n    test_it = test_ds.as_numpy_iterator ()\n    x = next(train_it )\n    key, subkey = jax.random.split(key)\n    params = stl.init(subkey, x[0], x[1], x[2])\n    tx = optax.adam(learning_rate =wandb.config.learning_rate )\n    state = train_state .TrainState .create(\n        apply_fn =stl.apply, params=params, tx=tx)\n    if _RESTORE_CHECKPOINT .value:\n        state = checkpoints .restore_checkpoint (_WORKDIR .value, state)\n    train_step_fn  = jax.jit(train_step )\n    eval_step_fn  = jax.jit(eval_step )\n    losses = []\n    init_step  = state.step\n    logging.info(""Starting at step %d"", init_step )\n    regularization  = wandb.config.regularization\n    batch_size  = _BATCH_SIZE .value\n    eval_steps  = int(num_test  / batch_size )\n    for i in range(init_step , _MAX_STEPS .value + 1):\n        batch = next(train_it )\n        scene = batch[0]\n        pos_product  = batch[1]\n        neg_product  = batch[2]\n        state, loss = train_step_fn (\n            state, scene, pos_product , neg_product ,\n64 | Chapter 5: Putting It All Together: Content-Based Recommender\n            regularization , batch_size )\n        losses.append(loss)\n        if i % _CHECKPOINT_EVERY_STEPS .value == 0 and i > 0:\n            logging.info(""Saving checkpoint"" )\n            checkpoints .save_checkpoint (\n              _WORKDIR .value, state, state.step, keep=3)\n        metrics = {\n            ""step"" : state.step\n        }\n        if i % _EVAL_EVERY_STEPS .value == 0 and i > 0:\n            eval_loss  = []\n            for j in range(eval_steps ):\n                ebatch = next(test_it)\n                escene = ebatch[0]\n                epos_product  = ebatch[1]\n                eneg_product  = ebatch[2]\n                loss = eval_step_fn (\n                  state, escene, epos_product , eneg_product )\n                eval_loss .append(loss)\n            eval_loss  = jnp.mean(jnp.array(eval_loss )) / batch_size\n            metrics.update({""eval_loss""  : eval_loss })\n        if i % _LOG_EVERY_STEPS .value == 0 and i > 0:\n            mean_loss  = jnp.mean(jnp.array(losses))\n            losses = []\n            metrics.update({""train_loss""  : mean_loss })\n            wandb.log(metrics)\n            logging.info(metrics)\n    logging.info(""Saving as %s"", _MODEL_NAME .value)\n    data = flax.serialization .to_bytes (state)\n    metadata  = { ""output_size""  : wandb.config.output_size  }\n    artifact  = wandb.Artifact (\n        name=_MODEL_NAME .value,\n        metadata =metadata ,\n        type=""model"")\n    with artifact .new_file (""pinterest_stl.model"" , ""wb"") as f:\n        f.write(data)\n    run.log_artifact (artifact )\nif __name__ == ""__main__"" :\n    app.run(main)\nA hyperparameter sweep  is a tuning service that helps you find optimal values for\nhyperparameters such as learning rate by running many trials of different values\nand searches for the best one. Having the configuration as a dictionary allows us to\nreproduce the best parameters by running a hyperparameter sweep and then saving\nthe best one for the final model.\nInput Pipeline | 65\nIn Figure 5-2 , you can see what a Weights & Biases hyperparameter sweep looks like.\nOn the left, we have all the runs in the sweep; each run is trying a different set of\nvalues that we have specified in the config dictionary. In the middle, we see how the\nfinal evaluation loss changes over time with the number of trials on the sweep. On the\nright, we have a plot indicating the importance of the hyperparameter in affecting the\nevaluation loss. Here we can see that the learning rate has the most effect on the eval\nloss, followed by the regularization amount.\nFigure 5-2. Weights & Biases hyperparameter sweep\nOn the bottom right of the figure, a parallel coordinates plot shows how each param‐\neter affects the evaluation loss. To read the plot, follow each line and see where it ends\nup on the final evaluation loss. The optimal hyperparameters can be found by tracing\nthe line from the bottom-right target value of evaluation loss back to the left, through\nthe values chosen for the hyperparameters. In this case, the optimal value selected is a\nlearning_rate  of 0.0001618, a regularization of 0.2076, and an output_size  of 64.\nThe rest of the code is mostly setting up the model and hooking up the input\npipeline to the model. Deciding when to log metrics and model serialization is mostly\nself-explanatory. The details can be read in the Flax documentation.\nIn saving the model, notice that two methods are used. One is a checkpoint, and the\nother is Flax serialization. We have both because the checkpoint is used when training\njobs are canceled and we need to recover the step at which the job was canceled so we\ncan resume training. The final serialization is used when the training is done.\n66 | Chapter 5: Putting It All Together: Content-Based Recommender\nWe also save a copy of the model as a Weights & Biases artifact . This way, the Weights\n& Biases platform can keep track of the hyperparameters that created the model, the\nexact code and the exact Git hash that generated the model, and the lineage of the\nmodel. This lineage consists of upstream artifacts used to generate the model (such\nas the training data), the state of the job used to create the model, and an added back\nlink to all future jobs that might use the artifact. This makes it easier to reproduce\nmodels at a point in time or trace back which model was used and at what time in\nproduction. This comes in super handy when you have a larger organization and\nfolks are hunting around for information on how a model was created. By using\nartifacts, they can simply look in one place for the code and training data artifacts to\nreproduce a model.\nNow that we have trained the models, we want to generate embeddings for the scene\nand the product database. The nice thing about using the dot product as a scoring\nfunction as opposed to using a model is that you can generate scene and product\nembeddings independently and then scale out these computations at inference time.\nThis kind of scaling will be introduced in Chapter 8 , but for now the relevant part of\nmake_embeddings.py  is shown in Example 5-7 .\nExample 5-7. Finding the top-k recommendations\n    model = models.STLModel (output_size =_OUTPUT_SIZE .value)\n    state = None\n    logging.info(""Attempting to read model %s"", _MODEL_NAME .value)\n    with open(_MODEL_NAME .value, ""rb"") as f:\n        data = f.read()\n        state = flax.serialization .from_bytes (model, data)\n    assert(state != None)\n    @jax.jit\n    def get_scene_embed (x):\n      return model.apply(state[""params"" ], x, method=models.STLModel .get_scene_embed )\n    @jax.jit\n    def get_product_embed (x):\n      return model.apply(\n      state[""params"" ],\n      x,\n      method=models.STLModel .get_product_embed\n      )\n    ds = tf.data.Dataset\n      .from_tensor_slices (unique_scenes )\n      .map(input_pipeline .process_image_with_id )\n    ds = ds.batch(_BATCH_SIZE .value, drop_remainder =True)\n    it = ds.as_numpy_iterator ()\n    scene_dict  = {}\n    count = 0\n    for id, image in it:\nInput Pipeline | 67\n      count = count + 1\n      if count % 100 == 0:\n        logging.info(""Created %d scene embeddings"" , count * _BATCH_SIZE .value)\n      result = get_scene_embed (image)\n      for i in range(_BATCH_SIZE .value):\n        current_id  = id[i].decode(""utf-8"")\n        tmp = np.array(result[i])\n        current_result  = [float(tmp[j]) for j in range(tmp.shape[0])]\n        scene_dict .update({current_id  : current_result })\n    scene_filename  = os.path.join(_OUTDIR.value, ""scene_embed.json"" )\n    with open(scene_filename , ""w"") as scene_file :\n      json.dump(scene_dict , scene_file )\nAs you can see, we simply use the same Flax serialization library to load the model,\nand then call the appropriate method of the model by using the apply  function. We\nthen save the vectors in a JSON file, since we have already been using JSON for the\nscene and product databases.\nFinally, we’ll use the scoring code in make_recommendations.py  to generate product\nrecommendations for sample scenes ( Example 5-8 ).\nExample 5-8. Core retrieval definition\ndef find_top_k (\n  scene_embedding ,\n  product_embeddings ,\n  k):\n  """"""\n  Finds the top K nearest product embeddings to the scene embedding.\n  Args:\n    scene_embedding: embedding vector for the scene\n    product_embedding: embedding vectors for the products.\n    k: number of top results to return.\n  """"""\n  scores = scene_embedding  * product_embeddings\n  scores = jnp.sum(scores, axis=-1)\n  scores_and_indices  = jax.lax.top_k(scores, k)\n  return scores_and_indices\ntop_k_finder  = jax.jit(find_top_k , static_argnames =[""k""])\nThe most relevant code fragment is the scoring code, where we have a scene embed‐\nding and want to use JAX to score all the product embeddings instead of a single\nscene embedding. Here we use Lax, a sublibrary of JAX that supplies direct API calls\nto XLA, the underlying ML compiler for JAX, in order to access accelerated functions\nlike top_k . In addition, we compile the function find_top_k  by using JAX’s JIT. Y ou\ncan pass pure Python functions that contain JAX commands to jax.jit  in order to\ncompile them to a specific target architecture such as a GPU using XLA. Notice we\n68 | Chapter 5: Putting It All Together: Content-Based Recommender\nhave a special argument called static_argnames ; this allows us to inform JAX that\nk is fixed and doesn’t change much so that JAX is able to compile a purpose-built\ntop_k_finder  for a fixed value of k.\nFigure 5-3  shows sample product recommendations for a scene in which a woman is\nwearing a red shirt. The products recommended include red velvet and dark pants.\nFigure 5-3. Recommended items for an indoor scene\nFigure 5-4  shows another scene: a woman is wearing a red coat outdoors, and the\nmatching accessories are a yellow handbag and yellow pants.\nWe have pregenerated some results that are stored as an artifact that you can view by\ntyping in the following command:\nwandb artifact  get building-recsys/recsys-pinterest/scene_product_results:v0\nOne thing you may notice is that the yellow bag and pants get recommended a\nlot. It may be possible that the embedding vector for the yellow bag is large, so\nit gets matched to a lot of scenes. This is called the popular item problem  and is\na common issue with recommendation systems. We cover some business logic to\nhandle diversity and popularity in later chapters, but this is a problem that can\nhappen with recommendation systems that you might want to keep an eye out for.\nInput Pipeline | 69",24877
39-Part II. Retrieval.pdf,39-Part II. Retrieval,"Figure 5-4. Recommended items for an outdoor scene\nSummary\nAnd with that, we conclude the first “Putting It All Together” chapter. We covered\nhow to use JAX and Flax to read real-world data, train a model, and find the top\nrecommended items for a look. If you haven’t played with the code yet, hop on over\nto the GitHub repo to give it a whirl! We hope that providing a real-world working\nexample of an end-to-end content-based recommender will give you a better feel for\nhow the theory translates into practice. Enjoy playing with the code!\n70 | Chapter 5: Putting It All Together: Content-Based Recommender\nPART II\nRetrieval\nHow do we get all the data in the right place to train a recommendation system? How do\nwe build and deploy systems for real-time inference?\nReading research papers about recommendation systems will often give the impres‐\nsion that they’re built via a bunch of math equations, and all the really hard work\nof using recommendation systems is in connecting these equations to the features\nof your problem. More realistically, the first several steps of building a production\nrecommendation system fall under systems engineering. Understanding how your\ndata will make it into your system, be manipulated into the correct structure, and\nthen be available in each of the relevant steps of the training flow often constitutes\nthe bulk of the initial recommendation system’s work. But even beyond this initial\nphase, ensuring that all the necessary components are fast enough and robust enough\nfor production environments requires yet another significant investment in platform\ninfrastructure.\nOften you’ll build a component responsible for processing the various types of data\nand storing them in a convenient format. Next, you’ll construct a model that takes\nthat data and encodes it in a latent space or other representation model. Finally, you’ll\nneed to transform an input request into the representation as a query in this space.\nThese steps usually take the form of jobs in a workflow management platform or\nservices deployed as endpoints. The next few chapters will walk you through the\nrelevant technologies and concepts necessary to build and deploy these systems—and\nthe awareness of important aspects of reliability, scalability, and efficiency.\nY ou might be thinking, “I’m a data scientist! I don’t need to know all this!” But you\nshould know that RecSys has an inconvenient duality: model architecture changes\noften affect the systems architecture. Interested in trying out those fancy transform‐\ners? Y our deployment strategy is going to need a new design. Maybe your clever\nfeature embeddings can solve the cold-start problem! Those feature embeddings will\nneed to serve your encoding layers and integrate with your new NoSQL feature store.\nDon’t panic! This part of the book is a walk through the Big Data Zoo.",2886
40-Chapter 6. Data Processing.pdf,40-Chapter 6. Data Processing,,0
41-Hydrating Your System.pdf,41-Hydrating Your System,,0
42-PySpark.pdf,42-PySpark,"CHAPTER 6\nData Processing\nIn the trivial recommender that we defined in Chapter 1 , we used the method\nget_availability ; and in the MPIR, we used the method get_item_popularities .\nWe hoped the choice of naming would provide sufficient context about their func‐\ntion, but we did not focus on the implementation details. Now we will start unpack‐\ning the details of some of this complexity and present the toolsets for online and\noffline collectors.\nHydrating Your System\nGetting data into the pipeline is punnily referred to as hydration . The ML and data\nfields have a lot of water-themed naming conventions; “(Data ∩ Water) Terms”  by\nPardis Noorzad covers this topic.\nPySpark\nSpark  is an extremely general computing library, with APIs for Java, Python, SQL,\nand Scala. PySpark’s role in many ML pipelines is for data processing and transform‐\ning the large-scale datasets.\nLet’s return to the data structure we introduced for our recommendation problem;\nrecall that the user-item matrix is the linear-algebraic representation of all the triples\nof users, items, and the user’s rating of the item. These triples are not naturally\noccurring in the wild. Most commonly, you begin with log files from your system; for\nexample, Bookshop.org may have something that looks like this:\n'page_view_id' : 'd15220a8e9a8e488162af3120b4396a9ca1' ,\n'anonymous_id' : 'e455d516-3c08-4b6f-ab12-77f930e2661f' ,\n'view_tstamp' : 2020-10-29 17:44:41+00:00,\n'page_url' : 'https://bookshop.org/lists/best-sellers-of-the-week' ,\n'page_url_host' : 'bookshop.org' ,\n73\n'page_url_path' : '/lists/bookshop-org-best-sellers-of-the-week' ,\n'page_title' : 'Best Sellers of the Week' ,\n'page_url_query' : None,\n'authenticated_user_id' : 15822493.0 ,\n'url_report_id' : 511629659.0 ,\n'is_profile_page' : False,\n'product_viewed' : 'list',\nThis is a made-up log file that may look similar to the backend data for Book‐\nshop.org’s best sellers of the week. These are the kinds of events that you consume\nfrom engineering and are likely stored in your columnar database. For data like this,\nutilizing SQL syntax will be our entry point.\nPySpark provides a convenient SQL API. Based on your infrastructure, this API will\nallow you to write what looks like SQL queries against a potentially massive dataset.\nExample Schemas\nThese example database schemas are only guesses at what Book‐\nshop.org may use, but they are modeled on the authors’ experience\nof looking at hundreds of database schemas at multiple companies\nover many years. Additionally, we attempt to distill these schemas\nto the components relevant to our topic. In real systems, you’ d\nexpect much more complexity but the same essential parts. Each\ndata warehouse and event stream will have its own quirks. Please\nconsult a data engineer near you.\nLet’s use Spark to query the preceding logs:\nuser_item_view_counts_qry  = """"""\nSELECT\n  page_views.authenticated_user_id\n  , page_views.page_url_path\n  , COUNT(DISTINCT page_views.page_view_id) AS count_views\nFROM prod.page_views\nJOIN prod.dim_users\nON page_views.authenticated_user_id = dim_users.authenticated_user_id\nWHERE DATE page_views.view_tstamp >= '2017-01-01'\nAND dim_users.country_code = 'US'\nGROUP BY\n  page_views.authenticated_user_id\n  , page_views.page_url_path\nORDER BY 3, page_views.authenticated_user_id\n""""""\nuser_item_view_counts_sdf  = spark.sql(user_item_view_counts_qry )\n74 | Chapter 6: Data Processing\nThis is a simple SQL query, assuming the preceding log schema, that would allow\nus to see, for each user-item pair, how many times that user has viewed that pair.\nThe convenience of writing pure SQL here means that we can use our experience in\ncolumnar databases to quickly ramp up on Spark.\nThe major advantage of Spark, however, is not yet on display. When executing the\npreceding code in a Spark session, this query will not be immediately run. It will\nbe staged for execution, but Spark waits until you use this data downstream in a\nway that requires immediate execution  before it begins doing so. This is called lazy\nevaluation , and it allows you to work on your data object without every change and\ninteraction immediately being applied. For more details, it’s worth consulting a more\nin-depth guide like Learning Spark  by Jules Damji et al. (O’Reilly), but there’s one\nmore important characteristic of the Spark paradigm that is essential to discuss.\nSpark is natively a distributed computing language. In particular, this means that the\npreceding query—even after we force it to execute—will store its data on multiple\ncomputers. Spark works via a driver program  in your program or notebook, which\ndrives a cluster manager , which in turn coordinates executors  on worker nodes.  When\nwe query data with Spark, instead of all that data being returned into a DataFrame\nin memory on the computer we’re using, parts of that data are sent to memory on\nthe executors. And when we do a transformation on the DataFrame, it is applied\nappropriately on the pieces of the DataFrame that are stored on each of the executors.\nIf this sounds a bit like magic, that’s because it’s obscuring a lot of technical details\nbehind several convenience layers. Spark is a layer of technology that allows the ML\nengineer to program as if they’re working on one machine, and have those changes\ntake effect on an entire cluster of machines. It’s not important to understand the\nnetwork structure when querying, but it is important to be aware of some of these\ndetails in case things go wrong; the ability to understand what the error output is\nreferring to is crucial in troubleshooting. This is all summarized in Figure 6-1 , which\nis a diagram from the Spark documentation .\nFigure 6-1. Component architecture of Spark 3.0\nHydrating Your System | 75\nIt’s important to note that all this does not come for free; both lazy evaluation and\ndistributed DataFrames come at the cost of needing additional thought when writing\nprograms. Even though Spark makes a lot of this work far easier, understanding how\nto write efficient code in this paradigm that works with the architecture but still\nachieves complicated goals can require a year’s worth of experience.\nReturning to recommendation systems—and in particular, the offline collector—we\nwant to use PySpark to build the types of datasets needed to train our models. One\nsimple thing to do with PySpark is to transform our logs data into the appropriate\nform for training a model. In our simple query, we applied a few filters to our data\nand grouped by user and item to get the number of views. A variety of other tasks\nmay fit naturally into this paradigm—perhaps adding user or item features stored in\nother databases, or high-level aggregations.\nIn our MPIR, we asked for get_item_popularities ; and we sort of assumed a few\nthings:\n•This would return the number of times each item was chosen.•\n•This method would be fast.•\nThe second point is important if the endpoint is going to be called in real time. So\nhow might Spark come into play?\nFirst, let’s assume we have a lot of data, enough that we can’t get it all to fit into\nour little MacBook Pro’s memory. Additionally, let’s continue to use the preceding\nschema. We can write an even simpler query:\nitem_popularity_qry  = """"""\nSELECT\n  page_views.page_url_path\n  , COUNT(DISTINCT page_views.authenticated_user_id) AS count_viewers\nFROM prod.page_views\nJOIN prod.dim_users\nON page_views.authenticated_user_id = dim_users.authenticated_user_id\nWHERE DATE page_views.view_tstamp >= '2017-01-01'\nAND dim_users.country_code = 'US'\nGROUP BY\n  page_views.page_url_path\nORDER BY 2\n""""""\nitem_view_counts_sdf  = spark.sql(item_popularity_qry )\n76 | Chapter 6: Data Processing",7790
43-Example User Similarity in PySpark.pdf,43-Example User Similarity in PySpark,"We can now write this aggregated list of (item, count)  pairs to an app database to\nserve get_item_popularities  (something that doesn’t require us to do any parsing\nwhen this is called), or potentially we can take a subset of the top- N of this list and\nstore it in memory to get the best items with respect to a particular ranking. Either\nway, we’ve separated concerns of parsing all our log data, and doing aggregation, from\nthe get_item_popularities  function call in real time.\nThis example used an overly simple data aggregation, one just as easy to do in\nsomething like PostgreSQL, so why bother? The first reason is scalability. Spark is\nreally built to scale horizontally, which means that as the data we need to access\ngrows, we merely add more worker nodes.\nThe second reason is that PySpark is more than just SparkSQL; anyone who’s done\ncomplicated SQL queries can probably agree that the power and flexibility of SQL\nis enormous, but frequently certain tasks that you want to achieve require a lot\nof creativity to carry out in the fully SQL environment. PySpark gives you all the\nexpressiveness of pandas DataFrames, Python functions and classes, and a simple\ninterface to apply Python code to the PySpark data structure’s user-defined functions\n(UDFs). UDFs are similar to lambda functions that you’ d use in pandas, but they’re\nbuilt and optimized for PySpark DataFrames. As you’ve probably experienced when\nwriting ML programs in smaller data regimes, at some point you switch away from\nusing only SQL to using pandas API functions to perform data transformations—so\ntoo will you appreciate this power at the Spark data scale.\nPySpark allows you to write what looks very much like Python and pandas code\nand have that code executed in a distributed fashion! Y ou don’t need to write code\nto specify which worker nodes operations should happen; that’s handled for you by\nPySpark. This framework isn’t perfect; some things you expect to work may require\na bit of care, and optimization of your code can require an additional level of\nabstraction, but generally, PySpark gives you a rapid way to move your code from one\nnode to a cluster and utilize that power.\nTo illustrate something a bit more useful in PySpark, let’s return to collaborative\nfiltering (CF) and compute some features more relevant for ranking.\nExample: User Similarity in PySpark\nA user similarity table allows you to map a user to other users who are relevant\nto the recommender. This recalls the assumption that two similar users like similar\nthings, and thus you can recommend to both users the items that one hasn’t seen.\nConstructing this user similarity table is an example of a PySpark job that you might\nsee as part of the offline collector’s responsibility. Even though in many cases ratings\nwould continue to stream in all the time, for the purposes of large offline jobs, we\noften think of a daily batch to update the essential tables for our model. In practice,\nin many cases this daily batch job suffices to provide features that are good enough\nHydrating Your System | 77\nfor most of the ML work downstream. Other important paradigms exist, but those\nfrequently marry  the more frequent updates with these daily batch jobs, instead of\ntotally eliminating them.\nThis architecture of daily batch jobs with smaller, more frequent batch jobs is called\nthe lambda architecture , and we’ll get more into the details of how and why later. In\nbrief, the two layers—batch and speed—which are distinguished (inversely) by the\nfrequency of processing and the volume per run of data they process. Note that the\nspeed layer may have varying frequencies associated with it, and it’s possible to have\ndifferent speed layers for hourly, and another speed layer for minute-frequency jobs\nthat do different things. Figure 6-2  provides an overview of the architecture.\nFigure 6-2. Overview of a lambda architecture\nIn the case of user similarity, let’s work on a batch job implementation of computing\na daily table. First we’ll need to get ratings from our schema before today. We’ll also\ninclude a few other filters that simulate how this query might look in real life:\nuser_item_ratings_qry  = """"""\nSELECT\n  book_ratings.book_id\n  book_ratings.user_id\n  , book_ratings.rating_value\n  , book_ratings.rating_tstamp\nFROM prod.book_ratings\nJOIN prod.dim_users\nON book_ratings.user_id = dim_users.user_id\nJOIN prod.dim_books\nON book_ratings.book_id = dim_books.dim_books\nWHERE\nDATE book_ratings.rating_tstamp\n78 | Chapter 6: Data Processing\nBETWEEN (DATE '2017-01-01')\n AND (CAST(current_timestamp() as DATE)\n  AND book_ratings.rating_value IS NOT NULL\nAND dim_users.country_code = 'US'\n  AND dim_books.book_active\n""""""\nuser_item_ratings_sdf  = spark.sql(user_item_ratings_qry )\nAs before, utilizing the SQL syntax to get the dataset into a Spark DataFrame is the\nfirst step, but now we have additional work on the PySpark side. A common pattern\nis to get the dataset you want to work with via simple SQL syntax and logic, and then\nuse the PySpark API to do more detailed data processing.\nLet’s first observe that we have no assumptions about uniqueness of a user-item\nrating. For the sake of this table, let’s decide that we’ll use the most recent rating for a\npair:\nfrom pyspark.sql.window  import Window\nwindows = Window().partitionBy (\n['book_id' , 'user_id' ]\n).orderBy(\ncol(""rating_tstamp"" ).desc()\n)\nuser_item_ratings_sdf .withColumn (\n""current_rating"" ,\nfirst(\nuser_item_ratings_sdf (""rating_tstamp"" )\n).over(windows).as(""max_rating_tstamp"" )\n).filter(""rating_tstamp = max_rating_tstamp"" )\nWe’ll now use current_rating  as our ratings column for the purpose of downstream\ncalculation. Recall from before our ratings-based definition of user similarity:\nUSimA,B=∑x ∈ℛA,BrA,x−rArB,x−rB\n∑x ∈ℛA,BrA,x−rA2∑x ∈ℛA,BrB,x−rB2\nThe important values we’ll need are as follows:\nr− , −\nThe rating corresponding to a user-item pair\nr−\nThe average rating across all items for a user\nHydrating Your System | 79\nThe rows are already the r− , −  values, so let’s compute user average ratings, r−,\nand the rating deviations:\nfrom pyspark.sql.window  import Window\nfrom pyspark.sql  import functions  as F\nuser_partition  = Window.partitionBy ('user_id' )\nuser_item_ratings_sdf  = user_item_ratings_sdf .withColumn (\n""user_average_rating"" ,\nF.avg(""current_rating"" ).over(user_partition )\n)\nuser_item_ratings_sdf  = user_item_ratings_sdf .withColumn (\n""rating_deviation_from_user_mean"" ,\nF.col(""current_rating"" ) - F.col(""user_average_rating"" )\n)\nNow our schema should look like this (we’ve formatted it slightly nicer than the\ndefault Spark output):\n+-------+-------+------------+-------------+\n|book_id|user_id|rating_value |rating_tstamp |\n+-------+-------+------------+-------------+\n+-------------+-------------------+-------------------------------+\ncurrent_rating |user_average_rating |rating_deviation_from_user_mean |\n+-------------+-------------------+-------------------------------+\nLet’s finish creating a dataset that contains our User Similarity calculations:\nuser_pair_item_rating_deviations  = user_item_ratings_sdf .alias(""left_ratings"" )\n.join(user_item_ratings_sdf .alias(""right_ratings"" ),\n  (\nF.col(""left_ratings.book_id"" ) == F.col(""right_ratings.book_id"" ) &\\nF.col(""left_ratings.user_id"" ) != F.col(""right_ratings.user_id"" )\n),\n""inner""\n).select(\nF.col(""left_ratings.book_id"" ),\nF.col(""left_ratings.user_id"" ).alias(""user_id_1"" ),\nF.col(""right_ratings.user_id"" ).alias(""user_id_2"" ),\n  F.col(""left_ratings.rating_deviation_from_user_mean"" ).alias(""dev_1""),\n  F.col(""right_ratings.rating_deviation_from_user_mean"" ).alias(""dev_2"")\n).withColumn (\n'dev_product' ,\nF.col(""dev_1"")*F.col(""dev_2"")\n)\nuser_similarities_sdf  = user_pair_item_rating_deviations .groupBy(\n""user_id_1"" , ""user_id_2""\n).agg(\nsum('dev_product' ).alias(""dev_product_sum"" ),\nsum(F.pow(F.col(""dev_1""),2)).alias(""sum_of_sqrd_devs_1"" ),\n80 | Chapter 6: Data Processing\nsum(F.pow(F.col(""dev_2""),2)).alias(""sum_of_sqrd_devs_2"" )\n).withColumn (\n""user_similarity"" ,\n(\nF.col(""dev_product_sum"" ) / (\nF.sqrt(F.col(""sum_of_sqrd_devs_2"" )) *\nF.sqrt(F.col(""sum_of_sqrd_devs_2"" ))\n)\n)\n)\nIn constructing this dataset, we begin by taking a self-join, which avoids matching\nthe same users with themselves but rather joins on books that match. As we do this\njoin, we take the rating deviation from the user’s mean ratings that we computed\npreviously. We also use this opportunity to multiply them together for the numerator\nin our user similarity function. In the last step, we’ll groupBy  again so that we can\nsum over all matching book IDs (by groupBy  on user_id_1  and user_id_2 ); we sum\nthe product and the powers of each set of deviations so that we can finally divide and\ngenerate a new column for our user similarity.\nWhile this computation isn’t particularly complex, let’s take note of a few things\nthat we might appreciate. First, we built our user similarity matrix in full from\nour records. This matrix may now be stored in a faster-access format so that if we\nwish to do operations in real time, it’s ready to go. Second, we did all these data\ntransformations in Spark, so we can run these operations on massive datasets and let\nSpark handle the parallelization onto the cluster. We even were able to do this while\nwriting code that looks a lot like pandas and SQL. Finally, all our operations were\ncolumnar and required no iteration-based calculation. This means this code will scale\nmuch better than some approaches. This also ensures that Spark can parallelize our\ncode well, and we can expect good performance.\nWe’ve seen how PySpark can be used to prepare our user similarity matrix. We have\nthis definition of affinity estimating the appropriateness of an item for a user; we can\ncollect each of those scores into a tabular form—user rows and item columns—to\nyield a matrix. As an exercise, can you take this matrix and generate the affinity\nmatrix?\nAffA,i=rA+∑U ∈ NAUSimA,U*rU,i−rA\n∑U ∈ NAUSimA,U\nFeel free to assume that NA is just the five nearest neighbors to A with respect to\nuser similarity.\nHydrating Your System | 81",10265
44-DataLoaders.pdf,44-DataLoaders,"DataLoaders\nDataLoaders  is a programming paradigm originating from PyTorch, but it has been\nembraced in other gradient-optimized ML workflows. As we begin to integrate\ngradient-based learning into our recommendation system architectures, we will face\nchallenges in our MLOps tooling. The first is related to training data size and avail‐\nable memory. DataLoaders are a way to prescribe how data is batched and sent to\nthe training loop efficiently; as datasets get large, careful scheduling of these training\nsets can have major effects on learning. But why must we think about batches  of data?\nThat’s because we’ll use a variant of gradient descent appropriate for large amounts of\ndata.\nFirst, let’s review the basics of mini-batched gradient descent . During training via\ngradient descent, we make a forward pass of our training sample through our model\nto yield a prediction, and we then compute the error and the appropriate gradient\nbackward through our model to update parameters. Batched gradient descent takes\nall our data in a single pass to compute the gradient for the training set and push\nit back through; this implies you have the entire training dataset in memory. As the\ndataset scales, this ranges from expensive to impossible; to avoid this, we can instead\ncompute gradients of the loss function for only a subset of the dataset at a time.\nThe simplest paradigm for this, called stochastic gradient descent  (SGD), computes\nthese gradients and parameter updates one sample at a time. The mini-batched\nversion performs our batched gradient descent, but over a series of subsets to form a\npartition of our dataset. In mathematical notation, we write the update rule in terms\nof Jacobians on the smaller batches:\nθ=θ−η*∇θJθ;xi:i+n;yi:i+n\nThis optimization serves a few purposes. First, it requires only potentially small\nsubsets of our data held in memory during the steps. Second, it requires far fewer\npasses than the purely iterative version in SGD. Third, the gradient operating on\nthese mini-batches can be organized as a Jacobian, and thus we have linear-algebraic\noperations that may be highly optimized.\n82 | Chapter 6: Data Processing\nJacobians\nThe mathematical notion of a Jacobian in the simplest sense is\nan organizational tool for a set of vector derivatives with relevant\nindexes. Y ou may recall that for functions of several variables, you\ncan take the derivative with respect to  each of those variables. For a\nsingle multivariable scalar function, the Jacobian is simply the row\nvector of first derivatives of the function—which happens to be the\ntranspose of the gradient.\nThis is the simplest case; the gradient of a multivariable scalar func‐\ntion may be written as a Jacobian. However, once we have a vector\nof (vector) derivatives, we can write that as a matrix; the utility here\nis really only in the notation, though. When you collect a series\nof multivariable scalar functions into a vector of functions, the\nassociated vector of gradients is a vector of vectors of derivatives.\nThis is called a Jacobian matrix , and it generalizes the gradient to\nvector-valued functions. As you’ve likely realized, layers of neural\nnetworks are a great source of vector-valued functions for which\nyou’ d like to derivate.\nIf you’re convinced mini-batches are useful, it’s time to discuss DataLoaders —a sim‐\nple PyTorch API for facilitating mini-batch access from a large dataset. The key\nparameters for a DataLoader are batch_size , shuffle , and num_workers . The batch\nsize is easy to understand: it’s the number of samples included in each batch (often\nan integer factor of the total size of the dataset). Often a shuffle operation is applied\nin serving up these batches; the shuffle allows batches in each epoch to be shown to\nthe network in a randomized order; this is intended to improve robustness. Finally,\nnum_workers  is a parallelization parameter for the CPU’s batch generation.\nThe utility of a DataLoader is really best understood via demonstration:\nparams = {\n         'batch_size' : _,\n         'shuffle' : _,\n         'num_workers' : _\n}\ntraining_generator  = torch.utils.data.DataLoader (training_set , params)\nvalidation_generator  = torch.utils.data.DataLoader (validation_set , params)\n// Loop over epochs\nfor epoch in range(max_epochs ):\n    // Training\n    for local_batch , local_labels  in training_generator :\n        // Model computations\n        [...]\nHydrating Your System | 83",4507
45-Feature Stores.pdf,45-Feature Stores,"// Validation\n    with torch.set_grad_enabled (False):\n        for local_batch , local_labels  in validation_generator :\n            // Model computations\n            [...]\nThe first important detail in this code is that any of its generators will be reading in\nmini-batches from your total dataset and can be instructed to load those batches in\nparallel. Note also that any differential steps in the model computations will now be\noperating on these mini-batches.\nIt’s easy to think of DataLoaders as merely a tool for code cleanliness (which, admit‐\ntedly, it does improve), but it’s important to not underestimate how the control\nof batch order, parallelization, and shape are significant features for training your\nmodel. Lastly, the structure of your code now looks like batch gradient descent, but it\nis taking advantage of mini-batching, further exposing what your code actually does\ninstead of the steps necessary to do it.\nDatabase Snapshots\nLet’s round out this section by stepping back from these fancy technologies to discuss\nsomething important and classic: snapshotting a production database.\nAn extremely likely scenario is that the engineers (potentially also you) who have\nbuilt the recommendations server are writing their logs and other application data\nto an SQL database. More likely than not, this database architecture and deployment\nare optimized for fast querying by the application across its most common use cases.\nAs we’ve discussed, those logs may be in an event-style schema, and there are other\ntables that may require aggregation and roll-up to make any sense. For example, a\ncurrent inventory  table may require knowledge of start-of-day inventory and then\naggregate a list of purchase events.\nAll told, the production SQL database is usually a crucial component in the stack\nthat’s geared to specific use. As the downstream consumer of this data, you may\nfind yourself wanting different schemas, wanting lots of access to this database, and\nperforming serious operations on this data. The most common paradigm is database\nsnapshotting . Snapshotting is a functionality provided by various flavors of SQL to\nperformantly make a clone of a database. While this snapshotting may take form in a\nvariety of ways, let’s focus on a few that serve to simplify our systems and ensure they\nhave the necessary data on hand:\n•A daily table snapshot may be tied to an as_of  field, or the state of this table on •\nthis day .\n84 | Chapter 6: Data Processing\n•A daily table snapshot may be limited by time to see what records have been •\nadded today .\n•An event table snapshot may be used to feed a set of events into an event stream•\nprocessor like Segment (note that you may also set up live event streams like\nKafka).\n•An hourly aggregated table can be used for status logging or monitoring.•\nIn general, the paradigm is usually to operate on snapshots for downstream data\nprocessing. Many of the kinds of data processing we mentioned earlier—like comput‐\ning user similarity—are operations that may require significant data reads.  It’s impor‐\ntant to not build ML applications that require extensive querying on the production\ndatabase , because doing so would likely decrease performance of the app and result\nin a slower user experience. This decrease will undermine the improvement made\npossible by your recommendations.\nOnce you’ve snapshotted the tables you’re interested in, you can often find a collec‐\ntion of data pipelines useful to transform that data into even more specific tables\nin your data warehouse  (where you should be doing most of your work anyway).\nTools like Dagster, dbt, Apache Airflow, Argo, and Luigi are popular data-pipeline\nand workflow orchestration tools for extract, transform, load (ETL) operations.\nData Structures for Learning and Inference\nThis section introduces three important data structures that will enable our recom‐\nmendation system to perform complex operations quickly. The goal of each structure\nis to sacrifice precision as little as possible, while speeding up access to the data in real\ntime. As you’ll see, these data structures form the backbone of the real-time inference\npipeline and approximate what takes place in the batch pipeline as accurately as\npossible.\nThe three data structures are as follows:\n•Vector search/ANN index•\n•Bloom filters for candidate filtering•\n•Feature stores•\nSo far, we’ve discussed the necessary components for getting data flowing in your\nsystem. These help organize data to make it more accessible during the learning and\ninference processes. Also, we’ll find some shortcuts to speed up inference during\nretrieval. Vector search will allow us to identify similar items at scale. Bloom filters\nwill allow us to rapidly evaluate many criteria for excluding results. Feature stores will\nprovide us with necessary data about users for recommendation inference.\nData Structures for Learning and Inference | 85\nVector Search\nWe have discussed user similarity and item similarity in terms of understanding\nthe relationships between those entities, but we haven’t talked about any  acceleration\nstructures  for these processes.\nFirst let’s discuss a bit of terminology;. If we think of a collection of vectors that\nrepresent entities with a similarity metric provided by a distance function, we refer to\nthis as a latent space.  The simple goal is to utilize our latent space and its associated\nsimilarity metric (or complementary distance metric) to be able to retrieve similar\nitems quickly. In our previous examples with similarity, we talked about neighbor‐\nhoods of users and how they can be utilized to build an affinity score between users\nand unseen items. But how do you find the neighborhood?\nTo understand this, recall that we defined neighborhoods of an element x, written\nNx, as the set of k elements in the latent space with the maximum similarity;\nor said differently, the set of jth order statistics for j≤k from the sample of item\nsimilarities to x. These k-nearest neighbors , as they’re often called, will be used as the\nset of elements considered similar to x.\nThese vectors from CF yield a few other useful side effects:\n•A simple recommender that randomly samples unseen items from a user neigh‐•\nborhood’s liked items\n•Predictions about features of a user, from known features of users in the neigh‐•\nborhood\n•User segmentation via taste similarity•\nSo how can we speed up these processes? One of the first significant improvements in\nthis area came from inverted indices. Utilizing inverted indices is at its core carefully\nconstructing a large hash between tokens of the query (for text-based search) and the\ncandidates.\nThis approach is great for tokenizable entities like sentences or small-lexicon collec‐\ntions. Given the ability to look up items that share one or many tokens with the\nquery, you can even use a general latent embedding to rank the candidate responses\nby similarity. This approach deserves extra consideration as you scale: it incurs a\nspeed cost because it entails two steps, and because the similarity distribution may\nnot be well correlated with the token similarity required to return many more candi‐\ndates than we need.\nClassic approaches to building a search system are based on large lookup tables\nand feel deterministic. As we move toward ANN lookup, we want to relax some of\nthat strong deterministic behavior and introduce data structures that make assump‐\ntions to prune  these large indices. Instead of building indices for only tokenizable\n86 | Chapter 6: Data Processing\ncomponents of your elements, you could precompute the k-d tree and use the indices\nas the index. The k-d tree would precompute the nearest neighbors in a batch process\n(which may be slow), to populate a top- k response for fast lookup. k-d trees are an\nefficient data structure for encoding the preceding neighborhoods but are notoriously\nslow to read from in higher dimensions. Using them instead to build inverted indices,\nthough, can be a great improvement.\nMore recently, explicitly using vector databases with vector search is becoming much\nmore possible and feasible. Elasticsearch has added this capability; Faiss  is a Python\nlibrary that helps you implement this functionality in your systems; Pinecone  is a\nvector-database system explicitly targeting this goal; and Weaviate  is a native vector-\ndatabase architecture that allows you to layer the previous token-based inverted\nindices and vector similarity search.\nApproximate Nearest Neighbors\nWhat  are this element’s k-nearest neighbors? Incredibly, approximate nearest neigh‐\nbors (ANN) can get very high accuracy compared to the actual nearest neighbors,\nand you get there faster with head-spinning speedups. Y ou often are satisfied with\napproximate solutions to these problems.\nOne open source library that specializes in these approximations is PyNNDescent ,\nwhich uses clever speedups via both optimized implementation and careful mathe‐\nmatical tricks. With ANN, you are opened up to two strategies as discussed:\n•The pre-index can be dramatically improved.•\n•On queries without a pre-indexing option, you can still expect good perfor‐•\nmance.\nIn practice, these similarity lookups are incredibly important for making your appli‐\ncations actually work. While we’ve mostly talked about recommendations for full\nknown catalogs of items, we cannot assume this in other recommendation contexts.\nThese include the following:\n•Query-based recommendations (like search)•\n•Contextual recommendations•\n•Cold-starting new items•\nAs we go, you will see more and more references to similarity in spaces and nearest\nneighbors; at each of those moments, think: “I know how to make this fast!”\nData Structures for Learning and Inference | 87\nBloom Filters\nBloom filters  are probabilistic data structures that allow us to test for set inclusion\nvery efficiently but with a downside: set exclusion is deterministic, but set inclusion\nis probabilistic. In practice, this means that asking the question “Is x in this set” never\nresults in a false negative but may result in a false positive!  Note that this type-I error\nincreases as the size of the bloom increases.\nVia vector search, we have identified a large pool of potential recommendations for\nthe user. From this pool, we need to do some immediate elimination. The most\nobvious type of high-level filtering that’s essential is to remove those items that the\nuser has previously not shown interest in or has already purchased.  Y ou’ve probably had\nthe experience of being recommended the same item, over and over, and thinking, “I\ndon’t want this; stop showing me this. ” From the simple CF models we’ve introduced,\nyou may now see why this could happen.\nThe system has identified a set of items via CF that you’re more likely to pick.\nWithout any outside influence, those computations will continue to return the same\nresults, and you’ll never escape those recommendations. As the system designer, you\nmay start with a heuristic:\nIf the user has seen this item recommended three times and never clicked, let’s not\nshow it to them anymore.\nThis is a totally reasonable strategy to improve freshness  (the idea of ensuring users\nsee new item recommendations) in your recommendation system. While this is a\nsimple strategy to improve your recommendations, how might you implement this at\nscale?\nA bloom filter may be used by defining the sets in question with the following: “Has\nthis user seen this item recommended three times and never clicked?” Bloom filters\nhave a caveat that they’re additive only: once something is in the bloom, you can’t\nremove it. This is not a problem when observing a binary state like this heuristic.\nLet’s construct a user-item ID to use as our hash in the bloom. Remember that the\nkey feature of the bloom filter is to quickly determine whether the hashed item is\nin the bloom. When we observe a user-item pair that satisfies the preceding criteria,\ntake that pair as an ID and hash it. Now, because that hashed pair can be easily\nreconstructed from a list of items for a user, we have a very fast way to filter.\nLet’s discuss a few technical details on this topic. First, you might want to do a variety\nof kinds of filtering—maybe freshness is one, and another may be items the user has\nalready bought, and a third could exclude items that have sold out.\nHere it would be good to implement each of these filters independently; the first two\ncan follow our user-item ID hashing as before, and the third one can be a hash only\non item IDs.\n88 | Chapter 6: Data Processing\nAnother consideration is populating the bloom filters. It’s best practice to build these\nblooms from a database during the offline batch jobs. On whatever schedule your\nbatch training is run, rebuild your blooms from the records storage to ensure you’re\nkeeping your blooms accurate. Remember that blooms don’t allow deletion, so in the\nprevious example, if an item goes from sold out to restocked, your batch refresh of\nyour blooms can pick up the availability again. In between batch retraining, adding\nto a bloom is also very performant, so you can continue to add to the bloom as you\nobserve more data that needs to be considered for the filtering in real time. Be sure\nthese transactions are logged to a table, though! That logging will be important when\nyou want to refresh.\nFun Aside: Bloom Filters as the Recommendation System\nBloom filters not only provide an effective way to eliminate some recommendations\nbased on conditions for inclusion, but can also be used to do the recommending\nitself! In particular, “ An Item/User Representation for Recommender Systems Based\non Bloom Filters”  by Manuel Pozo et al. shows that for high-dimensional feature\nsets with a lot of sparsity (as we discussed in Chapter 3 ), the type of hashing bloom\nfilters do can help overcome some of the key challenges in defining good similarity\nfunctions!\nLet’s observe that we can do two natural operations on sets via the bloom filter data\nstructures. First, consider two sets A and B, and associate to them bloom filters ℬℱA\nand ℬℱB. Then what’s the definition of A∩B ? Can we come up with a bloom filter\nfor this intersection? Y ep! Recall that our bloom filters are guaranteed to tell us when\nan element is not contained in the set, but if an element is in the set, the bloom\nfilter can respond with only a certain probability. In this case, we’ d simply look for\nelements that are in according to ℬℱA AND  in according to ℬℱB. Of course, the set\nof items returned as in each set is larger than the actual set (i.e., A ⊂ℬℱA), so the\nintersection will also be larger:\nA∩B ⊂ ℬℱA∩ℬℱB\nNote that you can compute the exact difference in cardinality via information about\nyour choice of hashing functions. Also note that the equation is an abuse of notation\nby calling ℬℱA the set of things returned by the bloom filter corresponding to A.\nSecond, we also need to construct the union. This is similarly easy by considering\nelements that are in according to ℬℱA OR in according to ℬℱB. And so, similarly:\nA∪B ⊂ ℬℱA∪ℬℱB\nData Structures for Learning and Inference | 89\nNow, if we consider items X and Y as concatenated vectors of potentially many\nfeatures, and hash those concatenated features, we are representing each of them as\nthe bitwise vectors of our bloom. From before, we saw that the intersection of two\nblooms makes sense, and in fact is equivalent to the bitwise AND  of their bloom\nrepresentations. This means two items’ feature similarities can be expressed by the\nbitwise and similarity of their bloom hashes:\nsimX,Y=ℬℱX∩ℬℱY =ℬℱX*bitwiseℬℱX\nFor static datasets, this method has real advantages, including speed, scalability, and\nperformance. Limitations are based on a variety of features and on the ability to\nchange the set of possible items. Later we will discuss locally sensitive hashing , which\nfurther iterates on lookup speed with lower risks of collision in high-dimensional\nspaces, and some similar ideas will reemerge.\nFeature Stores\nSo far, we have focused on recommendation systems that we might call pure collab‐\norative filtering . We’ve made use of the user- or item-similarity data only when\nattempting to make good recommendations. If you’ve been wondering, “Hey, what\nabout information about the actual users and items?” your curiosity will now be\nsated.\nThere are a huge variety of reasons you could be interested in features in addition to\nyour previous CF methods. Let’s list a few high-level concerns:\n•Y ou may wish to show new users a specific set of items first.•\n•Y ou may wish to consider geographic boundaries in your recommendations.•\n•Distinguishing between children and adults may be important for the types of•\nrecommendations they’re given.\n•Item features may be used to ensure high-level diversity in the recommendations•\n(more to come in Chapter 15 ).\n•User features can enable various kinds of experimental testing.•\n•Item features could be used to group items into sets for contextual recommenda‐•\ntions (more to come in Chapter 15 ).\nIn addition to these issues, another kind of feature is often essential: real-time fea‐\ntures. While the point of feature stores is to provide real-time access to all the neces‐\nsary features, it’s worthwhile to distinguish stable features that change infrequently\nfrom real-time features that we anticipate will change often.\n90 | Chapter 6: Data Processing\nSome important examples of a real-time feature store are dynamic prices, current\nitem availability, trending  status, wish-list status, and so on. These features may\nchange throughout the day, and we want their values in the feature store to be\nmutable in real-time via other services and systems. Therefore, the real-time feature\nstore will need to provide API access for feature mutation. This is something you may\nnot want to provide for stable  features.\nWhen we design our feature store, we’re likely to want the stable features to be built\nfrom data warehouse tables via ETLs and transformations, and we likely want the\nreal-time features to be built this way as well, but on a faster schedule or allowing API\naccess for mutation. In either case, the key quality of a feature store is very fast read\naccess . It’s often a good idea to separately build feature stores for offline training of\nmodels that can be built in test to ensure support for new models.\nSo how might the architecture and implementation look? See Figure 6-3 .\nFigure 6-3. Demonstration of a feature store\nDesigning a feature store involves designing pipelines that define and transform the\nfeatures into that store  (coordinated via things like Airflow, Luigi, Argo, etc.) and\noften look similar to the type of data pipelines used in building our collector. One\nadditional complication that the feature store needs to concern itself with is a speed\nlayer. During our discussion of the lambda architecture earlier in this chapter, we\nmentioned that we can think of batch data processing for the collector and a more\nrapid speed layer for intermediary updates, but this is even more important for the\nfeature store. The feature store may also need a streaming layer . This layer operates\non continuous streams of data and can perform data transformations on those; it\nthen writes the appropriate output to the online feature store in real time. This adds\ncomplexity because data transformations on streaming data present a very different\nset of challenges and often require different algorithmic strategies. Some technologies\nthat help here are Spark Streaming and Kinesis. Y ou’ll also need to configure the\nsystem to properly handle the data stream, the most common of which is  Kafka. Data\nstreaming layers involve many components and architectural considerations that fall\nData Structures for Learning and Inference | 91\noutside our scope; if you’re considering getting started with Kafka, check out Kafka:\nThe Definitive  Guide  by Gwen Shapira et al. (O’Reilly).\nA feature store also needs a storage layer ; many approaches exist here, but using a\nNoSQL database is common, especially in the online feature store. The reason is\nfaster retrieval and the nature of the data storage. Feature stores for recommendation\nsystems tend to be very key based (i.e., get the features for this user , or get the\nfeatures for this item ), which lend themselves well to key-value stores. Some example\ntechnologies here are DynamoDB, Redis, and Cassandra. The storage layer for an\noffline feature store may simply be an SQL-style database to reduce complexity, but\ninstead you’ll pay a tax of a delta between offline and online. This delta and others\nlike it are called training-serving skew .\nA unique but essential aspect of feature stores is the registry . A registry is incredibly\nuseful for a feature store because it coordinates existing features and information on\nhow they’re defined. A more sophisticated instance of a registry also includes input\nand output schemas with typing, and distributional expectations. These are contracts\nthat the data pipelines must adhere to and satisfy to avoid populating your feature\nstore with garbage data. Additionally, the registry’s definitions allow parallel data\nscientists and ML engineers to develop new features, use one another’s features, and\ngenerally understand the assumptions of features their models may utilize.\nOne important advantage of these registries is that they incentivize alignment\nbetween teams and developers. In particular, if you decide you care about country\nfor your user, and you see a feature country  in the registry, you’re more likely to use\nthat (or ask the developer who’s assigned to this feature in the registry) than to make\na new one from scratch. Practically, data scientists make hundreds of small decisions\nand assumptions when defining their models, and this removes some of that load\nthat’s relying on the existing resources.\nModel Registries\nA closely related concept to feature registries is model registries.\nThe concepts have a lot in common, but we caution you to think\nof them differently. A great model registry can have type contracts\nfor the input and output of your models, and can serve many of\nthe same benefits around alignment and clarity. A feature registry\nshould really be focused on definitions of the business logic and\nfeatures. Because feature engineering can also be model driven,\nspeaking clearly about the differences between these two things can\nbe challenging, so to sum it up, we’ll focus on what they serve: a\nmodel registry concerns itself with ML models and the relevant\nmetadata, whereas a feature registry concerns itself with features\nthat models will use.\n92 | Chapter 6: Data Processing\nFinally, we need to talk about serving  these features. Backed by the appropriately\nperformant storage layer, we need to serve via API request the necessary feature\nvectors. Those feature vectors are details about the user that the model will need\nwhen serving recommendations—for example, the user’s location or content age\nrestrictions. The API can serve back the entire set of features for the key or allow for\nmore specification. Often the responses are JSON serialized for fast data transfer. It’s\nimportant that the features being served are the most up-to-date set of features , and\nlatency here is expected to be < 100 ms for more serious industrial applications.\nOne important caveat here is that for offline training, these feature stores need to\naccommodate  time travel . Because our goal during training is to give the model the\nappropriate data to learn in the most generalizable way , when training our model, it’s\ncrucial to not give it access to features out of time. This is called data leakage  and\ncan cause massive divergence in performance between training and production. The\nfeature store for offline training thus must have knowledge of the features through\ntime, so that during training, a time index may be provided to get the features as they\nwere then. These as_of  keys can be tied to the historical training data as we replay  the\nhistory of what the user-item interactions looked like.\nWith these pieces in place—and the important monitoring this system needs—you’ll\nbe able to serve offline and online features to your models. In Part III , you will see\nmodel architectures that make use of them.\nData Leakage\nY ou’re likely familiar with the concept of leakage based on what you know about ML:\ncorrupted performance metrics result because the training of the model had access to\ndata that was supposed to be reserved for model performance evaluation.\nData leakage in ML is divided into feature leakage  and training example leakage . For\nrecommendation systems, data leakage has the additional challenge of temporal leak‐\nage, or nonstationarity leakage. The real danger is that in recommendation systems,\nwe see the same observational unit, a user, over and over, and observe a datum each\ntime we see them. When we see them, other aspects of the system may have changed,\nand in reality we want to use features in our model that are the most up to date\nas of that observation. Both in features and training examples, to avoid leakage we\nneed to always be thinking of our system’s timeline. This is why data preparation for\nrecommendation systems is inherently time dependent. Y ou will see in Chapter 11\nthat our accuracy metrics will need to explicitly consider train-test splitting with\nrespect to this time axis, and then be further grouped by the user. This also means\nthat training recommendation systems often requires more resources than many\nother task types.\nData Structures for Learning and Inference | 93",25961
46-Chapter 7. Serving Models and Architectures.pdf,46-Chapter 7. Serving Models and Architectures,"Summary\nWe’ve discussed not only the crucial components necessary to hydrate your system\nand serve recommendations, but also some of the engineering building blocks needed\nto make those components a reality. Equipped with data loaders, embeddings, feature\nstores, and retrieval mechanisms, we are ready to start constructing our pipeline and\nsystem topology.\nIn the next chapter, we’ll focus our sights on MLOps and the rest of the engineering\nwork required to build and iterate on these systems. It’s going to be important for us\nto think carefully about deployment and monitoring so our recommendation systems\nare constrained to life in IPython Notebooks.\nContinue onward to see the architectural considerations to move to production.\n94 | Chapter 6: Data Processing",779
47-Item-to-User Recommendations.pdf,47-Item-to-User Recommendations,"CHAPTER 7\nServing Models and Architectures\nAs we think about how recommendation systems utilize the available data to learn\nand eventually serve recommendations, it’s crucial to describe how the pieces fit\ntogether. The combination of the data flow and the jointly available data for learning\nis called the architecture . More formally, the architecture is the connections and\ninteractions of the system or network of services; for data applications, the architec‐\nture also includes the available features and objective functions for each subsystem.\nDefining the architecture typically involves identifying components or individual\nservices, defining the relationships and dependencies among those components, and\nspecifying the protocols or interfaces through which they will communicate.\nIn this chapter, we’ll spell out some of the most popular and important architectures\nfor recommendation systems.\nArchitectures by Recommendation Structure\nWe have returned several times to the concept of collector, ranker, and server, and\nwe’ve seen that they may be regarded via two paradigms: the online and the offline\nmodes. Further, we’ve seen how many of the components in Chapter 6  satisfy some of\nthe core requirements of these functions.\nDesigning large systems like these requires several architectural considerations. In\nthis section, we will demonstrate how these concepts are adapted based on the type of\nrecommendation system you are building. We’ll compare a mostly standard item-to-\nuser recommendation system, a query-based recommendation system, context-based\nrecommendations, and sequence-based recommendations.\n95",1649
48-Query-Based Recommendations.pdf,48-Query-Based Recommendations,"Item-to-User Recommendations\nWe’ll  start by describing the architecture of the system we’ve been building in the\nbook thus far. As proposed in Chapter 4 , we built the collector offline to ingest and\nprocess our recommendations. We utilize representations to encode relationships\nbetween items, users, or user-item pairs.\nThe online collector takes the request, usually in the form of a user ID, and finds a\nneighborhood of items in this representation space to pass along to the ranker. Those\nitems are filtered when appropriate and sent for scoring.\nThe offline ranker learns the relevant features for scoring and ranking, training on\nthe historical data. It then uses this model and, in some cases, item features as well for\ninference.\nIn the case of recommendation systems, this inference computes the scores associated\nto each item in the set of potential recommendations. We usually sort by this score,\nwhich you’ll learn more about in Part III . Finally, we integrate a final round of\nordering based on some business logic (described in Chapter 14 ). This last step is part\nof the serving, where we impose requirements like test criteria or recommendation\ndiversity requirements.\nFigure 7-1  is an excellent overview of the retrieval, ranking, and serving structure,\nalthough it depicts four stages and uses slightly different terminology. In this book,\nwe combine the filtering stage shown here into retrieval.\nQuery-Based Recommendations\nTo start off our process, we want to make a query. The most obvious example of\na query is a text query as in text-based search engines; however, queries may be\nmore general! For example, you may wish to allow search-by-image or search-by-tag\noptions. Note that an important type of query-based recommender uses an implicit\nquery: the user is providing a search query via UI choices or by behaviors. While\nthese systems are quite similar in overall structure to the item-to-user systems, let’s\ndiscover how to modify them to fit our use case.\n96 | Chapter 7: Serving Models and Architectures\nFigure 7-1. A four-stage recommendation system (adapted from an image by Karl Higley\nand Even Oldridge)\nWe want to integrate more context about the query into the first step of the request.\nNote that we don’t want to throw out the user-item matching components of this\nsystem. Even though the user is performing a search, personalizing the recommenda‐\ntions based on their taste is useful. Instead, we need to utilize the query as well; later\nwe will discuss various technical strategies, but a simple summary for now is to also\ngenerate an embedding for the query. Note that the query is like an item or a user but\nis sufficiently different.\nSome strategies might include similarity between the query and items, or co-\noccurrence of the query and items. Either way, we now have a query representation\nand user representation, and we want to utilize both for our recommendation. One\nsimple approach is to use the query representation for retrieval, but during the scor‐\ning, score via both query-item and user-item, combining them via a multiobjective\nloss. Another approach is to use the user for retrieval and then the query for filtering.\nArchitectures by Recommendation Structure | 97",3271
49-Sequence-Based Recommendations.pdf,49-Sequence-Based Recommendations,"Different  Embeddings\nUnfortunately, while we’ d love the same embedding space (for\nnearest-neighbors lookup) to work well for our queries and our\ndocuments (items, etc.), this is often not the case. The simplest\nexample is something like asking questions and hoping to find\nrelevant Wikipedia articles. This problem is often referred to as the\nqueries being “out of distribution” from the documents.\nWikipedia articles are written in a declarative informative article\nstyle, whereas questions are often brief and casual. If you were to\nuse an embedding model focused on capturing semantic meaning,\nyou’ d naively expect the queries to be located in significantly differ‐\nent subspaces than the articles. This means that your distance com‐\nputations will be affected. This is often not a huge problem because\nyou retrieve via relative distances, and you can hope that the shared\nsubspaces are enough to provide a good retrieval. However, it can\nbe hard to predict when these perform poorly.\nThe best practice is to carefully examine the embeddings on com‐\nmon queries and on target results. These problems can be especially\nbad on implicit queries like a series of actions taken at a particular\ntime of day to look up food recommendations. In this case, we\nexpect the queries to be wildly different from the documents.\nContext-Based Recommendations\nA context is quite similar to a query but tends to be more obviously feature based and\nfrequently less similar to the items/users distributions. Context  is usually the term\nused to represent exogenous features to the system that may have an effect on the\nsystem—i.e., auxiliary information such as time, weather, or location. Context-based\nrecommendation is similar to query based in that context is an additional signal\nthat the system needs to consider during recommendation, but more often than\nnot, the query should dominate the signal for recommendation, whereas the context\nshould not.\nLet’s take a simple example of ordering food. A query for a food-delivery recommen‐\ndation system would look like Mexican food ; this is an extremely important signal\nfrom the user looking for burritos or quesadillas of how the recommendations should\nlook. A context for a food-delivery recommendation system would look like it’s\nalmost lunchtime . This signal is useful but may not outweigh user personalization.\nPutting hard-and-fast rules on this weighting can be difficult, so usually we don’t, and\ninstead we learn parameters via experimentation.\n98 | Chapter 7: Serving Models and Architectures",2579
50-Encoder Architectures and Cold Starting.pdf,50-Encoder Architectures and Cold Starting,"Context features fit into the architecture similar to the way queries do, via learned\nweightings as part of the objective function. Y our model will learn a representation\nbetween context features and items, and then add that affinity into the rest of the\npipeline. Again, you can make use of this early in the retrieval, later in the ranking, or\neven during the serving step.\nSequence-Based Recommendations\nSequence-based recommendations  build  on context-based recommendations but with\na specific type of context. Sequential recommendations are based on the idea that the\nrecent items the user has been exposed to should have a significant influence on the\nrecommendations. A common example here is a music-streaming service, as the last\nfew songs that have been played can significantly inform what the user might want to\nhear next. To ensure that this autoregressive , or sequentially predictive, set of features\nhas an influence on recommendations, we can treat each item in the sequence as a\nweighted context for the recommendation.\nUsually, the item-item representation similarities are weighted to provide a collection\nof recommendations, and various strategies are used for combining these. In this\ncase, we normally expect the user to be of high importance in the recommendations,\nbut the sequence is also of high importance. One simple model is to think of the\nsequence of items as a sequence of tokens, and form a single embedding for that\nsequence—as in NLP applications. This embedding can be used as the context in a\ncontext-based recommendation architecture.\nNaive Sequence Embeddings\nThe combinatorics of one-embedding-per-sequence explode in\ncardinality; the number of potential items in each sequential slot\nis very large, and each item in the sequence multiplies those possi‐\nbilities together. Imagine, for example, five-word sequences, where\nthe number of possibilities for each item is close to the size of the\nEnglish lexicon, and thus it would be that size to the fifth power.\nWe provide simple strategies for dealing with this in Chapter 17 .\nWhy Bother with Extra Features?\nSometimes it is useful to step back and ask if a new technology is actually worth\ncaring about. So far in this section, we’ve introduced four new paradigms for thinking\nabout a recommender problem. That level of detail may seem surprising and poten‐\ntially even unnecessary.\nOne of the core reasons that things like context- and query-based recommendations\nbecome relevant is to deal with some of the issues mentioned before around sparsity\nand cold starting. Sparsity makes things that aren’t cold seem cold via the learner’s\nArchitectures by Recommendation Structure | 99\nunderexposure to them, but true cold starting also exists because of new items being\nadded to catalogs with high frequency in most applications. We will address cold\nstarting in detail, but for now, suffice it to say that one strategy for warm starting is to\nuse other features that are available even in this regime.\nIn applications of ML that are explicitly feature based, we rarely battle the cold-start\nproblem to such a degree, because at inference time we’re confident that the model\nparameters useful for prediction are well aligned with those features that are avail‐\nable. In this way, feature-included recommendation systems are bootstrapping from a\npotentially weaker learner that has more guaranteed performance via always-available\nfeatures.\nThe second analogy that the previous architectures are reflecting is that of boosting.\nBoosted models operate via the observation that ensembles of weaker learners can\nreach better performance. Here we are asking for some additional features to help\nthese networks ensemble with weak learners, to boost their performance.\nEncoder Architectures and Cold Starting\nThe previous problem framings of various types of recommendation problems point\nout four model architectures, each fitting into our general framework of collector,\nranker, and server. With this understanding, let’s discuss in a bit more detail how\nmodel architecture can become intertwined with serving architecture. In particular,\nwe also need to discuss feature encoders.\nThe key opportunity from encoder-augmented systems is that for users, items, or\ncontexts without much data, we can still form embeddings on the fly. Recall from\nbefore that our embeddings make the rest of our system possible, but cold-starting\nrecommendations is a huge challenge.\nThe two-towers architecture —or dual-encoder networks—introduced in “Sampling-\nBias-Corrected Neural Modeling for Large Corpus Item Recommendations”  by\nXinyang Yi et al. is shown in Figure 7-2  explicit model architecture is aimed at\nprioritizing features of both the user and items when building a scoring model for\na recommendation system. We’ll see a lot more discussion of matrix factorization\n(MF), which is a kind of latent collaborative filtering (CF) derived from the user-item\nmatrix and some linear algebraic algorithms. In the preceding section, we explained\nwhy additional features matter. Adding these side-car  features into an MF paradigm\nis possible and has shown to be successful—for example, applications CF for implicit\nfeedback , factorization machines , and SVDFeature . However, in this model we will\ntake a more direct approach.\n100 | Chapter 7: Serving Models and Architectures\nFigure 7-2. The two towers responsible for the two embeddings\nIn this architecture, we take the left tower to be responsible for items and the right\ntower to be responsible for the user and, when appropriate, context. These two tower\narchitectures are inspired by the NLP literature and, in particular, “Learning Text\nSimilarity with Siamese Recurrent Networks”  by Paul Neculoiu et al.\nLet’s detail how this model architecture is applied to recommending videos on Y ou‐\nTube. For a full overview of where this architecture was first introduced, see “Deep\nNeural Networks for Y ouTube Recommendations”  by Paul Covington et al. Training\nlabels will be given by clicks, but with an additional regression feature ri∈0, 1,\nwhere the minimum value corresponds to a click but trivial watch time, and the\nmaximum of the range corresponds to a full watch.\nAs we’ve mentioned, this model architecture will explicitly include features from both\nuser and items. The video features will consist of categorical and continuous features,\nlike VideoId , ChannelId , VideoTopic , and so on. An embedding layer is used for\nmany of the categorical features to move to dense representations. The user features\ninclude watch histories via bag of words and standard user features.\nThis model structure combines many of the ideas you’ve seen before but has relevant\ntakeaways for our system architecture. First is the idea of sequential training. Each\ntemporal batch  of samples should be trained in sequence to ensure that model drift is\nshown to the model; we will discuss prequential datasets in “Prequential validation”\non page 197. Next, we present an important idea for the productionizing of these\nkinds of models: encoders.\nEncoder Architectures and Cold Starting | 101\nIn these models, we have feature encoders as the early layers in both towers, and\nwhen we move to inference, we will still need these encoders. When performing the\nonline recommendations, we will be given UserId  and VideoId  and will first need to\ncollect their features. As discussed in “Feature Stores”  on page 90, the feature store\nwill be useful in getting these raw features, but we need to also encode the features\ninto the dense representations necessary for inference. This is something that can be\nstored in the feature store for known entities, but for unknown entities we will need\nto do the feature embedding at inference time.\nEncoding layers serve as a simple model for mapping a collection of features to\na dense representation. When fitting encoding layers as the first step in a neural\nnetwork, the common strategy is to take the first k layers and reuse them as an\nencoder model. More specifically, if ℒi, 0 ≤i≤k are the layers responsible for feature\nencoding, call EmbV=ℒkℒk− 1...ℒ0V  the function that maps a feature\nvector V to its dense representation.\nIn our previous system architecture, we would include this encoder as part of the fast\nlayer, after receiving features from the feature store. It’s also important to note that\nwe would still want to utilize vector search; these feature embedding layers are used\nupstream of the vector search and nearest neighbor searches.\nEncoder as a Service\nEncoders  and retrieval are a key part of the multistage recommen‐\ndation pipeline. We’ve spoken briefly about the latent spaces in\nquestion (for more details, see “Latent Spaces” on page 167), and\nwe’ve alluded to an encoder . Briefly, an encoder is the model that\nconverts users, items, queries, etc., into the latent space in which\nyou’ll perform nearest-neighbors search. These models can be\ntrained via a variety of processes, many of which will be discussed\nlater, but it’s important to discuss where they live once trained.\nEncoders  are often simple API endpoints that take the content to\nbe embedded and return a vector (a list of floats). Encoders often\nwork at the batch layer to encode all the documents/items that will\nbe retrieved, but they must also be connected to the real-time layer\nto encode the queries as they come in. A common pattern is to\nset up a batch endpoint and a single query endpoint to facilitate\noptimization for both modalities. These endpoints should be fast\nand highly available.\nIf you’re working with text data, a good starting place is to use\nBERT or GPT-based embeddings. The easiest at this time are pro‐\nvided as a hosted service from OpenAI.\n102 | Chapter 7: Serving Models and Architectures",9898
51-Deployment.pdf,51-Deployment,,0
52-Workflow Orchestration.pdf,52-Workflow Orchestration,"Deployment\nLike  many ML applications, the final output of a recommendation system is itself a\nsmall program that runs continuously and exposes an API to interact with it; batch\nrecommendations are often a powerful place to start, performing all the necessary\nrecommendations ahead of time. Throughout this chapter, we’ve seen the pieces\nembedded in our backend system, but now we will discuss the components closer to\nthe user.\nIn our relatively general architecture, the server is responsible for handing over the\nrecommendations, after all the work that comes before, and should adhere to a preset\nschema. But what does this deployment look like?\nModels as APIs\nLet’s  discuss two systems architectures that might be appropriate for serving your\nmodels in production: microservice and monolith.\nIn web applications, this dichotomy is well covered from many perspectives and\nspecial use cases. As ML engineers, data scientists, and potentially data platform\nengineers, it’s not necessary to dig deep into this area, but it’s essential to know the\nbasics:\nMicroservice architectures\nEach  component of the pipeline should be its own small program with a clear\nAPI and output schema. Composing these API calls allows for flexible and\npredictable pipelines.\nMonolithic architectures\nOne application should contain all the necessary logic and components for model\npredictions. Keeping the application self-contained means fewer interfaces that\nneed to be kept aligned and fewer rabbit holes to hunt around in when a location\nin your pipeline is being starved.\nWhatever you choose as your strategy, you’ll need to make a few decisions:\nHow large is the necessary application?\nIf your application will need fast access to large datasets at inference time, you’ll\nneed to think carefully about memory requirements.\nWhat access does your application need?\nWe’ve previously discussed using technologies like bloom filters and feature\nstores. These resources may be tightly coupled to your application (by building\nthem in memory in the application) or may be an API call away. Make sure your\ndeployment accounts for these relationships.\nDeployment | 103\nShould your model be deployed to a single node or a cluster?\nFor some model types, even at the inference step we wish to utilize  dis‐\ntributed computing. This will require additional configuration to allow for fast\nparallelization.\nHow much replication do you need?\nHorizontal scaling allows you to have multiple copies of the same service running\nsimultaneously to reduce the demand on any particular instance. This is impor‐\ntant for ensuring availability and performance. As we horizontally scale, each\nservice can operate independently, and various strategies exist for coordinating\nthese services and an API request. Each replica is usually its own containerized\napplication, and these APIs like CoreOS and Kubernetes are used to manage\nthese. The requests themselves must also be balanced to the different replicas via\nsomething like nginx.\nWhat are the relevant APIs that are exposed?\nEach application in the stack should have a clear set of exposed schemas and an\nexplicit communication about the types of other applications that may call to the\nAPIs.\nSpinning Up a Model Service\nSo what can you use to get your model into an application? A variety of frameworks\nfor application development are useful; some of the most popular in Python are Flask,\nFastAPI, and Django. Each has different advantages, but we’ll discuss FastAPI here.\nFastAPI is a targeted framework for API applications, making it especially well fit for\nserving ML models. It calls itself an asynchronous server gateway interface (ASGI)\nframework, and its specificity grants a ton of simplicity.\nLet’s take a simple example of turning a fit torch model into a service with the\nFastAPI framework. First, let’s utilize an artifact store to pull down our fit model.\nHere we are using the Weights & Biases artifact store:\nimport wandb, torch\nrun = wandb.init(project=Prod_model , job_type =""inference"" )\nmodel_dir  = run.use_artifact (\n'bryan-wandb/recsys-torch/model:latest' ,\ntype='model'\n).download ()\nmodel = torch.load(model_dir )\nmodel.eval(user_id)\nThis looks just like your notebook workflow, so let’s see how easy it is to integrate this\nwith FastAPI:\n104 | Chapter 7: Serving Models and Architectures\nfrom fastapi import FastAPI # FastAPI code\nimport wandb, torch\napp = FastAPI() # FastAPI code\nrun = wandb.init(project=Prod_model , job_type =""inference"" )\nmodel_dir  = run.use_artifact (\n'bryan-wandb/recsys-torch/model:latest' ,\ntype='model'\n).download ()\nmodel = torch.load(model_dir )\n@app.get(""/recommendations/ {user_id} "") # FastAPI code\ndef make_recs_for_user (user_id: int): # FastAPI code\nendpoint_name  = 'make_recs_for_user_v0'\nlogger.info(\n""{'type': 'recommendation_request',""\nf""'arguments': {'user_id' : {user_id}},""\nf""'response': {None}},"",\nf""'endpoint_name': {endpoint_name }""\n)\nrecommendation  = model.eval(user_id)\nlogger.log(\n""{'type': 'model_inference',""\nf""'arguments': {'user_id' : {user_id}},""\nf""'response': {recommendation }},""\nf""'endpoint_name': {endpoint_name }""\n)\n    return { # FastAPI code\n""user_id"" : user_id,\n""endpoint_name"" : endpoint_name ,\n""recommendation"" : recommendation\n}\nI hope you share my enthusiasm that we now have a model as a service in five addi‐\ntional lines of code. While this scenario includes simple examples of logging, we’ll\ndiscuss logging in greater detail later in this chapter to help you improve observability\nin your applications.\nWorkflow  Orchestration\nThe other component necessary for your deployed system is workflow orchestration.\nThe model service is responsible for receiving requests and serving results, but many\nsystem components need to be in place for this service to do anything of use. These\nworkflows have several components, so we will discuss them in sequence: containeri‐\nzation, scheduling, and CI/CD.\nDeployment | 105\nContainerization\nWe’ve  discussed how to put together a simple service that can return the results,\nand we suggested using FastAPI; however, the question of environments is now\nrelevant. When executing Python code, it is important to keep the environment\nconsistent if not identical. FastAPI is a library for designing the interfaces; Docker  is\nthe software that manages the environment that code runs in. It’s common to hear\nDocker described as a container or containerization tool: this is because you load a\nbunch of apps—or executable components of code—into one shared environment.\nWe have a few subtle things to note at this point. The meaning of  environment\nencapsulates both the Python environment of package dependencies and the larger\nenvironment, including the operating system or GPU drivers. The environment is\nusually initialized from a predetermined image  that installs the most basic aspects\nof what you’ll need access to and in many cases is less variable across services to\npromote consistency and standardization. Finally, the container is usually equipped\nwith a list of infrastructure code necessary to work wherever it is to be deployed.\nIn practice, you specify details of the Python environment via your requirements  file,\nwhich consists of a list of Python packages. Note that some library dependencies are\noutside Python and will require additional configuration mechanisms. The operating\nsystem and drivers are usually built as part of a base image; you can find these\non DockerHub or similar. Finally, infrastructure as code  is a paradigm wherein you\nwrite code to orchestrate the necessary steps in getting your container configured to\nrun in the infrastructure it will be deployed into. Dockerfile and Docker Compose\nare specific to the Docker container interfacing with infrastructure, but you can\nfurther generalize these concepts to include other details of the infrastructure. This\ninfrastructure as code begins to encapsulate provisioning of resources in your cloud,\nsetting up open ports for network communication, access control via security roles,\nand more. A common way to write this code is in Terraform. This book doesn’t\ndive into infrastructure specification, but infrastructure as code is becoming a more\nimportant tool to the ML practitioner. Many companies are beginning to attempt to\nsimplify these aspects of training and deploying systems including Weights & Biases\nor Modal.\nScheduling\nTwo  paradigms exist for scheduling jobs: cron and triggers. Later we’ll talk more\nabout the continuous training loop and active learning processes, but upstream of\nthose is your ML workflow. ML workflows are a set of ordered steps necessary to\nprepare your model for inference. We’ve introduced our notion of collector, ranker,\nand server, which are organized into a sequence of stages for recommendation sys‐\ntems—but these are the three coarsest elements of the system topology.\n106 | Chapter 7: Serving Models and Architectures\nIn ML systems, we frequently assume that there’s an upstream stage of the workflow\nthat corresponds to data transformations, as discussed in Chapter 6 . Wherever that\nstage takes place, the output of those transformations results in our vector store—\nand potentially the additional feature stores. The handoff between those steps and\nthe next steps in your workflow are the result of a job scheduler. As mentioned\npreviously, tools like Dagster and Airflow can run sequences of jobs with dependent\nassets. These kinds of tools are needed to orchestrate the transitions and to ensure\nthat they’re timely.\nCron  refers  to a time schedule where a workflow should begin—for example, hourly\nat the top of the hour or four times a day. Triggers  refers  to the instigation of a\njob run when another event has taken place—for example, if an endpoint receives a\nrequest, or a set of data gets a new version, or a limit of responses is exceeded. These\nare meant to capture more ad hoc relationships between the next job stage and the\ntrigger. Both paradigms are very important.\nCI/CD\nY our  workflow execution system is the backbone of your ML systems, often the\nbridge between the data collection process, the training process, and the deployment\nprocess. Modern workflow execution systems also include automatic validation and\ntracking so that you can audit the steps on the way to production.\nContinuous integration  (CI) is a term taken from software engineering to enforce\na set of checks on new code in order to accelerate the development process. In\ntraditional software engineering, this comprises automating unit and integration\ntesting, usually run after checking the code into version control. For ML systems, CI\nmay mean running test scripts against the model, checking the typed output of data\ntransformations, or running validation sets through the model and benchmarking the\nperformance against previous models.\nContinuous deployment  (CD) is also a term popularized in software engineering to\nrefer to automating the process of pushing new packaged code into an existing sys‐\ntem. In software engineering, deploying code when it has passed the relevant checks\nspeeds development and reduces the risk of stale systems. In ML, CD can involve\nstrategies like automatically deploying your new model behind a service endpoint in\nshadow (which we’ll discuss in “Shadowing”  on page 115) to test that it works as\nexpected under live traffic. It could also mean deploying a model behind a very small\nallocation of an A/B test or multiarm bandit treatment to begin to measure effects on\ntarget outcomes. CD usually requires effective triggering by the requirements it has to\nsatisfy before being pushed. It’s common to hear CD utilizing a model registry, where\nyou house and index variations on your model.\nDeployment | 107",11912
53-Alerting and Monitoring.pdf,53-Alerting and Monitoring,,0
54-Evaluation in Production.pdf,54-Evaluation in Production,"Alerting and Monitoring\nAlerting and monitoring take a lot of their inspiration from the DevOps world\nfor software engineering. Here are some high-level principles that will guide our\nthinking:\n•Clearly defined schemas and priors•\n•Observability•\nSchemas and Priors\nWhen  designing software systems, you almost always have expectations about how\nthe components fit together. Just as you anticipate the input and output to functions\nwhen writing code, in software systems you anticipate these at each interface. This\nis relevant not only for microservice architectures; even in a monolith architecture,\ncomponents of the system need to work together and often have boundaries between\ntheir defining responsibilities.\nLet’s make this more concrete via an example. Y ou’ve built a user-item latent space,\na feature store for user features, a bloom filter for client avoids (things the client\nspecifically tells you they don’t want), and an experiment index that defines which\nof two models should be used for scoring. First let’s examine the latent space; when\nprovided a user_id , we need to look up its representation, and we already have some\nassumptions:\n•The user_id  provided will be of the correct type. •\n•The user_id  will have a representation in our space. •\n•The representation returned will be of the correct type and shape.•\n•The component values of the representation vector will be in the appropriate•\ndomain. ( The support of representations in your latent space may vary day to day .)\nFrom here, we need look up the k ANN, which incurs more assumptions:\n•There are ≥k vectors in our latent space. •\n•Those vectors adhere to the expected distributional behavior of the latent space.•\nWhile these seem like relatively straightforward applications of unit tests, canonizing\nthese assumptions is important. Take the last assumption in both of the two services:\nhow can you know the appropriate domain for the representation vectors? As part\nof your training procedure, you’ll need to calculate this and then store it for access\nduring the inference pipeline.\n108 | Chapter 7: Serving Models and Architectures\nIn the second case, when finding nearest neighbors in high-dimensional spaces, well-\ndiscussed difficulties arise in distributional uniformity, but this can mean particularly\npoor performance for recommendations. In practice, we have observed a spiky nature\nto the behavior of k-nearest neighbors in latent spaces, leading to difficult challenges\ndownstream in ensuring diversity of recommendations. These distributions can be\nestimated as priors, and simple checks like KL divergence can be used online; we can\nestimate the average behavior of the embeddings and the difference between local\ngeometries.\nIn both cases, collecting and logging the output of this information can provide a rich\nhistory of what is going on with your system. This can shorten debugging loops later\nif model performance is low in production.\nReturning to the possibility of user_id  lacking a representation in our space: this is\nprecisely the cold-start problem! In that case, we need to transition over to a different\nprediction pipeline: perhaps user-feature-based, explore-exploit, or even hardcoded\nrecommendations. In this setting, we need to understand next steps when a schema\ncondition is not met and then gracefully move forward.\nIntegration Tests\nLet’s consider one higher-level challenge that might emerge in a system like this at the\nlevel of integration. Some refer to these issues as entanglement.\nY ou’ve learned through experimentation that you should find k= 20  ANNs in the\nitem space for a user to get good recommendations. Y ou make a call to your repre‐\nsentation space, get your 20 items, and pass them onto the filtering step. However,\nthis user is quite picky; they have previously made many restrictions on their account\nabout the kind of recommendations they allow: no shoes, no dresses, no jeans, no\nhats, no handbags—what’s a struggling recommendation system to do?\nNaively, if you take the 20 neighbors and pass them into the bloom, you’re likely to be\nleft with nothing! Y ou can approach this challenge in two ways:\n•Allow for a callback from the filter step to the retrieval (see “Predicate Push‐ •\ndown” on page 278 )\n•Build a user distribution and store that for access during retrieval•\nIn the first approach, you give access to your filter step to call the retrieval step\nwith a larger k until the requirements are satisfied after the bloom. Of course, this\nincurs significant slowdown as it requires multiple passes and ever-growing queries\nwith redundancy! While this approach is simple, it requires building defensively and\nknowing ahead of time what may go wrong.\nAlerting and Monitoring | 109\nIn the second approach, during training, you can sample from the user space to build\nestimates of the appropriate k for varying numbers of avoids by user. Then, giving\naccess to a lookup of total avoids by user to the collector can help defend against this\nbehavior.\nOver-Retrieval\nSometimes  people in information retrieval perform over-retrieval\nto mitigate issues of conflicting requirements from the search\nrequest, which can arise if the user makes a search and applies\nmany filters simultaneously. This is applicable in recommendation\nsystems as well.\nIf you retrieve only exactly the number of potential recommenda‐\ntions you need to serve to the user, downstream rules or poor per‐\nsonalization scores can sometimes cause a serious issue for serving\nup recommendations. This is why it is common to retrieve more\nitems than you anticipate showing to the user.\nObservability\nMany  tools in software engineering can assist with observability—understanding the\nwhys  of what’s going on in the software stack. Because the systems we are building\nbecome quite distributed, the interfaces become critical monitoring points, but the\npaths also become complex.\nSpans and traces\nCommon terms in this area are spans  and traces,  which refer to two dimensions of\na call stack, illustrated in Figure 7-3 . Given a collection of connected services, as in\nour preceding examples, an individual inference request will pass through some or\nall of those services in a sequence. The sequence of service requests is the trace . The\npotentially parallel time delays of each of these services is the span .\nFigure 7-3. The spans of a trace\n110 | Chapter 7: Serving Models and Architectures",6506
55-Continuous Training and Deployment.pdf,55-Continuous Training and Deployment,"The graphical representation of spans usually demonstrates how the time for one\nservice to respond comprises several other delays from other calls.\nObservability  enables you to see traces, spans, and logs in conjunction to appropri‐\nately diagnose the behavior of your system. In our example of utilizing a callback\nfrom the filter step to get more neighbors from the collector, we might see a slow\nresponse and wonder, “What has happened?” By viewing the spans and traces, we’ d be\nable to see that the first call to the collector was as expected, then the filter step made\na call to the collector, then another call to the collector, and so on, which built up a\nhuge span for the filter step. Combining that view with logging would help us rapidly\ndiagnose what might be happening.\nTimeouts\nIn the preceding example, we had a long process that could lead to a very bad user\nexperience. In most cases, we impose hard restrictions on how bad we let things get;\nthese are called timeouts .\nUsually, we have an upper bound on how long we’re willing to wait for our inference\nresponse, so implementing timeouts aligns our system with these restrictions. It’s\nimportant in these cases to have a  fallback.  In the setting of recommendation systems,\na fallback usually comprises things like the MPIR prepared such that it incurs mini‐\nmal additional delay.\nEvaluation in Production\nIf the previous section was about understanding what’s coming into your model in\nproduction, this one might be summarized as what’s coming out of your model in\nproduction. At a high level, evaluation in production can be thought of as extending\nall your model-validation techniques to the inference time. In particular, you are\nlooking at what the model actually is doing !\nOn one hand, we already have tools to do this evaluation. Y ou can use the same meth‐\nods to evaluate performance as you do for training, but now on real observations\nstreaming in. However, this process is not as obvious as we might first guess. Let’s\ndiscuss some of the challenges.\nSlow Feedback\nRecommendation systems fundamentally are trying to lead to item selection, and in\nmany cases, purchases. But if we step back and think more holistically about the pur‐\npose of integrating recommendation systems into businesses, it’s to drive revenue. If\nyou’re an ecommerce shop, item selection and revenue may seem easily associated: a\npurchase leads to revenue, so good item recommendation leads to revenue. However,\nwhat about returns? Or even a harder question: is this revenue incremental? One\nEvaluation in Production | 111\nchallenge with recommendation systems is that it can be difficult to draw a causal\narrow between any metric used to measure the performance of your models to the\nbusiness-oriented KPIs.\nWe call this slow feedback  because sometimes the loop from a recommendation to a\nmeaningful metric and back to the recommender can take weeks or even longer. This\nis especially challenging when you want to run experiments to understand whether a\nnew model should be rolled out. The length of the test may need to stretch quite a bit\nmore to get meaningful results.\nUsually, the team aligns on a proxy metric that the data scientists believe is a good\nestimator for the KPI, and that proxy metric is measured live. This approach has\na huge variety of challenges, but it often suffices and provides motivation for more\ntesting. Well-correlated proxies are often a great start to get directional information\nindicating where to take further iterations.\nModel Metrics\nSo, what are the key metrics to track for your model in production? Given that we’re\nlooking at recommendation systems at inference time, we should seek to understand\nthe following:\n•Distribution of recommendation across categorical features•\n•Distribution of affinity scores•\n•Number of candidates•\n•Distribution of other ranking scores•\nAs we discussed before, during the training process, we should be calculating broadly\nthe ranges of our similarity scores in our latent space. Whether we are looking at\nhigh-level estimations or finer ones, we can use these distributions to get warning\nsignals that something might be strange. Simply comparing the output of our model\nduring inference, or over a set of inference requests, to these precompute distribu‐\ntions can be extremely helpful.\nComparing distributions can be a long topic, but one standard approach is comput‐\ning KL-divergence  between the observed distribution and the expected distribution\nfrom training. By computing KL divergence between these, we can understand how\nsurprising  the model’s predictions are on a given day.\nWhat we’ d really like is to understand the receiver operating characteristic curve\n(ROC) of our model predictions with respect to one of our conversion types. How‐\never, this involves yet another integration to tie back to logging. Since our model\nAPI produces only the recommendation, we’ll still need to tie into logging from the\nweb application to understand outcomes! To tie back in outcomes, we must join the\n112 | Chapter 7: Serving Models and Architectures",5159
56-Deployment Topologies.pdf,56-Deployment Topologies,"model predictions with the logging output to get the evaluation labels, which can be\ndone via log-parsing technologies (like Grafana, ELK, or Prometheus). We’ll see more\nof this in Chapter 8 .\nReceiver Operating Characteristic Curve\nIf we assume that the relevance scores are estimating whether the\nitem will be relevant to the user, this forms a binary classification\nproblem. Utilizing these (normalized) scores, we can build an ROC\nto estimate over the distributions of queries when the relevance\nscore begins to accurately predict a relevant item via retrieval\nhistory. This curve can thus be used to estimate parameters like\nnecessary retrieval depth or even problematic queries.\nContinuous Training and Deployment\nIt may feel like we’re done with this story since we have models tracked and produc‐\ntion monitoring in place, but rarely are we satisfied with set-it-and-forget-it model\ndevelopment. One important characteristic of ML products is that models frequently\nneed to be updated to even be useful. Previously, we discussed model metrics and that\nsometimes performance in production might look different from our expectations\nbased on the trained models’ performance. This can be further exacerbated by model\ndrift.\nModel Drift\nModel drift is the notion that the same model may exhibit different prediction\nbehavior over time, merely due to changes in the data-generating process. A simple\nexample is a time-series forecasting model. When you build a time-series forecasting\nmodel, the especially unique property that is essential for good performance is autore‐\ngression : the value of the function covaries with previous values of the function. We\nwon’t go into detail on time-series forecasting, but suffice it to say: your best hope of\nmaking a good forecast is to use up-to-date data! If you want to forecast stock prices,\nyou should always use the most recent prices as part of your predictions.\nThis simple example demonstrates how models may drift, and forecasting models\nare not so different from recommendation models—especially when considering the\nseasonal realities of many recommendation problems. A model that did well two\nweeks ago needs to be retrained with recent data to be expected to continue to\nperform well.\nContinuous Training and Deployment | 113\nOne criticism of a model that drifts is “that’s the smoking gun of an overfit model, ”\nbut in reality these models require a certain amount of over-parameterization to be\nuseful. In the context of recommendation systems, we’ve already seen that quirks\nlike the Matthew effect have disastrous effects on the expected performance of a\nrecommender model. If we don’t consider things like new items in our recommender,\nwe are doomed to fail. Models can drift for a variety of reasons, often coming down\nto exogenous factors in the generating process that may not be captured by the\nmodel.\nOne approach to dealing with and predicting  stale models is to simulate these scenar‐\nios during training. If you suspect that the model goes stale mostly because of the\ndistribution changing over time, you can employ sequential cross-validation—train‐\ning on a contiguous period and testing on a subsequent period—but with a specified\nblock of time delay. For example, if you think your model performance is going to\ndecrease after two weeks because it’s being trained on out-of-date observations, then\nduring training you can purposely build your evaluation to incorporate a two-week\ndelay before measuring performance. This is called two-phase prediction comparison ,\nand by comparing the performances, you can estimate drift magnitudes to keep an\neye out in production.\nA wealth of statistical approaches can be used to rein in these differences. In lieu of a\ndeep dive into variational modeling for variability and reliability for your predictions,\nwe’ll discuss continuous training and deployment and open this peanut with a sledge\nhammer.\nDeployment Topologies\nLet’s  consider a few structures for deploying models that will not only keep\nyour models well in tune but also accommodate iteration, experimentation, and\noptimization.\nEnsembles\nEnsembles  are a type of model structure in which multiple models are built, and the\npredictions from those models are pooled together in one of a variety of ways. While\nthis notion of an ensemble is usually packaged into the model called for inference,\nyou can generalize the idea to your deployment topology.\nLet’s take an example that builds on our previous discussion of prediction priors.\nIf we have a collection of models with comparable performance on a task, we can\ndeploy them in an ensemble, weighted by their deviation from the prior distributions\nof prediction that we’ve set before. This way, instead of having a simple yes/no filter\non the output of your model’s range, you can more smoothly transition potentially\nproblematic predictions into more expected ones.\n114 | Chapter 7: Serving Models and Architectures\nAnother benefit of treating the ensemble as a deployment topology instead of only\na model architecture is that you can hot-swap  components of an ensemble as you\nmake improvements in specific subdomains of your observation feature space. Take,\nfor example, a life-time-value (LTV) model comprising three components: one that\npredicts well for new clients, another for activated clients, and a third for super-users.\nY ou may find that pooling via a voting mechanism performs the best on average,\nso you decide to implement a bagging approach. This works well, but later you\nfind a better model for the new clients. By using the deployment topology for your\nensemble, you can swap in the new model for the new clients and start comparing\nperformance in your ensemble in production. This brings us to the next strategy,\nmodel comparison.\nEnsemble Modeling\nEnsemble modeling  is popular in all kinds of ML, built upon\nthe simple notion that the mixture of expert opinions is strictly\nmore effective than a single estimator. In fact, assume for a\nmoment that you have M classifiers with error rate ϵ; then\nfor an N class classification problem, your error would be\nPy≥k= ∑kn*n\nkϵk*1 −ϵn−k, and the exciting part is that\nthis is smaller than ϵ for all values less than 0.5!\nShadowing\nDeploying two models, even for the same task, can be enormously informative. We\ncall this shadowing  when one model is “live” and the other is secretly also receiving\nall the requests and doing inference, and logging the results, of course. By shadowing\ntraffic to the other model, you get the best expectations possible about how the model\nbehaves before making your model live. This is especially useful when wanting to\nensure that the prediction ranges align with expectation.\nIn software engineering and DevOps, there’s a notion of staging  for software. It’s a\nhotly contested question of “how much of the real infrastructure should staging see, ”\nbut shadowing is the staging of ML models. Y ou can basically build a parallel pipeline\nfor your entire infrastructure to connect for shadow models, or you can just put them\nboth in the line of fire and have the request sent to both but use only one response.\nShadowing is also crucial for implementing experimentation.\nContinuous Training and Deployment | 115\nExperimentation\nAs good data scientists, we know that without a proper experimental framework, it’s\nrisky to advertise much about the performance of a feature or, in this case, model.\nExperimentation can be handled with shadowing by having a controller layer that\nis taking the incoming requests and orchestrating which of the deployed models to\ncurry the response along. A simple A/B experimentation framework might ask for\na randomization at every request, whereas something like a multiarmed bandit will\nrequire the controller layer to have notions of the reward function.\nExperimentation is a deep topic that we don’t have the knowledge or space to do\nadequate justice, but it’s useful to know that this is where experimentation can fit into\nthe larger deployment pipeline.\nModel Cascades\nA really nice extension of the concepts of ensembling and shadowing is model\ncascading , illustrated in Figure 7-4 . The simplified idea of a model cascade is that\nwe use model confidence to create a conditional ensemble. In particular, given an\ninference request, the model provides a prediction with a confidence estimate; when\nthe model confidence is high, that prediction is returned, but if the confidence is\nbelow a certain threshold, a downstream model is called and the ensemble is started.\nThere’s no reason to stop at two models; this method can be used to iteratively expand\nthe number of ensemble layers for any number of models that in training show\nimproved performance in an ensemble.\nFigure 7-4. Ensembles versus cascades\n116 | Chapter 7: Serving Models and Architectures",8948
57-The Evaluation Flywheel.pdf,57-The Evaluation Flywheel,,0
58-Logging.pdf,58-Logging,"Here are a few advantages of this approach:\n•Better expected performance overall, as ensembles usually increase performance•\n•Ensemble performance with lower average computation time•\n•Especially better performance in out-of-sample scenarios•\nThis method scales to larger pools of models and, while it may incur significant\ntraining efforts, finding the right ordering of the models can have significant effects\non model accuracy and latency.\nThe Evaluation Flywheel\nBy now, it’s likely obvious that a production ML model is far from a static object.\nProduction ML systems of any kind are subject to as many deployment concerns as a\ntraditional software stack, in addition to the added challenge of dataset shift and new\nusers/items. In this section, we’ll look closely at the feedback loops introduced and\nunderstand how the components fit together to continuously improve our system—\neven with little input from a data scientist or ML engineer.\nDaily Warm Starts\nAs we’ve now discussed several times, we need a connection between the continuous\noutput of our model and retraining. The first simplest example of this is daily warm\nstarts, which essentially ask us to utilize the new data seen each day in our system.\nAs might already be obvious, some of the recommendation models that show great\nsuccess are quite large. Retraining some of them can be a massive undertaking, and\nsimply rerunning everything  each day is often infeasible. So, what can be done?\nLet’s ground this conversation in the user-user CF example that we’ve been sketching\nout; the first step was to build an embedding via our similarity definition. Let’s recall:\nUSimA,B=∑x ∈ℛA,BrA,x−rArB,x−rB\n∑x ∈ℛA,BrA,x−rA2∑x ∈ℛA,BrB,x−rB2\nHere we remember that the similarity between two users is dependent on the shared\nratings and on each user’s average rating.\nOn a given day, let’s say X=x∣ xwas rated since yesterday by a user . Then we’ d\nneed to update our user similarities, but ideally we’ d leave everything else the same.\nTo update the user’s data, we see that all x rated by two users, A and BrA and rB,\nwould need to change, but we could probably skip these updates in many cases where\nThe Evaluation Flywheel | 117\nthe number of ratings by those users was large. All in all, this means for each x, we\nshould look up which users previously rated x and update the user similarity between\nthem and the new rater.\nThis is a bit ad hoc, but for many methods you can utilize these tricks to reduce a full\nretraining. This would avoid a full batch retraining, via a fast layer. Other approaches\nexist, like building a separate model that can approximate recommendations for\nlow-signal items. This can be done via feature models and can significantly reduce the\ncomplexity of these quick retrainings.\nLambda Architecture and Orchestration\nOn the more extreme end of the spectrum of these strategies is the lambda architec‐\nture; as discussed in Chapter 6 , the lambda architecture seeks to have a much more\nfrequent pipeline for adding new data into the system. The speed  layer is responsible\nfor working on small batches to perform the data transformations, and on model\nfitting to combine with the core model. As a reminder, many other aspects of the\npipeline should also be updated during these fast layers, like the nearest neighbors\ngraph, the feature store, and the filters.\nDifferent components of the pipeline can require different investments to keep upda‐\nted, so their schedules are an important consideration. Y ou might be starting to notice\nthat keeping all of these aspects in sync can be a bit challenging. If you have model\ntraining, model updating, feature store updates, redeployment, and new items/users\nall coming in on potentially different schedules, a lot of coordination may be neces‐\nsary. This is where  an orchestration tool  can become relevant. A variety of approaches\nexist, but a few useful technologies here are GoCD, MetaFlow, and KubeFlow; the\nlatter is more oriented at Kubernetes infrastructures. Another pipeline orchestration\ntool that can handle both batch and streaming pipelines is Apache Beam.\nGenerally, for ML deployment pipelines, we need to have a reliable core pipeline\nand the ability to keep the systems up to date as more data pours in. Orchestration\nsystems usually define the topology of the systems, the relevant infrastructure config‐\nurations, and the mapping of the code artifacts needing to be run—not to mention\nthe CRON schedules of when all these jobs need to run. Code as infrastructure\nis a popular paradigm that captures these goals as a mantra, so that even all this\nconfiguration itself is reproducible and automatable.\nIn all these orchestration considerations, there’s a heavy overlap with containerization\nand how these steps may be deployed. Unfortunately, most of this discussion is\nbeyond the scope of this book, but a simple overview is that containerized deploy‐\nment with something like Docker is extremely helpful for ML services, and managing\nthose deployments with various container management systems, like Kubernetes, is\nalso popular.\n118 | Chapter 7: Serving Models and Architectures\nLogging\nLogging  has come up several times already. Previously in this chapter, you saw that\nlogging was important for ensuring that our system was behaving as expected. Let’s\ndiscuss some best practices for logging and how they fit into our plans.\nWhen we discussed traces and spans earlier, we were able to get a snapshot of the\nentire call stack of the services involved in responding to a request. Linking the\nservices together to see the larger picture is incredibly useful, and when it comes to\nlogging, gives us a hint as to how we should be orienting our thinking. Returning to\nour favorite RecSys architecture, we have the following:\n•Collector receiving the request and looking up the embedding relevant to the•\nuser\n•Computing ANN on items for that vector•\n•Applying filters via blooms to eliminate potential bad recommendations•\n•Augmenting features of the candidate items and user via the feature stores•\n•Scoring of candidates via the ranking model and estimating potential confidence•\n•Ordering and application of business logic or experimentation•\nEach of these elements has potential applications of logging, but let’s now think about\nhow to link them together. The relevant concept from microservices is correlation\nIDs; a correlation ID  is simply an identifier that’s passed along the call stack to ensure\nthe ability to link everything later. As is likely obvious at this point, each of these\nservices will be responsible for its own logging, but the services are almost always\nmore useful in aggregate.\nThese days, Kafka is often used as the log-stream processor to listen for logs from\nall the services in your pipeline and to manage their processing and storing. Kafka\nrelies on a message-based architecture; each service is a producer, and Kafka helps\nmanage those messages to consumer channels. In terms of log management, the\nKafka cluster receives all the logs in the relevant formats, hopefully augmented with\ncorrelation IDs, and sends them off to an ELK stack. The ELK stack —Elasticsearch,\nLogstash, Kibana—consists of a Logstash component to handle incoming log streams\nand apply structured processing, Elasticsearch to build search indices to the log store,\nand Kibana to add a UI and high-level dashboarding to the logging.\nThis stack of technologies is focused on ensuring that you have access and observabil‐\nity from your logs. Other technologies focus on other aspects, but what should you be\nlogging?\nThe Evaluation Flywheel | 119\nCollector logs\nAgain, we wish to log during the following:\n•Collector receiving the request and looking up the embedding relevant to the•\nuser\n•Computing ANN on items for that vector•\nThe collector receives a request, consisting in our simplest example of user_id ,\nrequesting_timestamp , and any augmenting keyword elements (kwargs) that might\nbe required. A correlation_id  should be passed along from the requester or gen‐\nerated at this step. A log with these basic keys should be fired, along with the time‐\nstamp of request received. A call is made to the embedding store, and the collector\nshould log this request. Then the embedding store should log this request when\nreceived, along with the embedding store’s response. Finally, the collector should\nlog the response as it returns. This may feel like a lot of redundant information,\nbut the explicit parameters included in the API calls become extremely useful when\ntroubleshooting.\nThe collector now has the vector it will need to perform a vector search, so it will\nmake a call to the ANN service. Logging this call, and any relevant logic in choosing\nthe k for number of neighbors will be important, along with the ANN’s received API\nrequest, the relevant state for computing ANN, and ANN’s response. Back in the\ncollector, logging that response and any potential data augmentation for downstream\nservice requirements are the next steps.\nAt this point, at least six logs have been emitted—only reinforcing the need for a\nway to link these all together. In practice, you often have other relevant steps in\nyour service that should be logged (e.g., checking that the distribution of distances in\nreturned neighbors is appropriate for downstream ranking).\nNote that if the embedding lookup was a miss, logging that miss is obviously impor‐\ntant, as well as logging the subsequent request to the cold-start recommendation\npipeline. The cold-start pipeline will incur additional logs.\nFiltering and scoring\nNow we need to monitor the following steps:\n1.Applying filters via blooms to eliminate potential bad recommendations1.\n2.Augmenting features to the candidate items and user via the feature stores2.\n3.Scoring candidates via the ranking model, and potential confidence estimation3.\n120 | Chapter 7: Serving Models and Architectures\nWe should log the incoming request to the filtering service as well as the collection of\nfilters we wish to apply. Additionally, as we search the blooms for each item and rule\nthem in or out of the bloom, we should build up some structured logging of which\nitems are caught in which filters and then log all this as a blob for later inspection.\nResponses and requests should be logged as part of feature augmentation—where we\nshould log requests and responses to the feature store.\nAlso log the augmented features that end up attached to the item entities. This may\nseem redundant with the feature store itself, but understanding which features were\nadded during a recommendation pipeline is crucial  when looking back later to figure\nout why the pipeline might have behaved differently than anticipated.\nAt the time of scoring, the entire set of candidates should be logged with the features\nnecessary for scoring and the output scores. It’s extremely powerful to log this entire\ndataset, because training later can use these to get a better sense for real ranking sets.\nFinally, the response is passed to the next step with the ranked candidates and all\ntheir features.\nOrdering\nWe have one more step to go, but it’s an essential one: ordering and application of\nbusiness logic or experimentation . This step is probably the most important logging\nstep, because of how complicated and ad hoc the logic in this step can get.\nIf you have multiple intersecting business requirements implemented via filters at\nthis step, while also integrating with experimentation, you can find yourself seriously\nstruggling to unpack how reasonable expectations coming out of the ranker have\nturned into a mess by response time. Techniques like logging the incoming candi‐\ndates, keyed to why they’re eliminated, and the order of business rules applied will\nmake reconstructing the behavior much more tractable.\nAdditionally, experimentation routing will likely be handled by another service, but\nthe experiment ID seen in this step and the way that experiment assignment was\nutilized are the responsibility of the server. As we ship off the final recommendations,\nor decide to go another round, one last log of the state of the recommendation will\nensure that app logs can be validated with responses.\nNotes on Formatting\nStructured logs are your friend. Implementing a data structure to hold the relevant\ndata for your logs and then utilizing a log-formatter object  will significantly reduce\nthe difficulty in parsing and writing these logs. One often underappreciated feature\nof building message objects in code, and utilizing them as a running data structure\nthroughout your call stack, is tight coupling between logs and app logic.\nThe Evaluation Flywheel | 121",12826
59-Active Learning.pdf,59-Active Learning,"Tight coupling is often bemoaned in service-architecture discussions, but when that\ncoupling is between your logs and your actual objects of execution, this saves you a\nlot of headaches. When changing the objects used for your service, instead of having\nan additional step to ensure the logs reflect that, you can propagate those changes\nthrough automatically by using the same objects in tandem with a log formatter.\nThese processes can also make good use of testing, to ensure that the objects your\ncode cares about are visible in the logs, and these log-formatter objects can have\nenforced matching via unit tests. Finally, because we want to connect to downstream\nlog parsing and log searching, it will be invaluable to have a clear relationship\nbetween the log stack and the application stack via object parameters and keys in\nthe log data structure.\nActive Learning\nSo far, we have discussed using updating data to train on a much more frequent\nschedule, and we’ve discussed how to provide good recommendations, even when the\nmodel hasn’t seen enough data for those entities. An additional opportunity for the\nfeedback loop of recommendation and rating is active learning.\nWe won’t be able to go deep into the topic, which is a large and active field of\nresearch, but we will discuss the core ideas in relation to recommendation systems.\nActive learning  changes the learning paradigm a bit by suggesting that the learner\nshould not only be passively collecting labeled (maybe implicit) observations but also\nattempting to mine relations and preferences from them. Active learning determines\nwhich data and observations would be most useful in improving model performance\nand then seeks out those labels. In the context of RecSys, we know that the Matthew\neffect  is one of our biggest challenges, in that many potentially good matches for a\nuser may be lacking enough or appropriate ratings to bubble to the top during the\nrecommendations.\nWhat if we employed a simple policy: every new item to the store gets recommended\nas a second option to the first 100 customers. Two outcomes would result:\n•We would quickly establish data for our new item to help cold-start it.•\n•We would likely decrease the performance of our recommender.•\nIn many cases, the second outcome is worth enduring to achieve the first, but when?\nAnd is this the right way to approach this problem? Active learning provides a\nmethodical approach to these problems.\nAnother more specific advantage of active learning schemes is that you can broaden\nthe distribution of observed data. In addition, to just cold-start items, we can use\nactive learning to target broadening users’ interests. This is usually framed as an\n122 | Chapter 7: Serving Models and Architectures\nuncertainty-reduction technique, as it can be used to improve the confidence in\nrecommendations in a broader range of item categories. Here’s a simple example:\na user shops for only sci-fi books, so one day you show them a few extremely\nwell-liked Westerns to see whether that user might be open to occasionally getting\nrecommendations for Westerns. See “Propensity Weighting for Recommendation\nSystem Evaluation” on page 208  for more details.\nAn active learning system is instrumented as a loss function inherited from the\nmodel it’s trying to enhance—usually tied to uncertainty in some capacity—and it’s\nattempting to minimize that loss. Given a model ℳ trained on a set of observations\nand labels xi,yi, with loss ℒ, an active learner seeks to find a new observation,\nx, such that if a label was obtained, y, the loss would decrease via the model’s\ntraining including this new pair. In particular, the goal is to approximate the marginal\nreduction in loss due to each possible new observation and find the observation that\nmaximizes that reduction in the loss function:\nArgmaxxℒℳxi,yi−ℒℳxi,yi∪x\nThe structure of an active learning system roughly follows these steps:\n1.Estimate marginal decrease in loss due to obtaining one of a set of observations.1.\n2.Select the observation with the largest effect.2.\n3.Query  the user; i.e., provide the recommendation to obtain a label. 3.\n4.Update the model.4.\nIt’s probably clear that this paradigm requires a much faster training loop than our\nprevious fast retraining schemes. Active learning can be instrumented in the same\ninfrastructure as our other setups, or it can have its own mechanisms for integration\ninto the pipeline.\nTypes of optimization\nThe optimization procedure carried out by an active learner in a recommendation\nsystem has two approaches: personalized and nonpersonalized. Because RecSys is all\nabout personalization, it’s no surprise that we would, in time, want to push the utility\nof our active learning further by integrating the great details we already know about\nusers.\nWe can think of these two approaches as global loss minimization and local loss\nminimization. Active learning that isn’t personalized tends to be about minimizing\nthe loss over the entire system, not for only one user. (This split doesn’t perfectly\ncapture the ontology, but it’s a useful mnemonic). In practice, optimization methods\nare nuanced and sometimes utilize complicated algorithms and training procedures.\nThe Evaluation Flywheel | 123\nLet’s talk through some factors to optimize for nonpersonalized active learning:\nUser rating variance\nConsider  which items have the largest variance in user ratings to try to get more\ndata on those we find the most complicated in our observations.\nEntropy\nConsider  the dispersion of ratings of a particular item across an ordinal feature.\nThis is useful for understanding whether our set of ratings for an item is dis‐\ntributed uniformly at random.\nGreedy extend\nMeasure  which items seem to yield the worst performance in our current model;\nthis attempts to improve our performance overall by collecting more data on the\nhardest items to recommend well.\nRepresentatives or exemplars\nPick out items that are extremely representative of large groups of items; we\ncan think of this as “If we have good labels for this, we have good labels for\neverything like this. ”\nPopularity\nSelect  items that the user is most likely to have experience with to maximize the\nlikelihood that they’ll give an opinion or rating.\nCo-coverage\nAttempt  to amplify the ratings for frequently occurring pairs in the dataset; this\nstrikes directly at the CF structure to maximize the utility of observations.\nOn the personalized side:\nBinary prediction\nTo maximize the chances that the user can provide the requested rating, choose\nthe items that the user is more likely to have experienced. This can be achieved\nvia an MF on the binary ratings matrix.\nInfluence  based\nEstimate the influence of item ratings on the rating prediction of other items, and\nselect the items with the largest influence. This attempts to directly measure the\nimpact of a new item rating on the system.\nRating optimized\nObviously, there’s an opportunity to simply use the best rating or best rating\nwithin a class to perform active learning queries, but this is precisely the standard\nstrategy in recommendation systems to serve good recommendations.\n124 | Chapter 7: Serving Models and Architectures",7297
60-Summary.pdf,60-Summary,"User segmented\nWhen available, use user segmentation  and feature clusters within users to antic‐\nipate when users have opinions and preferences on an item by virtue of the\nuser-similarity structure.\nIn general, a soft trade-off exists between active learning that’s useful for maximally\nimproving your model globally and active learning that’s useful for maximizing the\nlikelihood that a user can and will rate a particular item. Let’s look at one particular\nexample that uses both.\nApplication: User sign-up\nOne  common hurdle to overcome in building recommendation systems is on-\nboarding new users. By definition, new users will be cold-starting with no ratings\nof any kind and will likely not expect great recommendations from the start.\nWe may begin with the MPIR for all new users—simply show them something  to get\nthem started and then learn as you go. But is there something better?\nOne approach you’ve probably experienced is the user onboarding flow: a simple set\nof questions employed by many websites to quickly ascertain basic information about\nthe user, to help guide early recommendation. If discussing our book recommender,\nthis might be asking what genres the user likes, or in the case of a coffee recommen‐\nder, how the user brews coffee in the morning. It’s probably clear that these questions\nare building up knowledge-based recommender systems and don’t directly feed into\nour previous pipelines but can still provide some help in early recommendations.\nIf instead we looked at all our previous data and asked, “Which books in particular\nare most useful for determining a user’s taste?, ” this would be an active learning\napproach. We could even have a decision tree of possibilities as the user answered\neach question, wherein the answer determines which next question is most useful\nto ask.\nSummary\nNow we have the confidence that we can serve up our recommendations, and even\nbetter, we have instrumented our system to gather feedback. We’ve shown how you\ncan gain confidence before you deploy and how you can experiment with new models\nor solutions. Ensembles and cascades allow you to combine testing with iteration, and\nthe data flywheel provides a powerful mechanism for improving your product.\nY ou may be wondering how to put all this new knowledge into practice, to which the\nnext chapter will speak. Let’s understand how data processing and simple counting\ncan lead to an effective—and useful!—recommendation system.\nSummary | 125",2500
61-Big Data Frameworks.pdf,61-Big Data Frameworks,"CHAPTER 8\nPutting It All Together: Data Processing\nand Counting Recommender\nNow that we have discussed the broad outline of recommender systems, this chapter\nwill put it into a concrete implementation so that we can talk about the choices of\ntechnologies and specifics of how the implementation works in real life.\nThis chapter covers the following topics:\n•Data representation with protocol buffers•\n•Data processing frameworks•\n•A PySpark sample program•\n•GloVE embedding model•\n•Additional foundational techniques in JAX, Flax, and Optax•\nWe will show step-by-step how to go from a downloaded Wikipedia dataset to a\nrecommender system that can recommend words from Wikipedia based on the\nco-occurrence with words in a Wikipedia article. We use a natural language example\nbecause words are easily understood, and their relationships are readily grasped\nbecause we can see that related words occur near one another in a sentence. Fur‐\nthermore, the Wikipedia corpus is easily downloadable and browsable by anyone\nwith an internet connection. This idea of co-occurrence can be generalized to any\nco-occurring collection of items, such as watching a video in the same session or\npurchasing cheeses in the same shopping bag.\nThis chapter will demonstrate concrete implementations of an item-item and a\nfeature-item recommender. Items in this case are the words in an article, and the\nfeatures are word-count similarity—a MinHash or a kind of locality sensitive hash\nfor words. Chapter 16  covers locality sensitive hash in more detail, but for now, we’ll\n127\nconsider these simple hashing functions to be encoding functions over content, such\nthat content with similar properties maps to similar co-domains. This general idea\ncan be used as a warm-start mechanism on a new corpus in the absence of logging\ndata, and if we have user-item features such as likes, these can be used as features for\na feature-item recommender. The principles of co-occurrence are the same, but by\nusing Wikipedia as an example, you can download the data and play with it by using\nthe tools provided.\nWarm and Cold Starts\nA cold start  occurs  when we do not have any information about a corpus or people’s\npreferences and resort to a best-guess approach such as recommending popular\nitems. On the other hand, if items naturally occur in typical groupings, like the\nselection and arrangement in the cheese aisle of a grocery store, then we call this a\nwarm start: using information like co-occurrence of cheeses with each other or with\nother items like salami as a means of starting out the recommender engine more\nintelligently.\nIn the Wikipedia example, even before we have users click articles, we’ll be able to\nwarm-start the word-to-word recommender simply based on how close words are\nto each other in a sentence. Similarly, if you had a bunch of items that naturally\nfall into some kind of hierarchical taxonomy, you might be able to warm-start your\nrecommender by having items that are in the same branch of the taxonomy count as\nco-occurring with one another.\nTech Stack\nA set of technologies used together is commonly called a technology stack , or tech\nstack . Each component of a tech stack can usually be replaced by other similar\ntechnologies. We will list a few alternatives for each component but not go into detail\nabout their pros and cons, as there can be many, and the situation of the deployment\nwill affect the choice of components. For example, your company might already use a\nparticular component, so for familiarity and support, you might wish to use that one.\nThis chapter covers some of the technology choices for processing the data that goes\ninto building a concrete implementation of a collector.\nThe sample code is available on GitHub . Y ou might want to clone the code into a local\ndirectory.\n128 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nData Representation\nThe first choice of technology we need to make will determine how we represent the\ndata. Some of the choices are as follows:\n•Protocol buffers•\n•Apache Thrift•\n•JSON•\n•XML•\n•CSV•\nIn this implementation, we’re mostly using protocol buffers because of the ease of\nspecifying a schema and then subsequently serializing and deserializing it.\nProtocol Buffers\nBefore protocol buffers were invented, people used to store their binary data in all\nsorts of custom formats that involved various syntax and specifications (like starting\na file with a magic number, followed by rules on how to parse and store various\ndata types like integers, strings, bytes, and floating-point numbers). Protocol buffers\nunified the storage of custom binary data by allowing users to specify a  schema , or a\nnamed representation of each field and the type of each field (like first_name  being\na string and age being an integer). This enables us to easily read and write structured\ndata in a binary format, and the parsing of the data is handled automatically by the\nprotocol buffer library.\nFor the file format, we’re using serialized protocol buffers that are uuencoded and\nwritten as a single line per record and then bzipped up for compression. This is just\nfor convenience so that we can parse the files easily without having dependencies on\ntoo many libraries. Y our company might instead store data in a data warehouse that is\naccessible by SQL, for example.\nProtocol buffers are generally easier to parse and handle than raw data. In our imple‐\nmentation, we will parse the Wikipedia XML into protocol buffers for easier handling\nusing xml2proto.py . Y ou can see from the code that XML parsing is a complicated\naffair, whereas protocol buffer parsing is as simple as calling the ParseFromString\nmethod, and all the data is then subsequently available as a convenient Python object.\nAs of June 2022, the Wikipedia dump is about 20 GB in size, and converting to\nprotocol buffer format takes about 10 minutes. Please follow the steps described in\nthe README in the GitHub repo for the most up-to-date steps to run the programs.\nData Representation | 129\nIn the proto  directory, take a look at some of the protocol messages defined. This, for\nexample, is how we might store the text from a Wikipedia page:\n// Generic text document.\nmessage TextDocument {\n  // Primary entity, in wikipedia it is the title.\n  string primary = 1;\n  // Secondary entity, in wikipedia it is other titles.\n  repeated string secondary = 2;\n  // Raw body tokens.\n  repeated string tokens = 3;\n  // URL. Only visible documents have urls, some e.g. redirect shouldn't.\n  string url = 4;\n}\nThe types supported and the schema definitions can be found on the protocol buffer\ndocumentation page. This schema is converted into code by using the protocol buffer\ncompiler. This compiler’s job is to convert the schema into code that you can call\nin different languages, which in our case is Python. The installation of the protocol\nbuffer compiler depends on the platform, and installation instructions can be found\nin the protocol buffer documentation .\nEach time you change the schema, you will have to use the protocol buffer compiler\nto get a new version of the protocol buffer code. This step can easily be automated by\nusing a build system like Bazel, but this is out of scope for this book. For the purposes\nof this book, we will simply generate the protocol buffer code once and check it into\nthe repository for simplicity.\nFollowing the directions on the GitHub README, download a copy of the Wikipedia\ndataset and then run xml2proto.py  to convert the data to a protocol buffer format.\nOptionally, use codex.py  to see what the protocol buffer format looks like. These steps\ntook 10 minutes on a Windows workstation using Windows Subsystem for Linux.\nThe XML parser used doesn’t parallelize very well, so this step is fundamentally\nserial. We’ll next discuss how we would distribute the work in parallel either among\nmultiple cores locally or on a cluster.\nBig Data Frameworks\nThe next technology we choose will process data at scale on multiple machines. Some\noptions are listed here:\n•Apache Spark•\n•Apache Beam•\n•Apache Flink•\n130 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nIn this implementation, we’re using Apache Spark in Python, or PySpark. The\nREADME in the repository shows how to install a copy of PySpark locally using\npip install .\nThe first step implemented in PySpark is tokenization and URL normalization. The\ncode is in tokenize_wiki_pyspark.py , but we won’t go over it here because a lot of\nthe processing is simply distributed natural language parsing and writing out the\ndata into protocol buffer format. We will instead talk in detail about the second\nstep, which is to make a dictionary of tokens (the words  in the article) and some\nstatistics about the word counts. However, we will run the code just to see what\nthe Spark usage experience looks like. Spark programs are run using the program\nspark-submit  as follows:\nbin/spark-submit\n--master =local[4]\n--conf=""spark.files.ignoreCorruptFiles=true""\ntokenize_wiki_pyspark.py\n--input_file =data/enwiki-latest-parsed  --output_file =data/enwiki-latest-tokenized\nRunning the Spark submit script allows you to execute the controller program, in this\ncase, tokenize_wiki_pyspark.py , on a local machine as we have in the command line—\nnote that the line local[4]  means use up to four cores. The same command can be\nused to submit the job to a YARN cluster for running on hundreds of machines, but\nfor the purposes of trying out PySpark, a decent enough workstation should be able\nto process all the data in minutes.\nThis tokenization program converts from a source-specific format (in this case, a\nWikipedia protocol buffer) into a more generic text document used for NLP . In\ngeneral, it’s a good idea to use a generic format that all your sources of data can\nbe converted into because that simplifies the data processing downstream. The data\nconversion can be done from each corpus into a standard format that is handled\nuniformly by all the later programs in the pipeline.\nAfter submitting the job, you can navigate to the Spark UI (shown in Figure 8-1 )\non your local machine at localhost:4040/stages/ . Y ou should see the job executing in\nparallel, using up all the cores in your machine. Y ou might want to play with the\nlocal[4]  parameter; using local[*]  will use up all the free cores on your machine. If\nyou have access to a cluster, you can also point to the appropriate cluster URL.\nFigure 8-1. Spark UI\nBig Data Frameworks | 131",10679
62-Cluster Frameworks.pdf,62-Cluster Frameworks,,0
63-PySpark Example.pdf,63-PySpark Example,"Cluster Frameworks\nThe nice thing about writing a Spark program is that it can scale from a single\nmachine with multiple cores to a cluster of many machines with thousands of cores.\nThe full list of cluster types can be found in the Spark “Submitting Applications”\ndocumentation .\nSpark can run on the following cluster types:\n•Spark Standalone cluster•\n•Mesos cluster•\n•YARN cluster•\n•Kubernetes cluster•\nDepending on the kind of cluster your company or institution has set up, most of\nthe time submitting the job is just a matter of pointing to the correct URL. Many\ncompanies such as Databricks and Google also have fully managed Spark solutions\nthat allow you to set up a Spark cluster with little effort.\nPySpark Example\nCounting words turns out to be a powerful tool in information retrieval, as we can\nuse handy tricks like term frequency, inverse document frequency (TF-IDF), which is\nsimply the count of words in the documents divided by the number of documents the\nword has occurred in. This is represented as follows:\ntfidf wordi=log10number of times wordihas occurred in corpus\nnumber of documents in corpus containing wordi\nFor example, because the word the appears frequently, we might think it is an impor‐\ntant word. But by dividing by the document frequency, the becomes less special and\ndrops in importance. This trick is quite handy in simple NLP to get a better-than-\nrandom weighting of word importance.\nTherefore, our next step is to run make_dictionary.py . As the name indicates, this\nprogram simply counts the words and documents and makes a dictionary with the\nnumber of times a word has occurred.\nWe have some concepts to cover in order for you to properly grok how Spark helps\nprocess data in a distributed manner. The entry point of most Spark programs is\nSparkContext . This Python object is created on the controller. The  controller  is the\ncentral program that launches workers that actually process the data. The workers\ncan be run locally on a single machine as a process or on many machines on the\ncloud as separate workers.\n132 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nSparkContext  can be used to create  resilient distributed datasets, or RDDs. These are\nreferences to data streams that can be manipulated on the controller, and processing\non the RDD can be farmed out to all the workers. SparkContext  allows you to load\nup data files stored on a distributed filesystem like Hadoop Distributed File System\n(HDFS) or cloud buckets. By calling the SparkContext ’s textFile  method, we are\nreturned a handle to an RDD. A stateless function can then be applied or mapped\non the RDD to transform it from one RDD to another by repeatedly applying the\nfunction to the contents of the RDD.\nFor example, this program fragment loads a text file and converts all lines to\nlowercase by running an anonymous lambda function that converts single lines to\nlowercase:\ndef lower_rdd (input_file : str,\n              output_file : str):\n  """"""Takes a text file and converts it to lowercase..""""""\n  sc = SparkContext ()\n  input_rdd  = sc.textFile (input_file )\n  input_rdd .map(lambda line: line.lower()).saveAsTextFile (output_file )\nIn a single-machine implementation, we would simply load up each Wikipedia arti‐\ncle, keep a running dictionary in RAM, and count each token and then add 1 to\nthe token count in the dictionary. A token  is an atomic element of a document\nthat is divided into pieces. In regular English, it would be a word, but Wikipedia\ndocuments have other entities such as the document references themselves that need\nto be kept track of separately, so we call the division into pieces tokenization  and the\natomic elements tokens . The single-machine implementation would take a while to go\nthrough the thousands of documents on Wikipedia, which is why we use a distributed\nprocessing framework like Spark. In the Spark paradigm, computation is broken into\nmaps, where a function is applied statelessly on each document in parallel. Spark also\nhas a reduce function, where the outputs of separate maps are joined together.\nFor example, suppose we have a list of word counts and want to sum up the values of\nwords that occur in different documents. The input to the reducer will be something\nlike this:\n•(apple, 10)•\n•(orange, 20)•\n•(apple, 7)•\nBig Data Frameworks | 133\nThen we call the Spark function reduceByKey(lambda a, b: a+ b) , which adds all\nthe values with the same key together and returns the following:\n•(orange, 20)•\n•(apple, 17)•\nIf you look at the code in make_dictionary.py , the map phase  is where we take a\ndocument as input and then break it into tuples of (token, 1). In the reduce phase ,\nthe map outputs are joined by the key, which in this case is the token itself, and the\nreduce function is simply to sum up all the counts of tokens.\nNote that the reduce function assumes that the reduction is associative—that is,\na+b+c=a+b+c=a+b+c. This allows the Spark framework to sum up\nsome parts of the token dictionary in memory on the map phase (in some frame‐\nworks, this is called the combine step , where you run part of the reduction on the\noutput of the map phase on the mapper machine) and then sum them up over several\npasses on the reduce phase.\nAs an optimization, we use the Spark function mapPartitions . Map runs the pro‐\nvided function once per line (for which we have encoded an entire Wikipedia docu‐\nment as a protocol buffer, uuencoded as a single text line), whereas mapPartitions\nruns it over an entire partition, which is many documents, usually 64 MB of them.\nThis optimization lets us construct a small Python dictionary over the entire partition\nso that we have many fewer token-count pairs to reduce. This saves on network band‐\nwidth so the mapper has less data to send to the reducer, and is a good tip in general\nfor these data processing pipelines to reduce network bandwidth (which is generally\nthe most time-consuming part of data processing compared to computation).\nGroup Theory\nBecause  we are math nerds, and also because group theory shows up a lot in reduc‐\ntion operations, we will briefly introduce an algebraic structure known as groups  so\nthat you clearly understand all the terms used in the reduction phase.\nThe concept of sets was mentioned in the introductory chapters; a set is a collection\nof items. The other concept you need to know is an operator. A binary operator  takes\ntwo items and returns another item that is in the set.\nExamples of sets that are commonly used are integers, real numbers, and matrices.\nExamples of binary operators are addition, multiplication, and composition.\nAn operator and a set denoted by the tuple (binary operator, a set of integers) form a\ngroup only if the group axioms are satisfied, namely:\n134 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nAn identity element exists.\nFor every element x in the group, there exists an element e such that\nx+e=e+x=e. For the addition operation, the identity is 0, and for multipli‐\ncation, the identity is called 1. This concept is important in the reduction step\nbecause in some frameworks the reduction step is initialized with the identity\nelement. For example, sums are usually initialized with 0, and products are\nusually initialized with 1.\nThe operator is associative.\nFor elements x,y,z in the set, x+y+z=x+y+z.\nAn inverse exists.\nFor every element x in the group, there exists a y in the group, such that\nx+y=e.\nAn operator can also be commutative. This isn’t a requirement to be a group, but\ngroups that have this property are called commutative groups . With commutativity, for\nelements x,y in the group, x+y=y+x. This property is helpful in the reduction\nstep because it allows the reducer to perform the operations in parallel and then\nreduce them together without worrying which operations occur in what order.\nIt is important to note that while addition over real numbers is associative and com‐\nmutative, addition of floating-point numbers isn’t. The reason is that floating-point\napproximately represents real numbers. So when you add a large number with a\nsmall number in floating-point, the small number isn’t represented accurately and\nmight simply be discarded. A more accurate and consistent way to add floating-point\nnumbers is to sort the list of numbers to be added first and add all the small numbers\nup before adding them to the large numbers. Adding two small numbers together first\nto make a larger number ensures that they do not get lost when being absorbed into\nthe accumulator (the sum). Thus, while addition of numbers is in theory associative\nand commutative with real numbers, you might get different results in practice with\nfloating-point numbers, depending on the order of operations.\nNext we show a complete Spark program that reads in documents in the protocol\nbuffer format of TextDocument  shown in the preceding code block and then counts\nhow often the words, or tokens, occur in the entire corpus. The file in the GitHub\nrepo is make_dictionary.py . The following code is presented slightly differently from\nthe repo file in that it is broken into three chunks for readability and the order of\nthe main and subroutines have been swapped for clarity. Here, we present first the\ndependencies and flags, then the main body, and then the functions being called by\nthe main body so that the purposes of the functions are clearer.\nBig Data Frameworks | 135\nFirst, let’s look at the dependencies. The main ones are the protocol buffer repre‐\nsenting the text document of the Wikipedia article, as discussed earlier. This is the\ninput we are expecting. For the output, we have the TokenDictionary  protocol\nbuffer, which mainly counts the occurrences of words in the article. We will use the\nco-occurrences of words to form a similarity graph of articles that we can then use\nas the basis of a warm-start recommender system. We also have dependencies on\nPySpark, the data processing framework we are using to process the data, as well\nas a flag library that handles the options of our program. The absl flags library is\npretty handy for parsing and explaining the purposes of command-line flags and also\nretrieving the set values of flags easily. Here are the dependencies and flags:\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n#\n""""""\n  This reads a doc.pb.b64.bz2 file and generates a dictionary.\n""""""\nimport base64\nimport bz2\nimport nlp_pb2 as nlp_pb\nimport re\nfrom absl import app\nfrom absl import flags\nfrom pyspark import SparkContext\nfrom token_dictionary  import TokenDictionary\nFLAGS = flags.FLAGS\nflags.DEFINE_string (""input_file"" , None, ""Input doc.pb.b64.bz2 file."" )\nflags.DEFINE_string (""title_output"" , None,\n                    ""The title dictionary output file."" )\nflags.DEFINE_string (""token_output"" , None,\n                    ""The token dictionary output file."" )\nflags.DEFINE_integer (""min_token_frequency"" , 20,\n                     ""Minimum token frequency"" )\nflags.DEFINE_integer (""max_token_dictionary_size"" , 500000,\n                     ""Maximum size of the token dictionary."" )\nflags.DEFINE_integer (""max_title_dictionary_size"" , 500000,\n                     ""Maximum size of the title dictionary."" )\nflags.DEFINE_integer (""min_title_frequency"" , 5,\n                     ""Titles must occur this often."" )\n# Required flag.\nflags.mark_flag_as_required (""input_file"" )\nflags.mark_flag_as_required (""token_output"" )\nflags.mark_flag_as_required (""title_output"" )\n136 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nNext, we have the main body of the program, which is where all the subroutines\nare called. We first create SparkContext , which is the entry point into the Spark\ndata processing system, and then call its textFile  method to read in the bzipped\nWikipedia articles. Please read the README on the repo to understand how it was\ngenerated. Next, we parse the text document and send the RDD to two processing\npipelines, one to make a dictionary for the body of the article and another to make a\ndictionary of the titles. We could choose to make a single unified dictionary for both,\nbut having them separate allows us to create a content-based recommender using the\ntoken dictionary and an article-to-article recommender using the title dictionary, as\ntitles are identifiers for the Wikipedia article. Here’s the main body:\ndef main(argv):\n  """"""Main function.""""""\n  del argv  # Unused.\n  sc = SparkContext ()\n  input_rdd  = sc.textFile (FLAGS.input_file )\n  text_doc  = parse_document (input_rdd )\n  make_token_dictionary (\n    text_doc ,\n    FLAGS.token_output ,\n    FLAGS.min_token_frequency ,\n    FLAGS.max_token_dictionary_size\n  )\n  make_title_dictionary (\n    text_doc ,\n    FLAGS.title_output ,\n    FLAGS.min_title_frequency ,\n    FLAGS.max_title_dictionary_size\n  )\nif __name__ == ""__main__"" :\n    app.run(main)\nFinally, we have the subroutines called by the main function, all decomposed into\nsmaller subroutines for counting the tokens in the article body and the titles:\ndef update_dict_term (term, dictionary ):\n    """"""Updates a dictionary with a term.""""""\n    if term in dictionary :\n        x = dictionary [term]\n    else:\n        x = nlp_pb.TokenStat ()\n        x.token = term\n        dictionary [term] = x\n    x.frequency  += 1\ndef update_dict_doc (term, dictionary ):\n    """"""Updates a dictionary with the doc frequency.""""""\n    dictionary [term].doc_frequency  += 1\nBig Data Frameworks | 137\ndef count_titles (doc, title_dict ):\n    """"""Counts the titles.""""""\n    # Handle the titles.\n    all_titles  = [doc.primary]\n    all_titles .extend(doc.secondary )\n    for title in all_titles :\n        update_dict_term (title, title_dict )\n    title_set  = set(all_titles )\n    for title in title_set :\n        update_dict_doc (title, title_dict )\ndef count_tokens (doc, token_dict ):\n    """"""Counts the tokens.""""""\n    # Handle the tokens.\n    for term in doc.tokens:\n        update_dict_term (term, token_dict )\n    term_set  = set(doc.tokens)\n    for term in term_set :\n        update_dict_doc (term, token_dict )\ndef parse_document (rdd):\n    """"""Parses documents.""""""\n    def parser(x):\n        result = nlp_pb.TextDocument ()\n        try:\n            result.ParseFromString (x)\n        except google.protobuf .message.DecodeError :\n            result = None\n        return result\n    output = rdd.map(base64.b64decode )\\n        .map(parser)\\n        .filter(lambda x: x is not None)\n    return output\ndef process_partition_for_tokens (doc_iterator ):\n    """"""Processes a document partition for tokens.""""""\n    token_dict  = {}\n    for doc in doc_iterator :\n        count_tokens (doc, token_dict )\n    for token_stat  in token_dict .values():\n        yield (token_stat .token, token_stat )\ndef tokenstat_reducer (x, y):\n    """"""Combines two token stats together.""""""\n    x.frequency  += y.frequency\n    x.doc_frequency  += y.doc_frequency\n138 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\n    return x\ndef make_token_dictionary (\n    text_doc ,\n    token_output ,\n    min_term_frequency ,\n    max_token_dictionary_size\n):\n    """"""Makes the token dictionary.""""""\n    tokens = text_doc .mapPartitions (process_partition_for_tokens )\n        .reduceByKey (tokenstat_reducer ).values()\n    filtered_tokens  = tokens.filter(\n        lambda x: x.frequency  >= min_term_frequency )\n    all_tokens  = filtered_tokens .collect()\n    sorted_token_dict  = sorted(\n        all_tokens , key=lambda x: x.frequency , reverse=True)\n    count = min(max_token_dictionary_size , len(sorted_token_dict ))\n    for i in range(count):\n        sorted_token_dict [i].index = i\n    TokenDictionary .save(sorted_token_dict [:count], token_output )\ndef process_partition_for_titles (doc_iterator ):\n    """"""Processes a document partition for titles.""""""\n    title_dict  = {}\n    for doc in doc_iterator :\n        count_titles (doc, title_dict )\n    for token_stat  in title_dict .values():\n        yield (token_stat .token, token_stat )\ndef make_title_dictionary (\n    text_doc ,\n    title_output ,\n    min_title_frequency ,\n    max_title_dictionary_size\n):\n    """"""Makes the title dictionary.""""""\n    titles = text_doc\n      .mapPartitions (process_partition_for_titles )\n      .reduceByKey (tokenstat_reducer ).values()\n    filtered_titles  = titles.filter(\n      lambda x: x.frequency  >= min_title_frequency )\n    all_titles  = filtered_titles .collect()\n    sorted_title_dict  = sorted(\n      all_titles , key=lambda x: x.frequency , reverse=True)\n    count = min(max_title_dictionary_size , len(sorted_title_dict ))\n    for i in range(count):\n        sorted_title_dict [i].index = i\n    TokenDictionary .save(sorted_title_dict [:count], title_output )\nBig Data Frameworks | 139\nAs you can see, Spark makes it easy to scale a program from a single machine to\nrun on a cluster of many machines! Starting from the main function, we create\nSparkContext , read in the input file as a text file, parse it, and then make the token\nand title dictionaries. The RDD is passed around as arguments of the processing\nfunction and can be used multiple times and fed to various map functions (such as\nthe token and title dictionary methods).\nThe heavy lifting in the make-dictionary methods is done by the process-partitions\nfunctions, which are map functions that are applied to entire partitions at once.\nPartitions  are large chunks of the input, typically about 64 MB in size and processed\nas one chunk so that we save on network bandwidth by doing map-side combines.\nThis is a technique to apply the reducer repeatedly on mapped partitions as well as\nafter joining by the key (which in this case is the token) and summing up the counts.\nThe reason we do this is to save on network bandwidth, which is typically the slowest\npart of data processing pipelines after disk access.\nY ou can view the output of the make_dictionary  phase by using the utility codex.py ,\nwhich dumps protocol buffers of different kinds registered in the program. Since all\nour data is serialized as bzipped and uuencoded text files, the only difference is which\nprotocol buffer schema is used to decode the serialized data, so we can use just one\nprogram to print out the first few elements of the data for debugging. Although it\nmight be much simpler to store data as JSON, XML, or CSV files, having a schema\nwill save you from future grief because protocol buffers are extensible and support\noptional fields. They are also typed, which can save you from accidental mistakes in\nJSON, such as not knowing whether a value is a string or float or int, or having a field\nas a string in some files and as an int in others. Having an explicit typed schema saves\nus from a lot of these mistakes.\nThe next step in the pipeline is make_cooccurrence.py . As the name implies, this\nprogram simply counts the number of times each token occurs with another token.\nThis is essentially a sparse way of representing a graph. In nlp.proto , each row of the\nsparse co-occurrence matrix is as follows:\n// Co-occurrence matrix row.\nmessage CooccurrenceRow {\n    uint64 index = 1;\n    repeated uint64 other_index = 2;\n    repeated float count = 3;\n}\nIn a co-occurrence matrix , each row i has an entry at column j that represents the\nnumber of times token j has co-occurred with token i. This is a handy way of associ‐\nating the similarity between tokens i and j because if they co-occur a lot, they must\nbe more related to each other than tokens that do not co-occur. In the protocol buffer\nformat, these are stored as two parallel arrays of other_index  and count . We use\nindices because they are smaller than storing raw words, especially with the varying\n140 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nencoding that protocol buffers use (i.e., the matrix of rows and columns indexed by\ntokens, and elements that are the co-occurrences of the indices). In this encoding,\nsmall integers take fewer bits to represent than large integers; since we reverse-sorted\nthe dictionary by frequency, the most commonly occurring tokens have the smallest\nindices.\nAt this stage, if you wanted to make a very simple recommender based on frequent\nitem similarity co-occurrence, you would look up the row for token i and return by\ncount order the tokens j. The simple recommender would make a good variant on\nthe popular item recommender as described in the earlier chapters.\nCustomers Also Bought\nThis  concept of co-occurrences will be developed further in Chap‐\nter 9 , but let’s take a moment to reflect on this concept of the MPIR\nand co-occurrences. When we look at the co-occurrence matrix\nfor items, we can take row sums or column sums to determine\nthe number of times each item has been seen (or purchased). That\nwas how we built the MPIR in Chapter 2 . If instead we look at\nthe MPIR for a particular row corresponding to an item the user\nhas seen, that’s simply  the conditional MPIR —i.e., the most popular\nitem, given that the user has seen item i.\nHowever, here we can choose to do an embedding or low-rank representation of the\nco-occurrence matrix. An embedding representation of a matrix is handy because\nit allows us to represent each item as a vector. One way to factor the matrix is via\nsingular value decomposition, or SVD (see “Latent Spaces” on page 167), but we\nwon’t be doing that here. Instead we will be learning GloVE embeddings, which were\ndeveloped for NLP .\nThe objective function of GloVE embedding is to learn two vectors such that their\ndot product is proportional to the log count of co-occurrence between the two\nvectors. The reason this loss function works is that the dot product will then be\nproportional to the log count of co-occurrence; thus, words that frequently occur\ntogether will have a larger dot product than words that do not. To compute the\nembeddings, we need to have the co-occurrence matrix handy, and luckily the previ‐\nous step in the pipeline has generated such a matrix for us to process.\nFeature-Item Versus Item-Item\nWe introduce feature-item recommenders in this section via the conversion step from\nwords to token IDs. The way we look up the embedding ID for the model is based\non the index—either features or items. For the top N popular words, we have a\none-to-one mapping from the dictionary index to the embedding ID. However, for\nBig Data Frameworks | 141",22748
64-GloVE Model Specification in JAX and Flax.pdf,64-GloVE Model Specification in JAX and Flax,"long-tailed words, we want them to map to the same value of embedding_id  if we can\nhelp it.\nOne cheap way of computing a feature from a word is called min-hashing : we find\n4 consecutive bytes of a word, compute the hash of these bytes, and find the mini‐\nmum hash of the overlapping 4 bytes. This process makes it more likely to relate\n*z*e*b*r*a* h*a*s*h*e*s* to zebras. This feature is then used to represent these sets\nof words as an equivalence class. All words that hash to the same MinHash value\nare in the same equivalence class. This allows us to handle any new long-tailed\nword naturally for the time being until a new dictionary is built. It might result in\nundesirable mistakes in certain applications, but for other applications where it might\nbe safe to do so, a feature-based representation of an item might be mixed into an\nembedding system as we have done.\nAnother alternate way to get feature embeddings is to train an autoencoder or some\nkind of embedding representation that is learned off the features of the item so that\nthe recommender might generalize to new, unseen items. However, for the sake of\nsimplicity and in this word embedding case, we simply use the MinHash for ease of\nunderstanding. The MinHash implementation can be seen at wikipedia/token_dictio‐\nnary.py .\nGloVE Model Definition\nFor this section, please refer to the code at train_coccurence.py .\nSuppose we have tokens i and j from the token dictionary. We know that they have\nco-occurred with each other N times. We want to somehow generate an embedding\nspace such that the vectors xi*xj are proportional to log( N). The arguments for\nlog count and the exact equation are derived in the “GloVe: Global Vectors for Word\nRepresentation”  by Jeffrey Pennington et al. We will show just the derived result:\nypredicted =xix˙j+ biasi+ biasj\nHere, x is the embedding lookup. In the code, we use 64-dimensional vectors, which\nare not too small as to have insufficient capacity to represent the embedding space\nbut are not too large that it would take up too much memory when we have an\nembedding for the entire dictionary. The bias terms are there to soak up the large\ncounts from very popular items such as the, a, and and that co-occur with many other\nterms.\n142 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nThe loss we want to minimize is the squared difference between the prediction and\nthe actual value:\nytarget = 1 + log10N\nweight = min 1,N/1000 . 75\nloss = weight * ypredicted −ytarget2\nThe weighting term in the loss function is to prevent domination by very popular\nco-occurrences as well as to downweight rarer co-occurrences.\nGloVE Model Specification  in JAX and Flax\nLet’s  look at the implementation of the GloVE model based on JAX and Flax. This is\nin the file wikipedia/models.py  on the GitHub repository:\nimport flax\nfrom flax import linen as nn\nfrom flax.training  import train_state\nimport jax\nimport jax.numpy  as jnp\nclass Glove(nn.Module):\n    """"""A simple embedding model based on gloVe.\n       https://nlp.stanford.edu/projects/glove/\n    """"""\n    num_embeddings : int = 1024\n    features : int = 64\n    def setup(self):\n        self._token_embedding  = nn.Embed(self.num_embeddings ,\n                                         self.features )\n        self._bias = nn.Embed(\n            self.num_embeddings , 1, embedding_init =flax.linen.initializers .zeros)\n    def __call__ (self, inputs):\n        """"""Calculates the approximate log count between tokens 1 and 2.\n        Args:\n          A batch of (token1, token2) integers representing co-occurence.\n        Returns:\n          Approximate log count between x and y.\n        """"""\n        token1, token2 = inputs\n        embed1 = self._token_embedding (token1)\n        bias1 = self._bias(token1)\n        embed2 = self._token_embedding (token2)\n        bias2 = self._bias(token2)\n        dot_vmap  = jax.vmap(jnp.dot, in_axes=[0, 0], out_axes =0)\nGloVE Model Definition  | 143\n        dot = dot_vmap (embed1, embed2)\n        output = dot + bias1 + bias2\n        return output\n    def score_all (self, token):\n        """"""Finds the score of token vs all tokens.\n        Args:\n          max_count: The maximum count of tokens to return.\n          token: Integer index of token to find neighbors of.\n        Returns:\n          Scores of nearest tokens.\n        """"""\n        embed1 = self._token_embedding (token)\n        all_tokens  = jnp.arange(0, self.num_embeddings , 1, dtype=jnp.int32)\n        all_embeds  = self._token_embedding (all_tokens )\n        dot_vmap  = jax.vmap(jnp.dot, in_axes=[None, 0], out_axes =0)\n        scores = dot_vmap (embed1, all_embeds )\n        return scores\nFlax is rather simple to use; all networks inherit from Flax’s linen neural network\nlibrary and are modules. Flax modules are also Python dataclasses, so any hyper-\nparameters for the module are defined at the start of the module as variables. We\nhave only two for this simple model: the number of embeddings we want, which\ncorresponds to the number of tokens in the dictionary, and the dimension of the\nembedding vectors. Next, in the setup of the module, we actually create the layers we\nwant, which is just the bias term and embedding for each token.\nThe next part of the definition is the default method that is called when we use\nthis module. In this case, we want to pass in a pair of tokens, i, j; convert them to\nembeddings, xi,xj; and then compute the predicted log( count (ypredicted )).\nIn this section of code, we encounter the first difference between JAX and\nNumPy—namely,  a vectorized map, or vmap . A vmap  takes in a function and applies\nit in the same way across axes of tensors; this makes coding easier because you just\nhave to think about how the original function operates on lower-rank tensors such\nas vectors. In this example, since we are passing in batches of pairs of tokens and\nthen embedding them, we actually have a batch of vectors, and so we want to run the\ndot product over the batch dimension. We pass in JAX’s dot function, which takes\nvectors, run it over the batch dimension (which is axis 0), and tell vmap  to return the\noutputs as another batch dimension as axis 0. This allows us to efficiently and simply\nwrite code for lower-dimensional tensors and obtain a function that can operate on\nhigher-dimensional tensors by vmap ping over the extra axes. Conceptually, it would\nbe as if we looped over the first dimension and returned an array of the dot products.\nHowever, by converting this process to a function, we allow JAX to push this loop\ninto JITable code that can be compiled to run fast on a GPU.\nFinally, we also declare the helper function score_all , which takes one token and\nscores it against all the other tokens. Again, we use vmap  to take the dot product\n144 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender",6984
65-Part III. Ranking.pdf,65-Part III. Ranking,"with the particular token xi but run it against all the other token embeddings. The\ndifference here is that since xi is already a vector, we don’t need to vmap  over it.\nTherefore, in in_axes , we supply [None, 0] , which means don’t vmap  over the axes\nof the first argument but instead vmap  over axis 0 of the second argument, which is\nthe batch of all the embeddings of all the tokens. Then we return the result, which is\nan array that is the dot product of xi against all other embeddings but without the\nbias terms. We don’t use the bias term in scoring because it was used in part to soak\nup the popularity of very common tokens, and our scoring function would be more\ninteresting if we just used the dot product part of it for scoring.\nGloVE Model Training with Optax\nNext, let’s take a look at wikipedia/train_coocurrence.py . Let’s look specifically at the\npart where the model is called to dig into some JAX specifics:\n@jax.jit\ndef apply_model (state, inputs, target):\n    """"""Computes the gradients and loss for a single batch.""""""\n    # Define glove loss.\n    def glove_loss (params):\n        """"""The GloVe weighted loss.""""""\n        predicted  = state.apply_fn ({'params' : params}, inputs)\n        ones = jnp.ones_like (target)\n        weight = jnp.minimum(ones, target / 100.0)\n        weight = jnp.power(weight, 0.75)\n        log_target  = jnp.log10(1.0 + target)\n        loss = jnp.mean(jnp.square(log_target  - predicted ) * weight)\n        return loss\n    grad_fn = jax.value_and_grad (glove_loss )\n    loss, grads = grad_fn(state.params)\n    return grads, loss\nThe first point you will notice is the function decorator, @jax.jit . This tells JAX that\neverything in the function is JITable. There are some requirements for a function to\nbe JITable—mostly that it is pure, which is a computer science term indicating that if\nyou call a function with the same arguments, you would expect the same result. That\nfunction should not have any side effects and shouldn’t rely on a cached state such\nas a private counter or random-number generator with implicit state. The tensors\nthat are passed in as arguments should probably also have fixed shape, because every\nnew shape would trigger a new JIT compilation. Y ou can give hints to the compiler\nthat certain parameters are constants with static_argnums , but these arguments\nshouldn’t change too frequently, or else a lot of time will be spent compiling a\nprogram for each of these constants.\nGloVE Model Definition  | 145\nOne consequence of this pure function philosophy is that the model structure and\nmodel parameters are separated. This way, the model functions are pure and the\nparameters are passed in to the model functions, allowing the model functions to\nbe jitted. This is why we apply the model’s apply_fn  to the parameters rather than\nsimply having the parameters as part of the model.\nThis apply_model  function can then be compiled to implement the GloVE loss that\nwe described earlier. The other new functionality that JAX provides above NumPy is\nautomatically computing gradients of functions. The JAX function value_and_grad\ncomputes the gradient of the loss with respect to the parameters. Since the gradient\nalways points in the direction in which the loss increases, we can use gradient descent\nto go the other way and minimize the loss. The Optax library has a few optimizers to\npick from, including SGD (stochastic gradient descent with momentum) and ADAM.\nWhen you run the training program, it will loop over the co-occurence matrix and\ntry to generate a succinct form of it by using the GloVE loss function. After about an\nhour, you should be able to see the highest-scoring term.\nThe nearest neighbors for “democracy, ” for example, are as follows: democ‐\nracy:1.064498, liberal:1.024733, reform:1.000746, affairs:0.961664, socialist:0.952792,\norganizations:0.935910, political:0.919937, policy:0.917884, policies:0.907138, and\n--date:0.889342.\nAs you can see, the query token itself is usually the highest-scoring neighbor, but this\nis not necessarily true, as a very popular token might actually be higher scoring to the\ntoken than the query token itself.\nSummary\nAfter reading this chapter, you should have a good overview of the basic ingredients\nfor assembling a recommender system. Y ou have seen how to set up a basic Python\ndevelopment environment; manage packages; specify inputs and outputs with flags;\nencode data in various ways, including using protocol buffers; and process the data\nwith a distributed framework with PySpark. Y ou also learned how to compress giga‐\nbytes of data into a few megabytes of a model that is able to generalize and quickly\nscore items, given a query item.\nTake some time to play with the code and read the documentation of the various\npackages referenced to get a good sense of the basics. These foundational examples\nhave widespread applications, and having a firm grasp on them will make your\nproduction environments more accurate.\n146 | Chapter 8: Putting It All Together: Data Processing and Counting Recommender\nPART III\nRanking\nWhat are the appropriate candidates for a given recommendation? Which of these candi‐\ndates is the best? What about the 10 best?\nSometimes the best recommender system is simply item availability, but in the major‐\nity of cases, you’re hoping to capture subtle signals about user preference to deliver\nexcellent recommendations among potentially millions of options. Personalization\nis the name of the game; while we previously focused on item-item similarity with\nrespect to external meaning, we need to start attempting to infer user taste and desire.\nWe’ d also better start making this an ML task eventually. Beyond discussions of\nfeatures and architectures, we’ll need to define the objective functions. At first blush,\nthe objective for recommendations is the simple binary “Did they like it?”—so maybe\nwe’re simply predicting the outcome of a Bernoulli trial. However, as we discussed in\nthe introduction, there are a variety of ways to get the signal about how much they\nliked it. Moreover, recommendation systems in most cases grant one kindness: you\nget multiple shots on goal. Usually you get to recommend a few options, so we are\nvery interested in predictions of which things they’ll like the most. In this part of the\nbook, we’ll take all that you’ve learned and start getting numbers out. We’ll also talk\nabout explicit loss functions used to train and evaluate your models.",6545
66-Chapter 9. Feature-Based and Counting-Based Recommendations.pdf,66-Chapter 9. Feature-Based and Counting-Based Recommendations,,0
67-Bilinear Factor Models Metric Learning.pdf,67-Bilinear Factor Models Metric Learning,"CHAPTER 9\nFeature-Based and Counting-Based\nRecommendations\nConsider this oversimplified problem: given a bunch of new users, predict which will\nlike our new mega-ultra-fancy-fun-item-of-novelty, or MUFFIN for short. Y ou may\nstart by asking which old users like MUFFIN; do those users have any aspects in\ncommon? If so, you could build a model that predicts MUFFIN affinity from those\ncorrelated user features.\nAlternatively, you could ask, “What are other items people buy with MUFFIN?”\nIf you find that others frequently also ask for JAM (just-awesome-merch), then\nMUFFIN may be a good suggestion for those who already have JAM. This would be\nusing the co-occurrence of MUFFIN and JAM as a predictor. Similarly, if your friend\ncomes along with tastes similar to yours—you both like SCONE, JAM, BISCUIT, and\nTEA—but your friend hasn’t yet had the MUFFIN, if you like MUFFIN, it’s probably\na good choice for your friend too. This is using the co-occurrence of items between\nyou and your friend.\nThese item relationship features will form our first ranking methods in this chapter;\nso grab a tasty snack and let’s dig in.\nBilinear Factor Models (Metric Learning)\nAs per the usual idioms about running in front of horses and walking after the cart,\nlet’s start our journey into ranking systems with what can be considered the  naive  ML\napproaches. Via these approaches, we will start to get a sense of where the rub lies\nin building recommendation systems and why some of the forthcoming efforts are\nnecessary at all.\n149\nLet’s begin again with our basic premise of recommendation problems: to estimate\nratings of item x by user i written as ri,x. Note the slight change in notation from\nearlier for reasons that will become clear momentarily.  In a usual ML paradigm, we\nmight claim that estimating this score is done via properties of the item and the user,\nand frequently those properties would be described as features, and thus i and x can\nbe the user and item vectors, respectively, composed of these features.\nNow, we consider user i with their collection of previously interacted-with items\nℛi, and consider ℐ=xx ∈ℛi the set of vectors associated to those items in this\nfeature space. We can then map this collection of vectors to a representation to yield a\ncontent-based feature vector for i. Figure 9-1  illustrates an example mapping.\nFigure 9-1. Content-to-feature vector\nThis extremely simple approach can turn a collection of item features and user-item\ninteractions into features of the user. Much of the following will be increasingly\nrich ways of doing this. Thinking very hard about the map, the features, and the\nrequirements for interaction  yields many of the key insights in the rest of the book.\nLet’s take the preceding mapping, i: =Fℐ, to be a simple aggregation like\ndimension-wise average. Then recognize that the mapping will provide a vector of\nthe same dimension as the items. Now we have a user vector in the same “space” as\nthe items, and we can ask a similarity question as we did in our discussion of latent\nspace in Chapter 3 .\nWe need to move back to the mathematical framings to set up how to use these\nvectors. Ultimately, we’re now in a latent space with users and items, but how can\nwe do anything with that? Well you may already remember how to compare vector\nsimilarity. Let’s define the similarity to be cosine-similarity :\n150 | Chapter 9: Feature-Based and Counting-Based Recommendations\nsimi,x=i·x\ni*x\nIf we precompose our similarity with vector normalization, this is simply the inner\nproduct— and this is an essential first step toward recommendation systems . For conve‐\nnience, let’s always assume this space we’re working in is after normalization, so all\nsimilarity measures are done on the unit sphere:\nri,x∼ simi,x=∑\nkik*xk\nThis now approximates our ratings. But wait, dear reader, where are the learnable\nparameters? Let’s go ahead and make this a weighted summation, via a diagonal\nmatrix A:\nri,x∼ simAi,x=∑\nkak*ik*xk\nThis slight generalization already puts us in the world of statistical learning. Y ou can\nprobably already see how A can be used to learn which of the dimensions in this\nspace are most important for approximating the ratings, but before we make that\nprecise, let’s generalize yet once more:\nri,x∼ simAi,x=∑\nk,lakl*ik*xl\nThis nets us even more parameters! We see that now simAi,x=iAx, and we are\nonly one step away from the familiar ground of linear regression. Currently, our\nmodel is in the form of a bilinear regression , so let’s utilize a little linear algebra. For\nthe sake of exposition, let i ∈ℝn, x ∈ℝm, and A ∈ℝn×m, and then we have this:\nvecti*xT∈ℝn*m\nWe can simplify to the following:\nsimAi,x=iAx =vecti*xT*vectA\nIf we make up notation for the right-hand side, you’ll find your friend linear regres‐\nsion waiting for you:\nvix: =vecti*xT,β: =vectA\nBilinear Factor Models (Metric Learning) | 151\nThus:\nri,x∼ simAi,x=vixβ\nWith this computation behind us, we see that whether we wish to compute binary\nratings, ordinal ratings, or likelihood estimation, the tools in our linear models\ntoolbox can enter the party. We have available to us regularization and optimizers and\nany other fun we’re interested in from the linear models world.\nIf these equations feel frustrating or painful, let me try to offer you a geometric\nmental model. Each item and user is in a high-dimensional space, and ultimately\nwe’re trying to figure out which ones are closest to one another. People frequently\nmisunderstand these geometries by imagining the tips of the vectors being near one\nanother; this is not the case. These spaces are extremely high-dimensional, which\nresults in the analogy being far from the truth. Instead, ask if the values are similarly\nlarge in some of the vector indices.  This is a much simpler, but also more accurate,\ngeometric view: there are some subspaces in the extremely high-dimensional space\nwhere the vectors point in the same direction.\nThis forms the foundation for where we are going but has serious limitations for\nlarge-scale recommender problems. Y ou will see, however, that the feature-based\nlearning still has its place in the cold-start regime.\nNote that in addition to the preceding approach of building content-based features\nfor a user, we may also have obvious user features that are obtained via queries to\nthe user, or implicitly via other data collection; examples of these features include\nlocation, age range, and height.\nIs User Space the Same as Item Space?\nIn this section, we’ve discussed ways to put users and items in the same latent spaces.\nWe claim that we can make comparisons between users and items by vector opera‐\ntions. In mathematics, vectors are elements of vector spaces, and (finite dimensional)\nvector spaces are defined by their number of dimensions and the values that the\nvectors have as elements. For example, if we say it’s a three-dimensional vector space\nwith 8-bit integers, that’s sufficient to specify a vector space.\nHowever, devilish details are lurking around. First, what does distance  mean in a\nspecified vector space? We have many conventional measures, but it’s important to\nensure that comparisons between two spaces are utilizing the same definitions of\ndistance. Another consideration is the process by which you define the vectors of the\nspace; if you arrive at your vectors via a dimension reduction from a larger space,\nthere are likely density properties that you can expect not to be present naively.\nWhere this is most relevant for ranking models and recommendation systems is\n152 | Chapter 9: Feature-Based and Counting-Based Recommendations",7719
68-Feature-Based Warm Starting.pdf,68-Feature-Based Warm Starting,"that we frequently arrive at user space and item space separately, and often compute\ndistance between user and item vectors.\nIs this OK? In many cases, it lacks firm theoretical footing but works well. One\nparticular case where this does have a firm theoretical footing is MF. Rather than a\nlong digression on geometric algebra, we will give the following guidance: if you’re\ninterested in comparing two vectors that aren’t in the same space, ask yourself if\nthey’re of the same dimension, if distance is defined the same in both spaces, and\nif the density priors are similar. In fact, at times, none of these is true and you can\nstill get away with a comparison. But for each of these potential risks, it’s worth a\nstop-and-think.\nOne explicit example of a troubling difference in two latent spaces is found in “Poin‐\ncaré Embeddings for Learning Hierarchical Representations”  by Maximilian Nickel\nand Douwe Kiela; this paper provides an interesting way to encode relationships\nbetween items in your latent space via the implicit geometry. However, your users\nmay not be encoded in a hyperbolic space. Tread carefully if you compute inner\nproducts between these and Euclidean embedded vectors!\nFeature-Based Warm Starting\nAs you saw in Chapter 7 , there are a variety of ways to use features alongside some\nof the collaborative filtering (CF) and MF approaches we’ve presented. In particular,\nyou saw how encoders built via a two-towers architecture can be used for fast\nfeature-based recommendations in the cold-start scenario. Let’s look into this deeper\nand think carefully about features for new users or items.\nIn Chapter 9 , we built our bilinear factor model as a simple regression and, in\nfact, saw that all the standard ML modeling approaches would apply. However, we\ntook the user embedding to be features learned from item interactions: that is, the\ncontent-based feature vector. If our goal is to build a recommendation algorithm that\ndoes not need a history of user ratings, obviously this construction will not suffice.\nWe might begin by asking if the preceding factor regression approach could work\nin the pure user-feature setting—leave aside worries about the inner product that\ndepended on a mutual embedding and just take everything to be pure matrices.\nWhile this is a reasonable idea that can yield some results, we may quickly identify\nthe coarseness of this model: each user would then need to provide answers to\nqueries qk such that i ∈ℝk. Because the dimensionality of these user vectors scales\nlinearly with the number of questions we’re willing and able to ask the user, we are\npassing along the difficulty of the problem to our user experience.\nFeature-Based Warm Starting | 153\nBecause we intend on using CF via MF as our core model, we’ d really like to find a\nway to smoothly transition from the feature-based model into this MF, ensuring we\ntake advantage of user/item ratings as they emerge. In “The Evaluation Flywheel”  on\npage 117, we discussed using inference results and their subsequent outcomes in real\ntime to update the model, but how do we account for that in the modeling paradigm?\nIn a latent-factor model obtained via MF, we have the following:\nuivx\nHere, ui has a Gaussian prior with zero mean; this is why new users won’t yield\nuseful ratings before they have interaction data. We thus say that the user-matrix\nhas zero-concentrated priors.  Our first strategy to including features in our MF is to\nsimply build a better priors distribution.\nMore mathematically: we learn a regression model Gi∼ ui for initialization of our\nlearned factor matrix, and this means we’re learning the following:\nsi,x∼ wixγ+αi+βx+uivx\nHere, our wixγ is now a standard bilinear feature regression from user and item\nfeatures, the bias terms are learned to estimate popularity or rank inflation , and our\nfamiliar MF terms are uivx.\nNote that this approach provides a general strategy for including features into an\nMF model. How we fit the factors-features model is totally up to us, as are the\noptimization methods we wish to employ.\nAlso note that instead of regression-based approaches, priors can be established\nvia k-nearest neighbors in a purely feature-based embedding space. This modeling\nstrategy is explored in great detail in “Eliciting Auxiliary Information for Cold Start\nUser Recommendation: A Survey”  by Nor Aniza Abdullah et al. Compare this with\nthe item-item content-based recommender from Chapter 5 , where the query is an\nitem and similarity in item space is the link between the last item and the next.\nWe have established a strategy and a collection of approaches to building our models\nvia features. We’ve even seen how our MF will fall over for new users, only to be saved\nby a feature-based model. So why not stick to features? Why introduce factors at all?\n154 | Chapter 9: Feature-Based and Counting-Based Recommendations",4942
69-Segmentation Models and Hybrids.pdf,69-Segmentation Models and Hybrids,,0
70-Tag-Based Recommenders.pdf,70-Tag-Based Recommenders,"Segmentation Models and Hybrids\nSimilar to our preceding discussion of warm-starting via features is the closely related\nconcept of demographic-based systems.  Note that demographic  in this context need not\nrefer explicitly to personally identifiable information and can refer to the user data\ncollected during the sign-up process. Simple examples from book recommendations\nmight include a user’s favorite genres, self-identified price preference, book-length\npreferences, and favorite author. Standard methods of clustering-based regression\ncan be helpful in converting a small set of user features into recommendations for\nnew users. For these coarse user features, building simple feature-based models like\nnaive Bayes, can be especially effective.\nMore generally, given user feature vectors, we can formulate a similarity measure\nand then user segments to make new-user recommendations. This should feel similar\nto feature-based recommenders, but instead of requiring usage of user features, we\nmodel the user’s containment in a segment and then build our factor model from the\nsegment to different items.\nOne way to imagine this approach is to consider the modeling problem as estimating\nthe following for C, a user cluster:\nrC,x: = Avgri,x∣ i ∈ C\nThen we estimate Pj ∈ C , the probability a user j is a member of C. We can\neasily imagine that we instead wish to use the probability associated with each cluster\nto build a bagging model, and have each cluster contributed to a weighted average\nrating.\nWhile these ideas may not seem like interesting extensions to what we’ve built\npreviously, in practice they can be enormously useful for fast, explainable recommen‐\ndations for new users.\nAlso note that nothing in this construction is particular to the users; we can consider\nthe dual model  that takes the clustering to be at the level of the items and performs\na similar process. Combining these models can provide the coarsest model of simply\nuser segments to item groups, and utilizing several of these modeling approaches\nsimultaneously can provide important and flexible models.\nTag-Based Recommenders\nOne  special case of the segmentation model for item-based recommenders is a tag-\nbased recommender . This is a quite common first recommender to try when you have\nsome human labels and need to quickly turn it into a working recommender.\nSegmentation Models and Hybrids | 155\nLet’s talk through a toy example: you have a personal digital wardrobe, where you’ve\nlogged many features about each article of clothing in your personal closet. Y ou want\nyour fashion recommender to give you suggestions for what else to wear, given that\nyou’ve selected one piece for the day. Y ou wake up and see that it’s rainy outside,\nso you start by choosing a cozy cardigan. The model you’ve trained has found that\ncardigan has tags outerwear  and cozy, which it knows correlate well with bottoms  and\nwarm —so it’s likely to recommend heavier jeans today.\nThe upside of a tag recommender is how explainable and understandable the recom‐\nmendations are. The downside is that performance is directly tied to the amount of\neffort that’s put into tagging items.\nLet’s discuss a slightly more involved example of a tag-based recommender that\none of the authors built in collaboration with Ashraf Shaik and Eric Bunch for\nrecommending blog posts.\nThe goal was to warm-start the blog-post recommender by utilizing high-quality tags\nthat classified the blogs into themes. One special aspect of this system was its rich\nhierarchical tagging maintained by the marketing team. In particular, each tag type\nhad several values, and there were 11 tag types with up to 10 values each. Blogs had\nvalues for each tag type and sometimes had multiple tags in a single tag type for the\nblog. This may sound a bit complicated, but suffice it to say that each blog post could\nhave some of the 47 tags, and the tags were further grouped into types.\nOne of the first potential tasks is to use those tags to build a simple recommender,\nand we did, but doing so would mean missing a significant additional opportunity\nwhen afforded such high-quality tag data: evaluating our embeddings.\nFirst, we needed to understand how we could build user embeddings. Our plan\nwas to average the blog embeddings a user had seen, a simple CF approach when\nyou have a clear item embedding. Thus we wanted to train the best embedding\nmodel possible for these blogs. We started by considering models like BERT but were\nunsure whether the highly technical content would be meaningfully captured by our\nembedding model. This led us to realize that we could use the tags as a classifier\ndataset for our embedding. If we could test several embedding models by training\na simple multilayer perceptron (MLP) to perform multilabel multiclassification for\neach tag type, where the input features were the embedding dimensions, then our\nembedding space would capture the content well.\nSome of the embedding models were of varying dimensions, and some were quite\nlarge, so we also first used a dimension reduction (UMAP) to a standard size before\nwe trained the MLP . We used F1 scores  to determine which of the embedding\nmodels led to the best classification model for tags, and we used visual inspection to\nensure the groups were as we’ d hoped. This worked quite well and showed that some\nembeddings were much better than others.\n156 | Chapter 9: Feature-Based and Counting-Based Recommendations",5519
71-Hybridization.pdf,71-Hybridization,,0
72-Limitations of Bilinear Models.pdf,72-Limitations of Bilinear Models,"Hybridization\nY ou saw in the previous section how to blend our MF with simpler models by\ntaking priors from the simpler models and learning how to transition away. Coarser\napproaches to this process of hybridization  exist:\nWeighted combinations of models\nThis  approach is incredibly powerful, and the weights can be learned in a stan‐\ndard Bayesian framework.\nMultilevel modeling\nThis  approach can include learning a model to select which recommendation\nmodel should be used, and then learning models in each regime. For example, we\ncould use a tree-based model on user features when the user has fewer than 10\nhistorical ratings and then use MF after that. A variety of multilevel approaches\nexist, including switching  and cascading , which correspond roughly to voting and\nboosting, respectively.\nFeature augmentation\nThis  allows multiple vectors of features to be concatenated and a larger model\nto be learned. By definition, if we wish to combine feature vectors with factor\nvectors, like those coming from a CF, we will expect substantial nullity. Learning\ndespite that nullity allows a somewhat naive combination of the different kinds of\nfeatures to be fed into the model and operated on in all regimes of user activity.\nWe can combine these models in a variety of useful ways. However, we take the\nposition that instead of more complicated combinations of several models that work\nwell in different paradigms, we will attempt to stick to a relatively straightforward\nmodel-service architecture by doing the following:\n•Training the best model we can by using MF-based CF•\n•Using user and item feature-based models for cold start•\nLet’s see why we think feature-based modeling might not be the best strategy, even if\nwe do it via neural networks and latent factor models.\nLimitations of Bilinear Models\nWe started this chapter by describing bilinear modeling  approaches, and immediately\nyou should take warning—they’re linear relationships. Y ou can immediately wonder,\n“ Are there really linear relationships between the features of my users and items and\nthe pairwise affinity?”\nThe answer to this question might depend on the number of features, or it might not.\nEither way, skepticism is appropriate, and in practice the answer is overwhelmingly\nLimitations of Bilinear Models | 157\nno. Y ou might think, “Well then, as it is a linear approximation, MF also cannot\nsucceed, ” but that’s not so clear-cut. In fact, MF suggests that the linear relationship is\nbetween the latent factors , not the actual features. This subtle difference makes a world\nof difference.\nOne important callout before we move on to simpler ideas is that neural networks\nwith nonlinear activation functions can be used to build feature-based methods. This\ndomain has had some successes, but ultimately a surprising and important result\nis that neural CF does not outperform matrix factorization . This doesn’t suggest\nthat there are no useful approaches for feature-based models utilizing MLPs, but it\ndoes defray some of our worries about MF being too linear . So why not use more\nfeature-based approaches?\nThe first most obvious challenge for content-based, demographic-based, and any\nother feature-based method is getting the features . Let’s consider the dual problems:\nFeatures for users\nIf we want to collect features for users, we need to either ask them a series of\nqueries or infer those features implicitly. Inferring these via exogenous signals is\nnoisy and limited, but each query that we ask the user increases the likelihood\nof onboarding drop-off. When we think of user-onboarding funnels, we know\nthat each additional prompt or question incurs another chance that the user will\nnot complete the onboarding. This effect accumulates quickly, and without users\nmaking it through the funnel, the recommendation system won’t be very useful.\nFeatures for items\nOn the flip side, creating features for items is a heavily manual task. While many\nbusinesses need to do this task to serve other purposes as well, it still incurs a\nsignificant cost in many cases. If the features are to be useful, they need to be\nof high quality, which incurs more debt. But most importantly, if the number of\nitems is extremely large, the cost may quickly get out of reach. For large-scale\nrecommendation problems, manually adding features is simply infeasible. This is\nwhere automatic feature-engineering models can help.\nAnother significant issue in these feature-based models is separability  or distinguisha‐\nbility . These models are not useful if the features cannot separate the items or users\nwell. This leads to compounding problems as the cardinality increases.\nFinally, in many recommendation problems, we start with the assumption that taste\nor preference is extremely personal. We fundamentally believe that our interest in a\nbook will have less to do with the number of pages and publication date than how it\nconnects with us and our personal experience ( our deepest apologies to anyone who\nbought this book based on page number and publication date ). CF—while simple in\nconcept—speaks better to these connections via a shared experience network .\n158 | Chapter 9: Feature-Based and Counting-Based Recommendations",5285
73-Counting Recommenders.pdf,73-Counting Recommenders,,0
74-Correlation Mining.pdf,74-Correlation Mining,"Counting Recommenders\nHere  we will use the simplest feature type, simple counting. Counting the frequency\nand pairwise frequencies will provide a simple but useful set of initial models.\nReturn to the Most-Popular-Item Recommender\nOur super simple scheme from before, implementing the MPIR, provided us with\na convenient toy model, but what are the practical considerations of deploying an\nMPIR? It turns out that the MPIR provides an excellent framework for getting\nstarted on a Bayesian approximation approach to recommendations. Note that in this\nsection, we’re not even considering a personalized recommender; everything here is\nreward maximization across the entire user population. We follow the treatment in\nStatistical Methods for Recommender Systems  by Deepak K. Agarwal and Bee-Chung\nChen (Cambridge University Press).\nFor the sake of simplicity, let’s consider click-through rate  (CTR ) as our simple metric\nto optimize. Our formulation is as follows: we have ℐ=i items available to recom‐\nmend and initially only one time period  in which to do it, and we’re interested in an\nallocation plan , or a set of proportions xi, ∑i ∈ℐxi= 1, for how to recommend items.\nThis can be seen as a very simple multiarmed bandit problem with the reward given\nby the following:\nRx,c=∑\ni ∈ℐci*N*xi\nHere, ci represents prior distributions of CTR for each item. It’s plain to see that\nmaximizing this reward is achieved by allocating all recommendations to the item\nwith greatest pi, i.e., picking the most popular item in terms of CTR.\nThis setup makes it obvious that if we have strong confidence in our priors, this prob‐\nlem seems trivial. So let’s move to a case where we have a mismatch in confidence.\nLet’s consider two time periods , N0 and N1, as indicating the number of user visits.\nNote that we think of 0 as the past and 1 as the future in this model. Let’s assume\nthat we offer only two items  and that, somewhat mysteriously, for one item we have\n100% confidence in its CTR in each time period: q0 and q1 will denote these rates,\nrespectively. In contrast, we have only priors for our second item: p0∼ Pθ0 and\np1∼ Pθ1 will denote these rates, respectively, and we regard θi as a state vector.\nWe again notate the allocations with xi,t, where now the second index refers to time\nperiod. Then we can simply compute the expected number of clicks as follows:\nEN0*x0p0−q0+N1*x1p1−q1+q0N0+q1N1\nCounting Recommenders | 159\nThis is maximized by assuming a distribution for p1 as a function of x0 and p0.\nWith distributional assumptions that p0 is gamma distributed and p1 is normally\ndistributed, we can treat this as a convex optimization problem to maximize the\nclicks. See Statistical Methods for Recommender Systems  for a full treatment of the\nstatistics.\nThis toy example extends in both dimensions to model larger item sets and more\ntime windows and provides us with relatively straightforward intuition about the\nrelationship between our priors for each item and time step during this step-forward\noptimization.\nLet’s put this recommender in context: we’ve started with item popularity and gener‐\nalized to a Bayesian recommender that learns with respect to user feedback. Y ou\nmight consider a recommender like this for a very trend-based recommendations\ncontext like news; popular stories are often important, but that can change rapidly,\nand we want to be learning from user behavior.\nCorrelation Mining\nWe’ve  seen ways to use correlations between features of items and recommendations,\nbut we should not forget to use correlations between items themselves. Think back to\nour early discussions of cheese in Chapter 2  (Figure 2-1 ); we said that our CF gave us\na way to find mutual cheese tastes to recommend new cheeses. This was built on the\nnotion of ratings, but we can abstract away from the ratings and simply look at the\ncorrelations of items a user chooses. Y ou can imagine for an ecommerce bookseller\nthat a user’s choice of one book to read may be useful in recommending others—\neven if that user chooses not to rate the first book. We also saw this phenomena in\nChapter 8  as we used the co-occurrence of tokens in Wikipedia entries.\nWe introduced the co-occurrence matrix as the multidimensional array of counts\nwhere two items, i and j, co-occur. Let’s take a moment to discuss co-occurrence a bit\nmore deeply.\nCo-occurrence  is context dependent; for our Wikipedia articles, we considered co-\noccurrence of tokens in an article. In the case of ecommerce, co-occurrence can be\ntwo items purchased by the same user. For ads, co-occurrence can be two things\nthat the user clicked, and so on. Mathematically, given users and items, we construct\nan incidence vector  for each user, the binary vector of one-hot encoded features for\neach item that they interacted with. Those vectors are stacked into a vector to yield\na #users × #items  matrix in which each row is a user, each column is an item,\nand the elements equal 1 when a user-item pair has interacted.\nTo be mathematically precise, a user-item incidence structure  is a collection of sets of\nuser interactions, yuu ∈ U , with items xii ∈ I, where U indexes users and I indexes\nitems.\n160 | Chapter 9: Feature-Based and Counting-Based Recommendations\nThe associated user-item incidence matrix , U, is the binary matrix with rows indexed\nby sets, and columns indexed by nodes, such that elements are as follows:\neyu,xi=1xi∈ yu\n0 otherwise\nThe co-occurrence of  xa and xb is the order of the set yu∣ xa∈ yuandxb∈ yu. We\ncan also write that as a matrix that can be computed via a simple formula; let Cℐ\nbe the co-occurrences matrix—i.e., the matrix with rows and columns indexed by\nxii ∈ I and with elements that are the co-occurrences of the indices. Then we use the\nfollowing:\nCℐ=ℐT*ℐ\nHigher-Order Co-occurrences\nY ou could imagine further generalizing this recommender to aggregate across several\nitems the user has seen. In practice, you could consider the last five items the user has\ninteracted with and then compute the conditional-MPIR recommendations for each\nand union them together.\nAlternatively, you could generalize to higher-order  co-occurrences. In other words,\ninstead of pairs of items that co-occur, look at triples, quadruples, or more. To read\none approach to this generalization, check out “Higher Order Co-occurrence Tensors\nfor Hypergraphs via Face-Splitting”  by one of the authors.\nAs mentioned in “Customers Also Bought” on page 141, we can build a new variant\nof our MPIR by considering the rows or columns of the co-occurence matrix. The\nconditional MPIR  is the recommender that returns the max of the elements in the row\ncorresponding to xi, given the user’s last interaction was the item xi.\nIn practice, we often think of the row corresponding to xi as a basis vector , i.e., a\nvector qxi with one nonzero element in the xith position:\nqxi,j=1j=xi\n0 otherwise=0\n⋮\n1\n⋮\n0\nCounting Recommenders | 161",7015
75-Summary.pdf,75-Summary,"Then we can consider max—or even softmax—of the preceding dot products:\nCℐ=ℐT·ℐ*qxi\nThis yields the vector of co-occurrence counts between xi and each other item. Here\nwe frequently will call qxi a query  to indicate that it’s the input to our co-occurrence\nrecommendation model.\nHow Do You Store This Data?\nWe can think about co-occurrence data in a lot of ways. The main\nreason is because we expect that co-occurrences for recommenda‐\ntion systems are incredibly sparse. This means that the preceding\nmethod of matrix multiplication—which is approximately On3—\nis going to be relatively slow to compute fewer nonzero entries.\nBecause of this and concerns about storing huge matrices full of\nzeros, computer scientists have taken seriously the problem of rep‐\nresenting sparse matrices.\nMax Grossman  claims there are 101 ways, but in practice there\nare only a few. JAX supports BCOO , or batched coordinate format ,\nwhich is essentially a list of coordinates for nonzero elements, and\nthen what those elements are.\nIn our binary case of interactions, those are 1s, and for the co-\noccurrence matrix, those are the counts. The structure of these\nmatrices can be written as follows:\n{\n  'indices' : indices,\n  'values' : values,\n  'shape': [user_dim , items_dim ]\n}\nPointwise Mutual Information via Co-occurrences\nAn early recommendation system for articles used pointwise mutual information , or\nPMI, which is closely related to co-occurrences. In the context of NLP , PMI attempts\nto express how much more frequent co-occurrence is than random chance. Given\nwhat we’ve seen before, you can think of this as a normalized co-occurrences model.\nComputational linguists frequently use PMI as an estimator for word similarity or\nword meaning following from the distributional hypothesis:\nY ou shall know a word by the company it keeps.\n—John R. Firth, British linguist\n162 | Chapter 9: Feature-Based and Counting-Based Recommendations\nIn the context of recommendation ranking, items with very high PMI are said to\nhave a highly meaningful co-occurrence. This can thus be used as an estimator for\ncomplementary  items: given you’ve interacted with one of them, you should interact\nwith the other.\nPMI is computed for two items, xi,xj, via the following:\npxi,xj\npxi*pxj=Cℐxi,xj* # total interactions\n#xi* #xj\nThe PMI calculation allows us to modify all our work on co-occurrence to a more\nnormalized computation, and thus is a bit more meaningful. This process is related to\nthe GloVE model we learned in “GloVE Model Definition” on page 142 . The negative\nPMI values allow us to understand when two things are not often witnessed together.\nThese PMI calculations can be used to recommend another item in a cart  when an\nitem has been added and you find those with very high PMI. It can be used as a\nretrieval method by looking at the set of items a user has already interacted with and\nfinding items that have high PMI with several of them.\nLet’s look at how to turn co-occurrences into other similarity measures.\nIs PMI a Distance Measurement?\nA good question to consider at this point is “Is PMI between two\nobjects a measurement of distance? Can I define similarity directly\nas the PMI between two items, and thus yield a convenient geome‐\ntry in which to consider distances?” The answer is no. Recall that\none of the axioms of a distance function is the triangle inequality; a\nuseful exercise is to consider why the triangle inequality would not\nbe true for PMI.\nBut all is not lost. In the next section, we’ll show you how to formu‐\nlate some important similarity measurements from co-occurrence\nstructures. Further, in the next chapter, we’ll discuss Wasserstein\ndistance, which allows you to turn the co-occurrence counts into\na distance metric directly. The key difference will be considering\nthe co-occurrence counts of all other items simultaneously as a\ndistribution.\nSimilarity from Co-occurrence\nEarlier, we discussed similarity measures and how they come from the Pearson corre‐\nlation. The Pearson correlation is a special case of similarity when we have explicit\nratings, so let’s instead look at when we don’t.\nCounting Recommenders | 163\nConsider incidence sets associated to users, yuu ∈ U , as we define three distance\nmetrics:\nJaccard similarity, Jac −\nThe ratio of shared items by two users to the total items those users have\ninteracted with\nSørensen-Dice similarity, DSC −\nTwice  the ratio of shared items by two users to the sum of total items each user\nhas interacted with\nCosine similarity, Cosim −\nThe ratio of shared items by two users to the product of total items each user has\ninteracted with\nThese are all very related metrics with slightly different strengths. Here are some\npoints to consider:\n•Jaccard similarity is a real distance metric that has some nice properties for•\ngeometry; neither of the other two is.\n•All three are on the interval 0, 1, but you’ll often see cosine extended to − 1, 1 •\nby including negative ratings.\n•Cosine can accommodate “thumbs-up/thumbs-down” by merely extending all•\ninteractions to have a polarity of ±1.\n•Cosine can accommodate “multiple interactions” if you allow the vectors to be•\nnonbinary and count the number of times a user interacts with an item.\n•Jaccard and Dice are related by the simple equation S= 2J/1 +J, and you can •\neasily compute one from the other.\nNotice that we’ve defined all these similarity measures between users. We’ll show in\nthe next section how to extend these definitions to items and how to turn these into\nrecommendations.\nSimilarity-Based Recommendations\nIn each of the preceding distance metrics, we’ve defined a similarity measure, but\nwe haven’t yet discussed how similarity measures turn into recommendations. As we\ndiscussed in “Nearest Neighbors” on page 35, we utilize similarity measures in our\nretrieval step; we wish to find a space in that items that are close  to one another\nare good recommendations. In the context of ranking, our similarity measure can be\nused directly to order the recommendations in terms of how likely the recommenda‐\ntion is relevant. In the next chapter, we’ll talk more about metrics of relevance.\n164 | Chapter 9: Feature-Based and Counting-Based Recommendations\nIn the preceding section, we looked at three similarity scores, but we need to expand\nour notion of the relevant sets for these measures. Let’s consider Jaccard similarity as\na prototype.\nGiven a user yu and an unseen item xi, let’s ask, “What is the Jaccard similarity\nbetween this user and item?” Let’s remember that Jaccard similarity is the similarity\nbetween two sets, and in the definition those sets were both incidence sets of users’\ninteractions . Here are three ways to use this approach for recommendations:\nUser-user\nUsing  our preceding definition, find the k users with maximum Jaccard similar‐\nity. Compute the percentage of these users who have interacted with xi. Y ou may\nalso wish to normalize this by popularity of the item xi.\nItem-item\nCompute  the set of users that each item has interacted with, and compute the\nk items most similar to xi with respect to Jaccard similarity of these item-user\nincidence sets. Compute the percentage of these items that are in yu’s set of\ninteractions. Y ou may also wish to normalize this by total interactions of yu or\nthe popularity of the similar items.\nUser-item\nCompute  the user yu’s set of items they’ve interacted with, and the set of items\nco-occurring with xi in any user’s incidence set of interaction. Compute the\nJaccard similarity between these two sets.\nFrequently in designing ranking systems, we specify the query , which refers to which\nnearest neighbors you’re looking for. We then specify how you use those neighbors\nto yield a recommendation. The items that may become the recommendation are the\ncandidates, but as you saw in the preceding example, the neighbors may not be the\ncandidates themselves. An additional complication is that you usually need to com‐\npute many candidate scores simultaneously, which requires optimized computations\nthat we’ll see in Chapter 16 .\nSummary\nIn this chapter, we’ve begun to dig deeper into notions of similarity—building on our\nintuition from retrieval that users’ preferences might be captured by the interactions\nthey’ve already demonstrated.\nWe started out with simple models based on features about users and built linear\nmodels relating them to our target outcomes. We then combined those simple models\nwith other aspects of feature modeling and hybrid systems.\nSummary | 165\nNext, we moved into discussing counting—in particular, counting the co-occurrence\nof items, users, or baskets. By looking at frequent co-occurrence, we can build models\nthat capture “If you liked a, you may like b. ” These models are simple to understand,\nbut we can use these basic correlation structures to build similarity measures, and\nthus latent spaces where ANN-based retrieval can yield good candidates for recom‐\nmendations.\nOne point that you may have noticed about the featurization of all the items and the\nbuilding of our co-occurrence matrices is that the number of features is astronomi‐\ncally large—one dimension for each item! This is the area of investigation we’ll tackle\nin the next chapter: how to reduce the dimensionality of your latent space .\n166 | Chapter 9: Feature-Based and Counting-Based Recommendations",9491
76-Chapter 10. Low-Rank Methods.pdf,76-Chapter 10. Low-Rank Methods,,0
77-Latent Spaces.pdf,77-Latent Spaces,"CHAPTER 10\nLow-Rank Methods\nIn the preceding chapter, we lamented at the challenge of working with so many\nfeatures. By letting each item be its own feature, we were able to express a lot of\ninformation about user preference and item-affinity correlations, but we were in big\ntrouble in terms of the curse of dimensionality. Combine this with the reality of very\nsparse features, and you’re in danger. In this chapter, we’ll turn to smaller feature\nspaces. By representing users and items as low-dimensional vectors, we can capture\nthe complex relationships between them in a more efficient and effective way. This\nallows us to generate more personalized and relevant recommendations for users\nwhile also reducing the computational complexity of the recommendation process.\nWe will explore the use of low-dimensional embeddings and discuss the benefits and\nsome of the implementation details of this approach. We will also look at code in JAX\nthat uses modern gradient-based optimization to reduce the dimension of your item\nor user representations.\nLatent Spaces\nY ou are already familiar with feature spaces, which are usually categorical or vector-\nvalued direct representations of the data. This can be the raw red, green, and blue\nvalues of an image, counts of items in a histogram, or attributes of an object like\nlength, width, and height. Latent features, on the other hand, do not represent any\nspecific real value feature of the items but are initialized randomly and then learned\nto suit a task. The GloVe embeddings we discussed in Chapter 8  are one such\nexample of a latent vector that was learned to represent the log count of words. Here\nwe will cover more ways to generate these latent features or embeddings.\n167\nFocus on Your “Strangths”\nThis chapter relies heavily on linear algebra, so it’s good to read up\non vectors, dot products, and norms of vectors before proceeding.\nHaving an understanding of matrices and the rank of matrices\nwill also be useful. Consider Linear Algebra and Its Applications  by\nGilbert Strang.\nOne of the reasons latent spaces are so popular is that they are usually lower in\ndimension than the features they represent. For example, if the user-item rating\nmatrix or interaction matrix (where the matrix entries are 1 if a user has interacted\nwith an item) is N × M dimensional, then factorizing the matrix into latent factors\nof N × K and K × M, where K is much smaller than N or M is an approximation of\nthe missing entries because we’re relaxing the factorization. K being smaller than N\nor M is usually called an information bottleneck —that is, we are forcing the matrix\nto be made up of a much smaller matrix. This means the ML model has to make\nup missing entries, which might be good for recommender systems. As long as\nusers have interacted with enough similar items, by forcing the system to have a\nlot less capacity in terms of degrees of freedom, then factorization can completely\nreconstruct the matrix, and the missing entries tend to get filled by similar items.\nLet’s see what happens, for example, when we factor a user-item matrix of 4 × 4 into a\n4 × 2 and a 2 × 4 vector using SVD.\nWe are supplying a matrix whose rows are users and whose columns are items. For\nexample, row 0 is [1, 0, 0, 1] , which means user 0 has selected item 0 and item 3.\nThese can be ratings or purchases. Now let’s look at some code:\nimport numpy as np\na = np.array([\n    [1, 0, 0 ,1],\n    [1, 0, 0 ,0],\n    [0, 1, 1, 0],\n    [0, 1, 0, 0]]\n)\nu, s, v = np.linalg.svd(a, full_matrices =False)\n# Set the last two eigenvalues to 0.\ns[2:4] = 0\nprint(s)\nb = np.dot(u * s, v)\nprint(b)\n# These are the eigenvalues with the smallest two set to 0.\ns = [1.61803399  1.61803399  0.         0.        ]\n# This is the newly reconstructed matrix.\n168 | Chapter 10: Low-Rank Methods",3872
78-Dot Product Similarity.pdf,78-Dot Product Similarity,"b = [[1.17082039  0.         0.         0.7236068  ]\n [0.7236068   0.         0.         0.4472136  ]\n [0.         1.17082039  0.7236068   0.        ]\n [0.         0.7236068   0.4472136   0.        ]]\nNotice that the user in row 1 now has a score for an item in column 3, and the\nuser in row 3 now has a positive score for the item in column 2. This phenomenon\nis generally known as matrix completion  and is a good property for recommender\nsystems to have because now we get to recommend new items to users. The general\nmethod of forcing the ML to go through a bottleneck that is smaller than the size\nof the matrix that it is trying to reconstruct is known as a low-rank approximation\nbecause the rank of the approximation is 2 but the rank of the original user-item\nmatrix is 4.\nWhat Is the Rank of a Matrix?\nAn N × M matrix may be considered as N row vectors (cor‐\nresponding to users) and M column vectors (corresponding to\nitems). When you consider the N vectors of dimension M, the\nrank of the matrix  is the volume of the polyhedron defined by the\nN vectors in M dimensions. This is often different from the way\nwe talk about the rank of matrices, however. While it’s the most\nnatural and precise definition, we instead say it’s the “minimum\nnumber of dimensions necessary to represent the vectors of the\nmatrix. ”\nWe will cover SVD in more detail later in the chapter. This was just to whet your\nappetite to understand how latent spaces are related to recommender systems.\nDot Product Similarity\nIn Chapter 3  we introduced similarity measures, but now we return to the dot\nproduct in the context of similarity because of their increased importance in latent\nspaces. After all, latent spaces are built on the assumption that distance is similarity.\nThe dot-product similarity is meaningful in recommendation systems because it\nprovides a geometric interpretation of the relationship between users and items in the\nlatent space (or potentially items and items, users and users, etc.). In the context of\nrecommendation systems, the dot product can be seen as a projection of one vector\nonto another, indicating the degree of similarity or alignment between the user’s\npreferences and the item’s characteristics.\nTo understand the geometric significance of the dot product, consider two vectors, u\nand p, representing the user and the product in the latent space, respectively. The dot\nproduct of these two vectors can be defined as follows:\nDot Product Similarity | 169\nu×p=upcosθ\nHere, ||u|| and ||p|| represent the magnitudes of the user and product vectors, and θ\nis the angle between them. The dot product is thus a measure of the projection of one\nvector onto another, which is scaled by the magnitudes of both vectors.\nThe cosine similarity, which is another popular similarity measure in recommenda‐\ntion systems, is derived directly from the dot product:\ncosine similarity u,p=u×p\nup\nThe cosine similarity ranges from –1 to 1, where –1 indicates completely dissimilar\npreferences and characteristics. A 0 indicates no similarity, and 1 indicates perfect\nalignment between the user’s preferences and the product’s characteristics. In the\ncontext of recommendation systems, cosine similarity provides a normalized measure\nof similarity that is invariant to the magnitudes of the user and product vectors. Note\nthat the choice of using cosine similarity versus L2 distance depends on the type of\nembeddings you’re using and the way you optimize the computations. In practice, the\nonly important feature is often the relative values.\nThe geometric interpretation of the dot product (and cosine similarity) in recom‐\nmendation systems is that it captures the alignment between user preferences and\nproduct characteristics. If the angle between the user and product vectors is small,\nthe user’s preferences align well with the product’s characteristics, leading to a higher\nsimilarity score. Conversely, if the angle is large, the user’s preferences and product’s\ncharacteristics are dissimilar, resulting in a lower similarity score. By projecting\nuser and item vectors onto each other, the dot-product similarity can capture the\ndegree of alignment between user preferences and item characteristics, allowing the\nrecommendation system to identify items that are most likely to be relevant and\nappealing to the user.\nAnecdotally, the dot product seems to capture popularity, as very long vectors tend\nto be easy to project on anything that isn’t completely perpendicular or pointing away\nfrom them. As a result, a trade-off exists between frequently recommending popular\nitems with a large vector length and longer-tail items that have smaller angular\ndifference with cosine distance.\nFigure 10-1  considers two vectors, a and b. With cosine similarity, the vectors are unit\nlength, so the angle is just the measure of similarity. However, with dot product, a\nvery long vector like c might be considered more similar to a than b even though the\nangle between a and b is smaller because of the longer length of c. These long vectors\ntend to be very popular items that co-occur with many other items.\n170 | Chapter 10: Low-Rank Methods",5217
79-Reducing the Rank of a Recommender Problem.pdf,79-Reducing the Rank of a Recommender Problem,"Figure 10-1. Cosine versus dot-product similarity\nCo-occurrence Models\nIn our Wikipedia co-occurrences examples, we determined that the co-occurrence\nstructure between two items could be used to generate measures of similarity. We\ncovered how PMI can take the counts of co-occurrence and make recommendations\nbased on very high mutual information between an item in the cart and others.\nAs we’ve discussed, PMI is not a distance metric but still has important similarity\nmeasures based on co-occurrence. Let’s return to this topic.\nRecall from earlier that PMI is defined as follows:\npxi,xj\npxi*pxj=Cℐxi,xj* # total interactions\n#xi* #xj\nNow let’s consider the discrete co-occurrence distribution , CDxi, defined as the collec‐\ntion of co-occurrences over all other xj:\nCDxi=Cℐxi,x1, ...,Cℐxi,xi, ...,Cℐxi,xN\nHere, j ∈1...N, and N is the total number of items. This represents the co-\noccurrence histogram between xi and all other items. By introducing this discrete\ndistribution, we can utilize another tool: the Hellinger distance.\nWe can measure distributional distance in a few ways, each with different advantages.\nFor our discussion, we will not go deeply into the differences and will stick to the\nsimplest but most appropriate. Hellinger distance is defined as follows:\nHP,Q=1 −∑inpiqi=1\n2∥P−Q ∥ 2\nCo-occurrence Models | 171\nP=pi and Q=qi are two probability density vectors. In our setting, P and Q can\nbe CDxi and CDxj.\nThe motivation behind this process is that we now have a proper distance between\nitems purely based on co-occurrences. We can use any dimension transformation or\nreduction on this geometry. Later we will show dimension-reduction techniques that\ncan use an arbitrary distance matrix and reduce the space to a lower-dimensional\nembedding that approximates it.\nWhat About Measure Spaces and Information Theory?\nWhile we’re discussing distributions, you may find yourself won‐\ndering, “Is there a distance between distributions such that distri‐\nbutions are points in a latent space?” Oh, you weren’t wondering\nthat? Well, OK. We’ll address it anyway.\nThe short answer is that we can measure the differences between\ndistributions. The most popular is Kullback–Leibler (KL) diver‐\ngence, which is usually described in a Bayesian sense as the amount\nof surprise in seeing the distribution P , when expecting the distri‐\nbution Q. However, KL is not a proper distance metric because it is\nasymmetric.\nAnother symmetric distance metric that has some nice proper‐\nties is the Hellinger distance. Hellinger distance is effectively the\n2-norm  measure theoretic distance. Additionally, Hellinger dis‐\ntance naturally generalizes to discrete distributions.\nIf this still hasn’t scratched your itch for abstraction, we can also\nconsider the total variation distance, which is the limit in the space\nof Fisher’s exact distance measures, which really means that it has\nall the nice properties of a distance of two distributions and no\nmeasure would ever consider them more dissimilar. Well, all the\nnice properties except for one: it’s not smooth. If you also want\nsmoothness for differentiability, you’ll need to approximate it via\nan offset.\nIf you ever need a distance between distributions, just use Hellin‐\nger.\nReducing the Rank of a Recommender Problem\nWe’ve  shown that as the number of items and users grows, we rapidly increase the\ndimensionality of our recommender problem. Because we’re representing each item\nand user as a column or vector, this scales like n2. One way to push back against this\ndifficulty is by rank reduction; recall our previous discussions about rank reduction\nvia factorization.\n172 | Chapter 10: Low-Rank Methods\nLike many integers, many matrices can be factored  into smaller matrices; for integers,\nsmaller  means of smaller value, and for matrices, smaller  means of smaller dimen‐\nsions. When we factor an N×M matrix, we will be looking for two matrices UN×d\nand Vd×M; note that when you multiply matrices together, they must share a dimen‐\nsion, and that dimension is eliminated, leaving the other two. Here, we’ll consider\nMFs when d≤N and d≤M. By factorizing a matrix, we ask for two matrices that\ntogether equal, or approximate, the original matrix:\nAi,j≃Ui,Vj\nWe seek a small value for d to reduce the number of latent dimensions. As you\nmay have already noticed, each of the matrices UN×d and Vd×M will correspond\nto rows or columns of the original ratings matrix. However, they’re expressed in\nfewer dimensions. This utilizes the idea of a low-dimensional latent space . Intuitively,\na latent space seeks to represent the same relationships as the full N×M dimensional\nrelationships in two sets of relationships: items versus latent features, and users versus\nlatent features.\nThese methods are also popular in other kinds of ML, but for our case, we’ll primarily\nbe looking at factorizing ratings or interaction matrices.\nMF via SVD\nSVD  and MF are closely related, but SVD is an important special case. The key\ndifference is in how the factorization is done and the types of matrices that each can\nbe applied to.\nFigure 10-2  shows how SVD works. The eigenvectors e1 and e2 correspond to the\nlargest two eigenvalues. e1 explains more of the data than e2 and lies along the\ndirection of the largest spread of points. Eigenvectors are always perpendicular to\neach other, so their dot product is always 0.\nFigure 10-2. Singular value decomposition\nReducing the Rank of a Recommender Problem | 173",5546
80-Optimizing for MF with ALS.pdf,80-Optimizing for MF with ALS,"SVD is a specific type of MF that decomposes a matrix into three separate matrices:\na left singular matrix, a diagonal matrix, and a right singular matrix. SVD can be\napplied to any real-valued matrix, but it is particularly well suited to dense matrices\nwith many nonzero entries; additionally, SVD matrices have properties useful for\nextracting particular kinds of relationships between latent features. The columns and\nrows of the singular matrices are the eigenvectors, and the values in the diagonal\nmatrix are the eigenvalues. This decomposition is handy in seeing how much infor‐\nmation in the original space is explained by the eigenvectors and corresponds to the\nmagnitude of the eigenvalue, so an eigenvector with a larger eigenvalue explains more\nof the original data than an eigenvector with a correspondingly smaller eigenvalue.\nMF decomposes a user-item matrix into two matrices that represent the preferences\nof users and the characteristics of items. This allows the recommendation system to\ngenerate personalized recommendations by matching the preferences of users with\nthe characteristics of items.\nFrequently, you must overcome a few challenges when considering MF:\n•The matrix you wish to factor is sparse and often non-negative and/or binary.•\n•The number of nonzero elements in each item vector can vary wildly, as we saw•\nin the Matthew effect.\n•Factorizing matrices is cubic in complexity.•\n•SVD and other full-rank methods don’t work without imputation, which itself is•\ncomplicated.\nWe’ll address these with some alternative optimization methods.\nOptimizing for MF with ALS\nThe basic optimization we wish to execute is to approximate as follows:\nAi,j≃Ui,Vj\nNotably, if you wish to optimize matrix entries directly, you’ll need to optimize\nd2*N*M elements simultaneously corresponding to the numbers of parameters\nin these factorizations. We can easily achieve a significant speedup, however, by\nalternating between tuning one matrix or the other. This is called alternating least\nsquares , commonly ALS, and it is a common approach to this problem. Instead\nof back-propagating updates to all terms in both matrices on each pass, you may\nupdate only one of the two matrices, which dramatically reduces the number of\ncomputations that need to take place.\n174 | Chapter 10: Low-Rank Methods\nALS seeks to switch back and forth between U and V, evaluating on the same loss\nfunction but updating the weights in only one matrix at a time:\nUU−η*U*∇U *DA,UV\nVV−η*V*∇V *DA,UV\nHere, η is the learning rate and D is our chosen distance function. We’ll present more\ndetails of this distance function momentarily. Before we move on, let’s consider a few\nof the intricacies here:\n•Each of these update rules requires the gradient with respect to the relevant•\nfactor matrix.\n•We update an entire factor matrix at a time, but we evaluate loss on the product•\nof the factor matrices versus the original matrix.\n•We have a mysterious distance function.•\n•By the way that we’ve constructed this optimization, we’re implicitly assuming•\nthat we’ll use this process to converge two well-approximating matrices (we often\nalso impose a limit on the number of iterations).\nIn JAX, these optimizations will be straightforward to implement, and we’ll see how\nsimilar the equational forms and the JAX code look.\nDistance Between Matrices\nWe can determine the distance between two matrices in a variety\nof ways. As we’ve seen before, different distance measurements for\nvectors yield different interpretations from the underlying space.\nWe won’t have as many complications for these computations, but\nit’s worth a short observation. The most obvious approach is one\nyou’ve already seen, observed mean squared error :\n∑ΩAi,j−Ui,Vj2\n1Ω\nOne useful alternative to the observed mean squared error can\nbe used when you have a single nonzero entry for a user vector\n(alternatively, a max rating). In that case, you could instead use a\ncross-entropy loss, which provides a logistic MF , and thus a proba‐\nbility estimate. For more details on how to implement this, see the\n“Matrix Factorization for Recommender Systems” tutorial  by Kyle\nChung.\nReducing the Rank of a Recommender Problem | 175",4251
81-Regularized MF Implementation.pdf,81-Regularized MF Implementation,"In our observed ratings, we expect (and see!) a large number of missing values and\nsome item vectors with an overrepresented number of ratings. This suggests that we\nshould consider nonuniformly weighted matrices. Next we’ll discuss how to account\nfor this and other variants with regularization.\nRegularization for MF\nWeighted alternating least squares  (W ALS) is similar to ALS but attempts to resolve\nthese two data issues more gracefully. In W ALS, the weight assigned to each observed\nrating is inversely proportional to the number of observed ratings for the user or\nitem. Therefore, observed ratings for users or items with few ratings are given more\nweight in the optimization process.\nWe can apply these weights as a regularization parameter in our eventual loss\nfunction:\n∑ΩAi,j− <Ui,Vj>2\nΩ+1\nN∑U\nOther regularization methods are important, and also popular, for MF. We’ll discuss\nthese two powerful regularization techniques:\n•Weight decay•\n•Gramian regularization•\nAs is often the case, weight decay  is our l2 regularization, which in this case is at the\nlevel of the Frobenius norm, i.e., the magnitude of the weight matrix. An elegant way\nto view this weight decay is that it’s minimizing the magnitude of the singular values .\nSimilarly, MF has another regularization technique that looks very standard but is\nquite different in calculation. This is via the Gramians —essentially regularizing the\nsize of the individual matrix entries, but there’s an elegant trick for the optimization.\nIn particular, a Gramian of a matrix U is the product UTU. The eagle-eyed among\nyou may recognize this term as the same term we previously used to calculate\nco-occurrences for binary matrices. The connection is that both are simply trying to\nfind efficient representations of dot products between a matrix’s rows and columns.\nThese regularizations are the Frobenius terms:\nRU,V=1\nN∑\niN\nUi\n22\n+1\nM∑\njM\nVj22\n176 | Chapter 10: Low-Rank Methods\nOr expanded, the equation looks like this:\nRU,V=1\nN∑\niN∑\nkd\nUi,k2+1\nM∑\njM∑\nld\nVj,l2\nAnd here are the Gramian terms:\nGU,V: =1\nN·M∑\niN∑\njM\nUi,Vj2\n=1\nN·M*∑\nk,ld\nUTU*VTVk,l.\nFinally, we have our loss function:\n1\nΩ∑\ni,j∈ ΩAij−Ui,Vj2+λR1\nN∑\niN∑\nkd\nUi,k2+1\nM∑\njM∑\nld\nVj,l2+λG1\nN·M\n*∑\nk,ld\nUTU*VTVk,l\nRegularized MF Implementation\nSo far, we’ve written a lot of math symbols, but all of those symbols have allowed us\nto arrive at a model that is extremely powerful. Regularized matrix factorization  is an\neffective model for medium-sized recommender problems. This model type is still in\nproduction for many serious businesses. One classic issue with MF implementations\nis performance, but because we’re working in JAX, which has extremely native GPU\nsupport, our implementation can actually be much more compact than what you may\nfind in something like a PyTorch example .\nLet’s work through how this model would look to predict ratings for a user-item\nmatrix via this doubly regularized model with Gramians.\nFirst we’ll do the simple setup. This will assume your ratings matrix is already on\nwandb:\nimport jax\nimport jax.numpy  as jnp\nimport numpy as np\nimport pandas as pd\nimport os, json, wandb, math\nfrom jax import grad, jit\nfrom jax import random\nReducing the Rank of a Recommender Problem | 177\nfrom jax.experimental  import sparse\nkey = random.PRNGKey(0)\nwandb.login()\nrun = wandb.init(\n    # Set entity to specify your username or team name\n    entity=""wandb-un"" ,\n    # Set the project where this run will be logged\n    project=""jax-mf"" ,\n    # associate the runs to the right dataset\n    config={\n      ""dataset"" : ""MF-Dataset"" ,\n    }\n)\n# note that we assume the dataset is a ratings table stored in wandb\nartifact  = run.use_artifact ('stored-dataset:latest' )\nratings_artifact  = artifact .download ()\nratings_artifact_blob  = json.load(\n    open(\n        os.path.join(\n            ratings_artifact ,\n            'ratings.table.json'\n        )\n    )\n)\nratings_artifact_blob .keys()\n# ['_type', 'column_types', 'columns', 'data', 'ncols', 'nrows']\nratings = pd.DataFrame ( # user_id, item_id, rating, unix_timestamp\n    data=ratings_artifact_blob ['data'],\n    columns=ratings_artifact_blob ['columns' ]\n)\ndef start_pipeline (df):\n    return df.copy()\ndef column_as_type (df, column: str, cast_type ):\n    df[column] = df[column].astype(cast_type )\n    return df\ndef rename_column_value (df, target_column , prior_val , post_val ):\n    df[target_column ] = df[target_column ].replace({prior_val : post_val })\n    return df\ndef split_dataframe (df, holdout_fraction =0.1):\n    """"""Splits a DataFrame into training and test sets.\n    Args:\n      df: a dataframe.\n      holdout_fraction: fraction of dataframe rows to use in the test set.\n178 | Chapter 10: Low-Rank Methods\n    Returns:\n      train: dataframe for training\n      test: dataframe for testing\n    """"""\n    test = df.sample(frac=holdout_fraction , replace=False)\n    train = df[~df.index.isin(test.index)]\n    return train, test\nall_rat = (ratings\n    .pipe(start_pipeline )\n    .pipe(column_as_type , column='user_id' , cast_type =int)\n    .pipe(column_as_type , column='item_id' , cast_type =int)\n)\ndef ratings_to_sparse_array (ratings_df , user_dim , item_dim ):\n    indices = (np.array(ratings_df ['user_id' ]), np.array(ratings_df ['item_id' ]))\n    values = jnp.array(ratings_df ['rating' ])\n    return {\n        'indices' : indices,\n        'values' : values,\n        'shape': [user_dim , item_dim ]\n    }\ndef random_normal (pr_key, shape, mu=0, sigma=1, ):\n    return (mu + sigma * random.normal(pr_key, shape=shape))\nx = random_normal (\n    pr_key = random.PRNGKey(1701),\n    shape=(10000,),\n    mu = 1.0,\n    sigma = 3.0,\n) # these hyperparameters are pretty meaningless\ndef sp_mse_loss (A, params):\n    U, V = params['users'], params['items']\n    rows, columns = A['indices' ]\n    estimator  = -(U @ V.T)[(rows, columns)]\n    square_err  = jax.tree_map (\n        lambda x: x**2,\n        A['values' ]+estimator\n    )\n    return jnp.mean(square_err )\nomse_loss  = jit(sp_mse_loss )\nNote that we’ve had to implement our own loss function here. This is a relatively\nstraightforward  mean square error (MSE) loss, but it’s taking advantage of the sparse\nnature of our matrix. Y ou may notice in the code that we’ve converted the matrix\nto a sparse representation, so it’s important that our loss function cannot only take\nReducing the Rank of a Recommender Problem | 179\nadvantage of that representation, but also be written to utilize the JAX device arrays\nand mapping/jitting.\nIs That Loss Function Really Right?\nIf you’re curious about this loss function that appeared like magic,\nwe understand. While writing this book, we were extremely uncer‐\ntain about what the best implementation of this loss function that\nleverages JAX would look like. There are actually many reasonable\napproaches to this kind of optimization. To that end, we wrote a\npublic experiment to benchmark several approaches on Colab .\nNext, we need to build model objects to handle our MF state as we train. This code,\nwhile essentially mostly template code, will set us up well to feed the model into a\ntraining loop in a relatively memory-efficient way. This model was trained on 100\nmillion entries for a few thousand epochs on a MacBook Pro in less than a day:\nclass CFModel(object):\n    """"""Simple class that represents a collaborative filtering model""""""\n    def __init__ (\n          self,\n          metrics: dict,\n          embeddings : dict,\n          ground_truth : dict,\n          embeddings_parameters : dict,\n          prng_key =None\n    ):\n        """"""Initializes a CFModel.\n        Args:\n        """"""\n        self._metrics  = metrics\n        self._embeddings  = embeddings\n        self._ground_truth  = ground_truth\n        self._embeddings_parameters  = embeddings_parameters\n        if prng_key  is None:\n            prng_key  = random.PRNGKey(0)\n        self._prng_key  = prng_key\n    @property\n    def embeddings (self):\n        """"""The embeddings dictionary.""""""\n        return self._embeddings\n    @embeddings .setter\n    def embeddings (self, value):\n        self._embeddings  = value\n    @property\n180 | Chapter 10: Low-Rank Methods\n    def metrics(self):\n        """"""The metrics dictionary.""""""\n        return self._metrics\n    @property\n    def ground_truth (self):\n        """"""The train/test dictionary.""""""\n        return self._ground_truth\n    def reset_embeddings (self):\n        """"""Clear out embeddings state.""""""\n        prng_key1 , prng_key2  = random.split(self._prng_key , 2)\n        self._embeddings ['users'] = random_normal (\n            prng_key1 ,\n            [\n              self._embeddings_parameters ['user_dim' ],\n              self._embeddings_parameters ['embedding_dim' ]\n            ],\n            mu=0,\n            sigma=self._embeddings_parameters ['init_stddev' ],\n        )\n        self._embeddings ['items'] = random_normal (\n            prng_key2 ,\n            [\n              self._embeddings_parameters ['item_dim' ],\n              self._embeddings_parameters ['embedding_dim' ]],\n            mu=0,\n            sigma=self._embeddings_parameters ['init_stddev' ],\n        )\ndef model_constructor (\n    ratings_df ,\n    user_dim ,\n    item_dim ,\n    embedding_dim =3,\n    init_stddev =1.,\n    holdout_fraction =0.2,\n    prng_key =None,\n    train_set =None,\n    test_set =None,\n):\n    if prng_key  is None:\n      prng_key  = random.PRNGKey(0)\n    prng_key1 , prng_key2  = random.split(prng_key , 2)\n    if (train_set  is None) and (test_set  is None):\n        train, test = (ratings_df\n            .pipe(start_pipeline )\n            .pipe(split_dataframe , holdout_fraction =holdout_fraction )\nReducing the Rank of a Recommender Problem | 181\n        )\n        A_train = (train\n            .pipe(start_pipeline )\n            .pipe(ratings_to_sparse_array , user_dim =user_dim , item_dim =item_dim )\n        )\n        A_test = (test\n            .pipe(start_pipeline )\n            .pipe(ratings_to_sparse_array , user_dim =user_dim , item_dim =item_dim )\n        )\n    elif (train_set  is None) ^ (test_set  is None):\n        raise('Must send train and test if sending one' )\n    else:\n        A_train, A_test = train_set , test_set\n    U = random_normal (\n        prng_key1 ,\n        [user_dim , embedding_dim ],\n        mu=0,\n        sigma=init_stddev ,\n    )\n    V = random_normal (\n        prng_key2 ,\n        [item_dim , embedding_dim ],\n        mu=0,\n        sigma=init_stddev ,\n    )\n    train_loss  = omse_loss (A_train, {'users': U, 'items': V})\n    test_loss  = omse_loss (A_test, {'users': U, 'items': V})\n    metrics = {\n        'train_error' : train_loss ,\n        'test_error' : test_loss\n    }\n    embeddings  = {'users': U, 'items': V}\n    ground_truth  = {\n        ""A_train"" : A_train,\n        ""A_test"" : A_test\n    }\n    return CFModel(\n        metrics=metrics,\n        embeddings =embeddings ,\n        ground_truth =ground_truth ,\n        embeddings_parameters ={\n            'user_dim' : user_dim ,\n            'item_dim' : item_dim ,\n            'embedding_dim' : embedding_dim ,\n            'init_stddev' : init_stddev ,\n        },\n        prng_key =prng_key ,\n182 | Chapter 10: Low-Rank Methods\n    )\nmf_model  = model_constructor (all_rat, user_count , item_count )\nWe should also set this up to log nicely to wandb so it’s easy to understand what is\nhappening during training:\ndef train():\n  run_config  = { # These will be hyperparameters we will tune via wandb\n      'emb_dim' : 10, # Latent dimension\n      'prior_std' : 0.1, # Std dev around 0 for weights initialization\n      'alpha': 1.0, # Learning rate\n      'steps': 1500, # Number of training steps\n  }\n  with wandb.init() as run:\n    run_config .update(run.config)\n    model_object  = model_constructor (\n        ratings_df =all_rat,\n        user_dim =user_count ,\n        item_dim =item_count ,\n        embedding_dim =run_config ['emb_dim' ],\n        init_stddev =run_config ['prior_std' ],\n        prng_key =random.PRNGKey(0),\n        train_set =mf_model .ground_truth ['A_train' ],\n        test_set =mf_model .ground_truth ['A_test' ]\n    )\n    model_object .reset_embeddings () # Ensure we are starting from priors\n    alpha, steps = run_config ['alpha'], run_config ['steps']\n    print(run_config )\n    grad_fn = jax.value_and_grad (omse_loss , 1)\n    for i in range(steps):\n      # We perform one gradient update\n      loss_val , grads = grad_fn(\n          model_object .ground_truth ['A_train' ],\n          model_object .embeddings\n      )\n      model_object .embeddings  = jax.tree_multimap (\n          lambda p, g: p - alpha * g,\n          # Basic update rule; JAX handles broadcasting for us\n          model_object .embeddings ,\n          grads\n      )\n      if i % 1000 == 0: # Most output in wandb; little bit of logging\n        print(f'Loss step {i}: ', loss_val )\n        print(f""""""Test loss: {\n            omse_loss (\n                model_object .ground_truth ['A_train' ],\n                model_object .embeddings\n            )}"""""")\n      wandb.log({\n          ""Train omse"" : loss_val ,\nReducing the Rank of a Recommender Problem | 183\n          ""Test omse"" : omse_loss (\n              model_object .ground_truth ['A_test' ],\n              model_object .embeddings\n           )\n      })\nNote that this code is using tree_multimap  to handle broadcasting our update rule,\nand we’re using the jitted loss from before in the omse_loss  call. Also, we’re calling\nvalue_and_grad  so we can log the loss to wandb as we go. This is a common trick\nyou’ll see for efficiently doing both without a callback.\nY ou can finish this off and start the training with a sweep:\nsweep_config  = {\n    ""name"" : ""mf-test-sweep"" ,\n    ""method""  : ""random"" ,\n    ""parameters""  : {\n        ""steps"" : {\n            ""min"": 1000,\n            ""max"": 3000,\n        },\n        ""alpha"" :{\n            ""min"": 0.6,\n            ""max"": 1.75\n        },\n        ""emb_dim""  :{\n            ""min"": 3,\n            ""max"": 10\n        },\n        ""prior_std""  :{\n            ""min"": .5,\n            ""max"": 2.0\n        },\n    },\n    ""metric""  : {\n        'name': 'Test omse' ,\n        'goal': 'minimize'\n    }\n}\nsweep_id  = wandb.sweep(sweep_config , project=""jax-mf"" , entity=""wandb-un"" )\nwandb.init()\ntrain()\ncount = 50\nwandb.agent(sweep_id , function =train, count=count)\n184 | Chapter 10: Low-Rank Methods\nIn this case, the hyperparameter optimization (HPO) is over our hyperparameters\nlike embedding dimension and the priors (randomized matrices). Up until now, we\nhave trained some MF models on our ratings matrix. Let’s now add regularization\nand cross-validation.\nLet’s translate the preceding math equations directly into code:\ndef ell_two_regularization_term (params, dimensions ):\n    U, V = params['users'], params['items']\n    N, M = dimensions ['users'], dimensions ['items']\n    user_sq = jnp.multiply (U, U)\n    item_sq = jnp.multiply (V, V)\n    return (jnp.sum(user_sq)/N + jnp.sum(item_sq)/M)\nl2_loss = jit(ell_two_regularization_term )\ndef gramian_regularization_term (params, dimensions ):\n    U, V = params['users'], params['items']\n    N, M = dimensions ['users'], dimensions ['items']\n    gr_user = U.T @ U\n    gr_item = V.T @ V\n    gr_square  = jnp.multiply (gr_user, gr_item)\n    return (jnp.sum(gr_square )/(N*M))\ngr_loss = jit(gramian_regularization_term )\ndef regularized_omse (A, params, dimensions , hyperparams ):\n  lr, lg = hyperparams ['ell_2'], hyperparams ['gram']\n  losses = {\n      'omse': sp_mse_loss (A, params),\n      'l2_loss' : l2_loss(params, dimensions ),\n      'gr_loss' : gr_loss(params, dimensions ),\n  }\n  losses.update({\n      'total_loss' : losses['omse'] + lr*losses['l2_loss' ] + lg*losses['gr_loss' ]\n  })\n  return losses['total_loss' ], losses\nreg_loss_observed  = jit(regularized_omse )\nWe won’t dive super deep into learning rate schedulers, but we will do a simple decay:\ndef lr_decay (\n    step_num ,\n    base_learning_rate ,\n    decay_pct  = 0.5,\n    period_length  = 100.0\n):\n    return base_learning_rate  * math.pow(\n        decay_pct ,\n        math.floor((1+step_num )/period_length )\n    )\nReducing the Rank of a Recommender Problem | 185\nOur updated train function will incorporate our new regularizations—which come\nwith some hyperparameters—and a bit of additional logging setup. This code makes\nit easy to log our experiment as it trains and configures the hyperparameters to work\nwith regularization:\ndef train_with_reg_loss ():\n    run_config  = { # These will be hyperparameters we will tune via wandb\n        'emb_dim' : None,\n        'prior_std' : None,\n        'alpha': None, # Learning rate\n        'steps': None,\n        'ell_2': 1, #l2 regularization penalization weight\n        'gram': 1, #gramian regularization penalization weight\n        'decay_pct' : 0.5,\n        'period_length' : 100.0\n    }\n    with wandb.init() as run:\n        run_config .update(run.config)\n        model_object  = model_constructor (\n            ratings_df =all_rat,\n            user_dim =942,\n            item_dim =1681,\n            embedding_dim =run_config ['emb_dim' ],\n            init_stddev =run_config ['prior_std' ],\n            prng_key =random.PRNGKey(0),\n            train_set =mf_model .ground_truth ['A_train' ],\n            test_set =mf_model .ground_truth ['A_test' ]\n        )\n        model_object .reset_embeddings () # Ensure we start from priors\n        alpha, steps = run_config ['alpha'], run_config ['steps']\n        print(run_config )\n        grad_fn = jax.value_and_grad (\n            reg_loss_observed ,\n            1,\n            has_aux=True\n        ) # Tell JAX to expect an aux dict as output\n        for i in range(steps):\n            (total_loss_val , loss_dict ), grads = grad_fn(\n                model_object .ground_truth ['A_train' ],\n                model_object .embeddings ,\n                dimensions ={'users': user_count , 'items': item_count },\n                hyperparams ={\n                    'ell_2': run_config ['ell_2'],\n                    'gram': run_config ['gram']\n                } # JAX carries our loss dict along for logging\n            )\n186 | Chapter 10: Low-Rank Methods\n            model_object .embeddings  = jax.tree_multimap (\n                lambda p, g: p - lr_decay (\n                    i,\n                    alpha,\n                    run_config ['decay_pct' ],\n                    run_config ['period_length' ]\n                ) * g, # update with decay\n                model_object .embeddings ,\n                grads\n            )\n            if i % 1000 == 0:\n                print(f'Loss step {i}:')\n                print(loss_dict )\n                print(f""""""Test loss: {\n                    omse_loss (model_object .ground_truth ['A_test' ],\n                    model_object .embeddings )}"""""")\n            loss_dict .update( # wandb takes the entire loss dictionary\n                {\n                    ""Test omse"" : omse_loss (\n                        model_object .ground_truth ['A_test' ],\n                        model_object .embeddings\n                    ),\n                    ""learning_rate"" : lr_decay (i, alpha),\n                }\n            )\n            wandb.log(loss_dict )\n sweep_config  = {\n    ""name"" : ""mf-HPO-with-reg"" ,\n    ""method""  : ""random"" ,\n    ""parameters""  : {\n      ""steps"": {\n        ""value"": 2000\n      },\n      ""alpha"" :{\n        ""min"": 0.6,\n        ""max"": 2.25\n      },\n      ""emb_dim""  :{\n        ""min"": 15,\n        ""max"": 80\n      },\n      ""prior_std""  :{\n        ""min"": .5,\n        ""max"": 2.0\n      },\n      ""ell_2"" :{\n        ""min"": .05,\n        ""max"": 0.5\n      },\n      ""gram"" :{\nReducing the Rank of a Recommender Problem | 187\n        ""min"": .1,\n        ""max"": .75\n      },\n      ""decay_pct""  :{\n        ""min"": .2,\n        ""max"": .8\n      },\n      ""period_length""  :{\n        ""min"": 50,\n        ""max"": 500\n      }\n    },\n    ""metric""  : {\n      'name': 'Test omse' ,\n      'goal': 'minimize'\n    }\n  }\n  sweep_id  = wandb.sweep(\n      sweep_config ,\n      project=""jax-mf"" ,\n      entity=""wandb-un""\n  )\nrun_config  = { # These will be hyperparameters we will tune via wandb\n      'emb_dim' : 10, # Latent dimension\n      'prior_std' : 0.1,\n      'alpha': 1.0, # Learning rate\n      'steps': 1000, # Number of training steps\n      'ell_2': 1, #l2 regularization penalization weight\n      'gram': 1, #gramian regularization penalization weight\n      'decay_pct' : 0.5,\n      'period_length' : 100.0\n  }\ntrain_with_reg_loss ()\nThe last step is to do this in a way that gives us confidence in the models we’re seeing.\nUnfortunately, setting up cross-validation for MF problems can be tricky, so we’ll\nneed to make a few modifications to our data structures:\ndef sparse_array_concatenate (sparse_array_iterable ):\n    return {\n        'indices' : tuple(\n            map(\n                jnp.concatenate ,\n                zip(*(x['indices' ] for x in sparse_array_iterable )))\n            ),\n        'values' : jnp.concatenate (\n            [x['values' ] for x in sparse_array_iterable ]\n        ),\n    }\n188 | Chapter 10: Low-Rank Methods\nclass jax_df_Kfold (object):\n    """"""Simple class that handles Kfold\n    splitting of a matrix as a dataframe and stores as sparse jarrays""""""\n    def __init__ (\n        self,\n        df: pd.DataFrame ,\n        user_dim : int,\n        item_dim : int,\n        k: int = 5,\n        prng_key =random.PRNGKey(0)\n    ):\n        self._df = df\n        self._num_folds  = k\n        self._split_idxes  = jnp.array_split (\n            random.permutation (\n                prng_key ,\n                df.index.to_numpy (),\n                axis=0,\n                independent =True\n            ),\n            self._num_folds\n        )\n        self._fold_arrays  = dict()\n        for fold_index  in range(self._num_folds ):\n        # let's create sparse jax arrays for each fold piece\n            self._fold_arrays [fold_index ] = (\n                self._df[\n                    self._df.index.isin(self._split_idxes [fold_index ])\n                ].pipe(start_pipeline )\n                .pipe(\n                    ratings_to_sparse_array ,\n                    user_dim =user_dim ,\n                    item_dim =item_dim\n                )\n            )\n    def get_fold (self, fold_index : int):\n        assert(self._num_folds  > fold_index )\n        test = self._fold_arrays [fold_index ]\n        train = sparse_array_concatenate (\n            [v for k,v in self._fold_arrays .items() if k != fold_index ]\n        )\n        return train, test\nEach hyperparameter setup should yield loss for each fold, so within wandb.init , we\nbuild a model with each fold:\nfor j in num_folds :\n  train, test = folder.get_fold (j)\n  model_object_dict [j] = model_constructor (\nReducing the Rank of a Recommender Problem | 189\n          ratings_df =all_rat,\n          user_dim =user_count ,\n          item_dim =item_count ,\n          embedding_dim =run_config ['emb_dim' ],\n          init_stddev =run_config ['prior_std' ],\n          prng_key =random.PRNGKey(0),\n          train_set =train,\n          test_set =test\n      )\nAt each step, we’ d like to not only compute the gradient for the training and evaluate\non the test but also compute gradients for all folds, evaluate on all the tests, and\nproduce the relevant errors:\nfor i in range(steps):\n    loss_dict  = {""learning_rate"" : step_decay (i)}\n    for j, M in model_object_dict .items():\n        (total_loss_val , fold_loss_dict ), grads = grad_fn(\n          M.ground_truth ['A_train' ],\n          M.embeddings ,\n          dimensions ={'users': 942, 'items': 1681},\n          hyperparams ={'ell_2': run_config ['ell_2'], 'gram': run_config ['gram']}\n        )\n        M.embeddings  = jax.tree_multimap (\n            lambda p, g: p - step_decay (i) * g,\n            M.embeddings ,\n            grads\n        )\nLogging should be losses per fold, and the aggregate loss should be the target metric.\nThis is because each fold is an independent optimization of the model parameters;\nhowever, we wish to see aggregate behavior across the folds:\n        fold_loss_dict  = {f'{k}_fold-{j}': v for k, v in fold_loss_dict .items()}\n        fold_loss_dict .update(\n                  {\n                      f ""Test omse_fold- {j}"": omse_loss (\n                        M.ground_truth ['A_test' ],\n                        M.embeddings\n                      ),\n                  }\n              )\n        loss_dict .update(fold_loss_dict )\n    loss_dict .update({\n      ""Test omse_mean"" : jnp.mean(\n        [v for k,v in loss_dict .items() if k.startswith ('Test omse_fold-' )]\n      )\n    })\n    wandb.log(loss_dict )\n190 | Chapter 10: Low-Rank Methods\nWe wrap up into one big training method:\ndef train_with_reg_loss_CV ():\n    run_config  = { # These will be hyperparameters we will tune via wandb\n        'emb_dim' : None, # Latent dimension\n        'prior_std' : None,\n        # Standard deviation around 0 that our weights are initialized to\n        'alpha': None, # Learning rate\n        'steps': None, # Number of training steps\n        'num_folds' : None, # Number of CV Folds\n        'ell_2': 1, #hyperparameter for l2 regularization penalization weight\n        'gram': 1, #hyperparameter for gramian regularization penalization weight\n    }\n    with wandb.init() as run:\n        run_config .update(run.config) # This is how the wandb agent passes params\n        model_object_dict  = dict()\n        for j in range(run_config ['num_folds' ]):\n            train, test = folder.get_fold (j)\n            model_object_dict [j] = model_constructor (\n                ratings_df =all_rat,\n                user_dim =942,\n                item_dim =1681,\n                embedding_dim =run_config ['emb_dim' ],\n                init_stddev =run_config ['prior_std' ],\n                prng_key =random.PRNGKey(0),\n                train_set =train,\n                test_set =test\n            )\n            model_object_dict [j].reset_embeddings ()\n            # Ensure we are starting from priors\n        alpha, steps = run_config ['alpha'], run_config ['steps']\n        print(run_config )\n        grad_fn = jax.value_and_grad (reg_loss_observed , 1, has_aux=True)\n        # Tell JAX to expect an aux dict as output\n        for i in range(steps):\n            loss_dict  = {\n              ""learning_rate"" : lr_decay (\n                i,\n                alpha,\n                decay_pct =.75,\n                period_length =250\n              )\n            }\n            for j, M in model_object_dict .items():\n            # Iterate through folds\n                (total_loss_val , fold_loss_dict ), grads = grad_fn(\n                # compute gradients for one fold\nReducing the Rank of a Recommender Problem | 191\n                    M.ground_truth ['A_train' ],\n                    M.embeddings ,\n                    dimensions ={'users': 942, 'items': 1681},\n                    hyperparams ={\n                      'ell_2': run_config ['ell_2'],\n                      'gram': run_config ['gram']\n                    }\n                )\n                M.embeddings  = jax.tree_multimap (\n                # update weights for one fold\n                    lambda p, g: p - lr_decay (\n                      i,\n                      alpha,\n                      decay_pct =.75,\n                      period_length =250\n                    ) * g,\n                    M.embeddings ,\n                    grads\n                )\n                fold_loss_dict  = {\n                  f '{k}_fold-{j}':\n                  v for k, v in fold_loss_dict .items()\n                }\n                fold_loss_dict .update( # loss calculation within fold\n                    {\n                        f ""Test omse_fold- {j}"": omse_loss (\n                          M.ground_truth ['A_test' ],\n                          M.embeddings\n                        ),\n                    }\n                )\n                loss_dict .update(fold_loss_dict )\n            loss_dict .update({ # average loss over all folds\n                ""Test omse_mean"" : np.mean(\n                    [v for k,v in loss_dict .items()\n                    if k.startswith ('Test omse_fold-' )]\n                ),\n                ""test omse_max"" : np.max(\n                    [v for k,v in loss_dict .items()\n                    if k.startswith ('Test omse_fold-' )]\n                ),\n                ""test omse_min"" : np.min(\n                    [v for k,v in loss_dict .items()\n                    if k.startswith ('Test omse_fold-' )]\n                )\n            })\n            wandb.log(loss_dict )\n192 | Chapter 10: Low-Rank Methods\n            if i % 1000 == 0:\n                print(f'Loss step {i}:')\n                print(loss_dict )\nHere’s our final sweeps configuration:\nsweep_config  = {\n    ""name"" : ""mf-HPO-CV"" ,\n    ""method""  : ""random"" ,\n    ""parameters""  : {\n      ""steps"": {\n        ""value"": 2000\n      },\n      ""num_folds"" : {\n        ""value"": 5\n      },\n      ""alpha"" :{\n        ""min"": 2.0,\n        ""max"": 3.0\n      },\n      ""emb_dim""  :{\n        ""min"": 15,\n        ""max"": 70\n      },\n      ""prior_std""  :{\n        ""min"": .75,\n        ""max"": 1.0\n      },\n      ""ell_2"" :{\n        ""min"": .05,\n        ""max"": 0.5\n      },\n      ""gram"" :{\n        ""min"": .1,\n        ""max"": .6\n      },\n    },\n    ""metric""  : {\n      'name': 'Test omse_mean' ,\n      'goal': 'minimize'\n    }\n  }\n  sweep_id  = wandb.sweep(sweep_config , project=""jax-mf"" , entity=""wandb-un"" )\nwandb.agent(sweep_id , function =train_with_reg_loss_CV , count=count)\nReducing the Rank of a Recommender Problem | 193\nThat may seem like a lot of setup, but we’ve really achieved a lot here. We’ve initial‐\nized the model to optimize the two matrix factors while simultaneously keeping the\nmatrix elements and the Gramians small.\nThis brings us to our lovely images.\nOutput from HPO MF\nLet’s  have a quick look at what the prior work has produced. First, Figure 10-3\nshows that our primary loss function,  observed mean square error (OMSE), is rapidly\ndecreasing. This is great, but we should take a deeper look.\nFigure 10-3. The loss during training\nLet’s also have a quick look to ensure that our regularization parameters ( Figure 10-4 )\nare converging. We can see that our L2 regularization could probably still decrease if\nwe were to continue for more epochs.\n194 | Chapter 10: Low-Rank Methods\nFigure 10-4. Regularization parameters\nWe’ d like to see our cross-validation laid out by fold and corresponding loss ( Fig‐\nure 10-5 ). This is a parallel coordinates chart ; its lines correspond to different runs\nthat are in correspondence with different choices of parameters, and its vertical axes\nare different metrics. The far-right heatmap axis corresponds to the overall total loss\nthat we’re trying to minimize. In this case, we alternate test loss on a fold and total\nloss on that fold. Lower numbers are better, and we hope to see individual lines\nconsistent across their loss per fold (otherwise, we may have a skewed dataset). We\nsee that choices of hyperparameters can interact with fold behavior, but in all the\nlow-loss scenarios (at the bottom), we see a high correlation between performance on\ndifferent folds (the vertical axes in the plot).\nReducing the Rank of a Recommender Problem | 195\nFigure 10-5. The loss during training\nNext up, which choices of hyperparameters have a strong effect on performance?\nFigure 10-6  is another parallel coordinates plot with the vertical axes corresponding\nto different hyperparameters. Generally, we’re looking for which domains on the\nvertical axes correspond to low loss on the far-right heatmap. We see that some of\nour hyperparameters like priors distribution and, somewhat surprisingly, ell_2  have\nvirtually no effect. However, small embedding dimension and small Gramian weight\ndefinitely do. A larger alpha also seems to correlate well with good performance.\nFigure 10-6. The loss by hyperparameter\n196 | Chapter 10: Low-Rank Methods\nFinally, we see that as we do a Bayesian hyperparameter search, we really do improve\nour performance over time. Figure 10-7  is a Pareto plot in which each dot in the\nscatterplot represents one run, and left to right is a time axis. The vertical axis is\noverall total loss, so lower is better, and it means that generally we’re converging\ntoward better performance. The  line inscribed along the bottom of the convex hull of\nthe scatter points is the Pareto frontier , or the best performance at that x value. Since\nthis is a time-series Pareto plot, it merely tracks the best performance in time.\nY ou may be wondering how and why we’re able to converge to better loss values\nin time. This is because we’ve conducted a Bayesian hyperparameter search, which\nmeans we selected our hyperparameters from independent Gaussians, and we upda‐\nted our priors for each parameter based on performance of previous runs. For\nan introduction to this method, see “Bayesian Hyperparameter Optimization—A\nPrimer”  by Robert Mitson. In a real setting, we’ d see less monotonic behavior in this\nplot, but we’ d always be hoping to improve.\nFigure 10-7. The Pareto frontier of the loss values\nPrequential validation\nIf we were to put the preceding approach into practice, we would need to capture our\ntrained models in a model registry for use in production. Best practice is to establish\na set of explicit evaluations against which to test a selection of models. In your basic\nML training, you’ve likely been encouraged to think about validation datasets; these\nmay take many forms, testing particular subsets of instances or features or even\ndistributed across covariates in a known way.\nOne useful framing for recommendation systems is to remember that they’re a\nfundamentally sequential dataset. With this in mind, let’s take another look at our\nratings data. Later we will talk more about sequential recommenders, but while we’re\ntalking about validation, it’s useful to mention how to take proper care.\nReducing the Rank of a Recommender Problem | 197\nNotice that all our ratings have an associated timestamp. To build a proper validation\nset, it’s a good idea to take that timestamp from the end of our data.\nHowever, you might be wondering, “When are different users active?” and “Is it\npossible that the later timestamps are a biased selection of the ratings?” These are\nimportant questions. To account for these questions, we should do a holdout by user.\nTo create this prequential dataset , where the test set follows directly after the training\nset in a chronological sequence, start by deciding on a desired size for validation, like\n10%. Next, group the data by user. Finally, employ rejection sampling, ensuring you\ndon’t use the most recent timestamp as the rejection criterion.\nHere’s a simple implementation for pandas using rejection sampling. This is not the\nmost computationally efficient implementation, but it will get the job done:\ndef prequential_validation_set (df, holdout_perc =0.1):\n    '''\n    We utilize rejection sampling.\n    Assign a probability to all observations, if they lie below the\n    sample percentage AND they're the most recent still in the set, include.\n    Otherwise return them and repeat.\n    Each time, take no more than the remaining necessary to fill the count.\n    '''\n    count = int(len(df)*holdout_perc )\n    sample = []\n    while count >0:\n      df['p'] = np.random.rand(len(df),1) #generate probabilities\n      x = list(\n          df.loc[~df.index.isin(sample)] # exclude already selected\n          .sort_values (['unix_timestamp' ], ascending =False)\n          .groupby('user_id' ).head(1) # only allow the first in each group\n          .query(""p < @holdout_perc"" ).index # grab the indices\n      )\n      rnd.shuffle(x) # ensure our previous sorting doesn't bias the users subset\n      sample += x[:count] # add observations up to the remaining needed\n      count -= len(x[:count]) # decrement the remaining needed\n    df.drop(columns=['p'], inplace=True)\n    test = df.iloc[sample]\n    train = df[~df.index.isin(test.index)]\n    return train, test\nThis is an effective and important validation scheme for inherently sequential\ndatasets.\n198 | Chapter 10: Low-Rank Methods",36976
82-WSABIE.pdf,82-WSABIE,,0
83-Dimension Reduction.pdf,83-Dimension Reduction,"WSABIE\nLet’s focus again on optimizations and modifications. Another optimization is to treat\nthe MF problem as a single optimization.\nThe paper “WSABIE: Scaling Up to Large Vocabulary Image Annotation”  by Jason\nWeston et al. also contains a factorization for just the item matrix. In this scheme,\nwe replace the user matrix with a weighted sum of the items a user has affinity to.\nWe cover web scale annotation by image embedding (WSABIE) and Warp loss in\n“W ARP”  on page 233. Representing a user as the average of items they like is a way\nof saving space and not needing a separate user matrix if there are large numbers of\nusers.\nLatent Space HPO\nA completely alternative way to do HPO for RecSys is via the latent\nspaces themselves! “Hyper-Parameter Optimization for Latent\nSpaces in Dynamic Recommender Systems”  by Bruno Veloso et\nal. attempts to modify the relative embeddings during each step to\noptimize the embedding model.\nDimension Reduction\nDimension-reduction techniques are frequently employed in recommendation sys‐\ntems to decrease computational complexity and enhance the accuracy of recommen‐\ndation algorithms. In this context, the primary concepts of dimension reduction for\nrecommendation systems include MF and SVD.\nThe matrix factorization method  decomposes the user-item interaction matrix\nA ∈ℝm×n into two lower-dimensional matrices, representing the user\nU ∈ℝm×r and item V ∈ℝn×r latent factors, respectively. This technique\ncan reveal the underlying data structure and offer recommendations based on a user’s\nprevious interactions. Mathematically, MF can be represented as follows:\nA ∼ U ×VT\nSVD  is a linear-algebra technique that decomposes a matrix ( A) into three matrices—\nthe left singular vectors ( U), the singular values ( Σ), and the right singular vectors\n(V). SVD can be utilized for MF in recommendation systems, where the user-item\ninteraction matrix is decomposed into a smaller number of latent factors. The mathe‐\nmatical representation of SVD is as follows:\nA=U×Σ×VT\nDimension Reduction | 199\nIn practice, though, rather than using a mathematical library to find the eigenvectors,\nfolks might use the power iteration method  to discover the eigenvectors approxi‐\nmately. This method is far more scalable than a full dense SVD solution that is\noptimized for correctness and dense vectors:\nimport jax\nimport jax.numpy  as jnp\ndef power_iteration (a: jnp.ndarray) -> jnp.ndarray:\n  """"""Returns an eigenvector of the matrix a.\n  Args:\n    a: a n x m matrix\n  """"""\n  key = jax.random.PRNGKey(0)\n  x = jax.random.normal(key, shape=(a.shape[1], 1))\n  for i in range(100):\n    x = a @ x\n    x = x / jnp.linalg.norm(x)\n  return x.T\nkey = jax.random.PRNGKey(123)\nA = jax.random.normal(key, shape=[4, 4])\nprint(A)\n[[ 0.52830553   0.3722206   -1.2219944   -0.10314374 ]\n [ 1.4722222    0.47889313  -1.2940298    1.0449569  ]\n [ 0.23724185   0.3545859   -0.172465    -1.8011322  ]\n [ 0.4864215    0.08039388  -1.2540827    0.72071517 ]]\nS, _, _ = jnp.linalg.svd(A)\nprint(S)\n[[-0.375782     0.40269807   0.44086716  -0.70870167 ]\n [-0.753597     0.0482972   -0.65527284   0.01940039 ]\n [ 0.2040088    0.91405433  -0.15798494   0.31293103 ]\n [-0.49925917  -0.00250015   0.5927009    0.6320123  ]]\nx1 = power_iteration (A)\nprint(x1.T)\n[[-0.35423845 ]\n [-0.8332922  ]\n [ 0.16189891 ]\n [-0.39233655 ]]\nNotice that the eigenvector returned by the power iteration is close to the first\ncolumn of S, but not quite. This is because the method is approximate. It relies on the\nfact that an eigenvector doesn’t change in direction when multiplied by the matrix.\nSo by repeatedly multiplying by the matrix, we eventually iterate onto an eigenvector.\nAlso notice that we solved for column eigenvectors instead of the row eigenvectors.\nIn this example, the columns are users, and the rows are items. It is important to\nplay with transposed matrices because a lot of ML involves reshaping and transposing\nmatrices, so getting used to them early is an important skill.\n200 | Chapter 10: Low-Rank Methods\nEigenvector Examples\nHere’s  a nice exercise for you: the second eigenvector is computed\nby subtracting out the first eigenvector after the matrix multiplica‐\ntion. This is telling the algorithm to ignore any component along\nthe first eigenvector in order to compute the second eigenvector.\nAs a fun exercise, hop over to Colab  and try computing the sec‐\nond eigenvector. Extending this to sparse vector representations is\nanother interesting exercise, as it allows you to start computing the\neigenvectors of sparse matrices, which is usually the form of matrix\nthat recommender systems use.\nNext, we construct a recommendation for a user by creating a column and then\ntaking the dot product with all the eigenvectors and finding the closest. We then find\nall the highest-scoring entries in the eigenvector that the user hasn’t seen and return\nthem as recommendations. So in the preceding example, if the eigenvector x1 was the\nclosest to the user column, then the best item to recommend would be item 3 because\nit is the largest component in the eigenvector and thus rated most highly if the user is\nclosest to the eigenvector x1. Here’s what this looks like in code:\nimport jax\nimport jax.numpy  as jnp\ndef recommend_items (eigenvectors : jnp.ndarray, user:jnp.ndarray) -> jnp.ndarray:\n  """"""Returns an ordered list of recommend items for the user.\n  Args:\n    eigenvectors: a nxm eigenvector matrix\n    user: a user vector of size m.\n  """"""\n  score_eigenvectors  = jnp.matmul(eigenvectors .T, user)\n  which_eigenvector  = jnp.argmax(score_eigenvectors )\n  closest_eigenvector  = eigenvectors .T[which_eigenvector ]\n  scores, items = jax.lax.top_k(closest_eigenvector , 3)\n  return scores, items\nS = jnp.array(\n[[-0.375782 ,    0.40269807 ],\n [-0.753597 ,    0.0482972 ],\n [ 0.2040088 ,   0.91405433 ],\n [-0.49925917 , -0.00250015 ]])\nu = jnp.array([-1, -1, 0, 0]).reshape(4, 1)\nscores, items = recommend_items (S, u)\nprint(scores)\n[ 0.2040088   -0.375782    -0.49925917 ]\nprint(items)\n[2 0 3]\nDimension Reduction | 201\nIn this example, a user has downvoted item 0 and item 1. The closest column\neigenvector is therefore column 0. We then select the closest eigenvector to the user,\norder the entries, and recommend item 2 to the user, which is the highest-scoring\nentry that the user has not seen.\nTwo techniques aim to extract the most relevant features from the user-item interac‐\ntion matrix and reduce its dimensionality, which can improve performance:\nPrincipal component analysis (PCA)\nThis  statistical technique transforms the original high-dimensional data into a\nlower-dimensional representation while retaining the most important informa‐\ntion. PCA can be applied to the user-item interaction matrix to reduce the\nnumber of dimensions and improve the computational efficiency of the recom‐\nmendation algorithm.\nNonnegative matrix factorization (NMF)\nThis  technique decomposes the nonnegative user-item interaction matrix\nA ∈ℝm×n* + into two nonnegative matrices W ∈ℝm×r* + and\nH ∈ℝr×n+. NMF can be utilized for dimension reduction in recommenda‐\ntion systems, where the latent factors are nonnegative and interpretable. The\nmathematical representation of NMF is A ≃ W ×H.\nMF techniques can be further extended to incorporate additional information, such\nas item content or user demographic data, through the use of side information. Side\ninformation can be employed to augment the user-item interaction matrix, allowing\nfor more accurate and personalized recommendations.\nFurthermore, MF models can be extended to handle implicit feedback data, where the\nabsence of interaction data is not equivalent to the lack of interest. By incorporating\nadditional regularization terms into the objective function, MF models can learn\na more robust representation of the user-item interaction matrix, leading to better\nrecommendations for implicit feedback scenarios.\nConsider a recommendation system that employs MF to model the user-item inter‐\naction matrix. If the system comprises many users and items, the resulting factor\nmatrices can be high-dimensional and computationally expensive to process. How‐\never, by using dimension-reduction techniques like SVD or PCA, the algorithm can\nreduce the dimensionality of the factor matrices while preserving the most important\ninformation about the user-item interactions. This enables the algorithm to generate\nmore efficient and accurate recommendations, even for new users or items with\nlimited interaction data.\n202 | Chapter 10: Low-Rank Methods",8679
84-Nonlinear Locally Metrizable Embeddings.pdf,84-Nonlinear Locally Metrizable Embeddings,"Isometric Embeddings\nIsometric embeddings  are a specific type of embedding that maintains distances\nbetween points in high-dimensional space when mapping them onto a lower-\ndimensional space. The term isometric  signifies that the distances between points in\nthe high-dimensional space are preserved precisely in the lower-dimensional space,\nup to a scaling factor.\nIn contrast to other types of embeddings, such as linear or nonlinear embeddings,\nwhich may distort the distances between points, isometric embeddings are preferable\nin numerous applications where distance preservation is essential. For example, in\nML, isometric embeddings can be employed to visualize high-dimensional data in\ntwo or three dimensions while preserving the relative distances between the data\npoints. In NLP , isometric embeddings can be utilized to represent the semantic\nsimilarities between words or documents while maintaining their relative distances in\nthe embedding space.\nOne popular technique for generating isometric embeddings is  multidimensional\nscaling  (MDS ). MDS operates by computing pairwise distances between the data\npoints in the high-dimensional space and then determining a lower-dimensional\nembedding that preserves these distances. The optimization problem is generally\nformulated as a constrained optimization problem, where the objective is to minimize\nthe difference between the pairwise distances in the high-dimensional space and the\ncorresponding distances in the lower-dimensional embedding. Mathematically, we\nwrite: minX∑i,jdij−xi−xj2.\nHere, dij denotes the pairwise distances in the high-dimensional space, and xi and xj\nrepresent points in the lower-dimensional embedding.\nAnother  approach for generating isometric embeddings is through the use of kernel\nmethods, such as kernel PCA or kernel MDS. Kernel methods work by implicitly\nmapping the data points into a higher-dimensional feature space, where the distances\nbetween the points are easier to compute. The isometric embedding is then calculated\nin the feature space, and the resulting embedding is mapped back to the original\nspace.\nIsometric embeddings have been employed in recommendation systems to represent\nthe user-item interaction matrix in a lower-dimensional space where the distances\nbetween the items are preserved. By preserving the distances between items in the\nembedding space, the recommendation algorithm can better capture the underlying\nstructure of the data and provide more accurate and diverse recommendations.\nIsometric embeddings can also be employed to incorporate additional information\ninto the recommendation algorithm, such as item content or user demographic\ndata. By using isometric embeddings to represent the items and the additional\nDimension Reduction | 203\ninformation,  the algorithm can capture the similarities between items based on both\nthe user-item interaction data and the item content or user demographics, leading to\nmore accurate and diverse recommendations.\nMoreover, isometric embeddings can also be used to address the cold-start problem\nin recommendation systems. By using the isometric embeddings to represent the\nitems, the algorithm can make recommendations for new items based on their simi‐\nlarities to the existing items in the embedding space, even in the absence of user\ninteractions.\nIn summary, isometric embeddings are a valuable technique in recommendation\nsystems for representing the user-item interaction matrix in a lower-dimensional\nspace where the distances between the items are preserved. Isometric embeddings\ncan be generated using MF techniques and can be employed to incorporate additional\ninformation, address the cold-start problem, and improve the accuracy and diversity\nof recommendations.\nNonlinear Locally Metrizable Embeddings\nNonlinear locally metrizable embeddings  are yet another method to represent the\nuser-item interaction matrix in a lower-dimensional space where the local distances\nbetween nearby items are preserved. By preserving the local distances between items\nin the embedding space, the recommendation algorithm can better capture the local\nstructure of the data and provide more accurate and diverse recommendations.\nMathematically, let X=x1,x2, . . . ,xn be the set of items in the high-dimensional\nspace, and Y=y1,y2, . . . ,yn be the set of items in the lower-dimensional space. The\ngoal of nonlinear locally metrizable embeddings is to find a mapping f:XY that\npreserves the local distances, i.e., for any xi,xj∈ X , we have this:\ndYfxi,fxj≃ dXxi,xj\nOne popular approach to generating nonlinear locally metrizable embeddings\nin recommendation systems is via autoencoder neural networks. Autoencoders\nwork by mapping the high-dimensional user-item interaction matrix onto a lower-\ndimensional space through an encoder network, and then reconstructing the matrix\nback in the high-dimensional space through a decoder network. The encoder and\ndecoder networks are trained jointly to minimize the difference between the input\ndata and the reconstructed data, with the objective of capturing the underlying\nstructure of the data in the embedding space:\nminθ,φ∑\ni= 1n\nxi−gφfθxi2\n204 | Chapter 10: Low-Rank Methods\nHere, fθ denotes the encoder network with parameters θ,gθ denotes the decoder\nnetwork with parameters θ, and · represents the Euclidean norm.\nAnother approach for generating nonlinear locally metrizable embeddings in recom‐\nmendation systems is through the use of  t-distributed stochastic neighbor embedding\n(t-SNE). t-SNE works by modeling the pairwise similarities between the items in\nthe high-dimensional space, and then finding a lower-dimensional embedding that\npreserves these similarities.\nA more popular approach in modern times is UMAP , which instead attempts to\nfit a minimal manifold that preserves density in local neighborhoods. UMAP is\nan essential technique for finding low-dimensional representations in complex and\nhigh-dimensional latent spaces; find it’s documentation at https://oreil.ly/NLqDg .\nThe optimization problem is typically formulated as a cost function C that meas‐\nures the difference between the pairwise similarities in the high-dimensional space\nand the corresponding similarities in the lower-dimensional embedding:\nCY= ∑i,jpij*logpij\nqij\nHere, pij denotes the pairwise similarities in the high-dimensional space, qij denotes\nthe pairwise similarities in the lower-dimensional space, and the sum is over all pairs\nof items i,j.\nNonlinear locally metrizable embeddings can also be used to incorporate additional\ninformation into the recommendation algorithm, such as item content or user demo‐\ngraphic data. By using nonlinear locally metrizable embeddings to represent the items\nand the additional information, the algorithm can capture the similarities between\nitems based on both the user-item interaction data and the item content or user\ndemographics, leading to more accurate and diverse recommendations.\nMoreover, nonlinear locally metrizable embeddings can also be used to address the\ncold-start problem in recommendation systems. By using the nonlinear locally met‐\nrizable embeddings to represent the items, the algorithm can make recommendations\nfor new items based on their similarities to the existing items in the embedding space,\neven in the absence of user interactions.\nIn summary, nonlinear locally metrizable embeddings are a useful technique in rec‐\nommendation systems for representing the user-item interaction matrix in a lower-\ndimensional space where the local distances between nearby items are preserved.\nNonlinear locally metrizable embeddings can be generated using techniques such as\nautoencoder neural networks or t-SNE and can be used to incorporate additional\ninformation, address the cold-start problem, and improve the accuracy and diversity\nof recommendations.\nDimension Reduction | 205",7961
85-Centered Kernel Alignment.pdf,85-Centered Kernel Alignment,,0
86-Affinity and p-sale.pdf,86-Affinity and p-sale,"Centered Kernel Alignment\nWhen  training neural networks, the latent space representations at each layer are\nexpected to express correlation structures between the incoming signals. Frequently,\nthese interstitial representations comprise a sequence of states transitioning from\nthe initial layer to the final layer. Y ou may naturally wonder, “How do these represen‐\ntations change throughout the layers of the network” and “How similar are these\nlayers?” Interestingly, for some architectures, this question may yield deep insight\ninto the network’s behavior.\nThis process of comparing layer representations is called correlation analysis . For an\nMLP with layers 1, ...,N, the correlations may be represented by an N×N matrix of\npairwise relationships. The idea is that each layer comprises a series of latent factors,\nand similar to correlation analysis for other features of a dataset, these latent features’\nrelationships may be simply summarized by their covariance.\nAffinity  and p-sale\nAs you’ve seen, MF is a powerful dimension-reduction technique that can yield an\nestimator for the probability of a sale (often shorted to p-sale ). In MF, the goal\nhas been to decompose this historical data on user behavior and the product sales\nmatrix into two lower-dimensional matrices: one that represents user preferences and\nanother that represents product characteristics. Now, let’s convert this MF model into\na sale estimator.\nLet R ∈ℝM×N be the historical data matrix, where M is the number of users and\nN is the number of products. The MF aims to find two matrices U ∈ℝM×d and\nV ∈ℝN×d, where d is the dimensionality of the latent space, such that:\nR ≃ U *VT\nThe probability of a sale , or equivalently a read, watch, eat, or click, can be predicted\nusing MF by first decomposing the historical data matrix into user and product\nmatrices, and then calculating a score that represents the likelihood of a user pur‐\nchasing a given product. This score can be calculated using the dot product of the\ncorresponding row in the user matrix and the column in the product matrix, followed\nby a logistic function to transform the dot product into a probability score.\nMathematically, the probability of a sale for a user u and a product p can be repre‐\nsented as follows:\nPu,p= sigmoid u*pT\n206 | Chapter 10: Low-Rank Methods\nHere, sigmoid is the logistic function that maps the dot product of the user and\nproduct vectors to a probability score between 0 and 1:\nsigmoid x= 1/ 1 +exp −x\nThe pT represents the transpose of the product vector. The dot product of the user\nand product vectors is a measure of the similarity between the user’s preferences and\nthe product’s characteristics, and the logistic function maps this similarity score to a\nprobability score.\nThe user and product matrices can be trained on the historical data by using various\nMF algorithms, such as SVD, NMF, or ALS. Once the matrices are trained, the dot\nproduct and logistic function can be applied to new user-product pairs to predict\nthe probability of a sale. The predicted probabilities can then be used to rank and\nrecommend products to the user.\nIt’s worth highlighting that, since the loss function for ALS is convex (meaning there\nis a single global minimum), the convergence can be fast when we fix either the user\nor item matrix. In this method, the user matrix is fixed and the item matrix is solved\nfor. Then the item matrix is fixed and the user matrix is solved for. The method\nalternates between the two solutions, and because the loss is convex in this regime,\nthe method converges quickly.\nThe dot product of the corresponding row in the user matrix and column in the\nproduct matrix represents the affinity score between the user and the product, or how\nwell the user’s preferences match the product’s characteristics. However, this score\nalone may not be a sufficient predictor of whether the user will actually purchase the\nproduct.\nThe logistic function applied to the dot product in the MF model transforms the\naffinity score into a probability score, which represents the likelihood of a sale. This\ntransformation takes into account additional factors beyond just the user’s preferen‐\nces and the product’s characteristics, such as the overall popularity of the product, the\nuser’s purchasing behavior, and any other relevant external factors. By incorporating\nthese additional factors, MF is able to better predict the probability of a sale, rather\nthan just an affinity score.\nA comparison library (however, not in JAX) for computing latent embeddings line‐\narly is libFM . The formulation for a factorization machine is similar to a GloVe\nembedding in that it also models the interaction between two vectors, but the dot\nproduct can be used for regression or binary classification tasks. The method can also\nbe extended to recommend more than two kinds of items beyond user and item.\nIn summary, MF produces probabilities of sale instead of just affinity scores by\nincorporating additional factors beyond the user’s preferences and the product’s\nAffinity  and p-sale | 207",5139
87-Simpsons and Mitigating Confounding.pdf,87-Simpsons and Mitigating Confounding,"characteristics, and transforming the affinity score into a probability score by using a\nlogistic function.\nPropensity Weighting for Recommendation System\nEvaluation\nAs you’ve seen, recommendation systems are evaluated based on user feedback,\nwhich is collected from the deployed recommendation system. However, this data is\ncausally influenced by the deployed system, creating a feedback loop that may bias\nthe evaluation of new models. This feedback loop can lead to confounding variables,\nmaking it difficult to distinguish between user preferences and the influence of the\ndeployed system.\nIf this surprises you, let’s consider for a moment what would have to be true for\na recommendation system to not causally influence the actions users take and/or\nthe outcomes that result from those actions. That would require assumptions like\n“the recommendations are completely ignored by the user” and “the system makes\nrecommendations at random. ” Propensity weighting can mitigate some of the worst\neffects of this problem.\nThe performance of a recommender system depends on many factors, including\nuser-item characteristics, contextual information, and trends, which can affect the\nquality of the recommendations and the user engagement. However, the influence\ncan be mutual: the user interactions influence the recommender, and vice versa. Eval‐\nuating the causal effect of a recommender system on user behavior and satisfaction\nis therefore a challenging task, as it requires controlling for potential confounding\nfactors—those that may affect both the treatment assignment (the recommendation\nstrategy) and the outcome of interest (the user’s response to the recommendations).\nCausal inference provides a framework for addressing these challenges. In the context\nof recommender systems, causal inference can help answer questions such as these:\n•How does the choice of recommendation strategy affect user engagement, such•\nas CTRs, purchase rates, and satisfaction ratings?\n•What is the optimal recommendation strategy for a given user segment, item•\ncategory, or context?\n•What are the long-term effects of a recommendation strategy on user retention,•\nloyalty, and lifetime value?\n208 | Chapter 10: Low-Rank Methods\nWe’ll round out this chapter by introducing one aspect of causal inference important\nto recommender systems, based on the concept of propensity score. We’ll introduce\npropensity to quantify the adjusted likelihood of some items being shown to the user.\nWe’ll then see how this interacts with the famous Simpson’s paradox.\nPropensity\nIn many data science problems, we are forced to contend with confounders and,\nnotably, the correlation between those confounders and a target outcome. Depending\non the setting, the confounder may be of a variety of forms. Interestingly, in recom‐\nmendation systems, that confounder can be the system itself! Offline evaluation of\nrecommendation systems is subject to confounders derived from the item selection\nbehavior of users and the deployed recommendation system.\nIf this issue seems a bit circular, it kind of is. This is sometimes called closed-loop\nfeedback . One approach to mitigation is propensity weighting, which aims to address\nthis problem by considering each feedback in the corresponding stratum based on\nthe estimated propensities. Y ou may recall that propensity  refers to the likelihood\nof a user seeing an item; by inversely weighting by this, we can offset the selection\nbias. Compared to the standard offline holdout evaluation, this method attempts to\nrepresent the actual utility of the examined recommendation models.\nUtilizing Counterfactuals\nOne  other approach to mitigating selection bias that we won’t dive\ninto is counterfactual evaluation , which estimates the actual utility\nof a recommendation model with propensity-weighting techniques\nmore similar to off-policy evaluation approaches in reinforcement\nlearning (RL). However, counterfactual evaluation often relies on\naccurate logging propensities in an open-loop setting where some\nrandom items are exposed to the user, which is not practical\nfor most recommendation problems. If you have the option to\ninclude randomized recommendations to users for rating, this\ncan help de-bias as well. One such setting where these methods\nmay be combined is in RL-based recommenders that use explore-\nexploit methods like a multiarmed bandit or other structured\nrandomization.\nInverse propensity scoring  (IPS) is a propensity-based evaluation method that lever‐\nages importance sampling to account for the fact that the feedback collected from the\ndeployed recommendation system is not uniformly random. The propensity score is\na balancing factor that adjusts the observed feedback distribution conditioned on the\npropensity score. The IPS evaluation method is theoretically unbiased if open-loop\nfeedback can be sampled from all possible items uniformly at random. In Chapter 3 ,\nwe discussed the Matthew effect, or “the rich get richer” for recommendation sys‐\nPropensity Weighting for Recommendation System Evaluation | 209\ntems; IPS is one way to combat this effect. Note the relationship here between the\ntwo ideas of the Matthew effect and Simpson’s paradox, when within different strata,\nselection effects create significant biasing.\nPropensity weighting is based on the idea that the probability of an item being\nexposed to a user by the deployed recommendation system (the propensity score)\naffects the feedback that is collected from that user. By reweighting the feedback\nbased on the propensity scores, we can adjust for the bias introduced by the deployed\nsystem and obtain a more accurate evaluation of the new recommendation model.\nTo apply IPS, we need to estimate the propensity scores for each item-user interaction\nin the collected feedback dataset. This can be done by modeling the probability that\nthe deployed system would have exposed the item to the user at the time of the\ninteraction. One simple approach is to use the popularity of the item as a proxy for\nits propensity score. However, more sophisticated methods can be used to model\nthe propensity scores based on user and item features, as well as the context of the\ninteraction.\nOnce the propensity scores are estimated, we can reweight the feedback by using\nimportance sampling. Specifically, each feedback is weighted by the inverse of its pro‐\npensity score so that items that are more likely to be exposed by the deployed system\nare downweighted, while items that are less likely to be exposed are upweighted. This\nreweighting process approximates a counterfactual distribution of feedback expected\nfrom surfacing recommendations from a uniform distribution of popularity.\nFinally, we can use the reweighted feedback to evaluate the new recommendation\nmodel via standard metrics for evaluation, as we’ve seen in this chapter. The effective‐\nness of the new model is then compared to that of the deployed system by using\nthe reweighted feedback, providing a fairer and more accurate evaluation of the new\nmodel’s performance.\nSimpson’s and Mitigating Confounding\nSimpson’s paradox  is predicated on the idea of a confounding variable that establishes\nstrata within which we see (potentially misleading) covariation. This paradox arises\nwhen the association between two variables is investigated but these variables are\nstrongly influenced by a confounding variable.\nIn the case of recommendation systems, this confounding variable is the deployed\nmodel’s characteristics and tendencies of selection. The propensity score is intro‐\nduced as a measure of a system’s deviation from an unbiased open-loop exposure\nscenario. This score allows for the design and analysis of offline evaluation of recom‐\nmendation models based on the observed closed-loop feedback, mimicking some of\nthe particular characteristics of the open-loop scenario.\n210 | Chapter 10: Low-Rank Methods\nTraditional descriptions of Simpson’s paradox often suggest stratification, a well-\nknown approach to identify and estimate causal effects by first identifying the under‐\nlying strata before investigating causal effects in each stratum. This approach enables\nthe measurement of the potential outcome irrespective of the confounding variable.\nFor recommendation systems, this involves stratifying the observed outcome based\non the possible values of the confounding variable, which is the deployed model’s\ncharacteristics.\nThe user-independent propensity score is estimated via a two-step generative process\nusing the prior probability that an item is recommended by the deployed model and\nthe conditional probability that the user interacts with the item, given that it is rec‐\nommended. Based on a set of mild assumptions (but too mathematically technical to\ncover here), the user-independent propensity score can be estimated using maximum\nlikelihood for each dataset.\nWe need to define the user-propensity score pu,i, which indicates the tendency—\nor frequency—of the deployed model to expose item i ∈ I  to user u ∈ U . In\npractice, we marginalize over users to get the user-independent propensity score\np* ,i. As described in “Unbiased Offline Recommender Evaluation for Missing-Not-\nat-Random Implicit Feedback”  by Longqi Y ang et al., the equation is as follows:\np* ,iαni*γ+ 1\n2\nHere, ni* is the total number of times item i interacted with, and γ is a parameter that\naffects the propensity distributions over items with different observed popularity. The\npower-law parameter γ affects the propensity distributions over items and depends\non the examined dataset; we estimate the γ parameter by using maximum likelihood\nfor each dataset.\nWith these estimates for propensity, we can then apply a simple inverse weighting\nwi=1\npi when calculating the effect of feedback. Finally, we can combine these\nweightings with propensity matching, to generate counterfactual recommendations;\nby collecting approximately equal propensity items into strata, we can then use these\nstrata as our confounding variable.\nPropensity Weighting for Recommendation System Evaluation | 211",10198
88-Environments.pdf,88-Environments,"Doubly Robust Estimation\nDoubly robust estimation (DRE) is a method that combines two\nmodels: one that models the probability of receiving the treatment\n(being recommended an item by the deployed model) and one\nthat models the outcome of interest (the user’s feedback on the\nitem). The weights used in DRE depend on the predicted proba‐\nbilities from both models. This method has the advantage that it\ncan still provide unbiased estimates even if one of the models is\nmisspecified.\nThe structural equations for a doubly robust estimator with pro‐\npensity score weighting and outcome model is as follows:\nΘ=∑wiYi−fXi\n∑wiTi−pi+ ∑wipi1 −pi2fXi−f*Xi\nHere, Yi is the outcome, Xi are covariates, Ti is the treatment, pi is\nthe propensity score, wi is the weight, fXi is the outcome model,\nand f*Xi is the estimated outcome model.\nFor a great introduction to these considerations, check out “Give\nMe a Robust Estimator—and Make It a Double! .\nSummary\nWhat a whirlwind! Latent spaces are one of the most  important aspects of recommen‐\ndation systems. They are the representations that we use to encode our users and\nitems. Ultimately, latent spaces are about more than dimension reduction; they are\nabout understanding a geometry in which measures of distance encode the meaning\nrelevant to your ML task.\nThe world of embeddings and encoders runs deep. We haven’t had time to discuss\nCLIP embeddings (image + text) or the Poincaré disk (naturally hierarchical distance\nmeasures). We didn’t dive deep into UMAP (a nonlinear density-aware dimension-\nreduction technique) or HNSW (a method for retrieval in latent spaces that respects\nlocal geometry well). Instead, we point you to the (contemporaneously published)\narticle by Vicki Boykis  on embeddings, the essay and guide to constructing embed‐\ndings  by Karel Minařík, or the beautiful visual guide to text embeddings  by Meor\nAmer from Cohere.\nWe’re now equipped with representations, but next we need to optimize. We’re build‐\ning personalized  recommendation systems, so let’s define the metrics that measure\nour performance on our task.\n212 | Chapter 10: Low-Rank Methods\nCHAPTER 11\nPersonalized Recommendation Metrics\nHaving  explored the powerful methodologies of MF and neural networks in the con‐\ntext of personalization, we are now equipped with potent tools to craft sophisticated\nrecommendation systems. However, the order of recommendations in a list may have\na profound impact on user engagement and satisfaction.\nOur journey so far has primarily been focused on predicting what a user may like,\nusing latent factors or deep learning architectures. However, the manner in which we\npresent these predictions, or more formally, how we rank these recommendations,\nholds paramount significance. Therefore, this chapter will shift our gaze from the\nprediction problem and will unravel the complex landscape of ranking in recommen‐\ndation systems.\nThis chapter is dedicated to understanding key ranking metrics including mean\naverage precision (mAP), mean reciprocal rank (MRR), and normalized discounted\ncumulative gain (NDCG). Each of these metrics takes a unique approach toward\nquantifying the quality of our rankings, catering to different aspects of the user\ninteraction.\nWe’ll dive into the intricacies of these metrics, unveiling their computational details\nand discussing their interpretation, covering their strengths and weaknesses, and\npointing out their specific relevance to various personalization scenarios.\nThis exploration forms an integral part of the evaluation process in recommendation\nsystems. It not only gives us a robust framework to measure the performance of\nour system but also provides essential insights into understanding how different algo‐\nrithms might perform in online settings. This will lay the foundation for future dis‐\ncussions on algorithmic bias, diversity in recommendations, and a multistakeholder\napproach to recommendation systems.\n213",3996
89-User Versus Item Metrics.pdf,89-User Versus Item Metrics,"In essence, the knowledge garnered in this chapter will be instrumental in fine-tuning\nour recommendation system, ensuring that we don’t just predict well but also recom‐\nmend in a way that truly resonates with individual user preferences and behaviors.\nEnvironments\nBefore we dig into defining the key metrics, we’re going to spend a few moments dis‐\ncussing the kinds of evaluation we can do. Evaluation for recommendation systems,\nas you’ll soon see, is frequently characterized by how relevant  the recommendations\nare for a user. This is similar to search metrics, but we add in additional factors to\naccount for where  in the list the most relevant items are.\nFor an extremely comprehensive view on evaluation of recommender systems, the\nrecent project RecList  builds a useful checklist-based framework for organizing met‐\nrics and evaluations.\nOften you’ll hear about evaluating recommenders in a few setups:\n•Online/offline•\n•User/item•\n•A/B•\nEach setup provides slightly different kinds of evaluations and tells you different\nthings. Let’s quickly break down the differences to set some assumptions about\nterminology.\nOnline and Offline\nWhen  we refer to online versus offline recommenders, we are referring to when\nyou’re running evals. In offline  evaluation , you start with a test/evaluation dataset,\noutside your production system, and compute a set of metrics. This is often the\nsimplest recommender to set up but has the highest expectation of existing data.\nUsing historical data, you construct a set of relevant responses, which you can then\nuse during simulated inference. This approach is the most similar to other kinds of\ntraditional ML, although with slightly different computations for the error.\nWhen we’re training large models, these datasets are similar to an offline dataset. We\npreviously saw prequential data, which is much more relevant in recommendation\nsystems than in lots of other ML applications. Sometimes you’ll hear people say\nthat “all recommenders are sequential recommenders” because of the importance of\nhistorical exposure to the recommender problem.\n214 | Chapter 11: Personalized Recommendation Metrics",2182
90-Recall and Precision.pdf,90-Recall and Precision,"Online evaluation  takes place during inference, usually in production. The tricky part\nis that you essentially never know the counterfactual outcomes. Y ou can compute\nspecific metrics on the online rankings: frequency and distributions of covariates,\nCTR/success rate, or time on platform, but ultimately these are different from the\noffline metrics.\nBootstrapping from Historical Evaluation Data\nOne  of the most common questions from people building a rec‐\nommender from scratch is “Where do you get the initial training\ndata?” This is a hard problem. Ultimately, you have to be clever\nto come up with a useful dataset. Consider our co-occurrence\ndata in the Wikipedia recommender; we didn’t require any user\ninteractions to get to a set of data to build a recommender. Boot‐\nstrapping from item to item is the most popular strategy, but you\ncan use other tricks as well. The simplest way to start moving into\nuser-item recommenders is to simply ask the user questions. If you\nask for preference information across a set of item features, you can\nbuild simple models that start to incorporate this.\nUser Versus Item Metrics\nBecause  recommender systems are personalization machines, it can be easy to think\nthat we always want to be making recommendations for the user and measuring the\nperformance as such. Subtleties exist, though. We want to be sure individual items\nare getting a fair chance, and sometimes looking at the other side of the equation\ncan help assess this. In other words, are the items getting recommended frequently\nenough to have a chance to find their niche? We should explicitly compute our\nmetrics over user and item axes.\nAnother aspect of item-side metrics is for set-based recommenders. The other items\nthat are recommended in context can have a significant effect on the performance of\na recommendation. As a result, we should be careful to measure the pairwise item\nmetrics in our large-scale evaluations.\nA/B Testing\nIt’s good to use randomized, controlled trials to evaluate how your new recommen‐\ndation model is performing. For recommendations, this is quite tricky. At the end\nof this chapter, you’ll see some of the nuance, but for now, let’s consider a quick\nreminder of how to think about A/B testing in a closed-loop paradigm.\nEnvironments | 215\nA/B tests ultimately attempt to estimate the effect size of swapping one model in for\nanother; effect size estimation is the process of measuring the causal impact of an\nintervention on a target metric. First, we would need to deploy two recommender\nmodels. We’ d also hope that there’s a reasonable randomization of users into each\nof the recommenders. However, what’s the randomization unit? It’s easy to quickly\nassume it’s the user, but what has changed about the recommender? Has the recom‐\nmender changed in a way that covaries with some properties of the distribution—e.g.,\nhave you built a new recommender that is less friendly toward seasonal TV specials\njust as we enter into the second week of November?\nAnother consideration with this sort of testing for recommendation systems is the\nlong-term compounding effects. A frequent rejoinder about a series of positive A/B\ntest outcomes over several years is “Have you tested the first recommender against\nthe last?” This is because populations change, both the users and the items. As you\nalso vary the recommender system, you frequently find yourself in a double-blind\nsituation where you’ve never seen this user or item population with any other recom‐\nmender. If all the effect sizes of every A/B test were additive across the industry, the\nworld GDP would likely be two to three times as large.\nThe way to guard against protests like this is via a long-term holdout , a random\nsubset of users (continually being added to) who will not be upgraded to new\nmodels through time. By measuring the target metrics on this set versus the most\ncutting-edge model in production, you’re always able to understand the long-term\neffects of your work. The downside of a long-term holdout? It’s hard to maintain, and\nit’s hard to sacrifice some of the effects of your work on a subset of the population.\nNow let’s finally get to the metrics already!\nRecall and Precision\nLet’s  begin by considering four recommender problems and how each may have\ndifferent implications for the kind of results you want.\nFirst, let’s consider entering a bookstore and looking for a book by a popular author.\nWe would say this is the recommender problem:\n•Provides a lot of recommendations•\n•Offers few possible relevant results•\nAdditionally, if the bookstore has a good selection, we’ d expect that all the relevant\nresults are contained in the recommendations because bookstores often carry most or\nall of an author’s oeuvre once they’ve become popular. However, many of the recom‐\nmendations—the books in the bookstore–are simply not relevant for this search.\n216 | Chapter 11: Personalized Recommendation Metrics\nSecond, let’s consider looking for a gas station nearby on a mapping app while in a\nlarge metro. We expect that a lot of gas stations are relatively close by, but you would\nprobably consider only the first couple—or maybe even only one, the first one that\nyou see. Thus a recommender for this problem has the following:\n•Many relevant results•\n•Few useful recommendations•\nIn the first scenario, the relevant results may be fully contained in the recommenda‐\ntions, and in the second scenario, the recommendations may be fully contained in the\nrelevant results.\nLet’s now look at more common scenarios.\nFor our third example, consider that you’re searching on a streaming video platform\nfor something to watch tonight when you’re feeling romantic. Streaming platforms\ntend to show a lot of recommendations—pages and pages from this one theme or\nanother. But on this night, and on just this platform, only a couple of movies or\nTV shows might really fit what you’re looking for. Our recommender, then, does the\nfollowing:\n•Provides many recommendations•\n•Offers only a few that are actually relevant•\nHowever, importantly, not all relevant results will be in the recommendations! As we\nknow, different platforms have different media, so some of the relevant results won’t\nappear in the recommendations no matter how many we look at.\nFourth, and finally, you’re a high-end coffee lover with distinguished tastes headed\ninto the local roaster for a third-wave, single-origin coffee. As an experienced coffee\nconnoisseur, you love high-quality coffees from all over the world and enjoy most\nbut not all origins. On any given day, your local cafe has only a few single-origin\nhand-brewed options. Despite your worldly palette, there are some popular terroirs\nthat you just don’t love. This little recommendation brew bar can be described as\nfollows:\n•Provides a few recommendations•\n•Offers many possible recommendations that are relevant•\nOn any given day, only some of the few recommendations may be relevant to you.\nRecall and Precision | 217",7077
91-Precision at k.pdf,91-Precision at k,"So those are our matching four scenarios. For the latter two, the intersection between\nrecommendation and relevance may be proportionally small or large—or even\nempty! The main idea is that the full size of the smaller sample is not always in use.\nNow that we’ve worked through a few examples, let’s see how they relate to the core\nmetrics for a recommender: precision and recall @ k (Figure 11-1 ). Focusing on\nexamples 3 and 4, we can see that only some of the recommendations intersect with\nthe options that are relevant. And only some of the relevant options intersect with\nthe recommendations. It’s often overlooked, but in fact these two ratios define  our\nmetrics —let’s go!\nFigure 11-1. Recall and precision sets\n@ k\nIn much of this chapter and RecSys metrics discussion, we say things like @ k. This\nmeans “at k, ” which should really be “in k” or “out of k. ” These are simply the size of\nthe set of recommendations. We often anchor the customer experience on how many\nrecommendations we can show the user without the experience suffering. We also\nneed to know the cardinality of the set of relevant items, which we call @ r. Note that\nwhile it may not feel like it’s possible to ever know this number, we assume this refers\nto “known relevant” options via our training or test data.\n218 | Chapter 11: Personalized Recommendation Metrics",1365
92-R-precision.pdf,92-R-precision,"Precision at k\nPrecision  is the ratio of the size of the set of relevant recommendations to k, the size of\nthe set of recommendations.\nPrecision @k=numrelevant\nk\nNotice that the size of the relevant items doesn’t appear in the formula. That’s OK; the\nsize of the intersection is still dependent on the size of the set of relevant items.\nLooking at our examples, 2 technically has the highest precision, but it’s a bit of a red\nherring because of the number of relevant results. This is one reason precision is not\nthe most common metric for evaluating recommendation systems.\nRecall at k\nRecall  is the ratio of the size of the set of relevant recommendations to r, the size of\nthe set of relevant items.\nBut wait! If the ratio is the relevant recommendations over the relevant items, where\nis k? k is still important here because the size of the set of recommendations con‐\nstrains the possible size of the intersection. Recall that these ratios are operating on\nthat intersection that is always dependent on k. This means you often consider the\nmax of r and k.\nIn scenario 3, we hope that some of the movies that fit our heart’s desire will be on the\nright streaming platform. The number of these divided by the count of all the media\nanywhere is the recall . If all your relevant movies are on this platform, you might call\nthat total recall .\nScenario 4’s café experience shows that recall is sometimes the inverse probability of\nan avoid; because you like so many coffees, we might instead find it easier to talk\nabout what you don’t like. In this case, the number of avoids in the offering will have\na large effect on the recall:\nRecall @k=k−Avoid @k\nnumrelevant\nThis is the core mathematical definition for recall and is often one of the first\nmeasurements we’ll consider because it’s a pure estimate of how your retrieval is\nperforming.\nRecall and Precision | 219",1903
93-mAP MMR NDCG.pdf,93-mAP MMR NDCG,,0
94-mAP Versus NDCG.pdf,94-mAP Versus NDCG,"R-precision\nIf we also have a ranking on our recommendations, we can take the ratio of relevant\nrecommendations to r in the top-r  recommendations. This improves this metric in\ncases where r is very small, as in examples 1 and 3.\nmAP, MMR, NDCG\nHaving  delved into the reliable domains of precision@ k and recall@ k, we’ve gained\nvaluable insights into the quality of our recommendation systems. However, these\nmetrics, as crucial as they are, can sometimes fall short in capturing an important\naspect of these systems: the order of recommendations .\nIn recommendation systems, the ordering in which we present suggestions carries\nsignificant weight and needs to be evaluated to ensure that it’s effective.\nThat’s why we’ll now journey beyond precision@ k and recall@ k to explore some key\nranking-sensitive metrics—namely, mean average precision (mAP), mean reciprocal\nrank (MRR), and normalized discounted cumulative gain (NDCG). These metrics\nconsider not only whether our recommendations are relevant but also whether they\nare well-ordered.\nThe mAP metric lends importance to each relevant document and its position, and\nMRR concentrates on the rank of the first relevant item. NDCG gives more impor‐\ntance to relevant documents at higher ranks. By understanding these metrics, you’ll\nhave an even more robust set of tools to evaluate and refine your recommendation\nsystems.\nSo, let’s carry on with our exploration, striking a balance between precision and\ncomprehensibility. By the end of this section, you will be well equipped to handle\nthese essential evaluation methods in a confident and knowledgeable manner.\nmAP\nThis  vital metric in recommendation systems is particularly adept at accounting for\nthe rank of relevant items. If, in a list of five items, the relevant ones are found\nat positions 2, 3, and 5, mAP would be calculated by computing precision@2, preci‐\nsion@3, and precision@5 and then taking an average of these values. The strength of\nmAP lies in its sensitivity to the ordering of relevant items, providing a higher score\nwhen these items are ranked higher.\nConsider an example with two recommendation algorithms A and B:\n•For algorithm A, we compute the mAP as follows:•\n(precision@2 + precision@3 + precision@5) / 3 = (1/2 + 2/3 + 3/5) / 3 = 0.6\n220 | Chapter 11: Personalized Recommendation Metrics\n•For algorithm B, which perfectly ranks the items, we calculate mAP as follows:•\nmAP = (precision@1 + precision@2 + precision@3) / 3 = (1/1 + 2/2 + 3/3) / 3 = 1\nThe generalized formula for mAP across a set of queries Q is shown here:\nmAP =1\nQ∑q= 1Q 1\nmq∑k= 1nPk*relk\nHere, Q is the total number of queries, mq is the number of relevant documents for\na specific query q, Pk stands for the precision at the _k_th cutoff, and relk is an\nindicator function equating to 1 if the item at rank k is relevant, and 0 otherwise.\nMRR\nAnother  effective metric used in recommendation systems is MRR. Unlike MAP ,\nwhich considers all relevant items, MRR primarily focuses on the position of the first\nrelevant item in the recommendation list. It’s computed as the reciprocal of the rank\nwhere the first relevant item appears.\nConsequently, MRR can reach its maximum value of 1 if the first item in the list is\nrelevant. If the first relevant item is found farther down the list, MRR takes a value\nless than 1. For instance, if the first relevant item is positioned at rank 2, the MRR\nwould be 1/2.\nLet’s look at this in the context of the recommendation algorithms A and B that we\nused earlier:\n•For algorithm A, the first relevant item is at rank 2, so the MRR equals 1/2 = 0.5.•\n•For algorithm B, which perfectly ranked the items, the first relevant item is at•\nrank 1, so the MRR equals 1/1 = 1.\nExtending this to multiple queries, the general formula for MRR is as follows:\nMRR =1\nQ∑i= 1Q 1\nranki\nHere, | Q| represents the total number of queries, and ranki is the position of the first\nrelevant item in the list for the _i_th query. This metric provides valuable insight into\nhow well a recommendation algorithm delivers a relevant recommendation right at\nthe top of the list.\nmAP, MMR, NDCG | 221\nNDCG\nTo further refine our understanding of ranking metrics, let’s step into the world of\nNDCG. Like mAP and MRR, NDCG also acknowledges the rank order of relevant\nitems but introduces a twist. It discounts the relevance of items as we move down\nthe list, signifying that items appearing earlier in the list are more valuable than those\nranked lower.\nNDCG begins with the concept of cumulative gain (CG), which is simply the sum of\nthe relevance scores of the top k items in the list. Discounted cumulative gain (DCG)\ngoes a step further, discounting the relevance of each item based on its position.\nNDCG, then, is the DCG value normalized by the ideal DCG (IDCG), the DCG that\nwe would obtain if all relevant items appeared at the very top of the list.\nAssuming we have five items in our list and a specific user for whom the relevant\nitems are found at positions 2 and 3, the IDCG@ k would be (1/log(1 + 1) + 1/log(2 +\n1)) = 1.5 + 0.63 = 2.13.\nLet’s put this into the context of our example algorithms A and B.\nFor algorithm A\n•DCG@5 = 1/log(2 + 1) + 1/log(3 + 1) + 1/log(5 + 1) = 0.63 + 0.5 + 0.39 = 1.52•\n•NDCG@5 = DCG@5 / IDCG@5 = 1.52 / 2.13 = 0.71•\nFor algorithm B\n•DCG@5 = 1/log(1 + 1) + 1/log(2 + 1) + 1/log(3 + 1) = 1 + 0.63 + 0.5 = 2.13•\n•NDCG@5 = DCG@5 / IDCG@5 = 2.13 / 2.13 = 1•\nThe general formula for NDCG can be represented as\nNDCG @k=DCG @k\nIDCG @k\nwhere\n•DCG @k= ∑i= 1kreli\nlog2i+ 1•\n•IDCG @k= ∑i= 1ℛ 1\nlog2i+ 1•\nand ℛ is the set of relevant documents.\nThis metric gives us a normalized score for how well our recommendation algorithm\nranks relevant items, discounting as we move further down the list.\n222 | Chapter 11: Personalized Recommendation Metrics",5894
95-RMSE from Affinity.pdf,95-RMSE from Affinity,"mAP Versus NDCG?\nBoth mAP and NDCG are holistic metrics that offer a comprehensive perspective\nof ranking quality by incorporating all relevant items and their respective ranks.\nHowever, the interpretability and use cases of these metrics can vary based on the\nspecifics of the recommendation context and the nature of relevance.\nWhile MRR does not consider all relevant items, it does provide an interpretable\ninsight into an algorithm’s performance, highlighting the average rank of the first\nrelevant item. This can be particularly useful when the topmost recommendations\nhold significant value.\nmAP , on the other hand, is a rich evaluation measure that effectively represents\nthe area under the precision-recall curve. Its average aspect confers an intuitive\ninterpretation related to the trade-off between precision and recall across different\nrank cutoffs.\nNDCG introduces a robust consideration of the relevance of each item and is\nsensitive to the rank order, employing a logarithmic discount factor to quantify\nthe diminishing significance of items as we move down the list. This allows it to\nhandle scenarios in which items can have varying degrees of relevance, extending\nbeyond binary relevance often used in mAP and MRR. However, this versatility of\nNDCG can also limit its interpretability because of the complexity of the logarithmic\ndiscount.\nFurther, although NDCG is well equipped for use cases where items carry distinct\nrelevance weights, procuring accurate ground-truth relevance scores can pose a\nsignificant challenge in practical applications. This imposes a limitation on the real-\nworld usefulness of NDCG.\nCumulatively, these metrics form the backbone of offline evaluation methodologies\nfor recommendation algorithms. As we advance in our exploration, we’ll cover online\nevaluations, discuss strategies to assess and mitigate algorithmic bias, understand\nthe importance of ensuring diversity in recommendations, and optimize recommen‐\ndation systems to cater to various stakeholders in the ecosystem.\nCorrelation Coefficients\nWhile  correlation coefficients like Pearson’s or Spearman’s can be employed to evalu‐\nate the similarity between two rankings (for instance, between the predicted and the\nground-truth rankings), they do not provide the exact same information as mAP ,\nMRR, or NDCG.\nCorrelation coefficients are typically used to measure the degree of linear association\nbetween two continuous variables, and in the context of ranking, they can indicate\nthe overall similarity between two ordered lists. However, they do not directly\nmAP, MMR, NDCG | 223",2629
96-Recommendation Probabilities to AUC-ROC.pdf,96-Recommendation Probabilities to AUC-ROC,"account for aspects such as the relevance of individual items, the position of relevant\nitems, or varying degrees of relevance among items, which are integral to mAP , MRR,\nand NDCG.\nFor example, say a user has interacted with five items in the past. A recommender\nsystem might predict that the user will interact with these items again but rank\nthem in the opposite order of importance. Even though the system has correctly\nidentified the items of interest, the reversed ranking would lead to poor performance\nas measured by mAP , MRR, or NDCG, but a high negative correlation coefficient\nwould be obtained because of the linear relationship.\nAs a result, while correlation coefficients can provide a high-level understanding\nof ranking performance, they are not sufficient substitutes for the more detailed\ninformation provided by metrics like mAP , MRR, and NDCG.\nTo utilize correlation coefficients in the context of ranking, it would be essential to\npair them with other metrics that account for the specific nuances of the recommen‐\ndation problem, such as the relevance of individual items and their positions in the\nranking.\nRMSE from Affinity\nRoot mean square error (RMSE) and ranking metrics like mAP , MRR, and NDCG\noffer fundamentally different perspectives when evaluating a recommendation system\nthat outputs affinity scores.\nRMSE is a popular metric for quantifying prediction error. It computes the square\nroot of the average of squared differences between the predicted affinity scores and\nthe true values. Lower RMSE signifies better predictive accuracy. However, RMSE\ntreats the problem as a standard regression task and disregards the inherent ranking\nstructure in recommendation systems.\nConversely, mAP , MRR, and NDCG are explicitly designed to evaluate the quality of\nrankings, which is essential in a recommendation system. In essence, while RMSE\nmeasures the closeness of predicted affinity scores to actual values, mAP , MRR, and\nNDCG assess the ranking quality by considering the positions of relevant items.\nTherefore, if your main concern is ranking items rather than predicting precise\naffinity scores, these ranking metrics are generally more appropriate.\nIntegral Forms: AUC and cAUC\nWhen  it comes to recommendation systems, we are producing a ranked list of items\nfor each user. As you’ve seen, these rankings are based on affinity, the probability\nor level of preference that the user has for each item. Given this framework, several\nmetrics have been developed to evaluate the quality of these ranked lists. One such\n224 | Chapter 11: Personalized Recommendation Metrics",2643
97-Summary.pdf,97-Summary,"metric is the AUC-ROC, which is complemented by mAP , MRR, and NDCG. Let’s\ntake a closer look at understanding these.\nRecommendation Probabilities to AUC-ROC\nIn a binary classification setup, the area under the receiver operating characteristic\ncurve  (AUC-ROC) measures the ability of the recommendation model to distinguish\nbetween positive (relevant) and negative (irrelevant) instances. It is calculated by\nplotting the true positive rate (TPR) against the false positive rate (FPR) at various\nthreshold settings and then computing the area under this curve.\nIn the context of recommendations, you can think of these “thresholds” as varying\nthe number of top items recommended to a user. The AUC-ROC metric becomes\nan evaluation of how well your model ranks relevant items over irrelevant ones,\nirrespective of the actual rank position. In other words, AUC-ROC effectively quan‐\ntifies the likelihood that a randomly chosen relevant item is ranked higher than a\nrandomly chosen irrelevant one by the model. This, however, doesn’t account for\nthe actual position or order of items in the list, only the relative ranking of positive\nversus negative instances. The affinity of a calibrated item may be interpreted as a\nconfidence measure by the model that an item is relevant, and when considering\nhistorical data, even uncalibrated affinity scores may make a great suggestion for the\nnumber of recommendations necessary to find something useful.\nOne serious implementation of these affinity scores might be to show users only\nitems over a particular score and otherwise tell them to come back later or use\nexploration methods to improve the data. For example, if you sold hygiene products\nand were considering asking customers to add some Aesop soap during checkout,\nyou may wish to evaluate the Aesop ROC and make this suggestion only when the\nobserved affinity passed the learned threshold. Y ou’ll also see these concepts used\nlater in “Inventory Health” on page 268 .\nComparison to Other Metrics\nLet’s put these in context with the other metrics:\nmAP\nThis metric expands on the idea of precision at a specific cutoff in the ranked list\nto provide an overall measure of model performance. It does this by averaging\nthe precision values computed at the ranks where each relevant item is found.\nUnlike AUC-ROC, mAP puts emphasis on the higher-ranked items and is more\nsensitive to changes at the top of the ranking.\nMRR\nUnlike  AUC-ROC and mAP , which consider all relevant items in the list, MRR\nfocuses only on the rank of the first relevant item in the list. It is a measure of\nIntegral Forms: AUC and cAUC | 225\nhow quickly the model can find a relevant item. If the model consistently places a\nrelevant item at the top of the list, it will have a higher MRR.\nNDCG\nThis metric evaluates the quality of the ranking by not only considering the order\nof recommendations but also taking into account the graded relevance of items\n(which the previous metrics don’t). NDCG discounts items further down the list,\nrewarding relevant items that appear near the top of the list.\nAUC-ROC provides a valuable aggregate measure of a model’s ability to differentiate\nbetween relevant and irrelevant items; mAP , MRR, and NDCG offer a more nuanced\nevaluation of the model’s ranking quality, considering factors like position bias and\nvarying degrees of relevance.\nNote that we sometimes compute the AUC per customer and then average. That’s\ncustomer AUC (cAUC), which can often provide a good expectation for a user’s\nexperience.\nBPR\nBayesian personalized ranking  (BPR) presents a Bayesian approach to the task of\nitem ranking in recommendation systems, effectively providing a probability frame‐\nwork to model the personalized ranking process. Instead of transforming the item\nrecommendation problem into a binary classification problem (relevant or not),\nBPR focuses on pairwise preferences: given two items, which does the user prefer?\nThis approach aligns better with the nature of implicit feedback that is common in\nrecommendation systems.\nThe BPR model uses a pairwise loss function that takes into account the relative order\nof a positive item and a negative item for a specific user. It seeks to maximize the\nposterior probability of the observed rankings being correct. The model is typically\noptimized using stochastic gradient descent or a variant thereof. It’s important to note\nthat BPR (unlike other metrics we’ve discussed, including AUC-ROC, mAP , MRR,\nand NDCG) is a model training objective rather than an evaluation metric. There‐\nfore, while the aforementioned metrics evaluate a model’s performance post-training,\nBPR provides a mechanism to guide the model learning process in a way that directly\noptimizes for the ranking task. A much deeper discussion of these topics is in “BPR:\nBayesian Personalized Ranking from Implicit Feedback”  by Steffen Rendle et al.\n226 | Chapter 11: Personalized Recommendation Metrics\nSummary\nNow that you know how to evaluate the performance of the recommendation systems\nthat you train, you may be wondering how to actually train them. Y ou may have\nnoticed that many of the metrics we introduced would not make very good loss\nfunctions; they involve a lot of simultaneous observations about sets and lists of\nitems. This would unfortunately make the signal that the recommender would be\nlearning from highly combinatorial. Additionally, the metrics we’ve presented really\nhave two aspects to consider: the binary metric associated to recall, and the rank\nweighting.\nIn the next chapter, you’re going to learn some loss functions that make excellent\ntraining objectives. The importance of these, we’re sure, won’t be lost on you.\nSummary | 227",5774
98-Chapter 12. Training for Ranking.pdf,98-Chapter 12. Training for Ranking,,0
99-Training an LTR Model.pdf,99-Training an LTR Model,"CHAPTER 12\nTraining for Ranking\nTypical  ML tasks usually predict a single outcome, such as the probability of being\nin a positive class for classification tasks, or an expected value for regression tasks.\nRanking, on the other hand, provides a relative ordering of sets of items. This kind\nof task is typical of search results or recommendations, where the order of items\npresented is important. In these kinds of problems, the score of an item usually\nisn’t shown to the user directly but rather is presented—maybe implicitly—with the\nordinal rank of the item: the item at the top of the list is numbered lower than the\nnext item.\nThis chapter presents various kinds of loss functions that ML algorithms can use\nduring training. These scores should estimate list orderings such that when compared\nto one another, they result in sets that are ordered more closely to the relevance\nordering observed in a training dataset. Here we will focus on introducing the\nconcepts and computations, which you’ll put to work in the next chapter.\nWhere Does Ranking Fit in Recommender Systems?\nBefore  we dive into the details of loss functions for ranking, we should talk about\nwhere ranking fits into the larger scheme of recommender systems as a whole.\nTypical large-scale recommenders have a retrieval phase, in which a cheap function is\nused to gather a decent number of candidate items into a candidate set. Usually, this\nretrieval phase is only item based. For example, the candidate set might include items\nrelated to recently consumed or liked items by a user. Or if freshness is important,\nsuch as for news data, the set might include the newest popular and relevant items for\nthe user. After items are gathered into a candidate set, we apply ranking to its items.\nAlso, since the candidate set is usually much smaller than the entire corpus of\nitems, we can use more expensive models and auxiliary features to help the ranking.\n229\nThese features could be user features or context features. User features could help\nin determining the items’ usefulness to the user, such as the average embedding of\nrecently consumed items. Context features could indicate details about the current\nsession, such as time of day or recent queries that a user has typed—a feature that\ndifferentiates the current session from others and helps in determining relevant\nitems. Finally, we have the representation of the items themselves, which can be\nanything from content features to learned embeddings that represent the item.\nThe user, context, and item features are then concatenated into one feature vector\nthat we will use to represent the item; we then score all the candidates at once and\norder them. The rank ordered set might then have extra filtering applied to it for\nbusiness logic, such as removing near duplicates or making the ranked set more\ndiverse in the kinds of items displayed.\nIn the following examples, we will assume that the items can all be represented by\na concatenated feature vector of user, context, and item features and that the model\ncould be as simple as a linear model with a weight vector W that is dotted with the\nitem vector to obtain a score for sorting the items. These models can be generalized\nto deep neural networks, but the final layer output is still going to be a scalar used to\nsort the items.\nNow that we have set the context for ranking, let’s consider ways we might rank a set\nof items represented by vectors.\nLearning to Rank\nLearning to rank  (LTR) is the name for the kind of models that score an ordered list\nof items according to their relevancy or importance. This technique is how we go\nfrom the potentially raw output of retrieval to a sorted list of items based on their\nrelevance.\nLTR problems have three main types:\nPointwise\nThe model treats individual documents in isolation and assigns them a score or\nrank. The task becomes a regression or classification problem.\nPairwise\nThe model considers pairs of documents simultaneously in the loss function. The\ngoal is to minimize the number of incorrectly ordered pairs.\nListwise\nThe model considers the entire list of documents in the loss function. The goal is\nto find the optimal ordering of the entire list.\n230 | Chapter 12: Training for Ranking",4297
100-Classification for Ranking.pdf,100-Classification for Ranking,,0
101-k-order Statistic.pdf,101-k-order Statistic,"Training an LTR Model\nThe training data for an LTR model typically consists of a list of items, and each\nitem has a set of features and a label (or ground truth). The features might include\ninformation about the item itself, and the label typically represents its relevance\nor importance. For instance, in our recommender systems, we have item features,\nand in the training dataset, the labels will show if the item is relevant to the user.\nAdditionally, LTR models sometimes make use of the query or user features.\nThe training process is about learning a ranking function by using these features and\nlabels. These ranking functions are then applied to the retrieved items before serving.\nLet’s see some examples of how these models are trained.\nClassification  for Ranking\nOne  way to pose the ranking problem is as a multilabel task. Every item appearing\nin the training set that is associated to the user is a positive example, while those\noutside would be negative. This is, in effect, a multilabel approach at the scale of\nthe set of items. The network could have an architecture with each item’s features\nas input nodes, and then some user features as well. The output nodes would be in\ncorrespondence with the items you wish to label.\nWith  a linear model, if X is the item vector and Y is the output, we learn W, where\nsigmoid WX = 1 if X is an item in the positive set; otherwise, sigmoid WX = 0.\nThis corresponds to the binary cross-entropy loss  in Optax.\nUnfortunately, the relative ordering of items isn’t taken into account in this setup,\nso this loss function consisting of sigmoid activation functions for each item won’t\noptimize ranking metrics very well. Effectively, this ranking is merely a downstream\nrelevance model  that only helps to filter those options retrieved in a previous step.\nAnother problem with this approach is that we have labeled everything outside of the\ntraining set to be negative, but the user might never have seen a new item that could\nbe relevant to a query—so it would be incorrect to label this new item as a negative\nwhen it is simply unobserved.\nY ou may have realized that the ranking needs to consider the relative positions in the\nlist. Let’s consider this next.\nRegression for Ranking\nThe most naive way to rank a set of items is simply to regress to the rank of a similar\nnumber like NDCG or our other personalization metrics that are rank respective.\nIn practice, this is achieved by conditioning the set of items against a query. For\nexample, we could pose the problem as regression to the NDCG, given the query as\nTraining an LTR Model | 231\nthe context of the ranking. Furthermore, we can supply the query as an embedding\ncontext vector to a feed-forward network that is concatenated with the features of the\nitems in the set and regress toward the NDCG value.\nThe query is needed as a context because a set of item’s ordering might be dependent\nupon the query. Consider, for example, typing into a search bar the query flowers .\nWe would then expect a set of items most representative of flowers to be in the top\nresults. This demonstrates that the query is an important consideration of the scoring\nfunction.\nWith a linear model, if X is the item vector and Y is the output, then we learn W,\nwhere WXi=NDCGi and NDCGi is the NDCG for item i. Regression can be\nlearned using the L2 loss  in Optax.\nUltimately, this approach is about attempting to learn the underlying features of items\nthat lead to higher-rank scores in your personalization metric. Unfortunately, this\nalso fails to explicitly consider the relative ordering of items. This is a pretty serious\nlimitation, which we’ll consider shortly.\nAnother consideration: what do we do for items that aren’t ranked outside of the\ntop-k training items? The rank we would assign them would be essentially random,\nas we do not know what number to assign them. Therefore, this method needs\nimprovement, which we’ll explore in the next section.\nClassification  and Regression for Ranking\nSuppose we have a web page such as an online bookstore, and users have to browse\nthrough and click items in order to purchase them. For such a funnel, we could break\nthe ranking into two parts. The first model could predict the probability of a click on\nan item, given a set of items on display. The second model could be conditioned on\na click-through and could be a regression model estimating the purchase price of the\nitem.\nThen, a full ranking model could be the product of two models. The first one\ncomputes the probability of clicking through an item, given a set of competing items.\nAnd the second one computes the expected value of a purchase, given that it had\nbeen clicked. Notice that the first and second model could have different features,\ndepending on the stage of the funnel a user is in. The first model has access to fea‐\ntures of competing items, while the second model might take into account shipping\ncosts and discounts applied that might change the value of an item. Thus, in this\nsetting, it would be advantageous to model both stages of the funnel with different\nmodels so as to make use of the most amount of information present at each stage of\nthe funnel.\n232 | Chapter 12: Training for Ranking\nWARP\nOne possible way to generate a ranking loss stochastically is introduced in “WSABIE:\nScaling Up to Large Vocabulary Image Annotation”  by Jason Weston et al. The loss is\ncalled weighted approximate rank pairwise  (W ARP). In this scheme, the loss function\nis broken into what looks like a pairwise loss. More precisely, if a higher-ranked item\ndoesn’t have a score that is greater than the margin (which is arbitrarily picked to be\n1) for a lower-rank item, we apply the hinge loss  to the pair of items. This looks like\nthe following:\nmax 0, 1 −scorepos +scoreneg\nWith a linear model, if Xpos is the positive item vector, and Xneg is the negative item\nvector, then we learn W, where WXpos−WXneg> 1. The loss for this is hinge loss ,\nwhere the predictor output is WXpos−WXneg and the target is 1.\nHowever, to compensate for the fact that an unobserved item might not be a true\nnegative, just something unobserved, we count the number of times we had to sample\nfrom the negative set to find something that violates the ordering of the chosen pair.\nThat is, we count the number of times we had to look for something where:\nscoreneg >scorepos − 1\nWe then construct a monotonically decreasing function of the number of times we\nsample the universe of items (less the positives) for a violating negative and look up\nthe weight for this number and multiply the loss with it. If it’s very hard to find a\nviolating negative, the gradient should therefore be lower because either we are close\nto a good solution already or the item was never seen before, so we should not be so\nconfident as to assign it a low score just because it was never shown to the user as a\nresult for a query.\nNote that W ARP loss was developed when CPUs were the dominant form of com‐\nputation to train ML models. As such, an approximation to ranking was used to\nobtain the rank of a negative item. The approximate rank  is defined as the number\nof samples with replacement in the universe of items (less the positive example)\nbefore we find a negative item whose score is larger than the positive by an arbitrary\nconstant, called a margin , of 1.0.\nTo construct the W ARP weight for the pairwise loss, we need a function to go from\nthe approximate rank of the negative item to the W ARP weight. A relatively simple\nbit of code to compute this is as follows:\nimport numpy as np\ndef get_warp_weights (n: int) -> np.ndarray:\nWARP | 233\n  """"""Returns N weights to convert a rank to a loss weight.""""""\n  # The alphas are defined as values that are monotonically decreasing.\n  # We take the reciprocal of the natural numbers for the alphas.\n  rank = np.arange(1.0, n + 1, 1)\n  alpha = 1.0 / rank\n  weights = alpha\n  # This is the L in the paper, defined as the sum of all previous alphas.\n  for i in range(1, n):\n    weights[i] = weights[i] + weights[i -1]\n  # Divide by the rank.\n  weights = weights / rank\n  return weights\nprint(get_warp_weights (5))\n[1.         0.75       0.61111111  0.52083333  0.45666667 ]\nAs you can see, if we find a negative immediately, the W ARP weight is 1.0, but if it\nis very difficult to find a negative that violates the margin, the W ARP weight will be\nsmall.\nThis loss function is approximately optimizing precision@ k, and thus a good step\ntoward improving rank estimates in the retrieved set. Even better, W ARP is computa‐\ntionally efficient via sampling and thus more memory efficient.\nk-order Statistic\nIs there a way to improve upon the W ARP loss and straight-up pairwise hinge loss?\nTurns out there are a whole spectrum of ways. In “Learning to Rank Recommenda‐\ntions with the k-order Statistic Loss” , Jason Weston et al. (including one of this book’s\ncoauthors) show how this can be done by exploring the variants of losses between\nhinge loss and W ARP loss. The authors of the paper conducted experiments on\nvarious corpora and show how the trade-off between optimizing for a single pairwise\nversus selecting a harder negative like W ARP affects metrics including mean rank and\nprecision and recall at k.\nThe key generalization is that instead of a single positive item considered during the\ngradient step, the model uses all of them.\nRecall again that picking a random positive and a random negative pair optimizes for\nthe ROC, or AUC. This isn’t great for ranking because it doesn’t optimize for the top\nof the list. W ARP loss, on the other hand, optimizes for the top of the ranking list for\na single positive item but does not specify how to pick the positive item.\nSeveral alternate strategies can be used for ordering the top of the list, including\noptimizing for mean maximum rank, which tries to group the positive items such\n234 | Chapter 12: Training for Ranking\nthat the lowest-scoring positive item is as near the top of the list as possible. To\nallow this ordering, we provide a probability distribution function over how we\npick the positive sample. If the probability is skewed toward the top of the positive\nitem list, we get a loss more like W ARP loss. If the probability is uniform, we get\nAUC loss. If the probability is skewed toward the end of the positive item list, we\nthen optimize for the worst case, like mean maximum rank. The NumPy function\nnp.random.choice  provides a mechanism from sampling from a distribution P.\nWe have one more optimization to consider: K, the number of positive samples to\nuse to construct the positive set. If K= 1, we pick only a positive random item from\nthe positive set; otherwise, we construct the positive set, order the samples by score,\nand sample from the positive list of size K by using the probability distribution P.\nThis optimization made sense in the era of CPUs when compute was expensive but\nmight not make that much sense these days in the era of GPUs and TPUs, which we\nwill talk about in the following warning.\nStochastic Losses and GPUs\nA word of caution about the preceding stochastic losses. They were\ndeveloped for an earlier era of CPUs when it was cheap and easy\nto sample and exit if a negative sample was found. These days,\nwith modern GPUs, making branching decisions like this is harder\nbecause all the threads on the GPU core have to run the same\ncode over different data in parallel. That usually means both sides\nof a branch are taken in a batch, so less computational savings\noccur from these early exits. Consequently, branching code that\napproximates stochastic losses like W ARP and k-order statistic loss\nappear less efficient.\nWhat are we to do? We will show in Chapter 13  how to approxi‐\nmate these losses in code. Long story short, because of the way\nvector processors like GPUs tend to work by processing lots of\ndata in parallel uniformly, we have to find a GPU-friendly way\nto compute these losses. In the next chapter, we approximate the\nnegative sampling by generating a large batch of negatives and\neither scoring them all lower than the negative or looking for the\nmost egregious violating negative or both together as a blend of\nloss functions.\nk-order Statistic | 235",12387
102-BM25.pdf,102-BM25,"BM25\nWhile  much of this book is targeted at recommending items to users, search ranking\nis a close sister study. In the space of information retrieval, or search ranking for\ndocuments, best matching 25  (BM25) is an essential tool.\nBM25 is an algorithm used in information-retrieval systems to rank documents based\non their relevance to a given query. This relevance is determined by considering\nfactors like TF-IDF. It’s a bag-of-words retrieval function that ranks a set of docu‐\nments based on the query terms appearing in each document. It’s also a part of\nthe probabilistic relevance framework and is derived from the probabilistic retrieval\nmodel.\nThe BM25 ranking function calculates a score for each document based on the query.\nThe document with the highest score is considered the most relevant to the query.\nHere is a simplified version of the BM25 formula:\nscoreD,Q=∑\ni= 1n\nIDFqi*fqi,D*k1 + 1\nfqi,D+k1 *1 −b+b*D\navgdl\nThe elements of this formula are as follows:\n•D represents a document.•\n•Q is the query that consists of words q1,q2, . . . ,qn. •\n•fqi,D is the frequency of query term qi in document D. •\n•D is the length of (the number of words in) the document D. •\n•avgdl is the average document length in the collection. •\n•k1 and b are hyperparameters. k1 is a positive tuning parameter that calibrates the •\ndocument term frequency scaling. b is a parameter that determines the scaling\nby document length: b= 1 corresponds to fully scaling the term weight by the\ndocument length, while b= 0 corresponds to no length normalization.\n•IDFqi is the inverse document frequency of query term qi, which measures the •\namount of information the word provides (whether it’s common or rare across all\ndocuments). BM25 applies a variant of IDF that can be computed as follows:\nIDFqi= logN−nqi+ 0 . 5\nnqi+ 0 . 5\n236 | Chapter 12: Training for Ranking\nHere, N is the total number of documents in the collection, and nqi is the\nnumber of documents containing qi.\nSimply, BM25 combines both term frequency (how often a term appears in a docu‐\nment) and inverse document frequency (how much unique information a term pro‐\nvides) to calculate the relevance score. It also introduces the concept of document\nlength normalization, penalizing too-long documents and preventing them from\ndominating shorter ones, which is a common issue in simple TF-IDF models. The\nfree parameters k1 and b allow the model to be tuned based on the specific character‐\nistics of the document set.\nIn practice, BM25 provides a robust baseline for most information-retrieval tasks,\nincluding ad hoc keyword search and document similarity. BM25 is used in many\nopen source search engines, such as Lucene and Elasticsearch, and is the de facto\nstandard for what is often called full-text search .\nSo how might we integrate BM25 into the problems we discuss in this book? The\noutput from BM25 is a list of documents ranked by relevance to the given query, and\nthen LTR comes into play. Y ou can use the BM25 score as one of the features in an\nLTR model, along with other features that you believe might influence the relevance\nof a document to a query.\nThe general steps to combine BM25 with LTR for ranking are as follows:\n1.Retrieve a list of candidate documents . Given a query, use BM25 to retrieve a list 1.\nof candidate documents.\n2.Compute features for each document . Compute the BM25 score as one of the fea‐ 2.\ntures, along with other potential features. This could include various document-\nspecific features, query-document match features, user interaction features, etc.\n3.Train/evaluate the LTR model . Use these feature vectors and their corresponding 3.\nlabels (relevance judgments) to train your LTR model. Or, if you already have a\ntrained model, use it to evaluate and rank the retrieved documents.\n4.Rank . The LTR model generates a score for each document. Rank the documents 4.\nbased on these scores.\nThis combination of retrieval (with BM25) and ranking (with LTR) allows you to\nfirst narrow the potential candidate documents from a possibly very large collection\n(where BM25 shines) and then fine-tune the ranking of these candidates with a\nmodel that can consider more complex features and interactions (where LTR shines).\nBM25 | 237",4303
103-Multimodal Retrieval.pdf,103-Multimodal Retrieval,,0
104-Summary.pdf,104-Summary,"It is worth mentioning that the BM25 score can provide a strong baseline in text\ndocument retrieval, and depending on the complexity of the problem and the amount\nof training data you have, LTR may or may not provide significant improvements.\nMultimodal Retrieval\nLet’s  take another look at this retrieval method, as we can find some powerful\nleverage. Think back to Chapter 8 : we built a co-occurrence model, which illustrated\nhow articles referenced jointly in other articles share meaning and mutual relevance.\nBut how would you integrate search into this?\nY ou may think, “Oh, I can search the names of the articles. ” But that doesn’t quite\nutilize our co-occurrence model; it underleverages that joint meaning we discovered.\nA classic approach may be to use something like BM25 on article titles or articles.\nMore modern approaches may do a vector embedding of the query and article titles\n(using something like BERT or other transformer models). However, neither of these\nreally capture both sides of what we’re looking for.\nConsider instead the following approach:\n1.Search with the initial query via BM25 to get an initial set of “anchors. ”1.\n2.Search with each anchor as a query via your latent model(s).2.\n3.Train an LTR model to aggregate and rank the union of the searches.3.\nNow we’re using a true multimodal retrieval, leveraging multiple latent spaces! One\nadditional highlight in this approach is that queries are often out of distribution from\ndocuments with respect to encoder-based latent spaces. This means that when you\ntype Who’s the leader of Mozambique? , this question looks fairly dissimilar to the\narticle title (Mozambique) or the relevant sentence as of summer 2023 (“The new\ngovernment under President Samora Machel established a one-party state based on\nMarxist principles. ”)\nWhen the embeddings are not text at all, this method becomes even more powerful:\nconsider typing text to search for an item of clothing and hoping to see an entire\noutfit that goes with it.\nSummary\nGetting things in the right order is an important aspect of recommendation systems.\nBy now, you know that ordering is not the whole story, but it’s an essential step in the\npipeline. We’ve collected our items and put them in the right order, and all that’s left\nto do is send them off to the user.\n238 | Chapter 12: Training for Ranking\nWe started with the most fundamental concept, learning to rank, and compared it\nwith some traditional methods. We then got a big upgrade with W ARP and WSABIE.\nThat led us to the k-order statistic, which involves utilizing more careful probabilistic\nsampling. We finally wrapped up with BM25 as a powerful baseline in text settings.\nBefore we conquer serving, let’s put these pieces together. In the next chapter, we’re\ngoing to turn up the volume and build some playlists. This will be the most intensive\nchapter yet, so go grab a beverage and a stretch. We’ve got some work to do.\nSummary | 239",2984
105-Chapter 13. Putting It All Together Experimenting and Ranking.pdf,105-Chapter 13. Putting It All Together Experimenting and Ranking,,0
106-Keep It Simple.pdf,106-Keep It Simple,"CHAPTER 13\nPutting It All Together:\nExperimenting and Ranking\nIn the last few chapters, we have covered many aspects of ranking, including various\nkinds of loss functions as well as metrics for measuring the performance of ranking\nsystems. In this chapter, we will show an example of a ranking loss and ranking\nmetric on the Spotify Million Playlist dataset .\nThis chapter encourages a lot more experimentation and is more open-ended than\nthe previous ones, whose goal was to introduce concepts and infrastructure. This\nchapter, on the other hand, is written to encourage you to roll up your sleeves and\nengage directly with loss functions and writing metrics.\nExperimentation Tips\nBefore  we begin digging into the data and modeling, let’s cover some practices that\nwill make your life easier when doing a lot of experimentation and rapid iteration.\nThese are general guidelines that have made our experimentation faster. As a result,\nwe’re able to rapidly iterate toward solutions that help us reach our objectives.\nExperimental code is different from engineering code in that the code is written to\nexplore ideas, not for robustness. The goal is to achieve maximum velocity while not\nsacrificing too much in terms of code quality. So you should think about whether a\npiece of code should be thoroughly tested or whether this isn’t necessary because the\ncode is present only to test a hypothesis and then it will be thrown away. With that in\nmind, here are some tips. Keep in mind that these tips are the opinion of the authors,\ndeveloped over time, and are not hard-and-fast rules, just some flavored opinions\nthat some may disagree with.\n241",1669
107-Keep Track of Changes.pdf,107-Keep Track of Changes,"Keep It Simple\nIn terms of the overall structure of research code, it’s best to keep it as simple as\npossible. Try not to overthink too much in terms of inheritance and reusability\nduring the early stages of the lifecycle of exploration. At the start of a project, we\nusually don’t know what it needs yet, so the preference should be keeping the code\neasily readable and simple for debugging. That means you don’t have to focus too\nmuch on code reuse because at the early stage of a project, many code changes will\noccur while the structure of the model, data ingestion, and interaction of various\nparts of a system are being worked out. When the uncertainties have been worked\nout, then you can rewrite the code into a more robust form, but refactoring too early\nactually slows velocity.\nA general rule of thumb is that it is OK to copy code three times and then refactor\nout into a library the fourth time, because you’ll have seen enough use cases to justify\nthe reuse of code. If refactoring is done too early, you might not have seen enough use\ncases of a piece of code to cover the possible use cases that it might need to handle.\nDebug Print Statements\nIf you’ve read a number of ML research papers, you may expect your data to be fairly\nclean and orderly at the start of a project. However, real-world data can be messy,\nwith missing fields and unexpected values. Having lots of print functions allows you\nto print and visually inspect a sample of the data and also helps in crafting the input\ndata pipelines and transformations to feed the model. Also, printing sample outputs\nof the model is useful in making sure the output is as expected.\nThe most important places to include logging are the input and output schema\nbetween components of your system; these help you understand where reality may be\ndeviating from expectations. Later, you can make unit tests to ensure that refactoring\nof the model doesn’t break anything, but the unit tests can wait for when the model\narchitecture is stable. A good rule of thumb is to add unit tests when you want to\nrefactor code or reuse or optimize the code to preserve functionality or when the\ncode is stable and you want to ensure that it doesn’t break a build. Another good use\ncase of adding print statements is when you inevitably run into not-a-number (NaN)\nerrors when running training code.\n242 | Chapter 13: Putting It All Together: Experimenting and Ranking\nIn JAX, you can enable NaN debugging by using the following lines:\nfrom jax import config\nconfig.update(""jax_debug_nans"" , True)\n@jax.jit\ndef f(x):\n  jax.debug.print(""Debugging {x}"", x=x)\nThe debug NaNs configuration setting will rerun a jitted function if it finds any NaNs,\nand the debug print function will print the value of the tensors even inside a JIT. A\nregular print won’t work inside a JIT because it is not a compilable command and is\nskipped over during the tracing, so you have to use the debug print function instead,\nwhich does work inside a JIT.\nDefer Optimization\nIn research code, there is a lot of temptation to optimize early—in particular, focusing\non the implementation of your models or system to ensure they’re efficient computa‐\ntionally or the code is elegant. However, research code is written for higher velocity in\nexperimentation, not execution speed.\nOur suggestion is do not optimize too early unless it hinders research velocity. One\nreason for this is the system might not be complete, so optimizing one part might not\nmake sense if another part of the system is even slower and is the actual bottleneck.\nAnother reason is the part that you are optimizing might not make it to the final\nmodel, so all the optimization work might go to waste if the code is refactored away\nanyway.\nFinally, optimization might actually hinder the ability to modify or inject newer\ndesign choices in terms of architecture or functionality. Optimized code tends to\nhave certain choices that were made that fit the current structure of the data flow\nbut might not be amenable to further changes. For example, in the code for this\nchapter, one possible optimization choice would have been to batch together playlists\nof the same size so that the code might be able to run in larger batches. However,\nat this point of the experimentation, that optimization would have been premature\nand distracting because it might make the metrics code more complicated. Our gentle\nadvice is to defer optimization until after the bulk of experimentation has been done\nand the architecture, loss functions, and metrics have been chosen and settled upon.\nExperimentation Tips | 243",4657
108-Understand Metrics Versus Business Metrics.pdf,108-Understand Metrics Versus Business Metrics,"Keep Track of Changes\nIn research code, too many variables are probably at play for you to change them\none at a time to see their effects. This problem is particularly noticeable with larger\ndatasets that require a lot of runs to determine which change causes which effects. So,\nin general, fixing a number of parameters and changing the code bit by bit is still a\ngood idea so that you can keep track of the change that causes the most improvement.\nParameters have to be tracked, but so do the code changes.\nOne way to keep track of changes is through services such as Weights & Biases that\nwe discussed in Chapter 5 . Keeping track of the exact code that led to a change\nand the parameters is a good idea so that experiments can be reproduced and\nanalyzed. Especially with research code that changes so frequently and is sometimes\nnot checked in, you have to be diligent in keeping a copy of the code that produced a\nrun somewhere, and MLOps tools allow you to track code and hyperparameters.\nUse Feature Engineering\nUnlike  in academic papers, most applied research is interested in a good outcome\nrather than a theoretically beautiful result. We’re not shackled by purist views that the\nmodel has to learn everything about the data by itself. Instead, we’re pragmatic and\nconcerned about good outcomes.\nWe should not discard practices like feature engineering, especially when we have\nlittle data or are crunched for time and need decent results fast. Using feature engi‐\nneering means that if you know whether a handcrafted feature is correlated positively\nor negatively with an outcome like the ranking of an item, then by all means add\nthese engineered features to the data. An example in recommender systems is having\nan attribute of the item being scored that matches something in the user’s profile.\nSo, if an item has the same artist or album in the user’s playlist, we can return a\nBoolean True; otherwise, we return False. This extra feature simply helps the model\nconverge faster, and the model can still use other latent features such as embeddings\nto compensate if the hand-engineered features don’t do so well.\nIt is generally a good practice to ablate the hand-engineered features once in a while.\nTo do this, hold back an experiment without some features to see if those features\nhave become obsolete over time or if they still benefit the business metrics.\n244 | Chapter 13: Putting It All Together: Experimenting and Ranking",2476
109-Spotify Million Playlist Dataset.pdf,109-Spotify Million Playlist Dataset,"Ablation\nAblation  in ML applications is the practice of measuring the change\nin performance of a model when a particular feature is removed.\nIn computer vision applications, ablation often refers to blocking\npart of the image or view field to see how it impacts the model’s\nability to identify or segment data. In other kinds of ML, it can\nmean strategically removing certain features.\nOne  gotcha with ablation is what to replace the feature with. Sim‐\nply zeroing out  the feature can significantly skew the output of the\nmodel. This is called zero-ablation , and can force the model to\ntreat that feature out of distribution, which yields less believable\noutcomes. Instead, some advocate for mean-ablation, or taking the\naverage or most common value of that feature. This allows the\nmodel to see much more expected values, and reduce these risks.\nHowever, this fails to consider the most important aspects of the\nkinds of models we’ve been working on—latent high-order interac‐\ntions. One of the authors has investigated a deeper approach to\nablation called causal scrubbing , in which you fix the ablation value\nto be sampled from the posterior distribution produced by other\nfeature values, i.e., a value that “makes sense” with the rest of the\nvalues the model will see at that time.\nUnderstand Metrics Versus Business Metrics\nSometimes, as ML practitioners, we obsess over the best possible metrics our models\ncan achieve. However, we should temper that enthusiasm as the best ML metric might\nnot totally represent the business interests at hand. Furthermore, other systems that\ncontain business logic might sit on top of our models and modify the output. As\na result, it is best not to obsess too heavily over ML metrics and to do proper A/B\ntests that contain business metrics instead since that’s the main measure of a good\noutcome with ML.\nThe best possible circumstance is to find a loss function that aligns well or predicts\nthe relevant business metric. This, unfortunately, is often not easy to find, especially\nwhen the business metrics are nuanced or have competing priorities.\nPerform Rapid Iteration\nDon’t  be afraid to look at results of runs that are rather short. There’s no need to do\na full pass over the data at the beginning, when you are figuring out the interaction\nbetween a model architecture and the data. It’s OK to do some rapid runs with minor\ntweaks to see how they change the metrics over a short number of time steps. In the\nSpotify Million Playlist dataset, we tweaked the model architecture by using 100,000\nExperimentation Tips | 245\nplaylists before doing longer runs. Sometimes the changes can be so dramatic that the\neffects can be seen immediately, even at the first test-set evaluation.\nNow that we have the basics of experimental research coding covered, let’s hop over\nto the data and code and play a bit with modeling music recommendations.\nSpotify Million Playlist Dataset\nThe code for this section can be found in this book’s GitHub repo . The documenta‐\ntion for the data can be found at Spotify Million Playlist Dataset Challenge .\nThe first thing we should do is take a look at the data:\nless data/spotify_million_playlist_dataset/data/mpd.slice.0-999.json\nThat should produce the following output:\n{\n    ""info"": {\n        ""generated_on"" : ""2017-12-03 08:41:42.057563"" ,\n        ""slice"": ""0-999"",\n        ""version"" : ""v1""\n    },\n    ""playlists"" : [\n        {\n            ""name"": ""Throwbacks"" ,\n            ""collaborative"" : ""false"",\n            ""pid"": 0,\n            ""modified_at"" : 1493424000 ,\n            ""num_tracks"" : 52,\n            ""num_albums"" : 47,\n            ""num_followers"" : 1,\n            ""tracks"" : [\n                {\n                    ""pos"": 0,\n                    ""artist_name"" : ""Missy Elliott"" ,\n                    ""track_uri"" : ""spotify:track:0UaMYEvWZi0ZqiDOoHU3YI"" ,\n                    ""artist_uri"" : ""spotify:artist:2wIVse2owClT7go1WT98tk"" ,\n                    ""track_name"" : ""Lose Control (feat. Ciara & Fat Man Scoop)"" ,\n                    ""album_uri"" : ""spotify:album:6vV5UrXcfyQD1wu4Qo2I9K"" ,\n                    ""duration_ms"" : 226863,\n                    ""album_name"" : ""The Cookbook""\n                },\n     }\n }\nWhen encountering a new dataset, it is always important to look at it and plan which\nfeatures to use to generate recommendations for the data. One possible goal of the\nSpotify Million Playlist Dataset Challenge is to see if the next tracks in a playlist can\nbe predicted from the first five tracks in the playlist.\n246 | Chapter 13: Putting It All Together: Experimenting and Ranking\nIn this case, several features might be useful for the task. We have track, artist, and\nalbum universal resource identifiers (URIs), which are unique identifiers for tracks,\nartists, and albums, respectively. And we have artist and album names and names\nof playlists. The dataset also includes numerical features like duration of a track and\nthe number of followers in a playlist. Intuitively, the number of followers of a playlist\nshould not affect the ordering of tracks in a playlist, so you might want to look for\nbetter features before using these possibly uninformative ones. Looking at the overall\nstatistics of features, you can also obtain a lot of insight:\nless data/spotify_million_playlist_dataset/stats.txt\nnumber of playlists  1000000\nnumber of tracks 66346428\nnumber of unique tracks 2262292\nnumber of unique albums 734684\nnumber of unique artists 295860\nnumber of unique titles 92944\nnumber of playlists  with descriptions  18760\nnumber of unique normalized  titles 17381\navg playlist  length 66.346428\ntop playlist  titles\n  10000 country\n  10000 chill\n   8493 rap\n   8481 workout\n   8146 oldies\n   8015 christmas\n   6848 rock\n   6157 party\n   5883 throwback\n   5063 jams\n   5052 worship\n   4907 summer\n   4677 feels\n   4612 new\n   4186 disney\n   4124 lit\n   4030 throwbacks\nFirst of all, notice that the number of tracks is more than the number of playlists.\nThis implies that quite a few tracks might have very little training data. So the\ntrack_uri  might not be a feature that generalizes very well. On the other hand,\nthe album_uri  and artist_uri  would generalize because they would occur multiple\ntimes in different playlists. For the sake of code clarity, we will mostly work with the\nalbum_uri  and artist_uri  as the features that represent a track.\nIn previous “Putting It All Together” chapters, we demonstrated the use of content-\nbased features or text token-based features that may be used instead, but direct\nembedding features are the clearest for demonstrating ranking. In a real-world\nSpotify Million Playlist Dataset | 247",6775
110-Building the Training Data.pdf,110-Building the Training Data,"application, embedding features and content-based features may be concatenated\ntogether to form a feature that generalizes better for recommendation ranking. For\nthe purposes of this chapter, we will represent a track as the tuple of ( track_id ,\nalbum_id , artist_id ), where the ID is an integer representing the URI. We will build\ndictionaries that map from the URI to the integer ID in the next section.\nBuilding URI Dictionaries\nSimilar to Chapter 8 , we will first start by constructing a dictionary for all the URIs.\nThis dictionary allows us to represent the text URI as an integer for faster processing\non the JAX side, as we can easily look up embeddings from integers as opposed to\narbitrary URI strings.\nHere is the code for make_dictionary.py :\nimport glob\nimport json\nimport os\nfrom typing import Any, Dict, Tuple\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport tensorflow  as tf\nFLAGS = flags.FLAGS\n_PLAYLISTS  = flags.DEFINE_string (""playlists"" , None, ""Playlist json glob."" )\n_OUTPUT_PATH  = flags.DEFINE_string (""output"" , ""data"", ""Output path."" )\n# Required flag.\nflags.mark_flag_as_required (""playlists"" )\ndef update_dict (dict: Dict[Any, int], item: Any):\n    """"""Adds an item to a dictionary.""""""\n    if item not in dict:\n        index = len(dict)\n        dict[item] = index\ndef dump_dict (dict: Dict[str, str], name: str):\n  """"""Dumps a dictionary as json.""""""\n  fname = os.path.join(_OUTPUT_PATH .value, name)\n  with open(fname, ""w"") as f:\n    json.dump(dict, f)\ndef main(argv):\n    """"""Main function.""""""\n    del argv  # Unused.\n    tf.config.set_visible_devices ([], 'GPU')\n248 | Chapter 13: Putting It All Together: Experimenting and Ranking\n    tf.compat.v1.enable_eager_execution ()\n    playlist_files  = glob.glob(_PLAYLISTS .value)\n    track_uri_dict  = {}\n    artist_uri_dict  = {}\n    album_uri_dict  = {}\n    for playlist_file  in playlist_files :\n        print(""Processing "" , playlist_file )\n        with open(playlist_file , ""r"") as file:\n            data = json.load(file)\n            playlists  = data[""playlists"" ]\n            for playlist  in playlists :\n                tracks = playlist [""tracks"" ]\n                for track in tracks:\n                  update_dict (track_uri_dict , track[""track_uri"" ])\n                  update_dict (artist_uri_dict , track[""artist_uri"" ])\n                  update_dict (album_uri_dict , track[""album_uri"" ])\n    dump_dict (track_uri_dict , ""track_uri_dict.json"" )\n    dump_dict (artist_uri_dict , ""artist_uri_dict.json"" )\n    dump_dict (album_uri_dict , ""album_uri_dict.json"" )\nif __name__ == ""__main__"" :\n    app.run(main)\nWhenever a new URI is encountered, we simply increment a counter and assign that\nunique identifier to the URI. We do this for tracks, artists, and albums and save it as a\nJSON file.\nAlthough we could have used a data processing framework like PySpark for this, it is\nimportant to take note of the data size. If the data size is small, like a million playlists,\nit would just be faster to do it on a single machine. We should be wise about when to\nuse a big data processing framework, and for small datasets it can sometimes be faster\nto simply run the code on one machine instead of writing code that runs on a cluster.\nBuilding the Training Data\nNow  that we have the dictionaries, we can use them to convert the raw JSON playlist\nlogs into a more usable form for ML training. The code for this is in make_train‐\ning.py :\nimport glob\nimport json\nimport os\nfrom typing import Any, Dict, Tuple\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nimport numpy as np\nimport tensorflow  as tf\nSpotify Million Playlist Dataset | 249\nimport input_pipeline\nFLAGS = flags.FLAGS\n_PLAYLISTS  = flags.DEFINE_string (""playlists"" , None, ""Playlist json glob."" )\n_DICTIONARY_PATH  = flags.DEFINE_string (""dictionaries"" , ""data/dictionaries"" ,\n                   ""Dictionary path."" )\n_OUTPUT_PATH  = flags.DEFINE_string (""output"" , ""data/training"" , ""Output path."" )\n_TOP_K = flags.DEFINE_integer (""topk"", 5, ""Top K tracks to use as context."" )\n_MIN_NEXT  = flags.DEFINE_integer (""min_next"" , 10, ""Min number of tracks."" )\n# Required flag.\nflags.mark_flag_as_required (""playlists"" )\ndef main(argv):\n    """"""Main function.""""""\n    del argv  # Unused.\n    tf.config.set_visible_devices ([], 'GPU')\n    tf.compat.v1.enable_eager_execution ()\n    playlist_files  = glob.glob(_PLAYLISTS .value)\n    track_uri_dict  = input_pipeline .load_dict (\n      _DICTIONARY_PATH .value, ""track_uri_dict.json"" )\n    print(""%d tracks loaded""  % len(track_uri_dict ))\n    artist_uri_dict  = input_pipeline .load_dict (\n      _DICTIONARY_PATH .value, ""artist_uri_dict.json"" )\n    print(""%d artists loaded""  % len(artist_uri_dict ))\n    album_uri_dict  = input_pipeline .load_dict (\n      _DICTIONARY_PATH .value, ""album_uri_dict.json"" )\n    print(""%d albums loaded""  % len(album_uri_dict ))\n    topk = _TOP_K.value\n    min_next  = _MIN_NEXT .value\n    print(""Filtering out playlists with less than %d tracks""  % min_next )\n    raw_tracks  = {}\n    for pidx, playlist_file  in enumerate (playlist_files ):\n        print(""Processing "" , playlist_file )\n        with open(playlist_file , ""r"") as file:\n            data = json.load(file)\n            playlists  = data[""playlists"" ]\n            tfrecord_name  = os.path.join(\n              _OUTPUT_PATH .value, ""%05d.tfrecord""  % pidx)\n            with tf.io.TFRecordWriter (tfrecord_name ) as file_writer :\n              for playlist  in playlists :\n                  if playlist [""num_tracks"" ] < min_next :\n                      continue\n                  tracks = playlist [""tracks"" ]\n                  # The first topk tracks are all for the context.\n                  track_context  = []\n250 | Chapter 13: Putting It All Together: Experimenting and Ranking\n                  artist_context  = []\n                  album_context  = []\n                  # The rest are for predicting.\n                  next_track  = []\n                  next_artist  = []\n                  next_album  = []\n                  for tidx, track in enumerate (tracks):\n                      track_uri_idx  = track_uri_dict [track[""track_uri"" ]]\n                      artist_uri_idx  = artist_uri_dict [track[""artist_uri"" ]]\n                      album_uri_idx  = album_uri_dict [track[""album_uri"" ]]\n                      if track_uri_idx  not in raw_tracks :\n                          raw_tracks [track_uri_idx ] = track\n                      if tidx < topk:\n                          track_context .append(track_uri_idx )\n                          artist_context .append(artist_uri_idx )\n                          album_context .append(album_uri_idx )\n                      else:\n                          next_track .append(track_uri_idx )\n                          next_artist .append(artist_uri_idx )\n                          next_album .append(album_uri_idx )\n                  assert(len(next_track ) > 0)\n                  assert(len(next_artist ) > 0)\n                  assert(len(next_album ) > 0)\n                  record = tf.train.Example(\n                    features =tf.train.Features (feature={\n                      ""track_context"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=track_context )),\n                      ""album_context"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=album_context )),\n                      ""artist_context"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=artist_context )),\n                      ""next_track"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=next_track )),\n                      ""next_album"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=next_album )),\n                      ""next_artist"" : tf.train.Feature(\n                      int64_list =tf.train.Int64List (value=next_artist )),\n                    }))\n                  record_bytes  = record.SerializeToString ()\n                  file_writer .write(record_bytes )\n    filename  = os.path.join(_OUTPUT_PATH .value, ""all_tracks.json"" )\n    with open(filename , ""w"") as f:\n        json.dump(raw_tracks , f)\nif __name__ == ""__main__"" :\n    app.run(main)\nThis code reads in a raw playlist JSON file, converts the URIs from textual identifiers\nto the index in the dictionary, and filters out playlists that are under a minimum size.\nIn addition, we partition the playlist such that the first five elements are grouped\ninto the context, or user that we are recommending items for, and the next items,\nSpotify Million Playlist Dataset | 251",8850
111-Reading the Input.pdf,111-Reading the Input,"which are the items we wish to predict for a given user. We call the first five elements\nthe context  because they represent a playlist and because there won’t be a one-to-one\nmapping between a playlist and a user if a user has more than one playlist. We\nthen write each playlist as a TensorFlow example in a TensorFlow record file for use\nwith the TensorFlow data input pipeline. The records will always contain five tracks,\nalbums, and artists for the context and at least five more next tracks for learning the\ninference tasks of predicting the next tracks.\nWe use TensorFlow objects here because of their compatibility with\nJAX and to introduce some very convenient data formats.\nWe also store unique rows of tracks with all the features, which is mostly for debug‐\nging and display should we need to convert a track_uri  into a human-readable form.\nThis track data is stored in all_tracks.json .\nReading the Input\nThe input is then read via input_pipeline.py :\nimport glob\nimport json\nimport os\nfrom typing import Sequence , Tuple, Set\nimport tensorflow  as tf\nimport jax.numpy  as jnp\n_schema = {\n   ""track_context"" : tf.io.FixedLenFeature ([5], dtype=tf.int64),\n   ""album_context"" : tf.io.FixedLenFeature ([5], dtype=tf.int64),\n   ""artist_context"" : tf.io.FixedLenFeature ([5], dtype=tf.int64),\n   ""next_track"" : tf.io.VarLenFeature (dtype=tf.int64),\n   ""next_album"" : tf.io.VarLenFeature (dtype=tf.int64),\n   ""next_artist"" : tf.io.VarLenFeature (dtype=tf.int64),\n}\ndef _decode_fn (record_bytes ):\n  result = tf.io.parse_single_example (record_bytes , _schema)\n  for key in _schema.keys():\n    if key.startswith (""next""):\n      result[key] = tf.sparse.to_dense (result[key])\n  return result\ndef create_dataset (\n    pattern: str):\n    """"""Creates a spotify dataset.\n252 | Chapter 13: Putting It All Together: Experimenting and Ranking\n    Args:\n      pattern: glob pattern of tfrecords.\n    """"""\n    filenames  = glob.glob(pattern)\n    ds = tf.data.TFRecordDataset (filenames )\n    ds = ds.map(_decode_fn )\n    return ds\nWe use the TensorFlow data’s functionality to read and decode the TensorFlow\nrecords and examples. For that to work, we need to supply a schema, or a dictio‐\nnary, telling the decoder the names and types of features to expect. Since we have\npicked five tracks each for the context, we should expect five each of track_context ,\nalbum_context , and artist_context . However, since the playlists themselves are\nof variable lengths, we tell the decoder to expect variable-length integers for the\nnext_track , next_album , and next_artist  features.\nThe second part of input_pipeline.py  is for reusable input code to load the dictionaries\nand track metadata:\ndef load_dict (dictionary_path : str, name: str):\n    """"""Loads a dictionary.""""""\n    filename  = os.path.join(dictionary_path , name)\n    with open(filename , ""r"") as f:\n        return json.load(f)\ndef load_all_tracks (all_tracks_file : str,\n                    track_uri_dict , album_uri_dict , artist_uri_dict ):\n  """"""Loads all tracks.\n  """"""\n  with open(all_tracks_file , ""r"") as f:\n    all_tracks_json  = json.load(f)\n  all_tracks_dict  = {\n    int(k): v for k, v in all_tracks_json .items()\n  }\n  all_tracks_features  = {\n    k: (track_uri_dict [v[""track_uri"" ]],\n        album_uri_dict [v[""album_uri"" ]],\n        artist_uri_dict [v[""artist_uri"" ]])\n    for k,v in all_tracks_dict .items()\n  }\n  return all_tracks_dict , all_tracks_features\ndef make_all_tracks_numpy (all_tracks_features ):\n  """"""Makes the entire corpus available for scoring.""""""\n  all_tracks  = []\n  all_albums  = []\n  all_artists  = []\n  items = sorted(all_tracks_features .items())\n  for row in items:\nSpotify Million Playlist Dataset | 253",3776
112-Modeling the Problem.pdf,112-Modeling the Problem,"k, v = row\n    all_tracks .append(v[0])\n    all_albums .append(v[1])\n    all_artists .append(v[2])\n  all_tracks  = jnp.array(all_tracks , dtype=jnp.int32)\n  all_albums  = jnp.array(all_albums , dtype=jnp.int32)\n  all_artists  = jnp.array(all_artists , dtype=jnp.int32)\n  return all_tracks , all_albums , all_artists\nWe also supply a utility function to convert the all_tracks.json  file into the entire\ncorpus of tracks for scoring in the final recommendations. After all, the goal is to\nrank the entire corpus, given the first five context tracks, and see how well they match\nthe given next track data.\nModeling the Problem\nNext, let’s think of how we will model the problem. We have five context tracks,\neach with an associated artist and album. We know that we have more tracks than\nplaylists, so for now we will simply ignore the track_id  and just use the album_id\nand artist_id  as features. One strategy could be to use one-hot encoding for the\nalbum and artist, and this would work well, but one-hot encoding tends to lead to\nmodels with high precision but less generalization.\nAn alternate way to represent identifiers is to embed them—that is, to make a lookup\ntable to an embedding of a fixed size that is lower dimensional than the cardinality of\nthe identifiers. This embedding can be thought of as a low-rank approximation to the\nfull-rank matrix of identifiers. We covered low-rank embeddings in earlier chapters,\nand we use that concept here as features to represent the album and artists.\nTake a look at models.py , which contains the code for SpotifyModel :\nfrom functools  import partial\nfrom typing import Any, Callable , Sequence , Tuple\nfrom flax import linen as nn\nimport jax.numpy  as jnp\nclass SpotifyModel (nn.Module):\n    """"""Spotify model that takes a context and predicts the next tracks.""""""\n    feature_size  : int\n    def setup(self):\n        # There are too many tracks and albums so limit by hashing.\n        self.max_albums  = 100000\n        self.album_embed  = nn.Embed(self.max_albums , self.feature_size )\n        self.artist_embed  = nn.Embed(295861, self.feature_size )\n    def get_embeddings (self, album, artist):\n        """"""\n        Given track, album, artist indices return the embeddings.\n254 | Chapter 13: Putting It All Together: Experimenting and Ranking\n        Args:\n            album: ints of shape nx1\n            artist: ints of shape nx1\n        Returns:\n            Embeddings representing the track.\n        """"""\n        album_modded  = jnp.mod(album, self.max_albums )\n        album_embed  = self.album_embed (album_modded )\n        artist_embed  = self.artist_embed (artist)\n        result = jnp.concatenate ([album_embed , artist_embed ], axis=-1)\n        return result\nIn the setup code, notice that we have two embeddings, for the albums and the\nartists. We have a lot of albums, so we show one way to reduce the memory foot‐\nprint of album embeddings: take the mod of a smaller number than the number of\nembeddings so that multiple albums might share an embedding. If more memory\nis available, you can remove the mod, but this technique is demonstrated here as a\nway of getting some benefit of having an embedding for a feature with very large\ncardinality.\nThe artist is probably the most informative feature, and the data includes far fewer\nunique artists, so we have a one-to-one mapping between the artist_id  and the\nembeddings. When we convert the tuple of (album_id, artist_id)  to an embed‐\nding, we do separate lookups for each ID and then concatenate the embeddings and\nreturn one complete embedding to represent a track. If more playlist data becomes\navailable, you might also want to embed the track_id . However, given that we have\nmore unique tracks than playlists, the track_id  feature will not generalize well until\nwe have more playlist data and the track_id  could occur more often as observations.\nA general rule of thumb is that a feature should occur at least 100 times to be useful;\notherwise, the gradients for that feature will not be updated very often, and it might\nas well be a random number because it is initialized as such.\nIn the call  section, we do the heavy lifting of computing the affinity of a context to\nother tracks:\ndef __call__ (self,\n                 track_context , album_context , artist_context ,\n                 next_track , next_album , next_artist ,\n                 neg_track , neg_album , neg_artist ):\n        """"""Returns the affinity score to the context.\n        Args:\n            track_context: ints of shape n\n            album_context: ints of shape n\n            artist_context: ints of shape n\n            next_track: int of shape m\n            next_album: int of shape m\n            next_artist: int of shape m\n            neg_track: int of shape o\n            neg_album: int of shape o\nSpotify Million Playlist Dataset | 255\n            neg_artist: int of shape o\n        Returns:\n            pos_affinity: affinity of context to the next track of shape m.\n            neg_affinity: affinity of context to the neg tracks of shape o.\n        """"""\n        context_embed  = self.get_embeddings (album_context , artist_context )\n        next_embed  = self.get_embeddings (next_album , next_artist )\n        neg_embed  = self.get_embeddings (neg_album , neg_artist )\n        # The affinity of the context to the other track is simply the dot\n        # product of each context embedding with the other track's embedding.\n        # We also add a small boost if the album or artist match.\n        pos_affinity  = jnp.max(jnp.dot(next_embed , context_embed .T), axis=-1)\n        pos_affinity  = pos_affinity  + 0.1 * jnp.isin(next_album , album_context )\n        pos_affinity  = pos_affinity  + 0.1 * jnp.isin(next_artist , artist_context )\n        neg_affinity  = jnp.max(jnp.dot(neg_embed , context_embed .T), axis=-1)\n        neg_affinity  = neg_affinity  + 0.1 * jnp.isin(neg_album , album_context )\n        neg_affinity  = neg_affinity  + 0.1 * jnp.isin(neg_artist , artist_context )\n        all_embeddings  = jnp.concatenate (\n        [context_embed , next_embed , neg_embed ], axis=-2)\n        all_embeddings_l2  = jnp.sqrt(\n        jnp.sum(jnp.square(all_embeddings ), axis=-1))\n        context_self_affinity  = jnp.dot(jnp.flip(\n        context_embed , axis=-2), context_embed .T)\n        next_self_affinity  = jnp.dot(jnp.flip(\n        next_embed , axis=-2), next_embed .T)\n        neg_self_affinity  = jnp.dot(jnp.flip(neg_embed , axis=-2), neg_embed .T)\n        return (pos_affinity , neg_affinity ,\n                context_self_affinity , next_self_affinity , neg_self_affinity ,\n                all_embeddings_l2 )\nLet’s dig into this a bit since this is the core of the model code. The first part is pretty\nstraightforward: we convert the indices into embeddings by looking up the album\nand artist embedding and concatenating them as a single vector per track. It is in\nthis location that you would add in other dense features by concatenation, or convert\nsparse features to embeddings as we have done.\nThe next part computes the affinity of the context to the next tracks. Recall that\nthe context is composed of the first five tracks, and the next track is the rest of the\nplaylist to be computed. We have several choices here for representing the context\nand computing the affinity.\nFor the affinity of the context, we have chosen the simplest form of affinity, that of a\ndot product. The other consideration is how we treat the context, since it is composed\nof five tracks. One possible way is to average all the context embeddings and use the\n256 | Chapter 13: Putting It All Together: Experimenting and Ranking",7767
113-Framing the Loss Function.pdf,113-Framing the Loss Function,"average as the representation for the context. Another way is to find the track with\nthe maximal affinity as the closest track in the context to that of the next track.\nDetails on various options can be found in “ Affinity Weighted Embedding”  by Jason\nWeston et al. We have found that if a user has diverse interests, finding the max\naffinity doesn’t update the context embeddings in the same direction as the next\ntrack, as using the mean embedding does. In the case of playlists, the mean context\nembedding vector should function just as well because playlists tend to be on a single\ntheme.\nNotice that we compute the affinity for the negative tracks as well. This is because\nwe want the next tracks to have more affinity to the context than the negative tracks.\nIn addition to the affinity of the context and next tracks to the context, we also\ncompute the L2 norm of the vectors as a way to regularize the model so it does not\noverfit on the training data. We also reverse the embedding vectors and compute\nwhat we call  self-affinity , or the affinity of the context, next, and negative embeddings\nto themselves, simply by reversing the list of vectors and taking the dot product. This\ndoes not exhaustively compute all the affinities of the set with itself; this again is left\nas an exercise for you as it builds intuition and skill in using JAX.\nThe results are then returned as a tuple to the caller.\nFraming the Loss Function\nNow, let’s look at train_spotify.py . We will skip the boilerplate code and just look at the\nevaluation and training steps:\ndef eval_step (state, y, all_tracks , all_albums , all_artists ):\n    result = state.apply_fn (\n            state.params,\n            y[""track_context"" ], y[""album_context"" ], y[""artist_context"" ],\n            y[""next_track"" ], y[""next_album"" ], y[""next_artist"" ],\n            all_tracks , all_albums , all_artists )\n    all_affinity  = result[1]\n    top_k_scores , top_k_indices  = jax.lax.top_k(all_affinity , 500)\n    top_tracks  = all_tracks [top_k_indices ]\n    top_artists  = all_artists [top_k_indices ]\n    top_tracks_count  = jnp.sum(jnp.isin(\n      top_tracks , y[""next_track"" ])).astype(jnp.float32)\n    top_artists_count  = jnp.sum(jnp.isin(\n      top_artists , y[""next_artist"" ])).astype(jnp.float32)\n    top_tracks_recall  = top_tracks_count  / y[""next_track"" ].shape[0]\n    top_artists_recall  = top_artists_count  / y[""next_artist"" ].shape[0]\n    metrics = jnp.stack([top_tracks_recall , top_artists_recall ])\n    return metrics\nSpotify Million Playlist Dataset | 257\nThe first piece of code is the evaluation step. To compute the affinities of the entire\ncorpus, we pass in the album and artist indices for every possible track in the corpus\nto the model and then sort them using jax.lax.top_k . The first two lines are the\nscoring code for recommending the next tracks from the context during recommen‐\ndations. LAX is a utility library that comes with JAX that contains functions outside\nof the NumPy API that are handy to work with vector processors like GPUs and\nTPUs. In the Spotify Million Playlist Dataset Challenge, one of the metrics is the\nrecall@k at the artist and track level. For the tracks, the isin  function returns the\ncorrect metric of the intersection of the next tracks and the top 500 scoring tracks\nof the corpus divided by the size of the set of next tracks. This is because the tracks\nare unique in the corpus. However, JAX’s isin  doesn’t support making the elements\nunique, so for the artist recall metric, we might count artists in the recall set more\nthan once. For the sake of computational efficiency, we use the multiple counts\ninstead so that the evaluation might be computed quickly on the GPU so as not to\nstall the training pipeline. On a final evaluation, we might want to move the dataset to\na CPU for a more accurate metric.\nWe use Weights & Biases again to track all the metrics, as depicted in Figure 13-1 . Y ou\ncan see how they fare with each other over several experiments:\nFigure 13-1. Weights & Biases experiment tracking\nNext, we will look at the loss functions, another juicy part that you can experiment\nwith in the exercises at the end of the chapter:\ndef train_step (state, x, regularization ):\n    def loss_fn(params):\n        result = state.apply_fn (\n            params,\n            x[""track_context"" ], x[""album_context"" ], x[""artist_context"" ],\n            x[""next_track"" ], x[""next_album"" ], x[""next_artist"" ],\n            x[""neg_track"" ], x[""neg_album"" ], x[""neg_artist"" ])\n      pos_affinity  = result[0]\n      neg_affinity  = result[1]\n      context_self_affinity  = result[2]\n      next_self_affinity  = result[3]\n      neg_self_affinity  = result[4]\n      all_embeddings_l2  = result[5]\n258 | Chapter 13: Putting It All Together: Experimenting and Ranking\n      mean_neg_affinity  = jnp.mean(neg_affinity )\n      mean_pos_affinity  = jnp.mean(pos_affinity )\n      mean_triplet_loss  = nn.relu(1.0 + mean_neg_affinity  - mean_pos_affinity )\n      max_neg_affinity  = jnp.max(neg_affinity )\n      min_pos_affinity  = jnp.min(pos_affinity )\n      extremal_triplet_loss  = nn.relu(\n                              1.0 + max_neg_affinity  - min_pos_affinity\n                                )\n      context_self_affinity_loss  = jnp.mean(nn.relu(0.5 - context_self_affinity ))\n      next_self_affinity_loss  = jnp.mean(nn.relu(\n                                0.5 - next_self_affinity )\n                                )\n      neg_self_affinity_loss  = jnp.mean(nn.relu(neg_self_affinity ))\n      reg_loss  = jnp.sum(nn.relu(all_embeddings_l2  - regularization ))\n      loss = (extremal_triplet_loss  + mean_triplet_loss  + reg_loss  +\n              context_self_affinity_loss  + next_self_affinity_loss  +\n              neg_self_affinity_loss )\n      return loss\n    grad_fn = jax.value_and_grad (loss_fn)\n    loss, grads = grad_fn(state.params)\n    new_state  = state.apply_gradients (grads=grads)\n    return new_state , loss\nWe have several losses here, some directly related to the main task and others that\nhelp with regularization and generalization.\nWe initially started with the mean_triplet_loss , which is simply a loss that states\nthat the positive affinity, or the affinity of the context tracks to the next tracks, should\nbe one more than the negative affinity, or the affinity of the context tracks to the\nnegative tracks. We will discuss how we experimented to obtain the other auxiliary\nloss functions.\nExperiment tracking, depicted in Figure 13-2 , is important in the process of improv‐\ning the model, as is reproducibility. We have tried as much as possible to make the\ntraining process deterministic by using random-number generators from JAX that\nare reproducible by using the same starting random-number generator seed.\nSpotify Million Playlist Dataset | 259\nFigure 13-2. Track recall experiments\nWe started with the mean_triplet_loss  and reg_loss , which is the regularization\nloss as a good baseline. These two losses simply make sure that the mean positive\naffinity of the context to the next track is one more than the negative affinity of the\ncontext to the negative tracks, and that the L2 norm of the embeddings does not\nexceed the regularization thresholds. These correspond to the metrics that did the\nworst. Notice that we do not run the experiment for the entire dataset. This is because\nfor rapid iteration, it might be faster to just run on a smaller number of steps first and\ncompare before interleaving occasionally with longer runs that use the entire dataset.\nThe next loss we added was the max_neg_affinity  and the min_pos_affinity . This\nloss was inspired in part by “Efficient Coordinate Descent or Ranking with Domina‐\ntion Loss”  by Mark A. Stevens and “Learning to Rank Recommendations with the\nk-Order Statistic Loss”  by Jason Weston et al. However, we do not use the entire\nnegative set but merely a subsample. Why? Because the negative set is noisy. Just\nbecause a user hasn’t added a particular track to a playlist doesn’t mean that the track\nis not relevant to the playlist. Maybe the user hasn’t heard the track yet, so the noise\nis due to lack of exposure. We also do not do the sampling step as discussed in the\nk-order statistic loss paper because sampling is CPU friendly but not GPU friendly. So\nwe combine ideas from both papers and take the largest negative affinity and make it\none less than the smallest positive affinity. The addition of this loss on the extremal\ntracks from both the next and negative sets gave us the next boost in performance in\nour experiments.\n260 | Chapter 13: Putting It All Together: Experimenting and Ranking",8760
114-Part IV. Serving.pdf,114-Part IV. Serving,"Finally, we added the self-affinity losses. These ensure that tracks from the context\nand next track sets have affinities of at least 0.5 and that the negative track affinities\nare at most 0. These are dot-product affinities and are more absolute as opposed to\nthe relative positive and negative affinities that make the positive affinity one more\nthan the negative affinities. In the long run, they didn’t help much, but they did help\nthe model converge faster in the beginning. We left them in because they still offer\nsome improvement on the evaluation metrics on the last training step. This wraps\nup the explanatory part of this “Putting It All Together” chapter. Now comes the fun\npart, the exercises!\nExercises\nWe offer a lot of exercises because playing with the data and code is helpful in\nbuilding out your intuition about different loss functions and ways of modeling\nthe user. Also, thinking about how to write the code allows you to improve your\nproficiency with using JAX. So we have a list of helpful exercises to try out that are\nfun and will help you understand the material provided in this book.\nTo wrap up this chapter, here are some interesting exercises to experiment with.\nDoing them should give you lots of intuition about loss functions and the way JAX\nworks, as well as a feel for the experimental process.\nHere are some easy exercises to start with:\n•Try out different optimizers (e.g., ADAM, RMSPROP).•\n•Try changing the feature sizes.•\n•Add in duration as a feature (take care on normalization!).•\n•What if you use cosine distance for inference and dot product for training?•\n•Add in a new metric, like NDCG.•\n•Play with distribution of positive versus negative affinities in the loss.•\n•Hinge loss with the lowest next track and the highest negative track.•\nContinue exploring with these more difficult exercises:\n•Try using the track names as features and see if they help generalize.•\n•What happens if you use a two-layer network for affinity?•\n•What happens if you use an LSTM to compute affinity?•\n•Replace track embeddings with correlation.•\n•Compute all the self-affinities in a set.•\nExercises | 261\nSummary\nWhat  does it mean to replace an embedding with a feature? In our example of posi‐\ntive and negative affinity, we used the dot product to compute the affinity between\ntwo entities, such as two tracks, x and y. Rather than having the features as latent,\nrepresented by embeddings, an alternative is to manually construct features that\nrepresent the affinity between the two entities, x and y. As covered in Chapter 9 , this\ncan be log counts or Dice correlation coefficient or mutual information.\nSome kind of counting feature can be made and then stored in a database. Upon\ntraining and inference, the database is looked up for each entity x and y, and the\naffinity scores are then used instead of or in conjunction with the dot product that\nis being learned. These features tend to be more precise but have less recall than an\nembedding representation. The embedding representation, being of low rank, has the\nability to generalize better and improve recall. Having counting features is synergistic\nwith embedding features because we can simultaneously improve precision with the\nuse of precise counting features and, at the same time, improve recall with the help of\nlow-rank features like embeddings.\nFor computing all n2 affinities of tracks to other tracks in a set, consider using JAX’s\nvmap  function. vmap  can be used to convert code that, for example, computes one\ntrack’s affinity with all the other tracks and makes it run for all tracks versus all other\ntracks.\nWe hope that you have enjoyed playing with the data and code and that your skill in\nwriting recommender systems in JAX has improved considerably after trying these\nexercises!\n262 | Chapter 13: Putting It All Together: Experimenting and Ranking\nPART IV\nServing\nWell, you can’t recommend that! Why sometimes the best recommendations aren’t right.\nOne of the authors, Bryan, has a big question for the Amazon recommendation\nteam: “Exactly how many vacuum cleaners do you think I need?” Just because\nBryan bought the fancy Dyson to clean up after his dog doesn’t mean he’s soon\ngoing to buy a second one, and yet his Amazon home page seems hell-bent on\nrecommending it. The reality is you’ll always need to include business logic—or basic\nhuman logic—that you want to include in the flow of your recommendation system\nto prevent silliness. Whether you’re facing contextually inappropriate recommenda‐\ntions, business infeasible recommendations, or simply the necessity to keep the set\nof recommendations a bit less monomaniacal, the last-step ordering can crucially\nimprove recommendations.\nBut hold your horses! Don’t think the ordering step is all switch cases and manually\noverriding your recommendation system. A synergy needs to exist between your\nranking and your serving. Bryan also has a story about a certain query-based recom‐\nmender he built for clothes: he wanted to implement a super-simple diversity filter\non his recommendations—checking that the clothes recommended were of different\nmerchandise classes. He made the output of his scoring model stack-rank the recom‐\nmendations by merchandise class, so he could pick a few from each to serve. Lo and\nbehold, the first week in production he was recommended 3, 4, even 5 backpacks\nout of 10 recommendations. Studious as users may be, this seemed erroneous and\nrequired a bit of QA. His error? Backpacks can be a member of up to three merch\nclasses, so they were sneaking into several diversity classes!\nTricky problems where the theory meets production recommendations are the sub‐\nject of this part of the book. We’ll talk about diverse recommendations as in this\nexample, but we’ll also discuss other important business priorities that factor into the\nserving part of a recommendation pipeline.",5960
115-Hard Ranking.pdf,115-Hard Ranking,"CHAPTER 14\nBusiness Logic\nBy now, you may be thinking, “Y es, our algorithmic ranking and recommendation\nhas arrived! Personalization for every user with latent understanding is how we run\nour business. ” Unfortunately, the business is rarely this simple.\nLet’s take a really straightforward example, a recipe recommendation system. Con‐\nsider a user who simply hates grapefruit (one of the authors of this book really\ndoes) but may love a set of other ingredients that go well with grapefruit: asparagus,\navocado, banana, butter, cashews, champagne, chicken, coconut, crab, fish, ginger,\nhazelnut, honey, lemon, lime, melon, mint, olive oil, onion, orange, pecan, pineapple,\nraspberry, rum, salmon, seaweed, shrimp, star anise, strawberry, tarragon, tomato,\nvanilla, wine, and yogurt. These ingredients are the most  popular to pair with grape‐\nfruit, and the user loves almost all of these.\nWhat’s the right way for the recommender to handle this case? It may seem like this\nis something that collaborative filtering (CF), latent features, or hybrid recommenda‐\ntions would catch. However, if the user likes all these shared flavors, the item-based\nCF model would not catch this well. Similarly, if the user truly hates  grapefruit, latent\nfeatures may not be sufficient to truly avoid it.\nIn this case, the simple approach is a great one: hard avoids . In this chapter, we’ll\ntalk about some of the intricacies of business logic intersecting the output of your\nrecommendation system.\nInstead of attempting to learn exceptions as part of the latent features that the model\nutilizes when making recommendations, it’s more consistent and simple to integrate\nthese business rules as an external step via deterministic logic. As an example: the\nmodel could remove all grapefruit cocktails that are retrieved instead of attempting to\nlearn to rank them lower.\n265",1886
116-Implementing Avoids.pdf,116-Implementing Avoids,"Hard Ranking\nY ou can come up with a lot of examples of these phenomena when you start thinking\nof situations similar to our grapefruit scenario. Hard ranking  usually refers to one of\ntwo kinds of special ranking rules:\n•Explicitly removing some items from the list before ranking.•\n•Using a categorical feature to rank the results by category. (Note that this can•\neven be done for multiple features to achieve a hierarchical hard ranking.)\nHave you ever observed any of the following?\n•A user bought a sofa. The system continues to recommend sofas to this user even•\nthough they won’t need a sofa for the next five years.\n•A user buys a birthday gift for a friend interested in gardening. Then the ecom‐•\nmerce site keeps recommending gardening tools despite the user having no\ninterest in it.\n•A parent wants to buy a toy for their child. But when the parent goes to the•\nwebsite where they usually buy toys, the site recommends several toys for a child\na few years younger—the parent hasn’t purchased from the site since the child\nwas that age.\n•A runner experiences serious knee pain and determines they can no longer go•\non long runs. They switch to cycling, which is lower impact. However, their local\nmeetup recommendations are still all running oriented.\nAll of these cases can be relatively easy to deal with via deterministic logic. For these\nsituations, we would prefer not to try to learn these rules via ML. We should assume\nthat for these types of scenarios, we will get low signal about these preferences:\nnegative implicit feedback is often lower in relevance, and many of the situations\nlisted are represented by details that you want the system to learn once and for all.\nAdditionally, in some of the previous examples, it can be upsetting or harmful to a\nrelationship with a user to have the preferences not respected.\nThe name for these preferences is avoids —or sometimes constraints, overrides, or\nhard rules. Y ou should think of them as explicit expectations of the system: “Don’t\nshow me recipes with grapefruit, ” “No more sofas, ” “I don’t like gardening, ” “My child\nis older than 10 now, ” and “Don’t show me trail runs. ”\nLearned Avoids\nNot all business rules are such obvious avoids that derive from explicit user feedback,\nand some derive from explicit feedback not directly related to specific items. It’s\n266 | Chapter 14: Business Logic\nimportant to include a wide variety of avoids when considering serving recommen‐\ndations.\nFor the sake of simplicity, let’s assume you’re building a fashion recommender system.\nExamples of more subtle avoids include the following:\nAlready owned items\nThese are items that users really need to purchase only once—for example,\nclothing users have bought through your platform or told you they already own.\nCreating a virtual closet  might be a way to ask users to tell you what they have, to\nassist in these avoids.\nDisliked features\nThese are features of items that the user can indicate disinterest in. During an\nonboarding questionnaire, you may ask users if they like polka dots or if they\nhave a favorite color palette. These are explicitly indicated pieces of feedback that\ncan be used for avoids.\nIgnored categories\nThis is a category or group of items that doesn’t resonate with the user. This can\nbe implicit but learned outside the primary recommender model. Maybe the user\nhas never clicked the Dresses category on your ecommerce website because they\ndon’t enjoy wearing them.\nLow-quality items\nOver time, you’ll learn that some items are simply low quality for most users. Y ou\ncan detect this via a high number of returns or low ratings from buyers. These\nitems ultimately should be removed from inventory, but in the meantime, it’s\nimportant to include them as avoids for all but the strongest signal of match.\nThese additional avoids can be implemented easily during the serving stage and can\neven include simple models. Training linear models to capture some of these rules\nand then applying them during serving can be a useful and reliable mechanism for\nimproving ranking. Note that the small models perform very fast inference, so little\nnegative impact usually results from including them in the pipeline. For larger-scale\nbehavior trends or higher-order factors, we expect our core recommendation models\nto learn these ideas.\nHand-Tuned Weights\nOn the other side of the spectrum of avoids is hand-tuned ranking . This technique\nwas popular in earlier days of search ranking, when humans would use analytics and\nobservation to determine what they thought were the most important features in a\nranking and then craft a multiobjective ranker. For example, flower stores may rank\nhigher in early May as many users search for Mother’s Day gifts. Since there could be\nHand-Tuned Weights | 267\nmany variable elements to track, these kinds of approaches don’t scale well and have\nbeen largely deemphasized in modern recommendation ranking.\nHowever, hand-tuned ranking can be incredibly useful as an avoid . While technically\nit’s not an avoid, we sometimes still call it that. An example of this in practice is\nto know that new users like to start with a lower-priced item while they’re learning\nwhether your shipping is trustworthy. A useful technique is to then uprank lower-\npriced items before the first order.\nWhile it may feel bad to consider building a hand-tuned ranking, it’s important to\nnot count this technique out. It has a place and is often a great place to start. One\ninteresting human-in-the-loop application of this kind of technique is for hand-tuned\nranking by experts. Back to our fashion recommender, a style expert may know that\nthis summer’s trending color is mauve, especially among the younger generation.\nThen can positively influence user satisfaction if the expert ranks these mauve items\nup for users in the right age persona.\nInventory Health\nA unique and somewhat contentious side of hard ranking is inventory health. Notori‐\nously hard to define, inventory health  estimates how good the existing inventory is for\nsatisfying user demand.\nLet’s  take a quick look at one way to define inventory health, via affinity scores and\nforecasting. We can do this by leveraging a demand forecast, which is an incredibly\npowerful and popular way to optimize the business: what are the expected sales in\neach category over the next N time periods? Building these forecasting models is\noutside the scope of this book, but the core ideas are well captured in the famous\nbook “Forecasting: Principles and Practice”  by Rob Hyndman and George Athanaso‐\npoulos (Otexts). For the sake of our discussion, assume that you’re able to roughly\napproximate the number of socks you’ll sell over the next month, broken down by\nsize and usage type. This can be a really instructive estimate for the number of socks\nof various types you should have on hand.\nHowever, it doesn’t stop there; inventory may be finite, and in practice inventory is\noften a major constraint on businesses that sell physical goods. With that caveat, we\nhave to turn to the other side of the market demand. If our demand outstrips our\navailability, we are ultimately disappointing users who don’t have access to the item\nthey desired.\nLet’s take an example of selling bagels; you’ve calculated average demand for poppy\nseed, onion, asiago cheese, and egg. On any given day, many customers will come to\nbuy a bagel with a clear preference in mind, but will you have enough of that bagel?\nEvery bagel you don’t sell is wasted; people like fresh bagels. This means that the\nbagels you recommend to each person are dependent on good inventory. Some users\n268 | Chapter 14: Business Logic\nare less picky; they can get one of two or three of the options and be just as happy. In\nthat case, it’s better to give them another bagel option and save the lowest inventory\nfor the picky ones. This is a kind of model refinement called optimization , which\nhas a huge number of techniques. We won’t get into optimization techniques, but\nbooks on mathematical optimization or operations research will provide direction.\nAlgorithms for Optimization  by Mykel J. Kochenderfer and Tim A. Wheeler (MIT\nPress) is a good place to start.\nInventory health ties back to hard ranking, because actively managing inventory\nas part of your recommendations is an incredibly important and powerful tool.\nUltimately, inventory optimization will degrade the perceived performance of your\nrecommendations, but by including it as part of your business rules, the overall\nhealth of your business and recommender system improves. This is why it is some‐\ntimes called global optimization .\nThe reason that these methods stir up heated discussions is that not everyone agrees\nthat the quality of recommendations for some users should be depressed to improve\nthose for the “greater good. ” Health of the marketplace and average satisfaction are\nuseful metrics to consider, but ensure that these are aligned with the north-star\nmetrics for the recommendation system at large.\nImplementing Avoids\nThe simplest approach to handling avoids is via downstream filtering. To do this,\nyou’ll want to apply the avoid rules for the user before the recommendations are\npassed along from the ranker to the user. Implementing this approach looks some‐\nthing like this:\nimport pandas as pd\ndef filter_dataframe (df: pd.DataFrame , filter_dict : dict):\n    """"""\n    Filter a dataframe to exclude rows where columns have certain values.\n    Args:\n        df (pd.DataFrame): Input dataframe.\n        filter_dict (dict): Dictionary where keys are column names\n        and values are the values to exclude.\n    Returns:\n        pd.DataFrame: Filtered dataframe.\n    """"""\n    for col, val in filter_dict .items():\n        df = df.loc[df[col] != val]\n    return df\nfilter_dict  = {'column1' : 'value1' , 'column2' : 'value2' , 'column3' : 'value3' }\nImplementing Avoids | 269\ndf = df.pipe(filter_dataframe , filter_dict )\nAdmittedly, this is a trivial but also relatively naive attempt at avoids. First, working\npurely in pandas will limit some of the scalability of your recommender, so let’s\nconvert this to JAX:\nimport jax\nimport jax.numpy  as jnp\ndef filter_jax_array (arr: jnp.array, col_indices : list, values: list):\n    """"""\n    Filter a jax array to exclude rows where certain columns have certain values.\n    Args:\n        arr (jnp.array): Input array.\n        col_indices (list): List of column indices to filter on.\n        values (list): List of corresponding values to exclude.\n    Returns:\n        jnp.array: Filtered array.\n    """"""\n    assert len(col_indices ) == len(values),\n    masks = [arr[:, col] != val for col, val in zip(col_indices , values)]\n    total_mask  = jnp.logical_and (*masks)\n    return arr[total_mask ]\nBut there are deeper issues. The next issue you may face is where that collection of\navoids is stored. An obvious place is somewhere like a NoSQL database keyed on\nusers, and then you can get all of the avoids as a simple lookup. This is a natural\nuse of feature stores, as you saw in “Feature Stores” on page 90. Some avoids may be\napplied in real time, while others are learned upon user onboarding. Feature stores\nare a great place to house avoids.\nThe next potential gotcha with our naive filter is that it doesn’t naturally extend\nto covariate avoids, or more complicated avoid scenarios. Some avoids are actually\ndependent on context—a user who doesn’t wear white after Labor Day, users who\ndon’t eat meat on Fridays, or coffee-processing methods that don’t mesh well with\ncertain brewers. All of these require conditional logic. Y ou might think that your\npowerful and effective recommendation system model can certainly learn these\ndetails, but this is true only sometimes. The reality is that many of these kinds of\nconsiderations are lower signal than the large-scale concepts your recommendation\nsystem should be learning, and thus are hard to learn consistently. Additionally, these\nkinds of rules are often ones you should require, as opposed to remain optimistic\nabout. For that reason, you often should explicitly specify such restrictions.\n270 | Chapter 14: Business Logic",12317
117-Diversification of Recommendations.pdf,117-Diversification of Recommendations,"This specification can often be achieved by explicit deterministic algorithms that\nimpose these requirements. For the coffee problem, one of the authors hand-built\na decision stump to handle a few bad combinations of coffee roast features and\nbrewers— anaerobic espresso?! Yuck!\nOur other two examples (not wearing white after Labor Day and not eating meat on\nFridays), however, are a bit more nuanced. An explicit algorithmic approach may be\ntricky to handle. How do we know that a user doesn’t eat meat on Fridays during one\nperiod of the year?\nFor these use cases, model-based avoids can impose these requirements.\nModel-Based Avoids\nIn our quest to include more complicated rules and potentially learn them, we may\nsound like we’re back in the realm of retrieval. Unfortunately, even with models like\nwide-and-deep with lots of parameters doing both user modeling and item modeling,\nlearning such high-level relationships can be tricky.\nWhile most of this book has focused on working fairly large and deep, this part of\nrecommendation systems is well suited for simple models. For feature-based binary\npredictions (should this be recommended), we certainly have a zoo of good options.\nThe best approach would obviously depend heavily on the number of features\ninvolved in implementing the avoid you wish to capture. It’s useful to remember\nthat many avoids that we’re considering in this section start out as assumptions or\nhypotheses: we think some users may not wear white after Labor Day, and then\nattempt to find features that model this outcome well. In this way, it can be more\ntractable using extremely simple regression models to find covarying features with\nthe outcome in question.\nAnother related piece of this puzzle is  latent representations. For our Friday vegetari‐\nans, we may be trying to infer a particular persona that we know has this rule. That\npersona is a latent feature that we hope to map from other attributes. It’s important to\nbe careful with this kind of modeling (in general, personas can be a bit nuanced and\nworthy of thoughtful decision making), but it can be quite helpful. It may seem like\nthe user-modeling parts of your large recommender model should learn these—and\nthey can! A useful trick is to pull forward personas learned from that model and\nregress them against hypothesized avoids to allow for more signal. However, the other\nmodel doesn’t always learn these personas because our loss functions for retrieval\nrelevance (and downstream for ranking) are attempting to parse out relevance for\nindividual users from the latent persona features—which may predict these avoids\nonly amid context features.\nModel-Based Avoids | 271\nAll in all, implementing the avoids is both very easy and very hard. When building\nproduction recommendation systems, the journey is not over when you get to serv‐\ning; many models factor into the final step of the process.\nSummary\nSometimes you need to rely on more classic approaches to ensuring that the recom‐\nmendations you’re sending downstream are satisfying essential rules of your business.\nLearning explicit or subtle lessons from your users can be turned into simple strate‐\ngies to continue to delight them.\nHowever, this is not the end of our serving challenge. Another kind of downstream\nconsideration is related to the kind of filtering we’ve done here but derives from user\npreference and human behavior. Ensuring that recommendations are not repeated,\nrote, and redundant is the subject of the next chapter on diversity in recommenda‐\ntions. We will also discuss how to balance multiple priorities simultaneously when\ndetermining exactly what to serve.\n272 | Chapter 14: Business Logic\nCHAPTER 15\nBias in Recommendation Systems\nWe’ve  spent much time in this book dissecting how to improve our recommenda‐\ntions, making them more personalized and relevant to an individual user. Along\nthe way, you’ve learned that latent relationships between users and user personas\nencode important information about shared preferences. Unfortunately, all of this has\na serious downside: bias.\nFor the purposes of our discussion, we’ll talk about the two most important kinds of\nbias for recommendation systems:\n•Overly redundant or self-similar sets of recommendations•\n•Stereotypes learned by AI systems•\nFirst, we’ll delve into the crucial element of diversity in recommendation outputs.\nAs critical as it is for a recommendation system to offer relevant choices to users,\nensuring a variety of recommendations is also essential. Diversity not only safeguards\nagainst overspecialization but also promotes novel and serendipitous discoveries,\nenriching the overall user experience.\nThe balance between relevance and diversity is delicate and can be tricky. This bal‐\nance challenges the algorithm to go beyond merely echoing users’ past behavior and\nencourages an exploration of new territories, hopefully providing a more holistically\npositive experience with the content.\nThis kind of bias is primarily a technical challenge; how do we satisfy the multiobjec‐\ntives of diverse recommendations and highly relevant ones?\nWe’ll consider the intrinsic and extrinsic biases in recommendation systems as an\noften unintended yet significant consequence of both the underlying algorithms and\nthe data they learn from. Systemic biases in data collection or algorithmic design can\n273",5431
118-Improving Diversity.pdf,118-Improving Diversity,"result in prejudiced outputs, leading to ethical and fairness issues. Moreover, they\nmay create echo chambers or filter bubbles, curtailing users’ exposure to a broader\nrange of content and inadvertently reinforcing preexisting beliefs.\nAt the end of this chapter, we will discuss the risks and provide resources to learn\nmore about them. We are not experts in AI fairness and bias, but all ML practitioners\nshould understand and seriously consider these topics. We aim to provide an intro‐\nduction and signposts.\nDiversification  of Recommendations\nOur first investment into fighting bias is to explicitly target more diversity in our\nrecommendation outputs. We’ll briefly cover two of the many goals you may pursue:\nintra-list diversity and serendipitous recommendations.\nIntra-list diversity  attempts  to ensure that there are a variety of types of items within\na single recommendation list. The idea is to minimize similarity between the recom‐\nmended items to reduce overspecialization and encourage exploration. High intra-list\ndiversity within a set of recommendations increases the user’s exposure to many\nitems they may like; however, the recommendations for any particular interest will be\nshallower, reducing the recall.\nSerendipitous recommendations  are both surprising and interesting to the user. These\nare often items that the user might not have discovered independently or that are\ngenerally far less popular in the system. Serendipity can be introduced into the rec‐\nommendation process by injecting nonobvious or unexpected choices—even if those\nhave a relatively lower affinity score with the user—to improve overall serendipity. In\nan ideal world, these serendipitous choices are high affinity relative to other items of\ntheir popularity, so they’re the “best of the outside choices. ”\nImproving Diversity\nNow that we have our measures of diversity, we can explicitly attempt to improve\nthem. Importantly, by adding diversity metrics as one of our objectives, we will\npotentially sacrifice performance on things like recall or NDCG. It can be useful to\nthink of this as a Pareto problem, or to impose a lower bound on ranking metric\nperformance that you’ll accept in pursuit of diversity.\nIn a Pareto problem , you have two priorities that often trade off\nwith each other. In many areas of ML, and more generally applied\nmathematics, certain outcomes have a natural tension. Diversity in\nrecommendations is an important example of a Pareto problem in\nrecommendation systems, but it’s not the only one. In Chapter 14 ,\nyou briefly saw global optimization, which is an extreme case of\ntrade-offs.\n274 | Chapter 15: Bias in Recommendation Systems\nOne simple approach to improve diversity metrics is reranking : a post-processing\nstep in which the initially retrieved recommendation list is reordered to enhance\ndiversity. Various algorithms for re-ranking consider not just the relevance scores but\nalso the dissimilarity among the items in the recommendation list. Re-ranking is a\nstrategy that can operationalize any external loss function, so using it for diversity is a\nstraightforward approach.\nAnother strategy is to break out of the closed loop of recommendation feedback\nthat we discussed in the section “Propensity Weighting for Recommendation System\nEvaluation” on page 208. As in multiarmed bandit problems,  explore-exploit trade-offs\ncan choose between exploiting what the model knows the user will like and exploring\nless certain options that may yield higher rewards. This trade-off can be used in\nrecommendation systems to ensure diversity by occasionally choosing to explore  and\nrecommend less obvious choices. To implement a system like this, we can use affinity\nas a reward estimate and propensity as an exploitation measure.\nInstead of using these posterior strategies, an alternative is to incorporate diversity\nas an objective in the learning process  or include a diversity regularization term in\nthe loss function. Multiobjective loss including pairwise similarity as a regularizer\ncan help train the model to learn diverse sets of recommendations. Y ou previously\nsaw that kinds of regularization can coach the training process to minimize certain\nbehaviors. One regularization term that can be used explicitly is similarity among\nrecommendations ; the dot product of each embedding vector in the recommendations\nto each other can approximate this self-similarity. Let ℛ=R1,R2, ...,Rk be the\nlist of embeddings for the recommendations, and then consider ℛ as a column\nmatrix—with each row a recommendation. Calculating ℛ’s Gramian would yield\nall our dot-product similarity calculations, and thus we can regularize by this term\nwith appropriate hyperparameter weighting. Note that this differs from our previous\nGramian regularization because we’re considering the recommendations for only an\nindividual query in this case.\nFinally, we can use rankings from multiple domains to boost recommendation\ndiversity. By integrating various ranking measures, the recommendation system can\nsuggest items from outside the user’s “mode, ” thus broadening the range of recom‐\nmendations. A vibrant discipline exists around multimodal recommendations, with\nthe PinnerSage paper  from Pinterest a particularly impressive implementation. In\nmany of the works about multimodal recommendations, the retrieval step returns\ntoo many recommendations near to the user’s query vector. This forces self-similarity\namong the retrieved list. Multimodality forces multiple query vectors to be used for\neach request, allowing a built-in diversity.\nLet’s look at another perspective on item self-similarity and think about how the\npairwise relationships between items can be used to this end.\nDiversification  of Recommendations | 275",5829
119-Sharding.pdf,119-Sharding,"Applying Portfolio Optimization\nPortfolio optimization , a concept borrowed from finance, can be an effective approach\nto enhance diversity in recommendation systems. The goal here is to create a “port‐\nfolio” of recommended items that balances the two key parameters: relevance and\ndiversity.\nAt its heart, portfolio optimization is about risk (in our case, relevance) and return\n(diversity). Here’s a basic approach for applying this optimization to recommendation\nsystems:\n1.Formulate an item representation such that the distance in the space is a good1.\nmeasure of similarity. This is in line with our previous discussions about what\nmakes a good latent space.\n2.Calculate pairwise distance between items. Y ou can do this by using whatever2.\ndistance metric that enriches your latent space. It is important to calculate these\npairwise distances across all items retrieved and be ready for consideration to\nreturn. Note that how you aggregate these distributions of distances can be\nnuanced.\n3.Evaluate affinity for the retrieved set. Note that calibrated affinity scores will3.\nperform better as they provide a more realistic estimate of return.\n4.Solve the optimization problem. Solving the problem will yield a weight for4.\neach item that balances the trade-off between relevance and diversity. Items\nwith higher weights are more valuable in terms of both diversity and relevance,\nand they should be prioritized in the recommendation list. Mathematically, the\nproblem looks like this:\nMaximize wT*r−λ*wT*C*w\nHere, w is a vector representing the weights (i.e., the proportion of each item\nin the recommendation list), r is the relevance score vector, C is the covariance\nmatrix (which captures the diversity), and λ is a parameter to balance relevance\nand diversity. The constraint here is that the sum of the weights equals 1.\nRemember, the hyperparameter λ trades off between relevance and diversity.\nThis makes it a critical part of this process and may require experimentation or\ntuning based on the specific needs of your system and its users. This would be\nstraightforward via hyperparameter optimization in one of many packages such\nas Weights & Biases.\n276 | Chapter 15: Bias in Recommendation Systems\nMultiobjective Functions\nAnother  related approach to diversity is to rank based on a multiobjective loss.\nInstead of the ranking stage being purely personalization affinity, introducing a\nsecond (or more!) ranking term can dramatically improve diversity.\nThe simplest approach here is something similar to what you learned in Chapter 14 :\nhard ranking. A business rule that may apply to diversity is limiting each item\ncategory to only one item. This is the simplest case of multiobjective ranking because\nsorting by a categorical column and selecting the max in each group will achieve\nexplicit diversity with respect to that covariate. Let’s move on to something more\nsubtle.\nIn “Stitching Together Spaces for Query-Based Recommendations” , one of this book’s\nauthors worked with coauthor Ian Horn to implement a multiobjective recommenda‐\ntion system that balanced both personalization and relevance to an image-retrieval\nproblem.\nThe goal was to provide personalized recommendations for clothing that were similar\nto clothes in an image the user uploaded. This means there are two latent spaces:\n•The latent space of personalized clothes to a user•\n•The latent space of images of clothing•\nTo solve this problem, we first had to make a decision: what was more important\nfor relevance? Personalization or image similarity? Because the product was centered\naround a photo-upload experience, we chose image similarity. However, we had\nanother fact to consider: each uploaded image contained several pieces of clothing.\nAs is popular in computer vision, we first segmented the model into several items and\nthen treated each item as its own query (which we called anchor-items ). This meant\nour image-similarity retrieval was multimodal, as we searched with several different\nquery vectors. After we gathered them all, we had to make one final ranking—a\nmultiobjective ranking for image similarity and personalization. The loss function we\noptimized is shown here:\nsi=α×1 −di+1 −α×ai\nThe α is a hyperparameter that represents the weighting, di is the image distance, and\nai is the personalization. We learn α experimentally. The last step was to impose some\nhard ranking to ensure that one recommendation came from each anchor.\nMultiobjective Functions | 277\nSo let’s sum this up:\n1.We used two latent spaces with distances to provide rankings.1.\n2.We did multimodal retrieval via image segmentation.2.\n3.We retrieved using only one of the rankings.3.\n4.Our final ranking was multiobjective, with hard ranking utilizing all our latent4.\nspaces and business logic.\nThis allowed our recommendations to be diverse  in the sense that they achieved\nrelevance in several areas of the query that corresponded to different items.\nPredicate Pushdown\nY ou may be happy and comfortable applying these metrics during serving—after all,\nthat’s the title for this part of the book—but before we move on from this topic, we\nshould talk about an edge case that can have quite disastrous consequences. When\nyou impose the hard rules from Chapter 14  and the diversity expectations discussed\nearlier in this chapter, and do a little multiobjective ranking, sometimes you arrive\nat…no recommendations.\nSay you start by retrieving k items, but after the sufficiently diverse combinations that\nalso satisfy business rules, there’s simply nothing left. Y ou might say, “I’ll just retrieve\nmore items; let’s crank up k!” But this has some serious issues: it can really increase\nlatency, depress match quality, and throw off your ranking model that is more tuned\nto lower-cardinality sets.\nA common experience, especially with diversity, is that different modes for the\nretrieval have vastly different match scores. To take an example from our fashion\nrecommender world: all jeans might be a better match than any shirt we have, but if\nyou’re looking for diverse categories of clothes to recommend, no matter how big the\nk, you’ll potentially be missing out on shirts.\nOne solution to this problem is predicate pushdown . This optimization technique\nis used in databases, specifically in the context of data retrieval. The main idea of\npredicate pushdown is to filter data as early as possible in the data-retrieval process,\nto reduce the amount of data that needs to be processed later in the query execution\nplan.\nFor traditional databases, you see predicate pushdown applied, for example, as “apply\nmy query’s where  clause in the database to cut down on I/O. ” It may achieve this\nby explicitly pulling the relevant columns to check the where  clause first, and then\ngetting the row IDs from those that pass before executing the rest of the query.\n278 | Chapter 15: Bias in Recommendation Systems\nHow does this help us in our case? The simple idea is if your vector store also has\nfeatures for the vectors, you can include the feature comparisons as part of retrieval.\nLet’s take an overly simple example: assume your items have a categorical feature\ncalled color , and for good diverse recommendations you want a nice set of at least\nthree colors in your five recommendations. To achieve this, you can do a top- k search\nacross each of the colors in your store (the downside is that your retrieval is C times\nas large, where C is the number of colors that exist) and then do ranking and diversity\non the union of these sets. This has a much higher likelihood of surviving your\ndiversity rule in the eventual recommendations. This is great! We expect that latency\nis relatively low in retrieval, so this tax of extra retrievals isn’t bad if we know where to\nlook.\nThis optimization technique can be applied on quite complicated predicates if your\nvector store is set up well for the kinds of filters you wish to impose.\nFairness\nFairness  in ML in general is a particularly nuanced subject that is ill-served by short\nsummaries. The following topics are important, and we invite you to consider the\nrobust references included here:\nNudging\nFairness  does not need to be only “equal probabilities for all outcomes”; it can\nbe fair with respect to a specific covariate. Nudging via a recommender—i.e.,\nrecommending items to emphasize certain behavior or buying patterns—can\nincrease fairness. Consider the work by Karlijn Dinnissen and Christine Bauer\nfrom Spotify about using nudging to improve gender representation in music\nrecommendations .\nFilter bubbles\nFilter bubbles are a downside of extreme collaborative filtering: a group of users\nbegin liking similar recommendations, the system learns that they should receive\nsimilar recommendations, and the feedback loop perpetuates this. For a deep\nlook into not only the concept but also mitigation strategies, consider “Mitigating\nthe Filter Bubble While Maintaining Relevance”  by Zhaolin Gao et al.\nHigh risk\nNot all applications of AI are equal in risk. Some domains are particularly harm‐\nful when AI systems are poorly guardrailed. For a general overview of the most\nhigh-risk circumstances and mitigation, consult Machine Learning for High-Risk\nApplications  by Patrick Hall et al. (O’Reilly).\nTrustworthiness\nExplainable models is a popular mitigation strategy for risky applications of\nAI. While explainability does not solve  the problem, it frequently provides a\nFairness | 279\npath toward identification and resolution. For a deep dive on this, Practicing\nTrustworthy Machine Learning  by Y ada Pruksachatkun et al. (O’Reilly) provides\ntools and techniques.\nFairness in recommendations\nBecause recommendation systems are so obviously susceptible to issues of AI\nfairness, much has been written on the topic. Each of the major social media\ngiants has employed teams working in AI safety. One particular highlight is\nthe Twitter Responsible AI team led by Rumman Chowdhury. Y ou can read\nabout the team’s work in “Can Auditing Eliminate Bias from Algorithms?”  by\nAlfred Ng.\nSummary\nWhile these techniques provide pathways to enhance diversity, it’s important to\nremember to strike a balance between diversity and relevance. The exact method or\ncombination of methods used may vary depending on the specific use case, the avail‐\nable data, the intricacies of the user base, and the kind of feedback you’re collecting.\nAs you implement recommendation systems, think about which aspects are the most\nkey in your diversity problem.\n280 | Chapter 15: Bias in Recommendation Systems\nCHAPTER 16\nAcceleration Structures\nSo what are acceleration structures? In computer science terminology, when you try\nto rank every item in a corpus one by one, the typical amount of time it would take\nif there are N items is proportional to N. This is called big O notation . So if you have\na user vector and you have a corpus of N items, it would take typically O(N) time to\nscore all the items in the corpus for one user. This is usually tractable if N is small\nand can fit into GPU RAM, typically N < 1 million items or so. However, if we have\na very large corpus of, say, a billion items, it might take a very long time if we also\nhave to make recommendations for a billion users. Then in big O notation it would\nbe O(1018) dot products to score a billion items for each and every one of a billion\nusers.\nIn this chapter, we will try to reduce the O(N * M) time to something sublinear in the\nnumber of items N and the number of users M. We will discuss strategies including\nthe following:\n•Sharding•\n•Locality sensitive hashing•\n•k-d Trees•\n•Hierarchical k-means•\n•Cheaper retrieval methods•\nWe’ll also cover the trade-offs related to each strategy and what they could be used\nfor. For all the following examples, we assume that the user and items are represented\nby embedding vectors of the same size and that the affinity between the user and\nitems is a simple dot product, cosine distance, or Euclidean distance. If we were to\nuse a neural network like a two-tower model to score the user and item, then possibly\n281",12243
120-Locality Sensitive Hashing.pdf,120-Locality Sensitive Hashing,"the only method that could be used to speed things up would be sharding or some\nkind of cheaper pre-filtering method.\nSharding\nSharding  is probably the simplest strategy to divide and conquer . Suppose you have\nk machines, N items, and M users. Using a sharding strategy, you can reduce the\nruntime to O(N * M / k). Y ou can do this by assigning each item a unique identifier,\nso you have tuples of ( unique_id , item_vector ). Then, by simply taking machine_id\n= unique_id % K , we can assign a subset of the corpus to a different machine.\nWhen a user needs a recommendation, we can then compute the top-scoring recom‐\nmendations either ahead of time or on demand by distributing the workload onto\nk machines, thus making the computation k times faster, except for the overhead in\ngathering the top results on the server and ordering them jointly. Note that if you\nwant, say, 100 top-scoring items, you would still have to obtain the top 100 results\nfrom each shard, collate them together, and then sort all the results jointly if you want\nto have the same results as in a brute-force method of scoring the entire corpus.\nSharding is useful in the sense that it can be combined with any of the other accelera‐\ntion methods and is not dependent on the representation having any specific form,\nsuch as being a single vector.\nLocality Sensitive Hashing\nLocality sensitive hashing  (LSH) is an interesting technique that converts a vector\ninto a token-based representation. This is powerful because if CPUs are readily avail‐\nable, we can use them to compute the similarity between vectors by using cheaper\ninteger arithmetic operations such as XOR and bit counting with specialized assembly\ninstructions rather than floating-point operations. Integer operations tend to be\nmuch faster on CPUs than floating-point operations, so we can compute similarity\nbetween items much faster than using vector operations.\nThe other benefit is that once items are represented as a series of tokens, a regular\nsearch engine database would be able to store and retrieve these items by using token\nmatching. Regular hashing, on the other hand, tends to result in vastly different\nhash codes if a slight change occurs in the input. This is not a criticism of the hash\nfunctions; they just have different uses for different kinds of data.\nLet’s walk through a couple of ways to convert a vector into a hash. LSH is different\nfrom regular hashing in that small perturbations to a vector should result in the\nsame hash bits as the hash of the original vector. This is an important property as it\nallows us to look up the neighborhood of a vector by using fast methods such as hash\nmaps. One simple hashing method is called the Power of Comparative Reasoning ,\n282 | Chapter 16: Acceleration Structures\nor Winner Take All hashing. In this hashing scheme, the vector is first permuted\nusing a known, reproducible permutation. We can generate this known permutation\nby simply shuffling the indices of all the vector dimensions with a random-number\ngenerator that accepts a seed and reliably reproduces the same exact shuffle sequence.\nIt is important that the permutation is stable over different versions of Python, as we\nwant to reproduce the hashing operation when generating the hashes as well as dur‐\ning retrieval time. Since we are using JAX’s random library and JAX is careful about\nthe reproducibility of permutations, we just directly use the permutation function in\nJAX. The hash code computation after that is simply a comparison between adjacent\ndimensions of the permuted vector, as shown in Example 16-1 .\nExample 16-1. Winner take all\ndef compute_wta_hash (x):\n  """"""Example code to compute some Winner take all hash vectors\n  Args:\n    x: a vector\n  Result:\n    hash: a hash code\n  """"""\n  key = jax.random.PRNGKey(1337)\n  permuted  = jax.random.permutation (key, x)\n  hash1 = permuted [0] > permuted [1]\n  hash2 = permuted [1] > permuted [2]\n  return (hash1, hash2)\nx1 = jnp.array([1, 2, 3])\nx2 = jnp.array([1, 2.5, 3])\nx3 = jnp.array([3, 2, 1])\nx1_hash = compute_wta_hash (x1)\nx2_hash = compute_wta_hash (x2)\nx3_hash = compute_wta_hash (x3)\nprint(x1_hash)\nprint(x2_hash)\nprint(x3_hash)\n(Array(False, dtype=bool), Array(True, dtype=bool))\n(Array(False, dtype=bool), Array(True, dtype=bool))\n(Array(True, dtype=bool), Array(False, dtype=bool))\nAs you can see, the vector x2 is slightly different from x1 and results in the same hash\ncode of 01, whereas x3 is different and results in a hash code of 10. The Hamming\ndistance  of the hash code is then used to compute the distance between two vectors,\nas shown in Example 16-2 . The distance is simply the XOR of the two hash codes,\nwhich results in 1 whenever the bits disagree, followed by bit counting.\nLocality Sensitive Hashing | 283",4853
121-k-d Trees.pdf,121-k-d Trees,"Example 16-2. Hamming function\nx = 16\ny = 15\nhamming_xy  = int.bit_count (x ^ y)\nprint(hamming_xy )\n5\nUsing the Hamming distance as shown here results in some speedup in the distance\ncomputation, but the major speedup will come from using the hash codes in a hash\nmap. For example, we could break up the hash code into 8-bit chunks and store\nthe corpus into shards keyed by each 8-bit chunk, which results in a 256× speedup\nbecause we have to look only in the hash map that has the same key as the query\nvector for nearest neighbors.\nThis has a drawback in terms of recall, though, because all 8 bits have to match in\norder for an item to be retrieved that matches the query vector. A tradeoff exists\nbetween the number of bits of the hash code used in hashing and the Hamming\ndistance computation. The larger the number of bits, the faster the search, because\nthe corpus is divided into smaller and smaller chunks. However, the drawback is that\nmore and more bits have to match, and thus all the hash code bits in a nearby vector\nin the original space might not match and thus might not be retrieved.\nThe remedy is to have multiple hash codes with different random-number generators\nand repeat this process a few times with different random seeds. This extra step is left\nas an exercise for you.\nAnother  common way to compute hash bits uses the Johnson-Lindenstrauss lemma ,\nwhich is a fancy way of saying that two vectors, when multiplied by the same random\nGaussian matrix, tend to end up in a similar location. However, the L2 distances\nare preserved, which means this hash function works better when using Euclidean\ndistance to train the embeddings rather than dot products. In this scheme, only the\nhash code computation differs; the Hamming distance treatment is exactly the same.\nThe speedup from LSH is directly proportional to the number of bits of the hash\ncode that have to be an exact match. Suppose only 8 bits of the hash code are used in\nthe hash map; then the speedup is 28, or 256 times the original. The trade-off for the\nspeed is having to store the hash map in memory.\nk-d Trees\nA common strategy for speeding up computation in computer science is  divide and\nconquer . In this scheme, the data is recursively partitioned into two halves, and only\nthe half that is relevant to the search query is searched. In contrast to a linear O(n) in\nthe number of items in the corpus scheme, a divide-and-conquer algorithm would be\nable to query a corpus in O(log2( n)) time, which is a substantial speedup if n is large.\n284 | Chapter 16: Acceleration Structures\nOne such binary tree for vector spaces is called a k-d tree . Typically, to build a k-d\ntree, we compute the bounding box of all the points in the collection, find the longest\nedge of the bounding box and split it down the middle of that edge in the splitting\ndimension, and then partition the collection into two halves. If the median is used,\nthe collection is more or less divided into two equal-numbered items; we say more\nor less  because there might be ties along that split dimension. The recursive process\nstops when a small number of items is left in the leaf node. Many implementations of\nk-d trees exist—for example, SciPy’s k-d tree .\nAlthough the speedup is substantial, this method tends to work when the number of\nfeature dimensions of the vector is low. Also, similar to other methods, k-d trees work\nbest when the L2 distance is the metric used for the embedding. Losses in retrieval\ncan occur if the dot product was used for the similarity metric, as the k-d tree makes\nmore sense for Euclidean space partitioning.\nExample 16-3  provides sample code for splitting a batch of points along the largest\ndimension.\nExample 16-3. Partitioning via a k-d tree\nimport jax\nimport jax.numpy  as jnp\ndef kdtree_partition (x: jnp.ndarray):\n  """"""Finds the split plane and value for a batch of vectors x.""""""\n  # First, find the bounding box.\n  bbox_min  = jnp.min(x, axis=0)\n  bbox_max  = jnp.max(x, axis=0)\n  # Return the largest split dimension and value.\n  diff = bbox_max  - bbox_min\n  split_dim  = jnp.argmax(diff)\n  split_value  = 0.5 * (bbox_min [split_dim ] + bbox_max [split_dim ])\n  return split_dim , split_value\nkey = jax.random.PRNGKey(42)\nx = jax.random.normal(key, [256, 3]) * jnp.array([1, 3, 2])\nsplit_dim , split_value  = kdtree_partition (x)\nprint(""Split dimension %d at value %f"" % (split_dim , split_value ))\n# Partition the points into two groups, the left subtree\n# has all the elements left of the splitting plane.\nleft = jnp.where(x[:, split_dim ] < split_value )\nright = jnp.where(x[:, split_dim ] >= split_value )\nSplit dimension  1 at value -0.352623\nk-d Trees | 285\nAs you can see from the code, the k-d tree partitioning code can be as simple as\nsplitting along the middle longest dimension. Other possibilities are splitting along\nthe median of the longest dimension or using a surface area heuristic .\nA k-d tree is constructed by repeatedly partitioning the data along only one spatial\ndimension at a time (usually along the largest axis aligned to the spread of data); see\nFigure 16-1 .\nFigure 16-1. k-d tree construction’s initial bounding box\nPartitions are recursively subdivided again, usually along the longest axis, until\nthe number of points in the partition is fewer than a chosen small number; see\nFigure 16-2 .\nThe k-d tree lookup time is O(log2( n)) in n, the number of items in the corpus.\nThe tree also requires a small overhead of memory to store the tree itself, which\nis dominated by the number of leaf nodes, so it would be best to have a minimal\nnumber of items in a leaf to prevent splits that are too fine.\nFigure 16-2. k-d tree construction recursively partitioned\n286 | Chapter 16: Acceleration Structures\nFrom the root node, repeatedly check whether the query point (e.g., the item we are\nseeking nearest neighbors for) is in the left or right child of the root node, as shown\nin Figure 16-3 . For example, use go_left = x[split_dim] < value_split[dim] .\nIn binary tree convention, the left child contains all points whose value at the split\ndimension are less than the split value. Hence if the query point’s value at the split\ndimension is less than the split value we go left, otherwise we go right. Recursively\ndescend down the tree until reaching the leaf node; then exhaustively compute\ndistances to all items in the leaf node.\nFigure 16-3. k-d tree query\nk-d Trees | 287",6515
122-Hierarchical k-means.pdf,122-Hierarchical k-means,"A k-d tree has a potential drawback. If an item is close to a splitting plane, that item\nwould be considered on the other side of the tree. As a result, the item would not\nbe considered as a nearest neighbor candidate. In some implementations of k-d trees,\ncalled  spill trees , both sides of a splitting plane are visited if the query point is close\nenough to the plane’s decision boundary. This change increases runtime a little bit for\nthe benefit of more recall.\nHierarchical k-means\nAnother  divide-and-conquer strategy that does scale to higher feature dimensions is\nk-means clustering . In this scheme, the corpus is clustered into k clusters and then\nrecursively clustered into k more clusters until each cluster is smaller than a defined\nlimit.\nAn implementation of k-means can be found at scikit-learn’s web page .\nTo build the clustering, first create cluster centroids at random from existing points\n(Figure 16-4 ).\nFigure 16-4. k-means initialization\nNext, we assign all points to the cluster they are closest to. Then for each cluster, we\ntake the average of all the assigned points as the new cluster center. We repeat until\ndone, which can be a fixed number of steps. Figure 16-5  illustrates this process. The\noutput is then k cluster centers of points. The process can be repeated again for each\ncluster center, splitting again into k more clusters.\n288 | Chapter 16: Acceleration Structures\nFigure 16-5. k-means clustering\nAgain, the speedup is O(log( n)) in the number of items, but k-means is better adapted\nto clustering higher-dimensional data points than k-d trees.\nThe querying for a k-means cluster is rather straightforward. Y ou can find the closest\ncluster to the query point and then repeat the process for all subclusters until a leaf\nnode is found; then all the items in the leaf node are scored against the query point.\nAn alternative to k-means is to perform SVD and use the first k eigenvectors as the\nclustering criteria. The use of SVD is interesting in that there exists closed form and\napproximate methods like power iteration  for computing the eigenvectors. Using the\ndot product to compute affinity might be better suited to vectors trained using the\ndot product as the affinity metric.\nTo learn more on this topic, you can consult “Label Partitioning for Sublinear\nRanking”  by Jason Weston et al. (including one of this book’s authors). The paper\ncompares LSH, SVD, and hierarchical k-means. Y ou’ll find a comparison of the\nspeedup and the loss in retrieval, with the brute-force as a baseline.\nGraph-Based ANN\nAn emerging trend in ANNs is using graph-based methods.\nLately,  hierarchical navigable small worlds  is a particularly popular\napproach. This graph algorithm  encodes proximity in multilayer\nstructures and then relies on the common maxim that “the number\nof connectivity steps from one node to another is often surprisingly\nsmall. ” In graph-based ANN methods, you often find one neighbor,\nand then traverse the edges connected to that neighbor to rapidly\nfind others.\nHierarchical k-means | 289",3097
123-Cheaper Retrieval Methods.pdf,123-Cheaper Retrieval Methods,,0
124-Part V. The Future of Recs.pdf,124-Part V. The Future of Recs,"Cheaper Retrieval Methods\nIf your corpus has the ability to do an item-wise cheap retrieval method, one way\nto speed up searches is to use the cheap retrieval method to obtain a small subset of\nitems and then use the more expensive vector-based methods to rank the subset. One\nsuch cheap retrieval method is to make a posting list of the top co-occurrences of\none item with another. Then when it comes to generating the candidates for ranking,\ngather all the top co-occurring items together (from a user’s preferred items, for\nexample) and then score them together with the ML model. In this way, we do not\nhave to score the entire corpus with the ML model but just a small subset.\nSummary\nIn this chapter, we showed a few ways to speed up the retrieval and scoring of items\nin a corpus, given a query vector, without losing too much in terms of recall and\nwhile still maintaining precision. No ANN method is perfect, as the acceleration\nstructures depend on the distribution of the data, and this varies from dataset to\ndataset. We hope that this chapter provides a launching pad for you to explore\nvarious ways to make retrieval faster and sublinear in the number of items in the\ncorpus.\n290 | Chapter 16: Acceleration Structures\nPART V\nThe Future of Recs\nI’m hungry for more. What are companies doing in Prod?\nWe’ve come so far, but there’s so much more! RecSys moves fast, and it’s worth\nknowing the concepts you’ll see at next year’s conferences. These ideas are already\nproven to some extent—none of them are pure science fiction—but they haven’t\nsettled into their final forms yet.\nBefore we get into these few very modern ideas, it’s also worth noting all the topics\nwe haven’t covered in this book. Our most heinous omissions are probably reinforce‐\nment learning techniques and ideas related to conformal methods. Both are deeply\nimportant aspects of recommendation systems, but both also require a significantly\ndifferent background and treatment, and thus are ill-suited to fit into the structure\nhere. Additionally, neither is well introduced to the JAX ecosystem, and thus they are\na much harder lift to scaffold.\nWhen your scale is big enough, and the previous chapters are no longer buttering\nyour toast, the following chapters will show you how to level up. At the time of\nwriting, all these methods are in production at Fortune 500 companies valued at\nmore than $10 billion. Learn these concepts and then go build the next TikTok.",2483
125-Order-Two Markov Chain.pdf,125-Order-Two Markov Chain,"CHAPTER 17\nSequential Recommenders\nIn our journey so far, you’ve learned about a variety of features that appear as explicit\nor as latent components in the recommendation problem. One kind of feature, which\nhas appeared implicitly, is the history of previous recommendations and interactions.\nY ou may wish to protest here: “ All of the work we’ve done so far considers the\nprevious recommendations and interactions! We’ve even learned about prequential\ntraining data. ”\nThat  is true, but it fails to account for more explicit relationships between the\nsequence of recommendations leading up to the inference request . Let’s look at an exam‐\nple to distinguish the two. Y our video-streaming website knows that you’ve previously\nseen all of Darren Aronofsky’s films, so when The Whale  is released, the website\nis very likely to recommend it. But this type of recommendation is different from\none you might receive after finishing episode 10 of Succession . Y ou may have been\nwatching Aronofsky films over a long time period— Pi many years ago and Black\nSwan  earlier this year. But you have been watching an episode of Succession  each\nnight this week, and your entire recent history is made up of Logan Roy. This latter\nexample is a sequential recommendation problem: using the most recent ordered list\nof interactions to predict what you’ll enjoy next.\nIn terms of the modeling objective, the recommenders we’ve seen use pairwise rela‐\ntionships between potential recommendations and historical interactions. Sequential\nrecommendation aims to predict users’ next actions based on the sequential inter‐\nactions in the past that may be of much higher order —i.e., combinations of interac‐\ntions among three or more items. Most sequential recommendation models involve\nsequential data-mining techniques such as Markov chains, recurrent neural networks\n(RNNs), and self-attention. These models usually take into consideration short-term\nuser behavior and are less sensitive, even oblivious, to the global user preferences that\nhave stabilized over time.\n293\nInitial work in sequential recommendations focused on modeling the transitions\nbetween successive items. These used Markov chains and translation-based methods.\nAs deep learning methods showed more and more promise in modeling sequential\ndata—such as their biggest success in NLP—there have been many attempts to\nuse neural network architectures to model sequential dynamics of a user’s interac‐\ntion history. Early successes in this direction include GRU4Rec using an RNN to\nmodel users’ sequential interactions. Recently, transformer architectures have demon‐\nstrated superior performance for sequential data modeling. The transformer architec‐\nture lends itself to efficient parallelization and is effective at modeling long-range\nsequences.\nMarkov Chains\nDespite  mining for relationships to historical recommendations, the models we’ve\nbeen considering often fail to capture sequential patterns in user behavior, thereby\ndisregarding the chronological order of user interactions. To address this shortcom‐\ning, sequential recommender systems were developed, incorporating techniques like\nMarkov chains to model the temporal dependencies between items.\nA Markov chain  is a stochastic model that operates on the principle of memoryless‐\nness. It models the probability of transitioning from one state to another—given the\ncurrent state—without considering the sequence of preceding events. Markov chains\nmodel the sequential behavior of users by considering each state as an item, and the\ntransition probabilities as the likelihood of a user interacting with a certain item after\nthe current one.\nThe first-order Markov chain, in which the future state depends solely on the current\nstate, was a common strategy in early sequential recommenders. Despite its simplic‐\nity, the first-order Markov chain is effective in capturing short-term, item-to-item\ntransition patterns, improving the quality of recommendations over nonsequential\nmethods.\nTake, for example, our preceding Succession  example. If you’re using only a first-order\nMarkov chain, a really great heuristic would be “What is the next episode in the\nseries, if it’s a series; otherwise, fall back on a collaborative filtering (CF) model. ” Y ou\ncan see that for a huge percentage of watch hours, this naive first-order chain would\nsimply tell the user to watch the next episode of a show. Not particularly enlightening,\nbut a good sign. When you abstract this out further, you start to get more powerful\nmethods.\nThe first-order assumption does not always hold in real-world applications, as user\nbehavior is often influenced by a longer history of interactions. To overcome this lim‐\nitation, higher-order Markov chains look further back: the next state is determined by\na set of previous states, providing a richer model of user behavior. Nevertheless, it’s\n294 | Chapter 17: Sequential Recommenders",4993
126-Other Markov Models.pdf,126-Other Markov Models,"crucial to select the appropriate order, as too high an order may lead to overfitting\nand sparsity of the transition matrix.\nOrder-Two Markov Chain\nLet’s consider an example of an order-two Markov chain  model using the weather.\nAssume we have three states: sunny ( S), cloudy ( C), and rainy ( R).\nIn an order-two Markov chain, the weather of today ( t) would depend on the weather\nof yesterday ( t− 1) and the day before yesterday ( t− 2). The transition probability\ncan be denoted as PStSt− 1,St− 2.\nThe Markov chain can be defined by a transition matrix that provides the probabili‐\nties of transitioning from one state to another. However, because we’re dealing with\nan order-two Markov chain, we would have a transition tensor instead. For simplicity,\nlet’s say we have the following transition probabilities:\nPSS,S= 0 . 7,PCS,S= 0 . 2,PRS,S= 0 . 1,\nPSS,C= 0 . 3,PCS,C= 0 . 4,PRS,C= 0 . 3,\n...\nY ou can visualize these probabilities in a three-dimensional cube. The first two\ndimensions represent the state of today and yesterday, and the third dimension\nrepresents the possible states of tomorrow.\nIf the weather was sunny for the last two days and we want to predict the weather\nfor tomorrow, we would look at the transition probabilities starting with S,S, which\nare PSS,S= 0 . 7 , PCS,S= 0 . 2 , and PRS,S= 0 . 1 . Therefore, according\nto our model, there’s a 70% chance that it will be sunny, a 20% chance that it will be\ncloudy, and a 10% chance that it will be rainy.\nThe probabilities in the transition matrix (or tensor) are typically estimated from\ndata. If you have a historical record of the weather for several years, you can count the\nnumber of times each transition occurs and divide by the total number of transitions\nto estimate the probability.\nThis is only a basic demonstration of an order-two Markov chain. In real applica‐\ntions, the states might be much more numerous and the transition matrix much\nlarger, but the principles remain the same.\nOther Markov Models\nA more advanced Markovian approach is the Markov decision process  (MDP ), which\nextends the Markov chain by introducing actions and rewards. In the context of rec‐\nommender systems, each action could represent a recommendation, and the reward\nMarkov Chains | 295\ncould be the user’s response to the recommendation. By incorporating user feedback,\nthe MDP can learn more personalized recommendation strategies.\nMDPs are defined by a tuple S,A,P,R, where S is the set of states, A is the set of\nactions, P is the state transition probability matrix, and R is the reward function.\nLet’s use a simplified MDP for a movie recommender system as an example:\nStates (S)\nThese could represent the genres of movies a user has watched in the past. For\nsimplicity, let’s say we have three states: Comedy ( C), Drama ( D), and Action ( A).\nActions (A)\nThese could represent the movies that can be recommended. For this example,\nlet’s say we have five actions (movies): Movies 1, 2, 3, 4, and 5.\nTransition probabilities ( P)\nThis represents the likelihood of transitioning from one state to another, given\na specific action. For instance, if the user just watched a Drama ( D) and we\nrecommend Movie 3 (which is an Action movie), the transition probability\nPAD,Movie 3 might be 0.6, indicating a 60% chance the user will watch\nanother Action movie.\nRewards ( R)\nThis is the feedback from the user after taking an action (recommendation). Let’s\nassume for simplicity that a user’s click on a recommended movie gives a reward\nof +1 and no click is a reward of 0.\nThe aim of the recommender system in this context is to learn a policy π:SA that\nmaximizes the expected cumulative reward. A policy dictates which action the agent\n(the recommender system) should take in each state.\nThis policy can be learned via reinforcement learning algorithms, such as Q-learning\nor policy iteration, which essentially learn the value of taking an action in a state (i.e.,\nrecommending a movie after the user has watched a certain genre), considering the\nimmediate reward and the potential future rewards.\nThe main challenge in a real-world recommender system scenario is that both the\nstate and action spaces are extremely large, and the transition dynamics and reward\nfunction can be complex and difficult to estimate accurately. But, the principles\ndemonstrated in this simple example remain the same.\nDespite the promising performance of Markov chain-based recommender systems,\nseveral challenges remain. The memorylessness  assumption of the Markov chain may\nnot hold in certain scenarios where long-term dependencies exist. Furthermore, most\nMarkov chain models treat user-item interactions as binary events (either interaction\n296 | Chapter 17: Sequential Recommenders",4808
127-RNN and CNN Architectures.pdf,127-RNN and CNN Architectures,"or no interaction), which oversimplifies the variety of interactions users may have\nwith items, such as browsing, clicking, and purchasing.\nNext, we’ll cover neural networks. We’ll see how some architectures you’re likely\nfamiliar with can be relevant to learning a sequential recommender task.\nRNN and CNN Architectures\nRecurrent neural networks  (RNNs) are a type of neural network architecture designed\nto recognize patterns in sequences of data, such as text, speech, or time series data.\nThese networks are recurrent  in that the outputs from one step in the sequence are\nfed back into the network as inputs while processing the next step. This gives RNNs a\nform of memory, which is helpful for tasks like language modeling, where each word\ndepends on the previous words.\nAt each time step, an RNN takes in an input (like a word in a sentence) and produces\nan output (like a prediction of the next word). It also updates its internal state,\nwhich is a representation of what it has “seen” last in the sequence. This internal\nstate is passed back into the network when processing the next input. As a result, the\nnetwork can use information from previous steps to influence its predictions for the\ncurrent step. This is what allows RNNs to effectively process sequential data.\nGRU4Rec  used  recurrent neural networks to model session-based recommendations\nin one of the first applications of neural network architectures to the recommenda‐\ntion problem. A session  refers to a single contiguous period of user interaction, like\ntime spent on a page without the user navigating away or turning off their computer.\nHere we will see a dramatic advantage of sequential recommendation systems: most\ntraditional recommendation methods rely on an explicit user ID to build a user-\ninterest profile. However, session-based recommendations operate over anonymous\nuser sessions that are often quite short to allow for a profile modeling. Moreover,\na lot of variance can occur in user motivations in different sessions. A solution via\nuser-agnostic recommendation that works for such recommendation situations is an\nitem-based model in which an item-item similarity matrix is calculated based on\nitems co-occurring within a single session. This precomputed similarity matrix is\nemployed at runtime to recommend the most similar item to the one last clicked.\nThis approach has obvious limitations such as relying only on the last clicked item.\nTo this end, GRU4Rec uses all the items in the session and models the session as\na sequence of items. The task of recommending items to be added translates to the\nprediction of the next item in the sequence.\nRNN and CNN Architectures | 297\nUnlike the small fixed-size vocabulary of languages, recommendation systems are\nrequired to reason over a large number of items that grows over time as more items\nare added. To handle this concern, pairwise ranking losses (e.g., BPR) are considered.\nGRU4Rec is further extended in GRU4Rec+ , which utilizes a new loss function\nspecifically designed for gains in top- k recommendation. These loss functions blend\ndeep learning and LTR to address neural recommendation settings.\nA different approach to neural networks for recommendations adopted CNNs for\nsequential recommendation. We won’t cover the basics of CNNs here, but you can\nconsult “How Do Convolutional Neural Networks Work?”  by Brandon Rohrer for the\nessentials.\nLet’s  discuss one method that has shown a lot of success, CosRec , as visualized in\nFigure 17-1 . This method (and others) starts with a structure similar to that of our\nMF used throughout most of the book: a user-item matrix. We assume that there are\ntwo latent factor matrices, Eℐ and EU, but let’s first focus on the item matrix.\nEach vector in the item matrix is an embedding vector for a single item, but we\nwish to encode sequences: take sequences of length L and collect those embedding\nvectors. We now have an L×D matrix with a row per each item in the sequence. Take\nadjacent rows as pairs and concatenate them for each vector in a three-tensor; this\neffectively captures the sequence as a series of pairwise transitions. This three-tensor\ncan be passed through a vectorized 2D CNN to yield a vector (of length L) that\nis concatenated with the original user vector and fed through a fully connected\nlayer. Finally, binary cross-entropy is our loss function to attempt to predict the best\nrecommendation.\nFigure 17-1. CosRec CNN\n298 | Chapter 17: Sequential Recommenders",4540
128-Self-Attentive Sequential Recommendation.pdf,128-Self-Attentive Sequential Recommendation,"Attention Architectures\nA term that is commonly associated with neural networks and that may ring a bell\nfor you by now is attention . This is because transformers, in particular the kind that\nappear in large language models (LLMs) like the generalized pretrained transformer,\nhave become a central focus among AI users.\nWe will give an extremely brief, and less technical, introduction to self-attention\nand the transformer here. For a more complete guide on transformers, consult the\nexcellent overview in “Transformers from Scratch”  by Brandon Rohrer.\nFirst, let’s state the key differentiating assumption about a transformer model: the\nembeddings are positional. We’re hoping to learn not only one embedding for every\nitem but also an embedding for every item-position pair. Therefore, when an article\nis the first in a session and the last in a session, those two instances are treated as two\nseparate items .\nAnother important notion is stacking. When building transformers, we often think\nof the architecture as a layer cake, with sections stacked on top of one another. The\nkey components are the embeddings, the self-attention layer, the skip-addition, and\nthe feed-forward layer. The most complicated operations happen in self-attention,\nso let’s focus on that first. We just discussed the positional embeddings, which\nare sent as a sequence of these embedding vectors; recall that a transformer is a\nsequence-to-sequence model! The skip-addition means that we push the embedding\nforward around  the self-attention layer (and the feed-forward layer above) and add it\nto the positional output of the attention layer. The feed-forward layer is an unexciting\nmultilayer perceptron that stays in the positional columns and uses a ReLU or GeLU\nactivation.\nReLU Versus GeLU\nReLU (Rectified Linear Unit) is an activation function\ndefined as fx= max 0,x. GeLU (Gaussian Error Lin‐\near Unit) is another activation function approximated as\nfx= 0 . 5x1 + tanh2\nπx+ 0 . 044715 x3, inspired by the\nGaussian cumulative distribution function. The intuition behind\nGeLU is that it tends to allow small values of x to pass through\nwhile smoothly saturating extreme values, potentially enabling\nbetter gradient flow for deep models. Both functions introduce\nnonlinearity in neural networks, with GeLU often demonstrating\nimproved learning dynamics over ReLU in certain contexts.\nAttention Architectures | 299",2437
129-Recency Sampling.pdf,129-Recency Sampling,"Here are some quick tips on self-attention:\n•The idea behind self-attention is that everything in the sequence affects every‐•\nthing else, in some manner.\n•The self-attention layer is learning four weight matrices per head.•\n•The heads are in 1-1 correspondence with the sequence length.•\n•We often call the weight matrices Q,K,O,V. Both Q and K get crossed with •\nthe positional embedding, but O and V are first crossed into an embedding-\ndimension-sized square matrix before dotting with the embedding. QE˙ and\nKE˙ multiply to create the eponymous attention  matrix, over which we take a\nrow-wise softmax to get the attention vector.\n•Some normalizations exist, but we’ll disregard them as inessential for under‐•\nstanding.\nWhen we want to speak accurately but briefly about attention, we usually say, “It\ntakes a sequence of positional embeddings and mushes them all together to learn how\nthey’re related. ”\nSelf-Attentive Sequential Recommendation\nSASRec  is the first transformer model we’ll consider. This autoregressive sequential\nmodel (similar to a causal language model) predicts the next user interaction from\npast user interactions. Inspired by the success of the transformer models in sequential\nmining tasks, the self-attention-based architecture is used for sequential recommen‐\ndation.\nWhen we say that the SASRec model is trained in an autoregressive manner, we\nmean that the self-attention is allowed to attend to only the earlier positions in the\nsequence; looking into the future is not permitted. In terms of the mushing we\nreferenced earlier, think of this as only mushing forward the influence. Some people\ncall this “causal” because it respects the causal arrow of time. The model also allows\nfor a learnable positional encoding, which means that the updates carry down to the\nembedding layer. This model uses two transformer blocks.\nBERT4Rec\nInspired by the BERT model in NLP , BERT4Rec  improves upon SASRec by training a\nbidirectional masked sequential (language) model.\nWhile BERT uses a masked language model for pretraining word embeddings,\nBERT4Rec uses this architecture to train end-to-end recommendation systems. It\ntries to predict the masked items in the user-interaction sequence. Similar to the\noriginal BERT model, the self-attention is bidirectional: it can look at both past and\n300 | Chapter 17: Sequential Recommenders",2396
130-Merging Static and Sequential.pdf,130-Merging Static and Sequential,"future interactions in the action sequence. To prevent leakage of future information\nand to emulate the realistic settings, only the last item in the sequence is masked\nduring inference. Using item masking, BERT4Rec outperforms SASRec. However, a\ndrawback of the BERT4Rec model is that it is quite compute intensive and requires\nmuch more training time.\nRecency Sampling\nSequential recommendation and the adoption of transformer architecture in these\ntasks has seen a lot of interest recently. These deep neural network models\nlike BERT4Rec and SASRec have shown improved performance over traditional\napproaches. However, these models suffer from slow training problems. A recently\npublished paper—ha ha, get it—addresses the question of improving training effi‐\nciency while achieving state-of-the-art performance. See “Effective and Efficient\nTraining for Sequential Recommendation Using Recency Sampling”  by Aleksandr\nPetrov and Craig Macdonald for details.\nThe two training paradigms we’ve just described for sequential models are autore‐\ngressive, which tries to predict the next item in the user-interaction sequence, and\nmasked, which tries to predict masked items in the interaction sequence. The autore‐\ngressive approach doesn’t use the beginning of the sequence as labels in the training\nprocess, and thus valuable information is lost. The masked approach, on the other\nhand, is only weakly related to the end goal of the sequential recommendation.\nThe paper by Petrov and Macdonald proposes a recency-based sampling of positive\nexamples from the sequences to build the training data. The sampling is designed\nto give more recent interactions higher chances of being sampled. However, because\nof the probabilistic nature of the sampling mechanism, even the oldest of the interac‐\ntions have nonzero chances of being chosen. An exponential function is employed\nas a sampling routine that interpolates between the masking-based sampling, where\neach interaction has equal probability of being sampled, and autoregressive sampling,\nwhere items from the end of the sequence are sampled. This showed superior per‐\nformance in sequential recommendation tasks while requiring much less training\ntime. Compare this approach to some of the other examples where we saw sampling\nprovide significant improvements in training recommender systems!\nMerging Static and Sequential\nPinterest  recently released “Rethinking Personalized Ranking at Pinterest: An End-\nto-End Approach”  by Jiajing Xu et al. describing its personalized recommendation\nsystem, which is built to leverage raw user actions. The recommendation task is\ndecomposed into modeling users’ long-term and short-term intentions.\nAttention Architectures | 301\nThe process of comprehending long-term user interests is accomplished by training\nan end-to-end embedding model, referred to as PinnerFormer, to learn from a\nuser’s historical actions on the platform. These actions are subsequently transformed\ninto user embeddings, which are designed for optimization based on anticipated\nlong-term future user activities.\nThis procedure employs an adapted transformer model to operate on users’ sequen‐\ntial actions with the intent to forecast their long-term future activities. Each user’s\nactivity is compiled into a sequence, encompassing their actions over a specific time\nwindow, such as one year. The graph neural network–based (GNN-based) PinnerSage\nembeddings, in conjunction with relevant metadata (for example, the type of action,\nthe timestamp, and so forth), are used to add features to each action in the sequence.\nDistinct from traditional sequential modeling tasks and sequential recommendation\nsystems, PinnerFormer is designed to predict extended future user activities rather\nthan the immediately subsequent action. This objective is achieved by training the\nmodel to foresee a user’s positive future interactions over a window of 14 days\nfollowing the embedding’s generation. In comparison, traditional sequential models\nwould anticipate only the subsequent action.\nThis alternate approach allows for the embedding generation to occur offline in a\nbatch-processing mode, resulting in significant reductions in infrastructure needs. In\ncontrast to most traditional sequential modeling systems, which operate in real time\nand incur substantial computational and infrastructure costs, these embeddings can\nbe produced in batches (for instance, on a daily basis) rather than every time a user\nperforms an action.\nA dense all-action loss is introduced in this methodology to facilitate batch training\nof the model. The objective here is not to predict the immediate next action but\nrather all the actions the user will undertake over the subsequent k days. The aim is\nto predict all occurrences of a user’s positive interactions at intervals such as T+ 3,\nT+ 8, and T+ 12 , thereby compelling the system to learn long-term intentions.\nWhile traditionally the last action’s embedding is used to make the prediction, the\ndense all-action loss employs randomly selected positions in the action sequence,\nand the corresponding embedding is used to predict all actions for each of those\npositions.\nBased on offline and online experimental results, the use of dense all-action loss\nto train for long-term user actions has significantly bridged the gap between batch\ngeneration and real-time generation of user embeddings. Moreover, to accommo‐\ndate users’ short-term interests, the transformer model retrieves the most recent\nactions for each user in real time, processing them along with the long-term user\nembeddings. \n302 | Chapter 17: Sequential Recommenders",5700
131-Summary.pdf,131-Summary,"Summary\nTransformers and sequential recommendation systems are really at the cutting edge\nof modern recommenders. These days, most research in recommendation systems is\nin the area of sequential datasets, and the hottest recommenders are using longer and\nlonger sequences for prediction. Two important projects are worthy of attention:\nTransformers4Rec\nThis  open source project is geared toward scalable transformer models by the\nNVIDIA Merlin team. For more details, see “Transformers4Rec: Bridging the\nGap Between NLP and Sequential/Session-Based Recommendation”  by Gabriel\nde Souza Pereira Moreira et al.\nMonolith\nAlso  known as the TikTok For Y ou page recommender, this is one of the most\npopular and exciting recommendation systems at this time. It is a fundamentally\nsequential recommender, with some elegant hybrid approaches. “Monolith: Real-\nTime Recommendation System with Collisionless Embedding Table”  by Zhuoran\nLiu et al. covers the architectural considerations.\nOur final step before this book concludes is to consider a few approaches to recom‐\nmendations. These don’t build exactly on top of what we’ve done but will use some of\nwhat we’ve done and introduce a few new ideas. Let’s sprint to the finish!\nSummary | 303",1257
132-Neural Message Passing.pdf,132-Neural Message Passing,"CHAPTER 18\nWhat’s Next for Recs?\nWe find ourselves in a transitionary time for recommendation systems. However, this\nis quite normal for this field, as it is in many segments of the tech industry. One of\nthe realities of a field that is so closely aligned with business objectives and with such\nstrong capabilities for business value is that the field tends to be constantly searching\nfor any and all opportunities to advance.\nIn this chapter, we’ll briefly introduce some of the modern views of where recommen‐\ndation systems are going. An important point to consider is that recommendation\nsystems as a science spread both depth first and breadth first simultaneously. Looking\nat the most cutting-edge research in the field means that you’re seeing deep optimiza‐\ntion in areas that have been under study for decades or areas that seem like pure\nfantasy for now.\nWe’ve chosen three areas to focus on in this final chapter. The first you’ve seen a\nbit of throughout this text: multimodal recommendations. This area is increasingly\nimportant as users turn to platforms to do more things. Recall that multimodal\nrecommendations occur when a user is represented by several latent vectors simulta‐\nneously.\nNext up is graph-based recommenders. We’ve discussed co-occurrence models, which\nare the simplest such models for graph-based recommendation systems. They go\nmuch deeper! GNNs are becoming an incredibly powerful mechanism for encoding\nrelations between entities and utilizing these representations, making them useful for\nrecommendations.\nFinally, we’ll turn our attention to large language models and generative AI. During\nthe writing of this book, LLMs have gone from something that a small subset of ML\nexperts understood to something mentioned on HBO comedy broadcasts. While a\nrush is occurring to find relevant applications of LLMs to recommendation systems,\n305\nthe industry already has confidence in applying these tools in certain ways. Also\nexciting, however, is the application of recommendation systems to LLM apps.\nLet’s see what’s coming next!\nMultimodal Recommendations\nMultimodal recommenders  allow  for the concession that users contain multitudes :\na single representation for a user’s preferences may not capture the entire story.\nSomeone shopping on a large everything-ecommerce website, for example, may be all\nof the following:\n•A dog owner who frequently needs items for their dog•\n•A parent who is always updating the closet for the growing baby•\n•A hobbyist race-car driver who buys the pieces necessary to drive their car on a•\ntrack\n•A LEGO investor who keeps hundreds of sealed boxes of Star Wars sets hidden•\naway in the closet\nThe methods you’ve learned throughout this book should do well at providing rec‐\nommendations for all of these users. However, you may notice in this list a few areas\nthat are conflicting:\n•If your child is very young, why do you buy LEGO sets already? Also, doesn’t•\nyour dog chew on them?\n•If your garage is full of LEGO sets, where do you keep all these car parts?•\n•Where do you put your dog in that two-seater Mazdaspeed MX-5 Miata?•\nY ou can probably think of other cases where some aspects of what you buy just don’t\nmatch up well with others. This leads to a problem of multimodality: several places in\nthe latent space of your interests coalesce into modes or medoids, but not only one.\nLet’s return to some of our geometric discussions from before: if you are using\nnearest neighbors to a user vector, then which of the medoids will take on the most\nimportance?\nThe way we approach this problem is by multimodality, or providing several vectors\nassociated to a single user. While a naive approach to scaling to consider all the\nmodes for a user would be to simply increase the dimensionality of the model on the\nitem side (to create more areas in which different types of items can be embedded\ndisjointly), this presents serious challenges at scale in terms of training and memory\nconcerns.\n306 | Chapter 18: What’s Next for Recs?\nOne of the first significant works in this area is coauthored by one of this book’s\nauthors and introduces an extension to MF to deal with this; see “Nonlinear Latent\nFactorization by Embedding Multiple User Interests”  by Jason Weston et al. The\ngoal is to build multiple latent factors simultaneously as we did in our other matrix\nfactorization methods, each factor hopefully taking on representation for one of the\nuser’s interests.\nThis is achieved by constructing a tensor that has its third tensor dimension represent\neach of the latent factors for distinct interests rather than encoding a user item\nfactorization matrix. The factorization is generalized to the tensor case, and the\nWSABIE loss you saw earlier is used to train.\nBuilding on this work, several years later Pinterest released PinnerSage, as we men‐\ntioned in Chapter 15 . This modifies some of the assumptions of the Weston et al.\npaper, by not assuming a known number of representations for each user. Addition‐\nally, this approach uses graph-based feature representations, which we’ll talk more\nabout in the next section. Finally, the last important modification that this method\nuses is clustering: it attempts to build the modes via clustering in item space.\nThe basic PinnerSage approach is to do the following:\n1.Fix item embeddings (they call these pins). 1.\n2.Cluster user interactions (unsupervised and unspecified in cardinality).2.\n3.Build cluster representations as the medoid of the cluster embeddings.3.\n4.Retrieve using medoid-anchored ANN search.4.\nPinnerSage is still considered to be near state of the art for large-scale multimodal\nrecommenders. Some systems take another approach to allow users to more directly\nmodify their “mode” by selecting the theme of what they’re looking for, while others\nhope to learn it from a sequence of interactions.\nNext up, we’ll look at how higher-order relationships between items or users can be\nexplicitly specified.\nGraph-Based Recommenders\nGraph neural networks  (GNNs) are a class of neural networks that use the structural\ninformation of data to build deeper representations of your data. They’ve proven\nespecially useful when dealing with relational or networked data, both of which have\nutility.\nGraph-Based Recommenders | 307\nOne moment of disambiguation before we continue: graphs  in the sense that we will\nuse them here refer to collections of nodes  and edges . These are purely mathematical\nconcepts, but generally we can think of nodes as the objects of interest and edges as\nthe relationships between them. These mathematical objects are useful for distilling\ndown the core of what is necessary for the kind of representation you wish to\nbuild. While the objects may seem very simple, we can add just the right amount of\ncomplexity in a variety of ways to capture more nuance.\nIn the simplest setups, each node on the graph represents an item or user, and each\nedge represents a relationship such as a user’s interaction with an item. However,\nuser-to-user and item-to-item networks are extremely powerful extensions as well.\nOur co-occurrence models are simple graph networks; however, we did not learn a\nrepresentation from these and instead directly took these as our models.\nLet’s consider a few examples of adding more structure to a graph to encode ideas:\nDirectionality\nThis  ordering on an edge’s vertices can be added to indicate a strict relationship\nof one node acting on the other; e.g., a user reads  a book but not the other way\naround.\nEdge decorations\nDescriptors  such as edge labels can be added to communicate features about the\nrelationships; e.g., two users share account credentials, and one of the users is\nidentified  as a child .\nMultiedges\nThese  can allow for relationships to have higher multiplicity, or allow for the\nsame two entities to have multiple relationships. In a graph of outfits with\nclothing items as nodes, each edge can be another clothing item that makes the\nother two go well together.\nHyper-edges\nA step further up the level of abstraction may add these edges, which connect\nmultiple nodes simultaneously. For video scenes, you may detect objects of vari‐\nous classes, and your graph may have nodes for those classes, but understanding\nnot only which pairs of object classes appear but which higher-order combina‐\ntions appear can be identified with hyper-edges.\nLet’s explore the basics of GNNs and how their representations are a bit different.\nNeural Message Passing\nIn GNNs our object of interest is assigned as the nodes in our graph. Usually, the\nmain objective in GNNs is to build powerful representations of the nodes and edges,\nor both, via their relationships.\n308 | Chapter 18: What’s Next for Recs?\nThe fundamental difference between GNNs and traditional neural networks is that\nduring the training, we’re explicitly using operators that transfer data between node\nrepresentations “along the edges. ” This is called message passing . Let’s start with an\nexample to prime the basic idea.\nLet nodes represent users, and their features are persona details such as demographic,\nonboarding survey question, etc. Let edges be the social network graph: are they\nfriends? And let’s add decoration to the edges, such as the number of DMs exchanged\nbetween them on the platform. If we are the social media company that wants to\nintroduce ad shopping to our platform, we may start with those persona features, but\nwe’ d ideally like to use something about this network of communication. In theory,\npeople who communicate and share content with each other a lot may have similar\ntastes. Somewhat tellingly, we introduce a concept called a  message function , which\nallows features to be sent from node to node. The message function uses features\nfrom each node and the edge between them, written mathematically as follows, for\nℎik the features at node i and ℎjk at node j, respectively:\nmijk=ℳℎik,ℎjk,eij\nThe features of the edge are eij and ℳ is some differentiable function. Note that the\nsuperscript k refers to the layer as is standard in back-prop notation. Here are two\nsimple examples:\n•mijk=ℎik means “take the features from a neighbor node” •\n•mijk=ℎik\ncij means “average by the number of edges between i and j"" •\nMany powerful message-passing schemes that use learning use approaches from\nother areas of ML—like adding an attention mechanism on node features—but this\nbook doesn’t dive deep into this theory.\nThe next function we’ll introduce is the  aggregation function , which takes as input the\ncollection of messages and aggregates them. The most common types of aggregation\nfunctions do the following:\n•Concatenate all the messages•\n•Sum all the messages•\n•Average all the messages•\n•Take the max of the messages•\nGraph-Based Recommenders | 309",10916
133-Random Walks.pdf,133-Random Walks,"Finally, we will use the output of the aggregation as part of our update function,\nwhich takes node features and aggregated message functions and then applies addi‐\ntional transformations. If you’ve been wondering, “Where does this model learn\nanything?” the answer is in the update function. The update function usually has a\nweight matrix associated to it, so as you train this neural network, you are learning\nthe weights in the update function. The simplest update functions multiply a weight\nmatrix by the vectorized output of your aggregation and then apply an activation\nfunction per vector.\nThis chain of message passing, aggregating, and updating is the core of GNNs and\nencompasses a broad capability. They’ve been useful for ML tasks of every kind,\nincluding recommendations. Let’s see some direct applications to recommendation\nsystems.\nApplications\nLet’s revisit some of the high-level ideas that GNNs may touch in the RecSys space.\nModeling user-item interactions\nIn other methods we’ve presented, such as matrix factorization, the interactions\nbetween users and items are considered, but the complex network among users or\nitems is not exploited. In contrast, GNNs can capture the complex connections in the\nuser-item interaction graph and then use the structure of this graph to make more\naccurate recommendations.\nThinking back to our message passing, it allowed us to “spread” the information of\nsome nodes (in this case, user and items) to their neighbors. An analogy for this\nwould be that as a user interacts more and more with items with specific features,\nsome of those features are imbued onto the user. This may sound similar to latent\nfeatures, because it is! These are ultimately helping the network build a latent repre‐\nsentation from the messages that pass features from items to user. This can be even\nmore powerful than other latent embedding methods, because you explicitly define\nthe structural relationships and how they communicate these features.\nFeature learning\nGNNs  can learn more expressive feature representations of nodes (users or items)\nin a graph by aggregating feature information from their neighbors, leveraging the\nconnections between nodes. These learned features can provide rich information\nabout users’ preferences or items’ characteristics, which can greatly enhance the\nperformance of recommendation systems.\n310 | Chapter 18: What’s Next for Recs?\nPreviously, we talked about how a user’s representations can learn from the items they\ninteract with, but items can also learn from one another. Similar to the way item-item\ncollaborative filtering (CF) allows items to pick up latent features from shared users,\nGNNs allow us to add potentially many other direct relationships between items.\nCold-start problem\nRecall  our cold-start problem: providing recommendations for new users or items is\ndifficult because of the lack of historical interactions. By using the features of nodes\nand the structure of the graph, GNNs can learn the embeddings for new users or\nitems, potentially alleviating the cold-start problem.\nIn some of our graphical representations of our user graph, the edges need not only\nexist between users with lots of prior recommendations. It’s possible to use other user\nactions to bootstrap  some early edges. Structural edges like “share a physical location”\nor “invited by the same user” or “answers onboarding questions similarly” can be\nenough to quickly bootstrap several user-user edges, which allow us to warm-start\nrecommendations for them.\nContext-aware recommendations\nGNNs can incorporate contextual information into the recommendation process. For\nexample, in a session-based recommendation, a GNN can model the sequence of\nitems a user has interacted with in a session as a graph, where each item is a node and\nthe sequential order forms edges. The GNN can then learn the dynamic and complex\ntransitions among items to make context-aware recommendations.\nThese high-level ideas should point to the opportunity in graph encoding for recom‐\nmender problems, but let’s look at two specific applications next: random walks and\nmetapaths.\nRandom Walks\nRandom walks in GNNs enable methods to use the user-item interaction graph to\nlearn effective node (i.e., user or item) embeddings. The embeddings are then used\nto make recommendations. In the context of graphs, a random walk is an iterative\nprocess of starting on a particular node and then stochastically moving to another\nconnected node via a randomized choice.\nOne popular random-walk-based algorithm for network embedding is DeepWalk,\nwhich has been adapted and extended in many ways for various tasks, including\nrecommendation systems.\nGraph-Based Recommenders | 311\nHere’s how a random-walk GNN approach might work in a recommendation context:\n1.Random walks generation: start by performing random walks on the interac‐1.\ntion graph. Starting from each node, make a series of random steps to other\nconnected nodes. This results in a set of paths, or “walks, ” that represent the\nrelationships between different nodes.\n2.Node embeddings: the sequences of nodes generated by the random walks are2.\ntreated similar to sentences in a corpus of text, and each node is treated like a\nword. Word2vec or similar language-modeling techniques are then used to learn\nembeddings for the nodes (vector representations), such that nodes appearing in\nsimilar contexts (in the same walks) have similar embeddings.\n3.Recommendations: once you have learned node embeddings, you can use them3.\nto make recommendations. For a given user, you might recommend items that\nare “close” to that user in the embedding space, according to a distance metric.\nThis can use all the techniques we’ve previously developed for recommendations\nfrom latent space representations.\nThis approach has some nice properties:\n•It can capture the high-order connections in the graph. Each random walk can•\nexplore a part of the graph that’s not directly connected to the starting node.\n•It can help with the sparsity problem in recommender systems because it uses the•\ngraph’s structure to learn representations, which requires less interaction data.\n•It naturally attempts to handle cold-start issues. For new users or items with few•\ninteractions, their embeddings can be learned from connected nodes.\nNevertheless, this approach has some challenges. Random walks can be computa‐\ntionally expensive on large graphs, and it might be difficult to choose appropriate\nhyperparameters, such as the length of the random walks. Also, this approach may\nnot work as well for dynamic graphs, where interactions change over time, since it\ndoesn’t inherently consider temporal information.\nThis method implicitly assumes that the nodes are heterogeneous, and so co-\nembedding them via connections is natural. While it was not an explicit requirement,\nthe type of sequence embeddings DeepWalk builds tends to structurally assume this.\nLet’s break this rule to accommodate learning between heterogeneous types in our\nnext architecture example, metapaths.\n312 | Chapter 18: What’s Next for Recs?",7183
134-Metapath and Heterogeneity.pdf,134-Metapath and Heterogeneity,,0
135-LLM Recommenders.pdf,135-LLM Recommenders,"Metapath and Heterogeneity\nMetapath  was introduced to improve explainable recommendations and integrate the\nideas of knowledge graphs with GNNs.\nA metapath  is a path in a heterogeneous network (or graph) that connects different\ntypes of nodes via different types of relationships. Heterogeneous networks contain\nvarious types of nodes and edges, representing multiple types of objects and interac‐\ntions. Beyond simply users and items, the node types can be “carts of items” or\n“viewing sessions” or “channel used for purchase. ”\nMetapaths can be used in GNNs for handling heterogeneous information networks\n(HINs). These networks provide a more comprehensive representation of the real\nworld. When used in a GNN, a metapath provides a scheme for the way information\nshould be aggregated and propagated through the network. It defines the type of\npaths to be considered when pooling information from a node’s neighborhood.\nFor example, in a recommender system, you might have a heterogeneous network\nwith users, movies, and genres as node types, and “watches” and “belongs to” as edge\ntypes. A metapath could be defined as “User - watches → Movie - belongs to →\nGenre - belongs to → Movie - watches → User. ” This metapath represents a way of\nconnecting two users through the movies they watch and the genres of those movies.\nA popular method that utilizes metapaths is the heterogeneous GNN (Hetero-GNN)\nand its variants. These models leverage the metapath concept to capture the rich\nsemantics in HINs, enhancing the learning of node representations.\nMetapath-based models have shown promising results in various applications, as they\nallow you to explicitly encode much more abstract relationships into the message-\npassing mechanisms we’ve mentioned.\nIf higher-order modeling is your thing, buckle up for the last concept we’ll cover in\nthis book. This topic is state of the art and full of high-level abstractions. Language-\nmodel-backed agents are at the absolute cutting edge of ML modeling.\nLLM Applications\nAll of the superlatives for LLMs have been used up. For that reason, we’ll just say this:\nLLMs are powerful and have a surprisingly large number of applications.\nLLMs are general models that allow users to interact with them via natural language.\nFundamentally, these models are generative (they write text) and auto-regressive\n(what they write is determined by what came before). Because LLMs can speak\nconversationally, they’ve been branded as general artificial agents . It’s natural to then\nask, “Can an agent recommend things for me?” Let’s start by examining how to use an\nLLM to make recommendations.\nLLM Applications | 313",2684
136-LLM Training.pdf,136-LLM Training,"LLM Recommenders\nNatural language is a wonderful interface to ask for recommendations. If you want\na coworker’s recommendation for lunch, maybe you’ll show up at their desk and\nsay nothing—hoping they’ll remember their latent knowledge of your preferences,\nidentify the time-of-day context, recall the availability of restaurants based on day-of-\nweek, and keep in mind that yesterday you had a pastrami sandwich.\nMore effectively, you could simply ask, “ Any suggestions for lunch?”\nLike your astute coworker, models may be more effective at providing recommenda‐\ntions if you simply ask them to. This approach also adds the capability of defining\nmore precisely the kind of recommendation you want. A popular application of\nLLMs is to ask them for recipes that use a set of ingredients. Thinking through this in\nthe context of the kind of recommenders we’ve built, building a recommender of this\nkind has some hurdles. It probably needs some user modeling, but it’s very dependent\non the items specified. This means that there’s a very low signal for each combination\nof specified items.\nAn LLM, on the other hand, is quite effective at the autoregressive nature of this\ntask: given a few ingredients, what’s most likely to be included next in the context of\na recipe. By generating several items like this, a ranking model can augment this to\nprovide a realistic recommender.\nLLM Training\nLarge generative language models of the type that have exploded in popularity are\ntrained in three stages:\n1.Pretraining for completion1.\n2.Supervised fine-tuning for dialogue2.\n3.Reinforcement learning from human feedback3.\nSometimes the latter two steps are combined into what is called Instruct . For an\nexceptionally deep dive into this topic, see the original InstructGPT paper “Training\nLanguage Models to Follow Instructions with Human Feedback”  by Long Ouyang et\nal.\n314 | Chapter 18: What’s Next for Recs?\nLet’s recall that text-completion tasks are equivalent to training the model to predict\nthe correct word in a sequence after seeing k previous ones. This may remind you of\nGloVe from Chapter 8 , or our discussion about sequential recommenders.\nNext up is fine-tuning for dialogue; this step is necessary to teach the model that the\n“next word or phrase” should sometimes be a response instead of an extension of the\noriginal statement.\nDuring this stage, the data used for this training is in the form of demonstration data ,\ni.e., pairs of statements and responses. Examples include the following:\n•A request and then a response to that request•\n•A statement and then a translation of that statement•\n•A long text and then a summarization of that text•\nFor recommendations, you can imagine that the first is highly relevant to the task we\nhope the model to demonstrate.\nFinally, we move to the reinforcement learning from human feedback (RLHF) stage;\nthe goal here is to learn a reward function that we can later use to further optimize\nour LLM. However, the reward model itself  needs to be trained. Interestingly for\nrecommendation systems enthusiasts like yourself, AI engineers do this via a ranking\ndataset.\nA large number of tuples—similar to the demonstration data we’ve seen—provide\nstatements and responses, although instead of only one response, there are multiple\nresponses. They are ranked (via a human labeler), and then for each pair of superior-\ninferior responses x,sup,inf , we evaluate the loss:\n•rsup=Θx,sup  is the reward model’s score for the superior response. •\n•rinf=Θx,inf  is the reward model’s score for the inferior response. •\nThe final loss is computed: −logσsup −inf .\nThis reward function is then used to fine-tune the model.\nOpenAI summarizes this approach via the diagram in Figure 18-1 .\nLLM Applications | 315\nFigure 18-1. Instruct methodology for model fine-tuning\n316 | Chapter 18: What’s Next for Recs?",3909
137-Instruct Tuning for Recommendations.pdf,137-Instruct Tuning for Recommendations,,0
138-Summary.pdf,138-Summary,"From this brief overview, you can see that these LLMs are trained to respond to\nrequests—something well suited for a recommender. Let’s see how to augment this\ntraining.\nInstruct Tuning for Recommendations\nIn the previous discussion of instruct pairs, we saw that ultimately the aim of the\ntraining was to learn a rank comparison between two responses. This kind of training\nshould feel quite familiar. In “TALLRec: An Effective and Efficient Tuning Frame‐\nwork to Align Large Language Model with Recommendation”  by Keqin Bao et al., the\nauthors use a similar setup to teach user preferences to the model.\nAs the paper mentions, historical interaction items are collected into two groups\nbased on their ratings: user likes and user dislikes. They collect this information into\nnatural language prompts to format a final “Rec Input”:\n1.User preference: item 1, . . . ,itemn] 1.\n2.User preference: item 1, . . . ,itemn] 2.\n3.Will the user enjoy the User preference, itemn+ 1]? 3.\nThese follow the same training pattern as InstructGPT noted previously. The authors\nachieve dramatically improved performance on recommender problems as compared\nto an untrained LLM for recommendations; however, those should be considered\nbaselines as it’s not their target task.\nLLM Rankers\nSo far in this chapter, we’ve thought of the LLM as a recommender in totality, but\ninstead, the LLM can be used as simply the ranker. The most trivial approach to this\nis to simply prompt the LLM with the relevant features of a user and a list of items\nand ask it to suggest the best options.\nWhile naive, variants on this approach have seen somewhat surprising results in very\ngeneric settings: “The user wants to watch a scary movie tonight and isn’t sure which\nwill be the best if he doesn’t like gore: movie-1, movie-2, etc. ” But we can do better.\nUltimately, as with LTR approaches, we can think of pointwise, pairwise, and list‐\nwise. If we wish to use an LLM for a pointwise ranking, we should constrain our\nprompting and responses to a setting in which these models may be useful. Take,\nfor example, a recommender for scientific papers; a user may wish to write what\nthey’re working on and have the LLM helpfully suggest papers of relevance. While a\ntraditional search problem, this is a setting in which our modern tools can bring a lot\nof utility: LLMs are effective at summarizing and semantic matching, which means\nthat semantically similar results may be found from a large corpus, and then the\nLLM Applications | 317\nagent can synthesize the output of those results into a cogent response. The biggest\nchallenge here is hallucination, or suggesting papers that may not exist.\nY ou can think of pairwise and listwise similarly: distilling the reference data into a\nshape that the unique capabilities of these LLMs can use to make significant assists.\nWhile we’re near the topic of search and retrieval, it’s important to mention one of the\nways in which recommendation can help LLM applications: retrieval augmentation.\nRecommendations for AI\nWe’ve  seen how LLMs can be used to generate recommendations, but how do rec‐\nommenders improve LLM applications? LLM agents are extremely general in their\ncapabilities but lack specificity on many tasks. If you ask an agent, “Which of the\nbooks I read this year were written by nonwestern authors?” the agent has no chance\nof success. Fundamentally, this is because the general pretrained models have no idea\nwhat books you’ve read this year.\nTo solve for this, you’ll want to leverage retrieval augmentation , i.e., providing rele‐\nvant information to the model from an existing data store. The data store may be\nan SQL database, a lookup table, or a vector database, but ultimately the important\ncomponent here is that somehow from your request, you’re able to find relevant\ninformation and then provide it to an agent.\nOne assumption we’ve made here is that your request is interpretable by your\nretrieval system. In the preceding example, you’ d like the system to automatically\nunderstand the “which of the books I read this year” phrase as an information-\nretrieval task equivalent to something like this:\nSELECT * FROM read_books\nWHERE CAST(finished_date , YEAR) = CAST(today(), YEAR)\nHere we’ve just made up an SQL database, but you can imagine schema to satisfy this\nrequest. Converting from the request to this SQL is now yet another task you need to\nmodel—maybe it’s the job of another agent request.\nIn other contexts, you want a full-scale recommender to help with the retrieval: if\nyou want users to ask an agent for a movie tonight, but also to continue to use your\ndeep understanding of each user’s tastes, you could first filter the potential movies by\nthe user’s preference and then send only movies your recommender model thinks are\ngreat for them. The agent can then service the text request from a subset of movies\nthat are already determined to be great.\nThe intersection of LLMs and recommendation systems is going to dominate\nmuch of the conversation in recommendation systems for a while. There’s a lot of\nlow-hanging  fruit in bringing the knowledge of recommender systems to this new\nindustry. As Eugene Y an recently said:\n318 | Chapter 18: What’s Next for Recs?\nI think the key challenge, and solution, is getting them [LLMs] the right information\nat the right time. Having a well-organized document store can help. And by using a\nhybrid of keyword and semantic search, we can accurately retrieve the context that\nLLMs need.\nSummary\nThe future of recommendation systems is bright, but the technology will continue\nto get more complicated. One of the major changes over the last five years has been\nan incredible shift to GPU-based training and the architectures that can use these\nGPUs. This is the primary motivation for why this book favors JAX over TensorFlow\nor Torch.\nThe methods in this chapter embrace bigger models, more interconnections, and\npotentially inference on a scale that’s hard to house in most organizations. Ultimately,\nrecommendation problems will always be solved via the following:\n•Careful problem framing•\n•Deeply relevant representations of users and items•\n•Thoughtful loss functions that encode the nuances of the task•\n•Great data collection•\nSummary | 319",6334
139-Index.pdf,139-Index,"Index\nSymbols\n@ k, 218\nA\nA/B testing, 116, 215\nablation, 245\nacceleration structures\ncheaper retrieval methods, 290\ndefinition of term, 281\nk-d trees, 284-288\nk-means clustering, 288\nlocality sensitive hashing (LSH), 282-284\nsharding, 282\nstrategies for, 281\nterminology related to, 86\nactive learning\nadvantages of, 122\ncore ideas of, 122\nimplementing, 123\noptimization, 123-125\nstructure of, 123\nadd-to-bag action, logging, 23\naffinity, 206, 256, 268\nagents, 37\naggregation function, 309\nAirflow, 85, 107\nalerting and monitoring\nintegration tests, 109\nobservability, 110\nschemas and priors, 108\nallocation plan, 159\nalternating least squares (ALS), 174ambient space, 40\nanchor-items, 277\napproximate nearest neighbors (ANN), 42, 87,\n108, 289\napproximate rank, 233\narchitectures\nalerting and monitoring, 108-111\ncontinuous training and deployment,\n113-117\ndefinition of term, 95\nencoder architectures and cold starting,\n100-102\nevaluation flywheel, 117-125\nevaluation in production, 111-113\nmodel deployment, 103-107\nmodel-service architectures, 157\nby recommendation structure, 95-100\narea under the receiver operating characteristic\ncurve (AUC-ROC), 225\nartificial intelligence (AI), 318\nasynchronous server gateway interface (ASGI),\n104\nattention architectures\nBERT4Rec, 300\nintroduction to, 299\nmerging static and sequential, 301\nrecency sampling, 301\nself-attentive sequential recommendation\n(SASRec), 300\nattention matrix, 300\nattribution, 26\n321\nAUC-ROC (area under the receiver operating\ncharacteristic curve), 225\nautoregression, 99, 113\navoids\nalternate terms for, 266\nhand-tuned rankings as, 268\nhard avoids, 265\nimplementing, 269-271\nmodel-based, 271\nB\nbasis vector, 161\nbatch processing, 44\nbatched coordinate format, 162\nBatchNorm layer, 55\nBayesian personalized ranking (BPR), 226\nBERT4Rec model, 300\nbest matching 25 (BM25) algorithm, 236\nbias\ndiversification of recommendations,\n274-276\nfairness, 279\nmultiobjective functions, 277\npredicate pushdown, 278\ntypes of, 273\nbig data frameworks (see also PySpark)\ncluster frameworks, 132\noptions for, 130\nbilinear factor models\nbasics of, 149-153\nlimitations of, 157\nbilinear regression, 151\nbinary cross-entropy loss, 231\nbinary prediction, 124\nbloom filters, 46, 88-90\nBM25 (best matching 25) algorithm, 236\nboosting, 100\nBPR (Bayesian personalized ranking), 226\nbroadcasting (in JAX), 9\nbusiness insight, 26\nbusiness logic\nchallenges of recommendation systems, 265\nhand-tuned weights, 267\nhard rankings, 19, 266\nimplementing avoids, 269-271\ninventory health, 268\nlearned avoids, 266model-based avoids, 271\nordering and application of, 121\nbusiness metrics, 245\nC\ncascading, 157\ncausal scrubbing, 245\nCD (continuous deployment), 107\ncentered kernel alignment, 206\nCF (see collaborative filtering)\nCG (cumulative gain), 222\nCI (continuous integration), 107\nclassification, for ranking, 231\nclick-through rate (CTR), 159\nclicks, logging, 22\nclickstream data, 23, 24\nclosed-loop feedback, 209\ncluster frameworks, 132\ncluster manager, 75\nCNN tower, 60\nco-coverage, 124\nco-occurrence\nco-occurrence matrix, 140\nco-occurrence models, 171\ncomputing of, 161\ndependent on context, 160\nhigher-order, 161\npointwise mutual information via, 162\nrepresenting sparse matrices, 162\nsimilarity from, 163-164\ncode\nexperimental code, 241-246\nobtaining and using code examples, xvi,\n128, 246\ncold starting, 99-102, 128, 311\ncollaborative filtering (CF)\nbasics of, 16\nfilter bubbles, 279\nitem-based, 34\nuser similarity for, 34-37\nvectors from, 86\ncollection (see data collection and user logging)\ncollector\ncollector logs, 120\nin explore-exploit systems, 39\noffline versus online, 44\nrole in recommendation systems, 4\n322 | Index\ncombine step, 134\ncomments and questions, xvii\ncommutative groups, 135\ncomplementary items, estimating, 163\nconditional MPIR, 141, 161\nconfounding variables, 210\nconstraints, 266\ncontainerization, 106\ncontent-based feature vector, 150\ncontent-based recommender\nconvolutional neural network definition, 55\ndataset used, 49\ndevelopment environment, 49\ninput pipeline, 58-70\nobtaining STL dataset images, 54\noptimizing, 56-58\nPython build systems, 51\nrandom-item recommender, 52-54\nrevision control software, 50\ncontext\ncontext-aware recommendations, 311\ncontext-based recommendations, 98\ndefinition of term, 98\ncontinuous deployment (CD), 107\ncontinuous integration (CI), 107\ncontinuous training and deployment\ndeployment topologies, 114-117\nmodel drift, 113\ncontroller program, 132\nconvolutional neural networks (CNNs)\ndefining, 55\nfor sequential recommendation, 298\ncorrelation\ncorrelation analysis, 206\ncorrelation coefficients, 223\ncorrelation identifiers, 119\ncorrelation metric spaces, 36\ncorrelation mining, 160-162\nPearson correlation, 35\ncosine similarity, 150, 164, 170\nCosRec, 298\ncounterfactual evaluation, 209\ncounting recommenders\ncorrelation mining, 160-162\nmost-popular-item recommender (MPIR),\n159pointwise mutual information via co-\noccurrences, 162\ncovariates, 21\nCron, 107\ncross-entropy loss, 231\nCTR (click-through rate), 159\ncumulative gain (CG), 222\ncurse of dimensionality, 167\nD\nDagster, 85, 107\ndaily warm starts, 117\ndata collection and user logging\ncollection and instrumentation, 23\ndata sparsity, 32-34, 99\nfunnels, 24\nwhat to log, 19-23\ndata processing\nacceleration structures for, 86\napproximating nearest neighbors, 87\nbig data frameworks, 130-142\ndata leakage, 93\ndata representation, 129\ndata warehouses, 85\ndebugging print statements, 242\nfeature stores, 90-93\nimporting data with PySpark, 73-77\nprescribing how data is batched, 82-84\nsnapshotting production databases, 84\ntechnology stacks, 128\ntesting for set inclusion, 88-89\nDataLoaders, 82-84\nDeepWalk algorithm, 311\ndemand forecast, 268\ndemographic-based systems, 155\ndemonstration data, 315\ndense all-action loss, 302\ndense representations, 15\ndeployment\ncontinuous training and deployment,\n113-117\nmodels as APIs, 103\nspinning up a model service, 104\nworkflow orchestration, 105\ndevelopment environment (see environments)\ndimensionality, 153, 167\ndimensionality reduction\nIndex | 323\ncentered kernel alignment, 206\nextracting most relevant features, 202\nfactorization, 172\nisometric embeddings, 203-204\nnonlinear locally metrizable embeddings,\n204\noptimizing for MF with ALS, 174-176\nprimary concepts of, 199-202\nregularization for MF, 176\nregularized MF implementation, 177-198\nWSABIE, 199\ndirectionality, 308\ndiscrete co-occurrence distribution, 171\ndissimilarity function, 34\ndistances, 34\ndistributed computing, 104\ndistributions, comparing, 112\ndiversification of recommendations, 274-276\ndivide and conquer strategy, 284\nDocker, 106\ndot-product similarity, 169, 275\ndoubly-robust estimation (DRE), 212\ndriver program, 75\ndrop offs, 24\ndual model, 155\ndual-encoder networks, 100\nE\nedge decorations, 308\nedges, 308\neigenvectors, 200, 201\nELK stack (Elasticsearch, Logstash, Kibana),\n119\nembedding models\nboosted models, 100\nGloVE embedding, 141-146\nisometric embeddings, 203-204\nnaive sequence embeddings, 99\nnonlinear locally metrizable embeddings,\n204\nonline and offline collectors, 45\nout of distribution queries, 98\nembedding vector, 55\nencoders\ndefinition of term, 102\nencoder as a service, 102\nensemble modeling, 114entropy, 124\nenvironments\nA/B testing, 215\ncontainerization and, 106\ndevelopment environment, 49\nhistorical evaluation data, 215\nonline and offline evaluation, 214\nuser versus item metrics, 215\nvirtual environments, 51\nevaluation flywheel\nactive learning, 122-125\ndaily warm starts, 117\nlambda architecture and orchestration, 118\nlogging, 119-122\nevaluation in production\nmodel metrics, 112\nslow feedback, 111\nevents, definition of term, 23\nexecutors, 75\nexperimentation\nA/B testing, 116\nexperimentation routing, 121\ntips for, 241-246\nexplore-exploit recommenders, 37, 275\nF\nFAANG (Meta [formerly Facebook], Apple\nAmazon, Netflix, and Google), xiv\nfactorization, 100, 172\nfairness, 279\nfallback, 111\nfalse positive rate (FPR), 225\nFastAPI, 106\nfeature augmentation/engineering, 157, 244,\n310\nfeature leakage, 93\nfeature stores, 90-93\nfeature-based recommendations\nbilinear factor models, 149-152\nfeature-based warm starting, 153-154\nhybridization, 157\nlimitations of bilinear models, 157\nsegmentation models, 155-156\nuser space versus item space, 152\nfeature-item recommenders, 141\nfeedback, 111, 208\nfilter bubbles, 279\n324 | Index\nfiltering, 45, 120\nFlax framework\nGloVE model specification, 143-146\nModule class, 60\noptimizing models with, 56-58\nforecasting, 268\nformatting, of logs, 121\nFPR (false positive rate), 225\nfreshness, 88\nFrobenius terms, 176\nfull-text search, 237\nfunnels, 24\nG\nGaussian Error Linear Unit (GeLU), 299\nGitHub/Git, 50, 128\nglobal optimization, 269\nGloVE embedding, 141-146\nGoogle Colab, 7\nGPU-based training, 319\ngradient descent\nmini-batched gradient descent, 82\nstochastic gradient descent, 82\nGramian regularization, 176, 275\ngraph neural networks (GNNs), 307-313\ngraph-based ANN, 289\ngreedy extend, 124\ngroup theory, 134\nGRU4Rec, 297\nH\nHamming distance, 283\nhand-tuned weights, 267\nhard avoids, 265\nhard rankings/ratings, 19, 266\nhard rules, 266\nhashing\nlocality sensitive hashing (LSH), 282-284\nmin-hashing, 142\nHellinger distance, 172\nheterogeneous information networks (HINs),\n313\nhierarchical k-means, 288\nhierarchical navigable small worlds, 289\nhigh-risk applications, 279\nhinge loss, 233HINs (heterogeneous information networks),\n313\nhistorical evaluation data, 215\nhorizontal scaling, 104\nhover action, logging, 21\nHPO (hyperparameter optimization), 185\nhuman-in-the loop ML, 46, 315\nhybridization, 157\nhydration, 73\n(see also data processing)\nhyper-edges, 308\nhyperparameter optimization (HPO), 185\nhyperparameter sweep, 65\nI\nideal DCG (IDCG), 222\nimage, 106\nimplicit feedback, 100\nimplicit queries, 96\nimplicit ratings, 19\nimportance sampling, 39\nimpressions, logging, 23\nincidence vector, 160\nincremental gains, 27\nindexing (in JAX), 9\nindustrial scale, 43\ninfluenced-based optimization, 124\ninformation bottleneck, 168\ninformation-retrieval tasks, 237\ninfrastructure as code, 106\nInstructGPT paper, 314\nintegration tests, 109\nintra-list diversity, 274\ninventory health, 268\ninverse propensity scoring (IPS), 209\nisometric embeddings, 203-204\nitem metrics, 215\nitem similarity, 17\nitem space, 152\nitem-based collaborative filtering, 34\nitem-item recommendations, 16, 165\nitem-to-user recommendations, 96\niteration, rapid, 245\nJ\nJaccard similarity, 164\nJacobian matrix, 83\nIndex | 325\nJAX framework\nbrief description of, xv, 7\nbroadcasting feature, 9\ndata immutability in, 8\ndesign philosophy of, 7\ndocumentation, 7\nexperimentation exercises, 261\nGloVE model specification, 143-146\nimplementing avoids in, 270\nindexing and slicing, 9\nJAX types, 7\njust-in-time compilation, 11\nNaN debugging, 243\noptimizing models with, 56-58\nrandom numbers, 10\nregularized MF implementation, 177-198\njobs, scheduling, 106\nJohnson-Lindenstrauss lemma, 284\njust-in-time (JIT) compilation, 11\nK\nk-d trees, 284-288\nk-means clustering, 288\nk-order statistic loss, 234\nKafka, 91, 119\nkernel methods, 203\nKullback–Leibler (KL) divergence, 112, 172\nL\nL2 loss function, 232\nLambda architecture, 78, 118\nlarge language models (LLMs)\nbasics of, 313\nInstruct tuning for recommendations, 317\nrankers, 317\nrecommendations for AI, 318\nrecommenders using, 314\ntraining, 314-317\nlatent space\ndefinition of term, 17\nhyper-parameter optimization for, 199\nlatent representations, 271\nlow-rank methods, 167-169\nutilizing, 86\nvector search and, 40\nLAX library, 258\nlazy evaluation, 75learning to rank (LTR) models, 230\nlikability, 40\nlinear regression, 151\nlistwise LTR models, 230\nLLMs (see large language models)\nlocality sensitive hashing (LSH), 282-284\nlogging (see also data collection and user log‐\nging)\nbest practices for, 119\ncollector logs, 120\ndebugging print statements, 242\nfiltering and scoring, 120\nformatting of, 121\nordering, 121\nlong-term holdout, 216\nloss functions\nbinary cross-entropy loss, 231\ndense all-action loss, 302\nframing for Spotify example, 257-261\nincluding diversity regularization terms in,\n275\nk-order statistic loss, 234\nL2 loss function, 232\nstochastic losses, 235\nweighted approximate rank pairwise\n(W ARP), 233\nlow-dimensional latent space, 173\nlow-rank methods\naffinity and p-sale, 206\nco-occurrence models, 171\ndimensionality reduction, 199-206\ndot-product similarity, 169, 275\nlow-rank approximation, 169\nlow-rank methods, 167-169\npropensity weighting for evaluation,\n208-211\nreducing rank of recommender problems,\n172-199\nLSH (locality sensitive hashing), 282-284\nLTR (learning to rank) models, 230\nM\nmachine learning (ML)\nbatch versus real-time processing, 43\nfairness in, 279\nhuman-in-the loop ML, 46\nnaive ML approaches, 149\n326 | Index\nrole in recommendation systems, xiii-xv\nZipf 's law, 29-32\nmAP (mean average precision), 225\nmap phase, 134\nmargin, 57\nMarkov chains, 294-297\nMarkov decision process (MDP), 295\nmatrix completion, 16, 169\nmatrix factorization (MF)\nchallenges of, 174\nexplanation of, 100\nHPO output, 194\nmatrix factorization method, 199\noptimizing with ALS, 174-176\nregularization for, 176\nregularized MF implementation, 177-198\nvia SVD, 173\nMatthew effect, 29-32, 114, 122, 174, 209\nmaximization of reward, 39\nMDP (Markov decision process), 295\nMDS (multidimensional scaling), 203\nmean average precision (mAP), 220, 225\nmean reciprocal rank (MRR), 221, 225\nmean square error (MSE) loss, 179\nmemorylessness principle, 294\nmessage function, 309\nmessage passing, 309\nMeta [formerly Facebook], Apple Amazon,\nNetflix, and Google (FAANG), xiv\nmetapath, 313\nmetric learning, 149-153\nmetric spaces, 36\nmetrics (see also ranking metrics)\nbusiness metrics, 245\nmodel metrics, 112\nuser versus item metrics, 215\nMF (see matrix factorization)\nmicroservice architectures, 103\nmin-hashing, 142\nmini-batched gradient descent, 82\nMLP (multilayer perceptron), 156\nmodels\nBERT4Rec model, 300\nbilinear models, 149-153, 157\ndeploying, 103-107\nensemble structure, 114\nevaluating in production, 111-113learning to rank (LTR) models, 230\nmodel cascades, 116\nmodel drift, 113\nmodel registries, 92\nmodel-based avoids, 271\nmultilevel modeling, 157\nPinnerFormer, 302\nrelevance models, 231\nSASRec model, 300\nshadowing, 115\nstale models, 114\nweighted combinations of, 157\nModule class (Flax), 60\nmonitoring (see alerting and monitoring)\nMonolith, 303\nmonolithic architectures, 103\nmost-popular-item recommender (MPIR), 6,\n141, 159\nmotivating problem framing, 3\nMRR (mean reciprocal rank), 225\nMSE (mean square error) loss, 179\nmultiarmed bandits, 37, 159\nmultidimensional scaling (MDS), 203\nmultiedges, 308\nmultilabel multiclassification, 156\nmultilabel tasks, 231\nmultilayer perceptron (MLP), 156\nmultilevel modeling, 157\nmultimodal recommendations, 275, 306\nmultimodal retrieval, 238\nmultiobjective functions, 277\nN\nnaive ML approaches, 149\nnaive sequence embeddings, 99\nNaN (not-a-number) errors, 242\nnatural language processing (NLP), 40-42\nNDCG (normalized discounted cumulative\ngain), 226\nnearest neighbors\nin collaborative filtering, 35\ndefinition of term, 35\nnearest-neighbors search, 42\nNetflix online competition, 17\nnew users, onboarding, 125\nnodes, 308\nnonlinear locally metrizable embeddings, 204\nIndex | 327\nnonnegative matrix factorization (NMF), 202\nnormalization, 131\nnormalized discounted cumulative gain\n(NDCG), 222, 226\nnot-a-number (NaN) errors, 242\nnudging, 279\nNumPy, 7\nO\nobservability, 110\nobserved mean square error (OMSE), 175, 194\noffline evaluation, 214\nonboarding funnels, 24\nonboarding new users, 125\nonline evaluation, 215\nonline versus offline ML systems, 43-47\nOptax library, 56-58, 145\noptimization\nactive learning, 123-125\ncontent-based recommender, 56-58\ndeferring, 243\ndimensionality reduction for MF with ALS,\n174-176\nglobal optimization, 269\nhyper-parameter optimization for latent\nspace, 199\ninfluenced-based optimization, 124\nportfolio optimization, 276\npredicate pushdown, 278\norchestration tools, 118\norder-two Markov chain, 295\nout of distribution queries, 98\nover-retrieval, 110\noverfitting, 114\noverrides, 266\nP\np-sale (probability of a sale), 206\npackages, 51\npage loads, logging, 21\npage views, logging, 21\npairwise LTR models, 230\nparallel coordinates chart, 195\nparallelization, 104\nPareto frontier, 197\nPareto problem, 274\npartitions, 140PCA (principal component analysis), 202\nPearson correlation, 35\nPinnerFormer model, 302\nPinnerSage paper, 275, 307\npointwise LTR models, 230\npointwise mutual information (PMI), 162\npopularity, 124, 170\npopularity bias, 29-32\nportfolio optimization, 276\npositive product, 56\npre-indexing, 87\nprecision, 216-220\npredicate pushdown, 278\nprequential validation, 197\nprincipal component analysis (PCA), 202\npriors, 108\nprobability of a sale (p-sale), 206\npropensity scores, 21\npropensity weighting, 208-211\nprotocol buffer compiler, 130\nprotocol buffers, 129\nPySpark\ncluster types, 132\nexample program, 132-142\nimporting data with, 73-77\ninstalling, 131\nSpark UI, 131\ntokenization and URL normalization, 131\nuser similarity in, 77-81\nPython build systems, 51\nPython Package Index, 51\nPython virtual environments, 51\nQ\nqueries\nimplicit queries, 96\nout of distribution queries, 98\nquery-based recommendations, 96\nquestions and comments, xvii\nR\nr-precision, 220\nrandom numbers (in JAX), 10\nrandom-item recommender, 52-54\nrandom-walk-based algorithms, 311\nrank inflation, 154\nrank reduction (see dimensionality reduction)\n328 | Index\nranker\nin explore-exploit systems, 39\nfiltering and scoring components, 45\nLLM rankers, 317\noffline versus online, 46\nrole in recommendation systems, 4\nranking metrics\nAUC and cAUC, 224\nBayesian personalized ranking (BPR), 226\nversus business metrics, 245\ncorrelation coefficients, 223\nevaluating recommendation systems,\n214-216\nmAP versus NDCG, 223\nmean average precision (mAP), 220\nmean reciprocal rank (MRR), 221\nnormalized discounted cumulative gain\n(NDCG), 222\nordering of recommendations, 220, 229\nrecall and precision, 216-220\nRMSE from affinity, 224\nranking training\nbest matching 25 (BM25) algorithm, 236\nexperimentation exercises, 261\nexperimentation tips, 241-246\nk-order statistic loss, 234\nlearning to rank (LTR) models, 230\nmultimodal retrieval, 238\noverview of, 262\nrole of ranking in recommender systems,\n229\nSpotify playlist example, 246-261\nstochastic losses, 235\ntraining LTR models, 231\nweighted approximate rank pairwise\n(W ARP), 233\nrapid iteration, 245\nratings (see also user-item ratings)\nhard and soft ratings, 19, 266\nimplicit ratings, 19\nvia similarity, 36\nRDDs (resilient distributed datasets), 133\nreal-time processing, 44\nrecall, 216-220\nreceiver operating characteristic curve (ROC),\n112\nrecency sampling, 301recommendation systems\nbias in, 273-280\nbloom filters as, 89\nchallenges of, 265\ncontent based (see content-based recom‐\nmender)\ncontext-based, 98\ncounting recommenders, 159-165\ndata sparsity, 32-34, 99\nevaluating, 208-211, 214-216\nexplore-exploit as, 37\nfairness in, 279\nfeature-based (see feature-based recommen‐\ndations)\nfeature-item recommenders, 141\nfuture of, 319\ngraph-based recommenders, 307-313\nintersection with LLMs, 318\nitem-to-user recommendations, 96\nkey components of, 4\nMatthew effect in, 29-32\nmost-popular-item recommender (MPIR),\n6, 141\nmultimodal recommendations, 275, 306\nquery-based recommendations, 96\nranking metrics, 213-226\nrole of machine learning in, xiii-xv\nrole of ranking in, 229\nsequence-based recommendations, 99\nsequential recommenders, 293-303\nsolving recommendation problems, 319\nsteps involved in building, xiv\nsystem design for, 43-47\ntag-based recommenders, 155\ntechnologies enabled by, 3\ntips for building, 18, 25, 54, 85\ntrivial recommender (TR), 5\nuse of term, xiii\nRecSys\ndefinition of term, xiii\nrelationship to natural language processing,\n40-42\nrole of machine learning in, xiv\nRectified Linear Unit (ReLU), 299\nrecurrent neural networks (RNNs), 297\nreduce phase, 134\nregistries, 92\nIndex | 329\nregression, for ranking, 231\nregularization, 61, 62, 176, 275\nregularized matrix factorization, 177-198\nreinforcement learning from human feedback\n(RLHF), 315\nrelevance models, 231\nReLU (Rectified Linear Unit), 299\nrepresentation space, 40\nreranking, 275\nresidual networks (Resnet), 55\nresilient distributed datasets (RDDs), 133\nretrieval\ncheaper retrieval methods, 290\ninformation-retrieval tasks, 237\nmultimodal retrieval, 238\nover-retrieval, 110\nretrieval augmentation, 318\nrevision control software, 50\nRLHF (reinforcement learning from human\nfeedback), 315\nRNNs (recurrent neural networks), 297\nROC (receiver operating characteristic curve),\n112\nroot mean square error (RMSE), 224\nS\nsampling techniques, 39\nSASRec (self-attentive sequential recommenda‐\ntion), 300\nscaling, horizontal, 104\nscheduling jobs, 106\nschemas, 108, 129\nscoring, 45, 120\nsegmentation models, 155-157\nself-affinity, 257\nself-attention, 300\nself-attentive sequential recommendation (SAS‐\nRec), 300\nsequential recommenders\nattention architectures, 299-302\nchallenges of, 293\ninfluence of recent exposures on, 99\nMarkov chains, 294-297\nMonolith, 303\norder of items and, 23\nRNN and CNN architectures, 297\nTransformers4Rec, 303sequential training, 101\nserendipitous recommendations, 274\nserialized protocol buffers, 129\nserver\nin explore-exploit systems, 39\noffline versus online, 46\nrole in recommendation systems, 4\nsessions, 297\nSGD (stochastic gradient descent), 82\nshadowing, 115\nsharding, 282\nShop the Look (STL) dataset, 49\nside-car features, 100\nsigmoid activation functions, 231\nsimilarity\namong recommendations, 275\nfrom co-occurrence, 163-164\nin collaborative filtering, 31\ncomputing, 17\nconverting to recommendations, 40\ncosine similarity, 150, 164, 170\ndot-product similarity, 169, 275\nitem similarity, 17\nsimilarity-based recommendations, 164\nuser similarity, 17, 34-37, 77-81\nvector similarity, 17\nSimpson's paradox, 210\nsingular value decomposition (SVD), 173, 199,\n289\nskipgram-word2vec model, 40\nslicing (in JAX), 9\nslow feedback, 111\nsnapshots, 84\nsoft ratings, 19\nSørensen-Dice similarity, 164\nspans, 110\nSpark, 73-77\nsparsity, 15, 32-34, 99\nspeed layer, 118\nspill trees, 288\nSpotify playlist example\nbuilding training data, 249-252\nbuilding URI dictionaries, 248\ncode and documentation for, 246\nframing loss functions, 257-261\nmodeling problems, 254-257\nreading input, 252-254\n330 | Index\nselecting features, 246-248\nstaging, 115\nstale models, 114\nSTL model, 60\nstochastic gradient descent (SGD), 82\nstochastic loss functions, 235\nstorage layer, 92\nstreaming layer, 91\nstring diagrams, 4\nstructured logs, 121\nSVD (see singular value decomposition)\nswitching, 157\nT\nt-distributed stochastic neighbor embedding (t-\nSNE), 205\ntag-based recommenders, 155\ntechnology stacks (tech stacks), 128\ntemporal batches, 101\nTensorFlow library, 58\nterm frequency, inverse document frequency\n(TF-IDF), 132\ntesting\nA/B testing, 116, 215\nintegration tests, 109\nfor set inclusions, 88\nunit tests, 108\ntight coupling, 122\ntime travel, 93\ntimeouts, 111\ntokenization, 131, 133\ntokens, 133\ntop-r recommendations, 220\ntotal recall, 219\nTPR (true positive rate), 225\nTR (trivial recommender), 5\ntraces, 110\ntraining (see also continuous training and\ndeployment; ranking training)\nGPU-based training, 319\nlarge language models (LLMs), 314-317\nsequential training, 101\ntraining example leakage, 93\nTransformers4Rec, 303\ntrending status, 91\ntriangle inequality, 37\ntrigger scheduling, 107triplet loss, 56\ntrivial recommender (TR), 5\ntrue positive rate (TPR), 225\ntrustworthiness, 279\ntwo-phase prediction comparison, 114\ntwo-towers architecture, 100\nU\nUniform Manifold Approximation and Projec‐\ntion (UMAP), 156, 205\nunit tests, 108\nURL normalization, 131\nuser metrics, 215\nuser rating variance, 124\nuser segmentation, 125\nuser sign-up, 125\nuser similarity\nin collaborative filtering, 17, 34-37\nin PySpark, 77-81\nuser-user recommendations, 165\nuser space, 152\nuser-independent propensity score, 211\nuser-item affinity score, 36\nuser-item incidence matrix, 161\nuser-item incidence structure, 160\nuser-item ratings\nbusiness insight and what people like, 26\ndata collection and user logging, 19-26\nGNNs for, 310\nNetflix online competition, 17\nsoft ratings, 19\nuser-item matrix, 13-16\nuser-item recommendations, 165\nuser-user versus item-item collaborative fil‐\ntering, 16\nuser-matrix, 154\nV\nvector search, 40, 86\nvector similarity, 17\nvectorized map (vmap), 144\nvirtual closets, 267\nW\nW ALS (weighted alternating least squares), 176\nwarm starts, 117, 128, 153-154\nIndex | 331\nW ARP (weighted approximate rank pairwise),\n233\nweb scale annotation by image embedding\n(WSABIE), 199\nweight decay, 176\nweighted alternating least squares (W ALS), 176\nweighted approximate rank pairwise (W ARP),\n233\nWeights & Biases, 51, 244, 258\nWikipedia corpus, 129\nWindows Subsystem for Linux (WSL), 49\nword2vec model, 40\nworker nodes, 75\nworkflow orchestration\nCI/CD, 107containerization, 106\nscheduling, 106\nWSABIE (web scale annotation by image\nembedding), 199\nWSL (Windows Subsystem for Linux), 49\nX\nXML parsing, 129\nZ\nzero-ablation, 245\nzero-concentrated priors, 154\nZipf 's law, 29-32\n332 | Index",25971
140-About the Authors.pdf,140-About the Authors,,0
141-Colophon.pdf,141-Colophon,"About the Authors\nBryan Bischof  leads AI at Hex and is an adjunct professor in the Rutgers University\nBusiness Analytics master’s program, where he teaches data science. Previously, he\nwas the head of data science at Weights & Biases and built the DS, ML, and data\nengineering teams. He has built recommendation systems for clothing (at Stitch Fix),\nrecommendation systems for technical blog posts (at Weights & Biases),  and the\nworld’s first recommendation system for coffee (at Blue Bottle Coffee); currently,\nhe is building recommendation systems for AI agents. His research interests are in\ngeometric methods for ML, including higher-order graph methods and topological\nmethods. His data visualization work appears in the popular book The Day It Finally\nHappens  by Mike Pearl. His Ph.D. is in pure mathematics. Y ou can find Bryan on\nLinkedIn at bryan-bischof  and on Twitter at @bebischof .\nHector Y ee  is a staff software engineer at Google, where he has worked on multiple\nprojects including creating the first content-based ranker on Image Search and self\ndriving car perception. He also worked on the Y ouTube recommender system and\nwas part of the team that won a technical Emmy Award for their work on personal‐\nized video ranking technology. He has an M.S. in computer graphics. Y ou can find\nHector on LinkedIn at yeehector  and on Twitter at @eigenhector .\nColophon\nThe animal on the cover of Building Recommendation Systems in Python and JAX\nis a European goldfinch ( Carduelis carduelis ). Known for its colorful feathers, this\npasserine bird (or perching bird) can be found in the open, wooded lowlands of\nEurope, North Africa, and western and central Asia. It has been introduced to many\nother countries over the years, including the United States, Canada, Mexico, Peru,\nArgentina, Australia, and New Zealand. In particular, within the United States, they\nhave established their homes in the western Great Lakes region.\nThe average European goldfinch is about 4.7–5.1 inches (12–13 centimeters) long,\nwith a wingspan of 8.3–9.8 inches (21–25 centimeters); it weighs roughly 0.5–0.67\nounces. Male and female European goldfinches are similar in appearance, with a red\nface, black and white head, black and yellow wings, white underparts, and medium-\nbrown upper parts. However, on closer inspection, male European goldfinches can\nbe distinguished by a larger, darker patch of red on their face, and black feathers on\ntheir shoulders (whereas the feathers on a female are brown). After breeding season,\nEuropean goldfinches shed their old feathers to make way for new growth; molt birds\nappear less colorful at first, but they regain their colors once their feathers regrow.\nIn terms of diet, European goldfinches prefer small seeds from thistles, cornflowers,\nand teasels; insects are mostly given to their young. These birds are also known to\nregularly visit residential gardens in Europe and North America, attracted by the\nbird feeders that contain seeds. Because of their pleasant songs, European goldfinches\nare commonly trapped and bred in captivity; there have been wildlife conversation\nattempts to limit bird trapping and the destruction of open space habitats to protect\nEuropean goldfinches.\nMany of the animals on O’Reilly covers are endangered; all of them are important to\nthe world.\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\nfrom British Birds . The cover fonts are Gilroy Semibold and Guardian Sans. The text\nfont is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the\ncode font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2023 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.  175  7x9.1975",3926
