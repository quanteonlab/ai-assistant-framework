filename,title,text,len
01-welcome.pdf,01-welcome,"welcome\nThank you for purchasing the MEAP edition of \nBuild a Large Language\nModel (From Scratch)\n.\nIn this book, I invite you to embark on an educational journey with me to\nlearn how to build Large Language Models (LLMs) from the ground up.\nTogether, we'll delve deep into the LLM training pipeline, starting from data\nloading and culminating in finetuning LLMs on custom datasets.\nFor many years, I've been deeply immersed in the world of deep learning,\ncoding LLMs, and have found great joy in explaining complex concepts\nthoroughly. This book has been a long-standing idea in my mind, and I'm\nthrilled to finally have the opportunity to write it and share it with you. Those\nof you familiar with my work, especially from my blog, have likely seen\nglimpses of my approach to coding from scratch. This method has resonated\nwell with many readers, and I hope it will be equally effective for you.\nI've designed the book to emphasize hands-on learning, primarily using\nPyTorch and without relying on pre-existing libraries. With this approach,\ncoupled with numerous figures and illustrations, I aim to provide you with a\nthorough understanding of how LLMs work, their limitations, and\ncustomization methods. Moreover, we'll explore commonly used workflows\nand paradigms in pretraining and fine-tuning LLMs, offering insights into\ntheir development and customization.\nThe book is structured with detailed step-by-step introductions, ensuring no\ncritical detail is overlooked. To gain the most from this book, you should\nhave a background in Python programming. Prior experience in deep learning\nand a foundational understanding of PyTorch, or familiarity with other deep\nlearning frameworks like TensorFlow, will be beneficial.\nI warmly invite you to engage in the \nliveBook discussion forum\n for any\nquestions, suggestions, or feedback you might have. Your contributions are\nimmensely valuable and appreciated in enhancing this learning journey.\n— Sebastian Raschka\nIn this book\nwelcome\n \n1 Understanding Large Language Models\n \n2 Working with Text\nData\n \n3 Coding Attention Mechanisms\n \n4 Implementing a GPT model from\nScratch To Generate Text\n \n5 Pretraining on Unlabeled Data\n \nAppendix A. Introduction to PyTorch\n \nAppendix B. References and Further\nReading\n \nAppendix C. Exercise Solutions\n \nAppendix D. Adding Bells and\nWhistles to the Training Loop",2412
02-1_Understanding_Large_Language_Models.pdf,02-1_Understanding_Large_Language_Models,"1 Understanding Large Language\nModels\nThis chapter covers\nHigh-level explanations of the fundamental concepts behind large\nlanguage models (LLMs)\nInsights into the transformer architecture from which ChatGPT-like\nLLMs are derived\nA plan for building an LLM from scratch\nLarge language models (LLMs), such as those offered in OpenAI's ChatGPT,\nare deep neural network models that have been developed over the past few\nyears. They ushered in a new era for Natural Language Processing (NLP).\nBefore the advent of large language models, traditional methods excelled at\ncategorization tasks such as email spam classification and straightforward\npattern recognition that could be captured with handcrafted rules or simpler\nmodels. However, they typically underperformed in language tasks that\ndemanded complex understanding and generation abilities, such as parsing\ndetailed instructions, conducting contextual analysis, or creating coherent and\ncontextually appropriate original text. For example, previous generations of\nlanguage models could not write an email from a list of keywords—a task\nthat is trivial for contemporary LLMs.\nLLMs have remarkable capabilities to understand, generate, and interpret\nhuman language. However, it's important to clarify that when we say\nlanguage models ""understand,"" we mean that they can process and generate\ntext in ways that appear coherent and contextually relevant, not that they\npossess human-like consciousness or comprehension.\nEnabled by advancements in deep learning, which is a subset of machine\nlearning and artificial intelligence (AI) focused on neural networks, LLMs\nare trained on vast quantities of text data. This allows LLMs to capture\ndeeper contextual information and subtleties of human language compared to\nprevious approaches. As a result, LLMs have significantly improved\nperformance in a wide range of NLP tasks, including text translation,\nsentiment analysis, question answering, and many more.\nAnother important distinction between contemporary LLMs and earlier NLP\nmodels is that the latter were typically designed for specific tasks; whereas\nthose earlier NLP models excelled in their narrow applications, LLMs\ndemonstrate a broader proficiency across a wide range of NLP tasks.\nThe success behind LLMs can be attributed to the transformer architecture\nwhich underpins many LLMs, and the vast amounts of data LLMs are trained\non, allowing them to capture a wide variety of linguistic nuances, contexts,\nand patterns that would be challenging to manually encode.\nThis shift towards implementing models based on the transformer\narchitecture and using large training datasets to train LLMs has\nfundamentally transformed NLP, providing more capable tools for\nunderstanding and interacting with human language.\nBeginning with this chapter, we set the foundation to accomplish the primary\nobjective of this book: understanding LLMs by implementing a ChatGPT-\nlike LLM based on the transformer architecture step by step in code.\n1.1 What is an LLM?\nAn LLM, a large language model, is a neural network designed to\nunderstand, generate, and respond to human-like text. These models are deep\nneural networks trained on massive amounts of text data, sometimes\nencompassing large portions of the entire publicly available text on the\ninternet.\nThe ""large"" in large language model refers to both the model's size in terms\nof parameters and the immense dataset on which it's trained. Models like this\noften have tens or even hundreds of billions of parameters, which are the\nadjustable weights in the network that are optimized during training to predict\nthe next word in a sequence. Next-word prediction is sensible because it\nharnesses the inherent sequential nature of language to train models on\nunderstanding context, structure, and relationships within text. Yet, it is a\nvery simple task and so it is surprising to many researchers that it can\nproduce such capable models. We will discuss and implement the next-word\ntraining procedure in later chapters step by step.\nLLMs utilize an architecture called the \ntransformer\n (covered in more detail in\nsection 1.4), which allows them to pay selective attention to different parts of\nthe input when making predictions, making them especially adept at handling\nthe nuances and complexities of human language.\nSince LLMs are capable of \ngenerating\n text, LLMs are also often referred to\nas a form of generative artificial intelligence (AI), often abbreviated as\ngenerative AI\n or \nGenAI\n. As illustrated in Figure 1.1, AI encompasses the\nbroader field of creating machines that can perform tasks requiring human-\nlike intelligence, including understanding language, recognizing patterns, and\nmaking decisions, and includes subfields like machine learning and deep\nlearning.\nFigure 1.1 As this hierarchical depiction of the relationship between the different fields suggests,\nLLMs represent a specific application of deep learning techniques, leveraging their ability to\nprocess and generate human-like text. Deep learning is a specialized branch of machine learning\nthat focuses on using multi-layer neural networks. And machine learning and deep learning are\nfields aimed at implementing algorithms that enable computers to learn from data and perform\ntasks that typically require human intelligence.\nThe algorithms used to implement AI are the focus of the field of machine\nlearning. Specifically, machine learning involves the development of\nalgorithms that can learn from and make predictions or decisions based on\ndata without being explicitly programmed. To illustrate this, imagine a spam\nfilter as a practical application of machine learning. Instead of manually\nwriting rules to identify spam emails, a machine learning algorithm is fed\nexamples of emails labeled as spam and legitimate emails. By minimizing the\nerror in its predictions on a training dataset, the model then learns to\nrecognize patterns and characteristics indicative of spam, enabling it to\nclassify new emails as either spam or legitimate.\nAs illustrated in Figure 1.1, deep learning is a subset of machine learning that\nfocuses on utilizing neural networks with three or more layers (also called\ndeep neural networks) to model complex patterns and abstractions in data. In\ncontrast to deep learning, traditional machine learning requires manual\nfeature extraction. This means that human experts need to identify and select\nthe most relevant features for the model.\nWhile the field of AI is nowadays dominated by machine learning and deep\nlearning, it also includes other approaches, for example, using rule-based\nsystems, genetic algorithms, expert systems, fuzzy logic, or symbolic\nreasoning.\nReturning to the spam classification example, in traditional machine learning,\nhuman experts might manually extract features from email text such as the\nfrequency of certain trigger words (""prize,"" ""win,"" ""free""), the number of\nexclamation marks, use of all uppercase words, or the presence of suspicious\nlinks. This dataset, created based on these expert-defined features, would then\nbe used to train the model. In contrast to traditional machine learning, deep\nlearning does not require manual feature extraction. This means that human\nexperts do not need to identify and select the most relevant features for a deep\nlearning model. (However, in both traditional machine learning and deep\nlearning for spam classification, you still require the collection of labels, such\nas spam or non-spam, which need to be gathered either by an expert or users.)\nThe upcoming sections will cover some of the problems LLMs can solve\ntoday, the challenges that LLMs address, and the general LLM architecture,\nwhich we will implement in this book.\n1.2 Applications of LLMs\nOwing to their advanced capabilities to parse and understand unstructured\ntext data, LLMs have a broad range of applications across various domains.\nToday, LLMs are employed for machine translation, generation of novel texts\n(see Figure 1.2), sentiment analysis, text summarization, and many other\ntasks. LLMs have recently been used for content creation, such as writing\nfiction, articles, and even computer code.\nFigure 1.2 LLM interfaces enable natural language communication between users and AI\nsystems. This screenshot shows ChatGPT writing a poem according to a user's specifications.\nLLMs can also power sophisticated chatbots and virtual assistants, such as\nOpenAI's ChatGPT or Google's Gemini (formerly called Bard), which can\nanswer user queries and augment traditional search engines such as Google\nSearch or Microsoft Bing.\nMoreover, LLMs may be used for effective knowledge retrieval from vast\nvolumes of text in specialized areas such as medicine or law. This includes\nsifting through documents, summarizing lengthy passages, and answering\ntechnical questions.\nIn short, LLMs are invaluable for automating almost any task that involves\nparsing and generating text. Their applications are virtually endless, and as\nwe continue to innovate and explore new ways to use these models, it's clear\nthat LLMs have the potential to redefine our relationship with technology,\nmaking it more conversational, intuitive, and accessible.\nIn this book, we will focus on understanding how LLMs work from the\nground up, coding an LLM that can generate texts. We will also learn about\ntechniques that allow LLMs to carry out queries, ranging from answering\nquestions to summarizing text, translating text into different languages, and\nmore. In other words, in this book, we will learn how complex LLM\nassistants such as ChatGPT work by building one step by step.\n1.3 Stages of building and using LLMs\nWhy should we build our own LLMs? Coding an LLM from the ground up is\nan excellent exercise to understand its mechanics and limitations. Also, it\nequips us with the required knowledge for pretraining or finetuning existing\nopen-source LLM architectures to our own domain-specific datasets or tasks.\nResearch has shown that when it comes to modeling performance, custom-\nbuilt LLMs—those tailored for specific tasks or domains—can outperform\ngeneral-purpose LLMs, such as those provided by ChatGPT, which are\ndesigned for a wide array of applications. Examples of this include\nBloombergGPT, which is specialized for finance, and LLMs that are tailored\nfor medical question answering (please see the \nFurther Reading and\nReferences\n section in Appendix B for more details).\nThe general process of creating an LLM includes pretraining and finetuning.\nThe term ""pre"" in ""pretraining"" refers to the initial phase where a model like\nan LLM is trained on a large, diverse dataset to develop a broad\nunderstanding of language. This pretrained model then serves as a\nfoundational resource that can be further refined through finetuning, a\nprocess where the model is specifically trained on a narrower dataset that is\nmore specific to particular tasks or domains. This two-stage training approach\nconsisting of pretraining and finetuning is depicted in Figure 1.3.\nFigure 1.3 Pretraining an LLM involves next-word prediction on large text datasets. A\npretrained LLM can then be finetuned using a smaller labeled dataset.\nAs illustrated in Figure 1.3, the first step in creating an LLM is to train it in\non a large corpus of text data, sometimes referred to as \nraw\n text. Here, ""raw""\nrefers to the fact that this data is just regular text without any labeling\ninformation\n[1]\n. (Filtering may be applied, such as removing formatting\ncharacters or documents in unknown languages.)\nThis first training stage of an LLM is also known as \npretraining\n, creating an\ninitial pretrained LLM, often called a \nbase\n or \nfoundation\n \nmodel\n. A typical\nexample of such a model is the GPT-3 model (the precursor of the original\nmodel offered in ChatGPT). This model is capable of text completion, that is,\nfinishing a half-written sentence provided by a user. It also has limited few-\nshot capabilities, which means it can learn to perform new tasks based on\nonly a few examples instead of needing extensive training data. This is\nfurther illustrated in the next section\n, Using transformers for different tasks\n.\nAfter obtaining a \npretrained\n LLM from training on large text datasets, where\nthe LLM is trained to predict the next word in the text, we can further train\nthe LLM on labeled data, also known as\n finetuning\n.\nThe two most popular categories of finetuning LLMs include \ninstruction-\nfinetuning\n and finetuning for \nclassification\n tasks. In instruction-finetuning,\nthe labeled dataset consists of instruction and answer pairs, such as a query to\ntranslate a text accompanied by the correctly translated text. In classification\nfinetuning, the labeled dataset consists of texts and associated class labels, for\nexample, emails associated with \nspam\n and \nnon-spam\n labels.\nIn this book, we will cover both code implementations for pretraining and\nfinetuning LLM, and we will delve deeper into the specifics of instruction-\nfinetuning and finetuning for classification later in this book after pretraining\na base LLM.\n1.4 Using LLMs for different tasks\nMost modern LLMs rely on the \ntransformer\n architecture, which is a deep\nneural network architecture introduced in the 2017 paper \nAttention Is All You\nNeed\n. To understand LLMs we briefly have to go over the original\ntransformer, which was originally developed for machine translation,\ntranslating English texts to German and French. A simplified version of the\ntransformer architecture is depicted in Figure 1.4.\nFigure 1.4 A simplified depiction of the original transformer architecture, which is a deep\nlearning model for language translation. The transformer consists of two parts, an encoder that\nprocesses the input text and produces an embedding representation (a numerical representation\nthat captures many different factors in different dimensions) of the text that the decoder can use\nto generate the translated text one word at a time. Note that this figure shows the final stage of\nthe translation process where the decoder has to generate only the final word (""Beispiel""), given\nthe original input text (""This is an example"") and a partially translated sentence (""Das ist ein""),\nto complete the translation.\nThe transformer architecture depicted in Figure 1.4 consists of two\nsubmodules, an encoder and a decoder. The encoder module processes the\ninput text and encodes it into a series of numerical representations or vectors\nthat capture the contextual information of the input. Then, the decoder\nmodule takes these encoded vectors and generates the output text from them.\nIn a translation task, for example, the encoder would encode the text from the\nsource language into vectors, and the decoder would decode these vectors to\ngenerate text in the target language. Both the encoder and decoder consist of\nmany layers connected by a so-called self-attention mechanism. You may\nhave many questions regarding how the inputs are preprocessed and encoded.\nThese will be addressed in a step-by-step implementation in the subsequent\nchapters.\nA key component of transformers and LLMs is the self-attention mechanism\n(not shown), which allows the model to weigh the importance of different\nwords or tokens in a sequence relative to each other. This mechanism enables\nthe model to capture long-range dependencies and contextual relationships\nwithin the input data, enhancing its ability to generate coherent and\ncontextually relevant output. However, due to its complexity, we will defer\nthe explanation to chapter 3, where we will discuss and implement it step by\nstep. Moreover, we will also discuss and implement the data preprocessing\nsteps to create the model inputs in \nchapter 2, Working with Text Data\n.\nLater variants of the transformer architecture, such as the so-called BERT\n(short for\n bidirectional encoder representations from transformers\n) and the\nvarious GPT models (short for \ngenerative pretrained transformers\n), built on\nthis concept to adapt this architecture for different tasks. (References can be\nfound in Appendix B.)\nBERT, which is built upon the original transformer's encoder submodule,\ndiffers in its training approach from GPT. While GPT is designed for\ngenerative tasks, BERT and its variants specialize in masked word prediction,\nwhere the model predicts masked or hidden words in a given sentence as\nillustrated in Figure 1.5. This unique training strategy equips BERT with\nstrengths in text classification tasks, including sentiment prediction and\ndocument categorization. As an application of its capabilities, as of this\nwriting, Twitter uses BERT to detect toxic content.\nFigure 1.5 A visual representation of the transformer's encoder and decoder submodules. On the\nleft, the encoder segment exemplifies BERT-like LLMs, which focus on masked word prediction\nand are primarily used for tasks like text classification. On the right, the decoder segment\nshowcases GPT-like LLMs, designed for generative tasks and producing coherent text sequences.\n\nGPT, on the other hand, focuses on the decoder portion of the original\ntransformer architecture and is designed for tasks that require generating\ntexts. This includes machine translation, text summarization, fiction writing,\nwriting computer code, and more. We will discuss the GPT architecture in\nmore detail in the remaining sections of this chapter and implement it from\nscratch in this book.\nGPT models, primarily designed and trained to perform text completion\ntasks, also show remarkable versatility in their capabilities. These models are\nadept at executing both zero-shot and few-shot learning tasks. Zero-shot\nlearning refers to the ability to generalize to completely unseen tasks without\nany prior specific examples. On the other hand, few-shot learning involves\nlearning from a minimal number of examples the user provides as input, as\nshown in Figure 1.6.\nFigure 1.6 In addition to text completion, GPT-like LLMs can solve various tasks based on their\ninputs without needing retraining, finetuning, or task-specific model architecture changes.\nSometimes, it is helpful to provide examples of the target within the input, which is known as a\nfew-shot setting. However, GPT-like LLMs are also capable of carrying out tasks without a\nspecific example, which is called zero-shot setting.\nTransformers versus LLMs\nToday's LLMs are based on the transformer architecture introduced in the\nprevious section. Hence, transformers and LLMs are terms that are often used\nsynonymously in the literature. However, note that not all transformers are\nLLMs since transformers can also be used for computer vision. Also, not all\nLLMs are transformers, as there are large language models based on recurrent\nand convolutional architectures. The main motivation behind these alternative\napproaches is to improve the computational efficiency of LLMs. However,\nwhether these alternative LLM architectures can compete with the\ncapabilities of transformer-based LLMs and whether they are going to be\nadopted in practice remains to be seen. (Interested readers can find literature\nreferences describing these architectures in the \nFurther Reading\n section at the\nend of this chapter.)\n1.5 Utilizing large datasets\nThe large training datasets for popular GPT- and BERT-like models represent\ndiverse and comprehensive text corpora encompassing billions of words,\nwhich include a vast array of topics and natural and computer languages. To\nprovide a concrete example, Table 1.1 summarizes the dataset used for\npretraining GPT-3, which served as the base model for the first version of\nChatGPT.\nTable 1.1 The pretraining dataset of the popular GPT-3 LLM\nDataset name\nDataset description\nNumber of\ntokens\nProportion in\n  \ntraining data\nCommonCrawl\n(filtered)\nWeb crawl data\n410 billion\n60%\nWebText2\nWeb crawl data\n19 billion\n22%\nBooks1\nInternet-based book\ncorpus\n12 billion\n8%\nBooks2\nInternet-based book\ncorpus\n55 billion\n8%\nWikipedia\nHigh-quality text\n3 billion\n3%\nTable 1.1 reports the number of tokens, where a token is a unit of text that a\nmodel reads, and the number of tokens in a dataset is roughly equivalent to\nthe number of words and punctuation characters in the text. We will cover\ntokenization, the process of converting text into tokens, in more detail in the\nnext chapter.\nThe main takeaway is that the scale and diversity of this training dataset\nallows these models to perform well on diverse tasks including language\nsyntax, semantics, and context, and even some requiring general knowledge.\nGPT-3 dataset details\nIn Table 1.1, it's important to note that from each dataset, only a fraction of\nthe data, (amounting to a total of 300 billion tokens) was used in the training\nprocess. This sampling approach means that the training didn't encompass\nevery single piece of data available in each dataset. Instead, a selected subset\nof 300 billion tokens, drawn from all datasets combined, was utilized. Also,\nwhile some datasets were not entirely covered in this subset, others might\nhave been included multiple times to reach the total count of 300 billion\ntokens. The column indicating proportions in the table, when summed up\nwithout considering rounding errors, accounts for 100% of this sampled data.\nFor context, consider the size of the CommonCrawl dataset, which alone\nconsists of 410 billion tokens and requires about 570 GB of storage. In\ncomparison, later iterations of models like GPT-3, such as Meta's LLaMA,\nhave expanded their training scope to include additional data sources like\nArxiv research papers (92 GB) and StackExchange's code-related Q&As (78\nGB).\nThe \nWikipedia\n corpus consists of \nEnglish-language Wikipedia. While the\nauthors of the GPT-3 paper didn't further specify the details,\n Books1 is likely\na sample from Project Gutenberg (\nhttps://www.gutenberg.org/\n), and \nBooks\n2\nis likely from \nLibge\nn (\nhttps://en.wikipedia.org/wiki/Library_Genesis\n).\nCommonCrawl\n is a filtered subset of the CommonCrawl database\n(\nhttps://commoncrawl.org/\n), and \nWebText2\n is the text of web pages from all\noutbound Reddit links from posts with 3+ upvotes.\nThe authors of the GPT-3 paper did not share the training dataset but a\ncomparable dataset that is publicly available is \nThe Pile\n(\nhttps://pile.eleuther.ai/\n). However, the collection may contain copyrighted\nworks, and the exact usage terms may depend on the intended use case and\ncountry. For more information, see the HackerNews discussion at\nhttps://news.ycombinator.com/item?id=25607809\n.\nThe pretrained nature of these models makes them incredibly versatile for\nfurther finetuning on downstream tasks, which is why they are also known as\nbase or foundation models. Pretraining LLMs requires access to significant\nresources and is very expensive. For example, the GPT-3 pretraining cost is\nestimated to be $4.6 million in terms of cloud computing credits\n[2]\n.\nThe good news is that many pretrained LLMs, available as open-source\nmodels, can be used as general purpose tools to write, extract, and edit texts\nthat were not part of the training data. Also, LLMs can be finetuned on\nspecific tasks with relatively smaller datasets, reducing the computational\nresources needed and improving performance on the specific task.\nIn this book, we will implement the code for pretraining and use it to pretrain\nan LLM for educational purposes. All computations will be executable on\nconsumer hardware. After implementing the pretraining code we will learn\nhow to reuse openly available model weights and load them into the\narchitecture we will implement, allowing us to skip the expensive pretraining\nstage when we finetune LLMs later in this book.\n1.6 A closer look at the GPT architecture\nPreviously in this chapter, we mentioned the terms GPT-like models, GPT-3,\nand ChatGPT. Let's now take a closer look at the general GPT architecture.\nFirst, GPT stands for \nG\nenerative \nP\nretrained \nT\nransformer and was originally\nintroduced in the following paper:\nImproving Language Understanding by Generative Pre-Training\n (2018)\nby \nRadford et al.\n from OpenAI, \nhttp://cdn.openai.com/research-\ncovers/language-unsupervised/language_understanding_paper.pdf\nGPT-3 is a scaled-up version of this model that has more parameters and was\ntrained on a larger dataset. And the original model offered in ChatGPT was\ncreated by finetuning GPT-3 on a large instruction dataset using a method\nfrom OpenAI's InstructGPT paper, which we will cover in more detail in\nchapter 7, Finetuning with Human Feedback To Follow Instructions\n. As we\nhave seen earlier in Figure 1.6, these models are competent text completion\nmodels and can carry out other tasks such as spelling correction,\nclassification, or language translation. This is actually very remarkable given\nthat GPT models are pretrained on a relatively simple next-word prediction\ntask, as illustrated in Figure 1.7.\nFigure 1.7 In the next-word pretraining task for GPT models, the system learns to predict the\nupcoming word in a sentence by looking at the words that have come before it. This approach\nhelps the model understand how words and phrases typically fit together in language, forming a\nfoundation that can be applied to various other tasks.\nThe next-word prediction task is a form of self-supervised learning, which is\na form of self-labeling. This means that we don't need to collect labels for the\ntraining data explicitly but can leverage the structure of the data itself: we can\nuse the next word in a sentence or document as the label that the model is\nsupposed to predict. Since this next-word prediction task allows us to create\nlabels ""on the fly,"" it is possible to leverage massive unlabeled text datasets to\ntrain LLMs as previously discussed in section \n1.5, Utilizing large datasets\n.\nCompared to the original transformer architecture we covered in section 1.4,\nUsing LLMs for different tasks\n, the general GPT architecture is relatively\nsimple. Essentially, it's just the decoder part without the encoder as illustrated\nin Figure 1.8. Since decoder-style models like GPT generate text by\npredicting text one word at a time, they are considered a type of\nautoregressive\n model. Autoregressive models incorporate their previous\noutputs as inputs for future predictions. Consequently, in GPT, each new\nword is chosen based on the sequence that precedes it, which improves\ncoherence of the resulting text.\nArchitectures such as GPT-3 are also significantly larger than the original\ntransformer model. For instance, the original transformer repeated the\nencoder and decoder blocks six times. GPT-3 has 96 transformer layers and\n175 billion parameters in total.\nFigure 1.8 The GPT architecture employs only the decoder portion of the original transformer. It\nis designed for unidirectional, left-to-right processing, making it well-suited for text generation\nand next-word prediction tasks to generate text in iterative fashion one word at a time.\nGPT-3 was introduced in 2020, which, by the standards of deep learning and\nlarge language model (LLM) development, is considered a long time ago.\nHowever, more recent architectures, such as Meta's Llama models, are still\nbased on the same underlying concepts, introducing only minor\nmodifications. Hence, understanding GPT remains as relevant as ever, and\nthis book focuses on implementing the prominent architecture behind GPT\nwhile providing pointers to specific tweaks employed by alternative LLMs.\nLastly, it's interesting to note that although the original transformer model\nwas explicitly designed for language translation, GPT models—despite their\nlarger yet simpler architecture aimed at next-word prediction—are also\ncapable of performing translation tasks. This capability was initially\nunexpected to researchers, as it emerged from a model primarily trained on a\nnext-word prediction task, which is a task that did not specifically target\ntranslation.\nThe ability to perform tasks that the model wasn't explicitly trained to\nperform is called an ""emergent behavior."" This capability isn't explicitly\ntaught during training but emerges as a natural consequence of the model's\nexposure to vast quantities of multilingual data in diverse contexts. The fact\nthat GPT models can ""learn"" the translation patterns between languages and\nperform translation tasks even though they weren't specifically trained for it\ndemonstrates the benefits and capabilities of these large-scale, generative\nlanguage models. We can perform diverse tasks without using diverse models\nfor each.\n1.7 Building a large language model\nIn this chapter, we laid the groundwork for understanding LLMs. In the\nremainder of this book, we will be coding one from scratch. We will take the\nfundamental idea behind GPT as a blueprint and tackle this in three stages, as\noutlined in Figure 1.9.\nFigure 1.9 The stages of building LLMs covered in this book include implementing the LLM\narchitecture and data preparation process, pretraining an LLM to create a foundation model,\nand finetuning the foundation model to become a personal assistant or text classifier.\nFirst, we will learn about the fundamental data preprocessing steps and code\nthe attention mechanism that is at the heart of every LLM.\nNext, in stage 2, we will learn how to code and pretrain a GPT-like LLM\ncapable of generating new texts. And we will also go over the fundamentals\nof evaluating LLMs, which is essential for developing capable NLP systems.\nNote that pretraining a large LLM from scratch is a significant endeavor,\ndemanding thousands to millions of dollars in computing costs for GPT-like\nmodels. Therefore, the focus of stage 2 is on implementing training for\neducational purposes using a small dataset. In addition, the book will also\nprovide code examples for loading openly available model weights.\nFinally, in stage 3, we will take a pretrained LLM and finetune it to follow\ninstructions such as answering queries or classifying texts -- the most\ncommon tasks in many real-world applications and research.\nI hope you are looking forward to embarking on this exciting journey!\n1.8 Summary\nLLMs have transformed the field of natural language processing, which\npreviously mostly relied on explicit rule-based systems and simpler\nstatistical methods. The advent of LLMs introduced new deep learning-\ndriven approaches that led to advancements in understanding,\ngenerating, and translating human language.\nModern LLMs are trained in two main steps.\nFirst, they are pretrained on a large corpus of unlabeled text by using the\nprediction of the next word in a sentence as a ""label.""\nThen, they are finetuned on a smaller, labeled target dataset to follow\ninstructions or perform classification tasks.\nLLMs are based on the transformer architecture. The key idea of the\ntransformer architecture is an attention mechanism that gives the LLM\nselective access to the whole input sequence when generating the output\none word at a time.\nThe original transformer architecture consists of an encoder for parsing\ntext and a decoder for generating text.\nLLMs for generating text and following instructions, such as GPT-3 and\nChatGPT, only implement decoder modules, simplifying the\narchitecture.\nLarge datasets consisting of billions of words are essential for\npretraining LLMs. In this book, we will implement and train LLMs on\nsmall datasets for educational purposes but also see how we can load\nopenly available model weights.\nWhile the general pretraining task for GPT-like models is to predict the\nnext word in a sentence, these LLMs exhibit ""emergent"" properties such\nas capabilities to classify, translate, or summarize texts.\nOnce an LLM is pretrained, the resulting foundation model can be\nfinetuned more efficiently for various downstream tasks.\nLLMs finetuned on custom datasets can outperform general LLMs on\nspecific tasks.\n[1]\n Readers with a background in machine learning may note that labeling\ninformation is typically required for traditional machine learning models and\ndeep neural networks trained via the conventional supervised learning\nparadigm. However, this is not the case for the pretraining stage of LLMs. In\nthis phase, LLMs leverage self-supervised learning, where the model\ngenerates its own labels from the input data. This concept is covered later in\nthis chapter\n[2]\n \nGPT-3, The $4,600,000 Language Model\n,\nhttps://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_\n4600000_language_model/",32898
03-2_Working_with_Text_Data.pdf,03-2_Working_with_Text_Data,"2 Working with Text Data\nThis chapter covers\nPreparing text for large language model training\nSplitting text into word and subword tokens\nByte pair encoding as a more advanced way of tokenizing text\nSampling training examples with a sliding window approach\nConverting tokens into vectors that feed into a large language model\nIn the previous chapter, we delved into the general structure of large language\nmodels (LLMs) and learned that they are pretrained on vast amounts of text.\nSpecifically, our focus was on decoder-only LLMs based on the transformer\narchitecture, which underlies the models used in ChatGPT and other popular\nGPT-like LLMs.\nDuring the pretraining stage, LLMs process text one word at a time. Training\nLLMs with millions to billions of parameters using a next-word prediction\ntask yields models with impressive capabilities. These models can then be\nfurther finetuned to follow general instructions or perform specific target\ntasks. But before we can implement and train LLMs in the upcoming\nchapters, we need to prepare the training dataset, which is the focus of this\nchapter, as illustrated in Figure 2.1\nFigure 2.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a\ngeneral text dataset, and finetuning it on a labeled dataset. This chapter will explain and code the\ndata preparation and sampling pipeline that provides the LLM with the text data for pretraining.\nIn this chapter, you'll learn how to prepare input text for training LLMs. This\ninvolves splitting text into individual word and subword tokens, which can\nthen be encoded into vector representations for the LLM. You'll also learn\nabout advanced tokenization schemes like byte pair encoding, which is\nutilized in popular LLMs like GPT. Lastly, we'll implement a sampling and\ndata loading strategy to produce the input-output pairs necessary for training\nLLMs in subsequent chapters.\n2.1 Understanding word embeddings\nDeep neural network models, including LLMs, cannot process raw text\ndirectly. Since text is categorical, it isn't compatible with the mathematical\noperations used to implement and train neural networks. Therefore, we need a\nway to represent words as continuous-valued vectors. (Readers unfamiliar\nwith vectors and tensors in a computational context can learn more in\nAppendix A, section A2.2 Understanding tensors.)\nThe concept of converting data into a vector format is often referred to as\nembedding\n. Using a specific neural network layer or another pretrained\nneural network model, we can embed different data types, for example,\nvideo, audio, and text, as illustrated in Figure 2.2.\nFigure 2.2 Deep learning models cannot process data formats like video, audio, and text in their\nraw form. Thus, we use an embedding model to transform this raw data into a dense vector\nrepresentation that deep learning architectures can easily understand and process. Specifically,\nthis figure illustrates the process of converting raw data into a three-dimensional numerical\nvector.\nAs shown in Figure 2.2, we can process various different data formats via\nembedding models. However, it's important to note that different data formats\nrequire distinct embedding models. For example, an embedding model\ndesigned for text would not be suitable for embedding audio or video data.\nAt its core, an embedding is a mapping from discrete objects, such as words,\nimages, or even entire documents, to points in a continuous vector space --\nthe primary purpose of embeddings is to convert non-numeric data into a\nformat that neural networks can process.\nWhile word embeddings are the most common form of text embedding, there\nare also embeddings for sentences, paragraphs, or whole documents.\nSentence or paragraph embeddings are popular choices for\n retrieval-\naugmented generation.\n Retrieval-augmented generation combines generation\n(like producing text) with retrieval (like searching an external knowledge\nbase) to pull relevant information when generating text, which is a technique\nthat is beyond the scope of this book. Since our goal is to train GPT-like\nLLMs, which learn to generate text one word at a time, this chapter focuses\non word embeddings.\nThere are several algorithms and frameworks that have been developed to\ngenerate word embeddings. One of the earlier and most popular examples is\nthe \nWord2Vec\n approach. Word2Vec trained neural network architecture to\ngenerate word embeddings by predicting the context of a word given the\ntarget word or vice versa. The main idea behind Word2Vec is that words that\nappear in similar contexts tend to have similar meanings. Consequently,\nwhen projected into 2-dimensional word embeddings for visualization\npurposes, it can be seen that similar terms cluster together, as shown in\nFigure 2.3.\nFigure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional\nscatterplot for visualization purposes as shown here. When using word embedding techniques,\nsuch as Word2Vec, words corresponding to similar concepts often appear close to each other in\nthe embedding space. For instance, different types of birds appear closer to each other in the\nembedding space compared to countries and cities.\nWord embeddings can have varying dimensions, from one to thousands. As\nshown in Figure 2.3, we can choose two-dimensional word embeddings for\nvisualization purposes. A higher dimensionality might capture more nuanced\nrelationships but at the cost of computational efficiency.\nWhile we can use pretrained models such as Word2Vec to generate\nembeddings for machine learning models, LLMs commonly produce their\nown embeddings that are part of the input layer and are updated during\ntraining. The advantage of optimizing the embeddings as part of the LLM\ntraining instead of using Word2Vec is that the embeddings are optimized to\nthe specific task and data at hand. We will implement such embedding layers\nlater in this chapter. Furthermore, LLMs can also create contextualized output\nembeddings, as we discuss in chapter 3.\nUnfortunately, high-dimensional embeddings present a challenge for\nvisualization because our sensory perception and common graphical\nrepresentations are inherently limited to three dimensions or fewer, which is\nwhy Figure 2.3 showed two-dimensional embeddings in a two-dimensional\nscatterplot. However, when working with LLMs, we typically use\nembeddings with a much higher dimensionality than shown in Figure 2.3. For\nboth GPT-2 and GPT-3, the embedding size (often referred to as the\ndimensionality of the model's hidden states) varies based on the specific\nmodel variant and size. It is a trade-off between performance and efficiency.\nThe smallest GPT-2 models (117M and 125M parameters) use an embedding\nsize of 768 dimensions to provide concrete examples. The largest GPT-3\nmodel (175B parameters) uses an embedding size of 12,288 dimensions.\nThe upcoming sections in this chapter will walk through the required steps\nfor preparing the embeddings used by an LLM, which include splitting text\ninto words, converting words into tokens, and turning tokens into embedding\nvectors.\n2.2 Tokenizing text\nThis section covers how we split input text into individual tokens, a required\npreprocessing step for creating embeddings for an LLM. These tokens are\neither individual words or special characters, including punctuation\ncharacters, as shown in Figure 2.4.\nFigure 2.4 A view of the text processing steps covered in this section in the context of an LLM.\nHere, we split an input text into individual tokens, which are either words or special characters,\nsuch as punctuation characters. In upcoming sections, we will convert the text into token IDs and\ncreate token embeddings.\nThe text we will tokenize for LLM training is a short story by Edith Wharton\ncalled \nThe Verdict\n, which has been released into the public domain and is\nthus permitted to be used for LLM training tasks. The text is available on\nWikisource at \nhttps://en.wikisource.org/wiki/The_Verdict\n, and you can copy\nand paste it into a text file, which I copied into a text file \n""the-verdict.txt""\nto load using Python's standard file reading utilities:\nListing 2.1 Reading in a short story as text sample into Python\nwith open(""the-verdict.txt"", ""r"", encoding=""utf-8"") as f:\n    raw_text = f.read()\nprint(""Total number of character:"", len(raw_text))\nprint(raw_text[:99])\nAlternatively, you can find this ""\nthe-verdict.txt"" \nfile in this book's\nGitHub repository at \nhttps://github.com/rasbt/LLMs-from-\nscratch/tree/main/ch02/01_main-chapter-code\n.\nThe print command prints the total number of characters followed by the first\n100 characters of this file for illustration purposes:\nTotal number of character: 20479\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \nOur goal is to tokenize this 20,479-character short story into individual words\nand special characters that we can then turn into embeddings for LLM\ntraining in the upcoming chapters.\nText sample sizes\nNote that it's common to process millions of articles and hundreds of\nthousands of books -- many gigabytes of text -- when working with LLMs.\nHowever, for educational purposes, it's sufficient to work with smaller text\nsamples like a single book to illustrate the main ideas behind the text\nprocessing steps and to make it possible to run it in reasonable time on\nconsumer hardware.\nHow can we best split this text to obtain a list of tokens? For this, we go on a\nsmall excursion and use Python's regular expression library \nre\n for illustration\npurposes. (Note that you don't have to learn or memorize any regular\nexpression syntax since we will transition to a pre-built tokenizer later in this\nchapter.)\nUsing some simple example text, we can use the \nre.split\n command with the\nfollowing syntax to split a text on whitespace characters:\nimport re\ntext = ""Hello, world. This, is a test.""\nresult = re.split(r'(\s)', text)\nprint(result)\nThe result is a list of individual words, whitespaces, and punctuation\ncharacters:\n['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\nNote that the simple tokenization scheme above mostly works for separating\nthe example text into individual words, however, some words are still\nconnected to punctuation characters that we want to have as separate list\nentries. We also refrain from making all text lowercase because capitalization\nhelps LLMs distinguish between proper nouns and common nouns,\nunderstand sentence structure, and learn to generate text with proper\ncapitalization.\nLet's modify the regular expression splits on whitespaces (\n\s\n) and commas,\nand periods (\n[,.]\n):\nresult = re.split(r'([,.]|\s)', text)\nprint(result)\nWe can see that the words and punctuation characters are now separate list\nentries just as we wanted:\n['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\nA small remaining issue is that the list still includes whitespace characters.\nOptionally, we can remove these redundant characters safely as follows:\nresult = [item for item in result if item.strip()]\nprint(result)\nThe resulting whitespace-free output looks like as follows:\n['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\nRemoving whitespaces or not\nWhen developing a simple tokenizer, whether we should encode whitespaces\nas separate characters or just remove them depends on our application and its\nrequirements. Removing whitespaces reduces the memory and computing\nrequirements. However, keeping whitespaces can be useful if we train models\nthat are sensitive to the exact structure of the text (for example, Python code,\nwhich is sensitive to indentation and spacing). Here, we remove whitespaces\nfor simplicity and brevity of the tokenized outputs. Later, we will switch to a\ntokenization scheme that includes whitespaces.\nThe tokenization scheme we devised above works well on the simple sample\ntext. Let's modify it a bit further so that it can also handle other types of\npunctuation, such as question marks, quotation marks, and the double-dashes\nwe have seen earlier in the first 100 characters of Edith Wharton's short story,\nalong with additional special characters:\ntext = ""Hello, world. Is this-- a test?""\nresult = re.split(r'([,.:;?_!""()\']|--|\s)', text)\nresult = [item.strip() for item in result if item.strip()]\nprint(result)\nThe resulting output is as follows:\n['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\nAs we can see based on the results summarized in Figure 2.5, our\ntokenization scheme can now handle the various special characters in the text\nsuccessfully.\nFigure 2.5 The tokenization scheme we implemented so far splits text into individual words and\npunctuation characters. In the specific example shown in this figure, the sample text gets split\ninto 10 individual tokens.\nNow that we got a basic tokenizer working, let's apply it to Edith Wharton's\nentire short story:\npreprocessed = re.split(r'([,.?_!""()\']|--|\s)', raw_text)\npreprocessed = [item.strip() for item in preprocessed if item.strip()]\nprint(len(preprocessed))\nThe above print statement outputs \n4649\n, which is the number of tokens in this\ntext (without whitespaces).\nLet's print the first 30 tokens for a quick visual check:\nprint(preprocessed[:30])\nThe resulting output shows that our tokenizer appears to be handling the text\nwell since all words and special characters are neatly separated:\n['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n2.3 Converting tokens into token IDs\nIn the previous section, we tokenized a short story by Edith Wharton into\nindividual tokens. In this section, we will convert these tokens from a Python\nstring to an integer representation to produce the so-called token IDs. This\nconversion is an intermediate step before converting the token IDs into\nembedding vectors.\nTo map the previously generated tokens into token IDs, we have to build a\nso-called vocabulary first. This vocabulary defines how we map each unique\nword and special character to a unique integer, as shown in Figure 2.6.\nFigure 2.6 We build a vocabulary by tokenizing the entire text in a training dataset into\nindividual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens\nare removed. The unique tokens are then aggregated into a vocabulary that defines a mapping\nfrom each unique token to a unique integer value. The depicted vocabulary is purposefully small\nfor illustration purposes and contains no punctuation or special characters for simplicity.\nIn the previous section, we tokenized Edith Wharton's short story and\nassigned it to a Python variable called \npreprocessed\n. Let's now create a list\nof all unique tokens and sort them alphabetically to determine the vocabulary\nsize:\nall_words = sorted(list(set(preprocessed)))\nvocab_size = len(all_words)\nprint(vocab_size)\nAfter determining that the vocabulary size is 1,159 via the above code, we\ncreate the vocabulary and print its first 50 entries for illustration purposes:\nListing 2.2 Creating a vocabulary\nvocab = {token:integer for integer,token in enumerate(all_words)}\nfor i, item in enumerate(vocab.items()):\n    print(item)\n    if i > 50:\n        break\n('!', 0)\n('""', 1)\n(""'"", 2)\n...\n('Has', 49)\n('He', 50)\nAs we can see, based on the output above, the dictionary contains individual\ntokens associated with unique integer labels. Our next goal is to apply this\nvocabulary to convert new text into token IDs, as illustrated in Figure 2.7.\nFigure 2.7 Starting with a new text sample, we tokenize the text and use the vocabulary to convert\nthe text tokens into token IDs. The vocabulary is built from the entire training set and can be\napplied to the training set itself and any new text samples. The depicted vocabulary contains no\npunctuation or special characters for simplicity.\nLater in this book, when we want to convert the outputs of an LLM from\nnumbers back into text, we also need a way to turn token IDs into text. For\nthis, we can create an inverse version of the vocabulary that maps token IDs\nback to corresponding text tokens.\nLet's implement a complete tokenizer class in Python with an \nencode\n method\nthat splits text into tokens and carries out the string-to-integer mapping to\nproduce token IDs via the vocabulary. In addition, we implement a \ndecode\nmethod that carries out the reverse integer-to-string mapping to convert the\ntoken IDs back into text.\nThe code for this tokenizer implementation is as in listing 2.3:\nListing 2.3 Implementing a simple text tokenizer\nclass SimpleTokenizerV1:\n    def __init__(self, vocab):\n        self.str_to_int = vocab #A\n        self.int_to_str = {i:s for s,i in vocab.items()} #B\n    \n    def encode(self, text): #C\n        preprocessed = re.split(r'([,.?_!""()\']|--|\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n        \n    def decode(self, ids): #D\n        text = "" "".join([self.int_to_str[i] for i in ids]) \n        \n        text = re.sub(r'\s+([,.?!""()\'])', r'\1', text) #E\n        return text\nUsing the \nSimpleTokenizerV1 \nPython class above, we can now instantiate\nnew tokenizer objects via an existing vocabulary, which we can then use to\nencode and decode text, as illustrated in Figure 2.8.\nFigure 2.8 Tokenizer implementations share two common methods: an encode method and a\ndecode method. The encode method takes in the sample text, splits it into individual tokens, and\nconverts the tokens into token IDs via the vocabulary. The decode method takes in token IDs,\nconverts them back into text tokens, and concatenates the text tokens into natural text.\nLet's instantiate a new tokenizer object from the \nSimpleTokenizerV1\n class\nand tokenize a passage from Edith Wharton's short story to try it out in\npractice:\ntokenizer = SimpleTokenizerV1(vocab)\n \ntext = """"""""It's the last he painted, you know,"" Mrs. Gisburn said with pardonable pride.""""""\nids = tokenizer.encode(text)\nprint(ids)\nThe code above prints the following token IDs:\n[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\nNext, let's see if we can turn these token IDs back into text using the decode\nmethod:\nprint(tokenizer.decode(ids))\nThis outputs the following text:\n'"" It\' s the last he painted, you know,"" Mrs. Gisburn said with pardonable pride.'\nBased on the output above, we can see that the decode method successfully\nconverted the token IDs back into the original text.\nSo far, so good. We implemented a tokenizer capable of tokenizing and de-\ntokenizing text based on a snippet from the training set. Let's now apply it to\na new text sample that is not contained in the training set:\ntext = ""Hello, do you like tea?""\ntokenizer.encode(text)\nExecuting the code above will result in the following error:\n...\nKeyError: 'Hello'\nThe problem is that the word ""Hello"" was not used in the \nThe Verdict\n short\nstory. Hence, it is not contained in the vocabulary. This highlights the need to\nconsider large and diverse training sets to extend the vocabulary when\nworking on LLMs.\nIn the next section, we will test the tokenizer further on text that contains\nunknown words, and we will also discuss additional special tokens that can\nbe used to provide further context for an LLM during training.\n2.4 Adding special context tokens\nIn the previous section, we implemented a simple tokenizer and applied it to\na passage from the training set. In this section, we will modify this tokenizer\nto handle unknown words.\nWe will also discuss the usage and addition of special context tokens that can\nenhance a model's understanding of context or other relevant information in\nthe text. These special tokens can include markers for unknown words and\ndocument boundaries, for example.\nIn particular, we will modify the vocabulary and tokenizer we implemented\nin the previous section, \nSimpleTokenizerV2\n, to support two new tokens,\n<|unk|>\n and \n<|endoftext|>\n, as illustrated in Figure 2.9.\nFigure 2.9 We add special tokens to a vocabulary to deal with certain contexts. For instance, we\nadd an <|unk|> token to represent new and unknown words that were not part of the training\ndata and thus not part of the existing vocabulary. Furthermore, we add an <|endoftext|> token\nthat we can use to separate two unrelated text sources.\nAs shown in Figure 2.9, we can modify the tokenizer to use an \n<|unk|>\n token\nif it encounters a word that is not part of the vocabulary. Furthermore, we add\na token between unrelated texts. For example, when training GPT-like LLMs\non multiple independent documents or books, it is common to insert a token\nbefore each document or book that follows a previous text source, as\nillustrated in Figure 2.10. This helps the LLM understand that, although these\ntext sources are concatenated for training, they are, in fact, unrelated.\nFigure 2.10 When working with multiple independent text source, we add <|endoftext|> tokens\nbetween these texts. These <|endoftext|> tokens act as markers, signaling the start or end of a\nparticular segment, allowing for more effective processing and understanding by the LLM.\nLet's now modify the vocabulary to include these two special tokens, \n<unk>\nand \n<|endoftext|>\n, by adding these to the list of all unique words that we\ncreated in the previous section:\nall_tokens = sorted(list(set(preprocessed)))\nall_tokens.extend([""<|endoftext|>"", ""<|unk|>""])\nvocab = {token:integer for integer,token in enumerate(all_tokens)}\n \nprint(len(vocab.items()))\nBased on the output of the print statement above, the new vocabulary size is\n1161 (the vocabulary size in the previous section was 1159).\nAs an additional quick check, let's print the last 5 entries of the updated\nvocabulary:\nfor i, item in enumerate(list(vocab.items())[-5:]):\n    print(item)\nThe code above prints the following:\n('younger', 1156)\n('your', 1157)\n('yourself', 1158)\n('<|endoftext|>', 1159)\n('<|unk|>', 1160)\nBased on the code output above, we can confirm that the two new special\ntokens were indeed successfully incorporated into the vocabulary. Next, we\nadjust the tokenizer from code listing 2.3 accordingly, as shown in listing 2.4:\nListing 2.4 A simple text tokenizer that handles unknown words\nclass SimpleTokenizerV2:\n    def __init__(self, vocab):\n        self.str_to_int = vocab\n        self.int_to_str = { i:s for s,i in vocab.items()}\n    \n    def encode(self, text):\n        preprocessed = re.split(r'([,.?_!""()\']|--|\s)', text)\n        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n        preprocessed = [item if item in self.str_to_int  #A\n                        else ""<|unk|>"" for item in preprocessed]\n \n        ids = [self.str_to_int[s] for s in preprocessed]\n        return ids\n        \n    def decode(self, ids):\n        text = "" "".join([self.int_to_str[i] for i in ids])\n \n        text = re.sub(r'\s+([,.?!""()\'])', r'\1', text) #B\n        return text\nCompared to the \nSimpleTokenizerV1\n we implemented in code listing 2.3 in\nthe previous section, the new \nSimpleTokenizerV2\n replaces unknown words\nby \n<|unk|> \ntokens.\nLet's now try this new tokenizer out in practice. For this, we will use a simple\ntext sample that we concatenate from two independent and unrelated\nsentences:\ntext1 = ""Hello, do you like tea?""\ntext2 = ""In the sunlit terraces of the palace.""\ntext = "" <|endoftext|> "".join((text1, text2))\nprint(text)\nThe output is as follows:\n'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\nNext, let's tokenize the sample text using the \nSimpleTokenizerV2\n on the\nvocab we previously created in listing 2.2:\ntokenizer = SimpleTokenizerV2(vocab)\nprint(tokenizer.encode(text))\nThis prints the following token IDs:\n[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\nAbove, we can see that the list of token IDs contains 1159 for the\n<|endoftext|> separator token as well as two 1160 tokens, which are used for\nunknown words.\nLet's de-tokenize the text for a quick sanity check:\nprint(tokenizer.decode(tokenizer.encode(text)))\nThe output is as follows:\n'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'\nBased on comparing the de-tokenized text above with the original input text,\nwe know that the training dataset, Edith Wharton's short story \nThe Verdict\n,\ndid not contain the words ""Hello"" and ""palace.""\nSo far, we have discussed tokenization as an essential step in processing text\nas input to LLMs. Depending on the LLM, some researchers also consider\nadditional special tokens such as the following:\n[BOS]\n (beginning of sequence): This token marks the start of a text. It\nsignifies to the LLM where a piece of content begins.\n[EOS]\n (end of sequence): This token is positioned at the end of a text,\nand is especially useful when concatenating multiple unrelated texts,\nsimilar to \n<|endoftext|>\n. For instance, when combining two different\nWikipedia articles or books, the \n[EOS]\n token indicates where one article\nends and the next one begins.\n[PAD]\n (padding): When training LLMs with batch sizes larger than one,\nthe batch might contain texts of varying lengths. To ensure all texts have\nthe same length, the shorter texts are extended or ""padded"" using the\n[PAD]\n token, up to the length of the longest text in the batch.\nNote that the tokenizer used for GPT models does not need any of these\ntokens mentioned above but only uses an \n<|endoftext|>\n token for\nsimplicity. The \n<|endoftext|>\n is analogous to the \n[EOS]\n token mentioned\nabove. Also, \n<|endoftext|>\n is used for padding as well. However, as we'll\nexplore in subsequent chapters when training on batched inputs, we typically\nuse a mask, meaning we don't attend to padded tokens. Thus, the specific\ntoken chosen for padding becomes inconsequential.\nMoreover, the tokenizer used for GPT models also doesn't use an \n<|unk|>\ntoken for out-of-vocabulary words. Instead, GPT models use a \nbyte pair\nencoding\n tokenizer, which breaks down words into subword units, which we\nwill discuss in the next section.\n2.5 Byte pair encoding\nWe implemented a simple tokenization scheme in the previous sections for\nillustration purposes. This section covers a more sophisticated tokenization\nscheme based on a concept called byte pair encoding (BPE). The BPE\ntokenizer covered in this section was used to train LLMs such as GPT-2,\nGPT-3, and the original model used in ChatGPT.\nSince implementing BPE can be relatively complicated, we will use an\nexisting Python open-source library called \ntiktoken\n(\nhttps://github.com/openai/tiktoken\n), which implements the BPE algorithm\nvery efficiently based on source code in Rust. Similar to other Python\nlibraries, we can install the tiktoken library via Python's \npip\n installer from the\nterminal:\npip install tiktoken\nThe code in this chapter is based on tiktoken 0.5.1. You can use the following\ncode to check the version you currently have installed:\nfrom importlib.metadata import version\nimport tiktoken\nprint(""tiktoken version:"", version(""tiktoken""))\nOnce installed, we can instantiate the BPE tokenizer from tiktoken as\nfollows:\ntokenizer = tiktoken.get_encoding(""gpt2"")\nThe usage of this tokenizer is similar to SimpleTokenizerV2 we implemented\npreviously via an \nencode\n method:\ntext = ""Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.""\nintegers = tokenizer.encode(text, allowed_special={""<|endoftext|>""})\nprint(integers)\nThe code above prints the following token IDs:\n[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\nWe can then convert the token IDs back into text using the decode method,\nsimilar to our \nSimpleTokenizerV2\n earlier:\nstrings = tokenizer.decode(integers)\nprint(strings)\nThe above code prints the following:\n'Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.'\nWe can make two noteworthy observations based on the token IDs and\ndecoded text above. First, the \n<|endoftext|>\n token is assigned a relatively\nlarge token ID, namely, 50256. In fact, the BPE tokenizer, which was used to\ntrain models such as GPT-2, GPT-3, and the original model used in\nChatGPT, has a total vocabulary size of 50,257, with \n<|endoftext|>\n being\nassigned the largest token ID.\nSecond, the BPE tokenizer above encodes and decodes unknown words, such\nas ""someunknownPlace"" correctly. The BPE tokenizer can handle any\nunknown word. How does it achieve this without using \n<|unk|>\n tokens?\nThe algorithm underlying BPE breaks down words that aren't in its\npredefined vocabulary into smaller subword units or even individual\ncharacters, enabling it to handle out-of-vocabulary words. So, thanks to the\nBPE algorithm, if the tokenizer encounters an unfamiliar word during\ntokenization, it can represent it as a sequence of subword tokens or\ncharacters, as illustrated in Figure 2.11.\nFigure 2.11 BPE tokenizers break down unknown words into subwords and individual\ncharacters. This way, a BPE tokenizer can parse any word and doesn't need to replace unknown\nwords with special tokens, such as <|unk|>.\nAs illustrated in Figure 2.11, the ability to break down unknown words into\nindividual characters ensures that the tokenizer, and consequently the LLM\nthat is trained with it, can process any text, even if it contains words that were\nnot present in its training data.\nExercise 2.1 Byte pair encoding of unknown words\nTry the BPE tokenizer from the tiktoken library on the unknown words\n""Akwirw ier"" and print the individual token IDs. Then, call the decode\nfunction on each of the resulting integers in this list to reproduce the mapping\nshown in Figure 2.11. Lastly, call the decode method on the token IDs to\ncheck whether it can reconstruct the original input, ""Akwirw ier"".\nA detailed discussion and implementation of BPE is out of the scope of this\nbook, but in short, it builds its vocabulary by iteratively merging frequent\ncharacters into subwords and frequent subwords into words. For example,\nBPE starts with adding all individual single characters to its vocabulary (""a"",\n""b"", ...). In the next stage, it merges character combinations that frequently\noccur together into subwords. For example, ""d"" and ""e"" may be merged into\nthe subword ""de,"" which is common in many English words like ""define"",\n""depend"", ""made"", and ""hidden"". The merges are determined by a frequency\ncutoff.\n2.6 Data sampling with a sliding window\nThe previous section covered the tokenization steps and conversion from\nstring tokens into integer token IDs in great detail. The next step before we\ncan finally create the embeddings for the LLM is to generate the input-target\npairs required for training an LLM.\nWhat do these input-target pairs look like? As we learned in chapter 1, LLMs\nare pretrained by predicting the next word in a text, as depicted in figure 2.12.\nFigure 2.12 Given a text sample, extract input blocks as subsamples that serve as input to the\nLLM, and the LLM's prediction task during training is to predict the next word that follows the\ninput block. During training, we mask out all words that are past the target. Note that the text\nshown in this figure would undergo tokenization before the LLM can process it; however, this\nfigure omits the tokenization step for clarity.\nIn this section we implement a data loader that fetches the input-target pairs\ndepicted in Figure 2.12 from the training dataset using a sliding window\napproach.\nTo get started, we will first tokenize the whole The Verdict short story we\nworked with earlier using the BPE tokenizer introduced in the previous\nsection:\nwith open(""the-verdict.txt"", ""r"", encoding=""utf-8"") as f:\n    raw_text = f.read()\n \nenc_text = tokenizer.encode(raw_text)\nprint(len(enc_text))\nExecuting the code above will return 5145, the total number of tokens in the\ntraining set, after applying the BPE tokenizer.\nNext, we remove the first 50 tokens from the dataset for demonstration\npurposes as it results in a slightly more interesting text passage in the next\nsteps:\nenc_sample = enc_text[50:]\nOne of the easiest and most intuitive ways to create the input-target pairs for\nthe next-word prediction task is to create two variables, \nx\n and \ny\n, where \nx\ncontains the input tokens and \ny\n contains the targets, which are the inputs\nshifted by 1:\ncontext_size = 4 #A\nx = enc_sample[:context_size]\ny = enc_sample[1:context_size+1]\nprint(f""x: {x}"")\nprint(f""y:\n      \n{y}"")\nRunning the above code prints the following output:\nx: [290, 4920, 2241, 287]\ny:      [4920, 2241, 287, 257]\nProcessing the inputs along with the targets, which are the inputs shifted by\none position, we can then create the next-word prediction tasks depicted\nearlier in figure 2.12, as follows:\nfor i in range(1, context_size+1):\n    context = enc_sample[:i]\n    desired = enc_sample[i]\n    print(context, ""---->"", desired)\nThe code above prints the following:\n[290] ----> 4920\n[290, 4920] ----> 2241\n[290, 4920, 2241] ----> 287\n[290, 4920, 2241, 287] ----> 257\nEverything left of the arrow (\n---->\n) refers to the input an LLM would\nreceive, and the token ID on the right side of the arrow represents the target\ntoken ID that the LLM is supposed to predict.\nFor illustration purposes, let's repeat the previous code but convert the token\nIDs into text:\nfor i in range(1, context_size+1):\n    context = enc_sample[:i]\n    desired = enc_sample[i]\n    print(tokenizer.decode(context), ""---->"", tokenizer.decode([desired]))\nThe following outputs show how the input and outputs look in text format:\n and ---->  established\n and established ---->  himself\n and established himself ---->  in\n and established himself in ---->  a\nWe've now created the input-target pairs that we can turn into use for the\nLLM training in upcoming chapters.\nThere's only one more task before we can turn the tokens into embeddings, as\nwe mentioned at the beginning of this chapter: implementing an efficient data\nloader that iterates over the input dataset and returns the inputs and targets as\nPyTorch tensors, which can be thought of as multidimensional arrays.\nIn particular, we are interested in returning two tensors: an input tensor\ncontaining the text that the LLM sees and a target tensor that includes the\ntargets for the LLM to predict, as depicted in Figure 2.13.\nFigure 2.13 To implement efficient data loaders, we collect the inputs in a tensor, x, where each\nrow represents one input context. A second tensor, y, contains the corresponding prediction\ntargets (next words), which are created by shifting the input by one position.\nWhile Figure 2.13 shows the tokens in string format for illustration purposes,\nthe code implementation will operate on token IDs directly since the \nencode\nmethod of the BPE tokenizer performs both tokenization and conversion into\ntoken IDs as a single step.\nFor the efficient data loader implementation, we will use PyTorch's built-in\nDataset and DataLoader classes. For additional information and guidance on\ninstalling PyTorch, please see section A.1.3, Installing PyTorch, in Appendix\nA.\nThe code for the dataset class is shown in code listing 2.5:\nListing 2.5 A dataset for batched inputs and targets\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n \nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.target_ids = []\n \n        token_ids = tokenizer.encode(txt) #A\n \n        for i in range(0, len(token_ids) - max_length, stride): #B\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n \n    def __len__(self): #C\n        return len(self.input_ids)\n \n    def __getitem__(self, idx): #D\n        return self.input_ids[idx], self.target_ids[idx]\nThe \nGPTDatasetV1\n class in listing 2.5 is based on the PyTorch \nDataset\n class\nand defines how individual rows are fetched from the dataset, where each\nrow consists of a number of token IDs (based on a \nmax_length\n) assigned to\nan \ninput_chunk\n tensor. The \ntarget_chunk\n tensor contains the corresponding\ntargets. I recommend reading on to see how the data returned from this\ndataset looks like when we combine the dataset with a PyTorch \nDataLoader\n -\n- this will bring additional intuition and clarity.\nIf you are new to the structure of PyTorch \nDataset\n classes, such as shown in\nlisting 2.5, please read section \nA.6, Setting up efficient data loaders\n, in\nAppendix A, which explains the general structure and usage of PyTorch\nDataset\n and \nDataLoader\n classes.\nThe following code will use the \nGPTDatasetV1\n to load the inputs in batches\nvia a PyTorch \nDataLoader\n:\nListing 2.6 A data loader to generate batches with input-with pairs\ndef create_dataloader_v1(txt, batch_size=4, \n        max_length=256, stride=128, shuffle=True, drop_last=True):\n    tokenizer = tiktoken.get_encoding(""gpt2"") #A \n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last) #C\n    return dataloader\nLet's test the \ndataloader\n with a batch size of 1 for an LLM with a context\nsize of 4 to develop an intuition of how the \nGPTDatasetV1\n class from listing\n2.5 and the \ncreate_dataloader_v1\n function from listing 2.6 work together:\nwith open(""the-verdict.txt"", ""r"", encoding=""utf-8"") as f:\n    raw_text = f.read()\n \ndataloader = create_dataloader_v1(\n    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\ndata_iter = iter(dataloader) #A\nfirst_batch = next(data_iter)\nprint(first_batch)\nExecuting the preceding code prints the following:\n[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\nThe \nfirst_batch\n variable contains two tensors: the first tensor stores the\ninput token IDs, and the second tensor stores the target token IDs. Since the\nmax_length\n is set to 4, each of the two tensors contains 4 token IDs. Note\nthat an input size of 4 is relatively small and only chosen for illustration\npurposes. It is common to train LLMs with input sizes of at least 256.\nTo illustrate the meaning of \nstride=1\n, let's fetch another batch from this\ndataset:\nsecond_batch = next(data_iter)\nprint(second_batch)\nThe second batch has the following contents:\n[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\nIf we compare the first with the second batch, we can see that the second\nbatch's token IDs are shifted by one position compared to the first batch (for\nexample, the second ID in the first batch's input is 367, which is the first ID\nof the second batch's input). The \nstride\n setting dictates the number of\npositions the inputs shift across batches, emulating a sliding window\napproach, as demonstrated in Figure 2.14.\nFigure 2.14 When creating multiple batches from the input dataset, we slide an input window\nacross the text. If the stride is set to 1, we shift the input window by 1 position when creating the\nnext batch. If we set the stride equal to the input window size, we can prevent overlaps between\nthe batches.\nExercise 2.2 Data loaders with different strides and context sizes\nTo develop more intuition for how the data loader works, try to run it with\ndifferent settings such as max_length=2 and stride=2 and max_length=8 and\nstride=2.\nBatch sizes of 1, such as we have sampled from the data loader so far, are\nuseful for illustration purposes. If you have previous experience with deep\nlearning, you may know that small batch sizes require less memory during\ntraining but lead to more noisy model updates. Just like in regular deep\nlearning, the batch size is a trade-off and hyperparameter to experiment with\nwhen training LLMs.\nBefore we move on to the two final sections of this chapter that are focused\non creating the embedding vectors from the token IDs, let's have a brief look\nat how we can use the data loader to sample with a batch size greater than 1:\ndataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n \ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(""Inputs:\n"", inputs)\nprint(""\nTargets:\n"", targets)\nThis prints the following:\nInputs:\n tensor([[   40,   367,  2885,  1464],\n        [ 1807,  3619,   402,   271],\n        [10899,  2138,   257,  7026],\n        [15632,   438,  2016,   257],\n        [  922,  5891,  1576,   438],\n        [  568,   340,   373,   645],\n        [ 1049,  5975,   284,   502],\n        [  284,  3285,   326,    11]])\n \nTargets:\n tensor([[  367,  2885,  1464,  1807],\n        [ 3619,   402,   271, 10899],\n        [ 2138,   257,  7026, 15632],\n        [  438,  2016,   257,   922],\n        [ 5891,  1576,   438,   568],\n        [  340,   373,   645,  1049],\n        [ 5975,   284,   502,   284],\n        [ 3285,   326,    11,   287]])\nNote that we increase the stride to 4. This is to utilize the data set fully (we\ndon't skip a single word) but also avoid any overlap between the batches,\nsince more overlap could lead to increased overfitting.\nIn the final two sections of this chapter, we will implement embedding layers\nthat convert the token IDs into continuous vector representations, which serve\nas input data format for LLMs.\n2.7 Creating token embeddings\nThe last step for preparing the input text for LLM training is to convert the\ntoken IDs into embedding vectors, as illustrated in Figure 2.15, which will be\nthe focus of these two last remaining sections of this chapter.\nFigure 2.15 Preparing the input text for an LLM involves tokenizing text, converting text tokens\nto token IDs, and converting token IDs into vector embedding vectors. In this section, we consider\nthe token IDs created in previous sections to create the token embedding vectors.\nIn addition to the processes outlined in Figure 2.15, it is important to note\nthat we initialize these embedding weights with random values as a\npreliminary step. This initialization serves as the starting point for the LLM's\nlearning process. We will optimize the embedding weights as part of the\nLLM training in chapter 5.\nA continuous vector representation, or embedding, is necessary since GPT-\nlike LLMs are deep neural networks trained with the backpropagation\nalgorithm. If you are unfamiliar with how neural networks are trained with\nbackpropagation, please read section A.4, \nAutomatic differentiation made\neasy\n, in Appendix A.\nLet's illustrate how the token ID to embedding vector conversion works with\na hands-on example. Suppose we have the following four input tokens with\nIDs 2, 3, 5, and 1:\ninput_ids = torch.tensor([2, 3, 5, 1])\nFor the sake of simplicity and illustration purposes, suppose we have a small\nvocabulary of only 6 words (instead of the 50,257 words in the BPE\ntokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-\n3, the embedding size is 12,288 dimensions):\nvocab_size = 6\noutput_dim = 3\nUsing the \nvocab_size\n and \noutput_dim\n, we can instantiate an embedding\nlayer in PyTorch, setting the random seed to 123 for reproducibility purposes:\ntorch.manual_seed(123)\nembedding_layer = torch.nn.Embedding(vocab_size, output_dim)\nprint(embedding_layer.weight)\nThe print statement in the preceding code example prints the embedding\nlayer's underlying weight matrix:\nParameter containing:\ntensor([[ 0.3374, -0.1778, -0.1690],\n        [ 0.9178,  1.5810,  1.3010],\n        [ 1.2753, -0.2010, -0.1606],\n        [-0.4015,  0.9666, -1.1481],\n        [-1.1589,  0.3255, -0.6315],\n        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\nWe can see that the weight matrix of the embedding layer contains small,\nrandom values. These values are optimized during LLM training as part of\nthe LLM optimization itself, as we will see in upcoming chapters. Moreover,\nwe can see that the weight matrix has six rows and three columns. There is\none row for each of the six possible tokens in the vocabulary. And there is\none column for each of the three embedding dimensions.\nAfter we instantiated the embedding layer, let's now apply it to a token ID to\nobtain the embedding vector:\nprint(embedding_layer(torch.tensor([3])))\nThe returned embedding vector is as follows:\ntensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\nIf we compare the embedding vector for token ID 3 to the previous\nembedding matrix, we see that it is identical to the 4th row (Python starts\nwith a zero index, so it's the row corresponding to index 3). In other words,\nthe embedding layer is essentially a look-up operation that retrieves rows\nfrom the embedding layer's weight matrix via a token ID.\nEmbedding layers versus matrix multiplication\nFor those who are familiar with one-hot encoding, the embedding layer\napproach above is essentially just a more efficient way of implementing one-\nhot encoding followed by matrix multiplication in a fully connected layer,\nwhich is illustrated in the supplementary code on GitHub at\nhttps://github.com/rasbt/LLMs-from-\nscratch/tree/main/ch02/03_bonus_embedding-vs-matmul\n. Because the\nembedding layer is just a more efficient implementation equivalent to the\none-hot encoding and matrix-multiplication approach, it can be seen as a\nneural network layer that can be optimized via backpropagation.\nPreviously, we have seen how to convert a single token ID into a three-\ndimensional embedding vector. Let's now apply that to all four input IDs we\ndefined earlier (\ntorch.tensor([2, 3, 5, 1])\n):\nprint(embedding_layer(input_ids))\nThe print output reveals that this results in a 4x3 matrix:\ntensor([[ 1.2753, -0.2010, -0.1606],\n        [-0.4015,  0.9666, -1.1481],\n        [-2.8400, -0.7849, -1.4096],\n        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\nEach row in this output matrix is obtained via a lookup operation from the\nembedding weight matrix, as illustrated in Figure 2.16.\nFigure 2.16 Embedding layers perform a look-up operation, retrieving the embedding vector\ncorresponding to the token ID from the embedding layer's weight matrix. For instance, the\nembedding vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is\nthe sixth instead of the fifth row because Python starts counting at 0). For illustration purposes,\nwe assume that the token IDs were produced by the small vocabulary we used in section 2.3.\nThis section covered how we create embedding vectors from token IDs. The\nnext and final section of this chapter will add a small modification to these\nembedding vectors to encode positional information about a token within a\ntext.\n2.8 Encoding word positions\nIn the previous section, we converted the token IDs into a continuous vector\nrepresentation, the so-called token embeddings. In principle, this is a suitable\ninput for an LLM. However, a minor shortcoming of LLMs is that their self-\nattention mechanism, which will be covered in detail in chapter 3, doesn't\nhave a notion of position or order for the tokens within a sequence.\nThe way the previously introduced embedding layer works is that the same\ntoken ID always gets mapped to the same vector representation, regardless of\nwhere the token ID is positioned in the input sequence, as illustrated in\nFigure 2.17.\nFigure 2.17 The embedding layer converts a token ID into the same vector representation\nregardless of where it is located in the input sequence. For example, the token ID 5, whether it's\nin the first or third position in the token ID input vector, will result in the same embedding\nvector.\nIn principle, the deterministic, position-independent embedding of the token\nID is good for reproducibility purposes. However, since the self-attention\nmechanism of LLMs itself is also position-agnostic, it is helpful to inject\nadditional position information into the LLM.\nTo achieve this, there are two broad categories of position-aware\nembeddings: relative \npositional embeddings\n and absolute positional\nembeddings.\nAbsolute positional embeddings are directly associated with specific\npositions in a sequence. For each position in the input sequence, a unique\nembedding is added to the token's embedding to convey its exact location.\nFor instance, the first token will have a specific positional embedding, the\nsecond token another distinct embedding, and so on, as illustrated in Figure\n2.18.\nFigure 2.18 Positional embeddings are added to the token embedding vector to create the input\nembeddings for an LLM. The positional vectors have the same dimension as the original token\nembeddings. The token embeddings are shown with value 1 for simplicity.\nInstead of focusing on the absolute position of a token, the emphasis of\nrelative positional embeddings is on the relative position or distance between\ntokens. This means the model learns the relationships in terms of ""how far\napart"" rather than ""at which exact position."" The advantage here is that the\nmodel can generalize better to sequences of varying lengths, even if it hasn't\nseen such lengths during training.\nBoth types of positional embeddings aim to augment the capacity of LLMs to\nunderstand the order and relationships between tokens, ensuring more\naccurate and context-aware predictions. The choice between them often\ndepends on the specific application and the nature of the data being\nprocessed.\nOpenAI's GPT models use absolute positional embeddings that are optimized\nduring the training process rather than being fixed or predefined like the\npositional encodings in the original Transformer model. This optimization\nprocess is part of the model training itself, which we will implement later in\nthis book. For now, let's create the initial positional embeddings to create the\nLLM inputs for the upcoming chapters.\nPreviously, we focused on very small embedding sizes in this chapter for\nillustration purposes. We now consider more realistic and useful embedding\nsizes and encode the input tokens into a 256-dimensional vector\nrepresentation. This is smaller than what the original GPT-3 model used (in\nGPT-3, the embedding size is 12,288 dimensions) but still reasonable for\nexperimentation. Furthermore, we assume that the token IDs were created by\nthe BPE tokenizer that we implemented earlier, which has a vocabulary size\nof 50,257:\noutput_dim = 256\nvocab_size = 50257\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\nUsing the \ntoken_embedding_layer\n above, if we sample data from the data\nloader, we embed each token in each batch into a 256-dimensional vector. If\nwe have a batch size of 8 with four tokens each, the result will be an 8 x 4 x\n256 tensor.\nLet's instantiate the data loader from section 2.6, \nData sampling with a\nsliding window\n, first:\nmax_length = 4\ndataloader = create_dataloader_v1(\n    raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\ndata_iter = iter(dataloader)\ninputs, targets = next(data_iter)\nprint(""Token IDs:\n"", inputs)\nprint(""\nInputs shape:\n"", inputs.shape)\nThe preceding code prints the following output:\nToken IDs:\n tensor([[   40,   367,  2885,  1464],\n        [ 1807,  3619,   402,   271],\n        [10899,  2138,   257,  7026],\n        [15632,   438,  2016,   257],\n        [  922,  5891,  1576,   438],\n        [  568,   340,   373,   645],\n        [ 1049,  5975,   284,   502],\n        [  284,  3285,   326,    11]])\n \nInputs shape:\n torch.Size([8, 4])\nAs we can see, the token ID tensor is 8x4-dimensional, meaning that the data\nbatch consists of 8 text samples with 4 tokens each.\nLet's now use the embedding layer to embed these token IDs into 256-\ndimensional vectors:\ntoken_embeddings = token_embedding_layer(inputs)\nprint(token_embeddings.shape)\nThe preceding print function call returns the following:\ntorch.Size([8, 4, 256])\nAs we can tell based on the 8x4x256-dimensional tensor output, each token\nID is now embedded as a 256-dimensional vector.\nFor a GPT model's absolute embedding approach, we just need to create\nanother embedding layer that has the same dimension as the\ntoken_embedding_layer\n:\ncontext_length = max_length\npos_embedding_layer = torch.nn.Embedding(context_lengthe, output_dim)\npos_embeddings = pos_embedding_layer(torch.arange(context_length))\nprint(pos_embeddings.shape)\nAs shown in the preceding code example, the input to the \npos_embeddings\n is\nusually a placeholder vector \ntorch.arange(context_length)\n, which\ncontains a sequence of numbers 0, 1, ..., up to the maximum input length − 1.\nThe \ncontext_length\n is a variable that represents the supported input size of\nthe LLM. Here, we choose it similar to the maximum length of the input text.\nIn practice, input text can be longer than the supported context length, in\nwhich case we have to truncate the text.\nThe output of the print statement is as follows:\ntorch.Size([4, 256])\nAs we can see, the positional embedding tensor consists of four 256-\ndimensional vectors. We can now add these directly to the token embeddings,\nwhere PyTorch will add the 4x256-dimensional \npos_embeddings\n tensor to\neach 4x256-dimensional token embedding tensor in each of the 8 batches:\ninput_embeddings = token_embeddings + pos_embeddings\nprint(input_embeddings.shape)\nThe print output is as follows:\ntorch.Size([8, 4, 256])\nThe \ninput_embeddings\n we created, as summarized in Figure 2.19, are the\nembedded input examples that can now be processed by the main LLM\nmodules, which we will begin implementing in chapter 3\nFigure 2.19 As part of the input processing pipeline, input text is first broken up into individual\ntokens. These tokens are then converted into token IDs using a vocabulary. The token IDs are\nconverted into embedding vectors to which positional embeddings of a similar size are added,\nresulting in input embeddings that are used as input for the main LLM layers.\n2.9 Summary\nLLMs require textual data to be converted into numerical vectors,\nknown as embeddings since they can't process raw text. Embeddings\ntransform discrete data (like words or images) into continuous vector\nspaces, making them compatible with neural network operations.\nAs the first step, raw text is broken into tokens, which can be words or\ncharacters. Then, the tokens are converted into integer representations,\ntermed token IDs.\nSpecial tokens, such as \n<|unk|>\n and \n<|endoftext|>\n, can be added to\nenhance the model's understanding and handle various contexts, such as\nunknown words or marking the boundary between unrelated texts.\nThe byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and\nGPT-3 can efficiently handle unknown words by breaking them down\ninto subword units or individual characters.\nWe use a sliding window approach on tokenized data to generate input-\ntarget pairs for LLM training.\nEmbedding layers in PyTorch function as a lookup operation, retrieving\nvectors corresponding to token IDs. The resulting embedding vectors\nprovide continuous representations of tokens, which is crucial for\ntraining deep learning models like LLMs.\nWhile token embeddings provide consistent vector representations for\neach token, they lack a sense of the token's position in a sequence. To\nrectify this, two main types of positional embeddings exist: absolute and\nrelative. OpenAI's GPT models utilize absolute positional embeddings\nthat are added to the token embedding vectors and are optimized during\nthe model training.",56613
04-3_Coding_Attention_Mechanisms.pdf,04-3_Coding_Attention_Mechanisms,"3 Coding Attention Mechanisms\nThis chapter covers\nExploring the reasons for using attention mechanisms in neural networks\nIntroducing a basic self-attention framework and progressing to an\nenhanced self-attention mechanism\nImplementing a causal attention module that allows LLMs to generate\none token at a time\nMasking randomly selected attention weights with dropout to reduce\noverfitting\nStacking multiple causal attention modules into a multi-head attention\nmodule\nIn the previous chapter, you learned how to prepare the input text for training\nLLMs. This involved splitting text into individual word and subword tokens,\nwhich can be encoded into vector representations, the so-called embeddings,\nfor the LLM.\nIn this chapter, we will now look at an integral part of the LLM architecture\nitself, attention mechanisms, as illustrated in Figure 3.1.\nFigure 3.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a\ngeneral text dataset, and finetuning it on a labeled dataset. This chapter focuses on attention\nmechanisms, which are an integral part of an LLM architecture.\n\nAttention mechanisms are a comprehensive topic, which is why we are\ndevoting a whole chapter to it. We will largely look at these attention\nmechanisms in isolation and focus on them at a mechanistic level. In the next\nchapter, we will then code the remaining parts of the LLM surrounding the\nself-attention mechanism to see it in action and to create a model to generate\ntext.\nOver the course of this chapter, we will implement four different variants of\nattention mechanisms, as illustrated in Figure 3.2.\nFigure 3.2 The figure depicts different attention mechanisms we will code in this chapter, starting\nwith a simplified version of self-attention before adding the trainable weights. The causal\nattention mechanism adds a mask to self-attention that allows the LLM to generate one word at a\ntime. Finally, multi-head attention organizes the attention mechanism into multiple heads,\nallowing the model to capture various aspects of the input data in parallel.\nThese different attention variants shown in Figure 3.2 build on each other,\nand the goal is to arrive at a compact and efficient implementation of multi-\nhead attention at the end of this chapter that we can then plug into the LLM\narchitecture we will code in the next chapter.\n3.1 The problem with modeling long sequences\nBefore we dive into the \nself-attention \nmechanism that is at the heart of LLMs\nlater in this chapter, what is the problem with architectures without attention\nmechanisms that predate LLMs? Suppose we want to develop a language\ntranslation model that translates text from one language into another. As\nshown in Figure 3.3, we can't simply translate a text word by word due to the\ngrammatical structures in the source and target language.\nFigure 3.3 When translating text from one language to another, such as German to English, it's\nnot possible to merely translate word by word. Instead, the translation process requires\ncontextual understanding and grammar alignment.\nTo address the issue that we cannot translate text word by word, it is common\nto use a deep neural network with two submodules, a so-called \nencoder\n and\ndecoder\n. The job of the encoder is to first read in and process the entire text,\nand the decoder then produces the translated text.\nWe already briefly discussed encoder-decoder networks when we introduced\nthe transformer architecture in chapter 1 (section 1.4, Using LLMs for\ndifferent tasks\n)\n. Before the advent of transformers, \nrecurrent neural networks\n(RNNs) were the most popular encoder-decoder architecture for language\ntranslation.\nAn RNN is a type of neural network where outputs from previous steps are\nfed as inputs to the current step, making them well-suited for sequential data\nlike text. If you are unfamiliar with RNNs, don't worry, you don't need to\nknow the detailed workings of RNNs to follow this discussion; our focus here\nis more on the general concept of the encoder-decoder setup.\nIn an encoder-decoder RNN, the input text is fed into the encoder, which\nprocesses it sequentially. The encoder updates its hidden state (the internal\nvalues at the hidden layers) at each step, trying to capture the entire meaning\nof the input sentence in the final hidden state, as illustrated in Figure 3.4. The\ndecoder then takes this final hidden state to start generating the translated\nsentence, one word at a time. It also updates its hidden state at each step,\nwhich is supposed to carry the context necessary for the next-word\nprediction.\nFigure 3.4 Before the advent of transformer models, encoder-decoder RNNs were a popular\nchoice for machine translation. The encoder takes a sequence of tokens from the source language\nas input, where a hidden state (an intermediate neural network layer) of the encoder encodes a\ncompressed representation of the entire input sequence. Then, the decoder uses its current hidden\nstate to begin the translation, token by token.\nWhile we don't need to know the inner workings of these encoder-decoder\nRNNs, the key idea here is that the encoder part processes the entire input\ntext into a hidden state (memory cell). The decoder then takes in this hidden\nstate to produce the output. You can think of this hidden state as an\nembedding vector, a concept we discussed in chapter 2.\nThe big issue and limitation of encoder-decoder RNNs is that the RNN can't\ndirectly access earlier hidden states from the encoder during the decoding\nphase. Consequently, it relies solely on the current hidden state, which\nencapsulates all relevant information. This can lead to a loss of context,\nespecially in complex sentences where dependencies might span long\ndistances.\nFor readers unfamiliar with RNNs, it is not essential to understand or study\nthis architecture as we will not be using it in this book. The takeaway\nmessage of this section is that encoder-decoder RNNs had a shortcoming that\nmotivated the design of attention mechanisms.\n3.2 Capturing data dependencies with attention\nmechanisms\nBefore transformer LLMs, it was common to use RNNs for language\nmodeling tasks such as language translation, as mentioned previously. RNNs\nwork fine for translating short sentences but don't work well for longer texts\nas they don't have direct access to previous words in the input.\nOne major shortcoming in this approach is that the RNN must remember the\nentire encoded input in a single hidden state before passing it to the decoder,\nas illustrated in Figure 3.4 in the previous section.\nHence, researchers developed the so-called \nBahdanau attention\n mechanism\nfor RNNs in 2014 (named after the first author of the respective paper),\nwhich modifies the encoder-decoder RNN such that the decoder can\nselectively access different parts of the input sequence at each decoding step\nas illustrated in Figure 3.5.\nFigure 3.5 Using an attention mechanism, the text-generating decoder part of the network can\naccess all input tokens selectively. This means that some input tokens are more important than\nothers for generating a given output token. The importance is determined by the so-called\nattention weights, which we will compute later. Note that this figure shows the general idea\nbehind attention and does not depict the exact implementation of the Bahdanau mechanism,\nwhich is an RNN method outside this book's scope.\n\nInterestingly, only three years later, researchers found that RNN architectures\nare not required for building deep neural networks for natural language\nprocessing and proposed the original \ntransformer\n architecture (discussed in\nchapter 1) with a self-attention mechanism inspired by the Bahdanau\nattention mechanism.\nSelf-attention is a mechanism that allows each position in the input sequence\nto attend to all positions in the same sequence when computing the\nrepresentation of a sequence. Self-attention is a key component of\ncontemporary LLMs based on the transformer architecture, such as the GPT\nseries.\nThis chapter focuses on coding and understanding this self-attention\nmechanism used in GPT-like models, as illustrated in Figure 3.6. In the next\nchapter, we will then code the remaining parts of the LLM.\nFigure 3.6 Self-attention is a mechanism in transformers that is used to compute more efficient\ninput representations by allowing each position in a sequence to interact with and weigh the\nimportance of all other positions within the same sequence. In this chapter, we will code this self-\nattention mechanism from the ground up before we code the remaining parts of the GPT-like\nLLM in the following chapter.\n3.3 Attending to different parts of the input with\nself-attention\nWe'll now delve into the inner workings of the self-attention mechanism and\nlearn how to code it from the ground up. Self-attention serves as the\ncornerstone of every LLM based on the transformer architecture. It's worth\nnoting that this topic may require a lot of focus and attention (no pun\nintended), but once you grasp its fundamentals, you will have conquered one\nof the toughest aspects of this book and implementing LLMs in general.\nThe ""self"" in self-attention\nIn self-attention, the ""self"" refers to the mechanism's ability to compute\nattention weights by relating different positions within a single input\nsequence. It assesses and learns the relationships and dependencies between\nvarious parts of the input itself, such as words in a sentence or pixels in an\nimage. This is in contrast to traditional attention mechanisms, where the\nfocus is on the relationships between elements of two different sequences,\nsuch as in sequence-to-sequence models where the attention might be\nbetween an input sequence and an output sequence, such as the example\ndepicted in Figure 3.5.\nSince self-attention can appear complex, especially if you are encountering it\nfor the first time, we will begin by introducing a simplified version of self-\nattention in the next subsection. Afterwards, in section 3.4, we will then\nimplement the self-attention mechanism with trainable weights, which is used\nin LLMs.\n3.3.1 A simple self-attention mechanism without trainable\nweights\nIn this section, we implement a simplified variant of self-attention, free from\nany trainable weights, which is summarized in Figure 3.7. The goal of this\nsection is to illustrate a few key concepts in self-attention before adding\ntrainable weights next in section 3.4.\nFigure 3.7 The goal of self-attention is to compute a context vector, for each input element, that\ncombines information from all other input elements. In the example depicted in this figure, we\ncompute the context vector \nz\n(2)\n. The importance or contribution of each input element for\ncomputing \nz\n(2)\n is determined by the attention weights \nα\n21\n to \nα\n2T\n. When computing \nz\n(2)\n, the\nattention weights are calculated with respect to input element \nx\n(2)\n and all other inputs. The exact\ncomputation of these attention weights is discussed later in this section.\nFigure 3.7 shows an input sequence, denoted as \nx\n, consisting of \nT\n elements\nrepresented as \nx\n(1)\n to \nx\n(T)\n. This sequence typically represents text, such as a\nsentence, that has already been transformed into token embeddings, as\nexplained in chapter 2.\nFor example, consider an input text like \n""Your journey starts with one step.""\nIn this case, each element of the sequence, such as \nx\n(1)\n, corresponds to a \nd\n-\ndimensional embedding vector representing a specific token, like ""Your."" In\nFigure 3.7, these input vectors are shown as 3-dimensional embeddings.\nIn self-attention, our goal is to calculate context vectors \nz\n(i)\n for each element\nx\n(i)\n in the input sequence. A \ncontext vector\n can be interpreted as an enriched\nembedding vector.\nTo illustrate this concept, let's focus on the embedding vector of the second\ninput element, \nx\n(2)\n (which corresponds to the token ""journey""), and the\ncorresponding context vector, \nz\n(2)\n, shown at the bottom of Figure 3.7. This\nenhanced context vector, \nz\n(2)\n, is an embedding that contains information\nabout \nx\n(2)\n and all other input elements \nx\n(1)\n to \nx\n(T)\n.\nIn self-attention, context vectors play a crucial role. Their purpose is to create\nenriched representations of each element in an input sequence (like a\nsentence) by incorporating information from all other elements in the\nsequence, as illustrated in Figure 3.7. This is essential in LLMs, which need\nto understand the relationship and relevance of words in a sentence to each\nother. Later, we will add trainable weights that help an LLM learn to\nconstruct these context vectors so that they are relevant for the LLM to\ngenerate the next token.\nIn this section, we implement a simplified self-attention mechanism to\ncompute these weights and the resulting context vector one step at a time.\nConsider the following input sentence, which has already been embedded\ninto 3-dimensional vectors as discussed in chapter 2. We choose a small\nembedding dimension for illustration purposes to ensure it fits on the page\nwithout line breaks:\nimport torch\ninputs = torch.tensor(\n  [[0.43, 0.15, 0.89], # Your     (x^1)\n   [0.55, 0.87, 0.66], # journey  (x^2)\n   [0.57, 0.85, 0.64], # starts   (x^3)\n   [0.22, 0.58, 0.33], # with     (x^4)\n   [0.77, 0.25, 0.10], # one      (x^5)\n   [0.05, 0.80, 0.55]] # step     (x^6)\n)\nThe first step of implementing self-attention is to compute the intermediate\nvalues \nω, \nreferred to as attention scores, as illustrated in Figure 3.8.\nFigure 3.8 The overall goal of this section is to illustrate the computation of the context vector \nz\n(2)\nusing the second input sequence, \nx\n(2)\n as a query. This figure shows the first intermediate step,\ncomputing the attention scores \nω\n between the query \nx\n(2)\n and all other input elements as a dot\nproduct. (Note that the numbers in the figure are truncated to one digit after the decimal point to\nreduce visual clutter.)\n\nFigure 3.8 illustrates how we calculate the intermediate attention scores\nbetween the query token and each input token. We determine these scores by\ncomputing the dot product of the query, \nx\n(2)\n, with every other input token:\nquery = inputs[1]  #A \nattn_scores_2 = torch.empty(inputs.shape[0])\nfor i, x_i in enumerate(inputs):\n    attn_scores_2[i] = torch.dot(x_i, query)\nprint(attn_scores_2)\nThe computed attention scores are as follows:\ntensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\nUnderstanding dot products\nA dot product is essentially just a concise way of multiplying two vectors\nelement-wise and then summing the products, which we can demonstrate as\nfollows:\nres = 0.\nfor idx, element in enumerate(inputs[0]):\n    \nres += inputs[0][idx] * query[idx]\nprint(res)\nprint(torch.dot(inputs[0], query))\nThe outputs confirms that the sum of the element-wise multiplication gives\nthe same results as the dot product:\ntensor(0.9544)\ntensor(0.9544)\nBeyond viewing the dot product operation as a mathematical tool that\ncombines two vectors to yield a scalar value, the dot product is a measure of\nsimilarity because it quantifies how much two vectors are aligned: a higher\ndot product indicates a greater degree of alignment or similarity between the\nvectors. In the context of self-attention mechanisms, the dot product\ndetermines the extent to which elements in a sequence attend to each other:\nthe higher the dot product, the higher the similarity and attention score\nbetween two elements.\nIn the next step, as shown in Figure 3.9, we normalize each of the attention\nscores that we computed previously.\nFigure 3.9 After computing the attention scores \nω\n21\n to \nω\n2T\n with respect to the input query x\n(2)\n,\nthe next step is to obtain the attention weights \nα\n21\n to \nα\n2T\n by normalizing the attention scores.\nThe main goal behind the normalization shown in Figure 3.9 is to obtain\nattention weights that sum up to 1. This normalization is a convention that is\nuseful for interpretation and for maintaining training stability in an LLM.\nHere's a straightforward method for achieving this normalization step:\nattn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\nprint(""Attention weights:"", attn_weights_2_tmp)\nprint(""Sum:"", attn_weights_2_tmp.sum())\nAs the output shows, the attention weights now sum to 1:\nAttention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\nSum: tensor(1.0000)\nIn practice, it's more common and advisable to use the softmax function for\nnormalization. This approach is better at managing extreme values and offers\nmore favorable gradient properties during training. Below is a basic\nimplementation of the softmax function for normalizing the attention scores:\ndef softmax_naive(x):\n    return torch.exp(x) / torch.exp(x).sum(dim=0)\n \nattn_weights_2_naive = softmax_naive(attn_scores_2)\nprint(""Attention weights:"", attn_weights_2_naive)\nprint(""Sum:"", attn_weights_2_naive.sum())\nAs the output shows, the softmax function also meets the objective and\nnormalizes the attention weights such that they sum to 1:\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\n \nIn addition, the softmax function ensures that the attention weights are\nalways positive. This makes the output interpretable as probabilities or\nrelative importance, where higher weights indicate greater importance.\nNote that this naive softmax implementation (\nsoftmax_naive\n) may encounter\nnumerical instability problems, such as overflow and underflow, when\ndealing with large or small input values. Therefore, in practice, it's advisable\nto use the PyTorch implementation of softmax, which has been extensively\noptimized for performance:\nattn_weights_2 = torch.softmax(attn_scores_2, dim=0)\nprint(""Attention weights:"", attn_weights_2)\nprint(""Sum:"", attn_weights_2.sum())\nIn this case, we can see that it yields the same results as our previous\nsoftmax_naive\n function:\nAttention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nSum: tensor(1.)\nNow that we computed the normalized attention weights, we are ready for the\nfinal step illustrated in Figure 3.10: calculating the context vector \nz\n(2)\n by\nmultiplying the embedded input tokens, \nx\n(i)\n, with the corresponding attention\nweights and then summing the resulting vectors.\nFigure 3.10 The final step, after calculating and normalizing the attention scores to obtain the\nattention weights for query \nx\n(2)\n, is to compute the context vector \nz\n(2)\n. This context vector is a\ncombination of all input vectors \nx\n(1) \nto x\n(T) \nweighted by the attention weights.\nThe context vector \nz\n(2)\n depicted in Figure 3.10 is calculated as a weighted\nsum of all input vectors. This involves multiplying each input vector by its\ncorresponding attention weight:\nquery = inputs[1] # 2nd input token is the query\ncontext_vec_2 = torch.zeros(query.shape)\nfor i,x_i in enumerate(inputs):\n    context_vec_2 += attn_weights_2[i]*x_i\nprint(context_vec_2)\nThe results of this computation are as follows:\ntensor([0.4419, 0.6515, 0.5683])\nIn the next section, we will generalize this procedure for computing context\nvectors to calculate all context vectors simultaneously.\n3.3.2 Computing attention weights for all input tokens\nIn the previous section, we computed attention weights and the context vector\nfor input 2, as shown in the highlighted row in Figure 3.11. Now, we are\nextending this computation to calculate attention weights and context vectors\nfor all inputs.\nFigure 3.11 The highlighted row shows the attention weights for the second input element as a\nquery, as we computed in the previous section. This section generalizes the computation to obtain\nall other attention weights.\nWe follow the same three steps as before, as summarized in Figure 3.12,\nexcept that we make a few modifications in the code to compute all context\nvectors instead of only the second context vector, \nz\n(2)\n.\nFigure 3.12\nFirst, in step 1 as illustrated in Figure 3.12, we add an additional for-loop to\ncompute the dot products for all pairs of inputs.\nattn_scores = torch.empty(6, 6)\nfor i, x_i in enumerate(inputs):\n    for j, x_j in enumerate(inputs):\n        attn_scores[i, j] = torch.dot(x_i, x_j)\nprint(attn_scores)\nThe resulting attention scores are as follows:\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nEach element in the preceding tensor represents an attention score between\neach pair of inputs, as illustrated in Figure 3.11. Note that the values in\nFigure 3.11 are normalized, which is why they differ from the unnormalized\nattention scores in the preceding tensor. We will take care of the\nnormalization later.\nWhen computing the preceding attention score tensor, we used for-loops in\nPython. However, for-loops are generally slow, and we can achieve the same\nresults using matrix multiplication:\nattn_scores = inputs @ inputs.T\nprint(attn_scores)\nWe can visually confirm that the results are the same as before:\ntensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\nIn step 2, as illustrated in Figure 3.12, we now normalize each row so that the\nvalues in each row sum to 1:\nattn_weights = torch.softmax(attn_scores, dim=1)\nprint(attn_weights)\nThis returns the following attention weight tensor that matches the values\nshown in Figure 3.10:\ntensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\nBefore we move on to step 3, the final step shown in Figure 3.12, let's briefly\nverify that the rows indeed all sum to 1:\nrow_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\nprint(""Row 2 sum:"", row_2_sum)\nprint(""All row sums:"", attn_weights.sum(dim=1))\nThe result is as follows:\nRow 2 sum: 1.0\nAll row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\nIn the third and last step, we now use these attention weights to compute all\ncontext vectors via matrix multiplication:\nall_context_vecs = attn_weights @ inputs\nprint(all_context_vecs)\nIn the resulting output tensor, each row contains a 3-dimensional context\nvector:\ntensor([[0.4421, 0.5931, 0.5790],\n        [0.4419, 0.6515, 0.5683],\n        [0.4431, 0.6496, 0.5671],\n        [0.4304, 0.6298, 0.5510],\n        [0.4671, 0.5910, 0.5266],\n        [0.4177, 0.6503, 0.5645]])\nWe can double-check that the code is correct by comparing the 2nd row with\nthe context vector \nz\n(2)\n that we computed previously in section 3.3.1:\nprint(""Previous 2nd context vector:"", context_vec_2)\nBased on the result, we can see that the previously calculated \ncontext_vec_2\nmatches the second row in the previous tensor exactly:\nPrevious 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\nThis concludes the code walkthrough of a simple self-attention mechanism.\nIn the next section, we will add trainable weights, enabling the LLM to learn\nfrom data and improve its performance on specific tasks.\n3.4 Implementing self-attention with trainable\nweights\nIn this section, we are implementing the self-attention mechanism that is used\nin the original transformer architecture, the GPT models, and most other\npopular LLMs. This self-attention mechanism is also called \nscaled dot-\nproduct attention\n. Figure 3.13 provides a mental model illustrating how this\nself-attention mechanism fits into the broader context of implementing an\nLLM.\nFigure 3.13 A mental model illustrating how the self-attention mechanism we code in this section\nfits into the broader context of this book and chapter. In the previous section, we coded a\nsimplified attention mechanism to understand the basic mechanism behind attention\nmechanisms. In this section, we add trainable weights to this attention mechanism. In the\nupcoming sections, we will then extend this self-attention mechanism by adding a causal mask\nand multiple heads.\nAs illustrated in Figure 3.13 the self-attention mechanism with trainable\nweights builds on the previous concepts: we want to compute context vectors\nas weighted sums over the input vectors specific to a certain input element.\nAs you will see, there are only slight differences compared to the basic self-\nattention mechanism we coded earlier in section 3.3.\nThe most notable difference is the introduction of weight matrices that are\nupdated during model training. These trainable weight matrices are crucial so\nthat the model (specifically, the attention module inside the model) can learn\nto produce ""good"" context vectors. (Note that we will train the LLM in\nchapter 5.)\nWe will tackle this self-attention mechanism in the two subsections. First, we\nwill code it step-by-step as before. Second, we will organize the code into a\ncompact Python class that can be imported into an LLM architecture, which\nwe will code in chapter 4.\n3.4.1 Computing the attention weights step by step\nWe will implement the self-attention mechanism step by step by introducing\nthe three trainable weight matrices \nW\nq\n, \nW\nk\n, and \nW\nv\n. These three matrices are\nused to project the embedded input tokens, \nx\n(i)\n, into query, key, and value\nvectors as illustrated in Figure 3.14.\nFigure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we\ncompute query (\nq\n), key (\nk\n), and value (\nv\n) vectors for input elements \nx\n. Similar to previous\nsections, we designate the second input, \nx\n(2)\n, as the query input. The query vector \nq\n(2) \nis obtained\nvia matrix multiplication between the input \nx\n(2)\n and the weight matrix \nW\nq\n. Similarly, we obtain\nthe key and value vectors via matrix multiplication involving the weight matrices \nW\nk\n and \nW\nv\n.\nEarlier in section 3.3.1, we defined the second input element \nx\n(2)\n as the query\nwhen we computed the simplified attention weights to compute the context\nvector \nz\n(2)\n. Later, in section 3.3.2, we generalized this to compute all context\nvectors \nz\n(1)\n ... z\n(T)\n for the six-word input sentence \n""Your journey starts with\none step.""\nSimilarly, we will start by computing only one context vector, \nz\n(2)\n, for\nillustration purposes. In the next section, we will modify this code to\ncalculate all context vectors.\nLet's begin by defining a few variables:\nx_2 = inputs[1] #A\nd_in = inputs.shape[1] #B\nd_out = 2 #C\nNote that in GPT-like models, the input and output dimensions are usually\nthe same, but for illustration purposes, to better follow the computation, we\nchoose different input (\nd_in=3\n) and output (\nd_out=2\n) dimensions here.\nNext, we initialize the three weight matrices \nW\nq\n, \nW\nk\n, and \nW\nv \nthat are shown\nin Figure 3.14:\ntorch.manual_seed(123)\nW_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nW_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\nNote that we are setting \nrequires_grad=False\n to reduce clutter in the\noutputs for illustration purposes, but if we were to use the weight matrices for\nmodel training, we would set \nrequires_grad=True\n to update these matrices\nduring model training.\nNext, we compute the query, key, and value vectors as shown earlier in\nFigure 3.14:\nquery_2 = x_2 @ W_query \nkey_2 = x_2 @ W_key \nvalue_2 = x_2 @ W_value\nprint(query_2)\nAs we can see based on the output for the query, this results in a 2-\ndimensional vector since we set the number of columns of the corresponding\nweight matrix, via \nd_out\n, to 2:\ntensor([0.4306, 1.4551])\nWeight parameters vs attention weights\nNote that in the weight matrices \nW\n, the term ""weight"" is short for ""weight\nparameters,"" the values of a neural network that are optimized during\ntraining. This is not to be confused with the attention weights. As we already\nsaw in the previous section, attention weights determine the extent to which a\ncontext vector depends on the different parts of the input, i.e., to what extent\nthe network focuses on different parts of the input.\nIn summary, weight parameters are the fundamental, learned coefficients that\ndefine the network's connections, while attention weights are dynamic,\ncontext-specific values.\nEven though our temporary goal is to only compute the one context vector,\nz\n(2)\n, we still require the key and value vectors for all input elements as they\nare involved in computing the attention weights with respect to the query \nq\n(2)\n,\nas illustrated in Figure 3.14.\nWe can obtain all keys and values via matrix multiplication:\nkeys = inputs @ W_key \nvalues = inputs @ W_value\nprint(""keys.shape:"", keys.shape)\nprint(""values.shape:"", values.shape)\nAs we can tell from the outputs, we successfully projected the 6 input tokens\nfrom a 3D onto a 2D embedding space:\nkeys.shape: torch.Size([6, 2])\nvalues.shape: torch.Size([6, 2])\nThe second step is now to compute the attention scores, as shown in Figure\n3.15.\nFigure 3.15 The attention score computation is a dot-product computation similar to what we\nhave used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we\nare not directly computing the dot-product between the input elements but using the query and\nkey obtained by transforming the inputs via the respective weight matrices.\nFirst, let's compute the attention score \nω\n22\n:\nkeys_2 = keys[1] #A\nattn_score_22 = query_2.dot(keys_2)\nprint(attn_score_22)\nThe results in the following unnormalized attention score:\ntensor(1.8524)\nAgain, we can generalize this computation to all attention scores via matrix\nmultiplication:\nattn_scores_2 = query_2 @ keys.T # All attention scores for given query\nprint(attn_scores_2)\nAs we can see, as a quick check, the second element in the output matches\nattn_score_22\n we computed previously:\ntensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\nThe third step is now going from the attention scores to the attention weights,\nas illustrated in Figure 3.16.\nFigure 3.16 After computing the attention scores \nω\n, the next step is to normalize these scores\nusing the softmax function to obtain the attention weights \nα\n.\nNext, as illustrated in Figure 3.16, we compute the attention weights by\nscaling the attention scores and using the softmax function we used earlier..\nThe difference to earlier is that we now scale the attention scores by dividing\nthem by the square root of the embedding dimension of the keys, (note that\ntaking the square root is mathematically the same as exponentiating by 0.5):\nd_k = keys.shape[-1]\nattn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\nprint(attn_weights_2)\nThe resulting attention weights are as follows:\ntensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\nThe rationale behind scaled-dot product attention\nThe reason for the normalization by the embedding dimension size is to\nimprove the training performance by avoiding small gradients. For instance,\nwhen scaling up the embedding dimension, which is typically greater than\nthousand for GPT-like LLMs, large dot products can result in very small\ngradients during backpropagation due to the softmax function applied to\nthem. As dot products increase, the softmax function behaves more like a\nstep function, resulting in gradients nearing zero. These small gradients can\ndrastically slow down learning or cause training to stagnate.\nThe scaling by the square root of the embedding dimension is the reason why\nthis self-attention mechanism is also called scaled-dot product attention.\nNow, the final step is to compute the context vectors, as illustrated in Figure\n3.17.\nFigure 3.17 In the final step of the self-attention computation, we compute the context vector by\ncombining all value vectors via the attention weights.\nSimilar to section 3.3, where we computed the context vector as a weighted\nsum over the input vectors, we now compute the context vector as a weighted\nsum over the value vectors. Here, the attention weights serve as a weighting\nfactor that weighs the respective importance of each value vector. Similar to\nsection 3.3, we can use matrix multiplication to obtain the output in one step:\ncontext_vec_2 = attn_weights_2 @ values\nprint(context_vec_2)\nThe contents of the resulting vector are as follows:\ntensor([0.3061, 0.8210])\nSo far, we only computed a single context vector, \nz\n(2)\n. In the next section, we\nwill generalize the code to compute all context vectors in the input sequence,\nz\n(1)\n to \nz\n(T)\n.\nWhy query, key, and value?\nThe terms ""key,"" ""query,"" and ""value"" in the context of attention mechanisms\nare borrowed from the domain of information retrieval and databases, where\nsimilar concepts are used to store, search, and retrieve information.\nA ""query"" is analogous to a search query in a database. It represents the\ncurrent item (e.g., a word or token in a sentence) the model focuses on or\ntries to understand. The query is used to probe the other parts of the input\nsequence to determine how much attention to pay to them.\nThe ""key"" is like a database key used for indexing and searching. In the\nattention mechanism, each item in the input sequence (e.g., each word in a\nsentence) has an associated key. These keys are used to match with the query.\nThe ""value"" in this context is similar to the value in a key-value pair in a\ndatabase. It represents the actual content or representation of the input items.\nOnce the model determines which keys (and thus which parts of the input)\nare most relevant to the query (the current focus item), it retrieves the\ncorresponding values.\n3.4.2 Implementing a compact self-attention Python class\nIn the previous sections, we have gone through a lot of steps to compute the\nself-attention outputs. This was mainly done for illustration purposes so we\ncould go through one step at a time. In practice, with the LLM\nimplementation in the next chapter in mind, it is helpful to organize this code\ninto a Python class as follows:\nListing 3.1 A compact self-attention class\nimport torch.nn as nn\nclass SelfAttention_v1(nn.Module):\n    def __init__(self, d_in, d_out):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n \n    def forward(self, x):\n        keys = x @ self.W_key\n        queries = x @ self.W_query\n        values = x @ self.W_value\n        attn_scores = queries @ keys.T # omega\n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\n        context_vec = attn_weights @ values\n        return context_vec\nIn this PyTorch code, \nSelfAttention_v1\n is a class derived from \nnn.Module\n,\nwhich is a fundamental building block of PyTorch models, which provides\nnecessary functionalities for model layer creation and management.\nThe \n__init__\n method initializes trainable weight matrices (\nW_query\n, \nW_key\n,\nand \nW_value\n) for queries, keys, and values, each transforming the input\ndimension \nd_in\n to an output dimension \nd_out\n.\nDuring the forward pass, using the forward method, we compute the attention\nscores (\nattn_scores\n) by multiplying queries and keys, normalizing these\nscores using softmax. Finally, we create a context vector by weighting the\nvalues with these normalized attention scores.\nWe can use this class as follows:\ntorch.manual_seed(123)\nsa_v1 = SelfAttention_v1(d_in, d_out)\nprint(sa_v1(inputs))\nSince \ninputs\n contains six embedding vectors, this result in a matrix storing\nthe six context vectors:\ntensor([[0.2996, 0.8053],\n        [0.3061, 0.8210],\n        [0.3058, 0.8203],\n        [0.2948, 0.7939],\n        [0.2927, 0.7891],\n        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\nAs a quick check, notice how the second row (\n[0.3061, 0.8210]\n) matches\nthe contents of \ncontext_vec_2\n in the previous section.\nFigure 3.18 summarizes the self-attention mechanism we just implemented.\nFigure 3.18 In self-attention, we transform the input vectors in the input matrix X with the three\nweight matrices, Wq, Wk, and Wv. Then, we compute the attention weight matrix based on the\nresulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute\nthe context vectors (Z). (For visual clarity, we focus on a single input text with \nn \ntokens in this\nfigure, not a batch of multiple inputs. Consequently, the 3D input tensor is simplified to a 2D\nmatrix in this context. This approach allows for a more straightforward visualization and\nunderstanding of the processes involved.)\nAs shown in Figure 3.18, self-attention involves the trainable weight matrices\nW\nq\n, W\nk\n,\n and \nW\nv\n. These matrices transform input data into queries, keys, and\nvalues, which are crucial components of the attention mechanism. As the\nmodel is exposed to more data during training, it adjusts these trainable\nweights, as we will see in upcoming chapters.\nWe can improve the \nSelfAttention_v1\n implementation further by utilizing\nPyTorch's \nnn.Linear\n layers, which effectively perform matrix multiplication\nwhen the bias units are disabled. Additionally, a significant advantage of\nusing \nnn.Linear\n instead of manually implementing\nnn.Parameter(torch.rand(...))\n is that \nnn.Linear\n has an optimized weight\ninitialization scheme, contributing to more stable and effective model\ntraining.\nListing 3.2 A self-attention class using PyTorch's Linear layers\nclass SelfAttention_v2(nn.Module):\n    def __init__(self, d_in, d_out, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n \n    def forward(self, x):\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        attn_scores = queries @ keys.T\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n        context_vec = attn_weights @ values\n        return context_vec\nYou can use the \nSelfAttention_v2\n similar to \nSelfAttention_v1:\ntorch.manual_seed(789)\nsa_v2 = SelfAttention_v2(d_in, d_out)\nprint(sa_v2(inputs))\nThe output is:\ntensor([[-0.0739,  0.0713],\n        [-0.0748,  0.0703],\n        [-0.0749,  0.0702],\n        [-0.0760,  0.0685],\n        [-0.0763,  0.0679],\n        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\nNote that \nSelfAttention_v1\n and \nSelfAttention_v2\n give different outputs\nbecause they use different initial weights for the weight matrices since\nnn.Linear\n uses a more sophisticated weight initialization scheme.\nExercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2\nNote that \nnn.Linear\n in \nSelfAttention_v2\n uses a different weight\ninitialization scheme as \nnn.Parameter(torch.rand(d_in, d_out)\n) used in\nSelfAttention_v1\n, which causes both mechanisms to produce different\nresults. To check that both implementations, \nSelfAttention_v1\n and\nSelfAttention_v2\n, are otherwise similar, we can transfer the weight\nmatrices from a \nSelfAttention_v2\n object to a \nSelfAttention_v1\n, such that\nboth objects then produce the same results.\nYour task is to correctly assign the weights from an instance of\nSelfAttention_v2\n to an instance of \nSelfAttention_v1\n. To do this, you need\nto understand the relationship between the weights in both versions. (Hint:\nnn.Linear\n stores the weight matrix in a transposed form.) After the\nassignment, you should observe that both instances produce the same outputs.\nIn the next section, we will make enhancements to the self-attention\nmechanism, focusing specifically on incorporating causal and multi-head\nelements. The causal aspect involves modifying the attention mechanism to\nprevent the model from accessing future information in the sequence, which\nis crucial for tasks like language modeling, where each word prediction\nshould only depend on previous words.\nThe multi-head component involves splitting the attention mechanism into\nmultiple ""heads."" Each head learns different aspects of the data, allowing the\nmodel to simultaneously attend to information from different representation\nsubspaces at different positions. This improves the model's performance in\ncomplex tasks.\n3.5 Hiding future words with causal attention\nIn this section, we modify the standard self-attention mechanism to create a\ncausal attention\n mechanism, which is essential for developing an LLM in the\nsubsequent chapters.\nCausal attention, also known as \nmasked attention\n, is a specialized form of\nself-attention. It restricts a model to only consider previous and current inputs\nin a sequence when processing any given token. This is in contrast to the\nstandard self-attention mechanism, which allows access to the entire input\nsequence at once.\nConsequently, when computing attention scores, the causal attention\nmechanism ensures that the model only factors in tokens that occur at or\nbefore the current token in the sequence.\nTo achieve this in GPT-like LLMs, for each token processed, we mask out\nthe future tokens, which come after the current token in the input text, as\nillustrated in Figure 3.19.\nFigure 3.19 In causal attention, we mask out the attention weights above the diagonal such that\nfor a given input, the LLM can't access future tokens when computing the context vectors using\nthe attention weights. For example, for the word ""journey"" in the second row, we only keep the\nattention weights for the words before (""Your"") and in the current position (""journey"").\nAs illustrated in Figure 3.19, we mask out the attention weights above the\ndiagonal, and we normalize the non-masked attention weights, such that the\nattention weights sum to 1 in each row. In the next section, we will\nimplement this masking and normalization procedure in code.\n3.5.1 Applying a causal attention mask\nIn this section, we implement the causal attention mask in code. We start with\nthe procedure summarized in Figure 3.20.\nFigure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply\nthe softmax function to the attention scores, zeroing out the elements above the diagonal and\nnormalizing the resulting matrix.\n\nTo implement the steps to apply a causal attention mask to obtain the masked\nattention weights as summarized in Figure 3.20, let's work with the attention\nscores and weights from the previous section to code the causal attention\nmechanism.\nIn the first step illustrated in Figure 3.20, we compute the attention weights\nusing the softmax function as we have done in previous sections:\nqueries = sa_v2.W_query(inputs)  #A\nkeys = sa_v2.W_key(inputs) \nattn_scores = queries @ keys.T\nattn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\nprint(attn_weights)\nThis results in the following attention weights:\ntensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\nWe can implement step 2 in Figure 3.20 using PyTorch's \ntril\n function to\ncreate a mask where the values above the diagonal are zero:\ncontext_length = attn_scores.shape[0]\nmask_simple = torch.tril(torch.ones(context_length, context_length))\nprint(mask_simple)\nThe resulting mask is as follows:\ntensor([[1., 0., 0., 0., 0., 0.],\n        [1., 1., 0., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0.],\n        [1., 1., 1., 1., 1., 1.]])\nNow, we can multiply this mask with the attention weights to zero out the\nvalues above the diagonal:\nmasked_simple = attn_weights*mask_simple\nprint(masked_simple)\nAs we can see, the elements above the diagonal are successfully zeroed out:\ntensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<MulBackward0>)\n \nThe third step in Figure 3.20 is to renormalize the attention weights to sum up\nto 1 again in each row. We can achieve this by dividing each element in each\nrow by the sum in each row:\nrow_sums = masked_simple.sum(dim=1, keepdim=True)\nmasked_simple_norm = masked_simple / row_sums\nprint(masked_simple_norm)\nThe result is an attention weight matrix where the attention weights above the\ndiagonal are zeroed out and where the rows sum to 1:\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<DivBackward0>)\nInformation leakage\nWhen we apply a mask and then renormalize the attention weights, it might\ninitially appear that information from future tokens (which we intend to\nmask) could still influence the current token because their values are part of\nthe softmax calculation. However, the key insight is that when we\nrenormalize the attention weights after masking, what we're essentially doing\nis recalculating the softmax over a smaller subset (since masked positions\ndon't contribute to the softmax value).\nThe mathematical elegance of softmax is that despite initially including all\npositions in the denominator, after masking and renormalizing, the effect of\nthe masked positions is nullified — they don't contribute to the softmax score\nin any meaningful way.\nIn simpler terms, after masking and renormalization, the distribution of\nattention weights is as if it was calculated only among the unmasked\npositions to begin with. This ensures there's no information leakage from\nfuture (or otherwise masked) tokens as we intended.\nWhile we could be technically done with implementing causal attention at\nthis point, we can take advantage of a mathematical property of the softmax\nfunction and implement the computation of the masked attention weights\nmore efficiently in fewer steps, as shown in Figure 3.21.\nFigure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention\nis to mask the attention scores with negative infinity values before applying the softmax function.\nThe softmax function converts its inputs into a probability distribution. When\nnegative infinity values (-∞) are present in a row, the softmax function treats\nthem as zero probability. (Mathematically, this is because \ne\n-\n∞\n approaches 0.)\nWe can implement this more efficient masking ""trick"" by creating a mask\nwith 1's above the diagonal and then replacing these 1's with negative infinity\n(\n-inf\n) values:\nmask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\nmasked = attn_scores.masked_fill(mask.bool(), -torch.inf)\nprint(masked)\nThis results in the following mask:\ntensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n       grad_fn=<MaskedFillBackward0>)\n \nNow, all we need to do is apply the softmax function to these masked results,\nand we are done:\nattn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\nprint(attn_weights)\nAs we can see based on the output, the values in each row sum to 1, and no\nfurther normalization is necessary:\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n       grad_fn=<SoftmaxBackward0>)\nWe could now use the modified attention weights to compute the context\nvectors via \ncontext_vec = attn_weights @ values\n, as in section 3.4.\nHowever, in the next section, we first cover another minor tweak to the\ncausal attention mechanism that is useful for reducing overfitting when\ntraining LLMs.\n3.5.2 Masking additional attention weights with dropout\nDropout\n in deep learning is a technique where randomly selected hidden\nlayer units are ignored during training, effectively ""dropping"" them out. This\nmethod helps prevent overfitting by ensuring that a model does not become\noverly reliant on any specific set of hidden layer units. It's important to\nemphasize that dropout is only used during training and is disabled afterward.\nIn the transformer architecture, including models like GPT, dropout in the\nattention mechanism is typically applied in two specific areas: after\ncalculating the attention scores or after applying the attention weights to the\nvalue vectors.\nHere, we will apply the dropout mask after computing the attention weights,\nas illustrated in Figure 3.22, because it's the more common variant in\npractice.\nFigure 3.22 Using the causal attention mask (upper left), we apply an additional dropout mask\n(upper right) to zero out additional attention weights to reduce overfitting during training.\nIn the following code example, we use a dropout rate of 50%, which means\nmasking out half of the attention weights. (When we train the GPT model in\nlater chapters, we will use a lower dropout rate, such as 0.1 or 0.2.)\nIn the following code, we apply PyTorch's dropout implementation first to a\n6×6 tensor consisting of ones for illustration purposes:\ntorch.manual_seed(123)\ndropout = torch.nn.Dropout(0.5) #A\nexample = torch.ones(6, 6) #B\nprint(dropout(example))\nAs we can see, approximately half of the values are zeroed out:\ntensor([[2., 2., 0., 2., 2., 0.],\n        [0., 0., 0., 2., 0., 2.],\n        [2., 2., 2., 2., 0., 2.],\n        [0., 2., 2., 0., 0., 2.],\n        [0., 2., 0., 2., 0., 2.],\n        [0., 2., 2., 2., 2., 0.]])\nWhen applying dropout to an attention weight matrix with a rate of 50%, half\nof the elements in the matrix are randomly set to zero. To compensate for the\nreduction in active elements, the values of the remaining elements in the\nmatrix are scaled up by a factor of 1/0.5 =2. This scaling is crucial to\nmaintain the overall balance of the attention weights, ensuring that the\naverage influence of the attention mechanism remains consistent during both\nthe training and inference phases.\nNow, let's apply dropout to the attention weight matrix itself:\ntorch.manual_seed(123)\nprint(dropout(attn_weights))\nThe resulting attention weight matrix now has additional elements zeroed out\nand the remaining ones rescaled:\ntensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n       grad_fn=<MulBackward0>\nNote that the resulting dropout outputs may look different depending on your\noperating system; you can read more about this inconsistency [here on the\nPyTorch issue tracker at \nhttps://github.com/pytorch/pytorch/issues/121595\n.\nHaving gained an understanding of causal attention and dropout masking, we\nwill develop a concise Python class in the following section. This class is\ndesigned to facilitate the efficient application of these two techniques.\n3.5.3 Implementing a compact causal attention class\nIn this section, we will now incorporate the causal attention and dropout\nmodifications into the \nSelfAttention\n Python class we developed in section\n3.4. This class will then serve as a template for developing \nmulti-head\nattention\n in the upcoming section, which is the final attention class we\nimplement in this chapter.\nBut before we begin, one more thing is to ensure that the code can handle\nbatches consisting of more than one input so that the \nCausalAttention\n class\nsupports the batch outputs produced by the data loader we implemented in\nchapter 2.\nFor simplicity, to simulate such batch inputs, we duplicate the input text\nexample:\nbatch = torch.stack((inputs, inputs), dim=0)\nprint(batch.shape) #A \nThis results in a 3D tensor consisting of 2 input texts with 6 tokens each,\nwhere each token is a 3-dimensional embedding vector:\ntorch.Size([2, 6, 3])\nThe following \nCausalAttention\n class is similar to the \nSelfAttention\n class\nwe implemented earlier, except that we now added the dropout and causal\nmask components as highlighted in the following code:\nListing 3.3 A compact causal attention class\nclass CausalAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n        super().__init__()\n        self.d_out = d_out\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.dropout = nn.Dropout(dropout)  #A\n        self.register_buffer(\n           'mask',\n           torch.triu(torch.ones(context_length, context_length),\n           diagonal=1)\n        )  #B\n \n    def forward(self, x):\n        b, num_tokens, d_in = x.shape  #C \nNew batch dimension b\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n \n        attn_scores = queries @ keys.transpose(1, 2)  #C\n        attn_scores.masked_fill_(  #D\n            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n \n        context_vec = attn_weights @ values\n        return context_vec\nWhile all added code lines should be familiar from previous sections, we now\nadded a \nself.register_buffer()\n call in the \n__init__\n method. The use of\nregister_buffer\n in PyTorch is not strictly necessary for all use cases but\noffers several advantages here. For instance, when we use the\nCausalAttention\n class in our LLM, buffers are automatically moved to the\nappropriate device (CPU or GPU) along with our model, which will be\nrelevant when training the LLM in future chapters. This means we don't need\nto manually ensure these tensors are on the same device as your model\nparameters, avoiding device mismatch errors.\nWe can use the \nCausalAttention\n class as follows, similar to \nSelfAttention\npreviously:\ntorch.manual_seed(123)\ncontext_length = batch.shape[1]\nca = CausalAttention(d_in, d_out, context_length, 0.0)\ncontext_vecs = ca(batch)\nprint(""context_vecs.shape:"", context_vecs.shape)\nThe resulting context vector is a 3D tensor where each token is now\nrepresented by a 2D embedding:\ncontext_vecs.shape: torch.Size([2, 6, 2])\nFigure 3.23 provides a mental model that summarizes what we have\naccomplished so far.\nFigure 3.23 A mental model summarizing the four different attention modules we are coding in\nthis chapter. We began with a simplified attention mechanism, added trainable weights, and then\nadded a casual attention mask. In the remainder of this chapter, we will extend the causal\nattention mechanism and code multi-head attention, which is the final module we will use in the\nLLM implementation in the next chapter.\nAs illustrated in Figure 3.23, in this section, we focused on the concept and\nimplementation of causal attention in neural networks. In the next section, we\nwill expand on this concept and implement a multi-head attention module\nthat implements several of such causal attention mechanisms in parallel.\n3.6 Extending single-head attention to multi-head\nattention\nIn this final section of this chapter, we are extending the previously\nimplemented causal attention class over multiple-heads. This is also called\nmulti-head attention\n.\nThe term ""multi-head"" refers to dividing the attention mechanism into\nmultiple ""heads,"" each operating independently. In this context, a single\ncausal attention module can be considered single-head attention, where there\nis only one set of attention weights processing the input sequentially.\nIn the following subsections, we will tackle this expansion from causal\nattention to multi-head attention. The first subsection will intuitively build a\nmulti-head attention module by stacking multiple \nCausalAttention\n modules\nfor illustration purposes. The second subsection will then implement the same\nmulti-head attention module in a more complicated but computationally more\nefficient way.\n3.6.1 Stacking multiple single-head attention layers\nIn practical terms, implementing multi-head attention involves creating\nmultiple instances of the self-attention mechanism (depicted earlier in Figure\n3.18 in section 3.4.1), each with its own weights, and then combining their\noutputs. Using multiple instances of the self-attention mechanism can be\ncomputationally intensive, but it's crucial for the kind of complex pattern\nrecognition that models like transformer-based LLMs are known for.\nFigure 3.24 illustrates the structure of a multi-head attention module, which\nconsists of multiple single-head attention modules, as previously depicted in\nFigure 3.18, stacked on top of each other.\nFigure 3.24 The multi-head attention module in this figure depicts two single-head attention\nmodules stacked on top of each other. So, instead of using a single matrix \nW\nv\n for computing the\nvalue matrices, in a multi-head attention module with two heads, we now have two value weight\nmatrices: \nW\nv1\n and \nW\nv2\n. The same applies to the other weight matrices, \nW\nq\n and \nW\nk\n. We obtain\ntwo sets of context vectors \nZ\n1\n and \nZ\n2\n that we can combine into a single context vector matrix \nZ\n.\nAs mentioned before, the main idea behind multi-head attention is to run the\nattention mechanism multiple times (in parallel) with different, learned linear\nprojections -- the results of multiplying the input data (like the query, key,\nand value vectors in attention mechanisms) by a weight matrix.\nIn code, we can achieve this by implementing a simple\nMultiHeadAttentionWrapper\n class that stacks multiple instances of our\npreviously implemented \nCausalAttention\n module:\nListing 3.4 A wrapper class to implement multi-head attention\nclass MultiHeadAttentionWrapper(nn.Module):\n    def __init__(self, d_in, d_out, context_length,\n                 dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n             for _ in range(num_heads)]\n        )\n \n    def forward(self, x):\n        return torch.cat([head(x) for head in self.heads], dim=-1)\nFor example, if we use this MultiHeadAttentionWrapper class with two\nattention heads (via \nnum_heads=2\n) and CausalAttention output dimension\nd_out=2\n, this results in a 4-dimensional context vectors\n(\nd_out*num_heads=4\n), as illustrated in Figure 3.25.\nFigure 3.25 Using the \nMultiHeadAttentionWrapper\n, we specified the number of attention heads\n(\nnum_heads\n). If we set \nnum_heads=2\n, as shown in this figure, we obtain a tensor with two sets of\ncontext vector matrices. In each context vector matrix, the rows represent the context vectors\ncorresponding to the tokens, and the columns correspond to the embedding dimension specified\nvia \nd_out=4\n. We concatenate these context vector matrices along the column dimension. Since we\nhave 2 attention heads and an embedding dimension of 2, the final embedding dimension is 2 × 2\n= 4.\nTo illustrate Figure 3.25 further with a concrete example, we can use the\nMultiHeadAttentionWrapper\n class similar to the \nCausalAttention\n class\nbefore:\ntorch.manual_seed(123)\ncontext_length = batch.shape[1] # This is the number of tokens\nd_in, d_out = 3, 2\nmha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\ncontext_vecs = mha(batch)\n \nprint(context_vecs)\nprint(""context_vecs.shape:"", context_vecs.shape)\nThis results in the following tensor representing the context vectors:\ntensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]],\n \n        [[-0.4519,  0.2216,  0.4772,  0.1063],\n         [-0.5874,  0.0058,  0.5891,  0.3257],\n         [-0.6300, -0.0632,  0.6202,  0.3860],\n         [-0.5675, -0.0843,  0.5478,  0.3589],\n         [-0.5526, -0.0981,  0.5321,  0.3428],\n         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 4])\nThe first dimension of the resulting \ncontext_vecs\n tensor is 2 since we have\ntwo input texts (the input texts are duplicated, which is why the context\nvectors are exactly the same for those). The second dimension refers to the 6\ntokens in each input. The third dimension refers to the 4-dimensional\nembedding of each token.\nExercise 3.2 Returning 2-dimensional embedding vectors\nChange the input arguments for the \nMultiHeadAttentionWrapper(...,\nnum_heads=2)\n call such that the output context vectors are 2-dimensional\ninstead of 4-dimensional while keeping the setting \nnum_heads=2\n. Hint: You\ndon't have to modify the class implementation; you just have to change one of\nthe other input arguments.\nIn this section, we implemented a MultiHeadAttentionWrapper that\ncombined multiple single-head attention modules. However, note that these\nare processed sequentially via \n[head(x) for head in self.heads]\n in the\nforward method. We can improve this implementation by processing the\nheads in parallel. One way to achieve this is by computing the outputs for all\nattention heads simultaneously via matrix multiplication, as we will explore\nin the next section.\n3.6.2 Implementing multi-head attention with weight splits\nIn the previous section, we created a \nMultiHeadAttentionWrapper\n to\nimplement multi-head attention by stacking multiple single-head attention\nmodules. This was done by instantiating and combining several\nCausalAttention\n objects.\nInstead of maintaining two separate classes, \nMultiHeadAttentionWrapper\nand \nCausalAttention\n, we can combine both of these concepts into a single\nMultiHeadAttention\n class. Also, in addition to just merging the\nMultiHeadAttentionWrapper\n with the \nCausalAttention\n code, we will make\nsome other modifications to implement multi-head attention more efficiently.\nIn the \nMultiHeadAttentionWrapper\n, multiple heads are implemented by\ncreating a list of \nCausalAttention\n objects (\nself.heads\n), each representing a\nseparate attention head. The \nCausalAttention\n class independently performs\nthe attention mechanism, and the results from each head are concatenated. In\ncontrast, the following \nMultiHeadAttention\n class integrates the multi-head\nfunctionality within a single class. It splits the input into multiple heads by\nreshaping the projected query, key, and value tensors and then combines the\nresults from these heads after computing attention.\nLet's take a look at the \nMultiHeadAttention\n class before we discuss it\nfurther:\nListing 3.5 An efficient multi-head attention class\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, \n                 context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, ""d_out must be divisible by num_heads""\n \n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads #A\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out) #B\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            'mask',\n             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n        )\n \n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n        keys = self.W_key(x) #C\n        queries = self.W_query(x) #C\n        values = self.W_value(x) #C\n \n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)#D\n \n        keys = keys.transpose(1, 2) #E\n        queries = queries.transpose(1, 2) #E\n        values = values.transpose(1, 2) #E\n \n        attn_scores = queries @ keys.transpose(2, 3)  #F \n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n  \n        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n \n        attn_weights = torch.softmax(\n            attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n \n        context_vec = (attn_weights @ values).transpose(1, 2) #I\n        #J\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec) #K\n        return context_vec\nEven though the reshaping (\n.view\n) and transposing (\n.transpose\n) of tensors\ninside the \nMultiHeadAttention\n class looks very complicated,\nmathematically, the \nMultiHeadAttention\n class implements the same concept\nas the \nMultiHeadAttentionWrapper\n earlier.\nOn a big-picture level, in the previous \nMultiHeadAttentionWrapper\n, we\nstacked multiple single-head attention layers that we combined into a multi-\nhead attention layer. The \nMultiHeadAttention\n class takes an integrated\napproach. It starts with a multi-head layer and then internally splits this layer\ninto individual attention heads, as illustrated in Figure 3.26.\nFigure 3.26 In the \nMultiheadAttentionWrapper\n class with two attention heads, we initialized two\nweight matrices \nW\nq1 \nand \nW\nq2 \nand computed two query matrices \nQ\n1\n and \nQ\n2\n as illustrated at the\ntop of this figure. In the \nMultiheadAttention\n class, we initialize one larger weight matrix \nW\nq \n,\nonly perform one matrix multiplication with the inputs to obtain a query matrix \nQ\n, and then split\nthe query matrix into \nQ\n1\n and \nQ\n2 \nas shown at the bottom of this figure. We do the same for the\nkeys and values, which are not shown to reduce visual clutter.\nThe splitting of the query, key, and value tensors, as depicted in Figure 3.26,\nis achieved through tensor reshaping and transposing operations using\nPyTorch's \n.view\n and \n.transpose\n methods. The input is first transformed (via\nlinear layers for queries, keys, and values) and then reshaped to represent\nmultiple heads.\nThe key operation is to split the \nd_out\n dimension into \nnum_heads\n and\nhead_dim\n, where \nhead_dim = d_out / num_heads\n. This splitting is then\nachieved using the \n.view\n method: a tensor of dimensions \n(b, num_tokens,\nd_out)\n is reshaped to dimension \n(b, num_tokens, num_heads, head_dim)\n.\nThe tensors are then transposed to bring the \nnum_heads\n dimension before the\nnum_tokens\n dimension, resulting in a shape of \n(b, num_heads,\nnum_tokens, head_dim)\n. This transposition is crucial for correctly aligning\nthe queries, keys, and values across the different heads and performing\nbatched matrix multiplications efficiently.\nTo illustrate this batched matrix multiplication, suppose we have the\nfollowing example tensor:\na = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], #A\n                    [0.8993, 0.0390, 0.9268, 0.7388],\n                    [0.7179, 0.7058, 0.9156, 0.4340]],\n \n                   [[0.0772, 0.3565, 0.1479, 0.5331],\n                    [0.4066, 0.2318, 0.4545, 0.9737],\n                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\nNow, we perform a batched matrix multiplication between the tensor itself\nand a view of the tensor where we transposed the last two dimensions,\nnum_tokens\n and \nhead_dim\n:\nprint(a @ a.transpose(2, 3))\nThe result is as follows:\ntensor([[[[1.3208, 1.1631, 1.2879],\n          [1.1631, 2.2150, 1.8424],\n          [1.2879, 1.8424, 2.0402]],\n \n         [[0.4391, 0.7003, 0.5903],\n          [0.7003, 1.3737, 1.0620],\n          [0.5903, 1.0620, 0.9912]]]])\nIn this case, the matrix multiplication implementation in PyTorch handles the\n4-dimensional input tensor so that the matrix multiplication is carried out\nbetween the 2 last dimensions \n(num_tokens, head_dim)\n and then repeated\nfor the individual heads.\nFor instance, the above becomes a more compact way to compute the matrix\nmultiplication for each head separately:\nfirst_head = a[0, 0, :, :]\nfirst_res = first_head @ first_head.T\nprint(""First head:\n"", first_res)\n \nsecond_head = a[0, 1, :, :]\nsecond_res = second_head @ second_head.T\nprint(""\nSecond head:\n"", second_res)\nThe results are exactly the same results that we obtained when using the\nbatched matrix multiplication \nprint(a @ a.transpose(2, 3))\n earlier:\nFirst head:\n tensor([[1.3208, 1.1631, 1.2879],\n        [1.1631, 2.2150, 1.8424],\n        [1.2879, 1.8424, 2.0402]])\n \nSecond head:\n tensor([[0.4391, 0.7003, 0.5903],\n        [0.7003, 1.3737, 1.0620],\n        [0.5903, 1.0620, 0.9912]])\nContinuing with MultiHeadAttention, after computing the attention weights\nand context vectors, the context vectors from all heads are transposed back to\nthe shape \n(b, num_tokens, num_heads, head_dim)\n. These vectors are then\nreshaped (flattened) into the shape \n(b, num_tokens, d_out)\n, effectively\ncombining the outputs from all heads.\nAdditionally, we added a so-called output projection layer (\nself.out_proj\n)\nto \nMultiHeadAttention\n after combining the heads, which is not present in\nthe \nCausalAttention\n class. This output projection layer is not strictly\nnecessary (see the References section in Appendix B for more details), but it\nis commonly used in many LLM architectures, which is why we added it here\nfor completeness.\nEven though the \nMultiHeadAttention\n class looks more complicated than the\nMultiHeadAttentionWrapper\n due to the additional reshaping and\ntransposition of tensors, it is more efficient. The reason is that we only need\none matrix multiplication to compute the keys, for instance, \nkeys =\nself.W_key(x)\n (the same is true for the queries and values). In the\nMultiHeadAttentionWrapper, we needed to repeat this matrix multiplication,\nwhich is computationally one of the most expensive steps, for each attention\nhead.\nThe \nMultiHeadAttention\n class can be used similar to the \nSelfAttention\nand \nCausalAttention\n classes we implemented earlier:\ntorch.manual_seed(123)\nbatch_size, context_length, d_in = batch.shape\nd_out = 2\nmha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\ncontext_vecs = mha(batch)\nprint(context_vecs)\nprint(""context_vecs.shape:"", context_vecs.shape)\nAs we can see based on the results, the output dimension is directly\ncontrolled by the \nd_out\n argument:\ntensor([[[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]],\n \n        [[0.3190, 0.4858],\n         [0.2943, 0.3897],\n         [0.2856, 0.3593],\n         [0.2693, 0.3873],\n         [0.2639, 0.3928],\n         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\ncontext_vecs.shape: torch.Size([2, 6, 2])\nIn this section, we implemented the \nMultiHeadAttention\n class that we will\nuse in the upcoming sections when implementing and training the LLM itself.\nNote that while the code is fully functional, we used relatively small\nembedding sizes and numbers of attention heads to keep the outputs readable.\nFor comparison, the smallest GPT-2 model (117 million parameters) has 12\nattention heads and a context vector embedding size of 768. The largest GPT-\n2 model (1.5 billion parameters) has 25 attention heads and a context vector\nembedding size of 1600. Note that the embedding sizes of the token inputs\nand context embeddings are the same in GPT models (\nd_in = d_out\n).\nExercise 3.3 Initializing GPT-2 size attention modules\nUsing the \nMultiHeadAttention\n class, initialize a multi-head attention\nmodule that has the same number of attention heads as the smallest GPT-2\nmodel (12 attention heads). Also ensure that you use the respective input and\noutput embedding sizes similar to GPT-2 (768 dimensions). Note that the\nsmallest GPT-2 model supports a context length of 1024 tokens.\n3.7 Summary\nAttention mechanisms transform input elements into enhanced context\nvector representations that incorporate information about all inputs.\nA self-attention mechanism computes the context vector representation\nas a weighted sum over the inputs.\nIn a simplified attention mechanism, the attention weights are computed\nvia dot products.\nA dot product is just a concise way of multiplying two vectors element-\nwise and then summing the products.\nMatrix multiplications, while not strictly required, help us to implement\ncomputations more efficiently and compactly by replacing nested for-\nloops.\nIn self-attention mechanisms that are used in LLMs, also called scaled-\ndot product attention, we include trainable weight matrices to compute\nintermediate transformations of the inputs: queries, values, and keys.\nWhen working with LLMs that read and generate text from left to right,\nwe add a causal attention mask to prevent the LLM from accessing\nfuture tokens.\nNext to causal attention masks to zero out attention weights, we can also\nadd a dropout mask to reduce overfitting in LLMs.\nThe attention modules in transformer-based LLMs involve multiple\ninstances of causal attention, which is called multi-head attention.\nWe can create a multi-head attention module by stacking multiple\ninstances of causal attention modules.\nA more efficient way of creating multi-head attention modules involves\nbatched matrix multiplications.",76325
05-4_Implementing_a_GPT_model_from_Scratch_To_Generate_Text.pdf,05-4_Implementing_a_GPT_model_from_Scratch_To_Generate_Text,"4 Implementing a GPT model from\nScratch To Generate Text\nThis chapter covers\nCoding a GPT-like large language model (LLM) that can be trained to\ngenerate human-like text\nNormalizing layer activations to stabilize neural network training\nAdding shortcut connections in deep neural networks to train models\nmore effectively\nImplementing transformer blocks to create GPT models of various sizes\nComputing the number of parameters and storage requirements of GPT\nmodels\nIn the previous chapter, you learned and coded the \nmulti-head attention\nmechanism, one of the core components of LLMs. In this chapter, we will\nnow code the other building blocks of an LLM and assemble them into a\nGPT-like model that we will train in the next chapter to generate human-like\ntext, as illustrated in Figure 4.1.\nFigure 4.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a\ngeneral text dataset, and finetuning it on a labeled dataset. This chapter focuses on implementing\nthe LLM architecture, which we will train in the next chapter.\nThe LLM architecture, referenced in Figure 4.1, consists of several building\nblocks that we will implement throughout this chapter. We will begin with a\ntop-down view of the model architecture in the next section before covering\nthe individual components in more detail.\n4.1 Coding an LLM architecture\nLLMs, such as GPT (which stands for \nGenerative Pretrained Transformer\n),\nare large deep neural network architectures designed to generate new text one\nword (or token) at a time. However, despite their size, the model architecture\nis less complicated than you might think, since many of its components are\nrepeated, as we will see later. Figure 4.2 provides a top-down view of a GPT-\nlike LLM, with its main components highlighted.\nFigure 4.2 A mental model of a GPT model. Next to the embedding layers, it consists of one or\nmore transformer blocks containing the masked multi-head attention module we implemented in\nthe previous chapter.\nAs you can see in Figure 4.2, we have already covered several aspects, such\nas input tokenization and embedding, as well as the masked multi-head\nattention module. The focus of this chapter will be on implementing the core\nstructure of the GPT model, including its \ntransformer blocks\n, which we will\nthen train in the next chapter to generate human-like text.\nIn the previous chapters, we used smaller embedding dimensions for\nsimplicity, ensuring that the concepts and examples could comfortably fit on\na single page. Now, in this chapter, we are scaling up to the size of a small\nGPT-2 model, specifically the smallest version with 124 million parameters,\nas described in Radford \net al.\n's paper, ""Language Models are Unsupervised\nMultitask Learners."" Note that while the original report mentions 117 million\nparameters, this was later corrected.\nChapter 6 will focus on loading pretrained weights into our implementation\nand adapting it for larger GPT-2 models with 345, 762, and 1,542 million\nparameters. In the context of deep learning and LLMs like GPT, the term\n""parameters"" refers to the trainable weights of the model. These weights are\nessentially the internal variables of the model that are adjusted and optimized\nduring the training process to minimize a specific loss function. This\noptimization allows the model to learn from the training data.\nFor example, in a neural network layer that is represented by a 2,048x2,048-\ndimensional matrix (or tensor) of weights, each element of this matrix is a\nparameter. Since there are 2,048 rows and 2,048 columns, the total number of\nparameters in this layer is 2,048 multiplied by 2,048, which equals 4,194,304\nparameters.\nGPT-2 versus GPT-3\nNote that we are focusing on GPT-2 because OpenAI has made the weights\nof the pretrained model publicly available, which we will load into our\nimplementation in chapter 6. GPT-3 is fundamentally the same in terms of\nmodel architecture, except that it is scaled up from 1.5 billion parameters in\nGPT-2 to 175 billion parameters in GPT-3, and it is trained on more data. As\nof this writing, the weights for GPT-3 are not publicly available. GPT-2 is\nalso a better choice for learning how to implement LLMs, as it can be run on\na single laptop computer, whereas GPT-3 requires a GPU cluster for training\nand inference. According to Lambda Labs, it would take 355 years to train\nGPT-3 on a single V100 datacenter GPU, and 665 years on a consumer RTX\n8000 GPU.\nWe specify the configuration of the small GPT-2 model via the following\nPython dictionary, which we will use in the code examples later:\nGPT_CONFIG_124M = {\n    ""vocab_size"": 50257,  # Vocabulary size\n    ""context_length"": 1024,      # Context length\n    ""emb_dim"": 768,       # Embedding dimension\n    ""n_heads"": 12,        # Number of attention heads\n    ""n_layers"": 12,       # Number of layers\n    ""drop_rate"": 0.1,     # Dropout rate\n    ""qkv_bias"": False     # Query-Key-Value bias\n}\nIn the \nGPT_CONFIG_124M\n dictionary, we use concise variable names for clarity\nand to prevent long lines of code:\n""vocab_size""\n refers to a vocabulary of 50,257 words, as used by the\nBPE tokenizer from chapter 2.\n""context_length""\n denotes the maximum number of input tokens the\nmodel can handle, via the positional embeddings discussed in chapter 2.\n""emb_dim""\n represents the embedding size, transforming each token into\na 768-dimensional vector.\n""n_heads""\n indicates the count of attention heads in the multi-head\nattention mechanism, as implemented in chapter 3.\n""n_layers""\n specifies the number of transformer blocks in the model,\nwhich will be elaborated on in upcoming sections.\n""drop_rate""\n indicates the intensity of the dropout mechanism (0.1\nimplies a 10% drop of hidden units) to prevent overfitting, as covered in\nchapter 3.\n""qkv_bias""\n determines whether to include a bias vector in the \nLinear\nlayers of the multi-head attention for query, key, and value\ncomputations. We will initially disable this, following the norms of\nmodern LLMs, but will revisit it in chapter 6 when we load pretrained\nGPT-2 weights from OpenAI into our model.\nUsing the configuration above, we will start this chapter by implementing a\nGPT placeholder architecture (\nDummyGPTModel\n) in this section, as shown in\nFigure 4.3. This will provide us with a big-picture view of how everything\nfits together and what other components we need to code in the upcoming\nsections to assemble the full GPT model architecture.\nFigure 4.3 A mental model outlining the order in which we code the GPT architecture. In this\nchapter, we will start with the GPT backbone, a placeholder architecture, before we get to the\nindividual core pieces and eventually assemble them in a transformer block for the final GPT\narchitecture.\nThe numbered boxes shown in Figure 4.3 illustrate the order in which we\ntackle the individual concepts required to code the final GPT architecture. We\nwill start with step 1, a placeholder GPT backbone we call \nDummyGPTModel\n:\nListing 4.1 A placeholder GPT model architecture class\nimport torch\nimport torch.nn as nn\n \nclass DummyGPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[""vocab_size""], cfg[""emb_dim""])\n        self.pos_emb = nn.Embedding(cfg[""context_length""], cfg[""emb_dim""])\n        self.drop_emb = nn.Dropout(cfg[""drop_rate""])\n        self.trf_blocks = nn.Sequential(\n            *[DummyTransformerBlock(cfg) for _ in range(cfg[""n_layers""])]) #A\n        self.final_norm = DummyLayerNorm(cfg[""emb_dim""]) #B\n        self.out_head = nn.Linear(\n            cfg[""emb_dim""], cfg[""vocab_size""], bias=False\n        )\n \n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\n \nclass DummyTransformerBlock(nn.Module): #C\n    def __init__(self, cfg):\n        super().__init__()\n \n    def forward(self, x): #D\n        return x\n \nclass DummyLayerNorm(nn.Module): #E\n    def __init__(self, normalized_shape, eps=1e-5): #F\n        super().__init__()\n \n    def forward(self, x):\n        return x\nThe \nDummyGPTModel\n class in this code defines a simplified version of a GPT-\nlike model using PyTorch's neural network module (\nnn.Module\n). The model\narchitecture in the \nDummyGPTModel\n class consists of token and positional\nembeddings, dropout, a series of transformer blocks\n(\nDummyTransformerBlock\n), a final layer normalization (\nDummyLayerNorm\n),\nand a linear output layer (\nout_head\n). The configuration is passed in via a\nPython dictionary, for instance, the \nGPT_CONFIG_124M\n dictionary we created\nearlier.\nThe \nforward\n method describes the data flow through the model: it computes\ntoken and positional embeddings for the input indices, applies dropout,\nprocesses the data through the transformer blocks, applies normalization, and\nfinally produces logits with the linear output layer.\nThe code above is already functional, as we will see later in this section after\nwe prepare the input data. However, for now, note in the code above that we\nhave used placeholders (\nDummyLayerNorm\n and \nDummyTransformerBlock\n) for\nthe transformer block and layer normalization, which we will develop in later\nsections.\nNext, we will prepare the input data and initialize a new GPT model to\nillustrate its usage. Building on the figures we have seen in chapter 2, where\nwe coded the tokenizer, Figure 4.4 provides a high-level overview of how\ndata flows in and out of a GPT model.\nFigure 4.4 A big-picture overview showing how the input data is tokenized, embedded, and fed to\nthe GPT model. Note that in our \nDummyGPTClass\n coded earlier, the token embedding is handled\ninside the GPT model. In LLMs, the embedded input token dimension typically matches the\noutput dimension. The output embeddings here represent the context vectors we discussed in\nchapter 3.\nTo implement the steps shown in Figure 4.4, we tokenize a batch consisting\nof two text inputs for the GPT model using the tiktoken tokenizer introduced\nin chapter 2:\nimport tiktoken\n \ntokenizer = tiktoken.get_encoding(""gpt2"")\nbatch = []\ntxt1 = ""Every effort moves you""\ntxt2 = ""Every day holds a""\n \nbatch.append(torch.tensor(tokenizer.encode(txt1)))\nbatch.append(torch.tensor(tokenizer.encode(txt2)))\nbatch = torch.stack(batch, dim=0)\nprint(batch)\nThe resulting token IDs for the two texts are as follows:\ntensor([[ 6109,  3626,  6100,   345], #A\n        [ 6109,  1110,  6622,   257]])\nNext, we initialize a new 124 million parameter \nDummyGPTModel\n instance and\nfeed it the tokenized \nbatch\n:\ntorch.manual_seed(123)\nmodel = DummyGPTModel(GPT_CONFIG_124M)\nlogits = model(batch)\nprint(""Output shape:"", logits.shape)\nprint(logits)\nThe model outputs, which are commonly referred to as logits, are as follows:\nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n \n        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n       grad_fn=<UnsafeViewBackward0>)\nThe output tensor has two rows corresponding to the two text samples. Each\ntext sample consists of 4 tokens; each token is a 50,257-dimensional vector,\nwhich matches the size of the tokenizer's vocabulary.\nThe embedding has 50,257 dimensions because each of these dimensions\nrefers to a unique token in the vocabulary. At the end of this chapter, when\nwe implement the postprocessing code, we will convert these 50,257-\ndimensional vectors back into token IDs, which we can then decode into\nwords.\nNow that we have taken a top-down look at the GPT architecture and its in-\nand outputs, we will code the individual placeholders in the upcoming\nsections, starting with the real layer normalization class that will replace the\nDummyLayerNorm\n in the previous code.\n4.2 Normalizing activations with layer\nnormalization\nTraining deep neural networks with many layers can sometimes prove\nchallenging due to issues like vanishing or exploding gradients. These issues\nlead to unstable training dynamics and make it difficult for the network to\neffectively adjust its weights, which means the learning process struggles to\nfind a set of parameters (weights) for the neural network that minimizes the\nloss function. In other words, the network has difficulty learning the\nunderlying patterns in the data to a degree that would allow it to make\naccurate predictions or decisions. (If you are new to neural network training\nand the concepts of gradients, a brief introduction to these concepts can be\nfound in \nSection A.4, Automatic Differentiation Made Easy\n in \nAppendix A:\nIntroduction to PyTorch\n. However, a deep mathematical understanding of\ngradients is not required to follow the contents of this book.)\nIn this section, we will implement \nlayer normalization\n to improve the\nstability and efficiency of neural network training.\nThe main idea behind layer normalization is to adjust the activations\n(outputs) of a neural network layer to have a mean of 0 and a variance of 1,\nalso known as unit variance. This adjustment speeds up the convergence to\neffective weights and ensures consistent, reliable training. As we have seen in\nthe previous section, based on the \nDummyLayerNorm\n placeholder, in GPT-2\nand modern transformer architectures, layer normalization is typically applied\nbefore and after the multi-head attention module and before the final output\nlayer.\nBefore we implement layer normalization in code, Figure 4.5 provides a\nvisual overview of how layer normalization functions.\nFigure 4.5 An illustration of layer normalization where the 5 layer outputs, also called\nactivations, are normalized such that they have a zero mean and variance of 1.\nWe can recreate the example shown in Figure 4.5 via the following code,\nwhere we implement a neural network layer with 5 inputs and 6 outputs that\nwe apply to two input examples:\ntorch.manual_seed(123)\nbatch_example = torch.randn(2, 5) #A\nlayer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\nout = layer(batch_example)\nprint(out)\nThis prints the following tensor, where the first row lists the layer outputs for\nthe first input and the second row lists the layer outputs for the second row:\ntensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n       grad_fn=<ReluBackward0>)\nThe neural network layer we have coded consists of a \nLinear\n layer followed\nby a non-linear activation function, \nReLU\n (short for Rectified Linear Unit),\nwhich is a standard activation function in neural networks. If you are\nunfamiliar with \nReLU\n, it simply thresholds negative inputs to 0, ensuring that\na layer outputs only positive values, which explains why the resulting layer\noutput does not contain any negative values. (Note that we will use another,\nmore sophisticated activation function in GPT, which we will introduce in the\nnext section).\nBefore we apply layer normalization to these outputs, let's examine the mean\nand variance:\nmean = out.mean(dim=-1, keepdim=True)\nvar = out.var(dim=-1, keepdim=True)\nprint(""Mean:\n"", mean)\nprint(""Variance:\n"", var)\nThe output is as follows:\nMean:\n  tensor([[0.1324],\n          [0.2170]], grad_fn=<MeanBackward1>)\nVariance:\n  tensor([[0.0231],\n          [0.0398]], grad_fn=<VarBackward0>)\nThe first row in the mean tensor above contains the mean value for the first\ninput row, and the second output row contains the mean for the second input\nrow.\nUsing \nkeepdim=True\n in operations like mean or variance calculation ensures\nthat the output tensor retains the same shape as the input tensor, even though\nthe operation reduces the tensor along the dimension specified via \ndim\n. For\ninstance, without \nkeepdim=True\n, the returned mean tensor would be a 2-\ndimensional vector \n[0.1324, 0.2170]\n instead of a 2×1-dimensional matrix\n[[0.1324], [0.2170]]\n.\nThe \ndim\n parameter specifies the dimension along which the calculation of the\nstatistic (here, mean or variance) should be performed in a tensor, as shown\nin Figure 4.6.\nFigure 4.6 An illustration of the dim parameter when calculating the mean of a tensor. For\ninstance, if we have a 2D tensor (matrix) with dimensions \n[rows, columns]\n, using \ndim=0\n will\nperform the operation across rows (vertically, as shown at the bottom), resulting in an output\nthat aggregates the data for each column. Using \ndim=1\n or \ndim=-1\n will perform the operation\nacross columns (horizontally, as shown at the top), resulting in an output aggregating the data for\neach row.\nAs Figure 4.6 explains, for a 2D tensor (like a matrix), using \ndim=-1\n for\noperations such as mean or variance calculation is the same as using \ndim=1\n.\nThis is because -1 refers to the tensor's last dimension, which corresponds to\nthe columns in a 2D tensor. Later, when adding layer normalization to the\nGPT model, which produces 3D tensors with shape \n[batch_size,\nnum_tokens, embedding_size]\n, we can still use \ndim=-1\n for normalization\nacross the last dimension, avoiding a change from \ndim=1\n to \ndim=2\n.\nNext, let us apply layer normalization to the layer outputs we obtained earlier.\nThe operation consists of subtracting the mean and dividing by the square\nroot of the variance (also known as standard deviation):\nout_norm = (out - mean) / torch.sqrt(var)\nmean = out_norm.mean(dim=-1, keepdim=True)\nvar = out_norm.var(dim=-1, keepdim=True)\nprint(""Normalized layer outputs:\n"", out_norm)\nprint(""Mean:\n"", mean)\nprint(""Variance:\n"", var)\nAs we can see based on the results, the normalized layer outputs, which now\nalso contain negative values, have zero mean and a variance of 1:\nNormalized layer outputs:\n tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n       grad_fn=<DivBackward0>)\nMean:\n tensor([[2.9802e-08],\n        [3.9736e-08]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.],\n        [1.]], grad_fn=<VarBackward0>)\nNote that the value 2.9802e-08 in the output tensor is the scientific notation\nfor 2.9802 × 10-8, which is 0.0000000298 in decimal form. This value is very\nclose to 0, but it is not exactly 0 due to small numerical errors that can\naccumulate because of the finite precision with which computers represent\nnumbers.\nTo improve readability, we can also turn off the scientific notation when\nprinting tensor values by setting \nsci_mode\n to False:\ntorch.set_printoptions(sci_mode=False)\nprint(""Mean:\n"", mean)\nprint(""Variance:\n"", var)\nMean:\n tensor([[    0.0000],\n        [    0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.],\n        [1.]], grad_fn=<VarBackward0>)\nSo far, in this section, we have coded and applied layer normalization in a\nstep-by-step process. Let's now encapsulate this process in a PyTorch module\nthat we can use in the GPT model later:\nListing 4.2 A layer normalization class\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n \n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\nThis specific implementation of layer Normalization operates on the last\ndimension of the input tensor x, which represents the embedding dimension\n(\nemb_dim\n). The variable \neps\n is a small constant (epsilon) added to the\nvariance to prevent division by zero during normalization. The \nscale\n and\nshift\n are two trainable parameters (of the same dimension as the input) that\nthe LLM automatically adjusts during training if it is determined that doing\nso would improve the model's performance on its training task. This allows\nthe model to learn appropriate scaling and shifting that best suit the data it is\nprocessing.\nBiased variance\nIn our variance calculation method, we have opted for an implementation\ndetail by setting \nunbiased=False\n. For those curious about what this means,\nin the variance calculation, we divide by the number of inputs \nn\n in the\nvariance formula. This approach does not apply Bessel's correction, which\ntypically uses \nn-1\n instead of \nn\n in the denominator to adjust for bias in sample\nvariance estimation. This decision results in a so-called biased estimate of the\nvariance. For large-scale language models (LLMs), where the embedding\ndimension n is significantly large, the difference between using n and n-1 is\npractically negligible. We chose this approach to ensure compatibility with\nthe GPT-2 model's normalization layers and because it reflects TensorFlow's\ndefault behavior, which was used to implement the original GPT-2 model.\nUsing a similar setting ensures our method is compatible with the pretrained\nweights we will load in chapter 6.\nLet's now try the \nLayerNorm\n module in practice and apply it to the batch\ninput:\nln = LayerNorm(emb_dim=5)\nout_ln = ln(batch_example)\nmean = out_ln.mean(dim=-1, keepdim=True)\nvar = out_ln.var(dim=-1, unbiased=False, keepdim=True)\nprint(""Mean:\n"", mean)\nprint(""Variance:\n"", var)\nAs we can see based on the results, the layer normalization code works as\nexpected and normalizes the values of each of the two inputs such that they\nhave a mean of 0 and a variance of 1:\nMean:\n tensor([[    -0.0000],\n        [     0.0000]], grad_fn=<MeanBackward1>)\nVariance:\n tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\nIn this section, we covered one of the building blocks we will need to\nimplement the GPT architecture, as shown in the mental model in Figure 4.7.\nFigure 4.7 A mental model listing the different building blocks we implement in this chapter to\nassemble the GPT architecture.\nIn the next section, we will look at the GELU activation function, which is\none of the activation functions used in LLMs, instead of the traditional ReLU\nfunction we used in this section.\nLayer normalization versus batch normalization\nIf you are familiar with batch normalization, a common and traditional\nnormalization method for neural networks, you may wonder how it compares\nto layer normalization. Unlike batch normalization, which normalizes across\nthe batch dimension, layer normalization normalizes across the feature\ndimension. LLMs often require significant computational resources, and the\navailable hardware or the specific use case can dictate the batch size during\ntraining or inference. Since layer normalization normalizes each input\nindependently of the batch size, it offers more flexibility and stability in these\nscenarios. This is particularly beneficial for distributed training or when\ndeploying models in environments where resources are constrained.\n4.3 Implementing a feed forward network with\nGELU activations\nIn this section, we implement a small neural network submodule that is used\nas part of the transformer block in LLMs. We begin with implementing the\nGELU\n activation function, which plays a crucial role in this neural network\nsubmodule. (For additional information on implementing neural networks in\nPyTorch, please see section A.5 Implementing multilayer neural networks in\nAppendix A.)\nHistorically, the ReLU activation function has been commonly used in deep\nlearning due to its simplicity and effectiveness across various neural network\narchitectures. However, in LLMs, several other activation functions are\nemployed beyond the traditional ReLU. Two notable examples are GELU\n(\nGaussian Error Linear Unit\n) and SwiGLU (\nSigmoid-Weighted Linear Unit\n).\nGELU and SwiGLU are more complex and smooth activation functions\nincorporating Gaussian and sigmoid-gated linear units, respectively. They\noffer improved performance for deep learning models, unlike the simpler\nReLU.\nThe GELU activation function can be implemented in several ways; the exact\nversion is defined as GELU(x)=x Φ(x), where Φ(x) is the cumulative\ndistribution function of the standard Gaussian distribution. In practice,\nhowever, it's common to implement a computationally cheaper\napproximation (the original GPT-2 model was also trained with this\napproximation):\nGELU(x) ≈ 0.5 \n⋅\n x \n⋅\n (1 + tanh[√((2/π)) \n⋅\n (x + 0.044715 \n⋅\n x^3])\nIn code, we can implement this function as PyTorch module as follows:\nListing 4.3 An implementation of the GELU activation function\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n \n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))\nNext, to get an idea of what this GELU function looks like and how it\ncompares to the ReLU function, let's plot these functions side by side:\nimport matplotlib.pyplot as plt\ngelu, relu = GELU(), nn.ReLU()\n \nx = torch.linspace(-3, 3, 100) #A\ny_gelu, y_relu = gelu(x), relu(x)\nplt.figure(figsize=(8, 3))\nfor i, (y, label) in enumerate(zip([y_gelu, y_relu], [""GELU"", ""ReLU""]), 1):\n    plt.subplot(1, 2, i)\n    plt.plot(x, y)\n    plt.title(f""{label} activation function"")\n    plt.xlabel(""x"")\n    plt.ylabel(f""{label}(x)"")\n    plt.grid(True)\nplt.tight_layout()\nplt.show()\nAs we can see in the resulting plot in Figure 4.8, ReLU is a piecewise linear\nfunction that outputs the input directly if it is positive; otherwise, it outputs\nzero. GELU is a smooth, non-linear function that approximates ReLU but\nwith a non-zero gradient for negative values.\nFigure 4.8 The output of the GELU and ReLU plots using matplotlib. The x-axis shows the\nfunction inputs and the y-axis shows the function outputs.\nThe smoothness of GELU, as shown in Figure 4.8, can lead to better\noptimization properties during training, as it allows for more nuanced\nadjustments to the model's parameters. In contrast, ReLU has a sharp corner\nat zero, which can sometimes make optimization harder, especially in\nnetworks that are very deep or have complex architectures. Moreover, unlike\nRELU, which outputs zero for any negative input, GELU allows for a small,\nnon-zero output for negative values. This characteristic means that during the\ntraining process, neurons that receive negative input can still contribute to the\nlearning process, albeit to a lesser extent than positive inputs.\nNext, let's use the GELU function to implement the small neural network\nmodule, \nFeedForward\n, that we will be using in the LLM's transformer block\nlater:\nListing 4.4 A feed forward neural network module\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[""emb_dim""], 4 * cfg[""emb_dim""]),\n            GELU(),\n            nn.Linear(4 * cfg[""emb_dim""], cfg[""emb_dim""]),\n        )\n \n    def forward(self, x):\n        return self.layers(x)\nAs we can see in the preceding code, the \nFeedForward\n module is a small\nneural network consisting of two \nLinear\n layers and a \nGELU\n activation\nfunction. In the 124 million parameter GPT model, it receives the input\nbatches with tokens that have an embedding size of 768 each via the\nGPT_CONFIG_124M\n dictionary where \nGPT_CONFIG_124M[""emb_dim""] = 768\n.\nFigure 4.9 shows how the embedding size is manipulated inside this small\nfeed forward neural network when we pass it some inputs.\nFigure 4.9 provides a visual overview of the connections between the layers of the feed forward\nneural network. It is important to note that this neural network can accommodate variable batch\nsizes and numbers of tokens in the input. However, the embedding size for each token is\ndetermined and fixed when initializing the weights.\nFollowing the example in Figure 4.9, let's initialize a new \nFeedForward\nmodule with a token embedding size of 768 and feed it a batch input with 2\nsamples and 3 tokens each:\nffn = FeedForward(GPT_CONFIG_124M)\nx = torch.rand(2, 3, 768) #A \nout = ffn(x)\nprint(out.shape)\nAs we can see, the shape of the output tensor is the same as that of the input\ntensor:\ntorch.Size([2, 3, 768])\nThe \nFeedForward\n module we implemented in this section plays a crucial role\nin enhancing the model's ability to learn from and generalize the data.\nAlthough the input and output dimensions of this module are the same, it\ninternally expands the embedding dimension into a higher-dimensional space\nthrough the first linear layer as illustrated in Figure 4.10. This expansion is\nfollowed by a non-linear GELU activation, and then a contraction back to the\noriginal dimension with the second linear transformation. Such a design\nallows for the exploration of a richer representation space.\nFigure 4.10 An illustration of the expansion and contraction of the layer outputs in the feed\nforward neural network. First, the inputs expand by a factor of 4 from 768 to 3072 values. Then,\nthe second layer compresses the 3072 values back into a 768-dimensional representation.\nMoreover, the uniformity in input and output dimensions simplifies the\narchitecture by enabling the stacking of multiple layers, as we will do later,\nwithout the need to adjust dimensions between them, thus making the model\nmore scalable.\nAs illustrated in Figure 4.11, we have now implemented most of the LLM's\nbuilding blocks.\nFigure 4.11 A mental model showing the topics we cover in this chapter, with the black\ncheckmarks indicating those that we have already covered.\nIn the next section, we will go over the concept of shortcut connections that\nwe insert between different layers of a neural network, which are important\nfor improving the training performance in deep neural network architectures.\n4.4 Adding shortcut connections\nNext, let's discuss the concept behind \nshortcut connections\n, also known as\nskip or residual connections. Originally, shortcut connections were proposed\nfor deep networks in computer vision (specifically, in residual networks) to\nmitigate the challenge of vanishing gradients. The vanishing gradient\nproblem refers to the issue where gradients (which guide weight updates\nduring training) become progressively smaller as they propagate backward\nthrough the layers, making it difficult to effectively train earlier layers, as\nillustrated in Figure 4.12.\nFigure 4.12 A comparison between a deep neural network consisting of 5 layers without (on the\nleft) and with shortcut connections (on the right). Shortcut connections involve adding the inputs\nof a layer to its outputs, effectively creating an alternate path that bypasses certain layers. The\ngradient illustrated in Figure 1.1 denotes the mean absolute gradient at each layer, which we will\ncompute in the code example that follows.\nAs illustrated in Figure 4.12, a shortcut connection creates an alternative,\nshorter path for the gradient to flow through the network by skipping one or\nmore layers, which is achieved by adding the output of one layer to the output\nof a later layer. This is why these connections are also known as skip\nconnections. They play a crucial role in preserving the flow of gradients\nduring the backward pass in training.\nIn the code example below, we implement the neural network shown in\nFigure 4.12 to see how we can add shortcut connections in the \nforward\nmethod:\nListing 4.5 A neural network to illustrate shortcut connections\nclass ExampleDeepNeuralNetwork(nn.Module):\n    def __init__(self, layer_sizes, use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.ModuleList([\n            # Implement 5 layers\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n        ])\n \n    def forward(self, x):\n        for layer in self.layers:\n            # Compute the output of the current layer\n            layer_output = layer(x)\n            # Check if shortcut can be applied\n            if self.use_shortcut and x.shape == layer_output.shape:\n                x = x + layer_output\n            else:\n                x = layer_output\n        return x\nThe code implements a deep neural network with 5 layers, each consisting of\na \nLinear\n layer and a \nGELU\n activation function. In the forward pass, we\niteratively pass the input through the layers and optionally add the shortcut\nconnections depicted in Figure 4.12 if the \nself.use_shortcut\n attribute is set\nto \nTrue\n.\nLet's use this code to first initialize a neural network without shortcut\nconnections. Here, each layer will be initialized such that it accepts an\nexample with 3 input values and returns 3 output values. The last layer\nreturns a single output value:\nlayer_sizes = [3, 3, 3, 3, 3, 1]  \nsample_input = torch.tensor([[1., 0., -1.]])\ntorch.manual_seed(123) # specify random seed for the initial weights for reproducibility\nmodel_without_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=False\n)\nNext, we implement a function that computes the gradients in the the model's\nbackward pass:\ndef print_gradients(model, x):\n    # Forward pass\n    output = model(x)\n    target = torch.tensor([[0.]])\n \n    # Calculate loss based on how close the target\n    # and output are\n    loss = nn.MSELoss()\n    loss = loss(output, target)\n    \n    # Backward pass to calculate the gradients\n    loss.backward()\n \n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            # Print the mean absolute gradient of the weights\n            print(f""{name} has gradient mean of {param.grad.abs().mean().item()}"")\nIn the preceding code, we specify a loss function that computes how close the\nmodel output and a user-specified target (here, for simplicity, the value 0)\nare. Then, when calling \nloss.backward()\n, PyTorch computes the loss\ngradient for each layer in the model. We can iterate through the weight\nparameters via \nmodel.named_parameters()\n. Suppose we have a 3×3 weight\nparameter matrix for a given layer. In that case, this layer will have 3×3\ngradient values, and we print the mean absolute gradient of these 3×3\ngradient values to obtain a single gradient value per layer to compare the\ngradients between layers more easily.\nIn short, the \n.backward()\n method is a convenient method in PyTorch that\ncomputes loss gradients, which are required during model training, without\nimplementing the math for the gradient calculation ourselves, thereby making\nworking with deep neural networks much more accessible. If you are\nunfamiliar with the concept of gradients and neural network training, I\nrecommend reading sections \nA.4, Automatic differentiation made easy\n and\nA.7 A typical training loop\n in \nappendix A\n.\nLet's now use the \nprint_gradients\n function and apply it to the model\nwithout skip connections:\nprint_gradients(model_without_shortcut, sample_input)\nThe output is as follows:\nlayers.0.0.weight has gradient mean of 0.00020173587836325169\nlayers.1.0.weight has gradient mean of 0.0001201116101583466\nlayers.2.0.weight has gradient mean of 0.0007152041653171182\nlayers.3.0.weight has gradient mean of 0.001398873864673078\nlayers.4.0.weight has gradient mean of 0.005049646366387606\nAs we can see based on the output of the \nprint_gradients\n function, the\ngradients become smaller as we progress from the last layer (\nlayers.4\n) to the\nfirst layer (\nlayers.0\n), which is a phenomenon called the vanishing gradient\nproblem.\nLet's now instantiate a model with skip connections and see how it compares:\ntorch.manual_seed(123)\nmodel_with_shortcut = ExampleDeepNeuralNetwork(\n    layer_sizes, use_shortcut=True\n)\nprint_gradients(model_with_shortcut, sample_input)\nThe output is as follows:\nlayers.0.0.weight has gradient mean of 0.22169792652130127\nlayers.1.0.weight has gradient mean of 0.20694105327129364\nlayers.2.0.weight has gradient mean of 0.32896995544433594\nlayers.3.0.weight has gradient mean of 0.2665732502937317\nlayers.4.0.weight has gradient mean of 1.3258541822433472\nAs we can see, based on the output, the last layer \n(layers.4\n) still has a larger\ngradient than the other layers. However, the gradient value stabilizes as we\nprogress towards the first layer (\nlayers.0\n) and doesn't shrink to a\nvanishingly small value.\nIn conclusion, shortcut connections are important for overcoming the\nlimitations posed by the vanishing gradient problem in deep neural networks.\nShortcut connections are a core building block of very large models such as\nLLMs, and they will help facilitate more effective training by ensuring\nconsistent gradient flow across layers when we train the GPT model in the\nnext chapter.\nAfter introducing shortcut connections, we will now connect all of the\npreviously covered concepts (layer normalization, GELU activations, feed\nforward module, and shortcut connections) in a transformer block in the next\nsection, which is the final building block we need to code the GPT\narchitecture.\n4.5 Connecting attention and linear layers in a\ntransformer block\nIn this section, we are implementing the \ntransformer block\n, a fundamental\nbuilding block of GPT and other LLM architectures. This block, which is\nrepeated a dozen times in the 124 million parameter GPT-2 architecture,\ncombines several concepts we have previously covered: multi-head attention,\nlayer normalization, dropout, feed forward layers, and GELU activations, as\nillustrated in Figure 4.13. In the next section, we will then connect this\ntransformer block to the remaining parts of the GPT architecture.\nFigure 4.13 An illustration of a transformer block. The bottom of the diagram shows input tokens\nthat have been embedded into 768-dimensional vectors. Each row corresponds to one token's\nvector representation. The outputs of the transformer block are vectors of the same dimension as\nthe input, which can then be fed into subsequent layers in an LLM.\nAs shown in Figure 4.13, the transformer block combines several\ncomponents, including the masked multi-head attention module from chapter\n3 and the \nFeedForward\n module we implemented in Section 4.3.\nWhen a transformer block processes an input sequence, each element in the\nsequence (for example, a word or subword token) is represented by a fixed-\nsize vector (in the case of Figure 4.13, 768 dimensions). The operations\nwithin the transformer block, including multi-head attention and feed forward\nlayers, are designed to transform these vectors in a way that preserves their\ndimensionality.\nThe idea is that the self-attention mechanism in the multi-head attention\nblock identifies and analyzes relationships between elements in the input\nsequence. In contrast, the feed forward network modifies the data\nindividually at each position. This combination not only enables a more\nnuanced understanding and processing of the input but also enhances the\nmodel's overall capacity for handling complex data patterns.\nIn code, we can create the \nTransformerBlock\n as follows:\nListing 4.6 The transformer block component of GPT\nfrom previous_chapters import MultiHeadAttention\n \nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in=cfg[""emb_dim""],\n            d_out=cfg[""emb_dim""],\n            block_size=cfg[""context_length""],\n            num_heads=cfg[""n_heads""], \n            dropout=cfg[""drop_rate""],\n            qkv_bias=cfg[""qkv_bias""])\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[""emb_dim""])\n        self.norm2 = LayerNorm(cfg[""emb_dim""])\n        self.drop_resid = nn.Dropout(cfg[""drop_rate""])\n \n    def forward(self, x):\n        #A\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_resid(x)\n        x = x + shortcut  # Add the original input back\n \n        shortcut = x #B\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_resid(x)\n        x = x + shortcut  #C \n        return x\nThe given code defines a \nTransformerBlock\n class in PyTorch that includes a\nmulti-head attention mechanism (\nMultiHeadAttention\n) and a feed forward\nnetwork (\nFeedForward\n), both configured based on a provided configuration\ndictionary (\ncfg\n), such as \nGPT_CONFIG_124M\n.\nLayer normalization (\nLayerNorm\n) is applied before each of these two\ncomponents, and dropout is applied after them to regularize the model and\nprevent overfitting. This is also known as \nPre-LayerNorm\n. Older\narchitectures, such as the original transformer model, applied layer\nnormalization after the self-attention and feed-forward networks instead,\nknown as \nPost-LayerNorm\n, which often leads to worse training dynamics.\nThe class also implements the forward pass, where each component is\nfollowed by a shortcut connection that adds the input of the block to its\noutput. This critical feature helps gradients flow through the network during\ntraining and improves the learning of deep models as explained in section\n4.4.\nUsing the \nGPT_CONFIG_124M\n dictionary we defined earlier, let's instantiate a\ntransformer block and feed it some sample data:\ntorch.manual_seed(123)\nx = torch.rand(2, 4, 768)  #A\nblock = TransformerBlock(GPT_CONFIG_124M)\noutput = block(x)\n \nprint(""Input shape:"", x.shape)\nprint(""Output shape:"", output.shape)\nThe output is as follows:\nInput shape: torch.Size([2, 4, 768])\nOutput shape: torch.Size([2, 4, 768])\nAs we can see from the code output, the transformer block maintains the\ninput dimensions in its output, indicating that the transformer architecture\nprocesses sequences of data without altering their shape throughout the\nnetwork.\nThe preservation of shape throughout the transformer block architecture is\nnot incidental but a crucial aspect of its design. This design enables its\neffective application across a wide range of sequence-to-sequence tasks,\nwhere each output vector directly corresponds to an input vector, maintaining\na one-to-one relationship. However, the output is a context vector that\nencapsulates information from the entire input sequence, as we learned in\nchapter 3. This means that while the physical dimensions of the sequence\n(length and feature size) remain unchanged as it passes through the\ntransformer block, the content of each output vector is re-encoded to integrate\ncontextual information from across the entire input sequence.\nWith the transformer block implemented in this section, we now have all the\nbuilding blocks, as shown in Figure 4.14, needed to implement the GPT\narchitecture in the next section.\nFigure 4.14 A mental model of the different concepts we have implemented in this chapter so far.\nAs illustrated in Figure 4.14, the transformer block combines layer\nnormalization, the feed forward network, including GELU activations, and\nshortcut connections, which we already covered earlier in this chapter. As we\nwill see in the upcoming chapter, this transformer block will make up the\nmain component of the GPT architecture we will implement\n4.6 Coding the GPT model\nWe started this chapter with a big-picture overview of a GPT architecture that\nwe called \nDummyGPTModel\n. In this \nDummyGPTModel\n code implementation, we\nshowed the input and outputs to the GPT model, but its building blocks\nremained a black box using a \nDummyTransformerBlock\n and \nDummyLayerNorm\nclass as placeholders.\nIn this section, we are now replacing the \nDummyTransformerBlock\n and\nDummyLayerNorm\n placeholders with the real \nTransformerBlock\n and\nLayerNorm\n classes we coded later in this chapter to assemble a fully working\nversion of the original 124 million parameter version of GPT-2. In chapter 5,\nwe will pretrain a GPT-2 model, and in chapter 6, we will load in the\npretrained weights from OpenAI.\nBefore we assemble the GPT-2 model in code, let's look at its overall\nstructure in Figure 4.15, which combines all the concepts we covered so far\nin this chapter.\nFigure 4.15 An overview of the GPT model architecture. This figure illustrates the flow of data\nthrough the GPT model. Starting from the bottom, tokenized text is first converted into token\nembeddings, which are then augmented with positional embeddings. This combined information\nforms a tensor that is passed through a series of transformer blocks shown in the center (each\ncontaining multi-head attention and feed forward neural network layers with dropout and layer\nnormalization), which are stacked on top of each other and repeated 12 times.\nAs shown in Figure 4.15, the transformer block we coded in Section 4.5 is\nrepeated many times throughout a GPT model architecture. In the case of the\n124 million parameter GPT-2 model, it's repeated 12 times, which we specify\nvia the \n""n_layers""\n entry in the \nGPT_CONFIG_124M\n dictionary. In the case of\nthe largest GPT-2 model with 1,542 million parameters, this transformer\nblock is repeated 36 times.\nAs shown in Figure 4.15, the output from the final transformer block then\ngoes through a final layer normalization step before reaching the linear output\nlayer. This layer maps the transformer's output to a high-dimensional space\n(in this case, 50,257 dimensions, corresponding to the model's vocabulary\nsize) to predict the next token in the sequence.\nLet's now implement the architecture we see in Figure 4.15 in code:\nListing 4.7 The GPT model architecture implementation\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[""vocab_size""], cfg[""emb_dim""])\n        self.pos_emb = nn.Embedding(cfg[""context_length""], cfg[""emb_dim""])\n        self.drop_emb = nn.Dropout(cfg[""drop_rate""])\n        \n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[""n_layers""])])\n       \n        self.final_norm = LayerNorm(cfg[""emb_dim""])\n        self.out_head = nn.Linear(\n            cfg[""emb_dim""], cfg[""vocab_size""], bias=False\n        )\n \n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        #A\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits\nThanks to the \nTransformerBlock\n class we implemented in Section 4.5, the\nGPTModel\n class is relatively small and compact.\nThe \n__init__\n constructor of this \nGPTModel\n class initializes the token and\npositional embedding layers using the configurations passed in via a Python\ndictionary, \ncfg\n. These embedding layers are responsible for converting input\ntoken indices into dense vectors and adding positional information, as\ndiscussed in chapter 2.\nNext, the \n__init__\n method creates a sequential stack of \nTransformerBlock\nmodules equal to the number of layers specified in \ncfg\n. Following the\ntransformer blocks, a \nLayerNorm\n layer is applied, standardizing the outputs\nfrom the transformer blocks to stabilize the learning process. Finally, a linear\noutput head without bias is defined, which projects the transformer's output\ninto the vocabulary space of the tokenizer to generate logits for each token in\nthe vocabulary.\nThe forward method takes a batch of input token indices, computes their\nembeddings, applies the positional embeddings, passes the sequence through\nthe transformer blocks, normalizes the final output, and then computes the\nlogits, representing the next token's unnormalized probabilities. We will\nconvert these logits into tokens and text outputs in the next section.\nLet's now initialize the 124 million parameter GPT model using the\nGPT_CONFIG_124M\n dictionary we pass into the cfg parameter and feed it with\nthe batch text input we created at the beginning of this chapter:\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\n \nout = model(batch)\nprint(""Input batch:\n"", batch)\nprint(""\nOutput shape:"", out.shape)\nprint(out)\nThe preceding code prints the contents of the input batch followed by the\noutput tensor:\nInput batch:\n tensor([[ 6109,  3626,  6100,   345], # token IDs of text 1\n         [ 6109,  1110,  6622,   257]]) # token IDs of text 2\n \nOutput shape: torch.Size([2, 4, 50257])\ntensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n \n        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n       grad_fn=<UnsafeViewBackward0>)\nAs we can see, the output tensor has the shape \n[2, 4, 50257]\n, since we\npassed in 2 input texts with 4 tokens each. The last dimension, 50,257,\ncorresponds to the vocabulary size of the tokenizer. In the next section, we\nwill see how to convert each of these 50,257-dimensional output vectors back\ninto tokens.\nBefore we move on to the next section and code the function that converts the\nmodel outputs into text, let's spend a bit more time with the model\narchitecture itself and analyze its size.\nUsing the \nnumel()\n method, short for ""number of elements,"" we can collect\nthe total number of parameters in the model's parameter tensors:\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f""Total number of parameters: {total_params:,}"")\nThe result is as follows:\nTotal number of parameters: 163,009,536\nNow, a curious reader might notice a discrepancy. Earlier, we spoke of\ninitializing a 124 million parameter GPT model, so why is the actual number\nof parameters 163 million, as shown in the preceding code output?\nThe reason is a concept called weight tying that is used in the original GPT-2\narchitecture, which means that the original GPT-2 architecture is reusing the\nweights from the token embedding layer inits output layer. To understand\nwhat this means, let's take a look at the shapes of the token embedding layer\nand linear output layer that we initialized on the \nmodel\n via the \nGPTModel\nearlier:\nprint(""Token embedding layer shape:"", model.tok_emb.weight.shape)\nprint(""Output layer shape:"", model.out_head.weight.shape)\nAs we can see based on the print outputs, the weight tensors for both these\nlayers have the same shape:\nToken embedding layer shape: torch.Size([50257, 768])\nOutput layer shape: torch.Size([50257, 768])\nThe token embedding and output layers are very large due to the number of\nrows for the 50,257 in the tokenizer's vocabulary. Let's remove the output\nlayer parameter count from the total GPT-2 model count according to the\nweight tying:\ntotal_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\nprint(f""Number of trainable parameters considering weight tying: {total_params_gpt2:,}"")\nThe output is as follows:\nNumber of trainable parameters considering weight tying: 124,412,160\nAs we can see, the model is now only 124 million parameters large, matching\nthe original size of the GPT-2 model.\nWeight tying reduces the overall memory footprint and computational\ncomplexity of the model. However, in my experience, using separate token\nembedding and output layers results in better training and model\nperformance; hence, we are using separate layers in our \nGPTModel\nimplementation. The same is true for modern LLMs. However, we will revisit\nand implement the weight tying concept later in chapter 6 when we load the\npretrained weights from OpenAI.\nExercise 4.1 Number of parameters in feed forward and attention modules\nCalculate and compare the number of parameters that are contained in the\nfeed forward module and those that are contained in the multi-head attention\nmodule.\nLastly, let us compute the memory requirements of the 163 million\nparameters in our \nGPTModel\n object:\ntotal_size_bytes = total_params * 4  #A\ntotal_size_mb = total_size_bytes / (1024 * 1024)  #B\nprint(f""Total size of the model: {total_size_mb:.2f} MB"")\nThe result is as follows:\nTotal size of the model: 621.83 MB\nIn conclusion, by calculating the memory requirements for the 163 million\nparameters in our \nGPTModel\n object and assuming each parameter is a 32-bit\nfloat taking up 4 bytes, we find that the total size of the model amounts to\n621.83 MB, illustrating the relatively large storage capacity required to\naccommodate even relatively small LLMs.\nIn this section, we implemented the GPTModel architecture and saw that it\noutputs numeric tensors of shape \n[batch_size, num_tokens, vocab_size]\n.\nIn the next section, we will write the code to convert these output tensors into\ntext.\nExercise 4.2 Initializing larger GPT models\nIn this chapter, we initialized a 124 million parameter GPT model, which is\nknown as ""GPT-2 small."" Without making any code modifications besides\nupdating the configuration file, use the GPTModel class to implement GPT-2\nmedium (using 1024-dimensional embeddings, 24 transformer blocks, 16\nmulti-head attention heads), GPT-2 large (1280-dimensional embeddings, 36\ntransformer blocks, 20 multi-head attention heads), and GPT-2 XL (1600-\ndimensional embeddings, 48 transformer blocks, 25 multi-head attention\nheads). As a bonus, calculate the total number of parameters in each GPT\nmodel.\n4.7 Generating text\nIn this final section of this chapter, we will implement the code that converts\nthe tensor outputs of the GPT model back into text. Before we get started,\nlet's briefly review how a generative model like an LLM generates text one\nword (or token) at a time, as shown in Figure 4.16.\nFigure 4.16 This diagram illustrates the step-by-step process by which an LLM generates text,\none token at a time. Starting with an initial input context (""Hello, I am""), the model predicts a\nsubsequent token during each iteration, appending it to the input context for the next round of\nprediction. As shown, the first iteration adds ""a"", the second ""model"", and the third ""ready"",\nprogressively building the sentence.\nFigure 4.16 illustrates the step-by-step process by which a GPT model\ngenerates text given an input context, such as ""Hello, I am,"" on a big-picture\nlevel. With each iteration, the input context grows, allowing the model to\ngenerate coherent and contextually appropriate text. By the 6th iteration, the\nmodel has constructed a complete sentence: ""Hello, I am a model ready to\nhelp.""\nIn the previous section, we saw that our current \nGPTModel\n implementation\noutputs tensors with shape \n[batch_size, num_token, vocab_size]\n. Now,\nthe question is, how does a GPT model go from these output tensors to the\ngenerated text shown in Figure 4.16?\nThe process by which a GPT model goes from output tensors to generated\ntext involves several steps, as illustrated in Figure 4.17. These steps include\ndecoding the output tensors, selecting tokens based on a probability\ndistribution, and converting these tokens into human-readable text.\nFigure 4.17 details the mechanics of text generation in a GPT model by showing a single iteration\nin the token generation process. The process begins by encoding the input text into token IDs,\nwhich are then fed into the GPT model. The outputs of the model are then converted back into\ntext and appended to the original input text.\nThe next-token generation process detailed in Figure 4.17 illustrates a single\nstep where the GPT model generates the next token given its input.\nIn each step, the model outputs a matrix with vectors representing potential\nnext tokens. The vector corresponding to the next token is extracted and\nconverted into a probability distribution via the softmax function. Within the\nvector containing the resulting probability scores, the index of the highest\nvalue is located, which translates to the token ID. This token ID is then\ndecoded back into text, producing the next token in the sequence. Finally, this\ntoken is appended to the previous inputs, forming a new input sequence for\nthe subsequent iteration. This step-by-step process enables the model to\ngenerate text sequentially, building coherent phrases and sentences from the\ninitial input context.\nIn practice, we repeat this process over many iterations, such as shown in\nFigure 4.16 earlier, until we reach a user-specified number of generated\ntokens.\nIn code, we can implement the token-generation process as follows:\nListing 4.8 A function for the GPT model to generate text\ndef generate_text_simple(model, idx, max_new_tokens, context_size): #A\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -context_size:] #B\n        with torch.no_grad():\n            logits = model(idx_cond)\n       \n        logits = logits[:, -1, :] #C\n        probas = torch.softmax(logits, dim=-1)  #D\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True) #E\n        idx = torch.cat((idx, idx_next), dim=1)  #F\n \n    return idx\nThe code snippet provided demonstrates a simple implementation of a\ngenerative loop for a language model using PyTorch. It iterates for a\nspecified number of new tokens to be generated, crops the current context to\nfit the model's maximum context size, computes predictions and then selects\nthe next token based on the highest probability prediction.\nIn the preceeding code, the \ngenerate_text_simple\n function, we use a\nsoftmax function to convert the logits into a probability distribution from\nwhich we identify the position with the highest value via \ntorch.argmax\n. The\nsoftmax function is monotonic, meaning it preserves the order of its inputs\nwhen transformed into outputs. So, in practice, the softmax step is redundant\nsince the position with the highest score in the softmax output tensor is the\nsame position in the logit tensor. In other words, we could apply the\ntorch.argmax\n function to the logits tensor directly and get identical results.\nHowever, we coded the conversion to illustrate the full process of\ntransforming logits to probabilities, which can add additional intuition, such\nas that the model generates the most likely next token, which is known as\ngreedy decoding\n.\nIn the next chapter, when we will implement the GPT training code, we will\nalso introduce additional sampling techniques where we modify the softmax\noutputs such that the model doesn't always select the most likely token, which\nintroduces variability and creativity in the generated text.\nThis process of generating one token ID at a time and appending it to the\ncontext using the \ngenerate_text_simple\n function is further illustrated in\nFigure 4.18. (The token ID generation process for each iteration is detailed in\nFigure 4.17.\nFigure 4.18 An illustration showing six iterations of a token prediction cycle, where the model\ntakes a sequence of initial token IDs as input, predicts the next token, and appends this token to\nthe input sequence for the next iteration. (The token IDs are also translated into their\ncorresponding text for better understanding.)\nAs shown in Figure 4.18, we generate the token IDs in an iterative fashion.\nFor instance, in iteration 1, the model is provided with the tokens\ncorresponding to ""Hello , I am"", predicts the next token (with ID 257, which\nis ""a""), and appends it to the input. This process is repeated until the model\nproduces the complete sentence ""Hello, I am a model ready to help."" after six\niterations.\nLet's now try out the \ngenerate_text_simple\n function with the \n""Hello, I\nam""\n context as model input, as shown in Figure 4.18, in practice.\nFirst, we encode the input context into token IDs:\nstart_context = ""Hello, I am""\nencoded = tokenizer.encode(start_context)\nprint(""encoded:"", encoded)\nencoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\nprint(""encoded_tensor.shape:"", encoded_tensor.shape)\nThe encoded IDs are as follows:\nencoded: [15496, 11, 314, 716]\nencoded_tensor.shape: torch.Size([1, 4])\nNext, we put the model into \n.eval()\n mode, which disables random\ncomponents like dropout, which are only used during training, and use the\ngenerate_text_simple\n function on the encoded input tensor:\nmodel.eval() #A\nout = generate_text_simple(\n    model=model,\n    idx=encoded_tensor, \n    max_new_tokens=6, \n    context_size=GPT_CONFIG_124M[""context_length""]\n)\nprint(""Output:"", out)\nprint(""Output length:"", len(out[0]))\nThe resulting output token IDs are as follows:\nOutput: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\nOutput length: 10\nUsing the \n.decode\n method of the tokenizer, we can convert the IDs back into\ntext:\ndecoded_text = tokenizer.decode(out.squeeze(0).tolist())\nprint(decoded_text)\nThe model output in text format is as follows:\nHello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\nAs we can see, based on the preceding output, the model generated gibberish,\nwhich is not at all like the coherent text shown in Figure 4.18. What\nhappened? The reason why the model is unable to produce coherent text is\nthat we haven't trained it yet. So far, we just implemented the GPT\narchitecture and initialized a GPT model instance with initial random\nweights.\nModel training is a large topic in itself, and we will tackle it in the next\nchapter.\nExercise 4.3 Using separate dropout parameters\nAt the beginning of this chapter, we defined a global \n""drop_rate""\n setting in\nthe \nGPT_CONFIG_124M\n dictionary to set the dropout rate in various places\nthroughout the GPTModel architecture. Change the code to specify a separate\ndropout value for the various dropout layers throughout the model\narchitecture. (Hint: there are three distinct places where we used dropout\nlayers: the embedding layer, shortcut layer, and multi-head attention module.)\n4.8 Summary\nLayer normalization stabilizes training by ensuring that each layer's\noutputs have a consistent mean and variance.\nShortcut connections are connections that skip one or more layers by\nfeeding the output of one layer directly to a deeper layer, which helps\nmitigate the vanishing gradient problem when training deep neural\nnetworks, such as LLMs.\nTransformer blocks are a core structural component of GPT models,\ncombining masked multi-head attention modules with fully connected\nfeed-forward networks that use the GELU activation function.\nGPT models are LLMs with many repeated transformer blocks that have\nmillions to billions of parameters.\nGPT models come in various sizes, for example, 124, 345, 762, and\n1542 million parameters, which we can implement with the same\nGPTModel\n Python class.\nThe text generation capability of a GPT-like LLM involves decoding\noutput tensors into human-readable text by sequentially predicting one\ntoken at a time based on a given input context.\nWithout training, a GPT model generates incoherent text, which\nunderscores the importance of model training for coherent text\ngeneration, which is the topic of subsequent chapters.",64870
06-5_Pretraining_on_Unlabeled_Data.pdf,06-5_Pretraining_on_Unlabeled_Data,"5 Pretraining on Unlabeled Data\nThis chapter covers\nComputing the training and validation set losses to assess the quality of\nLLM-generated text during training\nImplementing a training function and pretraining the LLM\nSaving and loading model weights to continue training an LLM\nLoading pretrained weights from OpenAI\nIn the previous chapters, we implemented the data sampling, attention\nmechanism and coded the LLM architecture. The core focus of this chapter is\nto implement a training function and pretrain the LLM, as illustrated in\nFigure 5.1.\nFigure 5.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a\ngeneral text dataset and finetuning it on a labeled dataset. This chapter focuses on pretraining the\nLLM, which includes implementing the training code, evaluating the performance, and saving\nand loading model weights.\nAs illustrated in Figure 5.1, we will also learn about basic model evaluation\ntechniques to measure the quality of the generated text, which is a\nrequirement for optimizing the LLM during the training process. Moreover,\nwe will discuss how to load pretrained weights, giving our LLM a solid\nstarting point for finetuning in the upcoming chapters.\nWeight parameters\nIn the context of LLMs and other deep learning models, \nweights\n refer to the\ntrainable parameters that the learning process adjusts. These weights are also\nknown as \nweight parameters\n or simply \nparameters\n. In frameworks like\nPyTorch, these weights are stored in linear layers, for example, which we\nused to implement the multi-head attention module in chapter 3 and the\nGPTModel\n in chapter 4. After initializing a layer (\nnew_layer =\ntorch.nn.Linear(...)\n), we can access its weights through the \n.weight\nattribute, \nnew_layer.weight\n. Additionally, for convenience, PyTorch allows\ndirect access to all a model's trainable parameters, including weights and\nbiases, through the method \nmodel.parameters()\n, which we will use later\nwhen implementing the model training.\n5.1 Evaluating generative text models\nWe begin this chapter by setting up the LLM for text generation based on\ncode from the previous chapter and discuss basic ways to evaluate the quality\nof the generated text in this section. The content we cover in this section and\nthe remainder of this chapter is outlined in Figure 5.2.\nFigure 5.2 An overview of the topics covered in this chapter. We begin by recapping the text\ngeneration from the previous chapter and implementing basic model evaluation techniques that\nwe can use during the pretraining stage.\nAs shown in Figure 5.2, the next subsection recaps the text generation we set\nup at the end of the previous chapter before we dive into the text evaluation\nand calculation of the training and validation losses in the subsequent\nsubsections.\n5.1.1 Using GPT to generate text\nIn this section, we set up the LLM and briefly recap the text generation\nprocess we implemented in chapter 4. We begin by initializing the GPT\nmodel that we will evaluate and train in this chapter, using the \nGPTModel\n class\nand \nGPT_CONFIG_124M\n dictionary from chapter 4:\nimport torch\nfrom chapter04 import GPTModel\nGPT_CONFIG_124M = {\n    ""vocab_size"": 50257,\n    ""context_length"": 256,  #A\n    ""emb_dim"": 768,\n    ""n_heads"": 12,\n    ""n_layers"": 12, \n    ""drop_rate"": 0.1,     #B\n    ""qkv_bias"": False\n}\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nConsidering the \nGPT_CONFIG_124M\n dictionary, the only adjustment we have\nmade compared to the previous chapter is reducing the context length\n(\ncontext_length\n) to 256 tokens. This modification reduces the\ncomputational demands of training the model, making it possible to carry out\nthe training on a standard laptop computer.\nOriginally, the GPT-2 model with 124 million parameters was configured to\nhandle up to 1,024 tokens. After the training process, at the end of this\nchapter, we will update the context size setting and load pretrained weights to\nwork with a model configured for a 1,024-token context length.\nUsing the \nGPTmodel\n instance, we adopt the \ngenerate_text_simple\n function\nintroduced in the previous chapter and introduce two handy functions,\ntext_to_token_ids\n and \ntoken_ids_to_text\n. These functions facilitate the\nconversion between text and token representations, a technique we will\nutilize throughout this chapter. To provide a clearer understanding, Figure 5.3\nillustrates this process before we dive into the code.\nFigure 5.3 Generating text involves encoding text into token IDs that the LLM processes into logit\nvectors. The logit vectors are then converted back into token IDs, detokenized into a text\nrepresentation.\nFigure 5.3 illustrates a three-step text generation process using a GPT model.\nFirst, the tokenizer converts input text into a series of token IDs, as discussed\nin chapter 2. Second, the model receives these token IDs and generates\ncorresponding logits, which are vectors representing the probability\ndistribution for each token in the vocabulary, as discussed in chapter 4. Third,\nthese logits are converted back into token IDs, which the tokenizer decodes\ninto human-readable text, completing the cycle from textual input to textual\noutput.\nIn code, we implement the text generation process as follows:\nListing 5.1 Utility functions for text to token ID conversion\nimport tiktoken\nfrom chapter04 import generate_text_simple\n \ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n    return encoded_tensor\n \ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0) # remove batch dimension\n    return tokenizer.decode(flat.tolist())\n \nstart_context = ""Every effort moves you""\ntokenizer = tiktoken.get_encoding(""gpt2"")\n \ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[""context_length""]\n)\nprint(""Output text:\n"", token_ids_to_text(token_ids, tokenizer))\nUsing the preceding code, the \nmodel\n generates the following text:\nOutput text:\n Every effort moves you rentingetic wasn\nم\n refres RexMeCHicular stren\nBased on the output, it's clear the model isn't yet producing coherent text\nbecause it hasn't undergone training. To define what makes text ""coherent"" or\n""high quality,"" we have to implement a numerical method to evaluate the\ngenerated content. This approach will enable us to monitor and enhance the\nmodel's performance throughout its training process.\nThe following section introduces how we calculate a \nloss metric\n for the\ngenerated outputs. This loss serves as a progress and success indicator of the\ntraining progress. Furthermore, in subsequent chapters on finetuning LLMs,\nwe will review additional methodologies for assessing model quality.\n5.1.2 Calculating the text generation loss\nThis section explores techniques for numerically assessing text quality\ngenerated during training by calculating a so-called text generation loss. We\ngo over this topic step-by-step with a practical example to make the concepts\nclear and applicable, beginning with a short recap of how the data is loaded\nfrom chapter 2 and how the text is generated via the \ngenerate_text_simple\nfunction from chapter 4.\nFigure 5.4 illustrates the overall flow from input text to LLM-generated text\nusing a five-step procedure.\nFigure 5.4 For each of the 3 input tokens, shown on the left, we compute a vector containing\nprobability scores corresponding to each token in the vocabulary. The index position of the\nhighest probability score in each vector represents the most likely next token ID. These token IDs\nassociated with the highest probability scores are selected and mapped back into a text that\nrepresents the text generated by the model.\nThe text generation process in Figure 5.4 outlines what the\ngenerate_text_simple\n function from chapter 4 does internally. We need to\nperform these same initial steps before we can compute a loss that measures\nthe generated text quality later in this section.\nFigure 5.4 outlines the text generation process with a small 7-token\nvocabulary to fit this image on a single page. However, our \nGPTModel\n works\nwith a much larger vocabulary consisting of 50,257 words; hence, the token\nIDs in the following codes will range from 0 to 50,256 rather than 0 to 6.\nAlso, Figure 5.4 only shows a single text example (\n""every effort moves""\n)\nfor simplicity. In the following hands-on code example that implements the\nsteps in Figure 5.4, we will work with two input examples (\n""every effort\nmoves""\n and \n""I really like""\n) as inputs for the GPT model:\nConsider the two input examples, which which have already been mapped to\ntoken IDs, corresponding to step 1 in Figure 5.4:\ninputs = torch.tensor([[16833, 3626, 6100],   # [""every effort moves"",\n                       [40,    1107, 588]])   #  ""I really like""]\nMatching these inputs, the `targets` contain the token IDs we aim for the\nmodel to produce:\ntargets = torch.tensor([[3626, 6100, 345  ],  # ["" effort moves you"",\n                        [588,  428,  11311]]) #  "" really like chocolate""]\nNote that the targets are the inputs but shifted one position forward, a concept\nwe covered chapter 2 during the implementation of the data loader. This\nshifting strategy is crucial for teaching the model to predict the next token in\na sequence.\nWhen we feed the \ninputs\n into the model to calculate logit vectors for the two\ninput examples, each comprising three tokens, and apply the softmax\nfunction to transform these logit values into probability scores, which\ncorresponds to step 2 in Figure 5.4:\nwith torch.no_grad(): #A\n    logits = model(inputs)\nprobas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\nprint(probas.shape)\nThe resulting tensor dimension of the probability score (\nprobas\n) tensor is as\nfollows:\ntorch.Size([2, 3, 50257])\nThe first number, 2, corresponds to the two examples (rows) in the \ninputs\n,\nalso known as batch size. The second number, 3, corresponds to the number\nof tokens in each input (row). Finally, the last number corresponds to the\nembedding dimensionality, which is determined by the vocabulary size, as\ndiscussed in previous chapters.\nFollowing the conversion from logits to probabilities via the softmax\nfunction, the \ngenerate_text_simple\n function from chapter 4 then converts\nthe resulting probability scores back into text, as illustrated in steps 3-5 in\nFigure 5.4.\nWe can implement steps 3 and 4 by applying the argmax function to the\nprobability scores to obtain the corresponding token IDs:\ntoken_ids = torch.argmax(probas, dim=-1, keepdim=True)\nprint(""Token IDs:\n"", token_ids)\nGiven that we have 2 input batches, each containing 3 tokens, applying the\nargmax function to the probability scores (step 3 in Figure 5.4) yields 2 sets\nof outputs, each with 3 predicted token IDs:\nToken IDs:\n tensor([[[16657], # First batch\n         [  339],\n         [42826]],\n        [[49906], # Second batch\n         [29669],\n         [41751]]])\nFinally, step 5 converts the token IDs back into text:\nprint(f""Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}"")\nprint(f""Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}"")\nWhen we decode these tokens, we find that these output tokens are quite\ndifferent from the target tokens we want the model to generate:\nTargets batch 1:  effort moves you\nOutputs batch 1:  Armed heNetflix\nThe model produces random text that is different from the target text because\nit has not been trained yet. We now get to the part where we evaluate the\nperformance of the model's generated text numerically via a so-called loss as\nillustrated in Figure 5.4. Not only is this useful for measuring the quality of\nthe generated text, but it's also a building block for implementing the training\nfunction later, which we use to update the model's weight to improve the\ngenerated text.\nFigure 5.5 We now implement the text evaluation function in the remainder of this section. In the\nnext section, we apply this evaluation function to the entire dataset we use for model training.\nPart of the text evaluation process that we implement in the remainder of this\nsection, as shown in Figure 5.5, is to measure ""how far"" the generated tokens\nare from the correct predictions (targets). The training function we implement\nlater in this chapter will use this information to adjust the model weights to\ngenerate text that is more similar to (or ideally matches) the target text.\nThe model training aims to increase the softmax probability in the index\npositions corresponding to the correct target token IDs, as illustrated in\nFigure 5.6. This softmax probability is also used in the evaluation metric we\nare implementing in the remainder of this section to numerically assess the\nmodel's generated outputs: the higher the probability in the correct positions,\nthe better.\nFigure 5.6 Before training, the model produces random next-token probability vectors. The goal\nof model training is to ensure that the probability values corresponding to the highlighted target\ntoken IDs are maximized.\nRemember that Figure 5.6 displays the softmax probabilities for a compact 7-\ntoken vocabulary to fit everything into a single figure. This implies that the\nstarting random values will hover around 1/7, which equals approximately\n0.14.\nHowever, the vocabulary we are using for our GPT-2 model has 50,257\ntokens, so most of the initial probabilities will hover around 0.00002 via\n1/50,257.\nFor each of the two input texts, we can print the initial softmax probability\nscores corresponding to the target tokens via the following code:\ntext_idx = 0\ntarget_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(""Text 1:"", target_probas_1)\n \ntext_idx = 1\ntarget_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\nprint(""Text 2:"", target_probas_2)\nThe 3 target token ID probabilities for each batch are as follows:\nText 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\nText 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])\nThe goal of training an LLM is to maximize these values, aiming to get them\nas close to a probability of 1. This way, we ensure the LLM consistently\npicks the target token—essentially the next word in the sentence—as the next\ntoken it generates.\nBackpropagation\nHow do we maximize the softmax probability values corresponding to the\ntarget tokens? The big picture is that we update the model weights so that the\nmodel outputs higher values for the respective token IDs we want to generate.\nThe weight update is done via a process called \nbackpropagation\n, a standard\ntechnique for training deep neural networks (see sections A.3 to A.7 in\nAppendix A for more details about backpropagation and model training).\nBackpropagation requires a loss function, which calculates the difference\nbetween the model's predicted output (here, the probabilities corresponding to\nthe target token IDs) and the actual desired output. This loss function\nmeasures how far off the model's predictions are from the target values.\nIn the remainder of this section, we calculate the loss for the probability\nscores of the two example batches, \ntarget_probas_1\n and \ntarget_probas_2\n.\nThe main steps are illustrated in Figure 5.7.\nFigure 5.7 Calculating the loss involves several steps. Steps 1 to 3 calculate the token probabilities\ncorresponding to the target tensors. These probabilities are then transformed via a logarithm and\naveraged in steps 4-6.\nSince we already applied steps 1-3 listed in Figure 5.7 to obtain\ntarget_probas_1\n and \ntarget_probas_2\n, we proceed with step 4, applying\nthe \nlogarithm\n to the probability scores:\nlog_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\nprint(log_probas)\nThis results in the following values:\ntensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])\nWorking with logarithms of probability scores is more manageable in\nmathematical optimization than handling the scores directly. This topic is\noutside the scope of this book, but I've detailed it further in a lecture, which is\nlinked in the reference section in appendix B.\nNext, we combine these log probabilities into a single score by computing the\naverage (step 5 in Figure 5.7):\navg_log_probas = torch.mean(log_probas)\nprint(avg_log_probas)\nThe resulting average log probability score is as follows:\ntensor(-10.7722)\nThe goal is to get the average log probability as close to 0 as possible by\nupdating the model's weights as part of the training process, which we will\nimplement later in section 5.2.\nHowever, in deep learning, the common practice isn't to push the average log\nprobability up to 0 but rather to bring the negative average log probability\ndown to 0. The negative average log probability is simply the average log\nprobability multiplied by -1, which corresponds to step 6 in Figure 5.7:\nneg_avg_log_probas = avg_log_probas * -1\nprint(neg_avg_log_probas)\nThis prints \ntensor(-10.7722)\n.\nThe term for this negative value, -10.7722 turning into 10.7722, is known as\nthe \ncross entropy \nloss in deep learning.\nPyTorch comes in handy here, as it already has a built-in \ncross_entropy\nfunction that takes care of all these 6 steps in Figure 5.7 for us.\nCross entropy loss\nAt its core, the cross entropy loss is a popular measure in machine learning\nand deep learning that measures the difference between two probability\ndistributions--typically, the true distribution of labels (here, tokens in a\ndataset) and the predicted distribution from a model (for instance, the token\nprobabilities generated by an LLM).\nIn the context of machine learning and specifically in frameworks like\nPyTorch, the \ncross_entropy\n function computes this measure for discrete\noutcomes, which is similar to the negative average log probability of the\ntarget tokens given the model's generated token probabilities, making the\nterms cross entropy and negative average log probability related and often\nused interchangeably in practice.\nBefore we apply the cross entropy function, let's briefly recall the shape of\nthe logits and target tensors:\nprint(""Logits shape:"", logits.shape)\nprint(""Targets shape:"", targets.shape)\nThe resulting shapes are as follows:\nLogits shape: torch.Size([2, 3, 50257])\nTargets shape: torch.Size([2, 3])\nAs we can see, the \nlogits\n tensor has three dimensions: batch size, number of\ntokens, and vocabulary size. The \ntargets\n tensor has two dimensions: batch\nsize and number of tokens.\nFor the cross \nentropy_loss\n function in PyTorch, we want to flatten these\ntensors by combining them over the batch dimension:\nlogits_flat = logits.flatten(0, 1)\ntargets_flat = targets.flatten()\nprint(""Flattened logits:"", logits_flat.shape)\nprint(""Flattened targets:"", targets_flat.shape)\nThe resulting tensor dimensions are as follows:\nFlattened logits: torch.Size([6, 50257])\nFlattened targets: torch.Size([6])\nRemember that the \ntargets\n are the token IDs we want the LLM to generate,\nand the \nlogits\n contain the unscaled model outputs before they enter the\nsoftmax function to obtain the probability scores.\nPreviously, we applied the softmax function, selected the probability scores\ncorresponding to the target IDs, and computed the negative average log\nprobabilities. PyTorch's \ncross_entropy\n function will take care of all these\nsteps for us:\nloss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\nprint(loss)\nThe resulting loss is the same that we obtained previously when applying the\nindividual steps shown in Figure 5.7 manually:\ntensor(10.7722)\nPerplexity\nPerplexity\n is a measure often used alongside cross entropy loss to evaluate\nthe performance of models in tasks like language modeling. It can provide a\nmore interpretable way to understand the uncertainty of a model in predicting\nthe next token in a sequence.\nPerplexity measures how well the probability distribution predicted by the\nmodel matches the actual distribution of the words in the dataset. Similar to\nthe loss, a lower perplexity indicates that the model predictions are closer to\nthe actual distribution.\nPerplexity can be calculated as \nperplexity = torch.exp(loss)\n, which\nreturns \ntensor(47678.8633)\n when applied to the previously calculated loss.\nPerplexity is often considered more interpretable than the raw loss value\nbecause it signifies the effective vocabulary size about which the model is\nuncertain at each step. In the given example, this would translate to the model\nbeing unsure about which among 47,678 words or tokens in the vocabulary to\ngenerate as the next token.\nIn this section, we calculated the loss for two small text inputs for illustration\npurposes. In the next section, we apply the loss computation to the entire\ntraining and validation sets.\n5.1.3 Calculating the training and validation set losses\nIn this section, we first prepare the training and validation datasets that we\nwill use to train the LLM later in this chapter. Then, we calculate the cross\nentropy for the training and validation sets, as illustrated in Figure 5.8, which\nis an important component of the model training process.\nFigure 5.8 After computing the cross entropy loss in the previous section, we now apply this loss\ncomputation to the entire text dataset that we will use for model training.\n\nTo compute the loss on the training and validation datasets as illustrated in\nFigure 5.8, we use a very small text dataset, the ""The Verdict"" short story by\nEdith Wharton, which we have already worked with in chapter 2. By\nselecting a text from the public domain, we circumvent any concerns related\nto usage rights. Additionally, the reason why we use such a small dataset is\nthat it allows for the execution of code examples on a standard laptop\ncomputer in a matter of minutes, even without a high-end GPU, which is\nparticularly advantageous for educational purposes.\nInterested readers can also use the supplementary code of this book to\nprepare a larger-scale dataset consisting of more than 60,000 public domain\nbooks from Project Gutenberg and train an LLM on these (see appendix D for\ndetails).\nThe cost of pretraining LLMs\nTo put the scale of our project into perspective, consider the training of the 7\nbillion parameter Llama 2 model, a relatively popular openly available LLM.\nThis model required 184,320 GPU hours on expensive A100 GPUs,\nprocessing 2 trillion tokens. At the time of writing, running an 8xA100 cloud\nserver on AWS costs around $30 per hour. A rough estimate puts the total\ntraining cost of such an LLM at around $690,000 (calculated as 184,320\nhours divided by 8, then multiplied by $30).\nThe following code loads the ""The Verdict"" short story we used in chapter 2:\nfile_path = ""the-verdict.txt""\nwith open(file_path, ""r"", encoding=""utf-8"") as file:\n    text_data = file.read()\nAfter loading the dataset, we can check the number of characters and tokens\nin the dataset:\ntotal_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\nprint(""Characters:"", total_characters)\nprint(""Tokens:"", total_tokens)\nThe output is as follows:\nCharacters: 20479\nTokens: 5145\nWith just 5,145 tokens, the text might seem too small to train an LLM, but as\nmentioned earlier, it's for educational purposes so that we can run the code in\nminutes instead of weeks. Plus, we will be loading pretrained weights from\nOpenAI into our \nGPTModel\n code at the end of this chapter.\nNext, we divide the dataset into a training and a validation set and use the\ndata loaders from chapter 2 to prepare the batches for LLM training. This\nprocess is visualized in Figure 5.9.\nFigure 5.9 When preparing the data loaders, we split the input text into training and validation\nset portions. Then, we tokenize the text (only shown for the training set portion for simplicity)\nand divide the tokenized text into chunks of a user-specified length (here 6). Finally, we shuffle\nthe rows and organize the chunked text into batches (here, batch size 2), which we can use for\nmodel training.\nFor visualization purposes, Figure 5.9 uses a \nmax_length=6\n due to spatial\nconstraints. However, for the actual data loaders we are implementing, we set\nthe \nmax_length\n equal to the 256-token context length that the LLM supports\nso that the LLM sees longer texts during training.\nTraining with variable lengths\nWe are training the model with training data presented in similarly-sized\nchunks for simplicity and efficiency. However, in practice, it can also be\nbeneficial to train an LLM with variable-length inputs to help the LLM to\nbetter generalize across different types of inputs when it is being used.\nTo implement the data splitting and loading visualized in Figure 5.9, we first\ndefine a \ntrain_ratio\n to use 90% of the data for training and the remaining\n10% as validation data for model evaluation during training:\ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data = text_data[split_idx:]\nUsing the \ntrain_data\n and \nval_data\n subsets, we can now create the\nrespective data loader reusing the \ncreate_dataloader_v1\n code from chapter\n2:\nfrom chapter02 import create_dataloader_v1\ntorch.manual_seed(123)\n \ntrain_loader = create_dataloader_v1(\n    train_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[""context_length""],\n    stride=GPT_CONFIG_124M[""context_length""],\n    drop_last=True,\n    shuffle=True\n)\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[""context_length""],\n    stride=GPT_CONFIG_124M[""context_length""],\n    drop_last=False,\n    shuffle=False\n)\nWe used a relatively small batch size in the preceding code to reduce the\ncomputational resource demand because we were working with a very small\ndataset. In practice, training LLMs with batch sizes of 1,024 or larger is not\nuncommon.\nAs an optional check, we can iterate through the data loaders to ensure that\nthey were created correctly:\nprint(""Train loader:"")\nfor x, y in train_loader:\n    print(x.shape, y.shape)\n \nprint(""\nValidation loader:"")\nfor x, y in val_loader:\n    print(x.shape, y.shape)\nWe should see the following outputs:\nTrain loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\ntorch.Size([2, 256]) torch.Size([2, 256])\n \nValidation loader:\ntorch.Size([2, 256]) torch.Size([2, 256])\nBased on the preceding code output, we have 9 training set batches with 2\nsamples and 256 tokens each. Since we allocated only 10% of the data for\nvalidation, there is only one validation batch consisting of 2 input examples.\nAs expected, the input data (\nx\n) and target data (\ny\n) have the same shape (the\nbatch size times the number of tokens in each batch) since the targets are the\ninputs shifted by one position, as discussed in chapter 2.\nNext, we implement a utility function to calculate the cross entropy loss of a\ngiven batch returned via the training and validation loader:\ndef calc_loss_batch(input_batch, target_batch, model, device):\n    input_batch, target_batch = input_batch.to(device), target_batch.to(device) #A\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(\n        logits.flatten(0, 1), target_batch.flatten()\n    )\n    return loss\nWe can now use this \ncalc_loss_batch\n utility function, which computes the\nloss for a single batch, to implement the following \ncalc_loss_loader\nfunction that computes the loss over all the batches sampled by a given data\nloader:\nListing 5.2 Function to compute the training and validation loss\ndef calc_loss_loader(data_loader, model, device, num_batches=None):\n    total_loss = 0.\n    if num_batches is None:\n        num_batches = len(data_loader) #A\n    else:\n        num_batches = min(num_batches, len(data_loader)) #B\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i < num_batches:\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            total_loss += loss.item() #C\n        else:\n            break\n    return total_loss / num_batches #D\nBy default, the \ncalc_loss_batch\n function iterates over all batches in a given\ndata loader, accumulates the loss in the \ntotal_loss\n variable, and then\ncomputes and averages the loss over the total number of batches.\nAlternatively, we can specify a smaller number of batches via \nnum_batches\nto speed up the evaluation during model training.\nLet's now see this \ncalc_loss_batch\n function in action, applying it to the\ntraining and validation set loaders:\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") #A\nmodel.to(device)\ntrain_loss = calc_loss_loader(train_loader, model, device) #B\nval_loss = calc_loss_loader(val_loader, model, device)\nprint(""Training loss:"", train_loss)\nprint(""Validation loss:"", val_loss)\nThe resulting loss values are as follows:\nTraining loss: 10.98758347829183\nValidation loss: 10.98110580444336\nThe loss values are relatively high because the model has not yet been\ntrained. For comparison, the loss approaches 0 if the model learns to generate\nthe next tokens as they appear in the training and validation sets.\nNow that we have a way to measure the quality of the generated text, in the\nnext section, we train the LLM to reduce this loss so that it becomes better at\ngenerating text, as illustrated in Figure 5.10.\nFigure 5.10 We have recapped the text generation process and implemented basic model\nevaluation techniques to compute the training and validation set losses. Next, we will go to the\ntraining functions and pretrain the LLM.\nAs shown in Figure 5.10, the next section focuses on pretraining the LLM.\nAfter model training, we implement alternative text generation strategies and\nsave and load pretrained model weights.\n5.2 Training an LLM\nIn this section, we finally implement the code for pretraining the LLM, our\nGPTModel\n. For this, we focus on a straightforward training loop, as illustrated\nin Figure 5.11, to keep the code concise and readable. However, interested\nreaders can learn about more advanced techniques, including l\nearning rate\nwarmup\n, \ncosine annealing\n, and \ngradient clipping\n, in \nAppendix D, Adding\nBells and Whistles to the Training Loop.\nFigure 5.11 A typical training loop for training deep neural networks in PyTorch consists of\nseveral steps, iterating over the batches in the training set for several epochs. In each loop, we\ncalculate the loss for each training set batch to determine loss gradients, which we use to update\nthe model weights so that the training set loss is minimized.\nThe flowchart in Figure 5.11 depicts a typical PyTorch neural network\ntraining workflow, which we use for training an LLM. It outlines eight steps,\nstarting with iterating over each epoch, processing batches, resetting and\ncalculating gradients, updating weights, and concluding with monitoring\nsteps like printing losses and generating text samples. If you are relatively\nnew to training deep neural networks with PyTorch and any of these steps are\nunfamiliar, consider reading sections A.5 to A.8 in \nAppendix A, Introduction\nto PyTorch\n.\nIn code, we can implement this training flow via the following\ntrain_model_simple\n function:\nListing 5.3 The main function for pretraining LLMs\ndef train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n                       eval_freq, eval_iter, start_context):\n    train_losses, val_losses, track_tokens_seen = [], [], [] #A\n    tokens_seen, global_step = 0, -1\n \n    for epoch in range(num_epochs): #B\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad() #C\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward() #D\n            optimizer.step() #E\n            tokens_seen += input_batch.numel()\n            global_step += 1\n \n            if global_step % eval_freq == 0: #F\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader, device, eval_iter)\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f""Ep {epoch+1} (Step {global_step:06d}): ""\n                      f""Train loss {train_loss:.3f}, Val loss {val_loss:.3f}"")\n \n        generate_and_print_sample(  #G\n            model, train_loader.dataset.tokenizer, device, start_context\n        )\n    return train_losses, val_losses, track_tokens_seen\nNote that the \ntrain_model_simple\n function we just created uses two\nfunctions we have not defined yet: \nevaluate_model\n and\ngenerate_and_print_sample\n.\nThe \nevaluate_model\n function corresponds to step 7 in Figure 5.11. It prints\nthe training and validation set losses after each model update so we can\nevaluate whether the training improves the model.\nMore specifically, the \nevaluate_model\n function calculates the loss over the\ntraining and validation set while ensuring the model is in evaluation mode\nwith gradient tracking and dropout disabled when calculating the loss over\nthe training and validation sets:\ndef evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval() #A\n    with torch.no_grad(): #B\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    model.train()\n    return train_loss, val_loss\nSimilar to \nevaluate_model\n, the \ngenerate_and_print_sample\n function is a\nconvenience function that we use to track whether the model improves during\nthe training. In particular, the \ngenerate_and_print_sample\n function takes a\ntext snippet (\nstart_context\n) as input, converts it into token IDs, and feeds it\nto the LLM to generate a text sample using the \ngenerate_text_simple\nfunction we used earlier:\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n        print(decoded_text.replace(""\n"", "" ""))  # Compact print format\n    model.train()\nWhile the \nevaluate_model\n function gives us a numeric estimate of the\nmodel's training progress, this \ngenerate_and_print_sampl\ne text function\nprovides a concrete text example generated by the model to judge its\ncapabilities during training.\nAdamW\nAdam\n optimizers are a popular choice for training deep neural networks.\nHowever, in our training loop, we opt for the \nAdamW\n optimizer. AdamW is a\nvariant of Adam that improves the weight decay approach, which aims to\nminimize model complexity and prevent overfitting by penalizing larger\nweights. This adjustment allows AdamW to achieve more effective\nregularization and better generalization and is thus frequently used in the\ntraining of LLMs.\nLet's see this all in action by training a GPTModel instance for 10 epochs\nusing an AdamW optimizer and the \ntrain_model_simple\n function we\ndefined earlier.\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\nnum_epochs = 10\ntrain_losses, val_losses, tokens_seen = train_model_simple(\n    model, train_loader, val_loader, optimizer, device,\n    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n    start_context=""Every effort moves you""\n)\nExecuting the \ntraining_model_simple\n function starts the training process,\nwhich takes about 5 minutes on a MacBook Air or a similar laptop to\ncomplete. The output printed during this execution is as follows:\nEp 1 (Step 000000): Train loss 9.781, Val loss 9.933\nEp 1 (Step 000005): Train loss 8.111, Val loss 8.339\nEvery effort moves you,,,,,,,,,,,,.                                     \nEp 2 (Step 000010): Train loss 6.661, Val loss 7.048\nEp 2 (Step 000015): Train loss 5.961, Val loss 6.616\nEvery effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n[...]  #A\nEp 9 (Step 000080): Train loss 0.541, Val loss 6.393\nEvery effort moves you?""  ""Yes--quite insensible to the irony. She wanted him vindicated--and by me!""  He laughed again, and threw back the window-curtains, I had the donkey. ""There were days when I\nEp 10 (Step 000085): Train loss 0.391, Val loss 6.452\nEvery effort moves you know,"" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\nAs we can see, based on the results printed during the training, the training\nloss improves drastically, starting with a value of 9.558 and converging to\n0.762. The language skills of the model have improved quite a lot. In the\nbeginning, the model is only able to append commas to the start context\n(\n""Every effort moves you,,,,,,,,,,,,""\n) or repeat the word \n""and""\n. At the\nend of the training, it can generate grammatically correct text.\nSimilar to the training set loss, we can see that the validation loss starts high\n(9.856) and decreases during the training. However, it never becomes as\nsmall as the training set loss and remains at 6.372 after the 10th epoch.\nBefore discussing the validation loss in more detail, let's create a simple plot\nthat shows the training and validation set losses side by side:\nimport matplotlib.pyplot as plt\ndef plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n    fig, ax1 = plt.subplots(figsize=(5, 3))\n    ax1.plot(epochs_seen, train_losses, label=""Training loss"")\n    ax1.plot(epochs_seen, val_losses, linestyle=""-."", label=""Validation loss"")\n    ax1.set_xlabel(""Epochs"")\n    ax1.set_ylabel(""Loss"")\n    ax1.legend(loc=""upper right"")\n    ax2 = ax1.twiny()  #A\n    ax2.plot(tokens_seen, train_losses, alpha=0)  #B\n    ax2.set_xlabel(""Tokens seen"")\n    fig.tight_layout()\n    plt.show()\n \nepochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\nplot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\nThe resulting training and validation loss plot is shown in Figure 5.12.\nFigure 5.12 At the beginning of the training, we observe that both the training and validation set\nlosses sharply decrease, which is a sign that the model is learning. However, the training set loss\ncontinues to decrease past the second epoch, whereas the validation loss stagnates. This is a sign\nthat the model is still learning, but it's overfitting to the training set past epoch 2.\nAs Figure 5.12 shows, both the training and validation losses start to improve\nfor the first epoch. However, the losses start to diverge past the second epoch.\nThis divergence and the fact that the validation loss is much larger than the\ntraining loss indicate that the model is overfitting to the training data. We can\nconfirm that the model memorizes the training data verbatim by searching for\nthe generated text snippets, such as \n""quite insensible to the irony"" \nin\nthe \n""The Verdict""\n text file.\nThis memorization is expected since we are working with a very, very small\ntraining dataset and training the model for multiple epochs. Usually, it's\ncommon to train a model on a much, much larger dataset for only one epoch.\nAs mentioned earlier, interested readers can try to train the model on 60,000\npublic domain books from Project Gutenberg, where this overfitting does not\noccur; see appendix B for details.\nIn the upcoming section, as shown in Figure 5.13, we explore sampling\nmethods employed by LLMs to mitigate memorization effects, resulting in\nmore novel generated text.\nFigure 5.13 Our model can generate coherent text after implementing the training function.\nHowever, it often memorizes passages from the training set verbatim. The following section\ncovers strategies to generate more diverse output texts.\nAs illustrated in Figure 5.13, the next section will cover text generation\nstrategies for LLM to reduce training data memorization and increase the\noriginality of the LLM-generated text before we cover weight loading and\nsaving and loading pretrained weights from OpenAI's GPT model.\n5.3 Decoding strategies to control randomness\nIn this section, we will cover text generation strategies (also called decoding\nstrategies) to generate more original text. First, we briefly revisit the\ngenerate_text_simple\n function from the previous chapter that we used\ninside the \ngenerate_and_print_sample\n earlier in this chapter. Then, we will\ncover two techniques,\n temperature scaling\n, and \ntop-k sampling\n, to improve\nthis function.\nWe begin by transferring the model back from the GPU to the CPU since\ninference with a relatively small model does not require a GPU. Also, after\ntraining, we put the model into evaluation model to turn off random\ncomponents such as dropout:\nmodel.to(""cpu"")\nmodel.eval()\nNext, we plug the \nGPTModel\n instance (\nmodel\n) into the \ngenerate_text_simple\nfunction, which uses the LLM to generate one token at a time:\ntokenizer = tiktoken.get_encoding(""gpt2"")\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(""Every effort moves you"", tokenizer),\n    max_new_tokens=25,\n    context_size=GPT_CONFIG_124M[""context_length""]\n)\nprint(""Output text:\n"", token_ids_to_text(token_ids, tokenizer))\nThe generated text is as follows:\nOutput text:\nEvery effort moves you know,"" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\nAs explained earlier in section 5.1.2, the generated token is selected at each\ngeneration step corresponding to the largest probability score among all\ntokens in the vocabulary.\nThis means that the LLM will always generate the same outputs even if we\nrun the \ngenerate_text_simple\n function above multiple times on the same\nstart context (\n""Every effort moves you""\n).\nThe following subsections introduce two concepts to control the randomness\nand diversity of the generated text: temperature scaling and top-k sampling.\n5.3.1 Temperature scaling\nThis section introduces temperature scaling, a technique that adds a\nprobabilistic selection process to the next-token generation task.\nPreviously, inside the \ngenerate_text_simple\n function, we always sampled\nthe token with the highest probability as the next token using \ntorch.argmax\n,\nalso known as \ngreedy decoding\n. To generate text with more variety, we can\nreplace the argmax with a function that samples from a probability\ndistribution (here, the probability scores the LLM generates for each\nvocabulary entry at each token generation step).\nTo illustrate the probabilistic sampling with a concrete example, let's briefly\ndiscuss the next-token generation process using a very small vocabulary for\nillustration purposes:\nvocab = { \n    ""closer"": 0,\n    ""every"": 1, \n    ""effort"": 2, \n    ""forward"": 3,\n    ""inches"": 4,\n    ""moves"": 5, \n    ""pizza"": 6,\n    ""toward"": 7,\n    ""you"": 8,\n} \ninverse_vocab = {v: k for k, v in vocab.items()}\nNext, assume the LLM is given the start context \n""every effort moves you""\nand generates the following next-token logits:\nnext_token_logits = torch.tensor(\n    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n)\nAs discussed in the previous chapter, Inside the \ngenerate_text_simple\n, we\nconvert the logits into probabilities via the softmax function and obtain the\ntoken ID corresponding the generated token via the argmax function, which\nwe can then map back into text via the inverse vocabulary:\nprobas = torch.softmax(next_token_logits, dim=0)\nnext_token_id = torch.argmax(probas).item()\nprint(inverse_vocab[next_token_id])\nSince the largest logit value, and correspondingly the largest softmax\nprobability score, is in the fourth position (index position 3 since Python uses\n0-indexing), the generated word is ""\nforward""\n.\nTo implement a probabilistic sampling process, we can now replace the\nargmax with the \nmultinomial\n function in PyTorch:\ntorch.manual_seed(123) \nnext_token_id = torch.multinomial(probas, num_samples=1).item()\nprint(inverse_vocab[next_token_id])\nThe printed output is \n""forward""\n just like before. What happened? The\nmultinomial\n function samples the next token proportional to its probability\nscore. In other words, \n""forward""\n is still the most likely token and will be\nselected by \nmultinomial\n most of the time but not all the time. To illustrate\nthis, let's implement a function that repeats this sampling 1000 times:\ndef print_sampled_tokens(probas):\n    torch.manual_seed(123)\n    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n    sampled_ids = torch.bincount(torch.tensor(sample))\n    for i, freq in enumerate(sampled_ids):\n        print(f""{freq} x {inverse_vocab[i]}"")\nprint_sampled_tokens(probas)\nThe sampling output is as follows:\n73 x closer\n0 x every\n0 x effort\n582 x forward\n2 x inches\n0 x moves\n0 x pizza\n343 x toward\nAs we can see based on the output, the word \n""forward""\n is sampled most of\nthe time (582 out of 1000 times), but other tokens such as \n""closer""\n,\n""inches""\n, and \n""toward""\n will also be sampled some of the time. This means\nthat if we replaced the \nargmax\n function with the \nmultinomial\n function inside\nthe \ngenerate_and_print_sample\n function, the LLM would sometimes\ngenerate texts such as ""\nevery effort moves you toward\n"", ""\nevery effort\nmoves you inches\n"", and ""\nevery effort moves you closer""\n instead of\n""\nevery effort moves you forward\n"".\nWe can further control the distribution and selection process via a concept\ncalled temperature scaling, where \ntemperature scaling\n is just a fancy\ndescription for dividing the logits by a number greater than 0:\ndef softmax_with_temperature(logits, temperature):\n    scaled_logits = logits / temperature\n    return torch.softmax(scaled_logits, dim=0)\nTemperatures greater than 1 result in more uniformly distributed token\nprobabilities, and Temperatures smaller than 1 will result in more confident\n(sharper or more peaky) distributions. Let's illustrate this by plotting the\noriginal probabilities alongside probabilities scaled with different temperature\nvalues:\ntemperatures = [1, 0.1, 5]  # Original, higher, and lower temperature\nscaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\nx = torch.arange(len(vocab))\nbar_width = 0.15\nfig, ax = plt.subplots(figsize=(5, 3))\nfor i, T in enumerate(temperatures):\n    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n                   bar_width, label=f'Temperature = {T}')\nax.set_ylabel('Probability')\nax.set_xticks(x)\nax.set_xticklabels(vocab.keys(), rotation=90)\nax.legend()\nplt.tight_layout()\nplt.show()\nThe resulting plot is shown in Figure 5.14.\nFigure 5.14 A temperature of 1 represents the unscaled probability scores for each token in the\nvocabulary. Decreasing the temperature to 0.1 sharpens the distribution, so the most likely token\n(here ""forward"") will have an even higher probability score. Vice versa, increasing the\ntemperature to 5 makes the distribution more uniform.\n\nA temperature of 1 divides the logits by 1 before passing them to the softmax\nfunction to compute the probability scores. In other words, using a\ntemperature of 1 is the same as not using any temperature scaling. In this\ncase, the tokens are selected with a probability equal to the original softmax\nprobability scores via the \nmultinomial\n sampling function in PyTorch.\nFor example, for the temperature setting 1, the token corresponding to\n""forward"" would be selected with about 60% of the time, as we can see in\nFigure 5.14.\nAlso, as we can see in Figure 5.14, applying very small temperatures, such as\n0.1, will result in sharper distributions such that the behavior of the\nmultinomial\n function selects the most likely token (here: \n""forward""\n) almost\n100% of the time, approaching the behavior of the argmax function. Vice\nversa, a temperature of 5 results in a more uniform distribution where other\ntokens are selected more often. This can add more variety to the generated\ntexts but also more often results in nonsensical text. For example, using the\ntemperature of 5 results in texts such as \n""every effort moves you pizza""\nabout 4% of the time.\nExercise 5.1\nUse the \nprint_sampled_tokens\n function to print the sampling frequencies of\nthe softmax probabilities scaled with the temperatures shown in Figure 5.13.\nHow often is the word \n""pizza""\n sampled in each case? Can you think of a\nfaster and more accurate way to determine how often the word \n""pizza""\n is\nsampled?\n5.3.2 Top-k sampling\nIn the previous section, we implemented a probabilistic sampling approach\ncoupled with temperature scaling to increase the diversity of the outputs. We\nsaw that higher temperature values result in more uniformly distributed next-\ntoken probabilities, which result in more diverse outputs as it reduces the\nlikelihood of the model repeatedly selecting the most probable token. This\nmethod allows for exploring less likely but potentially more interesting and\ncreative paths in the generation process. However, One downside of this\napproach is that it sometimes leads to grammatically incorrect or completely\nnonsensical outputs such as \n""every effort moves you pizza"".\nIn this section, we introduce another concept called \ntop-k sampling\n, which,\nwhen combined with probabilistic sampling and temperature scaling, can\nimprove the text generation results.\nIn top-k sampling, we can restrict the sampled tokens to the top-k most likely\ntokens and exclude all other tokens from the selection process by masking\ntheir probability scores, as illustrated in Figure 5.15.\nFigure 5.15 Using top-k sampling with k=3, we focus on the 3 tokens associated with the highest\nlogits and mask out all other tokens with negative infinity (-inf) before applying the softmax\nfunction. This results in a probability distribution with a probability value 0 assigned to all non-\ntop-k tokens.\nThe approach outlined in Figure 5.15 replaces all non-selected logits with\nnegative infinity value (\n-inf\n), such that when computing the softmax values,\nthe probability scores of the non-top-k tokens are 0, and the remaining\nprobabilities sum up to 1. (Careful readers may remember this masking trick\nfrom the causal attention module we implemented in chapter 3 in section\n3.5.1 \nApplying a causal attention mask\n.)\nIn code, we can implement the top-k procedure outlined in Figure 5.15 as\nfollows, starting with the selection of the tokens with the largest logit values:\ntop_k = 3\ntop_logits, top_pos = torch.topk(next_token_logits, top_k)\nprint(""Top logits:"", top_logits)\nprint(""Top positions:"", top_pos)\nThe logits values and token IDs of the top 3 tokens, in descending order, are\nas follows:\nTop logits: tensor([6.7500, 6.2800, 4.5100])\nTop positions: tensor([3, 7, 0])\nSubsequently, we apply PyTorch's \nwhere\n function to set the logit values of\ntokens that are below the lowest logit value within our top-3 selection to\nnegative infinity (\n-inf\n).\nnew_logits = torch.where(\n    condition=next_token_logits < top_logits[-1],  #A\n    input=torch.tensor(float('-inf')),  #B\n    other=next_token_logits  #C\n)\nprint(new_logits)\nThe resulting logits for the next token in the 9-token vocabulary are as\nfollows:\ntensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\nLastly, let's apply the softmax function to turn these into next-token\nprobabilities:\ntopk_probas = torch.softmax(new_logits, dim=0)\nprint(topk_probas)\nAs we can see, the result of this top-3 approach are 3 non-zero probability\nscores:\ntensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\nWe can now apply the temperature scaling and multinomial function for\nprobabilistic sampling introduced in the previous section to select the next\ntoken among these 3 non-zero probability scores to generate the next token.\nWe do this in the next section by modifying the text generation function.\n5.3.3 Modifying the text generation function\nThe previous two subsections introduced two concepts to increase the\ndiversity of LLM-generated text: temperature sampling and top-k sampling.\nIn this section, we combine and add these concepts to modify the\ngenerate_simple\n function we used to generate text via the LLM earlier,\ncreating a new \ngenerate\n function:\nListing 5.4 A modified text generation function with more diversity\ndef generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):\n    for _ in range(max_new_tokens):  #A\n        idx_cond = idx[:, -context_size:]\n        with torch.no_grad():\n            logits = model(idx_cond)\n        logits = logits[:, -1, :]\n        if top_k is not None:  #B\n            top_logits, _ = torch.topk(logits, top_k)\n            min_val = top_logits[:, -1]\n            logits = torch.where(\n                logits < min_val,\n                torch.tensor(float('-inf')).to(logits.device),\n                logits\n            )\n        if temperature > 0.0:  #C\n            logits = logits / temperature\n            probs = torch.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n        else:  #D\n            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx\nLet's now see this new \ngenerate\n function in action:\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=model,\n    idx=text_to_token_ids(""Every effort moves you"", tokenizer),\n    max_new_tokens=15,\n    context_size=GPT_CONFIG_124M[""context_length""],\n    top_k=25,\n    temperature=1.4\n)\nprint(""Output text:\n"", token_ids_to_text(token_ids, tokenizer))\nThe generated text is as follows:\nOutput text:\n Every effort moves you stand to work on surprise, a one of us had gone with random-\nAs we can see, the generated text is very different from the one we previously\ngenerated via the \ngenerate_simple\n function at the beginning of section 5.3\n(\n""Every effort moves you know,"" was one of the axioms he\nlaid...!""\n), which was a memorized passage from the training set.\nExercise 5.2\nPlay around with different temperatures and top-k settings. Based on your\nobservations, can you think of applications where lower temperature and top-\nk settings are desired? Vice versa, can you think of applications where higher\ntemperature and top-k settings are preferred? (It's recommended to also\nrevisit this exercise at the end of the chapter after loading the pretrained\nweights from OpenAI.)\nExercise 5.3\nWhat are the different combinations of settings for the \ngenerate\n function to\nforce deterministic behavior, that is, disabling the random sampling such that\nit always produces the same outputs similar to the \ngenerate_simple\nfunction?\nSo far, we covered how to pretrain LLMs and use them to generate text. The\nlast two sections of this chapter will discuss how we save and load the trained\nLLM and how we load pretrained weights from OpenAI.\n5.4 Loading and saving model weights in PyTorch\nIn this chapter, we have discussed how to numerically evaluate the training\nprogress and pretrain an LLM from scratch. Even though both the LLM and\ndataset were relatively small, this exercise showed that pretraining LLMs is\ncomputationally expensive. Thus, it is important to be able to save the LLM\nso that we don't have to rerun the training every time we want to use it in a\nnew session.\nAs illustrated in the chapter overview in Figure 5.16, we cover how to save\nand load a pretrained model in this section. Then, in the upcoming section,\nwe will load a more capable pretrained GPT model from OpenAI into our\nGPTModel\n instance.\nFigure 5.16 After training and inspecting the model, it is often helpful to save the model so that\nwe can use or continue training it later, which is the topic of this section before we load the\npretrained model weights from OpenAI in the final section of this chapter.\nFortunately, saving a PyTorch model is relatively straightforward. The\nrecommended way is to save a model's so-called \nstate_dict\n, a dictionary\nmapping each layer to its parameters, using the \ntorch.save\n function as\nfollows:\ntorch.save(model.state_dict(), ""model.pth"")\nIn the preceding code, \n""model.pth""\n is the filename where the \nstate_dict\n is\nsaved. The \n.pth\n extension is a convention for PyTorch files, though we could\ntechnically use any file extension.\nThen, after saving the model weights via the \nstate_dict\n, we can load the\nmodel weights into a new \nGPTModel\n model instance as follows:\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(torch.load(""model.pth""))\nmodel.eval()\nAs discussed in chapter 4, dropout helps prevent the model from overfitting\nto the training data by randomly ""dropping out"" of a layer's neurons during\ntraining. However, during inference, we don't want to randomly drop out any\nof the information the network has learned. Using \nmodel.eval()\n switches the\nmodel to evaluation mode for inference, disabling the dropout layers of the\nmodel\n.\nIf we plan to continue pretraining a model later, for example, using the\ntrain_model_simple\n function we defined earlier in this chapter, saving the\noptimizer state is also recommended.\nAdaptive optimizers such as AdamW store additional parameters for each\nmodel weight. AdamW uses historical data to adjust learning rates for each\nmodel parameter dynamically. Without it, the optimizer resets, and the model\nmay learn suboptimally or even fail to converge properly, which means that it\nwill lose the ability to generate coherent text. . Using \ntorch.save\n, we can\nsave both the model and optimizer \nstate_dict\n contents as follows:\ntorch.save({\n    ""model_state_dict"": model.state_dict(),\n    ""optimizer_state_dict"": optimizer.state_dict(),\n    }, \n    ""model_and_optimizer.pth""\n)\nThen, we can restore the model and optimizer states as follows by first\nloading the saved data via \ntorch.load\n and then using the \nload_state_dict\nmethod:\ncheckpoint = torch.load(""model_and_optimizer.pth"")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[""model_state_dict""])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[""optimizer_state_dict""])\nmodel.train();\nExercise 5.4\nAfter saving the weights, load the model and optimizer in a new Python\nsession or Jupyter notebook file and continue pretraining it for 1 more epoch\nusing the \ntrain_model_simple\n function.\n5.5 Loading pretrained weights from OpenAI\nPreviously, for educational purposes, we trained a small GPT-2 model using\na limited dataset comprising a short-story book. This approach allowed us to\nfocus on the fundamentals without the need for extensive time and\ncomputational resources.\nFortunately, OpenAI openly shared the weights of their GPT-2 models, thus\neliminating the need to invest tens to hundreds of thousands of dollars in\nretraining the model on a large corpus ourselves.\nIn the remainder of this section, we load these weights into our GPTModel\nclass and use the model for text generation. Here, \nweights\n refer to the weight\nparameters that are stored in the \n.weight\n attributes of PyTorch's \nLinear\n and\nEmbedding\n layers, for example. We accessed them earlier via\nmodel.parameters()\n when training the model.\nIn the next chapters, we will reuse these pretrained weights to finetune the\nmodel for a text classification task and follow instructions similar to\nChatGPT.\nNote that OpenAI originally saved the GPT-2 weights via TensorFlow, which\nwe have to install to load the weights in Python. Moreover, the following\ncode will use a progress bar tool called \ntqdm\n to track the download process,\nwhich we also have to install.\nYou can install these libraries by executing the following command in your\nterminal:\npip install tensorflow>=2.15.0  tqdm>=4.66\nThe download code is relatively long, mostly boilerplate, and not very\ninteresting. Hence, instead of devoting precious space in this chapter to\ndiscussing Python code for fetching files from the internet, we download the\ngpt_download.py\n Python module directly from this chapter's online\nrepository:\nimport urllib.request\nurl = (\n    ""https://raw.githubusercontent.com/rasbt/""\n    ""LLMs-from-scratch/main/ch05/""\n    ""01_main-chapter-code/gpt_download.py""\n)\nfilename = url.split('/')[-1]\nurllib.request.urlretrieve(url, filename)\nNext, after downloading this file to the local directory of your Python\nsession, readers are encouraged to briefly inspect the contents of this file to\nensure that it was saved correctly and contains valid Python code.\nWe can now import the \ndownload_and_load_gpt2\n function from the\ngpt_download.py\n file as follows, which will load the GPT-2 architecture\nsettings (\nsettings\n) and weight parameters (\nparams\n) into our Python session:\nfrom gpt_download import download_and_load_gpt2\nsettings, params = download_and_load_gpt2(model_size=""124M"", models_dir=""gpt2"")\nExecuting the proceeding codel downloads the following 7 files associated\nwith the 124M parameter GPT-2 model:\ncheckpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 63.9kiB/s]\nencoder.json: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00, 2.20MiB/s]\nhprams.json: 100%|██████████████████████████| 90.0/90.0 [00:00<00:00, 78.3kiB/s]\nmodel.ckpt.data-00000-of-00001: 100%|███████| 498M/498M [01:09<00:00, 7.16MiB/s]\nmodel.ckpt.index: 100%|█████████████████████| 5.21k/5.21k [00:00<00:00, 3.24MiB/s]\nmodel.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 2.46MiB/s]\nvocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 1.70MiB/s]\nUpdated download instructions\nIf the download code does not work for you, it could be due to intermittent\ninternet connection, server issues, or changes in how OpenAI shares the\nweights of the open-source GPT-2 model. In this case, please visit this\nchapter's online code repository at \nhttps://github.com/rasbt/LLMs-from-\nscratch\n for alternative and updated instructions, and please reach out via the\nManning Forum for further questions.\nAfter the execution of the previous code has been completed, let's inspect the\ncontents of \nsettings\n and \nparams\n:\nprint(""Settings:"", settings)\nprint(""Parameter dictionary keys:"", params.keys())\nThe contents are as follows:\nSettings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\nParameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\nBoth \nsettings\n and \nparams\n are Python dictionaries. The \nsettings\n dictionary\nstores the LLM architecture settings similarly to our manually defined\nGPT_CONFIG_124M\n settings. The \nparams\n dictionary contains the actual weight\ntensors. Note that we only printed the dictionary keys because printing the\nweight contents would take up too much screen space, however, we can\ninspect these weight tensors by printing the whole dictionary via\nprint(params)\n or by selecting individual tensors via the respective\ndictionary keys, for example, the embedding layer weights:\nprint(params[""wte""])\nprint(""Token embedding weight tensor dimensions:"", params[""wte""].shape)\nThe weights of the token embedding layer are as follows:\n[[-0.11010301 ... -0.1363697   0.01506208   0.04531523]\n [ 0.04034033 ...  0.08605453  0.00253983   0.04318958]\n [-0.12746179  ...  0.08991534 -0.12972379 -0.08785918]\n ...\n [-0.04453601 ...   0.10435229  0.09783269 -0.06952604]\n [ 0.1860082  ...  -0.09625227  0.07847701 -0.02245961]\n [ 0.05135201 ...   0.00704835  0.15519823  0.12067825]]\nToken embedding weight tensor dimensions: (50257, 768)\nWe downloaded and loaded the weights of the smallest GPT-2 model via the\ndownload_and_load_gpt2(model_size=""124M"", ...)\n setting. However,\nnote that OpenAI also shares the weights of larger models: \n""355M""\n, \n""774M""\n,\nand \n""1558M""\n. The overall architecture of these differently-sized GPT models\nis the same, as illustrated in Figure 5.17.\nFigure 5.17 GPT-2 LLMs come in several different model sizes, ranging from 124 million to 1,558\nmillion parameters. The core architecture is the same, with the only difference being the\nembedding sizes and the number of times individual components like the attention heads and\ntransformer blocks are repeated.\nAs illustrated in Figure 5.17, the overall architecture of the differently-sized\nGPT-2 models remains the same, except that different architectural elements\nare repeated different numbers of times, and the embedding size differs. The\nremaining code in this chapter is also compatible with these larger models.\nAfter loading the GPT-2 model weights into Python, we still need to transfer\nthem from the \nsettings\n and \nparams\n dictionaries into our \nGPTModel\n instance.\nFirst, we create a dictionary that lists the differences between the different\nGPT model sizes, as explained in Figure 5.17:\nmodel_configs = {\n    ""gpt2-small (124M)"": {""emb_dim"": 768, ""n_layers"": 12, ""n_heads"": 12},\n    ""gpt2-medium (355M)"": {""emb_dim"": 1024, ""n_layers"": 24, ""n_heads"": 16},\n    ""gpt2-large (774M)"": {""emb_dim"": 1280, ""n_layers"": 36, ""n_heads"": 20},\n    ""gpt2-xl (1558M)"": {""emb_dim"": 1600, ""n_layers"": 48, ""n_heads"": 25},\n}\nSuppose we are interested in loading the smallest model, \n""gpt2-small\n(124M)""\n. We can use the corresponding settings from the \nmodel_configs\ntable able to update our full-length \nGPT_CONFIG_124M\n we defined and used\nearlier throughout the chapter as follows:\nmodel_name = ""gpt2-small (124M)""\nNEW_CONFIG = GPT_CONFIG_124M.copy()\nNEW_CONFIG.update(model_configs[model_name])\nCareful readers may remember that we used a 256-token length earlier, but\nthe original GPT-2 models from OpenAI were trained with a 1,024-token\nlength, so we have to update the \nNEW_CONFIG\n accordingly:\nNEW_CONFIG.update({""context_length"": 1024})\nAlso, OpenAI used bias vectors in the multi-head attention module's linear\nlayers to implement the query, key, and value matrix computations. Bias\nvectors are not commonly used in LLMs anymore as they don't improve the\nmodeling performance and are thus unnecessary. However, since we are\nworking with pretrained weights, we need to match the settings for\nconsistency and enable these bias vectors:\nNEW_CONFIG.update({""qkv_bias"": True})\nWe can now use the updated \nNEW_CONFIG\n dictionary to initialize a new\nGPTModel\n instance:\ngpt = GPTModel(NEW_CONFIG)\ngpt.eval()\nBy default, the \nGPTModel\n instance is initialized with random weights for\npretraining. The last step to using OpenAI's model weights is to override\nthese random weights with the weights we loaded into the \nparams\n dictionary.\nFor this, we will first define a small \nassign\n utility function that checks\nwhether two tensors or arrays (\nleft\n and \nright\n) have the same dimensions or\nshape and returns the right tensor as trainable PyTorch parameters:\ndef assign(left, right):\n    if left.shape != right.shape:\n        raise ValueError(f""Shape mismatch. Left: {left.shape}, Right: {right.shape}"")\n    return torch.nn.Parameter(torch.tensor(right))\nNext, we define a \nload_weights_into_gpt\n function that loads the weights\nfrom the \nparams\n dictionary into a \nGPTModel\n instance \ngpt\n:\nListing 5.5 Loading OpenAI weights into our GPT model code\nimport numpy as np\n \ndef load_weights_into_gpt(gpt, params):\n    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])  #A\n    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n    \n    for b in range(len(params[""blocks""])):  #B\n        q_w, k_w, v_w = np.split(  #C\n            (params[""blocks""][b][""attn""][""c_attn""])[""w""], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.weight = assign(\n            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n        gpt.trf_blocks[b].att.W_key.weight = assign(\n            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n        gpt.trf_blocks[b].att.W_value.weight = assign(\n            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n \n        q_b, k_b, v_b = np.split(\n            (params[""blocks""][b][""attn""][""c_attn""])[""b""], 3, axis=-1)\n        gpt.trf_blocks[b].att.W_query.bias = assign(\n            gpt.trf_blocks[b].att.W_query.bias, q_b)\n        gpt.trf_blocks[b].att.W_key.bias = assign(\n            gpt.trf_blocks[b].att.W_key.bias, k_b)\n        gpt.trf_blocks[b].att.W_value.bias = assign(\n            gpt.trf_blocks[b].att.W_value.bias, v_b)\n \n        gpt.trf_blocks[b].att.out_proj.weight = assign(\n            gpt.trf_blocks[b].att.out_proj.weight, \n            params[""blocks""][b][""attn""][""c_proj""][""w""].T)\n        gpt.trf_blocks[b].att.out_proj.bias = assign(\n            gpt.trf_blocks[b].att.out_proj.bias, \n            params[""blocks""][b][""attn""][""c_proj""][""b""])\n \n        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n            gpt.trf_blocks[b].ff.layers[0].weight, \n            params[""blocks""][b][""mlp""][""c_fc""][""w""].T)\n        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n            gpt.trf_blocks[b].ff.layers[0].bias, \n            params[""blocks""][b][""mlp""][""c_fc""][""b""])\n        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n            gpt.trf_blocks[b].ff.layers[2].weight, \n            params[""blocks""][b][""mlp""][""c_proj""][""w""].T)\n        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n            gpt.trf_blocks[b].ff.layers[2].bias, \n            params[""blocks""][b][""mlp""][""c_proj""][""b""])\n \n        gpt.trf_blocks[b].norm1.scale = assign(\n            gpt.trf_blocks[b].norm1.scale, \n            params[""blocks""][b][""ln_1""][""g""])\n        gpt.trf_blocks[b].norm1.shift = assign(\n            gpt.trf_blocks[b].norm1.shift, \n            params[""blocks""][b][""ln_1""][""b""])\n        gpt.trf_blocks[b].norm2.scale = assign(\n            gpt.trf_blocks[b].norm2.scale, \n            params[""blocks""][b][""ln_2""][""g""])\n        gpt.trf_blocks[b].norm2.shift = assign(\n            gpt.trf_blocks[b].norm2.shift, \n            params[""blocks""][b][""ln_2""][""b""])\n \n    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[""g""])\n    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[""b""])\n    gpt.out_head.weight = assign(gpt.out_head.weight, params[""wte""])  #D\nIn the \nload_weights_into_gpt\n function, we carefully match the weights\nfrom OpenAI's implementation with our \nGPTModel\n implementation. To pick a\nspecific example, OpenAI stored the weight tensor for the output projection\nlayer for the first transformer block as \nparams[""blocks""][0][""attn""]\n[""c_proj""][""w""]\n. In our implementation, this weight tensor corresponds to\ngpt.trf_blocks[b].att.out_proj.weight\n, where \ngpt\n is a \nGPTModel\ninstance.\nDeveloping the \nload_weights_into_gpt\n function took a lot of guesswork\nsince OpenAI used a slightly different naming convention from ours.\nHowever, the \nassign\n function would alert us if we try to match two tensors\nwith different dimensions. Also, if we made a mistake in this function, we\nwould notice this as the resulting GPT model would be unable to produce\ncoherent text.\nLet's not try the \nload_weights_into_gpt\n out in practice and load the OpenAI\nmodel weights into our \nGPTModel\n instance \ngpt\n:\nload_weights_into_gpt(gpt, params)\ngpt.to(device)\nIf the model is loaded correctly, we can now use it to generate new text using\nour previous \ngenerate\n function:\ntorch.manual_seed(123)\ntoken_ids = generate(\n    model=gpt,\n    idx=text_to_token_ids(""Every effort moves you"", tokenizer),\n    max_new_tokens=25,\n    context_size=NEW_CONFIG[""context_length""],\n    top_k=50,\n    temperature=1.5\n)\nprint(""Output text:\n"", token_ids_to_text(token_ids, tokenizer))\nThe resulting text is as follows:\nOutput text:\n Every effort moves you toward finding an ideal new way to practice something!\nWhat makes us want to be on top of that?\nWe can be confident that we loaded the model weights correctly because the\nmodel can produce coherent text. A tiny mistake in this process would cause\nthe model to fail.\nIn the following chapters, we will work further with this pretrained model\nand fine-tune it to classify text and follow instructions.\nExercise 5.5\nCalculate the training and validation set losses of the GPTModel with the\npretrained weights from OpenAI on the ""The Verdict"" dataset.\nExercise 5.6\nReaders are encouraged to experiment with GPT-2 models of different sizes,\nfor example, the largest 1558M parameter model and compare the generated\ntext to the 124M model we loaded in this chapter.\n5.6 Summary\nWhen LLMs generate text, they output one token at a time.\nBy default, the next token is generated by converting the model outputs\ninto probability scores and selecting the token from the vocabulary that\ncorresponds to the highest probability score, which is known as ""greedy\ndecoding.""\nUsing probabilistic sampling and temperature scaling, we can influence\nthe diversity and coherence of the generated text.\nTraining and validation set losses can be used to gauge the quality of\ntext generated by LLM during training.\nPretraining an LLM involves changing its weights to minimize the\ntraining loss.\nThe training loop for LLMs itself is a standard procedure in deep\nlearning, using a conventional cross entropy loss and AdamW optimizer.\nPretraining an LLM on a large text corpus is time- and resource-\nintensive so we can load openly available weights from OpenAI as an\nalternative to pretraining the model on a large dataset ourselves.",75157
07-Appendix_A._Introduction_to_PyTorch.pdf,07-Appendix_A._Introduction_to_PyTorch,"Appendix A. Introduction to\nPyTorch\nThis chapter covers\nAn overview of the PyTorch deep learning library\nSetting up an environment and workspace for deep learning\nTensors as a fundamental data structure for deep learning\nThe mechanics of training deep neural networks\nTraining models on GPUs\nThis chapter is designed to equip you with the necessary skills and\nknowledge to put deep learning into practice and implement large language\nmodels (LLMs) from scratch.\nWe will introduce PyTorch, a popular Python-based deep learning library,\nwhich will be our primary tool for the remainder of this book. This chapter\nwill also guide you through setting up a deep learning workspace armed with\nPyTorch and GPU support.\nThen, you'll learn about the essential concept of tensors and their usage in\nPyTorch. We will also delve into PyTorch's automatic differentiation engine,\na feature that enables us to conveniently and efficiently use backpropagation,\nwhich is a crucial aspect of neural network training.\nNote that this chapter is meant as a primer for those who are new to deep\nlearning in PyTorch. While this chapter explains PyTorch from the ground\nup, it's not meant to be an exhaustive coverage of the PyTorch library.\nInstead, this chapter focuses on the PyTorch fundamentals that we will use to\nimplement LLMs throughout this book. If you are already familiar with deep\nlearning, you may skip this appendix and directly move on to chapter 2,\nworking with text data.\nA.1 What is PyTorch\nPyTorch \n(\nhttps://pytorch.org/\n) is an open-source Python-based deep learning\nlibrary. According to \nPapers With Code \n(\nhttps://paperswithcode.com/trends\n),\na platform that tracks and analyzes research papers, PyTorch has been the\nmost widely used deep learning library for research since 2019 by a wide\nmargin. And according to the \nKaggle Data Science and Machine Learning\nSurvey 2022\n (\nhttps://www.kaggle.com/c/kaggle-survey-2022\n), the number of\nrespondents using PyTorch is approximately 40% and constantly grows every\nyear.\nOne of the reasons why PyTorch is so popular is its user-friendly interface\nand efficiency. However, despite its accessibility, it doesn't compromise on\nflexibility, providing advanced users the ability to tweak lower-level aspects\nof their models for customization and optimization. In short, for many\npractitioners and researchers, PyTorch offers just the right balance between\nusability and features.\nIn the following subsections, we will define the main features PyTorch has to\noffer.\nA.1.1 The three core components of PyTorch\nPyTorch is a relatively comprehensive library, and one way to approach it is\nto focus on its three broad components, which are summarized in figure A.1.\nFigure A.1 PyTorch's three main components include a tensor library as a fundamental building\nblock for computing, automatic differentiation for model optimization, and deep learning utility\nfunctions, making it easier to implement and train deep neural network models.\nFirstly, PyTorch is a \ntensor library\n that extends the concept of array-oriented\nprogramming library NumPy with the additional feature of accelerated\ncomputation on GPUs, thus providing a seamless switch between CPUs and\nGPUs.\nSecondly, PyTorch is an \nautomatic differentiation engine\n, also known as\nautograd, which enables the automatic computation of gradients for tensor\noperations, simplifying backpropagation and model optimization.\nFinally, PyTorch is a \ndeep learning library\n, meaning that it offers modular,\nflexible, and efficient building blocks (including pre-trained models, loss\nfunctions, and optimizers) for designing and training a wide range of deep\nlearning models, catering to both researchers and developers.\nAfter defining the term deep learning and installing PyTorch in the two\nfollowing subsections, the remainder of this chapter will go over these three\ncore components of PyTorch in more detail, along with hands-on code\nexamples.\nA.1.2 Defining deep learning\nLLMs are often referred to as \nAI\n models in the news. However, as illustrated\nin the first section of chapter 1 (\n1.1 What is an LLM?\n) LLMs are also a type\nof deep neural network, and PyTorch is a deep learning library. Sounds\nconfusing? Let's take a brief moment and summarize the relationship between\nthese terms before we proceed.\nAI is fundamentally about creating computer systems capable of performing\ntasks that usually require human intelligence. These tasks include\nunderstanding natural language, recognizing patterns, and making decisions.\n(Despite significant progress, AI is still far from achieving this level of\ngeneral intelligence.)\nMachine learnin\ng represents a subfield of AI (as illustrated in figure A.2) that\nfocuses on developing and improving learning algorithms. The key idea\nbehind machine learning is to enable computers to learn from data and make\npredictions or decisions without being explicitly programmed to perform the\ntask. This involves developing algorithms that can identify patterns and learn\nfrom historical data and improve their performance over time with more data\nand feedback.\nFigure A.2 Deep learning is a subcategory of machine learning that is focused on the\nimplementation of deep neural networks. In turn, machine learning is a subcategory of AI that is\nconcerned with algorithms that learn from data. AI is the broader concept of machines being able\nto perform tasks that typically require human intelligence.\nMachine learning has been integral in the evolution of AI, powering many of\nthe advancements we see today, including LLMs. Machine learning is also\nbehind technologies like recommendation systems used by online retailers\nand streaming services, email spam filtering, voice recognition in virtual\nassistants, and even self-driving cars. The introduction and advancement of\nmachine learning have significantly enhanced AI's capabilities, enabling it to\nmove beyond strict rule-based systems and adapt to new inputs or changing\nenvironments.\nDeep learning\n is a subcategory of machine learning that focuses on the\ntraining and application of deep neural networks. These deep neural networks\nwere originally inspired by how the human brain works, particularly the\ninterconnection between many neurons. The ""deep"" in deep learning refers to\nthe multiple hidden layers of artificial neurons or nodes that allow them to\nmodel complex, nonlinear relationships in the data.\nUnlike traditional machine learning techniques that excel at simple pattern\nrecognition, deep learning is particularly good at handling unstructured data\nlike images, audio, or text, so deep learning is particularly well suited for\nLLMs.\nThe typical predictive modeling workflow (also referred to as \nsupervised\nlearning\n) in machine learning and deep learning is summarized in figure A.3.\nFigure A.3 The supervised learning workflow for predictive modeling consists of a training stage\nwhere a model is trained on labeled examples in a training dataset. The trained model can then\nbe used to predict the labels of new observations.\nUsing a learning algorithm, a model is trained on a training dataset consisting\nof examples and corresponding labels. In the case of an email spam classifier,\nfor example, the training dataset consists of emails and their \nspam\n and \nnot-\nspam\n labels that a human identified. Then, the trained model can be used on\nnew observations (new emails) to predict their unknown label (\nspam\n or \nnot\nspam\n).\nOf course, we also want to add a model evaluation between the training and\ninference stages to ensure that the model satisfies our performance criteria\nbefore using it in a real-world application.\nNote that the workflow for training and using LLMs, as we will see later in\nthis book, is similar to the workflow depicted in figure A.3 if we train them to\nclassify texts. And if we are interested in training LLMs for generating texts,\nwhich is the main focus of this book, figure A.3 still applies. In this case, the\nlabels during pretraining can be derived from the text itself (the next-word\nprediction task introduced in chapter 1). And the LLM will generate entirely\nnew text (instead of predicting labels) given an input prompt during\ninference.\nA.1.3 Installing PyTorch\nPyTorch can be installed just like any other Python library or package.\nHowever, since PyTorch is a comprehensive library featuring CPU- and\nGPU-compatible codes, the installation may require additional explanation.\nPython version\nMany scientific computing libraries do not immediately support the newest\nversion of Python. Therefore, when installing PyTorch, it's advisable to use a\nversion of Python that is one or two releases older. For instance, if the latest\nversion of Python is 3.13, using Python 3.10 or 3.11 is recommended.\nFor instance, there are two versions of PyTorch: a leaner version that only\nsupports CPU computing and a version that supports both CPU and GPU\ncomputing. If your machine has a CUDA-compatible GPU that can be used\nfor deep learning (ideally an NVIDIA T4, RTX 2080 Ti, or newer), I\nrecommend installing the GPU version. Regardless, the default command for\ninstalling PyTorch is as follows in a code terminal:\npip install torch\nSuppose your computer supports a CUDA-compatible GPU. In that case, this\nwill automatically install the PyTorch version that supports GPU acceleration\nvia CUDA, given that the Python environment you're working on has the\nnecessary dependencies (like pip) installed.\nAMD GPUs for deep learning\nAs of this writing, PyTorch has also added experimental support for AMD\nGPUs via ROCm. Please see \nhttps://pytorch.org\n for additional instructions.\nHowever, to explicitly install the CUDA-compatible version of PyTorch, it's\noften better to specify the CUDA you want PyTorch to be compatible with.\nPyTorch's official website (\nhttps://pytorch.org\n) provides commands to install\nPyTorch with CUDA support for different operating systems as shown in\nfigure A.4.\nFigure A.4 Access the PyTorch installation recommendation on \nhttps://pytorch.org\n to customize\nand select the installation command for your system.\n(Note that the command shown in figure A.4 will also install the\ntorchvision\n and \ntorchaudio\n libraries, which are optional for this book.)\nAs of this writing, this book is based on PyTorch 2.0.1, so it's recommended\nto use the following installation command to install the exact version to\nguarantee compatibility with this book:\npip install torch==2.0.1\nHowever, as mentioned earlier, given your operating system, the installation\ncommand might slightly differ from the one shown above. Thus, I\nrecommend visiting the \nhttps://pytorch.org\n website and using the installation\nmenu (see figure A4) to select the installation command for your operating\nsystem and replace \ntorch \nwith\n torch==2.0.1 \nin this command.\nTo check the version of PyTorch, you can execute the following code in\nPyTorch:\nimport torch\ntorch.__version__\nThis prints:\n'2.0.1'\nPyTorch and Torch\nNote that the Python library is named ""torch"" primarily because it's a\ncontinuation of the Torch library but adapted for Python (hence, ""PyTorch"").\nThe name ""torch"" acknowledges the library's roots in Torch, a scientific\ncomputing framework with wide support for machine learning algorithms,\nwhich was initially created using the Lua programming language.\nIf you are looking for additional recommendations and instructions for setting\nup your Python environment or installing the other libraries used later in this\nbook, I recommend visiting the supplementary GitHub repository of this\nbook at \nhttps://github.com/rasbt/LLMs-from-scratch\n.\nAfter installing PyTorch, you can check whether your installation recognizes\nyour built-in NVIDIA GPU by running the following code in Python:\nimport torch\ntorch.cuda.is_available()\nThis returns:\nTrue\nIf the command returns True, you are all set. If the command returns False,\nyour computer may not have a compatible GPU, or PyTorch does not\nrecognize it. While GPUs are not required for the initial chapters in this book,\nwhich are focused on implementing LLMs for educational purposes, they can\nsignificantly speed up deep learning-related computations.\nIf you don't have access to a GPU, there are several cloud computing\nproviders where users can run GPU computations against an hourly cost. A\npopular Jupyter-notebook-like environment is Google Colab\n(\nhttps://colab.research.google.com\n), which provides time-limited access to\nGPUs as of this writing. Using the ""Runtime"" menu, it is possible to select a\nGPU, as shown in the screenshot in figure A.5.\nFigure A.5 Select a GPU device for Google Colab under the \nRuntime/Change runtime type\n menu.\nPyTorch on Apple Silicon\nIf you have an Apple Mac with an Apple Silicon chip (like the M1, M2, M3,\nor newer models), you have the option to leverage its capabilities to\naccelerate PyTorch code execution. To use your Apple Silicon chip for\nPyTorch, you first need to install PyTorch as you normally would. Then, to\ncheck if your Mac supports PyTorch acceleration with its Apple Silicon chip,\nyou can run a simple code snippet in Python:\nprint(torch.backends.mps.is_available())\nIf it returns \nTrue\n, it means that your Mac has an Apple Silicon chip that can\nbe used to accelerate PyTorch code.\nExercise A.1\nInstall and set up PyTorch on your computer.\nExercise A.2\nRun the supplementary Chapter 2 code at \nhttps://github.com/rasbt/LLMs-\nfrom-scratch\n that checks whether your environment is set up correctly..\nA.2 Understanding tensors\nTensors represent a mathematical concept that generalizes vectors and\nmatrices to potentially higher dimensions. In other words, tensors are\nmathematical objects that can be characterized by their order (or rank), which\nprovides the number of dimensions. For example, a scalar (just a number) is a\ntensor of rank 0, a vector is a tensor of rank 1, and a matrix is a tensor of rank\n2, as illustrated in figure A.6\nFigure A.6 An illustration of tensors with different ranks. Here 0D corresponds to rank 0, 1D to\nrank 1, and 2D to rank 2. Note that a 3D vector, which consists of 3 elements, is still a rank 1\ntensor.\nFrom a computational perspective, tensors serve as data containers. For\ninstance, they hold multi-dimensional data, where each dimension represents\na different feature. Tensor libraries, such as PyTorch, can create, manipulate,\nand compute with these multi-dimensional arrays efficiently. In this context,\na tensor library functions as an array library.\nPyTorch tensors are similar to NumPy arrays but have several additional\nfeatures important for deep learning. For example, PyTorch adds an\nautomatic differentiation engine, simplifying \ncomputing gradients\n, as\ndiscussed later in section 2.4. PyTorch tensors also support GPU\ncomputations to speed up deep neural network training, which we will\ndiscuss later in section 2.8.\nPyTorch's has a NumPy-like API\nAs you will see in the upcoming sections, PyTorch adopts most of the\nNumPy array API and syntax for its tensor operations. If you are new to\nNumPy, you can get a brief overview of the most relevant concepts via my\narticle Scientific Computing in Python: Introduction to NumPy and\nMatplotlib at \nhttps://sebastianraschka.com/blog/2020/numpy-intro.html\n.\nThe following subsections will look at the basic operations of the PyTorch\ntensor library, showing how to create simple tensors and going over some of\nthe essential operations.\nA.2.1 Scalars, vectors, matrices, and tensors\nAs mentioned earlier, PyTorch tensors are data containers for array-like\nstructures. A scalar is a 0-dimensional tensor (for instance, just a number), a\nvector is a 1-dimensional tensor, and a matrix is a 2-dimensional tensor.\nThere is no specific term for higher-dimensional tensors, so we typically refer\nto a 3-dimensional tensor as just a 3D tensor, and so forth.\nWe can create objects of PyTorch's \nTensor\n class using the \ntorch.tensor\nfunction as follows:\nListing A.1 Creating PyTorch tensors\nimport torch\n \n \ntensor0d = torch.tensor(1) #A\n \n \ntensor1d = torch.tensor([1, 2, 3]) #B\n \n \ntensor2d = torch.tensor([[1, 2], [3, 4]]) #C\n \n \ntensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) #D\nA.2.2 Tensor data types\nIn the previous section, we created tensors from Python integers. In this case,\nPyTorch adopts the default 64-bit integer data type from Python. We can\naccess the data type of a tensor via the \n.dtype\n attribute of a tensor:\ntensor1d = torch.tensor([1, 2, 3])\nprint(tensor1d.dtype)\nThis prints:\ntorch.int64\nIf we create tensors from Python floats, PyTorch creates tensors with a 32-bit\nprecision by default, as we can see below:\nfloatvec = torch.tensor([1.0, 2.0, 3.0])\nprint(floatvec.dtype)\nThe output is:\ntorch.float32\nThis choice is primarily due to the balance between precision and\ncomputational efficiency. A 32-bit floating point number offers sufficient\nprecision for most deep learning tasks, while consuming less memory and\ncomputational resources than a 64-bit floating point number. Moreover, GPU\narchitectures are optimized for 32-bit computations, and using this data type\ncan significantly speed up model training and inference.\nMoreover, it is possible to readily change the precision using a tensor's \n.to\nmethod. The following code demonstrates this by changing a 64-bit integer\ntensor into a 32-bit float tensor:\nfloatvec = tensor1d.to(torch.float32)\nprint(floatvec.dtype)\nThis returns:\ntorch.float32\nFor more information about different tensor data types available in PyTorch, I\nrecommend checking the official documentation at\nhttps://pytorch.org/docs/stable/tensors.html\n.\nA.2.3 Common PyTorch tensor operations\nComprehensive coverage of all the different PyTorch tensor operations and\ncommands is outside the scope of this book. However, we will briefly\ndescribe relevant operations as we introduce them throughout the book.\nBefore we move on to the next section covering the concept behind\ncomputation graphs, below is a list of the most essential PyTorch tensor\noperations.\nWe already introduced the \ntorch.tensor()\n function to create new tensors.\ntensor2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(tensor2d)\nThis prints:\ntensor([[1, 2, 3],\n        [4, 5, 6]])\nIn addition, the \n.shape\n attribute allows us to access the shape of a tensor:\nprint(tensor2d.shape)\nThe output is:\ntorch.Size([2, 3])\nAs you can see above, \n.shape\n returns \n[2, 3]\n, which means that the tensor\nhas 2 rows and 3 columns. To reshape the tensor into a 3 by 2 tensor, we can\nuse the \n.reshape\n method:\nprint(tensor2d.reshape(3, 2))\nThis prints:\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\nHowever, note that the more common command for reshaping tensors in\nPyTorch is \n.view()\n:\nprint(tensor2d.view(3, 2))\nThe output is:\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\nSimilar to \n.reshape\n and \n.view\n, there are several cases where PyTorch offers\nmultiple syntax options for executing the same computation. This is because\nPyTorch initially followed the original Lua Torch syntax convention but then\nalso added syntax to make it more similar to NumPy upon popular request.\nNext, we can use \n.T\n to transpose a tensor, which means flipping it across its\ndiagonal. Note that this is similar from reshaping a tensor as you can see\nbased on the result below:\nprint(tensor2d.T)\nThe output is:\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\nLastly, the common way to multiply two matrices in PyTorch is the \n.matmul\nmethod:\nprint(tensor2d.matmul(tensor2d.T))\nThe output is:\ntensor([[14, 32],\n        [32, 77]])\nHowever, we can also adopt the \n@\n operator, which accomplishes the same\nthing more compactly:\nprint(tensor2d @ tensor2d.T)\nThis prints:\ntensor([[14, 32],\n        [32, 77]])\nAs mentioned earlier, we will introduce additional operations throughout this\nbook when needed. For readers who'd like to browse through all the different\ntensor operations available in PyTorch (hint: we won't need most of these), I\nrecommend checking out the official documentation at\nhttps://pytorch.org/docs/stable/tensors.html\n.\nA.3 Seeing models as computation graphs\nIn the previous section, we covered one of the major three components of\nPyTorch, namely, its tensor library. Next in line is PyTorch's automatic\ndifferentiation engine, also known as autograd. PyTorch's autograd system\nprovides functions to compute gradients in dynamic computational graphs\nautomatically. But before we dive deeper into computing gradients in the\nnext section, let's define the concept of a computational graph.\nA computational graph (or computation graph in short) is a directed graph\nthat allows us to express and visualize mathematical expressions. In the\ncontext of deep learning, a computation graph lays out the sequence of\ncalculations needed to compute the output of a neural network -- we will need\nthis later to compute the required gradients for backpropagation, which is the\nmain training algorithm for neural networks.\nLet's look at a concrete example to illustrate the concept of a computation\ngraph. The following code implements the forward pass (prediction step) of a\nsimple logistic regression classifier, which can be seen as a single-layer\nneural network, returning a score between 0 and 1 that is compared to the true\nclass label (0 or 1) when computing the loss:\nListing A.2 A logistic regression forward pass\nimport torch.nn.functional as F   #A\n \ny = torch.tensor([1.0])  #B\nx1 = torch.tensor([1.1]) #C\nw1 = torch.tensor([2.2]) #D\nb = torch.tensor([0.0])  #E\nz = x1 * w1 + b          #F\na = torch.sigmoid(z)     #G\n \nloss = F.binary_cross_entropy(a, y)\nIf not all components in the code above make sense to you, don't worry. The\npoint of this example is not to implement a logistic regression classifier but\nrather to illustrate how we can think of a sequence of computations as a\ncomputation graph, as shown in figure A.7.\nFigure A.7 A logistic regression forward pass as a computation graph. The input feature \nx\n1\n is\nmultiplied by a model weight \nw\n1\n and passed through an activation function \nσ\n after adding the\nbias. The loss is computed by comparing the model output \na\n with a given label \ny\n.\nIn fact, PyTorch builds such a computation graph in the background, and we\ncan use this to calculate gradients of a loss function with respect to the model\nparameters (here w1 and b) to train the model, which is the topic of the\nupcoming sections.\nA.4 Automatic differentiation made easy\nIn the previous section, we introduced the concept of computation graphs. If\nwe carry out computations in PyTorch, it will build such a graph internally by\ndefault if one of its terminal nodes has the \nrequires_grad\n attribute set to\nTrue\n. This is useful if we want to compute gradients. Gradients are required\nwhen training neural networks via the popular backpropagation algorithm,\nwhich can be thought of as an implementation of the \nchain rule\n from calculus\nfor neural networks, which is illustrated in figure A.8.\nFigure A.8 The most common way of computing the loss gradients in a computation graph\ninvolves applying the chain rule from right to left, which is also called reverse-model automatic\ndifferentiation or backpropagation. It means we start from the output layer (or the loss itself) and\nwork backward through the network to the input layer. This is done to compute the gradient of\nthe loss with respect to each parameter (weights and biases) in the network, which informs how\nwe update these parameters during training.\nPartial derivatives and gradients\nFigure A.8 shows partial derivatives, which measure the rate at which a\nfunction changes with respect to one of its variables. A gradient is a vector\ncontaining all of the partial derivatives of a multivariate function, a function\nwith more than one variable as input.\nIf you are not familiar or don't remember the partial derivatives, gradients, or\nthe chain rule from calculus, don't worry. On a high level, all you need to\nknow for this book is that the chain rule is a way to compute gradients of a\nloss function with respect to the model's parameters in a computation graph.\nThis provides the information needed to update each parameter in a way that\nminimizes the loss function, which serves as a proxy for measuring the\nmodel's performance, using a method such as gradient descent. We will\nrevisit the computational implementation of this training loop in PyTorch in\nsection 2.7, \nA typical training loop\n.\nNow, how is this all related to the second component of the PyTorch library\nwe mentioned earlier, the automatic differentiation (autograd) engine? By\ntracking every operation performed on tensors, PyTorch's autograd engine\nconstructs a computational graph in the background. Then, calling the \ngrad\nfunction, we can compute the gradient of the loss with respect to model\nparameter \nw1\n as follows:\nListing A.3 Computing gradients via autograd\nimport torch.nn.functional as F\nfrom torch.autograd import grad\n \ny = torch.tensor([1.0])\nx1 = torch.tensor([1.1])\nw1 = torch.tensor([2.2], requires_grad=True)\nb = torch.tensor([0.0], requires_grad=True)\n \nz = x1 * w1 + b \na = torch.sigmoid(z)\n \nloss = F.binary_cross_entropy(a, y)\n \ngrad_L_w1 = grad(loss, w1, retain_graph=True)  #A\ngrad_L_b = grad(loss, b, retain_graph=True)\n \nLet's show the resulting values of the loss with respect to the model's\nparameters:\nprint(grad_L_w1)\nprint(grad_L_b)\nThe prints:\n(tensor([-0.0898]),)\n(tensor([-0.0817]),)\nAbove, we have been using the grad function ""manually,"" which can be\nuseful for experimentation, debugging, and demonstrating concepts. But in\npractice, PyTorch provides even more high-level tools to automate this\nprocess. For instance, we can call \n.backward\n on the loss, and PyTorch will\ncompute the gradients of all the leaf nodes in the graph, which will be stored\nvia the tensors' \n.grad\n attributes:\nloss.backward()\nprint(w1.grad)\nprint(b.grad)\nThe outputs are:\n(tensor([-0.0898]),)\n(tensor([-0.0817]),)\nIf this section is packed with a lot of information and you may be\noverwhelmed by the calculus concepts, don't worry. While this calculus\njargon was a means to explain PyTorch's autograd component, all you need to\ntake away from this section is that PyTorch takes care of the calculus for us\nvia the \n.backward\n method -- we won't need to compute any derivatives or\ngradients by hand in this book.\nA.5 Implementing multilayer neural networks\nIn the previous sections, we covered PyTorch's tensor and autograd\ncomponents. This section focuses on PyTorch as a library for implementing\ndeep neural networks.\nTo provide a concrete example, we focus on a multilayer perceptron, which is\na fully connected neural network, as illustrated in figure A.9.\nFigure A.9 An illustration of a multilayer perceptron with 2 hidden layers. Each node represents\na unit in the respective layer. Each layer has only a very small number of nodes for illustration\npurposes.\nWhen implementing a neural network in PyTorch, we typically subclass the\ntorch.nn.Module\n class to define our own custom network architecture. This\nModule\n base class provides a lot of functionality, making it easier to build and\ntrain models. For instance, it allows us to encapsulate layers and operations\nand keep track of the model's parameters.\nWithin this subclass, we define the network layers in the \n__init__\nconstructor and specify how they interact in the forward method. The forward\nmethod describes how the input data passes through the network and comes\ntogether as a computation graph.\nIn contrast, the backward method, which we typically do not need to\nimplement ourselves, is used during training to compute gradients of the loss\nfunction with respect to the model parameters, as we will see in section 2.7, \nA\ntypical training loop\n.\nThe following code implements a classic multilayer perceptron with two\nhidden layers to illustrate a typical usage of the \nModule\n class:\nListing A.4 A multilayer perceptron with two hidden layers\nclass NeuralNetwork(torch.nn.Module):\n    def __init__(self, num_inputs, num_outputs):  #A\n        super().__init__()\n \n        self.layers = torch.nn.Sequential(\n                \n            # 1st hidden layer\n            torch.nn.Linear(num_inputs, 30),  #B\n            torch.nn.ReLU(),  #C\n \n            # 2nd hidden layer\n            torch.nn.Linear(30, 20),  #D\n            torch.nn.ReLU(),\n \n            # output layer\n            torch.nn.Linear(20, num_outputs),\n        )\n \n    def forward(self, x):\n        logits = self.layers(x)\n        return logits   #E\n \nWe can then instantiate a new neural network object as follows:\nmodel = NeuralNetwork(50, 3)\nBut before using this new \nmodel\n object, it is often useful to call \nprint\n on the\nmodel to see a summary of its structure:\nprint(model)\nThis prints:\nNeuralNetwork(\n  (layers): Sequential(\n    (0): Linear(in_features=50, out_features=30, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=30, out_features=20, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=3, bias=True)\n  )\n)\nNote that we used the \nSequential \nclass when we implemented the\nNeuralNetwork\n class. Using Sequential is not required, but it can make our\nlife easier if we have a series of layers that we want to execute in a specific\norder, as is the case here. This way, after instantiating \nself.layers =\nSequential(...)\n in the \n__init__\n constructor, we just have to call the\nself.layers\n instead of calling each layer individually in the\nNeuralNetwork\n's \nforward\n method.\nNext, let's check the total number of trainable parameters of this model:\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(""Total number of trainable model parameters:"", num_params)\nThis prints:\nTotal number of trainable model parameters: 2213\nNote that each parameter for which \nrequires_grad=True \ncounts as a\ntrainable parameter and will be updated during training (more on that later in\nsection 2.7, \nA typical training loop\n).\nIn the case of our neural network model with the two hidden layers above,\nthese trainable parameters are contained in the \ntorch.nn.Linear\n layers. A\nlinear\n layer multiplies the inputs with a weight matrix and adds a bias vector.\nThis is sometimes also referred to as a \nfeedforward\n or \nfully connected\n layer.\nBased on the \nprint(model)\n call we executed above, we can see that the first\nLinear\n layer is at index position 0 in the layers attribute. We can access the\ncorresponding weight parameter matrix as follows:\nprint(model.layers[0].weight)\nThis prints:\nParameter containing:\ntensor([[ 0.1174, -0.1350, -0.1227,  ...,  0.0275, -0.0520, -0.0192],\n        [-0.0169,  0.1265,  0.0255,  ..., -0.1247,  0.1191, -0.0698],\n        [-0.0973, -0.0974, -0.0739,  ..., -0.0068, -0.0892,  0.1070],\n        ...,\n        [-0.0681,  0.1058, -0.0315,  ..., -0.1081, -0.0290, -0.1374],\n        [-0.0159,  0.0587, -0.0916,  ..., -0.1153,  0.0700,  0.0770],\n        [-0.1019,  0.1345, -0.0176,  ...,  0.0114, -0.0559, -0.0088]],\n       requires_grad=True)\nSince this is a large matrix that is not shown in its entirety, let's use the\n.shape\n attribute to show its dimensions:\nprint(model.layers[0].weight.shape)\nThe result is:\ntorch.Size([30, 50])\n(Similarly, you could access the bias vector via \nmodel.layers[0].bias\n.)\nThe weight matrix above is a 30x50 matrix, and we can see that the\nrequires_grad\n is set to \nTrue\n, which means its entries are trainable -- this is\nthe default setting for weights and biases in \ntorch.nn.Linear\n.\nNote that if you execute the code above on your computer, the numbers in the\nweight matrix will likely differ from those shown above. This is because the\nmodel weights are initialized with small random numbers, which are different\neach time we instantiate the network. In deep learning, initializing model\nweights with small random numbers is desired to break symmetry during\ntraining -- otherwise, the nodes would be just performing the same operations\nand updates during backpropagation, which would not allow the network to\nlearn complex mappings from inputs to outputs.\nHowever, while we want to keep using small random numbers as initial\nvalues for our layer weights, we can make the random number initialization\nreproducible by seeding PyTorch's random number generator via\nmanual_seed\n:\ntorch.manual_seed(123)\nmodel = NeuralNetwork(50, 3)\nprint(model.layers[0].weight)\nThe result is:\nParameter containing:\ntensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n        ...,\n        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n       requires_grad=True)\nNow, after we spent some time inspecting the \nNeuraNetwork\n instance, let's\nbriefly see how it's used via the forward pass:\ntorch.manual_seed(123)\nX = torch.rand((1, 50))\nout = model(X)\nprint(out)\nThe result is:tensor([[-0.1262, 0.1080, -0.1792]],\n grad_fn=\n<AddmmBackward0>)\nIn the code above, we generated a single random training example \nX\n as a toy\ninput (note that our network expects 50-dimensional feature vectors) and fed\nit to the model, returning three scores. When we call \nmodel(x)\n, it will\nautomatically execute the forward pass of the model.\nThe forward pass refers to calculating output tensors from input tensors. This\ninvolves passing the input data through all the neural network layers, starting\nfrom the input layer, through hidden layers, and finally to the output layer.\nThese three numbers returned above correspond to a score assigned to each\nof the three output nodes. Notice that the output tensor also includes a\ngrad_fn\n value.\nHere, \ngrad_fn=<AddmmBackward0>\n represents the last-used function to\ncompute a variable in the computational graph. In particular, \ngrad_fn=\n<AddmmBackward0>\n means that the tensor we are inspecting was created via a\nmatrix multiplication and addition operation. PyTorch will use this\ninformation when it computes gradients during backpropagation. The\n<AddmmBackward0>\n part of \ngrad_fn=<AddmmBackward0>\n specifies the\noperation that was performed. In this case, it is an \nAddmm\n operation. \nAddmm\nstands for matrix multiplication (\nmm\n) followed by an addition (\nAdd\n).\nIf we just want to use a network without training or backpropagation, for\nexample, if we use it for prediction after training, constructing this\ncomputational graph for backpropagation can be wasteful as it performs\nunnecessary computations and consumes additional memory. So, when we\nuse a model for inference (for instance, making predictions) rather than\ntraining, it is a best practice to use the \ntorch.no_grad()\n context manager, as\nshown below. This tells PyTorch that it doesn't need to keep track of the\ngradients, which can result in significant savings in memory and\ncomputation.\nwith torch.no_grad():\n    out = model(X)\nprint(out)\nThe result is:\ntensor([[-0.1262,  0.1080, -0.1792]])\nIn PyTorch, it's common practice to code models such that they return the\noutputs of the last layer (logits) without passing them to a nonlinear\nactivation function. That's because PyTorch's commonly used loss functions\ncombine the softmax (or sigmoid for binary classification) operation with the\nnegative log-likelihood loss in a single class. The reason for this is numerical\nefficiency and stability. So, if we want to compute class-membership\nprobabilities for our predictions, we have to call the \nsoftmax\n function\nexplicitly:\nwith torch.no_grad():\n    out = torch.softmax(model(X), dim=1)\nprint(out)\nThis prints:\ntensor([[0.3113, 0.3934, 0.2952]]))\nThe values can now be interpreted as class-membership probabilities that\nsum up to 1. The values are roughly equal for this random input, which is\nexpected for a randomly initialized model without training.\nIn the following two sections, we will learn how to set up an efficient data\nloader and train the model.\nA.6 Setting up efficient data loaders\nIn the previous section, we defined a custom neural network model. Before\nwe can train this model, we have to briefly talk about creating efficient data\nloaders in PyTorch, which we will iterate over when training the model. The\noverall idea behind data loading in PyTorch is illustrated in figure A.10.\nFigure A.10 PyTorch implements a Dataset and a DataLoader class. The Dataset class is used to\ninstantiate objects that define how each data record is loaded. The DataLoader handles how the\ndata is shuffled and assembled into batches.\nFollowing the illustration in figure A.10, in this section, we will implement a\ncustom Dataset class that we will use to create a training and a test dataset\nthat we'll then use to create the data loaders.\nLet's start by creating a simple toy dataset of five training examples with two\nfeatures each. Accompanying the training examples, we also create a tensor\ncontaining the corresponding class labels: three examples below to class 0,\nand two examples belong to class 1. In addition, we also make a test set\nconsisting of two entries. The code to create this dataset is shown below.\nListing A.5 Creating a small toy dataset\nX_train = torch.tensor([\n    [-1.2, 3.1],\n    [-0.9, 2.9],\n    [-0.5, 2.6],\n    [2.3, -1.1],\n    [2.7, -1.5]\n])\ny_train = torch.tensor([0, 0, 0, 1, 1])\n \nX_test = torch.tensor([\n    [-0.8, 2.8],\n    [2.6, -1.6],\n])\ny_test = torch.tensor([0, 1])\nClass label numbering\nPyTorch requires that class labels start with label 0, and the largest class label\nvalue should not exceed the number of output nodes minus 1 (since Python\nindex counting starts at 0. So, if we have class labels 0, 1, 2, 3, and 4, the\nneural network output layer should consist of 5 nodes.\nNext, we create a custom dataset class, \nToyDataset\n, by subclassing from\nPyTorch's\n Dataset\n parent class, as shown below.\nListing A.6 Defining a custom Dataset class\nfrom torch.utils.data import Dataset\n \nclass ToyDataset(Dataset):\n    def __init__(self, X, y):\n        self.features = X\n        self.labels = y\n \n    def __getitem__(self, index):     #A\n        one_x = self.features[index]  #A\n        one_y = self.labels[index]    #A\n        return one_x, one_y           #A\n \n    def __len__(self):\n        return self.labels.shape[0]   #B\n \ntrain_ds = ToyDataset(X_train, y_train)\ntest_ds = ToyDataset(X_test, y_test)\nThis custom \nToyDataset\n class's purpose is to use it to instantiate a PyTorch\nDataLoader\n. But before we get to this step, let's briefly go over the general\nstructure of the \nToyDataset\n code.\nIn PyTorch, the three main components of a custom Dataset class are the\n__init__\n constructor, the \n__getitem__\n method, and the \n__len__\n method, as\nshown in code listing A.6 above.\nIn the \n__init__\n method, we set up attributes that we can access later in the\n__getitem__\n and \n__len__\n methods. This could be file paths, file objects,\ndatabase connectors, and so on. Since we created a tensor dataset that sits in\nmemory, we are simply assigning \nX\n and \ny\n to these attributes, which are\nplaceholders for our tensor objects.\nIn the \n__getitem__\n method, we define instructions for returning exactly one\nitem from the dataset via an \nindex\n. This means the features and the class\nlabel corresponding to a single training example or test instance. (The data\nloader will provide this \nindex\n, which we will cover shortly.)\nFinally, the \n__len__\n method constrains instructions for retrieving the length\nof the dataset. Here, we use the \n.shape\n attribute of a tensor to return the\nnumber of rows in the feature array. In the case of the training dataset, we\nhave five rows, which we can double-check as follows:\nprint(len(train_ds))\nThe result is:\n5\nNow that we defined a PyTorch Dataset class we can use for our toy dataset,\nwe can use PyTorch's \nDataLoader\n class to sample from it, as shown in the\ncode listing below:\nListing A.7 Instantiating data loaders\nfrom torch.utils.data import DataLoader\n \n \ntorch.manual_seed(123)\n \ntrain_loader = DataLoader(\n    dataset=train_ds,  #A\n    batch_size=2,\n    shuffle=True,  #B\n    num_workers=0  #C\n)\n \ntest_loader = DataLoader(\n    dataset=test_ds,\n    batch_size=2,\n    shuffle=False,  #D\n    num_workers=0\n)\nAfter instantiating the training data loader, we can iterate over it as shown\nbelow. (The iteration over the \ntest_loader\n works similarly but is omitted for\nbrevity.)\nfor idx, (x, y) in enumerate(train_loader):\n    print(f""Batch {idx+1}:"", x, y)\nThe result is:\nBatch 1: tensor([[-1.2000,  3.1000],\n                 [-0.5000,  2.6000]]) tensor([0, 0])\nBatch 2: tensor([[ 2.3000, -1.1000],\n                 [-0.9000,  2.9000]]) tensor([1, 0])\nBatch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\nAs we can see based on the output above, the \ntrain_loader\n iterates over the\ntraining dataset visiting each training example exactly once. This is known as\na training epoch. Since we seeded the random number generator using\ntorch.manual_seed(123)\n above, you should get the exact same shuffling\norder of training examples as shown above. However if you iterate over the\ndataset a second time, you will see that the shuffling order will change. This\nis desired to prevent deep neural networks getting caught in repetitive update\ncycles during training.\nNote that we specified a batch size of 2 above, but the 3rd batch only\ncontains a single example. That's because we have five training examples,\nwhich is not evenly divisible by 2. In practice, having a substantially smaller\nbatch as the last batch in a training epoch can disturb the convergence during\ntraining. To prevent this, it's recommended to set \ndrop_last=True\n, which\nwill drop the last batch in each epoch, as shown below:\nListing A.8 A training loader that drops the last batch\ntrain_loader = DataLoader(\n    dataset=train_ds,\n    batch_size=2,\n    shuffle=True,\n    num_workers=0,\n    drop_last=True\n)\nNow, iterating over the training loader, we can see that the last batch is\nomitted:\nfor idx, (x, y) in enumerate(train_loader):\n    print(f""Batch {idx+1}:"", x, y)\nThe result is:\nBatch 1: tensor([[-0.9000,  2.9000],\n        [ 2.3000, -1.1000]]) tensor([0, 1])\nBatch 2: tensor([[ 2.7000, -1.5000],\n        [-0.5000,  2.6000]]) tensor([1, 0])\nLastly, let's discuss the setting \nnum_workers=0\n in the \nDataLoader\n. This\nparameter in PyTorch's\n DataLoader\n function is crucial for parallelizing data\nloading and preprocessing. When \nnum_workers\n is set to 0, the data loading\nwill be done in the main process and not in separate worker processes. This\nmight seem unproblematic, but it can lead to significant slowdowns during\nmodel training when we train larger networks on a GPU. This is because\ninstead of focusing solely on the processing of the deep learning model, the\nCPU must also take time to load and preprocess the data. As a result, the\nGPU can sit idle while waiting for the CPU to finish these tasks. In contrast,\nwhen \nnum_workers\n is set to a number greater than zero, multiple worker\nprocesses are launched to load data in parallel, freeing the main process to\nfocus on training your model and better utilizing your system's resources,\nwhich is illustrated in figure A.11\nFigure A.11 Loading data without multiple workers (setting \nnum_workers=0\n) will create a data\nloading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left\nsubpanel. If multiple workers are enabled, the data loader can already queue up the next batch in\nthe background as shown in the right subpanel.\nHowever, if we are working with very small datasets, setting \nnum_workers\n to\n1 or larger may not be necessary since the total training time takes only\nfractions of a second anyway. On the contrary, if you are working with tiny\ndatasets or interactive environments such as Jupyter notebooks, increasing\nnum_workers\n may not provide any noticeable speedup. They might, in fact,\nlead to some issues. One potential issue is the overhead of spinning up\nmultiple worker processes, which could take longer than the actual data\nloading when your dataset is small.\nFurthermore, for Jupyter notebooks, setting \nnum_workers\n to greater than 0\ncan sometimes lead to issues related to the sharing of resources between\ndifferent processes, resulting in errors or notebook crashes. Therefore, it's\nessential to understand the trade-off and make a calculated decision on setting\nthe \nnum_workers\n parameter. When used correctly, it can be a beneficial tool\nbut should be adapted to your specific dataset size and computational\nenvironment for optimal results.\nIn my experience, setting \nnum_workers=4\n usually leads to optimal\nperformance on many real-world datasets, but optimal settings depend on\nyour hardware and the code used for loading a training example defined in\nthe \nDataset\n class.\nA.7 A typical training loop\nSo far, we've discussed all the requirements for training neural networks:\nPyTorch's tensor library, autograd, the \nModule\n API, and efficient data loaders.\nLet's now combine all these things and train a neural network on the toy\ndataset from the previous section. The training code is shown in code listing\nA.9 below.\nListing A.9 Neural network training in PyTorch\nimport torch.nn.functional as F\n \n \ntorch.manual_seed(123)\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)  #A\noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)  #B\n \nnum_epochs = 3\n \nfor epoch in range(num_epochs): \n    \n    model.train()\n    for batch_idx, (features, labels) in enumerate(train_loader):\n \n        logits = model(features)\n        \n        loss = F.cross_entropy(logits, labels)\n        \n        optimizer.zero_grad()   #C\n        loss.backward()         #D\n        optimizer.step()        #E\n    \n        ### LOGGING\n        print(f""Epoch: {epoch+1:03d}/{num_epochs:03d}""\n              f"" | Batch {batch_idx:03d}/{len(train_loader):03d}""\n              f"" | Train Loss: {loss:.2f}"")\n \n    model.eval()\n    # Optional model evaluation\nRunning the code in listing A.9 above yields the following outputs:\nEpoch: 001/003 | Batch 000/002 | Train Loss: 0.75\nEpoch: 001/003 | Batch 001/002 | Train Loss: 0.65\nEpoch: 002/003 | Batch 000/002 | Train Loss: 0.44\nEpoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13\nEpoch: 003/003 | Batch 000/002 | Train Loss: 0.03\nEpoch: 003/003 | Batch 001/002 | Train Loss: 0.00\nAs we can see, the loss reaches zero after 3 epochs, a sign that the model\nconverged on the training set. However, before we evaluate the model's\npredictions, let's go over some of the details of the preceding code listing.\nFirst, note that we initialized a model with two inputs and two outputs. That's\nbecause the toy dataset from the previous section has two input features and\ntwo class labels to predict. We used a stochastic gradient descent (\nSGD\n)\noptimizer with a learning rate (\nlr\n) of 0.5. The learning rate is a\nhyperparameter, meaning it's a tunable setting that we have to experiment\nwith based on observing the loss. Ideally, we want to choose a learning rate\nsuch that the loss converges after a certain number of epochs -- the number of\nepochs is another hyperparameter to choose.\nExercise A.3\nHow many parameters does the neural network introduced at the beginning of\nthis section have?\nIn practice, we often use a third dataset, a so-called validation dataset, to find\nthe optimal hyperparameter settings. A validation dataset is similar to a test\nset. However, while we only want to use a test set precisely once to avoid\nbiasing the evaluation, we usually use the validation set multiple times to\ntweak the model settings.\nWe also introduced new settings called \nmodel.train()\n and \nmodel.eval()\n.\nAs these names imply, these settings are used to put the model into a training\nand an evaluation mode. This is necessary for components that behave\ndifferently during training and inference, such as \ndropout\n or \nbatch\nnormalization\n layers. Since we don't have dropout or other components in our\nNeuralNetwork\n class that are affected by these settings, using \nmodel.train()\nand \nmodel.eval()\n is redundant in our code above. However, it's best practice\nto include them anyway to avoid unexpected behaviors when we change the\nmodel architecture or reuse the code to train a different model.\nAs discussed earlier, we pass the logits directly into the \ncross_entropy\n loss\nfunction, which will apply the softmax function internally for efficiency and\nnumerical stability reasons. Then, calling \nloss.backward()\n will calculate the\ngradients in the computation graph that PyTorch constructed in the\nbackground. The \noptimizer.step()\n method will use the gradients to update\nthe model parameters to minimize the loss. In the case of the SGD optimizer,\nthis means multiplying the gradients with the learning rate and adding the\nscaled negative gradient to the parameters.\nPreventing undesired gradient accumulation\nIt is important to include an \noptimizer.zero_grad()\n call in each update\nround to reset the gradients to zero. Otherwise, the gradients will accumulate,\nwhich may be undesired.\nAfter we trained the model, we can use it to make predictions, as shown\nbelow:\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_train)\nprint(outputs)\nThe results are as follows:\ntensor([[ 2.8569, -4.1618],\n        [ 2.5382, -3.7548],\n        [ 2.0944, -3.1820],\n        [-1.4814,  1.4816],\n        [-1.7176,  1.7342]])\nTo obtain the class membership probabilities, we can then use PyTorch's\nsoftmax function, as follows:\ntorch.set_printoptions(sci_mode=False)\nprobas = torch.softmax(outputs, dim=1)\nprint(probas)\nThis outputs:\ntensor([[    0.9991,     0.0009],\n        [    0.9982,     0.0018],\n        [    0.9949,     0.0051],\n        [    0.0491,     0.9509],\n        [    0.0307,     0.9693]])\nLet's consider the first row in the code output above. Here, the first value\n(column) means that the training example has a 99.91% probability of\nbelonging to class 0 and a 0.09% probability of belonging to class 1. (The\nset_printoptions \ncall is used here to make the outputs more legible.)\nWe can convert these values into class labels predictions using PyTorch's\nargmax function, which returns the index position of the highest value in each\nrow if we set \ndim=1\n (setting \ndim=0\n would return the highest value in each\ncolumn, instead):\npredictions = torch.argmax(probas, dim=1)\nprint(predictions)\nThis prints:\ntensor([0, 0, 0, 1, 1])\nNote that it is unnecessary to compute softmax probabilities to obtain the\nclass labels. We could also apply the \nargmax\n function to the logits (outputs)\ndirectly:\npredictions = torch.argmax(outputs, dim=1)\nprint(predictions)\nThe output is:\ntensor([0, 0, 0, 1, 1])\nAbove, we computed the predicted labels for the training dataset. Since the\ntraining dataset is relatively small, we could compare it to the true training\nlabels by eye and see that the model is 100% correct. We can double-check\nthis using the == comparison operator:\npredictions == y_train\nThe results are:\ntensor([True, True, True, True, True])\nUsing \ntorch.sum\n, we can count the number of correct prediction as follows:\ntorch.sum(predictions == y_train)\nThe output is:\n5\nSince the dataset consists of 5 training examples, we have 5 out of 5\npredictions that are correct, which equals 5/5 × 100% = 100% prediction\naccuracy.\nHowever, to generalize the computation of the prediction accuracy, let's\nimplement a \ncompute_accuracy\n function as shown in the following code\nlisting.\nListing A.10 A function to compute the prediction accuracy\ndef compute_accuracy(model, dataloader):\n \n    model = model.eval()\n    correct = 0.0\n    total_examples = 0\n    \n    for idx, (features, labels) in enumerate(dataloader):\n        \n        with torch.no_grad():\n            logits = model(features)\n        \n        predictions = torch.argmax(logits, dim=1)\n        compare = labels == predictions  #A\n        correct += torch.sum(compare)  #B\n        total_examples += len(compare)\n \n    return (correct / total_examples).item() #C\nNote that the following code listing iterates over a data loader to compute the\nnumber and fraction of the correct predictions. This is because when we work\nwith large datasets, we typically can only call the model on a small part of the\ndataset due to memory limitations. The \ncompute_accuracy\n function above is\na general method that scales to datasets of arbitrary size since, in each\niteration, the dataset chunk that the model receives is the same size as the\nbatch size seen during training.\nNotice that the internals of the \ncompute_accuracy\n function are similar to\nwhat we used before when we converted the logits to the class labels.\nWe can then apply the function to the training as follows:\nprint(compute_accuracy(model, train_loader))\nThe results is:\n1.0\nSimilarly, we can apply the function to the test set as follows:\n>>> print(compute_accuracy(model, test_loader))\nThis prints:\n1.0\nIn this section, we learned how we can train a neural network using PyTorch.\nNext, let's see how we can save and restore models after training.\nA.8 Saving and loading models\nIn the previous section, we successfully trained a model. Let's now see how\nwe can save a trained model to reuse it later.\nHere's the recommended way how we can save and load models in PyTorch:\ntorch.save(model.state_dict(), ""model.pth"")\nThe model's state_dict is a Python dictionary object that maps each layer in\nthe model to its trainable parameters (weights and biases). Note that\n""model.pth""\n is an arbitrary filename for the model file saved to disk. We can\ngive it any name and file ending we like; however, \n.pth\n and \n.pt\n are the most\ncommon conventions.\nOnce we saved the model, we can restore it from disk as follows:\nmodel = NeuralNetwork(2, 2) \nmodel.load_state_dict(torch.load(""model.pth""))\nThe \ntorch.load(""model.pth"")\n function reads the file \n""model.pth""\n and\nreconstructs the Python dictionary object containing the model's parameters\nwhile \nmodel.load_state_dict()\n applies these parameters to the model,\neffectively restoring its learned state from when we saved it.\nNote that the line \nmodel = NeuralNetwork(2, 2)\n above is not strictly\nnecessary if you execute this code in the same session where you saved a\nmodel. However, I included it here to illustrate that we need an instance of\nthe model in memory to apply the saved parameters. Here, the\nNeuralNetwork(2, 2)\n architecture needs to match the original saved model\nexactly.\nNow, we are well equipped to use PyTorch to implement large language\nmodels in the upcoming chapters. However, before we jump to the next\nchapter, the last section will show you how to train PyTorch models faster\nusing one or more GPUs (if available).\nA.9 Optimizing training performance with GPUs\nIn this last section of this chapter, we will see how we can utilize GPUs,\nwhich will accelerate deep neural network training compared to regular\nCPUs. First, we will introduce the main concepts behind GPU computing in\nPyTorch. Then, we will train a model on a single GPU. Finally, we'll then\nlook at distributed training using multiple GPUs.\nA.9.1 PyTorch computations on GPU devices\nAs you will see, modifying the training loop from section 2.7 to optionally\nrun on a GPU is relatively simple and only requires changing three lines of\ncode.\nBefore we make the modifications, it's crucial to understand the main concept\nbehind GPU computations within PyTorch. First, we need to introduce the\nnotion of devices. In PyTorch, a device is where computations occur, and\ndata resides. The CPU and the GPU are examples of devices. A PyTorch\ntensor resides in a device, and its operations are executed on the same device.\nLet's see how this works in action. Assuming that you installed a GPU-\ncompatible version of PyTorch as explained in section 2.1.3, Installing\nPyTorch, we can double-check that our runtime indeed supports GPU\ncomputing via the following code:\nprint(torch.cuda.is_available())\nThe result is:\nTrue\nNow, suppose we have two tensors that we can add as follows -- this\ncomputation will be carried out on the CPU by default:\ntensor_1 = torch.tensor([1., 2., 3.])\ntensor_2 = torch.tensor([4., 5., 6.])\nprint(tensor_1 + tensor_2)\nThis outputs:\ntensor([5., 7., 9.])\nWe can now use the \n.to()\n method\n[1]\n to transfer these tensors onto a GPU\nand perform the addition there:\ntensor_1 = tensor_1.to(""cuda"")\ntensor_2 = tensor_2.to(""cuda"")\nprint(tensor_1 + tensor_2)\nThe output is as follows:\ntensor([5., 7., 9.], device='cuda:0')\nNotice that the resulting tensor now includes the device information,\ndevice='cuda:0'\n, which means that the tensors reside on the first GPU. If\nyour machine hosts multiple GPUs, you have the option to specify which\nGPU you'd like to transfer the tensors to. You can do this by indicating the\ndevice ID in the transfer command. For instance, you can use\n.to(""cuda:0"")\n, \n.to(""cuda:1"")\n, and so on.\nHowever, it is important to note that all tensors must be on the same device.\nOtherwise, the computation will fail, as shown below, where one tensor\nresides on the CPU and the other on the GPU:\ntensor_1 = tensor_1.to(""cpu"")\nprint(tensor_1 + tensor_2)\nThis results in the following:\nRuntimeError      Traceback (most recent call last)\n<ipython-input-7-4ff3c4d20fc3> in <cell line: 2>()\n      1 tensor_1 = tensor_1.to(""cpu"")\n----> 2 print(tensor_1 + tensor_2)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\nIn this section, we learned that GPU computations on PyTorch are relatively\nstraightforward. All we have to do is transfer the tensors onto the same GPU\ndevice, and PyTorch will handle the rest. Equipped with this information, we\ncan now train the neural network from the previous section on a GPU.\nA.9.2 Single-GPU training\nNow that we are familiar with transferring tensors to the GPU, we can modify\nthe training loop from \nsection 2.7, A typical training loop\n, to run on a GPU.\nThis requires only changing three lines of code, as shown in code listing A.11\nbelow.\nListing A.11 A training loop on a GPU\ntorch.manual_seed(123)\nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\n \ndevice = torch.device(""cuda"")   #A\nmodel = model.to(device)   #B\n \noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n \nnum_epochs = 3\n \nfor epoch in range(num_epochs):\n    \n    model.train()\n    for batch_idx, (features, labels) in enumerate(train_loader):\n \n        features, labels = features.to(device), labels.to(device)    #C\n        logits = model(features)\n        loss = F.cross_entropy(logits, labels) # Loss function\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n        ### LOGGING\n        print(f""Epoch: {epoch+1:03d}/{num_epochs:03d}""\n              f"" | Batch {batch_idx:03d}/{len(train_loader):03d}""\n              f"" | Train/Val Loss: {loss:.2f}"")\n \n    model.eval()\n    # Optional model evaluation\n \nRunning the above code will output the following, similar to the results\nobtained on the CPU previously in section 2.7:\nEpoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75\nEpoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65\nEpoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44\nEpoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13\nEpoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03\nEpoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00\nWe can also use \n.to(""cuda"")\n instead of \ndevice = torch.device(""cuda"")\n.\nAs we saw in section 2.9.1, transferring a tensor to ""cuda"" instead of\ntorch.device(""cuda"")\n works as well and is shorter. We can also modify the\nstatement to the following, which will make the same code executable on a\nCPU if a GPU is not available, which is usually considered best practice\nwhen sharing PyTorch code:\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nIn the case of the modified training loop above, we probably won't see a\nspeed-up because of the memory transfer cost from CPU to GPU. However,\nwe can expect a significant speed-up when training deep neural networks,\nespecially large language models.\nAs we saw in this section, training a model on a single GPU in PyTorch is\nrelatively easy. Next, let's introduce another concept: training models on\nmultiple GPUs.\nPyTorch on macOS\nOn an Apple Mac with an Apple Silicon chip (like the M1, M2, M3, or newer\nmodels) instead of a computer with an Nvidia GPU, you can change\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nto\ndevice = torch.device(""mps"" if torch.backends.mps.is_available() else ""cpu"")\nto take advantage of this chip.\nExercise A.4\nCompare the runtime of matrix multiplication on a CPU to a GPU. At what\nmatrix size do you begin to see the matrix multiplication on the GPU being\nfaster than on the CPU? Hint: I recommend using the \n%timeit\n command in\nJupyter to compare the runtime. For example, given matrices \na\n and \nb\n, run the\ncommand \n%timeit a @ b\n in a new notebook cell.\nA.9.3 Training with multiple GPUs\nIn this section, we will briefly go over the concept of distributed training.\nDistributed training is the concept of dividing the model training across\nmultiple GPUs and machines.\nWhy do we need this? Even when it is possible to train a model on a single\nGPU or machine, the process could be exceedingly time-consuming. The\ntraining time can be significantly reduced by distributing the training process\nacross multiple machines, each with potentially multiple GPUs. This is\nparticularly crucial in the experimental stages of model development, where\nnumerous training iterations might be necessary to finetune the model\nparameters and architecture.\nMulti-GPU computing is optional\nFor this book, it is not required to have access to or use multiple-GPU. This\nsection is included for those who are interested in how multi-GPU computing\nworks in PyTorch.\nIn this section, we will look at the most basic case of distributed training:\nPyTorch's \nDistributedDataParallel\n (DDP) strategy. DDP enables\nparallelism by splitting the input data across the available devices and\nprocessing these data subsets simultaneously.\nHow does this work? PyTorch launches a separate process on each GPU, and\neach process receives and keeps a copy of the model -- these copies will be\nsynchronized during training. To illustrate this, suppose we have two GPUs\nthat we want to use to train a neural network, as shown in figure A.12.\nFigure A.12 The model and data transfer in DDP involves two key steps. First, we create a copy\nof the model on each of the GPUs. Then we divide the input data into unique minibatches that we\npass on to each model copy.\nEach of the two GPUs will receive a copy of the model. Then, in every\ntraining iteration, each model will receive a minibatch (or just batch) from the\ndata loader. We can use a \nDistributedSampler\n to ensure that each GPU will\nreceive a different, non-overlapping batch when using DDP.\nSince each model copy will see a different sample of the training data, the\nmodel copies will return different logits as outputs and compute different\ngradients during the backward pass. These gradients are then averaged and\nsynchronized during training to update the models. This way, we ensure that\nthe models don't diverge, as illustrated in figure A.13.\nFigure A.13 The forward and backward pass in DDP are executed independently on each GPU\nwith its corresponding data subset. Once the forward and backward passes are completed,\ngradients from each model replica (on each GPU) are synchronized across all GPUs. This ensures\nthat every model replica has the same updated weights.\nThe benefit of using DDP is the enhanced speed it offers for processing the\ndataset compared to a single GPU. Barring a minor communication overhead\nbetween devices that comes with DDP use, it can theoretically process a\ntraining epoch in half the time with two GPUs compared to just one. The time\nefficiency scales up with the number of GPUs, allowing us to process an\nepoch eight times faster if we have eight GPUs, and so on.\nMulti-GPU computing in interactive environments\nDDP does not function properly within interactive Python environments like\nJupyter notebooks, which don't handle multiprocessing in the same way a\nstandalone Python script does. Therefore, the following code should be\nexecuted as a script, not within a notebook interface like Jupyter. This is\nbecause DDP needs to spawn multiple processes, and each process should\nhave its own Python interpreter instance.\nLet's now see how this works in practice. For brevity, we will only focus on\nthe core parts of the previous code that need to be adjusted for DDP training.\nHowever, for readers who want to run the code on their own multi-GPU\nmachine or a cloud instance of their choice, it is recommended to use the\nstandalone script provided in this book's GitHub repository at\nhttps://github.com/rasbt/LLMs-from-scratch\n.\nFirst, we will import a few additional submodules, classes, and functions for\ndistributed training PyTorch as shown in code listing A.13 below.\nListing A.12 PyTorch utilities for distributed training\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nBefore we dive deeper into the changes to make the training compatible with\nDDP, let's briefly go over the rationale and usage for these newly imported\nutilities that we need alongside the \nDistributedDataParallel\n class.\nPyTorch's \nmultiprocessing\n submodule contains functions such as\nmultiprocessing.spawn\n, which we will use to spawn multiple processes and\napply a function to multiple inputs in parallel. We will use it to spawn one\ntraining process per GPU.\nIf we spawn multiple processes for training, we will need a way to divide the\ndataset among these different processes. For this, we will use the\nDistributedSampler\n.\nThe \ninit_process_group\n and \ndestroy_process_group\n are used to initialize\nand quit the distributed training mods. The \ninit_process_group\n function\nshould be called at the beginning of the training script to initialize a process\ngroup for each process in the distributed setup, and \ndestroy_process_group\nshould be called at the end of the training script to destroy a given process\ngroup and release its resources.\nThe following code in listing A.13 below illustrates how these new\ncomponents are used to implement DDP training for the \nNeuralNetwork\nmodel we implemented earlier.\nListing A.13 Model training with DistributedDataParallel strategy\ndef ddp_setup(rank, world_size):\n    os.environ[""MASTER_ADDR""] = ""localhost""   #A\n    os.environ[""MASTER_PORT""] = ""12345""       #B\n    init_process_group(\n        backend=""nccl"",                       #C\n        rank=rank,                            #D\n        world_size=world_size                 #E\n    )\n    torch.cuda.set_device(rank)               #F\ndef prepare_dataset():\n    \n...\n    \ntrain_loader = DataLoader(\n        \ndataset=train_ds,\n        \nbatch_size=2,\n        \nshuffle=False,\n                        \n#G\n        \npin_memory=True,\n                      \n#H\n        \ndrop_last=True,\n        \nsampler=DistributedSampler(train_ds)\n  \n#I\n    \n)\n    \n    \nreturn train_loader, test_loader\ndef main(rank, world_size, num_epochs):\n       \n#J\n    \nddp_setup(rank, world_size)\n    \ntrain_loader, test_loader = prepare_dataset()\n    \nmodel = NeuralNetwork(num_inputs=2, num_outputs=2)\n    \nmodel.to(rank)\n    \noptimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n    \nmodel = DDP(model, device_ids=[rank])\n    \nfor epoch in range(num_epochs):\n    \nfor features, labels in train_loader:\n            \nfeatures, labels = features.to(rank), labels.to(rank)\n  \n            \n...\n            \nprint(f""[GPU{rank}] Epoch: {epoch+1:03d}/{num_epochs:03d}""\n                  \nf"" | Batchsize {labels.shape[0]:03d}""\n                  \nf"" | Train/Val Loss: {loss:.2f}"")\n    \nmodel.eval()\n    \ntrain_acc = compute_accuracy(model, train_loader, device=rank)\n    \nprint(f""[GPU{rank}] Training accuracy"", train_acc)\n    \ntest_acc = compute_accuracy(model, test_loader, device=rank)\n    \nprint(f""[GPU{rank}] Test accuracy"", test_acc)\n    \ndestroy_process_group()\n                    \n#L\nif __name__ == ""__main__"":\n    \nprint(""Number of GPUs available:"", torch.cuda.device_count())\n    \ntorch.manual_seed(123)\n    \nnum_epochs = 3\n    \nworld_size = torch.cuda.device_count()\n    \nmp.spawn(main, args=(world_size, num_epochs), nprocs=world_size) #M\nBefore we run the code from listing A.13, here is a summary of how it works,\nin addition to the annotations above. We have a \n__name__ == ""__main__""\nclause at the bottom containing code that is executed when we run the code\nas a Python script instead of importing it as a module. This code first prints\nthe number of available GPUs using \ntorch.cuda.device_count()\n, sets a\nrandom seed for reproducibility and then spawns new processes using\nPyTorch's \nmultiprocesses.spawn\n function. Here, the \nspawn\n function\nlaunches one process per GPU setting \nnproces=world_size\n, where the world\nsize is the number of available GPUs. This \nspawn\n function launches the code\nin the \nmain\n function we define in the same script with some additional\narguments provided via \nargs\n. Note that the \nmain\n function has a \nrank\nargument that we don't include in the \nmp.spawn()\n call. That's because the\nrank\n, which refers to the process ID we use as the GPU ID, is already passed\nautomatically.\nThe \nmain\n function sets up the distributed environment via \nddp_setup\n --\nanother function we defined, loads the training and test sets, sets up the\nmodel, and carries out the training. Compared to the single-GPU training in\nsection 2.12, we now transfer the model and data to the target device via\n.\nto(rank)\n, which we use to refer to the GPU device ID. Also, we wrap the\nmodel via \nDDP\n, which enables the synchronization of the gradients between\nthe different GPUs during training. After the training finishes and we\nevaluate the models, we use \ndestroy_process_group()\n to cleanly exit the\ndistributed training and free up the allocated resources.\nEarlier, we mentioned that each GPU will receive a different subsample of\nthe training data. To ensure this, we set\nsampler=DistributedSampler(train_ds)\n in the training loader.\nThe last function to discuss is \nddp_setup\n. It sets the main node's address and\nport to allow for communication between the different processes, initializes\nthe process group with the NCCL backend (designed for GPU-to-GPU\ncommunication), and sets the \nrank\n (process identifier) and world size (total\nnumber of processes). Finally, it specifies the GPU device corresponding to\nthe current model training process rank.\nSelecting available GPUs on a multi-GPU machine\nIf you wish to restrict the number of GPUs used for training on a multi-GPU\nmachine, the simplest way is to use the \nCUDA_VISIBLE_DEVICES\n environment\nvariable. To illustrate this, suppose your machine has multiple GPUs, and\nyou only want to use one GPU, for example, the GPU with index 0. Instead\nof \npython some_script.py\n, you can run the code from the terminal as\nfollows:\nCUDA_VISIBLE_DEVICES=0 python some_script.py\nOr, if your machine has four GPUs and you only want to use the first and\nthird GPU, you can use\nCUDA_VISIBLE_DEVICES=0,2 python some_script.py\nSetting \nCUDA_VISIBLE_DEVICES\n in this way is a simple and effective way to\nmanage GPU allocation without modifying your PyTorch scripts.\nLet's now run this code and see how it works in practice by launching the\ncode as a script from the terminal:\npython ch02-DDP-script.py\nNote that it should work on both single- and multi-GPU machines. If we run\nthis code on a single GPU, we should see the following output:\nPyTorch version: 2.0.1+cu117\nCUDA available: True\nNumber of GPUs available: 1\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.62\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.32\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.11\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.07\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.02\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.03\n[GPU0] Training accuracy 1.0\n[GPU0] Test accuracy 1.0\nThe code output looks similar to the one in section 2.9.2, which is a good\nsanity check.\nNow, if we run the same command and code on a machine with two GPUs,\nwe should see the following:\nPyTorch version: 2.0.1+cu117\nCUDA available: True\nNumber of GPUs available: 2\n[GPU1] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.60\n[GPU0] Epoch: 001/003 | Batchsize 002 | Train/Val Loss: 0.59\n[GPU0] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.16\n[GPU1] Epoch: 002/003 | Batchsize 002 | Train/Val Loss: 0.17\n[GPU0] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\n[GPU1] Epoch: 003/003 | Batchsize 002 | Train/Val Loss: 0.05\n[GPU1] Training accuracy 1.0\n[GPU0] Training accuracy 1.0\n[GPU1] Test accuracy 1.0\n[GPU0] Test accuracy 1.0\nAs expected, we can see that some batches are processed on the first GPU\n(\nGPU0\n) and others on the second (\nGPU1\n). However, we see duplicated output\nlines when printing the training and test accuracies. This is because each\nprocess (in other words, each GPU) prints the test accuracy independently.\nSince DDP replicates the model onto each GPU and each process runs\nindependently, if you have a print statement inside your testing loop, each\nprocess will execute it, leading to repeated output lines.\nIf this bothers you, you can fix this using the rank of each process to control\nyour print statements.\nif rank == 0: # only print in the first process\nprint(""Test accuracy: "", accuracy)\nThis is, in a nutshell, how distributed training via DDP works. If you are\ninterested in additional details, I recommend checking the official API\ndocumentation at\nhttps://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html\nAlternative PyTorch APIs for multi-GPU training\nIf you prefer more straightforward ways to use multiple GPUs in PyTorch,\nyou can also consider add-on APIs like the open-source Fabric library, which\nI've written about in Accelerating PyTorch Model Training: Using Mixed-\nPrecision and Fully Sharded Data Parallelism\nhttps://magazine.sebastianraschka.com/p/accelerating-pytorch-model-\ntraining\n.\nA.10 Summary\nPyTorch is an open-source library that consists of three core\ncomponents: a tensor library, automatic differentiation functions, and\ndeep learning utilities.\nPyTorch's tensor library is similar to array libraries like NumPy\nIn the context of PyTorch, tensors are array-like data structures to\nrepresent scalars, vectors, matrices, and higher-dimensional arrays.\nPyTorch tensors can be executed on the CPU, but one major advantage\nof PyTorch's tensor format is its GPU support to accelerate\ncomputations.\nThe automatic differentiation (autograd) capabilities in PyTorch allow\nus to conveniently train neural networks using backpropagation without\nmanually deriving gradients.\nThe deep learning utilities in PyTorch provide building blocks for\ncreating custom deep neural networks.\nPyTorch includes \nDataset\n and \nDataLoader\n classes to set up efficient\ndata loading pipelines.\nIt's easiest to train models on a CPU or single GPU.\nUsing \nDistributedDataParallel\n is the simplest way in PyTorch to\naccelerate the training if multiple GPUs are available.\nA.11 Further reading\nWhile this chapter should be sufficient to get you up to speed, in addition, if\nyou are looking for more comprehensive introductions to deep learning, I\nrecommend the following books:\nMachine Learning with PyTorch and Scikit-Learn\n (2022) by Sebastian\nRaschka, Hayden Liu, and Vahid Mirjalili. ISBN 978-1801819312\nDeep Learning with PyTorch\n (2021) by Eli Stevens, Luca Antiga, and\nThomas Viehmann. ISBN 978-1617295263\nFor a more thorough introduction to the concepts of tensors, readers can find\na 15 min video tutorial that I recorded:\nLecture 4.1: Tensors in Deep Learning,\nhttps://www.youtube.com/watch?v=JXfDlgrfOBY\nIf you want to learn more about model evaluation in machine learning, I\nrecommend my article:\nModel Evaluation, Model Selection, and Algorithm Selection in Machine\nLearning\n (2018) by Sebastian Raschka, \nhttps://arxiv.org/abs/1811.12808\nFor readers who are interested in a refresher or gentle introduction to\ncalculus, I've written a chapter on calculus that is freely available on my\nwebsite:\nIntroduction to Calculus\n by Sebastian Raschka,\nhttps://sebastianraschka.com/pdf/supplementary/calculus.pdf\nWhy does PyTorch not call \noptimizer.zero_grad()\n automatically for us in\nthe background? In some instances, it may be desirable to accumulate the\ngradients, and PyTorch will leave this as an option for us. If you want to learn\nmore about gradient accumulation, please see the following article:\nFinetuning Large Language Models On A Single GPU Using Gradient\nAccumulation\n by Sebastian Raschka,\nhttps://sebastianraschka.com/blog/2023/llm-grad-accumulation.html\nThis chapter covered DDP, which is a popular approach for training deep\nlearning models across multiple GPUs. For more advanced use cases where a\nsingle model doesn't fit onto the GPU, you may also consider PyTorch's \nFully\nSharded Data Parallel\n (FSDP) method, which performs distributed data\nparallelism and distributes large layers across different GPUs. For more\ninformation, see this overview with further links to the API documentation:\nIntroducing PyTorch Fully Sharded Data Parallel (FSDP) API,\nhttps://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-\napi/\nA.12 Exercise answers\nExercise A.3:\nThe network has 2 inputs and 2 outputs. In addition, there are 2 hidden layers\nwith 30 and 20 nodes, respectively. Programmatically, we can calculate the\nnumber of parameters as follows:\nmodel = NeuralNetwork(2, 2)\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(""Total number of trainable model parameters:"", num_params)\nThis returns:\n752\nWe can also calculate this manually as follows:\nfirst hidden layer: 2 inputs times 30 hidden units plus 30 bias units.\nsecond hidden layer: 30 incoming units times 20 nodes plus 20 bias\nunits.\noutput layer: 20 incoming nodes times 2 output nodes plus 2 bias units.\nThen, adding all the parameters in each layer results in 2×30+30 + 30×20+20\n+ 20×2+2 = 752.\nExercise A.4:\nThe exact runtime results will be specific to the hardware used for this\nexperiment. In my experiments, I observed significant speed-ups even for\nsmall matrix multiplications as the following one when using a Google Colab\ninstance connected to a V100 GPU:\na = torch.rand(100, 200)\nb = torch.rand(200, 300)\n%timeit a@b\nOn the CPU this resulted in:\n63.8 µs ± 8.7 µs per loop\nWhen executed on a GPU:\na, b = a.to(""cuda""), b.to(""cuda"")\n%timeit a @ b\nThe result was:\n13.8 µs ± 425 ns per loop\nIn this case, on a V100, the computation was approximately four times faster.\n[1]\n This is the same .to() method we previously used to change a tensor's\ndatatype in section 2.2.2, Tensor data types.",80861
08-Appendix_B._References_and_Further_Reading.pdf,08-Appendix_B._References_and_Further_Reading,"Appendix B. References and\nFurther Reading\nB.1 Chapter 1\nCustom-built LLMs are able to outperform general-purpose LLMs as a team\nat Bloomberg showed via a version of GPT pretrained on finance data from\nscratch. The custom LLM outperformed ChatGPT on financial tasks while\nmaintaining good performance on general LLM benchmarks:\nBloombergGPT: A Large Language Model for Finance\n (2023) by Wu \net\nal.\n, \nhttps://arxiv.org/abs/2303.17564\nExisting LLMs can be adapted and finetuned to outperform general LLMs as\nwell, which teams from Google Research and Google DeepMind showed in a\nmedical context:\nTowards Expert-Level Medical Question Answering with Large\nLanguage Models\n (2023) by Singhal \net al.\n,\nhttps://arxiv.org/abs/2305.09617\nThe paper that proposed the original transformer architecture:\nAttention Is All You Need\n (2017) by Vaswani \net al.\n,\nhttps://arxiv.org/abs/1706.03762\nThe original encoder-style transformer, called BERT:\nBERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding\n (2018) by Devlin \net al.\n, \nhttps://arxiv.org/abs/1810.04805\nThe paper describing the decoder-style GPT-3 model, which inspired modern\nLLMs and will be used as a template for implementing an LLM from scratch\nin this book:\nLanguage Models are Few-Shot Learners\n (2020) by Brown \net al.\n,\nhttps://arxiv.org/abs/2005.14165\nThe original vision transformer for classifying images, which illustrates that\ntransformer architectures are not only restricted to text inputs:\nAn Image is Worth 16x16 Words: Transformers for Image Recognition\nat Scale \n(2020) by Dosovitskiy \net al.\n, \nhttps://arxiv.org/abs/2010.11929\nTwo experimental (but less popular) LLM architectures that serve as\nexamples that not all LLMs need to be based on the transformer architecture:\nRWKV: Reinventing RNNs for the Transformer Era\n (2023) by Peng \net\nal.\n, \nhttps://arxiv.org/abs/2305.13048\nHyena Hierarchy: Towards Larger Convolutional Language Models\n(2023) \nby Poli\n et al., \nhttps://arxiv.org/abs/2302.10866\nMamba: Linear-Time Sequence Modeling with Selective State Spaces\n(2023) by Gu and Dao, \nhttps://arxiv.org/abs/2312.00752\nMeta AI's model is a popular implementation of a GPT-like model that is\nopenly available in contrast to GPT-3 and ChatGPT:\nLlama 2: Open Foundation and Fine-Tuned Chat Models\n (2023) by\nTouvron \net al.\n, \nhttps://arxiv.org/abs/2307.09288\n1\nFor readers interested in additional details about the dataset references in\nsection 1.5, this paper describes the publicly available \nThe Pile\n dataset\ncurated by Eleuther AI:\nThe Pile: An 800GB Dataset of Diverse Text for Language Modeling\n(2020) by Gao\n et al.\n, \nhttps://arxiv.org/abs/2101.00027\n.\nThe following paper provides the reference for InstructGPT for finetuning\nGPT-3, which was mentioned in section 1.6 and will be discussed in more\ndetail in chapter 7:\nTraining Language Models to Follow Instructions with Human\nFeedback\n (2022) by \nOuyang et al.\n, \nhttps://arxiv.org/abs/2203.02155\nB.2 Chapter 2\nReaders who are interested in discussion and comparison of embedding\nspaces with latent spaces and the general notion of vector representations can\nfind more information in the first chapter of my book Machine Learning Q\nand AI:\nMachine Learning Q and AI\n (2023) by Sebastian Raschka,\nhttps://leanpub.com/machine-learning-q-and-ai\nThe following paper provides more in-depth discussions of how how byte\npair encoding is used as a tokenization method:\nNeural Machine Translation of Rare Words with Subword Units (2015)\nby Sennrich at al., \nhttps://arxiv.org/abs/1508.07909\nThe code for the byte pair encoding tokenizer used to train GPT-2 was open-\nsourced by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/encoder.py\nOpenAI provides an interactive web UI to illustrate how the byte pair\ntokenizer in GPT models works:\nhttps://platform.openai.com/tokenizer\nFor readers interested in coding and training a BPE tokenizer from the\nground up, Andrej Karpathy's GitHub repository \nminbpe\n offers a minimal and\nreadable implementation:\nA minimal implementation of a BPE tokenizer,\nhttps://github.com/karpathy/minbpe\nReaders who are interested in studying alternative tokenization schemes that\nare used by some other popular LLMs can find more information in the\nSentencePiece and WordPiece papers:\nSentencePiece: A Simple and Language Independent Subword\nTokenizer and Detokenizer for Neural Text Processing (2018) by Kudo\nand Richardson, \nhttps://aclanthology.org/D18-2012/\nFast WordPiece Tokenization (2020) by Song et al.,\nhttps://arxiv.org/abs/2012.15524\nB.3 Chapter 3\nReaders interested in learning more about Bahdanau attention for RNN and\nlanguage translation can find detailed insights in the following paper:\nNeural Machine Translation by Jointly Learning to Align and Translate\n(2014) by Bahdanau, Cho, and Bengio, \nhttps://arxiv.org/abs/1409.0473\nThe concept of self-attention as scaled dot-product attention was introduced\nin the original transformer paper:\nAttention Is All You Need\n (2017) by Vaswani et al.,\nhttps://arxiv.org/abs/1706.03762\nFlashAttentio\nn is a highly efficient implementation of self-attention\nmechanism, which accelerates the computation process by optimizing\nmemory access patterns. FlashAttention is mathematically the same as the\nstandard self-attention mechanism but optimizes the computational process\nfor efficiency:\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-\nAwarenes\ns (2022) by Dao \net al.\n, \nhttps://arxiv.org/abs/2205.14135\nFlashAttention-2: Faster Attention with Better Parallelism and Work\nPartitioning\n (2023) by Dao, \nhttps://arxiv.org/abs/2307.08691\nPyTorch implements a function for self-attention and causal attention that\nsupports FlashAttention for efficiency. This function is beta and subject to\nchange:\nscaled_dot_product_attention\n documentation:\nhttps://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\nPyTorch also implements an efficient \nMultiHeadAttention\n class based on\nthe \nscaled_dot_product\n function:\nMultiHeadAttention\n documentation:\nhttps://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\nDropout is a regularization technique used in neural networks to prevent\noverfitting by randomly dropping units (along with their connections) from\nthe neural network during training:\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n(2014) by Srivastava \net al.\n,\nhttps://jmlr.org/papers/v15/srivastava14a.html\nWhile using the multi-head attention based on scaled-dot product attention\nremains the most common variant of self-attention in practice, authors found\nthat it's possible to also achieve good performance without the value weight\nmatrix and projection layer:\nSimplifying Transformer Blocks\n (2023) by He and Hofmann,\nhttps://arxiv.org/abs/2311.01906\nB.4 Chapter 4\nThe layer normalization paper, titled ""Layer Normalization,"" introduces a\ntechnique that stabilizes the hidden state dynamics neural networks by\nnormalizing the summed inputs to the neurons within a hidden layer,\nsignificantly reducing training time compared to previously published\nmethods:\nLayer Normalization\n (2016) by Ba, Kiros, and Hinton,\nhttps://arxiv.org/abs/1607.06450\nPost-LayerNorm, used in the original Transformer model, applies layer\nnormalization after the self-attention and feed forward networks. In contrast,\nPre-LayerNorm, as adopted in models like GPT-2 and newer LLMs, applies\nlayer normalization before these components, which can lead to more stable\ntraining dynamics and has been shown to improve performance in some\ncases, as discussed in the following papers:\nOn Layer Normalization in the Transformer Architecture\n (2020) by\nXiong \net al.\n, \nhttps://arxiv.org/abs/2002.04745\nResiDual: Transformer with Dual Residual Connections\n (2023) by Tie\net al.\n, \nhttps://arxiv.org/abs/2304.14802\nA popular variant of LayerNorm used in modern LLMs is RMSNorm due to\nits improved computing efficiency. This variant simplifies the normalization\nprocess by normalizing the inputs using only the root mean square of the\ninputs, without subtracting the mean before squaring. This means it does not\ncenter the data before computing the scale. RMSNorm is described in more\ndetail in the following paper:\nRoot Mean Square Layer Normalization\n (2019) by Zhang and Sennrich,\nhttps://arxiv.org/abs/1910.07467\nThe GELU (Gaussian Error Linear Unit) activation function combines the\nproperties of both the classic ReLU activation function and the normal\ndistribution's cumulative distribution function to model layer outputs,\nallowing for stochastic regularization and non-linearities in deep learning\nmodels, as introduced in the following paper:\nGaussian Error Linear Units (GELUs)\n (2016) by Hendricks and\nGimpel, \nhttps://arxiv.org/abs/1606.08415\nThe GPT-2 paper introduced a series of transformer-based LLMs with\nvarying sizes—124M, 355M, 774M, and 1.5B parameters:\nLanguage Models are Unsupervised Multitask Learners\n (2019) by\nRadford \net al.\n, \nhttps://d4mucfpksywv.cloudfront.net/better-language-\nmodels/language_models_are_unsupervised_multitask_learners.pdf\nOpenAI's GPT-3 uses fundamentally the same architecture as GPT-2, except\nthat the largest version (175 billion) is 100x larger than the largest GPT-2\nmodel and has been trained on much more data. Interested readers can refer\nto the official GPT-3 paper by OpenAI and the technical overview by\nLambda Labs, which calculates that training GPT-3 on a single RTX 8000\nconsumer GPU would take 665 years:\nLanguage Models are Few-Shot Learners (2023) by Brown et al.,\nhttps://arxiv.org/abs/2005.14165\nOpenAI's GPT-3 Language Model: A Technical Overview,\nhttps://lambdalabs.com/blog/demystifying-gpt-3\nNanoGPT is a code repository with a minimalist yet efficient implementation\nof a GPT-2 model, similar to the model implemented in this book. While the\ncode in this book is different from nanoGPT, this repository inspired the\nreorganization of a large GPT Python parent class implementation into\nsmaller submodules:\nNanoGPT, a repository for training medium-sized GPTs,\nhttps://github.com/karpathy/nanoGPT\nAn informative blog post showing that most of the computation in LLMs is\nspent in the feed forward layers rather than attention layers when the context\nsize is smaller than 32,000 tokens:\n""In the long (context) run"" by Harm de Vries,\nhttps://www.harmdevries.com/post/context-length/\nB.5 Chapter 5\nA video lecture by the author detailing the loss function and applying a log\ntransformation to make it easier to handle for mathematical optimization:\nL8.2 Logistic Regression Loss Function,\nhttps://www.youtube.com/watch?v=GxJe0DZvydM\nThe following two papers detail the dataset, hyperparameter, and architecture\ndetails used for pretraining LLMs:\nPythia: A Suite for Analyzing Large Language Models Across Training\nand Scaling (2023) by Biderman \net al.\n, \nhttps://arxiv.org/abs/2304.01373\nOLMo: Accelerating the Science of Language Models (2024) by\nGroeneveld \net al.\n, \nhttps://arxiv.org/abs/2402.00838\nThe following supplementary code available for this book contains\ninstructions for preparing 60,000 public domain books from Project\nGutenberg for LLM training:\nPretraining GPT on the Project Gutenberg Dataset,\nhttps://github.com/rasbt/LLMs-from-\nscratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg\nChapter 5 discusses the pretraining of LLMs, and Appendix D covers more\nadvanced training functions, such as linear warmup and cosine annealing.\nThe following paper finds that similar techniques can be successfully applied\nto continue pretraining already pretrained LLMs, along with additional tips\nand insights:\nSimple and Scalable Strategies to Continually Pre-train Large Language\nModels (2024) by Ibrahim \net al.\n, \nhttps://arxiv.org/abs/2403.08763\nBloombergGPT is an example of a domain-specific large language model\n(LLM) created by training on both general and domain-specific text corpora,\nspecifically in the field of finance:\nBloombergGPT: A Large Language Model for Finance (2023) by Wu \net\nal.\n, \nhttps://arxiv.org/abs/2303.17564\nGaLore is a recent research project that aims to make LLM pretraining more\nefficient. The required code change boils down to just replacing PyTorch's\nAdamW\n optimizer in the training function with the \nGaLoreAdamW\n optimizer\nprovided by the \ngalore-torch\n Python package.\nGaLore: Memory-Efficient LLM Training by Gradient Low-Rank\nProjection (2024) by Zhao \net al.\n, \nhttps://arxiv.org/abs/2403.03507\nGaLore code repository, \nhttps://github.com/jiaweizzhao/GaLore\nThe following papers and resources share openly available, large-scale\npretraining datasets for LLMs that consist of hundreds of gigabytes to\nterabytes of text data:\nDolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining\nResearch by Soldaini \net al.\n 2024, \nhttps://arxiv.org/abs/2402.00159\nThe Pile: An 800GB Dataset of Diverse Text for Language Modeling by\nGao et al. 2020, \nhttps://arxiv.org/abs/2101.00027\nThe RefinedWeb Dataset for Falcon LLM: Outperforming Curated\nCorpora with Web Data, and Web Data Only, by Penedo \net al.\n (2023)\nhttps://arxiv.org/abs/2306.01116\nRedPajama by Together AI,\nhttps://github.com/togethercomputer/RedPajama-Data\nThe paper that originally introduced top-k sampling:\nHierarchical Neural Story Generation by Fan \net al.\n (2018),\nhttps://arxiv.org/abs/1805.04833\nBeam search (not cover in chapter 5) is an alternative decoding algorithm that\ngenerates output sequences by keeping only the top-scoring partial sequences\nat each step to balance efficiency and quality:\nDiverse Beam Sea\nrch: Decoding Diverse Solutions from Neural\nSequence Models by Vijayakumar \net al. (2016),\nhttps://arxiv.org/abs/1610.02424",13994
09-Appendix_C._Exercise_Solutions.pdf,09-Appendix_C._Exercise_Solutions,"Appendix C. Exercise Solutions\nThe complete code examples for the exercises answers can be found in the\nsupplementary GitHub repository at \nhttps://github.com/rasbt/LLMs-from-\nscratch\n.\nC.1 Chapter 2\nExercise 2.1\nYou can obtain the individual token IDs by prompting the encoder with one\nstring at a time:\nprint(tokenizer.encode(""Ak""))\nprint(tokenizer.encode(""w""))\n# ...\nThis prints:\n[33901]\n[86]\n# ...\nYou can then use the following code to assemble the original string:\nprint(tokenizer.decode([33901, 86, 343, 86, 220, 959]))\nThis returns:\n'Akwirw ier'\nExercise 2.2\nThe code for the data loader with \nmax_length=2 and stride=2\n:\ndataloader = create_dataloader(raw_text, batch_size=4, max_length=2, stride=2)\nIt produces batches of the following format:\ntensor([[  40,  367],\n        [2885, 1464],\n        [1807, 3619],\n        [ 402,  271]])\nThe code of the second data loader with \nmax_length=8 and stride=2\n:\ndataloader = create_dataloader(raw_text, batch_size=4, max_length=8, stride=2)\nAn example batch looks like as follows:\ntensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]])\nC.2 Chapter 3\nExercise 3.1\nThe correct weight assignment is as follows:\nsa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\nsa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\nsa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\nExercise 3.2\nTo achieve an output dimension of 2, similar to what we had in single-head\nattention, we need to change the projection dimension \nd_out\n to 1.\nd_out = 1\nmha = MultiHeadAttentionWrapper(d_in, d_out, block_size, 0.0, num_heads=2)\nExercise 3.3\nThe initialization for the smallest GPT-2 model is as follows:\nblock_size = 1024\nd_in, d_out = 768, 768\nnum_heads = 12\nmha = MultiHeadAttention(d_in, d_out, block_size, 0.0, num_heads)\n \nC.3 Chapter 4\nExercise 4.1\nWe can calculate the number of parameters in the feed forward and attention\nmodules as follows:\nblock = TransformerBlock(GPT_CONFIG_124M)\n \ntotal_params = sum(p.numel() for p in block.ff.parameters())\nprint(f""Total number of parameters in feed forward module: {total_params:,}"")\n \ntotal_params = sum(p.numel() for p in block.att.parameters())\nprint(f""Total number of parameters in attention module: {total_params:,}"")\nAs we can see, the feed forward module contains approximately twice as\nmany parameters as the attention module:\nTotal number of parameters in feed forward module: 4,722,432\nTotal number of parameters in attention module: 2,360,064\nExercise 4.2\nTo instantiate the other GPT model sizes, we can modify the configuration\ndictionary as follows (here shown for GPT-2 XL):\nGPT_CONFIG = GPT_CONFIG_124M.copy()\nGPT_CONFIG[""emb_dim""] = 1600\nGPT_CONFIG[""n_layers""] = 48\nGPT_CONFIG[""n_heads""] = 25\nmodel = GPTModel(GPT_CONFIG)\nThen, reusing the code from Section 4.6 to calculate the number of\nparameters and RAM requirements, we find the following:\ngpt2-xl:\nTotal number of parameters: 1,637,792,000\nNumber of trainable parameters considering weight tying: 1,557,380,800\nTotal size of the model: 6247.68 MB\nC.4 Chapter 5\nExercise 5.1\nWe can print the number of times the token (or word) ""pizza"" is sampled\nusing the \nprint_sampled_tokens\n function we defined in this section. Let's\nstart with the code we defined in section 5.3.1.\nThe ""pizza"" token is sampled 0x if the temperature is 0 or 0.1, and it is\nsampled 32× if the temperature is scaled up to 5. The estimated probability is\n32/1000 × 100% = 3.2%.\nThe actual probability is 4.3% and contained in the rescaled softmax\nprobability tensor (\nscaled_probas[2][6]\n).\nExercise 5.2\nTop-k sampling and temperature scaling are settings that have to be adjusted\nbased on the LLM and the desired degree of diversity and randomness in the\noutput.\nWhen using relatively small top-k values (e.g., smaller than 10) and the\ntemperature is set below 1, the model's output becomes less random and more\ndeterministic. This setting is useful when we need the generated text to be\nmore predictable, coherent, and closer to the most likely outcomes based on\nthe training data.\nApplications for such low k and temperature settings include generating\nformal documents or reports where clarity and accuracy are most important.\nOther examples of applications include technical analysis or code generation\ntasks, where precision is crucial. Also, question answering and educational\ncontent require accurate answers where a temperature below 1 is helpful.\nOn the other hand, larger top-k values (e.g., values in the range of 20 to 40)\nand temperature values above 1 are useful when using LLMs for\nbrainstorming or generating creative content, such as fiction.\nExercise 5.3\nThere are multiple ways to force deterministic behavior with the \ngenerate\nfunction:\n1\n. \nSetting to \ntop_k=None\n and applying no temperature scaling;\n2\n. \nSetting \ntop_k=1\n.\nExercise 5.4\nIn essence, we have to load the model and optimizer that we saved in the\nmain chapter:\ncheckpoint = torch.load(""model_and_optimizer.pth"")\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.load_state_dict(checkpoint[""model_state_dict""])\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\noptimizer.load_state_dict(checkpoint[""optimizer_state_dict""])\nThen, call the \ntrain_simple_function\n with \nnum_epochs=1\n to train the model\nfor another epoch.\nExercise 5.5\nWe can use the following code to calculate the training and validation set\nlosses of the GPT model:\ntrain_loss = calc_loss_loader(train_loader, gpt, device)\nval_loss = calc_loss_loader(val_loader, gpt, device)\nThe resulting losses for the 124M parameter are as follows:\nTraining loss: 3.754748503367106\nValidation loss: 3.559617757797241\nThe main observation is that the training and validation set performances are\nin the same ballpark. This can have multiple explanations.\n1\n. \nThe Verdict was not part of the pretraining dataset when OpenAI trained\nGPT-2. Hence, the model is not explicitly overfitting to the training set\nand performs similarly well on The Verdict's training and validation set\nportions. (The validation set loss is slightly lower than the training set\nloss, which is unusual in deep learning. However, it's likely due to\nrandom noise since the dataset is relatively small. In practice, if there is\nno overfitting, the training and validation set performances are expected\nto be roughly identical).\n2\n. \nThe Verdict was part of GPT -2's training dataset. In this case, we can't\ntell whether the model is overfitting the training data because the\nvalidation set would have been used for training as well. To evaluate the\ndegree of overfitting, we'd need a new dataset generated after OpenAI\nfinished training GPT-2 to make sure that it couldn't have been part of\nthe pretraining.\nExercise 5.6\nIn the main chapter, we experimented with the smallest GPT-2 model, which\nhas only 124M parameters. The reason was to keep the resource requirements\nas low as possible. However, you can easily experiment with larger models\nwith minimal code changes. For example, instead of loading the 1558M\ninstead of 124M model in chapter 5, the only 2 lines of code that we have to\nchange are the following:\nhparams, params = download_and_load_gpt2(model_size=""124M"", models_dir=""gpt2"")\nmodel_name = ""gpt2-small (124M)""\nThe updated code is as follows:\nhparams, params = download_and_load_gpt2(model_size=""1558M"", models_dir=""gpt2"")\nmodel_name = ""gpt2-xl (1558M)""",7732
10-Appendix_D._Adding_Bells_and_Whistles_to_the_Training_Loop.pdf,10-Appendix_D._Adding_Bells_and_Whistles_to_the_Training_Loop,"Appendix D. Adding Bells and\nWhistles to the Training Loop\nIn the appendix, we enhance the training function for the pretraining and\nfinetuning processes covered in chapters 5-7. This appendix, in particular,\ncovers \nlearning rate warmup\n, \ncosine decay\n, and \ngradient clipping\n in the first\nthree sections.\nThe final section then incorporates these techniques into the training function\ndeveloped in chapter 5 and pretrains an LLM.\nTo make the code in this appendix self-contained, we reinitialize the model\nwe trained in chapter 5.\nimport torch\nfrom previous_chapters import GPTModel\n \nGPT_CONFIG_124M = {\n    ""vocab_size"": 50257,  # Vocabulary size\n    ""ctx_len"": 256,       # Shortened context length (orig: 1024)\n    ""emb_dim"": 768,       # Embedding dimension\n    ""n_heads"": 12,        # Number of attention heads\n    ""n_layers"": 12,       # Number of layers\n    ""drop_rate"": 0.1,     # Dropout rate\n    ""qkv_bias"": False     # Query-key-value bias\n}\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.eval()\nAfter initializing the model, we also need to initialize the data loaders we\nused in chapter 5. First, we load the ""The Verdict"" short story:\nimport os\nimport urllib.request\n \nfile_path = ""the-verdict.txt""\nurl = ""https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt""\n \nif not os.path.exists(file_path):\n    with urllib.request.urlopen(url) as response:\n        text_data = response.read().decode('utf-8')\n    with open(file_path, ""w"", encoding=""utf-8"") as file:\n        file.write(text_data)\nelse:\n    with open(file_path, ""r"", encoding=""utf-8"") as file:\n        text_data = file.read()\nNext, we load the text_data into the data loaders:\nfrom previous_chapters import create_dataloader_v1\n \ntrain_ratio = 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntorch.manual_seed(123)\ntrain_loader = create_dataloader_v1(\n    text_data[:split_idx],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[""ctx_len""],\n    stride=GPT_CONFIG_124M[""ctx_len""],\n    drop_last=True,\n    shuffle=True\n)\nval_loader = create_dataloader_v1(\n    text_data[split_idx:],\n    batch_size=2,\n    max_length=GPT_CONFIG_124M[""ctx_len""],\n    stride=GPT_CONFIG_124M[""ctx_len""],\n    drop_last=False,\n    shuffle=False\n)\nNow that we have re-instantiated the model and data loaders we used in\nchapter 5, the next section will introduce the enhancements we make to the\ntraining function.\nD.1 Learning rate warmup\nThe first technique we introduce is \nlearning rate warmup\n. Implementing a\nlearning rate warmup can stabilize the training of complex models such as\nLLMs. This process involves gradually increasing the learning rate from a\nvery low initial value (\ninitial_lr\n) to a maximum value specified by the user\n(\npeak_lr\n). Starting the training with smaller weight updates decreases the\nrisk of the model encountering large, destabilizing updates during its training\nphase.\nSuppose we plan to train an LLM for 15 epochs, starting with an initial\nlearning rate of 0.0001 and increasing it to a maximum learning rate of 0.01.\nFurthermore, we define 20 warmup steps to increase the initial learning rate\nfrom 0.0001 to 0.01 in the first 20 training steps:\nn_epochs = 15\ninitial_lr = 0.0001\npeak_lr = 0.01\nwarmup_steps = 20\nNext, we implement a simple training loop template to illustrate this warmup\nprocess:\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\nlr_increment = (peak_lr - initial_lr) / warmup_steps #A\n \nglobal_step = -1\ntrack_lrs = []\n \nfor epoch in range(n_epochs):  #B\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n    \n        if global_step < warmup_steps: #C\n            lr = initial_lr + global_step * lr_increment\n        else:\n            lr = peak_lr\n        \n        for param_group in optimizer.param_groups: #D\n            param_group[""lr""] = lr\n        track_lrs.append(optimizer.param_groups[0][""lr""])\n        #E\nAfter running the preceding code, we visualize how the learning rate was\nchanged by the training loop above to verify that the learning rate warmup\nworks as intended:\nimport matplotlib.pyplot as plt\nplt.ylabel(""Learning rate"")\nplt.xlabel(""Step"")\ntotal_training_steps = len(train_loader) * n_epochs\nplt.plot(range(total_training_steps), track_lrs);\nplt.show()\nThe resulting plot is shown in Figure D.1.\nFigure D.1 The learning rate warmup increases the learning rate for the first 20 training steps.\nAfter 20 steps, the learning rate reaches the peak of 0.01 and remains constant for the rest of the\ntraining.\nAs shown in Figure D.1, the learning rate starts with a low value and\nincreases for 20 steps until it reaches the maximum value after 20 steps.\nIn the next section, we will modify the learning rate further so that it\ndecreases after reaching the maximum learning rate, which further helps\nimprove the model training.\nD.2 Cosine decay\nAnother widely adopted technique for training complex deep neural networks\nand LLMs is \ncosine decay\n. This method modulates the learning rate\nthroughout the training epochs, making it follow a cosine curve after the\nwarmup stage.\nIn its popular variant, cosine decay reduces (or decays) the learning rate to\nnearly zero, mimicking the trajectory of a half-cosine cycle. The gradual\nlearning decrease in cosine decay aims to decelerate the pace at which the\nmodel updates its weights. This is particularly important as it helps minimize\nthe risk of overshooting the loss minima during the training process, which is\nessential for ensuring the stability of the training during its later phases.\nWe can modify the training loop template from the previous section, adding\ncosine decay as follows:\nimport math\n \nmin_lr = 0.1 * initial_lr\ntrack_lrs = []\nlr_increment = (peak_lr - initial_lr) / warmup_steps\nglobal_step = -1\n \nfor epoch in range(n_epochs):\n    for input_batch, target_batch in train_loader:\n        optimizer.zero_grad()\n        global_step += 1\n \n        if global_step < warmup_steps:\n            lr = initial_lr + global_step * lr_increment  \n        else:# #B\n            progress = ((global_step - warmup_steps) / \n                        (total_training_steps - warmup_steps))\n            lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n        \n        for param_group in optimizer.param_groups:\n            param_group[""lr""] = lr\n        track_lrs.append(optimizer.param_groups[0][""lr""])\nAgain, to verify that the learning rate has changed as intended, we plot the\nlearning rate:\nplt.ylabel(""Learning rate"")\nplt.xlabel(""Step"")\nplt.plot(range(total_training_steps), track_lrs)\nplt.show()\nThe resulting learning rate plot is shown in Figure D.2.\nFigure D.2 The first 20 steps of linear learning rate warmup are followed by a cosine decay,\nwhich reduces the learning rate in a half-cosine cycle until it reaches its minimum point at the\nend of training.\nAs shown in Figure D.2, the learning rate starts with a linear warmup phase,\nwhich increases for 20 steps until it reaches the maximum value after 20\nsteps. After the 20 steps of linear warmup, cosine decay kicks in, reducing\nthe learning rate gradually until it reaches its minimum.\nD.3 Gradient clipping\nIn this section, we introduce \ngradient clipping\n, another important technique\nfor enhancing stability during LLM training. This method involves setting a\nthreshold above which gradients are downscaled to a predetermined\nmaximum magnitude. This process ensures that the updates to the model's\nparameters during backpropagation stay within a manageable range.\nFor example, applying the \nmax_norm=1.0\n setting within PyTorch's\nclip_grad_norm_\n function ensures that the norm of the gradients does not\nsurpass 1.0. Here, the term ""norm"" signifies the measure of the gradient\nvector's length, or magnitude, within the model's parameter space,\nspecifically referring to the L2 norm, also known as the Euclidean norm.\nIn mathematical terms, for a vector \nv\n composed of components \nv\n = [\nv\n1\n, \nv\n2\n, ...,\nv\nn\n], the L2 norm is described as:\nThis calculation method is also applied to matrices.\nFor instance, consider a gradient matrix given by:\nIf we aim to clip these gradients to a max_norm of 1, we first compute the L2\nnorm of these gradients, which is\nGiven that |\nG\n|\n2\n = 5 exceeds our \nmax_norm\n of 1, we scale down the gradients\nto ensure their norm equals exactly 1. This is achieved through a scaling\nfactor, calculated as \nmax_norm\n/|\nG\n|\n2\n = 1/5. Consequently, the adjusted gradient\nmatrix \nG'\n becomes\nTo illustrate this gradient clipping process, we would begin by initializing a\nnew model and calculating the loss for a training batch, similar to the\nprocedure in a standard training loop:\nfrom previous_chapters import calc_loss_batch\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nloss = calc_loss_batch(input_batch, target_batch, model, device)\nloss.backward()\nUpon calling the \n.backward()\n method in the preceding code snippet,\nPyTorch calculates the loss gradients and stores them in a \n.grad\n attribute for\neach model weight (parameter) tensor.\nFor illustration purposes, we can define the following\nfind_highest_gradient\n utility function to identify the highest gradient\nvalue by scanning all the \n.grad\n attributes of the model's weight tensors after\ncalling \n.backward()\n:\ndef find_highest_gradient(model):\n    max_grad = None\n    for param in model.parameters():\n        if param.grad is not None:\n            grad_values = param.grad.data.flatten()\n            max_grad_param = grad_values.max()\n            if max_grad is None or max_grad_param > max_grad:\n                max_grad = max_grad_param\n    return max_grad\nprint(find_highest_gradient(model))\nThe largest gradient value identified by the preceding code is as follows:\ntensor(0.0373)\nLet's now apply gradient clipping, which can be implemented with one line of\ncode, and see how this affects the largest gradient value:\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\nprint(find_highest_gradient(model))\nThe largest gradient value after applying the gradient clipping with the max\nnorm of 1 is substantially smaller than before:\ntensor(0.0166)\nIn the next section, we will put all the concepts covered in this appendix so\nfar into action and modify the LLM training function.\nD.4 The modified training function\nIn this final section of this appendix, we improve the \ntrain_model_simple\ntraining function we used in chapter 5 by adding the three concepts we\nintroduced: linear warmup, cosine decay, and gradient clipping. Together,\nthese methods help stabilize LLM training.\nThe code is as follows, with the changes compared to the\ntrain_model_simple\n annotated:\nfrom previous_chapters import evaluate_model, generate_and_print_sample\n \ndef train_model(model, train_loader, val_loader, optimizer, device, n_epochs,\n                eval_freq, eval_iter, start_context, warmup_steps=10,\n                initial_lr=3e-05, min_lr=1e-6):\n \n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n    tokens_seen, global_step = 0, -1\n \n    peak_lr = optimizer.param_groups[0][""lr""] #A\n    total_training_steps = len(train_loader) * n_epochs #B\n    lr_increment = (peak_lr - initial_lr) / warmup_steps #C\n \n    for epoch in range(n_epochs):\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()\n            global_step += 1\n \n            if global_step < warmup_steps: #D\n                lr = initial_lr + global_step * lr_increment  \n            else:\n                progress = ((global_step - warmup_steps) / \n                            (total_training_steps - warmup_steps))\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n                    1 + math.cos(math.pi * progress))\n \n            for param_group in optimizer.param_groups: #E\n                param_group[""lr""] = lr\n            track_lrs.append(lr)\n            loss = calc_loss_batch(input_batch, target_batch, model, device)\n            loss.backward()\n \n            if global_step > warmup_steps: #F\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            #G\n            optimizer.step() \n            tokens_seen += input_batch.numel()\n \n            if global_step % eval_freq == 0:\n                train_loss, val_loss = evaluate_model(\n                    model, train_loader, val_loader,\n                    device, eval_iter\n                )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                print(f""Ep {epoch+1} (Iter {global_step:06d}): ""\n                      f""Train loss {train_loss:.3f}, Val loss {val_loss:.3f}"")\n \n        generate_and_print_sample(\n            model, train_loader.dataset.tokenizer,\n            device, start_context\n        )\n \n    return train_losses, val_losses, track_tokens_seen, track_lrs\nAfter defining the \ntrain_model\n function, we can use it in a similar fashion to\ntrain the model compared to the \ntrain_model_simple\n method in chapter 5:\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M)\nmodel.to(device)\npeak_lr = 5e-4\noptimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n \nn_epochs = 15\ntrain_losses, val_losses, tokens_seen, lrs = train_model(\n    model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n    eval_freq=5, eval_iter=1, start_context=""Every effort moves you"",\n    warmup_steps=10, initial_lr=1e-5, min_lr=1e-5\n)\nThe training will take about 5 minutes to complete on a MacBook Air or\nsimilar laptop and print the following outputs:\nEp 1 (Iter 000000): Train loss 10.934, Val loss 10.939\nEp 1 (Iter 000005): Train loss 8.529, Val loss 8.843\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nEp 2 (Iter 000010): Train loss 6.400, Val loss 6.825\nEp 2 (Iter 000015): Train loss 6.116, Val loss 6.861\nEvery effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n... \nthe irony. She wanted him vindicated--and by me!""  He laughed again, and threw back his head to look up at the sketch of the donkey. ""There were days when I\nEp 15 (Iter 000130): Train loss 0.101, Val loss 6.707\nEvery effort moves you?""  ""Yes--quite insensible to the irony. She wanted him vindicated--and by me!""  He laughed again, and threw back his head to look up at the sketch of the donkey. ""There were days when I\nLike chapter 5, the model begins to overfit after a few epochs since it is a\nvery small dataset, and we iterate over it multiple times. However, we can see\nthat the function is working since it minimizes the training set loss.\nReaders are encouraged to train the model on a larger text dataset and\ncompare the results obtained with this more sophisticated training function to\nthe results that can be obtained with the \ntrain_model_simple\n function used\nin chapter 5.",15356
