filename,title,text,len
01-Copyright.pdf,01-Copyright,"978-1-098-15999-3\n[LSI]Financial Data Engineering\nby Tamer Khraisha\nCopyright © 2025 Tamer Khraisha. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( https://oreilly.com ). For more information, contact our corporate/institu‐\ntional sales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Michelle Smith\nDevelopment Editor:  Jill Leonard\nProduction Editor:  Gregory Hyman\nCopyeditor:  Liz Wheeler\nProofreader:  Sonia SarubaIndexer:  nSight, Inc.\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nOctober 2024:  First Edition\nRevision History for the First Edition\n2024-10-09: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098159993  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Financial Data Engineering , the cover\nimage, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the author and do not represent the publisher’s views. While\nthe publisher and the author have used good faith efforts to ensure that the information and instructions\ncontained in this work are accurate, the publisher and the author disclaim all responsibility for errors or\nomissions, including without limitation responsibility for damages resulting from the use of or reliance\non this work. Use of the information and instructions contained in this work is at your own risk. If any\ncode samples or other technology this work contains or describes is subject to open source licenses or the\nintellectual property rights of others, it is your responsibility to ensure that your use thereof complies\nwith such licenses and/or rights.\nTo my wife Marti, my constant cheerleader and greatest inspiration, and to our wonder‐\nful baby Mark, whose laughter is the melody that accompanies my writing. This book is\na small reflection  of the love and joy you bring into my world.",2206
02-Table of Contents.pdf,02-Table of Contents,"Table of Contents\nForeword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xiii\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xv\nPart I. Foundations of Financial Data Engineering\n1.Financial Data Engineering Clarified . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nDefining Financial Data Engineering                                                                           4\nFirst of All, What Is Finance?                                                                                     5\nDefining Data Engineering                                                                                       10\nDefining Financial Data Engineering                                                                      12\nWhy Financial Data Engineering?                                                                               13\nVolume, Variety, and Velocity of Financial Data                                                   14\nFinance-Specific Data Requirements and Problems                                             19\nFinancial Machine Learning                                                                                     21\nThe Disruptive FinTech Landscape                                                                         27\nRegulatory Requirements and Compliance                                                           31\nThe Financial Data Engineer Role                                                                               32\nDescription of the Role                                                                                              32\nWhere Do Financial Data Engineers Work?                                                          33\nResponsibilities and Activities of a Financial Data Engineer                              36\nSkills of a Financial Data Engineer                                                                          38\nSummary                                                                                                                         42\n2.Financial Data Ecosystem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  43\nSources of Financial Data                                                                                             44\nPublic Financial Data                                                                                                 44\nv\nSecurity Exchanges                                                                                                    47\nCommercial Data Vendors, Providers, and Distributors                                     47\nSurvey Data                                                                                                                 53\nAlternative Data                                                                                                         54\nConfidential and Proprietary Data                                                                          54\nStructures of Financial Data                                                                                         55\nTime Series Data                                                                                                         55\nCross-Sectional Data                                                                                                 56\nPanel Data                                                                                                                   57\nMatrix Data                                                                                                                 59\nGraph Data                                                                                                                  60\nText Data                                                                                                                      67\nTypes of Financial Data                                                                                                 68\nFundamental Data                                                                                                      68\nMarket Data                                                                                                                70\nTransaction Data                                                                                                        73\nAnalytics Data                                                                                                             76\nAlternative Data                                                                                                         76\nReference Data                                                                                                            77\nEntity Data                                                                                                                  80\nBenchmark Financial Datasets                                                                                    80\nCenter for Research in Security Prices                                                                    81\nCompustat Financials                                                                                                81\nTrade and Quote Database                                                                                        81\nInstitutional Brokers’ Estimate System                                                                   81\nIvyDB OptionMetrics                                                                                                82\nTrade Reporting and Compliance Engine                                                              82\nOrbis Global Database                                                                                               82\nSDC Platinum                                                                                                             83\nStandard & Poor’s Dow Jones Indices                                                                     83\nAlternative Datasets                                                                                                   83\nSummary                                                                                                                         85\n3.Financial Identification  Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\nFinancial Identifiers                                                                                                       87\nFinancial Identifier and Identification System Defined                                       88\nThe Need for Financial Identifiers                                                                           89\nWho Creates Financial Identification Systems?                                                     90\nDesired Properties of a Financial Identifier                                                               93\nUniqueness                                                                                                                  93\nGlobality                                                                                                                      94\nScalability                                                                                                                     96\nvi | Table of Contents\nCompleteness                                                                                                              97\nAccessibility                                                                                                                 98\nTimeliness                                                                                                                    98\nAuthenticity                                                                                                                99\nGranularity                                                                                                                  99\nPermanence                                                                                                               100\nImmutability                                                                                                             102\nSecurity                                                                                                                      102\nFinancial Identification Systems Landscape                                                            103\nInternational Securities Identification Number                                                   104\nClassification of Financial Instruments                                                                106\nFinancial Instrument Short Name                                                                         107\nCommittee on Uniform Security Identification Procedures                             108\nLegal Entity Identifier                                                                                              109\nTransaction Identifiers                                                                                            110\nStock Exchange Daily Official List                                                                        112\nTicker Symbols                                                                                                         113\nDerivative Identifiers                                                                                               114\nFinancial Instrument Global Identifier                                                                 116\nFactSet Permanent Identifier                                                                                  119\nLSEG Permanent Identifier                                                                                     119\nDigital Asset Identifiers                                                                                           120\nIndustry and Sector Identifiers                                                                              121\nBank Identifiers                                                                                                        122\nSummary                                                                                                                       125\n4.Financial Entity Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  127\nFinancial Entity Defined                                                                                             128\nFinancial Named Entity Recognition                                                                        129\nNamed Entity Recognition Described                                                                  129\nHow Does Named Entity Recognition Work?                                                     134\nApproaches to Named Entity Recognition                                                           141\nNamed Entity Recognition Software Libraries                                                    149\nFinancial Entity Resolution                                                                                        150\nEntity Resolution Described                                                                                   151\nThe Importance of Entity Resolution in Finance                                                151\nHow Does Entity Resolution Work?                                                                      156\nApproaches to Entity Resolution                                                                           164\nEntity Resolution Software Libraries                                                                    170\nSummary                                                                                                                       170\nTable of Contents | vii\n5.Financial Data Governance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171\nFinancial Data Governance                                                                                        171\nFinancial Data Governance Defined                                                                     171\nFinancial Data Governance Justified                                                                     172\nData Quality                                                                                                                 174\nDimension 1: Data Errors                                                                                       175\nDimension 2: Data Outliers                                                                                    177\nDimension 3: Data Biases                                                                                       179\nDimension 4: Data Granularity                                                                              180\nDimension 5: Data Duplicates                                                                               182\nDimension 6: Data Availability and Completeness                                             185\nDimension 7: Data Timeliness                                                                               187\nDimension 8: Data Constraints                                                                              188\nDimension 9: Data Relevance                                                                                189\nData Integrity                                                                                                               190\nPrinciple 1: Data Standards                                                                                    190\nPrinciple 2: Data Backups                                                                                       191\nPrinciple 3: Data Archiving                                                                                    191\nPrinciple 4: Data Aggregation                                                                                192\nPrinciple 5: Data Lineage                                                                                        193\nPrinciple 6: Data Catalogs                                                                                       194\nPrinciple 7: Data Ownership                                                                                  195\nPrinciple 8: Data Contracts                                                                                     195\nPrinciple 9: Data Reconciliation                                                                            197\nData Security and Privacy                                                                                           198\nData Privacy                                                                                                              201\nData Anonymization                                                                                               203\nData Encryption                                                                                                       209\nAccess Control                                                                                                          210\nSummary                                                                                                                       212\nPart II. The Financial Data Engineering Lifecycle\n6.Overview of the Financial Data Engineering Lifecycle. . . . . . . . . . . . . . . . . . . . . . . . . .  215\nFinancial Data Engineering Lifecycle Defined                                                        215\nCriteria for Building the Financial Data Engineering Stack                                  218\nCriterion 1: Open Source Versus Commercial Software                                    218\nCriterion 2: Ease of Use Versus Performance                                                      224\nCriterion 3: Cloud Versus On Premises                                                                227\nCriterion 4: Public Versus Private Versus Hybrid Cloud                                   235\nCriterion 5: Single Versus Multi-Cloud                                                                238\nviii | Table of Contents\nCriterion 6: Monolithic Versus Modular Codebase                                            240\nSummary                                                                                                                       242\n7.Data Ingestion Layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  243\nData Transmission and Arrival Processes                                                                243\nData Transmission Protocols                                                                                 244\nData Arrival Processes                                                                                             249\nData Ingestion Formats                                                                                              256\nGeneral-Purpose Formats                                                                                       256\nBig Data Formats                                                                                                      257\nIn-Memory Formats                                                                                                258\nStandardized Financial Formats                                                                            258\nData Ingestion Technologies                                                                                      269\nFinancial APIs                                                                                                           269\nFinancial Data Feeds                                                                                                274\nSecure File Transfer                                                                                                 275\nCloud Access                                                                                                             275\nWeb Access                                                                                                                277\nSpecialized Financial Software                                                                               277\nData Ingestion Best Practices                                                                                     277\nMeet Business Requirements                                                                                  277\nDesign for Change                                                                                                   278\nEnforce Data Governance                                                                                       278\nPerform Benchmarking and Stress Testing                                                          279\nSummary                                                                                                                       279\n8.Data Storage Layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  281\nPrinciples of Data Storage System Design                                                               281\nPrinciple 1: Business Requirements                                                                      282\nPrinciple 2: Data Modeling                                                                                     283\nPrinciple 3: Transactional Guarantee                                                                    284\nPrinciple 4: Consistency Tradeoffs                                                                        287\nPrinciple 4: Scalability                                                                                             288\nPrinciple 5: Security                                                                                                 290\nData Storage Modeling                                                                                               290\nSQL Versus NoSQL                                                                                                  291\nPrimary Versus Secondary                                                                                      292\nOperational Versus Analytical                                                                                292\nNative Versus Non-Native                                                                                      293\nMulti-Model Versus Polyglot Persistence                                                             293\nData Storage Models                                                                                                    294\nThe Data Lake Model                                                                                              294\nTable of Contents | ix\nThe Relational Model                                                                                              301\nThe Document Model                                                                                             314\nThe Time Series Model                                                                                            319\nThe Message Broker Model                                                                                    323\nThe Graph Model                                                                                                     329\nThe Warehouse Model                                                                                            335\nThe Blockchain Model                                                                                            343\nSummary                                                                                                                       346\n9.Data Transformation and Delivery Layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  347\nData Querying                                                                                                              347\nQuerying Patterns                                                                                                    347\nQuery Optimization                                                                                                351\nData Transformation                                                                                                   357\nTransformation Operations                                                                                    357\nTransformation Patterns                                                                                         368\nComputational Requirements                                                                                374\nData Delivery                                                                                                                382\nData Consumers                                                                                                       382\nDelivery Mechanisms                                                                                              383\nSummary                                                                                                                       384\n10. The Monitoring Layer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  385\nMetrics, Events, Logs, and Traces                                                                              386\nMetrics                                                                                                                       386\nEvents                                                                                                                         388\nLogs                                                                                                                            388\nTraces                                                                                                                         389\nData Quality Monitoring                                                                                            390\nPerformance Monitoring                                                                                            392\nCost Monitoring                                                                                                          397\nBusiness and Analytical Monitoring                                                                         400\nData Observability                                                                                                       404\nSummary                                                                                                                       406\n11. Financial Data Workflows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\nWorkflow-Oriented Software Architectures                                                            407\nWhat Is a Data Workflow?                                                                                          408\nWorkflow Management Systems                                                                               410\nFlexibility                                                                                                                   410\nConfigurability                                                                                                         410\nDependency Management                                                                                      411\nx | Table of Contents\nCoordination Patterns                                                                                             412\nScalability                                                                                                                  413\nIntegration                                                                                                                 413\nTypes of Financial Data Workflows                                                                          414\nExtract-Transform-Load Workflows                                                                     414\nStream Processing Workflows                                                                                417\nMicroservice Workflows                                                                                         420\nMachine Learning Workflows                                                                                424\nSummary                                                                                                                       429\n12. Hands-On Projects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  431\nPrerequisites                                                                                                                 431\nProject 1: Designing a Bank Account Management System Database with\nPostgreSQL                                                                                                                432\nConceptual Model: Business Requirements                                                         432\nLogical Model: Entity Relationship Diagram                                                       434\nPhysical Model: Data Definition and Manipulation Language                         436\nProject 1: Local Testing                                                                                           436\nProject 1: Clean Up                                                                                                  441\nProject 1: Summary                                                                                                 442\nProject 2: Designing a Financial Data ETL Workflow with Mage and Python  442\nProject 2: Workflow Definition                                                                              442\nProject 2: Database Design                                                                                     443\nProject 2: Local Testing                                                                                           444\nProject 2: Clean Up                                                                                                  448\nProject 2: Summary                                                                                                 448\nProject 3: Designing a Microservice Workflow with Netflix Conductor,\nPostgreSQL, and Python                                                                                         448\nProject 3: Workflow Definition                                                                              448\nProject 3: Database Design                                                                                     450\nProject 3: Local Testing                                                                                           452\nProject 3: Clean Up                                                                                                  456\nProject 3: Summary                                                                                                 457\nProject 4: Designing a Financial Reference Data Store with OpenFIGI,\nPermID, and GLEIF APIs                                                                                       457\nProject 4: Prerequisites                                                                                            458\nProject 4: Local Testing                                                                                           458\nProject 4: Clean Up                                                                                                  459\nProject 4: Summary                                                                                                 460\nConclusion                                                                                                                    460\nFollow Updates on These Projects                                                                            460\nReport Issues or Ask Questions                                                                                 460\nTable of Contents | xi\nThe Path Forward: Trends Shaping Financial Markets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  461\nAfterword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  465\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  467\nxii | Table of Contents",31215
03-Foreword.pdf,03-Foreword,"Foreword\nThe common metaphor for data in financial services is that of oil, lifeblood, or more\ngenerally life-giving fuel. However, equally important is how firms use this fuel to the\nbenefit of their business. Data is often raw material that needs to be processed,\nrefined and blended before it can be used. How this material is then used in decision-\nmaking workflows is a critical differentiator. From strategy formulation, product\ndevelopment down to process implementation, operations and reporting to investors,\ncustomers and regulators, how firms manage information can be the difference\nbetween thriving, surviving or indeed declining.\nThe rapid changes in how data is captured, aggregated, distilled, consumed and dis‐\ntributed have led to faster cycle times, new insights and a much more close-knit inte‐\ngration of computer science into business operations. The wide variety of traditional\nas well as new data sets that often don’t fit the mold of traditional computer science,\nbrings opportunities for business differentiation and provides the burgeoning field of\nfinancial data engineering with the raw materials it needs.\nThis variety has fostered rapid development of data governance and a better apprecia‐\ntion of the various aspects of data quality. Data pipelines do not exist in isolation but\nare shaped by the context of business goals, external reporting considerations as well\nas legal and commercial constraints. Cloud transition and the range of data sets, tools\nand data engineering techniques available can make us feel spoiled for choice. Invari‐\nably, trade-offs take place, quality can be in the eye of the beholder and the right com‐\nbination of data and engineering methods bridges business and technology. However,\ngetting this combination right is no mean feat and there are many challenges that can\neasily derail any financial data engineering project.\nPerhaps a more apt metaphor for data in financial services is that of food with data as\nthe ingredients and financial data engineering being similar to the process of cooking.\nThe number of both basic ingredients and spices has grown and so has the number of\nculinary techniques and recipes, leading to a proliferation of options on the menu.\nHow the kitchen is staffed is a key differentiator.\nxiii\nWhat Tamer Khraisha has done in his book, Financial Data Engineering , is to provide\nus with a comprehensive guide that helps to better understand the vast and varied\nlandscape of financial information and how best to apply financial data engineering\nconcepts to make the most of it. It provides ‘data chefs’ a structured overview of\nfinancial data engineering and the raw materials to work with: from the different\ntypes of data sets and their identification to a structured treatment of the different\naspects of data quality and data integrity. Subsequently, we get to work on these raw\nmaterials step by step, treating the entire financial data engineering lifecycle and tak‐\ning the business context and trade-offs into account. Whether cloud adoption, data‐\nbase and data warehouse developments, tokenization, machine learning or gen AI,\nTamer puts it into business context.\nI especially liked that Tamer interlaced his overview and guidelines with many real-\nworld case studies and ends the book with several in-depth data engineering projects.\nThis will help practitioners build and finesse their own judgement as to what tools\nand data sets are fit for purpose and what controls or guardrails make sense.\nWith his training in financial economics, background in network science and as a fin‐\ntech practitioner, Tamer bridges business and technology and brings a unique per‐\nspective to the field of financial data engineering. This book connects different topics\nthat are often treated in a dispersed way, not least in financial services organizations\nthemselves.\nThis book will be a source to technologists looking to get a better appreciation of the\nbusiness context of financial data engineering, as well as to those in financial services\ntrying to get a better sense of the art of the possible in financial data engineering. It\nwill be helpful to people fresh in the job market but will also serve as a reference to\nexperienced practitioners.\nFinancial data engineering bridges the old divide between computer science and busi‐\nness and this book will be a great help to successfully navigate that road.\n— Martijn Groot\nFinancial Data Management expert\nAuthor of Managing Financial Information in the\nTrade Lifecycle (Elsevier, 2008) and A Primer in\nFinancial Data Management (Elsevier, 2017)\nSt. Paul’s Bay, Malta, September 2024\nxiv | Foreword",4706
04-Preface.pdf,04-Preface,,0
05-Prerequisites.pdf,05-Prerequisites,"Preface\nWith this book, you will learn fundamental concepts, common challenges, best prac‐\ntices, innovative frameworks, and cutting-edge technologies essential for successfully\ndesigning and building data-driven financial products and services. This book is\nintended to establish foundational knowledge that is accessible to individuals from\ndiverse backgrounds, be they finance, computer science, software engineering, or aca‐\ndemic research. It covers a wide range of carefully selected topics chosen for their\nmarket, technological, and scientific relevance. Each concept in the book is presented\nin straightforward language, accompanied by case studies and finance-specific exam‐\nples for deeper insights. Moreover, to facilitate practical application, the final chapter\npresents four hands-on projects addressing various data-driven challenges related to\nfinancial markets.\nTo fully appreciate the story, read the chapters in order, though each chapter can also\nbe read on its own if you prefer.\nWho Should Read This Book?\nThis book serves a wide audience. This includes individuals working at institutions\nsuch as banks, investment firms, financial data providers, asset management compa‐\nnies, security exchanges, regulatory bodies, financial software vendors, and many\nmore. It is designed for data engineers, software developers, quantitative developers,\nfinancial analysts, and machine learning practitioners who are managing and/or\nworking with financial data and financial data-driven products. Furthermore, the\nbook appeals to scholars and researchers working on data-driven financial analysis,\nreflecting the growing interest in big data research in the financial sector. Whether\nyou’re a practitioner seeking insights into data-driven financial services, a scholar\ninvestigating finance-related problems, or a newcomer eager to venture into the\nfinancial field with a technology-oriented role, this book is designed to meet your\nneeds.\nxv",1981
06-Conventions Used in This Book.pdf,06-Conventions Used in This Book,"Prerequisites\nTo get the most out of the hands-on exercises in Chapter 12 , I recommend having\nsome basic knowledge of the following:\n•Python programming\n•SQL and PostgreSQL\n•Using tools like Python JupyterLab, Python Notebooks, and Pandas\n•Running Docker containers locally\n•Basic Git commands\nHowever, if you’re unfamiliar with all of these, don’t worry! Y ou can still dive into the\nprojects and learn as you go along.\nWhat to Expect from This Book\nThis book aims to combine two domains—data engineering and finance—each\nencompassing numerous concepts, practices, theories, problems, and applications.\nThe sheer number of topics within each of these two domains exceeds the scope of a\nsingle book. Consequently, I’ve aimed to achieve a balance among various considera‐\ntions to provide a comprehensive yet scoped exploration.\nThe key consideration is determining how to allocate emphasis between finance and\ndata engineering throughout the discussion. The initial five chapters ( Part I ) predom‐\ninantly lean toward finance, which may cover familiar ground for people with experi‐\nence in the field. Reading these initial chapters is highly recommended for those\nwithout a finance background and those who are seeking to refresh their knowledge\nof financial data and its associated challenges. Similarly, the last seven chapters\n(Part II ) focus primarily on data engineering, offering a comprehensive treatment of\ndata storage concepts, data modeling, databases, workflows, data ingestion mecha‐\nnisms, data transformations, and more. Even if you’re well-versed in data engineer‐\ning, these chapters will prove invaluable for understanding their specific applications\nand challenges within the financial domain.\nxvi | Preface\n1Gil Press, “Cleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says” ,\nForbes , March 23, 2016.Another consideration is finding the right balance between covering a wide range of\ntopics and providing in-depth explanations. The choice of topics in this book was\ndriven by the author’s experience, a literature review, market analysis, expert insights,\nregulatory requirements, and industry standards. At its core, this book is about finan‐\ncial data and the associated methodological and technological challenges. However,\nit’s important to note this is not meant to be a book on financial analysis, financial\nmachine learning, or statistics. Nonetheless, given that a substantial portion of AI and\nmachine learning projects revolve around data preparation and preprocessing—\napproximately 50–80% of the time1—this book can significantly aid in streamlining\nthese crucial tasks. In addition, some topics receive more extensive coverage due to\ntheir significance, while others are briefly addressed as their detailed explanation\nexceeds the book’s scope and length.\nLastly, a major and intricate consideration is finding the right balance between\nemphasizing timeless (immutable) principles and illustrating contemporary issues.\nWhile a book emphasizing immutable concepts remains valuable over time, it’s\nequally beneficial for readers to gain insight into current challenges and receive prac‐\ntical guidance through case studies and technologies. This book aims to strike a deli‐\ncate balance between foundational concepts and practical applications, offering\nreaders a comprehensive understanding of both immutable principles and current\ntrends in the field. For example, Chapter 8  covers the most popular topic in data engi‐\nneering: databases. Due to the rapid evolution of database technologies, this chapter\ntakes an abstract approach, focusing on data storage models rather than specific tech‐\nnologies. Each model is examined in terms of its primary use cases, data modeling\nprinciples, technological implementations, and applications within the financial\ndomain. This approach ensures relevance and longevity in light of emerging database\ninnovations.\nBook Resources and References\nIn this book, I use more than one thousand references spanning various sources: sci‐\nentific journals, books, blog posts, online articles, opinion pieces, and white papers.\nMany of these references are cited throughout the chapters to support the content\nand offer a solid foundation for the information delivered. For those who want to dig\ndeeper, an extensive list of additional references is available on the GitHub page  for\nthis book. I will periodically update this list to include new and relevant references.\nPreface | xvii",4547
07-OReilly Online Learning.pdf,07-OReilly Online Learning,"Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nThis element indicates a warning or caution.\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://oreil.ly/FinDataEngCode .\nShould you encounter any challenges while setting up or executing any step in the\nprojects outlined in Chapter 12 , please don’t hesitate to create an issue on the project’s\nGitHub repository . I will make sure to reply to you in a very short time.\nIf you have a technical question or a problem using the code examples, you may also\nsend an email to support@oreilly.com .\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. Y ou do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing examples from O’Reilly\nxviii | Preface",1480
08-Part I. Foundations of Financial Data Engineering.pdf,08-Part I. Foundations of Financial Data Engineering,"books does require permission. Answering a question by citing this book and quoting\nexample code does not require permission. Incorporating a significant amount of\nexample code from this book into your product’s documentation does require per‐\nmission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “ Financial Data Engi‐\nneering  by Tamer Khraisha (O’Reilly). Copyright 2025 Tamer Khraisha,\n978-1-098-15999-3. ”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com .\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nOur unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com .\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-889-8969 (in the United States or Canada)\n707-827-7019 (international or local)\n707-829-0104 (fax)\nsupport@oreilly.com\nhttps://oreilly.com/about/contact.html\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/FinancialDataEngineering .\nFor news and information about our books and courses, visit https://oreilly.com .\nPreface | xix\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media .\nWatch us on Y ouTube: https://youtube.com/oreillymedia .\nAcknowledgments\nWriting this book has been an incredible journey, and it would not have been possible\nwithout the support, guidance, and encouragement of many individuals.\nFirst and foremost, special thanks to Michelle Smith, the content acquisition editor,\nfor giving me the opportunity to write this book and believing in my vision from the\nbeginning.\nI am very thankful to Jill Leonard, the book editor, for her careful attention to detail\nand for guiding me through the editing process with patience and expertise. Her hard\nwork and dedication ensured that every aspect of this book was thoroughly checked\nand refined.\nI want to express my thanks to Greg Hyman, the production editor, for his essential\nrole in overseeing the production process and ensuring that the final version of the\nbook met the highest standards.\nI would like to express my deepest gratitude to the technical reviewers who provided\ninvaluable input and keen insights that greatly enhanced the quality of this work. My\nsincere thanks go to Aakash Atul Alurkar, Rahul Arulkumaran, Brian Buzzelli, Shi‐\nvani Gole, Martijn Groot, Pankaj Gupta, Ganesh Harke, Abdullah Karasan, Vipul\nBharat Marlecha, Mukund Sarma, Chandra Shukla, William Jamir Silva, and Kushan\nVora.\nSpecial thanks to Prof. Rosario Nunzio Mantegna, Y ousef Ibrahim, and Máté Sándor\nfor their valuable comments, input, and support.\nLastly, I would like to extend my sincere gratitude to the authors of the numerous ref‐\nerences, books, articles, and blog posts that have greatly contributed to this book.\nWith around one thousand sources consulted, your work has been invaluable in\nshaping the content and providing the insights needed for this project.\nxx | Preface\nPART I\nFoundations of Financial\nData Engineering\nThe first part of this book consists of five chapters, focusing on the core concepts and\nfundamental elements of financial data, along with the challenges associated with its\nmanagement. These chapters discuss the various systems and practices employed\nacross financial markets to manage financial data effectively.\nChapter 1  provides an introduction to the fundamentals of finance, highlighting its\nunique data challenges and the foundational ideas of financial data engineering.\nChapter 2  dives into the complexities of the financial data ecosystem, examining its\nstructure and key characteristics. Chapters 3 and 4 discuss financial identification and\nentity systems, emphasizing their significance as critical data engineering challenges\nwithin financial markets. Lastly, Chapter 5  presents a detailed framework for estab‐\nlishing robust data governance practices within financial institutions.\nTogether, these chapters establish the basic principles and concepts required to\nunderstand and practice financial data engineering.",4822
09-First of All What Is Finance.pdf,09-First of All What Is Finance,"CHAPTER 1\nFinancial Data Engineering Clarified\nGiven all the payments, transfers, trades, and numerous financial activities that take\nplace on a daily basis, can you imagine how much data the global financial sector\ngenerates? According to a 2011 report by McKinsey Global Institute , the banking and\ninvestment sector in the US alone stores and manages more than one exabyte of data.\nTo put that in perspective, an exabyte is the equivalent of one billion gigabytes, and it\ntranslates into trillions of digital records. The same report shows that on average,\nfinancial services firms generate and store more data than firms in other sectors.\nSome statistics are even more astonishing; for instance, JPMorgan Chase, the largest\nbank in the United States by market capitalization, manages more than 450 petabytes\nof data . Bank of New Y ork Mellon, a global financial services company specializing in\ninvestment management and investment services, manages over 110 million giga‐\nbytes of global financial data .\nNaturally, we might extrapolate these estimates and figures to tens or even hundreds\nof exabytes if we take into account the global context and the constantly expanding\nfinancial landscape. As a result, data sits at the heart of the financial system, serving\nas both the input for different financial operations and the output generated from\nthem. Importantly, to guarantee a healthy and well-functioning system, a reliable and\nsecure data infrastructure is needed for generating, exchanging, storing, and consum‐\ning all kinds of financial data. In addition, this infrastructure must adhere to the\nfinancial sector’s specific requirements, constraints, practices, and regulations. This is\nwhere financial data engineering comes into the scene. To get started, this chapter\nwill introduce you to finance, financial data engineering, and the role and skills of the\nfinancial data engineer.\n3\nDefining  Financial Data Engineering\nData engineering has always been a vibrant and innovative field from both industry\nand research standpoints. If you are a data engineer, you are likely aware of how\nmany data-related technologies are released and popularized every year. Several fac‐\ntors drive these developments:\n•The growing importance of data as a key input in the creation of digital products\nand services\n•Large digital companies, such as LinkedIn, Netflix, Google, Meta, and Airbnb,\ntransitioning the data frameworks they developed internally to handle massive\nvolumes of data and traffic to open source projects\n•The impressive success of open source alternatives, which has fueled interest\nfrom individuals and businesses in developing and evaluating new tools and\nideas\nAs an industry practice, data engineering  has undergone several conceptual and tech‐\nnological evolution episodes. Without offering a detailed historical account, I would\nsimply say that the birth of data engineering started with the introduction of Struc‐\ntured Query Language (SQL) and data warehousing in the 1970s/1980s. Companies\nlike IBM and Oracle were early pioneers in the field, playing a key role in developing\nand popularizing many of the fundamental principles of data engineering.\nUntil the early 2000s, data engineering responsibilities were primarily handled by\ninformation technology (IT) teams. Roles such as database administrator, database\ndeveloper, and system administrator were prevalent in the data job market.\nWith the global rise and adoption of the internet and social media, the so-called big\ndata  revolution marked a major step toward contemporary data engineering. Using\nthe release date of Apache Hadoop as a reference, I would say that the big data era\nstarted around 2005. Pioneers like Google, Airbnb, Meta, Microsoft, Amazon, and\nNetflix have popularized a more specialized and advanced version of data engineer‐\ning. This includes big data frameworks, open source tools, cloud computing, alterna‐\ntive data, and streaming technologies.\nThe financial sector has actively participated in this dynamic environment as both an\nobserver and an adopter of data technologies. This active involvement stems from the\nfinancial industry’s continuous evolution in response to market demands and regula‐\ntory changes, which often necessitates the adoption of new technologies. Importantly,\ndata engineering practices in finance are heavily domain driven, given the distinct\nrequirements of the financial sector in terms of security, governance, and regulation,\nas well as the complex nature of the financial data landscape and financial data man‐\nagement challenges.\n4 | Chapter 1: Financial Data Engineering Clarified\nConsidering these factors, this book will present financial data engineering as a\ndomain-driven field within data engineering, specifically tailored to the financial sec‐\ntor, thereby setting it apart from traditional data engineering. To further justify the\nneed for financial data engineering, the upcoming sections will provide a brief intro‐\nduction to the finance domain, outline the data-related challenges encountered in\nfinancial markets, offer definitions of data engineering and financial data engineer‐\ning, and provide an overview of the role and responsibilities of a financial data\nengineer.\nFirst of All, What Is Finance?\nDespite the extensive use of the term finance , there could be a lot of confusion about\nwhat it really means. This is because finance is a multifaceted concept that can be\napproached from different angles (see Figure 1-1 ). To prepare you with a basic\ndomain knowledge, the next sections present a short conceptual illustration of\nfinance from four main perspectives: economics, market, science, and technology.\nFigure 1-1. Main areas of finance\nDefining  Financial Data Engineering | 5\nFinance as an economic function\nIn economic theory, finance is an  institution that mediates between agents who are in\ndeficit  (who need more money than they have) and those in  surplus  (who have more\nmoney than they spend). To secure funds, agents in deficit offer to borrow money\nfrom agents with a surplus in exchange for an interest payment.\nThis perspective highlights the vital role of finance in the economy: it offers individu‐\nals a means to invest their savings, allows families to purchase a house through a\nmortgage, provides businesses with capital to get started, empowers universities to\ninvest their assets and expand their campus, and enables governments to finance\npublic projects to fulfill societal needs.\nFor economists, finance is one of the primary drivers of economic growth . This is why\ngood economies tend to have large, efficient, and inclusive financial markets. To\nensure financial markets’ stability and fairness, several regulatory agencies and regu‐\nlations were established.\nA major subject that financial economists often investigate is market equilibrium ,\nwhich describes a state where demand and supply intersect, resulting in a stable mar‐\nket price. In financial markets, this price is commonly represented by the interest\nrate, with supply and demand reflecting the quantity of money in circulation. When\ndemand exceeds supply, interest rates typically rise, whereas if supply surpasses\ndemand, interest rates tend to decrease. Entities such as central banks were estab‐\nlished to implement monetary policies aimed at maintaining market interest rates as\nclosely aligned with equilibrium as possible.\nFinance as a market\nTo enable individuals and companies to engage efficiently in financial activities,\nfinancial  markets  have emerged, hosting a vast array of financial institutions, prod‐\nucts, and services. Nowadays, if we take a well-developed financial sector, we can find\na large variety of market players. These may include the following:\n•Commercial banks (e.g., HSBC, Bank of America)\n•Investment banks (e.g., Morgan Stanley, Goldman Sachs)\n•Asset managers (e.g., BlackRock, The Vanguard Group)\n•Security exchanges (e.g., New Y ork Stock Exchange [NYSE], London Stock\nExchange, Chicago Mercantile Exchange)\n•Hedge funds (e.g., Citadel, Renaissance Technologies)\n•Mutual funds (e.g., Vanguard Mid-Cap Value Index Fund)\n•Insurance companies (e.g., Allianz, AIG)\n•Central banks (e.g., Federal Reserve, European Central Bank)\n6 | Chapter 1: Financial Data Engineering Clarified\n1If you want to learn more in depth about financial instruments, I encourage you to read Investments  by Zvi\nBodie, Alex Kane, and Alan Marcus (McGraw Hill, 2023).\n•Government-sponsored enterprises (e.g., Fannie Mae, Freddie Mac)\n•Regulators (e.g., Securities and Exchange Commission)\n•Industry trade groups (e.g., Securities Industry and Financial Markets\nAssociation)\n•Credit rating agencies (e.g., S&P Global Ratings, Moody’s)\n•Data vendors (e.g., Bloomberg, London Stock Exchange Group [LSEG])\n•FinTech companies (e.g., Revolut, Wise, Betterment)\n•Big tech companies (e.g., Amazon Cash, Amazon Pay, Apple Pay, Google Pay)\nThe terms “financial institution, ” “financial firm, ” “financial com‐\npany, ” and “financial organization” might often be used inter‐\nchangeably. However, from an economic theory standpoint,\n“financial institution” may be the most appropriate term to use, as\nit represents an abstract concept encompassing any company,\nagency, firm, or organization that serves a specific purpose or func‐\ntion within financial markets. For this reason, I will be mostly\nusing the term “financial institution” throughout this book.\nThe primary unit of exchange in financial markets is commonly referred to as a\nfinancial asset , instrument, or security . There is a large number of financial assets that\ncan be bought and sold in financial markets. Here are a few:1\n•Shares of companies (e.g., common stocks)\n•Fixed income instruments (e.g., corporate bonds, treasury bills)\n•Derivatives (e.g., options, futures, swaps, forwards)\n•Fund shares (e.g., mutual funds, exchange-traded funds)\nGiven the large and diverse number of financial instruments and transactions, finan‐\ncial markets are further classified into categories, such as the following:\n•Money markets (for liquid short-term exchanges)\n•Capital markets (long-term exchanges)\n•Primary markets (for new issues of instruments)\n•Secondary markets (for already issued instruments)\nDefining  Financial Data Engineering | 7\n•Foreign exchange markets (for trading currencies)\n•Commodity markets (for trading raw materials such as gold and oil)\n•Equity markets (for trading stocks)\n•Fixed-income markets (for trading bonds)\n•Derivatives markets (for trading derivatives)\nInvestopedia: The Online Resource for Financial Education\nIf you want a quick introduction to a specific financial term, Investopedia  is the place\nto go. Investopedia is the world’s leading source of online financial and investment\ncontent. This includes information on financial terminology, definitions, news,\ninvestments, and financial education. Investopedia is a valuable resource for anyone\ninterested in learning more about finance, whether they are a novice learner or an\ninvestor looking to gain more in-depth financial knowledge. Investopedia’s articles\nare written, reviewed, and fact-checked by financial experts, which adds to the credi‐\nbility and quality of the published content.\nInterestingly, financial markets are highly reliant on and driven by research and\nmethodologies developed by finance departments at prominent universities and spe‐\ncialized finance institutes. The next section will briefly explore the nature and key\nareas of financial research.\nFinance as a research field\nFinance is a well-known and extensive field of academic and empirical research. One\nmajor area of investigation is asset pricing theory , which aims to understand and cal‐\nculate the price of claims to risky (uncertain) assets (e.g., stocks, bonds, derivatives,\netc.). Within this theory, low prices often translate into a high rate of return, so we\ncan think of financial asset pricing theory as a way to explain why certain financial\nassets pay (or should pay) higher average returns than others.\nAnother major field of financial  research is risk management , which focuses on meas‐\nuring and managing the uncertainty around the future value of a financial asset or a\nportfolio of assets. Other areas of investigation include portfolio management, corpo‐\nrate finance, financial accounting, credit scoring, financial engineering, stock predic‐\ntion, and performance evaluation.\nTo publish financial research findings, a variety of peer-reviewed journals have been\nestablished. Some of these journals offer broad coverage, while others are more speci‐\nalized. Here are some examples:\n8 | Chapter 1: Financial Data Engineering Clarified\nThe Journal of Finance\nCovers theoretical and empirical research on all major areas of finance\nThe Review of Financial Studies\nCovers theoretical and empirical topics in financial economics\nThe Journal of Banking and Finance\nCovers theoretical and empirical topics in finance and banking, with a focus on\nfinancial institutions and money and capital markets\nQuantitative Finance\nCovers theoretical and empirical interdisciplinary research on quantitative meth‐\nods of finance\nThe Journal of Portfolio Management\nCovers topics related to finance and investing, such as risk management, portfo‐\nlio optimization, and performance measurement.\nThe Journal of Financial Data Science\nCovers data-driven research in finance using machine learning, artificial intelli‐\ngence, and big data analytics\nThe Journal of Securities Operations & Custody\nCovers  topics and issues related to securities trading, clearing, settlement, finan‐\ncial standards, and more\nIn addition to academic journals, a large number of conferences, events, and summits\nare regularly held to share and discuss the latest developments in financial research.\nExamples include the Western Finance Association meetings, the American Finance\nAssociation meetings, and the Society for Financial Studies Cavalcades. Furthermore,\nglobally renowned certifications like the Chartered Financial Analyst (CFA) are avail‐\nable to aspirant financial specialists who wish to acquire strong ethical and technical\nfoundations in investment research and portfolio management.\nFinance as a technology\nFinally, finance can refer to the set of technologies and tools enabling all kinds of\nfinancial transactions and activities. Examples include the following:\n•Payment systems (mobile, contactless, real-time, digital wallets, gateways, etc.)\n•Blockchain and distributed ledger technology (DLT)\n•Financial market infrastructures (e.g., Euroclear, Clearstream, Fedwire, T2,\nCHAPS)\n•Trading platforms\n•Stock exchanges (e.g., NYSE, NASDAQ, Tokyo Stock Exchange)\nDefining  Financial Data Engineering | 9",14916
10-Defining Data Engineering.pdf,10-Defining Data Engineering,"2Data management is a broader term than data engineering. It refers to all plans and policies put in place to\nmake sure that data is strategically managed and optimized for business value creation. To read about data\nmanagement, I highly recommend Data Management at Scale  by Piethein Strengholt (O’Reilly, 2023).•Stock market data systems\n•Automated teller machine (ATM)\n•Order management systems (OMSs)\n•Risk management systems\n•Algorithmic trading and high-frequency trading (HFT) systems\n•Smart order routing (SOR) systems\nThis diverse array of technologies in the financial sector is crucial for maintaining the\nefficiency and reliability of global financial markets.\nDefining  Data Engineering\nNow that we have a foundational understanding of finance, let’s explore what finan‐\ncial data engineering is. To do this, I’ll first explain traditional data engineering, as it\nis a widely recognized term in the industry.\nIf we Google the words “what is data engineering, ” we get more than two billion\nsearch results. That’s quite a lot, but to be more pragmatic, we can do a more\nadvanced inquiry by searching Google Scholar for all papers and books where the\nterm “data engineering” occurs in the title. Such a query returns a relatively large\nnumber of results (around 2,290 scientific publications), as shown in Figure 1-2 .\nFigure 1-2. Google Scholar search for publications with data engineering in the title\nI highly recommend you read some of the publications that Google Scholar returns\nfor data engineering. Interestingly, you will quickly notice that there is quite a high\nvariety of definitions for data engineering. This is expected, as the field of data engi‐\nneering sits at the intersection between multiple fields, including software engineer‐\ning, infrastructure engineering, data analysis, networking, software and data\narchitecture, data governance, and other data management-related areas.2\n10 | Chapter 1: Financial Data Engineering Clarified\nFor illustrative purposes, let’s consider the following selected definitions:\nData engineering is the development, implementation, and maintenance of systems\nand processes that take in raw data and produce high-quality, consistent information\nthat supports downstream use cases, such as analysis and machine learning. Data engi‐\nneering is the intersection of security, data management, DataOps, data architecture,\norchestration, and software engineering.\n—Joe Reis and Matt Housley, Fundamentals of Data Engineering  (O’Reilly, 2022)\nData engineering is all about the movement, manipulation, and management of data.\n—Lewis Gavin, What Is Data Engineering?  (O’Reilly 2019)\nData engineering is the process of designing and building systems that let people col‐\nlect and analyze raw data from multiple sources and formats. These systems empower\npeople to find practical applications of the data, which businesses can use to thrive.\n—Dremio\nAs you can see, all three definitions are quite different, but if we make an effort to\nextract the main defining elements, we can infer that data engineering revolves\naround the design  and implementation  of an infrastructure  that enables an organiza‐\ntion to retrieve  data from one or more sources, transform  it, store  it in a target destina‐\ntion, and make it consumable  by end users. Naturally, in practice, the complexity of\nsuch a process would depend on the technical and business requirements and con‐\nstraints, which vary on a case-by-case basis. Given this context, I will use the follow‐\ning definition of data engineering throughout this book:\nData engineering is a field of practice and research that focuses on designing and\nimplementing data infrastructure intended to reliably and securely perform tasks such\nas data ingestion, transformation, storage, and delivery. This infrastructure is tailored\nto meet varying business requirements, industry practices, and external factors such as\nregulatory compliance and privacy considerations.\nThroughout this book, we’ll focus on the concept of financial  data infrastructure  as\nthe cornerstone of financial data engineering. Along the way, we will examine the\ncomponents  of a financial data infrastructure, which include physical (hardware) and\nvirtual (software) resources and systems for storing, processing, managing, and trans‐\nmitting financial data. Furthermore, we will discuss the essential capabilities  and fea‐\ntures  of a financial data infrastructure, such as security, traceability, scalability,\nobservability, and reliability.\nWith this definition in mind, let’s now proceed to clarify the meaning of financial\ndata engineering.\nDefining  Financial Data Engineering | 11",4721
11-Volume Variety and Velocity of Financial Data.pdf,11-Volume Variety and Velocity of Financial Data,"3For a good reference on these challenges, see Antoni Munar, Esteban Chiner, and Ignacio Sales, “ A Big Data\nFinancial Information Management Architecture for Global Banking” , presented at the 2014 International\nConference on Future Internet of Things and Cloud (IEEE, August 2014): 385–388.Defining  Financial Data Engineering\nFinancial data engineering shares most of the traditional data engineering tools, pat‐\nterns, practices, and technologies. However, when designing and building a financial\ndata infrastructure, relying only on traditional data engineering is not sufficient. Y ou\nare very likely going to deal with domain-specific issues such as the complex financial\ndata landscape (e.g., a large number of data sources, types, vendors, structures, etc.),\nthe regulatory requirements for reporting and governance, the challenges related to\nentity and identification systems, the special requirements in terms of speed and vol‐\nume, and a variety of constraints on delivery, ingestion, storage, and processing.3\nGiven such domain-driven particularities, financial data engineering deserves to be\ntreated as a specialized field that sits at the intersection between traditional data engi‐\nneering, financial domain knowledge, and financial data (as illustrated in Figure 1-3 ).\nMore formally, this book defines financial data engineering as follows:\nFinancial data engineering is the domain-driven practice of designing, implementing,\nand maintaining data infrastructure to enable the collection, transformation, storage,\nconsumption, monitoring, and management of financial data coming from mixed\nsources, with different frequencies, structures, delivery mechanisms, formats, identifi‐\ners, and entities, while following secure, compliant, and reliable standards.\nFigure 1-3. Financial data engineering and related fields\n12 | Chapter 1: Financial Data Engineering Clarified\n4To know more about this topic, check this excellent reference: Tanya S. Beder and Cara M. Marshall’s Finan‐\ncial Engineering: The Evolution of a Profession  (Wiley, 2011).\nDon’t confuse financial data engineering with financial engineer‐\ning. Financial engineering is an interdisciplinary applied field that\nuses mathematics, statistics, econometrics, financial theory, and\ncomputer science to develop financial investment strategies, finan‐\ncial products, and financial processes.4\nDomain-Driven Design\nDesigning software systems following domain-specific knowledge and requirements\nis a common practice. The most prominent approach in this context is Domain-\nDriven Design  (DDD). DDD emphasizes modeling and designing the business\ndomain to ensure that the software aligns with business requirements in terms of\nquality and features. This approach necessitates close collaboration between engineers\nand domain experts to establish a common understanding and a unified language,\nknown as the “ubiquitous language, ” which is consistently used throughout the\nproject.\nDDD divides a given business problem into domains. A domain is the problem space\nthat the software application is being developed to solve. For example, in a banking\napplication, domains could be accounts, payments, transactions, customers, cash\nmanagement, and liquidity management. Domains can further be decomposed into\nsubdomains; for example, cash management may have subdomains such as collec‐\ntions management and cash flow forecasting, which are bounded by a given context.\nNow that you know what financial data engineering is, you may be wondering why it\nmatters to financial institutions and markets and why we should write a book about\nit. The next section addresses these questions in detail.\nWhy Financial Data Engineering?\nOne of the main goals of this book is to illustrate how financial data engineering is\nunique in terms of the domain-driven elements that characterize it. To understand\nwhy the market demands financial data engineering, it is crucial to examine the main\nfactors shaping and driving data-driven needs and trends in the financial sector. The\nnext few sections will provide a detailed account of these factors.\nWhy Financial Data Engineering? | 13\nVolume, Variety, and Velocity of Financial Data\nOne of the primary factors that have been transforming the financial sector is big\ndata . In this book, big data is simply defined as a combination of three attributes:\nlarge size ( volume ), high dimensionality and complexity ( variety ), and speed of gener‐\nation ( velocity ). Let’s explore each of these Vs in detail.\nVolume\nWhen referencing big data, it is hard to deny that it is primarily about size. Data can\nbe large, either in absolute  or relative  terms. Data is said to be large in absolute terms\nif it gets generated in a remarkably enormous and nonlinear quantity. An absolute\nincrease in data size is often the result of socio-technological changes that induce a\nstructural alteration to the data generation process. For example, in the past, card\npayments were primarily reserved for major purchases and were relatively limited,\nwhereas today, the widespread adoption of card and mobile payment methods has\ntransformed everyday transactions, with people now using cards and phones to pay\nfor almost everything, from groceries to electronics. This, in turn, has led to a\n(remarkable) absolute increase in the amount of payment data being generated and\ncollected.\nIn addition, the rapid development and adoption of digital automated technologies,\nin particular electronic exchange mechanisms, have resulted in an absolute increase\nin the sheer volume of financial data generated. The emergence of high-frequency\ntrading is a good example. For instance, a single day’s worth of data from the New\nY ork Stock Exchange’s high-frequency dataset , Trade and Quotes (TAQ), comprises\napproximately 2.3 billion records. With the implementation of high-frequency trad‐\ning technologies, financial data began to be recorded at incredibly fine intervals,\nincluding the millisecond (one-thousandth of a second), microsecond (one-millionth\nof a second), and even nanosecond (one-billionth of a second) levels.\nOn the other hand, data is considered relatively large if its size is big compared to\nother existing datasets. Improved data collection is perhaps the main driver behind\nthe relative increase in financial data volumes. This has been facilitated by technolog‐\nical advancements enabling more efficient data collection, regulatory requirements\nimposing stricter data collection and reporting requirements, the increasing complex‐\nity of financial instruments necessitating the collection of data for risk management,\nand the growing demand for data-driven insights within the financial sector. As an\nexample, the Options Price Reporting Authority (OPRA), which collects and consoli‐\ndates all the trades and quotes from member option exchanges in the United States,\n14 | Chapter 1: Financial Data Engineering Clarified\n5See the Operating Metrics file in the official OPRA document library page .reported an astonishing peak rate of 45.9 million messages per second in February\n2024.5\nWith large volumes of financial data comes a new space of opportunities:\n•Overcoming sample selection bias that might exist in small datasets\n•Enabling investors and traders to access high-frequency market data\n•Capturing patterns and financial activities not represented in small datasets\n•Monitoring and detecting frauds, market anomalies, and irregularities\n•Enabling the use of advanced machine learning and data mining techniques that\ncan capture complex and nonlinear signals\n•Alleviating the problem of high dimensionality in machine learning, where the\nnumber of features is significantly high compared to the number of observations\n•Facilitating the development of financial data products that are derived from\ndata, improve with data, and produce additional data\nHowever, such opportunities come with technical challenges, mostly related to data\nengineering:\n•Collecting and storing large volumes of financial data from various sources\nefficiently\n•Designing querying systems that enable users to retrieve extensive datasets\nquickly\n•Building a data infrastructure capable of handling any data size seamlessly\n•Establishing rules and procedures to ensure data quality and integrity\n•Aggregating large volumes of data from multiple sources\n•Linking records across multiple high-frequency datasets\nThe frequency at which data is generated and collected greatly impacts financial data\nvolumes. A process that produces one million records per second generates signifi‐\ncantly larger data volumes compared to a process that produces one thousand records\nper second. This rate of data generation is known as data velocity  and will be dis‐\ncussed in the following section.\nWhy Financial Data Engineering? | 15\nVelocity\nData velocity refers to the speed at which data is generated and ingested. Recent years\nhave seen an increase in the velocity of data generation in financial markets. High-\nfrequency trading, financial transactions, financial news feeds, and finance-related\nsocial media posts all produce data at high speeds.\nWith increased financial data velocity, new opportunities emerge:\n•Quicker reaction times as data arrives shortly after generation\n•Deeper and more immediate insights into intraday dynamics, such as price fluc‐\ntuations and patterns emerging within an hour, minute, or second\n•Enhanced market monitoring\n•Development of new trading strategies, including algorithmic trading and high-\nfrequency trading\nCrucially, high data velocity introduces critical challenges for data infrastructures:\nVolume\nHow to build event-driven systems that can handle the arrival of large amounts\nof data in real time\nSpeed\nHow to build a data infrastructure that can reliably cope with the speed of infor‐\nmation transmission in financial markets\nReaction time\nHow to build pipelines that can react as quickly as possible to new data arrival yet\nguarantee quality checks and reliability\nVariety/multistream\nHow to handle the arrival of many types of data from multiple sources in real\ntime\nThe exponential increase in financial data volumes and the velocity of data generation\ndoesn’t occur uniformly. Alongside this growth, new data types, formats, and struc‐\ntures have emerged to fulfill various business and technical requirements. The follow‐\ning section will explore this diversity of data in depth.\n16 | Chapter 1: Financial Data Engineering Clarified\n6Alternative datasets and their use cases in finance are discussed in detail in Chapter 2 .Variety\nThe third feature that defines big data is variety, which refers to the presence of many\ndata types, formats, or structures. To better describe this concept, let’s illustrate the\nthree types of structures that data can have:\nStructured data\nThis data has a clear format and data model, is easy to organize and store, and is\nready to analyze. The most common example is tabular data organized as rows\nand columns.\nSemi-structured data\nThis type of data lacks a straightforward tabular format but has some structural\nproperties that make it manageable. Often, semi-structured data is parsed and\nstored in a tabular format for ease of use. Examples include XML and JSON,\nwhich store data in a hierarchical tree-like format.\nUnstructured data\nThis data lacks any predefined structure or formatting and requires parsing and\npreprocessing using specialized techniques before analysis. The majority of data\nworldwide is unstructured, including formats like PDF, HTML, text, video, and\naudio.\nThe variety of financial data has significantly increased in recent years. For example,\nthe US Securities and Exchange Commission’s Electronic Data Gathering, Analysis,\nand Retrieval system (EDGAR) receives and handles about two million filings a year .\nSuch filings can be complex documents, many of which contain multiple attach‐\nments, scores of pages, and several thousands of pieces of information or details.\nAnother example is alternative data sources such as news, weather, satellite images,\nsocial media posts, and web search activities, which have been shown to be highly val‐\nuable for financial analysis and product development.6\nIncreased variety in financial data opens up new opportunities:\n•Incorporating new variables into financial analysis for enhanced predictions\n•Capturing new economic and financial activities that can’t be analyzed using\nstructured data alone\n•Facilitating the development and integration of innovative financial products like\nnews analytics, fraud detection, and financial networks\n•Enhancing regulatory capabilities to capture complex market structures for more\neffective oversight\nWhy Financial Data Engineering? | 17\nHowever, data variety also presents several data engineering challenges:\n•Building a data infrastructure capable of efficiently storing and managing diverse\ntypes of financial data, including structured, semi-structured, and unstructured\nformats\n•Implementing data aggregation systems to consolidate different data types into a\nsingle access point\n•Developing methodologies for cleaning and transforming new structures of\nfinancial data\n•Establishing specialized pipelines to process varied types of financial data, such\nas natural language processing for text and deep learning for images\n•Implementing identification and entity management systems to link entities\nacross a wide range of data sources\nBe Wary of the Curse of Dimensionality in Financial Data\nAlthough the volume of financial data has experienced significant growth in recent\ndecades, special consideration should be given to the ratio of data observations to the\nnumber of variables. For example, assume we have an initial sample of 10 firms and 5\nfeatures about their performance (e.g., revenues, net profit, and so on). If data increa‐\nses in size both in terms of observations (more firms) and variables (more features),\nthen we can conduct reliable analysis using the new, larger sample. However, if the\nincrease in data size concerns mainly the features and not the observations (i.e., no\nmore firms added to the dataset), then we might encounter the issue known as the\ncurse of dimensionality . It states that for a statistical or machine model to produce\nvalid predictions, the number of observations needs to grow exponentially with the\nnumber of features. Some researchers have argued  that this has been the case in some\nfinancial applications, such as asset management. A number of techniques can be\nused to counteract the curse of dimensionality: data augmentation  (collecting or gen‐\nerating more data) and dimensionality reduction  (reducing the number of features in\nthe data).\n18 | Chapter 1: Financial Data Engineering Clarified",14921
12-Finance-Specific Data Requirements and Problems.pdf,12-Finance-Specific Data Requirements and Problems,"Finance-Specific  Data Requirements and Problems\nThe financial industry has always witnessed constant transformation: new players\njoining and disrupting the competitive landscape, new technologies emerging and\nrevolutionizing the way financial markets function, new data sources expanding the\nspace of opportunities, and new standards and regulations getting released, promo‐\nted, and enforced.\nGiven these dynamics, the financial industry sets itself apart in terms of the issues\nand challenges that its participants face. A few key ones are listed here:\n•There is a lack of a standardization in some key areas:\n—Identification system for financial data\n—Classification system for financial assets and sectors\n—Financial information exchange\n•Lack of established data standards for financial transaction processing\n•Dispersed and diverse sources of financial data\n•Adoption of multiple data formats by companies, data vendors, providers, and\nregulators\n•Complexity in matching and identifying entities within financial datasets\n•Lack of reliable methods to define, store, and manage financial reference data\n(discussed in Chapter 2 )\n•Lack of relevant data for understanding and managing various financial prob‐\nlems due to poor data collection processes (e.g., granular data on financial mar‐\nket dependencies and exposures necessary for systemic risk analysis)\n•The constant need to adapt data and tech infrastructure to meet new market and\nregulatory demands (e.g., the EU’s Instant Payments Regulation  requires all pay‐\nment service providers to offer 24/7 euro payments within seconds, necessitating\nupgrades to legacy systems)\n•The constant need to record, store, and share financial data for various regula‐\ntory and market purposes (e.g., the EU’s Central Electronic System of Payment\nInformation  mandates payment system providers to track cross-border payment\ndata and share it with the tax authorities of EU member states).\n•Absence of standardized practices for cleaning and ensuring the quality of finan‐\ncial data\n•Difficulty in aggregating data across various silos and divisions within financial\ninstitutions\nWhy Financial Data Engineering? | 19\n•Creating consolidated tapes, which integrate market data from multiple sources\nincluding trade and quote information across various venues, continues to pose\ntechnological and market challenges\n•Balancing innovation and competitiveness with regulatory compliance\n•Persisting concerns regarding security, privacy, and performance in cloud migra‐\ntion strategies\n•Continued reliance on legacy technological systems due to organizational inertia\nand risk aversion\nOver the years, a number of industry and regulatory initiatives were proposed to\ntackle these issues. For example, to facilitate a standardized delivery of financial serv‐\nices and products, the United States established the Accredited Standards Committee\nX9 (ASC X9)  to create, maintain, and promote voluntary consensus standards for the\nfinancial industry. In addition to setting national standards in the United States, ASC\nX9 can submit standards to the International Organization for Standardization (ISO)\nin Geneva, Switzerland, to be considered an international ISO standard. ASC X9\ndevelops standards for many different areas and technologies, including electronic\nlegal orders for financial institutions, electronic benefits and mobile payments, finan‐\ncial identifiers, fast payment systems, cryptography, payment messages, and more.\nAdditionally, international agencies such as the Association of National Numbering\nAgencies (ANNA) were established to coordinate and foster the adoption of ISO-\nbased financial identifiers (covered in Chapter 3 ). Frameworks such as eXtensible\nBusiness Reporting Language  (XBRL) (discussed  in Chapter 7 ) were developed to\nstandardize the communication and reporting of business information. Following the\nfinancial crisis of 2007–2008, the financial industry realized the need for a standard‐\nized identifier for legal entities involved in market transactions, which led to the\ndevelopment  of the celebrated Legal Entity Identifier (LEI), discussed in Chapter 3 .\nFurthermore, financial market players have also been actively contributing and pro‐\nviding solutions to the above-mentioned problems. To give a few examples, Bloom‐\nberg is currently promoting its Financial Instrument Global Identifier  (FIGI)  as an\nopen standard for identifying financial instruments; LSEG released its Permanent\nIdentifier  (PermID) to complement existing market identifiers; and financial institu‐\ntions such as JPMorgan have been pioneers in promoting market practices such as\nValue at Risk (V AR)  in the 90s, and more recently the use of financial APIs  to support\nfast, real-time data transactions.\n20 | Chapter 1: Financial Data Engineering Clarified",4885
13-Financial Machine Learning.pdf,13-Financial Machine Learning,"Financial Machine Learning\nMachine learning (ML) stands out as one of the most promising investments for\nshaping the future of the financial industry. To understand what machine learning is,\nit’s better first to understand what artificial intelligence (AI) is. Although there is no\nwell-accepted definition of artificial intelligence, in its simplest form, AI aims to\nunderstand the nature of intelligence to build systems that can reliably perform tasks\nthat usually would require human intelligence, such as speech recognition, visual per‐\nception, decision-making, and language understanding. Figure 1-4  illustrates the var‐\nious fields of inquiry in artificial intelligence.\nFigure 1-4. An outline of artificial  intelligence fields\nWhy Financial Data Engineering? | 21\n7Tom Mitchell, Machine Learning (McGraw-Hill, 1997), p. 2.Machine learning stands out as a highly popular and significant subfield within AI. It\nfocuses on building systems that can discover patterns from data, learn from their\nmistakes, and make predictions. The key word in machine learning is learning , which\nthe computer scientist Tom Mitchell eloquently illustrates as follows:7\nA computer program is said to learn from experience E with respect to some class of\ntasks T and performance measure P , if its performance at tasks in T, as measured by P ,\nimproves with experience E.\nMachine learning scientists and practitioners often develop models based on three\ntypes of learning: supervised, unsupervised, and reinforcement learning. Let’s explore\neach in detail.\nSupervised learning\nSupervised learning describes a learning approach that relies on an annotated\n(labeled) dataset comprised of a set of explanatory variables (called features ) and a\nresponse variable (called a label ). In a supervised setting, the model is trained to iden‐\ntify patterns using explanatory variables. The training process involves showing the\nmodel the actual value (label) it should have predicted, hence the term “supervised, ”\nand allowing it to learn from its mistakes (as illustrated in Figure 1-5 ).\nFigure 1-5. Supervised learning process\n22 | Chapter 1: Financial Data Engineering Clarified\n8For a good reference on performance metrics, I recommend Aurélien Géron’s book, Hands-On Machine\nLearning with Scikit-Learn, Keras, and TensorFlow  (O’Reilly, 2022).When building a supervised system, modelers start by fitting one or more models on\ntraining data, where features and labels are known, via a selected optimization pro‐\ncess such as gradient descent . Successively, the fit model(s) is tested on a second chunk\nof the data, called the validation dataset. The goal of the validation dataset is to allow\nthe machine learning expert to fine-tune the so-called model hyperparameters  via a\nprocess called regularization . Regularization is a technique used to achieve a balance\nbetween bias (how well a model learns the training data) and variance  (how good the\nmodel is at generalizing to new instances unseen during training). Finally, a test data‐\nset is used to evaluate the performance of the model that did best on the validation\ndataset. Performance metrics include accuracy, precision, root mean square error\n(RMSE), and mean square error (MSE), to name a few.8\nSupervised learning can be divided into two categories: classification , which predicts a\nclass label for a categorical variable, and regression , which predicts a quantity for a\nnumerical variable. Linear regression, autoregressive models, generalized additive\nmodels, neural networks, and tree-based models are well-known regression methods.\nFor classification tasks, methods such as logistic regression, support vector machines,\nlinear discriminant analysis, tree models, and artificial neural networks are com‐\nmonly used.\nIn finance, supervised learning is extensively employed for both classification and\nregression tasks. Examples of financial regression problems include stock price fore‐\ncasting, volatility estimation and prediction, asset pricing, and risk assessment. Clas‐\nsification problems are also plenty in finance, for example, credit scoring, default\nprediction, corporate action prediction, fraud detection, and credit risk rating.\nWhy Financial Data Engineering? | 23\n9For an overview of clustering techniques, I recommend the official documentation of scikit-learn .Unsupervised learning\nUnsupervised learning is used to extract patterns and relationships within data\nwithout relying on known target response values (labels). Unlike supervised learning,\nit does not have a teacher (supervisor) correcting the model based on knowledge of\nthe correct answer (as illustrated in Figure 1-6 ).\nFigure 1-6. Unsupervised learning process\nThere are two main types of unsupervised learning: clustering , where a model is\ntrained to learn and find groups (clusters) in the data, and density estimation , which\ntries to summarize the distribution of the data. Examples of clustering techniques\ninclude k-means, k-nearest neighbor, principal component analysis, and hierarchical\nclustering, while the kernel density estimator is perhaps the most common example\nof density estimation techniques.9\nUnsupervised learning applications in finance are still in their early stages, but the\nfuture trend is promising. For example, clustering can be used to group similar finan‐\ncial time series, cluster stocks into groups based on sector or risk profile, analyze cus‐\ntomer and market segmentation, and find similar firms or customers to assign similar\nscores or ratings.\n24 | Chapter 1: Financial Data Engineering Clarified\n10An excellent reference on reinforcement learning in finance is the book by Ashwin Rao and Tikhon Jelvis,\nFoundations of Reinforcement Learning with Applications in Finance  (CRC Press, 2022).Reinforcement learning\nIn reinforcement learning, an artificial agent is placed in an environment where it can\nperform a sequence of actions over a state space and learn to make better decisions\nvia a feedback mechanism. The key difference between this technique and supervised\nlearning is that the feedback from the teacher is not about providing the right answer\n(true label); instead, the agent is given a reward (positive or negative) in order to\nencourage certain behaviors (actions) and punish others (see Figure 1-7 ).\nFigure 1-7. Reinforcement learning process\nAs many financial activities entail decision-making by agents, there has been a con‐\nsiderable interest among financial practitioners and researchers in reinforcement\nlearning, which centers on optimal decision-making. Financial applications of rein‐\nforcement learning include portfolio selection and optimization, optimal trade execu‐\ntion, and market-making.10\nWhy Financial Data Engineering? | 25\nGenerative AI and Large Language Models in Finance\nRecently, an emerging field of AI, known as generative AI, has received remarkable\nattention following the introduction by OpenAI of the large language model (LLM)\nChatGPT. In generative AI, a system is trained to generate one or more outcomes,\nsuch as text, image, and video, in response to prompts. In the case  of ChatGPT, the\nuser interacts with a chatting machine in a conversational way, where the machine\ncan answer a wide variety of questions, solve problems, generate code, make and\nchallenge statements, admit its mistakes, and at the same time reject inappropriate\nprompts.\nLanguage modeling is quite popular in finance. For example, it’s used in sentiment\nanalysis, news classification, named entity recognition, fraud detection, and question\nanswering. However, as of the time of writing of this book, no LLM has been tuned\nand adapted for the financial domain. As a first step in this direction, Bloomberg \ndeveloped BloombergGPT , a 50-billion parameter LLM developed with a mixed\napproach that focuses on financial domain-specific capabilities, while also maintain‐\ning a competitive performance on general-purpose tasks.\nBloombergGPT was trained on a massive dataset consisting of English financial\ndocuments such as filings, news, press releases, web-scraped documents, and social\nmedia pulled from the Bloomberg archives. This data was augmented with public\ndatasets such as The Pile, the Colossal Clean Crawled Corpus (C4), and Wikipedia. In\ntotal, this has led to a comprehensive dataset of almost 700 billion tokens, half\ndomain-specific and half general-purpose. The resulting model has outperformed\nexisting open source models on financial-specific tasks while guaranteeing on par and\nsometimes better performance on general language tasks.\nAs an alternative to BloombergGPT, a group of researchers, in collaboration with the\nAI4Finance Foundation, developed and released FinGPT , an open source framework\nthat allows the development of LLMs for the financial domain. The data sources used\nto develop FinGPT include financial news, social media, filings, trends, and academic\ndatasets.\nGenAI applications are rapidly expanding within the finance industry. For instance,\nFactSet, a leading data and analytics provider, has launched Portfolio Commentary .\nThis tool uses AI to generate explanations of portfolio performance attribution analy‐\nsis—the practice of explaining portfolio performance relative to a benchmark—\nwithin FactSet’s renowned Portfolio Analytics application.\nApplied machine learning systems rely on data and computational resources; thus,\nhaving access to more data and computing power leads to better and faster predic‐\ntions. In finance, where computational resources and datasets have grown, financial\nmachine learning has emerged as a promising yet challenging area of research and\npractice.\n26 | Chapter 1: Financial Data Engineering Clarified",9807
14-The Disruptive FinTech Landscape.pdf,14-The Disruptive FinTech Landscape,"11The problem of false discoveries is well-known in finance. For an introduction to this topic, please refer to the\narticle by Campbell R. Harvey, Y an Liu, and Heqing Zhu, “… and the Cross-Section of Expected Returns” ,\nReview of Financial Studies  29, no. 1 (January 2016): 5–68.According to Marcos López de Prado , a leading hedge fund manager and quantitative\nanalyst, financial machine learning has proven to be very successful and is likely to be\na major factor in shaping the future of financial markets, but it shouldn’t be ignored\nthat it presents major challenges that need to be taken into consideration. Perhaps the\nmost relevant challenge that’s worth mentioning is the problem of false discoveries.\nThis refers to the practice of finding what seems like a valid pattern in the data, yet in\nreality is a spurious relationship.11 Other challenges include the interpretability/\nexplainability of the models, performance, costs, and ethics.\nFor financial institutions to effectively invest in and leverage financial machine learn‐\ning, they must ensure they are machine learning ready. This involves having the right\nteam with expertise in both finance and machine learning, a sufficient quality and\nquantity of financial data for ML algorithms, a robust data infrastructure, dedicated\nML-oriented data pipelines, DevOps (or MLOps) practices for seamless deployment\nand integration, and monitoring tools. With this foundation, financial data engineer‐\ning becomes crucial. Financial data engineers collaborate closely with financial ML\nscientists and ML engineers to define data requirements, automate data transforma‐\ntions, perform quality checks, and structure ML workflows for fast and  high-\nperformance computations.\nThe Disruptive FinTech Landscape\nFollowing the 2007–2008 financial crisis, traditional financial institutions have faced\na significant increase in regulatory requirements. Consequently, the focus of market\nparticipants has shifted substantially toward compliance. At the same time, as cus‐\ntomers became more accustomed to using services online, demand for simple and\nuser-friendly online financial products has increased. These factors paved the way for\na new wave of technological innovation in the financial sector, commonly known as\nFinTech .\nThe term FinTech has emerged as a market portmanteau to describe both innovative\ntechnologies developed for the financial sector and the startup firms that develop\nthese technologies. FinTech firms have attracted particular attention in the media and\nthe market due to their innovative, flexible, and experimental approach. Not being\nconstrained by regulatory debt, FinTechs have been employing modern and noncon‐\nventional approaches to solving and improving a wide range of financial problems,\nsuch as payments, lending, investment, fraud detection, and cryptocurrency. Tradi‐\ntional financial institutions would lack this flexibility due to factors such as\nWhy Financial Data Engineering? | 27\norganizational inertia, regulatory constraints, security concerns, and a lack of innova‐\ntive culture.\nThe main distinguishing features of FinTech services are specialization  and personali‐\nzation . As small firms, FinTechs tend to focus on penetrating only specific and niche\nareas of the financial system. Figure 1-8  illustrates the different areas of specialization\nof FinTech firms. As the figure illustrates, the FinTech landscape spans all segments of\nthe financial sector, from fundamental functions such as payments and investment to\nmore specialized areas such as regulatory compliance (often called regtech ) and\nanalytics.\nFigure 1-8. A breakdown of the main FinTech investment areas\nMoreover, the FinTech business model has demonstrated competitiveness through its\ncustomizable and personalized offerings. For example, digital wealth management\nplatforms like Betterment and Wealthfront provide clients with detailed surveys to\nassess their financial goals and risk preferences, enabling them to offer investment\nplans tailored to each investor’s unique objectives and expectations.\nOverall, the FinTech market has seen rapid growth since its inception. According to a\nreport published by Boston Consulting Group , as of 2023, there were roughly 32,000\nFinTech firms globally, securing more than $500 billion in funding. The same report\npredicts that by 2030, the annual revenue of the FinTech sector is expected to reach\n$1.5 trillion, with banking FinTech representing 25% of the overall banking\nevaluations.\n28 | Chapter 1: Financial Data Engineering Clarified\nPayments: The Dynamic Heart of Innovation in Financial Technology\nIn the rapidly evolving landscape of financial technology, payments represent the\nmost active and vibrant area of innovation. As the central mechanism for the\nexchange of value, payments have undergone a dramatic transformation, driven by\ntechnological advancements, changing consumer expectations, and regulations.\nToday, the payments sector is not merely about transferring money; it is about creat‐\ning the seamless, secure, and instantaneous financial experiences that are integral to\nour daily lives.\nWith the proliferation of smartphones, the rise of ecommerce, and the demand for\nreal-time transactions, the payments industry is at the forefront of FinTech innova‐\ntion. This sector is pioneering the use of cutting-edge technologies such as cloud\ncomputing, real-time systems, blockchain, open banking APIs, biometric authentica‐\ntion, and artificial intelligence to enhance security, efficiency, and user experience. As\na result, payments are reshaping the financial ecosystem, promoting financial inclu‐\nsion, and driving economic growth.\nThere are many types of entities involved in the payment ecosystem. Examples\ninclude the following:\nIssuers\nFinancial institutions that provide consumers with payment cards (credit, debit,\nor prepaid), such as JPMorgan Chase, Bank of America, and Citibank.\nAcquirers\nFinancial institutions that manage and process payment transactions for mer‐\nchants, like Wells Fargo Merchant Services, First Data, and Elavon.\nPayment processors\nCompanies that handle the technical aspects of processing transactions between\nissuers and acquirers, including PayPal, Square, and Stripe.\nPayment networks\nNetworks that connect issuers and acquirers to facilitate card transactions, such\nas Visa, Mastercard, and American Express.\nPayment gateways\nServices that act as an intermediary in electronic financial transactions. Examples\ninclude Authorize.net, Braintree, and Cybersource.\nDigital wallet providers\nCompanies offering electronic devices or software for storing payment informa‐\ntion and making transactions, such as Apple Pay, Google Wallet, and Samsung\nPay.\nWhy Financial Data Engineering? | 29\nFinTech startups\nInnovative companies creating new payment solutions using advanced technol‐\nogy, including Revolut, Stripe, and Square.\nSettlement institutions\nEntities that facilitate the final transfer of funds between financial institutions to\ncomplete payment transactions, such as Fedwire and the Clearing House Inter‐\nbank Payments System (CHIPS).\nPayment infrastructures\nSystems and networks that facilitate the processing, authorization, and settlement\nof financial transactions between parties. Examples include the Society for\nWorldwide Interbank Financial Telecommunication (SWIFT) for financial mes‐\nsaging and interbank communication and Australia’s New Payments Platform\n(NPP) for fast payments.\nAdditionally, there are several payment-related regulatory frameworks currently\nbeing developed and adopted. For instance, the European Union is moving forward\nwith the following regulations:\nInstant Payments Regulation\nRequires all payment service providers (PSPs) to offer the capability to send and\nreceive euro payments within seconds, 24/7, across the EU.\nCentral Electronic System of Payment Information (CESOP)\nMandates payment system providers to track cross-border payment data and\nshare it with the tax authorities of EU member states.\nMarkets in Crypto-Assets (MiCA) regulation\nAims to harmonize EU market rules for crypto-assets.\nEurosystem Collateral Management System (ECMS)\nA unified system for managing assets used as collateral in Eurosystem credit\noperations.\nThird  Payment Services Directive (PSD3) and Payment Services Regulation (PSR)\nSeek to further harmonize the payment market and reduce national variations.\nE-Money Directive\nProvides the legal framework for issuing and managing electronic money within\nthe European Union.\nFor financial data engineering aspirants, diving into payments promises not just a\ncareer path but a gateway to shaping the future of digital transactions and financial\nservices on a global scale.\n30 | Chapter 1: Financial Data Engineering Clarified",8898
15-The Financial Data Engineer Role.pdf,15-The Financial Data Engineer Role,"12To read more about the topic of systemic risk, I recommend Jaime Caruana’s article, “Systemic Risk: How to\nDeal With It?” , Bank for International Settlements (February 2010).\n13To read more about FMI regulation, see “Principles for Financial Market Infrastructures” , the Bank for Inter‐\nnational Settlements (April 2012), and “Core Principles for Systemically Important Payment Systems” , the\nBank for International Settlements (January 2001).To thrive in this technology-intensive, high-performance, and data-driven landscape,\naspiring FinTech companies must prioritize their software and data engineering\nstrategies. To compete with and/or collaborate with incumbent financial institutions,\nFinTechs must ensure the highest standards of quality, reliability, and security. In this\ncontext, financial data engineers play a crucial role by designing efficient and reliable\ndata ingestion, processing, and analysis pipelines that can scale and seamlessly inte‐\ngrate with other solutions.\nRegulatory Requirements and Compliance\nFinancial institutions, and banks in particular, have a special status in the economic\nsystem. This is justified by the fact that the financial sector forms a complex network\nof asset cross-holdings, ownerships, investments, and transactions among financial\ninstitutions. As a consequence, a market shock that leads to the failure of one or more\nfinancial institutions can trigger a cascade of failures that might destabilize the entire\nfinancial system and cause an economic meltdown.12 The global financial crisis of\n2007–2008 is the best example of such a scenario.\nTo avoid costly financial crises, the financial sector has been subjected to a large num‐\nber of regulations, both national and international. Crucially, a significant part of\nfinancial regulatory requirements concerns the way banks should collect, store,\naggregate, and report data. For example, following the financial crisis of 2007–2008,\nthe Basel Committee on Banking Supervision noted that banks, and in particular\nGlobal Systemically Important Banks  (G-SIBs), lacked a data infrastructure that could\nallow for quick aggregation of risk exposures to identify hidden risks and risk con‐\ncentrations. To overcome this problem, the Basel Committee issued a list of 13 princi‐\nples on data governance and infrastructure  that banks need to implement to\nstrengthen their risk data aggregation and reporting capabilities.\nBeyond banks, other financial institutions are also considered systemically important.\nThese include Financial Market Infrastructures  (FMIs), which facilitate the processing,\nclearing, settlement, and custody of payments, securities, and transactions. Examples\nof FMIs are stock exchanges, multilateral trading facilities, central counterparties,\ncentral securities depositories, trade repositories, payment systems, clearing houses,\nsecurities settlement systems, and custodians. FMIs are critical to the functioning of\nfinancial markets and the broader economy, making them subject to extensive\nregulation.13\nWhy Financial Data Engineering? | 31",3108
16-Where Do Financial Data Engineers Work.pdf,16-Where Do Financial Data Engineers Work,"Occasionally, regulators may require financial institutions to collect new types of\ndata. For example, the European directive known as the Markets in Financial Instru‐\nments Directive , or MiFID, requires firms providing investment services to collect\ninformation regarding their clients’ financial knowledge to assess whether their level\nof financial literacy matches the complexity of the desired investments.\nTo comply with regulations, financial institutions need dedicated financial data engi‐\nneering and management teams to design and implement a robust data infrastruc‐\nture. This infrastructure must capture, process, and aggregate all relevant data and\nmetadata from multiple sources while ensuring high standards of security and opera‐\ntional and financial resilience. It should enable risk and compliance officers to\nquickly and accurately access the data needed to demonstrate regulatory compliance.\nFinancial data engineers will also be tasked with creating and enforcing a financial\ndata governance framework that guarantees data quality and security, thereby\nincreasing trust among management, stakeholders, and regulators. In Chapter 5 ,\nFinancial Data Governance, we will explore these topics in detail.\nThe Financial Data Engineer Role\nThe financial data engineer is at the core of everything we’ve discussed so far. Work‐\ning in the financial industry can be a very rewarding and exciting career. A decade\nago, the most in-demand roles in finance were analytical, such as financial engineers,\nquantitative analysts (or quants), and analysts. But with the digital revolution that\ntook place with big data, the cloud, and FinTech, titles such as data engineer, data\narchitect, data manager, and cloud architect have established themselves as primary\nroles within the financial industry. In this section, I will provide an overview of a\nfinancial data engineer’s role, responsibilities, and skills.\nDescription of the Role\nThe role of a financial data engineer is in high demand, though the title, required\nskills, and responsibilities can vary significantly between positions. For example, the\ntitle of a financial data engineer could be any of the following:\n•Financial data engineer\n•Data engineer, finance\n•Data engineer, fintech\n•Data engineer, finance products\n•Data engineer, data analytics, and financial services\n•Financial applications data engineer\n•Platform data engineer, financial services\n32 | Chapter 1: Financial Data Engineering Clarified\n•Software engineer, financial data platform\n•Software engineer, financial ETL pipelines\n•Data management developer, FinTech\n•Data architect, finance platform\nIn many cases, other titles that don’t include the term “data engineering” involve, to a\nlarge extent, practices and skills related to financial data engineering. For example,\nthe role of a machine learning engineer could involve many responsibilities concern‐\ning the creation, deployment, and maintenance of reliable analytical data pipelines for\nmachine learning. The role of quantitative developer, common among financial insti‐\ntutions, often involves tasks relating to developing data pipelines, data extraction, and\ndata transformations.\nIt is important to know that the role of a financial data engineer is neither a closed\ncircle nor a professional lock-in. Even though financial domain knowledge is a major\nplus for financial data engineering roles, many financial institutions would accept\npeople with data engineering experience who come from different backgrounds. Sim‐\nilarly, working as a financial data engineer would easily allow you to fit into other\ndomains, given the rich variety of technical problems and challenges you might\nencounter in the financial industry.\nWhere Do Financial Data Engineers Work?\nThe demand for financial data engineers primarily arises from financial institutions\nthat generate and store data and are willing or required to invest in data-related tech‐\nnologies. Let’s consider a few examples.\nFinTech\nFinTech firms are technology oriented and data driven; therefore, they are one of the\nbest places to work as a financial data engineer. One of the main advantages of work‐\ning for a FinTech is that you get to witness the entire lifecycle of product develop‐\nment. This provides engineers a solid overview of how data, business, and technology\nare combined to make a successful product. Another advantage is that you get to con‐\ntribute original ideas and solutions to major infrastructural and software problems\n(e.g., choosing a database or finding a financial data vendor).\nCommercial banks\nCommercial banks are financial institutions that accept deposits from individuals\nand institutions while providing loans to consumers and investors, process a signifi‐\ncant volume of daily transactions, and adhere to numerous regulatory requirements.\nTo effectively manage their internal operations and ensure timely reporting, commer‐\ncial banks typically employ teams of software and data engineers. These are\nThe Financial Data Engineer Role | 33\nresponsible  for developing and maintaining database systems, data aggregation and\nreporting mechanisms, customer analytics infrastructure, and transactional systems\nfor various banking activities such as accounts, transfers, withdrawals, and deposits.\nWorking as a data engineer at a commercial bank offers the opportunity to gain val‐\nuable insights into industry standards and best practices related to security, reliability,\nand compliance.\nInterestingly, commercial banks frequently form collaboration agreements  with Fin‐\nTech firms to extend their services to the public. These partnerships necessitate a\nrobust data infrastructure that facilitates secure and efficient server communication,\noften through financial APIs. Consequently, banks and FinTech firms need to hire\nfinancial data engineers to design and implement backends for data collection, trans‐\nmission, aggregation, and integration.\nInvestment banks\nAn investment bank is a financial institution that provides corporate finance and\ninvestment services, such as mergers and acquisitions, leveraged buyouts, and initial\npublic offerings (IPOs). Unlike commercial banks, investment banks do not accept\ndeposits or give loans. Sometimes, they invest their own money via proprietary\ntrading.\nInvestment banks engage in various activities that involve the generation, extraction,\ntransformation, and analysis of financial data. These include building and backtesting\ninvestment strategies, asset pricing, company valuation, and market forecasting. This\nrequires frequent and easy access to different types of financial data. Additionally,\ninvestment banks must regularly report compliance-related data to regulatory author‐\nities. To facilitate quick and straightforward access to this data, investment banks\nneed a team of financial data engineers to design and maintain systems for data col‐\nlection, transformation, aggregation, and storage.\nAsset management firms\nAsset management firms are financial institutions that provide investment and asset\nmanagement services to customers looking to invest their money. These can be inde‐\npendent entities or divisions within a large financial institution. Typically, asset man‐\nagers operate on an institutional level, with clients such as mutual funds, pension\nfunds, insurance companies, universities, and sovereign wealth funds.\nTo provide investment services, asset managers require access to a wide array of\nfinancial data to build investment strategies, construct portfolios, analyze financial\nmarkets, manage risks, and report on behalf of their clients. To manage such data,\nasset management firms employ in-house financial data engineers to design and\nmaintain effective data strategies, governance, and infrastructure. Even when using\n34 | Chapter 1: Financial Data Engineering Clarified\nthird-party data management solutions, in-house engineers are crucial for overseeing\nand enhancing the data infrastructure.\nHedge funds\nHedge funds are financial institutions that actively invest a large pool of money in\nvarious market positions (buy and sell) and asset classes (equity, fixed income, deriv‐\natives, alternative investments) to generate above-market returns. To meet their\nfinancial return objectives, hedge funds build and test (backtest) a large number of\ncomplex investment strategies and portfolio combinations.\nTo achieve their goals, hedge funds rely on a large number of heterogeneous financial\ndata sources from various providers. Financial engineers and quantitative developers\nworking at hedge funds need high-quality and timely access to financial data. More‐\nover, hedge funds may invest in algorithmic and high-frequency strategies, which\nrequire robust and efficient data infrastructure for easy data read and write opera‐\ntions. This environment makes hedge funds an ideal workplace for financial data\nengineers.\nRegulatory institutions\nA variety of national and international regulatory bodies have been established to\noversee financial markets. Examples include national entities like central banks and\nlocal market regulators, as well as international bodies such as the Bank for Interna‐\ntional Settlements, its Committee on Payments and Market Infrastructures, and the\nFinancial Stability Board.\nThese institutions perform a wide variety of activities that require significant invest‐\nments in financial data engineering and management. For example, if a regulatory\nagency establishes mandatory reporting and filing requirements, it requires a scalable\ndata infrastructure capable of processing and storing all the reported data. Addition‐\nally, regulatory agencies might provide their members with principles and best practi‐\nces on financial data infrastructure and governance system design. This requires\ninternal teams of data engineers, data managers, and industry experts who can\ndevelop and formulate market recommendations.\nFinancial data vendors\nData vendors are key players in financial markets, providing subscription-based\naccess to financial data collected from numerous sources. Notable examples include\nBloomberg, LSEG, and FactSet. Due to their business model, these companies face\nvarious challenges related to data collection, curation, formatting, ingestion, storage,\nand delivery. Consequently, they offer some of the best opportunities for developing a\ncareer in financial data engineering.\nThe Financial Data Engineer Role | 35",10568
17-Responsibilities and Activities of a Financial Data Engineer.pdf,17-Responsibilities and Activities of a Financial Data Engineer,"Security exchanges\nSecurity exchanges are centralized venues where buyers and sellers of financial secur‐\nities conduct their transactions. Prominent examples include the New Y ork Stock\nExchange, NASDAQ, and the London Stock Exchange.\nExchanges need to record all activities and transactions that they facilitate on a daily\nbasis. Some exchanges offer paid subscriptions to their transaction and quotation\ndata. Additionally, they manage tasks like symbology, i.e., assigning identifiers and\ntickers to listed securities. All this makes exchanges an ideal place to develop a career\nas a financial data engineer, especially if you want to be at the heart of the financial\ncenter.\nBig tech firms\nBig tech companies such as Google, Amazon, Meta, and Apple have developed into\nmajor platforms for user interactions, transactions, and various online activities . Tech\ncompanies rely on two mechanisms to expand their activities: user data and network\neffect. The more activities happen on an online platform, the more data can be collec‐\nted. Data is then used to study customer behavior to offer new products and services.\nThis, in turn, encourages others to join the platform, which generates yet more data,\nand so on.\nRelying on these self-reinforcing mechanisms, tech giants like Amazon, Apple, Goo‐\ngle, and Alibaba have expanded into financial services, offering products like pay‐\nments, insurance, loans, and money management. This move capitalizes on their\nextensive customer data, wide-reaching networks, and advanced technology, leading\nto the creation of user-friendly services such as mobile device payments. Conse‐\nquently, dedicated teams of data engineers, finance specialists, and machine learning\nexperts are required to support these operations.\nResponsibilities and Activities of a Financial Data Engineer\nThe financial data engineer’s set of tasks and responsibilities will depend on the\nnature of the job and business problems, the hiring institution, and, most impor‐\ntantly, the firm’s data maturity.\nData maturity  is an important concept that relates to data strategy . A data strategy is a\nlong-term plan that describes the roadmap of objectives, people, processes, rules,\ntools, and technologies required to manage an organization’s data assets. To measure\ndata strategy progress, data maturity approaches are often used. With a data maturity\nframework, an organization can illustrate the stages of development toward data usa‐\nbility, analytical capabilities, and integration. To further illustrate the concept, I bor‐\nrow and build on the framework proposed by Joe Reis and Matt Housley in their\nbook, Fundamentals of Data Engineering , which organizes data maturity into three\nsteps: starting with data, scaling with data, and leading with data.\n36 | Chapter 1: Financial Data Engineering Clarified\nStarting with data\nA financial institution that is starting with data is at the very early stage of its data\nmaturity. Note that this doesn’t necessarily mean that the institution is new; old insti‐\ntutions (e.g., traditional banks) might decide to initiate digital transformation plans\nto automate and modernize their operations (e.g., cloud migration).\nWhen starting with data, the financial data engineer’s responsibilities are likely to be\nbroad and span multiple areas such as data engineering, software engineering, data\nanalytics, infrastructure engineering, and web development. This early phase prioriti‐\nzes speed and feature expansion over quality and best practices.\nScaling with data\nDuring this stage, the financial institution needs to assess its processes, identify bot‐\ntlenecks, and determine current and future scaling requirements. With these insights\nin hand, the institution can proceed to enhance the scalability, reliability, quality, and\nsecurity of its financial data infrastructure. The primary objective here is to eliminate/\nhandle any technological constraints that may be an obstacle to the company’s\ngrowth.\nDuring this stage, financial data engineers will be able to focus on adopting best prac‐\ntices for building reliable and secure systems, e.g., codebase quality, DevOps, gover‐\nnance, security, standards, microservices, system design, API and database scalability,\ndeployability, and a well-established financial data engineering lifecycle.\nLeading with data\nOnce a financial institution reaches the stage at which it is able to lead the market\nwith data, it is considered data driven. In this stage, all processes are automated,\nrequiring minimal manual intervention; the product can scale to any number of\nusers; internal processes and governance rules are well established and formalized;\nand feature requests go through a well-defined development process.\nDuring this stage, financial data engineers can specialize in and focus on specific\naspects of the financial data infrastructure. There will always be space for further\noptimizations via roles and departments like site reliability engineering, platform\nengineering, data operations, MLOps, FinOps, data contracts, and new integrations.\nThe Financial Data Engineer Role | 37",5167
18-Skills of a Financial Data Engineer.pdf,18-Skills of a Financial Data Engineer,"Skills of a Financial Data Engineer\nFinancial data engineers bring together three types of skills: financial domain knowl‐\nedge, technical data engineering skills, and soft and business skills. We’ll briefly illus‐\ntrate these skillsets in the upcoming sections.\nFinancial domain knowledge\nHaving a good understanding of finance, financial markets, and financial data is an\nessential and competitive asset in any finance-related job, including financial data\nengineering. Examples of financial domain skills include the following:\n•Understanding the different types of financial instruments (stocks, bonds, deriva‐\ntives, etc.)\n•Understanding the different players in financial markets (banks, funds,\nexchanges, regulators, etc.)\n•Understanding the data generation mechanisms in finance (trading, lending,\npayments, reporting, etc.)\n•Understanding company reports (balance sheet, income statement, prospectus,\netc.)\n•Understanding the market for financial data (vendors, providers, distributors,\nsubscriptions, delivery mechanisms, coverage, etc.)\n•Understanding financial variables and measures (price, quote, volume, yield,\ninterest rate, inflation, revenue, assets, liability, capitalization, etc.)\n•Understanding financial theory terms (risk, uncertainty, return, arbitrage, volatil‐\nity, etc.)\n•Understanding of compliance and privacy concepts (personally identifiable\ninformation (PII), anonymization, etc.)\n•Knowledge of financial regulation and data protection laws is a plus (Basel rules,\nMiFID, Solvency II, GDPR, the EU’s General Data Protection Regulation, etc.)\nTechnical data engineering skills\nFinancial data engineering requires strong technical skills, which can vary across\nfinancial institutions, depending on their business needs, products, technological\nstack, and data maturity. Crucially, it’s important to keep in mind that the data engi‐\nneering landscape is quite dynamic, with new technologies emerging and diffusing\nevery year. For this reason, this book will focus more on immutable and technology-\nagnostic principles and concepts rather than on tools and technologies. But to give\nyou an illustrative and nonexhaustive overview of the current landscape (as of 2024),\nexpect as a financial data engineer to be asked about your knowledge of the following\nareas:\n38 | Chapter 1: Financial Data Engineering Clarified\nDatabase query and design\n•Experience with relational database management systems (RDBMSs) and related\nconcepts, in particular Oracle, MySQL, Microsoft SQL Server, and PostgreSQL\n•Solid knowledge of database internals and properties such as transactions, trans‐\naction control, ACID (atomicity, consistency, isolation, durability), BASE (basi‐\ncally available, soft state, evenutally consistent), locks, concurrency management,\nW AL (write-ahead logging), and query planning\n•Experience with data modeling and database design\n•Experience with the SQL language, including advanced concepts such as user-\ndefined functions, window functions, indexing, clustering, partitioning, and\nreplication\n•Experience with data warehouses and related concepts and design patterns\nCloud skills\n•Experience with cloud providers (Amazon Web Services, Azure, Google Cloud\nPlatform, Databricks, etc.)\n•Experience with cloud data warehousing (Redshift, Snowflake, BigQuery,\nCosmos, etc.)\n•Experience with serverless computing (lambda functions, AWS Glue, Google\nWorkflows, etc.)\n•Experience with different cloud runtimes (Amazon EC2, AWS Fargate, cloud\nfunctions, etc.)\n•Experience with infrastructure as code (IaC) tools such as Terraform\nData workflow  and frameworks\n•Experience with ETL (extract, transform, load) workflow solutions (AWS Glue,\nInformatica, Talend, Alooma, SAP Data Services, etc.)\n•Experience with general workflow tools such as Apache Airflow, Prefect, Luigi,\nAWS Glue, and Mage\n•Experience with messaging and queuing systems such as Apache Kafka and\nGoogle Pub/Sub\n•Experience in designing and building highly scalable and reliable data pipelines\n(dbt, Hadoop, Spark, Hive, Cassandra, etc.)\nInfrastructure\n•Experience with containers and container orchestration such as Docker, Kuber‐\nnetes, AWS Fargate, and Amazon Elastic Kubernetes Service (EKS)\n•Experience with version control using Git, GitHub, GitLab, feature branches, and\nautomated testing\nThe Financial Data Engineer Role | 39\n•Experience with system design and software architecture (distributed systems,\nbatch, streaming, lambda architecture, etc.)\n•Understanding of the Domain Name System (DNS), TCP , firewalls, proxy\nservers, load balancing, virtual private networks (VPNs), and virtual private\nclouds (VPCs)\n•Experience building integrations with and reporting datasets for payments,\nfinance, and business systems like Stripe, NetSuite, Adaptive, Anaplan, and\nSalesforce\n•Experience working in a Linux environment\n•Experience with software architecture diagramming and design tools such as\ndraw.io, Lucidchart, CloudSkew, and Gliffy\nProgramming languages and frameworks\n•Experience with Object-Oriented Programming (OOP)\n•Experience optimizing data infrastructure, codebase, tests, and data quality\n•Experience with generating data for reporting purposes\n•Experience working with Pandas, PySpark, Polars, and NumPy\n•Experience working with financial vendor APIs and feeds like the Bloomberg\nServer API, LSEG’s Eikon, FactSet APIs, the OpenFIGI API, and LSEG’s PermID\n•Experience with web development frameworks such as Flask, FastAPI, and\nDjango\n•Understanding of software engineers’ best practices, Agile methodologies, and\nDevOps\nAnalytical skills\n•Knowledge of data matching and record linkage techniques\n•Knowledge of financial text analysis such as extracting entities from text, fraud\ndetection, and know your customer (KYC)\n•Knowledge of financial data cleaning techniques and quality metrics\n•Experience performing financial data analysis and visualization using various\ntools such as Microsoft Power BI, Apache Superset, D3.js, Tableau, and Amazon\nQuickSight\n•Basic experience in machine learning algorithms and generative AI\n40 | Chapter 1: Financial Data Engineering Clarified\nBusiness and soft skills\nFor most financial institutions, data represents a valuable asset. Therefore, financial\ndata engineers need to ensure that their work aligns with the data strategy and vision\nof their institution. To do so, they can complement their technical skills with business\nand soft skills such as the following:\n•Ability to comprehend technical aspects of the product and technology to com‐\nmunicate effectively with engineers, and to explain these concepts in simpler\nterms to finance and business stakeholders\n•Understanding the value generated by financial data and its associated infrastruc‐\nture for the institution\n•Collaborating closely with finance and business teams to identify their data\nrequirements\n•Staying informed about the evolving financial and data technology landscape\n•Establishing policies for company members to access and request new financial\ndata\n•Interest in data analysis and machine learning, leveraging financial data\n•Proactively gathering and analyzing high-value financial data needs from busi‐\nness and analyst teams, and clearly communicating deliverables, timelines, and\ntradeoffs\n•Providing guidance and education on financial data engineering, expectations\nfrom a financial data engineer, and how to search, find, and access financial data\n•Participating in the assessment of new financial data sources, technologies, prod‐\nucts, or applications suitable for the company’s business\nCertainly, not every job demands proficiency in all these skills. Instead, a tailored\ncombination is often sought based on the specific business needs. Throughout this\nbook, you’ll learn about several of the aforementioned skills, diving deeply into some\nwhile experiencing an overview of others, all with demonstrations of their impor‐\ntance and practical application within the financial domain.\nThe Financial Data Engineer Role | 41",8106
19-Sources of Financial Data.pdf,19-Sources of Financial Data,"Summary\nThis chapter provided an overview of financial data engineering, summarized as\nfollows:\n•Defining financial data engineering and outlining its unique challenges\n•Justifying the need for financial data engineering and illustrating its applications\n•Describing the role and responsibilities of the financial data engineer\nNow that you have an idea about financial data engineering, it’s time to learn about\nthe most important asset in this field: financial data. In Chapter 2 , you will gain a\nthorough understanding of financial data, including its sources, types, structures, and\ndistinguishing features. Y ou will also learn about key benchmark financial datasets\nthat are widely used in the financial industry.\n42 | Chapter 1: Financial Data Engineering Clarified\nCHAPTER 2\nFinancial Data Ecosystem\nAs discussed throughout this book, data will be placed at the heart of financial market\noperations. As digital transformation and cloud migration efforts make their way\nthrough the financial arena, more and more financial activities will leave a digital\ntrace. Consequently, the amount, variety, and speed of financial data generation is\nconstantly increasing. This, in turn, has given rise to a vast and complex financial\ndata landscape, encompassing numerous sources, types, structures, providers, deliv‐\nery methods, and datasets essential for market participants to conduct their financial\nand business activities.\nThis chapter breaks down the key components of the financial data ecosystem and\nillustrates their features and issues.\nThese include the following key areas:\n•The different sources of financial data\n•The data structures used in finance, along with their technical details\n•The variety of data types that the financial industry generates and consumes\nThen, I’ll give an overview of some of the most important financial datasets used by\npractitioners and researchers.\n43",1926
20-Public Financial Data.pdf,20-Public Financial Data,"Sources of Financial Data\nFinancial data can originate from a variety of sources, so let’s establish a few baselines.\nIn this book, the term data source will be used to indicate the location, entity, or\nmechanism behind the generation of financial data. These sources may differ in a\nnumber of ways:\nAccessibility\nWhether the data is open access, commercially licensed, or proprietary\nUsability\nWhether the data source is easy to use and extract data from, and whether the\ndata is provided in a raw format or in a clean structure that is ready to use\nCoverage\nWhether the data source offers complete and wide coverage of financial data\nReliability\nWhether a source is trusted, secure, timely, and durable\nQuality\nWhether the data provided by the sources is of high quality (i.e., absence of\nerrors, biases, duplicates, etc.)\nDocumentation\nWhether additional information is available on the data structures, fields, and\ncontent of the data source\nThere are many different data sources in the financial data ecosystem. In this book, I\ncategorize them into six main types: publicly available data, data generated by security\nexchanges, commercial data provided by vendors and distributors, survey data, alter‐\nnative data, and confidential data. Let’s explore the main features of each data source\nin detail.\nPublic Financial Data\nPublic data is information that is freely accessible by anyone, without a license or fees,\nprimarily through the internet. Public financial data exists for a number of reasons,\nsuch as regulatory disclosure and reporting requirements, publicly released data by\ngovernmental and nongovernmental organizations, public research data, and free\nstock market APIs. Let’s explore each of these sources in detail.\nRegulatory disclosure requirements\nMany financial institutions are subject to data disclosure and reporting obligations\nestablished by national and international regulations. Such requirements aim to\nincrease market transparency, efficiency, investor protection, and liquidity, as well as\n44 | Chapter 2: Financial Data Ecosystem\n1If interested in this topic, I recommend Itay Goldstein and Liyan Y ang’s article, “Information Disclosure in\nFinancial Markets” , Annual Review of Financial Economics  9 (November 2017): 101–125, and Anat R. Admati\nand Paul Pfleiderer’s “Forcing Firms to Talk: Financial Disclosure Regulation and Externalities” , The Review of\nFinancial Studies  13, no. 3 (July 2000): 479–519.reduce the cost of capital for firms. For example, in the United States, companies are\nrequired to submit different reports about their operations to the Securities and\nExchange Commission (SEC) as established by the Securities Act of 1933, the Securi‐\nties Exchange Act of 1934, the Trust Indenture Act of 1939, and the Investment Com‐\npany Act of 1940.\nA variety of SEC public forms are available  for companies to file periodic financial\nstatements and various other disclosures. The most common form is the 10-K , which\nis filed annually and provides a comprehensive overview of the company’s financial\nperformance. The 10-Q , a comparable but less detailed document, is submitted fol‐\nlowing each of the three quarters for which a 10-K filing is not required. A special\nform, called 8-K, is used to notify investors or the SEC about special and relevant\nevents that might occur between 10-K and 10-Q filings. Another well-known type of\ndisclosure is the prospectus , which is a document providing details about a stock,\nbond, or mutual fund investment offering to the public.\nThe main advantage of regulatory disclosures is that they constitute a reliable,\nunbiased, and regular source of data, covering a wide range of regulated entities. The\nmain challenge, however, is that reporting data is often provided in raw and unstruc‐\ntured or semi-structured formats (e.g., HTML, TXT, or XML). Another issue is the\npotential for incomplete data due to the tradeoff between market efficiency and dis‐\nclosure precision, which arises from regulators’ demands that businesses reveal just\nenough data to ensure market efficiency without jeopardizing competition.1\nElectronic Data Gathering, Analysis, and Retrieval (EDGAR)\nEDGAR  is the primary data management system used by the SEC, designed to pro‐\nvide a seamless experience for businesses and individuals required to file reports with\nthe agency. The EDGAR Business Office (EBO) is the official owner of EDGAR and is\nresponsible for its administration and maintenance, strategic planning and develop‐\nment, and SEC staff customer support. The EDGAR database is public and free, pro‐\nviding access to millions of company and individual filings. The system handles\nroughly 3,000 filings per day, serves about 3,000 terabytes of data annually, and\naccommodates an average of 40,000 new filers per year.\nSources of Financial Data | 45\nPublic institutional and governmental data\nA number of national and international financial institutions regularly collect, curate,\nand freely publish financial and economic data. Such institutions are often regulatory\nor multinational agencies. Examples include the following:\nWorld Bank Open Data\nPublished  and maintained by the World Bank, this resource provides a large col‐\nlection of financial and macroeconomic indicator datasets.\nData.europa.eu\nPublished by the European Union, this site provides a central point of access to\nopen data published by European institutions and national data portals. It pro‐\nvides access to 1,536,561 datasets, grouped into 179 catalogs and originating from\n36 countries.\nThe ECB Statistical Data Warehouse\nPublished by the European Central Bank (ECB), this resource provides data on\nall Eurozone economic, financial, and monetary statistics released by the ECB\nand Eurostat in relation to monetary policy.\nFederal Reserve Economic Data (FRED)\nPublished  by the Federal Reserve Bank of St. Louis, this database provides a cen‐\ntral point of of access to hundreds of thousands of economic data time series col‐\nlected from national, international, public, and private sources.\nPublic research data\nResearchers in financial markets may create and make publicly available research-\noriented datasets that can be freely accessed and used for further empirical investiga‐\ntion. For example, Professor Kenneth French has created an online library, the Ken\nFrench Data Library , that provides public and freely accessible financial data on US\nmonthly Fama/French 3 factors and 5 factors, as well as other economic and financial\nvariables. Another example is datasets made public on Kaggle, where researchers and\nmachine learning practitioners publish their models alongside the datasets that they\nused for training.\nFree stock market APIs\nA number of commercial companies, both financial and nonfinancial, offer free\nonline financial data services such as news, prices, quotes, press releases, financial\nreports, and more. Examples include Y ahoo Finance, Google Finance, and Alpha\nVantage. Such services frequently have APIs that enable programmatic access to\nfinancial data (e.g., Y ahoo’s API ).\n46 | Chapter 2: Financial Data Ecosystem",7204
21-Security Exchanges.pdf,21-Security Exchanges,,0
22-Commercial Data Vendors Providers and Distributors.pdf,22-Commercial Data Vendors Providers and Distributors,"While free APIs provide convenient and cost-free access to finan‐\ncial data, I recommend using them with caution. For classroom-\nlevel financial analysis or experimental purposes, it’s perfectly fine\nto use such data. However, for real financial applications and aca‐\ndemic research, the data might lack the necessary quality guaran‐\ntees. Potential quality issues may include errors, lack of adjustment\nfor corporate actions, biases, incompleteness, and lack of proper\nidentifiers. Companies like Y ahoo and Google do not prioritize\nfinancial data services as a key element of their competitive strat‐\negy, so they may not be motivated to offer the highest quality finan‐\ncial data for free.\nSecurity Exchanges\nMarket exchanges like the London Stock Exchange, New Y ork Stock Exchange, and\nChicago Mercantile Exchange gather and manage diverse datasets on the transactions\nthey handle. Being the hub of transaction activities, these exchanges provide detailed\nand up-to-date data crucial for trading and investment purposes. Additionally, they\nmaintain historical archives and reference data, offering valuable insights for analysis.\nIt’s important to note that not all market trading takes place on official exchange ven‐\nues. Some financial products, like bonds and various derivatives, are traded over the\ncounter (OTC). This term refers to trading conducted off-exchange through broker-\ndealer networks. Consequently, statistics on OTC transactions are often unavailable\nor challenging to gather and access.\nCommercial Data Vendors, Providers, and Distributors\nOne of the most valuable and reputable sources of financial data are commercial\ncompanies specializing in financial data and software. These providers collect and\ncurate data from a large number of sources, including regulatory filings, news agen‐\ncies, surveys, company websites, brokers, banks, rating agencies, company reports,\nbilateral agreements with companies, exchange venues, and many more.\nCase Study: Morningstar Data Acquisition Process\nMorningstar, Inc. , is a well-known financial data vendor that collects, analyzes, and\nprovides financial market data, in particular stock and fund data. In one of their 8-K\nfilings , Morningstar experts provided a detailed overview of their data collection and\nacquisition process. Here’s an excerpt from the original filing:\nWe [Morningstar] collect most of our data from original source documents that are\npublicly available, such as regulatory filings and fund company documents. This is\nthe main source of operations data for securities in our open-end, closed-end,\nSources of Financial Data | 47\nexchange-traded fund, and variable annuity databases, as well as for financial state‐\nment data in our equity database. This information is available at no cost.\nFor performance-related information (including total returns, net asset values, divi‐\ndends, and capital gains), we receive daily electronic updates from individual fund\ncompanies, transfer agents, and custodians. We don’t need to pay any fees to obtain\nthis performance data. In some markets we supplement this information with a stan‐\ndard market feed such as Nasdaq for daily net asset values, which we use for quality\nassurance and filling in any gaps in fund-specific performance data. We also receive\nmost of the details on underlying portfolio holdings for mutual funds, closed-end\nfunds, exchange-traded funds, and variable annuities electronically from fund com‐\npanies, custodians, and transfer agents.\n...Our separate account and hedge fund databases require more specialized informa‐\ntion, which we obtain by sending surveys to the management companies. We also\nsurvey for some specialized portfolio and operations data in our other databases to\nenhance our proprietary portfolio statistics. We supplement information gathered\nelectronically or through surveys by licensing a few third-party data feeds for market\nindices and individual securities.\nAs you can see, financial data vendors rely on a variety of data sources in their data\ncollection process. This is done to supplement and enhance their data offers. Clearly,\nfinancial data vendors are continually expanding their offerings and services by\nincluding new and larger datasets derived from new sources.\nCompared to other data sources, commercial datasets offer the following advantages:\n•Providing structured and standardized data that is highly suited for analysis, thus\nreducing the need for extensive data cleaning and preprocessing\n•Enriching financial datasets with additional fields and identifiers for better\nanalysis\n•Providing comprehensive documentation on data usage and field metadata\n•Providing a wide range of data delivery options and formats that can suit various\nbusiness applications\n•Providing customized solutions and packages that fit different business needs\n•Providing customer support\nThere is a large variety of financial data vendors, some of which create their own con‐\ntent (often called data providers); others specialize in aggregating or distributing data,\nwhile others act as both distributors and creators of financial data. Generally speak‐\ning, financial data companies compete along the following axes:\nData coverage\nThe universe of financial instruments, entities, sectors, and variables is quite\nmassive. Some vendors focus on a subset of financial data (e.g., asset classes,\n48 | Chapter 2: Financial Data Ecosystem\ngeographical  areas, and sectors), while others tend to act as global aggregators or\nserve as a one-stop shop, offering a breadth of financial data coverage (such as\nprices, quotes, news, press releases, macroeconomic data, ratings, and volatility\nindicators).\nDelivery mechanisms\nDifferent providers offer their data via one or more delivery mechanisms. These\nmay include SFTP (Simple File Transfer Protocol), cloud, data feed, API, desktop,\nweb access, and others.\nDelivery formats\nProviders can differ in the file formats they use for data delivery. Examples\ninclude CSV (comma-separated values), XML, HTML, JSON, SQL query, and\nZip Archive.\nData history\nSome providers, especially older ones, could have longer historical coverage than\nothers. Additionally, some providers might provide point-in-time snapshots of\ntheir data, while others provide only the latest data releases.\nDelivery frequency\nData can be provided continuously (real time) or with fixed frequency such as\nsecond, minute, day, market closing time, week, or month.\nData standardizations and adjustments\nSome providers might apply data transformations or standardization, while oth‐\ners would deliver the data in its original format. Here are a few examples:\nAdjusted stock prices\nA vendor might deliver adjusted stock prices that take into account corporate\nevents such as stock splits and dividends, while another provider would leave\nthe data unadjusted.\nData aggregation\nSome vendors provide data at the exchange level, while others aggregate the\ndata across exchanges. If you are interested in trading on a specific exchange,\nyou might want to check if a data provider sells data for that exchange.\nStandardization of accounting figures  across countries\nThis might impact analysis if a company uses a specific accounting method\nthat gets lost during standardization.\nData reliability and quality\nAccuracy, quality, and timely access to financial data are crucial to financial insti‐\ntutions. Providers who guarantee high data quality, low error and bias rates, and\nhigh availability are likely to be more trusted.\nSources of Financial Data | 49\nValue-added services\nProviders who enrich the data with extra fields, identifiers, documentation, and\ncustomer support are likely to have a competitive advantage.\nPricing\nMany subscription plans are available, and they vary by vendor. Some offer large\npackages that can be more expensive, while others offer various package sizes\nthat can fit multiple usage patterns and needs.\nTechnical limitations\nVendors might impose certain limits and quotas on their servers, such as the\nmaximum daily requests, the maximum number of instruments that can be quer‐\nied, or the request timeout.\nThe market for financial data is competitive and innovative, and it is continuously\nevolving to accommodate new products, markets, technologies, data types, and deliv‐\nery mechanisms. One of the winning strategies that financial data vendors seem to\ninvest in is the one-stop shop  model, where a financial data vendor provides an inte‐\ngrated platform with access to a wide range of financial data as well as complemen‐\ntary services such as analytics and insights, export options, artificial intelligence tools,\nvisualizations, messaging and chatting, trading, and search capabilities. Another\ncompetitive strategy is the network effect , where the value of a data vendor’s solution\nincreases as more people use it. The more users and traders engage with a specific\ndata vendor platform, the more appealing it becomes for new customers to join.\nA significant share of the financial data market is dominated by a few players. This\nincludes Bloomberg, LSEG, FactSet, S&P Global Market Intelligence, Morningstar,\nSIX Financial Information, Nasdaq Data Link, NYSE, Exchange Data International\n(EDI), Intrinio, and WRDS. There are also smaller players that tend to offer innova‐\ntive products focused on a specific market niche. For example, firms such as Pitch‐\nBook and DealRoom provide private market data on startups, private equity, and\nventure capital.\nNext, I’ll present an introductory overview of some of the most important providers\nin the market.\nBloomberg\nBloomberg is the whale in the financial data market, comprising almost one-third of\nthe total market share. Bloomberg’s flagship product is the Bloomberg Terminal , a\ncomputer system well-known for its black interface that provides users with access to\nreal-time market data, news, quotes, insights, and a number of valuable complemen‐\ntary services. Bloomberg is best suited for buy-side and asset management professio‐\nnals such as traders and portfolio managers.\n50 | Chapter 2: Financial Data Ecosystem\nA valuable feature of Bloomberg is Instant Bloomberg (IB), a messaging service that\nallows users to chat with a large pool of financial professionals who are using the\nBloomberg Terminal. Additionally, users of the Bloomberg Terminal can place trad‐\ning orders via an end-to-end secure trading service (Tradebook). Recently, Bloom‐\nberg introduced BloombergGPT, an AI-powered language model that helps financial\nprofessionals with challenging language tasks such as sentiment analysis, news classi‐\nfication, question answering, and more. Additionally, Bloomberg provides API serv‐\nices that allow developers to access Bloomberg data programmatically using various\nprogramming languages.\nLSEG Eikon\nLSEG Eikon  is Bloomberg’s main competitor, and it holds a significant market share.\nSimilar to Bloomberg, Eikon has a rich collection of financial datasets, a feature-rich\nuser interface, developer APIs, an instant messaging service (LSEG Messenger), and\ntrade execution capabilities. Compared to Bloomberg, Eikon has much cheaper\noptions for more limited offerings.\nFactSet\nFactSet  offers an affordable solution to access real-time financial data, combining\nproprietary, third-party, and user data. Some of FactSet’s advantages include the user-\nfriendly UI, PowerPoint integrations for PitchBooks, personalization options, a large\nvariety of alternative datasets, and portfolio analysis tools.\nS&P Global Market Intelligence\nS&P Global Market Intelligence  is a leading financial data and market intelligence\nservice provider. It provides financial and industry data, analytics, news, and\nresearch. Among the most well-known solutions of S&P GMI is Capital IQ, a web-\nbased platform that provides a rich set of data points on company financials, transac‐\ntions, estimates, private company data, ownership data, and more.\nWharton Research Data Services\nWharton Research Data Services (WRDS)  is the leading platform in financial and\nbusiness research and analysis and is among the most popular data distributors in\nfinance. WRDS offers a globally accessed, user-friendly web interface with more than\n600 datasets from more than 50 data vendors across multiple domains, with a partic‐\nular focus on finance. Through WRDS, users can access multiple data sources, docu‐\nmentation, analytics, and query-building tools simultaneously. WRDS establishes\ndistribution and resale agreements with data vendors and allows their clients to\naccess vendor data directly through their platform. For some datasets, WRDS\nrequires their clients to buy and maintain a separate license for the data.\nSources of Financial Data | 51\nHow Do I Choose a Financial Data Vendor?\nMost financial institutions use services from at least one financial data vendor. How‐\never, finding and choosing the right vendor can be quite challenging. My advice is to\nstart by formalizing your company’s or project’s financial data needs. Then, using the\nvendor differentiating criteria mentioned previously, you can ask yourself some key\nquestions:\nWhat type of data am I looking for?\nFor example, if you are looking for data on Chinese stock prices, you might want\nto find a local data provider specializing in the Chinese market.\nWhich fields  are mandatory, and which are optional?\nFor example, if you want adjusted stock prices, you need to choose a data pro‐\nvider that provides already adjusted data or the necessary fields to perform the\nadjustment.\nWhat is the universe of assets that I want?\nFor example, if you want data on European and American stocks, as well as\nfixed-income data, you might need a global data provider with international cov‐\nerage (e.g., Bloomberg).\nDo I need data quality guarantees?\nSome providers ensure their data is free from errors, biases, and other quality\nissues, while others offer less assurance, necessitating data cleaning.\nWhat is the planned budget?\nFor example, if your company has a limited budget, you might need to look for a\ndata provider that offers small packages or on-demand pricing.\nWhat purpose do I need the data for?\nFor example, buy-side professionals (e.g., traders) often need a wide variety of\ndata, such as prices, news, macroeconomic variables, and market volatility, while\nsell-side professionals (e.g., investment bankers) need more specific data, such as\nasset prices and corporate events.\nWhat data update frequency do I want?\nFor example, if you have a machine learning model that you train weekly, then\nthere is no need for very frequent updates. However, if you are working at an\nalgorithmic trading company, then data would be needed at a very high\nfrequency.\nDo I need specific  delivery mechanisms?\nFor example, if you are developing a web application, you will very likely need to\nfetch data via an API. However, if you have data extraction running in the back‐\nground, receiving files via SFTP or direct website download should work fine.\n52 | Chapter 2: Financial Data Ecosystem",15225
23-Alternative Data.pdf,23-Alternative Data,"Do I need predictable delivery performance?\nSome providers may offer guarantees on latency, throughput, and uptime, which\nare crucial for time-sensitive applications.\nCan I tolerate vendor limitations?\nFor example, if your workload is predictable, you can adapt your application and\nconsumption patterns to the limitations established by the data vendor. However,\nif your data needs are unpredictable or growing, you might want to find a data\nvendor that does not impose strict limitations or negotiate custom quotas.\nDo I need a simple solution?\nFor example, if you want a simple user interface with Microsoft Excel or CSV\nexport capabilities, you may not want to choose a very sophisticated financial\ndata solution.\nSurvey Data\nA survey is a set of questions designed to collect information from a specific group of\npeople on a particular topic. Survey data can enhance existing datasets and provide\ndeeper insights. For instance, banks might ask clients to complete a survey to assess\ntheir financial knowledge and risk appetite level. This information can help assess\nand tailor investment plans to fit each customer’s profile. Similarly, financial data\nvendors may rely on surveys as a mechanism to gather data from companies by send‐\ning them questions related to their operations, performance, and structure.\nSurvey data could serve as the main ingredient in creating other types of data. For\nexample, the Institute for Supply Management (ISM) publishes the Purchasing Man‐\nagers’ Index  (PMI) , a monthly figure used by market agents as an indicator of the\ndirection of the economy. To calculate the PMI, the ISM sends a survey to a list of\ncompanies that make up a representative sample of the entire economy. Once\nrespondents return the filled-in surveys, the index is computed by aggregating and\nweighing the individual responses.\nThe main advantage of survey data is its flexibility, enabling the questioner to design\nthe questions to guarantee valuable answers and organize the information optimally.\nHowever, a few challenges might arise. For example, if the survey is voluntary, some\nrespondents might have no incentive to provide answers, which can bias the final\noutcome. In other cases, some candidates may choose not to complete the survey.\nThis could be due to a reluctance to report poor financial performance, organiza‐\ntional inertia, concerns about reputational risk, competition, or issues related to secu‐\nrity and confidentiality. Additionally, the questioner might (intentionally or\nunintentionally) induce framing bias  in the survey by choosing questions intended to\nforce the respondent to give a desired answer. For this reason, a strong emphasis on\nethical practices and bias checks is highly recommended when conducting surveys.\nSources of Financial Data | 53",2831
24-Structures of Financial Data.pdf,24-Structures of Financial Data,"Alternative Data\nIn recent years, the term “alternative data” has emerged in the financial world to\ndescribe data sources not traditionally used in financial analysis and investment.\nThese nonconventional data sources are not inherently embedded within the finan‐\ncial system, unlike trading venues, payment systems, and commercial banking. The\nsources of alternative data are quite heterogeneous. A few are mentioned here:\n•Satellites (e.g., collecting shipping images)\n•Social media (e.g., tweets)\n•Articles\n•Blog posts (e.g., Medium, Substack)\n•News channels (e.g., Thomson Reuters, Bloomberg)\n•Weather observers\n•Online clicking patterns and browsing history\n•Shipping and pipeline trackers\n•Emails and chats\n•Product and service reviews\n•Security, social, and environmental scores and ratings\nConfidential  and Proprietary Data\nFinancial institutions generate and store large amounts of data when conducting their\ninternal operations. The most essential and valuable type of internal data involves\ncontextual, nontransactional information about the various business entities. This\nincludes, for example, data about clients, employees, suppliers, products, subscrip‐\ntions, offerings, discounts, pricing, financial structures (e.g., cost centers), and loca‐\ntional information (e.g., branches).\nA second type of internal business data is transactions . In performing their daily\nactivities, financial institution clients generate a large amount of transaction data,\nsuch as payments, deposits, trading, purchases, credit card transactions, and money\ntransfers.\nFinally, financial institutions might collect internal analytical data  from research\nteams and analysts conducting market and financial analysis. This includes, for\nexample, sales analysis and forecasting, customer segmentation, credit scoring,\ndefault probabilities, investment strategies, stock predictions, macroeconomic analy‐\nsis, and customer churn probabilities.\nIn specific situations, financial institutions must report various types of confidential\ndata to regulatory bodies. This includes trade details, large transaction information,\n54 | Chapter 2: Financial Data Ecosystem",2187
25-Panel Data.pdf,25-Panel Data,"suspicious activity reports, data gathered during anti-money laundering (AML) and\nknow your customer (KYC) processes, as well as information related to tax, compli‐\nance, liquidity, risk management, and capital adequacy.\nNow that you have an idea about the different sources of financial data, the next sec‐\ntion will discuss the various structures in which financial data can be represented.\nStructures of Financial Data\nFinancial data can be stored and represented using various structures. These may\nrange from simple data structures such as tabular times series, cross-section, or panel\ndata to more complex structures such as matrices, graphs, and text. Throughout this\nsection, we will explore these structures in detail.\nTime Series Data\nA time series is a collection of records for one or more variables associated with a sin‐\ngle entity, observed at specific intervals over time. Time series are indexed in time\norder and can be represented mathematically as follows:\n=X1,X2, . . . . , XN\nwhere N is the length of the time series and X is the observed variable, or:\n=Xt,t∈T\nwhere T is a time index set and X is the observed variable.\nIn tabular format, a time series with S variables observed over N periods can be repre‐\nsented as shown in Table 2-1 . The first column stores the temporal dimension ( TN),\nwhile the other columns ( XS) store the time series values for each variable. A single\ncell ( XSN) records the time series values observed at a specific time for a given\nvariable.\nTable 2-1. Tabular representation of time series\nTime/variable X1X2X3… XS\nT1X11X21X31… XS1\nT2X12X22X32… XS2\n… … … … … …\nTNX1NX2NX3N… XSN\nStructures of Financial Data | 55\nThe time series is the most common data structure in financial markets, mainly due\nto the dynamic and transactional nature of financial activities that happen over time.\nExamples of financial activities that generate time-series data include trading, pricing,\npayment, investment, estimation, valuation, risk measures, volatility figures, corpo‐\nrate events, performance and accounting, and many more.\nExtensive literature on financial time series has been produced where a large number\nof temporal phenomena have been investigated. For an excellent treatment of finan‐\ncial time series analysis, I recommend the seminal book Analysis of Financial Time\nSeries  by Ruey Tsay (Wiley, 2010).\nCross-Sectional Data\nA cross-section is a collection of records for one or more variables observed across\nmultiple entities at a single point in time. In cross-sectional data, the time series\ndimension is irrelevant, and the emphasis is on the variables themselves. Entities in a\ncross-section often share common characteristics, such as firms within the same sec‐\ntor, investments in a specific strategy, fund managers, and more.\nA cross-section dataset can be represented mathematically as follows:\nXt=z,i,j,i= 1, . . . , N,j= 1, . . . . . , S\nwhere t is the time index (fixed), i is the entity index, and j is the variable index.\nIn tabular format, we can draw a cross-section of N entities and S variables as shown\nin Table 2-2 . The first column stores the names of the entities ( EN), while the other\ncolumns store the cross-section value of the S variables ( VS). A table cell ( Xi,j) stores\nthe cross-section value of a given entity for a given variable.\nTable 2-2. Tabular representation of cross-section data\nEntity/variable V1V2V3… VS\nE1X11X21X31… XS1\nE2X12X22X32… XS2\n… … … … … …\nENX1NX2NX3N… XSN\nFinancial cross-section data is generated from the presence of many participants in\nfinancial markets, including financial institutions, brokers, investors, consumers,\ntraders, and managers, as well as a variety of other entities such as financial assets,\ninvestment strategies, and consumer and firm choices and behaviors, among others.\n56 | Chapter 2: Financial Data Ecosystem\nFinancial cross-sectional data can be used to understand and explain significant\npoint-in-time differences between financial entities, such as why different stocks have\ndifferent returns, why firms behave differently, or why performance across fund man‐\nagers varies. Furthermore, cross-sectional analysis is a key tool for identifying corre‐\nlations, causations, or anomalies across different entities, which can be critical for\ninvestment decisions, policy formulation, and understanding market dynamics.\nPanel Data\nPanel data combines both time series and cross-sectional structures, representing\ndata for a set of variables across multiple entities at different points in time. Mathe‐\nmatically, a panel has the following form:\nXi,j,t,i= 1, . . . , N,j= 1, . . . . . , S,t= 1, . . . . . , T\nwhere i is the entity index, j is the variable index, and t is the time index.\nIn tabular format, we can draw a panel of two entities, two time periods, and three\nvariables as shown in Table 2-3 . The first column stores the names of the entities, the\nsecond column stores the time, and the last three columns store the observed panel\nvalues for all three variables. This panel representation is called wide format , as it\nexpands horizontally when new variables are added to the panel. Another option is\nthe long format, where the variable name and value are stored in separate columns,\nexpanding the table vertically with new entries. An example is shown in Table 2-4 .\nTable 2-3. Wide tabular representation of panel data\nEntity Time V1V2V3\nE1t=1 X111X112X113\nE1t=2 X121X122X123\nE2t=1 X211X212X213\nE2t=2 X221X222X223\nTable 2-4. Long tabular representation of panel data\nEntity Time Variable name Variable value\nE1t=1 V1X111\nE1t=2 V2X122\nE2t=1 V1X211\nE2t=2 V2X222\nStructures of Financial Data | 57\nFinancial panel data arises from numerous market entities engaging in various activi‐\nties over time. For instance, stocks are continuously traded and priced, companies\nregularly submit quarterly and annual reports, and individuals conduct daily pay‐\nments and transactions. Hence, most financial datasets are essentially panel datasets.\nPanel data may vary in terms of their cross-section and time series components. For\nexample, annual balance sheets tend to have a large cross-section component (many\nfirms) and a small history. On the other hand, some stock price panel datasets tend to\nhave a long history (e.g., high-frequency trading prices) and a smaller cross-section\ncomponent (asset universe).\nAn important characteristic of panel datasets is their degree of balance . Assume a\npanel has N entities, T time periods, and S variables. The panel is said to be balanced\nif all N × T × S observations are available; otherwise, the panel is called unbalanced .\nTable 2-5  shows a balanced panel where data are available for all entities, time peri‐\nods, and variables. Table 2-6  instead illustrates an unbalanced panel as it has six cells\nwith null values, resulting in half of the observations being missing.\nTable 2-5. Balanced panel\nEntity Time V1V2V3\nE1t=1 X111X112X113\nE1t=2 X121X122X123\nE2t=1 X211X212X213\nE2t=2 X221X222X223\nTable 2-6. Unbalanced panel\nEntity Time V1V2V3\nE1t=1 Null X112Null\nE1t=2 X121X122X123\nE2t=1 X211Null X213\nE2t=2 Null Null Null\nSimilar to time-series data, a large literature, both theoretical and empirical, has been\nproduced for panel data analysis. For an excellent introduction, I highly recommend\nBadi H. Baltagi’s seminal book Econometric Analysis of Panel Data  (Springer, 2021).\n58 | Chapter 2: Financial Data Ecosystem",7527
26-Graph Data.pdf,26-Graph Data,"Matrix Data\nA matrix is a two-dimensional array of elements arranged as rows and columns. Here\nis an example:\n2 5 7\n3 4 6\n7 7 3\nThis is a matrix with three rows and three columns, often referred to as a 3 × 3\nmatrix.\nThe use of matrix structures is quite common in finance. The best example is perhaps\nportfolio optimization theory, in particular Markowitz’s mean-variance optimization\n(MVO). According to MVO, when constructing a financial investment portfolio,\nthree elements should be taken into consideration:\n•The expected return on the assets in the portfolio\n•The variance (risk) of the asset returns\n•The covariances between the asset returns\nFor example, let’s imagine a portfolio with three assets: A, B, and C. Let’s denote the\nexpected return of any asset i with ERi, the variance with varRi, and the cova‐\nriance between two assets i and j with covRi,Rj. Using MVO, two matrix representa‐\ntions would be constructed as follows:\n•3 × 1 portfolio return matrix\nERA\nERB\nERC\n•3 × 3 covariance matrix\nvarRAcovRA,RBcovRA,RC\ncovRB,RAvarRBcovRB,RC\ncovRC,RAcovRC,RBvarRC\nStructures of Financial Data | 59\n2If interested in learning more about portfolio theory, I recommend the excellent book by Jack Clark Francis\nand Dongcheol Kim, Modern Portfolio Theory:  Foundations, Analysis, and New Developments  (Wiley, 2013).\n3For a good introduction to network science, I highly recommend Mark Newman’s book Networks , 2nd ed.\n(Oxford University Press, 2018).Using these matrix representations, MVO relies on matrix algebra to optimize the\nportfolio’s expected return for a given level of risk appetite. Portfolio optimization\nmeans choosing the best asset allocation strategy (how much to invest in each asset)\nto achieve a desired investment goal.2\nGraph Data\nFinancial markets are an outstanding example of a complex system of myriad rela‐\ntionships, transactions, dependencies, and flows. Understanding such complex inter‐\nactions can provide valuable information and insights into market structures,\nsystematic risks, contagion mechanisms, dominant market positions, fraudulent\nbehavior, and market inefficiencies.\nThe kind of analysis that focuses on studying the complex interactions in a system is\ncalled network analysis  or network science.3 To employ network science in finance, tra‐\nditional time-series or cross-section data would lack the depth and granularity\nrequired to build and analyze financial networks. To this end, graph data is required.\nThis type of data is also frequently referred to as connections, networks, or nodes and\nlinks. A graph dataset consists of two sets of data: a set of nodes (aka vertices)\ntogether with node attributes (e.g., name, country, type, and so on), and a set of links\n(aka edges) with link attributes (e.g., type, value, sign, and so on).\nWhen working with graph data, an important challenge concerns the decision about\nhow to structure the data for quick access and analysis. Structuring nodes’ data and\ntheir attributes is straightforward, and a tabular structure will do the job. However,\nthings get more complicated when it comes to structuring link data. To this end, spe‐\ncial graph structures are often used. Generally speaking, a graph dataset can be repre‐\nsented in four main ways:\nNetwork visualization\nA 2-D drawing of the nodes (often as circles) and links (using straight lines). This\nmethod is useful for illustrative purposes and works best with small networks.\nAdjacency matrix\nAn N × N matrix (N being the number of nodes). If there is a link between nodes\ni and j, it stores 1 at positions (i, j) and (j, i).\n60 | Chapter 2: Financial Data Ecosystem\nAdjacency list\nAn array of length N (N being the number of nodes) where each item in the array\ncontains the index of the node and a list of its neighbors represented via a linked\nlist.\nEdge list\nA simple array that stores all edges of a graph. An item of an edge list is a tuple\nwhere the first element is the source node and the second is the target, with\noptional elements that may represent link attributes such as weight, sign, and\ntime.\nFigure 2-1  visually illustrates these four types of graph data representations.\nFigure 2-1. Different  representations of graph data\nFinancial markets rely on a range of graph data types and representations. In the\nupcoming sections, I will discuss six pertinent graph structures: simple, directed,\nweighted, temporal, multipartite, and multiplex.\nStructures of Financial Data | 61\nSimple graphs\nA simple graph consists of a set of homogeneous nodes (nodes of the same type, e.g.,\ncompanies) and a set of homogeneous, unweighted, and undirected links. Figure 2-2\nillustrates an example with two nodes (A, B).\nFigure 2-2. A simple graph with two nodes\nThe adjacency matrix, adjacency list, and edge list representations of simple graphs\nare similar to those shown in Figure 2-1 . In financial markets, simple graphs can rep‐\nresent the presence  of relationships between entities, such as payment agreements\nbetween banks, partnership agreements, and statistical correlations between financial\nassets.\nDirected graphs\nIn a directed graph, links have a direction indicating an orientation  between the\nnodes. Directed graphs are used to represent relationships where nodes at each end of\nthe link play different roles. For example, A borrows from B, where A is the borrower\nand B is the lender. Figure 2-3  illustrates an example with two nodes, A and B, where\nnode A points to node B but not vice versa.\nFigure 2-3. A simple directed graph with two nodes\nThe adjacency matrix of a directed network will have a value of 1 at position (i, j) if\nthere is a link pointing from node i to j, but will have a value of 0 in position (j, i) if j\ndoesn’t have a link pointing toward i. Similarly, a directed graph’s adjacency and edge\nlist will only store records for nodes that point toward other nodes. Figure 2-4  illus‐\ntrates the four representations for a directed graph.\nFigure 2-4. Directed graph representations\n62 | Chapter 2: Financial Data Ecosystem\nDirected graphs can represent a variety of financial activities, such as cross-holding\namong banks, transfers, transactions, and interbank lending.\nWeighted graphs\nIn weighted graphs, links are assigned a numerical value to indicate the relationship’s\nmagnitude or strength. When representing a weighted graph, the weight needs to be\nadded to the adjacency matrix, adjacency list, and edge list as illustrated in Figure 2-5 .\nFigure 2-5. Weighted graph representations\nMost types of graphs can be weighted (including simple and directed graphs). In\nfinance, weights can represent the value of the assets one bank holds at another in a\ncross-holding network, the amount of money transferred in a transaction, or the\nnumber of securities sold in a market trade.\nMultipartite graphs\nA graph representation is called multipartite  when it includes more than one type of\nnode and only allows links between different types of nodes. Graphs with two types of\nnodes are referred to as bipartite , those with three types of nodes as tripartite , and the\ngeneralized case with k types is called k-partite . A k-partite projection , often applied to\nbipartite graphs, is a common operation that builds a graph that contains one type of\nthe k available node types, and edges exist based on whether two nodes share a com‐\nmon link to another type of node in the original network. Figure 2-6  presents a bipar‐\ntite graph along with its projected graph and adjacency matrix.\nFigure 2-6. Bipartite graph representations\nStructures of Financial Data | 63\n4For more on this topic, see Celina M.H. de Figueiredo’s article “The P versus NP—Complete Dichotomy of\nSome Challenging Problems in Graph Theory” , Discrete Applied Mathematics  160, no. 18 (December 2012):\n2681–2693.The leftmost graph in Figure 2-6  has two types of nodes, shaded and white, and links\nexist only between a shaded and a white node. The middle graph illustrates a bipartite\nprojection on one type of node (the shaded nodes in the leftmost graph). The projec‐\nted graph shows that nodes A and C have a link because in the bipartite graph on the\nleft, both A and C share a connection to K. The same goes for C and B, which share a\nconnection to M. The adjacency matrix in Figure 2-6  has a block structure, where\nlinks between the same type of nodes will be absent and represented with zeros (sha‐\nded blocks) whereas links between different types of nodes exist in the white blocks.\nExamples of bipartite relationships in finance include the following:\nInterlocking directorate\nNodes of type “person” (node type 1) act as a member of the board of directors\n(link) of one or more firms (node type 2).\nSyndicated lending\nMultiple lenders (node type 1) jointly provide a loan (link) to one or more bor‐\nrowing entities (node type 2).\nCorporate control hierarchies\nParent firms (node type 1) have ownership rights (link) over child firms (node\ntype 2).\nCorrespondence banking\nA correspondent banking relationship is an arrangement between two banks,\ntypically in different countries, where one bank (the correspondent bank) pro‐\nvides services on behalf of another bank (the respondent bank).\nWorking with multipartite graphs is much easier when nodes are labeled by category,\noften referred to as a colored graph in mathematics. For instance, a bipartite graph\ncan be colored with two colors, where each node category has the same color.\nWithout such labels, identifying a k-partite graph can be challenging, especially when\nk is greater than two.4\n64 | Chapter 2: Financial Data Ecosystem\nTemporal graphs\nIn a temporal graph, links have a temporal dimension indicating the time at which\nthey were active. The network representation of temporal graphs is snapshot based or\ntime based. As illustrated in Figure 2-7 , both the network and adjacency representa‐\ntions are temporal snapshots storing the state of links at each time period. For exam‐\nple, nodes A and B established a link at times t1, t3, and t4, but they didn’t interact at\ntime t2. The adjacency matrix is represented as a multidimensional matrix storing a\nsnapshot for each time period. The edge list stores all links and the time at which they\nwere active.\nFigure 2-7. Temporal graph representations\nMany financial relationships are temporal in nature, such as interbank lending, trad‐\ning, payments, transactions, and many more.\nMultilayer graphs\nA multilayer graph is a complex structure used to represent relationships between dif‐\nferent types of nodes and links. The most common type is the multiplex  graph, where\nnodes are of the same type but have different types of links depending on the interac‐\ntion context. For instance, a financial institution may operate in various markets such\nas commercial banking, wealth management, and financial consulting.\nMultilayer network representations are very useful in finance. They can reveal poten‐\ntial cascading mechanisms that might amplify a local shock, help understand power\nstructures spanning multiple areas, and enhance fraud detection (e.g., when the same\nindividual operates in different systems using the same connection patterns). An\nillustration of multiplex graph representation is provided in Figure 2-8 .\nStructures of Financial Data | 65\nFigure 2-8. Multiplex graph representation\nWhat Are the Sources of Financial Graph Data?\nAn important feature of financial graph data is that it is often derived from other\ntypes of data sources. For example, to build a graph of the interbank loan market , you\nwould need a historical panel of interbank transactions containing information on\nborrowers, lenders, and the transaction amount and time. Similarly, say you want to\nbuild a bipartite network of interlocking directorates  to see which directors sit on\nmultiple boards, then you would need a dataset (e.g., the LSEG Officers and Directors\ndataset) that contains information on executive individuals and the companies they\nare associated with. In some cases, more work is required. For example, if you want to\nbuild a network of similarities between stocks , you would first need a historical panel\nof price data (e.g., CRSP), and then you need to compute the pair-wise similarity\n(correlation) between the stocks in order to get the links needed to build the graph.\n66 | Chapter 2: Financial Data Ecosystem",12495
27-Types of Financial Data.pdf,27-Types of Financial Data,"5For a good introduction to the different sources of financial text data, I recommend Mirjana Pejić Bach, Živko\nKrstić, Sanja Seljan, and Lejla Turulja’s article, “Text Mining for Big Data Analysis in Financial Sector: A Liter‐\nature Review” , Sustainability  11, no. 5 (January 2019): 1277.\n6More on this topic in Marjan Van de Kauter, Diane Breesch, and Véronique Hoste’s “Fine-Grained Analysis of\nExplicit and Implicit Sentiment in Financial News Articles” , Expert Systems with Applications  42, no. 11 (July\n2015): 4999–5010.It is possible in some cases to find connection datasets curated by financial data ven‐\ndors. For example, S&P Global provides a number of datasets about business and\ncompany relationships , such as ownership, supply chain, investment, and many more\ntypes of relationships.\nText Data\nText data refers to information available in any unstructured or semi-structured for‐\nmat. Its extensive use and versatility make it arguably the most available type of data\nin financial markets . With the advent of generative AI and large language models\n(LLMs), the importance of text data has significantly increased. Generative models\nsuch as GPT-4 and BloombergGPT are trained on massive amounts of text data. For\nspecialized financial tasks such as fraud detection, sentiment analysis, know your cus‐\ntomer (KYC), and anti-money laundering (AML), finance domain-specific text data\nis essential for customizing and fine-tuning an LLM.\nIt can be any text-based medium, including news, analyst reports, company filings,\nprospectus documents, emails, social media posts, system logs, patent documents,\nlegal and technical documents, research papers, web pages, blog posts, and much\nmore.5\nText data can take various forms, including plain text consisting of words and senten‐\nces, semi-structured documents with keys and values, tabular formats, or complex\nspecialized data structures like word embeddings .\nFinancial text data exhibits certain characteristics that set it apart from other forms of\ntextual data. Terms such as “liability, ” “risk, ” “default, ” and “exposure” may carry nega‐\ntive connotations in certain contexts, yet they are commonly used and frequent ter‐\nminology in financial markets. Additionally, financial documents can contain both\nexplicit content (such as a firm merging with another) and implicit content (factual\ninformation implying a positive or negative emotion).6\nNow that you’re familiar with financial data’s main sources and structures, the follow‐\ning section will explore the various types of financial data employed for analytical and\noperational purposes.\nStructures of Financial Data | 67",2681
28-Fundamental Data.pdf,28-Fundamental Data,"Types of Financial Data\nThe financial data structures we discussed in the previous section can represent a\nvariety of financial variables, content, and phenomena. This section will categorize\nthese financial phenomena by discussing the most crucial types of financial data in\ndetail, illustrating their features and challenges.\nFundamental Data\nFinancial fundamental data, also called financial statements data, balance sheet data,\nor corporate finance data, conveys information about firms’ structures, operations,\nand performance. Firms themselves generate and produce fundamental data, partly\nbecause it’s a regulatory requirement and partly because it helps the management\nunderstand the financial and operational situation of the firm. Many companies have\naccounting departments liable for generating and maintaining financial statements,\nbut it is also possible to do so via accounting service providers such as consulting and\naudit firms. In general, there are three main financial statements:\nBalance sheet\nProvides figures on what a firm owns (assets), what it owes (liabilities), and what\nits shareholders own ( shareholders’ equity )\nIncome statement\nProvides figures on a firm’s financial performance over a specific period, such as\nthe annual revenues and net profit\nCash flow statement\nProvides information on a firm’s cash movements (in and out), which can help\ndetermine whether the firm is generating enough cash to carry out its operations\nEach financial statement contains a large number of items representing different\nquantities. A special type of financial statement item is ratios,  which represent rela‐\ntionships between two or more items. For example, a popular ratio is Return on\nEquity  (ROE), which is calculated by dividing the company’s net income by the value\nof its shareholders’ equity. A high ROE is an indicator that the company is efficient at\ngenerating net income using shareholders’ equity.\nFinancial fundamental data statements have a number of characteristics that need to\nbe taken into account. First, due to the time it takes to prepare them, fundamental\ndata statements are released with low frequency,  such as quarterly, each semester, or\nyearly. Second, fundamental data reports are often published with a time lapse , mean‐\ning that data for a specific period is released at a future date. Data can be reported in\nthe form of reinstatements , which happens when a figure gets revised and corrected\nafter it has been released. Data backfilling  can also take place, which happens when a\n68 | Chapter 2: Financial Data Ecosystem\nfirm and its entire fundamental data history get added to a dataset that has never had\ninformation on that firm before.\nIf not handled properly, these characteristics could lead to non-point-in-time  (PIT)\nfundamental data . PIT data is data that is recorded with a timestamp reflecting the\nfiling or release data. With PIT data, one can ask the questions “When was the data\nreleased?” and “What data was known at that time?” On the other hand, non-PIT\ndata is stamped with the fiscal year-end date or the latest update date. Non-PIT data\nreflects the latest data release, and it gets overwritten when an update or reinstate‐\nment happens. Table 2-7  illustrates the difference. With three versions of the data\navailable, a PIT dataset would keep track of all historical snapshots (101, 110, 120),\nwhile a non-PIT dataset would show only the latest version (120).\nTable 2-7. PIT versus non-PIT data\nData type Preliminary result Fiscal year-end release Correction\nVersion 101 110 120\nPIT 101 110 120\nNon-PIT 120 120 120\nNon-PIT data poses several challenges. First is the problem of look-ahead bias , which\nhappens when conducting historical studies and assuming that data was known at a\nspecific historical moment, which, due to a release time lapse, may not be the case.\nSecond is reproducibility , which becomes challenging if non-PIT data used for past\nresearch is updated and may not yield the same results. The third challenge is trans‐\nparency , which may be an issue if, for example, a company updates an accounting\nrecord to hide past fraudulent behavior.\nReading Financial Highlights from a Bank Annual Report\nBeing able to understand the main items in an annual report is a good skill, even for\nfinancial data engineers. The table below illustrates the annual financial highlights of\nan imaginary bank, ADK. If you check the annual report of any real bank, you will\nfind a similar table. Examining the figures for 2020, we can see that the bank realized\nmore total net revenues  compared to 2021 ($bln 102 versus 99) but with a lower net\nincome  ($bln 23 versus 25). This translates into lower returns for shareholders, which\nwe can see from the return  on common equity  figure of 11% compared to 16% in 2019.\nADK is quite a large bank, which we can see from the total assets  figure that equals\n$2,987,245 million (almost $3 trillion) and the headcount  of 150,432. Moreover, ADK\nseems to be financially stable, which we can infer from the Tier 1 capital ratio  of 15%,\na measure introduced by the Basel III accord, which tells how stable a financial insti‐\ntution is by comparing its core equity against its risk-weighted assets. Basel III estab‐\nlished that the Tier 1 capital ratio must be at least 6%. ADK probably has a large\nmarket share, which we can see from its considerable market capitalization of\nTypes of Financial Data | 69",5500
29-Market Data.pdf,29-Market Data,"$240,876 million (roughly $240 billion). A dividend was distributed every year to\nshareholders, which we know from the cash dividends per share  figure.\nItem 2020 2019 2018\nTotal net revenue ($ mln) 102,000 99,340 96,345\nTotal noninterest expense ($ mln) 55,000 49,000 45,865\nNet income ($ mln) 23,100 25,987 19,628\nCash dividends per share ($) 3.5 2 1.7\nReturn on common equity 11% 16% 8%\nTier 1 capital ratio 15.0% 14.1% 14%\nTotal capital ratio 18.9% 17% 17%\nLoans ($ mln) 986,865 956,586 916,865\nDeposits ($ mln) 1,298,456 1,187,456 1,062,456\nTotal assets ($ mln) 2,987,245 2,445,853 2,200,119\nTotal stockholders’ equity ($ mln) 72,876 71,664 70,345\nMarket capitalization ($ mln) 250,876 300,897 280,986\nHeadcount 150,432 144,983 139,709\nMarket Data\nMarket data includes price and related information for financial instruments traded\non market venues such as the New Y ork Stock Exchange (NYSE) or in off-exchange\nOTC markets. It is extensively available for many types of instruments, such as stocks,\nderivatives, indexes, currencies, bonds, and funds. Moreover, market data is released\nfrequently, often becoming available just seconds after its generation.\nA financial instrument’s market data may include fields such as the following:\n•Identifiers such as tickers, e.g., IBM\n•Trading venue code such as the exchange code, e.g., NYSE\n•Last day (adjusted) closing price, e.g., $40.30\n•Current day open price, e.g., $40.15\n•Current day highest price, e.g., $42.30\n•Current day lowest price, e.g., $39.42\n•Current day price range, e.g., $39–42.30\n•Current day volume, e.g., 50,000\n70 | Chapter 2: Financial Data Ecosystem\n7To see a real example, check out the information provided by Y ahoo Finance for Google stock .\n8For a more comprehensive list, check CME Group’s list of match algorithms .\n9For a deeper explanation of the order book mechanism, I recommend Jean-Philippe Bouchaud, Julius Bonart,\nJonathan Donier, and Martin Gould’s book, Trades, Quotes and Prices: Financial Markets Under the Microscope\n(Cambridge University Press, 2018).\n•Latest bid/ask prices and quantities, e.g., bid of $40.02 × 2200 and ask of $40.03 ×\n3000\n•Last trade/quote date, e.g., today at 14:12:10 p.m.\nIn financial markets, the term bid is used to indicate the price at\nwhich buyers are willing to buy a financial asset, while the ask is\nthe price at which sellers are willing to sell an asset.\nSome stock exchanges and media platforms enrich market data with fundamentals\nsuch as market capitalization, revenues, and net profit, as well as various financial\nratios and corporate event dates, such as the latest dividend date.7\nAt the core of market data generation lies the order book , an electronic ledger of buy\nand sell orders for a specific security at a given trading venue. Order books have two\nrepresentations: Market by Order  (MBO) and Market by Price  (MBP). MBO shows\neach individual order separately, detailing the price, quantity, and time of entry for\neach buy and sell order. In contrast, MBP aggregates orders by price level, consolidat‐\ning them into price buckets to display the total quantity of orders at each price level\ninstead of showing each order individually.\nOrders in an order book can be matched through an Order Matching System  (OMS),\nwhich uses algorithms to ensure the best execution. The two most common match\nalgorithms are price/time priority (also known as First In, First Out, or FIFO) and\npro rata.8 Price/time priority matches orders based on price first, and then by the\ntime of entry at the same price level; for example, if two buy orders are placed at the\nsame price, the earlier one gets matched first. Pro rata matching, on the other hand,\nmatches orders at the same price level proportionally based on their sizes; for\ninstance, if there are two buy orders at the same price, one for 100 shares and another\nfor 200 shares, and a sell order for 150 shares arrives, the matching will be split pro‐\nportionally, giving 50 shares to the first order and 100 shares to the second.9\nAn example of an order book is illustrated in Table 2-8 . The example shows that the\nhighest bid price level is $55.11, while the lowest ask price level is $55.13. The highest \nbid and lowest ask are called the top of the book , while the difference between them is\ncalled the bid/ask spread  (=$0.02). The order book depth  is a measure of the number of\nTypes of Financial Data | 71\n10For an excellent reference on this topic, I recommend Aswath Damodaran’s Investment Valuation: Tools and\nTechniques for Determining the Value of Any Asset , 3rd ed. (Wiley, 2012).\n11To calculate the NBBO, stock exchanges report their best bid/ask prices to a system called the Securities Infor‐\nmation Processor (SIP), which aggregates all the quotes in a single NBBO and releases it to the market.distinct price levels available in an order book. Another term, the market depth,  is\noften used as an indicator of liquidity and is defined as the size of market orders that\ncan be executed without causing a large impact on the price level.\nTable 2-8. Example of an order book\nBuy-side Sell-side\nBid volume Bid price Ask price Ask volume\n220 $ 55.11 $ 55.13 1000\n1000 $ 55.11 $ 55.14 500\n50 $ 55.9 $ 55.66 200\n20 $ 55.55 $ 55.66 10\n560 $ 55.1 $ 66.68 50\n… … … …\nDifferent types of orders can be submitted to an order book. For example, a market\norder  is used to buy or sell at the best bid/ask price available. A limit order  is an order\nto sell at a specified minimum ask price or buy at a maximum bid price. A stop-loss\norder  is triggered and submitted to buy or sell if the price level reaches a predefined\nmaximum or minimum value. A trailing stop order  is triggered if the market price\nmoves up or down by a specific percentage or dollar amount.\nMarket data exhibits several features that are worth considering. First, market data,\nsuch as prices, are computed quantities. When an investment firm submits an order\nto buy or sell a financial asset, the order price is often calculated using a specific valu‐\nation method. For example, company stock prices can be calculated using methods\nsuch as the net present value, earning multiples, the dividend discount model, or the\ndiscounted cash flow model.10\nImportantly, when different market participants submit orders with varying prices\nacross different markets, determining the final market price for investors becomes\ncrucial. Ideally, the bid price is the highest among all bid prices, and the ask price is\nthe lowest among ask prices. To enforce this rule, regulators such as the SEC passed\nthe Regulation National Market System (NMS) regulation , which introduced the\nNational Best Bid and Offer  (NBBO). With the NBBO in action, brokers are required\nto ensure that their client orders are executed against the best bid and ask prices,\nwhich need to be at least equal to the NBBO.11\n72 | Chapter 2: Financial Data Ecosystem",6987
30-Transaction Data.pdf,30-Transaction Data,"12Readers interested in a comprehensive coverage of the financial transaction lifecycle can read the excellent\nbook by Robert P . Baker, The Trade Lifecycle: Behind the Scenes of the Trading Process  (Wiley, 2015).Nevertheless, not all financial markets ensure an NBBO. This is particularly true in\nOTC markets like foreign exchange (Forex) markets where currencies are traded.\nThere is no enforced exchange rate for a currency pair in the Forex market. The\nexchange rate you will get is specific to the Forex quote provider, which can be your\nbank or, most commonly, a Forex broker . This means that the quote you see on Goo‐\ngle for EUR/USD is not the market price, and you should not expect it to match the\none you get offered when you convert money from one currency to another.\nPrice discreteness  is another important attribute of market data. It means that the\nprice of a listed stock cannot change by less than a specific amount known as tick size.\nFor example, according to SEC Rule 612 on the minimum pricing increment :\nThe Rule prohibits market participants from displaying, ranking, or accepting quota‐\ntions, orders, or indications of interest in any NMS stock priced in an increment\nsmaller than $0.01 if the quotation, order, or indication of interest is priced equal to or\ngreater than $1.00 per share. If the quotation, order, or indication of interest is priced\nless than $1.00 per share, the minimum pricing increment is $0.0001.\nFinally, a quite important feature of market data, and in particular price data, is its\nnonsynchronous history . This means that given two securities, A and B, security A\ncould trade at least once every minute, while security B trades only once every five\nhours. Such discrepancies in market behavior are often due to market liquidity vary‐\ning from one security to another.\nTransaction Data\nA financial transaction is a legal agreement between two or more parties to exchange\nfinancial securities, services, goods, risks, or commodities in return for money.\nFinancial markets generate massive volumes of transaction data triggered by activities\nsuch as investments, payments, trading, hedging, speculation, and lending.\nMost financial transactions are conducted either fully or partially electronically. Gen‐\nerally speaking, a financial transaction can be characterized by at least five elements:\ntransaction specifications, transaction parties, initiation date, settlement date, and\nsettlement method.12 Next, we discuss each of these five elements in detail.\nTransaction specifications\nBefore initiating a transaction, parties agree on its details and specifications. In secur‐\nities exchange, this phase is often called pre-trade. To provide examples, a stock pur‐\nchase transaction would specify the name and identifiers of the shares being\nexchanged, the quantity of each share, the price, and the currency. For financial\nTypes of Financial Data | 73\ninstruments such as options on stocks, additional details would be needed, such as\noption type (call versus put), expiry date, and exercise price. For cross-border pay‐\nments, parties may need to agree on the amount, currency pair, Forex conversion\nrate, and any spread margin.\nInitiation date\nThe initiation date of a transaction is the date at which parties enter into an agree‐\nment to execute a transaction.\nSettlement date\nThe settlement date is when a transaction is finalized, and the exchange of money and\ntransfer of ownership takes place. In securities exchange, this phase is called post-\ntrade. Importantly, in many financial transactions, the settlement date falls on a\nfuture date, following the initiation date. Such a delay can be due to various reasons\nsuch as the transaction verification process, technological constraints, errors due to\nmissing information, bureaucratic steps such as the issuance of a certificate of owner‐\nship (e.g., stock certificate), lengthy payment process, and any manual intervention\nsteps that may be necessary (e.g., review).\nFinancial market participants have been investing in technologies to shorten the\ntransaction settlement time. The term straight-through processing  (STP) is often used\nto designate the ideal transaction processing system that does not involve manual\nintervention, leading to decreased processing time, reduced operational risks, and\nlowered costs. Additionally, financial markets have adopted common conventions for\nsettlement periods. For example, the current convention in securities transactions is T\n+2 (trade date plus two days),  which indicates that a security transaction should be\nsettled two business days after the transaction initiation date. Recently, the term\nsame-day affirmation  (SDA) or T0 has received attention from markets and regula‐\ntors as it promotes the idea of completing the transaction verification process on the\nsame day the transaction took place. Reducing the time of financial transaction lifecy‐\ncles via STP and SDA can be quite challenging, as it requires significant investments\nby multiple market participants in a data infrastructure that provides real-time trans‐\naction data, while at the same time meeting the requirements of security and reliabil‐\nity (luckily, this is the topic of this book!).\nSettlement method\nFinancial market transactions can be settled differently based on the transaction type.\nIn securities markets, the most common settlement method is Delivery versus Pay‐\nment  (DVP), which guarantees that the cash payment for securities is made either\nbefore or at the same time as the delivery of the securities. In a Delivery versus Free\n(DVF) mechanism, the delivery of securities occurs for free, for example, when deliv‐\nering the collateral to a securities loan.\n74 | Chapter 2: Financial Data Ecosystem\nIn payments, multiple settlement mechanisms exist. A common example is Payment\nversus Payment  (PvP), widely used in Forex transactions. In a PvP system, the pay‐\nment of one currency occurs only if the payment of the corresponding currency takes\nplace. Another prominent mechanism is Real-Time Gross Settlement  (RTGS), where\nfunds are transferred in real time on a gross basis, meaning transactions are pro‐\ncessed continuously throughout the day and settled individually. Conversely, in a\nDeferred Net Settlement  (DNS) mechanism, transactions are accumulated over a\nperiod (e.g., end of the day) and then settled in batches at specific intervals. In DNS,\nonly the net difference between debits and credits is settled. For example, if Citibank\nis to pay JPMorgan Chase $1.5 million, and JPMorgan Chase is to pay Citibank $1\nmillion, a DNS system would aggregate this to a single payment of $500,000 from Cit‐\nibank to JPMorgan Chase. Conversely, an RTGS system would require two separate\npayments for the full amounts ($1.5 million to JPMorgan Chase and $1 million to\nCitibank).\nTransaction parties\nFinancial market transactions are often conducted by two parties, the buyer and the\nseller. However, transactions are not risk free. The term settlement risk  refers to the\nrisk that one side of a transaction does not honor their contractual obligation, e.g.,\nfailure to pay the amount due, failure to pay on time, or failure to transfer the asset\nownership. To mitigate such risk, a third party is often involved in financial transac‐\ntions to act as a guarantor. Examples include the following:\nClearing house\nClearing houses act as intermediaries between sellers and buyers. They facilitate\nand guarantee a successful settlement of transactions by acting as buyers for the\nseller and sellers for the buyer. Examples of clearing houses include exchange\nclearing house divisions such as Nasdaq Clearing and CME Clearing, as well as\ncredit card clearing houses such as Visa and Mastercard.\nCentral Securities Depository (CSD)\nA CSD is a specialized financial institution that holds financial securities such as\nshares and derivatives on behalf of other institutions. Similar to a clearing house,\nCSDs act as a third party to financial transactions to guarantee successful settle‐\nment and transfer of security ownership against the payment of money. A promi‐\nnent example of a CSD is Clearstream.\nCustodian\nA custodian or custodian bank is a financial institution that offers securities and\npost-trade services to institutional investors, asset managers, banks, and other\nfinancial institutions. Custodians provide a range of services, such as holding and\nsafekeeping their client’s securities, conducting transactions on their client’s\nbehalf, and providing clearing and settlement services.\nTypes of Financial Data | 75",8686
31-Analytics Data.pdf,31-Analytics Data,,0
32-Reference Data.pdf,32-Reference Data,"Payment systems\nPayment systems facilitate the secure transfer of funds between financial institu‐\ntions while effectively managing settlement risks. The most prominent example is\nRTGS systems such as the Federal Reserve’s Fedwire in the United States, the\nBank of England’s CHAPS in the UK, and the European Central Bank’s T2 (part\nof TARGET2 services). RTGSs are typically preferred for fast transactions and\nreduced settlement risk. Other services allow for deferred and netted payments,\nsuch as the US Clearing House Interbank Payments System (CHIPS). These sys‐\ntems are typically used for less-time-critical payments, which makes them less\nexpensive than RTGSs.\nAnalytics Data\nA valuable type of financial data is analytics data, which is often derived from other\ntypes of data such as fundamental, market, and transaction data. This data is typically\ncomputed using simple formulas, statistical models, machine learning techniques,\nand financial theories. Examples include news and market sentiment analytics (nov‐\nelty, score, relevance, impact), financial risk measures (e.g., Value at Risk, option\nGreeks, bond duration, implied volatility), market indexes (e.g., MSCI Global\nIndexes), ESG (environmental, social, and governance) scores, stock analysis, com‐\npany valuation, estimates (e.g., Institutional Brokers’ Estimate System estimates), and\ncompetition analysis.\nThe main advantage of analytics data is that it offers pre-calculated patterns and sig‐\nnals, ready for immediate use in decision-making. Nevertheless, the calculation meth‐\nodology used by the analytics data provider might be proprietary, which makes it a\nmystery box. Furthermore, as the same data becomes accessible to all market partici‐\npants, maintaining a sustainable competitive advantage becomes increasingly difficult\n(sooner or later, someone will get the same data and replicate your strategy).\nAlternative Data\nAlternative data refers to a variety of nonconventional data that can be used for finan‐\ncial analysis, investment, or forecasting. It is regarded as nonconventional since it\ndoes not primarily originate within the traditional context of financial markets, such\nas trading, prices, transactions, and corporate finance operations. Examples include\nnews, social media posts, satellite images, patents, Google trends, consumer behavior,\ntechnology analysis, political analysis, environmental and reliability scores, and many\nmore. A large number of alternative datasets are available through data vendors, but\nit is also possible to extract similar data from the internet and other public\nrepositories.\n76 | Chapter 2: Financial Data Ecosystem\n13For more on this topic, see Ashby Monk, Marcel Prins, and Dane Rook’s article, “Rethinking Alternative Data\nin Institutional Investment” , Journal of Financial Data Science  1, no. 1 (Winter 2019): 14–31.The main advantage of alternative data lies in its novelty and diversity. A financial\ninstitution equipped with the expertise and resources to extract and clean alternative\ndata sources stands to gain a competitive informational advantage, which can signifi‐\ncantly improve portfolio analysis and returns. For example, one study  used satellite\nimage data to show that the number of cars in the parking lots of a sample of public\nUS companies may be used as a predictor of financial performance.\nA variety of challenges might be encountered when working with alternative data.\nThe main issue is that these datasets are often available in an unstructured and raw\nformat, necessitating substantial investment to structure the data and extract valuable\ninsights. Another challenge is the lack of standardized entity identifiers and refer‐\nences, unlike conventional financial datasets. Moreover, alternative data can be imbal‐\nanced or incomplete because its observations and events are not consistently\ncaptured, unlike the systematic data collection for traditional datasets such as publicly\ntraded stocks. Finally, alternative datasets can easily be biased yet provide no addi‐\ntional information to detect such biases.13\nReference Data\nFinancial reference data is a type of metadata used to identify, classify, and describe\nfinancial instruments, products, and entities. It is crucial for various financial opera‐\ntions, including transactions, trading, clearing and settlement, regulatory reporting\nand compliance, data consolidation, and investment.\nTo understand what reference data is, it’s helpful first to review the concept of a finan‐\ncial instrument. Simply put, a financial instrument is a contract between two parties\nthat has monetary value and can be traded on financial markets. Examples include\ndebt instruments such as bonds and loans, equity instruments such as stocks, deriva‐\ntive instruments such as options and futures, and foreign exchange instruments such\nas currency pairs and currency futures. Importantly, each financial instrument comes\nwith its own set of terms and conditions that define its contractual specifications. In\nthis context, reference data is typically used as a metadata resource to describe these\nterms and conditions. Table 2-9  presents a few examples of reference data for several\nfinancial assets.\nTypes of Financial Data | 77\nTable 2-9. Reference data for different  asset classes\nAsset category Reference data fields\nFixed income\n(bonds)•Issuer information:  name, country, sector\n•Security identifiers:  ISIN; CUSIP, etc.\n•Instrument information:  issue date, maturity date, coupon rate and frequency, currency, current price,\nyield to maturity, accrued interest\n•Bond features:  callable, putable, convertible, payment schedule, settlement terms\n•Credit information:  credit rating, credit spread\nStocks •Basic information:  identifiers  such as ticker or ISIN, company name, exchange, sector, industry\n•Price and volume information:  current, open, close, high, low prices, volume, average volume\n•Dividend information:  dividend yield, dividend per share, dividend payment date, ex-dividend date\n•Corporate actions:  stock splits, dividend distribution, buybacks, and mergers and acquisitions\n•Fundamental information:  earnings per share, price-to-earnings ratio, book value per share\nFunds •Fund structure:  umbrella fund, subfunds, share classes\n•Identification  information:  fund/subfund/share class name, type, identifiers  such as ISIN, inception\ndate, currency, domiciliation\n•Fund management:  fund manager, management company, custodian, distributor, investment\nstrategy, and fees such as management and advisory fees\n•Performance:  net asset value (NAV), NAV calculation frequency, historical returns, benchmark index\n•Holdings and allocation:  top holdings, sector, geographical, and asset allocation\n•Risk profile:  volatility, Sharpe ratio, beta, alpha, hedging policy\n•Investment, distribution, and dividends : distribution frequency, distribution restrictions, dividend\nyield, subscription and redemption terms, cutoff  times, minimum investment\nDerivatives •Basic information:  instrument type, underlying asset, identifiers,  classification\n•Contract specifications:  contract size, expiration date, settlement date, exercise style, strike price\n•Pricing information:  premium, mark-to-market price, bid/ask price\n•Underlying asset information:  underlying asset price, volatility, dividend yield\n•Risk metrics:  delta, gamma, theta, implied volatility, etc.\n•Trading information:  open interest, volume\n•Counterparty information:  name, credit rating, collateral requirements\n•Corporate action adjustments:  stock splits on the underlying asset\nManaging reference data is one of the most outstanding challenges in financial mar‐\nkets. In some cases, reference data is simple. For example, a stock is a standard finan‐\ncial instrument widely understood to represent an ownership stake in a firm. Stock\nreference data includes identifiers such as its ticker, the dividend rate if it pays divi‐\ndends, and the legal issuing entity. Nevertheless, exotic and more complex financial\ninstruments have been introduced with the evolution of financial markets. This is\nparticularly the case with financial derivative instruments. For example, identifying\nan option contract requires information about the underlying asset, strike price, expi‐\nration date, option type (e.g., call or put), style (American versus European), and pos‐\nsibly other factors such as dividend yields or implied volatility. Furthermore,\nderivative instruments come in great variety, each with unique characteristics and\n78 | Chapter 2: Financial Data Ecosystem\n14For example, consider a stock option with a strike price of $100 and 100 shares underlying it. If a two-for-one\nstock split event takes place, managing reference data involves adjusting the option specifications to reflect\n200 shares with a new strike price of $50, ensuring all related systems and records are updated accurately.\n15A more detailed discussion of this problem can be found in Allan D. Grody, Fotios Harmantzis, and Gregory\nJ. Kaple’s article, “Operational Risk and Reference Data: Exploring Costs, Capital Requirements and Risk Miti‐\ngation” , Journal of Operational Risk  1, no. 3 (2006).terms. Such characteristics might even change with time, either through contractual\nadjustments or market events, or be customized to meet clients’ specific needs.14\nMany financial transactions and settlements fail due to operational errors, often\nlinked to poor reference data. These errors can stem from inaccuracies in settlement\ninstructions, trade-specific details, client and counterparty information, instrument\ndata, or corporate actions data. For example, incorrect client details, mismatched\naccount information, or erroneous security identifiers can lead to misrouted pay‐\nments and failed trades. Consequently, maintaining accurate and up-to-date reference\ndata is crucial for smooth transaction processing and minimizing operational risks.15\nThis has led to the development of proprietary or firm-specific reference data\ndescriptions. As a result, market participants and regulators have struggled to agree\non standard terms, definitions, and formats for financial instruments, which is the\nprimary challenge in reference data management.\nAnother significant challenge with reference data is the absence of a unified identifi‐\ncation system for financial instruments. Various financial identifiers have been cre‐\nated, each serving different purposes and possessing unique features. Market\nparticipants’ use of different identifiers makes it increasingly difficult to match finan‐\ncial instruments across different identification systems. This issue is explored in\ngreater detail in Chapter 3 .\nSeveral initiatives have been launched to address the challenges of reference data in\nfinancial markets. The International Organization for Standardization established a\ndedicated committee , ISO/TC 68/SC 8, whose scope is “standardization in the field of\nreference data for financial services. ” In the United States, the Dodd-Frank Wall Street\nReform and Consumer Protection Act of 2010 mandated that the Office  of Financial\nResearch  (OFR) prepare and publish a financial instrument reference database . In the\nEuropean Union, the Financial Instruments Reference Data System  (FIRDS) was estab‐\nlished to collect and publish reference data on financial instruments . This system\noperates under Article 27 of Regulation (EU) No 600/2014 (MiFIR) and Article 4 of\nRegulation (EU) No 596/2014 (MAR). Managed by the European Securities and Mar‐\nkets Authority (ESMA), FIRDS ensures the availability and transparency of financial\ninstrument data, aiding in regulatory compliance and market supervision.\nTypes of Financial Data | 79",11842
33-Entity Data.pdf,33-Entity Data,,0
34-Center for Research in Security Prices.pdf,34-Center for Research in Security Prices,"In the financial data market, several commercial reference data solutions stand out.\nNotable examples include Bloomberg’s reference data  and LSEG’s reference data\nofferings. Reference data could be offered in a form that matches the requirements set\nby specific regulatory regimes, such as MiFID II, Europe’s Securities Financing Trans‐\nactions Regulation (SFTR), and the Basel Committee’s Fundamental Review of the\nTrading Book (FRTB). Additionally, some reference data services focus on financial\nentities, such as SwiftRef , which provides comprehensive international payment ref‐\nerence data. SwiftRef offers detailed information on BICs (Business Indentifier\nCodes), IBAN (International Bank Account Number) validation, and various other\nidentifiers crucial for identifying entities involved in global payments.\nIn Chapter 3 , we will explore the most essential types of reference data, focusing\nspecifically on financial security identifiers.\nEntity Data\nEntity data includes information about corporate entities and their characteristics. It\nis frequently used alongside reference data to offer more detailed insights into a spe‐\ncific entity. Examples of entity data elements include the company name, identifiers,\nyear of establishment, legal corporate form, ownership structure, sector classification,\nassociated individuals, credit rating, ESG score, risk exposures, major corporate\nevents, and more.\nA number of commercial entity datasets are available, such as LSEG’s legal entity data\nand services , Moody’s Orbis database , and SwiftRef ’s entity data . These datasets are\nused for many useful purposes such as corporate finance analysis, risk management\n(e.g., supplier or credit risk), and compliance and financial crime prevention through\nanti-money laundering (AML), know your customer (KYC), and client due diligence\n(CDD).\nBenchmark Financial Datasets\nAs you have seen from our discussion thus far, financial data is abundantly available\nfrom multiple sources and in a variety of types and structures. To extract analytical\nand economic value from this data, commercial vendors, financial institutions, and\nresearchers create financial  datasets . I define a financial dataset as a bundled collec‐\ntion of variables and data points that provide information on a specific financial\nentity or topic, such as loans, stock prices, index prices, bond markets, and derivative\nmarkets. A financial dataset can include a mix of data types, such as fundamentals,\nmarket data, transactions, analytics, reference data, and entity data.\n80 | Chapter 2: Financial Data Ecosystem",2621
35-Compustat Financials.pdf,35-Compustat Financials,,0
36-Trade and Quote Database.pdf,36-Trade and Quote Database,,0
37-IvyDB OptionMetrics.pdf,37-IvyDB OptionMetrics,"A considerable number of financial datasets already exist in the market, and new\ndatasets are continuously being created and published. Next, I’ll provide an overview\nof some of the world’s most trusted and used financial datasets.\nCenter for Research in Security Prices\nFinancial datasets provided by the Center for Research in Security Prices (CRSP) are\namong the most prominent and trusted sources of market data. In particular, the\nCRSP US Stock dataset  provides comprehensive daily and monthly stock market data\non over 32,000 securities listed on US stock exchanges such as the NYSE and NAS‐\nDAQ and on a broad range of market indexes. The CRSP US Stock dataset contains\ninformation on price and quote data as well as external identifiers, shares outstand‐\ning, market capitalization, delisting information, and corporate action details.\nCompustat Financials\nCompustat Financials , also called Compustat Fundamentals or Compustat Global, is \nthe world benchmark dataset for company fundamentals. It provides standardized\ninformation on more than 80,000 international public companies. Compustat pro‐\nvides point-in-time snapshots of fundamental data, allowing researchers to conduct\nreliable historical and backtesting analyses. The dataset is quite comprehensive, with\nover 3,000 fields conveying information on financial statements, ratios, corporate\nactions, industry specifications, identifiers, and more.\nTrade and Quote Database\nThe daily Trade and Quote (TAQ) dataset  provides daily high-frequency data on all\ntrades and quotes that take place on the NYSE, NASDAQ, and other regional\nexchanges. TAQ data is available starting from 2003. The distinguishing feature of\nTAQ is the time precision it offers: seconds (HHMMSS) since 1993, milliseconds\n(HHMMSSxxx) since 2003, microseconds (HHMMSSxxxxxx) since 2015, and nano‐\nseconds (HHMMSSxxxxxxxxx) since 2016.\nInstitutional Brokers’ Estimate System\nThe Institutional Brokers’ Estimate System (I/B/E/S)  is a database maintained by\nLSEG, serving as the market benchmark for stock analysts’ earnings estimates for\npublicly traded companies. Over 950 firms and more than 19,000 analysts from 90+\ncountries regularly contribute to the I/B/E/S. The database provides extensive cover‐\nage of over 60,000 companies, with data available from 1976 for North America and\nfrom 1987 for international markets.\nBenchmark Financial Datasets | 81",2427
38-Trade Reporting and Compliance Engine.pdf,38-Trade Reporting and Compliance Engine,,0
39-SDC Platinum.pdf,39-SDC Platinum,"IvyDB OptionMetrics\nIvyDB  is the market benchmark dataset for historical equity and index options data.\nThe most popular version of IvyDB is the US database, but since 2008, IvyDB Europe\nhas been available as well. IvyDB provides a rich set of options data fields, such as\ndaily quotes, identifiers, volume, computed implied volatility, option Greeks, open\ninterest, interest rates, maturity, exercise, and exercise price. Information on the\nunderlying instruments is also available, including closing prices, dividends, and cor‐\nporate action information.\nTrade Reporting and Compliance Engine\nTrade Reporting and Compliance Engine (TRACE)  is a financial dataset on bond and\nfixed-income transactions. TRACE represents an indispensable data source as most\nbond transactions happen OTC, making it a less transparent market than centrally\nexchanged instruments such as stocks.\nTRACE itself is a program created by the Financial Industry Regulatory Authority\n(FINRA) to enable market participants who are FINRA members to report their\nfixed-income OTC transactions. FINRA members are required to report their trans‐\nactions within 15 minutes of execution, which is then made available in real time to\nTRACE subscribers. Data available via TRACE includes fields such as execution time,\nprice, volume, and bond yield.\nOrbis Global Database\nOrbis  is the industry’s primary resource for global entity data. Published by Moody’s\nAnalytics, it contains information on more than 450 million private and listed com‐\npanies worldwide, offering detailed financial information for many of them.\nIn addition to its global coverage, Orbis provides comparable information on com‐\npany ownership structure and financial strength metrics. Orbis collects data from\nmore than 170 data providers and hundreds of its own data sources. The data is fur‐\nther enriched and standardized to enable easy querying, analysis, and comparison.\n82 | Chapter 2: Financial Data Ecosystem",1982
40-Standard  Poors Dow Jones Indices.pdf,40-Standard  Poors Dow Jones Indices,,0
41-Alternative Datasets.pdf,41-Alternative Datasets,"SDC Platinum\nSDC Platinum , offered by the London Stock Exchange Group (LSEG), is a premier \nsource of comprehensive global corporate finance and market deal event data. It pro‐\nvides detailed information on various financial transactions, including mergers and\nacquisitions, alliances, private equity, venture capital, new issues, leveraged buyouts,\nand syndicated loans, among many others.\nStandard & Poor’s Dow Jones Indices\nStandard & Poor’s Dow Jones Indices (SPDJI)  is a leading global index provider and a\nprimary source of historical index data, offering a wide range of indexes across vari‐\nous markets, including equities, derivatives, fixed income, and commodities. Exam‐\nples include the S&P 500, S&P MidCap 400, and S&P SmallCap 600, which are widely\nrecognized as leading indicators of US equity market performance. S&P DJI provides\ndetailed data features such as index names, constituents and their weights, closing\nprices, market capitalization, constituent company information, and index-related\nevents.\nAlternative Datasets\nThe datasets illustrated so far are general in scope and generated through the tradi‐\ntional mechanisms of financial markets. Interestingly, alternative, more specialized\ndatasets have been gaining popularity among market participants. Let’s look at a few\nexamples.\nBitSight Security Ratings\nBitSight Security  is a world leader in cybersecurity rating and related analytics. Bit‐\nSight ratings convey comparable insights and visibility into a company’s cybersecurity\nrisk. It provides adaptive ratings correlated with the changing ransomware risk land‐\nscape. BitSight ratings are calculated using objective and observable factors such as\nserver software, open ports, TLS/SSL (Transport Layer Security/Secure Sockets Layer)\ncertificates and configuration, web application headers, and system security.\nBenchmark Financial Datasets | 83\nGlobal New Vehicle Registrations\nThe Global New Vehicle Registrations dataset , offered by S&P Global Mobility, pro‐\nvides daily information and analysis on vehicle registrations from more than 150\ncountries, 350 brands, multiple fuel types (diesel, petrol, etc.), and body types (e.g.,\ncar, van, SUV). The dataset provides valuable information that can be used to analyze\ntrends in the automotive market, such as the transition to electric vehicles.\nWeather Source\nThe Weather Source dataset  provides hourly and daily weather-related data for a large\nnumber of locations worldwide. Weather Source collects and standardizes weather\ndata from many input sources and provides weather insights relevant to different\nbusinesses.\nPatent data\nPatent data is unstructured data that conveys information on patent details such as\nthe inventor and assignee name, related patent citations, patent abstract, patent sum‐\nmary, detailed description, and claims. This data is often used to understand techno‐\nlogical innovation problems. In recent years, the use of patent data has gained\nincreasing importance in financial analysis. One of the primary sources of patent data\nis the United States Patent and Trademark Office (USPTO) , which provides public\naccess to detailed patent and trademark information. Another useful source is Google\nPatents, which aggregates patent data from a variety of sources, including the USPTO,\nand makes it easily searchable via its search engine.\nIn conclusion, it’s important to keep in mind that the number of datasets created and\nused by financial markets is immense, and we have only scratched the surface in this\nsection. As a financial data engineer, one of the most valuable skills that you can\ndevelop is the ability to search and navigate the financial data landscape and identify\nthe right dataset for a particular business problem.\n84 | Chapter 2: Financial Data Ecosystem",3838
42-Summary.pdf,42-Summary,"Summary\nThis chapter provided an overview of the financial data landscape, which we can\nsummarize as follows:\n•Classifying and explaining the sources of financial data\n•Distinguishing between the different structures used to represent financial data\n•Illustrating the main types of financial data generated by the various market\nactivities\n•Providing a short list of benchmark datasets widely recognized among market\nparticipants and researchers\nNow that you have an understanding of what financial data engineering entails and\nthe intricacies of the financial data ecosystem, we’ll shift gears with the next few chap‐\nters, where you will learn about specific financial data engineering topics. The follow‐\ning three chapters will address in depth the following problems, which have been\nselected based on their prominent importance for financial markets:\n•Financial identification systems and their main features and challenges\n(Chapter 3 )\n•The process and methods for financial entity recognition and resolution\n(Chapter 4 )\n•Financial data governance through data quality, integrity, privacy, and\nsecurity ( Chapter 5 )\nLet’s keep going!\nSummary | 85",1176
43-Chapter 3. Financial Identification Systems.pdf,43-Chapter 3. Financial Identification Systems,,0
44-Who Creates Financial Identification Systems.pdf,44-Who Creates Financial Identification Systems,"CHAPTER 3\nFinancial Identification  Systems\nA key aspect of financial data, such as prices and transactions, is that it can provide\ninformational value only if we can reliably assign each record to its corresponding\nentity. Being able to filter a dataset to get data for a specific entity unlocks the ability\nto analyze the data in meaningful ways.\nTo this end, financial market participants have developed and employed different\ntypes of financial identifiers. Nevertheless, data identification remains notably chal‐\nlenging and is widely regarded as one of the most critical problems in financial data\nmanagement. The outstanding issue of reference data management, presented in the\nprevious chapter, fundamentally revolves around financial identification and the\nmatching of various identifiers that reference the same financial market entity.\nThis chapter will discuss the problem of financial data identification, illustrate the\ndesired properties of financial identification systems, and examine the key features\nand limitations of current systems.\nIf you are going to become a financial data engineer, dealing with financial identifiers\nand knowing how to manage their shortcomings will be one of the main challenges\nyou will regularly face. So, let’s dive into this issue.\nFinancial Identifiers\nThe predominantly digital nature of financial market operations and transactions\nnecessitates recording and querying them through information and database systems.\nAt its heart, a reliable financial information system is an identification system: a way\nof telling who interacts with whom, a way of distinguishing one financial entity from\nanother, and a way of finding all records that belong to the same entity. Consequently,\nwell-identified financial data will deliver valuable insights and a significant competi‐\ntive edge. To learn more, let’s further detail what financial identifiers and\n87\nidentification  systems are, why they are essential for financial markets, and who cre‐\nates and maintains financial data identification systems.\nFinancial Identifier  and Identification  System Defined\nFrom our discussion so far, you’ve seen how financial data is produced in large vol‐\numes and various formats, which are subsequently organized into more coherent col‐\nlections known as financial datasets. Typically, each financial dataset must have an\nobservation unit  (or statistical unit ) that represents the object for which information\n(data points ) is available. For example, a company fundamentals dataset would have\nthe company as the unit of observation. To distinguish data points for one unit from\nthe others, such as company A ’s data from company B’s, a data identifier (company\nidentifier in our example) should be attached to each data point. Without an identi‐\nfier, financial datasets would be of no practical use. Figure 3-1  illustrates the concept.\nThe tables shown in the figure convey information about the annual revenues of dif‐\nferent companies observed at multiple periods. The data in the left table is not very\nuseful as it is hard to determine which firm the statistics are referencing. When the\ncompany_id identifier is added in the right table, it becomes possible to distinguish\nbetween several companies that the data references.\nFigure 3-1. Unidentified  dataset (left)  versus identified  dataset (right)\nIn this book, I define a financial identifier and financial identification system as\nfollows:\nA financial identifier is a character sequence associated with a particular financial\nentity (e.g., company, individual, transaction, asset, document, sector group, event,\netc.), enabling accurate identification of said entity across one or more financial data‐\nsets or information systems. A financial identifier can be any combination of numeric\ndigits (0-9), alphabet letters (a-z, A-Z), and symbols. A financial identification system\ncreates principles and procedures for generating, interpreting, storing, assigning, and\nmaintaining financial identifiers.\n88 | Chapter 3: Financial Identification  Systems\n1If you are interested in this topic, I highly recommend Martijn Groot’s book, Managing Financial Information\nin the Trade Lifecycle: A Concise Atlas of Financial Instruments and Processes  (Elsevier, 2008).There are a few more key terms that are important to establish. First, calling an iden‐\ntifier a code, ID, or symbol is common. Second, financial identification systems can\ngenerate identifiers following an encoding  system or as arbitrary IDs . An encoding\nsystem converts words, letters, numbers, and symbols into a short, standardized for‐\nmat for identification, communication, and storage. The reverse process is decoding,\nwhich converts the code sequence back to its original form to make it easier to under‐\nstand. Identifiers that do not adhere to an encoding system are often described as\narbitrary; they are randomly created and assigned and have no particular meaning.\nThird, the field that deals with building financial identification systems is frequently\nreferred to as symbology , a term you will often hear when learning and working with\nfinancial identifiers.\nThe Need for Financial Identifiers\nFinancial data identifiers serve various purposes, one of the most common being the\nidentification of financial instruments and entities involved in market transactions.\nThis type of identification data is often referred to as reference data, as we discussed\nin Chapter 2 .\nIn any financial transaction, it is crucial to include the identifiers of the exchanged\ninstruments and the entities participating in the agreement. Additionally, the transac‐\ntion itself may be assigned an identifier for tracking purposes. As a result, identifiers\nare integral throughout the entire lifecycle of a financial transaction, from pre-trade\nto trade to post-trade settlement. They facilitate swift and efficient market transac‐\ntions, enhance communication among market participants, increase transparency,\nand reduce operational costs and errors.1\nAnother important use of financial identifiers is for reporting purposes. Regulatory\nauthorities can demand various information from financial institutions, such as mar‐\nket exposure, capital, risk concentration, liquidity, assets and liabilities, and trades.\nFinancial institutions need to aggregate data from multiple sources and divisions to\nprepare the required report. Financial identifiers are crucial in this situation since\nthey enable data collection and consolidation, ensuring accurate and timely report‐\ning. In addition, by adding identifiers to the reported data, regulators would find it\nsimple to examine the information and judge the reporting institution’s compliance.\nSeveral regulatory frameworks, such as MiFID II, mandate that reporting institutions\nuse certain financial identities when reporting data. This, in turn, imposes an extra\nFinancial Identifiers  | 89\n2For an interesting read, see Richard Y oung’s article, “The Identifier Challenge: Attributes of MiFID II That\nCannot Be Ignored” , Journal of Securities Operations & Custody  9, no. 4 (Autumn 2017): 313–320.obligation on reporting institutions to establish a reliable financial identification\nsystem.2\nFinancial identifiers are essential for exchange listing and trading purposes. To list\nand trade a financial security on a trading venue, it must be assigned an identifier.\nThis enables investors, traders, and market makers to easily locate, track, buy, sell,\nand analyze financial instruments.\nLast but not least, financial identifiers are required as an essential data field when per‐\nforming financial data analysis. A substantial portion of financial analysis is cross-\nsectional, where the focus is on studying the behavior and differences among\ndifferent financial entities (e.g., assets, companies, etc.). Using financial identities\nwould enable the analyst to pick the right data sample, run quality checks and filters,\neliminate duplicates, and match the same entity across numerous datasets.\nWho Creates Financial Identification  Systems?\nVarious organizations, spanning both public and private sectors, may generate and\nassign a financial identifier. Some organizations develop recommendations for finan‐\ncial identification systems but do not issue the identifier for those who require it; oth‐\ners issue identifiers based on existing standards or recommendations; and still others\ndevelop and issue the identifier. Let’s explore this variety of roles and functions with\nsome examples.\nInternational Organization for Standardization (ISO)\nThe International Organization for Standardization  (ISO) is an independent organiza‐\ntion that creates and promotes voluntary and consensus-based international stand‐\nards for various technical and nontechnical fields. It is composed of representatives\nfrom the national standards organizations of 169 countries.\nThroughout the years, the ISO has demonstrated considerable interest and involve‐\nment in developing international financial identifiers. For example, the ISO standard\nknown as the International Securities Identification Number (ISIN) has emerged as\nthe leading identifier in international security trading, clearing, and settlement. Later\nin this chapter, I will cover the ISIN identifier as well as other ISO-based identifiers in\ndetail.\nCrucially, the ISO does not issue and assign identifiers for market participants;\ninstead, this job is delegated to so-called National Numbering Agencies, which we\nwill examine in the following section.\n90 | Chapter 3: Financial Identification  Systems\nHow Does the ISO Develop a Standard?\nFamiliarity with industry standards is essential for a financial data engineer to excel.\nHowever, how do organizations such as the ISO create a standard? As reported by the\nofficial website , the ISO does not unilaterally decide when to create a standard;\ninstead, it responds to a market need raised by stakeholders such as companies, con‐\nsumer associations, academia, NGOs, and government and consumer groups. A typi‐\ncal scenario involves an industry group reporting the need for a standard to its\nnational member, who subsequently approaches the ISO.\nOnce a market need for a standard emerges, the ISO appoints a committee composed\nof independent technical experts nominated by ISO members. A committee may have\nsubcommittees and working groups. For example, ISO/TC 68 is the ISO committee\ntasked with overseeing financial services standards globally, and it has three main\nsubcommittees:\nISO/TC 68/SC 2\nCovers information security in financial services\nISO/TC 68/SC 8\nCovers reference data for financial services\nISO/TC 68/SC 9\nCovers information exchange for financial services\nThe nominated committee starts the process by discussing the nature, scope, and key\nelements of the standard and submitting a draft proposal that meets the market need.\nThe draft is then shared for further review and recommendations. Reaching a final\nagreement is consensus based and relies on a voting mechanism. If a consensus is not\nreached, then the draft will be modified and voted on again. Developing an ISO stan‐\ndard typically takes three years from initial proposal to final publication.\nNational Numbering Agencies\nA National Numbering Agency  (NNA) is a national organization that issues and pro‐\nmotes ISO-based financial identifiers. Each country is free to assign the role of NNA\nto a local market player, which can be a stock exchange, central bank, regulator, clear‐\ning house, financial data provider, or custodian. For example, the UK NNA is the\nLondon Stock Exchange, and Luxembourg’s is Clearstream Banking (a central securi‐\nties depositary). Some countries don’t have an NNA; in such cases, a Substitute Num‐\nbering Agency (SNA) is appointed. Examples of SNAs include CUSIP Global Services\nin the US and WM Datenservice in Germany.\nThe Association of National Numbering Agencies (ANNA) was established to coordi‐\nnate between the different national NNAs. ANNA collects and aggregates identifier\nFinancial Identifiers  | 91\ndata from all its members into a global centralized dataset called the ANNA Service\nBureau (ASB) to ensure data quality and guarantee global interoperability.\nFinancial data vendors\nMost financial data vendors create their own identification systems to support the\ndevelopment of their products and services. For instance, when aggregating data\nfrom many sources, the data vendor might face the issue of identifier heterogeneity,\nwhich can be hard to resolve. It may also be the case that the data has no identifier, for\nexample, when working with unstructured data such as news and text. Additionally,\nexternal identifiers might lack some of the properties the vendor needs (e.g., unique‐\nness). In such cases, creating a new vendor-based identification system is a common\npractice. Vendors such as S&P Global Market Intelligence, LSEG, Bloomberg, and\nFactSet all have their own in-house identifiers.\nIn certain instances, financial data vendors develop their identifiers because they cre‐\nate, rather than aggregate, their content. For example, news analytics providers such\nas RavenPack create and provide structured news and event data from unstructured\ncontent for a large number of entities. Since RavenPack independently extracts enti‐\nties from text, it has developed its proprietary unique entity identifier system, known\nas the RavenPack Unique Entity Identifier.\nFinally, given the increasing value of financial identifier data (e.g., for reference data\nmanagement), financial data vendors may find it a profitable opportunity to create\nand license their financial identifiers.\nFinancial institutions\nFinancial institutions create identifiers to facilitate internal processes, such as transac‐\ntions, account management, client identification, and payment card number genera‐\ntion. Additionally, they might collect financial data from various sources, both\ninternal and external, which may come with different identifiers. In these cases, a\nfinancial institution may opt to create its internal identification system to allow for a\nflexible design that matches business requirements and enables easy data retrieval and\naggregation.\nIn conclusion, financial identifiers are critical to the efficiency, interoperability, and\ntransparency of financial markets. All market participants deal with identifiers, either\nas users or as participants in their development. At this point, you may be wondering\nwhy it is so difficult to design a financial identification system that works in all cir‐\ncumstances and meets all market demands. To answer this question, we must first\nexamine the desirable attributes of financial identification systems and understand\nthe various constraints and challenges involved. This is what I’ll talk about next.\n92 | Chapter 3: Financial Identification  Systems",15014
45-Desired Properties of a Financial Identifier.pdf,45-Desired Properties of a Financial Identifier,,0
46-Globality.pdf,46-Globality,"Desired Properties of a Financial Identifier\nDesigning and maintaining a good financial identification system is one of the most\npersistent challenges in the financial industry. The difficulty stems from the need to\nbalance various desirable properties, each of which may conflict with others. In this\nsection, I build upon the framework proposed in the seminal work of Jules J. Berman,\nPrinciples and Practice of Big Data  (Academic Press), to derive a minimal set of\ndesired properties for a financial identification system. These properties include\nuniqueness, globality, scalability, completeness, accessibility, authenticity, granularity,\npermanence, immutability, and security. Let’s explore each property in detail.\nUniqueness\nUniqueness refers to the quality of being one of a kind. For instance, fingerprints are\nunique to each human; no two humans on earth can have the same fingerprint. In the\nsame way, a financial identification system must uniquely identify financial entities\nand never assign the same identifier to distinct entities. Figure 3-2  illustrates the con‐\ncept of uniqueness. The identification system on the left is not unique since it assigns\nthe identifier 56H128 to two separate entities: WTK Inc. and XYZ Inc. However, the\nsystem on the right is unique as it assigns a distinct identifier to all three financial\nentities.\nFigure 3-2. Unique (right) versus nonunique (left)  identification  systems\nDesired Properties of a Financial Identifier  | 93\nCrucially, the concept of a unique identifier can become ambiguous when applied to\nfinancial markets. To illustrate how, let’s consider the following scenarios:\n•A company is listed on two stock exchanges, A and B. Should the listed stock\nhave two identifiers, one for each market, or a single identifier for both?\n•A financial instrument is trading in two countries; do we need separate identifi‐\ners for each country?\n•A financial instrument can have multiple issues. Should we treat each issue as a\nunique instrument or use a single identifier for all?\n•Following a company merger or acquisition, does the resulting entity represent a\nnew, unique company or maintain the identity of the original?\n•A financial transaction contains multiple instructions that relate to various\ninstruments. Should each instruction have its own identifier or simply use the\nparent transaction identifier?\nQuite challenging, no? Now, I imagine that you might have a solution for the above\nscenarios. But this is exactly where the issue lies; different market participants make\ndifferent assumptions about what a unique financial identifier should be, and this has\nled to the development and adoption of numerous identifiers that differ in the way\nthey define uniqueness. This can explain the common practice of using multiple\nidentifiers within a dataset, for example, to identify the security with one identifier\nand the market where it is traded with another. If you are designing a financial identi‐\nfication system, my advice for you is to carefully think about the concept of unique‐\nness and discuss it with your business team to avoid serious shortcomings that might\narise later.\nGlobality\nFinancial markets are complex and dynamic systems that are in continuous evolution\nand expansion. To illustrate how, let’s consider the following facts:\n•New companies are established and listed on the market.\n•New financial instruments are created, listed, and traded in different markets and\njurisdictions.\n•Trading activities take place in various venues, such as centralized exchanges,\ntrading platforms, and OTC markets.\n•Consumer demand and preferences for financial products and services are con‐\nstantly evolving, creating opportunities for the introduction of new offerings.\n•Financial transactions span multiple countries, markets, and jurisdictions.\n•New markets are established for exchanging new financial products.\n94 | Chapter 3: Financial Identification  Systems\n•New financial entities are recognized and extracted from financial data.\n•New exchange platforms and mechanisms gain popularity (e.g., cryptocurren‐\ncies).\n•New types of data on financial activities are recorded and consumed.\n•New financial regulatory requirements are published and enforced.\n•New market standards are released, promoted, and adopted.\n•New financial technologies emerge and diffuse among market participants.\nThese dynamics, among others, have contributed to a significant increase in the num‐\nber and variety of financial entities that require identification. To meet this demand, a\nfinancial identification system must be able to accommodate the expanding ecosys‐\ntem of financial activities and entities. I call such property globality . For example, a\nglobal identification system can do the following things:\n•Expand from assigning identifiers at a national level to an international scale\n•Expand its scope from assigning identifiers solely within centralized markets to\nOTC and other types of markets\n•Expand its coverage to include various financial instruments such as indices,\nderivatives, digital assets, and loans\nFigure 3-3  illustrates a straightforward example showcasing the concept of globality.\nThe system on the top is global, as it expands its coverage to new areas that emerge\nfrom market expansion. The system on the bottom is nonglobal, as it covers only\nthree areas and can’t expand further.\nFigure 3-3. Global (top) versus nonglobal (bottom) identification  systems\nDesired Properties of a Financial Identifier  | 95",5577
47-Accessibility.pdf,47-Accessibility,"Several identifiers, in particular vendor-specific identifiers, are limited to certain mar‐\nkets (e.g., stocks, bonds), exchanges (e.g., NYSE trades and quotes), or jurisdictions\n(e.g., US stocks, UK stocks). However, some identifiers have been developed to cover\na broader range of market entities. The best example is perhaps Bloomberg’s Finan‐\ncial Instrument Global Identifier (FIGI), which we will discuss in more detail later in\nthis chapter.\nScalability\nA scalable financial identification system does not exhaust its pool of assignable iden‐\ntifiers. Several reasons could lead to a shortage in the available supply of identifiers in\na financial identification system:\nRapid market growth\nIf the number of financial entities requiring identifiers increases rapidly, the iden‐\ntification system may struggle to keep up with the demand. One example is the\nissuance of a high volume of short-term financial instruments, such as commer‐\ncial papers, repurchase agreements, and certificates of deposit. These instruments\ntypically have short maturities and are frequently rolled over, creating a constant\ndemand for new identifiers. If each instrument requires a unique identifier, it can\nquickly reduce the available pool of new identifiers.\nLimited character length\nIf the identifier format is limited in character length, exclusively employs\nnumeric characters, or imposes format constraints, there may be a finite number\nof possible combinations, causing the identification system to exhaust its identifi‐\ners soon. For example, the Issuer Identification Number (IIN), presented later in\nthis chapter, was expanded from a six-digit to an eight-digit format due to a\nshortage in the supply of assignable identifiers . Another example is the SEDOL\nidentifier, discussed later in this chapter, which was changed from a numeric to\nan alphanumeric format following plans to expand its market coverage.\nPoor allocation strategy\nIf the identification system does not allocate the identifiers optimally, it might\nlead to early exhaustion. Examples of poor allocation strategies include the\nfollowing:\nCategory-based allocation\nAllocating identifiers based on specific categories (e.g., different types or\nclasses of stocks, bonds, derivatives) can lead to the exhaustion of available\nidentifiers for a particular category if it grows more quickly than others. For\ninstance, if bond instruments are allocated identifiers from 00000-29999 and\nstock instruments from 30000-49999, the bond category might run out of\nidentifiers much earlier if there is a surge in bond issuance.\n96 | Chapter 3: Financial Identification  Systems\nReserved ranges\nReserving large ranges of identifiers for future use or specific purposes, such\nas special market events, regulatory reporting, or new financial instruments,\ncan significantly reduce the pool of identifiers available for general use.\nAddressing these issues entails proactive planning, market growth projection, peri‐\nodic evaluations, and adaptability of the identification system to the ever-changing\nfinancial market ecosystem.\nCompleteness\nCompleteness requires that an identifier be assigned to each uniquely identifiable\nentity covered by the identification system. In other words, if an identification system\nis created to identify a set of financial entities, then each entity in the set must have an\nidentifier. Table 3-1  illustrates the concept by comparing complete and incomplete\nidentifiers. Identification system A is incomplete because it lacks identifiers for enti‐\nties 3, 4, and 6. In contrast, Identification system B is complete, as it assigns identifi‐\ners to all six entities.\nTable 3-1. Comparing complete and incomplete identifiers\nEntity Identifier  A (incomplete) Identifier  B (complete)\nEntity 1 19982243 A5J234HS\nEntity 2 87987924 B5J874GS\nEntity 3 NULL T3H7Z589\nEntity 4 NULL GQ16B437\nEntity 5 23987912 N9M3F16S\nEntity 6 NULL K485GV1Z\nA financial identification system may suffer from incompleteness for various reasons.\nFor instance, a newly established company identification system might not backfill\nthe history of its covered universe because some companies have failed, gone bank‐\nrupt, delisted, merged with other companies, or changed their names. Another com‐\nmon scenario arises when merging different datasets with different identifiers. For\nexample, if you join data using a US-based identifier with data using a global identi‐\nfier, many entities might lack the US identifier.\nDesired Properties of a Financial Identifier  | 97",4580
48-Authenticity.pdf,48-Authenticity,"3In 2011, the European Commission concluded  that Standard and Poor’s (S&P) was abusing its dominant posi‐\ntion and violating antitrust rules as the unique issuing agency of US security ISINs by charging European\nfinancial institutions high access fees. The Commission made it legally binding for S&P to abolish licensing\nfees paid by banks for using US ISINs.Accessibility\nFinancial identifier data should be accessible, meaning it should not be restricted by\nlicense fees or usage limits, or monopolized by market players. Limited access to\nfinancial identifiers can result in market inefficiencies and a lack of transparency\nwhen conducting transactions and reporting.3 Logically, there are situations where\nfinancial identifiers, like credit card numbers or bank account IDs, must be secured.\nIn some cases, financial identifiers are freely available (e.g., on online trading plat‐\nforms or stock exchange websites). However, collecting and aggregating this data in a\nstructured format for a large number of entities can be overwhelming. Financial data\nvendors provide such products, often sold as reference data, but these are often sub‐\nscription based.\nSome market players launched initiatives to promote open-access financial identifi‐\ners. For example, LSEG openly released its Permanent Identifier (PermID) system ,\nwhich provides comprehensive coverage across a wide variety of entity types, includ‐\ning companies, financial instruments, issuers, funds, and people. Another example is\nOpenFIGI, an open-access system that allows anyone to request Bloomberg’s Finan‐\ncial Instrument Global Identifier (FIGI), which I discuss later in this chapter.\nTimeliness\nThe timeliness of a financial identification system refers to its ability to process and\ngenerate identifiers quickly and efficiently. When a new financial entity enters the\nmarket or is created within a system (e.g., a new issue of a financial instrument), a\ntimely financial identification system should do the following:\n•Enable market participants to request an identifier quickly, ideally in real or near-\nreal time\n•Process requests quickly, allowing the issuing entity to allocate identifiers\npromptly\n•Make accessible the newly allocated identifiers to other market participants\nwithout delay\nEfficient and timely generation and dissemination of financial identifiers are essential\nfor enhancing the efficiency of financial market operations and transactions.\n98 | Chapter 3: Financial Identification  Systems",2519
49-Permanence.pdf,49-Permanence,"4For more details, see Angeliki Skoura,, Julian Presber, and Jang Schiltz’s article, “Luxembourg Fund Data\nRepository” , Data  5, no. 3 (July 2020): 62.Authenticity\nAn authentic financial identification system can confirm whether a specific identifier\nwas generated by it. To draw an analogy, this is the same as telling whether a watch\ndisplaying the label Rolex  is indeed an authentic Rolex.\nFinancial identifiers are often designed following a specific symbology specification,\nwhich can be a formula or standardized format. This, in turn, enables the develop‐\nment of programs that can verify the identifier’s adherence to these specifications. A\ncommon practice involves the addition of a check digit  to the identifier string. When a\ncheck digit is included, typically at the end of the identifier, an algorithm can be used\nto authenticate the identifier based on this digit. Later in this chapter, I will illustrate\nvarious examples of identifier check digits and their calculation logic.\nGranularity\nIn financial markets, it is quite common to observe hierarchical arrangements and\nstructures within and between entities:\n•Asset origination chain (e.g., issuing company → securities → stocks → common\nstocks → issues).\n•Trading context (e.g., international → continental → national → market).\n•Pyramid organizational structures (e.g., CEO → president → vice-president →\nmiddle management → team leaders → employees) .\n•Company ownership structures (e.g., parent company A owns B (child), B owns\nD, and D owns F, G, K).\n•Asset classifications, where assets are organized in groups, categories, and subca‐\ntegories (e.g., derivatives → options → stock options → call stock option).\n•Sector classifications, which structure economic sectors using an industry taxon‐\nomy (e.g., sectors → industry groups → industries → sub-industries).\n•Packaged transactions such as syndicated loans (package →facilities).\n•Complex financial instruments such as multi-leg options or interest rate swaps,\nwhere the same instrument has multiple child instruments.\n•In the fund industry, an umbrella fund structure allows for the creation of multi‐\nple subfunds, each with distinct investment strategies, further divided into vari‐\nous share classes tailored to different investor needs.4\nDesired Properties of a Financial Identifier  | 99\nA financial identification system is granular  if it can scale to provide different levels of\ndetails that reflect market hierarchies. Figure 3-4  illustrates this concept. In this\nexample, the entity requiring identification should be recognizable at international,\nnational, and market levels. The identification system on the left is nongranular\nbecause it uses the same identifier for the entity across all levels. In contrast, the sys‐\ntem on the right is granular because it assigns a unique identifier to the entity at each\nlevel.\nFigure 3-4. Granular versus nongranular financial  identification  systems\nPermanence\nA reliable financial identification system must ensure that identifiers and their associ‐\nated data are permanent and durable. This is essential for guaranteeing market trust\nand confidence and allowing the tracking and referencing of financial instruments\nand transactions over time. For example, a bank client should be able to go to the\nbank at any time and ask for a statement about their past transactions. If the cus‐\ntomer closes their accounts with the bank, their identification data should not disap‐\npear from the system.\nIn some cases, a financial identifier can be interrupted; for example, a commercial\ndata vendor might stop maintaining an identifier as they develop a new identification\nsystem or switch to using an industry-wide system. Additionally, an identification\nsystem might be replaced with a new one if it fails to meet certain desired properties,\nsuch as uniqueness, globality, and granularity. The concept of permanent versus non‐\npermanent identifiers is shown in Figure 3-5 . The identification system on the left is\npermanent as it could return the associated data at time t and time t+n. In contrast,\nthe system on the right is nonpermanent as it returns data at time t but returns none\nat time t+n.\n100 | Chapter 3: Financial Identification  Systems\nFigure 3-5. Permanent versus nonpermanent identification  systems\nDesired Properties of a Financial Identifier  | 101",4410
50-Immutability.pdf,50-Immutability,,0
51-International Securities Identification Number.pdf,51-International Securities Identification Number,"Immutability\nA robust financial identification system must ensure the immutability of its identifi‐\ners over time. This means that an identifier associated with a specific entity should\nneither change nor be reassigned to another entity. If the identified entity ceases to\nexist, its identifier should never be reused. Such a property is essential for maintain‐\ning transparency, traceability, and effective monitoring.\nBesides temporal immutability, identifiers must persist through all stages of financial\nactivities. For example, a transaction’s lifecycle, which typically comprises multiple\nsteps from initiation, through execution, to settlement, should consistently use the\nsame identifier. Table 3-2  illustrates the concept of mutable versus immutable identi‐\nfiers. Identifier A is immutable, as all three entities have the same identifier in 2005\nand 2022. Identifier B is not mutable, as entities 1 and 3 changed their identifier in\n2022. Moreover, the identifier of entity 1 used in 2005 was reassigned to entity 3 in\n2022.\nTable 3-2. Mutable versus immutable identifiers\nEntity Year Identifier  A (immutable) Identifier  B (mutable)\nEntity 1 2005 XZY743 XZY743\nEntity 2 2005 ABC376 ABC376\nEntity 3 2005 MNT098 MNT098\nEntity 1 2022 XZY743 KKH654\nEntity 2 2022 ABC376 ABC376\nEntity 3 2022 MNT098 XZY743\nCorporate events such as mergers, acquisitions, spin-offs, corporate\nrestructurings, and rebranding might change a company’s struc‐\nture, potentially resulting in the creation of a new entity. In these\ncases, a new identifier may be assigned.\nSecurity\nSecurity must be prioritized when designing a financial identification system, as these\nsystems are vulnerable to malicious attacks. Consider the damage that an attacker\ncould inflict, for instance, if they succeeded in hacking an identification system and\nirreversibly changing its identifiers. Even worse is the scenario in which sensitive\nidentifying data is compromised and utilized to commit financial crimes against the\nentities identified in the data.\nTo summarize, developing an optimal financial identification system is challenging\nsince it involves multiple complex properties that may be difficult to achieve in a sin‐\ngle system. For this reason, financial markets have developed a plethora of financial\n102 | Chapter 3: Financial Identification  Systems\nidentification systems, each serving a particular purpose and offering unique charac‐\nteristics. Some of these systems are widely adopted, while others are gaining attention\nin response to evolving market needs. The next section will provide you with an over‐\nview of current financial identification systems.\nFinancial Identification  Systems Landscape\nThe current landscape of financial identification systems is extensive, comprising\nopen source and proprietary systems designed with different properties and for vari‐\nous purposes. Several standardization efforts have been initiated, yet a universally\naccepted standard has not been established. To give you an idea of the current scale, a\nWikipedia search for “security identifier”  yields results for 16 different identifiers.\nThe ISO conducted an inventory of the current national and international financial\nidentification standards  and identified a total of 19 different systems. These identifi‐\ners and several others are illustrated in Figure 3-6 .\nFigure 3-6. Financial identification  systems in use globally\nFinancial Identification  Systems Landscape | 103\n5CUSIP and SEDOL are discussed in the next few sections. For details on WKN, refer to the definition offered\nby Börse Frankfurt .In the following sections, I will discuss some of the most important financial identifi‐\ncation systems used by markets, along with their key characteristics and challenges.\nInternational Securities Identification  Number\nThe International Securities Identification  Number  (ISIN) is a 12-character alphanu‐\nmeric code, defined in ISO 6166 , that uniquely identifies a wide range of financial\nsecurities such as stocks, bonds, derivatives, fund share classes, and indexes.\nIn principle, an ISIN should represent a set of fungible financial instruments. For\ninstance, the common stocks of a given company are fungible with each other,\nregardless of the issuance date, as they all share the same specifications. In this sce‐\nnario, the same ISIN is typically used to represent the company’s common stock.\nHowever, if the company issues different classes of stocks, such as preferred stocks,\neach class would receive a different ISIN. Similarly, multiple bond issues from the\nsame company would have distinct ISINs, since each issue has unique specifications\nsuch as the start date, end date, and interest rate. The same principle applies to more\ncomplex financial products, such as derivatives.\nISIN codes are issued and maintained by each country’s National Numbering Agency\nor the Substitute Numbering Agency. The ISIN system has gained widespread market\nadoption and is increasingly recognized as the global standard for financial instru‐\nment identification. Due to their reliability, ISIN identifiers are predominantly used\nfor trading, settlement, clearing, and regulatory reporting.\nThe 12 characters of an ISIN can be divided into three parts: the first two characters\nare the ISO 3166-1 alpha-2 code  of the issuing country. The middle nine alphanu‐\nmeric characters represent the security identifier. This nine-digit alphanumeric code\nis often called the National Securities Identifying Number (NSIN). Examples of\nNSINs include CUSIP in the USA, SEDOL in the UK, and WKN (“Wertpapierkenn‐\nnummer”) in Germany.5 The last character is a check digit computed using the mod‐\nulus 10 double-add-double or Luhn algorithm. Figure 3-7  offers a visual breakdown\nof the ISIN.\nFigure 3-7. Structural breakdown of the ISIN code\n104 | Chapter 3: Financial Identification  Systems\n6Example taken from the ISIN organization web page .\n7Keep in mind that many financial identification systems are always expanding, so just because an identifier\ndoesn’t cover a certain market segment now doesn’t imply it won’t tomorrow.One potential drawback of the ISIN is its international scope, which means it does\nnot specify the trading location or currency. If a given stock trades on multiple differ‐\nent exchanges, the associated ISIN will be the same. For instance, IBM’s common\nstock is listed on almost 25 trading platforms and exchanges around the world. In\nthis case, some identifiers for IBM stock would vary depending on where it is traded\n(e.g., the ticker). However, IBM stock will have only one ISIN for each security.6 To\novercome this limitation, the Market Identifier  Code  (MIC), defined in ISO 10383 , is\ncommonly used alongside the ISIN to specify exchanges, trading venues, and both\nregulated and nonregulated markets.\nAnother limitation is that the ISIN does not provide detailed information about the\ncontract terms or variations within a particular type of security. In other words, an\nISIN cannot describe a contract uniquely.\nAn ISIN might become inactive or be replaced with a new one following corporate\nactions such as mergers and acquisitions, company name changes, stock splits, and\nthe redemption/conversion of debt instruments.\nAnother limitation of the ISIN is that it does not cover all instruments, especially\nover-the-counter instruments. In this case, alternative instrument identifiers are often\nused.7\nLuhn Algorithm: The Industry Standard for\nFinancial Identifier  Validation\nThe Luhn algorithm or Luhn formula, also known as the modulus 10 or mod 10 algo‐\nrithm, named after its creator, IBM scientist Hans Peter Luhn, and specified in\nISO/IEC 7812-1 , is a widely used checksum formula for financial identifier validation.\nThe algorithm is not meant to be a security measure against malicious attacks but\nrather a mechanism to protect against errors and distinguish valid identifiers from\nincorrect ones.\nTo validate an identifier such as ISIN using the Luhn formula, six steps are required:\n1.If the identifier already contains the check digit, remove it. In most cases, the\ncheck digit is located at the end of the string. The remaining string constitutes\nthe payload .\n2.Starting from the rightmost digit in the payload, double the value of every second\ndigit.\nFinancial Identification  Systems Landscape | 105",8444
52-Transaction Identifiers.pdf,52-Transaction Identifiers,"3.If doubling the digit results in a number that is greater than 9 (e.g., 18), then sum\nthe digits of the doubled number to get a single digit (e.g., 12 becomes 1 + 2 = 3).\n4.Sum all the digits.\n5.Compute the sum’s modulus 10 as (10 − ( sum mod 10 )) mod 10. The result of\nthis operation can be interpreted as the smallest number that needs to be added\nto the sum (possibly 0) to make it a multiple of 10.\n6.If the modulus 10 obtained in step 5 is equal to the check digit, then the number\nis valid. Otherwise, it’s not.\nFor an example, let’s validate the ISIN of Microsoft, US5949181045:\n1.To get a numerical string, we convert the country letters to digits by taking the\nASCII code (American Standard Code for Information Interchange) of the capi‐\ntal letter and subtracting 55. The ASCII code for U is 85, while for S, it’s 83;\ntherefore, the new digit is [85-55][83-55]5949181045 = 30285949181045.\n2.Remove the check digit (=5) to get the payload → 3028594918104.\n3.Starting from the right, we double the value of each second digit, and we get [6] 0\n[4] 8 [10] 9 [8] 9 [2] 8 [2] 0 [8].\n4.Take the sum of the multidigit numbers (>9). We have one, which is [10], and by\nsumming 1 + 0, we get 1; therefore, the new sequence is [6] 0 [4] 8 [1] 9 [8] 9 [2]\n8 [2] 0 [8].\n5.Add up all the digits: 6 + 0 + 4 + 8 + 1 + 9 + 8 + 9 + 2 + 8 + 2 + 0 + 8 = 65.\n6.Calculate the check digit using the formula: 10 – (65 mod 10) mod 10 = 5.\n7.As the final result (5) is equal to the check digit, we can tell that the identifier is a\nvalid ISIN.\nA Python implementation for this ISIN check digit validation is available in the Git‐\nHub repo for this book .\nClassification  of Financial Instruments\nThe Classification  of Financial Instruments  (CFI) is a six-letter code defined in ISO\n10962  that describes and classifies financial instruments. It was developed to address\na major problem in financial markets, namely the need for a uniform and consistent\napproach to categorizing and grouping financial instruments. The ISO has appointed\nSIX Group, the national numbering agency of Switzerland, as the maintenance\nagency for the CFI code. SIX publishes a list of all CFI codes and modifications on its\nwebsite .\n106 | Chapter 3: Financial Identification  Systems\nSince July 1, 2017, CFI codes are globally assigned alongside the ISIN when a new\nfinancial instrument is issued. In most cases, an instrument’s CFI remains unmodi‐\nfied during its lifespan. However, corporate-related events such as changes to voting\nrights or ownership restrictions could cause a CFI to change.\nIn a CFI code, the first letter indicates the instrument category, the second denotes\nthe subcategory, and the remaining letters indicate various attributes of the instru‐\nment. Both the second and remaining letters are optional. For example, the CFI code\nESVXXX is for equity (E), common/ordinary share (S) with voting right (V), while\ncode FXXXXX indicates a future (F), and OCXXXX is an option (O) of type call (C).\nFigure 3-8  offers a visual illustration of the CFI code.\nFigure 3-8. Structural breakdown of the CFI code\nFinancial Instrument Short Name\nThe Financial Instrument Short Name  (FISN), defined in ISO 18774 , is a human-\nreadable code used to provide a consistent, uniform, and global description of finan‐\ncial instruments. Since July 1, 2017, the FISN has been globally assigned alongside the\nISIN and CFI at the time of issuance of a new financial instrument.\nUnlike ISINs, which are primarily used for clearing and settlement, FISNs are utilized\nby market participants to enhance the readability, efficiency, reliability, data consis‐\ntency, and transparency of financial services transactions and reference data.\nThe FISN code has a maximum length of 35 alphanumeric characters. Of these, 15\nare reserved for the issuer name, and 19 for the description of the instrument, with\nthe character “/” as the separator between the issuer name and description. Figure 3-9\nillustrates the structure of the FISN code.\nFigure 3-9. Structural breakdown of the FISN code\nFinancial Identification  Systems Landscape | 107\n8The syndicated lending market is where a group of financial institutions (the syndicate) jointly extend a loan\n(often large loans) to a single borrower.Committee on Uniform Security Identification  Procedures\nThe Committee on Uniform Security Identification  Procedures  (CUSIP) is a nine-\ncharacter alphanumeric code used to identify financial securities in the United States\nand Canada. CUSIP codes are mainly used for trading, settlement, and clearing. They\nare issued and managed by CUSIP Global Services (CGS) , which acts as the US\nnational numbering agency and is operated by FactSet Research Systems, Inc., as of\nthe time this book was written.\nThe first six characters of a CUSIP uniquely identify an issuer (company, municipal‐\nity, agency). The seventh and eighth characters identify the instrument type and issue\nusing a hierarchical alphanumeric convention. The last character is a check digit.\nFor US securities, the CUSIP number makes up the middle nine characters of a US\nISIN. For example, Amazon’s ISIN is US0231351067, and Amazon’s CUSIP is\n023135106. Figure 3-10  offers a visual representation of the CUSIP code.\nFigure 3-10. Structural breakdown of the CUSIP code\nTo check the validity of a CUSIP code, the following steps are required:\n1.Convert non-numeric characters to digits according to their ordinal position in\nthe alphabet plus 9 (e.g., A = (1 + 9) = 10).\n2.Convert the characters * to 36, @ to 37, and # to 38.\n3.Multiply every even digit by 2. If the result is a two-digit number, add the digits\ntogether (e.g., 13 → 1 + 3 = 4).\n4.Get the sum of all values.\n5.Get the floored value of the sum: (10 – (sum modulo 10)) modulo 10.\nA Python implementation for ISIN check digit validation is available in the GitHub\nrepo for this book .\nRecently, a new identifier, called CUSIP Entity Identifier  (CEI), was introduced by\nCGS  to identify legal parties involved in the syndicated lending market.8 The CEI was\ndeveloped in collaboration with the Loan Syndications and Trading Association\n(LSTA) and syndicated loan solution providers.\n108 | Chapter 3: Financial Identification  Systems\nLegal Entity Identifier\nThe Legal Entity Identifier  (LEI), defined in ISO 17442 , is a 20-character alphanu‐\nmeric code used to identify legal entities engaged in financial transactions. Each LEI\ncontains information about an entity’s ownership structure and thus answers the\nquestions of “who is who” and “who owns whom. ” This concept aligns closely with\nknow your customer (KYC) practices in financial markets, which aim to verify the\nidentity of clients within financial institutions. The LEI identifier is a global, unique,\nand freely accessible identifier. Its official maintainer is the Global Legal Entity Identi‐\nfier Foundation (GLEIF).\nAs illustrated in Figure 3-11 , the first four characters of the LEI form a prefix that\nidentifies the Local Operating Unit (LOU) that issued the LEI. Characters 5 to 18\nconstitute the entity identifier assigned by the LOU. Finally, characters 19 and 20 are\ntwo check digits.\nFigure 3-11. Structural breakdown of the LEI identifier\nISO 17442 classifies  the following as legal entities:\n•All financial intermediaries\n•Banks and finance companies\n•International branches\n•All entities that issue equity, debt, or other securities for other capital structures\n•All entities listed on an exchange\n•All entities that trade financial instruments or are otherwise parties to financial\ntransactions, including business entities, pension funds, and investment vehicles\nsuch as collective investment funds (at umbrella and subfund levels) and other\nspecial purpose vehicles that have a legal form\n•All entities under the purview of a financial regulator and their affiliates, subsid‐\niaries, and holding companies\n•Sole traders (as an example of individuals acting in a business capacity)\nOn April 4th, 2019, ANNA and GLEIF launched a collaborative initiative to associate\nISIN identifiers with the LEI identifiers of their issuers. This initiative aims to\nenhance market transparency by establishing a direct link between the issuer and the\nissuance of securities.\nFinancial Identification  Systems Landscape | 109\nLehman Brothers and the Establishment of the Legal Entity Identifier\nThe introduction of the LEI has been celebrated as a major milestone in financial\nmarkets, generating numerous discussions and publications about its beneficial\nimpact on improving market efficiency and stability. The question is, why all this\nattention?\nThe issue started shortly after the collapse of Lehman Brothers in 2008. As explained\nby a McKinsey report , “ After Lehman’s demise, participants in the global financial sys‐\ntem could not assess their exposure to Lehman, its subsidiaries, and each other\nbecause there was no standard system for identifying counterparties in the maze of\nsubsidiaries and affiliates from which banks, insurers, asset managers, and other mar‐\nket participants transact. ”\nTo give an illustrative example, when Lehman Brothers collapsed in 2008, it had an\nestimated 8,000 subsidiaries working in various jurisdictions , while Morgan Stanley\nhad roughly 3,500 subsidiaries. Consider the potential number of bilateral agreements\nand transactions between any of Lehman’s 8,000 subsidiaries and Morgan Stanley’s\n3,500 and assume that such agreements are specified using different identification\nsystems. Now, assume that you want to calculate the total exposure of Morgan Stanley\nto Lehman Brothers. This would require aggregating millions of positions, each iden‐\ntified using the subsidiary’s convention.\nUnsurprisingly, it took quite a lot of time for Lehman Brothers’s counterparties to cal‐\nculate their total exposure to the consolidated entity. To mitigate this issue and ensure\nthat financial institutions can quickly aggregate their exposures and risk information,\nthe ISO led an initiative to develop an international legal entity identification stan‐\ndard, resulting in the LEI.\nIn addition to the LEI, the ISO created the Entity Legal Forms  (ELF), described in ISO\n20275 , to uniquely identify entity forms/types globally using a standardized four-\ncharacter code. GLEIF maintains ELF codes that can be accessed and downloaded\nfrom its web page .\nTransaction Identifiers\nIdentifiers are the backbone of the lifecycle of financial transactions. During its pro‐\ncessing, a single transaction often traverses various systems and parties. Conse‐\nquently, the same transaction might be recorded with different IDs as it moves from\none system to the next. This makes it difficult to unambiguously identify transactions\nand all related messages across disparate systems and databases. To overcome this\nissue, unique transaction identifiers have been developed.\n110 | Chapter 3: Financial Identification  Systems\n9In finance, a swap is a type of derivative instrument in which two parties agree to exchange financial instru‐\nments, cash flows, or payments over a specified period of time.\n10To explore the utility of UTIs in financial markets, I recommend reading Swift’s article on “The Unique Trans‐\naction Identifier and Its Value in Securities Settlement” .\n11Refer to BIS (editors): Committee on Payments and Market Infrastructures Board of the International Orga‐\nnization of Securities Commissions Technical Guidance, “Harmonisation of the Unique Transaction Identi‐\nfier” , February 2017. One example is the Unique Swap Identifier  (USI),9 specific to the United States and\nmandated  by the Commodity Futures Trading Commission (CFTC) and the Securi‐\nties and Exchange Commission (SEC) as part of the Dodd-Frank Act. The USI is a\nfixed-length identifier assigned to all swap transactions, identifying the transaction\nuniquely throughout its lifecycle. To expand the scope of the USI identifier for global\nreporting of financial transactions, the Unique Trade Identifier  (UTI) was introduced .\nTo ensure consistency across different jurisdictions and reporting platforms, the ISO\nintroduced the ISO 23897 —the Unique Transaction Identifier  (UTI) standard. It pro‐\nvides specifications aimed at standardizing transaction identifiers globally, ensuring\nunique identification throughout a financial transaction’s lifecycle. This is essential\nfor facilitating transaction reporting, improving traceability, and reducing operational\nerrors, especially as cross-border trading expands.10\nThe format of these transaction identifiers is generally similar. ISO 23897 specifies a\nmax length of 52 characters for UTI code, but a variable length is possible. There is a\nconsensus that a unique transaction identifier should include a prefix identifying the\nissuing entity, followed by a string that uniquely identifies the transaction. A market\npreference has emerged in favor of using international standards such as LEI identifi‐\ners for the prefix part.11 For example, in 2013, the ISDA working group proposed a\nbest practice recommendation for the UTI , where the prefix consists of characters 7–\n16 of the LEI, followed by a 32-character string, as illustrated in Figure 3-12 .\nFigure 3-12. ISDA-based Unique Trade Identifier  (UTI) structure\nFinancial Identification  Systems Landscape | 111",13446
53-Derivative Identifiers.pdf,53-Derivative Identifiers,"12Sedol technical documentation is available on the LSE website .\n13Information taken from the official company page of HSBC on the LSE website . Stock Exchange Daily Official  List\nThe Stock Exchange Daily Official  List  (SEDOL) is a seven-character alphanumeric\ncode mainly used to identify securities traded on the London Stock Exchange (LSE)\nand other smaller exchanges in the UK. The LSE (the National Numbering Agency of\nthe UK) assigns SEDOL codes upon request from the security issuer.12\nOver the years, the SEDOL system has expanded, and SEDOL codes can now be\nissued at the country level to represent securities listed in multiple jurisdictions.\nSEDOLs are issued globally across all jurisdictions and multiple asset classes. If a\nsecurity is traded on a different exchange in a different country, it will be assigned a\nseparate SEDOL code. This makes SEDOLs unique across countries, which is ideal\nfor international trading and security identification.\nFor UK securities, SEDOLs are embedded within the UK ISIN codes by adding the\ncountry code at the beginning, followed by two padding zeros, then the SEDOL, and\nfinally, the ISIN check code. For example, the UK banking group HSBC has a SEDOL\ncode of 0540528 and an ISIN code of GB0005405286.13\nSEDOL codes issued prior to March 2004 were exclusively numeric. Afterward, the\nSEDOL system moved to an alphanumeric format that starts with an alphabetic char‐\nacter, followed by five alphanumeric characters and a trailing numeric check digit.\nFigure 3-13  illustrates the structure of SEDOL.\nFigure 3-13. The structural breakdown of the SEDOL identifier\nTo check if a SEDOL is valid, the check digit is chosen to make the weighted sum of\nall SEDOL characters a multiple of 10. The steps to validate a SEDOL are as follows:\n1.Convert non-numeric characters to digits according to their ordinal position in\nthe alphabet plus 9 (A = (1 + 9) = 10).\n2.Multiply each of the first six numbers by their corresponding weight: 1 for first\nposition, 3 for second, 1 for third, 7 for fourth, 3 for fifth, 9 for sixth.\n112 | Chapter 3: Financial Identification  Systems\n3.Get the sum of all values.\n4.Get the floored value of sum: (10 – (sum modulo 10)) modulo 10.\nA Python implementation for SEDOL check digit validation is available in the Git‐\nHub repo for this book .\nTicker Symbols\nA ticker symbol is a short and unique series of letters assigned to financial securities\n(mostly stocks) for listing and trading purposes. There is no standard for tickers, and\nthey can be generated and assigned by various organizations, including exchanges\nand trading venues, financial data providers, and financial institutions.\nThe allocation process and formatting conventions of tickers are specific to each issu‐\ning organization. Among US exchanges, tickers are typically one to four characters\nlong, and they resemble the company name when possible. For example, the ticker\nfor Apple, Inc. on the NYSE is AAPL, while Ford Motor has the ticker F. Some\nexchanges, such as NASDAQ, add a fifth symbol to their tickers  to convey informa‐\ntion about the trading status and special features of the stock. For example, in BRK.A,\nthe first three represent the stock symbol for Berkshire Hathaway, Inc. (BRK), and the\nlast letter (.A) indicates that the shares are of class A type, which traditionally holds\nmore voting rights.\nIn addition to exchanges, financial data providers assign proprietary ticker symbols\nto financial instruments. For example, Bloomberg created its own Bloomberg ticker\nto identify a financial entity uniquely within the Bloomberg ecosystem. A Bloomberg\nticker can include the exchange-specific ticker, the market sector, the exchange code,\ninstrument-specific information (e.g., bond maturity, option expiry, option type,\netc.), and the Bloomberg database (e.g., EQUITY for stocks).\nAnother well-known proprietary ticker symbol is the Refinitiv  Instrument Code\n(RIC), which is issued and maintained by LSEG. RIC tickers are mainly used to look\nup information on specific financial instruments on LSEG platforms. The main com‐\nponent of the RIC is the security’s ticker symbol with an optional character that iden‐\ntifies the exchange. For example, the RIC symbol IBM.N refers to IBM stock (IBM)\ntraded on the New Y ork Stock Exchange (.N).\nStock tickers can vary by exchange and country, which implies that they may not be\nunique across different exchanges and countries. As a result, to reliably identify a\nstock, both the ticker and the exchange or country of listing are often required.\nTickers are not immutable and might change to reflect corporate actions such as\nmergers and acquisitions. For example, prior to the 1999 merger with Mobil Oil,\nExxon used the phonetic spelling of the company (XON) as its ticker symbol. After\nthe merger, the symbol changed to XOM.\nFinancial Identification  Systems Landscape | 113\n14An option is financial derivative instrument that gives its holder the right, but not the obligation, to buy or\nsell a specific quantity of an underlying asset at a given strike price on or before a specified future date,\ndepending on the option style.Importantly, tickers are not guaranteed to remain unique, as they can be reassigned\nover time. For example, until 2017, the ticker SNOW was assigned by NYSE to Intra‐\nwest Resorts Holdings, Inc. In May 2017, Henry Crown and Company and KSL Capi‐\ntal Partners acquired Intrawest and transformed it into a privately owned company.\nAfter delisting Intrawest, the ticker SNOW was reassigned to Snowflake. A Wikipedia \nsearch of both company names would confirm this, as illustrated in Figure 3-14 .\nFigure 3-14. NYSE tickers for Intrawest and Snowflake\nDerivative Identifiers\nDerivatives are among the most exchanged financial securities in financial markets.\nAs a quick reminder, a derivative is a contract that derives its value from an underly‐\ning financial asset or variable such as stocks, commodities, foreign exchange, interest\nrates, indexes, and many more.\nCrucially, identifying derivative instruments is more challenging than identifying\nother types of financial instruments. First, several constituent elements must be con‐\nsidered to identify a derivative instrument. Second, due to their flexible and custom‐\nizable nature, derivatives can easily turn into very complex products. Third, a\nsubstantial deal of derivatives is traded OTC, complicating their tracking and identifi‐\ncation. Nonetheless, as I will demonstrate next, market participants have developed\nvarious initiatives to identify both exchange-traded and OTC derivative instruments.\nOption symbol\nOption symbols are derived symbols used to identify an option and its characteristics\non a given exchange.14 The current market standard for option symbols is based on\nthe Options Clearing Corporation ’s (OCC) Options Symbology Initiative  (OSI). The\nOSI format consists of a 21-character alphanumeric code that can be split into four\nparts:\n114 | Chapter 3: Financial Identification  Systems\n•A root (ticker) symbol of the underlying stock or ETF (exchange-traded fund).\n•An expiration date, which is six digits in the format YYMMDD.\n•An option type, either C for a call or P for put.\n•The strike price. This is represented by the price times 1,000, with the front pad‐\nded with 0s to 8 digits. The decimal point falls three places from the right in the\noptions symbol: 00000.000.\nThe structure of the OSI option symbol is illustrated in detail in Figure 3-15 .\nFigure 3-15. The structural breakdown of the OSI option symbol\nCFI, UPI, and OTC ISIN\nTo identify OTC derivatives, a combined identification scheme has been put in place\nthat relies on three identifiers:\nOTC ISIN\nThis is allocated by the Derivatives Service Bureau  (DSB), with the initial two\ncharacters starting with the custom “EZ” code.\nUnique Product Identifier  (UPI)\nThis is defined in ISO 4914  for the identification of OTC derivative products.\nClassification  of financial  instruments (CFI)\nThis is generated by the DSB as part of the OTC ISIN generation process.\nThe three identifiers are combined to provide different levels of detail. Malavika Sol‐\nanki, of the management team at the DSB, illustrated the combined use of the three\nidentifiers with the following example : at the highest level, the CFI can tell that a\nderivative instrument is a “single currency, fix-float, interest rate swap with a constant\nnotional schedule and cash delivery. ” The UPI would tell a bit more about the prod‐\nuct, for example, that it has a “three-month reference rate term, a USD reference rate,\nand that the name of the reference rate was USD-LIBOR-BBAR. ” Finally, the OTC\nISIN can provide more granular details about the specific instrument that has been\ntransacted, such as “the standardized ISO reference rate name, the price multiplier\nassociated with the instrument, the full name and short names of the instrument, the\nexpiry date. ”\nFinancial Identification  Systems Landscape | 115",9087
54-Financial Instrument Global Identifier.pdf,54-Financial Instrument Global Identifier,"Alternative Instrument Identifier\nThe Alternative Instrument Identifier  (AII) has been adopted within the European\nUnion for reporting purposes to identify derivatives traded on regulated markets\nwithout an ISIN assigned to them. Rather than being a code, the AII is a concatena‐\ntion of descriptive fields that identify the instrument. The fields include the exchange\ncode, exchange product code, derivative type, put/call identifier, expiry/delivery/\nprompt date, and strike price, as illustrated in Figure 3-16 .\nFigure 3-16. The structural breakdown of the AII identifier\nFinancial Instrument Global Identifier\nThe Financial Instrument Global Identifier  (FIGI) is a 12-character alphanumeric ID\ncovering hundreds of millions of active and inactive instruments around the world.\nThe history of FIGI started in 2009 when Bloomberg decided to release its Open\nSymbology (BSYM) system for identifying financial securities across asset classes.\nThe BSYM provides a library of identifiers for hundreds of millions of securities,\nknown as Bloomberg Global Identifiers (BBGIDs). In 2014, the BSYM symbology\nsystem was adopted by the Object Management Group (OMG), a nonprofit standards\nconsortium, in order to promote it as an open industry standard. Subsequently, the\nBBGID was renamed to FIGI. Since its introduction, FIGI has received a lot of mar‐\nket attention. For example, the Accredited Standards Committee X9 adopted FIGI as\nan official US data identification standard .\nFIGI codes are unique across all markets and countries and remain unchanged once\nissued. For example, if IBM stock is traded on 12 stock exchanges, there will be 12\ndifferent FIGIs.\nFurthermore, FIGI was designed as a global system capable of identifying any type of\nfinancial instrument, including stocks, bonds, derivatives, loans, indexes, funds, and\ndigital assets. FIGIs are also free to access. An open source tool called OpenFIGI  was\nreleased to identify, map, and request a free FIGI via an API.\n116 | Chapter 3: Financial Identification  Systems\n15To explore for yourself, please use this link .The FIGI identification system has a hierarchical structure composed of three levels:\nGlobal FIGI\nThis is the most granular level, identifying a financial asset at the trading-venue\nlevel. It is unique to a specific instrument at a particular trading venue.\nComposite global FIGI\nThis level aggregates multiple venue-level FIGI identifiers within the same coun‐\ntry, providing a broader identification that encompasses all trading venues for a\nspecific instrument within that country.\nShare class global FIGI\nThis level further aggregates FIGI identifiers to cover financial instruments\nacross multiple countries. It provides a global view of a single instrument regard‐\nless of the country and venue.\nFor example, Amazon trades on multiple stock exchanges in the US. When Amazon\ncommon stock trades on the New Y ork Stock Exchange, it has the FIGI code\nBBG000BVPXP1. When the same stock trades on the NASDAQ Global Select\nexchange, it has a FIGI of BBG000BVQ4Z3. But if you don’t care about which US\nexchange Amazon trades on, you can use the Amazon composite FIGI\n(BBG000BVPV84) to generically reference Amazon stock traded in the US. If you\nwant to globally identify Amazon common stock regardless of trading venue and\ncountry, the share class code (BBG001S5PQL7) can be used.15 Figure 3-17  illustrates\nthe hierarchical structure of FIGI.\nFigure 3-17. Hierarchical representations of FIGI\nFinancial Identification  Systems Landscape | 117\nAll three levels of FIGI identifiers have the same structure and limitations. The iden‐\ntifier may contain only letters in [B, C, D, F, G, H, J, K, L, M, N, P , Q, R, S, T, V , W , X,\nY , Z], and zero to nine digits.\nThe structure of FIGI codes is as follows:\n•The first two characters identify the certified issuer that created the FIGI code.\nCurrently, Bloomberg generates the majority of FIGI codes, hence the presence\nof the prefix BB in most identifiers.\n•The third character is the letter G, used to indicate that it’s a global identifier.\n•Characters 4–11 are alphanumeric characters that constitute the reference ID.\n•A trailing check digit.\nFigure 3-18  illustrates this structure visually.\nFigure 3-18. The structural breakdown of FIGI\nThe check digit procedure is based on the Luhn algorithm, following five steps:\n1.Get the identifier, e.g., BBG000BLNQ16, and remove the check digit,\nBBG000BLNQ1.\n2.Convert non-numeric characters to digits according to their ordinal position in\nthe alphabet plus 9 (A = (1 + 9) = 10). In our example, we get [11][11][16][0][0]\n[0][11][21][23][26][1].\n3.Double every second digit: [11][22][16][0][0][0][11][42][23][52][1].\n4.Compute the sum of the resulting values: 1 + 1 + 2 + 2 + 1 + 6 + 0 + 0 + 0 + 1 + 1\n+ 4 + 2 + 2 + 3 + 5 + 2 + 1 = 34.\n5.Get the floored value of the sum: (10 – (34 modulo 10)) modulo 10 = 6, which is\nthe check digit attached to the identifier.\nA Python implementation for FIGI check digit validation is available in this book’s\nGitHub repo .\n118 | Chapter 3: Financial Identification  Systems",5163
55-FactSet Permanent Identifier.pdf,55-FactSet Permanent Identifier,,0
56-Bank Identifiers.pdf,56-Bank Identifiers,"FactSet Permanent Identifier\nThe FactSet permanent identifier  is a proprietary identification system developed by\nFactSet to offer a stable and unified identifier. It includes three levels:\nSecurity\nIdentifying the security globally\nRegional\nIdentifying the security at the regional level per currency (e.g., US/USD)\nListing\nIdentifying the security at the market level (e.g., US/NYSE/USD)\nFactSet provides the FactSet Symbology API , a symbol/identifier resolution service\nthat allows users to map a wide variety of identifiers to FactSet’s native symbology or\nthird-party identifiers such as CUSIPs, SEDOLS, ISINs, Bloomberg FIGI, and many\nmore. In addition, FactSet offers the FactSet Concordance API , which enables users\nto programmatically match the FactSet identifier for a specific entity based on\nattributes, such as name, URL, and location.\nLSEG Permanent Identifier\nThe Permanent Identifier  (PermID)  is a unique identifier used within the London\nStock Exchange Group (LSEG) information model to identify and reference various\nobjects, including organizations, instruments, funds, issuers, and people. It ensures\nthat these objects are accurately and unambiguously referenced and linked together,\neven as their relationships change over time.\nWhat makes PermID especially valuable is that it’s linked to a unique web address or\nUniform Resource Identifier (URI), offering a permanent, direct link to the identified\nentity. LSEG has made PermID open source, enabling access via web pages or\nthrough API-based entity search and matching services.\nFor example, the following PermID URLs can be used to retrieve information about\nthe technology company Apple:\nhttps://permid.org/1-4295905573\nProvides organizational details identifying Apple, Inc.\nhttps://permid.org/1-8590932301\nContains instrument details identifying Apple’s ordinary shares.\nhttps://permid.org/1-25727408109\nIncludes quote information identifying Apple’s ordinary shares on the New Y ork\nStock Exchange.\nFinancial Identification  Systems Landscape | 119\n16More on blockchain internals will be discussed in Chapter 8 .Digital Asset Identifiers\nFollowing the emergence and diffusion of blockchain and distributed ledger technol‐\nogies, digital assets  have been established as a new type of financial market entity.\nAccording to the ISO 22739 vocabulary for blockchain , a digital asset is an “asset that\nexists only in digital form or which is the digital representation of another asset. ” In\nmany cases, digital assets are referred to as digital tokens . A digital token can be fungi‐\nble if it’s identical and interchangeable with similar assets (e.g., one dollar in New\nY ork is the same as one dollar in Australia), or nonfungible  if they are unique and\nnondivisible (e.g., a painting or a boat). The most common place for exchanging digi‐\ntal assets is a blockchain , which, according to ISO 22739 vocabulary, refers to a “dis‐\ntributed ledger with confirmed blocks organized in an append-only, sequential chain\nusing cryptographic links. ”16 The process of converting something to a digital asset\nand adding it to a blockchain system is called tokenization .\nOver the past years, a wide variety of fungible digital assets have been developed, for\nexample the following:\nCrypto assets\nAssets developed using cryptographic techniques. The most notable examples are\ncryptocurrencies such as Bitcoin and Ethereum. Crypto assets can serve as a\nmeans of payment or investment.\nSecurity token\nA financial security or instrument converted into a digital token and exchanged\non a distributed ledger like blockchain (e.g., Microsoft Stock Token). Similar to\ntraditional security certificates, security tokens represent an ownership right in a\ncompany or asset.\nNonfungible tokens\nThese represent ownership rights over a unique digital asset such as images, vid‐\neos, and games.\nUtility token\nA token used to access a specific service or feature within a blockchain-based\necosystem.\n120 | Chapter 3: Financial Identification  Systems\nAs digital assets have grown in popularity, the requirement to identify the issued,\nstored, and transacted digital tokens and crypto assets has become more urgent. A\nfirst initiative was established with the introduction of the Digital Token Identifier\n(DTI), defined in ISO 24165-1 , to identify fungible tokens and digital ledgers. In\ndefining the DTI, the ISO realized the need for a new methodology for assigning the\nidentifier. This is because digital assets are often not associated with a particular issu‐\ning entity, such as central banks, unlike traditional assets.\nThe standard helps in reducing confusion and increasing trust in the crypto assets\nmarket by providing a universal method of identification.\nIndustry and Sector Identifiers\nIndustry and sector identifiers are classification frameworks used to categorize and\ngroup businesses into various categories and subcategories. Financial firms use indus‐\ntry identifiers to analyze market exposure, classify stocks, measure concentration\nrisks, and understand cross-industry differences. Two main industry classification\nframeworks are widely used: the Standard Industrial Classification  (SIC) and the\nNorth American Industry Classification  System  (NAICS).\nThe SIC identifier is a four-character numerical code used to classify industries based\non their primary activities. The SIC system was created in 1937 by the US govern‐\nment to facilitate economic analysis across industries and agencies and to promote\nstandardization and uniformity in the collection and recording of industrial data.\nAs illustrated in Figure 3-19 , the official SIC classification code consists of three parts.\nThe first two digits, which are mandatory, identify the major sector group. The third\ndigit further categorizes the business into an industry group. The fourth digit pro‐\nvides the most granular classification, specifying the particular sector of the business.\nFor example, SIC code 6021 (National Commercial Banks)  belongs to industry group\n602 (Commercial Banks), which is part of major group 60 (Depositary Institutions),\nwhich belongs to the division of (Finance, Insurance, and Real Estate).\nFigure 3-19. The structural breakdown of the SIC code\nFinancial Identification  Systems Landscape | 121\nThe SIC system had several shortcomings. First, it produced ambiguous, mis‐\nmatched, and overlapping classifications generated by SIC. Second, the four-digit sys‐\ntem restricted the addition of new, emerging business sectors and industries. To\novercome these issues, the SIC system was replaced in 1997 by the North American\nIndustrial Classification System (NAICS), which introduced a more flexible six-\ncharacter numeric code.\nAs shown in Figure 3-20 , the first two digits of an NAICS code indicate the major\nsector of the business. The third digit indicates the subsector, and the fourth digit\ndesignates the industry group. The fifth digit indicates the specific industry of opera‐\ntion, while the sixth code specifies the national industry. For example, NAICS code\n522110  is used to identify commercial banks. The first two digits, 52, define the sector\n(Finance and Insurance), the first three digits (522) identify the subsector (Credit\nIntermediation and Related Activities), the first four digits (5221) define the industry\ngroup (Depository Credit Intermediation), and the last two digits identify the indus‐\ntry (Commercial Banking).\nFigure 3-20. The structural breakdown of the NAICS code\nOther industrial classification frameworks are in use worldwide. These include the\nStatistical Classification of Economic Activities in the European Community, known\nas NACE and predominantly used in the European Union, the UK Standard Indus‐\ntrial Classification of Economic Activities (UK SIC), and the Australian and New\nZealand Standard Industrial Classification (ANZSIC).\nBank Identifiers\nBanks are the cornerstones of financial markets. They provide a secure and reliable\nplace for individuals and organizations to deposit, transfer, and invest money, obtain\nloans, and make credit and debit card payments. Additionally, they support various\nonline applications and offer a platform for FinTech firms to offer their services.\nTherefore, bank identifiers are essential for identifying banks, customers, and pay‐\nment cards.\nThe most prominent example is the Business Identifier  Code  (BIC), also called SWIFT\nor Bank Identifier Code. This alphanumeric code, defined in ISO 9362 , is used to\nidentify banks, financial institutions, and nonfinancial institutions worldwide when\nconducting international money transfers and routing exchanging messages. SWIFT\nissues BIC codes.\n122 | Chapter 3: Financial Identification  Systems\nA SWIFT/BIC consists of either 8 or 11 alphanumeric characters that identify the\ncountry, city, bank, and optionally the branch:\nBank code\nFour-character alphabetic characters identifying the bank. It usually looks like a\nshortened version of that bank’s name.\nCountry code\nA two-character ISO 3166-1 alpha-2 code indicating the country where the bank\nis located.\nLocation code\nTwo-character alphanumeric code that designates where the bank’s main office is.\nBranch code\nAn optional three-character alphanumeric code representing a specific branch.\nXXX is used to indicate the bank’s head office.\nFigure 3-21  illustrates the structure of the BIC. The example shown refers to the Ital‐\nian bank UniCredit Banca. The displayed code can be read as follows: UNCR identi‐\nfies UniCredit Banca, IT is the country code for Italy, MM is the office location code\nfor Milan, and XXX indicates the head office.\nFigure 3-21. The structural breakdown of the BIC code\nThe use of the BIC code to identify financial institutions is quite common, especially\nwhen identifying financial institutions in international transactions. Nevertheless,\ncertain countries employ their own local bank or branch identification codes. For\ninstance, Australia uses the Bank State Branch  (BSB), a six-digit code, to identify\nbranches of Australian financial institutions. Similarly, in the United States, financial\ninstitutions involved in various payment operations are identified using the ABA\nRouting Number , consisting of nine digits.\nAnother well-known bank identifier is the International Bank Account Number\n(IBAN). It is defined in ISO 13616  and serves as an international system of unique\ncodes used to identify bank accounts when conducting money transfers. The IBAN\nsystem was originally developed for use within the EU, but it was later adopted by\nregions such as the Caribbean and Middle East.\nFinancial Identification  Systems Landscape | 123\nThe length of the IBAN varies by country, but it cannot exceed 34 characters. For\ninstance, Belgian IBANs have 16 alphanumeric characters, Luxembourg’s are 20, and\nGermany’s are 22.\nAn IBAN code is composed of three main parts (see Figure 3-22  for an illustration):\n•Country code following the ISO 3166-1 alpha-2 convention.\n•Two check digits.\n•Basic Bank Account Number  (BBAN)—up to 30 alphanumeric characters that\ninclude the bank code, branch identifier, and account number. The length of\nBBAN may vary across countries.\nFigure 3-22. The structural breakdown of the IBAN number for Luxembourg\nAn IBAN code is validated through a mod-97 algorithm (as described by ISO 7064 )\nthat works as follows:\n1.Make sure the IBAN has a valid length. For example, let’s take a random Luxem‐\nbourgish IBAN: LU280019400644750000. The length is 20, which matches the\ncountry’s specified length.\n2.Move the first four characters to the end of the string →\n0019400644750000LU28.\n3.Convert non-numeric characters to digits according to their ordinal position in\nthe alphabet plus 9 (A = (1 + 9) = 10). In our example, we get\n0019400644750000213028.\n4.Treat the result as an integer and check if the modulo 97 of the number is equal\nto 1, 19400644750000213028 mod 97 = 1.\nA Python implementation for IBAN validation is available in the GitHub repo for this\nbook .\nLast but not least, an important bank identifier is the payment card number or Pri‐\nmary Account Number  (PAN), defined in ISO/IEC 7812 . This is used to define pay‐\nment cards and identify their issuer and cardholder. Most payment cards, such as\ncredit, debt, and gift cards, have their PAN laser-printed on the front (yes, it’s your\ncredit card number!).\n124 | Chapter 3: Financial Identification  Systems",12573
57-Summary.pdf,57-Summary,"17Here is an exercise: take the Luhn algorithm that we defined previously in the International Securities Identifi‐\ncation Number and use it to validate a credit or debt card number (BE CAREFUL: don’t use your own card\nnumber; a list of test credit card numbers is available online ).The PAN is a numeric code with variable length ranging from 8 to 19 digits. The first\nsix to eight digits represent the Issuer Identification  Number  (IIN), which identifies\nthe card issuer. The first digit in the IIN is the Major Industry Identifier  (MII), which\nidentifies the industry/sector of the card issuer (for example, 4 and 5 are commonly\nused to identify financial institutions). The remaining characters are the individual\naccount numbers used to identify the cardholder’s account. The last digit is a check\ndigit, which can be validated using the Luhn algorithm.17 Figure 3-23  illustrates the\nstructure of the PAN.\nFigure 3-23. The structural breakdown of a 16-digit PAN Identifier\nSummary\nThis chapter offered an in-depth treatment of financial identifiers and identification\nsystems, which can be summarized as follows:\n•Defining financial identifiers and identification systems and highlighting their\ncritical role in financial market operations.\n•Providing an overview of the entities involved in the creation, issuance, and\nmaintenance of financial identifiers and identification systems.\n•Outlining the desired properties of an optimal financial identification system.\n•Discussing existing financial identification systems in detail, highlighting their\nmain features and shortcomings.\nThe key takeaway from this chapter is that financial identification presents significant\nchallenges for financial markets. In addition, as new products and market require‐\nments emerge, the complexity of the issue continues to evolve. I highly recommend\nyou stay informed about emerging trends in the development, standardization, and\nadoption of financial identification systems, as well as the evolving characteristics of\nexisting systems.\nSummary | 125\nNow that you have a solid understanding of financial identification systems, let’s\nmove on to explore another related and crucial problem in financial markets: finan‐\ncial entity systems. Y ou build a financial entity system when you want to extract, rec‐\nognize, identify, and match financial entities within and across various sources of\nfinancial data.\nThe next chapter is all about financial entity systems—let’s keep moving!\n126 | Chapter 3: Financial Identification  Systems",2561
58-Financial Named Entity Recognition.pdf,58-Financial Named Entity Recognition,"CHAPTER 4\nFinancial Entity Systems\nIn the last chapter, you learned about financial identifiers and identification systems\nand their critical role in financial markets. Importantly, before a financial entity can\nbe identified, it must first be extracted and ready for identification. However, in\nfinance, it’s quite common for data to exist in an unstructured format, where entities\nare not immediately identifiable. In fact, analysts estimate  that the vast majority of\ndata in the world exists in unstructured formats, such as text, video, audio, and\nimages. Moreover, it is quite frequent that different identifiers are used to reference\nthe same financial entity across both structured and unstructured data. These factors\ncollectively pose significant challenges when trying to extract value and insights from\nthe data.\nTo this end, many financial institutions develop systems to extract, recognize, iden‐\ntify, and match financial entities within financial datasets. These systems, which I will\ncall financial  entity systems  (FESs), constitute the main topic of this chapter. As a\nfinancial data engineer, understanding FESs and the challenges they entail is essential\nin navigating today’s complex financial data landscape.\nIn the first part of this chapter, I will clarify the notion of financial entities and pro‐\nvide an overview of their various types. Next, I will illustrate the problem of financial\nentity extraction and recognition using a popular FES called named entity recognition .\nAfter that, I’ll cover the issue of financial data matching and record linkage using\nanother FES known as entity resolution .\n127\nFinancial Entity Defined\nGenerally speaking, the term entity  refers to any real-world object that can be recog‐\nnized and identified. By narrowing the scope to financial markets, we can use the\nterm financial  entity  to denote any real-world entity operating within financial mar‐\nkets. In this book, I define financial entity and financial entity systems as follows:\nA financial entity is a real-world object that may be recognized, identified, referenced,\nor mentioned as an essential part of financial market operations, activities, reports,\nevents, or news. A financial entity may be human or not. It can be tangible (e.g., an\nATM machine), intangible (e.g., common stock), fungible (e.g., one-dollar bills), or\ninfungible (e.g., loans). A financial entity system is an organized set of technologies,\nprocedures, and methods for extracting, identifying, linking, storing, and retrieving\nfinancial entities and related information from different sources of financial data and\ncontent.\nAs financial markets evolve and expand, so do the diversity and types of financial\nentities. A frequently used benchmark classification system  categorizes entities into\nfour main groups: individuals (PER), corporations (ORG), places (LOC), and miscel‐\nlaneous entities (MISC).\nNaturally, based on your institution’s needs, it might be necessary to categorize enti‐\nties into a broader or more granular range. For example, let’s say that your financial\ninstitution decides to collect data on the digital asset market. In this case, you might\nwant to create a new entity type (digital asset) to represent objects such as cryptocur‐\nrencies, digital currency, utility tokens, security tokens, stablecoins, bitcoin, and\nmany more. Other examples include the following:\n•Persons , e.g., bankers, traders, directors, account holders, investors, market mak‐\ners, regulators, brokers, financial advisors\n•Locations , e.g., New Y ork, Japan, Africa, Benelux (Belgium, the Netherlands, and\nLuxembourg)\n•Nationalities,  e.g., Italian, Australian, Chinese\n•Companies , e.g., Bloomberg L.P ., JPMorgan Chase & Co., Aramco, Ferrero\n•Organizations , e.g., Securities and Exchange Commission, European Central\nBank, London Stock Exchange, International Monetary Fund\n•Sectors , e.g., financial services, food industry, agriculture, construction, micro‐\nchips\n•Currency , e.g., dollar ($), pound (£), euro (€)\n128 | Chapter 4: Financial Entity Systems",4115
59-Named Entity Recognition Described.pdf,59-Named Entity Recognition Described,"•Commodity , e.g., gold, copper, silver, wheat, coffee, oil, steel\n•Financial security , e.g., stocks, bonds, derivatives\n•Corporate events , e.g., mergers, acquisitions, leveraged buyouts, syndicated loans,\nalliances, partnerships\n•Financial variables , e.g., interest rate, inflation, volatility, index value, rating, prof‐\nits, revenues\n•Investment strategies , e.g., passive investment, active investment, value investing,\ngrowth investing, indexing\n•Corporate and market hierarchies , e.g., parent company, holding company, subsid‐\niary, branch\n•Products , e.g., iPhone, Alexa, Siri, Dropbox, Gmail\nNow that you know what financial entities are and how to categorize them, let’s move\non to understand how to identify and extract these entities from financial data. As\npreviously mentioned, the systems designed for this purpose are referred to as named\nentity recognition  (NER) systems.\nFinancial Named Entity Recognition\nAs a financial data engineer, if you ever get assigned to a project that involves recog‐\nnizing and identifying financial entities from unstructured or semi-structured text,\nyou will likely design and build an NER system. In this section, I will first define NER\nand give a few illustrative examples. Then, I will describe how NER works and the\nsteps involved in designing an NER system. Third, I will give an overview of the avail‐\nable methods and techniques for conducting NER. Lastly, I will discuss a few exam‐\nples of open source and commercial software libraries and tools that you can use to\ndo NER.\nNamed Entity Recognition Described\nNER, also known as entity extraction, entity identification, or entity chunking, is the\ntask of detecting and recognizing named entities in text, such as persons, companies,\nlocations, events, symbols, time, and more. NER is a key problem in finance, given\nthe large volumes of finance-related text generated on a daily basis (e.g., filings, news,\nreports, logs, communications, messages) combined with the growing demand  for\nadvanced strategies for working with unstructured and text data.\nFinancial Named Entity Recognition | 129\nThe outcome of NER analysis is used in a variety of financial applications, such as\nenriching financial datasets with entity data, information extraction (e.g., extracting\nrelevant financial information from financial reports and filings), text summarization\n(e.g., ensuring adherence to legal requirements), fraud detection (identifying suspi‐\ncious entities and transactions), adverse media screening (i.e., screening an entity\nagainst a negative source of information), sentiment analysis (assessing market senti‐\nment from news and social media), risk management (e.g., recognizing potential\nfinancial risks and exposures), and extracting actionable insights from financial news,\nmarket events, players, competition, trends, and products.\nRavenPack Analytics: The Market Leader in\nFinancial Named Entity Recognition\nThe market for products and services that depend on named entity recognition meth‐\nods is rapidly expanding. Prominent names in this field include RavenPack, Info‐\nNgen, OptiRisk Systems, and LSEG’s Machine Readable News.\nRavenPack News Analytics (RNA)  is the world-leading news insights and analytics\nresource. RavenPack collects and analyzes unstructured content from more than\n40,000 sources such as Dow Jones Newswires, the Wall Street Journal , Barron’s , MT\nNewswires, PR Newswire, Alliance News, MarketWatch, The Fly, and providers of\nregulatory news, press releases, and articles.\nRavenPack News Analytics computes 20+ years of point-in-time data and provides\nevent and sentiment data on more than 350,000 entities in over 130 countries, includ‐\ning the following:\n•110,000+ global, public, and private companies across all sectors\n•165,000+ macro entities such as places, currencies, persons, and organizations\n•7,000+ key business and geopolitical and macroeconomic events detected and\nenriched with sentiment and relevance scores\nFor each record in RavenPack News Analytics, information is available on:\n•The entity (e.g., name, domicile, RavenPack’s unique entity identifiers, and other\nidentifiers)\n•Event Category\n•Event Sentiment Score (how negative or positive an event is [range -1 → 1])\n•Event Similarity Days (how novel is the event, measured as the number of days\npassed [range 0 → 365] since a similar event occurred)\n•Event Relevance Score (how relevant is an event [range 0 → 100] based on where\nit occurs—e.g., a headline has a high ERS)\n130 | Chapter 4: Financial Entity Systems\nTo identify and extract these relevant aspects from news data, RavenPack built a pro‐\nprietary named entity recognition system. RavenPack maintains a database of prede‐\nfined entities with more than 50 distinct entity types to provide timely and high-\nquality data. Moreover, RavenPack expands and extends its database as new and\nrelevant types of entities or events appear in the market.\nIn building its NER system, RavenPack faced a special requirement for the financial\nsector: entity names may change over time, and the same name may refer to different\nentities at different times. This might lead to problems such as survivorship bias,\nwhere only the latest assignee or an identifier or surviving entities are considered,\nskewing the data and the analysis. To solve this issue, RavenPack constructed a point-\nin-time-aware NER system.\nThe main idea behind NER is to take an annotated text such as…\nGoogle has invested more than $1 Billion in Renewable Energy projects in the United\nStates over the past 5 years\n… and produce a new block of text that highlights the position and type of entities, as\nillustrated in Figure 4-1 . In this example, six types of entities are recognized: com‐\npany, currency, amount, sector, time, and location.\nFigure 4-1. An illustration of the outcome of NER\nFor the sake of illustration, let’s walk through a practical example. A well-known\nfinancial dataset is LSEG Loan Pricing Corporation DealScan , which offers compre‐\nhensive coverage of the syndicated loans market. A syndicated loan (also known as a\nsyndicated facility) is a special type of loan where a group of lenders (the syndicate)\njointly provide a large loan to a company or an organization. Within the syndicate,\ndifferent agents assume various roles (e.g., participating bank, lead arranger, docu‐\nmentation agent, security agent, etc.). LSEG and similar data providers collect infor‐\nmation about syndicated loans from multiple sources, with SEC filings such as 8-Ks\nas the primary source.\nFinancial Named Entity Recognition | 131\nLet’s consider a scenario where your team is tasked with creating a dataset on syndi‐\ncated loans using a collection of SEC filings. Y our first step involves extracting data\nfrom the text, identifying various elements that characterize a syndicated facility, and\nthen organizing this information into a structured format. Let’s take the following\nexample of an SEC filing for a syndicated facility agreement given to an Australian\ncompany (the text below is quoted and highlighted from the SEC filing ):\nExhibit 10.1\nSYNDICATED FACILITY AGREEMENT\ndated as of  September 18, 2012\namong\nTHE MAC SERVICES GROUP PTY LIMITED  ,\nas Borrower,\nTHE LENDERS NAMED HEREIN,\nJ .P . MORGAN AUSTRALIA LIMITED  ,\nas Australian Agent and Security Trustee  ,\nJPMORGAN CHASE BANK, N.A.  ,\nas US Agent  ,\nJPMORGAN CHASE BANK, N.A.,\nas Issuing Bank\nand\nJPMORGAN CHASE BANK, N.A.,\nas Swing Line Lender\nJ.P . MORGAN SECURITIES LLC  ,\nas Lead Arranger  and  Sole Bookrunner\n…\nThe Borrower has requested the Lenders to extend credit, in the form of  Loans or Credits\n(as hereinafter  defined),  to the Borrower in an aggregate principal amount at any time\noutstanding not in excess of  AUD$300,000,000  .\nAs you can see, the text includes details regarding the borrower, lenders, and their\nrespective roles, as well as information about the facility type, amount, and currency.\nLeveraging NER, we can extract this information and construct a structured dataset.\nFor simplicity, let’s design a dataset with three tables: one to store facility data,\nanother for borrower details, and a third for lender information. Figure 4-2  shows\nwhat the Entity Relationship Model  of our dataset looks like. In the facility table, the\nfacility_id is an arbitrarily assigned unique identifier. In the borrower and lender\ntables, the facility_id is present as a foreign key, meaning that records will exist in\nthese tables only for facilities that exist in the facility table.\n132 | Chapter 4: Financial Entity Systems\nFigure 4-2. Entity Relationship Model (ERM) of the syndicated loan database\nThe result of a successful NER-based entity extraction would look like the data\npresent in Tables 4-1, 4-2, and 4-3.\nTable 4-1. Facility table\nfacility_id facility_date facility_amount facility_currency facility_type\n89763 2012-09-18 300,000,000 AUD Loans or Credits\nTable 4-2. Borrower table\nfacility_id borrower_name borrower_country\n89763 The Mac Services Group PTY Limited Australia\nTable 4-3. Lender table\nfacility_id lender lender_role\n89763 J.P. Morgan Australia Limited Australian Agent and Security Trustee\n89763 JPMorgan Chase Bank, N.A. US Agent\n89763 JPMorgan Chase Bank, N.A. Issuing Bank\n89763 JPMorgan Chase Bank, N.A. Swing Line Lender\n89763 J.P. Morgan Securities LLC Lead Arranger\n89763 J.P. Morgan Securities LLC Bookrunner\nCrucially, although an NER system can identify the occurrence of a specific entity in\nthe text, it typically does not link it to the corresponding real-world object. For exam‐\nple, if you refer back to Figure 4-1 , Google was labeled as COMPANY , but at this\npoint, we still don’t know which real-world company this is. To accomplish this task,\nan additional technique, called named entity disambiguation  (NED) or entity linking,\nis often used.\nFinancial Named Entity Recognition | 133",10007
60-How Does Named Entity Recognition Work.pdf,60-How Does Named Entity Recognition Work,"Many books treat NED as a separate problem from NER and dedicate a separate sec‐\ntion to it. However, for financial applications, linking the identified entities to their\nreal-world matches is essential. For this reason, I consider NED an additional step in\nthe NER process. Figure 4-3  demonstrates how NED works in conjunction with NER\nto link the recognized entity (COMPANY) to its specific real-world counterpart\n(Google).\nFigure 4-3. Named entity recognition and disambiguation\nIn NED, entities identified in the text are mapped to their unique real-world counter‐\nparts using a knowledge base . A knowledge base is a central repository that contains\ninformation about a vast array of subjects. These can be general-purpose or special‐\nized and may be public or private. For example, Wikipedia is a well-known public,\ngeneral-purpose knowledge base, while Investopedia serves a similar role but focuses\nspecifically on finance. Other notable examples include GeoNames, Wikidata, DBpe‐\ndia, and YAGO. Financial institutions and data vendors may also create proprietary\nknowledge bases tailored to their specific needs using their own data.\nHow Does Named Entity Recognition Work?\nIn this section, we will explore the various steps involved in building an NER system.\nAs illustrated in Figure 4-4 , the first step is data preprocessing, which ensures the data\nis structured, cleaned, harmonized, and ready for analysis. The second step, entity\nextraction, involves identifying the locations of all candidate entities. In the third step,\nthese candidate entities are categorized into their respective entity types. Subse‐\nquently, the quality and completeness of the extracted data and the performance of\nthe model are assessed in the evaluation step. Finally, the recognized entities can\n134 | Chapter 4: Financial Entity Systems\noptionally be linked to their unique real-world counterparts through the disambigua‐\ntion process.\nNote that NER is an iterative process. Once the model is evaluated, the modeler can\ndetermine if improvements in data preprocessing, model selection, or training tech‐\nniques are necessary to enhance the NER system’s performance.\nFigure 4-4. Named entity extraction and disambiguation process\nData preprocessing\nMethodologically speaking, NER is a subtask of the field of natural language process‐\ning (NLP) . As with most NLP tasks, NER achieves good results if applied to clean and\nhigh-quality data. A variety of NLP-specific data preparation techniques can be used\nwith NER. These include the following:\nTokenization\nTokenization is the process of breaking down the text into smaller units called\ntokens . Word tokenization breaks down the text into single words; for example,\n“Google invests in Renewable Energy” becomes [“Google” , “invests” , “in” , “Renew‐\nable” , “Energy”]. Sentence tokenization breaks down text into smaller individual\nsentences; for example, “Google invests in Renewable Energy” gets converted into\n[“Google” , “invests in” , “Renewable Energy”].\nStop word removal\nStop words are common and frequent words that have very little or no value for\nmodeling or performance. For example, the English words “is, ” “the, ” and “and”\nare often classified as stop words. In most NLP tasks, including NER, stop words\nare filtered out.\nCanonicalization\nIn NLP , the form and conjugation of the word are often of no value. For example,\nthe words “invest, investing, invests, invested” convey the same type of action;\ntherefore, they can all be mapped to their base form, i.e., “invest. ” The process of\nmapping words in a text to their root/base forms is known as canonicalization .\nTwo types of canonicalization techniques are often used: stemming  and lemmati‐\nzation . Stemming is a heuristic technique that involves removing affixes from a\nword to produce its stem. This method is quick and efficient but can produce\nFinancial Named Entity Recognition | 135\nimprecise results, as it often leads to over-stemming (reducing words too much)\nor under-stemming (not reducing them enough). To address the limitations of\nstemming, lemmatization techniques are often used. Using vocabulary and mor‐\nphological analysis, a lemmatizer tries to infer the dictionary form (lemma) of\nwords based on their intended meaning. There are several common lemmitiza‐\ntion techniques:\nLowercase conversion\nThis consists of converting all words to lowercase.\nSynonym replacement\nThis technique involves replacing words with one of their synonyms.\nContractions removal\nContractions are words written as a combination of a shortened word with\nanother word. Contraction removal consists of transforming the words in a\ncontraction into their full-length form, e.g., “she’ d invest in stocks” becomes\n“she would invest in stocks. ”\nStandardization (normalization) of date and time formats\nFor example, dates are converted to YYYYMMDD format, and timestamps\nto YYYMMDDHH24MMSS.\nNER is highly sensitive to data preprocessing, where even minor\nchanges can significantly impact the results. It’s essential to care‐\nfully assess the consequences of each preprocessing step. For exam‐\nple, converting all words to uppercase could disrupt rules dictating\nentity characteristics, such as the expectation that country names\nbegin with uppercase letters.\nEntity extraction\nDuring entity extraction, an algorithm is applied to a corpus of clean text to detect\nand locate candidate entities. In this step, the NER system designer should know\nwhich type of entities they are looking for in the text. The extraction process is a seg‐\nmentation  problem, where the goal is to find all meaningful segments of text that rep‐\nresent an entity. In this case, the name “Bank of England” needs to be identified as a\nsingle entity, even if the word “England” could also be a meaningful entity.\nSince the goal of this step is to locate references to an entity, it might produce correct\nyet imperfect results. For example, unnecessary tokens might be included, as in\n“Banking giant JP Morgan Chase” . In other cases, some tokens might be omitted, such\nas missing “Inc. ” in “JP Morgan Chase Inc. ” or “Michael” in “Michael Bloomberg. ”\n136 | Chapter 4: Financial Entity Systems\n1Ashitha Shivaprasad and Sherin Elizabeth Varghese, “Gold Climbs Over 1% After Fed Signals End of Rate\nHikes” , Reuters (December 2023).Entity categorization\nOnce all candidate entities in the text have been extracted, the next step is to accu‐\nrately map each valid entity to its corresponding entity type. For example, “Bank of\nAmerica” should be classified as a company (COMP), “United States” as a country\n(LOC), “Bill Gates” as a person (PER), and any other token should be labeled as “O”\nto indicate that it is not a relevant entity.\nThe main challenge in this step is language ambiguity. For example, the words bear\nand bull are frequently used to indicate two species of animals. However, in financial\nmarkets, the word bull is often used to indicate an upward trend in the market, while\nbear describes a receding market.\nAnother example involves similar names that could refer to different entities. For\ninstance, “JP Morgan” might describe the well-known financial institution JPMorgan\nChase, but it could also refer to John Pierpont Morgan, the American financier who\nfounded J.P . Morgan Bank.\nTo illustrate the NER process up to this step, we should be able to take a text such\nas…\nGold prices rose more than 1% on Wednesday after the U.S. Federal Reserve flagged an\nend to its interest rate hike cycle and indicated possible rate cuts next year.1\n…and produce a structured categorization, as illustrated in Table 4-4 . In this example,\nfive types of entities were extracted: commodity (CMDTY), variable (V AR), national‐\nity (NAL), organization (ORG), and miscellaneous (O).\nTable 4-4. Outcome of entity extraction and categorization of a news title\nentity_type text\nCMDTY Gold\nVAR Prices\nNAL U.S.\nORG Federal Reserve\nO rose more than 1% on Wednesday after the\nO flagged  an end to its interest rate hike cycle and indicated possible rate cuts next year.\nFinancial Named Entity Recognition | 137\nEntity disambiguation\nIf you aim to extend beyond merely extracting entities, which is crucial in numerous\nfinancial applications, you must proceed to disambiguate the identified and validated\nentities. This involves establishing a link between each correctly recognized entity in\nthe data and its unique real-world counterpart.\nThe entity disambiguation step can present some challenges. One major issue is name\nvariations. For example, a company can be mentioned in multiple ways, such as Bank\nof America, Bank of America Corporation, BoA, or BofA. Entity ambiguity is another\nchallenge. For example, Bloomberg can refer to the company Bloomberg L.P . or its\nCEO, Michael Bloomberg. Finally, the knowledge bases used to disambiguate the\nentities might not always contain up-to-date information on all specific or novel enti‐\nties that emerge in the market.\nIf we take our example, illustrated in Table 4-4 , adding entity disambiguation would\nresult in real-world references, as illustrated in Table 4-5 . This example is illustrative,\nand more precise references could be used. For instance, the spot and future prices\ncould be linked to a specific commodity exchange such as CME.\nTable 4-5. Outcome of an entity extraction, categorization, and disambiguation of a\nnews title\nentity_type text reference\nCMDTY Gold Chemical element with symbol AU\nVAR Prices Spot price and future price on commodity\nexchanges\nNAL U.S. Country in North America\nORG Federal Reserve Central Bank of the United States of America\nO rose more than 1% on Wednesday after the\nO flagged  an end to its interest rate hike cycle and\nindicated possible rate cuts next year.\nEvaluation\nEvaluating the performance of NER systems in terms of their accuracy and efficiency\nis the last step in NER. An accurate NER system should detect and recognize all valid\nentities, correctly assign them to the appropriate entity types, and optionally link\nthem to their real-world counterparts. Besides analytical performance, NER systems\nmust also be assessed based on their computational efficiency, which includes run‐\ntime, memory consumption, storage requirements, CPU usage, and scalability to\nhandle large-scale financial applications with millions of records.\n138 | Chapter 4: Financial Entity Systems\nTo compute performance metrics for an NER system, four kinds of results are\nneeded:\nFalse positive (FP)\nAn instance incorrectly identified as an entity by the NER system\nFalse negative (FN)\nAn instance that the NER system fails to classify as an entity, even though it is an\nactual entity in the ground truth\nTrue positive (TP)\nAn instance correctly identified as an entity by the NER system\nTrue negative (TN)\nAn instance correctly identified as a nonentity, consistent with the ground truth\nThese four values are often represented in a special tabular format known as a confu‐\nsion matrix , as illustrated in Figure 4-5 .\nFigure 4-5. Confusion matrix of NER\nTo compute the confusion matrix of a given NER model, you need\nto have a ground truth dataset with the actual values. The ground\ntruth is mainly used for model training, where predicted values are\ncompared against their true counterparts. This is usually a major\nchallenge in NER, especially if you have big datasets. Y ou, as a\nfinancial data engineer, will play a primary role in building and\nmaintaining a labeled database to be used as the ground truth for\nNER systems.\nUsing the confusion matrix, the following performance evaluation metrics can be\ncomputed:\nAccuracy\nAccuracy measures the overall performance of the NER model and answers the\nquestion, “Out of all the classifications that were made, how many were correct?”\nIn NER, this can be used as a measure of the ability of the model to distinguish\nFinancial Named Entity Recognition | 139\nbetween what is an entity from what is not. Accuracy works well as an evaluation\nmetric if the cost of false positives and false negatives is more or less similar. This\ncan be represented as a formula as follows:\nAccuracy =TP+TN\nTP+TN+FP+FN\nPrecision\nPrecision measures the proportion of true positives to the number of all positives\nthat the model predicted. It answers the question, “Of all instances that were clas‐\nsified as true positives, how many are correct?” In NER, this could be interpreted\nas the percentage of tokens (words or sentences) that were correctly recognized\nas entities out of all the tokens that are actually entities. A low precision value\nwould indicate that the model is not good at avoiding false positives. Precision is\na good measure when the cost of false positives is quite high. This can be repre‐\nsented as a formula as follows:\nPrecision =TP\nFP+TP\nRecall\nRecall measures the true positive rate of the model by answering the question,\n“Out of all instances that should be classified as true positives, how many were\ncorrectly classified as such?” Low recall indicates that the model is not good at\navoiding false negatives. The recall is a good measure to use when the cost of a\nfalse negative is high. This can be represented as a formula as follows:\nRecall =TP\nTP+FN\nF1 score\nThe F1 score is a harmonic mean of precision and recall. It is widely used when\nthe class representation in the data is imbalanced or when the cost of both false\npositives and false negatives is high. In financial NER, this is likely to be the case,\nas the vast majority of data tokens are not entities and the cost of mistakes is\nhigh. This can be represented as a formula as follows:\nF1score =2 *Recall *Precision\nRecall +Precision\n140 | Chapter 4: Financial Entity Systems",13875
61-Approaches to Named Entity Recognition.pdf,61-Approaches to Named Entity Recognition,"2Have a look at the confusion matrix Wikipedia page  for more details.Additional evaluation metrics can be derived from the confusion matrix.2 In many\nresearch papers on NER, the F1 score is used as the default metric. However, I highly\nrecommend that you compute all four metrics to have an overview of your NER per‐\nformance from different angles. For example, a low precision might tell you that you\nhave a rule in your model that easily classifies a token as an entity. Similarly, a low\nrecall might tell you that your model hardly classifies an entity as such; maybe your\nrules are too strict.\nNow that you understand the necessary steps for developing an NER system, let’s\nexplore the main modeling approaches that can be employed to build and operation‐\nalize an NER system.\nApproaches to Named Entity Recognition\nNumerous NER methods and techniques have been proposed in academic literature\nand by market participants. Frequently, these solutions are tailored or fine-tuned to\nsuit particular domains. In this book, I will offer a taxonomy of seven modeling\napproaches: lexicon-based, rule-based, feature-engineering-based machine learning,\ndeep learning, large language models, wikification, and knowledge graphs.\nOne thing to keep in mind is that these approaches aren’t necessarily mutually exclu‐\nsive. In many cases, especially when building complex NER systems, developers\nemploy a combination of techniques. In the upcoming sections, I will discuss each of\nthe seven approaches with some level of detail.\nLexicon/dictionary-based approach\nThis approach works by first constructing a lexicon or dictionary of vocabulary using\nexternal sources and then matching text tokens with entity names in the dictionary. A\nfinancial dataset, like reference or entity datasets, can function as a lexicon. Lexicons\nare flexible and can be tailored to any domain. For this reason, this approach could be\na good choice for domain-specific tasks where the universe of entities is small or con‐\nstant, or evolves slowly. Examples include sector names, financial instrument classes,\nand company names. Other examples might include accounting or legal texts, which\nrely on standard principles and formal language that doesn’t change much over time.\nLexicons serve a dual purpose in NER. They can function as the primary extraction\nmethod or complement other techniques, as I’ll illustrate later. Furthermore, a lexi‐\ncon can be used for entity disambiguation. For example, a lexicon mapping company\nnames to their identities can handle both recognition and disambiguation tasks.\nFinancial Named Entity Recognition | 141\nThe main advantages of lexicons are processing speed and simplicity. If you have a\nlexicon, then the extraction process can be viewed as a simple dictionary lookup.\nKeep in mind, however, that lexicons cannot recognize new entities that are not in the\ndictionary (e.g., new types of financial instruments). Additionally, lexicons are highly\nsensitive to the quality of data preprocessing and the presence of errors. As they can‐\nnot deal with exceptions or erratic data types, lexicons tend to guarantee better per‐\nformance on high-quality data. Finally, lexicons might produce false positives if the\ncontext is not taken into account. For example, a stock ticker lexicon might contain\nthe symbol AAPL for Apple, Inc. However, the abbreviation AAPL may also refer to\n“ American Association of Professional Landmen” or “ American Academy of Psychia‐\ntry and the Law. ”\nRule-based approach\nThe rule-based approach employs a set of rules, created either manually or automati‐\ncally, to recognize the presence of an entity in text. For example:\n•Rule N.1:  the number after currency symbols is a monetary value, e.g., $200.\n•Rule N.2:  the word after Mrs. or Mr. is a person’s name.\n•Rule N.3:  the word before a company suffix is a company name, e.g., Inc., Ltd.,\nInc., Incorporated, Corporation, etc.\n•Rule N.4:  alphanumeric strings could be security identifiers if they match the\nlength of the identifier and can be validated with a check-digit method.\nSimilar to the lexicon approach, rule-based methods tend to be domain-specific,\nmaking their transferability to other domains challenging. They are also particularly\nsensitive to data preprocessing issues, exceptions, and textual ambiguity, which can\nresult in an large set of rules. Complex rule-based approaches are difficult to main‐\ntain, hard to understand, and can be slow to run. Therefore, they are recommended\nin cases where the language is either simple or subject to formal standards, such as\naccounting, annual reports, or SEC filings.\nFeature-engineering machine learning approach\nLexicon- and rule-based methods commonly face challenges when complex data pat‐\nterns need to be identified for accurate NER. In such cases, modeling presents a com‐\npelling alternative. One prominent method involves feature-engineering machine\nlearning, wherein a multiclass classification model is trained to predict and categorize\nwords in a text. Being supervised, this approach requires the existence of labeled data\nfor training.\n142 | Chapter 4: Financial Entity Systems\n3For a detailed discussion on how to design features for NER, see Lev Ratinov and Dan Roth’s article, “Design\nChallenges and Misconceptions in Named Entity Recognition” , in Proceedings of the Thirteenth  Conference on\nComputational Natural Language Learning (CoNLL-2009) : 147–155, and Rahul Sharnagat’s “Named Entity\nRecognition: A Literature Survey” , Center For Indian Language Technology  (June 2014): 1–27. \n4To learn more about context aggregation, see the method proposed in Hai Leong Chieu and Hwee Tou Ng’s\n“Named Entity Recognition with a Maximum Entropy Approach” , in Proceedings of the Seventh Conference on\nNatural Language Learning at HLT-NAACL 2003 : 160–163.\n5To learn more about this advanced technique, see Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong\nZhang’s “Named Entity Recognition Through Classifier Combination” , in Proceedings of the Seventh Confer‐\nence on Natural Language Learning at HLT-NAACL  2003: 168–171.To apply supervised machine learning, the modeler must select, and in most cases\nengineer, a set of features for each token.3 To give a few examples, features can be\nsomething like the following:\n•Part-of-speech tagging (noun, verb, auxiliary, etc.)\n•The word type (all-capitalized, all-digits, alphanumeric, etc.)\n•Whether it’s a courtesy title (Mr., Ms., Miss, etc.)\n•The word match from a lexicon or gazetteer (e.g., San Francisco: City in\nCalifornia)\n•Whether the previous word is a courtesy title\n•Whether the word is a currency symbol ( ¥, $, etc.)\n•Whether the previous word is a currency symbol\n•Whether the word is at the beginning or end of the paragraph\n•Context aggregation features that capture the surrounding context of a word\n(e.g., the previous and subsequent n words)4\n•Prediction of another ML classifier5\nOnce all relevant features have been carefully engineered, a variety of algorithms can\nbe used. Among the most popular choices are logistic regression, Random Forests,\nConditional Random Fields, Hidden Markov Models, support vector machines, and\nMaximum Entropy Models.\nFeature-based models offer several advantages, such as speed of training and feature\ninterpretability. However, several challenges might arise, such as the need for finan‐\ncial domain expertise, the complexity of feature engineering, difficulty modeling non‐\nlinear patterns, and the inability to capture complex contexts for longer sentences.\nThis is where more advanced machine learning techniques, such as deep learning,\ncome into play, which I will introduce next.\nFinancial Named Entity Recognition | 143\n6For a good survey of the use of deep learning in NER, see Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li’s\n“ A Survey on Deep Learning for Named Entity Recognition” , IEEE Transactions on Knowledge and Data Engi‐\nneering  34, no. 1 (January 2020): 50–70.\n7A good read on the use of Transformers for NER is offered by Cedric Lothritz, Kevin Allix, Lisa Veiber, Jac‐\nques Klein, and Tegawendé François D. Assise Bissyande in “Evaluating Pretrained Transformer-Based Mod‐\nels on the Task of Fine-Grained Named Entity Recognition” , in Proceedings of the 28th International\nConference on Computational Linguistics  (2020): 3750–3760.Deep learning approach\nIn recent years, deep learning (DL) has established itself as the state-of-the-art\napproach for NER.6 DL is a prominent subfield of machine learning that works by\nlearning a hierarchical representation of data via a neural network composed of mul‐\ntiple layers and a set of activation functions. A neural network can be thought of as a\ncomputational graph where each layer of nodes performs nonlinear function compo‐\nsitions of simpler functions produced at the previous layer. Interestingly, this process\nof repeated composition of functions has significant modeling power, which has con‐\ntributed to the success of deep learning in solving complex problems.\nThere are several advantages to applying DL to NER. First, the modeler doesn’t need\nto worry about the complexities involved in feature engineering, as deep neural net‐\nworks are capable of learning and extracting features automatically. Second, DL can\nmodel a large number of complex and nonlinear patterns in the data. Third, neural\nnetworks can capture long-range correlations and context dependencies in the text.\nFourth, DL offers high flexibility through network specifications (depth, width, lay‐\ners, hyperparameters, etc.), which allows the modeling of a large number of domain-\nspecific problems on large datasets.\nA wide variety of network structures exist within the DL field. The ones that have\nshown remarkable success in NER-related tasks are Recurrent Neural Networks and\ntheir variants, such as Long Short-Term Memory, Bidirectional Long Short-Term\nMemory, and, most recently, attention mechanism-based models, such as\nTransformers.7\nDeep learning is a powerful and advanced technique. However, I advise against using\nit by default for your NER task. DL models are hard to interpret and may require spe‐\ncial hardware (e.g., a graphics processing unit, or GPU) and time to train. Try a sim‐\nple approach first. If it doesn’t work, then use more complex techniques.\nGiven the remarkable performance of complex models like DL in text-related tasks,\ndevelopment has extended to even more sophisticated models, such as large language\nmodels (LLMs), which I’ll explore next.\n144 | Chapter 4: Financial Entity Systems\nLarge language models\nA large language model (LLM) is an advanced type of generative artificial intelligence\nmodel designed to learn and generate human-like text. Most LLMs leverage a deep\nlearning architecture known as a Transformer, proposed in the seminal paper “ Atten‐\ntion Is All Y ou Need” . Techniques such as Reinforcement Learning from Human\nFeedback (RLHF) are often used to align LLMs to human preferences. LLMs may also\nutilize other techniques such as transfer learning, active learning, ensemble learning,\nembeddings, and others.\nLLMs are quite massive, often trained on vast amounts of text data and comprising\nmillions or even billions of parameters. General-purpose LLMs are commonly known\nas foundational models , highlighting their versatility and wide-ranging applicability\nacross numerous tasks. Prominent examples include OpenAI’s Generative Pre-\ntrained Transformer (GPT) series, such as GPT-3 and GPT-4, Google’s BERT (Bidir‐\nectional Encoder Representations from Transformers), Meta’s Llama, Mistral, and\nClaude. LLMs are capable of performing a wide range of general-purpose natural lan‐\nguage processing tasks, including text generation, summarization, entity recognition,\ntranslation, question answering, and more.\nLLMs can also be fine-tuned to specific domains. Fine-tuning is the process of\nretraining a pre-trained LLM on a domain-specific dataset, allowing it to adapt its\nknowledge and language understanding to suit better the terminology, vocabulary,\nsyntax, and context of the target domain. For example, the FinBERT  is a domain-\nspecific adaptation of the BERT model, fine-tuned specifically for the financial\ndomain. It is trained on a vast amount of financial texts, such as news articles, earn‐\nings reports, and financial statements, to understand and process financial language\nand terminology effectively. FinBERT can be used for various tasks in the financial\ndomain, including sentiment analysis, named entity recognition, text classification,\nand more.\nLLMs can be a powerful technique for financial NER. This is because they are able to\nunderstand and process complex and domain-specific language, recognizing entities\nsuch as financial instruments, accounting, and regulatory terms, as well as company\nand person names within the context of financial markets. For example, an LLM may\nbe able to distinguish “ Apple Inc. ” as a tech company listed on NASDAQ from the\nword “apple” as a fruit, using contextual clues from surrounding text. They can also\nidentify financial terms such as “S&P 100, ” “NASDAQ Composite, ” and “Dow Jones\nIndustrial Average” as indexes rather than just random phrases. Similarly, LLMs may\nbe able to distinguish between terms like “call option” and “put option, ” understand‐\ning that they refer to specific types of financial derivatives, despite their similar\nstructure.\nFinancial Named Entity Recognition | 145\nCrucially, while LLMs may show outstanding performance in many financial lan‐\nguage processing tasks, they can still encounter challenges with specialized and evolv‐\ning financial terminology. For example, financial terms such as “interest rate swap”\n(CDS), “collateralized debt obligation” (CDO), and “mortgage-backed securities”\n(MBS) necessitate a deep understanding of financial instruments and their contexts.\nSimilarly, terms such as “bonds” and “equity” have completely different meanings in\nfinance than in the general sense. Furthermore, terms like “bitcoin, ” “blockchain, ”\n“cryptocurrency, ” and “DeFi” (decentralized finance) have emerged relatively recently\nand require continuous model updates to stay current.\nRetrieval-Augmented Generation\nRetrieval-augmented generation (RAG) is an advanced technique employed to\nenhance the factual grounding, contextual relevance, and response accuracy of lan‐\nguage models, especially in specialized domains like finance. RAG operates by retriev‐\ning relevant information from external sources, such as databases or documents, and\nincorporating this data into a language model’s input prompt, thereby providing addi‐\ntional context to produce more accurate and contextually relevant responses.\nIn finance-specific tasks like financial NER, RAG enhances accuracy by disambiguat‐\ning entities, staying up-to-date with rapidly changing information, and integrating\ndomain-specific knowledge from financial documents and databases. This capability\nmakes RAG particularly effective for identifying financial entities and handling com‐\nplex jargon. Crucially, the success of RAG largely depends on the availability of relia‐\nble external data sources, highlighting its foundation in data engineering.\nAnother major challenge with LLMs is hallucination , which happens when an LLM\ngenerates irrelevant, factually wrong, or inconsistent content. Interpretability and\ntransparency represent additional challenges, particularly in finance, where regula‐\ntory compliance and trust in decision-making are crucial.\nWikification\nWikification is an entity disambiguation technique that links recognized named enti‐\nties to their corresponding real-world Wikipedia page. Figure 4-6  illustrates this tech‐\nnique through an example. In the first step (entity recognition), two entities (Seattle\nand Amazon) are identified. In the next step, the identified entities are linked to their\nunique matching Wikipedia page.\n146 | Chapter 4: Financial Entity Systems\nFigure 4-6. Wikification  process\nSeveral wikification techniques have been proposed, the majority of which utilize\nsimilarity metrics to determine which Wikipedia page is most similar to the recog‐\nnized entity. One prominent implementation was first presented in Silviu Cucerzan’s\ngroundbreaking work . Cucerzan proposed a knowledge base that incorporates the\nfollowing elements:\nArticle entity/concept\nMost Wikipedia articles have an entity/concept associated with them.\nEntity class\nPerson, location, organization, and miscellaneous.\nEntity surface forms\nThe terms used to reference the entity in text.\nContexts\nTerms that co-occur or describe the entity.\nTags\nSubjects the entity belongs to.\nFor example, the term Berkeley  can refer to a large number of real-world entities,\nincluding places, people, schools, and hotels. Assume we are interested in identifying\nthe University of California, Berkeley. In this case, the entity type is school or univer‐\nsity; the context could be California, a public university, or a research university; tags\nmight include education, research, science, and others; and the entity surface form\nmight be simply Berkeley.\nAn entity is disambiguated by first identifying its surface form. Subsequently, two\nvector representations that encode contexts and tags are constructed: one for the\nWikipedia context that occurs in the document and another for the Wikipedia entity.\nFinancial Named Entity Recognition | 147\nFinally, the assignment to a Wikipedia page is made via a process that maximizes the \nsimilarity between the document and entity vectors.\nKnowledge graphs\nKnowledge graphs have become an essential technique in internet-based information\nsearch and have been widely applied in entity disambiguation. There isn’t yet a clear\ndefinition of what a knowledge graph is . Still, it basically involves gathering different\ntypes of facts, knowledge, and content from many sources, organizing them into a\nnetwork of nodes and links, and using it to provide more information to users upon\nsubmitting a search query. In other words, a knowledge graph can be thought of as a\nnetwork of real-world entities—i.e., persons, locations, materials, events, and organi‐\nzations—related together via labeled directed edges. Figure 4-7  presents a simple\nillustrative example of a knowledge graph around the company Dell Technologies.\nThe graph illustrates Dell Technologies and several related entities, such as its CEO,\nMichael Dell, and its supplier, Intel Corporation.\nFigure 4-7. Illustrative example of a knowledge graph\nThe power of knowledge graphs stems from their extreme flexibility, which allows\nthem to encompass a wide range of elements and interactions. This, in turn, can\nimprove search results and reveal hidden data links that might otherwise go undetec‐\nted using more traditional approaches.\n148 | Chapter 4: Financial Entity Systems",19068
62-Entity Resolution Described.pdf,62-Entity Resolution Described,"Knowledge graphs have been proposed as an advanced approach to entity disambigu‐\nation within NER systems. A well-known implementation is the Accurate Online Dis‐\nambiguation of Named Entities , or AIDA. It constructs a “mention-entity” graph,\nwhere nodes represent mentions of entities found in the text, as well as the potential\nentities these mentions could refer to. These nodes are connected with weighted links\nbased on the similarity between the context of the mention and the context of each\nentity. This helps the system figure out which entity the mention is most likely refer‐\nring to. Additionally, AIDA connects the entities themselves with each other using\nweighted links. This allows AIDA to capture coherence among entities within the\ngraph, aiding in the disambiguation process.\nAIDA utilizes the densest subgraph algorithm  to search the mention-entity graph. The\ndensest subgraph algorithm helps identify the most densely connected subgraph\nwithin the larger graph. In the context of AIDA, this subgraph represents the set of\nmentions and entities that are most closely related to each other based on their con‐\nnections and similarities. By identifying this densest subgraph, AIDA can determine\nthe most coherent and relevant set of mentions and entities for a given context.\nTwo challenges may arise when finding such dense subgraphs. First, you need a relia‐\nble definition of the notion of a dense subgraph that ensures coherence and context\nsimilarity. Second, dense-subgraph problems are computationally expensive and\nalmost NP-hard problems. This means that a heuristic or efficient algorithm is\nneeded to guarantee a fast graph search to find the optimal dense subgraph.\nNamed Entity Recognition Software Libraries\nPractitioners in industry and academia have created several software tools for NER.\nSeveral open source tools are available, including spaCy, NLTK, OpenNLP , CoreNLP ,\nNeuroNER, polyglot, and GATE.\nIn addition to open source solutions, financial institutions and data providers build\nproprietary NER solutions. The most famous example is RavenPack analytics, which\nwe discussed earlier in this chapter. Another prominent example is NERD (Named\nEntity Recognition and Disambiguation) , developed by S&P Global’s AI accelerator,\nKensho. NERD is one of the few entity recognition and disambiguation tools tailored\nspecifically for financial entities. NERD takes a text document as input and identifies\nmentions of named entities such as companies, organizations, and people. It also\nlinks the extracted entities to their real-world entries in the S&P Global comprehen‐\nsive Capital IQ database.\nFinancial Named Entity Recognition | 149\n8One thing to keep in mind is that AutoML may be too generic to deal with the peculiarities of NER. For more\non this issue, see Matteo Paganelli, Francesco Del Buono, Marco Pevarello, Francesco Guerra, and Maurizio\nVincini’s “ Automated Machine Learning for Entity Matching Tasks” , in the Proceedings of the 24th Interna‐\ntional Conference on Extending Database Technology (EDBT 2021),  Nicosia, Cyprus, March 23–26, 2021: 325–\n330.FactSet provides a Natural Language Processing API  that can be used to recognize\nand locate a wide range of entities in structured and semi-structured texts. This\nincludes companies, people, locations, health conditions, drug names, numbers,\nmonetary values, and dates. In addition to NER, the API allows entity disambiguation\nby finding the best matching FactSet identifiers for companies and people found in\nthe text.\nAnother tool that might be used for NER is Automated Machine Learning  (AutoML).\nThese solutions offer simple and user-friendly interfaces to automatically choose,\ntrain, and tune the best ML model/algorithm for a particular problem. One of the\nmain advantages of AutoML is that it allows nonexperts to use sophisticated ML\nmodels. Examples of AutoML tools include open source libraries such as Auto-\nsklearn, AutoGluon, AutoKeras, and H20 AutoML, as well as cloud-based managed\nsolutions such as Google AutoML and Amazon Sagemaker.8\nAWS offers a specialized NLP AutoML service called Amazon Comprehend. Com‐\nprehend already has trained NER capabilities that you can immediately interact with,\nand it also offers the option to customize an NER system to your specific task (e.g.,\ndetecting financial entities). In addition, AWS introduced Bedrock, a managed ser‐\nvice that allows users to build and fine-tune generative AI applications with founda‐\ntion models.\nFinancial Entity Resolution\nOnce entities have been recognized and identified, a system should be available\nwhereby the data associated with a unique entity in one dataset can be matched with\ndata held in another dataset for the same unique entity. This process is very common\nin finance and is known as entity resolution (ER). In this section, you will learn what\nER is and why it is important in finance. Then, you will learn how ER systems work\nand the different approaches to ER. Finally, I will present a list of software libraries\nand tools available for performing ER.\n150 | Chapter 4: Financial Entity Systems",5174
63-The Importance of Entity Resolution in Finance.pdf,63-The Importance of Entity Resolution in Finance,"Entity Resolution Described\nEntity resolution, also known as record linkage or data matching, refers to the pro‐\ncess of identifying and matching records that refer to the same unique entity within a\nsingle data source or across multiple sources, particularly when a unique identifier is\nunavailable. When ER is applied to a single dataset, it is often done to identify and\nremove duplicate records ( record deduplication ). When it is applied to multiple data‐\nsets, the goal is to match and aggregate all relevant information about an entity\n(record linkage ).\nMathematically, let’s represent two data sources as A and B and denote records in A\nas a and records in B as b. The set of records that represent identical entities in A and\nB can be written as:\nM=a,b;a=b;a∈A;b∈B\nAnd the set of records that represent distinct entities as:\nU=a,b;a≠b;a∈A;b∈B\nAs we will see later in this chapter, the main objective of an ER system is to distin‐\nguish the set of matches M from the set of non-matches U.\nThe Importance of Entity Resolution in Finance\nEntity resolution is a common practice and represents a main challenge in the finance\ndomain. As a financial data engineer, you will likely encounter the need to develop an\nER system. Various industry initiatives have been established to address the financial\nER problem. For instance, the Financial Entity Identification and Information Inte‐\ngration (FEIII) Challenge  was initiated to create methodologies for aligning the vari‐\nous financial entity identification schemes and identifiers. Despite these efforts, the\nproblem remains unresolved for several reasons, which I will outline next.\nFinancial Entity Resolution | 151\nMultiple identifiers\nAs you learned in Chapter 3 , financial markets rely on a large number of data identi‐\nfication systems, each developed with a specific goal, structure, and scope. As such, it\nis typical that different financial datasets come with different identifiers. One finan‐\ncial identifier is typically sufficient to identify and distinguish unique entities when\nworking with a single dataset. However, in many cases, people need to work with\nmultiple datasets at once. For example, financial analysts or machine learning experts\nmight require a sample of data and features that span multiple data sources. To this\nend, different datasets might need to be merged via an ER system to create a compre‐\nhensive dataset for the analysis.\nFigure 4-8  illustrates a basic ER example where two datasets with different identifiers\nare matched. The table on the left contains six records identified by identifier B, while\nthe table on the right holds data for the same records but uses identifier A. ER is per‐\nformed by matching identifiers A and B, as depicted by the arrows. The resulting\nidentifier mapping is as follows: 111 maps to BBB, 333 maps to AAA, and 222 maps\nto CCC.\nFigure 4-8. Entity resolution in the presence of two different  identifiers\nKeep in mind that if the datasets you want to merge use the same data identifier, then\nthe task becomes a simple database join operation, and there would be no need to\ndevelop an ER system.\nMissing identifiers\nIn some cases, a financial dataset may lack a proper identifier or may have an arbi‐\ntrary identifier that does not match the specific one you need. For instance, data gen‐\nerated from nonregulated or decentralized markets, such as OTC, may not include\nappropriate data identifiers. A stock prices dataset might use the stock ticker as an\nidentifier, while you may require the ISIN. Another common scenario involves agents\nengaged in financial activities who may intentionally obscure their identities to\n152 | Chapter 4: Financial Entity Systems\ncommit fraud. In such cases, an ER system is essential to identify entities based on the\navailable data attributes. Figure 4-9  illustrates the process of ER where identifiers are\nassigned to an unidentified dataset. The table on the right displays multiple features\nwithout entity identifiers. Using ER, records are mapped to their corresponding iden‐\ntifiers, as indicated by the arrows.\nFigure 4-9. Entity resolution with unidentified  data\nEntity Resolution for Fraud Detection and Identify Verification\nOne of the most important applications of ER in finance is fraud detection and iden‐\ntity verification. ER can help identify financial records that can be linked to the same\nreal-life person or company using features such as name, email, bank account, coun‐\ntry code, address, phone number, etc. Additionally, ER can identify anomalous activi‐\nties when criminals attempt to conceal their identity and unlawful intentions by\nomitting crucial information or presenting it in an inaccurate manner.\nOne of the most common types of financial crimes is money laundering , an activity\nthat makes illegally generated money look as if it comes from a legitimate source. A\nvariety of money laundering schemes exist, and they continue to emerge over time. A\ntypical example involves the same individual appearing as the owner of numerous\ncompanies, some of which provide no actual service but only shift money between\ndifferent ends.\nIn banking, it is a common practice to verify the identity of an applicant when open‐\ning a bank account or conducting a financial transaction to ensure they are who they\nclaim to be. This process is known as know your customer  (KYC) and is aimed at pre‐\nventing a pervasive form of fraud known as identity fraud . ER can be used for KYC to\nidentify potential fraudsters who use different identities, email addresses, phone num‐\nbers, and other patterns to open new bank accounts and conduct financial\ntransactions.\nFinancial Entity Resolution | 153\nData aggregation and integration\nInformation regarding various operations and activities within financial institutions\nis typically decentralized and scattered across multiple divisions. Data integration\nrefers to the process of combining these multiple data sources to provide a compre‐\nhensive view of the organization. This process is highly relevant for financial institu‐\ntions for purposes such as regulatory reporting and risk monitoring. In Chapter 5 ,\nyou will learn more about the importance of data aggregation in the financial sector.\nTo facilitate data integration, an ER system would be needed to match data across the\ndifferent units and divisions within a financial institution. Figure 4-10  provides a\nsimple example illustrating this process. In this scenario, data originates from two\ndivisions, 1 and 2. The data from each division is initially mapped to a common iden‐\ntifier before being merged into a single unified dataset.\nFigure 4-10. Entity resolution for data aggregation\n154 | Chapter 4: Financial Entity Systems\nData deduplication\nA frequent problem with financial data is the presence of duplicates, i.e., multiple\nrecords that convey the same information about an entity. Duplicate records are often\nencountered when using nonstandard identifiers such as person or company names,\nwhich can be recorded with multiple variations. Chapter 5  will have a dedicated sec‐\ntion detailing the problem of financial data duplicates.\nThe process of identifying and removing data duplicates is called data deduplication.\nSince deduplication requires matching similar entities in the same dataset, it can be\ntreated as an ER problem. Figure 4-11  shows an example illustrating this process. The\ntable on the left contains two duplicate instances, (1,2) and (7,8). Using ER, it is possi‐\nble to identify these duplicates and perform data deduplication, as shown in the table\non the right.\nFigure 4-11. Entity resolution for data deduplication\nFinancial Entity Resolution | 155",7776
64-How Does Entity Resolution Work.pdf,64-How Does Entity Resolution Work,"How Does Entity Resolution Work?\nA typical ER process involves five iterative steps, which I illustrate in Figure 4-12 . In\nthe first step, preprocessing  is applied to the input datasets to ensure their high quality\nfor the task. The second step, blocking , is often required to reduce computational\ncomplexity when matching large datasets. In the third step, candidate pair records are\ngenerated  and compared  using a selected methodology. Successively, comparisons are\nclassified  into matches, non-matches, or possible matches. Finally, in the fifth step, the\ngoodness of the matching process is evaluated . In the next few sections, we will\nexplore each of these five steps in detail.\nFigure 4-12. Entity resolution process\nData preprocessing\nER is highly sensitive to the quality of the input datasets. Therefore, before starting\nthe matching process, it is crucial that the necessary rules are established and applied\nfor quality assessment and data standardization. Such rules are particularly important\nfor the data fields that will be used in the matching process, especially identifier fields.\nTable 4-6  illustrates an example where three datasets store data about the same finan‐\ncial entity using different formatting styles.\n156 | Chapter 4: Financial Entity Systems\n9For a good read on this topic, please see Erhard Rahm and Hong Hai Do’s article, “Data Cleaning: Problems\nand Current Approaches” , IEEE Data Eng. Bull . 23, no. 4 (December 2000): 3–13.\nTable 4-6. Nonstandardized data representations\nEntity name Headquarter Market capitalization Ex-dividend date\nDataset 1 JP Morgan Chase New York City $424.173B Jul 05, 2023\nDataset 2 JPMorgan Chase & Co. New York City, NY $424,173,000,000 2023-07-05\nDataset 3 J.P. Morgan Chase & Co. New York $424,000.173M 5/7/23\nAs the table shows, the three records are the same but look different as they use dif‐\nferent formats. Keep in mind that formatting heterogeneity may occur within the\nsame dataset.9\nTo guarantee optimal data-matching results, data should be standardized using a con‐\nsistent formatting method. The most common approach involves rule-based tech‐\nniques, which employ a set of data transformation rules such as the following:\n•Remove dots from entity names (e.g., J.P . Morgan Chase & Co. → JP Morgan\nChase & Co).\n•Remove stop words (e.g., The Bank of America → Bank of America).\n•Expand abbreviations (e.g., Corp. → Corporation).\n•Remove postfixes (e.g., FinTech firm → FinTech).\n•Names should appear as “Given name, Surname” .\n•Convert dates to the format “YYYY/MM/DD” .\n•Parse fields into smaller segments (e.g., divide a field that contains full addresses\nlike “270 Park Avenue, New Y ork, NY” into multiple fields for the city, state, and\nstreet).\n•Infer missing fields (e.g., zip code can be inferred from the street address).\n•Remove duplicate records.\nWhen performing data preprocessing, make sure you don’t modify\nthe original tables. Instead, make a new copy of the data and apply\nthe transformations to it.\nFinancial Entity Resolution | 157\n10For more on this topic, refer to Mikhail Bilenko, Beena Kamath, and Raymond J. Mooney’s “ Adaptive Block‐\ning: Learning to Scale Up Record Linkage” , in the Sixth International Conference on Data Mining (ICDM’06)\n(IEEE, 2006): 87–96.Indexing\nOnce the input datasets are cleaned and standardized, they should be ready for\nmatching. In a typical scenario, the matching process will involve a comparison\nbetween each element in the first dataset with all elements in the second one. If the\ndatasets at hand are small, then such a comparison can be done in a reasonable\namount of time. However, with large datasets, the computational complexity may\nincrease significantly. Consider a scenario where you want to match two datasets with\n500k records each. If all pair-wise comparisons were to be performed, there would be\na total of 500,000 × 500,000 or 250 billion candidate comparisons. Even at a process‐\ning speed of one million comparisons per second, it would still take 69 hours to\nmatch the two datasets. If both datasets have one million records each, then it will\ntake around 11 days!\nCrucially, in most ER problems, the majority of pair-wise comparisons will result in\nnon-matches. This is because records in the first dataset often match a small subset of\nrecords in the second dataset. For this reason, it is common to observe that the num‐\nber of pair-wise comparisons increases quadratically with the number of data records\n(i.e., O(x^2), where x approximates the number of records in the datasets to match),\nwhile the number of true matches increases linearly.10\nTo overcome this issue, a number of data optimization techniques have been devel‐\noped. Such techniques are often referred to as indexing , which aims to reduce the\nnumber of pair-wise comparisons needed by generating pair records that are likely to\nmatch and filter out the rest. The most common indexing technique is called blocking .\nIt works by splitting the datasets to match into a smaller number of blocks and per‐\nforming pair-wise comparisons among the records within each block only. To per‐\nform the splitting, a blocking key  needs to be defined using one or more features from\nthe datasets. For example, a blocking key might place records in the same block if\nthey have the same zip code or country.\nBlocking presents a few challenges. First, it is highly sensitive to data quality. Small\nvariations in the data  might lead a blocking key to place a record in the wrong block.\nSecond, blocking might entail a tradeoff between computational complexity and\nblock granularity . By defining a very specific blocking key, you will end up with many\nblocks, which is good for performance. But this comes at the risk of excluding true\nmatches. On the other hand, using a more generic blocking key could result in a\nsmall number of blocks, which will lead to a large number of pair-wise comparisons\nthat increase computational complexity.\n158 | Chapter 4: Financial Entity Systems\nFigure 4-13  illustrates a simple blocking process. In this example, we have two data‐\nsets, A and B, that contain company information such as the market capitalization,\nthe headquarters’ country, and the exchange market on which the company is listed.\nIf we were to perform all pair-wise comparisons, we would need to do 6 × 6 = 36\ncomparisons. However, using blocking criteria that group records in blocks based on\nthe headquarters’ country and exchange market, we reduce the number of pair com‐\nparisons to five.\nFigure 4-13. A simple blocking process\nIn addition to blocking, a number of other indexing techniques have been developed .\nExamples include Sorted Neighborhood Indexing, Q-Gram-Based Indexing, Suffix\nArray-Based Indexing, Canopy Clustering, and String-Map-Based Indexing.\nFinancial Entity Resolution | 159\n11The LCS implementation used to compute the similarities is the Python SequenceMatcher  class in the difflib\npackage .Comparison\nOnce the candidate pairs have been generated, the next step involves the actual com‐\nparison between the records. The traditional approach to record comparison is based\non pair similarity. This is often performed by aggregating all features into a single\nstring and then comparing the string similarity between the pairs. Alternatively, com‐\nparing pair features individually by computing their similarities and combining them\ninto a single similarity score is also possible.\nGenerally speaking, similarity scores are normalized to be between 0 and 1. A pair\nhas a perfect match if its similarity score is 1, whereas a non-match is indicated by a\nscore of 0. The comparison is called exact matching  if it only allows for either a match\nor a non-match. Crucially, it is normal for similarity ratings to fall within the 0–1\nrange, in which case the matching is approximate  or fuzzy . Approximate matching\nmay occur due to differences in the datasets, such as the number of features (one\ndataset has a feature that the other does not), different formats (e.g., values reported\nin different currencies), information granularity (i.e., one dataset has a more granular\nidentifier than the other), and information precision (one dataset rounds values to\ntwo decimals while the other uses three).\nDuring the comparison phase, there are three types of matching scenarios:\nOne-to-one\nEach record in the first dataset can only have one match in the second dataset\n(e.g., matching the same financial transaction in two datasets).\nOne-to-many\nOne record in the first dataset may have numerous matches in the second dataset\n(e.g., matching all transactions in one dataset associated with a specific credit\ncard in another dataset).\nMany-to-many\nNumerous records from the first dataset can be matched to multiple records\nfrom the second dataset (e.g., matching multiple transactions within a trade\nrecorded in a broker’s database with transactions recorded by the clearing house\nor stock exchange).\nAs an illustrative example, Table 4-7  shows the similarity scores for the five candidate\npairs from Figure 4-11 . Records are first standardized (numbers expressed without\ndecimals or multiples; all letters are uppercase), and then concatenated in a single\nstring. Successively, the similarity is calculated between the concatenated strings\nusing the Longest Common Substring  (LCS) algorithm.11\n160 | Chapter 4: Financial Entity Systems\nTable 4-7. Illustration of record comparison\nRecord pair Pair string Similarity score\n(a1, b3) a1: “$200000000000USANYSE”\nb3: “$200110000000USANYSE”0.9\n(a3, b1) a4: “$55200000000UKLSE”\nb1: “$552000000000PORTUGALLSE”0.75\n(a4, b2) a4: “$300550000000USANYSE”\nb2: “$300550000000USANYSE”1\n(a5, b6) a5: “$100000000FRANCELSE”\nb6: “£95000000FRANCELSE”0.81\n(a6, b4) a6: “$900000000JAPANCME”\nb6: “$199876000JAPANNASDAQ”0.51\nIn addition to the LCS algorithm, there are several other methods available for com‐\nputing pair similarities. These include Jaro–Winkler approximate string comparison,\nLevenshtein distance, edit distance, Jaccard similarity, Q-gram distance, and more.\nClassification\nOnce all similarities have been computed, the next step is the classification of the can‐\ndidate pairs into matching categories. In its most basic form, classification is binary:\nmatch or non-match. However, a less restrictive approach allows for three classes:\nmatch, non-match, and potential match. In either case, a match indicates a pair that\nrefers to the same real-world entity in both datasets, while a non-match means that\nrecords in the pair refer to two different entities. A potential match is a pair of\nrecords that are likely to be a match but require a final clerical review for\nconfirmation.\nA variety of pair classification methods have been proposed, including the threshold-\nbased approach, rule-based approach, probabilistic approach, and machine learning\napproach. Later in this chapter, we will discuss these models in more detail. To make\na simple example, let’s use a basic threshold-based approach to classify the results of\nthe previous step (comparison) that were reported in Table 4-7 . Let’s assume that a\nmatch has a similarity score greater than or equal to 0.9, a potential match has a score\nof 0.8 and above, and anything below 0.8 is a non-match. Using this approach, the\noutcome of the classification is illustrated in Table 4-8 .\nTable 4-8. Illustration of a threshold-based pair classification\nRecord pair Similarity score Classification\n(a1, b3) 0.9 MATCH\n(a3, b1) 0.75 NON-MATCH\n(a4, b2) 1 MATCH\nFinancial Entity Resolution | 161\nRecord pair Similarity score Classification\n(a5, b6) 0.81 POTENTIAL MATCH\n(a6, b4) 0.51 NON-MATCH\nEvaluation\nThe final step in an ER process is performance evaluation. A highly performant ER\nsystem is able to find and correctly classify all valid matches in the input datasets.\nAdditionally, it needs to ensure computational efficiency in terms of runtime, mem‐\nory consumption, storage needs, and CPU usage.\nIn most cases, ER systems are implemented for real-world financial applications;\ntherefore, they need to scale to large applications with millions of records. Measuring\ncomputational complexity (e.g., in terms of O() notation) is fundamentally impor‐\ntant, even if optimization techniques such as indexing are applied. This is especially\nimportant when developing a streaming-based real-time record linkage system. In\nthis case, complexity metrics and disk and memory usage figures can orient the\nimplementation in terms of hardware, data infrastructure, and algorithmic optimiza‐\ntions. Additionally, as proposed by Elfeky et al. in their research paper , performance\ncan be measured in terms of the effectiveness of indexing techniques in reducing the\nnumber of record pairs to be matched ( reduction ratio ) while at the same time captur‐\ning all valid matches ( pair completeness ).\nTo evaluate the quality of the matching results of an ER system, a common practice is\nto use the binary classification quality metrics employed in machine learning and\ndata mining, which we used for evaluating NER systems. In building such metrics,\nfour numbers need to be calculated. True positives  are the number of pairs correctly\nclassified as matches, while true negatives  are pairs correctly classified as non-\nmatches. Similarly, false positives  are non-matches that were mistakenly classified as\nmatches, while false negatives  are pairs that were classified as non-matches, but in\nreality, they refer to actual matches. Figure 4-14  shows the confusion matrix repre‐\nsentation of these figures.\nFigure 4-14. Confusion matrix of ER\n162 | Chapter 4: Financial Entity Systems\nBased on these four metrics, a variety of quality measures can be calculated . For\nexample, accuracy detects the ability of the system to make a correct classification\n(match vs. non-match). Precision measures the ability of the system to correctly clas‐\nsify true matches (i.e., how good the system is at avoiding false positives). Recall is\nanother metric that measures the ability of the system to detect all true matches (i.e.,\nhow good the system is at avoiding false negatives). The F1 score is a harmonic mean\nof precision and recall and is used to find a balance between recall and precision.\nLet’s use our Table 4-8  example to compute these four metrics. As illustrated in\nTable 4-9 , the final predictions are available in the column called “Predicted class\nafter human review, ” while the ground truth values are available in the column\n“Ground truth class. ”\nTable 4-9. Final ER classifications  and their ground truth value\nRecord pair Predicted class Predicted class after human review Ground truth class\n(a1, b3) MATCH MATCH MATCH\n(a3, b1) NON-MATCH NON-MATCH NON-MATCH\n(a4, b2) MATCH MATCH MATCH\n(a5, b6) POTENTIAL-MATCH MATCH MATCH\n(a6, b4) NON-MATCH NON-MATCH MATCH\nFrom the data in Table 4-7 , we can compute the confusion matrix values as follows:\n•TP: 3\n•TN: 1\n•FP: 0\n•FN: 1\nThen, we can compute the four quality metrics, as illustrated in Table 4-10 .\nTable 4-10. Computed quality metrics\nQuality measure Value\nAccuracy 0.8\nPrecision 1\nRecall 0.75\nF1 score 0.85\nFinancial Entity Resolution | 163",15355
65-Approaches to Entity Resolution.pdf,65-Approaches to Entity Resolution,"As a general performance metric, an accuracy of 0.8 is not bad, but it wouldn’t be\nideal in a critical application. The precision value of 1 tells us that the model doesn’t\nproduce false positives; if a pair is classified as a match, then it will be a match with\n100% certainty. Recall tells us that the model couldn’t find all true matches and made\na few false negative classifications. The F1 score of 0.85 shows an OK model perfor‐\nmance, but one that is still not ideal for a good ER system.\nApproaches to Entity Resolution\nNumerous ER techniques have been proposed in the literature and by market partici‐\npants. Such techniques are often named and classified differently; therefore, I sum‐\nmarize them into three categories: deterministic linkage, probabilistic linkage, and\nmachine learning. These aren’t necessarily mutually exclusive, and they can be com‐\nbined to build an ER system. For example, a simple rule-based approach can be used\nto match high-quality records, while a probabilistic or machine learning approach is\nused for records with poor data quality. In the following sections, I will illustrate each\napproach in some detail.\nDeterministic linkage\nThe simplest ER technique, known as deterministic linkage, performs data matching\nvia a set of deterministic rules based on the available data fields. Various deterministic\nlinkage methods have been proposed, including link tables, exact matching, and rule-\nbased matching, which I’ll cover next.\nLink tables.    A link table contains a mapping between two or more data identifiers. If\ntwo datasets use different identifiers mapped in a link table, then the datasets can be\nmatched via an SQL join operation between them and the link table. Figure 4-15\nillustrates this approach.\nFor financial applications, link tables need to be built with a point-in-time  feature to\nkeep track of the possibility that identifiers might change, get reassigned, or become\ninactive. To this end, a good financial link table would include additional information\nsuch as the start and end date of the link, the link status, and any additional com‐\nments. For example, Table 4-11  illustrates a link table that contains three links, where\nonly one link (a1, b55) is active and has no end date, while link (a4, a20) ended in\n31-12-2007 as the stock got delisted, and link (199, b44) ended in 20-01-1995 because\nthe company merged with another one.\n164 | Chapter 4: Financial Entity Systems\nFigure 4-15. An ER process using a link table\nTable 4-11. Example of a link table\nIdentifier  AIdentifier  B Link start date Link end date Status Comment\na1 b55 01-02-1990 - Active\na4 b20 20-01-2005 31-12-2007 Inactive Stock delisted\na99 b44 20-01-1995 20-01-1995 Inactive Merged with another company\nThe main advantages of link tables are simplicity, performance, and readability. How‐\never, they might be laborious to construct and require extensive maintenance and\nupdating.\nFinancial institutions might create their own link tables internally. This is where you,\nas a financial data engineer, will play a major role. Additionally, a variety of financial\nlink tables are available as commercial products. This includes, for example, reference\ndatasets that match different financial identifiers and other instrument characteris‐\ntics. Another example is the famous data distributor Wharton Research Data Services\n(WRDS), which has created its own linking suite  to enable users to link tables\nbetween the most popular databases on the WRDS platform.\nFinancial Entity Resolution | 165\nAnother notable example involves the series of initiatives established  by the Global\nLegal Entity Identifier Foundation (GLEIF) in partnership with market participants\nto link the LEI with other financial identifiers. The result includes a list of open\nsource link tables, such as the BIC-to-LEI, ISIN-to-LEI, and MIC-to-LEI mappings.\nCase Study: CRSP/Compustat Merged (CCM) Link Table\nOne of the most common use cases of entity resolution in finance is merging stock\nprice data with company fundamentals data. If you have ever checked a financial\nnews website, you will notice that data about stock close/open prices, bid/ask prices,\nand volume are available, together with fundamental data such as market capitaliza‐\ntion and dividend distributions. This is done by matching data across price and fun‐\ndamental datasets for the same entity.\nA good example of a price/fundamentals ER system is the CRSP/Compustat Merged\nDatabase (CCM) . The CCM database is a link table that matches historical events and\nmarket data from the CRSP database with company fundamentals data from S&P’s\nCompustat database (both discussed in Chapter 2 ). As described by the vendor docu‐\nmentation , the identifiers used in creating the link table are the following:\nGVKEY\nCompustat’s company identifier.\nID\nCompustat’s issue identifier. One GVKEY may be associated with multiple\nGVKEYs.\nPRIMISS\nCompustat’s primary security identifier.\nPERMCO\nCRSP’s company identifier.\nPERMNO\nCRSP’s issue identifier. One PERMCO may be associated with multiple PERM‐\nNOs.\nThe resulting link table matches all security identifiers with information on the link\nstart date, link end date, CRSP identifiers, and Compustat identifiers.\nExact matching.    In exact matching, records in two datasets are linked via a common\nunique identifier or via a linkage key  that combines a set of data attributes into a sin‐\ngle matching key. If a common unique identifier is available in both datasets, then the\nmatching process becomes a simple SQL join operation on the unique key. The issue\nhere is that financial datasets often use different identifiers. Additionally, an identifier\n166 | Chapter 4: Financial Entity Systems\n12For an overview on this topic, have a look at Olivier Binette and Rebecca C. Steorts’ “(Almost) All of Entity\nResolution” , Science Advances  8, no. 12 (March 2022): eabi8021.may exist only from a certain point in time, and old records might lack identification.\nThe same procedure can be followed with a linkage key, but instead of a unique iden‐\ntifier, a linkage key is constructed to merge the datasets. For linkage keys to provide\ngood results, data must be of high quality (complete, standardized, deduplicated, and\nwithout errors).\nRule-based matching.    A less restrictive approach to deterministic linking is the rule-\nbased approach, where a set of rules is established to determine whether a pair of\nrecords constitutes a match. The primary benefits of this approach include the flexi‐\nbility to define and incorporate rules, speed, interpretability, and simplicity. On the\nnegative side, defining the rules may require considerable time and dataset-related\ndomain knowledge. Moreover, as the datasets increase in complexity and vary in\nquality, you might end up with a large number of rules that can impact maintainabil‐\nity and performance.\nA simple rule-based approach involves computing the similarity between records and\nclassifying a pair as a match if it exceeds a given threshold (e.g., if the similarity is >\n0.8, then it’s classified as a match; otherwise, it’s a non-match). This method offers a\ngood alternative to exact matching as it accommodates minor variations in the data\nattributes.\nProbabilistic linkage\nWhen a unique identifier is missing or the data contains errors and missing values,\ndeterministic record linkage may deliver poor results. Probabilistic linkage , also\nknown as fuzzy matching , was developed to overcome this issue. Probabilistic meth‐\nods have demonstrated superior linkage quality compared to deterministic\napproaches.\nProbabilistic linkage takes a statistical approach to data matching by computing prob‐\nability distributions and weights of the different attributes in the data. For example,\nassuming there are many fewer people with the surname “Bloomberg” than there are\npeople with the surname “Smith” in any two datasets, the weight given for the agree‐\nment of values should be smaller when two records have the surname value “Smith”\nthan when two records have the surname value “Bloomberg. ” This is because it is\nconsiderably more likely that two randomly selected records will have the surname\nvalue “Smith” than it is that they will have the surname value “Bloomberg. ”\nTo formalize these concepts, a variety of probabilistic linkage techniques have been\ndeveloped.12 However, to illustrate the main idea, let’s take as an example the\nwell-known  framework of Fellegi-Sunter (a theory of record linkage) . Fellegi and\nFinancial Entity Resolution | 167\nSunter proposed a decision-theoretic linkage theory that classifies a candidate com‐\nparison pair into one of three categories: link, non-link, and possible link. Pairs are\nanalyzed independently. In their analysis, Fellegi and Sunter demonstrated that opti‐\nmal matching can be achieved via a threshold-based strategy of likelihood ratios\nunder the assumption that the attributes are independent of each other. To illustrate\nthe main idea, let’s first define what the likelihood ratio is.\nLet λ represent the agreement/disagreement pattern between two records in a given\npair. Agreement can be expressed as a binary value (0 or 1) or, if needed, using more\nspecific values. Using a binary agreement scale, if we have three attributes, then λ can\nbe (1,1,1) if both records agree on all attributes, (1,1,0) if they agree on the first two\nbut not the third, and so on. Let’s denote the set of all possible agreement patterns by\nδ. For example, our three attributes can be represented in δ= 8 (2 × 2 × 2) agreement\npatterns.\nLet’s assume we have two datasets we want to match, A and B. We create the product\nspace as A ×B to obtain all possible comparison pairs (assume we don’t do indexing,\nfor the sake of simplicity). Then, we partition the product space into two sets:\nmatches (M) and non-matches (U).\nDenote by Pλ∈δ∣s∈M the probability of observing the agreement pattern λ for\na pair of records that are actually a match, and Pλ∈δ∣s∈U the probability of\nobserving λ for a pair of records that is not a match. The likelihood ratio is then\ndefined as:\nR=Pλ∈δ∣s∈M\nPλ∈δ∣s∈U\nFor example, if we consider our three attributes to be market capitalization, exchange\nmarket, and name, then the likelihood of a pair in full agreement can be written as:\nR=Pagree on capitalization ,agree on name ,agree on exchange ∣s∈M\nPagree on capitalization ,agree on name ,agree on exchange ∣s∈U.\nIf they agree on all attributes but the exchange, then the likelihood is:\nR=Pagree on capitalization ,agree on name ,disagree on exchange ∣s∈M\nPagree on capitalization ,agree on name ,disagree on exchange ∣s∈U\nThe ratio R is referred to as matching weight.  Based on likelihood ratios, Fellegi and\nSunter proposed the following decision rule:\n•If R⩾tupper, then call the pair a link (match).\n•If R⩽tlower, then call the pair a non-link (non-match).\n168 | Chapter 4: Financial Entity Systems\n13A good example is Kunho Kim and C. Lee Giles’ “Financial Entity Record Linkage with Random Forests” , in\nProceedings of the Second International Workshop on Data Science for Macro-Modeling  (June 2016): 1–2.\n14A good example is Peter Christen’s “ Automatic Record Linkage Using Seeded Nearest Neighbour and Support\nVector Machine Classification” , in Proceedings of the 14th ACM SIGKDD International Conference on Knowl‐\nedge Discovery and Data Mining  (August 2008): 151–159. \n15A good read on deep learning for ER is Nihel Kooli, Robin Allesiardo, and Erwan Pigneul’s “Deep Learning\nBased Approach for Entity Resolution in Databases” , in Asian Conference on Intelligent Information and Data‐\nbase Systems (ACIIDS 2018 ), Lecture Notes in Computer Science, vol. 10752 (Springer, 2018): 3–12. \n16For a good overview on this topic, I recommend Aris Gkoulalas-Divanis, Dinusha Vatsalan, Dimitrios Karapi‐\nperis, and Murat Kantarcioglu’s “Modern Privacy-Preserving Record Linkage Techniques: An Overview” ,\nIEEE Transactions on Information Forensics and Security  16 (September 2021): 4966–4987.\n17Some effort has been made in this direction, for example Amr Ebaid, Saravanan Thirumuruganathan, Walid\nG. Aref, Ahmed Elmagarmid, and Mourad Ouzzani’s “Explainer: Entity Resolution Explanations” , in the 2019\nIEEE 35th International Conference on Data Engineering (ICDE)  (IEEE, 2019): 2000–2003.•If tlower<R<tupper, then call the pair a potential link.\nFor details on how to calculate the probabilities and thresholds, I refer the reader to\nthe seminal work of Thomas N. Herzog, Fritz J. Scheuren, and William E. Winkler,\nData Quality and Record Linkage Techniques  (Springer).\nSupervised machine learning approach\nA limitation of deterministic and probabilistic approaches is that they tend to be spe‐\ncific to the datasets at hand and fail when there are complex relationships between the\ndata attributes. Machine learning approaches excel in this area, as they are mainly\nfocused on generalization and pattern recognition.\nThe supervised machine learning approach to record linkage trains a binary classifi‐\ncation model to predict and classify matches in the datasets. As a supervised techni‐\nque, it requires training data containing the true match status (match or non-match).\nOnce trained on the labeled data, the model can be used to predict new matches for\nunlabelled data. Tree-based models,13 support vector machines,14 and deep learning15\ntechniques are among the most popular machine learning approaches used in ER.\nDeveloping a supervised machine learning model for ER can be quite challenging.\nFirst, the model needs to consider the imbalanced nature of the data-matching prob‐\nlem, where most pairs correspond to true non-matches, while only a small fraction\nare true matches. Second, obtaining labeled training data can be quite challenging\nand time-consuming, especially for large datasets. Third, labeled data may not be\navailable or accessible due to privacy issues. To solve this issue, a special type of ER,\ncalled privacy-preserving record linkage, has been proposed.16 Finally, an ML-based\napproach to ER might present interpretability and explainability challenges, especially\nwhen employing advanced techniques such as deep learning and boosted trees.17\nFinancial Entity Resolution | 169",14399
66-Entity Resolution Software Libraries.pdf,66-Entity Resolution Software Libraries,,0
67-Chapter 5. Financial Data Governance.pdf,67-Chapter 5. Financial Data Governance,"Entity Resolution Software Libraries\nEntity resolution is a well-known problem with a lengthy history of development and\napplication. Many software programs for ER have been developed by individuals and\norganizations. As of the time of writing this book, there are open source tools like\nfastLink, Dedupe, Splink, JedAI, RecordLinkage, Zingg, Ditto, and DeepMatcher.\nAdditionally, on the commercial side, several vendors offer ER tools and solutions\nsuch as TigerGraph, Tamr, DataWalk, Senzing, Hightouch, and Quantexa.\nSummary\nIn this chapter, you learned about two primary challenges commonly encountered by\nfinancial institutions: named entity recognition (NER) and entity resolution (ER).\nNER entails extracting and identifying financial entities from both structured and\nunstructured financial datasets. Conversely, ER focuses on the critical task of match‐\ning data pertaining to the same entity across multiple financial datasets.\nThe landscape of challenges and solutions in financial NER and ER is dynamic,\nevolving alongside data, technologies, and changing market requirements. To excel at\nthese tasks and gain a competitive edge, it’s essential that you stay current with the\nlatest updates, methodologies, technologies, and industry best practices around\nfinancial NER and ER. Consider exploring machine learning techniques and natural\nlanguage processing tools, and enrich your financial domain knowledge to enhance\nthe accuracy and efficiency of your NER and ER systems.\nLooking ahead, the next chapter will present and discuss the critical problem of\nfinancial data governance, exploring concepts and best practices for ensuring data\nquality, integrity, security, and privacy in the financial domain.\n170 | Chapter 4: Financial Entity Systems",1781
68-Financial Data Governance.pdf,68-Financial Data Governance,,0
69-Financial Data Governance Justified.pdf,69-Financial Data Governance Justified,"CHAPTER 5\nFinancial Data Governance\nAs financial markets expand, so do the methods and use cases for how financial data\nis collected, stored, and used. This has generated concerns within the financial indus‐\ntry as well as broader governing bodies that are pushing for solid data controls, qual‐\nity assurance, privacy rules, and increased security measures. As a result, data\ngovernance frameworks have emerged as a promising approach to defining and\nimplementing rules and principles for guiding data practices within financial\ninstitutions.\nThis chapter will provide a practical framework for financial data governance based\non three key components: data quality, data integrity, and data security and privacy.\nFirst, I’ll cover the basics of financial data governance. Then, I’ll go into depth about\neach of the three components in the sections that follow.\nFinancial Data Governance\nData governance is critical to securing financial data, ensuring regulatory compli‐\nance, and fostering trust among stakeholders. By implementing robust data gover‐\nnance practices, financial institutions can safeguard sensitive information, adhere to\nlegal requirements, and maintain the integrity of their financial operations.\nFinancial Data Governance Defined\nBefore defining what financial data governance is, let’s examine a few existing defini‐\ntions. For example:\nData governance is everything you do to ensure data is secure, private, accurate, avail‐\nable, and usable. It includes the actions people must take, the processes they must fol‐\nlow, and the technology that supports them throughout the data life cycle.\n— Google Cloud\n171\nData governance is, first and foremost, a data management function to ensure the\nquality, integrity, security, and usability of the data collected by an organization.\n— Eryurek et al., Data Governance: The Definitive  Guide  (O’Reilly, 2021)\nAs you can see, data governance can be considered a data management function, a\nprocess, or simply a set of technological and cultural practices. Interestingly, all the\nabove definitions share a common purpose, that of ensuring data quality, security,\nintegrity, availability, and usability. Building on these ingredients, I define financial\ndata governance as follows:\nFinancial data governance is a technical and cultural framework that establishes a set\nof rules, roles, practices, controls, and implementation guidelines to ensure the quality,\nintegrity, security, and privacy of financial data in compliance with both general and\nfinancial domain–specific internal and external policies, standards, requirements, and\nregulations.\nThere isn’t a one-size-fits-all solution for financial data governance frameworks. Two\nfinancial institutions may follow the same set of principles to establish financial data\ngovernance; however, the final implementations are very likely to be unique to each\ninstitution. The reason has to do with the nature of the issues that different financial\ninstitutions might face in terms of data quality, security, privacy, integrity, and more.\nIt also depends on the financial institution’s internal organizational structure, culture,\nprocess standardization and harmonization, senior management support, and\nparticipation.\nFinancial Data Governance Justified\nDefining and enforcing an effective financial data governance framework requires a\nnonnegligible investment. On the one hand, a concrete and functional data gover‐\nnance framework needs to be defined, implemented, and integrated within the finan‐\ncial institution’s data infrastructure. On the other hand, employees and data users\nneed to be trained and prepared to adhere to the established data governance princi‐\nples in their daily work. As such, it is important to first understand the value proposi‐\ntion of a financial data governance framework for your institution.\nImportantly, financial organizations are among those that require and benefit from\ndata governance the most. This can be explained by two main factors: performance\nand risk management.\nFinancial data governance impacts performance in several ways. First, data gover‐\nnance drives high data quality standards, which is a major input to most financial\noperations and decisions. Brian Buzzelli attributes operational inefficiency  (inefficient\nuse of input to produce output) in the financial industry to poor data quality . It\nimpacts financial institutions’ ability to conduct business efficiently, gain insights into\nmarket activity, make informed investment decisions, respond on time to new events,\nand communicate accurate figures to stakeholders. Second, financial data governance\n172 | Chapter 5: Financial Data Governance\nsaves employees the nuisance of constantly checking and rechecking data quality,\nintegrity, privacy, and security. Third, with solid data governance principles in place,\ndevelopers and business teams feel more confident in the quality and compliance of\ntheir applications and products.\nAnother critical reason for implementing data governance in financial institutions is\nto effectively manage risks associated with data, which can have significant implica‐\ntions for these institutions. Such risks include the following:\n•Cyberattacks intended to steal data, damage organizational resources, or interfere\nwith the operation of confidential systems\n•Data breaches where sensitive data falls into the hands of unauthorized persons\nor organizations\n•Discriminatory biases built into financial applications\n•Erratic data injected in models that distorts the results\n•Data loss due to lack of backups, snapshots, or archives\n•Absence of firm-level risk oversight due to decentralized data processes\n•Lack of visibility into the data processing steps\n•Privacy risks when sharing data with third parties\n•Impact on model prediction quality due to bad data\n•Financial and reputational risks due to nonconformance with legal and regula‐\ntory requirements\nRegulators around the world have put forth substantial efforts toward creating and\nenforcing laws and regulations that address the above risks. Some examples are listed\nhere:\n•Sarbanes–Oxley Act\n•Bank Secrecy Act\n•Basel Committee on Banking Supervision’s standard number 239 (BCBS 239)\n•European Union’s (EU’s) Solvency II Directive\n•California Consumer Privacy Act (CCPA)\n•EU’s General Data Protection Regulation (GDPR)\nIn order to adhere to these regulations, financial institutions must now establish and\nimplement robust data governance frameworks. Consequently, compliance has\nemerged as the primary driver for the adoption of data governance within the finan‐\ncial sector.\nFinancial Data Governance | 173",6722
70-Dimension 1 Data Errors.pdf,70-Dimension 1 Data Errors,"1For an overview of data quality frameworks, see Corinna Cichy and Stefan Rass’ “ An Overview of Data Qual‐\nity Frameworks” , IEEE Access  7 (2019): 24634–24648.\n2This approach of defining data quality attributes based on the needs of data consumers, rather than on theo‐\nretical findings, is brilliantly illustrated in the work of Richard Y . Wang and Diane M. Strong in their paper\n“Beyond Accuracy: What Data Quality Means to Data Consumers” , Journal of Management Information Sys‐\ntems  12, no. 4, (1996): 5–33.\n3For a good read on this, see Lukas Budach, Moritz Feuerpfeil, Nina Ihde, Andrea Nathansen, Nele Noack,\nHendrik Patzlaff, Felix Naumann, and Hazar Harmouch, “The Effects of Data Quality on Machine Learning\nPerformance” , arXiv  preprint arXiv:2207.14529 (July 2022).The topic of data governance is quite vast and can be complex to navigate. Practition‐\ners, researchers, financial institutions, and consulting firms regularly publish data\ngovernance studies and guidelines. In this chapter, I will present a practical data gov‐\nernance framework centered on three major areas that are common to all financial\ninstitutions: data quality, data integrity, and data security and privacy.\nData Quality\nData quality measures how well a dataset satisfies its intended use in various opera‐\ntional and analytical applications. For financial institutions, data has a primary role as\ninput in the decision-making and product development process. Consequently, the\nproblem of financial data quality needs to be handled with care by financial data engi‐\nneers, analysts, and machine learning experts. Some use the term data downtime  to\nrefer to periods during which data is not accessible or is unusable due to quality-\nrelated issues. In a data-driven financial institution, prolonged or frequent data\ndowntimes can severely impact efficiency, erode customer trust, interrupt research\nendeavors, and influence management and investment decisions.\nA Data Quality Framework  (DQF) is required to ensure financial data quality. The\ndefinition and specifications of a DQF may vary from one institution to another\nbased on internal and external factors.1 In this chapter, I will share the main ingredi‐\nents that you, as a financial data engineer, can leverage to define and build a DQF.\nSuch ingredients are often called data quality dimensions  (DQDs). A DQD refers to\nthose attributes or indicators of data quality that, if measured correctly, can convey \ninformation about the overall quality of financial data.\nThere isn’t a fixed list of DQDs. As new requirements and data issues emerge, various\nDQDs can be identified and measured. Furthermore, the relevance of particular qual‐\nity dimensions can vary depending on the specific problem being addressed and the\nneeds of data consumers.2 For example, certain aspects of data quality can have a\ngreater influence on the performance of machine learning models.3\nTherefore, educating your team on defining DQDs can greatly benefit your financial\ninstitution. To establish a baseline, in the following sections, I will present nine DQDs\n174 | Chapter 5: Financial Data Governance\n4If you want to learn more about this issue, I recommend Ramazan Gençay, Michel Dacorogna, Ulrich A. Mul‐\nler, Olivier Pictet, and Richard Olsen’s An Introduction to High-Frequency Finance  (Elsevier, 2001).that are particularly relevant to financial data: errors, outliers, biases, granularity,\nduplicates, availability and completeness, timeliness, constraints, and relevance. Note\nthat while the needs of your organization are unique and may vary, these DQDs are\nfairly universal and likely to apply to your business.\nDimension 1: Data Errors\nData errors are digital records that have been recorded erroneously, and therefore\nreflect invalid or incorrect values. The presence of data errors can compromise the\nvalue, accuracy, and reliability of the data and negatively impact reporting, analysis,\nand decision-making.\nData errors are quite common in financial data and represent the most frequent data\nquality issue for financial institutions. A global survey  of over 1,100 financial execu‐\ntives and professionals conducted by BlackLine in 2018 revealed that 55% of respond‐\nents were not completely confident in their institution’s ability to spot financial data\nerrors before reporting results. The survey shows that 7 in 10 respondents believe that\ntheir institution made important decisions based on inaccurate or out-of-date finan‐\ncial data. The majority of C-level respondents agreed that there would be a negative\nimpact if financial inaccuracies were not detected before reporting. The negative\nimpacts included harm to the company’s image, trouble obtaining new investments,\nrising debt levels, penalties, and even jail time.\nThere isn’t a fixed list of financial data error types; rather, they materialize with the\nintroduction of data sources, products, pipelines, and various manipulation and\ntransformation operations. But to give a few examples, financial data errors can\ninvolve the following issues:\n•Random measurement errors (e.g., $9.345 instead of $9.335)\n•Wrong decimal places (e.g., a price of $111.34 instead off $11.134)\n•Decimal precision (e.g., an exchange rate of 1.345 instead of 1.3458)\n•Negative prices (e.g., one Apple stock is worth $-200)\n•Dummy and test quotes (submitted to test latency or other technical\nspecifications)4\n•Extra or removed digits (e.g., $10000 instead of $1000)\n•Invalid date (e.g., option maturity on 01-01-1345)\n•Inverted exchange rates (e.g., 1 dollar equals 1.27 pounds instead of 1 pound\nequals 1.27 dollars)\nData Quality | 175\n•Rounding (e.g., price of $1.01 rounded to $1)\n•Misspelled entity names (e.g., Bnk of America)\n•Typos (e.g., $0900)\n•Invalid formatting (e.g., 01-2022-01)\nLet’s walk through an example. Suppose you want to convert one billion euros to dol‐\nlars. Let’s assume that the Forex quote you are supposed to use is 1 EUR = 1.07291\nUSD. Using this exchange rate, the converted sum is $1,072,910,000. Now, assume\nthat the exchange rate is slightly different due to a data error, say 1.07191. In this case,\nthe newly converted sum is $1,071,909,999, which is $1,000,000 less! Similarly, if a\ndecimal precision error happens, say the exchange rate is 1.072, the converted sum\nwould be $1,072,000,000, leading to a loss of $910,000.\nAn important aspect to keep in mind about financial data is the\nnature of its correctness or trueness. While certain financial vari‐\nables possess an absolute and indisputable value in specific scenar‐\nios—like the number of shares sold in a market transaction—\nothers, such as derivative prices or Forex quotes, are subject to esti‐\nmates, averages, or provider-specific values. For instance, a\nEUR/USD quote may differ among Forex brokers, with no univer‐\nsal market quote for reference. Therefore, it’s vital to assess finan‐\ncial data errors against the appropriate reference value.\nData errors can also significantly impact the robustness of financial analysis. For\ninstance, a research article from the Journal of Fixed Income  estimates that around\n7.7% of transaction reports in the Trade Reporting and Compliance Engine (TRACE)\ndatabase are erroneous records. If these errors are not considered, liquidity measure‐\nments based on this data may be skewed toward indicating a more liquid market than\nis the case.\nTo handle financial data errors, a few steps are required. In the first step, you need to\nidentify and detect data errors. Error detection in financial data can occur at either\nthe single-record or dataset level, with the former focusing on individual data points\n(e.g., an error with a single transaction) and the latter analyzing multiple records\nsimultaneously, often producing aggregated error metrics like the error ratio (e.g., for\nstatistical analysis purposes).\nCrucially, financial data errors vary in complexity, making detection difficult at times.\nFor example, a computer algorithm can easily detect an intraday price jump from\n$100 to $0.100. However, a more subtle error might require a more in-depth investi‐\ngation, such as an intraday price of $50, followed by three prices of $40 and then a\nfifth price of $50. For simple errors, rule-based approaches are often used (e.g., if the\nprice is negative → error). For more complex errors, statistical and data mining\n176 | Chapter 5: Financial Data Governance",8500
71-Dimension 2 Data Outliers.pdf,71-Dimension 2 Data Outliers,"5For a good introduction to this topic, I recommend reading Chapter 9 of Pang-Ning Tan, Michael Steinbach,\nVipin Kumar, and Anuj Karpatne’s Introduction to Data Mining , 2nd ed. (Pearson Education, 2019). techniques  have been traditionally employed, such as Pearson correlation, z-score,\npercentile analysis, and Mahalanobis distance.5 A more advanced technique involves\nthe computation of the value of the erroneous record using theoretical or quantitative\nmodels such as financial asset pricing.\nOnce detected, errors need to be checked against business-defined tolerance and\nimpact levels. A tolerance level can be something like an error ratio < 0.01%. Once\nthe error is checked against the tolerance ratio, the next action depends on its busi‐\nness priority. An error with a Forex exchange rate may significantly impact the busi‐\nness if it converts large sums of money and, therefore, it needs to be given high\npriority.\nA challenging situation arises when the data containing errors comes from a third\nparty and is not produced by the final data consumer. In this case, detecting and cor‐\nrecting the errors might be difficult as there is no valid ground truth to compare the\ndata against. When this happens, a useful approach is cross-dataset validation, which\nconsists of comparing data from one source against an alternative data source that\nrecords similar but high-quality data. In a research article from the Journal of Finance ,\nthe authors analyzed error rates in CRSP (a stock price dataset) and Compustat (a\ncompany fundamentals dataset) and found that errors happen with a very low fre‐\nquency, but the impact of existing errors is substantial. In the same paper, the authors\nsuggest that their methodology could be generalized as a means of data quality assess‐\nment for competing databases.\nDimension 2: Data Outliers\nIn basic terms, a data outlier is a data observation that differs significantly from the\nothers. For example, consider a stock price time series with 100 observations, 99 of\nwhich have a value less than 1000, but one record has a value of 1,000,000. It is typical\nto refer to this last observation as an outlier. The presence of outliers in financial data\nmay adversely affect the robustness of statistical analysis and bias machine learning\nmodels.\nOutliers within financial data might arise due to various factors. In market and trans‐\naction time series, outliers commonly result from the inherent high noise level in the\ndata. Additionally, outliers may signal fraudulent or anomalous financial activities\nlike money laundering and credit card fraud. Furthermore, some records may seem\nlike outliers due to systematic issues (e.g., data transmission errors), structural breaks\n(sudden shifts in market conditions), poorly formatted or unadjusted data, or meas‐\nurement errors (e.g., errors in price quoting).\nData Quality | 177\n6For more on this topic, see Carson Kai-Sang Leung, Ruppa K. Thulasiram, and Dmitri A. Bondarenko’s “ An\nEfficient System for Detecting Outliers from Financial Time Series” , in Flexible and Efficient  Information Han‐\ndling: Proceedings of the 23rd British National Conference on Databases, BNCOD ‘06 , Belfast, Northern Ireland,\nUK, July 18–20, 2006. (Springer Berlin Heidelberg, 2006): 190–198.\n7For more on this topic, see Kangbok Lee, Y easung Jeong, Sunghoon Joo, Y eo Song Y oon, Sumin Han, and\nHyeoncheol Baik’s “Outliers in Financial Time Series Data: Outliers, Margin Debt, and Economic Recession” ,\nMachine Learning with Applications  10 (December 2022): 100420.\n8For a good read on this topic, see Christian T. Brownlees and Giampiero M. Gallo’s “Financial Econometric\nAnalysis at Ultra-High Frequency: Data Handling Concerns” , Computational Statistics & Data Analysis  51, no.\n4 (December 2006): 2232–2245.\n9For more on this topic, I highly recommend John Adams, Darren Hayunga, Sattar Mansi, David Reeb, and\nVincenzo Verardi’s “Identifying and Treating Outliers in Finance” , Financial Management  48, no. 2 (March\n2019): 345–384.To identify financial data outliers, researchers have proposed several methods. Some\nuse statistical techniques such as principal component analysis, z-score, percentile\nanalysis, and kurtosis, while others use machine learning techniques such as cluster‐\ning, classification, and anomaly detection.6 Keep in mind that financial outlier detec‐\ntion might be a challenging task whose difficulty depends on the type and structure of\nthe data. For example, outliers in financial time series data are different in terms of\ndetection and treatment than in cross-section data.7\nOnce detected, outliers need to be treated following a specific method. The most\ncommon outlier treatment methods among financial researchers are winsorization\nand trimming . Trimming is the simplest approach, which works by removing outliers\nfrom the dataset. The main challenge with trimming is that if you trim too much, you\nrisk altering the statistical properties or coverage of the dataset, while if you trim too\nlittle, you might still end up with an unstable and noisy dataset.8\nWinsorization, on the other hand, involves limiting extreme values in the data to a\nspecified percentile. In a 90% winsorization, for instance, all observations above the\n95th percentile are set to equal the value of the 95th percentile, and all observations\nbelow the 5th percentile are set to equal the value of the 5th percentile.9\nAnother popular and reliable technique is scaling . This can be performed by taking a\ndataset’s logarithm or square root value. Scaling helps to normalize the data distribu‐\ntion and reduce the impact of extreme values. For instance, consider a dataset con‐\ntaining stock prices where some stocks have significantly higher prices than others.\nBy applying logarithmic scaling to the stock prices, the dataset’s range can be com‐\npressed, making it easier to compare and analyze percentage changes or returns\nacross different stocks. This normalization step helps in recognizing trends and pat‐\nterns without being heavily impacted by the extreme values of high-priced stocks.\nAnother technique involves trimmed estimators, which are statistical measures cre‐\nated by excluding a portion of the extreme values from the dataset through\n178 | Chapter 5: Financial Data Governance",6361
72-Dimension 4 Data Granularity.pdf,72-Dimension 4 Data Granularity,"truncation.  For instance, the 5% trimmed mean is computed by averaging the values\nwithin the 5% to 95% range, removing the lowest and highest 5% of the data.\nDimension 3: Data Biases\nData biases refer to inherent distortions that impact the representation of the sub‐\njects/entities that constitute the data. These biases can result in erroneous conclu‐\nsions, skewed patterns, biased decisions, and discrimination.\nA large number of biases may emerge in financial data. To illustrate with an example,\nlet’s say you want to analyze the performance of fund managers (a common area of\nstudy in finance) and get a dataset for your study from a data provider. Interestingly,\nmany commercial fund datasets are collected via a voluntary reporting mechanism.\nIn this setting, fund managers may or may not decide to report their performance\nhighlights to the data vendor.\nFor instance, when a fund performs well, and the manager aims to attract more capi‐\ntal, they may disclose its figures to draw market attention. Conversely, if the fund\nunderperforms or the manager prefers not to attract new investors, they might opt\nnot to report the performance figures. This kind of behavior might lead to self-\nselection bias. Such bias will distort the dataset and give the impression that some\nfunds are always outperforming.\nAs institutional investors, hedge funds are exposed to a number of risks, and in some\ncases, this might lead to failure/bankruptcy. If a hedge fund dataset systematically\nexcludes/removes failed or poorly performing funds from its archive, this could lead\nto a type of bias called survivorship bias,  where only the successful funds appear in the\ndataset. Similar to self-selection bias, survivorship bias may convey an over-\noptimistic image of the fund industry’s performance.\nIn some cases, a new hedge fund could be added to the dataset together with its full\nhistory. Even though it looks natural, this behavior might lead to backfilling  bias, also\nknown as instant history bias . This bias can be relevant if only funds with a strong\ntrack record choose to join the database, which would distort historical performance\nstatistics for the hedge fund industry.\nAnother significant bias in financial data is look-ahead bias , which occurs when con‐\nducting historical studies using information that would not have been accessible dur‐\ning the analyzed period. For instance, consider a scenario where a company releases\nits annual report for the year 2019 in March 2020, as is typically the case. Suppose\nyou’re a financial analyst conducting backtesting to evaluate your investment strate‐\ngy’s performance. In that case, it’s crucial to avoid assuming that the company’s\nannual report was available before March 2020 (e.g., December 31, 2019), even if\nyou’re analyzing data from a later date.\nData Quality | 179\n10For more on this topic, see Sagar P . Kothari, Jay Shanken, and Richard G. Sloan’s “ Another Look at the Cross‐\nSection of Expected Stock Returns” , The Journal of Finance  50, no. 1 (March 1995): 185–224.\n11For a good read, start with Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan, “ A Survey on Bias and Fairness in Machine Learning” , ACM Computing Surveys (CSUR)  54, no. 6\n(July 2021): 1–35.\n12For more on the Lipper database, see Mila Getmansky, Andrew W . Lo, and Shauna X. Mei’s chapter, “Sifting\nThrough the Wreckage: Lessons from Recent Hedge-Fund Liquidations” , in The World of Hedge Funds: Char‐\nacteristics and Analysis , H Gifford Fong, ed. (World Scientific Publishing Company, 2005): 7–47.Detecting and correcting biases in financial data is a challenging task. As a financial\ndata engineer or analyst, make sure you understand how the data was generated and\nrecorded. Consider collecting more data to adjust for possible biases. Furthermore, if\nyou are extracting a data sample from a large dataset, design a methodology that pro‐\nactively prevents biases, for example, by assessing the data sample representation.\nAnother good practice is to compare two datasets to check for potential bias that\nexists in one but not the other.10 Finally, I highly recommend keeping an eye on the\nlatest research on bias and fairness in data analysis and machine learning.11\nOn the data vendors’ side, efforts have been made to detect and correct biases in their\nproducts. For example, among the most reputable hedge fund data sources is LSEG’s\nLipper Fund Data , formerly known as the Trading Advisor Selection System  (TASS)\ndatabase. Lipper tracks and publishes fund-related information such as its profile,\nperformance, and investment strategies. Funds report to Lipper voluntarily, which, as\nwe saw earlier in this section, can lead to various biases. To account for this issue, Lip‐\nper keeps two separate databases: the graveyard  database, which records data on\ndefunct funds or funds that haven’t been reported for a long time, and the live data‐\nbase, which records data for actively reporting funds.12\nDimension 4: Data Granularity\nData granularity describes the level of detail within a dataset, with highly granular\ndata offering detailed observations about individual entities, while low-granularity\ndata typically provides summarized or aggregated information at a higher level. To\nbetter illustrate the concept within the finance domain, let’s consider a few examples:\nFinancial portfolios\nA financial portfolio is a collection of investments created to achieve a specific\nfinancial goal, considering elements such as diversification, risk appetite, and\nexpected returns. Portfolio data can be available either in aggregated form, pro‐\nviding an overview of portfolio performance, risk, and investment strategy, or in\na more detailed form, including details about individual portfolio constituents\nand their respective allocations, such as Apple stock at 5%, US government bonds\nat 20%, and so on.\n180 | Chapter 5: Financial Data Governance\nFinancial indices\nA financial index is a single aggregated metric constructed to represent and track\nthe performance of a specific category of financial assets. For example, the S&P\n500 index offers an aggregated metric of the top 500 public companies in the\nUnited States by market capitalization. Index data may be available at the index\nlevel (single metric) or at the constituent level (the individual assets included in\nthe index and their corresponding weights).\nFinancial exposures\nFinancial institutions hold assets with each other, such as interbank loans, securi‐\nties, and cash. A financial institution might disclose its total exposure to another\ninstitution at the aggregated level (Bank A holds $1bln assets with the rest of the\nsystem) or at the individual institution level (e.g., Bank A holds $1mln at Bank B,\n$33mln at Bank C, and so on).\nFinancial time series\nThese can be recorded with high temporal accuracy, such as every second,\nminute, or hour, or with lower granularity, such as daily, weekly, or monthly\naggregates.\nFinancial transactions\nThese can be stored at the individual transaction level or as summaries, such as\nmonthly purchases.\nThe level of data granularity is an essential factor in determining the type of analysis\nthat can be performed on the data. Highly granular data enables deeper insights and\nthe identification of meaningful patterns utilizing advanced analytical approaches.\nImportantly, granular data comes with increased storage requirements, processing\ntime, and the potential for privacy concerns. Therefore, it is recommended to keep in\nmind the challenges and tradeoffs associated with managing and analyzing highly\ngranular data.\nGranular financial data may not always be available. Portfolio composition data, for\nexample, may not be disclosed since it would reveal the firm’s investment strategy,\nwhich might harm its competitive position. Another reason this information may be\nwithheld is data confidentiality, like with customer transaction details. In some cases,\ndetailed data may not be collected in the first place, for example, in noncentralized\nmarkets such as OTC markets.\nTo overcome the issue of data granularity, you can try to collect detailed data. Alter‐\nnatively, aggregated data may be decomposed into its constituent elements using stat‐\nistical and machine learning techniques. For example, network scientists who analyze\nfinancial networks are often limited by privacy issues when collecting data about the\ntopology of a financial network. As mentioned earlier, the best example is banks’\nexposures to each other, where data is available at the aggregate level only. To\nData Quality | 181",8713
73-Dimension 5 Data Duplicates.pdf,73-Dimension 5 Data Duplicates,"13To test the code quickly online, you can use OneCompiler .overcome this issue, researchers have proposed network reconstruction  and link pre‐\ndiction  methods to infer and construct the network structure at the bank-to-bank\nlevel.\nDimension 5: Data Duplicates\nData duplicates are repeated records that represent the same data observation. Dupli‐\ncate data is a widespread issue in finance, and its consequences can range from negli‐\ngible to severe. For example, a duplicate record in a data sample used in a financial\nstudy may not lead to serious consequences; however, if a financial transaction\nappears multiple times on a user account, then it might impact the available balance.\nData duplicates may occur for a variety of reasons. First, there are human errors,\nwhich happen when a person adds the same data entry into a system multiple times.\nSecond, duplicates may be inserted automatically by a machine when the system is\nnot properly built to ensure data uniqueness. For example, a household submits a\nloan application twice. Third, data duplicates might emerge when merging multiple\ndata sources improperly, for example, using nonunique identifiers.\nCrucially, despite the apparent simplicity of the problem, detecting duplicates may be\nquite challenging. In the simplest case, a data record is considered duplicate if the val‐\nues of all its fields exactly match those of another record. For example, two financial\naccounts are considered duplicates if they match the account holder’s first name, last\nname, social security number, and account number. In a more complex scenario, two\nduplicate records may be recorded differently, thus making it harder to identify. For\nexample, if names are recorded with different formatting (e.g., J.P . Morgan versus\nJPMorgan), then the duplicate records would have a nonperfect match. In other\ncases, the presence of exact duplicates may be due to inconsistency in the data record‐\ning mechanism. For example, an investment of $10,000 in stock A may be recorded as\nan investment of $5,000 in stock A twice, while an investment of $10,000 in stock B is\nrecorded only once.\nDetecting and treating duplicate records can also vary in complexity. The best strat‐\negy is to always think in advance about the duplicate generation mechanism and add\nthe necessary checks, constraints, and validations to prevent duplication. For exam‐\nple, having a well-defined unique identifier for each record is a good practice. If the\nunique identifier is not sufficient, then it is possible to add constraints on a subset of\ndata fields that ensure no two records share the same set of values. Let’s walk through\nan example of how to prevent duplicates before data insertion in PostgreSQL:13\n182 | Chapter 5: Financial Data Governance\n-- PostgreSQL\nCREATE EXTENSION  btree_gist ;\nCREATE TABLE company(\n   record_key  INT PRIMARY KEY,\n   company_id  VARCHAR NOT NULL,\n   company_name  VARCHAR NOT NULL,\n   dividend_date  DATE NOT NULL,\n   dividend_amount  DECIMAL(10,2) NOT NULL,\n   EXCLUDE USING gist (company_id  WITH =, company_name  WITH <>)\n);\nINSERT INTO company VALUES(1, 'JPM', 'JP Morgan' , '2020-11-20' , 10);\nINSERT INTO company VALUES(2, 'BOA', 'Bank of America' , '2022-01-08' , 5);\nINSERT INTO company VALUES(1, 'JPM', 'JP Morgan' , '2019-05-01' , 8);\nINSERT INTO company VALUES(3, 'JPM', 'J.P. Morgan' , '2023-06-01' , 3);\npsql:commands.sql:12: ERROR:  duplicate key value violates unique constraint \n""company_pkey"" DETAIL:  Key (record_key)=(1) already exists.\npsql:commands.sql:12: ERROR:  conflicting key value violates exclusion con-\nstraint ""company_company_id_company_name_excl""\nDETAIL:  Key (company_id, company_name)=(JPM, J.P. Morgan) conflicts with exist-\ning key (company_id, company_name)=(JPM, JP Morgan).\nIn this example, we created a table that stores data on company dividend distribu‐\ntions. Each company has a unique ID and a human-readable name, while each record\nhas a unique record key. We want to avoid having two records with the same record\nkey or the same company ID but with a different company name. To achieve this, we\ncan add a primary key constraint on the record key field and an exclusion constraint\non the company ID and company name. After that, we test the implementation by\ntrying to make four inserts, two of which violate the two constraints.\nThe first two insert statements will execute successfully, and two records will be cre‐\nated. However, for the third statement, we will get an error that says the uniqueness\nconstraint was violated as they share the same record key (= 1). For the fourth insert,\nwe will get an error saying that you are trying to insert a record with a new format\n(J.P . Morgan and JP Morgan).\nIf data has already been generated and stored with potential duplicates, then a variety\nof solutions are possible. In the simplest case, duplicates share the same field values.\nTo identify them, we can use aggregation or analytical queries. Let’s consider the fol‐\nlowing example:\n-- PostgreSQL\n-- create\nCREATE TABLE company (\n  company_id  INTEGER PRIMARY KEY,\n  company_name  VARCHAR,\n  company_headquarters  VARCHAR\n);\n \nData Quality | 183\n-- insert\nINSERT INTO company VALUES (1, 'Company A' , 'New York' );\nINSERT INTO company VALUES (2, 'Company B' , 'California' );\nINSERT INTO company VALUES (3, 'Company A' , 'New York' );\n \n-- fetch \nSELECT company_name , company_headquarters , count(company_id ) AS record_count  \nFROM company\nGROUP BY company_name , company_headquarters\n company_name | company_headquarters | record_count \n--------------+----------------------+--------------\n Company B    | California           |            1\n Company A    | New York             |            2\nIn the above table, two duplicates store the same data for Company B. To detect these\nduplicates, we used a GROUP BY  statement that counts the number of records that\nshare the same company name and company headquarters.\nThe GROUP BY  is an aggregation tool that reduces the number of rows to summary\ngroups. If we want to keep the original data without aggregation, we can use a win‐\ndow function such as ROW_NUMBER()  to assign an ordered sequential number to each\nrecord in a group. This way, deduplication can be performed by taking the records\nwith row_num  = 1 and discarding those with higher row numbers. Here is an illustra‐\ntive example:\nSELECT company.*, \nROW_NUMBER () OVER (PARTITION  BY company_name , company_headquarters ) AS row_num \nFROM company\nORDER BY company_name , company_headquarters , row_num\n company_id | company_name | company_headquarters | row_num \n------------+--------------+----------------------+---------\n          1 | Company A    | New York             |       1\n          3 | Company A    | New York             |       2\n          2 | Company B    | California           |       1\nA more challenging scenario occurs when a dataset contains duplicates, but they can‐\nnot be directly identified due to data quality issues. For example, we might know the\nsubset of columns that could identify duplicates, but the data may be recorded differ‐\nently, so it won’t be detected via an exact match. In this case, the data deduplication\nprocess becomes an entity resolution (data matching) task. We discussed entity reso‐\nlution systems in Chapter 4 . One thing to keep in mind is that data deduplication is a\nspecial type of entity resolution , as it involves only one dataset that is matched against\nitself to resolve similar entities.\nAnother subtle scenario with duplicates happens when two records look the same,\nbut they are not duplicates because a distinguishing field is missing. Examples include\ntransaction data where the date of the transaction is available but the time is missing;\nmultiple data at the security level (e.g., company stocks) that look like duplicates but,\n184 | Chapter 5: Financial Data Governance",7980
74-Dimension 6 Data Availability and Completeness.pdf,74-Dimension 6 Data Availability and Completeness,"in reality, refer to different security issues (e.g., different stock issues) that are missing\nissue IDs; or a financial option is available twice, but one is a call (buy-side), and\nanother is a put (sell-side).\nDimension 6: Data Availability and Completeness\nA crucial dimension of data quality is the completeness of a dataset, indicating\nwhether it contains all necessary information required for its intended analytical or\noperational purposes. A dataset is considered incomplete or unavailable when essen‐\ntial data attributes or observations are missing. In finance, issues of data availability\nand incompleteness are quite common . This happens for a variety of reasons, such as\nthe following:\nVoluntary data reporting\nIf the data collection process involves a voluntary data reporting mechanism, e.g.,\nsurvey data, then it is likely that some respondents will decline to report their\ndata for one reason or another, e.g., to hide bad performance. Additionally,\nrespondents might report data with different frequencies (e.g., Firm A responds\nto all surveys, while Firm B responds to some and skips others).\nSecurity and confidentiality  concerns\nUnless enforced by law, financial firms might have a number of concerns about\nthe security and confidentiality of their data. This might generate a high level of\nrisk aversion toward data sharing.\nMarket factors\nMarket factors such as liquidity, sentiment, and risk might impact the data gener‐\nation process. For example, a liquid stock that trades frequently will have a large\nnumber of price observations per day, while an illiquid instrument might trade\nonce every six hours and have a few daily records. This type of behavior is called\nnonsynchronous trading .\nTechnological reasons\nIf a financial firm or the market lacks adequate data collection infrastructure, it\ncan lead to data collection gaps. For instance, a considerable amount of data on\nover-the-counter (OTC) market transactions remains unrecorded due to the\nabsence of a centralized entity responsible for collecting and aggregating such\ndata.\nPublication delay\nIf there is a latency between the time data is created and the time it is published,\nit could be considered unavailable. This might happen with company fundamen‐\ntal data, created at the end of the fiscal year but released a few months later.\nData Quality | 185\nData Time to Live (TTL)\nTTL is a database mechanism that sets a period of time after which data will be\nconsidered expired and no longer visible to queries and database statistics. TTL\ndoesn’t necessarily mean that the data was deleted, as this might happen at a later\npoint in time.\nIn certain circumstances, the existence of a specific type of data may be optional; in\nthese situations, the consequences of missing data are minor and may not require any\ncorrection effort. However, in many cases, incomplete or missing data can cause a\nnumber of problems for financial institutions. For example, missing data may lead to\nbiased and unreliable financial models, which in turn can impact investment deci‐\nsions and product development. Incomplete data may also mean less visibility and\ninsight into market activities and patterns, thus foregoing potentially profitable\nopportunities and reducing trust in the data within a financial institution. Gaps in\ncustomer-related data may impact the business and sales teams’ ability to understand\nconsumer segments and offer personalized services. Moreover, missing data may\ndelay reporting and releases, which can impact market sentiment and expert\nestimations.\nTo effectively deal with missing financial data, it is crucial to understand the mecha‐\nnisms that cause data missingness. Following the existing literature , three forms of\nmissing data are often discussed:\nMissing Completely at Random\nData on variable X is said to be Missing Completely at Random (MCAR) if the\nmechanism that leads to observations of X being missing is independent of X\nitself and any other variable in the dataset, whether observable or missing. The\nmissingness happens randomly and without any systematic pattern. For example,\na hedge fund that reports its performance data to a data provider might incur a\ntechnical issue preventing some of its data from being submitted.\nMissing at Random\nIf data on variable X is Missing at Random (MAR), then the missingness mecha‐\nnism is independent of X itself but is systematically related to one or more fea‐\ntures in the dataset. For instance, a hedge fund firm might opt not to disclose\nperformance data due to confidentiality concerns surrounding its investment\nstrategy. Here, the missingness is unrelated to the fund’s performance but rather\nto its investment secrets.\nMissing Not at Random\nThis happens when observations on variable X are missing for reasons related to\nX itself. To continue with our example, a hedge fund might decide not to report\nits performance data because it had a bad performance and wants to hide it from\ninvestors or because they are doing very well and they don’t want to attract more\ninvestors or media coverage.\n186 | Chapter 5: Financial Data Governance",5179
75-Data Integrity.pdf,75-Data Integrity,"A variety of techniques have been developed for treating missing financial data . The\nsimplest and most common technique is likewise deletion , where observations with\nmissing data are removed from the dataset. Another technique is the omitted variable\napproach , which drops the variable with missing values from the dataset. In some\ncases, dropping observations or variables might lead to biased or sparse datasets. A\nfamily of techniques called imputation  is often used to overcome this issue. Imputa‐\ntion aims to estimate the missing values in a dataset using a specific method. One\nbasic imputation technique is the mean substitution , which replaces missing values for\nvariable X with the average value of X. Another imputation technique is filling in a\nmissing value in one dataset with another value in another dataset, e.g., via entity res‐\nolution. Last but not least, a practical approach to data imputation is regression,\nwhere a model is used to produce an estimate of the missing value. Models can be\nmachine learning based (e.g., linear regression) or financial models (e.g., Capital\nAsset Pricing Model, Value at Risk, etc.).\nDimension 7: Data Timeliness\nTimeliness of data is a critical dimension of data quality within financial institutions.\nThis section will discuss two key aspects related to data timeliness:\n•Is the data available and accessible at the time it is expected to be?\n•Does the data reflect the most recent observations?\nMany financial datasets are used in a time-critical context, e.g., algorithmic trading,\nand if data is not available in the expected window, then the data is of no use. On the\nother hand, if the available data does not reflect the latest facts, e.g., the latest Forex\nquote or the latest analyst estimate, then it might lead to wrong business decisions\nand lost revenues. Financial markets use the term stale price  to describe an outdated\nor no longer accurate quoted value of a financial asset or instrument.\nA variety of factors may influence financial data timeliness. The most common fac‐\ntors are latency, market closure, time lags, or lengthy processes in the data generation,\ningestion, and transformation mechanisms. For example, complex data pipelines are\nlikely to be time-consuming and delay data availability. Another factor is the data\nrefresh rate , which is the frequency with which data is refetched and updated to reflect\nthe latest observations. Refresh frequencies may vary from real time to regular\nschedules.\nFurthermore, many applications rely on data caching , a strategy where a copy of the\ndata is stored in a temporary storage location that allows fast access. However, cached\ndata can become outdated over time, requiring periodic updates or replacements to\nensure alignment with the most recent data. This problem, known as cache invalida‐\ntion, poses one of the most common challenges in software development .\nData Quality | 187\nDimension 8: Data Constraints\nThe data constraints dimension reflects the degree to which data conforms to prede‐\nfined technical and business rules and limitations. Examples of such constraints\ninclude the following:\nExtension constraint\nData is stored in allowed formats only (e.g., CSV files).\nSchema constraint\nData follows a predefined schema structure that defines the mandatory fields and\ntheir data type.\nNon-null constraint\nA data field does not contain null values.\nRange constraint\nA data field contains values that fall within a given range (e.g., price >=0, year\n>=1990).\nValue choice constraint\nA field can assume values from a fixed list of choices (e.g., country names).\nUniqueness constraint\nA record must be unique across a dataset.\nReferential integrity constraint\nValues in one field are allowed only if they exist in another referenced field. For\nexample, an online purchase transaction cannot be stored if it contains a non‐\nexistent product.\nRegular expression patterns\nA field contains values that match a given string pattern (e.g., an email pattern, a\nfinancial identifier pattern).\nCross-field  validation\nThis ensures that a field satisfies a certain condition in relation to one or more\nfields. For example, the date of issuance of a derivative contract cannot be earlier\nthan the date of expiry.\nBased on your business needs, additional constraints may be defined. An important\nthing to remember is that violating a given constraint may not always signal a bad\ndata quality issue. For example, a schema change that involves adding or deleting a\ngiven field might be done to enrich data quality and correct existing errors. As\nanother example, a value choice constraint may be violated if the list of allowed\noptions is outdated.\n188 | Chapter 5: Financial Data Governance\n14For a good read on this topic, see Pat Langley’s “Selection of Relevant Features in Machine Learning” , in Pro‐\nceedings of the AAAI Fall Symposium on Relevance (1994),  vol. 184 (AAAI Press, 1994): 245–271.Dimension 9: Data Relevance\nData relevance is an important data quality dimension, determining the degree to\nwhich available data aligns with the specific problem or purpose it aims to address.\nRelevance ensures that the data is actionable and contributes effectively to gaining\ninsights and understanding the problem at hand. For example, in his interesting anal‐\nysis published in Getting It Wrong: How Faulty Monetary Statistics Undermine the Fed,\nthe Financial System, and the Economy  (MIT Press, 2011), William Barnett illustrates\nhow the lack of adequate financial data to assess financial systemic risks has been\nidentified as one of the main factors leading to the great financial crisis of 2007–2008.\nIn the following years, several initiatives have been launched to review and enhance\nthe data collection processes in financial markets. For example, in 2020 the Bank of\nEngland conducted a Data Collection review  to identify the challenges the industry\nfaces in providing data, the issues the bank encounters in receiving and using it, and\nthe necessary steps to address these problems. As another example, in response to the\nsignificant impact of swaps, especially credit default swaps, during the 2007–2008\nfinancial crisis, the Dodd-Frank Wall Street Reform and Consumer Protection Act\n(Dodd-Frank Act) established swap data repositories  (SDRs)  to serve as entities\nresponsible for swap data reporting and recordkeeping. According to the Dodd-\nFrank Act, all swaps, whether cleared or uncleared, must be reported to registered\nSDRs.\nIn recent years, the importance of data relevance has increased alongside the rise of\nmachine learning and generative AI. If the available data features (variables) are not\npertinent to the analytical problem or do not provide insights into the patterns ana‐\nlysts aim to capture, developing accurate models becomes challenging.14 Similarly,\nfine-tuning a language model requires contextual data that matches the specific\nrequirements and conditions of the task or problem at hand.\nThe question of what data is relevant is ultimately a function of the problem at hand.\nFor example, investment firms may employ a range of trading strategies, each of\nwhich requires different types of data. For instance, day trading  demands real-time\nintraday price, volume, volatility, and market liquidity data. Swing trading  relies on\ntechnical and fundamental indicators, market sentiment, and medium-term price\ntrends. Trend trading  depends on moving averages, trend lines, and momentum indi‐\ncators. Arbitrage trading  needs data on order books, market liquidity, transaction\ncosts, trading fees, real-time price discrepancies, and Forex rates. Mean reversion\ntrading  uses data on moving averages, Relative Strength Index (RSI), Bollinger Bands,\nand Moving Average Convergence Divergence (MACD); systematic trading  requires\nData Quality | 189",7925
76-Principle 2 Data Backups.pdf,76-Principle 2 Data Backups,"15To learn more about trading strategies and their data requirements, I recommend Eugene A. Durenard’s\nProfessional Automated Trading: Theory  and Practice  (Wiley, 2013).time series data for transactions (price and volume), orders, and news (economic\nreleases and events).15\nData Integrity\nThroughout its lifecycle, data goes through a number of transformations, movements,\nand adjustments as well as aggregation, matching, and more. In this context, the con‐\ncept of data integrity  is often used to indicate a set of principles established to ensure \nconsistent, traceable, usable, and reliable data. Depending on the institution type and\nbusiness requirements, there may be a number of ways data integrity could be\nensured. To offer a general overview, this section outlines nine key data integrity\nprinciples—standards, backups, archiving, aggregation, lineage, catalogs, ownership,\ncontracts, and reconciliation—all of which hold significant relevance within the\nfinancial sector.\nPrinciple 1: Data Standards\nAs financial markets have grown in size and complexity, the terms standard  and\nstandardization  have become keywords. According to authors Spivak and Brenner in\ntheir book Standardization Essentials  (CRC Press, 2001), the term standard “denotes a\nuniform set of measures, agreements, conditions, or specifications between parties. ”\nThe process of formulating, developing, and implementing standards is called stand‐\nardization. Standards differ in their nature, function, and acceptance. Spivak and\nBrenner provide a general framework by categorizing standards into the following\ntaxonomy:\n•Physical standards or units of measure\n•Terms, definitions, classes, grades, ratings, or symbols\n•Test methods, recommended practices, guides, and other applications to prod‐\nucts and processes\n•Standards for systems and services, in particular, quality standardization and\nrelated aspects of management system standards for quality and the environment\n•Standards for health, safety, consumers, and the environment\nFinancial industry participants have made several calls for standardization , acknowl‐\nedging its role in increasing market efficiency, confidence, and stability and reducing\ncosts. For example, research conducted by McKinsey  found that the adoption of the\nLegal Entity Identifier standard (which we discussed in detail in Chapter 3 ) could\n190 | Chapter 5: Financial Data Governance",2441
77-Principle 7 Data Ownership.pdf,77-Principle 7 Data Ownership,"save the global banking industry around USD $2–4 billion annually in client\nonboarding costs.\nFurthermore, standards play a crucial role in financial markets by promoting consis‐\ntency across key aspects of processes, products, and services, including quality, com‐\npatibility, interoperability, comparability, and reliability. For instance, the ISO 21586\nstandard , which specifies the description of banking products or services, was intro‐\nduced to ensure uniformity in descriptions of banking products and services (BPoS)\nacross various financial institutions, enabling customers to understand and compare\nthem effectively.\nFurthermore, as domain experts develop standards, they distill best practices and\ncodify the most recent technologies and expertise, which saves market participants\nthe effort of reinventing the wheel. A few great examples of this are accounting stand‐\nards (e.g., generally accepted accounting principles , or GAAP), risk management stand‐\nards (e.g., value at risk, or V AR), and data quality standards (e.g., ISO 8000: Data\nQuality).\nPrinciple 2: Data Backups\nOne of the primary operational risks faced by financial institutions is the loss of data,\nwhich can occur for various reasons. For example, data may be destroyed by accident,\ncorrupted, converted incorrectly, overwritten with a later version, mixed with the\nincorrect sort of data, lost during a hardware failure, or simply buried in a large pile\nof log files where it is hard to detect.\nData backups  are an effective method for mitigating the risk of data loss. A data\nbackup is a copy of the original data stored in a different place that can be recovered\nin a case of a data loss accident.\nA number of factors need to be considered when building a data backup strategy. I\nrecommend approaching the problem through a data backup lifecycle . This includes\ndefining data backup steps and elements such as when to back up (e.g., scheduled, on\ndemand, or event driven), what data to back up (operational, analytical, client), the\nnumber of backups to create, backup security (e.g., through encryption), backup stor‐\nage locations (multizones, geographies, data centers), recovery tests and plans, reten‐\ntion time, and deletion.\nPrinciple 3: Data Archiving\nA variety of data within financial institutions may reach a point where they are no\nlonger actively utilized but are still necessary for future reference, reviews, audits,\nelectronic discovery, litigation, and compliance purposes. Examples include financial\ntransactions, customer information, and regulatory reports.\nData Integrity | 191\nIn addition, regulatory frameworks such as the Bank Secrecy Act (BSA), Federal\nDeposit Insurance Corporation Improvement Act (FDICIA), and General Data Pro‐\ntection Regulation (GDPR) have established several data retention requirements that\nfinancial institutions need to comply with.\nTo manage this data retention challenge, a common approach is data archiving , which\nrefers to the process of moving data that is no longer actively used out of production\nsystems and onto a separate long-term storage system.\nData archives should not be confused with data backups. Although\nboth are concerned with keeping data in secondary storage, they\nserve different purposes. Data backups are part of a disaster recov‐\nery strategy and are meant to manage data loss accidents. Data\narchives, on the other hand, serve a data retention purpose.\nIn addition to compliance and risk management, data archiving lowers storage costs\nby moving data from more costly high-performance primary storage to significantly\nless expensive secondary storage (e.g., hard disk drives [HDDs]).\nA data archival policy  is often established to manage data archival. It comprises ele‐\nments  such as a data retention policy , data archival software, and data access and dis‐\ncovery functionalities.\nPrinciple 4: Data Aggregation\nFinancial institutions engage in diverse operations, including lending, payments,\ninvestments, risk management, insurance, proprietary trading, portfolio manage‐\nment, and more. Often, a single institution performs multiple activities simultane‐\nously. For instance, a commercial bank may accept deposits, offer loans, market\nfinancial investment products, and provide insurance.\nIn traditional settings, individual activities within a financial institution are often\noverseen within distinct organizational silos, each maintaining data about its opera‐\ntions using separate systems. An inherent challenge with such silos is the complexity\ninvolved in consolidating data across a large number of business units, legal entities,\nand disparate data storage systems. This, in turn, may jeopardize a financial institu‐\ntion’s capacity to generate an aggregated view of its activities and risks. Such con‐\nstraints were one of the primary reasons that banks could not adequately assess their\nrisk exposures and concentration before and during the 2007–2008 financial crisis.\nOne thing to keep in mind is that data aggregation is a capability of\na data infrastructure, and it doesn’t necessarily mean having all data\nin the same place.\n192 | Chapter 5: Financial Data Governance\nIn response, the Basel Committee published a set of 13 principles  for designing and\nimplementing data aggregation capabilities in financial institutions, primarily banks.\nLogically, the final implementation of these principles will vary from one financial\ninstitution to another. In addition, variations in internal structures among banks may\nfacilitate or hinder the complete implementation of all 13 principles. Indeed, in\nassessing the adherence to the guidelines, the Basel Committee observed that banks\nwere not fully compliant , mainly due to the complexity of their internal IT infrastruc‐\nture.\nSeveral other practices in financial markets emphasize the aggregation of data. For\ninstance, asset and fund managers typically maintain an Investment Book of Record\n(IBOR), a centralized database consolidating investment-related information like\ntransactions, positions, and holdings. Another example is the Accounting Book of\nRecords  (ABOR), which aggregates accounting-related data on assets, liabilities, trans‐\nactions, costs, net asset value, and charts of accounts. Another good example are con‐\nsolidated tapes , which refer to systems that aggregate data from different trading\nvenues, offering a unified source of information on trading activity across multiple\nmarkets.\nPrinciple 5: Data Lineage\nAs a financial data engineer, a practical way to think about data is through the data\nlifecycle  approach, also called the information lifecycle . The term data lifecycle is fre‐\nquently used in industry  to indicate the different phases that data goes through from\nthe initial creation, or ingestion, onward. Based on each financial institution’s particu‐\nlar context, the data lifecycle’s phases and complexity may vary. At a high level, there\nare five main phases: extraction, transformation, storage, usage, and archiving. How‐\never, this process can quickly evolve into a complex chain of steps involving a multi‐\ntude of actions and operations. Consequently, maintaining visibility into the data\nlifecycle becomes increasingly challenging, potentially exposing financial institutions\nto costly risks and errors.\nTo gain visibility into the data lifecycle, financial institutions need to develop a data\nlineage  framework. The term data lineage is used to describe the discrete steps\ninvolved in the generation, movement, transformation, storage, delivery, and archiv‐\ning of data. In other words, it is a feature of the data infrastructure that allows users\nand engineers to track a given data object throughout its lifecycle. Knowing how the\ndata was generated and what actions were applied to it at each step builds confidence\nin the data infrastructure, pipelines, and lifecycle. Nowadays, the visibility of data lin‐\neage is a valuable feature that is top of mind for consumers, managers, and regulators.\nData lineage can be implemented in a number of ways. The most appealing and user-\nfriendly approach is lineage graphs , which display a graphical visualization of data\nprocessing history. Importantly, visualization tools may not be performant when the\nData Integrity | 193\nprocessing logic becomes more complex and intricate. In such cases, detailed step-by-\nstep descriptions of the processing logic are required.\nAudit Trails: A Financial Data Lineage Approach\nWhen working in finance, a common term that you might frequently encounter is\naudit trail . An audit trail is a special implementation of data lineage that builds a\nchronological step-by-step data recording system that tracks financial activities such\nas accounting transactions, financial transactions, trades, buy and sell quote submis‐\nsions, and any financial activity that can be tracked from its origination onward. An\naudit trail is often used when an auditor or regulator wants to examine the origin of a\ncertain figure (e.g., earnings per share) or a given financial activity (e.g., quote to sell\na specific number of securities). The importance of audit trails has increased remark‐\nably after the flash crash of 2010, where a trader intentionally submitted an excep‐\ntional set of orders (a practice called spoofing ) in order to manipulate the market in\ntheir favor. Thanks to audit trails, regulators were able to identify the person respon‐\nsible for the orders. Systems such as the Order Audit Trail System (OATS) and Con‐\nsolidated Audit Trail (CAT) were established to automate the recording of\ninformation on orders, quotes, and other trade-related data from all shares traded on\nthe National Market System (NMS). Such systems streamline the lifecycle of an order\nfrom reception through execution or cancellation for simple tracking and auditing.\nPrinciple 6: Data Catalogs\nFinancial institutions produce and consume vast quantities and varieties of financial,\neconomic, operational, and business data. To create an efficient and reliable data-\ndriven culture, data producers and consumers must be able to search and find all the\ndata they need quickly.\nIn this context, the idea of data catalogs  has received particular attention. In simple\nterms, a data catalog is a set of metadata (i.e., data that describes or summarizes other\ndata) combined with search and data management tools that allow a data producer or\nconsumer to find and document a data asset within a financial institution. In other\nwords, you can think of a data catalog as a central and searchable inventory of all data\nassets.\nData catalogs can be implemented in a variety of ways based on your data consump‐\ntion needs and the complexity of your data assets. For instance, it can be a database\nwhere you store and search the metadata directly or a full-fledged application with\nfeatures such as a UI, search and discovery, metadata management, user permission,\nand API integration. For a practical example of such tools, have a look at the open-\nsource Python-based library Comprehensive Knowledge Archive Network (CKAN) .\n194 | Chapter 5: Financial Data Governance",11248
78-Principle 8 Data Contracts.pdf,78-Principle 8 Data Contracts,"16For an excellent treatment of data catalogs, I recommend Ole Olesen-Bagneux’s The Enterprise Data Catalog\n(O’Reilly, 2023).\n17For more on this topic, see Ali M. Al-Khouri’s “Data Ownership: Who Owns “My Data?” , International Jour‐\nnal of Management & Information Technology  2, no. 1 (November 2012): 1-8.\n18For a detailed study on this topic, I recommend Marshall Van Alstyne, Erik Brynjolfsson, and Stuart Mad‐\nnick’s “Why Not One Big Database? Principles for Data Ownership” , Decision Support Systems  15, no. 4\n(December 1995): 267-284.To see a minimal data catalog in action, check the online data catalog of LSEG Data &\nAnalytics .\nCrucially, the more advanced and complex a data catalog, the higher the maintenance\nand curation burden. Conversely, having a sparse or out-of-date data catalog may lead\nto higher resource use and more wasted time than not having any.16\nPrinciple 7: Data Ownership\nData ownership is one of the most valuable data governance practices for data-driven\norganizations. It’s important to note that the term data ownership is used in two dif‐\nferent contexts. On the one hand, it can refer to the legal owner of a given data asset.\nThis topic emerged following the considerable increase in data collection practices\nand the adoption of third-party storage solutions such as the cloud. As more and\nmore data is collected about people and organizations, concerns have emerged\nregarding who the final owner of the data is. Similarly, with the widespread adoption\nof public cloud storage solutions, questions have emerged regarding who owns the\ndata stored in the cloud.17\nLooking at it differently, data ownership involves designating an individual or team\nwith the task of overseeing the collection, cleansing, maintenance, sharing, and man‐\nagement of a particular data asset within a financial institution. These individuals are\noften known as data owners  and are typically selected for their domain expertise. The\nrationale is that data owners, being subject matter experts, are better equipped and\nmotivated to maintain and manage a specific data asset compared to a centralized\nteam that may lack the requisite domain knowledge to comprehend the data fully.18\nPrinciple 8: Data Contracts\nOne of the main factors that can significantly impact the quality of data within organ‐\nizations is the communication structure. A well-known adage, called Conway’s law , is\noften cited in this context. It states that “organizations which design systems are con‐\nstrained to produce designs which are copies of the communication structures of\nthese organizations. ” Following this principle, the term data contract  has recently\nemerged as a promising approach to organizing requirements and expectations\naround data. Let’s consider how industry experts define data contracts:\nData Integrity | 195\n19For a good read on data contracts, I highly recommend the book by Andrew Jones, Driving Data Quality with\nData Contracts: A Comprehensive Guide to Building Reliable, Trusted, and Effective  Data Platforms  (Packt,\n2023).Data contracts are API-like agreements between Software Engineers who own services\nand Data Consumers that understand how the business works in order to generate\nwell-modeled, high-quality, trusted, real-time data.\n— Chad Sanderson, “The Rise of Data Contracts”\nA data contract is an agreed interface between the generators of data and its consum‐\ners. It sets the expectations around that data, defines how it should be governed and\nfacilitates the explicit generation of quality data that meets the business requirements.\n— Andrew Jones, Driving Data Quality with Data Contracts  (Packt, 2023)\nWith a data contract, data consumers can define their data-related needs (e.g., struc‐\nture, semantics, relations, formatting, fields, frequency, typing, rounding, privacy, and\nterms of use) and establish an agreement with data engineers to receive data that\nmatches their expectations. This allows data consumers to concentrate on analysis\nand product development rather than worrying about data generation and engineer‐\ning. Data engineers, on the other hand, do not need to be concerned about making\nmodifications to the database or data model that may lead to production issues. This\nis why it is called a contract: both sides agree to it and each needs to hold their end of\nthe deal. Naturally, a data contract can be revised and changed (e.g., a new field is\nrequired, a new owner is assigned, etc.) which would result in a new modified\nagreement.\nCurrently, general guidelines for implementing data contracts don’t exist. Depending\non the institution and its data strategy, different data contract definitions may be\nestablished.19 To give an illustrative example, assume that the analytics team needs\ndaily price data for the top 100 US stocks by market capitalization, with no null pri‐\nces, a price range of 0–1000000 and no missing observations, and the data must be\nready by 10:00 a.m. on each working day. In this case, following the data contract\nspecification proposed by Jochen Christ and Simon Harre , we can build a basic con‐\ntract as follows:\ndataContractSpecification : 0.0.1\nid: stock-price-extraction\ninfo:\n  title: Daily Adjusted Stock Price Extraction\n  version: 0.0.1\n  description : daily extraction of the adjusted stock price of the top 100 U.S  \n               stocks by market capitalization.\n  owner: Analytics Team\n  contact:\n    name: John Smith (Analytics Team Lead)\n    email: john.smith@example.com\n196 | Chapter 5: Financial Data Governance",5599
79-Data Security and Privacy.pdf,79-Data Security and Privacy,"servers:\n  production :\n    type: Snowflake\n    project: daily_adjusted_prices_prod\n    dataset: snowflake_adjusted_prices_top_100_latest_v1\nterms:\n  usage: Data can be used for financial analysis, backtesting, and machine  \n         learning use cases.\n  sla: 10:00 AM of each working day.\n  daily_record_count : 100 observations   \n  limitations : >\n    Not suitable for intra-day financial time series analysis.\n    Data may be missing some identifiers such as ISIN.\n    Max data processing per day: 10 Gigabytes.\n    Max instrument requests per day: 1000 instruments.\n  cost: 0.01$ per instrument request\nschema:\n  type: json-schema\n  specification :\n    adjusted_prices_top_100 :\n      description : One record per instrument and date.\n      type: object\n      fields:\n        price_date :\n          type: timestamp\n          format: date-time\n          nullable : false\n          description : time of the price observation\n        adjusted_price :\n          type: numeric\n          precision : 4 decimals\n          range: 0-1000000   \n          nullable : false\n          description : The adjusted price value.\n        instrument_ticker :\n          type: string\n          description : The ticker identifier of the stock\n          nullable : false\nIt’s important to note that data contracts are not about specs or tools, but rather are a\ndesign pattern that emphasizes automating data quality and governance across vari‐\nous systems. The final specifications and the tools you will employ depend on your\nspecific business requirements.\nPrinciple 9: Data Reconciliation\nIn financial markets, the same financial record often appears across different systems\nowned by various counterparties, posing challenges for maintaining consistent\nrecords. This issue is addressed through data reconciliation, which involves aligning\ndiverse sets of records to create a unified view of financial transactions and balances.\nData Integrity | 197\n20To read more about portfolio reconciliation and the technological side of it, I recommend the ISDA ’s paper,\n“Portfolio Reconciliation in Practice” .\n21For more on payment reconciliation, see “Payment Reconciliation 101: How It Works and Best Practices for\nBusinesses” , by Stripe.This process helps minimize errors and discrepancies, thereby ensuring operational\nintegrity, financial stability, compliance, and customer trust.\nA typical data reconciliation process in financial markets is portfolio reconciliation,\nwhere records of holdings, transactions, and positions are compared and verified\nbetween two or more counterparties, such as financial institutions or investment\nmanagers. The International Swaps and Derivatives Association (ISDA) has identified\nportfolio reconciliation as one of the most advantageous practices in mitigating\noperational and credit risks in OTC markets.20\nFor instance, in the fund industry, multiple entities may hold the same portfolio\nexposure data for a specific fund, such as a management company and a custodian.\nFor instance, suppose a mutual fund is managed by Management Company A, and its\nassets are held in custody by Custodian B. Management Company A reports a $50\nmillion exposure in technology stocks for the fund, while Custodian B, responsible\nfor safekeeping and reporting the fund’s assets, shows a $49.5 million exposure in the\nsame sector. To ensure consistency and accuracy, portfolio reconciliation is necessary.\nThis process involves comparing the data from both Management Company A and\nCustodian B to resolve any discrepancies and provide a unified view of the fund’s\nholdings.\nSimilarly, in payment processes, multiple entities are involved in storing ledger\nrecords, often resulting in discrepancies in ledger balances among banks, FinTech\ncompanies, and service providers like BaaS providers. These discrepancies can stem\nfrom data duplication, incomplete or inaccurate record-keeping practices, delays or\nerrors during system upgrades, and the inherent complexity of reconciling multiple \nsystems of record.21\nData Security and Privacy\nFinancial institutions deal with highly sensitive and valuable data such as customer\nfinancial data, money transfers, transactions, investment strategies, and credit card\nnumbers. Consequently, the financial industry has traditionally been a primary target\nfor cyberattacks. To give a few examples, according to a report by Capgemini , Cit‐\nigroup US suffered a data breach in 2011 that resulted in data on more than 360K\ncustomers being leaked. Citigroup Japan reported a similar breach that affected\naround 92K customers. Again in 2011, Bank of America suffered a data breach that\ncost the bank around $10mln.\n198 | Chapter 5: Financial Data Governance\nAdditionally, certain aspects of the financial industry make it particularly vulnerable\nto security threats. First, safeguarding intellectual property through patents and copy‐\nrights has been shown to be more challenging and ambiguous  in the financial indus‐\ntry than in other industries such as manufacturing. Second, due to the complex\ninterdependencies of financial systems, a security breach in one financial institution\nmay cause a cascading shock that affects the entire system. Third, the monetary\nimpact of a security breach at a financial institution can be consequential, given that\nfinancial data is directly connected to client funds.\nGiven these considerations, data security and privacy have traditionally been regar‐\nded as a top priority by financial institutions and regulatory bodies. If you have\nworked at a financial institution, you must have noticed that security is given signifi‐\ncant weight in practically all discussions. Importantly, while the terms privacy and\nsecurity may be used interchangeably, they actually refer to distinct problems. To\nillustrate the difference, I will rely on international standards developed specifically\nfor information security and privacy.\nIn terms of data security, standard ISO 27001— Information Security Management Sys‐\ntems  (ISMS)  is the primary worldwide reference. It specifies a set of guidelines for\ndeveloping an ISMS system that protects data from cyber threats. The standard\nguides organizations through the stages required for ISMS development, such as\nassessing vulnerability risks, developing policies and procedures for data protection,\ntraining employees, and managing incidents.\nOn the other hand, standard ISO 27701 on developing a Privacy Information Manage‐\nment System  (PIMS)  builds on top of ISO 27001 to ensure that a system is in place to\nensure that personally identifiable  information  (PII) is handled in accordance with\ndata legislation and regulations (we discuss PII in detail in the “Data Privacy” on page\n201 section). The standard guides organizations through the steps needed to ensure\nthe protection of PII, compliance with data regulations, and transparency about how\norganizations handle personal data.\nFrom the two standards presented above, we can deduce that in designing for security\nand privacy, we consider different risks. When designing for security, we presume\nthat an adversary may launch a cyberattack against our organization. When designing\nfor privacy, we assume that personal data is not being handled in accordance with\nthe law.\nData Security and Privacy | 199\nWhat Types of Cyberattacks Are Committed\nAgainst Financial Institutions?\nFinancial institutions can be exposed to a variety of cyberattacks. These include but\nare not limited to the following:\nMalware\nMalicious software installed on a device connected to the internal institution sys‐\ntem. It can be a virus, spyware, Trojan, or other. If installed successfully, malware\ncan enable the hacker to access sensitive data and compromise critical systems.\nRansomware\nA special type of malware where the hacker gains access to the institution’s data\nand holds it hostage in exchange for the payment of a ransom.\nSpoofing\nWhen a cybercriminal manages to impersonate and replicate the website of a\nfinancial institution (often banks) that looks and functions the same way. If users\nare not careful, they may end up providing their personal information to the fake\nwebsite.\nSpam and phishing\nAn email-based cybercrime where a person sends emails to random people solic‐\niting them to send banking or credit card details.\nDistributed denial-of-service attack (DDoS)\nThis occurs when a cybercriminal floods a financial institution’s server with\ninternet requests.\nCorporate account takeover\nWhen a cybercriminal gains access to a corporate account associated with a\nfinancial institution. This may allow the attacker to initiate fraudulent money\ntransfers and other transactions.\nBrute force attacks\nWhen a hacker tries to guess the access credentials of a user via trial and error.\nAlthough it might seem outdated, it might still work if passwords are not strong\nenough.\nSQL injection\nA code injection technique where malicious SQL statements are inserted into a\nspecific part of the application accessible to the user in order to manipulate the\nfinal query submitted to the backend database. A successful SQL injection attack\nmight lead to data loss, a breach, or corruption.\n200 | Chapter 5: Financial Data Governance",9338
80-Data Privacy.pdf,80-Data Privacy,"22For a practical introduction to GDPR, I strongly recommend Paul Voigt and Axel von dem Bussche’s The ER\nGeneral Data Protection Regulation (GDPR): A Practical Guide , 1st ed. (Springer, 2017).An important thing to keep in mind is that cybercriminals are creative (newer ways of\nconducting cyberattacks are continuously being invented) and adaptive (work‐\narounds are developed to circumvent existing security measures). For this reason,\nensuring data security at financial institutions requires continuous monitoring, test‐\ning, and reinforcement.\nThe literature and practices surrounding data security and privacy are vast. Different\nfinancial organizations may confront different security issues and approach privacy\nin various ways. Furthermore, planning for security and privacy frequently involves a\nlarge number of individuals, including managers, chief information officers, chief\nsecurity officers, chief technology officers, security experts, network experts, infra‐\nstructure engineers, software engineers, data engineers, data architects, and data ana‐\nlysts. This book will concentrate on four major issues of interest to financial data\nengineers: data privacy regulations, data anonymization, data encryption, and access\ncontrol.\nData Privacy\nData privacy is a data governance practice that ensures data is collected, stored, pro‐\ncessed, and shared in accordance with data protection laws and regulations, as well as\nthe data subject’s general interests. Data privacy guarantees that sensitive information\nis not used for reasons other than those consented to by the data subject or estab‐\nlished by law. Personally identifiable  information  (PII) is possibly the most sensitive\nsort of information. PII refers to any data that can be used either alone or in combi‐\nnation with other data to identify an individual. This can include direct identifiers\nsuch as a person’s name, address, social security number, or email address, as well as\nindirect identifiers like birth date, phone number, IP address, or biometric data. In\nthe context of financial markets, financial PII may refer to bank account details,\ncredit card numbers, investment account information, and any other identifiers that\ncould potentially reveal an individual’s financial identity.\nA number of data regulations and laws have been devised and put into effect interna‐\ntionally in recent years. The most prominent and comprehensive framework is the\nEuropean Union’s General Data Protection Regulation  (GDPR) .22 In a nutshell,\nGDPR’s main goal is to give EU citizens more control over their personal data. It\napplies to all EU citizens as well as the entities that do business with them, including\nthose not based in EU countries. GDPR distinguishes between two types of data pro‐\ncessing entities: a data controller  and a data processor . A data controller is an entity\nthat collects personal data and determines the purposes (why) for which and the\nData Security and Privacy | 201\nmeans (how) by which personal data is processed. If multiple data controllers are\ninvolved, then the entity is called a joint controller . On the other hand, a data pro‐\ncessor is an entity that processes data on behalf of the controller. In most cases, the\ndata processor is an external third-party entity (e.g., a payroll company).\nIndividual rights defined in GDPR relate mostly to the collection, usage, sharing,\ntransfer, and deletion of personal data. Such rights can be grouped into three cate‐\ngories that you are likely to encounter:\nRight to access\nEU individuals have the right to access and request a copy of their personal data,\nas well as clarifications on how their data is processed, stored, and used.\nRight to be forgotten\nEU individuals have the right to request the deletion of their personal data or\nreject having their data processed.\nData portability\nWhen feasible, EU individuals should have the right to have their personal data\ntransmitted from one data controller to another.\nA variety of similar data protection laws have been introduced worldwide, such as:\n•The California Consumer Privacy Act (CCPA) in the US\n•The Gramm–Leach–Bliley Act\n•Canada’s Personal Information Protection and Electronic Documents Act\n(PIPEDA)\n•Japan’s Act on Protection of Personal Information (APPI)\n•Brazil’s General Data Protection Law\nWith the adoption of these regulations, the demand for privacy-preserving features in\nsystem design has increased considerably. As a financial data engineer, your responsi‐\nbility within this context is related to data collection, visibility, and utilization. For\nexample, if a user consents to the usage of their data for marketing purposes only,\nthen such a restriction needs to be taken into account in the data pipeline(s) that pro‐\ncess customer data. A good approach to enforcing data privacy is through data con‐\ntracts, where all privacy-related requirements are established and agreed upon by\nboth data producers and consumers.\n202 | Chapter 5: Financial Data Governance",5061
81-Data Anonymization.pdf,81-Data Anonymization,"23For a comparative study on this topic, I recommend Lorrie Faith Cranor, Kelly Idouchi, Pedro Giovanni Leon,\nManya Sleeper, and Blase Ur’s “ Are They Actually Any Different? Comparing Thousands of Financial Institu‐\ntions’ Privacy Practices” , presented at the 12th Annual Workshop on the Economics of Information Security\n(WEIS 2013).\n24A key distinction to remember under GDPR is the difference between anonymization and pseudonymization.\nAccording to GDPR, anonymization refers to the process of irreversibly removing personal identifiers from\ndata so that individuals can no longer be identified, making the data completely anonymous and not subject\nto GDPR. Pseudonymization, on the other hand, involves processing data in such a way that individuals can‐\nnot be identified without additional information, which is kept separately and protected. Unlike anonymized\ndata, pseudonymized data is still considered personal data under GDPR and remains subject to its regulations,\nas the potential to reidentify individuals exists if the additional information is accessed.When integrating data privacy elements into system design, a tradeoff might emerge\nbetween data confidentiality and data sharing. While limiting data sharing might\nincrease data security and confidentiality, it can also limit prospects for meaningful\ninnovation, both within the financial institution and with external partners. On the\nother side, enabling excessive data sharing exposes the company to security breaches,\nlegal penalties, and reputational risks.\nEnsuring privacy requires having a culture of privacy in the first place. Explaining the\nimpact that privacy infringement can have on the employees is an essential first step.\nThis encourages a due diligence mindset when handling data. Additionally, having\nmanagement support and interest in enforcing data privacy plays a crucial role.\nDepending on these and other factors, various financial institutions may invest differ‐\nently in data privacy standards.23\nOn the methodological side, a variety of data privacy techniques exist. The most\neffective of such techniques is data anonymization, which I explain in detail in the\nnext section.\nData Anonymization\nData anonymization is a data governance practice that ensures both data security and\nprivacy via transformations that obscure the identifiability of the data. If data is prop‐\nerly anonymized, then it loses its essential identification elements and cannot be\nlinked to specific data objects. This in turn makes it useless if it falls into the wrong\nhands. In financial institutions, anonymization can be adopted as a good data prac‐\ntice, but it may also be mandated by law. For example, GDPR states  that for data to be\nexempt from certain GRPD privacy restrictions, it needs to be anonymized.24\nData Security and Privacy | 203\n25For an introduction to risk-based data anonymization techniques, I highly recommend the book by Khaled El\nEmam and Luk Arbuckle, Anonymizing Health Data: Case Studies and Methods to Get You Started  (O’Reilly,\n2013).Anonymization strategy\nInterestingly, if you check any reference about data anonymization, you will notice\nthat there exists a large discrepancy in the presentation and categorization of data\nanonymization techniques. Consequently, to establish a baseline, I will initially out‐\nline the key factors to consider when devising or choosing a data anonymization\napproach.\nThe first element of an anonymization strategy is the identifiability  spectrum . At one\nend of the identifiability spectrum, data is completely identifiable. One way to achieve\nfull identifiability is via direct identifiers , which refer to values in a dataset that can\ndirectly identify a data object without additional information. Examples of direct\nidentifiers could be client name, social security number, financial security identifier,\ncompany name, and credit card number. For example, if you know that the ISIN of a\ncompany is US5949181045, then you can easily find out that this is Microsoft Corpo‐\nration. On the other hand, indirect identifiers  or quasi-identifiers  are values that, when\ncombined with other variables in the data, can identify a data object. Examples of\nindirect identifiers include company domiciliation, market capitalization, price, and\nthe name of the CEO. If your data has information about a company whose CEO in\n2023 is Satya Nadella, then it is very likely that we are talking about Microsoft Corpo‐\nration. Following this logic, we can place direct identifiers on one extreme of the\nspectrum followed by indirect identifiers. As you obscure or remove data identifiers,\nyou anonymize the data more and more, and it becomes more difficult to identify\ndata objects. At the other end of the spectrum, data is completely anonymized and it\nis not possible to distinguish one data object from another. To decide where to be on\nthe spectrum of identifiability, you need to consider the risks and costs of reidentifi‐\ncation. The higher such risks and costs, the higher the threshold is on the spectrum.25\nThe second element of a data anonymization strategy is analytical integrity . Suppose\nthat a financial institution agrees to share some of its internal data with a group of\nexternal researchers working on a specific project. To this end, the institution decides\nto anonymize the data. In this case, the anonymization strategy should take into\naccount the fact that data should still be valid for analysis. For example, if the dataset\nencompasses certain correlations between the variables that cannot be randomly\naltered, then it is important to use an anonymization technique that preserves such\nfeatures in the data.\nReversibility  is the third element, which denotes the possibility of reversing the ano‐\nnymization process by reidentifying the data. If the anonymization is done for data-\nsharing purposes, then it may not be necessary to reverse the process as it concerns a\n204 | Chapter 5: Financial Data Governance\ncopy of the data intended for external use. However, if anonymization is done for\ninternal purposes, e.g., credit card numbers, then it may be necessary to introduce\nreversibility into the anonymization strategy.\nThe fourth element is simplicity . A large number of anonymization techniques are\navailable and they differ in their implementation complexity and interpretability.\nSimple methods are easy to implement and reverse, while complex techniques require\nmore time and effort.\nFifth, anonymization can be performed statically or dynamically. In static anonymiza‐\ntion, data is anonymized and then stored in the final destination for safe future use.\nDynamic anonymization, also called interactive anonymization, applies anonymiza‐\ntion on the fly to the result of a query or request and not to the entire dataset. When\ndynamic anonymization is used, an important variable that needs to be taken into\naccount is performance and the speed at which data gets anonymized.\nFinally, anonymization can be applied in a deterministic or nondeterministic way. In\ndeterministic anonymization, the outcome of anonymization is always the same even\nif repeated multiple times. For example, if the name John Smith gets replaced by\nXXYREQ12, then repeating the process again would replace John Smith with\nXXYREQ12. In a nondeterministic anonymization, this is not a requirement. For\nexample, the name John Smith can be replaced by a randomly generated string that\ncan change every time you implement the anonymization.\nData anonymization is an investment that requires effort, expertise,\nand integration into your financial data infrastructure. For effi‐\nciency reasons, I recommend that you first classify your data based\non its sensitivity/confidentiality (e.g., Class A is public data, Class B\nis for internal use, Class G is strictly confidential, etc.) and then\nanonymize just the most critical data classes.\nAnonymization techniques\nAfter outlining your anonymization needs, you can employ a range of techniques for\nimplementation. But before moving forward, it’s important to differentiate between\nan anonymization technique and a measure of its effectiveness.\nAn anonymization technique takes a raw dataset as input and returns an anonymized\ndataset as output. An anonymization effectiveness measure evaluates the level of ano‐\nnymity of an anonymized dataset. Examples of anonymization effectiveness tech‐\nniques include k-anonymity , l-diversity , and t-closeness.  To illustrate the idea,\nk-anonymity,  for example, checks whether the information for each data object\nData Security and Privacy | 205\n26To learn more about these measures, I highly recommend the publication by Ninghui Li, Tiancheng Li, and\nSuresh Venkatasubramanian, “t-Closeness: Privacy Beyond k-Anonymity and l-Diversity” , in the 2007 IEEE\n23rd International Conference on Data Engineering  (IEEE, 2006): 106-115.\ncontained  in the dataset cannot be distinguished from at least k – 1 data objects\nwhose information also appears in the dataset.26\nThe rest of this section will focus on five common anonymization techniques: gener‐\nalization, suppression, distortion, swapping, and masking. To illustrate each of these,\nlet’s take an initial data sample ( Table 5-1 ) and see how each technique applies to it.\nY ou can always implement your own anonymization technique.\nHowever, before crafting your own solution, you should first con‐\nsider the available solutions and their use cases. Data anonymiza‐\ntion is a major field of study, and you are very likely going to find\nwhat you need in the literature.\nTable 5-1. Original data before anonymization\nID Company name CEO Headquarters Revenues Market capitalization\nXYA12F Standard Steel Corporation John Smith New York $45 mln $400 mln\nBFG76D Northwest Bank Lesly Charles Las Vegas $5.5 bln $50 bln\nM47GK General Bicycles Corporation Mary Jackson Chicago $650 mln $10 bln\nUsing the generalization technique involves substituting values with less specific yet\nconsistent alternatives. For example, instead of indicating the exact numbers for reve‐\nnues and market capitalization, we can use ranges. Table 5-2  illustrates the outcome\nof this generalization strategy.\nTable 5-2. Anonymized data after  generalization\nID Company name CEO Headquarters Revenues Market capitalization\nXYA12F Standard Steel Corporation John Smith New York $0–100 mln $0–1 bln\nBFG76D Northwest Bank Lesly Charles Las Vegas $5–10 bln $0–100 bln\nM47GK General Bicycles Corporation Mary Jackson Chicago $0–1 bln $0–50 bln\nAnother technique is suppression , which simply removes or drops an entire field from\na dataset. For example, in our data sample, we may want to suppress the direct identi‐\nfiers ID and company name by replacing all values with ********. Table 5-3  illustrates\nthe outcome of suppression.\n206 | Chapter 5: Financial Data Governance\nTable 5-3. Anonymized data after  suppression\nID Company name CEO Headquarters Revenues Market capitalization\n******** ******** John Smith New York $45 mln $400 mln\n******** ******** Lesly Charles Las Vegas $5.5 bln $50 bln\n******** ******** Mary Jackson Chicago $650 mln $10 bln\nAnother effective technique is distortion , which applies mostly to numerical fields\nand works by adding a certain noise to the values to alter their true value. For exam‐\nple, one can simply generate a random number from a given probability distribution\nand add it to the value of each record in the column. A large number of formulas can\nbe used for distortion. For illustrative purposes, let’s assume that we want to alter the\nvalues for revenues by multiplying each number by 1.1, and market capitalization by\n1.3. The outcome of this anonymization process is shown in Table 5-4 .\nTable 5-4. Anonymized data after  distortion\nID Company name CEO Headquarters Revenues Market capitalization\nXYA12F Standard Steel Corporation John Smith New York $49.5 mln $520 mln\nBFG76D Northwest Bank Lesly Charles Las Vegas $6.05 bln $65 bln\nM47GK General Bicycles Corporation Mary Jackson Chicago $715 mln $13 bln\nThe next technique is swapping, which works by shuffling the data within one or\nmore fields. For example, in our original data, we could shuffle the company and\nCEO names as illustrated in Table 5-5 .\nTable 5-5. Anonymized data after  swapping\nID Company name CEO Headquarters Revenues Market capitalization\nXYA12F Northwest Bank John Smith New York $45 mln $400 mln\nBFG76D General Bicycles Corporation Lesly Charles Las Vegas $5.5 bln $50 bln\nM47GK Standard Steel Corporation Mary Jackson Chicago $650 mln $10 bln\nOne of the more popular techniques is masking, which obfuscates sensitive data by\nusing a modified version with modified characters. For example, the ID field in our\ndata sample can be masked by keeping the first character and replacing numbers with\n0 and alphabetic characters with 1, as shown in Table 5-6 .\nTable 5-6. Anonymized data after  masking\nID Company name CEO Headquarters Revenues Market capitalization\nX11001 Standard Steel Corporation John Smith New York $45 mln $400 mln\nB11001 Northwest Bank Lesly Charles Las Vegas $5.5 bln $50 bln\nM0011 General Bicycles Corporation Mary Jackson Chicago $650 mln $10 bln\nData Security and Privacy | 207\n27See for example Cynthia Dwork’s “Differential Privacy: A Survey of Results” , in the Proceedings of the 5th\nInternational Conference on the Theory  and Applications of Models of Computation  (Springer, 2008): 1–19.In addition to these basic techniques, a variety of more advanced options are avail‐\nable. One prominent example is differential  privacy , a mathematically rigorous tech‐\nnique that has proven to be quite reliable.27 Another special technique that has found\napplications in finance, and in particular financial machine learning, is synthetic data .\nSynthetic data is machine-generated data that mirrors the properties of original\nsensitive  data. For example, if we are able to infer the probability distribution of a\nsensitive dataset (e.g., user account balance), then we can use such probability distri‐\nbution to generate a synthetic sample that preserves the same statistical properties of\nthe original dataset.\nPayment Tokenization\nOne of the most important applications of data anonymization in finance is payment\ntokenization. This is a security technique that uses cryptographic algorithms to con‐\nvert sensitive payment information such as credit card and bank account data into a\nunique, random string of characters called a “token. ” Successively, when conducting\npayment transactions, the token is used instead of the real payment details. If an\nunauthorized party gets their hands on a token, they won’t be able to do anything\nwith it.\nWithin the payment industry, several participants provide tokenization services,\nincluding payment processors and third-party tokenization vendors. Some payment\nservices providers, such as Stripe , provide tokenization-enabled payment hardware or\nsoftware as part of their service. Once payment tokens are generated, they are stored\nand secured in a secure vault managed by the tokenization service provider.\nWith payment tokenization, a business only needs to store the tokens of their custom‐\ners. When processing client transactions, the business can send the token to the toke‐\nnization service provider, which in turn maps the token to the original payment data\nsecurely. This technique can be quite useful for businesses that process recurring\ntransactions such as subscriptions or store customer profile details.\nVarious methods can be employed to generate tokens in payment tokenization. The\nsimplest approach is Random Number Generation (RNG), where tokens are gener‐\nated using a random number generator to produce a string of numbers or alphanu‐\nmeric characters. For added security, mathematical algorithms such as hashing or\nencryption can be employed. A technique called Format-Preserving Encryption\n(FPE) can be used to encrypt the card number in such a way that the resulting token\nretains the format of the original card number (e.g., same length and structure).\n208 | Chapter 5: Financial Data Governance",16271
82-Access Control.pdf,82-Access Control,"28Interested readers are encouraged to read Jonathan Katz and Y ehuda Lindell’s Introduction to Modern Cryp‐\ntography: Principles and Protocols  (Chapman and Hall/CRC, 2021).\nDon’t take it for granted that anonymization is bulletproof. Always\nkeep in mind that reidentification risks are present and can change\nand evolve as a result of multiple factors. For example, in 2006,\nNetflix launched a one-million-dollar open competition to enhance\nits movie recommender engine. To this end, Netflix publicly dis‐\nclosed one hundred million records exposing hundreds of thou‐\nsands of user ratings from 1999 to 2005. Although the released\ndataset contained no direct identifiers, two researchers were able to\nreidentify a subset of people in the data by cross-referencing Net‐\nflix data with IMDB.com  ratings .\nData Encryption\nData encryption is a fundamental practice in information security and privacy. It\ninvolves converting data into an unreadable format, rendering it meaningless to\nunauthorized individuals. Essentially, encryption transforms plain-text (unencryp‐\nted) data into ciphertext (encrypted) using a cryptographic algorithm. This process\nutilizes an encryption key to encode the data and a decryption key to decode it back\nto plain text. This way, if encrypted data falls into the wrong hands, then without the\ndecryption key, they will simply see gibberish text. However, this assumes that the\ndecryption key is kept safe!\nData can be encrypted in different states: at rest, in transit, and in use. Data at rest\nrefers to data residing in a storage location such as a hard disk or cloud storage. Data\nin transit refers to data that is being transferred from one location to another over a\nnetwork. Data in use is any data that is being processed or data that is temporarily\nheld in memory.\nThe field of cryptography is quite vast, and a full discussion of encryption methods\nand techniques is beyond the scope of this book.28 Nevertheless, to ensure the security\nof a financial data infrastructure, financial data engineers will benefit from having a\nbasic understanding of the essential concepts and principles of data encryption.\nAn important distinction to understand is between symmetric  and asymmetric\nencryption. In symmetric encryption, only one (symmetric) key is used to encrypt\nand decrypt the data. Symmetric keys are often considered more efficient to generate\nand faster at data encryption/decryption. However, they need to be shared and stored\ncarefully. This might occasionally necessitate encrypting the key itself using a differ‐\nent encryption key, which could result in a cycle of dependency. The most popular\nsymmetric encryption method is the Advanced Encryption Standard (AES),\ndeveloped  by the US National Institute of Standards and Technology (NIST).\nData Security and Privacy | 209\nNotably,  companies like Google utilize the AES to encrypt their data at the storage\nlevel .\nOn the other hand, asymmetric encryption  uses a pair of keys, called public and pri‐\nvate, to encrypt and decrypt the data. Anyone with the public key can encrypt and\nsend data; however, only the private key can decrypt the data. Due to this double-key\nfeature, asymmetric encryption is often considered more secure. However, asymmet‐\nric encryption could be more computationally expensive, especially for large data\npackets, as it relies on large encryption keys.\nThe most popular asymmetric encryption technique is Rivest–Shamir–Adleman\n(RSA). RSA generates the keys via a factorization operation of two prime numbers.\nThe public RSA can be used to encrypt the data, but only the person who knows the\nprime numbers can decrypt the data. RSA keys can be very large (e.g., 2,048 or 4,096\nbits are typical sizes), and thus their usage might have an impact on performance.\nNowadays, the use of data encryption has become a default and recommended prac‐\ntice both for data security and compliance purposes. This goes without saying for the\nfinancial sector. To give an example, let’s take the well-known standard, ISO 9564—\nPersonal Identification Number (PIN) management and security . ISO 9564 specifies\nprinciples and requirements for reliable and secure management of cardholder Per‐\nsonal Identification Numbers (PINs). PIN codes are used in many places such as\nautomated teller machine (ATM) systems, point-of-sale (POS) terminals, and vend‐\ning machines. When inserting the PIN, it needs to be transmitted to the issuer for\nverification. To secure PIN transmission, ISO 9564 requires that the PIN be encryp‐\nted, and specifies a list of approved encryption algorithms : Triple Data Encryption\nAlgorithm (TDEA), RSA, and Advanced Encryption Standard (AES).\nAccess Control\nAccess control is a data security practice that allows firms to manage access to their\ndata and related resources. A secure access control policy defines who has access to\nwhat, what type of access privileges are assigned, and a disaster management strategy\nto deal with access anomalies or incidents.\nThe two main components of access control are authentication  and authorization .\nAuthentication is an identity verification process that checks who is making the\naccess request. Once a user has been authenticated, authorization verifies which\nresources the user has access to and what access privileges they have.\nAccess control management is a continuous and primary function within any finan‐\ncial institution. The typical approach to access control involves creating and enforc‐\ning a set of guidelines and protocols to be followed when granting access and\nprivileges, as well as a monitoring system to alert against unauthorized access or\nexcessive rights. Although such procedures might differ from one institution to\n210 | Chapter 5: Financial Data Governance\nanother, a number of best practices have emerged over the years. For example, a quite\neffective principle is that of least privileges , which states that a user or application\nshould be granted the minimal amount of permissions required to perform their\ntasks. Excess privileges can be quite dangerous and lead to consequential incidents,\nespecially if the user is not aware of them. For example, if a new user is given access\nand read/write/update/delete privileges to the production database, then data is\nexposed to deletion or corruption risk.\nAnother best practice is multifactor authentication , which requires going through sep‐\narate factors to log in—for example, logging in via email plus entering a code that is\nsent to a linked mobile device. Of similar importance is the practice of access control\naudit logs , which involves monitoring and collecting information on user activities to\ndetect anomalous or unexpected privileges.\nCase Study: Payment Card Industry Data Security Standard\nTo give an empirical overview of how the financial industry formulates data security\nrequirements and policies, let’s illustrate the widely used Payment Card Industry Data\nSecurity Standard, or for short, PCI DSS.\nPCI DSS is a set of policies and procedures intended to ensure the security of pay‐\nment card transactions and associated data. PCI DSS is defined by the PCI Security\nStandards Council (SSC). Compliance with PCI DSS is not mandatory by law or reg‐\nulation. However, it is highly recommended for any institution that stores, processes,\nand transmits cardholder data. In some cases, such institutions might be required to\ncomply with PCI DSS due to a contractual clause. Furthermore, businesses that\ndemonstrate compliance with PCI DSS are more likely to be trusted in the market.\nCardholder data can be identification data such as PAN, cardholder name, expiry\ndate, and service code, as well as authentication data such as the magnetic stripe data,\ncard verification code (CVC), and PIN code.\nTo comply with PCI DSS, 12 requirements have been established, which can be grou‐\nped into 6 major goals:\n•Build and maintain secure networks for conducting card transactions. The rec‐\nommended approach suggests installing reliable firewalls to block and prevent\nunauthorized access to the network.\n•Protect cardholder data. The standard recommends storing only what’s necessary\nfor business operations. Card authentication data should never be stored. When\ntransmitting card data over a network, it should be encrypted.\n•Maintain a vulnerability management program to protect against attacks, and\nperform regular updates and patches of antivirus and operating systems.\n•Implement strong access measures via access policies, authentication, and\nauthorization, and ensure the physical security of data.\nData Security and Privacy | 211",8715
83-Part II. The Financial Data Engineering Lifecycle.pdf,83-Part II. The Financial Data Engineering Lifecycle,"•Regularly monitor and test networks by tracking all access to network resources\nand cardholder data and testing security systems.\n•Maintain an information security policy that highlights the duties and responsi‐\nbilities of the personnel and the potential consequences of noncompliance.\nFor more details on the standard specifications, consult the PCI DSS Quick Reference\nGuide available on the official PCI DSS web page .\nOngoing efforts are continuously improving security within financial markets. For\ninstance, the European Union is planning to introduce the Digital Operational Resil‐\nience Act (DORA)  to enhance the digital operational resilience of financial institu‐\ntions. DORA sets requirements for ICT risk management, incident reporting, and\ntesting to ensure firms can withstand, respond to, and recover from all types of ICT-\nrelated disruptions.\nSummary\nThis chapter provided an overview of financial data governance, which we can sum‐\nmarize as follows:\n•Defining financial data governance and illustrating its critical importance within\nthe financial domain\n•Introduction to data quality, with a discussion of nine dimensions relevant to\nfinancial data\n•Examination of data integrity through the lens of nine fundamental principles\npertinent to financial data\n•Illustration of primary security and privacy challenges and best practices affect‐\ning most financial institutions\nFinancial data governance can apply to any aspect of your institutions’ data infra‐\nstructure, strategy, and business operations. As you read further in this book, you will\nobserve how the different practices and principles discussed in this chapter are\nemployed.\nOver the next five chapters, we will go through the financial data engineering lifecy‐\ncle, where you will learn about the different layers you will need to implement when\ndesigning a financial data infrastructure.\n212 | Chapter 5: Financial Data Governance\nPART II\nThe Financial Data Engineering\nLifecycle\nIn the first part of this book, we explored the major ideas and problems around the\nmanagement of financial data, including the complexity of the financial data land‐\nscape, the diversity and nonuniversality of financial data identifiers, the problems of\nfinancial entity recognition and resolution, and financial data governance.\nIn the second part of this book, spanning Chapters 6 through 12, the focus shifts to\nthe technological aspects of financial data engineering. This includes the models,\ntools, frameworks, software systems, hardware components, libraries, design patterns,\nand networking systems needed to design and implement a financial data infrastruc‐\nture.\nChapter 6  will begin by introducing the financial data engineering lifecycle (FDEL), a\nconceptual framework that will be used to organize the many components of a finan‐\ncial data infrastructure into four structured layers: ingestion, storage, transformation\nand delivery, and monitoring. Subsequently, Chapters 7 through 10 will cover each of\nthese layers in detail. After that, Chapter 11  will discuss the various workflow archi‐\ntectures that are commonly used to implement the FDEL. Chapter 12  concludes with\nfour hands-on projects designed to familiarize you with key practices and technolo‐\ngies in financial data engineering.",3325
84-Chapter 6. Overview of the Financial Data Engineering Lifecycle.pdf,84-Chapter 6. Overview of the Financial Data Engineering Lifecycle,,0
85-Financial Data Engineering Lifecycle Defined.pdf,85-Financial Data Engineering Lifecycle Defined,"CHAPTER 6\nOverview of the Financial\nData Engineering Lifecycle\nAs a financial data engineer, navigating the multitude, diversity, and complexity of\navailable technological options can be overwhelming. Without a systematic approach\nin mind, this complexity may lead to chaotic situations and accumulating costly tech‐\nnical debt. Therefore, this chapter introduces a structured approach to financial data\nengineering, organizing its components into a layered architecture called the financial\ndata engineering lifecycle  (FDEL). This framework draws inspiration from the founda‐\ntional work of Joe Reis and Matt Housley  on data engineering lifecycles.\nIn this chapter, I will introduce the FDEL and outline its four layers: ingestion, stor‐\nage, transformation and delivery, and monitoring. Following this, I will discuss spe‐\ncific criteria that financial data engineers can consider when selecting technologies to\nsupport the FDEL. Please note that since there is so much information to cover, I’ll\nsave the details of each FDEL layer for Chapters 7 through 10.\nFinancial Data Engineering Lifecycle Defined\nData engineering is a fast-moving and continuously evolving field. However, if there\nis one constant that characterizes and defines the job of a data engineer, it is the fact\nthat it revolves around systems that perform a series of actions for the extraction,\ntransformation, storage, and consumption of data.\nTo move beyond the simple view of data engineering as merely a series of data-related\ntasks, authors Joe Reis and Matt Housley introduced the Data Engineering Lifecycle\n(DEL) . This concept offers a structured framework that formalizes the various stages\nof data engineering. According to the authors, the DEL “comprises stages that turn\n215\n1Layered architectures are quite common in software engineering. Their main advantages include simplicity,\nmaintainability, familiarity, and cost. To learn more about this topic, I recommend Mark Richards and Neal\nFord’s Fundamentals of Software  Architecture: An Engineering Approach  (O’Reilly, 2020).\nraw data ingredients into a useful end product, ready for consumption by analysts,\ndata scientists, ML engineers, and others. ”\nIt is essential to differentiate between the data lifecycle and the data\nengineering lifecycle. While they are related, the data lifecycle is a\nconceptual model that describes the stages data goes through from\ncreation to archival or deletion. On the other hand, the data engi‐\nneering lifecycle is a practical framework that outlines the pro‐\ncesses, patterns, and tools used to manage data throughout its\nlifecycle, ultimately delivering a final product to data consumers.\nThe DEL can be adapted to incorporate several processes or stages depending on how\ncomplicated the data lifecycle and business needs are. In their original work, Joe Reis\nand Matt Housley divided the DEL into five stages: data generation, storage, inges‐\ntion, transformation, and serving.\nWhen applied to the financial domain, the data engineering lifecycle needs to be\nadapted to account for a number of domain-specific elements. These originate from\nthe strict data governance and regulatory requirements for financial institutions, legal\nand content licensing constraints, the complexity of the financial data landscape,\nindustry-specific software standards and protocols, and the unique demands for low\nlatency, high throughput, and other performance constraints that characterize finan‐\ncial operations.\nTo offer a domain-focused perspective that incorporates these considerations, this\nbook introduces the financial  data engineering lifecycle  (FDEL), a layered framework1\nconsisting of four layers: ingestion layer, storage layer, transformation and delivery\nlayer,  and monitoring layer . Figure 6-1  illustrates this framework.\nIn the ingestion layer, data engineers design and implement an infrastructure for han‐\ndling the generation and reception of data coming from different sources and in dif‐\nferent formats, volumes, and frequencies. This layer is quite critical, as errors or\nperformance bottlenecks in ingestion are likely to propagate downstream and impact\nthe entire FDEL. In Chapter 7 , we will explore the ingestion layer in depth, covering\nvarious ingestion processes, patterns, technologies, and formats.\nFollowing is the storage layer, where the focus is on selecting and optimizing data\nstorage models and technologies to meet various business requirements. Choosing\nthe right data storage system for a given problem is one of the most important deci‐\nsions throughout the FDEL. A poor data storage choice can very easily translate into\n216 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nbottlenecks, degraded performance, and rocketing costs, which in turn can jeopardize\nproduct development and analytical capabilities. Chapter 8  will detail the varieties of\ndata storage models and illustrate their data modeling principles, internal features,\nand financial use cases.\nFigure 6-1. Layers of the financial  data engineering lifecycle\nNext is the transformation and delivery layer, which performs a number of business-\ndefined transformations and computations that produce high-quality data that is\nready for consumption by its intended data consumers. In Chapter 9 , I will thor‐\noughly explore this layer, examining the key types of transformations applicable to\nfinancial data, the computational demands of different transformation processes, and\nthe various mechanisms for data delivery.\nFinally, the monitoring layer is there to make sure that issues related to data process‐\ning, data quality, performance, costs, bugs, and analytical errors are all monitored and\ntracked to allow efficient and timely fixes. Chapter 10  will be dedicated to this layer.\nThe FDEL framework will provide several advantages if correctly applied. Y ou may\nbenefit greatly, for instance, in terms of modularity and separation of responsibility.\nThe larger the firm is, the larger and more complex its data engineering processes are\ngoing to become. This suggests that, from an organizational perspective, it could be\nfeasible to assign distinct teams to each FDEL layer. For example, a team may be dedi‐\ncated to optimizing and securing the ingestion layer, another for ensuring the perfor‐\nmance, scalability, and security of the storage layer, and so on. It is important to note\nthat the FDEL is an iterative  process where feedback and requirements from one layer\nmay impact the design, constraints, and implementation of another layer.\nOver the next four chapters, I will detail these layers, highlighting the domain-specific\nchallenges they present in financial markets, and offering concrete examples to pro‐\nvide deeper insights.\nFinancial Data Engineering Lifecycle Defined  | 217",6880
86-Criteria for Building the Financial Data Engineering Stack.pdf,86-Criteria for Building the Financial Data Engineering Stack,,0
87-Criterion 1 Open Source Versus Commercial Software.pdf,87-Criterion 1 Open Source Versus Commercial Software,"2According to the Basel framework , operational risk is defined as “the risk of loss resulting from inadequate or\nfailed internal processes, people and systems or from external events. ”Criteria for Building the Financial Data Engineering Stack\nWhen implementing the FDEL, it’s essential to choose the best tools and technologies\nto design and architect each layer, i.e., the technological stack. This is no small feat. If\nyou want to learn about data engineering technologies and you do a quick search, you\nwill find hundreds of different tools and frameworks. This makes it hard to know\nwhere to start since there is practically a jungle of tools to choose from. To make it\nmore complex, these diverse tools can be used together and sometimes compete to\nprocess financial data throughout its lifecycle.\nIn this section, I will outline six criteria that financial institutions can employ to\ninform their technological decisions. These criteria serve as a fundamental and gen‐\neral guideline but are not exhaustive. Depending on your organization’s unique\nneeds, you may need to identify additional criteria. Furthermore, in the following five\nchapters, I will discuss more granular criteria that are tailored to each layer of the\nFDEL.\nCriterion 1: Open Source Versus Commercial Software\nA crucial and highly debated decision in financial institutions concerns the choice\nbetween proprietary commercial software and open source software. Proprietary\ncommercial software is distributed under a purchased license that restricts user access\nto the source code. On the other hand, open source software is typically distributed\nwith a license to access, use, modify, sell, and distribute the source code.\nThe main advantages of commercial software include the following:\nVendor accountability\nThe software provider ensures frequent updates, bug fixes, security, and cus‐\ntomer support. This is typically ensured via a Service-Level Agreement  (SLA),\nwhich is a guarantee by the vendor to commit to a certain level of quality, avail‐\nability, and performance of the offered service. Vendors might need to pay a pen‐\nalty if their SLA is not met. SLAs and security are two of the most important\nfactors behind the widespread reliance on proprietary software by the financial\nsector, given the risk aversion culture in financial markets that is driven by regu‐\nlatory and security concerns. Vendor accountability and SLAs can help financial\ninstitutions measure and manage their operational risks, which is a regulatory\nrequirement under frameworks such as Basel III.2\n218218 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nProduct integrations\nThe vendor guarantees seamless integration between different commercial prod‐\nucts, which in turn can be a major cost-saving factor. Financial institutions rely\nheavily on trust when conducting a new activity or project. Therefore, they are\nmore likely to adopt a new product or service that integrates with their current\nofferings if it is offered by a reliable software provider.\nEnterprise-grade features\nThe vendor offers specific features for large corporations, including scalability,\nsecurity, and compliance. This is particularly relevant for financial institutions\noperating on a large scale, with stringent requirements of audit and monitoring.\nUser-friendly experience\nCommercial products are often offered with an easy-to-use interface and offer\nrich documentation and guides. Employees at financial institutions are often\nusers rather than developers; therefore, a user-friendly interface is likely to be\nmore welcomed than a complex one.\nOn the negative side, commercial software may come with disadvantages such as the\nfollowing:\nCost\nCommercial software licenses can be very expensive and include a variable part\nthat is hard to predict (e.g., support fees, hidden features). For large financial\ninstitutions that have critical applications, the risks can easily outweigh the costs.\nBulky products\nMany commercial software tools come in a single package that includes a large\nnumber of features. Some financial institutions (e.g., FinTech firms) may not\nneed all of these features and end up using only a fraction of what the tool offers.\nVendor lock-in\nThe more a company relies on commercial tools, the harder it is to switch to\nother options, including open source. Vendor lock-in gets stronger for factors\nsuch as network effect (where product value increases with its user base), and\nrisk aversion (uncertainty from switching to another solution).\nLack of customization\nCommercial software is proprietary, meaning that financial clients may not able\nto adapt or modify the product to their unique needs or client expectations.\nTraditionally, financial institutions have been skewed toward proprietary software\nbecause internal control rules, policies, standards, procedures, and IT audit checklists\nall lay out strict requirements for support agreements. The top financial services soft‐\nware vendors include Microsoft, FIS Global, SAP , Oracle, IBM, and NCR\nCorporation.\nCriteria for Building the Financial Data Engineering Stack | 219\nThe Oracle Database: Why Is It Widely Used by Banks?\nTo illustrate why financial institutions have traditionally preferred commercial soft‐\nware, let’s take as an example the Oracle Database. If you have ever wondered what\ntype of database systems big banks use for their core operations, the answer is likely\nto include a mention of Oracle. It is perhaps the most popular enterprise-level data‐\nbase system among financial institutions. Interestingly, according to the DB-Engines\nranking , as of 2024, Oracle is considered the most popular database system in the\nworld.\nThe Oracle Database, a commercial SQL database management system by Oracle\nCorporation, is predominantly used by financial institutions for online transaction\nprocessing (OLTP) purposes. It is used to store and manage core business data\nencompassing credit, accounts, loans, and transactions.\nOver time, new database systems have emerged in the market, and some are gaining\ntraction. However, to date, Oracle remains the gold standard for financial services.\nFirst of all, Oracle has been in the market since 1979; therefore, it is considered a\nmature and reliable product. Throughout its history, Oracle has consistently added\nfeatures to its flagship products , placing the company at the forefront of database\ntechnology. By offering many features, the Oracle database could fit the many appli‐\ncations, requirements, and use cases that have emerged in the financial sector.\nSecond, the Oracle Database is flexible because it can run on Windows and various\nflavors of Unix. Additionally, a variety of Oracle database editions are available , rang‐\ning from large-scale editions such as the Oracle Database Enterprise Edition to single-\nuser editions like the Oracle Database Personal Edition. Oracle Database products are\navailable on Oracle hardware as well as being offered by several service providers on\npremises, on cloud, or as a hybrid cloud installation.\nThird, Oracle clients can access Oracle support services through the company’s own\nsupport team or third-party consultants. This allows financial institutions to develop\na feeling of trust and security as they have a reliable partner to call when needed.\nIn conclusion, maturity, stability, and support offer reliability against failures and\nrisks, which is essential for critical applications such as finance. This explains the high\npopularity of Oracle among financial institutions and in the market in general.\n220 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nThe main alternative to commercial financial software is open source software.\nUnlike proprietary solutions, open source is typically licensed for free, and its devel‐\nopment and maintenance is led by the public and/or by associations such as the\nApache Foundation. The most popular type of open source license is the Apache 2.0\nlicense , which allows users to obtain, modify, distribute, sublicense, and patent the\nsoftware as well as use it for commercial purposes that can include proprietary\nsoftware.\nA large number of open source software options are available. Examples include\nLinux, PostgreSQL, Firefox, Kubernetes, PyTorch, Apache Spark, Apache Cassandra,\nApache Airflow, and many more.\nThe main advantages of open source include the following:\nCost\nOpen source software is often available under free licensing. Nevertheless, com‐\npared to commercial software, open source might entail indirect costs such as\nmaintenance, upgrades, and feature development costs.\nCustomization\nBecause the source code is available to anyone to use and modify, companies can\nintroduce new features that accommodate their business needs.\nCommunity\nOpen source software tends to have a large community of developers who\nactively add new features and fix existing bugs. The main advantage of commu‐\nnity contributions is that they aggregate the skills and knowledge of multiple peo‐\nple from different backgrounds and experiences. This, in turn, can guarantee that\nthe final solution is very likely the best in the market.\nTransparency\nBecause the entire codebase is available to the public, open source software tends\nto offer more transparency into its internal workings, unlike proprietary soft‐\nware, which is a black box.\nCriteria for Building the Financial Data Engineering Stack | 221\nOn the other hand, open source software could face the following drawbacks:\nSupport\nUsers can freely report a bug or ask for a new feature in open source software,\nbut this usually takes time as contributors tend to dedicate a variable amount of\ntime to maintain the software. For mission-critical applications such as financial\npayments, this may not be acceptable.\nDocumentation\nOpen source projects may lack up-to-date, detailed documentation and usage\ninstructions.\nComplexity\nThe evolution of open source projects is characterized by frequent updates to\naddress bugs and introduce new functionalities. Consequently, as the codebase\ngrows with additional features, it may become increasingly complex to\ncomprehend.\nCompatibility\nA major issue with open source software is compatibility with other software\napplications, which can jeopardize product development and integration efforts.\nSecurity\nWith open source software, malicious actors and cybercriminals find and exploit\ncode vulnerabilities more easily.\nConfidentiality\nOpen source is based on the idea of community-driven collaboration and knowl‐\nedge sharing. For financial institutions, sharing their codebase might be impossi‐\nble if it becomes confidential or provides a competitive advantage that the\ninstitution wants to protect.\nThe use of open source software in financial services has significantly increased in the\nlast few years. According to a report published by the Fintech Open Source Founda‐\ntion (FINOS) , 2022 saw a 43% increase in the number of GitHub repositories with\ncode contributors from the financial services industry. The report indicates that most\nof the contributions financial institutions make to open source projects are related to\nweb development, cloud and containerization software, AI/ML, and continuous\nintegration/continuous delivery (CI/CD).\nHaving said all of this, how would you choose between open source and proprietary\nsoftware?\nFirst, the pros and cons of open source and proprietary software can vary depending\non the specific business problem and context; therefore, no single solution can fit all\ncircumstances. In some cases, certain problems can have a well-accepted solution\n222 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\namong market participants. For example, if your application requires high standards\nof security and reliable 24/7 customer support, then commercial software might be\nthe right choice. On the other hand, if you are exploring the possibility of employing\nAI to build new product features, you might want to use open source libraries. Nowa‐\ndays, it is common for the same financial institution to use both commercial and\nopen source software.\nSome commercial software products are based on open source\nframeworks. These frameworks can either be provided as managed\nservices by vendors or form the foundation for distinct solutions.\nFor instance, PostgreSQL, an open source database management\nsystem, serves as the basis for Amazon’s Relational Database Ser‐\nvice (RDS) for PostgreSQL, a managed service offered by AWS.\nConversely, Amazon Redshift, a proprietary data warehousing ser‐\nvice, is built on top of PostgreSQL but is a separate product offered\nby AWS.\nSecond, consider your budget. If you are a small startup developing cutting-edge\nfinancial technology, you will likely be limited in your budget. In this case, using\nopen source might be the best option. On the other hand, if you are a large multina‐\ntional financial institution with big teams and departments, spending the money on\ncommercial software might be more efficient as it will save the team time and effort\nwhen fixing bugs and dealing with security issues. Large corporations prefer to use\ncommercial products as they offer a solution that standardizes internal operations\nacross departments and teams.\nThird, the level of technical expertise is a crucial factor. If you are planning on devel‐\noping software that managers and accountants will use, then unless there is a big IT\ndepartment, these professionals are likely to have limited expertise in software devel‐\nopment to understand and interact with the codebase. In this case, vendor software is\nlikely to cause fewer headaches. On the other hand, if your institution is well-\nequipped with software engineers who can support and help users, it would be possi‐\nble to take advantage of the flexibility and developmental freedom that open source\nsoftware offers.\nFourth, consider the urgency and impact on the competitive advantage of adopting a\ngiven software tool. If a financial institution feels behind compared to the market\nwith open source adoption, then it might need to accelerate the adoption process.\nHowever, if the company is currently experimenting with open source for potential\nuses, then it might be possible to do it slowly.\nCriteria for Building the Financial Data Engineering Stack | 223",14517
88-Criterion 2 Ease of Use Versus Performance.pdf,88-Criterion 2 Ease of Use Versus Performance,"In-House Proprietary Financial Software\nA lot of innovative financial institutions create their own in-house software, mainly\nin response to customer demand and changing client expectations, which can be chal‐\nlenging to fulfill by relying on third-party software vendors. For example, to boost the\ntime-to-market of its new financial products, the US financial institution JPMorgan\nChase developed Kapital , an advanced financial risk management and pricing soft‐\nware system. More recently, JPMorgan Chase leveraged the Python ecosystem to\nbuild Athena, a cross-asset platform for trading, risk management, and analytics used\ninternally as well as by external clients. According to a presentation given by Misha\nTselman , executive director at JPMorgan Chase, at PyData 2017, Athena has thou‐\nsands of users, more than 1,500 Python developers contributing to it, and uses over\n150,000 Python modules.\nAnother prominent example is Aladdin , an end-to-end portfolio and investment\nmanagement platform developed by the largest asset manager in the world, Black‐\nRock. Given its significant success and reliability, the Aladdin platform is no longer\nonly an internal tool; rather, it is significantly influencing the way the financial sector\nworks. Scholars use the term platform economy  to describe such a scenario, where\nplatforms play a major role in facilitating market activities.\nIf you are interested in learning more about this topic, I highly recommend the paper\nby Dirk A. Zetzsche, William A. Birdthistle, Douglas W . Arner, and Ross P . Buckley,\n“Digital Finance Platforms: Toward a New Regulatory Paradigm” , European Banking\nInstitute (EBI) Working Paper Series No. 58 (2020): 273.\nCriterion 2: Ease of Use Versus Performance\nWhen choosing software or data technology, a frequent tradeoff arises between com‐\nplexity and performance. Software technology can be complex if it has a steep learn‐\ning curve: it may be difficult to understand or master the syntax, many elements must\nbe managed in the code, it is demanding to set up and use, hard to debug, challenging\nto rerun on other machines, difficult to integrate with other software, and hard to\nextend with new features, deploy, or share with others. On the other hand, software is\nperformant if it optimizes dimensions such as execution time, latency, throughput,\nI/O access, memory usage/footprint, carbon footprint, dependency and package\nmanagement, scalability, security, or other custom metrics specific to the business\noperations.\nThe reason why there is a tradeoff between complexity and performance is that per‐\nformance optimization often requires low-level interaction and configuration of the\ntechnology. Consider, for example, the difference between driving a manual versus an\nautomatic car. To make a more concrete example, let’s think about programming lan‐\nguages. A programming language such as C can be considered by many as a\n224 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\n3For a high-level introduction to the CPU, GPU, and FPGA, I recommend Intel’s article “Compare Benefits of\nCPUs, GPUs, and FPGAs for Different oneAPI Compute Workloads” . \nchallenging  language to work with due to its strict syntax requirements, exposure to\nlow-level details, manual garbage collection, static typing, and memory management.\nOnce you learn how to work with languages like C, your performance advantages\ncould be substantial.\nOn the other hand, programming languages such as Python are well-known for their\nfriendly syntax, usability, automatic garbage collection, and ease of integration. How‐\never, a number of features of Python make it less performant in terms of speed than\nC, such as the reliance on dynamic typing and the multistep compilation process to\nmachine code.\nThe same logic applies to hardware. For example, engineers can leverage different\ncomputer processors and accelerators such as Central Processing Units (CPUs),\nGraphical Processing Units (GPUs), and Field Programmable Gate Arrays (FPGAs)\nto perform various tasks.3 The CPU is the computer’s main processor, and it excels in\nperforming a wide range of tasks. However, certain applications might require cus‐\ntom optimizations (e.g., high parallelism or low latency), which can be achieved\nthrough the use of specialized processors such as a GPU and FPGA. Crucially, using a\nGPU and FPGA is more complex and requires good knowledge of their internals,\ncost, and implementation principles.\nWhen comparing hardware and software technologies, pay close\nattention. Not everything is as easy as it looks on the surface. For\ninstance, it’s not unusual to come across articles that claim some‐\nthing like a GPU outperforms a CPU, or C# is superior to Python,\nand so forth. Even though they could have some truth to them,\nthese generalizations have three problems. First, it might be chal‐\nlenging to compare technologies that are created according to dif‐\nferent design principles. Second, technologies and the concepts\nbehind their design change throughout time, and as a result, a\ntechnology that was once superior to another may not be any\nlonger. Third, depending on the task that they are assessed against,\ntechnologies may perform differently. What I recommend in this\ncase is to find a trusted reference from literature that you can rely\non to derive a solid foundation for comparison.\nTraditionally, financial market participants and researchers have thoroughly tested\nand benchmarked a number of technologies to certain business and technological\nrequirements. For example, with time-critical or large-scale financial applications\nsuch as algorithmic trading, Monte Carlo simulations, asset pricing, and fraud\nCriteria for Building the Financial Data Engineering Stack | 225\n4For a benchmark study, I recommend S. Borağan Aruoba and Jesús Fernández-Villaverde’s “ A Comparison of\nProgramming Languages in Macroeconomics” , Journal of Economic Dynamics and Control  58 (September\n2015): 265-273.detection,  financial institutions might prefer to use performant languages such as C,\nC++, Rust, and Java.4 However, for less critical or small applications or situations\nwhere users are not highly skilled in software engineering internals, a more user-\nfriendly language such as Python, R, or Julia might be a better choice.\nThe Quest for Low Latency in Financial Markets\nLow latency is a key differentiator when it comes to performance in financial markets.\nGenerally speaking, the notion of latency is often used to describe the time it takes for\na request to travel over a network to its destination and receive a response back from\nit. In many financial domains, a firm’s ability to grasp market opportunities or other‐\nwise lose money is largely dependent on how quickly it reacts to market events. This\nis particularly the case in electronic trading systems, and more specifically, high-\nfrequency and algorithmic trading.\nThere are several aspects to latency. First, it may concern the speed at which financial\ndata is harvested and distributed to the many players involved in the trading system.\nThe faster a financial institution can access market data and make trading choices, the\nmore probable it is to capitalize on important short-term opportunities. The second\naspect relates to the speed of the order execution flow. The faster an order reaches its\ndestination, the more likely it is that the firm will make profits out of it. A millisecond\ndifference in data access and trade processing time can equate to millions of dollars of\ngain or loss. For example, when executing an arbitrage strategy to profit from tiny dif‐\nferences in asset prices in two or more markets, the investing firm will have a tiny\ntemporal window before the market reaches parity.\nWhat markets consider “low” latency is not well defined but rather an evolving para‐\ndigm. Different financial market segments may operate and make improvements at\ndifferent orders of time magnitude, such as the second, millisecond (thousandth of a\nsecond), microsecond (millionth of a second), and nanosecond (billionth of a sec‐\nond). For example, real-time payments may be considered fast at the second or milli‐\nsecond level, while high-frequency traders are chasing the microsecond and\nnanosecond.\nNumerous variables can influence latency in the financial markets. For example, a\ntrading firm that speculates on short-term market opportunities will be impacted by\nthe distance between its trading system and the trading venue, the distance between\ntrading venues (in case of arbitrage), the choice of the programming language (e.g.,\nC++  or Rust), hardware (e.g., CPU or FPGA), the efficiency of the trading program\n(e.g., time complexity), and cabling (e.g., copper versus fiber versus microwave).\n226 | Chapter 6: Overview of the Financial Data Engineering Lifecycle",8929
89-Criterion 3 Cloud Versus On Premises.pdf,89-Criterion 3 Cloud Versus On Premises,"Due to the potential for large returns, low-latency trading firms constantly invest a\ngreat deal of resources and effort to accelerate their trading systems. One approach is\ncalled Direct Market Access  (DMA), a trading arrangement that allows traders and\ninvestors to place orders directly into the order book of an exchange, bypassing inter‐\nmediaries such as brokers.\nStandardized communication protocols, such as FIX, are essential for DMA, as they\nallow traders, brokers, and exchanges to exchange trade-related data using a unified\nlanguage (we discuss FIX in Chapter 7 ). DMA usually involves a sophisticated and\ncostly technology setup. Consequently, it is commonly offered as a service by sell-side\nfirms and specialized technology providers.\nIn addition, to provide markets with the opportunity to gain a speed advantage, stock\nexchanges started providing trading firms with the option to position their trading\nservers in close proximity to, or even within the same data center as, the exchange\nservers.\nThis strategy, called colocation , allows trading firms to access financial data through\ndata feeds that stream data right from the source as soon as it appears in the exchange\nserver. Colocation has raised some issues of fairness in financial markets , as the aver‐\nage investor or market maker will get the information at a later date, when they will\nhave less advantage to react.\nAnother direction toward low latency in financial markets relies on the use of special‐\nized hardware such as a field-programmable gate array (FPGA). An FPGA is a config‐\nurable type of integrated circuit that can be programmed after manufacturing to suit\nwhatever purpose and needs you want. As such, FPGAs have been widely adopted by\nfinancial markets to build systems that react to market events and process orders in\nnanoseconds, as well as to perform complex/parallel computations efficiently.\nCriterion 3: Cloud Versus On Premises\nIn today’s technological landscape, a major and critical technological choice often\nemerges between cloud-based and on-premises infrastructures. In an on-premises\nserver infrastructure, the firm owns, controls, and maintains a group of servers (often\ncalled commodity servers) for its storage and computing purposes. Alternatively, a\nfirm can use the cloud to host or lease servers at a third-party cloud provider or use\nthe vendor’s managed servers and services directly.\nThere are pros and cons to using either on premises or the cloud, and making the\nright decision may be quite challenging. Generally speaking, on premises is viewed by\nmany as the traditional (or legacy) way of managing server infrastructure, while the\ncloud represents the modern and user-friendly approach. This, however, doesn’t\nmean that on premises is bad and the cloud is good. Let’s explore in more detail the\nmain differences and features of both solutions.\nCriteria for Building the Financial Data Engineering Stack | 227\nOn premises\nIn an on-premises setting, the financial institution is fully responsible for its servers\nand data infrastructure. This means that the institution owns, configures, and man‐\nages its servers, computing environments, networking, security, scalability policies,\nlogs, and user access.\nAs you might have guessed, the main advantage of this infrastructure choice is that\nyou have full control of your digital assets. If managed properly, an on-premises\ninfrastructure can guarantee maximum security. Having all your data and software\nreside within your organization’s premises provides a feeling of safety and peace.\nSecurity is a major concern for highly regulated businesses like finance; hence, on\npremises is a very typical choice.\nNevertheless, the on-premises approach has a number of potential downsides. Cost is\nperhaps the main issue. To maintain an on-premises infrastructure, institutions need\nto have a dedicated IT department responsible for software and hardware mainte‐\nnance, availability, updates, security, license purchases, and user support. Such costs\nmight rise exponentially, especially in large institutions or in the presence of poor\nmanagement and usage practices.\nKeep in mind that cost is not always a downside to on-premises\nsolutions. In some cases, it might be the best cost-efficient solution\nif configured and managed with high quality standards as well as\nsoftware-aware and hardware-aware best practices. For example, in\n2023, X (formerly known as Twitter) announced that it imple‐\nmented a cloud exit strategy  by moving data and workload out\nfrom the cloud and onto its own on-premises servers. The move\nresulted in a 60% decrease in cloud data storage size and a 75%\ndecrease in cloud data processing costs.\nThe second downside is scalability. In an on-premises setting, institutions have a cer‐\ntain number of servers that they manage. As long as the infrastructure can handle the\nload, there should be no problems. However, if the load increases much more than\nthe current server infrastructure can handle, then either the system will experience\ndowntime issues or more servers need to be added. Adding more servers can be\ncostly and time-consuming; and once added, they can’t simply be removed and\nreturned. One solution to this issue is to overprovision by having extra server\ncapacity all the time. However, this strategy can lead to extra costs due to unused\nresources.\nThe third drawback is that institutions may find themselves spending less time on\nproduct and feature development and more time on server maintenance and time-\nconsuming technical, bureaucratic processes. Large financial institutions may tolerate\n228 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nthis; however, for smaller institutions such as FinTech firms, the burden may not be\nsustainable.\nMainframe Architecture\nAs far as the financial sector is concerned, it is hard to avoid mentioning mainframes .\nIn a few words, a mainframe is a highly performant, scalable, and resilient computer\nendowed with large amounts of memory and processing power. The main advantage\nof mainframe computers is their ability to handle massive volumes of simultaneous\ntransactions and requests while at the same time ensuring reliability, security, avail‐\nability, high throughput, and low latency.\nThis feature makes mainframes well-suited for core financial applications such as\nbanking, customer order processing, payment processing, and various mission-\ncritical tasks that process billions of transactions on a daily basis. If you have ever\ninteracted with an automated teller machine (ATM) or point-of-sale terminal at a\nretail shop, your request likely involved a mainframe at some point.\nThe market pioneer in mainframes is IBM , which has been producing mainframe\ncomputers since 1952. IBM mainframes have evolved over time to meet the diverse\nrequirements of resilience, security, and performance. The primary mainframe sys‐\ntem offered by IBM is the IBM Z System, which includes various generations such as\nthe IBM z15, IBM z14, and IBM z13. Complementing these mainframes are a range\nof related IBM offerings, including IBM’s proprietary operating system, known as\nIBM z/OS; IBM’s transaction processing system, known as the Customer Information\nControl System (CICS); IBM’s relational database management system DB2; and the\nmiddleware communication system IBM MQ. This integrated and secure ecosystem\nhas enabled IBM to maintain a significant market share within the financial sector,\nparticularly for payment and transaction processing.\nCloud computing\nA major alternative and competitor of the on-premises model is the cloud. In this\nbook, we define the cloud as:\nA general-purpose technology that enables the development and delivery of comput‐\ning and storage services over a networked system.\nIn this definition, the term general-purpose technology  refers to a technology whose \nadoption and impact span many sectors within the economy, substantially impacting\npreexisting social and economic structures. The best examples of general-purpose\ntechnologies are electricity, the internet, and artificial intelligence. The cloud is\nalready a general-purpose technology, given its wide range of applications and\nCriteria for Building the Financial Data Engineering Stack | 229\n5For a detailed study on the cloud as a general-purpose technology, I recommend Federico Etro’s “The Eco‐\nnomic Impact of Cloud Computing on Business Creation, Employment and Output in Europe” , Review of\nBusiness and Economics  no. 2 (January 2009): 179–208.adoption  in businesses, governments, hospitals, research centers, financial institu‐\ntions, educational institutions, the military, and many more.5\nThe most important thing about the cloud is how it’s delivered, not just the services it\ndelivers. Many of the underlying technologies powering the cloud already exist and\nare utilized in various applications. This includes, for example, software and hard‐\nware virtualization technologies, multicore technologies, networking (network com‐\nputing, software-defined networking, software-defined security, W AN), data centers,\ndatabase management systems, file storage systems, open source tools, machine\nlearning, and DevOps, to name a few. However, with the cloud, such technologies\nhave been abstracted and architectured in a novel way that allows for the delivery of\nscalable, highly available, managed, and pay-as-you-go services through a networked\nsystem. The public internet is the primary delivery network, but it is also possible to\nestablish a private cloud that operates on premises or in a data center managed by a\ncloud provider. In the next section, we will explore in detail the differences among\npublic, private, and hybrid clouds.\nCloud products can be classified into three main categories:\nSoftware-as-a-service  (SaaS)\nSaaS products are delivered to customers via the internet and are entirely man‐\naged and maintained by the cloud provider. With a SaaS solution, users can easily\nget started with the product, often instantly upon confirming a subscription plan,\nand they do not need to worry about configuration, software licensing, installa‐\ntion, or upgrades. Users might still need to confirm who can access their SaaS\nand what privileges they have. Examples of SaaS products include Gmail, GDrive,\nBigQuery, Dropbox, Snowflake, and many more. Sometimes, SaaS solutions get\nassigned more precise names, such as database-as-a-service and AI-as-a-service.\nPlatforms-as-a-service (PaaS)\nPaaS provides online platforms for developing applications via APIs and operat‐\ning system services. Consumers of PaaS control the settings of the hosting envi‐\nronment, the application, and access policies but not the infrastructure, storage,\nand networking. SaaS apps are typical users of PaaS. Examples of PaaS include\nthe Google App Engine and DigitalOcean App Platform.\nInfrastructure-as-a-service (IaaS)\nIaaS is the lowest level of cloud offering and provides raw physical resources\n(CPU, RAM, storage, and networking) as a service, with the user having nearly\ncomplete control over the configuration of the instances. The majority of SaaS\n230 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nand PaaS apps are based on IaaS. Examples of IaaS include AWS EC2 and Google\nCompute Engine.\nThe cloud market is growing into a complex landscape of provid‐\ners, services, and products. It is important to keep in mind that\nthere are differences between a cloud provider and a cloud-based\nservice provider. Cloud providers own and provide a large number\nof data centers distributed all over the world. These providers are\ncalled hyperscalers  and include major players such as AWS, Micro‐\nsoft, and Google. On the other hand, there exist a large number of\ncloud-based service providers that develop and offer services\nthrough the cloud. The best example is Snowflake, which offers a\ncloud-based data warehouse solution that runs on AWS, Google, or\nMicrosoft.\nIn the financial sector, cloud computing has drawn a lot of interest, especially as a\nmeans to boost innovation and save expenses. According to a 2022 report by McKin‐\nsey, the adoption of the cloud is on most financial institutions’ agendas. A large num‐\nber of strategic partnerships have been announced between major cloud providers/\nservice providers and financial institutions. For example, BlackRock, the largest asset\nmanager in the world, has partnered with Snowflake , a cloud data warehouse pro‐\nvider, to offer a cloud-based version of its investment platform Aladdin; Goldman\nSachs established a collaboration with AWS  to create cloud-based data management\nand analytics solutions for the financial sector; JPMorgan Chase established Fusion ,\nand State Street created Alpha , both cloud-based integrated platforms for investment\nand financial data management.\nMigrating to the cloud can bring a large number of benefits for financial institutions.\nThese include the following:\n•Quicker time to market, facilitated by the ease with which resources can be pro‐\nvisioned and managed.\n•More innovation as developers can test and experiment with new ideas without\nhaving to buy hardware and incur unnecessary costs.\n•Continuous access to novel technologies developed by cloud providers, which\nwould otherwise be very costly to develop in-house.\n•Cost savings due to the pay-as-you-go model, where users pay only for what they\nuse. This shifts the cost structure from fixed IT capital expenditure to variable\noperating costs based on demand.\n•Scalability, where users can scale resources up and down based on their current\nand planned needs without having to purchase physical servers.\nCriteria for Building the Financial Data Engineering Stack | 231\n•Better collaboration as the cloud allows for easy and quick sharing of resources,\nfiles, and proof-of-concept demonstrations.\n•Advanced security and data protection features such as Identity and Access Man‐\nagement (IAM), backups, encryption, centralized management, and audit logs.\n•Cloud providers offer multiregion options for data storage and resource provi‐\nsioning. This increases operational resilience as it reinforces availability. Such a\nfeature is quite crucial for financial applications. For example, for a digital bank‐\ning firm that is branchless,  online services must always be on and available at any\ntime of the day.\n•Compliance: cloud computing’s multiregion feature allows financial institutions\nto pin their customer data to a specific region to meet the data privacy require‐\nments of that location.\nLogically, as with most technologies, cloud computing has its drawbacks. Examples\ninclude the following:\n•Internet access, which raises the risk of access loss in case of internet or service\ndowntime\n•Lack of full control over the underlying infrastructure and data\n•Security and privacy concerns\n•Regulatory constraints\n•Integration challenges with existing systems\n•Vendor lock-in\n•Unforeseen costs and consumption patterns that defy the original migration\ngoals\nIs the Cloud Secure?\nOne of the major perceptions about cloud technologies is their lower degree of secu‐\nrity compared to hosting data in-house on an on-premises infrastructure. The funda‐\nmental justification for this opinion is that shifting data to a third-party location\nentails giving up complete ownership and control of that data as well as opening it to\nthe public since you are likely to access it via the internet.\nEven though such concerns are rational, cloud computing isn’t that insecure. Cloud\nservice providers make significant investments in cloud security, regularly hire secu‐\nrity professionals, and use cutting-edge security measures. For example, in August\n2023, Google blocked a massive denial-of-service attack (DoS) on its infrastructure\nthat reached 398 million requests per second (rps). To put things in perspective, this\n232 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\ntwo-minute attack generated more requests than the total amount of Wikipedia page\nviews during the entire month of September 2023.\nNevertheless, cloud security should be carefully examined and assessed before mak‐\ning any major decision. I highly recommend checking the cloud vendor certifications\nand compliance with the different security regulations and standards for different sec‐\ntors. Examples of security certifications include the following:\nISO/IEC 27001:2022\nInformation security, cybersecurity, and privacy protection\nISO/IEC 27017:2015\nGuidelines for information security controls applicable to the provision and use\nof cloud services\nISO/IEC 27018:2019\nCode of practice for protection of personally identifiable information (PII) in\npublic clouds acting as PII processors\nCloud providers often include a comprehensive list of their compliance offerings\ngrouped by country, sector, region, and more. Examples include the following:\n•Google compliance offerings for financial services\n•AWS compliance programs and certifications\n•Microsoft compliance offerings\nAdditionally, most cloud providers have offerings tailored to the needs of the finan‐\ncial sector. Examples include IBM’s Cloud for Financial Services , Google’s Cloud for\nFinancial Services , AWS’s Cloud Solutions for Financial Services , Microsoft Cloud for\nFinancial Services , and Snowflake’s AI Data Cloud for Financial Services . What dis‐\ntinguishes these cloud offerings is the focus on data protection, privacy, security,\nfraud detection, and several other features that financial institutions require.\nTo evaluate the suitability of cloud services for highly confidential financial data, Ilya\nEpshteyn from AWS proposed a framework consisting of five key factors , which I rec‐\nommend as an example of a general guideline.\nWhen considering a cloud migration decision, keep in mind that it must be done\nthrough a cloud migration strategy that matches your institution’s goals and require‐\nments with what the cloud offers. Otherwise, migration efforts might easily run into\nunexpected issues such as excessive costs, technological limitations, security and\ncompliance issues, scalability, and control.\nCriteria for Building the Financial Data Engineering Stack | 233\n6For a good introduction, see Jamil Mina, Armin Warda, Rafael Marins, and Russ Miles’ Digitalization of\nFinancial Services in the Age of Cloud  (O’Reilly, 2022).\n7An excellent reference on cloud economics is Joe Weinman’s Cloudonomics: The Business Value of Cloud Com‐\nputing, + Website  (Wiley Online Library, 2012).\n8To learn more about cloud migration strategies, see “What Is a Cloud Migration Strategy?”  by VMware.\n9For an interesting read on this topic, I recommend Thomas Boillat and Christine Legner’s “From On-Premise\nSoftware to Cloud Services: The Impact of Cloud Computing on Enterprise Software Vendors’ Business Mod‐\nels”, Journal of Theoretical  and Applied Electronic Commerce Research  8, no. 3 (December 2013): 39–58.A full treatment of cloud migration strategies is beyond the scope of the book.6 How‐\never, from a high-level view, a cloud strategy is, in the first place, an economic strat‐\negy.7 If moving to the cloud is going to increase your costs or not have a major impact\non your costs or revenues, then it might not be worth it. Consider calculating your\ncloud strategy’s return on investment (ROI)  and proceed gradually based on your\nanalysis. For example, it might initially seem more appealing to migrate your analyti‐\ncal data to a cloud-based managed data warehouse service for machine learning pur‐\nposes. Successively, other types of data can be migrated to a data lake, NoSQL\ndatabase, and other data stores. Should this be beneficial, you might consider migrat‐\ning your virtual machines and gradually the rest (or a significant part) of your infra‐\nstructure.8\nSecond, consider analyzing the potential impact of cloud migration on your business\nmodels and the value creation logic. For example, businesses need to get used to the\nidea of web-based services and delivery mechanisms, pay-per-use expenditure mod‐\nels, managed services, and the shared responsibility model.9\nThird, consider the technological limitations of the cloud and their impact on your\nbusiness needs. Cloud technologies are very powerful, but they have limitations.\nSome of these limitations are inherently derived from the technology itself (e.g., the\nconnection limit in a PostgreSQL database) or cloud-related limitations, such as the\nshared resources model of the cloud, which might limit the amount of RAM or CPU\ndedicated to a virtual machine.\nFourth, your strategy must carefully assess your company’s data governance and com‐\npliance requirements to ensure you keep your system resilient and secure. Cloud\nservices are user-friendly and relatively easy to use. But with such ease comes a chal‐\nlenge, which is the risk of misusing cloud services and creating chaotic architectures.\nThis, in turn, might impact the level of control and quality of the data and infrastruc‐\nture you host in the cloud.\n234 | Chapter 6: Overview of the Financial Data Engineering Lifecycle",21342
90-Criterion 4 Public Versus Private Versus Hybrid Cloud.pdf,90-Criterion 4 Public Versus Private Versus Hybrid Cloud,"Criterion 4: Public Versus Private Versus Hybrid Cloud\nWith the adoption of cloud computing worldwide, three cloud models have emerged:\npublic, private, and hybrid. In this section, we will explore the main features of each\nmodel and illustrate their relevance to financial institutions.\nPublic cloud\nIn a public cloud model, the underlying infrastructure (compute, storage, networks,\netc.) is owned and managed by a third-party vendor and accessible by users over the\npublic internet. Examples of public cloud providers include Amazon Web Services,\nGoogle Cloud, Microsoft Azure, IBM Cloud, and Oracle. The public cloud is the\nmost popular model and is often the default choice.\nA key feature of the public cloud is multitenancy , meaning that cloud service users\nshare the underlying infrastructure while remaining isolated through virtualized\nenvironments. Multitenancy allows the cloud vendor to optimize the use of resources\nand reduce costs.\nDue to multitenancy, the public cloud model advocates the shared responsibility prin‐\nciple , wherein both the cloud provider and users assume distinct yet overlapping\nresponsibilities to safeguard the security of services and data hosted in the public\ncloud. The cloud provider oversees physical infrastructure security and maintains a\nlogical separation between client data and resources. Simultaneously, clients are\nresponsible for ensuring application-level security, such as user access and permis‐\nsions, data encryption, backups, and multifactor authentication.\nThe main advantages of the public cloud include the following:\nSimplicity\nPublic cloud providers focus on making their services user-friendly to cater to\nthe diverse needs of all sectors and clients they serve.\nFlexible pricing\nPublic cloud users can pay on demand while also capitalizing on discounts\noffered for committed usage, where they reserve a resource for a predetermined\nduration.\nScalability\nCloud providers strategically plan and provision their infrastructure to accom‐\nmodate variable resource needs, ensuring clients can access the necessary\nresources whenever needed.\nMinimal configuration  and maintenance burden\nThe cloud provider manages and maintains the data centers hosting the physical\nservers.\nCriteria for Building the Financial Data Engineering Stack | 235\nOn the negative side, the public cloud may not be ideal for the following reasons:\n•Data confidentiality and security, as data is stored at a third-party location and\naccessed through the public internet\n•Excessive costs, which may arise with large-scale applications, miscalculated\nresource needs, or unexpected spikes in demand\n•Limited infrastructure control\nGiven their security and confidentiality requirements, financial institutions looking\nto adopt the cloud might find the public cloud the most risky option. However, the\npublic cloud might be a good solution if used for less confidential types of data, e.g.,\nmachine learning. Among financial institutions, FinTech firms are major public cloud\nusers, given the extra features it provides in terms of products and flexibility, which\nare ideal for innovation.\nMisconfiguration  Risk in Public Clouds:\nThe Capital One Data Breach Case\nThe most critical security threat when using public cloud services is cloud misconfi‐\nguration . Since the public cloud is built on the shared responsibility principle, cus‐\ntomers may incorrectly set access to their resources, leaving them vulnerable to\ncybercriminals’ attacks. For example, a common misconfiguration happens when a\nresource is accidentally made public or given excessive permissions that can be easily\nexploited to gain access to sensitive data.\nIn 2019, over 100,000 social security numbers, credit card details, and client financial\ninformation were stolen as a consequence of a cloud data breach at the US financial\ncompany Capital One . The breach was possible thanks to a misconfiguration in per‐\nmission settings that allowed one resource to access data stored in the storage service\nAWS S3 that Capital One used to store its client data. The misconfigured resource was\nattacked and used to gain access to client data in S3.\nCloud providers have implemented measures to help users detect and mitigate cloud\nmisconfiguration issues . Nevertheless, if you use a public cloud service, I highly rec‐\nommend that you assign primary importance to cloud misconfiguration and vulnera‐\nbility risks.\nPrivate cloud\nIn the private cloud model, an organization has a dedicated cloud infrastructure that\nis not shared with others. The dedicated infrastructure can be owned and hosted by\nthe organization itself, owned by the organization and hosted at a third-party vendor,\nor owned by the vendor and rented by the organization. The rise of the private cloud\n236 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\nprimarily stemmed from large corporations with data centers seeking to replicate the\npublic cloud model internally.\nThe private cloud’s main advantages are control and security, which makes it an ideal\nsolution for many regulated industries, such as financial services. If a financial insti‐\ntution builds and deploys a private cloud on its data centers, then security is the full\nresponsibility of the institution. If a private cloud is deployed at a third-party data\ncenter, then the servers’ physical security is the cloud provider’s responsibility. In both\ncases, the security of the data and user access management is the responsibility of the\ninstitution.\nAs a main downside of the private cloud, it is considerably harder to set up and\ndeploy, especially if managed fully by the institution in-house. The private cloud\nmight require significant up-front investment in infrastructure and IT talent and lead\nto ongoing costs in terms of maintenance and personnel.\nThe private cloud has received considerable attention among financial institutions,\nwhich tend to favor security and compliance over other features. Large financial insti‐\ntutions already owning big data centers might benefit greatly from switching or\nadapting to a private cloud setting. For example, Bank of America invested heavily in\nbuilding an internal private cloud, which, according to a Business Insider  article ,\nhelped save $2 billion in annual infrastructure expenditure and reduced the number\nof servers from 200,000 to 70,000 and its data centers from 60 to 23. This reduction\nwas possible through the use of virtualization technologies, which led to a reduced\nneed for servers.\nAs a viable and more practical alternative, cloud providers offer the option to deploy\na Virtual Private Cloud  (VPC) , which is deployed within a public cloud infrastruc‐\nture. A VPC offers the best of both worlds: the private cloud’s security and reliability\nand the public cloud’s scalability and convenience. A VPC can allow a financial insti‐\ntution to isolate compute resources and network traffic for customers’ workloads that\ninvolve highly confidential data.\nHybrid cloud\nThe hybrid cloud model combines public and private clouds to take advantage of\nboth. In a hybrid setting, a private cloud is often used for operations that involve con‐\nfidential data, such as client transactions and financial reporting, while the public\ncloud is used for high-volume and less sensitive operations, such as log analysis,\nmachine learning, and web-based applications. The public cloud can also be dedica‐\nted to handle occasional spikes in workload from the applications running in a pri‐\nvate cloud, a technique known as cloud bursting .\nA hybrid cloud can be an ideal solution for financial institutions, offering the flexibil‐\nity and scalability of the public cloud while ensuring regulatory compliance and\nsecurity  through the private cloud. Moreover, the hybrid cloud model allows for cost\nCriteria for Building the Financial Data Engineering Stack | 237",7967
91-Criterion 5 Single Versus Multi-Cloud.pdf,91-Criterion 5 Single Versus Multi-Cloud,"optimization by allocating resources and workload between public and private cloud\nbased on demand and circumstances.\nThe main disadvantages of a hybrid cloud are the cost involved in building and main‐\ntaining it and the additional operational complexity that derives from the need to\ncoordinate and orchestrate two clouds. Alternatively, a company can build a hybrid\ncloud via a combination of a VPC and public cloud. In this case, the maintenance\nburden of the private cloud is lower as the cloud vendor maintains the underlying\ninfrastructure.\nCriterion 5: Single Versus Multi-Cloud\nMulti-cloud is a cloud strategy whereby an organization uses services from multiple\ncloud providers.\nStarting with a single cloud provider is often the default and simplest solution. This is\nbecause most cloud providers offer similar products, e.g., virtual machines, managed\nSQL databases, file storage, and access management. However, as new business\nrequirements and features get added, limitations might emerge in the cloud offerings\nof the trusted cloud provider. A company that uses cloud provider A might discover\nthat a database service from provider B aligns more closely with the technical and\ncost prerequisites of the business, thus making it a more favorable option.\nThe choice of a public cloud provider may depend on factors that\nare not necessarily technology related. For example, among large\nfinancial institutions, it is common to see a preference for Micro‐\nsoft Azure cloud services. This is due to the long-standing relation‐\nship of trust and security that many companies have developed\nwith Microsoft. When formulating a cloud migration strategy, con‐\nsider an approach that encompasses trust and relationships, as well\nas considerations regarding offerings, technology, features, and\npricing.\nA multi-cloud strategy can also be more economical. For example, Google’s ware‐\nhouse solution BigQuery charges the user based on the amount of bytes that their\nqueries fetch. On the other hand, Snowflake’s data warehouse solution charges based\non the duration of the query. If your queries are data-intensive but fast, then Snow‐\nflake might be more economical, while if your queries take some time but don’t fetch\nlarge amounts of data, then BigQuery might be more appealing.\nCloud pricing can be tricky and hard to understand. Before making your choice, con‐\nsider carefully all costs involved in purchasing a cloud service, particularly managed\ndatabase services. A large number of factors might impact costs other than the single-\nunit on-demand price reported on the vendor’s website. Even though the individual\ncost might seem low, if you can’t predict your scaling needs, costs can easily\n238 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\naccumulate  to a large sum. Additionally, pay careful attention if you decide to use\ncloud cost benchmarking, as it might be misleading  or biased if you don’t understand\nthe settings or context in which they were conducted.\nIn some cases, multi-cloud can be a feature of a cloud-based product itself. For exam‐\nple, Snowflake cloud-based data warehousing relies on the principle of separation\nbetween storage and computing. This means that data can be stored and managed\nindependently of the compute resources that are required to interact with the data.\nSnowflake allows clients to choose the cloud platform (Azure, AWS, or Google) on\nwhich to deploy the Snowflake service . This strategy makes it more appealing for cli‐\nents to use Snowflake, as their data doesn’t need to leave their cloud infrastructure of\nchoice.\nA multi-cloud strategy can mitigate the risk of vendor lock-in. The term vendor lock-\nin refers to a situation where the cost and effort needed to switch from one cloud pro‐\nvider to another is so high that the client is essentially stuck with the current cloud\nprovider. Vendor lock-in may be disadvantageous in case the provider makes changes\nto their services, increases price, or incurs downtime issues. By adopting a multi-\ncloud strategy, you maintain your flexibility to adapt to changes introduced by a\ngiven provider and make use of each provider’s best features.\nThe main drawback of the multi-cloud strategy is the additional management and\noperational burden needed to maintain the resources hosted on different cloud plat‐\nforms. Using two clouds means securing both and making sure access is coordinated\nand managed properly. Establishing a secure and private connection between multi‐\nple clouds can be both complex and expensive, often requiring intricate VPN config‐\nurations or other advanced networking solutions. Additionally, consistency and\nreliability checks must be implemented if data needs to be moved between clouds.\nMoreover, if resources hosted on different clouds become tightly coupled, this may\neasily increase costs, impact performance, and jeopardize maintainability.\nCase Study: Multi-Cloud Strategy at Wells Fargo\nIn 2021, Wells Fargo, a US multinational financial services company, announced a\nnew digital infrastructure strategy  that uses services from two public cloud providers\nas well as third-party-owned data centers for private cloud and traditional hosting\nservices.\nWells Fargo’s strategy relies predominantly on Microsoft Azure’s public cloud to drive\ninnovation across all departments and ensure a secure and trusted environment for\nstrategic business workloads. It uses Azure Cloud as the main foundation for most\nday-to-day data and analytical needs and to empower employee collaboration. How‐\never, it also leverages the Google Cloud platform for more advanced data and analyti‐\ncal workloads, such as artificial intelligence, and to develop personalized customer\nsolutions.\nCriteria for Building the Financial Data Engineering Stack | 239",5862
92-Criterion 6 Monolithic Versus Modular Codebase.pdf,92-Criterion 6 Monolithic Versus Modular Codebase,"Furthermore, Wells Fargo’s digital strategy complements its public cloud infrastruc‐\nture with third-party-owned data centers, leveraging private cloud and traditional\nhosting services to create a secure, reliable, and flexible digital foundation.\nIn general, Wells Fargo’s hybrid multi-cloud architecture is viewed as a promising\nfuture trend in cloud computing since it provides the greatest flexibility for meeting\nbusiness demands in terms of performance, security, cost, scalability, and innovation.\nTo explore this trend in detail, I highly recommend the excellent work of Paul Ziko‐\npoulos, Christopher Bienko, Chris Backer, Chris Konarski, and Sai Vennam, Cloud\nWithout Compromise  (O’Reilly, 2021).\nCriterion 6: Monolithic Versus Modular Codebase\nA major decision that companies have to make when building a data infrastructure is\nthe organizational style of the codebase and application assets. Codebase organization\nstyle is not concerned with how the software works but rather how it is structured,\nlinked, and deployed. Code structure can be crucial in determining the application’s\nreadability, scalability, and reliability. To this end, two main codebase styles are often\nproposed, primarily monolithic  and modular,  which I briefly illustrate in this section.\nMonolith architecture\nIn a monolith architecture, the codebase is typically organized in a single location\nand is characterized by tight coupling between its constituent elements. As a conse‐\nquence, when making a change to a monolith, it is very likely that you will have to\ndeploy the entire application and not just the part that you changed. In its simplest\nstate, you can think of a monolith as a single GitHub repository that stores the entire\napplication codebase.\nIt is also possible to have a distributed monolith, which organizes the codebase in\nmultiple locations, but due to tight coupling, a change in one place requires changes\nand redeployment of all or several of the other locations.\nMonolith architectures may provide a number of advantages, such as the following:\n•Ease of deployment, as you don’t deal with the pitfalls of distributed systems\n•High levels of cohesion and consistency\n•Simpler development workflow, as all components of the code are visible to the\ndeveloper\n•Easier to monitor\n•Easier to test (e.g., end-to-end)\n•High throughput and performance (no need to communicate with many other\nservices)\n240 | Chapter 6: Overview of the Financial Data Engineering Lifecycle\n•Simplifies code reusability (all code is in one place; use what you need!)\nThe monolith architecture may be ideal for simple and lightweight financial applica‐\ntions, both for developers and for the application’s extensibility and performance.\nNevertheless, monolith architectures may lead to a very complex and hard-to-predict\napplication codebase, making it quite challenging to scale, extend, understand, and\ndebug. The more components and dependencies an application has, the harder it is to\nunderstand the impact of a local change on the system’s overall behavior.\nMonolith architectures tend to be seen as a bad thing by default.\nThis, however, is not necessarily true. A monolith architecture is a\nchoice. It can be pretty good in some contexts but should be treated\ncarefully and on a case-by-case basis to avoid its pitfalls. In 2023,\nAmazon Prime Video decided to switch back to a monolith archi‐\ntecture from a distributed microservices architecture . The move to\na monolith architecture helped achieve higher scalability and resil‐\nience, and reduced infrastructure costs by 90%.\nModular architecture\nIn a modular architecture, an application is split into smaller modules that can be\ndeveloped, deployed, tested, and scaled separately. Different teams may own different\nmodules and manage the resource and feature requirements of each separately.\nThe most popular modular architecture pattern is the microservices , a term used to\ndescribe small, autonomous applications that work together to achieve a common\ngoal. In many cases, a microservice architecture is produced from refactoring a com‐\nplex monolith codebase that has reached the limits of scalability and performance. In\nChapter 11 , we will talk about microservice workflows, where you will learn more\nabout microservice-related concepts and design patterns.\nIf you are considering a refactoring project of your monolith to a\nmicroservice architecture, make sure you plan the migration ahead\nof time and produce the necessary software architecture metrics\nand evaluations. A number of best practices exist in this regard. For\nexample, software architecture evaluation techniques  can be used\nto assess the quality and reliability requirements of a proposed/\nchosen software architecture. Moreover, tools such as the Modular‐\nity Maturity Index (MMI), fitness functions, and software metrics\ncan be used to assess which components of the system need to be\nrefactored, replaced, or left as they are.\nCriteria for Building the Financial Data Engineering Stack | 241",5086
93-Chapter 7. Data Ingestion Layer.pdf,93-Chapter 7. Data Ingestion Layer,"Summary\nThis chapter provided a general overview of the technical implementation aspects of\nfinancial data engineering. It covered the financial data engineering lifecycle as a\nframework for organizing the various layers of a financial data infrastructure: inges‐\ntion, storage, transformation and delivery, and monitoring.\nFollowing that, the chapter outlined a set of six criteria intended to assist financial\ninstitutions in making informed decisions when evaluating technological alternatives\nfor the FDEL stack. These criteria serve as a general guideline applicable to all layers\nof the FDEL. In the next chapters, you will learn about additional criteria specific to\neach layer within the FDEL.\nThe next four chapters will cover each of the four FDEL layers in depth to provide\nmore technical details. More specifically, Chapter 7  will cover the ingestion layer,\nChapter 8  the storage layer, Chapter 9  the transformation and delivery layer, and\nChapter 10  the monitoring layer.\n242 | Chapter 6: Overview of the Financial Data Engineering Lifecycle",1072
94-Data Transmission Protocols.pdf,94-Data Transmission Protocols,"CHAPTER 7\nData Ingestion Layer\nIn Chapter 2 , we learned about the different sources and mechanisms that generate\nfinancial data. This included public, market, alternative, and internal sources. Once\ndata is generated at the source, its lifecycle within a financial data infrastructure\nbegins with ingestion. As this chapter will show, data ingestion isn’t as simple as the\nterm may sound. In today’s complex financial data landscape, data ingestion has\nexpanded to encompass a large variety of data transmission and arrival processes as\nwell as ingestion technologies, mechanisms, and formats.\nData ingestion serves as the foundational layer for information exchange in financial\nmarket operations. It facilitates communication among financial institutions and\nmarket participants for initiating transactions like payments, settlements, and trades.\nIt also supports the exchange of inquiries and notifications between financial entities.\nFurthermore, compliance with regulatory requirements relies on efficient data inges‐\ntion practices, enabling financial firms and brokers to transmit various financial\ndetails and compliance reports to regulatory bodies. In addition, data ingestion is\nessential for disseminating market data and delivering real-time updates, critical for\nmaintaining liquidity and operational efficiency throughout financial markets.\nThis chapter will examine the data ingestion layer as the primary entry point for\nreceiving transmitted data and as a crucial bridge facilitating communication and\ninformation exchange among financial institutions and other entities.\nData Transmission and Arrival Processes\nData ingestion is a process wherein data gets transmitted from a given source, travels\nover a network, and arrives at its destination. Understanding the details of such a\nprocess is critical for designing a reliable financial data infrastructure that meets dif‐\nferent business and technical requirements. With such knowledge, financial data\n243\nengineers can optimize time and mission-critical financial applications, design cost-\neffective and efficient data pipelines, anticipate scalability needs, manage security, and\nguide technology choices such as database management systems.\nIn this section I’ll explain the standard transmission protocols that enable most data\ntransactions worldwide and will provide an examination of the many types of data\narrival patterns.\nData Transmission Protocols\nWhen designing a system that transmits and receives data, transmission protocols,\nalso known as communication protocols or network protocols, are an essential com‐\nponent. Simply put, a transmission protocol is a set of rules, techniques, and defini‐\ntions that allow two or more agents or machines to exchange data over a network\nsuch as the internet.\nUnderstanding industry transmission protocols is crucial to mastering the art of data\ningestion. A data infrastructure is reliable only if it can access, deliver, and ingest data\nover a network. This is particularly relevant to the financial industry, which relies\nsubstantially on data transfers due to the large volume of financial transactions.\nVarious communication protocols and standards are employed throughout the data\ntransmission lifecycle, each serving specific purposes. A variety of models have been\ndeveloped to establish a reference framework. Such models rely on the idea of organ‐\nizing network protocols and the technologies used to implement them into distinct\nlayers. In such a design, each protocol belongs to one layer, and each layer is interes‐\nted in the services that it offers to the layer above. This defines the service model  of\nthe layer. In this architecture, each service performs certain actions that belong to it,\nuses input/instructions from the service below, and conforms to the requirements of\nthe service above. For example, the service model of layer N might involve encrypted\nmessage delivery between systems. This could be realized by implementing nonen‐\ncrypted message delivery at layer N-1 and incorporating layer N functionality to\nencrypt messages.\nThe most popular internet protocol model is the seven-layer Open Systems Intercon‐\nnection  (OSI) model, defined in standard ISO/IEC 7498 , and the four-layer internet\nprotocol suite known as the TCP/IP , developed by the Department of Defense (DoD).\nBoth models are illustrated in Figure 7-1 .\n244 | Chapter 7: Data Ingestion Layer\nFigure 7-1. Seven-layer OSI model versus four-layer TCP/IP model\nKeep in mind that communication layer models such as OSI and\nTCP/IP are conceptual frameworks intended to discuss networking\nin a structured way. Reality may be more complex than a model.\nFor example, some protocols or functionalities might be hard to\nplace in one single layer and multiple layers may duplicate the same\nfunctionality. Additionally, layers might have dependencies that\ncould blur the line between them. Nevertheless, reference models\nare very useful for thinking about such a complex topic.\nIn the following sections, I will briefly discuss the various network layers of the\nTCP/IP model. I chose to illustrate the TCP/IP model rather than OSI due to its sim‐\nplicity and popularity among engineers.\nApplication layer\nThis is the topmost layer, where network applications and their protocols reside.\nApplications in this layer are distributed over multiple end systems, and they\nexchange packets of information via the service protocols. Such protocols include the\nfollowing:\nHypertext Transfer Protocol (HTTP)\nUsed to serve web page requests\nSimple Mail Transfer Protocol (SMTP)\nUsed to send and exchange emails\nData Transmission and Arrival Processes | 245\nFile Transfer Protocol (FTP)\nUsed to transfer files between two systems\nDomain Name System (DNS)\nUsed to resolve web page names into their address\nSecure Shell (SSH)\nUsed to secure login to remote servers and execute commands, or secure file\ntransfer through the Secure File Transfer Protocol  (SFPT)\nAdvanced Message Queuing Protocol (AMQP)\nUsed for message-oriented communication between applications\nMessage Queuing Telemetry Transport (MQTT)\nDesigned for lightweight, message-oriented communication\nTransport layer\nThis layer is responsible for transferring application layer packets between applica‐\ntion endpoints in a reliable, optimized, and guaranteed way. Transport layer messages\nare commonly called segments . The most important transport layer protocols include\nthe Transmission Control Protocol  (TCP) and User Datagram Protocol  (UDP). TCP is a\nconnection-oriented  protocol, meaning that the sender and receiver must establish a\ncontinuous connection before exchanging data segments. This feature makes TCP\na reliable protocol, as it acknowledges message reception and resends data again in\ncase it doesn’t arrive. TCP breaks down an information packet from the application\nlayer into a small set of segments and submits it for delivery to the next layer, the net‐\nwork layer. In the network layer, segments are represented as datagrams.\nAdditionally, TCP offers functionalities such as flow and congestion control, which\nregulate the data transmission rate and dynamically adapt to network congestion\nthrough message segmentation and breakdown. Furthermore, TCP ensures secure\ndelivery by keeping track of all transmitted segments and confirming their successful\ndelivery.\nOn the other hand, UDP is a faster but less reliable protocol than TCP . UDP is a con‐\nnectionless  protocol, meaning that data transmission happens via a fire-and-forget\nmechanism without establishing a continuous connection. This means that it offers\nfewer delivery guarantees compared to TCP .\nA significant improvement over TCP is the Transport Layer Security  (TLS) protocol,\nalso referred to as TLS/SSL, which is designed to facilitate secure communication\nacross insecure networks. TLS can be positioned between the application and trans‐\nport layers. It encrypts data before transmission via TCP and decrypts it upon arrival\n246 | Chapter 7: Data Ingestion Layer\n1For more on this topic, I recommend reading this blog post by Google Cloud: “ Authorize with SSL/TLS\nCertificates” .at the transport layer of the receiving end. In addition to encryption, TLS also man‐\nages identity verification (authentication), facilitates the exchange of encryption keys\nbetween hosts, and mitigates Man-in-the-Middle (MITM) attacks through the TLS\nhandshake process .\nTLS is very important to understand because you will encounter it several times when\ndesigning a financial data infrastructure. A common use case for TLS is enabling\nencrypted connections to a database engine.1 Similarly, when using HTTPS instead of\nHTTP to serve web requests and design APIs, you are adding a TLS/SSL layer for\nencryption.\nElectronic Banking Internet Communication Standard\nInternet protocols are essential in developing financial communication protocols. A\ngood example in this context is the Electronic Banking Internet Communication Stan‐\ndard  (EBICS), developed in Germany and adopted by France, Switzerland, and other\ncountries to exchange instructions between financial institutions and corporations\nover the internet. The EBICS standard has been primarily used to initiate Single Euro\nPayments Area  (SEPA) exchanges over the internet, such as SEPA Direct Debits and\nSEPA Credit Transfers.\nAccording to the official technical specifications , the TCP/IP internet protocol suite\nhad a decisive influence on the design of EBICS. Data is transmitted as packages via\nIP addresses or URLs that get resolved to IPs. Package transfer and delivery are moni‐\ntored and guaranteed via the TCP protocol. The client and the EBICS server commu‐\nnicate using the HTTP protocol at the application layer. The XML file format has\nbeen selected as the protocol language at the application layer. To ensure security and\ndata encryption, EBICS relies on TLS to secure communications via HTTPS.\nNetwork layer\nThis layer moves information units known as datagrams  between two hosts. When a\nsource host wants to send data to a target host, it includes a transport layer segment\nalong with the target host’s address. Upon reception, the network layer ensures the\nsegment is delivered to the transport layer on the target host.\nData Transmission and Arrival Processes | 247\n2To learn more about VPCs and subnets, I recommend checking the documentation of AWS on IP addressing\nfor your VPCs and subnets .The most prominent network layer protocol is the internet protocol  (IP), with its ver‐\nsion 4 (IPv4) being the most prevalent. IPv6 serves as its successor and has been pro‐\ngressively integrated into the public internet infrastructure since around 2006. To add\nan extra encryption layer on top of the IP protocol, the IPsec  can be used. IPsec is\ncommonly used with Virtual Private Networks (VPNs).\nThe IP protocol submits datagrams independently to the receiving host. A datagram\nmight include several fields about its content, destination, and relevant details for the\nreceiving end. Once received by the host, the datagrams are assembled again through\nthe TCP protocol at the transport layer.\nUnderstanding IP addressing is of primary importance for financial data engineers,\nparticularly in the context of designing distributed systems such as Apache Cassandra\nfor distributed databases or Apache Spark for distributed computing environments.\nIn these setups, a typical strategy involves provisioning a defined number of nodes\n(machines) and linking them to form a cluster. Each node in the cluster is uniquely\nidentified by an IP address, which can be used by the cluster manager and the worker\nnodes to establish connections and communicate with each other.\nAnother interesting use case of IP addressing is with Virtual Private Clouds (VPCs).\nOne of the main concepts behind VPCs is the subnet , which refers to a range of IP\naddresses assigned to the VPC. IP addresses within a subnet are private, meaning\nthey are inaccessible via the public internet but solely accessible through the VPC’s\ninternal network. When you provision and launch an instance of a given resource\ninto a VPC, a primary private IP address from the subnet range is assigned to the\ninstance.2\nNetwork access layer\nIn this layer, datagrams coming from the network layer get routed through a network\nof routers that connect the network layer interfaces of the source and target. Data‐\ngrams are moved from one node (router) to the next until they reach their destina‐\ntion. Examples of network interface protocols include the Ethernet, WiFi, and Data\nOver Cable Service Interface Specification (DOCSIS) protocols employed in cable\naccess networks.\nThe physical transmission of the individual data bits through the network happens\nvia hardware devices that directly interface with a network, such as coaxial, optical,\nfiber, or twisted pair cables.\n248 | Chapter 7: Data Ingestion Layer",13057
95-Data Arrival Processes.pdf,95-Data Arrival Processes,"Speed Physics in Financial Markets\nY ou might wonder how the physical networking layer is relevant to financial markets.\nInterestingly, it holds significant relevance and has been transforming certain market\ninfrastructures, such as trading.\nAn illustrative example is the establishment of the transatlantic fiber-optic line Hiber‐\nnia Express , designed to link London and New Y ork with ultra-low latency in\nresponse to demand from banks, exchanges, and trading firms. Thanks to Hibernia\nExpress, London and New Y ork experienced a five-millisecond reduction in latency\ncompared to existing high-speed network services. In the era of automated trading,\neven a five-millisecond difference holds substantial importance to computers.\nHigh-frequency traders, who have been engaged in a technical arms race to accelerate\ntrade execution time, are speculating on an experimental form of fiber-optic cable ,\ncalled hollow-core fiber , to speed up their trades by billionths of a second.\nData Arrival Processes\nData may arrive at the ingestion layer with different temporal and structural patterns.\nIn this section, I will use the data arrival process  (DAP) concept to identify and\ndescribe the characteristics of a certain data ingestion pattern. A variety of DAPs may\nexist. Let’s examine six that are particularly relevant to financial data engineering.\nScheduled data arrival process\nIn a scheduled DAP , data is ingested into the system according to a predetermined\nschedule and ingestion specifications. This process is generally more manageable as it\nfollows a predictable pattern, with known details such as the arrival time, data type,\nformat, volume, and ingestion method.\nScheduled DAPs are quite common in financial markets. For example, they may be\nused in the following circumstances:\n•Arrival of company annual report filings at specific intervals such as by the end\nof the year, the end of March of each year, or at a future announced date\n•Arrival of historical financial data snapshots from a financial data vendor on a\ndaily, weekly, monthly, or yearly basis\nData Transmission and Arrival Processes | 249\n•Arrival of financial institutions’ required regulatory filings on a predefined date\n•Arrival of data from continuous model training, stress testing, scenario analysis,\nand other financial quantitative analysis\nKnowing when the data is expected to arrive can greatly enhance financial data engi‐\nneers’ jobs. This is because they can plan data ingestion jobs, anticipate the capacity\nand computational needs of the data infrastructure, and provision the necessary\nresources on a predictable basis (e.g., on a specific day of the week).\nThe main drawback of scheduled DAPs is the delay between data generation and\narrival, which can impact data timeliness. Additionally, if scheduled jobs run without\nfetching any data, it can waste resources.\nEvent-driven data arrival process\nWhen the arrival of data is contingent upon the occurrence of an event that cannot be\npredicted in advance, the DAP becomes event-driven. Event-driven DAPs are very\ncommon in financial markets. These include the arrival of the following types of data:\n•Trade and quote data upon submission and execution in the market\n•Financial information messages\n•Transaction data upon the execution of a financial operation, such as payments,\ntransfers, and others\n•Client files for loan and credit card applications\n•Data streams from financial data vendors\n•News data\n•Company updates and announcements\n•Social media posts\nThe primary advantage of event-driven DAPs is that data is available shortly (or\nimmediately) after its generation. This allows financial institutions to act promptly in\nresponse to new information and have the most recent, up-to-date market and opera‐\ntional insights. For this reason, event-driven systems are often associated with real-\ntime systems. A system is considered real-time if its response time—typically set at a\nvery low value—is essential to its proper functioning. Moreover, event-driven DAPs\ncan save costs and optimize resource allocation as they switch resource utilization\nfrom a fixed to an on-demand pattern.\n250 | Chapter 7: Data Ingestion Layer\nWhat Exactly Is a Real-Time System?\nThe term real-time  is used widely in a variety of contexts. To many people, real-time\nwould mean “instantaneously” or “immediately. ” For software and system engineers,\nreal-time may refer to time-related characteristics of a system or application, such as\nprocessing time, response time, or latency. Given the significance of real-time systems\nin finance, I will provide a detailed description to clear up any misconceptions. To do\nso, I will draw a summary from the seminal work of Seppo J. Ovaska and Philip A.\nLaplante, Real-Time Systems Design and Analysis: Tools for the Practitioner  (Wiley-\nIEEE Press, 2011).\nAuthors Laplante and Ovaska define a real-time system as “a mapping of a set of\ninputs into a set of outputs. ” The time between a system’s reception of input and the\ngeneration of the final output is called the system’s response time. The nature and\nspeed of the response time depend on the system’s features and purpose.\nBuilding on this, the authors define a real-time system as a “computer system that\nmust satisfy bounded response-time constraints or risk severe consequences, includ‐\ning failure. ” Failure, in this case, means that the system is not able to function or meet\none of the requirements of its design specifications. More specifically, a failed real-\ntime system is one that cannot satisfy timing or deadline constraints established in its\nspecifications. In short, a system does not necessarily need to respond instantane‐\nously to be considered real-time; it must simply define and meet response time crite‐\nria in its specifications.\nAt this point, however, all systems may be considered real-time as there is always\nsome sort of time constraint. To further clarify this point, Laplante and Ovaska clas‐\nsify real-time systems into three categories:\nSoft real-time systems\nA failure to meet a response time constraint leads to performance degradation\nbut not system failure.\nHard real-time systems\nA failure to meet a single deadline can lead to complete or major system failure.\nFirm real-time systems\nMissing a few deadlines may not be consequential, but missing more than a few\nmay lead to complete or major system failure.\nAccording to business and operational considerations, real-time systems can be cate‐\ngorized as soft, hard, or firm. For instance, if an ATM machine occasionally fails to\nrespond to requests within its internal time limit (e.g., 10 seconds), it might lead to\nsome customer dissatisfaction, but it remains tolerable, thus qualifying as a soft real-\ntime system.\nConversely, in the context of a hedge fund engaged in high-frequency trading, delays \nin receiving data beyond expected deadlines could result in significant financial\nData Transmission and Arrival Processes | 251\n3Systems that can handle duplicate ingestions reliably are often called idempotent.  To learn more about this\ntopic, I recommend the article “Idempotency and Ordering in Event-Driven Systems” , by Wade Waldron. losses,  necessitating the classification of the system as hard or firm. Another example\nis financial systems that involve Forex currency conversions. This process frequently\nincludes a Request for Quote  (RFQ), where a market participant requests a price quote\nfrom a liquidity provider or Forex broker to either buy or sell a specified amount in a\nparticular currency pair. RFQs include an expiry time, during which the liquidity\nprovider commits to honoring the quoted price. Failing to settle a Forex transaction\nwithin the RFQ’s expiry time can expose the requesting institution to market risks,\npotentially resulting in financial losses.\nImportantly, it is still common for people (even engineers) to think of real-time sys‐\ntems as being “instantaneous. ” To understand why, let’s take the technological concept\nof real-time payments  (RTPs). According to Stripe , RTPs are “instant payments that\nare processed immediately and continuously, 24/7. ” In this context, instant refers to\nthe fact that money is moved between bank accounts in seconds instead of hours or\ndays (as is the case with traditional payment systems). For humans, several seconds\nmay feel instantaneous, thus the use of the term instant in this context.\nNonetheless, I highly recommend following a systematic approach similar to the one\noutlined above when creating real-time financial systems. Software and data engi‐\nneers must carefully understand and incorporate time constraints and limitations into\ntheir systems and investigate potential ways to change a soft real-time system into a\nhard or firm one, or vice versa.\nWith the emergence of cloud computing, event-driven data processing technologies\nhave remarkably increased in popularity. These include message brokers (detailed in\nChapter 8 ), such as Amazon Simple Notification Service (SNS), Amazon Managed\nStreaming for Apache Kafka (MSK), and Google Pub/Sub, as well as event-driven ser‐\nverless computing platforms (illustrated in Chapter 9 ), such as AWS Lambda and\nGoogle Cloud Functions.\nAs event-driven DAPs are unpredictable, designing an event-driven financial data\ninfrastructure requires careful attention. One essential feature to focus on concerns\nthe ability of the infrastructure to scale to accommodate varying workloads and occa‐\nsional spikes. Moreover, due to the data-intensive nature of event-driven DAPs, data\nquality issues such as errors, outliers, and timeliness may easily arise. Furthermore,\nevent-driven DAPs may incur the issue of duplicate or concurrent ingestions, which\nhappens when the same data or file gets ingested twice or more. This requires careful\nattention when designing the data infrastructure, as it might impact data consistency\nand potentially cause system failures.3\n252 | Chapter 7: Data Ingestion Layer\n4For a detailed study on this topic, I recommend Sunila Gollapudi’s “ Aggregating Financial Services Data\nWithout Assumptions: A Semantic Data Reference Architecture” , in the Proceedings of the 2015 IEEE 9th\nInternational Conference on Semantic Computing  (IEEE, 2015): 312–315.Homogeneous data arrival process\nIn a homogenous DAP , the ingested data has predetermined consistent properties.\nFor example, if you purchase a subscription to a dataset provided by a financial data\nprovider, you are likely to know the kind of data, the schema, the ingestion format,\nand other details.\nA homogeneous DAP is simpler to manage and maintain, and it helps ensure data\nintegrity and consistency. In the financial industry, a number of projects have been\nunderway to standardize and universalize data input and exchange formats. The sec‐\ntion “Data Ingestion Formats” on page 256 will illustrate examples of standardized\nfinancial data formats.\nImportantly, you should avoid overfitting your financial data architecture to handle\nonly one type or format of data. This may cause problems if a data attribute changes\nor a new data type is ingested.\nHeterogeneous data arrival process\nIn a heterogeneous DAP , ingested data may possess variable attributes such as exten‐\nsion, format, type, content, schema, and others. Heterogeneous DAPs are quite com‐\nmon in finance. For example, financial data vendors provide their data in different\nformats and structures. In addition, for optimization purposes, different types of data\nmay be stored and transmitted in specific formats (a topic that will be covered in the\nsection “Data Ingestion Formats” on page 256  in this chapter). Furthermore, different\ninternal systems within financial institutions may generate data with their unique for‐\nmats and structures.4\nWhen designing for heterogeneous DAPs, the financial data infrastructure must\naccount for various ingestible data types and possess the necessary capability to han‐\ndle each. This complexity makes optimizing the infrastructure more challenging, but\nit also increases the financial institution’s flexibility to ingest and accommodate new\ndata sources.\nSuch flexibility is critical in today’s fast-changing financial data landscape, where new\ndata sources emerge regularly, and the amount and speed with which data is gener‐\nated has grown significantly. Being able to accommodate a new data source means\nadding new analytical capabilities, developing new products, and gaining compre‐\nhensive insights into market trends, sales, customers, and operations.\nData Transmission and Arrival Processes | 253\nSingle-item data arrival process\nIn a single-item DAP , data is ingested either on a record-at-a-time or file-at-a-time\nbasis. Think, for example, about the arrival of information related to a payment trans‐\naction, bank loan application, market order, analytical report, or piece of news.\nThe main advantages of single-item DAPs are traceability and transactional guaran‐\ntee. When the ingestion process concerns a single data item, it is typically easier to\ntrace its lifecycle through system logs. Moreover, inserting one record at a time allows\nfor easier data integrity and constraint checks.\nAs an illustration, let’s say we have a database that stores customer financial transac‐\ntions. A simple SQL-based single-item ingestion into this table would look like this:\n-- PostgreSQL\nINSERT INTO customer_transactions  (\n  user_id, \n  transaction_id , \n  time, \n  transaction_type , \n  amount, \n  communication\n  ) VALUES (\n    195, \n    'XT4h4Y453' , \n    '2024-01-20 10:09:42' , \n    '1985-02-10' , \n    'Credit Transfer' , \n    'online course subscription fees'\n  )\nIn some circumstances, single-item DAPs may lead to performance bottlenecks. For\nexample, if the number of data ingestions is remarkably high, then it may jeopardize\nthe system’s ability to handle all incoming requests. This can happen due to a max\nconnection limit on the database side (discussed in Chapter 8 ) or quota limit on an\nAPI side (covered in section “Data Ingestion Technologies”  on page 269 in this chap‐\nter). Additionally, ingesting a large number of records one at a time can be very slow,\nwhich in turn can impact data quality dimensions such as timeliness.\nBulk data arrival process\nIn a bulk DAP , data is ingested in large chunks. Rather than processing one record at\na time, a bulk DAP handles blocks of data or files that may contain hundreds or even\nmillions of records simultaneously.\nBulk DAP offers performance advantages by processing large data volumes in a single\nrequest, saving overhead costs. This is ideal for tasks like bulk data loading, migration\nbetween storage systems, data archival processes, and regulatory reporting. For\n254 | Chapter 7: Data Ingestion Layer\n5The full guide is available on Snowflake’s website  and it assumes you have satisfied all the necessary requisites\nsuch as granting roles and permissions between Snowflake and AWS S3, creating the Snowflake table, and\nother tasks.\n6To upload files into a stage location, you need to use the Snowflake PUT command .instance, when switching database systems, dumping data in a format compatible\nwith the new system is far more efficient than copying records individually.\nTo give an example, let’s consider how Snowflake’s bulk loading works.5 Assume you\nhave a bunch of CSV files stored in the Amazon cloud storage service AWS S3, and\nyou are using Snowflake as a data warehouse. Y ou wish to upload the data from your\nCSV files into your Snowflake table.\nThe first thing to do is to create a FILE FORMAT  object in Snowflake that describes the\ntype and format of the data to be loaded. Let’s say that our CSVs use the semicolon,\n“;” , as a field delimiter, and we want to inform Snowflake to skip the first line in each\nfile as it represents the header. The following command creates the desired format\nobject:\n-- Snowflake SQL\nCREATE OR REPLACE FILE FORMAT s3csvformat\n   TYPE = 'CSV'\n   FIELD_DELIMITER  = ';'\n   SKIP_HEADER  = 1;\nNext, we want to create the so-called stage object, which tells Snowflake the location\nwhere the files are stored (staged). Snowflake provides several types of stage objects ,\nbut in this example, we will use the recommended Named stage,6 which may be cre‐\nated as follows:\n-- Snowflake SQL\nCREATE OR REPLACE STAGE s3_csv_stage\n  FILE_FORMAT  = s3csvformat\n  URL = 's3://snowflake-docs' ;\nFinally, to load the data from the stage location into the Snowflake table, we can exe‐\ncute the following command:\n-- Snowflake SQL\nCOPY INTO destination_table\n  FROM @s3_csv_stage /myfiles/\n  PATTERN='.*daily_prices.csv'\n  ON_ERROR  = 'skip_file' ;\nThe PATTERN  clause specifies that the command should load data from any file that\nmatches the specified regular expression .*daily_prices.csv , which matches any\nfile that ends with daily_prices.csv . Furthermore, the command specifies that if an\nerror occurs when loading a specific file, skip it and proceed with the remaining files.\nData Transmission and Arrival Processes | 255",17241
96-Data Ingestion Formats.pdf,96-Data Ingestion Formats,,0
97-In-Memory Formats.pdf,97-In-Memory Formats,"Now that we’ve covered the various data transmission and arrival processes, let’s\nexplore the different types of data formats that can be ingested into a financial data\ninfrastructure.\nData Ingestion Formats\nData can be ingested in a variety of formats. A data format is used to indicate the\nextension or encoding used to store the data on a machine. In general, there is no\nstandardized classification of the many data formats that a financial data infrastruc‐\nture may support. To establish a baseline, this section will illustrate the most common\ntypes of data formats that financial data engineers might encounter when working\nwith financial data.\nGeneral-Purpose Formats\nGeneral-purpose formats are widely used data formats with broad applicability and\nextensive adoption within the financial markets. Examples include the following:\n•Comma-separated values (CSV) files are text files with comma-separated values\n(,).\n•Tab-separated values (TSV) files are text files with tab-separated values (\t).\n•Text files (TXT) are text files with lines delimited by a line separator (\n).\n•JavaScript Object Notation (JSON) files are structured as a collection of name/\nvalue pairs.\n•Extensible Markup Language (XML) files structure and store data in a hierarchi‐\ncal format using custom tags.\n•Microsoft Excel files work with Microsoft Excel (e.g., XLSX and XLS) .\n•Compressed files are compressed using a compression algorithm such as GZip or\nZip.\nFinancial markets use these formats for a variety of reasons and purposes. For exam‐\nple, Microsoft Excel files are quite popular  among financial professionals and\naccountants due to their reliability (Microsoft as maintainer), simplicity, and\nadvanced analytical capabilities. CSV and TSV formats are widely used for storing\nand sharing financial time series and tabular data. TXT files are used to store textual\nfinancial content such as reports, news, entity and reference data, and many more.\nJSON and XML are widely used for programmatic and web-based financial data\n256 | Chapter 7: Data Ingestion Layer\n7XML ’s popularity has remarkably increased in recent years. To read more about this topic, see “Making Life\nEasier in an XML World” , by Denise Warzel.exchange due to their user-friendly nature and dependable technical specifications.\nConsequently, they frequently form the basis for financial data standards.7\nThe downside of using general-purpose formats is that their flexibility increases the\nchance of data errors or quality issues. Moreover, these formats may not be efficient\nwhen dealing with large data volumes. To address this concern, more specialized for‐\nmats can be employed, which will be discussed in the following sections.\nBig Data Formats\nGeneral-purpose formats such as CSV and XML can easily encounter performance\nissues when ingesting and exchanging large amounts of data. To solve this issue, sev‐\neral big data formats have been developed, including Apache Parquet, Apache Avro,\nand ORC.\nApache Parquet is an open source, column-oriented data file format that supports\nefficient and economical data storage and retrieval. Parquet offers efficient data com‐\npression, decompression, schema evolution, and encoding algorithms to handle com‐\nplex and large data. Parquet is accessible in several languages, including Python, C++,\nand Java. Parquet files are widely used. For example, the leading cloud data ware‐\nhouse provider, Snowflake, reports that Parquet is the file format most often used by\nits customers  to upload data to the Snowflake platform.\nColumn-Oriented Versus Row-Oriented File Formats\nWhen dealing with file formats, it’s important to understand the difference between\ncolumn-oriented and row-oriented formats. In row-oriented formats (e.g., Post‐\ngreSQL ’s internal data format), data is stored on disk row by row. These formats are\npreferable for small datasets, strict data consistency requirements, or applications\nwith heavy write/update operations, such as financial systems handling transactions\nlike payments and clearing.\nIn contrast, column-oriented formats like Parquet store data on disk column by col‐\numn. This format is highly advantageous for read-intensive and big data applications.\nQueries are more efficient and economical because read-intensive applications often\nretrieve only a subset of columns, avoiding unnecessary querying of other columns.\nAdditionally, column-wise storage enhances compression efficiency as it optimally\ncompresses data of the same type within a single column, unlike row-oriented for‐\nmats, which compress heterogeneous data within a single row.\nData Ingestion Formats | 257",4684
98-Standardized Financial Formats.pdf,98-Standardized Financial Formats,"8For a comparison of Avro and Parquet formats, see “ AVRO vs. PARQUET” by Snowflake .Apache Avro is another common big data format known for its row-oriented struc‐\nture and compact binary encoding, which helps reduce file storage size. Avro stores\ndata definitions, types, and protocols in an easily readable JSON format, while the\nactual data is stored in a highly optimized binary format. Avro is schema dependent,\nmeaning that the data and its schema are stored and transmitted together in the same\nfile. As a result, Avro is preferred over Parquet when frequent schema changes occur,\nas merging schemas from multiple files can be quite costly.8\nA third common big data format is Optimized Row Columnar (ORC), a column-\noriented, binary format primarily used for storing Hive data in Hadoop environ‐\nments. ORC is renowned for its exceptional performance  in terms of data processing\nspeed and storage efficiency, making it well-suited for handling large volumes of data.\nIn-Memory Formats\nIn many applications, data is frequently read and processed in memory. Crucially, dif‐\nferent software programs may store data in memory using different formats. If data\nmoves from one application to another during a data processing pipeline, then each\napplication needs to convert the data to its in-memory format before processing it.\nThis is a costly operation as it often involves data serialization and deserialization,\nwhich in turn can impact performance.\nTo solve this issue, a variety of in-memory data formats have been developed. A\nprominent example is Apache Arrow , a standardized, column-oriented, language-\nagnostic data format for structuring and representing tabular datasets in memory.\nApache Arrow can be used to develop a data infrastructure that processes data across\nmultiple systems using an out-of-the-box standardized format.\nAnother noteworthy example is the Resilient Distributed Dataset (RDD) abstraction\ncreated for Apache Spark to enable reliable, fault-tolerant, and parallel computations\nin memory.\nStandardized Financial Formats\nMarket participants can exchange financial information in any of the formats covered\nthus far, such as CSV , JSON, TXT, and XML. However, if each financial institution\nemploys its own convention to structure its financial messages and communications,\nmarkets will incur significant costs in understanding and extracting information\nfrom each and every format. For instance, imagine a network of one thousand trad‐\ning firms where each firm submits trade request messages using its own JSON or\nXML structure. This scenario would result in hundreds of different message formats\nthat every trader would need to understand.\n258 | Chapter 7: Data Ingestion Layer\n9See, for example, Aldane Haldane, Robleh D. Ali and Paul Nahai-Williamson’s “Towards a Common Financial\nLanguage” , presented at the Securities Industry and Financial Markets Association Symposium on “Building a\nGlobal Legal Entity Identifier Framework, ” New Y ork, 2012.To address this challenge, financial market participants have been working on creat‐\ning standardized formats for financial information. This effort is often described by\nindustry experts as establishing a “common financial language” for data exchange.9\nSeveral initiatives have been proposed, leading to the development and adoption of\nmultiple standards. Examples include the following:\n•Financial products Markup Language (FpML)\n•Financial Information eXchange (FIX)\n•Interactive Financial eXchange (IFX)\n•Market Data Definition Language (MDDL)\n•Financial Electronic Data Interchange (FEDI)\n•Open Financial Exchange (OFX)\n•eXtensible Business Reporting Language (XBRL)\n•Financial transaction card-originated messages (ISO 8583)\n•Securities—Scheme for messages (ISO 15022)\n•Universal Financial Industry Message Scheme (ISO 20022)\n•SWIFT proprietary messages\nIn the following sections, we will go over some of these financial data standards in\ndepth.\nFinancial Information eXchange (FIX)\nFinancial Information eXchange (FIX)  is an electronic communication protocol\nwidely used to exchange financial transaction information between financial institu‐\ntions such as banks, trading firms, brokers/dealers, security exchanges, and even reg‐\nulators. FIX is a nonproprietary open standard owned and maintained by the FIX\nTrading Community member firms. FIX was originally developed to exchange pre-\ntrade and trade equities trading messages. Over time, its scope has expanded to\ninclude support for post-trade activities, as well as transactions in fixed income, for‐\neign exchange, and listed derivatives markets.\nA full account of the technical specifications of the FIX infrastructure  is beyond the\nscope of this section. However, the FIX system can be broken down into (1) a stan‐\ndardized message format, (2) a FIX order routing network, and (3) a FIX engine nec‐\nessary to submit and receive FIX messages. The classical encoding of FIX messages is\nData Ingestion Formats | 259\n10The sample message was taken from the “Sample Messages Document” , by NYSE. A dictionary of all FIX tags\nand their meaning is available online .\n11Detailed technical specifications on FIX networks and engines are available at the FIX community website .called tagvalue encoding , which structures a message as a chain of tag/value pairs.\nTags are integers that identify the field, followed by the “=” character (hexadecimal\n0x3D), and finally, the value of that field, encoded in the ISO 8859-1 character set.\nEach tag/value pair is separated by the ISO 6429:1992 Start of Heading control char‐\nacter <SOH> (hexadecimal value 0x01). Other encodings include FIXML, which lev‐\nerages XML and JSON to format the message.\nFor example, a single buy order FIX message looks like this:10\n8=FIX.4.2^A 9=145^A 35=D^A 34=4^A 49=ABC_DEFG01^A 52=20090323-15:40:29^A \n56=CCG^A 115=XYZ^A 11=NF 0542/03232009^A 54=1^A 38=100^A 55=CVS^A 40=1^A 59=0^A \n47=A^A 60=20090323-15:40:29^A 21=1^A 207=N^A 10=139^A\nThe fields can be broken down as follows:\n8=FIX.4.2 FIX version number\n9=145 Body Length: 145 bytes\n35=D Message Type: New Order - Single\n34=4 Message Sequence Number: 4\n49=ABC_DEFG01 Sender Company ID: ABC_DEFG01\n52=20090323-15:40:29 Sending Time: March 23, 2009, 15:40:29\n56=CCG Target Company ID: CCG\n115=XYZ On Behalf Of Company ID: XYZ\n11=NF 0542/03232009 Client Order ID: NF 0542/03232009\n54=1 Side: Buy\n38=100 Order Quantity: 100 shares\n55=CVS Symbol: CVS\n40=1 Order Type: Market\n59=0 Time In Force: Day\n47=A Special Instructions: Agency single order\n60=20090323-15:40:29 Transaction Time: March 23, 2009, 15:40:29\n21=1 Handling Instructions: Automated execution\n207=N Security Exchange: NASD OTC\n10=139 Checksum: 139\nAccording to the FIX protocol, to exchange FIX messages between two financial\ninstitutions, both must have a FIX engine that communicates over a FIX routing net‐\nwork. Several FIX network routing options are available, including the internet,\nleased lines, point-to-point VPNs, and Hub-and-Spoke. The FIX engine needs to be\nimplemented and connected to the selected routing network to exchange messages.11\n260 | Chapter 7: Data Ingestion Layer\n12For more details, check the official XBRL Essentials guide .eXtensible Business Reporting Language (XBRL)\neXtensible Business Reporting Language (XBRL)  is an XML-based open international\nstandard for digital business and financial reporting. XBRL is managed and main‐\ntained by XBRL International, a nonprofit consortium. In a nutshell, XBRL provides\na standardized, accurate, and reliable way to represent and exchange business and\naccounting data in both human-readable and machine-readable formats. XBRL is\nwidely used by regulators, companies, governments, data providers, investors, ana‐\nlysts, and accountants.\nThe main building block of XBRL is an XBRL instance , which refers to the collection\nof business facts that an XBRL document contains. More technically, an XBRL is an\nXML file whose root element is <xbrli:xbrl>.\nAnother important element is XBRL facts , which represent individual pieces of infor‐\nmation in an XBRL instance. For example, a fact may say that Heckler & Brothers\nInc. ’s 2018 revenues were $5 billion. The fact is reported as a value of 5b against a cor‐\nresponding concept representing “Revenues, ” in addition to associated contextual\ninformation for the units (dollars), the period (2018), and the entity (“Heckler &\nBrothers Inc. ”). Technically, facts are represented by elements in an XBRL document.\nAdditionally, there are XBRL concepts , which can be used to describe the meaning of\nfacts. For instance, “ Assets, ” “Liabilities, ” and “Net Income” are examples of these con‐\ncepts. Technically, concepts are represented as element definitions in an XML\nschema.\nFinally, XBRL taxonomies  correspond to collections of concept definitions. Taxono‐\nmies are typically created to represent a given reporting regime, such as international\nfinancial reporting standards (IFRS) and generally accepted accounting principles\n(GAAP) standards, as well as for reporting requirements of various regulators and\ngovernment agencies. A taxonomy is used to define what needs to be reported clearly.\nAt a technical level, a taxonomy is an XML schema document containing element\ndefinitions and a collection of XML documents with additional information associ‐\nated with concept definitions.\nXBRL allows for defining business rules that can constrain and verify what kind of\ndata can be reported. Rules can be logical or mathematical, and they are often used to\ncontrol the data quality of XBRL documents.12\nData Ingestion Formats | 261\nFinancial products Markup Language (FpML)\nFinancial products Markup Language (FpML)  is an open source, XML-based infor‐\nmation exchange standard designed for the electronic trading and processing of\nfinancial derivatives instruments. The standard is defined and maintained by the\nFpML Standards Committee.\nAmong the most distinguishing aspects of derivative markets is the flexibility in\ndefining and shaping derivative contracts to meet specific client requirements. More‐\nover, a large portion of derivative trading happens over the counter (OTC), meaning\nthat such transactions are conducted business-to-business and not through a central‐\nized trading venue.\nWith such flexibility comes a large variety of data communication and representation\nstyles. On the one hand, this has been considered necessary as it allows two parties to\ncustomize a derivative product to meet specific client needs. Consequently, attempts\nto standardize OTC derivative communications have not gained much traction, as\nthey were doomed to become obsolete quite fast once new requirements emerged.\nThis situation has led to a manual data exchange process between trading parties,\nwhich is prone to errors.\nHowever, with the increase in derivative trading volume and the establishment of\nnew requirements for derivative trade processing, standardization became more\nappealing. In this regard, FpML was introduced to automate the flow of information\nacross the entire derivative trading network (of partners and clients), independent of\nthe underlying software or hardware infrastructure supporting the related activities.\nThe standard was initially developed for interest rate derivatives such as swaps, but it\nhas been extended to other classes of derivatives, structured products, bonds, and\ncommercial loans since then. Moreover, FpML has evolved to cover the different\nstages of a derivative transaction, such as pre-trade, trade, and post-trade.\nAn FpML message is encoded using Unicode Transformation Format (UTF)-8 or\nUTF-16 and uses XML as the file format. FpML may include a number of elements\nwhose values are restricted to a limited set of values (e.g., currencies). Such restricted\nsets are called domains . FpML relies on two types of domain codings. First are\ndomains that don’t change frequently throughout the life of the specification, which\nare coded using XML schema enumerations. The second type is domains coded using\na strategy defined by the Architecture Working Group, referred to as schemes . A\nscheme is associated with a Uniform Resource Identifier (URI). Three categories of\ncoding schemes exist:\n•External coding scheme with a URI assigned by an external body such as an open\nstandards organization or a market participant\n•External coding scheme without a URI; in such a case, FpML assigns a URI\n262 | Chapter 7: Data Ingestion Layer\n13The source of this example is the official FpML Coding Schemes documentation .•An FpML-defined coding scheme, defined and versioned by FpML, which also\nassigns the URI\nFor example, an FpML-defined scheme is actionTypeScheme,  which codes the action\ntypes as defined by the European Securities and Markets Authority (ESMA).13 As of\nthe time this book was written, the URI for this scheme is http://www.fpml.org/coding-\nscheme/action-type-1-0,  and its coding scheme is shown in Table 7-1 .\nTable 7-1. FpML-defined  action type scheme\nCode Description\nC Cancel (a termination of an existing contract)\nE Error (a cancellation of a wrongly submitted report)\nM Modify (a modification  of details of a previously reported derivative contract)\nN New (a derivative contract reported for the first  time)\nO Other (any other amendment to the report)\nV Valuation update (an update of a contract valuation)\nZ Compression (a compression of the reported contract)\nC Cancel (a termination of an existing contract)\nFpML ’s flexibility, enabled by its use of schemes, makes it well-suited for handling\ncustom exchange definitions and requirements in the derivatives markets. Those\ninterested in exploring the specifics of these schemes can refer to the comprehensive\nFpML Coding Schemes documentation .\nOpen Financial Exchange (OFX)\nOpen Financial Exchange (OFX)  is a widely adopted open standard for the electronic\nexchange of financial data and instructions between financial institutions, businesses,\nand customers. OFX allows direct connection between customers and institutions\nwithout requiring an intermediary.\nOpen Financial Exchange relies on open specifications that anyone can implement\n(e.g., financial institution, software development firm, transaction processor, or other\nparty). A client-server model is used to design the OFX system. The client submits a\nrequest (e.g., HTTP) to an OFX server, and the server replies with a response to the\nclient. OFX defines the request/response message structure and provides guidelines\nfor building the infrastructure for supporting message exchange. OFX uses widely\naccepted open standards for networking (such as TCP/IP and HTTP), data format‐\nting (such as XML), and TLS.\nData Ingestion Formats | 263\nOFX is widely utilized by financial institutions for a variety of applications and use\ncases. Since 1997, it has been the leading direct API standard  for banks to provide\ndata to financial applications. It is currently in use at over 7,000 financial institutions.\nOFX can be implemented and adapted across a wide range of frontend applications\nand platforms. Moreover, it is an extensible standard, allowing for the straightforward\naddition of new services as needed.\nReaders are encouraged to check the official documentation  for more details on\nOFX’s technical implementation and message structure.\nUniversal Financial Industry Message Scheme (ISO 20022)\nAs messaging standards increased in scale, sophistication, and variety, financial mar‐\nket participants, in collaboration with the ISO, initiated discussions to make uniform\nthe message standardization process. This resulted in the introduction of the highly\ncelebrated ISO 20022—Universal Financial Industry Message Scheme .\nISO 20022 is an open and global standard that aims to streamline financial market\ncommunication and messaging using a common language. Its general-purpose design\nmakes it suitable for the majority of use cases, irrespective of the business domain,\ncommunication network, or counterparty.\nA distinguishing feature of ISO 20022 is its model-based approach. When you use\nISO 20022 to develop a new message, the outcome is a model that defines and\ndescribes all parts of the message exchange and communication protocol between\nparticipants. The modeling method consists of four levels: scope, conceptual, logical,\nand physical. These levels are developed as one progresses from the business process\nand its associated features and components to the development of the final instance\nof the message model in a given syntax (typically XML).\nHaving said this, it is important to remember that ISO 20022 is not a single commu‐\nnication standard in and of itself but rather a standard that describes a development\nmethodology for creating financial message models.\nISO 20022 models and their related message components are available online via a\ncentral repository  organized into two areas:\nThe Data Dictionary\nContains industry model elements such as business concepts, message concepts,\nand data types. These elements are called dictionary items and serve as reusable\ncomponents for future models.\nThe Business Process Catalog\nContains model message definitions and syntax implementations.\n264 | Chapter 7: Data Ingestion Layer\nAccording to the official ISO documentation , the modeling methodology of ISO\n20022 consists of eight parts:\nISO 20022-1\nMetamodel for all models and the repository\nISO 20022-2\nUML profile to create models that conform to the ISO 20022-1 Metamodel\nISO 20022-3\nModeling method to produce models\nISO 20022-4\nXML schema generation rules to transform a logical-level model into a physical-\nlevel implementation\nISO 20022-5\nReverse engineering guidelines to extract relevant information from existing\nmessages\nISO 20022-6\nMessage transport characteristics\nISO 20022-7\nRegistration process description\nISO 20022-8\nAbstract Syntax Notation One (ASN.1) generation rules to transform a logical\nmodel into an ASN.1-based physical level\nISO 20022 messages follow a four-block convention for naming. For instance, a well-\nknown message is the “FinancialInstitutionToFinancialInstitutionCustomerCredit‐\nTransfer” message, represented in the ISO 20022 convention, as shown in Figure 7-2 .\nIn this example, “PACS” denotes “Payment Clearing and Settlement, ” indicating the\nmessage’s business domain of payment and settlement instructions. The “008” seg‐\nment serves as the message type identifier, specifying the type of transaction—in this\ncase, a financial institution to financial institution customer credit transfer. The “001”\ndesignation represents the variant number, indicating the global message definition.\nLastly, “12” identifies the message version within the ISO system.\nFigure 7-2. Structure of an ISO 20022 message\nData Ingestion Formats | 265\nISO 20022 Variants\nVariants  and versions are pivotal aspects of ISO 20022, allowing for the creation of\nsimplified and purpose-specific versions of the global message to suit specific\nrequirements.\nOne of ISO 20022’s nicest features is the concept of a variant. Each ISO 20022 mes‐\nsage has a global message definition , typically represented with variant number 001.\nOther variants (>001) can be created to produce restricted versions of a global mes‐\nsage definition. For example, a straight-through processing (STP) variant of a global\nmessage definition may exclude all the options that would require manual processing\nof the message instance, and thus ensures the STP of the messages.\nEach variant can have multiple versions, which is the last component you see in the\nmessage identifier shown in Figure 7-2 . Versions are independent of the variant; for\nexample, variant 001 can have versions 001.001 and 001.002, while variant 002 may\nhave versions 002.001, 002.002, and 002.003.\nISO 20022 variants play a crucial role by enabling customization of global message\ndefinitions to align with specific operational and processing needs in financial trans‐\nactions. In addition, they simplify the adoption of ISO 20022 message definitions by\nreducing complexity and providing clarity on how to apply message definitions in\nspecific contexts.\nExamples of some of the most used ISO 20022 messages include the following:\npain.001—Credit Transfer\nCustomer-initiated credit transfers to banks\npain.013—Request to Pay\nRequests payment from a payer\npain.002—Payment Status Report\nStatus updates on initiated payments\npacs.008—FI to FI Customer Credit Transfer\nCustomer credit transfers between banks\npacs.003—FI to FI Customer Direct Debit\nDirect debit transactions between banks\npacs.002—FI to FI Payment Status Report\nStatus updates on financial institution payments\n266 | Chapter 7: Data Ingestion Layer\n14To see the full message body and have a more detailed idea about the meaning of the tags, download the full\nmessage data online . \n15To learn more about the ISO 20022 registration process, see the official web page on the development of new\nISO 20022 message definitions .camt.05x\nVarious account reporting messages (e.g., camt.054—Bank to Customer Debit/\nCredit  can be used to notify an account holder of debit and/or credit entries\nreported to their account)\nHere is a simple XML snippet that illustrates a PACS.008.001 message. Each tag\nwithin the message has been annotated with a comment detailing its meaning or\npurpose:14\n<FIToFICstmrCdtTrf>   Financial  Institution  Credit Transfer  \n    <GrpHdr>   Group Header \n      <MsgId>123456789 </MsgId>   Message Identification  \n      <CreDtTm> 2022-05-20T14:30:00 </CreDtTm>   Creation  Date and Time \n      <NbOfTxs> 1</NbOfTxs>   Number of Transactions  \n      <CtrlSum> 1000.00 </CtrlSum>   Control Sum (Total Amount) \n    </GrpHdr>\n    <CdtTrfTxInf>   Credit Transfer  Transaction  Information  \n      <PmtId>  Payment Identification  \n        <EndToEndId> 00001 </EndToEndId>   End-to-End  Identification  \n      </PmtId>\n      <Amt>  Amount \n        <InstdAmt  Ccy=""USD"" >1000.00 </InstdAmt>   Instructed  Amount \n      </Amt>\n      <Cdtr>  Creditor  \n        <Nm>John Smith </Nm>  Name of the Creditor  \n      </Cdtr>\n      <CdtrAcct>   Creditor  Account \n        <Id>  Identification  \n          <IBAN>GB29NWBK60161331926819 </IBAN>  IBAN Number\n        </Id>\n      </CdtrAcct>\n      <RmtInf>   Remittance  Information  \n        <Ustrd>Invoice payment for services  rendered. </Ustrd>   \n      </RmtInf>\n    </CdtTrfTxInf>\n  </FIToFICstmrCdtTrf>\nAny community user or organization can use the ISO 20022 modeling methodology\nto develop and submit a proposal for a new model or a modification of an existing\nmodel. Candidate models are reviewed and approved by three registration bodies: the\nRegistration Management Group (RMG), the Registration Authority (RA), and the\nStandards Evaluation Groups (SEGs).15\nData Ingestion Formats | 267\n16For a detailed analysis, I recommend checking the implications of ISO 20022 on the payment industry. For\nthis reason, I highly recommend the paper by Steve Goswell, “ISO 20022: The Implications for Payments Pro‐\ncessing and Requirements for Its Successful Use” , Journal of Payments Strategy & Systems  1, no. 1 (Autumn\n2006): 42–50.ISO 20022 has seen remarkable adoption and acceptance among market participants.\nThis includes all domains where financial data is exchanged, including payments,\nsecurities trading and settlement, credit and debit card transactions, foreign exchange\ntransactions, and many more.16 For example, SWIFT introduced ISO 20022 in March\n2023 and established a migration plan  in which both SWIFT proprietary MT (mes‐\nsage type/text) messages and ISO 20022 will coexist until November 2025. After that,\nSWIFT messages will be completely based on ISO 20022.\nCase Study: Society for Worldwide Interbank\nFinancial Telecommunication (SWIFT)\nA good example demonstrating the use of messages in financial markets is the Society\nfor Worldwide Interbank Financial Telecommunication (SWIFT). SWIFT is a\nBelgium-based cooperative that provides a secure and reliable messaging system for\nfinancial transactions worldwide. SWIFT does not hold or manage financial assets.\nInstead, it offers a platform for exchanging financial messages, such as money and\nsecurities transfer instructions. Over 11,000 financial institutions globally are connec‐\nted to the SWIFT system.\nSWIFT provides different messaging formats and schemas. The most common is the\nFIN message, which follows a store-and-forward mode: messages sent from the\nsource are stored at a central intermediary location before being transmitted to the\nrecipient. Another format is InterAct, which is XML-based and offers features such as\nreal-time messaging and query-and-response capabilities. Finally, FileAct messages\nare used to transfer files, such as large batches of messages or other payment-related\nfiles. Many of SWIFT’s messaging formats are based on ISO standards, such as ISO\n15022 and, more recently, ISO 20022.\nSWIFT messages are categorized using the convention MT (message type/text) , fol‐\nlowed by three digits that indicate the message category, group, and type (e.g.,\nMTxxx). There are nine message categories in the SWIFT system; for example,\nMT1xx is for customer payments and checks, and MT5xx is for securities market\ntransactions.\nConsider a scenario where Corporation A wants to send $500,000 to Corporation B.\nCorporation A initiates the transaction by submitting payment instructions to its\nbank (Bank 1) using an MT101 message (or ISO 20022 pain.001). Upon receiving the\nmessage, Bank 1 issues a credit transfer request to Corporation B’s bank (Bank 2)\nusing an MT103 message (or ISO 20022 pacs.008). Figure 7-3  illustrates the flow.\n268 | Chapter 7: Data Ingestion Layer",26000
99-Data Ingestion Technologies.pdf,99-Data Ingestion Technologies,,0
100-Financial APIs.pdf,100-Financial APIs,"Figure 7-3. A SWIFT transfer\nIn this example, it was assumed that Bank 1 and Bank 2 have a corresponding bank‐\ning relationship, which allows them to directly exchange messages such as MT101\nand MT103. However, if they do not have a corresponding banking relationship , the\npayment process would involve one or more intermediary or correspondent banks.\nRecently, SWIFT launched SWIFT GPI (Global Payments Innovation) to enhance the\ncross-border payment experience, addressing the industry’s demands for greater\nspeed, traceability, and transparency.\nNow that you have an understanding of data ingestion processes and formats, let’s\nexplore the various technological options for integrating a data ingestion mechanism\ninto a financial data infrastructure.\nData Ingestion Technologies\nTo enable and integrate data ingestion capabilities into a financial data infrastructure,\none or more ingestion mechanisms need to be implemented. To this end, a number of\ntechnological options are available to meet varying business and technical needs. In\nthis section, I will discuss six of the most common ingestion technologies used in\nfinancial markets.\nFinancial APIs\nAPI stands for application programming interface,  and it refers to a wide range of soft‐\nware implementations that allow one software component to interact with another.\nAn API defines the rules, protocols, and methods for interacting between two soft‐\nware types.\nAt the highest level, an API can be described in terms of a client and a server. A client\nusing one application sends a request to a server operating in another application.\nThe server processes the request and returns a response to the client. For instance,\nwhen you open a social media application to upload a new image, you (the client)\ninteract with the app provider’s server via an API.\nData Ingestion Technologies | 269\nTechnically, there are several approaches to designing and implementing APIs. These\ninclude SOAP APIs, RPC APIs, GraphQL, WebSocket APIs, and REST APIs. Among\nthese, REST APIs, which stands for Representational State Transfer, are the most pop‐\nular. REST APIs primarily use the HTTP protocol for communication. They can\nimplement various HTTP request methods such as GET (to retrieve data, e.g.,\naccount balance or transaction history), POST (to initiate a backend process), and\nPUT (to create or update a resource on the backend).\nAlthough both PUT and POST can be used to create or update a\nresource, the main difference is that PUT is idempotent . This\nmeans that making the same PUT request multiple times will pro‐\nduce the same result each time. For example, if you submit a\nrequest through your banking application to pay an electricity bill,\nsubmitting the same request again should not result in the bill\nbeing paid twice.\nAPIs can be written in most programming languages, including Java, C, C++,\nNode.js, and Python. Java and Python APIs are quite common among data engineers.\nWithin the Python ecosystem, frameworks such as Flask, FastAPI, and Django are\nwidely used to program web-based APIs. To provide a short example, consider the\nfollowing statement:\n# Bash\ncurl -X POST \\n     -H ""Content-Type: application/json"" \\n     -d '[{""idType"": ""TICKER"", ""idValue"": ""AAPL"", ""exchCode"": ""UN""}]' \\n     https://api.openfigi.com/v2/mapping\nThis curl  command is making a POST request to the OpenFIGI API ( https://\napi.openfigi.com/v2/mapping ) to fetch mapping information for the financial\ninstrument with the ticker symbol AAPL (Apple Inc.) on the exchange with code UN\n(New Y ork Stock Exchange). To test this API request, open a terminal or command\nline window on your computer, paste the command, and press Enter. The response is\nexpected to have a structure similar to this:\n[\n  {\n    ""data"": [\n      {\n        ""figi"": ""BBG000B9XVV8"" ,\n        ""name"": ""APPLE INC"" ,\n        ""ticker"" : ""AAPL"",\n        ""exchCode"" : ""UN"",\n        ""compositeFIGI"" : ""BBG000B9XRY4"" ,\n270 | Chapter 7: Data Ingestion Layer\n        ""uniqueID"" : null,\n        ""securityType"" : ""Common Stock"" ,\n        ""marketSector"" : ""Equity"" ,\n        ""shareClassFIGI"" : ""BBG001S5N8V8"" ,\n        ""uniqueIDFutOpt"" : null,\n        ""securityType2"" : ""Common Stock"" ,\n        ""securityDescription"" : ""AAPL""\n      }\n    ]\n  }\n]\nAPIs are everywhere. In finance, they are extensively used for all kinds of purposes.\nPayment APIs, often known as payment gateway APIs, are a major application field.\nIn simple terms, a payment gateway is a technology that facilitates the acceptance and\nprocessing of electronic payments between merchants and financial institutions. This\nincludes credit and debit card payments, digital wallets, and bank transfers. At the\ncore of a payment gateway technology is an API that handles all the payment lifecycle\nphases and allows the involved entities (merchant, processor, gateway, financial insti‐\ntution, etc.) to talk to each other. Among the most known payment gateway APIs are\nSquare, Stripe, PayPal, Authorize.Net, and Adyen.\nAPIs play a pivotal role in accessing data from financial data vendors. These APIs are\ndesigned to allow clients to retrieve and import data programmatically, enabling the\ndevelopment of data-driven financial applications. For instance, Bloomberg offers the\nServer API (SAPI)  to allow customers to access Bloomberg Terminal data through\nboth proprietary and third-party applications. Similarly, LSEG (formerly Refinitiv)\nprovides the Eikon Data API , a Python-based library that enables users to access\nEikon data using Python.\nWhen ingesting financial data from external sources, check if the\ndata provider has an API in place. Using the API can be quite con‐\nvenient and accelerate development. At the same time, make sure\nyou check any vendor-specific API limitations that might impact\nyour application, such as a single-request size limit (e.g., 100 prices\nper request), request rate limits (e.g., 1K requests/day), request\ntimeout, and maximum concurrent requests. If you’ d like to learn\nmore, check the FactSet’s Formula API limitations available on the\nvendor’s official web page .\nData Ingestion Technologies | 271\nFinancial Data Sharing with Open Finance,\nOpen Banking, and Financial APIs\nRecently, there has been growing interest  among financial market participants in\nopen finance initiatives. These initiatives seek to establish a digital ecosystem that\nenables seamless sharing of financial data between financial institutions and third-\nparty service providers. The primary driver of this new paradigm is to foster collabo‐\nration among market participants to produce better goods and services driven by\nfinancial data.\nThe most popular example of an open finance initiative is commonly referred to as\nopen banking , in which traditional banking institutions and third-party service pro‐\nviders such as FinTech firms collaborate to provide innovative financial products. To\nfacilitate collaboration, banks and FinTechs share data using ad hoc financial APIs.\nVarious regulatory frameworks have been developed to promote open finance. For\nexample, the second Payment Services Directive (PSD2) , adopted in 2015 by the EU,\nimposed an obligation on banks to facilitate access to payment data for third-party\nservice providers via a secure interface. To promote a wider open finance ecosystem\nthat goes beyond payment account data, the EU proposed the Financial Data Access\nlegislation , whose main goal is “to establish a framework governing access to and use\nof customer data in finance. ”\nSpecial types of open banking enablers, regulated under frameworks such as PSD2 in\nEurope, include Account Information Service Providers  (AISPs) and Payment Initiation\nService Providers  (PISPs). An AISP , or a company with an AIS license, collects, aggre‐\ngates, and facilitates access to a user’s financial information across accounts held with\nvarious institutions. PISPs, on the other hand, facilitate direct payments from the\nconsumer’s bank account to online merchants.\nVarious enablers have emerged to offer platforms for open banking. For instance,\nTink, a Swedish company later acquired by Visa, facilitates connections to over 6,000\nbanks throughout Europe. Another example is Powens, a French company that con‐\nnects over 1,800 institutions across numerous European countries, providing an open\nfinance platform to its clients.\nCrucially, it is important to keep in mind that financial APIs are not simply a bunch\nof FastAPI or Flask methods. Instead, APIs should be designed with business, user,\nand application requirements in mind. For example, JPMorgan Chase classifies finan‐\ncial APIs into data and service APIs . Data APIs are mainly used to request financial\ndata, but they can be designed in a way that they can be essential tools for building\napplications, enable external collaborations, and be reusable  across multiple\n272 | Chapter 7: Data Ingestion Layer\n17To learn more about these and other API optimization topics, I highly recommend the book by De Brajesh,\nAPI Management: An Architect’s Guide to Developing and Managing APIs for Your Organization  (APress,\n2017).\n18For a good reference on API security, see Neil Madden’s API Security in Action  (Manning, 2020).departments,  channels, and product lines. Service APIs, on the other hand, are used\nto create and trigger an instance of a service, such as initiating a payment, a balance\ninquiry, or requesting a change from the bank. Many firms use the term API integra‐\ntion to denote a strategy where applications are connected via their APIs. The goal is\nto create an infrastructure in which data exchange and communication occur seam‐\nlessly through APIs, facilitating creativity and innovation.\nWhen designing a financial API, the two most important elements to consider are\nperformance and security. API performance is often measured by the ability of the\nsystem to scale to a large number of concurrent requests, as well as its request\nresponse time. Measures such as “hits per sec and “requests per sec” are often used for\nthis purpose. The idea is to measure the ability of an API to handle a large number of\nrequests/hits in one second. Common API performance optimization techniques\ninvolve load balancing, caching, rate limiting, and throttling, among others.17\nAs for security, the primary elements to consider are authentication and authoriza‐\ntion to control how and who can interact with the API. Tools such as firewalls,\nauthentication tokens, OAuth 2.0, API keys, and API gateways are often used for this\npurpose.18 In addition, APIs need to be protected against advanced malicious attacks\nsuch as SQL injection (SQLi). In SQLi, a cybercriminal exploits application vulnera‐\nbilities to inject malicious input into an API request that alters the behavior of a back‐\nend SQL query. For example, let’s take a naive scenario where the user is required to\ninsert their user ID to access their account balance. Upon providing such credential\n(e.g., user ID: 267), an API request is sent to the backend, which then executes the\nfollowing SQL query:\n-- SQL\nSELECT first_name , last_name , account_balance  \nFROM user_accounts  \nWHERE user_id = 267\nNow, if the API doesn’t handle SQL injection properly, then it is possible to provide\ninput such as user ID “267 OR 1=1” . In this case, your backend may end up executing\nthe following query:\n-- SQL\nSELECT first_name , last_name , account_balance  \nFROM user_accounts   \nWHERE user_id = 267 OR 1=1\nData Ingestion Technologies | 273",11634
101-Secure File Transfer.pdf,101-Secure File Transfer,"19For a detailed analysis of SQL injection, I recommend the seminal paper by William G. Halfond, Jeremy Vie‐\ngas, and Alessandro Orso, “ A Classification of SQL Injection Attacks and Countermeasures” , in the Proceed‐\nings of the International Symposium on Secure Software  Engineering , vol. 1, (March 2006): 13–15.\n20For an overview of this issue, see “The Relentless Rise of Real-Time Data” , by LSEG.Because the SQL condition 1=1 always evaluates to TRUE , the entire WHERE  statement\nwill be true, regardless of whether the provided user ID is correct. In this case, the\nentire list of users and their account balances will be queried, which may lead to\nmajor data breaches.19\nFinancial Data Feeds\nIn the financial industry, the term “data feed” refers to a mechanism designed to\ndeliver the latest financial data and updates to traders, investment firms, and financial\ninstitutions. The main sources of data feeds include stock exchanges, financial news\nproviders, and market data vendors.\nFinancial data feeds can be designed to transmit data either as historical snapshots or,\nmore commonly, in live mode. When provided in live mode, a data feed offers a con‐\ntinuous stream of real-time data, essential in scenarios where timely data access is\ncritical. Moreover, data feeds often include features that allow users to configure the\ntiming, location, and specific data they want to receive. Financial data feeds may vary\nin terms of latency, throughput, and delivery guarantees.\nExamples of financial data feeds include S&P Global’s Xpressfeed , which offers access\nto over 200 datasets and allows users to customize data extraction and delivery loca‐\ntions. LSEG’s Real-Time – Ultra Direct  is another example, providing a high-\nperformance, low-latency real-time market data feed. In addition, Bloomberg\nprovides the Bloomberg Market Data Feed (B-PIPE) . Exchange venues also offer\nmarket data feeds, such as the NYSE Trades Data Feed  and NASDAQ Market Data\nFeeds , which stream trading data as it happens and often provide the lowest latency\ndue to being the original data source. Finally, news feeds, like those from MT News‐\nwires , are a common type of data feed, delivering real-time news headlines and text.\nOne challenge when dealing with financial data feeds is information overload. This\nhappens when a large volume of data from one or more feeds overwhelms the exist‐\ning financial data infrastructure. The adoption of cloud technology has alleviated this\nissue by allowing financial institutions to store and retrieve data at any scale without\nthe need to manage and maintain the underlying infrastructure.20\n274 | Chapter 7: Data Ingestion Layer",2700
102-Cloud Access.pdf,102-Cloud Access,"Secure File Transfer\nFile transfer is one of the most frequent and regular operations in the financial sector.\nBanks, for example, transfer files for loan applications, transaction history, and\naccount information; investment firms transfer trade data, portfolio composition, and\nintellectual property files like investment strategy or trading algorithms; insurance\ncompanies transfer client policy and personal information files; and regulated finan‐\ncial institutions must submit various filings to comply with regulatory reporting\nrequirements.\nImportantly, files transferred by financial institutions often contain sensitive informa‐\ntion. As a result, file sharing needs to be secured to meet the financial sector’s security\nand privacy requirements. To this end, a widely used technology is the Secure File\nTransfer Protocol  (SFTP), which leverages the SSH protocol to encrypt data and com‐\nmands that one machine submits to another. SFTP is secure, reliable, and platform\nindependent.\nFile transfer via SFTP is a good solution for bulk and large file transfers. It is also\noften used when there isn’t an API available for data exchange. Crucially, SFTP may\nnot be the best option for all use cases. For example, it may not be ideal in high-speed\nand large-volume data-driven systems characterized by demanding workloads. Fur‐\nthermore, SFTP may require a security policy to manage passwords, keys, and user\naccess, which can increase the complexity of file transfer.\nVarious alternatives have been proposed to address these issues. For instance, man‐\naged SFTP solutions, like Managed File Transfer (MFT) , provide enhanced\nenterprise-level functionality for security, performance, compliance, and reporting\nbeyond what standard SFTP offers. Additionally, protocols such as FTPS (a more\nsecure form of FTP), SCP (Secure Copy Protocol), and WebDAV (Web Distributed\nAuthoring and Versioning) can be utilized depending on the specific business\nrequirements.\nCloud Access\nWith the widespread adoption of cloud computing across nearly every sector, cloud-\nbased data sharing and access has emerged as a reliable and convenient way to\nexchange data. In its simplest setup, a data provider creates a storage bucket or cloud\ndatabase within a dedicated and isolated cloud environment. It then uploads the data\nand authorizes the target user to access and manipulate it. Subsequently, whenever\nnew data updates are available, the provider uploads them to the storage location,\nenabling immediate access for the user.\nOne convenient aspect of this access method is that users can leverage various cloud-\nbased features when working with the data, including user interfaces, querying capa‐\nbilities, data management, search functions, and more. Furthermore, there are\nData Ingestion Technologies | 275\ncost-saving  benefits and seamless integration experience with other cloud services,\nparticularly for clients already utilizing the cloud. For instance, if your data pipelines\nare already processed in the cloud, having a new data source accessible directly\nthrough the cloud simplifies workflow management. Additionally, data updates occur\ncontinuously with minimal intervention required on the user’s part.\nHere are a few examples. In 2022, Bloomberg and Google announced a new partner‐\nship that will allow mutual customers to easily access B-PIPE, Bloomberg’s real-time\nmarket data feed, through Google Cloud. Similarly, CME Group, the world’s leading\nderivatives marketplace, partnered with Google Cloud  to provide fast and reliable\nmarket data access to their customers.\nCase Study: FactSet Integration with AWS Redshift and Snowflake\nFactSet is a well-known financial data vendor that provides content to more than\n160,000 investment professionals worldwide. FactSet has expanded its data delivery\noptions in the past few years to include cloud-based access. For example, it is now\npossible to access 100+ FactSet proprietary and third-party financial datasets through\npopular cloud data warehouse services such as AWS Redshift  and Snowflake .\nWith this cloud-based delivery, FactSet saves its clients the need to clean, model, and\nnormalize the data. It is already populated in SQL tables and is ready for users to\nquery. Data is constantly updated and added to existing tables. In addition, if the user\nrelies on both FactSet proprietary data and third-party vendor data offered via Fact‐\nSet, cloud delivery allows centralization and integration of disparate data sources into\na single platform such as Redshift or Snowflake.\nCloud providers have recently increased their market competition in the financial\ndata market. This is expressed by the emergence of the so-called financial data mar‐\nketplaces. These managed cloud solutions allow financial data providers to distribute\nand share their data through a single cloud interface. This may be convenient for\nfinancial data providers as it eliminates the need to build and maintain an infrastruc‐\nture for data storage, distribution, billing, and user management. Examples include\nAWS Data Exchange for Financial Services  and Google Cloud Datashare for financial\nservices . Cloud marketplace data can be delivered in a variety of ways. For example,\nGoogle Datashare distributes data in batches through its managed warehouse solu‐\ntion BigQuery and in real time through its managed messaging service Pub/Sub.\n276 | Chapter 7: Data Ingestion Layer",5479
103-Web Access.pdf,103-Web Access,,0
104-Specialized Financial Software.pdf,104-Specialized Financial Software,,0
105-Data Ingestion Best Practices.pdf,105-Data Ingestion Best Practices,,0
106-Design for Change.pdf,106-Design for Change,"Web Access\nA user-friendly and straightforward way to access financial data is through a dedica‐\nted web page provided by a financial institution or data vendor. In this web access\nmode, financial data can be downloaded in file format, queried and visualized using a\nquery builder, or quickly parsed and analyzed by the user.\nThis mode is convenient for handling small datasets or when speed is not critical,\nsuch as for scheduled data extractions. However, for faster data access or bulk file\ndownloads, alternative data ingestion technologies like SFTP , API, or cloud-based\nsolutions are more suitable.\nSpecialized Financial Software\nIn certain financial data exchange settings, specialized or dedicated software is imple‐\nmented. This is particularly common when setting up standardized systems for\nsecure financial messaging, payments, transactions, and other market operations.\nAn example of specialized financial software is FIX engines , which are software appli‐\ncations that enable two institutions to exchange FIX messages (discussed in the previ‐\nous section). The FIX engine handles the network connection, transmits and receives\nFIX messages, and validates the submitted messages against the FIX protocol and\nformat.\nData Ingestion Best Practices\nWhen building a data ingestion layer, it is important to make sure it is rock solid. A\npoorly designed data ingestion layer may easily become the bottleneck of your infra‐\nstructure and impact the entire financial data engineering lifecycle. Adhering to best\npractices can help ensure the resilience of this layer. This section discusses a few.\nMeet Business Requirements\nFirst of all, the data ingestion layer needs to meet the business requirements estab‐\nlished by your financial institution. Don’t overcomplicate the ingestion layer; if your\norganization wants to work and exchange CSV and Excel files, then build a simple\ningestion layer that can process these formats. If an API is not going to bring great\nbenefits, don’t build one and instead use a simple filesystem or cloud storage solution.\nHowever, if your company has complex data ingestion needs, consider building an\nextensible and flexible layer that can handle new types of formats and ingestion\nmechanisms.\nData Ingestion Best Practices | 277",2303
107-Perform Benchmarking and Stress Testing.pdf,107-Perform Benchmarking and Stress Testing,"21For a good reference on this problem, I recommend Chris Pickles’ “Securities Standards Migration: ISO 15022\nvs ISO 20022” , Journal of Securities Operations & Custody  1, no. 3 (Spring 2008): 289–300. \n22To learn about these techniques, I suggest Chaitanya K. Rudrabhatla’s “Comparison of Zero Downtime Based\nDeployment Techniques in Public Cloud Infrastructure” , in the 2020 Fourth International Conference on I-\nSMAC (IoT in Social, Mobile, Analytics and Cloud)  (IEEE, 2020): 1082–1086.Design for Change\nChange is a constant in financial markets. New practices, standards, and regulations\nare continually taking place across the industry. For example, it is not unusual for\nmarkets to begin migrating to a newly published standard, and, in the meantime, a\nnew standard emerges to replace it.21 For this reason, always consider the change\ndynamics that might affect your data ingestion layer. There isn’t a fixed recipe for\nmanaging change, but you can consider best practices such as the following:\nIncremental change\nMake small and gradual changes.\nIsolated change\nMake sure you develop and test your changes in isolation and avoid release\nincompatibility.\nDocumented change\nMake sure you describe the what, how, and why of your changes so others can\nunderstand what will change.\nZero downtime\nRoll out changes using a reliable technique with a rollback option to ensure zero\ndowntime and avoid disruption to end users. Examples include Blue/Green, Can‐\nary, and Rolling deployment techniques.22\nEnforce Data Governance\nEnforcing data governance at the ingestion layer is recommended in financial appli‐\ncations. Examples of good practices include the following:\nData validation\nAdd validators to check the conformity of ingested data to defined acceptance\ncriteria. For example, you can validate if a file format is CSV or XML, validate an\ningested message against standard requirements (e.g., FIX or XBRL), or validate\ningested data against errors and quality issues.\nLogging and reporting\nConsider having an audit log that records all ingestion events. This is useful for\ntracking wrongful or malicious ingestions and for regulatory reporting.\n278 | Chapter 7: Data Ingestion Layer",2225
108-Summary.pdf,108-Summary,"23Data poisoning is a type of attack where data is intentionally ingested to alter the performance or behavior of\na machine learning model. For more on this topic, I recommend Antonio Emanuele Cinà, Kathrin Grosse,\nAmbra Demontis, Sebastiano Vascon, Werner Zellinger, Bernhard A. Moser, Alina Oprea, Battista Biggio,\nMarcello Pelillo, and Fabio Roli’s “Wild Patterns Reloaded: A Survey of Machine Learning Security Against\nTraining Data Poisoning” , ACM Computing Surveys  55, no. 13s (July 2023): 1–39.\n24For a reference on benchmarking financial data ingestion, I recommend Manuel Coenen, Christoph Wagner,\nAlexander Echler, and Sebastian Frischbier’s “Benchmarking Financial Data Feed Systems” , in the Proceedings\nof the 13th ACM International Conference on Distributed and Event-based Systems  (June 2019): 252–253. Lineage and visibility\nImplement a mechanism that allows ingested data to be traced back to its origin.\nThe goal is to determine when, how, and where a particular piece of data entered\nthe system.\nSecurity\nThe data ingestion layer is the entry point for your financial data infrastructure.\nThis means that it can be exploited by cybercriminals to ingest malicious data or\nsoftware. To this end, ensuring the security of your data ingestion needs to be a\ntop priority. Malicious data can be ingested in different ways, such as malware\nfiles, SQL ingestion, and data poisoning.23 Security can be ensured via proper\nauthentication and authorization policies, user permission management, virus\nscanning, allowed file formats (e.g., you may want to discard zip or pickled files),\nAPI security, and more.\nPerform Benchmarking and Stress Testing\nA good practice when building a data ingestion layer is performing a stress test to\ncheck the infrastructure’s ability to handle variable workloads and ingestion scenar‐\nios. This is particularly essential when designing an event-driven or real-time data\ningestion layer. A useful testing technique is benchmarking, which can be useful\nwhen choosing between different ingestion technologies. A benchmarking tool can be\nused for assessing the performance of a given ingestion technology by simulating a\nrealistic workload scenario.24\nSummary\nIn this chapter, you explored the first layer of the FDEL: ingestion. This layer acts as\nthe entry point to a financial data infrastructure, enabling data to be ingested through\nvarious arrival processes, transmission protocols, formats, and technologies. This\nchapter covered various aspects of ingestion specific to financial data, focusing on the\nunique requirements and preferences of market participants.\nThe ingestion layer should be regarded as a vital component of your FDEL. Bottle‐\nnecks at this stage can lead to performance issues further downstream. Additionally, a\nSummary | 279\nrobust ingestion system is becoming increasingly essential for financial institutions\ndue to the growing trends in data sharing, the expanding variety of data formats and\narrival processes, and the increasing volumes of ingested data.\nAt this point, you may be asking yourself, “Where does the ingested data go?” This\ntakes us to the next layer, storage, which will be covered in Chapter 8 . In this layer, a\ndata storage system is used to store and retrieve data according to specific business\nand technical requirements. As you might have guessed, this is the layer where you\nimplement the most popular technology in data engineering: a database.\nLet’s continue!\n280 | Chapter 7: Data Ingestion Layer",3542
109-Chapter 8. Data Storage Layer.pdf,109-Chapter 8. Data Storage Layer,,0
110-Principle 3 Transactional Guarantee.pdf,110-Principle 3 Transactional Guarantee,"CHAPTER 8\nData Storage Layer\nIn the previous chapter, you learned how the data ingestion layer works, including\nthe mechanisms, technologies, and formats used to ingest data into a financial data\ninfrastructure. Once ingested, data must be stored and persisted in a storage location\nfor further processing and querying. This is where the data storage layer comes into\nthe picture.\nTo help you understand how to build a robust storage layer, this chapter will provide\nyou with the necessary fundamentals and concepts, along with illustrations of tech‐\nnologies and their applications in finance. First, you’ll learn how to approach the\ndesign of a data storage system (DSS) using appropriate criteria. Next, the concept of \na data storage model (DSM) and its categorization criteria will be introduced. Then, I\nwill present a comprehensive list of DMSs relevant to the financial industry, high‐\nlighting each DMS’s key features, data modeling concepts, technical implementations,\nand financial applications.\nPrinciples of Data Storage System Design\nThroughout this book, I use the term data storage system  (DSS) to denote a software\nimplementation that enables the storage and retrieval of data. In many cases, people\nuse the term “database” to refer to a storage solution. However, databases are only one\ntype of DSS, albeit a popular one.\nAs a financial data engineer, knowing how to assess, choose, design, and implement a\nDSS should be one of your primary skills and areas of knowledge. The DSS is a core\ncomponent in several financial applications, such as trading, payment, and messaging\nplatforms. Making the wrong DSS choice can be quite costly and lead to a notable\nimpact on the performance and reliability of your infrastructure. If a change is\nrequired later, you might find yourself dealing with a complex and expensive data\n281\nmigration project that wastes resources and time. This is particularly true when other\napplications and layers have already been built on top of the existing DSS.\nDesigning the appropriate DSS may look overwhelming at first glance. This is\nbecause of the vast number of technologies, patterns, constraints, business require‐\nments, and marketing materials that affect the decisions around a DSS. To address\nthis challenge, this section will provide a set of universally applicable principles that\ncan guide your DSS design and implementation strategy.\nPrinciple 1: Business Requirements\nAlthough data engineering appears to be a strictly technological discipline, the truth\nis that it is highly driven by business requirements and customer needs. I highly rec‐\nommend that you do not proceed with the design of a DSS without first including the\nbusiness team’s feedback into your decision-making process. As a financial data engi‐\nneer, you don’t necessarily need to be a business expert, but by knowing just enough\nto understand business expectations, you are likely to make the best design choices.\nBusiness requirements can vary in terms of complexity (e.g., what and how many fea‐\ntures are required?), flexibility (are they strict requirements?), technical feasibility\n(are there technical limitations?), predictability (can you anticipate the requirements\nin advance?), timing (have the requirements already been formulated?), and stability\n(do requirements change with time?).\nExamples of data-related business requirements include the following:\n•Ease of data access (e.g., a user interface)\n•Schema flexibility (e.g., a new feature requires a new field in the data schema)\n•User scalability (e.g., a high number of concurrent users)\n•Speed of data access (e.g., for high-frequency trading)\n•Querying capabilities (e.g., complex analytical queries and filters)\n•Storage needs (e.g., storing a massive dataset such as market transactions)\n•Data sharing (e.g., to collaborate with external companies)\n•Data aggregation (e.g., for regulatory reporting)\nDSS design and business requirements are matched through an iterative and collabo‐\nrative approach. To succeed in this process, financial data engineers need to commu‐\nnicate in business terms and translate the requirements into technical\nimplementation. Along the way, they need to spot all possible bottlenecks or technical\nlimitations and reach an agreement with the business team to achieve feasibility.\n282 | Chapter 8: Data Storage Layer\nPrinciple 2: Data Modeling\nData modeling is a crucial, yet often underestimated, practice in DSS design. In gen‐\neral, data modeling focuses on how data is organized, connected, and stored to meet\nboth technical and business requirements. Importantly, the data engineering commu‐\nnity still lacks a widely recognized data modeling framework. Nevertheless, market\npractitioners have typically relied on a popular approach that breaks down data mod‐\neling into three sequential phases: conceptual , logical , and physical . These phases are\ndeveloped as one progresses from business needs to a comprehensive data storage\nspecification, which is the desired end state.\nThe conceptual phase is technology independent and involves a communicative pro‐\ncess where data engineers and stakeholders together discuss all their data needs.\nThink of the conceptual phase as a whiteboard session where various members of\nyour team meet to discuss the initial data requirements. The main outcome of this\nphase is identifying a model for data, relationships, constraints, and querying needs\nthat are essential to the business application. At this stage, no decisions are made\nabout how the data will be stored or which DSS will be used for this purpose.\nSubsequently, during the logical phase, the conceptual model is mapped to a blue‐\nprint of structured and technical constructs such as rows, columns, tables, and docu‐\nments that a DSS can implement. At the end of the logical phase, you still haven’t\npicked a specific DSS, but you should clearly know how the data will appear in the\nDSS and what DSM to use (for example, relational or document models). Later in this\nchapter, we will discuss DMSs in detail.\nFinally, the physical phase translates the logical model into the DSS language, which\nis often known as Data Definition  Language  (DDL). This stage’s main outcome\ninvolves choosing a specific DSS (e.g., a relational database management system) and\ndeveloping a clear understanding of how data is stored (e.g., on disk, in memory, or\nhybrid), and how data is replicated, sharded, partitioned, or distributed.\nData Modeling Use Case in Finance:\nReference Data and Financial Standards\nIn Chapter 2 , we discussed the well-known problem of financial reference data, i.e.,\nmetadata used to describe financial instruments. The main challenge with reference\ndata is the lack of a universally accepted framework for its representation and format‐\nting, especially for complex financial instruments like derivatives. This issue is an\nexcellent example of a financial domain problem that can be addressed through data\nmodeling. Data modeling offers a robust solution by providing structured and stan‐\ndardized conceptual models for representing, collecting, and storing reference data.\nFor an excellent treatment of this topic, I recommend Robert Mamayev’s book Data\nModeling of Financial Derivatives: A Conceptual Approach  (Apress, 2013), in which he\nPrinciples of Data Storage System Design | 283\n1For a comprehensive overview of data modeling, I highly recommend Graeme Simsion and Graham Witt’s\nData Modeling Essentials , 3rd ed. (Morgan Kaufmann, 2004).\n2This definition is derived from the one provided by PostgreSQL documentation .\n3The acronym ACID was coined in 1983 by Andreas Reuter and Theo Härder in their seminal work, “Princi‐\nples of Transaction-Oriented Database Recovery” , ACM Computing Surveys (CSUR)  15, no. 4 (December\n1983): 287–317.illustrates how to structure and describe derivatives such as futures, forwards,\noptions, swaps, and forward rate agreements using advanced data modeling tech‐\nniques.\nSimilarly, the development and formulation of financial standards like ISO 20022 can\nbe viewed through the lens of data modeling. As discussed in Chapter 7 , creating an\nISO 20022 message model involves conceptual, logical, and physical stages, mirroring\nthe methodology outlined in this section.\nAn important thing to keep in mind is that data modeling is an iterative process. Data\nengineers and business teams may continually reorganize, restructure, and optimize\ndata models to fit new or revised business needs.1\nPrinciple 3: Transactional Guarantee\nOne of the most important features of a DSS is its ability to ensure data consistency\nand reliability. In other words, the data must always accurately reflect its true state.\nFor example, if you have $10,000 in your bank account and purchase a book for $50,\nyour new balance should be $9,950, not $9,000. Similarly, if you purchase a book for\n$50 and at the same time you also purchase a car for $10,000, then you either end up\nwith $9,950 or $0 but not –$50.\nThis feature is known as a transactional guarantee . Here, “transaction” describes an\ninternal DSS mechanism that allows the bundling together of multiple instructions\ninto a single, all-or-nothing operation.2 In the data technology landscape, transac‐\ntional guarantees are commonly implemented using two main models: ACID  and\nBASE .\nACID is the most popular model,3 and it stands for:\nAtomicity\nA DSS transaction either succeeds completely (gets committed) or, in case of a\nfault, gets entirely aborted, reverting the DSS to the state before the transaction\nstarted. In our previous book and car example, atomicity would mean that either\nboth items are bought (a book and a car) or neither is bought and the account\nbalance remains intact. A violation of atomicity would occur if we attempted to\n284 | Chapter 8: Data Storage Layer\n4The definition I present for snapshot isolation is provided in an operational way, which is done for the sake of\nsimplicity. This concept can be further explored from a more rigorous, mathematical point of view; for exam‐\nple, see Andrea Cerone and Alexey Gotsman’s “ Analysing Snapshot Isolation” , Journal of the ACM (JACM)  65,\nno. 2 (January 2018): 1–41.purchase both the book and the car in a single transaction, where the purchase of\nthe book succeeds but the car purchase fails.\nConsistency\nDSS transactions need to preserve structural integrity and enforce defined con‐\nstraints. Consider again our previous example of a $10,050 transaction with an\navailable balance of $10,000. Assume that the account balance has a non-negative\nconstraint. In this scenario, the transaction should fail to prevent the balance\nfrom becoming negative, ensuring consistency. Maintaining this consistency is\ntypically the responsibility of DSS engineers, who establish and implement data\nconsistency checks, constraints, and validations based on business requirements,\nboth at the DSS level as well as within the applications that interact with it.\nIsolation\nConcurrent DSS transactions get executed in isolation from one another while\nensuring integrity and resolving conflicts. For instance, if you attempt two sepa‐\nrate transactions—one to buy a book and another to buy a car—and only one can\nsucceed, the DSS identifies this conflict and ensures proper resolution. Maintain‐\ning isolation is primarily the responsibility of the DSS. A notable strategy is snap‐\nshot isolation , where each transaction within the DSS perceives a consistent\nsnapshot of data that includes all committed changes up to the transaction’s\ninitiation.4\nDurability\nThe DSS preserves and persists the committed transactions even in case of a sys‐\ntem failure (e.g., a power outage or crash). In our example, suppose you just\ncompleted the book purchase for $50. If the transaction is completed and you\nreceive a notification of success, then the DSS must ensure that your purchase\nhas been recorded in nonvolatile storage such as a hard drive or SSD. One of the\nmost reliable techniques for ensuring durability is Write-Ahead-Logging  (W AL),\nwhose main idea is to first “log” all changes to be applied to the data before per‐\nsisting them to disk.\nACID properties are of primary importance for the design of financial DSSs. Exam‐\nples include financial transaction processing, where money or ownership of financial\ninstruments is transferred between customer accounts, and order matching systems\nthat execute buy and sell orders. In all of these applications, it’s critical to maintain a\nconsistent state of the data by ensuring that transactions either complete entirely or\nPrinciples of Data Storage System Design | 285\nnot at all. Throughout this chapter, I will provide detailed technical illustrations of\nhow various DSSs implement and ensure ACID properties.\nMany data storage solution providers make the ACID compliance\npromise. However, they don’t all implement it in the same manner.\nSome may use a portion of the ACID properties (e.g., atomicity) or\ndevelop their own version of ACID. When designing a financial\napplication with strict requirements in terms of data consistency\nand a transactional guarantee, ensure you thoroughly comprehend\nthe ACID features of the chosen solution. If not, this might influ‐\nence your product’s market reputation.\nIn some cases, enforcing ACID compliance may impact performance or may not even\nbe that necessary. In other cases, ACID might even be impractical to achieve. This is\nwhere a lighter version of ACID, called BASE, has been introduced.\nBASE stands for:\nBasically available\nInstead of enforcing immediate consistency, BASE-modeled DSSs focus on\nensuring availability by distributing and replicating data across a set of nodes in a\ndata storage cluster. In the event of a failure in one node, data would still be avail‐\nable through another node that holds a replica of the data.\nSoft state\nAs the requirement of immediate consistency is relaxed, the BASE model dele‐\ngates the responsibility of achieving data consistency to the engineer instead of\nthe DSS itself. In other words, it is the developer’s problem to ensure data consis‐\ntency, and it is no longer a feature of the DSS itself.\nEventually consistent\nEven though BASE does not aim for immediate consistency, this does not mean\nthat it never achieves it: it is eventually achieved, meaning that data will converge\nto a consistent state at some point in the future. No guarantee is made, however,\nabout when consistency will be achieved.\nThe BASE model is quite useful for designing distributed and big data storage solu‐\ntions, especially for analytical purposes where speed and scalability are more impor‐\ntant than strict consistency. For instance, a trading platform might analyze vast\namounts of data to provide traders with insights and market trends. In this scenario,\nthe BASE model could be suitable, as it prioritizes processing large datasets quickly,\neven if some data might be slightly outdated.\n286 | Chapter 8: Data Storage Layer",15145
111-Principle 4 Scalability.pdf,111-Principle 4 Scalability,"5This example was inspired by the excellent illustration of the CAP theorem by ByteByteGo on Y ouTube .\nPrinciple 4: Consistency Tradeoffs\nA distributed DSS consists of several machines, or nodes, working together to store\nand manage data collectively. Designing and building such systems can be highly\ncomplex. Several consistency tradeoff theorems have been formalized to assist engi‐\nneers with this challenge. The most prominent is the CAP theorem, which stands for\nconsistency, availability, and partition tolerance. According to the CAP theorem, if a\nnetwork partition  occurs, where some nodes in a distributed DSS are unable to com‐\nmunicate due to a network failure, the system can guarantee at most two out of the\nfollowing features:\nConsistency\nAll users always see the same latest version of the data, regardless of which node\nthey interact with.\nAvailability\nThe DSS responds to all user requests at all times, though it may not always pro‐\nvide the most recent data.\nPartition tolerance\nThe data storage system continues to operate even when there is a network\npartition.\nKeep in mind that consistency in ACID is different from consis‐\ntency in the CAP theorem. In the CAP theorem, consistency means\nthat all nodes maintain a consistent view of the data. Conversely, in\nthe context of ACID, consistency ensures that the database remains\nin a valid and correct state.\nTo illustrate the CAP theorem with an example, suppose we have a bank with two\nATMs connected over a network.5 Assume the bank requires that customer balances\nnever drop below zero, but there is no central database system to ensure this condi‐\ntion. Instead, a copy of the database is stored on both ATM instances, and as users\ncarry out their operations on their accounts, the two ATMs communicate over the\nnetwork to ensure consistency.\nNow, suppose that a network partition happens, and the two ATMs can no longer\ncommunicate. In such a case, you, as the designer of the DSS, need to decide whether\nto prioritize consistency or availability. If you prioritize consistency, you are likely to\nrefuse all ATM operations (withdrawal, deposit, balance checks) until the partition is\nPrinciples of Data Storage System Design | 287\n6To learn more about consistency tradeoffs, I highly recommend Daniel Abadi’s “Consistency Tradeoffs in\nModern Distributed Database System Design: CAP Is Only Part of the Story” , Computer  45, no. 2 (February\n2012): 37–42.resolved. This guarantees balance consistency, but the ATM services will not be avail‐\nable to customers. If, on the other hand, you favor availability, then you may allow\neach ATM to perform deposit and withdrawal operations but at the risk of ending in\nan inconsistent state until the partition is resolved (e.g., withdrawing the entire bal‐\nance from both ATMs, ending up with a negative balance).\nKeep in mind that the CAP theorem is quite simplistic and basic. In real applications,\ntradeoffs might be more complex than just 100% availability or 100% consistency. For\nexample, in our ATM example, rather than completely blocking or allowing all ATM\noperations, the bank may still allow for balance inquiry or small money withdrawals\nduring a partition and only refuse large withdrawals and deposits.\nAs an extension of the CAP theorem, the PACELC theorem was introduced. PACELC\nconsiders two system scenarios: first, if the system has a network partition, it must\ndecide how to trade off consistency and availability; second, if the system is running\nnormally without a network partition, it must still decide how to trade off consistency\nand latency. The latency/consistency tradeoff exists only if the distributed DSS repli‐\ncates data.6\nWhen working with distributed DSSs, several of them offer the option to tune the\nconsistency level. For example, Azure Cosmos DB offers five levels of consistency\nguarantees , which, from strongest to weakest, are strong, bounded staleness, session,\nconsistent prefix, and eventual; Amazon DynamoDB offers either strong or eventual\nconsistency levels; and Cassandra offers several consistency levels  that also range\nfrom weak to strong.\nPrinciple 4: Scalability\nScalability is a highly desired property of DSSs. Generally, a system is considered scal‐\nable if it can effectively handle varying or increasing amounts of workload. Impor‐\ntantly, scalability can be achieved differently depending on the component or layer of\nthe DSS. To simplify, we can categorize DSS scalability into two main aspects: storage\nscalability and compute scalability.\nA DSS achieves storage scalability if it can seamlessly store increasing volumes of data\nwithout hitting a space limitation. In more practical terms, storage scalability means\nthat the system can store data at any scale: gigabytes, terabytes, or even petabytes.\n288 | Chapter 8: Data Storage Layer\n7To learn more about database stress testing, see Bert Scalzo’s Database Benchmarking and Stress Testing: An\nEvidence-Based Approach to Decisions on Architecture and Technology  (Apress, 2018).\n8See, for example, “How We Stress Test and Benchmark CockroachDB for Global Scale” , by Stan Rosenberg,\nAlexander Shraer, William Kulju, and Alex Lunev.Conversely, compute scalability in a DSS refers to its ability to efficiently handle vary‐\ning data read and write requests. A common measure of compute scalability is the\nmaximum number of concurrent read/write requests that the DSS can accommodate.\nScalability can be achieved in multiple ways. One approach is vertical scalability,\nwhere a single machine is replaced with a bigger one that has more storage, CPU, and\nRAM. On the other hand, horizontal scalability requires a distributed system that\nscales by adding a new node to an existing cluster of connected nodes.\nWhen designing a DSS, it is crucial to evaluate its capability to handle anticipated\nworkloads. This is often done via stress testing and benchmarking. A database stress\ntest works by simulating the generation of a large amount of data, queries, and con‐\ncurrent requests while observing how the DSS behaves in terms of response time,\nerrors, and reliability. The goal, in lay terms, is to stress the DSS to identify its opera‐\ntional limits and where it may encounter system failures or breakpoints.\nA database benchmark  is a special type of database stress test that relies on well-\ndefined and industry-accepted testing methodology. Examples include Transaction\nProcessing Performance Council (TPC) standards  such as TPC-C and TPC-E.7 TPC-\nC simulates an order-entry environment where a population of users concurrently\nsubmit a variety of transactions against a database. Transactions may vary in com‐\nplexity and include placing and fulfilling orders, keeping track of payments, confirm‐\ning order status, and checking the level of stock in the warehouse. TPC-C evaluates\nperformance based on the number of new-order transactions processed per minute.\nTPC-E is a more sophisticated benchmark that simulates a brokerage firm receiving\nand fulfilling various customer transactions related to trades, account inquiries, and\nmarket research. To execute client requests, the brokerage firm interacts with finan‐\ncial markets and updates account information. Performance is measured using the\ntransactions per second (TPS) metric.\nA best practice is to conduct both industry benchmark tests and internally defined ad\nhoc tests to gain insights into specific behaviors of the DSS that may not be fully\naddressed by standardized benchmarks. Commercial database vendors frequently\nemploy this dual approach to showcase the robustness and reliability of their\nproducts.8\nPrinciples of Data Storage System Design | 289",7759
112-Principle 5 Security.pdf,112-Principle 5 Security,,0
113-Primary Versus Secondary.pdf,113-Primary Versus Secondary,"Principle 5: Security\nEnsuring strong database security is crucial in financial applications. When designing\nfor security, a variety of processes, tools, and controls can be put in place to protect a\nDSS against malicious or accidental threats. These include, for example, the\nfollowing:\n•Data encryption at rest (e.g., customer account information residing in a\ndatabase)\n•Data encryption in transit (e.g., when transmitting payment information over a\nnetwork)\n•Database access permissions and roles\n•Separation of database servers from other application servers\n•Backups and data recovery plans\n•Real-time security information and event monitoring\nData security features vary across different DSSs. Therefore, it’s essential to stay\ninformed about your institution’s specific security requirements and ensure that the\nchosen DSS can effectively meet them.\nData Storage Modeling\nTo implement a DSS, a large number of technological choices are available. However,\ncomparing such technologies is not trivial. Some technologies share several features,\nyet others may be based on noncomparable design principles. Additionally, on occa‐\nsion, even the lines between the different data technologies might become blurry as\nthey introduce more of the same features. To this end, this book will take a different\napproach by relying on the concept of data storage models  (DSMs) rather than data\nstorage technologies.\nA DSM refers to the logical design and structure of a DSS, which determines the\nmanner in which data can be stored, modeled, optimized, accessed, and manipulated.\nThe concept of a DSM is closely related to the general practice of data modeling dis‐\ncussed earlier. To better contextualize things, consider a DSM as a focused subset\nwithin data modeling that specifically addresses the logical data modeling phase.\nDSMs are technology agnostic, allowing them to be implemented using a variety of\ntechnologies, with the possibility of employing the same technology for implement‐\ning different DSMs. As a result, conceptualizing in terms of DSM is considerably eas‐\nier and less susceptible to technology-specific limitations.\nIn today’s data engineering landscape, a variety of DSMs are available. In the follow‐\ning sections, I will cover eight DSMs in depth, highlighting their main features and\n290 | Chapter 8: Data Storage Layer\n9For more on this topic, read Yishan Li and Sathiamoorthy Manoharan’s “ A Performance Comparison of SQL\nand NoSQL Databases” , in the 2013 IEEE Pacific  Rim Conference on Communications, Computers and Signal\nProcessing (PACRIM)  (IEEE, 2013): 15–19. \n10For more details, see Rick Cattell’s “Scalable SQL and NoSQL Data Stores” , ACM SIGMOID Record  39, no. 4\n(May 2011): 12–27.applications in finance. But first, let’s illustrate a number of popular criteria that are\noften used to categorize DSMs.\nSQL Versus NoSQL\nIf you are part of the data engineering community, you often hear discussions com‐\nparing NoSQL and SQL. However, many argue that the SQL versus NoSQL compari‐\nson lacks a clear technical basis. But to illustrate the main idea, I will briefly explain\nits origin.\nIn data engineering, the most trusted type of DSSs have traditionally been relational\ndatabase management systems, or more simply, SQL databases (illustrated in detail in\nthe section “The Relational Model” on page 301). With the massive increase in data\nvolume, variety, and velocity, traditional SQL databases hit scalability and perfor‐\nmance limitations. In a nutshell, SQL databases could only scale vertically, which is a\nlimited scaling strategy since it is constrained by the maximum capacity of a single\nserver.\nTo overcome this issue, people began exploring more scalable and high-performance\nalternatives. This led to the development of a new family of database technologies\nsuch as document, graph, key-value, and wide-column databases. To distinguish these\ndatabase technologies from traditional SQL databases, they were grouped under the\nlabel NoSQL, which stands for “Not only SQL. ” NoSQL databases tackled the issue of\nscalability by adopting a horizontal scaling approach that relies on distributed sys‐\ntems and partitioning principles.\nImportantly, SQL databases adhere to internationally recognized SQL standards,\nwhich have contributed to their reliability and trust within the data engineering com‐\nmunity. In contrast, NoSQL databases are diverse and lack standardized behavior or\ncommon standards. Moreover, to ensure high performance, some NoSQL databases\nsacrifice several of the core features that gained SQL popularity, such as integrity con‐\nstraints, ACID transactions, and advanced querying capabilities. Keep in mind, how‐\never, that NoSQL is not always more performant than SQL; it mostly depends on the\nfunction for which it is used.9 Additionally, it is worth noting that later developments\nin database technologies made it possible for SQL databases to scale horizontally, thus\nreducing the gap between SQL and NoSQL.10\nData Storage Modeling | 291",5075
114-Native Versus Non-Native.pdf,114-Native Versus Non-Native,"11For example, Netflix created an orchestration engine called Conductor which uses Dynomite for primary per‐\nsistence storage. On top of that, Elasticsearch is used as a secondary indexing database to allow workflow\nsearch and discovery. For more on Conductor, read the official documentation . Primary Versus Secondary\nA primary DSS is designed to act as the secure and permanent repository for data\nstorage.\nThink of the primary DSS as the stronghold of your data. But in many cases, the pri‐\nmary DSS may not have all the features needed to interact with the data. This is where\na secondary DSS comes into the scene. A secondary DSS reads and stores a copy of\ndata from a primary database and allows users to perform more advanced data\nquerying and filtering operations.\nA secondary DSS may not maintain the same level of data consistency as the primary\nDSS, but it is generally intended for use cases where this discrepancy does not cause\nsignificant issues.\nA secondary DSS is typically utilized in situations where the primary DSS stores mas‐\nsive amounts of log or text data, which might be valuable if analyzed using sophistica‐\nted search queries. In this scenario, a search engine-like secondary DSS (e.g.,\nElasticsearch) may be implemented to regularly fetch and index log data from the pri‐\nmary DSS (more on Elasticsearch in “The Document Model” on page 314). Every\ntime you see an additional monitoring dashboard bundled with an application such\nas an orchestration engine or a managed database, it’s very likely that a secondary\ndatabase is used for this purpose.11\nOperational Versus Analytical\nA DSS can be designed to handle different types of business processes. In data engi‐\nneering, a distinction is often made between operational and analytical processes.\nOperational processes refer to the day-to-day operations and transactions within a\nbusiness. Examples may include the following:\n•Find customer account details.\n•Update the account balance.\n•Record and track financial transactions.\n•Execute a payment or fund transfer orders.\nAnalytical processes, on the other hand, are concerned with understanding what is\ngoing on within a business; for example:\n292 | Chapter 8: Data Storage Layer",2245
115-Data Storage Models.pdf,115-Data Storage Models,"12Example inspired by the excellent article “Native vs. Non-Native Graph Database” , by John Stegeman.•Assess the performance of the trading desk.\n•Monitor the effect of different investment strategies.\n•Understand the main cost and profit drivers.\nDSSs intended to handle operational processes are often called online transaction pro‐\ncessing  (OLTP), while those designed for analytical processes are known as online\nanalytical processing  (OLAP). Generally speaking, OLTPs prioritize reliability and\ntransactional guarantee, while OLAPs favor speed and advanced querying capabili‐\nties. A typical scenario involves the use of OLTP as a primary DSS for business trans‐\nactions and an OLAP as a secondary DSS that stores a copy of OLTP data for\nanalytical purposes.\nNative Versus Non-Native\nA DSS can serve a single function or a variety of functions. For instance, it might be\ndesigned for storing and querying tabular data exclusively, or it could handle docu‐\nment, graph, and key-value data as well. Crucially, various types of design optimiza‐\ntions may be required for supporting each function. This is where a relevant\ndistinction is often drawn between native  and non-native  DSSs.\nTo illustrate the difference, let’s say we want a DSS to store and query graph data. A\nDSS that is specifically designed and optimized for graph data (e.g., Neo4j) is called\nnative , whereas a DSS that is capable of handling graph data but was not primarily\ndesigned for graphs in mind (e.g., PostgreSQL) is called non-native.12\nChoose a native DSS when your application’s core functionality relies on an opti‐\nmized DSS tailored for specific tasks (for example, choosing a native graph DSS for a\nnetwork analysis application).\nMulti-Model Versus Polyglot Persistence\nHaving in mind the difference between native and non-native DSSs, another impor‐\ntant distinction is often made between multi-model and polyglot persistence DSS pat‐\nterns. Polyglot persistence uses different native DSSs for each function. For instance,\nyou can use PostgreSQL for storing user account information data, Neo4j for manag‐\ning social network data and graph-based queries, MongoDB for storing and querying\nuser-generated content and posts, and Redis for caching frequently accessed data for\nimproved performance.\nAlternatively, a multi-model DSS supports multiple DSMs within a single integrated\nenvironment. Such a system may offer support for different DSMs either natively or\nData Storage Modeling | 293",2512
116-The Data Lake Model.pdf,116-The Data Lake Model,"through extensions. For example, PostgreSQL is a native relational DSS, but it also\noffers extensions for geospatial data (PostGIS) and image data (PostPic). On the other\nhand, solutions such as Microsoft Azure Cosmos DB offer native support for docu‐\nment, graph, and key-value data models.\nThe polyglot persistence approach offers better performance, but it may add signifi‐\ncant overhead. In contrast, the multi-model is much simpler but may end up behav‐\ning like the proverbial “Jack of all trades, master of none, ” which means that while it\ncan accomplish a lot, it won’t provide outstanding performance at any one function.\nKeep in mind that data storage technologies are continuously\nevolving and adding new features and functionalities. If a given\ndata storage technology doesn’t support a specific feature today, it\ndoesn’t mean it won’t tomorrow. A missing feature is often an opti‐\nmization choice that can be changed in a future release.\nData Storage Models\nNow that you understand what a DSM is, let’s explore the various types of DSMs you\nmight encounter when building a DSS for financial data. Remember, these models are\nnot mutually exclusive. Financial institutions often use a mix of DSMs to construct\nDSSs for a range of operational and analytical purposes.\nThe Data Lake Model\nA data lake serves as a centralized repository capable of storing massive amounts of\ndata in raw, unprocessed formats. Data lakes are the most flexible type of DSS since\nthey allow for reliable and cheap storage of any type of data: big or small, structured\nor unstructured, static or stream. In a typical scenario, data lakes are used to store\nlarge amounts of miscellaneous data, including files, news text, documents, logs, data\nsnapshots, archives, and backups.\nWhy data lakes?\nData lakes provide several features that financial institutions often seek. These\ninclude the following:\nData variety and agility\nData lakes offer a flexible storage solution to ingest, store, and analyze any type of\ndata without the need to define a schema or enforce a structure. Data lakes are\nsaid to implement schema on read,  meaning that the data schema is generated on\nthe fly during data query . This is in contrast to schema on write,  where data is\nfirst modeled and structured before being stored.\n294 | Chapter 8: Data Storage Layer\nSimple data ingestion\nData ingestion into a data lake is simple and cost-effective because it does not\nnecessitate transformations or harmonization, thereby lowering data ingestion\nexpenses.\nData integration\nData lakes are useful for organizations that want to consolidate all their data in\none central place. This can be practical for purposes such as compliance, data\naggregation, risk management, and customer analysis, as well as experimenting\nwith new ideas and datasets. Moreover, it eliminates data silos within the\norganization.\nData archiving\nData lakes offer a cost-effective solution for data archiving and retention.\nData analysis and insights\nData lakes store data in their original raw format. This allows organizations to\nperform ad hoc data querying and transformation and access historical snapshots\nof data.\nData governance\nData lakes can be designed with data governance practices, including access con‐\ntrol, cataloging, auditing, reporting, and compliance.\nSeparation of storage and compute\nData lakes can scale to accommodate large volumes of data and access patterns. If\nyou implement a computation layer on top of a data lake, this means that you\nhave a separation of storage and compute layers. With such separation, you can\nscale and manage storage and computation independently.\nIt’s important to remember, however, that data lakes are not query-optimized in the\nsame manner as relational databases or data warehouses. Furthermore, performance\nmay be impacted by a data lake’s growing size or deeply nested hierarchical structure.\nTechnological implementations of data lakes\nAs a DSM, a data lake is an abstract idea that conceptualizes a centralized repository\nfor storing raw data, independent of any specific technology. If a data lake aligns with\nyour use case, the next step would involve selecting a technology to implement it. The\nclassical solution has traditionally been the Hadoop Distributed File System  (HDFS) .\nHDFS is a part of the Hadoop ecosystem, which is extensively used to build big data\napplications. It is an open source, distributed, scalable, and fault-tolerant file storage\nsolution for working with large amounts of data. It runs on commodity servers\n(mostly on premises), thus making it a cost-effective solution. It is written in Java and\nhas several configuration options that can match various business needs. In addition\nData Storage Models | 295\n13For more details on this data lake architecture pattern, I recommend reading Franck Ravat and Y an Zhao’s\n“Data Lakes: Trends and Perspectives” , in Database and Expert Systems Applications: 30th International Con‐\nference, DEXA 2019 , Proceedings , Part I (August 2019): 304–313.\nto HDFS, other prominent open source data lake solutions include MooseFS, Ceph,\nand GlusterFS.\nWith the emergence of the cloud, a major preference shift occurred toward building\ndata lakes using managed cloud storage solutions. Examples include AWS’s Simple\nStorage Service (S3), Azure Blob Storage, Google Cloud Storage, and DigitalOcean\nSpaces. Cloud-based data lakes are considerably easier to build and use; they scale\nseamlessly and require minimal configuration and maintenance. Y ou can even create\na basic cloud data lake in less than a minute if you already have an account!\nNevertheless, don’t get overexcited with the simplicity of cloud storage solutions.\nWhen designing an enterprise-wide data lake solution, quite a few challenges and fac‐\ntors need to be considered to avoid building a monster. Two things worth discussing\nin this regard are data modeling and data governance, which I will cover in more\ndetail in the next two sections.\nData modeling with data lakes\nWhen designing data lakes, data modeling is rarely a major discussion point. The rea‐\nson is that data lakes are not assumed to enforce a specific schema or structure on the\ningested data. Even though this might be a valid point, data lakes can still have their\nown data models and architecture. To illustrate how data modeling works for data\nlakes, let’s take a cloud-based data lake solution such as AWS S3 as our desired tech‐\nnological implementation.\nTo start with data lake data modeling, bucket architecture is the first thing to con‐\nsider. Buckets are the main containers of objects stored in S3. Think of buckets as the\nlogical data model for your data lake, which are distinguished based on your business\nrequirements and can be assigned user permissions independently of the other buck‐\nets. For example, one bucket may be dedicated to storing log data, another for finan‐\ncial vendor data, and a third for analytics data. Alternatively, it is common to\norganize buckets into zones , such as landing zones for raw files, staging zones for\nenriched files, business zones for analytics and research data, and trusted zones for\nanonymized and analysis-ready data.13\nThere is no single recipe for how and how many buckets to create,\nbut I can recommend keeping in mind architecture simplicity:\ndon’t create too many buckets such that you won’t know which data\nis where, and don’t create one huge bucket to store all kinds of data.\n296 | Chapter 8: Data Storage Layer\nThe second element of data lake modeling is a folder structure that organizes data\nitems based on their categories and relationships. The most common folder structure\nis the tree-like hierarchical structure where parent folders contain subfolders, which\nin turn may contain other subfolders and so on. A simplistic zone-based folder struc‐\nture might look like this:\nLanding zone\nStores files at the second level, e.g., /landing_bucket/year/month/day/hour/minute/\nsecond\nStaging zone\nCleans and aggregates the data at the daily level, e.g., /staging_bucket/year/\nmonth/day\nBusiness zone\nSplits daily data based on business areas, e.g., /business_bucket/business_area/\nyear/month/day\nIn the landing zone, raw files have schema on read rather than schema on write. This\nis because they are stored as they arrive without any modification. As data gets trans‐\nformed in the staging and business zones, it is a good practice to implement schema\non write, meaning that you enforce data schema into the files. If you want to apply\nanonymization to the data, you may want to create a trusted zone where you store\ndata that has been anonymized for safe use and analysis.\nA good practice when organizing your data lake folder structure is to avoid having\ndifferent file formats within the same folder. For example, assume that the ingestion\nlayer loads files in gzip, CSV , and TXT formats. To model this properly, consider a\nlanding zone structure like the following:\n•gzip files: /landing_bucket/gzip/year/month/day/hour/minute/second\n•CSV files: /landing_bucket/csv/year/month/day/hour/minute/second\n•TXT files: /landing_bucket/txt/year/month/day/hour/minute/second\nBy design, a data lake may accommodate any type of data from any source. All you\nhave to do is specify a bucket name and drop data in it. However, this simplicity can\npotentially lead to several issues. Think of the Downloads  folder on your PC; if you\nare like me, then it is simply a big dump of files of all types and formats; you don’t\nknow what’s in there, whether it is in the right place, and how to navigate and search\nfor files. Similarly, without the right controls in place, data lakes can easily turn into\ndata swamps : an out-of-control place for dumping any data from any source without\nany checks or validation rules. To avoid this issue, you need to implement a data gov‐\nernance layer on top of a data lake. Let’s discuss this topic in more detail.\nData Storage Models | 297\nData governance\nCreating and using a data lake might sound very easy: (1) log in to AWS, (2) select S3,\n(3) create a bucket, (4) drop files there! This works, and I did it many times. However,\narchitecting a reliable and scalable enterprise-wide data lake is a much more challeng‐\ning task than this. Large data lakes, in particular, are likely to pose significant issues to\nusers, especially when it comes to understanding things like the following:\n•Metadata (information about the data)\n•Sources and movements of the data\n•Transformations and changes applied to the data\n•The architecture of the data (buckets, folders, formats, etc.)\n•The level of data quality, consistency, and integrity\n•Data access and retrieval methods, which are less intuitive than just using SQL\nTo address these issues, it is recommended to implement a data governance layer on\ntop of the data lake. Common considerations to keep in mind include the following:\nPrivacy\nData coming from various sources may include sensitive user information, such\nas personally identifiable information (PII). Therefore, the data lake should be\ndesigned to anonymize and store this data in compliance with predefined privacy\nrules. For instance, you can separate buckets that store sensitive data from those\nthat store anonymized data.\nSecurity\nData lakes must be protected against malicious attacks, data loss, and unauthor‐\nized access. This is especially important given the extensive use of cloud-based\ndata lakes, which are vulnerable to the risks associated with cloud misconfigura‐\ntion, i.e., the improper or incorrect setup of cloud resources, services, or settings.\nPolicies to achieve this include encrypting data both at rest and in transit, imple‐\nmenting access controls to specify who can read, upload, and delete files, and\nestablishing data retention rules, such as security locks, to ensure that archived\ndata is never deleted.\nQuality controls\nData must be checked for quality issues as it gets ingested into the data lake.\nNote, however, that quality controls may not be needed for all types of data. For\ninstance, while log data generally does not require quality checks, business and\nfinancial data do benefit from them.\n298 | Chapter 8: Data Storage Layer\nData cataloging\nA data catalog is a valuable tool that provides users of a data lake with functional‐\nities to search for data, metadata, versions, ingestion history, and other attributes\nassociated with the data stored within the data lake.\nImportantly, there is no one-size-fits-all approach to implementing a data governance\nlayer on top of a data lake. I suggest beginning with the four elements previously\nmentioned (privacy, security, quality control, and data cataloging). Keep in mind that\nfor complex data lakes, data governance might be a substantial investment. Some\ncloud services, such as AWS Lake Formation , provide data lake solutions that include\nbuilt-in data governance capabilities.\nContinuous technological advancements are consistently enhancing the reliability of\ndata lakes. For example, the data lakehouse  model was created to introduce more\nstructure into data lakes. The main idea of a data lakehouse is to create an integrated\ndata architecture that combines a data lake, data warehouse (covered later in this\nchapter), and related analytics and processing services.\nFor example, using AWS, you can build an architecture where you upload and store\nraw CSV files in an S3 bucket (data lake) and query them with SQL queries using\nservices such as Amazon Athena or Amazon Redshift Spectrum. Another example is\nSnowflake, which allows users to interact with data stored in a data lake of choice\nusing the Snowflake compute layer. A recommended approach to achieve this is by\nstoring the data in Apache Iceberg tables  that use the Apache Parquet format. Apache\nIceberg is an open source table format for massive analytic datasets, supporting fea‐\ntures such as ACID transactions, schema evolution, partitioning, and table snapshots.\nSimilarly, technologies such as Apache Hudi  were introduced to bring database and\ndata warehouse capabilities such as ACID transactions and record-level data updates/\ndeletes to data lakes.\nFinancial use cases of data lakes\nData lakes have attracted significant attention within the financial industry . Many\nfinancial institutions are increasingly investing in building their own data lakes, often\nleveraging managed cloud storage solutions.\nSeveral reasons can explain the financial sector’s excitement for data lakes. An impor‐\ntant factor is cloud migration. As many financial institutions are moving to the cloud,\ndata lake solutions such as AWS S3 appear to be particularly appealing because they\nare simple to set up and can store any type or size of data.\nData Storage Models | 299\nSecond, financial institutions process a large amount of unstructured data, such as\nreports, prospectuses, client files, news data, logs, and more. Data lakes offer an effi‐\ncient and reliable solution for storing, sharing, and archiving such data.\nThe third factor is data aggregation, which is both a regulatory requirement and an\nenabler of innovation. By consolidating data in one location, financial institutions can\nquickly respond to regulatory inquiries that demand aggregated information, such as\ntotal risk exposure. Moreover, emerging trends in data-driven product development\nand analytics require data from diverse sources, often scattered across decentralized\nsilos within financial institutions. Data lakes enable these institutions to seamlessly\nconsolidate and aggregate data from various sources and silos into a centralized,\naccessible, and scalable repository.\nFourth is compliance. Financial institutions have a lot of compliance requirements\nthat relate to data privacy, security, retention, and protection. Data lakes may offer a\nnumber of desired compliance-related features. For example, AWS S3 offers Write-\nOnce-Read-Many  (WORM) storage features  such as S3 Glacier Vaults and S3 Object\nLock to ensure the integrity of archived data in accordance with US SEC and FINRA\nrules . A number of features are also available to ensure security. For example, S3 pro‐\nvides the Block Public Access  feature, which simply blocks all public access to S3 buck‐\nets. In this case, financial institutions deploy their S3 data lake within a VPC and\nenable access through VPC endpoints for Amazon .\nCase Study: NASDAQ Data Lakehouse with\nAWS S3 and Redshift Spectrum\nNASDAQ is a multinational financial services firm operating several financial market\nplatforms worldwide, particularly the NASDAQ Stock Exchange. NASDAQ manages\nthe matching of buy and sell operations on a massive scale and at a rapid speed while\nproviding data-feeding services for its prices and quotes. As a result, a large number\nof daily records are generated, including orders, quotes, trades, and cancellations. By\n2020, NASDAQ was processing around 70,000 records on a daily basis. These records\nneed to be loaded for reporting and billing purposes before the market opens the fol‐\nlowing day.\nTo support its large-scale data needs, NASDAQ invested in a data lake solution lever‐\naging AWS S3 . This strategic decision enabled NASDAQ to decouple the compute\nlayer from the storage layer, allowing independent scaling of each component. Using\nS3, NASDAQ gained the capability to manage numerous concurrent read and write\noperations on large datasets seamlessly, without encountering contention issues.\n300 | Chapter 8: Data Storage Layer",17688
117-The Relational Model.pdf,117-The Relational Model,"Using AWS Identity and Access Management (AWS IAM), NASDAQ established a\ncomprehensive access control policy for data stored on S3. Additionally, NASDAQ\ntook advantage of the lower costs of archival storage offered through Amazon S3 Gla‐\ncier. Moreover, NASDAQ leveraged the Amazon S3 Object Lock feature  to protect\nobjects from deletion or modification, ensuring compliance.\nFor the compute layer, NASDAQ chose Amazon Redshift Spectrum , a service that\nallows users to perform SQL queries on data stored in Amazon S3 buckets. The\nresulting architecture is a data lakehouse that combines a data lake (S3) with a data\nwarehouse (Redshift Spectrum), each scalable independently to support various levels\nof storage and computing.\nThe Relational Model\nThe popular and highly trusted DSSs are built on the relational DSM, which organi‐\nzes data in rows and columns, forming a table or relation. These tables can be related\nto each other through a common attribute (column), hence the term “relational. ”\nFigure 8-1  illustrates the concept.\nFigure 8-1. Components of the relational DSM\nA collection of tables, along with other objects such as views, indexes, stored proce‐\ndures, and functions, within a relational model is often referred to as a schema . Multi‐\nple schemas can coexist within the same database . For this reason, it is common to\nrefer to a relational DSS as a relational database management system  (RDMS) or rela‐\ntional database.\nInteraction with a relational database happens through a declarative  language known\nas Structured Query Language , or more commonly SQL. For this reason, relational\ndatabases are also commonly called SQL databases.\nThe relational DSM foundations were laid out by IBM’s expert Edgar Codd in the\n1970s. After extensive research on the relational model, Codd proposed a list of 12\nrules  that must be observed to develop a true relational database.\nData Storage Models | 301\n14To learn more about the history of SQL, I highly recommend Donald D. Chamberlin’s “Early History of\nSQL ” , IEEE Annals of the History of Computing  34, no. 4 (October–December 2012): 78–82.Why relational databases?\nRelational databases are widely trusted and heavily relied upon by financial market\nparticipants, often serving as the default choice for data storage. Let’s explore the rea‐\nsons behind this preference.\nSQL standards.    The origins  of the SQL  language  go back  to 1974  when  Donald\nChamberlin  and Raymond  Boyce  from  IBM  released  the essay  “SEQUEL:  A Struc‐\ntured English Query Language.” Chamberlin and Boyce relied on Codd’s framework\nto develop  an intuitive  and user-friendly  language  to interface  with  relational\ndatabases.14\nAs the language gained wide popularity and a strong reputation among market par‐\nticipants, the American National Standards Institute  (ANSI) intervened in 1986 to\ndevelop and promote a standard for SQL. The ANSI Database Technical Committee\n(ANSI X3H2) within the Accredited Standards Committee (ASC) X3 developed the\nfirst SQL standard, known as ANSI X3.135-1986. In 1987, the ISO started developing\nan international version of SQL standards, further solidifying SQL ’s presence on the\nglobal stage. Since then, the standard has been defined and revised under ISO/IEC\n9075 . What ISO/IEC 9075 offers is a set of features and requirements that need to be\nimplemented in order for a database to become SQL compliant. Figure 8-2  illustrates\nthe historical evolution of SQL standards.\nThanks to ISO/IEC 9075, the SQL language has evolved along a path of continuous\nimprovement, resulting in a rich array of features that continue to make SQL a\nfavored choice among engineers and financial markets to this day. One thing to keep\nin mind is that SQL standards are advisory, not mandatory technical specifications. It\nis quite possible to create an SQL database system that implements only a subset of\nSQL standards.\n302 | Chapter 8: Data Storage Layer\nFigure 8-2. Overview of SQL standards\nACID transactions.    By design, relational databases offer the strongest ACID transac‐\ntional guarantee among all DSSs.\nAtomicity is guaranteed by executing each statement or block of statements within a\nsingle transaction. A transaction either succeeds completely or is aborted without\nleaving partial or incomplete results. This is possible thanks to a transaction log\nmechanism known as Write-Ahead-Logging  (W AL). Before a transaction is executed,\nit is logged into the W AL log. The database system may utilize the W AL to roll back\nchanges and return the database to its initial state if a transaction fails or is interrup‐\nted. Otherwise, the transaction is marked as committed.\nData Storage Models | 303\nConsistency is primarily achieved via the use of constraints such as uniqueness, non-\nnull values, primary key, foreign keys, and checks. Foreign key constraints are a dis‐\ntinguished feature in SQL used to ensure referential integrity . This integrity ensures\nthat references are valid, i.e., if the presence of one value in table A requires the pres‐\nence of a specific referenced value in table B, then the value in table B must exist. For\nexample, within a customer order table, a foreign key constraint would prevent\nrecording an order for a product referenced from another table (such as the products\ntable) if that product does not exist in the referenced products table.\nConsistency is also guaranteed via concurrency control mechanisms that handle the\nconcurrent execution of transactions. Among the most popular concurrency control\nmechanisms is Multi-Version Concurrency Control  (MVCC), a snapshot-based mech‐\nanism that allows each transaction to see a snapshot of the data (data version) at the\ntime of the transaction initiation, regardless of what changes take place later. This\nway, transactions are protected against data inconsistencies that might emerge from\nthe actions of other concurrent transactions.\nIsolation is guaranteed via mechanisms such as locking, snapshot isolation (e.g.,\nMVCC), and isolation levels. Database locks are a mechanism that allows a given\ntransaction to place a flag on a database object (e.g., table or row) to prevent others\nfrom concurrently performing specific actions on the same object. Locking is termed\nimplicit  when performed automatically by the database and explicit  when intention‐\nally initiated by the user. Furthermore, locking is called pessimistic  if the lock is placed\nwhile changing a DB object and optimistic  if placed only when the changes are being\ncommitted to the DB.\nExplicit locking may be complex and may affect your application’s\nreliability. Key concerns in this area include lock contention, sus‐\npension, timeouts, and deadlocks. Deadlocks, where two transac‐\ntions are waiting on one another to release locks, are particularly\ncommon. Additionally, locks differ in terms of the set of lock\nmodes with which they conflict; therefore, make sure you under‐\nstand the side effects of using a specific lock.\nIsolation levels  are used to control the visibility of concurrent transactions. The SQL\nstandard defines four isolation levels: Read Uncommitted , Read Committed , Repeata‐\nble Read , and Serializable . These levels are defined in terms of four phenomena: dirty\nread , nonrepeatable read , phantom read , and serialization anomaly . As this topic is a\nbit long to illustrate in detail, I refer the reader to the excellent documentation page of\nPostgreSQL on transaction isolation .\n304 | Chapter 8: Data Storage Layer\nFinally, durability is ensured via mechanisms such as W AL. By first logging transac‐\ntion steps into the W AL log, there is a guarantee that once a transaction is committed,\nits results will not be lost even in the event of subsequent failures.\nAnalytical querying.    One of the strongest arguments in favor of using SQL databases is\ntheir advanced querying capabilities. This includes table joins, aggregations, window\nfunctions, common table expressions, subqueries, stored procedures, functions,\nviews, and many more. One major advantage of such querying features is the flexibil‐\nity they provide in terms of data modeling. This is because you don’t need to guess all\nyour queries in advance before designing and creating an SQL database. Other types\nof DSM, such as the document model, require queries to be defined in advance in\norder to obtain good performance.\nSchema enforcement.    Relational databases require and enforce a data schema, which\ndefines table structure, column names, data types, constraints, and more. This is an\nessential feature if data needs to adhere to a predefined structure. Schema enforce‐\nment improves data quality and integrity by ensuring consistency in how data is\nstored and accessed.\nKeep in mind, however, that in many real-world applications, schemas can change\nquite often due to changing business needs. This is an important factor to consider\nwhen assessing the suitability of the relational model to your business problem.\nWith later revisions of the SQL standard, new data types such as JSON  were added,\nwhich introduced some sort of schema flexibility into SQL databases. By creating a\ncolumn of type JSON, an SQL database allows users to store data with variable struc‐\ntures, just like a JSON file.\nData modeling with relational databases\nData modeling is predominantly used in SQL databases. This is because of their stan‐\ndardized design principles and features, the flexibility they offer in terms of table\norganization, and the business and technical intuitiveness of the relational DSM.\nAs we discussed earlier in this chapter, data modeling is a technique used to define\nand assess the data requirements necessary to support various business operations\nwithin the corresponding data infrastructure of financial institutions. That said, data\nmodeling needs to always start with a business discussion (conceptual phase) that you\nlater translate into an actual database design (logical and physical phases). We can\ndiscuss a lot about how to define and fulfill each of these three phases. However, to\nkeep things simple and within scope, I will limit my treatment of SQL data modeling\nto the following techniques: normalization, constraints, and indexing. For each tech‐\nnique, I will briefly illustrate a few business use cases.\nData Storage Models | 305\n15For a good read on this topic, see “ Anomalies in Relational Model”  by GeeksForGeeks.\n16For an excellent illustration of these normal forms with examples, I suggest checking the article “Normaliza‐\ntion in DBMS”  by Study Tonight.Normalization.    Normalization is the most popular SQL data modeling technique. It\ndefines a table structure following several so-called normal forms . Normalization\nensures that the following conditions are met:\n•Data redundancy and repetition are minimized\n•Data is organized in small tables instead of one large table\n•Tables have well-defined relationships and references\n•Updating or adding data will be done in as few places as possible\n•Ability to add and remove data without altering or refactoring the tables\n•Tables are neutral to queries (i.e., your table design is not constrained by the\nqueries you want to perform)\n•Tables offer clear and informative data for business users\n•Y ou avoid the problem of insertion, update, and delete anomalies15\nThere are several types of normal forms; however, most databases are normalized\nusing the following three forms:16\nFirst normal form (1NF)\nThis form ensures that all columns are single valued. A field may not contain\ncomposite values or nested records. If such nested values exist, they should be\nexpanded to multiple rows. An example is illustrated in Figure 8-3 . The table on\nthe left is not in 1NF as it contains a multivalued column (transaction_id), while\nthe table on the right is in 1NF as it contains no multivalued records.\nFigure 8-3. First normal form conversion\nSecond normal form (2NF)\nThis form eliminates partial dependency . If a table has a composite primary key\nwith two or more columns, then all the columns (not included in the primary\nkey) must depend on the entire composite primary key and not on a part of it. A\n306 | Chapter 8: Data Storage Layer\ntable exhibits partial dependency when a nonprimary key column depends on\nonly a portion of the primary key. Figure 8-4  shows an example. The right table\nhas a composite primary key including account_id and account_type_id. In this\ncase, the column account_type_description depends only on account_type_id,\nhence generating a partial dependency. To normalize in 2NF, the\naccount_type_id should be stored in a separate table, as illustrated on the right\nside of the figure.\nFigure 8-4. Second normal form conversion\nThird  normal form (3NF)\nThis form eliminates transitive dependency . All columns should only depend on\nthe primary key, not on any nonprimary key columns. If a nonprimary key col‐\numn depends on another nonprimary key column, the table is said to have a\ntransitive dependency. See Figure 8-5  for an example. The table on the left side of\nthe figure has a single-column primary key defined for transaction_id. Both\naccount_id and amount depend on transaction_id, but max_transaction_limit\ndepends on account_id, which is not a primary key. This generates transitive\ndependency. To normalize in 3NF, we need to create a separate table that stores\nthe max_transaction_limit, as illustrated on the right side of the figure.\nFigure 8-5. Third  normal form conversion\nData Storage Models | 307\nRelational database systems often prioritize normalization as their main modeling\nstrategy. By eliminating redundancy, normalization ensures data integrity by mini‐\nmizing the number of places that store the same piece of data and establishing reliable\nrelationships between the tables. Think of this from a compliance perspective. For\nexample, if a client of a financial institution requests that their personal data be upda‐\nted or erased, the institution will only need to check one or a few tables (e.g., a client\ntable) to find all of the individual’s information. If data was not normalized, the same\npersonal information may be duplicated in many places, making it difficult to find,\naggregate, update, or delete.\nConstraints.    Constraints are a crucial feature of relational databases. They enforce\ndata integrity and consistency during insert, update, and delete operations. The main\nadvantage of defining constraints at the database level is that it decouples the con‐\nstraint checks and management from the application that interacts with and manipu‐\nlates the database.\nExamples of SQL constraints include the following:\n•The not-null constraint ensures that values in a given column are never null.\n•The uniqueness constraint ensures that values in a given column are unique.\n•The check constraint ensures that values in a given column satisfy a given value\ncondition (e.g., >=0).\n•The primary key constraint ensures that values in one or more columns can be\nused as the primary unique identifier in the table.\n•The foreign key constraint ensures that values in one or more columns in one\ntable match the values in a given column in another table.\nFor a comprehensive overview of these constraints and how to create them, I recom‐\nmend the official PostgreSQL documentation on constraints . Keep in mind that con‐\nstraints may have an impact on performance as they add additional work for the DB\nto do. The best way to approach DB constraints is by mapping them to business con‐\nstraints during data modeling. For example, a check constraint may be added to\nensure that the account balance never goes below zero, while a foreign key constraint\nmay be added to guarantee that a product is not sold if it’s not in the inventory, and so\non.\nIndexing of relational databases.    If you are going to work with relational databases,\nindexing is a must-have strategy. SQL databases physically organize data on disk in\ndata  files,  with each row called a record  and each column within a row called a field.\nTo search for a specific record, an SQL database can perform a full scan of all data\nfiles. However, this is an expensive operation, especially when the data size gets very\nlarge. This is where indexing is needed.\n308 | Chapter 8: Data Storage Layer\n17For a good overview of SQL database indexing, see the official PostgreSQL documentation on indexes .An index is a specialized data structure stored separately from data files in index files.\nIt is optimized to enable rapid search and retrieval of specific data records. Indexing\nis typically applied to the column(s) that you will use in your filtering statement. For\nexample, suppose you are tasked with the creation of a bank transactions table that\nstores the transaction_id, user_id, transaction type, transaction_time, and transac‐\ntion_amount. Y our business team tells you that they want to query this transaction\ntable by filtering on the user_id. Without an index, a query will scan the entire trans‐\naction table, which may be extremely slow and costly. To overcome this issue, you can\nadd an index on the user_id column. If the business team tells you that they want\ninstead to query user_id for a given time interval, then consider adding a composite\nindex on both the user_id and transaction_time columns.\nRelational databases offer a variety of indexes, and you can add as many indexes as\nyou want.17 However, it is important to keep in mind that indexes get regularly upda‐\nted when you add, modify, or delete records. If such operations are quite frequent,\nthen index updates might impact performance. Additionally, be careful how you\ndefine composite indexes. For example, if you create a composite index on columns\nA, B, and C, then a query that filters by B only, or C only, or B and C, is likely to not\nbenefit from the index. Furthermore, if you create an index on a column of one type\n(e.g., integer) and then cast the column to another type during a query (e.g., string),\nthen the index is likely to be useless. A recommended practice is to monitor index\nusage to see if it’s being utilized by the DSS.\nTechnological implementations of relational databases\nA large number of database technologies have been developed following the relational\nDSM. These include commercial solutions such as Oracle Database, Microsoft SQL\nServer, IBM Db2, IBM Informix, and MySQL Enterprise Edition as well as open\nsource alternatives such as PostgreSQL, MySQL, and MariaDB.\nA full account of the differences between all these technologies is beyond the scope of\nthis book. A general criterion that I recommend following is the degree of compli‐\nance with the SQL standard. Some database technologies, such as PostgreSQL, are\nwell-known for their high compliance with the SQL standard. Others, such as\nMySQL, may exclude some aspects of the standard  in return for reliability or speed,\nas well as include features and extensions that are not necessarily part of the standard.\nData Storage Models | 309\n18For a good comparison between relational database management system features, consult Wikipedia .The database’s technological specifications are another distinguishing criterion. One\nSQL database system may have a functionality that others may not have. These speci‐\nfications include the following:18\n•The operating system that the database system can run on (e.g., Linux, Windows,\netc.)\n•ACID properties\n•Concurrency control and locking mechanisms\n•Data size limits (e.g., max table size, max row size, etc.)\n•Supported data types (e.g., varchar, integer, numeric, JSON, etc.)\n•Supported constraints (e.g., foreign keys, etc.)\n•Data connectors\n•Security, roles, access control, and encryption\n•Data replication and partitioning\n•Backup and recovery\n•Support for materialized views\n•Supported index types (other than basic B-tree indexes)\nAn essential factor that might impact the choice of your relational database technol‐\nogy is scalability. Traditionally, relational databases have supported vertical scaling,\nwhere a small machine is replaced with a larger one with more RAM, CPU, and stor‐\nage. This strategy has several advantages, such as simplicity and data consistency and\nintegrity. However, it also has some drawbacks. First, it creates a single point of failure\nas it involves a single machine; second, it can easily hit capacity and load balancing\nlimits if the workload grows quickly or unexpectedly; third, there are physical limita‐\ntions in terms of how much CPU and RAM a single machine can have. This is where\nhorizontal scaling comes into play.\nIn horizontal scaling, the database system relies on a system of connected machines/\nnodes to distribute the load. If the workload grows, additional nodes can be added to\nhandle the increased demand. A typical approach for achieving SQL horizontal scal‐\ning is through read replicas , which are additional copies of the primary database that\nare regularly synchronized with it, either synchronously or asynchronously. These\nreplicas are often dedicated to handling read operations (such as SELECT queries),\nwhile the primary database continues to handle write operations (such as INSERT,\nUPDATE, DELETE). It is also possible for some systems to have read/write replicas.\n310 | Chapter 8: Data Storage Layer\n19A detailed account of this problem is offered in the blog post “Why Sharding Is Bad for Business” , by Michelle\nGienow.Another horizontal scaling strategy is sharding , which involves partitioning the data‐\nbase into chunks (shards) based on a shard key (e.g., date range) and distributing\nthem across multiple machines. This way, the traffic load gets distributed among the\nshards. Importantly, sharding-based horizontal scalability can be difficult to manage.\n19 For example, a significant amount of maintenance effort may be needed to ensure\nthe database’s integrity, involving operations such as data resharding, rebalancing,\nand partitioning. Furthermore, it may require incorporating complicated logic into\nyour application’s read/write methods to route requests to the relevant shard.\nTo overcome this issue, a new generation of horizontally scalable SQL databases\nemerged: distributed SQL databases . These databases offer native built-in scalability\nwithout the need to manually manage shards. Examples include Google Spanner,\nYugabyteDB, and CockroachDB. If your scalability needs are remarkably large, then\nsuch databases may be the ideal solution. For example, Deutsche Bank leveraged\nGoogle Spanner to achieve one of the biggest IT migrations  in the history of the\nEuropean banking industry, involving 19 million Postbank (Deutsche Bank’s retail\nbranch) product contracts alongside the data of 12 million customers.\nFinancial use cases of relational databases\nRelational databases are extensively used within the financial sector. This can be\nexplained by a number of factors. First, a significant part of financial data is tabular\ntime series and panel data, which is perfectly suited for the relational DSM.\nSecond, financial professionals perform a lot of analysis related to business intelli‐\ngence, forecasting, pricing, risk management, and modeling. Relational databases are\nan ideal solution for these tasks due to their powerful and intuitive analytical query‐\ning capabilities.\nThird, relational databases offer a lot of the reliability features that most financial\napplications require, such as data consistency, integrity, and a transactional guarantee.\nFor example, financial payment applications require transactions to be idempotent,\nmeaning if they are executed multiple times, the outcome should be consistent. If you\npurchase an item for $100 and the application processes your payment twice, idem‐\npotency ensures your account is debited only once for $100. SQL databases can\nachieve idempotency through mechanisms like uniqueness constraints and idempo‐\ntency tokens. For instance, if each payment is assigned a unique idempotency token\nand the column storing these tokens has a uniqueness constraint, any attempt to\ninsert the same payment twice will be rejected by the database due to the violation of\nthe uniqueness constraint.\nData Storage Models | 311\nCase Study: Payment Processing Applications with CockroachDB\nPayments are one of the most essential operations in financial markets. A large num‐\nber of players are involved in payment processing, including SaaS applications, online\nshops, ecommerce websites, retailers, payment gateways, card networks, payment\nprocessors, and payment service providers. Crucially, designing a payment system can\nbe quite challenging, as it needs to ensure the following:\n•Scalability (imagine how easily the volume of payments can increase)\n•Data durability (people won’t like it if they can’t find their historical payment\nrecords)\n•High availability and zero downtime (it’s not a pleasant experience if you can’t\npay)\n•Data consistency and correctness (people won’t tolerate mistakes with their\nmoney)\nHow do you build a payment system that meets all the above requirements? First, let’s\ntry to understand how payments are processed. According to Stripe , a typical card\npayment transaction involves the following steps:\n1.The customer initiates a payment by providing their payment details (e.g., credit\ncard) at the business payment channel (e.g., online shop).\n2.A payment gateway receives the transmitted information, encrypts it, and passes\nit to the payment processor.\n3.The payment processor forwards the information to the acquiring bank (the\nonline shop’s bank), which in turn forwards the information to the issuing bank\n(the customer bank) through the relevant card network (e.g., Visa or Master‐\ncard).\n4.The issuing bank verifies the request and responds with an approval or rejection\nmessage sent back to the payment processor via the same path (card network and\nacquiring bank).\n5.The payment processor communicates the transaction outcome to the business,\nwhich proceeds to conclude the purchase or inform the customer of any issue.\n6.If the transaction is concluded, a clearing and settlement process takes place in\nwhich the transaction amount is transferred from the issuing bank into the\nacquiring bank, which in turn deposits the funds into the business’s account.\n312 | Chapter 8: Data Storage Layer\nImportantly, when data passes through this chain of information exchange, some of\nthe involved entities store a portion of the payment data for reasons such as transac‐\ntion reviewing, reporting, reconciliation, and fraud detection. Due to the mission-\ncritical nature of payments, a highly available, scalable, and reliable database system is\nrequired. This is where distributed SQL databases can excel.\nA prominent solution in this market is CockroachDB, a cloud native, distributed\ndatabase based on standard SQL and developed by Cockroach Labs . CockroachDB\noffers a number of valuable features for mission-critical applications:\n•Simple horizontal scalability where users can add additional nodes as needed.\nInterestingly, even though it’s a distributed system, CockroachDB works as a sin‐\ngle logical database, enabling data access from any node, regardless of its\nlocation.\n•Support for transaction-heavy workloads with distributed atomic transactions .\n•High availability with no downtime, achieved via a consensus algorithm .\n•Multiactive availability (each node can serve both read and write requests).\n•Support for multiregion and multi-cloud (ideal for compliance that restricts data\nresidency to a specific region).\nTo give an example of the application of CockroachDB in the financial sector, let’s\ntake the case of Shipt , an American delivery service owned by Target Corporation. As\nan online ecommerce company, Shipt payment services are core to its business model.\nThe Shipt team was tasked with the creation of a distributed database system that\nmeets the requirements of a reliable payment system, in line with what we discussed\nearlier in this section. In particular, Shipt wanted a multiregion payment service that\ncould ensure correctness  throughout the entire payment cycle.\nBy leveraging CockroachDB, the Shipt engineering team managed to build a reliable,\ncorrect, cloud native, multiregion, and highly available payment data management\nsystem. Regional replication allowed Shipt to achieve lower transaction latency. Fur‐\nthermore, to ensure idempotency throughout the payment lifecycle, Shipt relied on\nidempotency tokens . Overall, by building its system on top of CockroachDB, Shipt\ncan now support its business growth across different regions and markets with a resil‐\nient and scalable payment architecture.\nData Storage Models | 313",28992
118-The Document Model.pdf,118-The Document Model,"The Document Model\nA highly reputable DSM is the document model, which stores information in a docu‐\nment format such as JSON or XML. This section will specifically focus on JSON-\nbased DSMs, which are the predominant choice in the industry. A document looks\nlike the following:\n{\n     ""document_id"" : 1,\n     ""legal_name"" : ""Microsoft Corporation"" ,\n     ""type"": ""public company"" ,\n     ""isin"": ""US5949181045"" ,\n     ""symbol"" : ""MSFT"",\n     ""sector"" : ""Information technology"" ,\n     ""products"" : [\n            ""Software development"" ,\n            ""Computer hardware"" ,\n            ""Social networking"" ,\n            ""Cloud computing"" ,\n            ""Video games""\n     ]\n}\nA DSS designed to store and query documents is called a document-oriented data‐\nbase, or simply a document database.\nWhy document databases?\nDocument databases are extensively used for all kinds of purposes, powering some of\nthe world’s largest applications. Among their most desired features are the following:\nSchema flexibility\nA document database can store any document, regardless of its content structure.\nIn other words, document databases do not enforce schemas natively, unlike SQL\ndatabases. This allows businesses to quickly develop an application and easily\nchange its logic. Y ou can add, change, or rename the document fields based on\nyour business requirements. Make sure, however, to be aware of the potential\nside effects of such flexibility; if not managed properly, it might impact data\nintegrity and consistency. At some point, you might even want to enforce a\nschema in the documents. This is often achieved via a schema validation mecha‐\nnism implemented on the application side.\nDocument modeling\nDocument formats such as JSON are quite familiar and might be easier to work\nwith. Moreover, unlike relational or other types of data storage formats, docu‐\nments map directly to objects such as hash tables or classes in most popular pro‐\ngramming languages, without the need to add an Object Relational Mapping\n(ORM) layer to your application.\n314 | Chapter 8: Data Storage Layer\nHorizontal scalability\nDocument databases are distributed by design, which means that they can scale\nhorizontally. This makes them an ideal choice for modern data-intensive applica‐\ntions. Moreover, being distributed, document databases provide resiliency and\navailability through multinode data replication.\nACID (atomic) transactions\nDocument databases support ACID transactions, at least for single-document\ntransactions, and in many cases for multidocument transactions as well. Cru‐\ncially, it is often a less strict version of ACID that focuses on achieving data con‐\nsistency via atomicity and an isolation guarantee (e.g., using snapshot isolation).\nPerformance\nDocument databases are quite performant when it comes to high-volume data\nreads/writes. This can be partly explained by the distributed architecture of\ndocument databases, which splits the load among the many nodes that make up\nthe system. Furthermore, unlike traditional SQL databases, document databases\ndo not necessarily enforce schema checks, standards, constraints, and ACID\nproperties, which in turn increases query performance. Keep in mind that this is\nnot a shortcoming, but rather a design choice.\nData modeling with document databases\nIn document databases, data modeling must be seen from a somewhat different view‐\npoint than that of the SQL DSM. Technically speaking, document databases are not\ndesigned for performing complex queries and table joins as is customary with SQL\ndatabases. This means that the range of queries that you can perform in a document\ndatabase will be constrained by your data model. For this reason, the first step in\ndocument data modeling needs to be a business discussion to think and define in\nadvance the queries and filters that users will want to perform. This is often called\nquery-driven data modeling . User-defined queries should serve as the foundation for\nthe subsequent data modeling stages. In this section, I will discuss three document\ndata modeling techniques: document and collection structure, denormalization, and\nindexing.\nDocument and collection structure.    The central concept of a document database is the\ndocument . Y ou can think of documents as the equivalent of rows in the relational\nmodel. A document stores data objects as a set of key-value pairs. It can store many\ntypes of data, including strings, integers, dates, Booleans, arrays, and even subdocu‐\nments. Documents are then organized in collections , which are the equivalent of\ntables in the relational model.\nUnlike SQL databases, there are no standardized rules for modeling a document data‐\nbase. In this case, the modeling process is mostly driven by business needs. As far as\nData Storage Models | 315\ncollections are concerned, the main thing to consider is that document databases do\nnot allow for performing joins; therefore, collections should be modeled in a way that\nminimizes inter-collection relationships and at the same time maximizes intra-\ncollection data cohesion. For example, a separate collection may be created for each\nbusiness entity, such as users, products, transactions, subscriptions, reference data,\nprices, and quotes.\nManaging schema changes in a NoSQL database can be challeng‐\ning, especially in environments where the schema is not strictly\nenforced and evolves over time. A number of good practices can be\nadopted to mitigate this issue. One such practice is schema version‐\ning, which can be implemented by defining and storing your docu‐\nment schema in a separate location, and adding a schema_version\nfield to your documents. Each version of the schema should have a\nunique identifier. Furthermore, you can implement schema valida‐\ntion on the application side to check if your document matches the\nreferenced schema. Make sure you document and communicate\nschema changes clearly within the team.\nDenormalization.    To ensure intra-collection cohesion, all related data needs to be\nstored in the same document. This strategy is often called denormalization , as it\ninvolves making redundant copies of the data in multiple collections to increase per‐\nformance and avoid data joins. As an alternative, it is possible to use references  that\nlink documents across collections. Y ou can think of references as a soft version of for‐\neign keys in SQL databases. For example, a user ID field in the transactions  collection\ncan be used to find a related document in the contacts  collection.\nIndexing of document databases.    Document databases are designed for large amounts\nof data. As such, indexes are often used to improve query performance. An index\nstores a small portion of the collection data in an easy-to-traverse data structure.\nWithout appropriate indexing, a document database will have to scan the entire col‐\nlection when searching for a specific document.\nThe two primary types of indexes often supported by document databases are single-\nfield indexes and composite  indexes. Deciding which and how many indexes to create\ndepends on your queries and it is crucial for improving the performance of your\napplication. If a large number of your queries filter by a single field only, then it’s bet‐\nter to create a single-field index for each of these fields. If you also have queries that\nrepeatedly filter by multiple fields, then you can create a composite index on those\nfields. Other types of advanced document indexes exist. For example, multikey\n316 | Chapter 8: Data Storage Layer\n20For a general overview, consult the official MongoDB page on indexing .indexes are used to index array fields, while text indexes are used to support text\nsearch queries on string content.20\nImportantly, there are a few factors to consider while indexing a document database.\nFirst, although indexes improve read performance, they will negatively impact write\noperations as they must also update the index. This is especially the case for applica‐\ntions with a high write-to-read ratio. Second, several document databases impose a\nlimit on the number of indexes that a collection can have. If you run out of indexes,\nthen you might not be able to meet your business or performance requirements any‐\nmore. Third, indexing might become more challenging if you have a highly nested or\ncomplex document structure. For this reason, consider making your documents as\nflat as possible to achieve optimal indexing results.\nTechnological implementations of document databases\nThere are a large number of data storage technologies that implement and support\nthe document model. The most prominent examples include open source options\nsuch as MongoDB and Apache CouchDB, as well as managed commercial solutions\nsuch as Amazon DynamoDB, Azure Cosmos DB, and Google Cloud Firestore.\nCloud-based solutions such as DynamoDB, Google Firestore, and Azure Cosmos DB\nare commonly preferred as they offer tight integration with other cloud services as\nwell as reduced infrastructure and security overheads.\nMoreover, a few specialized document-oriented data storage options are available.\nThe most prominent example is Elasticsearch , which can be described as a distributed\nsearch and analytics engine. It is built on the Apache Lucene search software, which\nimplements a well-known technique called inverted indexing. Elasticsearch is often\nused as a secondary DSS, where data from one or more primary DSSs is regularly\npulled and indexed to allow easy and fast searching.\nFinancial use cases of document databases\nDocument databases have received considerable attention within the financial indus‐\ntry. This is particularly the case for financial applications that require scalability,\nlatency, availability, schema flexibility, and integration capabilities, which are charac‐\nteristic of document databases.\nLet’s take MongoDB, for example. Financial institutions have leveraged MongoDB for\nvarious business applications. A common example is the development of payment\nsystems . MongoDB’s flexible data schema enables payment applications to accept and\nenrich any payment data structure and type. In addition, MongoDB may offer the\nnecessary scalability and availability features that are essential  to payment systems.\nData Storage Models | 317\nMoreover, MongoDB’s API-based data management is an ideal fit for the payment\nbusiness, which relies significantly on APIs.\nCase Study: Wells Fargo’s Next-Generation\nCard Payments System with MongoDB\nWells Fargo is a well-known financial services corporation with a remarkable global\nreach. To modernize its credit card payment system, Wells Fargo launched the Cards\n2.0 initiative. Based on an industry case study , the main goals of the initiative were\nthe following:\n•Ensuring a seamless multichannel experience for credit card customers (branch,\nmobile, digital, etc.)\n•Reusable and scalable data APIs to enable the quick rollout of changes across\nmultiple channels\n•Reduced dependency on third-party card processors\nTo achieve success with Cards 2.0, Wells Fargo had to reduce its reliance on main‐\nframe infrastructures. The reason, according to the case study, is that “while main‐\nframes hold critical System of Record (SOR) data, they often bring technical debt,\ndependencies, and are increasingly costly to manage—not least because there’s a\nshortage of people with the right skills to maintain them. ”\nWhile the bank is responsible for issuing the cards, the gateway that tracks all the\ntransactions is the Fiserv mainframe system. Relying on data ingestion mechanisms\nbased on a mainframe was not ideal for modern multichannel applications. To solve\nthis problem, the engineering team at Wells Fargo designed a modern data infrastruc‐\nture powered by MongoDB. The new solution included batch (Apache Spark) and\nreal-time (Apache Kafka) systems that listen and pull the data from the mainframe\nand upload it to MongoDB. The resulting MongoDB-based DSS was an Operational\nData Store (ODS) that could serve data to many business channels.\nThe Wells Fargo team didn’t expose MongoDB collections directly to data consumers.\nInstead, data APIs were designed to serve various types of data (e.g., accounts, trans‐\nactions, etc.). Such data APIs are then imported and used in the various microser‐\nvices. Using MongoDB, Wells Fargo was able to handle over 20+ terabytes of daily\ndata and move from a monolith architecture to a modular architecture of more than\n80 microservices.\nAnother noteworthy example of a reliable and scalable document database technol‐\nogy for financial services is AWS DynamoDB. DynamoDB is a key-value and\ndocument-oriented database designed for high speed, throughput, availability, scala‐\nbility, and millisecond latency. To illustrate its capabilities, on 2022 Amazon Prime\n318 | Chapter 8: Data Storage Layer",12976
119-The Time Series Model.pdf,119-The Time Series Model,"Day, Amazon DynamoDB reliably handled 105.2 million requests per second  across\ntrillions of API calls with single-digit millisecond performance. In addition, Dyna‐\nmoDB can be natively integrated with AWS Lambda to execute a data processing\nlogic each time a DynamoDB table is updated. For example, the Amazon finance\ntechnologies (FinTech) payment transmission team, which supports and manages\nproducts for the accounts payable (AP) team, implemented a similar architecture  to\nensure the scalability and timely processing of remittances at Amazon.\nThe Time Series Model\nIn Chapter 2 , we talked about time series data and illustrated its main characteristics.\nIn essence, a time series is a sequence of measurements or observations of one or\nmore variables tracked at increments in time. Examples include temperature,\nresource consumption by a computer, events, clicks, and stock prices.\nFinancial time series data such as stock prices are now generated in large volumes\nand velocities and are widely employed to analyze temporal market dynamics. This\nincludes the analysis of historical trends, averages, variances, anomalies, correlations,\nstationarity, and cycles.\nMoreover, as financial time series data has increased in volume, variety, and velocity,\nits range of application has significantly expanded. Time series analysis is now inte‐\ngral to financial analysis and forecasting and can be used in predicting stock prices,\nmarket risks, interest rates, foreign exchange fluctuations, and many more.\nAs a result, demand has emerged for a new type of a DSM to enable efficient storing,\nmanipulating, and querying of time series data. I shall refer to this model as the time\nseries DSM . A notable increase in the popularity of time series databases has been\nobserved in recent years . To understand why, the next section will highlight the key\ncharacteristics that distinguish time series databases.\nWhy time series databases?\nA time series database is a specialized type of DSS designed specifically to implement\nthe time series DSM. Time series databases offer several advantages, such as the\nfollowing:\n•A specialized engine designed for storing and processing time series data, provid‐\ning substantial performance enhancements tailored for applications that handle\nlarge volumes of time series data, such as trading systems.\n•Built-in functionalities to perform efficient time-based aggregations, such as tem‐\nporal grouping of data (yearly, monthly, daily,..) and temporal transformations\n(e.g., simple moving average, exponential moving average, cumulative sum,\npercentile).\nData Storage Models | 319\n•Fast queries enabled through optimized time series indiexs and in-memory\ncaching.\n•Simple data model based on the time association of entities.\n•Data immutability: time series databases are often designed for immutability and\nappend-only behavior—i.e., once recorded, data is rarely updated.\n•Efficient data lifecycle management by keeping recent data in memory, and delet‐\ning or compressing old data for efficient storage.\nKeep in mind that the time series DSM is a specialized model; therefore, it should be\nconsidered only when your business and application requirements are centered\naround the efficient storage and processing of time series data. If the time series\ndimension of your application is just one of many aspects, then you might want to go\nfor a more general-purpose DSM such as the SQL or document model.\nData modeling with time series\nData modeling in the time series DSM is as simple as the structure of time series data:\na timestamp and associated data. The equivalent of a table in a time series database is\noften called a measurement . A measurement is a container of at least three main ele‐\nments: time, fields, and metadata. Table 8-1  illustrates an example.\nTable 8-1. A measurement in a time series database\nTime Price Ticker Exchange\n2019-02-18T16:12:01Z 15.45 ABC NYSE\n2019-02-18T16:13:01Z 15.41 ABC NYSE\n2019-02-18T16:20:01Z 15.44 ABC NYSE\n2019-03-06T17:33:20Z 345.45 ZYK NYSE\n2019-03-06T17:33:25Z 345.47 ZYK NYSE\nIn Table 8-1 , the first column is time, which in a time series database must always be\npresent. The next column, price, is a field. Field values represent the actual data in a\nmeasurement, and each measurement should have at least one field. In our example,\nwe are observing the price of stocks over time. The final two columns, ticker and\nexchange, are referred to as metadata or tags. Tags are optional, although they are\nquite useful. Most time series databases construct indexes based on tags rather than\ntime or field columns. A single row in a measurement is called a point . A set of points\nthat share the same set of tag values is called a series . For example, the first three rows\nin the measurement shown in Table 8-1  form a series, since they hold data related to\nthe price of the ticker ABC on the NYSE exchange.\n320 | Chapter 8: Data Storage Layer\nTechnological implementations of time series databases\nA variety of time series database implementations have been developed in the market.\nThese can be divided into native and non-native implementations.\nA non-native implementation is not primarily designed for time series data, but it can\nbe used for this purpose. For example, it is possible to use an SQL database such as\nPostgreSQL to create a time series table like the one shown in Table 8-1 . To boost per‐\nformance, an index like a B-tree or BRIN (Block Range Index) can be created on one\nor more tag columns plus the timestamp column. Furthermore, PostgreSQL supports\nindex-based table clustering, which can enhance the efficiency of queries that filter by\na given temporal interval. Similarly, a document database such as MongoDB can be\nused to store a time series as a set of documents in a collection indexed by time and\ntag keys.\nCrucially, while SQL and document databases may be able to handle time series data,\nthey are not optimized for this purpose. Two options are available to overcome this\nissue. First is time series extensions created on top of general-purpose databases. A\nprominent example is TimescaleDB, which extends PostgreSQL with time series char‐\nacteristics. TimescaleDB is quite popular due to its performance, scalability, and com‐\npatibility with PostgreSQL and the SQL query language. As another example,\nMongoDB introduced time series collections , which allow for optimized storage and\nretrieval of time series data. To do this, MongoDB time series collections use a colum‐\nnar storage format and store data in time order. Moreover, upon the creation of a\ntime series collection, MongoDB automatically creates a composite index on the time\nand metadata fields.\nThe second and most reliable option for handling time series data is native purpose-\nbuilt time series databases. A good example is kdb+, developed and maintained by\nKX. Kdb+ is designed with high performance and scalability features in mind. It\nstores the data in a columnar format, allowing for efficient data compression and fast\nqueries. In addition, it uses an in-memory compute engine for fast processing and\nquerying in real time. Furthermore, kdb+ supports the q language, which is known\nfor its powerful and efficient querying capabilities.\nIn-Memory Databases\nSome database systems may frequently be referred to as in-memory databases or as\nhaving in-memory characteristics. What this means is that the database system keeps\nall or part of its data in memory (i.e., RAM). By storing data in memory, the database\nachieves very low response times (i.e., low latency) by eliminating the need to fetch\nthe data from disk. It is well-known empirically that RAM-based access is substan‐\ntially faster than disk access, precisely random disk access. In-memory data access\ncapabilities can be crucial for time-critical applications such as trading and online\nstores. A common data management strategy with in-memory databases involves\nData Storage Models | 321\n21To learn more, read InfluxDB’s documentation on the InfluxDB storage engine .storing the most current data in memory while keeping historical data in a separate\nlocation.\nThere are different types of in-memory databases. On the one hand, there are native\nin-memory databases such as Redis and kdb+. Among financial institutions, the Ora‐\ncle TimesTen in-memory database  has been widely used. Alternatively, there are in-\nmemory extensions designed to improve the performance of existing DSSs. For\nexample, the Amazon DynamoDB Accelerator was created to add in-memory capa‐\nbilities to DynamoDB tables. To boost the performance of their primary database\nproduct, Db2, IBM introduced BLU Acceleration, which adds in-memory processing\ncapabilities and columnar storage to enhance the speed of data analytics. Open source\nin-memory storage and computing frameworks such as Apache Ignite also exist.\nTwo things are worth noting when working with in-memory databases. First, they are\nnot to be treated as a primary DSS as data in memory is considered volatile (if the\nmachine is shut down, the data will be lost). Second, data stored in memory may\nbecome invalid and require a refresh. Such a practice is called cache invalidation  and\nmay be a challenging issue if not understood properly.\nAnother popular time series database is InfluxDB, developed and marketed by Influ‐\nxData. InfluxDB is a native time series database designed for high performance and\nintuitiveness. InfluxData uses InfluxQL , an SQL-like query language for querying\ndata in InfluxDB databases. InfluxDB implements a data model that organizes data\naround buckets, databases, retention policies, series, measurements, tag keys, tag val‐\nues, and field keys. InfluxDB’s engine relies on an optimized data storage format\ncalled a Time-Structured Merge Tree (TSM) and an indexing technique called Time\nSeries Index (TSI).21\nFinancial use cases of time series databases\nTime series databases were originally developed for financial applications. As new\nmarket structures such as high-frequency and electronic trading mechanisms\nemerged, the volume of trades, quotes, and prices experienced a substantial increase.\nFor example, the NYSE, which currently allows trading for all 8,000+ US securities,\nreports that an average of 2.4 billion shares exchange hands on a daily basis . In addi‐\ntion to volume, the speed of data generation has also increased. Nowadays, high-\nfrequency trading at the NYSE occurs at the nanosecond level (i.e., one-billionth of a\nsecond). Consequently, a single second of trading can have hundreds of trades and\nquotes and reveal a variety of market patterns.\n322 | Chapter 8: Data Storage Layer",10771
120-The Message Broker Model.pdf,120-The Message Broker Model,"22For a comparative study on this topic, I recommend Fazl Barez, Paul Bilokon, and Ruijie Xiong’s “Bench‐\nmarking Specialized Databases for High-frequency Data” , arXiv  preprint; arXiv:2301.12561 (January 2023).\n23In asynchronous communication, an application sends a request or message in a fire-and-forget  mode, mean‐\ning that a response isn’t expected from the target receiver or consumer.Multiple firms, in particular those involved in high-frequency and algorithmic trad‐\ning, can leverage this fine-grained market view. However, to stay competitive, rapid\ndata access is crucial. This is where highly performant DSSs, such as time series data‐\nbases, are required.\nKdb+ has become a primary choice for financial markets, particularly in high-\nfrequency trading, due to its exceptional performance and speed.22 One of the main\nchallenges  that firms using kdb+ face is the need for fast access and analysis of critical\nmarket data in real time.\nThe Message Broker Model\nIn distributed systems, it is common for multiple applications to need to communi‐\ncate with each other to complete specific tasks. A typical scenario involves one type of\napplication, known as producers,  generating and storing data messages asynchro‐\nnously23 in a shared data store, and another type of application, called consumers,\nreading and processing the messages from the same store. This communication pat‐\ntern is known as the producer-consumer pattern . Figure 8-6  provides an example.\nA DSM that follows the producer-consumer pattern is called a message broker DSM,\nand a technology that implements the message broker DSM is simply called a message\nbroker . In the next few sections, you will learn about the main features of message\nbrokers, their data modeling principles, technological implementations, and, lastly,\ntheir use cases in the financial sector.\nFigure 8-6. The message broker model\nData Storage Models | 323\nWhy message brokers?\nMessage brokers are widely used in developing data-intensive systems, particularly in\nevent-driven, distributed, and streaming architectures. In these contexts, data is gen‐\nerated and consumed by various applications following different patterns and scales.\nTo meet such requirements, message brokers offer the ideal features in terms of sim‐\nplicity, speed, and scalability. Importantly, a message broker is not to be used as a pri‐\nmary or persistent DSS but rather as an intermediary that facilitates the exchange of\nmessages between applications.\nBy decoupling data production and consumption, message brokers enable producer\nand consumer applications to work and scale independently. If a small number of\nmessages are being produced, a few consumers can handle the workload. However, if\nthe number of messages increases significantly, additional consumers can be added to\nhandle the increased load. Moreover, producers need not worry about who the con‐\nsumers of the data will be, which allows for flexibility in adding different types of\nconsumers based on business needs.\nA distinguishing feature of message brokers is their fault tolerance. Consumers and\nproducers can be easily replaced in case of failure without impacting the state of the\nmessage broker. Moreover, message brokers are quite simple to use as applications\nonly need to know the topic for publishing or consuming messages. Let’s discuss mes‐\nsage topics in the next section.\nData modeling with message brokers\nMessage broker data modeling revolves around two main concepts: topics and mes‐\nsage schema, which I will illustrate in this section.\nTopic modeling.    Topics are the main building blocks of message brokers. A topic is a\nunique container of messages that publishers and subscribers need to specify when\ncommunicating with each other. Think of topics as the equivalent of tables in SQL\ndatabases.\nTopic modeling involves defining the topics and their target consumers and produc‐\ners. They can be created based on business requirements. In some cases, certain mes‐\nsage brokers support multiple types of topics, each optimized for a specific need. For\nexample, suppose you have an online application offering customer support for five\ncategories of issues, each handled by a different backend system. If you decide to use\na message broker, then you may want to create five topics, each handling a different\ntype of client request. In this case, if Topic 1 handles issues of Type A, then only mes‐\nsages related to issue Type A will be published to that topic, and only backend sys‐\ntems that handle issue Type A will be able to process messages from Topic 1.\n324 | Chapter 8: Data Storage Layer\nNaturally, large-scale applications can have hundreds or even thousands of topics. In\nthis case, establishing data governance policies for topic management becomes criti‐\ncal. This might include the people and methods involved in creating a topic, the pro‐\nducer and consumer applications that are permitted to use it, the type and format of\ndata that can be submitted to the topic, data quality checks and validations, data ano‐\nnymization, encryption, and data loss management.\nMessage schemas.    Message brokers do not enforce a schema on message structure,\noffering flexibility in message definition. However, if consumers don’t know what\ninformation to expect from producers, things might get out of control. This situation\nmay force applications to manage multiple formats, which adds unnecessary\ncomplexity.\nTherefore, it is a good practice to define a message schema for each topic initially and\nthen implement validators on both the producer and consumer sides. These valida‐\ntors ensure that the message content is checked for correctness before it is published\nor consumed.\nThe most popular format for message definition is JSON. However, it is also possible\nto use XML or other formats.\nMessage Schema Registry\nOne of the best practices in designing message brokers is the establishment of a mes‐\nsage schema registry. This consists of a centralized repository for storing, managing,\nversioning, and validating schemas for topic message data. Think of the schema regis‐\ntry as data governance practices that allow producers and consumers to communicate\nover a well-defined data contract in the form of a schema.\nApache Kafka, which is discussed in more detail later in this section, is a leading\nexample of a message broker that supports a schema registry . The schema registry in\nKafka facilitates schema evolution and compatibility, enabling producers and con‐\nsumers to update their schemas while preserving data integrity and cross-version\ncompatibility. This capability is especially valuable for ensuring that data written to\nKafka remains well structured and can be accurately interpreted by downstream\nconsumers.\nA technical aspect to keep in mind about message brokers is that they typically\nrequire messages to be serialized before being submitted to a topic. The message con‐\nsumer then deserializes the message back into its original structure. Serialization is\nthe process of converting a data object into a format optimized for transmission and\nstorage, typically a byte stream. Deserialization reverses this process, converting the\nbyte stream back into the original data object. While custom serializers and\nData Storage Models | 325\n24For more on this topic, consult the Confluent Developer website .deserializers  can be created, it is common to use built-in ones for formats such as\nJSON, Avro, and Protocol Buffers.24\nTechnological implementations of message brokers\nSeveral technological options are available for implementing a message broker. Exam‐\nples include Apache Kafka, RabbitMQ, Redis, Google Pub/Sub, Apache ActiveMQ,\nAmazon SQS, Amazon SNS, and Azure Service Bus. Making a full comparison\nbetween such technologies is beyond the scope of this book. However, the following\ncriteria can be used to perform a technical assessment:\nBusiness need\nSome message brokers are designed for specific use cases, such as message\nqueues (e.g., AWS SQS), while others are better suited for multiconsumer notifi‐\ncations (e.g., AWS SNS).\nPerformance\nMessage brokers differ in throughput and message read/write latency. For\ninstance, Apache Kafka excels in throughput, whereas Redis is known for its low\nlatency.\nDelivery mode\nMessage brokers offer different levels of message delivery guarantees, such as “ At\nMost Once, ” “ At Least Once, ” and “Exactly Once. ” For more details, refer to\nCloudflare’s documentation page .\nMessage persistence\nSome message brokers purge messages from their store upon delivery to con‐\nsumers (e.g., Apache ActiveMQ and RabbitMQ), while others store messages in a\ncommit log where the same message can be consumed by multiple consumers\nand at a later point in time.\nScalability\nMessage brokers can vary in their scaling capabilities. Some excel at scaling mes‐\nsage production, enabling the publication of a large number of messages per sec‐\nond, while others are more effective at scaling consumption, allowing for many\nconcurrent consumers. For example, Apache Kafka is well-known for its scalabil‐\nity features. One way this can be achieved is by allowing a topic to be further\ndivided into partitions. This way, instead of having one consumer blocking a\ntopic, partitions allow multiple consumers to read messages from the same topic\nby referencing the partition ID within the topic from which they want to con‐\nsume data.\n326 | Chapter 8: Data Storage Layer\nMessage prioritization\nSome message brokers, such as RabbitMQ and ActiveMQ, offer message prioriti‐\nzation features whereby high-priority messages are consumed before low-priority\nmessages.\nMessage ordering\nMessage ordering refers to the order in which messages are consumed from a\ntopic. While some message brokers may not enforce any specific ordering, others\nensure that messages are consumed following a specific order, such as consuming\nthe first messages received first.\nWhen developing a cloud-based data infrastructure, managed cloud message brokers\ncan be an excellent choice. These solutions are specifically engineered to seamlessly\nintegrate with other cloud services. For example, you can configure topics to auto‐\nmatically receive messages when a file is uploaded to cloud storage. Then, a cloud\nfunction can consume that message from the same topic to perform a specific task\nrelated to that file.\nFinancial use cases of message brokers\nMessage brokers have found a variety of applications in the financial sector, particu‐\nlarly in the development of event-driven and real-time systems. They are extensively\nused for managing operations involving a continuous stream of high-volume data\nsuch as payments, credit card transactions, loan applications, ATMs, and mobile noti‐\nfications. Message brokers can streamline tasks such as fraud analysis, application and\ntransaction approvals or rejections, credit authorizations, and client notifications.\nFor example, At PayPal, Apache Kafka has been used extensively  to support various\ncritical operations such as first-party tracking, metrics streaming, database synchro‐\nnization, log aggregation, batch processing, risk management, and analytics, with\neach of these use cases processing over 100 billion messages daily. PayPal’s Kafka\ninfrastructure includes 1,500 brokers across 85+ clusters, 20,000 topics, and nearly\n2,000 Mirror Maker nodes, achieving 99.99% availability. In the 2022 Retail Friday,\nPayPal’s Kafka processed a traffic volume of 21 million messages per second, which\ntotaled about 1.3 trillion messages in a single day. Similarly, Cyber Monday of the\nsame year resulted in 19 million messages per second, totaling 1.23 trillion messages\nin a single day.\nAnother significant application of message brokers in finance is the development of\nhighly scalable and efficient applications that handle real-time financial data sharing,\nmessaging, and streaming. For example, a financial data provider can leverage\nApache Kafka to offer various types of data by organizing them into different topics.\nSubsequently, clients are subscribed to a specific topic based on their subscription\nplan. Kafka uses Access Control Lists  (ACLs) as an authorization mechanism to deter‐\nmine which users are allowed to perform which actions on Kafka resources.\nData Storage Models | 327\n25For an interesting real-world application of this idea, I highly recommend watching the presentation given by\ntwo AWS experts  on how to build a real-time financial data feed as a service on AWS. Moreover, within the same topic, it is possible to further categorize the data into par‐\ntitions (e.g., by ticker) and offer clients the option to subscribe to a subset of the par‐\ntitions. Kafka has the concept of a consumer group , which can be used to allow\ndifferent clients to consume the same data from the same topic/partition using a\nKafka consumer offset. An offset is just a pointer that indicates the position within a\npartition of the next message to be consumed by a consumer group.25\nCase Study: Real-Time Fraud Detection\nat ING Group with Apache Kafka\nING Group is a Dutch multinational financial services corporation that provides a\nwide range of services such as retail and commercial banking, investment banking,\nand insurance services. ING, like other commercial banking firms, has a mechanism\nin place to identify fraud in its online banking activities. However, over time, this sys‐\ntem became overly complex, expensive, unreliable, and challenging to scale effectively\nto manage the growing volume of data required for real-time fraud detection.\nTo overcome these issues, the ING engineering team decided to leverage Apache\nKafka as the main event bus  to support real-time fraud detection and other data\nstreaming use cases. In a presentation given by ING experts Timor Timuri and\nRichard Bras in 2018 , they illustrated the high-level use case of Apache Kafka.\nTo handle client-sensitive data, the ING team introduced security into the messages\nstored in Kafka via a symmetric end-to-end encryption module. This works by first\nencrypting the data before publishing it to a topic and then decrypting it upon con‐\nsumption by the consumer.\nThe Protocol Buffers (protobuf) format was chosen for message serialization/deseri‐\nalization. Before publishing, messages are serialized into protobuf, and when con‐\nsumers receive the message, they deserialize it.\nIn addition, the ING engineering team created a number of predefined client settings\nto allow users who just want to use Kafka and do not care about the details (e.g., pub‐\nlish messages to a topic). This was provided via a simple and easy-to-use interface.\nFurthermore, test components were developed to simulate message publication, and\nmonitoring was introduced to evaluate Kafka’s performance. For compliance pur‐\nposes, all events are persisted in a Cassandra data store.\nTo ensure availability, ING’s team adopted a multi–data center strategy to replicate\ndata and ensure availability in case of downtimes or disasters.\nCrucially, while Kafka initially served for real-time fraud detection at scale, it later\nexpanded to become a primary development platform supporting a wide array of\n328 | Chapter 8: Data Storage Layer",15402
121-The Graph Model.pdf,121-The Graph Model,"applications. By 2018, ING managed 200 topics, processing 1 billion messages daily\nwith peak rates of 20,000 messages per second. Kafka is utilized across over 100 teams\nat ING, catering to 120 distinct use cases.\nThe Graph Model\nIn Chapter 2 , we explored graph data, illustrating its structures, types, and financial\napplications. One of the main advantages of graph data is its ability to reveal complex\npatterns and mechanisms that simpler data types (e.g., time series) cannot uncover.\nAs a result, a wide range of applications have been created, the primary functionality\nof which revolves around the storing and processing of graph data. The graph DSM is\nintended for such use cases. A technology that implements the graph DSM is known\nas a graph DSS or, more commonly, a graph database .\nIn the following sections, we will examine the key aspects of the graph DSM, graph\ndatabases, graph data modeling principles, technological implementations, and,\nfinally, financial use cases.\nWhy a graph model?\nThe graph DSM is used when designing graph-oriented applications that prioritize\nthe storing and analysis of data relationships. Examples of such requirements include\nthe following:\nCentrality analysis\nThis aims at identifying the most important nodes in a network. For instance, it\ncan be applied to identify Systemically Important Financial Institutions  (SIFIs),\nwhose failure could potentially disrupt the entire financial system, or market par‐\nticipants who play major roles as intermediaries, facilitating various transactions.\nSearch and pathfinding\nThis aims at locating nodes with specific attributes or determining the most effi‐\ncient path between two nodes within a graph. A widely used approach is shortest\npath analysis, which identifies the path between nodes with the fewest edges or\nminimal weights.\nCommunity detection\nThis aims at finding cohesive regions or subgraphs of connected nodes within a\ngraph. Such subgraphs are called communities. Examples include stock market\nclusters (stocks that move together), interbank lending clusters (e.g., banks that\nfrequently transact with each other), and fraud detection (e.g., groups of transac‐\ntions that might be linked to fraud rings).\nData Storage Models | 329\nContagion analysis\nThis examines how a shock impacting one or a set of nodes propagates through‐\nout the rest of the network. Applications include financial stress testing, cascade\nfailures, and systematic risk analysis.\nLink prediction\nThis aims at identifying pairs of nodes that are likely to establish a link in the\nfuture (e.g., two financial institutions establishing a correspondence banking\nrelationship).\nIt’s important to remember that the graph data element must be crucial to your appli‐\ncation to justify using a graph DSM. If your application needs to store graph data but\ndoes not require specialized graph capabilities, then a general-purpose model such as\nSQL may suffice.\nFor instance, consider a financial graph where nodes represent banks and links repre‐\nsent asset exposure (e.g., how many assets bank A holds at bank B). If bank A is pri‐\nmarily concerned with its direct exposure to the network, an SQL DSM can handle\nthis efficiently. Y ou can create a table with three columns: source_bank, target_bank,\nand exposure. To get your bank’s total exposure, you simply run SELECT tar\nget_bank, exposure WHERE source_bank = ""YOUR_BANK_NAME"" . However, if you\nneed to calculate the exposure of a bank to which your bank is exposed (similar to\nfinding the friends of your friend), the SQL model would require complex joins and\nrecursive operations to achieve this. This is where the graph DSM excels, as its stor‐\nage and processing logic is specifically designed for such tasks.\nData modeling with graph databases\nGraph data modeling is quite intuitive and flexible. It involves defining a domain  that\nconsists of nodes and links , each with their own attributes and labels. Graph data\nmodeling is often called whiteboard friendly, as it can be visually drawn on a white‐\nboard.\nGraph data modeling involves two primary aspects: node modeling and link model‐\ning. In node modeling, you define the different categories of nodes that can exist in\nthe graph, such as persons, organizations, countries, or assets. Each node category is\ncharacterized by specific attributes that provide descriptive information, including\ncategory labels and additional fields for unique identification.\nIn link modeling, the focus shifts to defining the relationships or connections\nbetween different node categories. This involves specifying the types of links that can\nexist and assigning link attributes that describe the characteristics of these connec‐\ntions. Figure 8-7  provides an example of graph data modeling.\n330 | Chapter 8: Data Storage Layer\nFigure 8-7. An example graph data model\nKeep in mind that the graph DSM does not enforce a rigid schema, as is the case with\nthe SQL model. New types of nodes and links can be easily added by defining their\nlabel and attributes, and existing nodes and links can be adapted to reflect new busi‐\nness requirements.\nTo enhance performance, graph data modeling may include the definition of indexes\non nodes and links. These indexes are typically based on attributes associated with\nnodes or links.\nTechnological implementations of graph databases\nThe graph DSM can be implemented using a variety of technological options. The\nsimplest but least performant approach involves the use of multi-model non-native\nDSSs such as relational databases. In this case, nodes, links, and their attributes are\ndefined in normalized tables, and indexes can be added to improve search perfor‐\nmance. If you recall the section on graph data in Chapter 2 , this would be similar to\nthe edge list  graph representation. This solution is recommended if your application\ndoesn’t prioritize graph data processing or if you want to perform basic graph queries.\nSome relational databases have ad hoc extensions that provide graph database func‐\ntionalities. The best example is Apache AGE , a PostgreSQL extension developed to\nprovide graph processing and analytics capabilities. AGE provides users with the\ncapability to compose graph queries using a hybrid query language that integrates\nSQL with openCypher, an open source implementation of the Cypher graph query\nlanguage originally developed by Neo4j.\nNon-native graph implementations have several limitations. In particular, relational\ndatabases are built on the assumption of independence between the records, which\nexplains their good performance when querying a set of rows. However, graph data\nrevolves around relationships between data points. This led to the development of\nData Storage Models | 331\n26For more details on these features, consult the official Neoo4j documentation page .\n27To learn more about this topic, see David Montag’s “Understanding Neo4j Scalability”  white paper,\nNeotechnology  (January 2013).\nnative graph databases designed specifically to handle such interconnected data\nstructures.\nNeo4j, created by Neo4j, Inc., is the leading native graph DSS, known for its extensive\nfeature set that supports diverse graph-based applications. Neo4j uses a proprietary\nquerying language called Cypher  to query data stored in a Neo4j database. To boost\nperformance, Neo4j’s processing engine relies on a special indexing strategy called\nindex-free adjacency . It works by assigning each node a direct reference to its adjacent\nnodes, which implies that accessing relationships and associated data is as simple as a\nmemory pointer lookup. In addition, Neo4j supports simple and composite node and\nlink attributes indexes, constraints, and atomic transactions, as well as functions, sub‐\nqueries, patterns, and clauses.26 For more advanced graph algorithms and machine\nlearning tasks, the Neo4j Graph Data Science library  is available.\nA major challenge with graph DSSs is scalability. Solutions such as\nNeo4j can scale to handle billions of nodes and links, which is\nmore than enough for the vast majority of use cases. However,\nsharding is not native to graph databases. Unlike relational data‐\nbases, where rows are independent, graph data is interconnected,\nmaking it difficult to partition efficiently as graph density increases.\nMathematically speaking, the problem of partitioning a graph data‐\nset across a set of nodes is near impossible (NP-complete) to per‐\nform for large graphs.27 Nevertheless, several graph database\ntechnologies (e.g., TigerGraph) have made progress along this way\nand offer distributed graph processing capabilities.\nAnother implementation of graph storage systems is managed services. One example\nis Amazon Neptune , which offers a scalable, secure, and cost-efficient managed graph\ndatabase. Neptune allows users to query data using the popular graph query lan‐\nguages Apache TinkerPop Gremlin, W3C’s SPARQL, and openCypher. Another\nprominent example is TigerGraph , which provides a rich platform for native graph\ndata storage and analysis at scale.\nFinancial use cases of graph databases\nGraph databases have been applied to solve a variety of problems in the financial sec‐\ntor. One prominent use case is fraud detection. Many forms of financial fraud exist:\ncredit card fraud, loan fraud, wire fraud, tax fraud, identity theft, insurance fraud, and\nmoney laundering. In simple scenarios, fraud is easy to detect with traditional tools\n332 | Chapter 8: Data Storage Layer\n28An example of a company that offers such services is Financial Network Analysis (FNA) .and databases. However, in today’s environment, fraudsters use sophisticated tech‐\nniques and tricks to commit their crimes and hide their actions and identities.\nThe best way to detect such complex patterns is by designing a framework that com‐\nbines network analysis with a graph database to build a fraud graph . Such a graph\nrecords the connections between the actors, transactions, and other pertinent data to\nhelp experts capture anomalous trends in the data and create applications that can\nidentify fraudulent activity. Figure 8-8  naively illustrates the concept. In this example,\nthe real account holder is associated with a unique set of information such as address,\nemail, phone number, SSN, etc. A fraudster, however, might use a variety of\naddresses, phone numbers, and other pieces of information and combine them in\ncomplex ways to hide their activities. When represented as a graph, such hidden rela‐\ntionships can be detected more easily. This task can be represented as an entity reso‐\nlution problem where the goal is to match nodes that represent the same entity.\nFigure 8-8. Graph-based fraud detection\nGraph databases also find applications in financial risk analysis. For instance, finan‐\ncial assets can be structured hierarchically, bundled into new assets, segmented into\ntranches, and distributed across complex networks of ownership and transactions.\nThis complicates the task of risk management and reduces regulatory oversight. One\napproach to address this challenge involves constructing a financial assets graph . This\ngraph models and stores the relationships among different types of financial assets,\nenabling financial institutions to track their assets and account for dependencies in\ntheir pricing models. Additionally, graph databases are useful for simulating various \nshocks and scenarios to test the stability of correlated asset portfolios, interbank net‐\nworks, and various types of systemic risks.28\nData Storage Models | 333\nCase Study: Anti–Money Laundering at Banking Circle with Neo4j\nBanking Circle (BC)  is a banking-as-a-service (BaaS) company that offers real-time\nand low-cost payment services by providing the clients with direct access to clearing\nin multiple jurisdictions. BC serves a large client base, including over 250 regulated\nbusinesses, financial institutions, and marketplaces. It allows clients to move and con‐\nvert money in real time securely and compliantly. In 2021, BC processed over 250 bil‐\nlion euros in payment volume. Importantly, as the volume of payments that BC\nprocessed increased, so did the number of fraud attempts.\nTo detect fraud, BC initially relied on a traditional rule-based approach , which\nworked by assigning risk scores by searching and catching certain words, amounts,\nand locations and sending them to fraud analysts for manual review. However, the\ncompany realized that this approach was slow, expensive, and generated a high level\nof false positives. As a result, it decided to adopt a data-driven AML approach that\nleverages graph and machine learning techniques. The outcome was a modern frame‐\nwork called SCAM, or System for Catching Attempted Moneylaundering, which con‐\nsisted of an ensemble of different machine learning models.\nTo capture complex relationships, SCAM relies on multiple network representations\nof various data features (e.g., accounts, payments, entities, countries, etc.). Using\nNeo4j’s Graph Data Science (GDS) framework , BC was able to conduct various types\nof advanced graph analysis.\nFor example, community detection algorithms were used to generate features that\ndetect high-risk clusters within the network. A community detection algorithm works\nby finding clusters of similar nodes; for example, if a fraudster uses similar profile\nattributes, they are likely to fall within the same cluster. This technique, combined\nwith other network features such as risk scores of neighboring nodes and distances to\ntax havens and known fraudsters, has significantly improved the reliability of SCAM.\nBC was able to transform a laborious, sluggish procedure into a data-driven, scalable,\nflexible solution that leverages cutting-edge technology. False negatives were reduced\nby 10–25% and the number of overall alerts escalated for manual review was halved.\nBC continues to experiment with Neo4j algorithms and plans to add more graph fea‐\ntures to tune their model further.\n334 | Chapter 8: Data Storage Layer",14165
122-The Warehouse Model.pdf,122-The Warehouse Model,"The Warehouse Model\nOne of the most common business needs in data-driven organizations is an\nenterprise-wide system for integrated and structured data storage, access, analytics,\nand reporting. The most popular solution to this need is data warehousing , a concept\nthat has been around since the 1970s. Bill Inmon and Ralph Kimball, the two pio‐\nneers of this field, define a data warehouse as:\nA data warehouse is a subject-oriented, integrated, nonvolatile, and time-variant col‐\nlection of data in support of management’s decisions.\n—Bill Inmon\nA data warehouse is a copy of transaction data specifically structured for query and\nanalysis.\n—Ralph Kimball\nI will use the term warehouse DSM  to describe a system that periodically gathers, con‐\nsolidates, and structures data from various sources within a company into a central\nrepository intended for various analytical purposes. A DSS system used to implement\nthe warehouse DSM is called a warehouse DSS or data warehouse. Figure 8-9  illus‐\ntrates a typical data warehouse architecture. As the figure shows, data from heteroge‐\nneous sources (left side) get organized into a central data warehouse (middle) which\nthen serves a variety of analytical data needs (right side).\nFigure 8-9. A data warehouse architecture\nData Storage Models | 335\nWhy data warehouses?\nThe warehouse DSM offers several advantages, including the following:\nStructure\nData warehouses introduce a consistent structure to data, regardless of the origi‐\nnal formats provided by the source systems.\nAdvanced analytics\nPerforming advanced data queries for business intelligence and reporting is a\nfundamental and frequent activity within organizations. Data warehouses pro‐\nvide intuitive SQL-like querying capabilities that enable a wide range of data\nqueries.\nScalability\nData warehouses can scale to handle large volumes of data and complex read/\nwrite operations.\nSubject-oriented\nData warehouses are often designed to allow users to query and analyze data\nabout a particular subject or functional area (such as sales, customers, payments,\netc.)\nIntegrated\nData warehouses consolidate, structure, and harmonize data from different sour‐\nces (e.g., departments or business units) while ensuring consistency and data\nquality.\nNonvolatile\nOnce data is uploaded to a data warehouse, it remains stable and does not\nchange. If any changes occur to a data record, a new record is added instead of\nupdating the existing ones.\nTime-variant\nData warehouses record data with a timestamp, allowing users to perform accu‐\nrate point-in-time historical analysis.\nAt this point, you might be wondering about the difference between the warehouse\nDSM and other models that have similar features, such as the data lake or SQL DSMs.\nSimilar to data warehouses, data lakes can also be used to consolidate data into a cen‐\ntral location. Data lakes lack default mechanisms to ensure data structure consistency\nand homogeneity, and they do not offer advanced querying capabilities like a data\nwarehouse does.\n336 | Chapter 8: Data Storage Layer\n29For a comprehensive comparative study of the difference between Inmon and Kimball models, see Lamia Y es‐\nsad and Aissa Labiod’s “Comparative Study of Data Warehouses Modeling Approaches: Inmon, Kimball and\nData Vault” , in 2016 International Conference on System Reliability and Science (ICSRS)  (IEEE, 2016): 95–99.The primary difference between a relational DSM and a data warehouse DSM can be\nunderstood through the distinction between OLAP and OLTP . The relational DSM is\nprimarily meant for OLAP-oriented applications, emphasizing transactional guaran‐\ntees and single-row lookups/inserts/updates (often called Data Manipulation Lan‐\nguage , or DML). The data warehouse DSM is instead aimed toward OLAP-oriented\napplications that require complex analytical read/write operations with less focus on\npoint lookups and DML. However, it’s important to note that both the relational and\nwarehouse models can coexist. In many cases, a significant portion of the data inges‐\nted into a data warehouse originates from relational databases that support day-to-\nday business operations.\nData modeling with data warehouses\nWhen building a data warehouse, data modeling is critical. Over the years, two com‐\npeting approaches to data modeling have emerged for data warehouses: the subject\nmodeling approach  of Bill Inmon and the dimensional modeling approach  of Ralph\nKimball.29\nIn the Inmon approach, data is sourced from various operational systems across the\norganization and integrated into a centralized data warehouse. Subsequently, based\non the specific needs of individual departments or business units, dedicated data\nmarts are derived from the data warehouse. In this model, the data warehouse and\nthe data marts are physically separate entities, each with its own storage and charac‐\nteristics. Data within the data marts is considered consistent and reliable, as it origi‐\nnates from the centralized source of truth—the data warehouse itself. Users from\ndifferent departments typically employ specialized tools to access and analyze the\ndata within their respective data marts. Figure 8-10  offers an illustration of the Inmon\narchitecture. The main advantage of this approach is flexibility, as new data marts can\nbe created to meet new business needs. On the negative side, such an architecture\nintroduces a maintenance burden due to the physical separation between the data\nwarehouse and the data marts.\nThe Kimball approach takes a different perspective. Instead of isolating the data\nwarehouse from the data marts, Kimball suggested a user requirement-driven\napproach that defines the data marts in advance and integrates them within the data\nwarehouse itself. Figure 8-11  illustrates the concept.\nData Storage Models | 337\nFigure 8-10. Inmon data warehouse architecture\nFigure 8-11. Kimball data warehouse architecture\nThe definition of data marts is done via a technique called dimensional modeling . Its\nmain idea is to divide data into fact and dimension  tables. A fact table contains data\non quantifiable business entities or events, while a dimension table stores data related\nto the context of the events. Examples of facts include financial payments, monetary\ntransfers, and online sales. Dimensions may be information such as product, user,\nand country details. Facts and dimensions may be used to categorize data into\ndata mart–specific tables based on various business processes such as revenues, prod‐\nucts, sales, employees, suppliers, and branches.\n338 | Chapter 8: Data Storage Layer\n30For an excellent reference on dimensional modeling, I recommend Ralph Kimball and Margy Ross’s The Data\nWarehouse Toolkit: The Definitive  Guide to Dimensional Modeling , 3rd ed. (Wiley, 2013).In dimensional modeling, two primary types of data warehouse schemas are preva‐\nlent: the star schema and the snowflake schema. The star schema features a central\nfact table connected directly to multiple dimension tables, resembling a star-shaped\narchitecture. On the other hand, the snowflake schema is a normalized version of the\nstar schema that has a single fact table and several direct and indirect dimension\ntables. Figure 8-12  illustrates the difference.\nFigure 8-12. Star schema versus snowflake  schema\nStar schemas are generally easier to understand and simpler to query because they\ninvolve fewer table JOIN operations. Snowflake schemas, on the other hand, can be\nmore effective at ensuring data integrity and consistency as they remove redundancy\nthrough data normalization.30\nData Storage Models | 339\n31For instance, the NYSE transitioned to IBM Netezza to manage its growing data volumes, which traditional\nSQL databases were unable to handle effectively. Read more in “ At NYSE, the Data Deluge Overwhelms Tradi‐\ntional Databases” , by Tom Groenfeldt.\n32For a deeper understanding of how and why columnar formats are used in data warehouses, I highly recom‐\nmend reading about Google BigQuery’s columnar storage format Capacitor .Technological implementations of data warehousing\nDue to the high demand for data warehousing, numerous technological implementa‐\ntions have been developed. As data warehouses emphasize structured data and\nadvanced querying capabilities, most technologies in this domain support SQL and\nrelational data modeling. Leading commercial solutions include high-performance\nintegrated systems like IBM Netezza, IBM Integrated Analytics System (IAS), Db2,\nTeradata, Oracle Exadata, and Micro Focus Vertica Enterprise. Financial institutions\noften rely on these solutions for high-performance processing of large datasets.31\nIn recent years, cloud-based data warehousing solutions have seen a notable increase\nin popularity. Examples include Amazon Redshift, Snowflake, Google BigQuery, and\nAzure Synapse Analytics. This surge is largely attributed to the convenient features\nthat the cloud offers in terms of scalability, managed infrastructure, on-demand pric‐\ning, reliability, and seamless integration with other cloud services. For example, with\na data warehousing service such as Google BigQuery, you can easily create a database\nand begin writing and reading data to it in a matter of minutes. As a serverless plat‐\nform, BigQuery handles all infrastructure management, so you don’t have to worry\nabout provisioning or scaling resources.\nCloud-based data warehouses commonly adopt column-oriented storage, enhancing\nquery performance and reducing costs by retrieving only the columns requested by\nusers. This storage method also benefits from efficiency gains through compression\nand deduplication techniques, optimizing data storage on disk.32\nCloud data warehouses achieve scalability by decoupling the compute and storage\nlayers. For example, using Snowflake as a data warehouse, your data will be stored\nand managed in separate persistence storage (e.g., AWS S3) . In contrast, compute\nresources are virtualized and can be independently scaled up or down by the user,\nseparate from storage.\nTo enhance storage efficiency and query performance, cloud warehouses leverage the\nconcept of data partitioning  and clustering . The idea is to split a table into a set of stor‐\nage units called partitions that can be managed and queried separately. With cluster‐\ning, data within each partition is physically ordered into small blocks of data. When a\nquery is executed against a table, the database engine will first filter the partitions to\nscan (partition pruning) and then apply block filtering (pruning) to each partition.\nThis way, only the minimum amount of data is queried, thus increasing performance\nand reducing query cost. BigQuery requires the user to configure the partitioning\n340 | Chapter 8: Data Storage Layer\nand clustering keys (from existing columns) , while Snowflake implements a more\ndynamic and managed approach to partitioning and clustering called micro-\npartitioning .\nWhen using cloud data warehouses, it’s crucial to consider vendor-\nspecific limits, quotas, and usage guidelines. For instance, as OLAP\nsystems, data warehouses are optimized for running complex ana‐\nlytical queries with moderate frequency. Attempting to support\nthousands of concurrent users may exceed maximum connection\nlimits. For example, BigQuery allows up to 100 concurrent connec‐\ntions for interactive queries, while Snowflake, with auto-scaling,\nsupports up to 80 concurrent connections. Furthermore, while data\nwarehouses do support data updates and DML operations, they are\nnot primarily designed for these tasks.\nFinancial use cases of data warehouses\nData warehouses are extensively used in the financial sector to address various busi‐\nness needs. One major use case is the consolidation of data from diverse business\nsilos (e.g., risk, revenue, loans) into a unified data warehouse that serves as the pri‐\nmary source for analytics. This consolidation facilitates the extraction of valuable\nbusiness insights from the vast volumes of data generated through daily operations\nwithin financial institutions. For instance, this enables business teams to effectively\nunderstand and manage different types of risks such as credit, financial, operational,\nand compliance risks. In Chapter 5  on financial data governance, we discussed how\ndata aggregation and consolidation capabilities have become a regulatory require‐\nment for banks following the crisis of 2007–2008.\nData warehouses can enable financial institutions to track financial, operational, and\ndata quality indicators over time. Maintaining point-in-time historical data is a com‐\nmon business requirement in financial markets. A well-designed data warehouse may\nensure this by enforcing an append-only policy, which allows new data to be written\nwhile maintaining the immutability of existing data. This principle also applies to\nexternally sourced financial data. For instance, financial vendor data is intended for\nreading and analysis rather than modification, making a data warehouse an ideal sol‐\nution for such use cases.\nFurthermore, data warehouses are widely used in financial markets for facilitating\ndata sharing. For example, various financial data vendors now offer the option to\ndeliver their data to clients via the cloud. This delivery mechanism is particularly\nconvenient with cloud-based data warehouses. Refer to the section “Data Ingestion\nTechnologies” on page 269  to learn more about this topic.\nData Storage Models | 341\nCase Study: BlackRock’s Aladdin Data Cloud Powered by Snowflake\nBlackRock is the world’s largest and leading investment management firm and a pro‐\nvider of financial technology. As of 2023, BlackRock’s assets under management\nreached around 8.5 trillion US dollars . BlackRock’s jewel technology is known as\nAladdin , an integrated solution that combines sophisticated risk analysis features with\nportfolio management, trading, and operations tools in a single platform. Aladdin has\nestablished itself as one of the most prominent examples of financial technologies. In\nparticular, during the 2008 financial crisis, Aladdin showed its remarkable capabilities\nby allowing companies like Microsoft to aggregate their risk exposures to different\nbanks (a feature that many financial institutions lacked at the time) .\nTo enhance Aladdin’s data-driven capabilities, BlackRock launched Aladdin Data\nCloud , a managed data solution that allows users to combine Aladdin portfolio data\nwith non-Aladdin data, perform timely analysis, and develop custom dashboards and\napplications using Aladdin Studio—BlackRock’s platform for developers. To this goal,\nBlackRock decided to partner with the cloud-based data warehousing company\nSnowflake . The main reasons for choosing Snowflake were performance, scalability,\nand concurrency management.\nBy bringing together Aladdin, a leader in investment management technology, and\nSnowflake’s Data Cloud, the Aladdin Data Cloud is set to allow its clients to expand\nthe range of data-driven applications across their organizations. Each Aladdin Data\nCloud client receives an independent, centrally managed data store preloaded with\nrich front-to-back Aladdin datasets, which can then be supplemented with propriet‐\nary and other third-party data sources, allowing organizations to access and query\ntheir business-critical data on a single, cloud-based platform.\nSnowflake has several features that make it attractive for building data applications.\nFor example, it isolates work environments via virtual warehouses , which are clusters\nwith dedicated CPU, memory, and temporary storage. In a simple setup, each client\ncan be dedicated to a separate virtual warehouse. Virtual warehouses offer the flexibil‐\nity to scale resources up and down, thus customizing work environments for different\nclient needs. In addition, Snowflake has advanced and secure data-sharing functional‐\nities, making it ideal for working with a large base of users and accounts. Moreover,\nSnowflake developed its own SQL language  with rich command features and SQL\nstandard-compliant syntax. Additionally, Snowflake provides a list of configurable\nparameters  that can be tuned to control the behavior of a user account, queries, ses‐\nsions, and objects.\n342 | Chapter 8: Data Storage Layer",16447
123-The Blockchain Model.pdf,123-The Blockchain Model,"33For a good introduction to cryptocurrencies, I highly recommend Arvind Narayanan, Joseph Bonneau,\nEdward Felten, Andrew Miller, and Steven Goldfeder’s Bitcoin and Cryptocurrency Technologies: A Compre‐\nhensive Introduction  (Princeton University Press, 2016).\n34For a deeper introduction to blockchain databases, I highly recommend Chapter 26 of Avi Silberschatz, Henry\nF. Korth, and S. Sudarshan’s 2019 Database Systems Concepts , 7th ed. (McGraw Hill, 2019).The Blockchain Model\nBlockchain is one of the most promising technological trends in today’s financial\nservices landscape. It is the underlying technology that enables the so-called crypto‐\ncurrencies,  such as Bitcoin and Ethereum.33 In this section, I will briefly walk through\nthe basic idea behind blockchain and its applicability as a data storage system for\nfinancial data.34\nAt the most basic level, a blockchain is a data structure that stores data as a chain of\nlinked information blocks. The most prominent application of blockchain technology\nis in the creation of digital ledgers . A digital ledger is a record-keeping book of\naccounts that keeps track of a business’s financial transactions. Think, for example, of\nwhen you deposit or withdraw money from your bank account. As per common\nsense, you trust that your bank will maintain a correct and truthful ledger and not\nmodify it by deleting or updating book records.\nA blockchain data ledger performs a similar function and guarantees data immutabil‐\nity, but following different design principles. To illustrate the idea, let’s represent a\nblockchain as a linked list of information blocks, as illustrated in Figure 8-13 . The\nfirst block in a blockchain is called the genesis block . Each subsequent block contains\nits own data (e.g., transaction records) as well as a hash pointer  that stores both a\npointer to the previous block and a hash of that older block. A hashing function, H(),\nis used to create a cryptographic hash of each block’s data. A block hash serves as a\nunique identifier of each block.\nFigure 8-13. Blockchain as a linked list\nTo illustrate why this structure is tamper resistant, consider an adversary attempting\nto manipulate the blockchain by altering data entries in block 2. This will automati‐\ncally update the block’s hash. As a result, the hash pointer in block 3 will no longer\nData Storage Models | 343\n35For more on performance-related issues in blockchains, read Mayank Raikwar, Danilo Gligoroski, and Goran\nVelinov’s “Trends in Development of Databases and Blockchain” , in the 2020 Seventh International Conference\non Software  Defined  Systems (SDS)  (IEEE, 2020): 177–182. \n36To learn more about blockchain database design patterns, I recommend MongoDB’s guide “Blockchain Data‐\nbase: A Comprehensive Guide” .correspond to the updated hash of block 2. To conduct a successful attack, the adver‐\nsary must alter all hash pointers up to the most recent block in the list (i.e., the head).\nIn addition to data immutability, blockchain-based data ledgers have shown high reli‐\nability when implemented as a distributed system. The term distributed ledger technol‐\nogy (DLT) is often used to describe these systems. In a DLT, a decentralized network\nof nodes cooperatively maintains the integrity of the ledger. Each node keeps a copy\nof the blockchain and can verify and validate any operation that alters the blockchain.\nFor example, if a node wants to add a new block to the chain, the other nodes in the\nnetwork need to agree and reach a consensus on such an operation. The same applies\nto our earlier example of the adversary attack: even if the adversary were able to\nchange all the hash pointers, the changes would still need to be validated and\naccepted by the other nodes in the network. A variety of consensus mechanisms are\navailable, including Proof of Work, Proof of Stake, and Byzantine Consensus.\nAs a financial data engineer, you might be wondering if blockchain is a good solution\nas a data storage system. Technically speaking, a blockchain can be used to store\nfinancial transaction data, but it comes with a price. First, blockchains have very limi‐\nted data querying capabilities. Second, as more nodes are added to the network, per‐\nformance may get worse in terms of throughput, latency, or capacity.35 For example,\nthe decentralized nature of blockchain introduces latency in both storing and retriev‐\ning data. In applications requiring fast data access, the time taken for reaching con‐\nsensus, propagating blocks, and verifying transactions can lead to slower response\ntimes.\nTo overcome these issues, blockchain-based databases (or blockchain databases for\nshort) were introduced. The main idea behind a blockchain database is to combine\nthe best of both worlds: the performance and power of a traditional database system\n(e.g., SQL or document databases) with the immutability, cryptographic verifiability,\nintegrity, decentralized control, and transaction traceability of a blockchain.\nCrucially, when deployed internally within a financial institution, a blockchain data‐\nbase doesn’t need to be decentralized. The institution in this case acts as the adminis‐\ntrative authority that controls the blockchain.36\nA few commercial solutions for blockchain databases already exist. The most promi‐\nnent is BigchainDB , which uses MongoDB as the distributed database under the hood\nand offers blockchain characteristics. Another example is Amazon Quantum Ledger\n344 | Chapter 8: Data Storage Layer\nDatabase (QLDP) , a fully managed ledger database for creating an immutable, trans‐\nparent, and cryptographically verifiable transaction log.\nRipple: A Blockchain-Based Global Payment Ecosystem\nA successful story in the commercial implementation of blockchain technology is\nRipple. Behind this is an American company called Ripple Labs, Inc. It is a unique\nplayer in this industry as it created the first financial services platform and network\nthat leverages blockchain, tokenization, and cryptocurrency technologies for enter‐\nprises. Ripple’s primary use case is to enable secure, instant, and low-cost cross-\nborder financial transactions and settlements.\nRipple’s primary offering is RippleNet, a blockchain-based infrastructure for pay‐\nments. As of the end of 2024, RippleNet connects a network of over 500 participants.\nXRP is Ripple Labs’s native cryptocurrency (digital asset). It is used as a bridge cur‐\nrency in Ripple’s ecosystem to facilitate liquidity for cross-border transactions. XRP is\nindependent of RippleNet but can be utilized within the network for certain services.\nXRP is a bridge asset , or an asset that businesses and financial institutions can use to\nmake a bridge transfer between two different fiat currencies. In this scenario, a finan‐\ncial institution can purchase an equivalent amount of XRP and send it through Rip‐\nple’s network.\nThe underlying blockchain ledger technology that powers Ripple is called XRP\nLedger (XRPL). Interestingly, XRPL differs from Bitcoin’s blockchain . Bitcoin relies\non an energy-intensive Proof-of-Work (PoW) mechanism for transaction validation,\nwhile Ripple uses the faster and more efficient mechanism known as the XRP Ledger\nConsensus Protocol , or XRP LCP for short. For an intuitive introduction to XRP LCP ,\nsee the CoinBrain website . This consensus mechanism is key to XRPL ’s efficiency,\nenabling rapid transactions at low costs. XRPL has been released to the public, and it\nis now an open source blockchain protocol .\nTo illustrate an example (sourced from the Ripple website ), let’s say that Bank A\nintends to transfer $10,000 to Bank B in euros using the Ripple platform. The banks\nagree on a Forex rate on Ripple, following which Bank A converts the USD to XRP at\nthe agreed rate and transfers it via the XRP Ledger to Bank B. Upon receipt, which\nhappens in a matter of seconds, Bank B converts the XRP to euros, with minimal\ntransaction fees compared to traditional banking methods. This use of XRP as a\nbridge currency via RippleNet demonstrates its role in facilitating fast and cost-\neffective cross-border transactions.\nData Storage Models | 345",8220
124-Chapter 9. Data Transformation and Delivery Layer.pdf,124-Chapter 9. Data Transformation and Delivery Layer,"Ripple has helped countries create their own central bank digital currencies (CBDCs)\nthrough its Ripple CBDC platform. Furthermore, Ripple is a member of the ISO\n20022 Standards Body, becoming the first member of the ISO organization dedicated\nto Distributed Ledger Technology (DLT).\nIn conclusion, blockchain presents a complex landscape with ongoing efforts aimed\nat exploring its feasibility for high-storage and high-performance applications.\nResearchers and developers are actively investigating various approaches to enhance\nblockchain’s capabilities, such as sharding, optimizing consensus algorithms, Layer 2\nprotocols, and sidechains, and are exploring hybrid architectures that combine block‐\nchain with traditional databases.\nSummary\nThis chapter covered the second layer of the financial data engineering lifecycle: stor‐\nage. This layer facilitates the choice and implementation of various data storage mod‐\nels and systems, supporting the storage and retrieval of data within a financial data\ninfrastructure.\nCrucially, data is not merely ingested and stored. To unlock its full potential in driv‐\ning informed decision-making and operational excellence, data must undergo a sys‐\ntematic transformation into structures that align with the diverse needs of\nstakeholders within financial institutions. This takes us to the next layer: transforma‐\ntion and delivery. This layer serves as the bridge between raw data ingestion and its\nutilization by end users.\nLet’s explore this critical layer further in the next chapter.\n346 | Chapter 8: Data Storage Layer",1591
125-Data Querying.pdf,125-Data Querying,,0
126-Querying Patterns.pdf,126-Querying Patterns,"CHAPTER 9\nData Transformation and Delivery Layer\nBy now, you should be familiar with the first two layers of the financial data engineer‐\ning lifecycle: ingestion and storage. The next layer is the transformation and delivery\nlayer, where two major things happen: first, the data undergoes a range of transfor‐\nmations, from basic preprocessing and cleaning to complex analytical calculations;\nsecond, the data is delivered to its end users following pre-established agreements.\nKeep in mind that all layers can impact one another based on business needs and\ntechnical limitations. As a result, the decisions you make in the ingestion and storage\nlayers are likely to have an impact on this layer and vice versa.\nThroughout this chapter, you will learn the essential concepts of data transformation\nand delivery and their applications in the financial domain. This includes data query‐\ning patterns, query optimization, transformations, computational requirements, data\nconsumers, and delivery mechanisms.\nData Querying\nThe most common operation performed in the transformation and delivery layer is\ndata querying. Before making any modifications, data is always queried. The perfor‐\nmance of a financial data infrastructure is largely dependent on querying patterns\nand optimization. Therefore, as a financial data engineer, you will play a vital role in\ndefining and optimizing the querying needs and patterns for your team and data con‐\nsumers. Let’s find out how.\nQuerying Patterns\nA data querying pattern is a repeated request for information made by several data\nconsumers on a regular basis. A pattern can be either detected by analyzing user\nrequests or anticipated during an early design phase. Being able to understand and\n347\nanticipate querying patterns will provide you with great input for the choice and\ndesign of your data storage model as well as optimizing the cost and execution time\nof your data queries.\nThere is no fixed catalog for querying patterns; they are primarily determined by\nbusiness requirements and data consumer needs. However, in this section, I will\nintroduce several common querying patterns that you might encounter in financial\napplications.\nTime series queries\nTime series queries are essential in finance, used to retrieve data for a specific finan‐\ncial entity or quantity over a given period of time. Using standard SQL pseudocode,\nwe can express a basic time series query:\n-- SQL\nSELECT time_column , attribute_1 , attribute_2  \nFROM financial_entity_table  \nWHERE entity_name  = 'A' \nAND date BETWEEN '2022-01-01'  AND '2022-02-01'\nFinancial use cases for time series queries include the following:\n•Give me the closing adjusted price for stock A from January 2022 until February\n2022.\n•Give me all transactions for client A for the past month.\n•Give me a company’s sales and revenues over the past six months.\nCross-section queries\nA cross-section query is used to obtain data for a set of financial entities at a specific\npoint in time. A simplified SQL pseudocode for a cross-section query might look\nsomething like this:\n-- SQL\nSELECT entity_name , attribute_1 , attribute_2  \nFROM financial_entity_table  \nWHERE entity_name  in ('A', 'B', 'C', 'D') \nAND date = '2022-02-01'\nCross-section queries are commonly used to investigate differences between financial\nentities at a given point in time. Examples include the following:\n•Give me the market capitalization of the top five companies listed at the NYSE\nfor February 1, 2022.\n•Give me the credit rating of all publicly traded companies in the pharmaceutical\nsector in the United States on January 10, 2022.\n348 | Chapter 9: Data Transformation and Delivery Layer\n1A simple moving average is the average of the previous K data points. For example, take the vector [1,2,3,4,5].\nThe moving average over two data points becomes [1, 1.5, 3, 4.5, 6].\n2Try it yourself on CoderPad . Panel queries\nA panel query combines time series and cross-section dimensions. The user asks for\ndata on multiple financial entities for a range of dates. A pseudo-SQL for a panel\nquery may look like this:\n-- SQL\nSELECT time_coluumn , entity_name , attribute_1 , attribute_2  \nFROM financial_entity_table  \nWHERE entity_name  in ('A', 'B', 'C', 'D') \nAND date BETWEEN '2022-01-01'  AND '2022-02-01'\nPanel queries are mainly used to analyze the intertemporal differences between finan‐\ncial entities—for example the following:\n•Give me the online purchasing activities of our top 1,000 clients in the past\nmonth.\n•Give me the volume of trades for stocks A, B, and C over the past month.\nAnalytical queries\nAnalytical queries are advanced statements that perform computations on the data.\nThey are often supported and used in SQL and data warehousing systems as well as\nspecialized databases such as time series and graph databases.\nThe most common type of analytical query is grouping, in which rows that have the\nsame values are grouped into summary rows using an aggregation function such as\nsum, min, max, average, standard deviation, and others. For instance, if you want to\nfind the maximum daily price for each unique combination of stock symbols and\ndates in your database, you would write an SQL query similar to this:\n-- SQL\nSELECT stock_symbol , date, MAX(price)\nFROM price_table\nGROUP BY stock_symbol , date\nGrouping reduces the number of rows into a set of groups based on your aggregation\nfields. In some cases, you don’t want the number of rows to shrink, but rather want to\napply an aggregation query that spans multiple time windows. For example, suppose\nyou want to calculate the simple moving average of the adjusted closing price for each\nstock in your dataset.1 In this case, a special type of SQL method, called window func‐\ntions , can be used. Here is how it works:2\nData Querying  | 349\n-- SQL\ncreate table adjsted_price  (date DATE, symbol VARCHAR(5), price NUMERIC);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-10' , 'A', 1);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-11' , 'A', 2);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-12' , 'A', 3);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-13' , 'A', 4);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-10' , 'B', 10);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-11' , 'B', 20);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-12' , 'B', 30);\ninsert into adjsted_price  (date, symbol, price) values ('2022-02-13' , 'B', 40);\n \nSELECT symbol, date, AVG(price) \nOVER (PARTITION  BY symbol ORDER BY date ASC ROWS BETWEEN 2 \nPRECEDING  AND CURRENT ROW) AS ""Moving Average""\nFROM adjsted_price\nORDER BY symbol, date ASC\nA variety of data storage systems implement window and analytical functions. For\nexample, relational DSSs like PostgreSQL implements the SQL standard window\nfunctions, while others such as Oracle implement additional functions  on top of the\nstandard ones. Specialized databases, such as InfluxDB time series database, offer an\nimpressive collection of time-based analytical functions . Similarly, graph databases\nsuch as Neo4j offer a wide range of graph-oriented functions  that can be executed\nwith the Cypher language, as well as a specialized library called Graph Data Science\nthat offers advanced graph algorithms and visualization features that can be used to\nsolve advanced business problems such as fraud detection.\nRecently, cloud data warehouse solutions such BigQuery and Snowflake  have intro‐\nduced features enabling users to create and interact with machine learning models\nusing advanced analytical SQL queries. These capabilities allow users to build various\nmodels (e.g., regression, classification, anomaly detection, etc.) and make predictions\non new data, all seamlessly integrated within the data warehouse environment. Using\nBigQuery , the syntax for creating a model may look like the following:\n-- SQL\n-- Create a linear regression model\nCREATE MODEL `mydataset .my_model `\nOPTIONS(MODEL_TYPE ='LINEAR_REG' ) AS\nSELECT\n  feature_column1 ,\n  feature_column2 ,\n  label\nFROM\n  `mydataset .my_training_table_data `;\n350 | Chapter 9: Data Transformation and Delivery Layer",8344
127-Query Optimization.pdf,127-Query Optimization,"When calling the model, the syntax would look like this:\n-- SQL\nSELECT *\nFROM\n  ML.PREDICT(MODEL `mydataset .my_model `, (\n    SELECT\n      feature_column1 ,\n      feature_column2\n    FROM\n      `mydataset .my_test_table_data `\n  ));\nBefore moving to the next section, an essential thing to keep in mind is that data\nquerying needs can evolve and change over time. It’s impractical and costly to switch\ndata storage systems due to query limitations. Therefore, it’s essential to consider the\nbroader context, prioritize the needs of the business and data consumers, and develop\na strategy to manage both current and future query requirements effectively.\nQuery Optimization\nOnce you’ve identified your querying needs and patterns, the next step is to think\nabout and implement a query optimization strategy. If queries are not optimized,\ntheir performance can be significantly impacted. For simplicity, I will define perfor‐\nmance in terms of query speed and cost. A query optimization strategy can be\napproached from two perspectives: the database side and the user side. Let’s explore\neach in detail.\nDatabase-side query optimization\nOn the database side, there are three main query optimization techniques: indexing,\npartitioning, and clustering. Indexing is the most common technique. To understand\nthe need for indexing, let’s illustrate its main use case. Imagine you have a database\nthat contains price data for 1,000 stocks. Y ou want to run a time series query that\nfetches data for one stock (stock A). The database engine doesn’t know where the data\nfor stock A is physically located on disk. For this reason, the only way to execute the\nquery is to read all data for all stocks and keep stock A ’s records only. This type of\ndata read is called full-scan , as the entire table is scanned. As you can imagine, this is\nan expensive operation, given the time it takes to do a full scan. Can we do better?\nY es, we need to add an index!\nAn index is an optimized data structure that holds information about the disk loca‐\ntion of the data stored in your database. When the right index is added, the database\nengine will try to use it to look up the location of the data and avoid querying unnec‐\nessary records. Now you might wonder, how do I know which index to add? The\nanswer depends on your querying patterns. A rule of thumb is to add an index on the\nData Querying  | 351\n3Logarithmic speed, denoted as O(ln(n)), is widely regarded as efficient in terms of performance. To illustrate\nthis concept in simple terms, imagine you have a very large table and you’re searching for a specific row. Now\nsuppose that you perform your search in multiple iterations, and at each iteration, you divide the table in half\nand only search one half. This approach dramatically reduces the search space with each iteration. Any algo‐\nrithm that operates in this manner achieves logarithmic complexity. While structures like B-trees are more\ncomplex, they share the same underlying strategy of efficiently narrowing down search areas to quickly find\ndesired data.columns that you use in your WHERE  clause. For example, let’s consider the following\nquery:\n-- SQL\nSELECT time_column , attribute_1 , attribute_2  \nFROM financial_entity_table  \nWHERE entity_name  = 'A' \nAND date BETWEEN '2022-01-01'  AND '2022-02-01'\nAs you can see, the WHERE  clause uses two columns: entity_name  and date . For this\nreason, it makes sense to add an index on both columns. In this case, the index is\ncalled composite  as it is created on multiple columns. If your query were to only filter\nby entity_name , then it’s better to create a single-column index instead. Now you\nmight ask, how do I know what type of index I need to add? There are indeed a vari‐\nety of database indexes. To illustrate with an example, let’s take PostgreSQL as a refer‐\nence as it has a large variety of index types .\nThe standard index implementation in PostgreSQL is B-tree,  a tree-based data struc‐\nture that achieves logarithmic complexity.3 The reason behind the popularity of B-\ntree indexes is their ability to handle a wide range of query operators: less than ( <),\nless than or equal to ( <=), equal to ( =), greater than ( >), greater than or equal to ( >=),\nand between. Other types of data storage systems, such as MongoDB and Neo4,j use\nB-tree as their default index implementation. Therefore, unless you have any special\nquery needs, the B-tree is generally a good indexing strategy. Keep in mind, however,\nthat indexes are stored on disk and grow in size as your data does. In addition,\nindexes get updated as you update, insert, and delete records from the database. For\nthis reason, you must aim to create the least number of indexes when possible.\nOther types of indexes are often used for more specific use cases. For example, the\nBlock Range Index  (BRIN) is a lightweight index that can be used with very large\ntables, where the indexed columns have a strong correlation with the physical order\nof data in the table. A typical scenario might involve a large stock price time series\ndataset where data is organized by date. When performing time series queries involv‐\ning a wide temporal range (e.g., last month), PostgreSQL can utilize the BRIN index\nto quickly identify and skip over data blocks that do not contain relevant dates.\nIn some cases, indexes such as B-tree and BRIN are complemented with clustering ,\nwhich involves physically rearranging data based on one or more columns, often\n352 | Chapter 9: Data Transformation and Delivery Layer\nthose used to create an index. This rearrangement is advantageous when your queries\nfrequently access a range of ordered values, such as in time series queries. It’s impor‐\ntant to remember that clustering is a one-time operation. If the data is updated, re-\nclustering will be required.\nLast but not least, a powerful database optimization technique is partitioning . It works\nby dividing the data into logical and physical partitions/tables using one or more par‐\ntition keys. Queries filtering by these keys will exclusively scan relevant partitions,\nminimizing unnecessary data access. A partition key is often chosen to minimize the\nnumber of scanned partitions. For example, suppose you have high-frequency stock\nprice data that arrives daily, and assume that most of your queries include a day range\nfilter (e.g., the last three days, first 10 days of last month,...). In this case, it makes\nsense to partition your table by date to have one partition for each day. This way, even\nif you have 1,000 partitions, if you query data for one date, then only one partition\nwill be scanned.\nQuery Planner\nY ou might ask yourself, how do databases figure out the most optimal way to execute\na query and choose which index and scanning strategy to perform? This is all done by\nthe so-called query planner/optimizer.\nEach database system implements its own query planner, which is responsible for fig‐\nuring out the most efficient execution plan for each user query. Typically, query plan‐\nners represent queries as trees, allowing for multiple execution strategies that yield\nidentical results. These trees are typically read from the bottom up, with data being\nretrieved initially from one or more locations, and then aggregated as you move\nupwards to produce the final results. The query planner’s goal is to select the optimal\nplan for execution, which may involve substituting the original query with one or\nmore optimized versions to enhance performance. Some databases implement more\ncomplex techniques. For example, PostgreSQL ’s query planner might resort to an\nadvanced optimization technique based on genetic algorithms when planning for\ncomplex queries (e.g., when there is a large number of join operations).\nCommon strategies that query planners often rely on include the following:\nSequential scan\nScans the entire table. This is used when the table is small or if indexing is not\nproperly done.\nIndex scan\nPerforms an index traversal to find all matching records. The planner might per‐\nform an index scan if any index satisfies the WHERE  condition.\nData Querying  | 353\nIndex-only scan\nPerformed if all queried columns are part of an index, returning tuples directly\nfrom index entries.\nParallel scan\nMultiple processes fetch a subset of the requested data simultaneously, thereby\nspeeding up query execution.\nPartition pruning\nUsed with partitioned database systems (e.g., BigQuery and Snowflake) to mini‐\nmize the number of partitions to scan.\nBlock pruning\nUsed mostly with clustered tables, and determines which blocks of data to read,\nthus saving disk access and accelerating query execution.\nMany database systems offer a command called EXPLAIN , which provides a detailed\nbreakdown of the execution plan that the query planner generates for a given query\nstatement. To read more about query planners, I recommend having a look at Post‐\ngreSQL ’s Planner/Optimizer , BigQuery’s Dremel engine , and Redshift’s query\noptimizer .\nIn conclusion, it is essential to remember that database optimization may be a time-\nconsuming and challenging task (yet fun as well!). As such, give it some serious\nthought and include the business team to ensure that all of their demands are met.\nFurthermore, be sure to provide a summary of all the technical limitations that may\narise along the way, and consider how they can affect future data and querying needs.\nUser-side query optimization\nDatabase-side optimization is only half the story; the other half concerns optimizing\nthe way users interact with the data. Y ou can have the best indexing, clustering, and\npartitioning strategy in place, but users need to follow the right querying approach to\nbenefit from such optimizations. There is no unique recipe for telling users how to\nquery the data—it’s all based on the querying needs and the database optimization\nput in place. Let me give you a few illustrative examples.\n354 | Chapter 9: Data Transformation and Delivery Layer\nScenario 1.    If you add a single-column index on column A, try to avoid queries that\ndo not reference column A in their search conditions. While there’s no guarantee the\nquery planner will use the index every time, doing so could potentially result in nota‐\nble performance gains.\nScenario 2.    If you add a composite index on columns A, B, and C, then make sure\nthat you filter by the leading (leftmost) columns such as [A], [A, B], or [A, B, C]. Fil‐\ntering by [B] or [C] or [B, C] will lead to inefficient index scans. I highly recommend\nthat you read more about this topic, and the PostgreSQL documentation page  is a\ngood place to start.\nScenario 3.    When querying, select only the necessary columns rather than perform‐\ning SELECT * . This is particularly relevant when working with column-oriented data‐\nbases that store data on disk column-wise. In this case, querying only a small subset\nof the columns will significantly improve your query speed and reduce inefficient\ndata reads.\nScenario 4.    If you use the SQL pattern matching operator LIKE , then try to anchor the\nconstant string at the beginning; for example, do column LIKE 'ABC%' , but not col\numn LIKE '%ABC' . With ABC%,  an index such as B-tree is very likely to know which\nrecords to consider as it knows what they start with, but with %ABC,  the index doesn’t\nknow which strings end with ABC, and it might need to scan the full index. Y ou can\nread more about this topic in the PostgreSQL documentation .\nScenario 5.    When processing large amounts of data, try to modularize your queries.\nFor example, say you have a very large stock price time series table and you want to\nperform a given transformation on the entire table. In this case, consider using an\nincremental loading and processing approach instead of applying your operation to\nthe entire table in one run. For instance, you can query your data one day or one\nmonth at a time and apply the transformations on each batch separately. The main\nadvantage of this approach is that in case of a failure with one batch, you don’t need\nto query the entire dataset again.\nData Querying  | 355\nScenario 6.    When performing complex queries that involve joins and aggregations,\nmake sure to process a minimal amount of data. For instance, consider a scenario\nwhere you need to get monthly transaction amounts for specific customers using two\ntables: transactions and customers. As illustrated in Figure 9-1 , one way to do this is\nby first joining the transaction and customer tables, then aggregating the results by\nmonth and customer, and finally filtering for the desired customers. One issue with\nthis approach is that you might join potentially large tables, which can be a resource-\nand time-intensive operation. A more efficient strategy would involve delaying the\njoin operation to a later stage and applying filtering up front to reduce the queried\ndataset size.\nFigure 9-1. An inefficient  versus efficient  data processing approach\n356 | Chapter 9: Data Transformation and Delivery Layer",13184
128-Data Transformation.pdf,128-Data Transformation,,0
129-Transformation Operations.pdf,129-Transformation Operations,"4For a good reference on database internals, I highly recommend Alex Petrov’s Database Internals: A Deep-\nDive into How Distributed Data Systems Work  (O’Reilly, 2019).\nBased on your institution’s requirements and the needs of data consumers, you, as a\nfinancial data engineer, should be able to anticipate and assess additional scenarios\nand use cases. I recommend consistently striving to understand and discover ways to\nimprove the efficiency of your queries. Use the EXPLAIN  command to analyze the\nquery optimizer plans, try to learn about database internals and optimization strate‐\ngies, and pick up some advanced SQL knowledge.4\nData Transformation\nOnce you have determined your querying strategy and optimized your database\naccordingly, the next step is to develop and implement your data transformations.\nThe main purpose of data transformations is to prepare data for various use cases\nacross your organization’s departments and teams. In this section, you will learn\nabout the different types of transformation operations and patterns commonly used\nin finance, along with the computational requirements essential for their\nimplementation.\nTransformation Operations\nIn its most basic form, a data transformation involves converting a raw, unprocessed\ndataset into a structured format suitable for its intended business application. Impor‐\ntantly, there isn’t a universal list of transformations that you need to apply to your\ndata. Therefore, as a financial data engineer, one of your key responsibilities will be to\ndiscuss and define the specific transformations to implement based on business\nrequirements and the needs of data consumers.\nAvoid applying any transformation directly to the raw data.\nInstead, store the raw data in an archive location and apply your\ntransformations on a copy of the data.\nI’ll provide a general overview in the following sections, outlining some of the funda‐\nmental transformations typically applied to financial data.\nData Transformation | 357\n5Some authors suggested adopting a data consumer-centric approach to data quality definition and manage‐\nment. To learn more about this topic, I highly recommend Richard Y . Wang and Diane M. Strong’s “Beyond\nAccuracy: What Data Quality Means to Data Consumers” , Journal of Management Information Systems  12, no.\n4 (1996): 5–33.Format conversion\nThe first transformation applied to financial data typically involves format conver‐\nsion. It consists of converting the data from its source format into another format that\nis easier to work with. For example, you might convert data in CSV format to SQL\ntables, or convert data in JSON format into a collection in a document database. Once\nthe data is converted into the desired format, subsequent transformations become\neasier. Figure 9-2  illustrates a common format conversion scenario in the financial\nindustry, where raw data arriving in CSV or XLSX (Excel) formats is transformed\ninto a tabular format within a relational database.\nFigure 9-2. An example of format conversion\nData cleaning\nData cleaning involves the detection and correction of data quality issues such as\nerrors, biases, duplicates, invalid formats, outliers, incorrect/corrupt values, missing\ndata, and many others.\nData cleaning is a crucial step in the transformation layer, and it should be handled\ncarefully. First of all, you need to make sure that all data quality issues are properly\nidentified and understood. This assessment can vary based on your business problem;\ntherefore, it needs to be discussed with the business team and the data consumers.5\nOnce an agreement has been reached, quality checks and corrective measures need to\nbe put in place. The nature of these remedial measures is often dependent on the\nseverity of the data quality issue. For example, in the context of fraud detection, a data\noutlier might signal potentially suspicious activity that should be investigated further.\nBut if we take intraday high-frequency price data, you could easily find outliers, but\nthey often don’t have a significant impact.\n358 | Chapter 9: Data Transformation and Delivery Layer\n6For more on this topic, check the excellent article by Lou Lindley, “Working with High-Frequency Market\nData: Data Integrity and Cleaning (Part 1)” .Deciding When to Clean Financial Data\nDetermining when and which data to clean within your financial data infrastructure\nis a critical aspect of data integrity. While it may seem intuitive to perform cleaning as\nearly as possible, this approach requires careful consideration. Often, the rationale for\ncleaning early is that downstream systems may struggle to identify data issues origi‐\nnating from upstream sources, especially if they lack the necessary contextual infor‐\nmation to diagnose the problem.\nConsider high-frequency market data as an example. While academic research often\nhighlights the importance of data cleaning and anomaly detection for such data,\nadvances in modern data infrastructure are reducing this need. Electronic trading has\ngreatly reduced data anomalies, and data feed providers receive quick feedback from\nclients—particularly trading firms that promptly test new feeds, protocols, and con‐\nnections. This immediate feedback allows for quick fixes, meaning that data feeds\nmay not always need cleaning.\nHowever, there are exceptions. For example, if you’re working with data from a redis‐\ntributor that modifies the raw data, cleaning becomes essential. This is also the case\nfor OTC markets or those with outdated systems. Additionally, data that requires\nmanual entry or scraping (e.g., SEC filings) should be cleaned to ensure accuracy.\nWhen data cleaning is applied only to historical data, but real-time data feeds are pro‐\ncessed differently, discrepancies may occur, potentially undermining the reliability of\nyour application and analysis. This issue arises because the cleaned historical data\nmight not align with real-time data due to variations in the treatment of anomalies or\nerrors.\nSeen from a different angle, it’s even more effective to delegate the identification and\nhandling of “anomalies” to downstream consumers, such as trading units. In many\ncases, these anomalies represent real market behavior that convey useful information.\nMoreover, data errors and outliers may serve as input to incorporate within your\ntrading strategies to adapt them to handle issues like market disruptions and trading\ninterruption events (e.g., gateway failures and failovers, trading and regulatory halts,\nposition limit breaches, sequence gaps, circuit breakers, and order book failures).6\nData Transformation | 359\nGenerally, there are three types of actions used to clean financial data:\nDeletion\nLow-quality records are deleted from the dataset. Examples include erroneous\nquotes, invalid prices, duplicate transactions, etc. When performing deletion,\nmake sure to consider its impact on the analytical integrity and consistency of the\ndata.\nCorrection\nLow-quality records are replaced with their correct values. For example, a nega‐\ntive price is replaced with a positive one. Corrections need to be based on well-\nthought-out assumptions about the data (e.g., a price cannot be negative). In\nsome instances, corrections may require notifying the entity that submitted\nincorrect data to resubmit it in the proper format (e.g., reporting financial data\nthat doesn’t conform to a financial messaging standard).\nEnrichment\nNew fields are added to assist in detecting or mitigating the impact of low-quality\nrecords. This approach is useful when errors are difficult to detect, allowing the\nfinal data consumer to decide how to handle them. For example, if outlier detec‐\ntion is complex, a statistical model can generate an outlier probability for each\nrecord, which is then stored in a new column.\nWhen performing data cleaning, it is important to keep in mind the two data gover‐\nnance principles of lineage and ownership (which we discussed in Chapter 5 ). Data\nlineage eliminates the possibility of data cleaning becoming a mystery box by guaran‐\nteeing visibility of the cleaning steps, decisions, and rules. Similarly, data ownership\nensures that only the data owners have the authority to determine and implement\ndata cleaning procedures, which makes assigning accountability for the data quality\nprocess more straightforward.\nData adjustments\nData adjustments are transformations applied to the data to account for specific data\ncharacteristics, events, and rules, or to produce more informative or analytically\ndesired features.\nIn finance, the most frequently applied adjustment concerns corporate actions . Such\nactions refer to important decisions made by companies that can significantly impact\ntheir stock value. The two most common types of corporate actions are stock splits\nand dividend distributions . A stock split involves the issuance of several new stocks\nfor each existing stock to increase liquidity and make it more affordable for investors\nto buy the company’s shares. For example, a 1:2 split means that each stock will be\nsplit in two, so if an investor holds 10 stocks, after the split they will end up with 20\nstocks. A stock split does not change the total market  capitalization of the company,\n360 | Chapter 9: Data Transformation and Delivery Layer\nbut it impacts the stock price. To adjust for a split, the price of the stock needs to be\ndivided by the split ratio. For example, if the price of a stock is $400 and a stock split\nof 1:2 takes place, the new stock price is $200.\nSimilarly, a dividend distribution event happens when a company decides to distrib‐\nute part of its earnings to its shareholders. In such a case, the stock price needs to be\nadjusted to take into account the paid dividends. The standard approach to dividend\nadjustment consists of subtracting the dividend amount per share from the stock\nprice. For example, if a company announces a dividend distribution equal to $1 per\nshare, and the stock price is $11, the dividend-adjusted price will be 11 – 1 = $10.\nWhen working with stock price data, pay particular attention to corporate action\nadjustments. First, determine whether the data source you are using incorporates\nthese adjustments. Second, if the data is unadjusted, ensure this is clearly communi‐\ncated and documented, and consider sourcing corporate action data separately. Some\ndata sources, such as CRSP US Stock Databases , provide both stock price data and\ncorporate actions information. There are also specialized sources for corporate\nactions, such as S&P’s Managed Corporate Actions , LSEG’s Equity Corporate Actions ,\nand NYSE’s Corporate Actions , among many others.\nAnother common technique is calendar adjustment , which modifies a financial time\nseries to remove calendar effects. For instance, the number of working days in a\nmonth can vary each year due to holidays, making it difficult to compare total pro‐\nduction across months. One solution is to adjust the dataset by calculating the\nmonthly average or considering daily figures.\nData standardization\nData standardization is a critical transformation step that seeks to store and format\ndata according to a predefined set of conventions and standards. Examples of stand‐\nardizations applied to financial data include the following:\n•Date and time are formatted using the ISO 8601—Date and Time Format stan‐\ndard , which follows the YYYY-MM-DD convention.\n•Country names are represented following the ISO 3166—Country Codes stan‐\ndard  (e.g., US for the United States of America).\n•Currencies are represented using the ISO 4217—Currency Codes standard  (e.g.,\nUSD for US dollars).\n•Table and column names are lowercase, and spaces are replaced with an under‐\nscore (e.g., first_name).\n•Monetary values are standardized  to use one numerical format (e.g., EUR 10 mil‐\nlion, EUR 10,000,000, or EUR 10000000).\nData Transformation | 361\n•Monetary values are rounded off to a specified level of precision to ensure con‐\nsistency and comparability across financial records.\n•Standardized identifiers are used for financial instruments, such as ISIN or FIGI.\nThese standardizations help maintain data quality, facilitate data integration, and\nensure compatibility across different financial systems and applications.\nHow Simple Is It to Round Financial Data?\nRounding is one of the most common transformations applied to financial data.\nInterestingly, despite its apparent simplicity, rounding might actually require careful\nconsideration. First of all, a large variety of rounding algorithms exist , each tailored to\na specific use case. Certain methods, such as Bankers’ Rounding , are quite common in\nfinance. This method aims to evenly distribute rounding errors, thereby reducing\npotential bias in the data. For example, 2.5 would be rounded to 2, while 3.5 would be\nrounded to 4.\nFurthermore, deciding on the rounding precision might depend on the specific finan‐\ncial variable and accepted market practices. For example, a common practice is to set\nthe rounding precision in line with the Minimum Price Increment  (MPI), also called\nminimum tick size. This represents the smallest possible price change in a financial\ninstrument’s price. MPIs vary depending on the asset class, market regulations, and\nthe specific trading venue. For example, in the United States, the MPI is typically\n$0.01 for stocks priced above $1, while for stocks priced below $1, it can be $0.0001.\nIn Forex markets, the term Point in Percentage  (PIP) is used to denote the smallest\namount by which the quote can change. A typical PIP is equal to one basis point, or\n0.0001, but it can vary among trading venues and Forex brokers depending on the\ncurrency pair and the lot size traded. Following this logic, you might decide to set the\nrounding precision equal to the MPI; for example, an MPI of 0.0001 means a decimal\nprecision of 4.\nIn addition, rounding might depend on whether you want to round the decimal\nplaces or significant figures. To illustrate, let’s consider a currency pair in which one\ncurrency has a much higher value than the other, say 0.00247839. It is possible to\nround this number to five decimal places, in which case it becomes 0.00248. But if we\nwant to round to five significant figures, we would get 0.0024784.\nTo further explore this topic, I highly recommend the book by Brian Buzzelli, Data\nQuality Engineering in Financial Services  (O’Reilly, 2022).\nIn today’s financial markets, a key data engineering challenge is harmonizing and\nstandardizing the diverse data formats, structures, and types from various sources.\nWhether it’s market data, transactions, or external feeds, achieving consistency and\ninteroperability is essential for accurate analysis and AI-driven insights. To give a\n362 | Chapter 9: Data Transformation and Delivery Layer\nrecent example of how markets are approaching this problem, J.P . Morgan’s Fusion, a\ncloud-native data platform for institutional investors, launched data containers in\nMay 2024. These containers use a common semantic layer to model and normalize\ndata across multiple providers, sources, and formats—giving investors a consistent,\nintegrated view of their data.\nData filtering\nData filtering is an analytical transformation step whereby a financial dataset is exam‐\nined to exclude or rearrange its records according to predefined criteria. In other\nwords, data filtering applies a filter to a dataset, transforming it into a new dataset\nbased on the filter criteria.\nA wide range of data filters are used in finance. Here are a few examples:\nCompany filter\nThis consists of excluding companies from a financial dataset that don’t satisfy\ncertain conditions. For instance, an article written by Priyank Gandhi and Hanno\nLustig  details a study using the CRSP dataset, excluding inactive firms (legal enti‐\nties without business activities) and firms incorporated outside the US to ensure\na uniform regulatory regime.\nCalendar filter\nThis filter is used to align accounting standards across firms in a dataset. For\nexample, in an article written by Laura Xiaolei Liu and Lu Zhang , they excluded\nfirms that do not have a December fiscal year-end.\nLiquidity filter\nThis filter is used to ensure that the securities included in a data sample have at\nleast a certain number of active trading days during a specific time interval. In\none research paper , the authors applied a filter to exclude stocks with less than\ntwo hundred days of active trading in a year to have a sample with enough data\nfor computing liquidity measures. Similarly, when working with option data, it is\ncommon to filter out options whose expiration falls outside a given interval (e.g.,\nbetween 10 and 400 days) as such options might behave erratically near expiry\ndue to liquidity features (the article “The Puzzle of Index Option Returns”  pro‐\nvides a good example).\nSize filter\nThis filter excludes firms whose market capitalization is below or above a certain\nvalue, for example, when studying a specific segment of the market. In one study ,\nthe authors excluded microcap stocks, defined at the fifth percentile of market\ncapitalization within each country. This was done because minicap stocks often\nsuffer from stale prices and volumes due to market illiquidity as well as their neg‐\nligible economic significance (<0.04%) on the overall market value.\nData Transformation | 363\nCoverage filter\nThis filter excludes firms that do not have enough observations in the dataset.\nThis is often done to reduce the impact of noise generated by such observations.\nFor example, the authors of a study on factors that drive global stock returns\napplied a similar filter by requiring a stock to have at least 12 monthly observa‐\ntions within the sample period to be considered for inclusion in the study.\nSector filter\nThis involves including entities belonging to a particular sector, market segment,\nindustry, or subindustry. This filter is applied because certain sectors can be sub‐\nject to special regulation or have different asset and investment structures. For\nexample, in their seminal work, The Cross-Section of Expected Stock Returns\n(Wiley), Fama and French conducted their analysis by excluding financial firms,\nas high leverage for these firms does not indicate the same thing for nonfinancial\nfirms, i.e., distress.\nWhen applying data filters, it’s essential to understand their purpose and the potential\nimpact on data quality and integrity. Improper filtering can introduce biases, such as\nnonrepresentative sample bias or imbalanced datasets. As a financial data engineer,\nyou should discuss these considerations with the end users of the data. Additionally,\nensure that filters are applied correctly and do not modify or delete the original data.\nIf a filtered dataset is needed, create a new table for it.\nFeature engineering\nFeature engineering is an advanced analytical transformation step used to extract new\nfeatures from raw data to support statistical and machine learning modeling. Feature\nengineering is often necessary when existing features do not adequately represent the\nproblem at hand. A feature is a variable or measurable quantity used as input in vari‐\nous modeling tasks. It can be numeric, categorical, binary, or text based. In database\nterms, a feature can be thought of as a new column in a given table. It is a well-known\nempirical fact that the performance of machine learning systems is heavily dependent\non the feature representation of the input data .\nThe practice of feature engineering is quite flexible; data analysts and machine learn‐\ning experts have the freedom to experiment with and derive novel features from a\ngiven dataset to represent different aspects of the data that are relevant to the situa‐\ntion at hand.\nFeature engineering can be data driven (e.g., based on statistical correlations or pat‐\nterns in the data) or domain driven (e.g., based on a financial theory). Moreover,\nfeature  engineering requirements may vary based on the type of data being analyzed.\nFor example, there is feature engineering for text data, visual data, time series data,\ngraph data, and stream data. A full account of these techniques is beyond the scope of\nthis book, and for this, I recommend the excellent reference Feature Engineering for\n364 | Chapter 9: Data Transformation and Delivery Layer\n7For a practical guide to this topic, I recommend reading Jason Brownlee’s articles on the de-trending  and de-\nseasonalization  of time series. Machine Learning and Data Analytics , edited by Guozhu Dong and Huan Liu (CRC\nPress, 2018). Nevertheless, to give it a minimal treatment, let’s go through some\nexamples of feature engineering.\nThe most general techniques for feature engineering include the following:\nNormalization\nRescaling the data to fit within a predefined range, e.g., [0–1]. This is often done\nto prevent some features from having a dominant impact during model training.\nScaling\nRescaling the data to have a similar scale, such as a standard deviation of 1, to\nensure a model considers all features equally. Common scaling techniques\ninclude min-max scaling  and standard scaling .\nEncoding\nTransforming a feature from categorical to numerical representation; e.g., female\nis 1 and male is 2. Common encoding techniques include one-hot encoding  and\nlabel encoding .\nDimensionality reduction\nTransforming a set of features from a high-dimensional space into a lower-\ndimensional one. Examples include principal component analysis  and t-SNE .\nEmbedding\nA technique used to create numerical representations of complex real-world\nobjects that machine learning systems can use in model training. As an illustra‐\ntive example, an embedding might take an image, audio, text, or a graph and con‐\nvert it into multidimensional numerical representations known as vectors. Using\nvectors, machine learning systems can efficiently process input data and identify\nsimilarities among different data items (e.g., two similar images). A special type\nof database, called a vector database , has been developed to allow ML-driven\napplications to store and retrieve vector datasets.\nIn finance, a wide range of domain-specific feature engineering techniques are often\napplied. For example, when conducting financial time series analysis, it is common to\nperform steps such as de-trending  and de-seasonalization.7 De-trending is the process\nof removing a trend cycle from the data. A trend cycle refers to a consistent increase\nor decrease of a financial time series over time. On the other hand, de-seasonalization\nremoves seasonal patterns from the time series. A seasonal pattern refers to a specific\nevent in the time series that occurs with a fixed and known frequency such as daily,\nData Transformation | 365\n8In financial data, several seasonal behaviors have been observed. For example, the January Effect  refers to a\nseasonal pattern where prices of stocks tend to increase during the month of January of each year.\n9To read more about this topic, I recommend John Y . Campbell, Andrew W . Lo, and A. Craig MacKinlay’s The\nEconometrics of Financial Markets , vol. 2. (Princeton University Press, 1997).\n10For more on this topic, see William M. Cready and Ramachandran Ramanan’s “The Power of Tests Employ‐\ning Log-Transformed Volume in Detecting Abnormal Trading” , Journal of Accounting and Economics  14, no. 2\n(June 1991): 203–214.weekly, every January, etc.8 By applying de-trending and de-seasonalization, new fea‐\ntures are created.\nAnother widely used technique in finance is stationary differentiation. This involves\ncalculating the difference between each consecutive pair of observations over time\n(i.e., xt−xt− 1,xt− 1−xt− 2, . . . . ). It is commonly used to transform a non-\nstationary financial time series into a stationary one. In simple words, a stationary\ntime series is a series whose statistical properties do not change over time. Stationar‐\nity is a desired property, as it makes it simpler to perform data analysis using classical\nstatistical methods (e.g., inferential statistics). Converting a price time series to a\nreturn series is one such application of differentiation that financial analysts often\nperform. This is done by taking the percentage difference between two consecutive\nprices (i.e., xt−xt− 1/xt− 1).9\nIn addition, a popular feature engineering technique applied by financial analysts is\nlog transformation, where each value x is replaced with log(x). Expressing data on a\nlogarithmic scale can be helpful for analytical purposes. For example, if a price time\nseries is expressed in a log scale, the difference between two log prices approximates\nthe percentage change (i.e., logpt−logpt− 1∼pt−pt− 1/pt− 1). Furthermore,\nthe log transformation is frequently used to transform skewed financial data to con‐\nform to normality, which is a desired feature in financial analysis.10\nFinally, an advanced form of feature engineering in finance is the creation of factors.\nIn financial investment literature, “factors” refer to common asset characteristics that\nexplain variations in returns and risks across stocks, bonds, and other assets. They\ncan explain why certain assets go up or down at the same time and why certain assets\nmay yield higher returns compared to others.\nThere are two main types of factors: macroeconomic factors and style factors. Macro‐\neconomic factors capture risks that affect broad segments of the financial markets\nand impact multiple asset classes simultaneously. Examples include interest rate (the\nimpact of interest rate changes), inflation (the effect of price level changes), and\neconomic  growth (variations in the business cycle). Style factors, on the other hand,\nexplain returns and risks within individual assets or asset classes. Examples include\nvalue (assets undervalued relative to their fundamentals), momentum (assets with\nupward price trends), low volatility (assets with a lower risk profile), quality (assets of\n366 | Chapter 9: Data Transformation and Delivery Layer\n11Factors are often employed in an investment strategy called factor investing. To learn more about this, I rec‐\nommend David Blitz and Milan Vidojevic’s “The Characteristics of Factor Investing” , Journal of Portfolio\nManagement  45, no. 3 (2019): 69–86.financially robust companies), and growth (assets or companies with strong earnings\ngrowth potential). These factors are engineered features derived from fundamental or\nmarket data to enhance portfolio returns and manage risk.11\nAdvanced analytical computations\nAn advanced type of data transformation may involve the computation of one or\nmore quantities based on an algorithm or a model. Generally speaking, financial data\nengineers seldom perform scientific modeling or develop machine learning algo‐\nrithms. However, with the rise of data products, data platforms, and analytics engi‐\nneering, there are contexts where financial data engineers, particularly those with\ninterdisciplinary backgrounds, might become involved in the modeling process.\nExamples of financial applications that require advanced computations include the\nfollowing:\n•Algorithmic trading\n•Financial recommender systems (e.g., robo advisors)\n•Fraud detection\n•Anti-money laundering\n•Named entity recognition\n•Entity resolution\n•Optical character recognition (e.g., recognizing the digits on a credit card image)\nTo develop such systems, financial data engineers may need to handle tasks such as\ndata collection, cleaning, quality assurance, and control checks. They are also respon‐\nsible for selecting suitable DSMs and DSSs tailored to each application’s require‐\nments. Furthermore, financial data engineers may be involved in building machine\nlearning pipelines, deploying them in production, and collecting metrics and model\nartifacts.\nData Transformation | 367",28133
130-Transformation Patterns.pdf,130-Transformation Patterns,"Transformation Patterns\nThe next stage after defining your transformation operations is to design and imple‐\nment your transformation patterns. A transformation pattern defines how a given\ndata infrastructure handles and performs transformations. To illustrate the concept,\nlet’s have a look at a few examples.\nBatch versus streaming transformations\nIn batch transformation, data is divided into discrete chunks (batches) that undergo\nseparate processing. A data chunk can be defined in various ways. For instance, if\ndata is received in file formats like CSV or JSON, each file can be treated as a chunk\nand processed individually. Alternatively, if the files can be grouped based on specific\ncriteria (e.g., by date), a chunk might encompass all files belonging to a particular\ngroup (e.g., all files for a given day). Batch file transformation is a common practice\nin finance, especially when data is delivered through files. Examples include financial\ndata vendors, which distribute their data in daily, weekly, and monthly file batches,\nand financial reporting, which is often done via files submitted to the regulator’s data\ninfrastructure.\nBatch transformations are often used with scheduled data arrival processes, which we\ndiscussed in Chapter 7 . In this case, the batch transformation is scheduled to run on a\npredefined interval (e.g., hourly, daily, weekly). Alternatively, a batch can be trans‐\nformed once it is complete. For instance, a file batch with a maximum size of 10 will\nbe transformed when it contains 10 files.\nWhen data needs to be transformed as soon as it arrives, and not wait for a batch to\ncomplete, then a streaming transformation  is used. In this case, there is no fixed\nschedule for data transformation; it runs continuously and processes data immedi‐\nately upon its arrival. Streaming transformations are most commonly used with\nevent-driven arrival processes, which were also covered in Chapter 7 . This is the case,\nfor example, with real-time financial applications such as payments, fraud detection,\nprice feeds, and news. In these applications, the system needs to respond with mini‐\nmum latency and, therefore, cannot wait for a batch to complete.\nStreaming transformations are often performed on independent data. For instance, in\na scenario where a bank allows clients to submit loan applications online, each appli‐\ncation can be processed immediately upon submission, without the need to wait for\nother applications to form a batch.\nIn certain scenarios, data arrival may occur in real time or be event driven, yet\nstreaming transformation may not be necessary. For instance, when bank clients fill\nout a questionnaire to evaluate their financial literacy, the data does not require\n368 | Chapter 9: Data Transformation and Delivery Layer\nimmediate processing. Instead, it can be processed in batches at regular intervals or as\nsoon as a batch is complete. Using batch transformation to process real-time or\nevent-driven data is a common practice employed to handle massive volumes of data\nor reduce the costs of continuous monitoring for new data arrivals.\nFigure 9-3  illustrates how batch and streaming transformations work. In batch trans‐\nformation (top), data files arrive and get ingested into a data lake. Subsequently, files\nare grouped in separate batches based on date. After that, each batch is transformed\nseparately in the transformation layer. Once transformed, the data is stored in a given\ntarget location, such as a warehouse. In the streaming transformation case (bottom),\ndata arrives in JSON format. Each JSON gets immediately ingested into a message\nbroker. Subsequently, each message is processed as soon as possible in the transfor‐\nmation layer and stored in the final data warehouse.\nFigure 9-3. Standard batch and streaming transformation patterns\nData Transformation | 369\n12For an insightful overview of financial markets, I recommend reading Nikita Ivanov, “In-Memory Computing\nCan Digitally Transform Financial Services and FinTech Applications for Capital Markets” , Forbes , January 5,\n2021.Memory-based versus disk-based transformations\nA given data transformation, whether batch or streaming, can be processed either\ncompletely in memory or involving intermediary disk persistence steps. The best way\nto show the difference is with an illustrative example, as shown in Figure 9-4 . A basic\ndisk-based transformation is illustrated in the figure’s upper part. In this setting, files\nare first ingested into a data lake. Subsequently, the transformation layer applies two\ntransformation iterations on the files (e.g., cleaning + feature engineering) and saves\nthe final results in a data warehouse. Note, however, that between the two iterations,\nthe transformation layer had to store intermediary results back to the data lake and\nread it again for the second iteration. In the lower part of the figure, a memory-based\ntransformation does the same thing, but instead of saving intermediary results to the\ndata lake, it keeps it in memory and passes it as such to the next interaction.\nFigure 9-4. Disk-based versus memory-based data transformations\nY ou might be wondering by now why this is necessary. The answer lies in the signifi‐\ncant difference in data access speed between RAM and disk (random disk access, to\nbe precise). This is the reason why in-memory software solutions for data storage and\nprocessing are quite popular.12\nMany financial applications rely on memory-based data transformations, especially in\ntime-critical scenarios like trading and fraud detection. For instance, in high-\nfrequency and algorithmic trading, real-time data from feeds is processed directly in\nmemory for immediate action. If data were to be stored on disk first, then the speed\nadvantage would be lost.\n370 | Chapter 9: Data Transformation and Delivery Layer\nDisk-based access modes don’t all operate slowly. When compared\nto RAM access, random disk access—where data is retrieved at ran‐\ndom locations on disk—is especially sluggish. Sequential disk\naccess, on the other hand, is very quick since data records are \nretrieved in a predetermined order. Sequential disk access is lever‐\naged by technologies like Apache Kafka  in order to achieve high-\nperformance data read/write operations.\nApache Spark: An In-Memory Computing Framework\nWhen discussing data computing frameworks, it is difficult to avoid mentioning\nApache Spark,  a unified framework for large-scale data analytics. A bit of history is\nimportant to help understand the emergence of Spark. Early efforts at processing\nlarge data volumes led to the development of Apache Hadoop , a rich ecosystem of\nopen source tools designed for managing multinode clusters and processing massive\ndatasets. Hadoop’s ecosystem incorporates two key components: the Hadoop Dis‐\ntributed File System (which we discussed in Chapter 8 ) for distributed storage with\nhigh-throughput access to data, and MapReduce for large-scale parallel data process‐\ning. Crucially, MapReduce’s internal design processes data on disk, which was a major\ndownside in terms of performance. In response, Apache Spark emerged as the next\nevolution in the Hadoop ecosystem. Compared to Hadoop MapReduce, Spark has\ndemonstrated significant performance improvements .\nOne of Spark’s primary advantages is its native ability to perform computations in\nmemory. While Spark can handle computations on disk when data exceeds memory\ncapacity, data engineers typically configure sufficiently large Spark clusters to enable\nin-memory processing. Spark’s core memory data abstraction is known as the Resil‐\nient Distributed Dataset  (RDD), which is immutable and can be distributed across\nnodes within a Spark cluster.\nSpark is a rich framework that combines Spark Core (execution engine), Spark SQL\n(for structured data querying and processing), Spark Streaming (for streaming pro‐\ncessing), the Spark machine learning library, MLib (for building machine learning\nmodels), and GraphX (for graph data processing).\nFurthermore, Spark seamlessly integrates with several programming languages such\nas Scala, Java, Python, and R, allowing developers to write Spark programs in any of\nthese languages. Among data engineers, the Python API known as PySpark is particu‐\nlarly popular and widely used.\nSpark can be deployed on premises and on the cloud (e.g., using a cluster of Amazon\nEC2 instances). Managed cloud solutions such as Amazon EMR allow users to con‐\nfigure a Spark cluster without the need to deal with cluster and node configurations.\nAdditionally, some cloud-based AI and analytics frameworks are powered by Apache\nSpark, for example, Databricks and Azure Synapse Analytics.\nData Transformation | 371\n13For more on this topic, I recommend reading A. Madhavi and T. Sivaramireddy’s “Real-Time Credit Card\nFraud Detection Using Spark Framework” , in Machine Learning Technologies and Applications: Proceedings of\nICACECS 2020  (Springer, 2021): 287–298.Apache Spark has multiple applications in finance. For example, it can be used to per‐\nform fast data transformations that involve massive financial datasets, such as high-\nfrequency trades and quotes. Fraud detection is another use. For an illustrative\narchitecture design, consider starting with a Spark ML model, such as logistic regres‐\nsion, designed for fraud detection. Data begins by entering an event stream through a\nmessage broker like Kafka, continues to Spark Streaming for real-time processing,\nand finally undergoes fraud verification using the deployed Spark ML model.13\nAnother scenario that requires deciding between in-memory and disk-based trans‐\nformations involves choosing whether to perform transformations dynamically in\nmemory or precomputing and storing them in a database beforehand. This can be the\ncase, for example, when performing feature engineering. Doing feature engineering\ndynamically in memory can be advantageous for large and changing datasets, ena‐\nbling real-time processing without the overhead of storing and managing precompu‐\nted features. This approach allows flexibility in adapting to changing data\nrequirements and quality, but it may require substantial computational resources\n(e.g., RAM) and execution time. On the other hand, precomputing and persisting fea‐\ntures in a DSS can enhance performance by reducing computation time and memory\nconsumption during model training and inference, which is optimal in scenarios\ninvolving expensive queries and complex feature engineering steps. Furthermore, this\napproach ensures consistency and reproducibility of features across different stages of\nmodel development and deployment. Therefore, the choice should be guided by bal‐\nancing computational efficiency, reproducibility, traceability, data freshness, and scal‐\nability needs.\nFull versus incremental data transformations\nIn a full data transformation pattern, the entire dataset (or its complete history) is\ntransformed in one go, regardless of whether parts of it have been modified. This\napproach is typically used with small datasets that don’t change frequently. Its main\nadvantages are simplicity (no need for sophisticated logic to process parts of the\ndata), consistency (the entire dataset is processed uniformly), and simpler error han‐\ndling (if an error occurs during transformation, it’s easy to detect since the entire\noperation will fail). This approach has drawbacks such as being resource intensive\n(especially with large datasets), limited scalability (if the dataset becomes large, pro‐\ncessing it as a whole can become impractical or time-consuming), and potential\nlatency issues (which may affect time-critical applications).\n372 | Chapter 9: Data Transformation and Delivery Layer\nAlternatively, incremental data transformation involves transforming only new or\nupdated data, rather than the entire dataset. This approach is commonly used with\nlarge datasets or applications that continuously generate and update data. Its main\nadvantages include resource efficiency, scalability, low latency, and reduced costs.\nNote, however, that incremental data transformation may introduce additional com‐\nplexity, as it requires implementing a change detection mechanism (e.g., Change Data\nCapture), data ingestion logic to handle only new or updated records, and data pro‐\ncessing logic capable of inserting or updating records without overwriting existing\nones.\nChange Data Capture\nChange Data Capture  (CDC)  is an essential concept in data engineering. It refers to\nthe capability of a data infrastructure to detect changes—such as inserts, updates, or\ndeletes—in an upstream data source and propagate them across downstream systems\nthat consume the data. A common scenario is the propagation of changes from an\noperational database to an analytical system such as a warehouse or a data lake. CDC\nis extensively used to ensure data consistency and integrity across various systems,\nsupport up-to-date real-time analytics, and help organizations migrate from on-\npremises to the cloud.\nA CDC mechanism can operate in either a push or pull manner. In a push-based\nCDC, the source data storage system sends data changes to downstream applications.\nIn a pull-based CDC, downstream applications regularly poll the source data storage\nsystem to retrieve data changes.\nSeveral methods exist for implementing CDC. A straightforward approach involves\nadding a timestamp column that records the time of the latest changes. Downstream\nsystems can then capture updates by querying data with timestamps greater than the\nlast extracted timestamp. The main disadvantage of this method is that it may not\neffectively capture deletes.\nAnother common method relies on database triggers. A trigger is a stored procedure\nin a database that executes a specific function when certain events, such as inserts,\nupdates, or deletes, occur. Triggers can propagate changes immediately but may add\nadditional load to the source database. Some data storage solutions, such as Mongo‐\nDB, implement a separate trigger layer that scales independently of the database\nserver.\nFinally, a highly reliable CDC approach involves using database logs. To ensure dura‐\nbility and consistency, many database systems log all changes into a transaction log\nbefore persisting them to the database. Logs are quite reliable as they capture all\nchanges made to the data, along with metadata about who made the changes and\nwhen. The main challenge with this approach is the potential complexity of setting it\nup and maintaining it.\nData Transformation | 373",14718
131-Computational Requirements.pdf,131-Computational Requirements,"Computational Requirements\nWhen designing the transformation layer, you must assess and plan your computa‐\ntional requirements. The main factors to consider are computing environments and\nperformance.\nComputational performance\nHaving a framework that outlines performance requirements and optimization\nstrategies is essential for a reliable transformation layer. This framework should\ninclude features such as computational speed, throughput, efficiency, and scalability.\nComputational speed.    Speed is a critical requirement for financial data transforma‐\ntions. It is typically measured by the time difference between the start and end time of\na data transformation execution. A related and more specific concept is latency ,\nwhich denotes the time it takes for a signal or message to travel over a network to its\ndestination and receive a response back from it. Common latency metrics include the\naverage latency , which is the mean time it takes for two systems to exchange mes‐\nsages, and latency jitter , which is the variation in latencies around the average value.\nThroughout the rest of this section, I will use the term computational speed to\ndescribe the difference between the expected and the actual processing time of a\ngiven data transformation. This presupposes that you, as a financial engineer, should\ndiscuss and determine the ideal execution time for the many financial data transfor‐\nmations and keep an eye on your infrastructure to make sure it performs up to par.\nThis can be linked to the data timeliness dimension that you learned about in\nChapter 5 .\nIn general, the higher the computational speed you aim for, the\nmore technically challenging it becomes. It is simpler to reduce\ncomputation time from hours to minutes than from seconds to\nmicroseconds. For this reason, treat it as an economic problem by\nestimating the marginal gain from every unit of improvement in\ncomputational speed. Certain types of financial data transforma‐\ntions (e.g., monthly price extractions) may not require significant\ncomputational speed.\n374 | Chapter 9: Data Transformation and Delivery Layer\nTo guarantee fast data transformations, a common practice involves the definition of\na Service-Level Agreement (SLA) that outlines the average duration of data transfor‐\nmations (e.g., five seconds) or the time by which a specific data transformation\nshould have been finished (every Friday at 10 a.m.).\nThe Cost of Speed in Financial Markets\nSpeed in financial markets comes with its own set of risks. For instance, the drive\ntoward instant payment and settlement processes increases exposure to fraud risks, as\nthe system has limited time to analyze, detect, and prevent fraudulent activities and\nensure compliance. Moreover, this can expose participants to liquidity, market, and\ncredit risks. For example, during settlement, financial institutions must ensure they\nhave sufficient liquidity to handle instant payments, particularly during peak times.\nInstant payments also expose market participants to credit risk if counterparties fail\nto settle transactions promptly.\nHigh-frequency trading is another area where speed is key, with rapid data access and\nquick decision-making being critical elements. However, relying on real-time market\ndata for fast trading might, in some cases, be risky because the data may contain\nerrors or noise, leaving the trading system with limited time to properly assess the\nquality of the data.\nFurthermore, fast trading systems like high-frequency and algorithmic trading can\namplify market volatility, particularly during periods of stress. In addition, the pursuit\nof speed has contributed to events like flash crashes, where market prices drop\nsharply and recover within minutes. The reliance on sophisticated algorithms and\nhigh-speed trading infrastructure increases the risk of technical failures, such as soft‐\nware bugs, hardware malfunctions, or connectivity issues.\nFast order execution in trading poses challenges for regulatory compliance as well.\nFor instance, rapid order cancellations or modifications by high-frequency traders\ncan strain market surveillance systems responsible for upholding fair market practi‐\nces and preventing market manipulation.\nThese examples highlight the distinctive characteristics of data engineering in finance\ncompared to other industries. While speed may not pose major risks in certain sec‐\ntors, in finance, rapid transactions and decisions can introduce substantial risks that\nmust be carefully considered when designing financial data infrastructures.\nData Transformation | 375\nOnce an SLA is defined, the next step would be to compare the actual data transfor‐\nmation time against the SLA specification. Should the SLA be violated, you, as a\nfinancial data engineer, need to understand the main causes and propose potential\nsolutions. Factors that might impact data transformation execution time include the\nfollowing:\n•Low-quality data that requires more steps to clean\n•Poor querying patterns that take more time than necessary\n•Dispersed data that requires multiple queries to collect\n•A wrong data storage model (e.g., using a data lake instead of a data warehouse\nfor structured data)\n•Large batch jobs\n•Limitations on the database side (e.g., max concurrent connections)\n•Poor database design (e.g., missing or bad indexes)\n•Too much complexity in the transformation logic\n•Insufficient compute resources (e.g., low RAM or CPU)\n•Shared resources that need to respond to requests coming from a variety of appli‐\ncations\n•Bad queuing strategy (e.g., jobs are not ordered and executed in the right priority,\nbig or time-consuming batch jobs run before small interactive requests, lack of\nasynchronous or parallel execution capabilities)\n•Wrong transformation patterns (e.g., executing data transformations with syn‐\nchronous blocking can slow down performance compared to asynchronous\nevent-driven or streaming solutions; more on this in Chapter 11 )\n•Network issues\n•Too many processing layers\n•Inefficient data distribution among data centers (data is far from the processing\nengine)\nIdeally, you should anticipate and test for such issues in advance. As discussed in\nChapter 8 , changing your data storage system is a costly operation that should be\navoided. Similarly, once your data transformation layer is defined and implemented,\nyou don’t want to migrate or alter it.\n376 | Chapter 9: Data Transformation and Delivery Layer\n14For more context regarding this issue, I recommend the paper by John W . Lockwood, Adwait Gupte, Nishit\nMehta, Michaela Blott, Tom English, and Kees Vissers, “ A Low-Latency Library in FPGA Hardware for High-\nFrequency Trading (HFT)” , in the 2012 IEEE 20th Annual Symposium on High-Performance Interconnects\n(IEEE, 2012): 9–16.\n15For more on database throughput, see Felipe Cardeneti Mendes, Piotr Sarna, Pavel Emelyanov, and Cynthia\nDunlop’s Database Performance at Scale: A Practical Guide  (Apress, 2023): 254.Throughput.    In data-intensive applications, an important performance measure is\nthroughput. This refers to the amount of data that a system can process in a given\ntime interval. Applications designed for high throughput emphasize the total amount\nof work that can be done in a given period of time. This is often measured using met‐\nrics such as bits per second (bit/s or bps), megabytes per second, or records per\nsecond.\nIn today’s financial markets, especially trading systems, high throughput is essential\ndue to the continuous influx of a massive number of orders that require immediate\nprocessing.14 From an engineering perspective, it’s crucial to define the desired level\nof throughput clearly. Simply aiming for “high throughput” is not specific enough. As\nsuch, it’s essential to define a target throughput level through discussions with your\nbusiness team and stakeholders.\nThe level of throughput may depend on a variety of factors, including the characteris‐\ntics of the physical and networking infrastructure, the size and type of the data being\nprocessed in each request, and the type of operation being performed. For instance,\nwhen optimizing database read/write throughput, it is important to keep in mind that\nreading and writing have different internal mechanisms.15\nComputational efficiency .    The term computational efficiency is used to describe how\nwell data transformations make use of available resources. An efficient data transfor‐\nmation minimizes resource consumption, which is increasingly important given the\ncurrent emphasis on the environmental impact of data computations, such as carbon\nfootprint.\nThe term algorithmic efficiency  is often used in this context to measure the computa‐\ntional resources that an algorithm uses. Generally speaking, the more time and space\nan algorithm consumes, the less efficient it is. Interestingly, in 2020, a group of\nresearchers from MIT’s Computer Science and Artificial Intelligence Laboratory\n(CSAIL) presented evidence showing that data-intensive tasks benefit more from\nalgorithmic efficiency than hardware improvement .\nData Transformation | 377\nHigh-Performance Computing in Finance\nFinance has a lot of computationally expensive problems. Examples include risk valu‐\nation, derivative pricing, stress testing, scenario analysis, and Credit Value Adjust‐\nments (CV As). Such problems are computationally expensive due to their nonlinear\nand high-dimensional nature, which results in a massive number of computations\nthat need to be performed. I won’t go into the details of these problems, but for those\ninterested, I suggest reading the S&P Global blog entry “ Accelerating CV A Calcula‐\ntions Using Quasi Monte Carlo Methods”  on accelerating CV A calculations that illus‐\ntrates the complexity of computing CV As.\nTo address such computational challenges, the industry has responded with two\nstreams of improvement: software-side and hardware-side improvements. Software-\nside improvements often entail the development of more efficient algorithms and\ncomputational models. On the hardware side, financial markets have expressed inter‐\nest in what is called High-Performance Computing  (HPC). HPC refers to the practice\nof aggregating and connecting multiple computing resources to solve complex com‐\nputation problems efficiently. HPC isn’t just one technology, but rather a model to\nbuild powerful computing environments .\nDesigning an HPC cluster leverages several concepts and technologies such as parallel\ncomputing, distributed computing, virtualization, Central Processing Units (CPUs),\nGraphics Processing Units (GPUs), in-memory computation, networking, and many\nmore.\nAn HPC cluster can be implemented in a variety of ways. For example, an Apache\nSpark cluster can be considered an HPC. A cluster of connected EC2 instances is also\nan HPC environment. More complex forms of HPC clusters combine heterogeneous\ntypes of machines that serve different purposes (e.g., compute-optimized with high\nCPU count, memory-optimized, GPU-accelerated). Some organizations develop\ncustom-built HPC supercomputers, highly optimized for superior speed and perfor‐\nmance compared to standard computing systems. Supercomputers are often bench‐\nmarked using the floating-point  operations per second  (FLOPS) measure .\nFor more on HPC in finance, see Michael Alan Howarth Dempster, Juho Kanniainen,\nJohn Keane, and Erik Vynckier, eds., High-Performance Computing in Finance: Prob‐\nlems, Methods, and Solutions  (CRC Press, 2018).\n378 | Chapter 9: Data Transformation and Delivery Layer\nScalability.    Computational performance is very often dependent on the scalability\nfeatures of the underlying infrastructure. When there is an increase in the number of\ndata transformations or sudden peaks in workload, the computational capacity needs\nto scale proportionally to efficiently manage the increased load.\nTaking a cloud context as a reference, scalability can be achieved by either adding\nmore resources (e.g., additional EC2 instances or cloud functions) or by upgrading\nthe capacity of existing resources (e.g., replacing smaller EC2 instances with larger\nones). Scalability can be implemented following different strategies, such as the\nfollowing:\nManual scaling\nWhere you manually scale existing resources to meet new demands\nDynamic scaling\nWhere you configure an autoscaling policy to automatically provision more\nresources in reaction to larger demands (e.g., if total CPU usage > 90% → provi‐\nsion X resources)\nScheduled scaling\nWhen you configure an autoscaling policy to provision more resources on a\ngiven schedule (e.g., every Wednesday between 9 a.m. and 4 p.m., provision 200\nmore EC2 instances)\nPredictive scaling\nWhere a machine learning model is used to predict and provision resources\nbased on historical usage patterns\nScalability requires careful consideration. In particular, when planning the scaling\nrequirements for the transformation layer, it’s important to also assess the scalability\nfeatures of the storage layer. For example, if you have a max concurrency limit on the\ndata storage layer (e.g., 500 concurrent requests), it’s essential to properly manage the\nnumber of concurrent requests arriving from the transformation layer to avoid over‐\nloading the storage layer. Ideally, the design of both the storage and transformation\nlayers should be iterative, ensuring compatibility in scalability features to achieve\noptimal performance.\nComputing environments\nOnce you have identified your computational requirements, the next step is choosing\na computing environment that aligns with these specifications. Such an environment\ntypically comprises three key components: software (e.g., operating system, program‐\nming language, libraries, frameworks), hardware (e.g., storage, RAM, CPU, GPU),\nand networking (e.g., TCP/IP , VPC, etc.).\nData Transformation | 379\nThere is a large variety of computing environments that can be configured to run the\ntransformation layer tasks. Traditionally, many financial institutions, in particular\nbanks, have relied on mainframe computing environments  that run on languages\nsuch as Common Business Oriented Language (COBOL). However, modern financial\napplications are leveraging open source technologies and the cloud as a viable and\nsimpler alternative.\nIf we choose to leverage cloud infrastructure, there are several approaches to consider\nwhen setting up the computing environment for the data transformation layer. The\nmost flexible strategy is infrastructure-as-a-service (IaaS), where you provision and\nconfigure several computing machines to perform the data transformations. In this\nsetting, you will be responsible for installing the required programming languages\n(e.g., Python, Java, Go, etc.), configuring security policies (e.g., ingress and egress\nrules), and installing all necessary packages (e.g., PySpark, Apache Airflow, etc.). The\nmain advantage of IaaS is the extra control it offers over environment configuration.\nHowever, managing, configuring, and securing your instances under IaaS can poten‐\ntially lead to a waste of time and effort.\nAs an alternative to IaaS, cloud providers offer several managed services that alleviate\nthe need to configure and maintain the compute instances. In such settings, the user\ninteracts with a declarative interface where they define the desired configuration of\ntheir environment. The cloud provider takes responsibility for provisioning, scaling,\nand managing the underlying infrastructure, offering greater convenience and often\nreducing administrative overhead for users compared to the IaaS model. For example,\nAWS offers Managed Workflows  for Apache Airflow  (MW AA), a managed service for\nrunning the data workflow management solution Apache Airflow (which we will dis‐\ncuss in “Extract-Transform-Load Workflows” on page 414 ).\nAnother popular choice for cloud-based computing environments is serverless cloud\nfunctions, which allow users to deploy and run code in a variety of languages without\nprovisioning or managing servers. Cloud functions are ideal for short-time data\ntransformation operations (several seconds or minutes). Examples include AWS\nLambda and Google Cloud Functions (first and second generations). One of the main\nadvantages of cloud functions is that they can be integrated with a wide range of\nother services and handle event-based workloads. For example, it is possible to con‐\nfigure a cloud function to run upon the arrival of a file in storage (e.g., Amazon S3),\nupon the arrival of a message in a queue (e.g., Amazon Kinesis), or with database\nupdates (e.g., Amazon DynamoDB).\n380 | Chapter 9: Data Transformation and Delivery Layer\nCase Study: FINRA’s Transition to AWS Lambda\nfor OATS Data Validation\nThe Financial Industry Regulatory Authority (FINRA) is a US nongovernmental\norganization responsible for protecting investors and ensuring market integrity by\noverseeing and regulating broker-dealers. To monitor the trading practices of mem‐\nber firms, FINRA has an integrated audit trail system of orders, quotes, and trade\nevents for National Market System (NMS) stocks and over-the-counter (OTC) equi‐\nties. To record such data, FINRA ’s audit trail relies on the Order Audit Trail System\n(OATS). Using OATS data, along with other sources of market and reference data,\nFINRA is able to reconstruct the lifecycle of a trade—from origination through com‐\npletion or cancellation—and monitor the practices of its members.\nMember firms submit daily OATS data to FINRA, totaling more than 50,000 files\neach day. Upon receipt, FINRA verifies the data’s completeness and proper formatting\nagainst more than 200 rules, processing up to half a trillion validations daily. To han‐\ndle the significant and variable processing demands, FINRA needed a scalable, cost-\nefficient, and secure solution .\nThree options were explored—Apache Ignite on Amazon EC2, Apache Spark on\nAmazon EMR, and AWS Lambda. AWS Lambda emerged as the optimal choice due\nto its scalability, efficient data partitioning, robust monitoring, high performance,\ncost-effectiveness, and minimal maintenance needs. This choice supported FINRA ’s\ngoal of transitioning to a real-time processing model.\nSecurity was a critical factor, and AWS met FINRA ’s stringent data-protection\nrequirements, including encryption of data in transit and at rest. The new system was\ndeveloped in three months, with data ingested into Amazon S3 via FTP and validated\nusing AWS Lambda functions. A controller, running on Amazon EC2, manages data\nfeeds into AWS Lambda and outgoing notifications, as well as external data sources\nlike stock symbol reference files.\nTo ensure continuous operation and reduce processing time, the new architecture lev‐\nerages AWS Lambda’s data-caching abilities and uses Amazon SQS for input/output\nnotifications.\nData Transformation | 381",19068
132-Data Delivery.pdf,132-Data Delivery,,0
133-Metrics Events Logs and Traces.pdf,133-Metrics Events Logs and Traces,"Data Delivery\nOnce data has been transformed, the next step is to deliver it to the final consumers\nto extract actionable insights. Financial data engineers must determine who the ulti‐\nmate data consumers are and understand their specific needs, and then create the\nappropriate mechanisms to deliver the data. The following two sections will provide a\nbrief overview of this process.\nData Consumers\nAny user, application, business unit, team, or system that makes use of data generated\nby their company’s data infrastructure is considered a data consumer. Human data\nconsumers can engage in the data engineering lifecycle to varying extents. For exam‐\nple, compliance officers and marketing teams often specify their data needs and rely\non data engineers to handle the rest. In contrast, analysts and machine learning spe‐\ncialists may be more involved in defining the data source, type, and necessary trans‐\nformations.\nData consumers may also differ in terms of their data governance duties and respon‐\nsibilities. For instance, senior individuals in control of the data in a certain domain\nare known as data owners. In a bank, for instance, the finance director may be the\nowner of client financial data. A related role is that of the data steward,  who is\nresponsible for maintaining and guaranteeing the quality and consistency of data as\ndefined by the data owner. Data custodians are in charge of protecting the data by\nadhering to the rules outlined in the data governance framework.\nFinancial data engineers, being the data producers, must ensure that data consumers\nhave clear and well-defined expectations and requirements to facilitate effective data\ndelivery. A reliable way to formalize such an agreement between data consumers and\nproducers is through data contracts, which were discussed in Chapter 5 . A data con‐\ntract can detail all consumer requirements such as the data type, fields, formats, con‐\nstraints, conversions, SLA, and many more. Data contracts are often owned by the\ndata owner or delegated to the data steward. With a data contract, one can be sure\ndata producers know exactly what data consumers want, thus avoiding miscommuni‐\ncation issues around the data.\n382 | Chapter 9: Data Transformation and Delivery Layer\nDelivery Mechanisms\nData can be delivered to its final consumers in a variety of ways. These include the\nfollowing:\n•Direct database access via a user interface (e.g., Snowflake UI, pgAdmin for Post‐\ngreSQL, Compass for MongoDB, etc.). This mechanism is often intended for\npeople with basic SQL knowledge who want to conduct exploratory data analysis.\n•Direct file access via a user interface (e.g., the S3 web interface). This is intended\nfor easy interaction and sharing of the data.\n•Programmatic access to databases and file repositories via APIs, JDBC (Java\nDatabase Connectivity), ODBC (Open Database Connectivity), and client libra‐\nries such as AWS Boto3. This delivery mechanism is widely used by applications,\nsoftware engineers, and data engineers alike.\n•Reports that contain essential summaries and aggregations of data used for sup‐\nporting decision-making.\n•Dashboard access that displays metric and summary data in a visual format, typi‐\ncally via a single web page. Dashboards are feature rich, and come with a variety\nof tools to visualize, explore, filter, compare, zoom, and query the data in a user-\nfriendly way. They are quite useful for stakeholders who don’t have much exper‐\ntise or the time to perform raw data querying and analysis.\n•Email delivery.\nAn important thing to keep in mind when designing a delivery mechanism is the\nneed to provide users with the means to search and find the data they are looking for.\nAs more data gets generated and stored in various locations, it becomes harder for\ndata consumers to know where to find what they are looking for. Some good practices\ncan be adopted in this regard. For example, the final location of the data needs to be\nspecified in the data contract. Additionally, a data catalog can be created as a central\nsearch engine that allows users to search and find the data they are seeking.\nData Delivery | 383\nSummary\nThis chapter discussed the third layer in the financial data engineering lifecycle: the\ntransformation and delivery layer. Key topics covered include data querying patterns,\nquery optimization, transformation operations and patterns, data consumers, and\ndelivery mechanisms. Throughout the chapter, these concepts are applied within the\ncontext of financial markets, providing practical examples and case studies for better\ninsights.\nOne thing to keep in mind is that financial data transformations, along with their\nassociated patterns and optimizations, are fundamentally driven by business require‐\nments. Custom transformations may need to be developed to specifically address\nunique business needs and challenges. This adaptive approach ensures that the finan‐\ncial data engineering lifecycle effectively supports and aligns with business objectives\nin the dynamic financial landscape.\nAs you learned toward the end of this chapter, once data has been transformed and\nprepared for its intended purposes, it is delivered to its final consumers, seemingly\nmarking the end of its lifecycle. At this point, you might wonder if additional steps\nare still required. Crucially, ensuring optimal and flawless functioning across the\ningestion, storage, transformation, and delivery layers is never completely assured.\nThis underscores the critical importance of the final layer—monitoring—which will\nbe the focus of the next chapter.\n384 | Chapter 9: Data Transformation and Delivery Layer\nCHAPTER 10\nThe Monitoring Layer\nAfter designing and implementing the ingestion, storage, transformation, and deliv‐\nery layers, the final layer to build is the monitoring layer . This layer is crucial for\ntracking and reporting on the financial data infrastructure’s performance, reliability,\nand quality.\nData monitoring is a continuous task requiring close collaboration between financial\ndata engineers and business teams. It enables financial institutions to operate\nsecurely, compliantly, and efficiently in a rapidly changing environment. More specif‐\nically, monitoring is needed for the following functions:\nOperational continuity and efficiency\nAs financial data infrastructures grow in complexity, monitoring becomes vital to\nensure operational continuity, system availability, efficiency, cost optimization,\nand optimal performance.\nCompliance\nEffective monitoring is crucial for financial institutions to meet regulatory\nrequirements. It enables the detection and prevention of fraud and other suspi‐\ncious activities, while also facilitating accurate and timely regulatory reporting.\nRisk management\nFinancial institutions face a range of risks, including financial, credit, fraud, and\noperational risks, each with potentially significant costs. Monitoring plays a criti‐\ncal role in promptly and effectively detecting and mitigating these risks.\nWhen designing this layer, the first question you should ask yourself is what compo‐\nnents of your financial data infrastructure you need to monitor. This question is criti‐\ncal since monitoring can be a resource-intensive and costly commitment, so you\nmust be clear on your monitoring plan from the start.\n385",7405
134-Metrics.pdf,134-Metrics,"Monitoring every possible issue may not be feasible, which is an\nimportant consideration when designing the monitoring layer.\nThere is always the potential for unexpected and unforeseen prob‐\nlems to arise.\nThere is no one-size-fits-all approach to monitoring, as financial data infrastructures\nmay differ in terms of design patterns, data management maturity, software compo‐\nnents, level of automation, and data governance policies. However, the following five\ncategories are applicable to (almost) all monitoring approaches: (1) metrics, events,\nlogs, and traces, (2) data quality monitoring, (3) performance monitoring, (4) cost\nmonitoring, and (5) business and analytical monitoring. The following sections will\nexamine each of these in more depth.\nMetrics, Events, Logs, and Traces\nThe main building blocks of monitoring revolve around the generation and utiliza‐\ntion of four fundamental types of data: metrics, events, logs, and traces. These ele‐\nments provide essential inputs for financial data engineers to diagnose, understand,\nand resolve technical and nontechnical issues within the financial data infrastructure.\nMetrics\nMetrics are quantitative measurements that provide information about a particular\naspect of a system, process, variable, or activity. Metric values are typically observed\nover a specific time interval or frequency, such as daily, hourly, or in real time. In\naddition, metric observations are often enriched with metadata represented as a set of\nkey-value pairs, called tags. Tags are used to identify a given instance of a metric. A\nunique combination of a metric and its associated tags is called a time series.\nTable 10-1  illustrates this concept by displaying a time series view of the 30-day vola‐\ntility metric across three distinct stocks traded on NASDAQ. Specifically, the table\ncontains three separate time series: the initial two rows for AAPL-NASDAQ, the sub‐\nsequent two rows for GOOGL-NASDAQ, and the final row for MSFT-NASDAQ.\nTable 10-1. Metrics\nTimestamp Metric Value Tags\n2023-07-09 09:00:00 Volatility (30-day) 0.025 Ticker: AAPL, Exchange: NASDAQ\n2023-07-10 09:00:00 Volatility (30-day) 0.027 Ticker: AAPL, Exchange: NASDAQ\n2023-08-11 09:00:00 Volatility (30-day) 0.020 Ticker: GOOGL, Exchange: NASDAQ\n2023-08-12 09:00:00 Volatility (30-day) 0.022 Ticker: GOOGL, Exchange: NASDAQ\n2023-03-11 09:00:00 Volatility (30-day) 0.014 Ticker: MSFT, Exchange: NASDAQ\n386 | Chapter 10: The Monitoring Layer\nInterestingly, this way of organizing metrics as time series is a key reason why many\nengineers prefer using time series databases for tracking and managing metrics.\nCommon examples include Prometheus, Graphite, and InfluxDB. Advanced visuali‐\nzation tools like Grafana can be configured to retrieve data from the metric time ser‐\nies database, allowing users to explore and visualize metrics.\nData Monitoring with InfluxDB:  Case Studies from Financial Markets\nFinancial institutions worldwide employ and continually invest in data monitoring\ntechnologies. One common choice involves using time series databases such as\nInfluxDB.\nA good example is PayPal, a global leader in the international online payments sector,\nserving hundreds of millions of customers in over 200 markets around the world.\nWhen PayPal decided to become container-based to modernize its infrastructure, the\nIT team sought a scalable monitoring solution that could unify metric collection,\nstorage, visualization, and smart alerting within the same platform. The final solution\ninvolved using InfluxDB Enterprise (the enterprise version of InfluxDB) and\nInfluxData’s open source plug-ins, such as Telegraf .\nAnother example is Capital One, a US financial institution specializing in credit\ncards, car loans, and other products. Capital One generates a large amount of infra‐\nstructure, application, and business process metrics. This data plays a crucial role in\nmaintaining high-performance systems and ensuring uninterrupted service. Capital\nOne sought an advanced monitoring solution capable of ensuring high resilience and\navailability across multiple regions, while also integrating seamlessly with its machine\nlearning systems to analyze data and generate predictive metrics. To this end, Capital\nOne created a fault-tolerant system with disaster recovery features  based on InfluxDB\nEnterprise, AWS, and the visualization tool Grafana .\nAnother notable example is ProcessOut, a platform specializing in payment analytics\nand routing. ProcessOut provides two main products : Telescope, which analyzes\ntransaction data to aid clients in understanding payment failures, and Smart Router,\nwhich assists clients in selecting the optimal payment system provider for specific\ntransactions. ProcessOut chose InfluxDB  to provide its consumers with the finest ser‐\nvice while proactively monitoring and alerting them, depending on their payment\nactions. InfluxDB offers the functionality to collect, store, and manage critical pay‐\nment information (logs, metrics, and events).\nExamples like these, which you can check out online , highlight the critical role of data\nmonitoring in the financial sector. Financial institutions prioritize monitoring not\njust as a necessity, but as a means to create business value and drive innovation.\nMetrics, Events, Logs, and Traces  | 387",5371
135-Events.pdf,135-Events,,0
136-Data Quality Monitoring.pdf,136-Data Quality Monitoring,"Events\nEvents are structured data emitted by an application during runtime. They are usually\ntriggered by a specific type of activity and tend to belong to a predefined list of possi‐\nble values. Examples include client HTTP requests such as POST and GET, response\nstatus (e.g., 404 NotFound), cloud resource operation (e.g., Amazon S3 object-level\nAPI activity such as GetObject, DeleteObject, and PutObject), and user permission\nchanges (e.g., AmazonS3ReadOnlyAccess).\nThe structured nature of events makes them well-suited for storage and retrieval in\ntabular representations. That’s why event data is commonly stored in SQL databases.\nLogs\nLogs are semi-structured data emitted by an application during runtime, providing\ngreater flexibility in terms of content, structure, and triggering mechanisms com‐\npared to events. Logs are typically classified into five standard levels based on the\nseverity of the issue. These levels, arranged from lowest to highest severity, are listed\nhere:\nDebug logs\nUsed for diagnosing system issues in testing and development environments.\nInfo logs\nInformation on normal system operations, which can be useful in case something\ndoesn’t look normal. For example, you may want to log detailed records about\ntransactions, including timestamps, amounts, trade submissions, account details\nand updates, and status (success/failure). Such information can be critical for\nauditing, compliance reporting, and resolving disputes or discrepancies.\nWarning logs\nInformation about potential issues that might lead to future problems if not\naddressed. These are used when something unexpected happens, but you want\nyour application to continue running.\nError logs\nInformation about serious issues that affect the operation being executed but not\nthe service or the application as a whole. They require a developer’s intervention\nto fix. Examples include errors in processing a payment, a trade, or a loan\napplication.\n388 | Chapter 10: The Monitoring Layer\nCritical logs\nInformation about issues affecting the entire service or application and requiring\nimmediate intervention, for example, if the primary database of a web applica‐\ntion becomes completely inaccessible.\nThe extra flexibility of logs compared to events makes them essential for uncovering\nthe root causes of issues. Logs can be general-purpose, such as errors related to soft‐\nware bugs or missing resources. They can also be software-specific, such as database-\nrelated errors like deadlocks, transaction conflicts, query timeouts, and the maximum\nconnection limits reached.\nA variety of tools can be used to store and query logs. Many companies use the ELK\nstack: Elasticsearch, Logstash, and Kibana. Cloud-based tools are also common,\nincluding Azure Monitor Logs, Google Cloud Logging, and Amazon CloudWatch.\nTraces\nTraces represent a more advanced form of system behavior records, capturing com‐\nprehensive details about each step taken by a process as it progresses through one or\nmore systems. Examples include the following:\nBusiness operation traces\nStore information describing the steps involved in completing a business opera‐\ntion. For instance, tracing a trade order lifecycle from submission until execution\nor failure; tracing a payment flow initiation, authorization, processing, validation,\nand settlement; and tracing a loan application from submission through credit\nscoring, approval, and final disbursement.\nApplication traces\nStore information about an application’s behavior as it executes multiple compo‐\nnents. This includes function calls, API requests, condition checks, input/output,\nresource usage, timeout, and execution time.\nSecurity incident traces\nStore information on the events and activities associated with security incidents\nwithin a system.\nMetrics, Events, Logs, and Traces  | 389\nUnique Transaction Identifiers:  Cornerstones of\nEffective  Transaction Tracing\nTo ensure effective tracing, a trace identifier, also referred to as a transaction ID or\ncorrelation ID, is essential for uniquely identifying and monitoring the path of a par‐\nticular transaction or request across a complex system. Such identifiers ensure that all\nrelated activities can be linked together, providing a complete view of the transaction’s\nlifecycle and ensuring transparency throughout the process.\nIn Chapter 3 , you learned about transaction identifiers such as the Unique Transac‐\ntion Identifier (UTI) defined in ISO 23897. The UTI was developed to offer a consis‐\ntent reference that interlinks all related messages in an end-to-end financial\ntransaction, allowing every party involved to refer to the transaction easily. This\nunique identifier remains unchanged throughout the various events of a transaction\nlifecycle, including amendments and version changes, thereby enhancing transpar‐\nency and visibility across the settlement and reconciliation value chain.\nSeveral frameworks, technologies, and tools come into play when designing a solu‐\ntion for storing and managing traces. OpenLineage is an open platform designed for\ncollecting and analyzing data lineage. It tracks events related to data pipelines, includ‐\ning datasets, jobs, and executions. Apache Atlas is a metadata management and data\ngovernance tool that helps with data lineage, classification, and auditing. Apache NiFi\nenables the automated and efficient movement of data between systems, aiding in\ndata lineage and flow management. For distributed tracing, Jaeger and Zipkin are\nwidely used to monitor and troubleshoot errors and identify performance and latency\nbottlenecks in complex, microservices-based architectures.\nData Quality Monitoring\nData generated and used by financial institutions must consistently meet high quality\nstandards. Chapter 5  focused on financial data governance and extensively addressed\nthe topic of data quality, discussing dimensions such as errors, duplicates, relevance,\nbiases, completeness, timeliness, and outliers.\nThe traditional approach to data quality monitoring starts with the definition and\ndevelopment of Data Quality Metrics  (DQMs). These metrics are quantitative meas‐\nures used to assess and summarize the quality of specific aspects or dimensions of\ndata. Defining the right DQMs depends on several factors that relate to client expect‐\nations (e.g., a data quality clause in an SLA), business requirements (e.g., complete\nsales and customer data), and technical standards (e.g., formatting, decimals, etc.).\nFor this reason, there isn’t a fixed list of DQMs that fit all purposes. Instead, they are\ndefined in an iterative and continuous process that involves financial data engineers,\nthe business team, and data analysts/machine learning experts.\n390 | Chapter 10: The Monitoring Layer\nWhile there’s no one-size-fits-all approach, there are several DQM techniques that are\ncommonly utilized. The usage of ratios  is one such approach. For example, the error\nratio  indicates the percentage of erroneous records in a dataset, the duplicate ratio\ncomputes the percentage of duplicated records compared to all records within a data‐\nset, and a missing values ratio  can be used to measure the rate of available data com‐\npared to a fully complete dataset. Temporal data quality metrics may also be used. For\nexample, to ensure data timeliness, metrics can be constructed to measure the age of a\ndata item in a dataset and compare it against a reference date (e.g., < 1 week ago).\nAnother metric may check data arrival time against an expected arrival time (e.g.,\nevery day at 10 a.m.).\nOnce DQMs have been defined, a corresponding monitoring process must be imple‐\nmented. A well-known approach in this regard is data profiling . This involves parsing\nand checking a given dataset to understand its content, formatting, structure, and the\nrelationships between its records. By performing data profiling, potential data quality\nissues within the dataset can be identified. Profiling can be performed column-wise,\nwhere each column is examined separately, or row-wise, where all columns and their\nrelationships are analyzed. At the end of a data profiling check, a short report can be\ngenerated to summarize the main features and issues of the input dataset and provide\nrecommendations for addressing such issues. Figure 10-1  illustrates some of the ele‐\nments a typical data profile report might contain.\nFigure 10-1. A simple example of a data profile  report\nData Quality Monitoring | 391",8554
137-Performance Monitoring.pdf,137-Performance Monitoring,"Data profiling is a comprehensive procedure that scans the whole dataset to under‐\nstand its data quality attributes. However, a complete data profile report may not\nalways be required. Alternatively, a simpler approach could be to define and test data\nquality rules against one or more data records. A rule could state, for example, that\nthe error ratio should not exceed 1%.\nOnce DQMs and the data quality monitoring process have been established, the next\nstep is selecting a data quality tool or developing custom scripts to identify, validate,\nand flag any data quality issues. Commercial data quality tools include the Ataccama\nONE Data Quality Suite, Informatica Data Quality, Precisely Trillium, SAS Data\nQuality, and Talend Data Fabric. There are also open source data quality tools such as\nthe Python libraries Great Expectations, Soda Core, and DataCleaner.\nIt is essential to recognize that data quality monitoring is a continuous and evolving\nprocess. As the financial data landscape changes, with new data types, quality dimen‐\nsions, and business requirements emerging, it is crucial to regularly revisit and refine\nyour data quality monitoring process. This practice ensures the continued relevance \nand effectiveness of your data quality monitoring in safeguarding the quality of your\ndata.\nPerformance Monitoring\nPerformance refers to the ability of the financial data infrastructure to meet key\noperational criteria such as speed, latency, throughput, availability, scalability, and\nresource utilization. These performance indicators directly impact business perfor‐\nmance by influencing critical aspects such as the following:\nTime to Insight (TTI)\nThe time it takes to obtain actionable insights from data from the point it was\ngenerated. A shorter TTI enables fast delivery of financial data to end users,\nensuring efficient decision-making, customer satisfaction, timely reporting, and\nrevenue growth.\nTime to Market (TTM)\nThe time it takes for a product to progress from a conceptualized idea to its\nintroduction into the market.\nInnovation velocity\nThe pace at which data-driven innovations are generated and implemented\nData cycle time\nThe time required to complete an entire cycle of data analysis, starting from\nproblem formalization to obtaining actionable insights.\n392 | Chapter 10: The Monitoring Layer\nVarious metrics are frequently used to monitor data infrastructure performance.\nSome are software-agnostic and apply to any type of application:\nRAM usage\nThe amount of memory being used by the application\nCPU usage\nThe percentage of CPU being used by the application\nStorage usage\nThe amount of disk storage being used by the application\nExecution time\nThe time needed to execute a task or process a request\nRequests per second (RPS)\nThe number of requests made to an application every second\nIngress/egress bytes (bytes/sec)\nThe amount of network traffic (in bytes) entering and leaving an application\nUptime/downtime\nThe ratio of time a system is operational to the total time observed\nAPI response time\nThe time it takes for an API endpoint to process and respond to a request\nConcurrent users\nThe number of users accessing an application simultaneously\nAdditionally, performance metrics can be tailored to specific software systems. For\nexample, database management systems may have metrics such as the following:\nRead/write operations\nCount of read and write operations performed on the database within a specific\ntemporal window\nActive connections\nNumber of currently open database connections\nConnection pool usage\nUtilization of connections within a connection pool\nQuery count\nTotal number of queries processed by the database during a specified interval\nPerformance Monitoring | 393\nTransaction runtime\nDuration taken to execute a database transaction\nReplication lag\nThe delay between the time data is written to the primary database instance and\nthe time it is synched with read replicas\nY ou can even have more granular metrics defined for a specific software component.\nFor example, database queries may be monitored via metrics such as the following:\nRecords read\nThe number of data records read by the database.\nBlocks read\nThe number of data blocks read by the database.\nBytes scanned\nThe total number of bytes fetched by a query.\nNumber of output rows\nThe final number of rows returned by a query.\nScan time\nThe time spent by the database scanning the data.\nSort time\nThe time spent by the database sorting the data.\nAggregation time\nThe time spent by the database aggregating the data.\nPeak memory\nThe maximum amount of memory used during query execution.\nQuery plans\nThe execution steps that a data storage system follows to fetch data in response to\na query. Monitoring query plans can help detect costly queries and unused DB\nobjects such as indexes and provide insights into query performance.\nAdditionally, you can implement performance metrics specific to your business’s\ntechnical needs. Examples from financial markets include the following:\nTrade execution latency\nThe time taken from when a trade order is placed until it is executed, which is\ncritical for optimizing trading strategies and minimizing execution risks.\nTrade execution error rate\nThe frequency of failed transactions or trade orders, indicating operational\ninefficiencies.\n394 | Chapter 10: The Monitoring Layer\nTrade settlement time\nThe duration from when a trade is executed to when it is settled, reflecting the\nefficiency of the settlement process.\nMarket data latency\nThe time it takes for financial market data to be received from the exchange or\ndata provider to the trading system, which impacts the speed of decision-making.\nAlgorithmic trading performance\nIncludes metrics such as algorithm execution time and success rate, which can\nhighlight the effectiveness and reliability of algorithmic trading strategies.\nRisk exposure calculation time\nThe time taken to compute and update risk exposure metrics, which is critical for\nmanaging and mitigating financial risks in real time.\nCustomer transaction processing time\nThe duration from when a customer initiates a transaction (e.g., deposit, with‐\ndrawal) to its completion, which can impact customer satisfaction and opera‐\ntional efficiency.\nCase Study: Monitoring Real-Time Financial Data Feeds\nIn today’s fast-paced financial markets, real-time data feeds have become increasingly\npopular for providing the critical information necessary for making timely decisions.\nThese feeds deliver up-to-the-second market data, including prices, volumes, and\norder book information, which are indispensable for traders, financial institutions,\nand automated trading systems. The reliance on accurate and timely data to capture\nmarket opportunities and manage risks effectively makes financial market feeds inte‐\ngral to the modern financial ecosystem. To ensure efficient and timely data delivery,\nproviders of data feeds and related infrastructure need to thoroughly monitor the\nperformance of their systems. Among the most important data feed performance\nmetrics to monitor are the following.\nNetwork Metrics\nPackets per second (PPS)\nThe number of data packets transmitted and received per second.\nPacket size distribution\nThe size of data packets being transmitted and received.\nRound-trip time (RTT)\nThe time for a signal to travel from sender to receiver and back.\nOne-way latency\nTracks the time for a signal to travel from sender to receiver.\nPerformance Monitoring | 395\nBandwidth utilization\nMeasures how much of the available network bandwidth is being used. Network\nbandwidth refers to the maximum rate at which data can be transmitted over a\nnetwork connection in a given amount of time. Bandwidth utilization is about\nefficiency and capacity usage.\nData transfer rate (throughput)\nMeasures the actual amount of data transmitted per second. It’s about the speed\nof data movement.\nSystem Metrics\nCPU usage\nPercentage of CPU capacity used.\nCPU time in I/O wait (iowait)\nTime CPU spends waiting for I/O operations to complete.\nCPU interrupt time (irq)\nTime CPU spends handling software interrupts.\nMemory utilization\nPercentage of memory being used.\nProcessing Metrics\nBackpressure\nTracks the system’s ability to handle incoming data rates. Backpressure occurs if\nthe system receives more message requests than it can handle.\nBuffer/queue  sizes\nMonitor data waiting to be processed.\nA particular type of performance metrics are incident response and management\nmetrics, which are used to evaluate the effectiveness of an organization at tackling\nand preventing system downtime/outage issues. Examples include the following:\nMean Time to Detect (MTTD)\nThe expected time it takes to detect an issue or incident after it has occurred.\nMean Time Before Failure (MTBF)\nThe expected time between two failures/downtime in a repairable system. The\nhigher the MTBF, the more reliable and available the system is, and the more\neffective the data engineering team is at preventing future issues.\n396 | Chapter 10: The Monitoring Layer",9189
138-Cost Monitoring.pdf,138-Cost Monitoring,"Mean Time to Recovery (MTTR)\nThe expected time before a system recovers from a failure and becomes fully\noperational. A low MTTR indicates that the engineering team is quite effective at\nresolving the problem and that issue-fixing is efficient (e.g., through DevOps,\nDataOps, automated testing, etc.).\nMean Time to Failure (MTTF)\nThe expected time until a nonrepairable failure occurs. A nonrepairable failure\nrequires replacing the failed system with a new one.\nMean Time to Acknowledge (MTTA)\nThe expected time from when an incident is reported to when someone starts\nworking on the issue. A low MTTA indicates high team responsiveness and an\neffective alerting system.\nIncident metrics are of primary importance in today’s always-on financial market\ninfrastructures. The costs associated with outages and downtime in financial markets\ncan be substantial, potentially leading to significant repercussions, especially given\nthe interconnected and high-frequency/high-volume nature of financial activities.\nTurning to technological implementations, there are numerous tools and frameworks\ndesigned for performance monitoring. Some are integrated into data engineering\ntools; for example, Apache Spark includes a UI interface for monitoring cluster status\nand resource usage. Open source tools such as Prometheus , designed primarily for\nmetrics data, provide a rich set of monitoring and alerting features, including a multi‐\ndimensional data model, flexible query language (PromQL), and multiple modes of\ndashboarding and visualizations. Another common approach involves using time ser‐\nies databases like InfluxDB, alongside advanced visualization tools such as Grafana.\nAnother popular and user-friendly option is cloud-based monitoring solutions. Using\nthe cloud, it is possible to establish an alerting policy to get notified when\nperformance-related issues occur. For example, Google Cloud provides a managed\nalerting service  where you can create an alerting policy  that defines the circumstances\nunder which an incident is created and the notification channel through which you\nwant to be notified.\nCost Monitoring\nCost monitoring is the process of keeping an eye on a financial data infrastructure’s\nexpenditures to ensure that they are within acceptable bounds and to spot patterns of\nexcessive usage. The significance of cost monitoring has grown alongside the wide‐\nspread adoption and migration to cloud services. While the cloud provides a plethora\nof accessible services with flexible on-demand pricing, managed scalability, and\nsimplified configuration, there exists a potential risk of unforeseen, excessive, or mis‐\ninterpreted cost structures.\nCost Monitoring | 397\nTo illustrate the issue, let’s consider one of the most attractive cloud computing strate‐\ngies: serverless computing . This term refers to a cloud-based application development\nmodel where the cloud provider takes full care of infrastructure configuration and\nmanagement. This includes all features such as load balancing, auto-scaling, availabil‐\nity, security, operating system management, logging, monitoring, and storage man‐\nagement. Services such as AWS Lambda and Google BigQuery are among the most\npopular services that people often associate with serverless computing.\nCrucially, the economics of serverless computing can be misunderstood, potentially\nresulting in unforeseen expenses. This is due to the variability in cost-effectiveness of\nthe serverless model, which is dependent on factors like execution patterns and work‐\nload volume. Pennies can add up to thousands of dollars when running a massive\nnumber of jobs.\nFurthermore, in services like AWS Lambda, pricing isn’t solely determined by the fre‐\nquency and duration of function invocations, as many might assume with pay-as-you-\ngo models. Y ou also indirectly pay based on the memory allocated to your function,\nwhich directly influences the cost of your function executions. Moreover, additional\ncharges may be added if AWS Lambda reads data from AWS storage or transfers data\nbetween regions.\nAs of the time of writing this book, invoking an on-demand AWS Lambda function\ncosts $0.0000000021 per millisecond for 128 MB of RAM and $0.0000000167 per\nmillisecond for 1,024 MB of RAM. This looks relatively cheap. However, costs can\nescalate significantly based on the nature of your workload. When assessing applica‐\ntion scalability, a commonly used metric is transactions per second (TPS), also\nreferred to as hits per second (HPS) or requests per second (RPS). Suppose our appli‐\ncation processes a specific number of RPS over a total of 100,000 seconds in a month.\nUsing this information, let’s explore a few scenarios:\nScenario 1: 100 RPS with a duration of 10 seconds each\n•Cost per invocation : $0.0000000021 × 10,000 milliseconds (10 seconds) =\n$0.000021\n•Monthly cost : $0.000021 × 100 RPS × 100,000 seconds = $210\nScenario 2: Increasing execution time to 1 minute (60 seconds)\n•Cost per invocation : $0.0000000021 × 60,000 milliseconds (60 seconds) =\n$0.000126\n•Monthly cost : $0.000126 × 100 RPS × 100,000 seconds = $1,260\n398 | Chapter 10: The Monitoring Layer\n1To learn more about the economics of serverless computing, I highly recommend Adam Eivy and Joe Wein‐\nman’s “Be Wary of the Economics of ‘Serverless’ Cloud Computing” , IEEE Cloud Computing  4, no. 2 (March–\nApril 2017): 6–12.Scenario 3: Increasing allocated memory to 1,024 MB (1 GB)\n•Cost per invocation : $0.0000000167 × 60,000 = $0.001002\n•Monthly cost : $0.001002 × 100 RPS × 100,000 seconds = $10,020\nThese calculations show how increasing execution time and memory allocation can\nsignificantly impact the monthly cost of running AWS Lambda functions. Adjust‐\nments in these parameters should be carefully considered based on performance\nrequirements and budget constraints. Additionally, when assessing solutions like\nLambda and other alternatives, it is important to consider your future scaling needs\nrather than just your current usage.1\nVarious practices and tools are available to manage cloud costs. For instance, cloud\nproviders offer budgeting options  that let users set target budgets and receive notifi‐\ncations if costs exceed predefined thresholds. Another promising approach is FinOps,\na recent trend in cloud cost monitoring and management, which has been defined by\nthe FinOps Foundation  as:\nAn operational framework and cultural practice which maximizes the business value of\ncloud, enables timely data-driven decision making, and creates financial accountability\nthrough collaboration between engineering, finance, and business teams.\nA FinOps framework requires five pillars:\n•Cross-functional collaboration among engineering, finance, and business teams\nto enable fast product delivery while ensuring financial and cost control.\n•Each team takes ownership of its cloud usage and costs.\n•A central team establishes and promotes FinOps best practices.\n•Teams take advantage of the cloud’s on-demand and variable cost model while\noptimizing the tradeoffs among speed, quality, and cost in their cloud-based\napplications.\n•Decisions are driven by the cloud’s business value, such as increased revenue,\ninnovative product development, reduced fixed costs, and faster feature releases.\nFinOps is an iterative and learning-driven process. Within organizations, the matur‐\nity of FinOps is commonly assessed using the FinOps Maturity Model (FMM) , which\nconsists of three stages: Crawl, Walk, and Run. As an organization progresses from\nCrawl to Run level, it moves from being reactive, where issues are fixed as they occur,\nCost Monitoring | 399",7706
139-Business and Analytical Monitoring.pdf,139-Business and Analytical Monitoring,"to being proactive, where teams are able to anticipate cloud usage patterns and\nexpenses and incorporate such information into their cloud architecture design\ndecisions.\nCase Study: Financial Transaction Cost Monitoring with Abel Noser\nTransaction cost monitoring and analysis is a common practice in financial markets.\nIt entails monitoring and analyzing the expenses related to executing financial trans‐\nactions, such as trades and investments. This process is vital for investment firms,\nasset managers, brokers, and other financial institutions to ensure that they are\nachieving optimal execution while minimizing costs.\nSeveral companies specialize in transaction monitoring services for the financial sec‐\ntor. An industry leader in this market is Abel Noser , now part of Trading Technolo‐\ngies. Financial institutions submit their transaction data to Abel Noser, which\nconducts comprehensive cost analysis and returns insights to the clients. Over 350\nglobal clients from the US, as well as other parts of the world, report their data to Abel\nNoser.\nAbel Noser is also a data vendor, providing institutional investor datasets that contain\ntransaction-related information such as the instrument traded, price, quantity, date,\ntrade direction (buy or sell), commissions paid, and other market fields. Abel Noser’s\ndata is an essential source for scientific research on institutional trading.\nBusiness and Analytical Monitoring\nA more advanced form of data monitoring involves observing statistical, analytical,\nand business-related dimensions. This monitoring approach ensures that not only is\nthe raw data monitored, but also the contextual and analytical insights that drive stra‐\ntegic decision-making.\nFor example, commercial banks must diligently and continuously monitor the status\nof their lending activities to ensure clients make timely payments and detect/predict\ndefaulting clients. Moreover, to ensure resiliency and regulatory compliance, banks\nneed to monitor their financial, credit, and operational risks as well as their expo‐\nsures, risk concentration, capital adequacy, and leverage situation. Similarly, institu‐\ntions such as investment firms need to continuously monitor their portfolio\nperformance, asset allocation, diversification, and investment strategies.\n400 | Chapter 10: The Monitoring Layer\nRisk Monitoring in Financial Institutions:\nData Engineering Challenges\nRisk is an inherent aspect of financial institution operations, particularly in the bank‐\ning sector. Consequently, most banking regulations emphasize principles and frame‐\nworks to ensure banks’ resilience against the classes of risks they are exposed to.\nThe primary framework for international banking regulation  consists of the three\nBasel Accords—Basel I, Basel II, and the more recent Basel III. The Basel framework\ngroups bank risks into three broad categories:\nMarket risk\nThe risk of financial loss deriving from movements in market prices.\nCredit risk\nThe risk of financial loss deriving from a borrower or counterparty not being\nable to meet the agreed-upon obligations.\nOperational risk\nThe risk of financial loss resulting from poorly designed or failed internal pro‐\ncesses, people, and systems or from external events.\nTo be managed, these risks must be quantitatively measured. To do this, risk data is\nneeded. Each risk category requires different sets of data fields. Market risk measure‐\nment requires data such as portfolio composition, historical prices, volatility, interest\nrates, and correlations. Credit risk measurement requires data such as credit ratings,\ncredit scores, exposure data, collateral data, default data, probability of default data\n(likelihood of creditors defaulting on their obligations), and loss given default data\n(losses incurred in case of a default event). Finally, operational risk measurement\nrequires detailed operational loss and risk event data as well as internal control data.\nImportantly, effectively managing the diverse types of risk data poses significant data\nengineering challenges related to data collection, aggregation, integration, normaliza‐\ntion, quality assurance, and timely delivery. In Chapter 5 , we discussed the problem\nof data aggregation in financial institutions and illustrated how it mainly applies to\nrisk data. As a reminder, Basel III introduced the Principles for Effective  Risk Data\nAggregation and Risk Reporting  to ensure the adequacy of financial institutions’ data\ninfrastructure for managing and aggregating risk data efficiently.\nTo address operational risk, a common strategy involves establishing and maintaining\nan operational event risk  database , which stores historical records stemming from\noperational incidents. This database may contain elements such as the event date,\ndescription (e.g., what happened), primary causes (e.g., human error), business line\n(e.g., risk management), and control point (e.g., trading desk). To learn more about\nthis, consult the paper by Niclas Hageback, “The Operational Risk Framework Under\na Retail Perspective” .\nBusiness and Analytical Monitoring | 401\n2For more on model risk, see Jon Danielsson, Kevin R. James, Marcela Valenzuela, and Ilknur Zer’s “Model\nRisk of Risk Models” , Journal of Financial Stability  23 (April 2016): 79–91.Financial institutions relying on data analytics and machine learning must actively\nmonitor the statistical, quality, and performance aspects of both their data and mod‐\nels. Within the machine learning community, concepts such as concept drift  and data\ndrift  are frequently used in the monitoring and detection of changes in the underlying\ndata distribution or model relationships.\nConcept drift occurs when a machine learning model no longer accurately reflects the\noriginal problem it was trained to solve. For example, a well-performing fraud detec‐\ntion model trained on a specific financial dataset may become less effective over time\nif fraudsters adapt and modify their fraudulent tactics. As a result, the model’s predic‐\ntions may become less accurate because the underlying problem it was designed to\naddress has evolved.\nData drift happens when the input data distribution to a machine learning model\nchanges over time. Consider an ML model trained to forecast a company’s stock price\nbased on a given data sample. Suppose that throughout this sample, the stock price\nwas relatively stable (low volatility). If the market becomes more volatile over time,\nthe model might struggle to predict accurately because the statistical characteristics of\nthe data have changed.\nInterestingly, the ideas of concept and data drift are relatively familiar to finance. In\nfinancial markets, particularly among risk managers, the term model risk  is used to\ndescribe the risk that financial models used in asset pricing, trading, forecasting, and\nhedging will generate inconsistent or misleading results.2\nOne prevalent area of business monitoring among financial institutions involves the\nmonitoring of various fraudulent and illegal activities when conducting transactions.\nExamples include the following:\nMoney laundering\nThe process of concealing the origins of illegally obtained money, typically by\nmeans of transfers involving foreign banks or offshore companies.\nTerrorism financing\nProviding financial support to terrorist organizations or individuals to facilitate\nacts of terrorism.\nFraud\nDeceptive practices intended to secure unfair or unlawful financial gain, often\ninvolving misrepresentation or omission of information.\n402 | Chapter 10: The Monitoring Layer\nCorruption\nDishonest or fraudulent conduct by those in power, typically involving bribery,\nembezzlement, or abuse of authority for personal gain.\nAnother good example is market manipulation and securities fraud practices, which\ninvolve illegally influencing the supply or demand of financial instruments to gain\nadvantage from market reactions. Examples include the following:\nPump and dump\nThis involves artificially inflating the price of a stock or other asset through false\nor misleading statements (pump). Once the price is high enough, the manipula‐\ntor sells their holdings at a profit (dump), causing the price to collapse, and leav‐\ning other investors with losses.\nSpoofing\nThis involves placing orders without the intention of executing them in order to\ncreate a false impression of demand or supply in the market. Once other traders\nreact to these orders, the spoofer cancels or modifies their original orders.\nInsider trading\nTrading securities based on material nonpublic information, which can generate\nunfair advantages and distortions in market prices.\nFront running\nWhen a broker or trader executes orders on a security for their own account\nwhile taking advantage of inside knowledge of a future transaction that is\nexpected to impact the security’s price substantially.\nChurning\nExcessive trading of a client’s brokerage account by a broker to generate commis‐\nsions without regard for the client’s investment goals.\nWash trading\nSimultaneously buying and selling the same financial instruments to create artifi‐\ncial trading volume or a false impression of market activity without having expo‐\nsure to market risk.\nLastly, an integral aspect of financial institution monitoring revolves around evaluat‐\ning investment portfolio performance. This entails selecting, calculating, and moni‐\ntoring key performance indicators like the Sharpe ratio, Sortino ratio, Treynor ratio,\nand information ratio. To illustrate one example, the Sharpe ratio is a measure of risk-\nadjusted return, indicating how well an investment performs relative to its risk level.\nThese metrics allow institutions to assess the effectiveness of their investment strate‐\ngies, enabling informed and timely decisions for optimizing portfolio composition\nand performance.\nBusiness and Analytical Monitoring | 403",9994
140-Data Observability.pdf,140-Data Observability,"Data Observability\nAs the technological stacks supporting data infrastructures have grown in complexity\nand scale, the requirements for monitoring have evolved accordingly. This has led to\nthe emergence of a more advanced and comprehensive monitoring approach known\nas observability .\nObservability elevates monitoring to a new level, allowing software and data engi‐\nneering teams to handle issues proactively and gain deep insights into the internal\nstate and behavior of a given software application or infrastructure. Authors Charity\nMajors, Liz Fong-Jones, and George Miranda ( Observability Engineering , O’Reilly,\n2022) describe a software system as observable if you can do the following (the fol‐\nlowing list is a direct quote):\n•Understand the inner workings of your application\n•Understand any system state your application may have gotten itself into, even\nnew ones you have never seen before and couldn’t have predicted\n•Understand the inner workings and system state solely by observing and inter‐\nrogating with external tools\n•Understand the internal state without shipping any new custom code to handle it\n(because that implies you needed prior knowledge to explain it)\nObservability can be applied across various dimensions of IT systems, encompassing\ndata, applications, infrastructure, networks, security, and business. Data observability\nis a vital area in this regard. Author Andy Petrella of Fundamentals of Data Observa‐\nbility  (O’Reilly, 2023) defines data observability as:\nThe capability of a system to generate information on how the data influences its\nbehavior and, conversely, how the system affects the data.\nWith data observability, financial data engineers and other team members should\neasily be able to ask and answer questions such as: “Why is workflow A running\nslowly?” , “Why is data not available yet for client X?” , “What caused the data quality\nissue in dataset Y?” , “Why is the data pipeline for report Z delayed?” , or “Where is the\nbottleneck in a transformation or ingestion process?”\nThe main building blocks of data observability are metrics, events, logs, and traces.\nIn addition, data observability systems leverage concepts such as automated monitor‐\ning and logging, root cause analysis, data lineage, contextual observability, Service-\nLevel Agreements (SLAs), telemetry/OpenTelemetry, instrumentation, tracking,\ntracing, and alert analysis and triaging.\n404 | Chapter 10: The Monitoring Layer\n3For a comprehensive discussion of observability data management systems, I highly recommend Suman Kar‐\numuri, Franco Solleza, Stan Zdonik, and Nesime Tatbul’s “Towards Observability Data Management at Scale” ,\nACM Sigmod Record  49, no. 4 (March 2021): 18–23.OpenTelemetry: The Open Source Standard for Observability\nOpenTelemetry  is an open source, vendor-neutral framework comprising a set of\nAPIs, SDKs, libraries, agents, and tools to enable observability in software applica‐\ntions. It allows engineers to collect telemetry data, such as traces, metrics, and logs,\nfrom their applications and services to gain insights into their behavior, performance,\nand reliability. OpenTelemetry provides standardized instrumentation and integration\ncapabilities for various programming languages, frameworks, and platforms, making\nit easier to instrument applications consistently across different environments. It\nallows exporter applications to send telemetry data to various backends and observa‐\nbility platforms for storage, analysis, visualization, and alerting.\nA properly implemented data observability system can bring a lot of benefits for\nfinancial institutions, such as the following:\n•Higher data quality (e.g., by observing data quality metrics)\n•Operational efficiency (e.g., reduced TTD and TTR)\n•Facilitated communication and trust between different team members (e.g., risk\nmanagement and trading desk)\n•Gains in client trust by being able to detect and understand the issues they might\nface\n•Enabling complex data ingestion and transformation while maintaining reliabil‐\nity and visibility into the system\n•Regulatory compliance (e.g., by keeping data privacy and security under control)\nFinancial data engineers play a vital role in embedding data observability capabilities\nwithin the financial data infrastructure. Observability is fundamentally a data engi‐\nneering challenge. It entails instrumenting systems to generate vast amounts of heter‐\nogeneous data points, which must be efficiently indexed, stored, and queried in near-\nreal time. This ensures comprehensive visibility into the behavior of various\ncomponents of the financial data infrastructure, including data ingestion, storage,\nprocessing systems, workflows, quality, compliance, and governance.3\nIt’s important to remember that implementing a data observability system requires a\nsignificant investment in both technology and people. For this reason, I highly rec‐\nommend you evaluate your business needs, technical constraints, and growth\nData Observability | 405",5067
141-Chapter 11. Financial Data Workflows.pdf,141-Chapter 11. Financial Data Workflows,"perspectives before investing in a large-scale data observability system. In many cases,\na simple monitoring strategy is all you need. As your organization grows and its tech‐\nnological stack becomes more complex, investing in data observability becomes\nincreasingly beneficial and, eventually, essential.\nSummary\nThis chapter discussed the final layer in the financial data engineering lifecycle: moni‐\ntoring. Monitoring is critical for guaranteeing the reliability, performance, security,\nand compliance of financial data infrastructures.\nThis chapter explained and illustrated the significance of metric, event, log, and trace\nmonitoring in tracking application activities and diagnosing and solving potential\nissues. Furthermore, it explored the key components of data quality, performance,\nand cost monitoring, outlining their techniques and giving practical advice and\nfinancial use cases.\nIn addition, it emphasized the importance of business and analytical monitoring in\nproviding actionable insights and supporting informed decision-making within\nfinancial institutions. Finally, the chapter included a brief review of data observability,\nan emerging topic, highlighting its crucial role in providing deep insights into the\ninternals and behavior of various data infrastructure components.\nBy this point of the book, you should have a solid grasp of the various layers compris‐\ning the financial data engineering lifecycle. To further simplify and enhance the mod‐\nularity of financial data infrastructures, smaller data processing components, known\nas data workflows, are commonly developed. The next chapter will cover these data\nworkflows in detail.\n406 | Chapter 10: The Monitoring Layer",1724
142-What Is a Data Workflow.pdf,142-What Is a Data Workflow,"CHAPTER 11\nFinancial Data Workflows\nThroughout this part of the book, you have explored the fundamental layers of the\nfinancial data engineering lifecycle: ingestion, storage, transformation and delivery,\nand monitoring. Adhering to this structured framework is crucial for building and\nmaintaining efficient financial data infrastructures.\nImportantly, as your company expands, the financial data infrastructure will become\nlarger and more complex, generating many interdependencies and links between the\ndifferent layers. To address this challenge, further refinement is required to build\nmultiple independent and specialized data processing components. Data engineers\ncall these components data pipelines or workflows.\nThis chapter will explore the foundational concepts of data workflows, introducing\nthe ideas of workflow-oriented software architectures and workflow management\nsystems. Then, it will cover the main types and characteristics of financial data work‐\nflows that can be implemented to handle financial data processing tasks.\nWorkflow-Oriented  Software Architectures\nBefore explaining workflows, it’s important to understand the business rationale\nbehind their practicality. First, as a natural part of the growth journey of any digital\ncompany, the technological stack tends to increase in size and complexity, creating\nnumerous interdependencies, logical flows, and interactions among various system\ncomponents. For this reason, the software development community has identified the\nneed for architectural designs and tools that organize software transactions into\nstructured workflows that can be defined, coordinated, managed, monitored, and\nscaled following a specific business logic. In this book, I will use the term workflow-\noriented  software  architecture  (WOSA) to describe this design pattern.\n407\nThe concept of WOSA is highly applicable to financial markets, where many financial\noperations are organized as a series of actions carried out in a specific sequence. A\nnotable example is financial transactions, which generally follow a defined workflow\nconsisting of multiple steps. Examples follow:\n•The lifecycle of financial trades involves several steps such as trade initiation,\norder placement, trade execution, trade capture, enrichment, validation, verifica‐\ntion, confirmation, clearing, and settlement.\n•The lifecycle of credit card payments involves a chain of steps that may involve\nthe customer, merchant, payment gateway, payment processor, acquiring bank,\nissuing bank, and card network. Along the way, the same payment goes through\nseveral steps that start with submission, authorization, balance and fraud checks,\nconfirmation or rejection, clearing, and settlement.\nIn essence, WOSAs focus on organizing and executing complex processes through\nstructured workflows, enhancing modularity, efficiency, and manageability in soft‐\nware systems. An important category within workflows is the data workflow,\ndesigned specifically for the processing and management of data. Let’s examine some\nof its technical aspects and concepts in the following section.\nWhat Is a Data Workflow?\nA data workflow is a repeatable process that involves applying a structured sequence\nof data processing steps to an initial dataset, producing a desired output dataset.\nData workflows are fundamental to data engineering. As a financial data engineer,\nyou will frequently build financial data workflows. To excel at this task, an important\nskill to learn is workflow abstraction. This involves defining a conceptual framework\nfor your workflow that disregards technological implementation details to guarantee\ngeneralization. It is similar to how concepts of data storage models and data storage\nsystems were used in Chapter 8  to abstract the specifics of storage technologies.\nOne of the most widely adopted data workflow abstractions is the dataflow  paradigm .\nIts core principle involves organizing an application into a directed computational\ngraph. In this graph, nodes represent individual computation steps, while links\nexpress data dependencies or the flow of data between these computations. Typically,\nthe computational graph underlying a workflow is designed without cycles. When\nstructured in this manner, the graph is referred to as a Directed Acyclic Graph\n(DAG).\nThe most basic type of computational DAG is the linear DAG. It organizes tasks in a\nsequential order where each task N can have at most one preceding task (N–1) and\none subsequent task (N+1). Figure 11-1  illustrates a three-task linear computational\nDAG.\n408 | Chapter 11: Financial Data Workflows\nFigure 11-1. Linear DAG\nFrequently, computational DAGs are more complex than the linear case. For example,\na specific computing task may depend on multiple preceding tasks. Figure 11-2  illus‐\ntrates such a scenario, where computation task 6 relies on tasks 3, 4, and 5.\nFigure 11-2. Complex DAG\nWhen defining a computational DAG, a fundamental question arises: “What consti‐\ntutes a task?” While there isn’t a universal answer to this question, several best practi‐\nces have been suggested. For example, a task needs to be an atomic unit of execution,\nmeaning that it either succeeds or fails as a whole. Furthermore, it is considered a\nbest practice to ensure that tasks are idempotent, meaning that running the same task\nmultiple times will yield the same result as running it once, without causing any\nunexpected or undesirable side effects. This can be useful since it can allow you to\nadd task checkpoint features to your DAG, allowing you to resume DAG execution\nfrom the failed step rather than execute it all over again.\nAnother consideration is the size of tasks within a computational DAG. They should\nstrike a balance: they need to be small enough to facilitate debugging and traceability,\nbut not so small that they introduce unnecessary overhead and degrade DAG perfor‐\nmance. Conversely, overly large tasks simplify the DAG structure but can constrain\ndebuggability and lead to costly retry operations.\nMoreover, a good practice is to associate tasks with business logic. This can involve\nbasic tasks such as data quality checks or more complex computations like modeling\nand analytics.\nWhat Is a Data Workflow?  | 409",6323
143-Workflow Management Systems.pdf,143-Workflow Management Systems,,0
144-Flexibility.pdf,144-Flexibility,,0
145-Scalability.pdf,145-Scalability,"Workflow  Management Systems\nOnce a workflow abstraction has been defined, the next step is to design a system that\nprovides the infrastructure and tools necessary for building, orchestrating, and moni‐\ntoring a WOSA. In this context, I’ll use the term workflow  management system\n(WMS) to refer to such systems. When developing a WMS, various properties must\nbe considered. In the following subsections, I will briefly discuss the most common\nones.\nFlexibility\nA flexible WMS is capable of managing a diverse range of tasks and workflows. For\ninstance, it should support the creation of simple linear workflows as well as more\ncomplex ones. Additionally, it should offer flexibility in how workflows are initiated,\nallowing for scheduled executions as well as triggering based on specific events, such\nas the arrival of files or data in designated storage locations.\nConfigurability\nA configurable WMS enables users to define workflow specifications according to\ntheir needs. For instance, many WMSs allow users to define workflows and tasks\nusing infrastructure-as-code (IaC) methodologies. This can be achieved through pro‐\ngramming languages like Python or Domain-Specific Languages (DSLs) such as\nJSON.\nWith IaC, users can programmatically define workflow and task specifications such as\ndependency structure, input/output parameters, error handling, timeout policy, retry\nlogic, status update, workflow triggers, concurrent execution limit, logging, and many\nmore. Figure 11-3  illustrates a typical approach to workflow and task definition,\nwhich contains all the information necessary to define the behavior of a workflow.\n410 | Chapter 11: Financial Data Workflows\nFigure 11-3. A workflow  and task definition  template\nDependency Management\nWMSs vary in how they manage and configure dependencies between tasks and\nworkflows. In simple WMSs, users are only able to define static and linear workflows.\nHowever, more advanced WMSs may allow for parallel workflow/task execution,\ndynamic and conditional task creation, and information passing between tasks. To\nillustrate, (a) in Figure 11-4  displays a workflow where tasks 3 and 4 are conditionally\nexecuted after task 2, depending on whether a specific condition is satisfied; if not,\ntask 5 follows task 2. In contrast, (b) in Figure 11-4  illustrates a workflow that dynam‐\nically generates a variable number of tasks (N) at runtime, based on the input\nreceived from task 1.\nWorkflow  Management Systems | 411\nFigure 11-4. Workflow  with conditional tasks (a), and workflow  with dynamic tasks (b)\nCoordination Patterns\nIn every WMS, components like workflows and tasks are designed to interact with\neach other. Various communication patterns have been developed to describe how\nthese interactions occur. Two notable patterns are the Synchronous Blocking Pattern\n(SBP) and the Asynchronous Non-Blocking Pattern  (ANBP).\nThe SBP involves one component calling another and waiting for a response, which is\ntypical in systems using HTTP and RESTful APIs. In a WOSA relying on an SBP , each\ntask within a workflow is executed sequentially within the same process, with each\ntask waiting for the previous one to complete before proceeding.\nConversely, the Asynchronous Non-Blocking Pattern operates in a fire-and-forget\nmode, where a component sends a request or message to another component without\nwaiting for a response or confirming its processing. In a WMS, this pattern can be\nimplemented by running each workflow task independently on separate processes or\nmachines and having a dedicated task queue that gathers incoming messages for exe‐\ncution. Figure 11-5  provides a visual representation of this concept.\n412 | Chapter 11: Financial Data Workflows",3770
146-Types of Financial Data Workflows.pdf,146-Types of Financial Data Workflows,"Figure 11-5. Synchronous Blocking Pattern versus Asynchronous Non-Blocking Pattern\nScalability\nWMS scalability is typically assessed based on its capacity to handle concurrent exe‐\ncutions of workflows and tasks. The scalability requirements vary depending on the\nnature of the WMS. Event-driven and high-volume WMSs, for instance, demand\nrobust scalability capabilities to manage large volumes of tasks triggered by real-time\nevents or continuous streams of data. In contrast, scheduled batch-oriented WMSs\nmay have less stringent scalability needs as they operate on predefined schedules and\nprocess tasks in batches.\nIntegration\nA highly desired feature of WMSs is their capability to seamlessly integrate with other\ntools and technologies, especially those that the WMS is designed to manage or coor‐\ndinate. For example, when orchestrating applications in the cloud, it might be more\nconvenient to rely on a cloud-based WMS as it will easily integrate with the cloud\nservices running the applications.\nWorkflow  Management Systems | 413",1056
147-Extract-Transform-Load Workflows.pdf,147-Extract-Transform-Load Workflows,"1For a comprehensive treatment of data warehouse-oriented ETL, I highly recommend the seminal work of Joe\nCaserta and Ralph Kimball, The Data Warehouse ETL Toolkit: Practical Techniques for Extracting, Cleaning,\nConforming, and Delivering Data  (Wiley, 2013).Many WMSs incorporate operators, which are predefined tasks enabling seamless\nintegration with a variety of technologies. These operators facilitate common and\nessential operations within workflows. For instance, a database operator allows users\nto establish connections and interact with specific types of databases like Postgres,\nRedshift, Snowflake, and BigQuery. This capability streamlines workflow execution\nby providing ready-made functionalities for interacting with diverse technological\nenvironments.\nNow that you have established a foundational understanding of workflows, let’s\nexplore some of the most common types of data workflows, outlining their key char‐\nacteristics, the technologies they employ, and their applications in the financial\ndomain.\nTypes of Financial Data Workflows\nThis section explores four fundamental types of data workflows that are central to\nfinancial data engineering: Extract-Transform-Load (ETL) workflows  streamline data\nmovement and transformation; microservices workflows  facilitate modular and scal‐\nable application architectures; machine learning workflows  automate model training\nand deployment pipelines; and streaming workflows  enable real-time data processing\nand analytics. Each type plays a critical role in enhancing efficiency and responsive‐\nness across diverse data-driven applications and industries. Let’s explore each of these\nworkflow types in more detail.\nExtract-Transform-Load Workflows\nThe predominant type of data workflow is Extract-Transform-Load (ETL), a three-\nphase workflow in which data is retrieved from one or more sources, transformed\nusing predefined business logic, and saved in a target destination.\nIn a typical ETL workflow, raw data files are initially extracted and stored in a staging\narea within a data lake. Following this, various transformations are applied to each\nfile. This can range from data quality checks and cleaning operations (e.g., drop\nduplicates, handle null values, remove erroneous records, standardize fields, etc.) to\ncomputation and data enrichment steps (e.g., scaling, normalization, feature engi‐\nneering, etc.). Finally, transformed data is stored in the enterprise data warehouse,\nwhich serves as the central repository from which data consumers access and utilize\nthe data they need.1 Figure 11-6  illustrates this ETL use case.\n414 | Chapter 11: Financial Data Workflows\n2For more on ETL patterns, I recommend Vasileios Theodorou, Alberto Abelló, Maik Thiele, and Wolfgang\nLehner’s “Frequent Patterns in ETL Workflows: An Empirical Approach” , Data & Knowledge Engineering  112\n(November 2017): 1–16.\nFigure 11-6. Illustration of the ETL workflow\nETL workflows may vary in terms of complexity and requirements. Traditional ETL\nworkflows are often built as linear DAGs that process data in batches with a prede‐\nfined schedule (e.g., daily or weekly). However, challenges can arise when new busi‐\nness problems create additional scalability and performance requirements. For\nexample, you might need to implement parallel processing in your workflow to\naccommodate larger data volumes. This can involve, for example, splitting a large file\ninto small chunks and transforming each into a separate dynamic task. Another\nmajor problem occurs when new types of data arrival processes must be handled.\nNowadays, it is common to have data arriving in regular batches, as well as single data\nentries generated through event-based mechanisms.2\nA large variety of ETL WMSs are available. These include commercial enterprise\ntools (e.g., IBM DataStage, Oracle Data Integrator, Talend, Informatica), open source\ntools (e.g., Apache Airflow, Prefect, Mage, Apache Spark), cloud-based tools (e.g.,\nAWS Glue, Google Dataprep), and custom tools. These tools vary in terms of the fea‐\ntures that we illustrated previously in “Workflow Management Systems” on page 410 .\nWith so many ETL tools available, it can be overwhelming to choose the right one. I\nrecommend starting by defining your workflows and identifying the features you\nneed based on your business requirements. Subsequently, evaluate the various\noptions in the context of your existing technological stack. For example, if you are\nrunning all of your stack on AWS, then you might benefit from using Amazon Man‐\naged Workflows for Apache Airflow or AWS Glue. Importantly, I highly recommend\nnot using a complex tool to perform basic ETL operations. It might be that all you\nneed is a simple job scheduler that runs a Python script on a provisioned server or\nmanaged services such as AWS Lambda or AWS Batch, or Python shell jobs in AWS\nGlue.\nTypes of Financial Data Workflows  | 415\nApache Airflow:  Why Is It So Popular?\nApache Airflow, developed and released by Airbnb in 2015, has become the leading\nopen source tool for data workflow management. Airflow can be deployed as a con‐\ntainerized application and is also available as a managed service from cloud providers\nlike Google and Amazon.\nAirflow’s high popularity is due to its extensive feature set and the frequent addition\nof new features and optimizations. It functions as an IaC tool, allowing workflows to\nbe defined as DAGs written in Python. In addition, it offers a wide range of operators,\ntriggers, customizable plug-ins, a flexible flow design (dynamic tasks, cross-DAG\ndependencies), SLA features, a security model, a rich UI, and operational control\noptions.\nApache Airflow is fundamentally an orchestration engine with great versatility. It’s\ndesigned to schedule, manage, and monitor different types of workflows, including\ndata pipelines, machine learning pipelines, batch processing pipelines, reporting and\nmonitoring pipelines, data migration, and data quality checks.\nLike most tools, Airflow has several drawbacks to consider. First, while Airflow sup‐\nports passing data between tasks using XCOM, it still does not offer a highly reliable\nand scalable method for cross-task data sharing. Second, Airflow is primarily\ndesigned for batch processing and does not natively support real-time or event-driven\ndata streams. Third, being a workflow orchestrator rather than a dedicated data pro‐\ncessing framework, Airflow lacks some performance and scalability features required\nfor handling large volumes of data. In such cases, integrating specialized tools like\nApache Spark for intensive data processing within an Airflow DAG can be a viable\noption.\nETL workflows are essential for financial institutions. They are extensively used to\nautomate the extraction of subscription-based data from financial data vendors.\nAnother common use case is data aggregation, where various types of data are peri‐\nodically retrieved from multiple sources and consolidated into a single repository,\nsuch as a data warehouse. ETL workflows also automate the generation of risk and\nanalytical reports for internal use, dashboarding systems, and compliance. Last but \nnot least, they are used in financial research involving historical data analysis and\nprocessing, financial analysis, risk calculations, portfolio reconciliation, and perfor‐\nmance tracking.\n416 | Chapter 11: Financial Data Workflows",7477
148-Stream Processing Workflows.pdf,148-Stream Processing Workflows,"Stream Processing Workflows\nA stream processing workflow consists of a sequence of data processing steps where\ndata flows as a continuous stream of events and is processed in real time, enabling\nimmediate reactions to new information. Figure 11-7  illustrates the concept visually.\nFigure 11-7. Illustration of a basic stream processing workflow\nStream workflow management solutions prioritize scalability for large data streams,\nfast transformations, in-memory computing, event-driven capabilities, and asynchro‐\nnous non-blocking data processing.\nA typical streaming architecture consists of four main components:\nIngestion layer\nHandles the real-time reception of incoming data traffic from various sources.\nFor example, a dedicated application can be deployed on AWS EC2 and load-\nbalanced using AWS Elastic Load Balancing (ELB).\nMessage broker\nStores the event stream and acts as a high-performance intermediary between\ndata producers and consumers. Examples include Apache Kafka and Google\nPub/Sub.\nStream processing engine\nConsumes data from a topic and applies transformations, analytics, or checks.\nExamples include Apache Storm, Apache Flink, Spark Streaming, and cloud\nfunctions such as AWS Lambda.\nData storage system\nPersists the data for further consumption. Examples include Apache Cassandra,\nAWS DynamoDB, Google Bigtable, and Google Firestore.\nFigure 11-8  illustrates this architecture.\nTypes of Financial Data Workflows  | 417\nFigure 11-8. Example of a typical stream processing architecture\nMore complex stream workflow architectures can be built. One common pattern is\nthe lambda architecture , which typically consists of three layers:\nSpeed layer\nHandles real-time stream processing and creates real-time views for instant\ninsights.\nBatch layer\nConsumes the same data stream to perform historical analysis, persisting a relia‐\nble view of the data (batch view).\nServing layer\nResponds to queries made to both the speed and batch views.\nThe three layers can be implemented in various ways, but a basic illustration is shown\nin Figure 11-9 . As depicted, data is first received by AWS ELB and then ingested into\na Kafka topic. Once the new data enters Kafka, it is simultaneously consumed by the\nbatch and speed layers’ consumer groups. The speed layer generates real-time views\nfor low-latency access to recent data, while the batch layer operates in a lower latency\nbut higher throughput mode. This ensures that data is prepared, cleaned, prepro‐\ncessed, aggregated, checked against existing historical data, and reliably persisted for\nlater use.\nFigure 11-9. Illustration of a lambda architecture\nLambda architectures are often criticized for their complexity and the maintenance\noverhead they entail. A simpler alternative, known as the kappa architecture ,\n418 | Chapter 11: Financial Data Workflows\n3To learn more about the Lambda and Kappa architectures, I highly recommend James Warren and Nathan\nMarz’s Big Data: Principles and Best Practices of Scalable Real-Time Data Systems  (Manning, 2015).\n4For a good discussion of this topic, I recommend “Fraud Detection with Apache Kafka, KSQL and Apache\nFlink” , by Kai Waehner. To better understand how to assess the requirements of stream processing systems, I\nrecommend the paper by Michael Stonebraker, U ǧur Çetintemel, and Stan Zdonik, “The 8 Requirements of\nReal-Time Stream Processing” , ACM SIGMOID Record  34, no. 4 (December 2005): 42–47.integrates  batch and real-time processing into a unified workflow that treats all data\nas a continuous stream. This approach leverages a single high-performance stream\nprocessing engine, simplifying both implementation and management.3 Figure 11-10\nprovides an example. Initially, data is received by AWS ELB and then ingested into a\nKafka topic. Once the data enters Kafka, it is consumed by the stream processing\nlayer, which utilizes an Apache Flink engine to process these events in real time, per‐\nforming various transformations. The processed data is subsequently stored in a Cas‐\nsandra database, allowing for efficient querying and further analysis.\nFigure 11-10. A kappa architecture\nStream processing finds numerous applications within the financial sector, with one\nprominent use case being real-time fraud detection and analytics. Financial institu‐\ntions face significant threats from payment and credit card fraud, as highlighted by\nthe 2023 Association for Financial Professionals Payments Fraud and Control Survey\nreport , underwritten by JPMorgan, which revealed that 65% of participants experi‐\nenced payment fraud attacks or attempts in 2022. Effective fraud detection systems\nnecessitate automation to reduce manual workload, high accuracy to minimize false\npositives and maintain customer satisfaction, scalability to handle large transaction\nvolumes, and speed for real-time assessment and transaction execution.\nReal-time stream processing architectures provide a robust solution to tackle these\nchallenges. For instance, combining Apache Kafka as a messaging stream broker with\nApache Spark ML for predictive modeling and Apache Spark Streaming for real-time\nstream processing can be highly effective. Similar architectures leveraging tools like\nApache Flink and Apache Storm also offer robust alternatives.4 Managed cloud\nstreaming solutions cater specifically to these needs, such as Azure Stream Analytics,\nwhich includes built-in machine learning capabilities for anomaly detection , Amazon\nManaged Services for Apache Flink, and Google Cloud Dataflow. In addition, AWS\nprovides Elastic MapReduce (EMR), a versatile platform enabling users to build and\nTypes of Financial Data Workflows  | 419",5727
149-Microservice Workflows.pdf,149-Microservice Workflows,"manage big data environments encompassing Apache Spark and Apache Flink,\namong other tools.\nAnother application of stream processing in financial markets is for powering data\nfeeds. Streaming architectures can be used to efficiently ingest, process, and analyze\nreal-time market data sourced from stock exchanges, data providers, and trading\nplatforms. Furthermore, financial institutions are progressively embracing streaming\nsolutions to update their data infrastructure. This involves migrating diverse opera‐\ntional data from legacy mainframe systems to cloud-based platforms, a process\nknown as mainframe offload .\nMicroservice Workflows\nIn Chapter 6 , you were introduced to the concept of microservices. As a reminder,\nmicroservices are small, self-contained, and independently deployable applications\nthat work together. There is a growing trend toward using microservice architectures\nto effectively implement and bring business ideas to life. Crucially, for microservices\nto collaborate effectively, coordination is essential. This typically involves defining\nand managing microservice workflows, which I will be discussing in this section.\nPrior to designing a microservice workflow, it’s crucial to assess several factors. First\nof all, determining what qualifies as a microservice requires careful consideration\nfrom both technical and business perspectives. From a technical standpoint, it’s\nimportant to strike a balance between high cohesion and low coupling. This means\nminimizing dependencies between microservices while ensuring that related applica‐\ntion logic remains cohesive within each microservice. Figure 11-11  illustrates the\nconcept.\nFigure 11-11. Coupling versus cohesion\n420 | Chapter 11: Financial Data Workflows\n5To learn more about how to use DDD in designing microservices, consult Microservices Patterns: With Exam‐\nples in Java  by Chris Richardson (Manning, 2018).From a business standpoint, a significant challenge involves deciding how to formal‐\nize and integrate business logic across multiple microservices. Various approaches\nexist for this purpose, including object-oriented principles such as the Single Respon‐\nsibility Principle (SRP), procedural patterns like the transaction script pattern, and\ncomprehensive frameworks such as Domain-Driven Design (DDD).\nDDD is quite elegant and provides a reliable approach for designing business logic in\na microservice architecture. In DDD, developers design the so-called domain models\nto capture and represent the business requirements and unique components of a\ndomain, establishing a conceptual basis for building software applications. A business\nmodel includes the entities, relationships, rules, and logic that define a specific\ndomain or problem space within an application.\nDDD strives to establish a ubiquitous language  that fosters shared understanding\namong domain experts and developers. Ubiquitous language helps bridge the com‐\nmunication gap between technical and nontechnical stakeholders, facilitating more\neffective collaboration and a clearer definition of requirements.\nAnother essential concept in DDD is bounded context  (BC), which can be thought of\nas the setting that defines the meaning of a word. Just as “bond” can refer to different\nthings depending on the context, a BC establishes clear and distinct definitions for\nterms and concepts within a specific domain area. This helps break down complex\ndomains into manageable subdomains, ensuring clarity and alignment with business\nneeds and allowing developers to focus on a specific area of the domain without\nbeing overwhelmed by the complexity of the entire domain.5\nAfter you have defined and created your microservices, the next step is to organize\nthem in a workflow. A microservice workflow coordinates multiple microservices to\nensure they operate following a specific business logic, thereby achieving cohesive\napplication functionality. To illustrate the idea, let’s consider a simple example of an\nonline store order processing system based on microservices, as depicted in\nFigure 11-12 . As shown in the figure, when a customer submits a purchase order, six\nmicroservices collaborate to manage the various stages of the order lifecycle.\nTypes of Financial Data Workflows  | 421\nFigure 11-12. Microservice-oriented online store order processing system\nIn real-world scenarios, microservice workflows frequently exhibit greater complexity\nthan the linear structure depicted in Figure 11-12 . This happens when microservices\nare deployed in a distributed system. Here, establishing reliable communication\nmechanisms becomes critical for coordinating microservices and maintaining work‐\nflow consistency. Traditional application designs typically rely on ACID database\ntransactions for this purpose. However, in distributed microservice systems, it is tech‐\nnically cumbersome to implement distributed ACID transactions.\nAs a reliable alternative, microservice engineers often rely on a popular design pat‐\ntern known as the saga pattern . A saga organizes a microservice workflow into a\nsequence of local transactions, where each transaction executes its logic and commu‐\nnicates with the next via update messages or events. Two primary approaches exist for\ncoordinating a saga: choreography and orchestration.\nIn choreography-based sagas, microservices directly communicate and coordinate\nwith each other to complete the workflow. Each microservice acts as both a partici‐\npant and coordinator within the collaborative process. This means that each micro‐\nservice needs to be aware of the business process, the services with whom to interact,\nthe tasks to execute, and the messages to exchange.\nOn the other hand, orchestration-based sagas introduce a central orchestrator to\nmanage the workflow execution. In this approach, participating microservices do not\nneed to be aware that they are part of a larger orchestration process. Instead, they\nsimply follow instructions provided by the orchestrator. Orchestration-based sagas\ncommonly leverage a message broker to facilitate communication and ensure the\norderly execution of tasks across distributed microservices.\n422 | Chapter 11: Financial Data Workflows\nLet’s look at the two saga patterns. Part (a) in Figure 11-13  represents a choreography-\nbased saga. In this instance, services interact directly with each other using specific\ntopics. For example, Services A and B communicate with Service E via Topic E, while\nServices E and F use Topic B to interact with Service B, and Service C communicates\nwith Service G through Topic G. In contrast, (b) in Figure 11-13  illustrates an\norchestration-based saga, where all service communication is mediated by a central\norchestrator. Each service receives instructions from the orchestrator via its designa‐\nted topic and communicates back to the orchestrator through the same topic.\nFigure 11-13. Choreography-based saga (a), and orchestration-based saga (b)\nIn terms of technological implementation, a microservice workflow management sys‐\ntem can be designed in various ways. If we consider the more prevalent\norchestration-based approach, a fundamental setup typically includes three\ncomponents:\n•A core orchestration engine responsible for managing workflows and facilitating\ncommunication among microservices. This engine could be a custom-built appli‐\ncation using languages like Java or Python, an open source tool such as Orkes\nConductor, or a cloud-managed solution such as AWS Step Functions or Google\nWorkflows.\n•A backend database to store details about workflow executions, ideally suited for\nOLTP systems such as PostgreSQL.\n•A message broker for handling the queuing and exchange of messages between\nmicroservices. Options include Apache Kafka, Google Pub/Sub, or Redis,\ndepending on specific requirements and preferences.\nTypes of Financial Data Workflows  | 423",7960
150-Machine Learning Workflows.pdf,150-Machine Learning Workflows,"6For an interesting case study from the financial sector, see how Danske Bank, the largest bank in Denmark,\nmoved from a monolith codebase into a microservice-oriented architecture, as presented in Antonio Buc‐\nchiarone, Nicola Dragoni, Schahram Dustdar, Stephan T. Larsen, and Manuel Mazzara’s “From Monolithic to\nMicroservices: An Experience Report from the Banking Domain” , IEEE Software  35, no. 3 (May–June 2018):\n50–55.\nKeep in mind that the simpler your workflows are, the easier it will\nbe to choose and design a workflow management solution. If you\nfind yourself in a situation where existing solutions do not meet\nyour microservice workflow requirements, it may be beneficial to\ncarefully review and streamline the underlying structure and logic\nof your workflows before opting for more complex or custom\nsolutions.\nMicroservices have been making a substantial impact across diverse industries,\nincluding the financial sector. A key driver behind this is the emergence of FinTech\nfirms, leveraging cutting-edge technologies and design patterns to create disruptive\nfinancial solutions. By segmenting their applications into smaller microservices, Fin‐\nTech firms gain the required agility to develop, update, and deploy individual func‐\ntionalities independently.\nMeanwhile, traditional financial institutions are embracing microservices as part of\ntheir digital transformation and reengineering endeavors aimed at fostering innova‐\ntion while adapting to the evolving technological landscape. A prominent trend in\nthis realm is the advent of platform and open banking , wherein banks evolve into\ninterconnected ecosystems of financial services. This often involves establishing stra‐\ntegic partnerships with FinTech firms to integrate and deploy their offerings seam‐\nlessly within the traditional banking infrastructure.\nThrough the utilization of microservices, banks can initiate new collaborations with\nFinTech firms by creating isolated microservices for each distinct application, facili‐\ntating streamlined integration and enabling rapid deployment of innovative financial\nsolutions.6\nMachine Learning Workflows\nA machine learning project involves applying scientific techniques and algorithms to\nanalyze and detect patterns from the data, build predictive models, or automate\ndecision-making processes. These projects typically include stages such as data collec‐\ntion, preprocessing, model selection, training, testing, evaluation, and deployment.\nGiven their structured and logical nature, ML projects are often organized as data\n424 | Chapter 11: Financial Data Workflows\n7For a more detailed discussion of the value and the need for ML workflows, I suggest Hui Miao, Ang Li, Larry\nS. Davis, and Amol Deshpande’s “Towards Unified Data and Lifecycle Management for Deep Learning” , in the\n2017 IEEE 33rd International Conference on Data Engineering (ICDE)  (IEEE, 2017): 571–582.workflows to ensure their systematic and effective execution as well as optimal data\nand lifecycle management.7\nIn real-world scenarios, ML workflows can be designed in multiple ways, depending\non the unique business needs, data characteristics, and technical requirements. To\nprovide a basic perspective, I will divide the stages involved in an ML workflow into\nthree categories: data related, modeling related, and deployment related. Let’s explore\neach category in some detail.\nData-related steps:\nData extraction\nThis very first step involves identifying and extracting all required data from var‐\nious sources, such as databases, APIs, or files.\nQuality checks\nOnce the data is extracted, quality checks are performed to ensure multiple qual‐\nity dimensions such as accuracy, validity, completeness, timeliness, and many\nmore.\nPreprocessing\nOnce quality checks are completed, preprocessing steps are applied to get the\ndata ready for model training. This includes tasks such as feature engineering,\nscaling, normalization, encoding, embedding, enrichment, imputation, and many\nmore.\nModeling steps:\nModel selection\nIn this phase, you choose the appropriate machine learning algorithms, models,\nand optimization techniques based on the nature of your business problem and\nneeds, data quality attributes, and performance requirements.\nTraining\nThe selected model is trained on the preprocessed data to learn meaningful pat‐\nterns from the data and achieve generalizability.\nEvaluation\nOnce trained, the model’s performance is evaluated using various metrics and\ntechniques to assess its ability to generalize to new, unseen data.\nTypes of Financial Data Workflows  | 425\nModel deployment steps:\nDeployment\nAfter successful evaluation, the trained model is packaged and deployed into pro‐\nduction or operational environments, where it can process requests for making\npredictions or classifications on new data.\nServing\nThe deployed model is exposed to its final consumers via APIs or other interfaces\nto serve predictions in real-time or batch mode, depending on the business\nrequirements at hand.\nModel feedback\nContinuous monitoring and feedback mechanisms are put in place to assess the\nmodel’s performance in production, collect feedback from users, and introduce\nimprovements or updates as necessary.\nFor production ML workflows, additional features are often required. First, a model\nregistry  is often implemented to store and version persistent various ML workflow\nsteps, including their code, parameters, and data output. This is useful as it enables a\nbusiness to keep track of historical workflows, ensure point-in-time reproducibility,\nshare ML models across teams, and ensure compliance and transparency.\nSecond, a highly desired feature of ML workflows is checkpointing . This involves peri‐\nodically saving the workflow’s state—including model parameters, data processing\nstages, and execution context—to persistent storage. In case of a failure, this allows\nthe workflow to reload and resume from the last saved checkpoint.\nThird, ML data modeling often requires a specialized, analytical approach. Feature\nstores  represent a formal method for implementing this. A feature store is a central‐\nized repository for storing, managing, and serving precomputed and curated\nmachine learning features. Feature stores provide several benefits. First, they enable\nfeature reuse by storing developed features for quick access and sharing across ML\nmodels and teams, thereby saving time and fostering efficiency in model develop‐\nment and cross-team cooperation. Second, they ensure feature consistency by cen‐\ntralizing feature definitions and development documentation, which helps maintain\nuniformity across large organizations. Last, they enhance security and data gover‐\nnance by tracking the data used for model training and deployment.\nFourth, an ML workflow often demands specific computing resources to ensure opti‐\nmal performance. This can entail leveraging advanced technologies like GPUs for\naccelerated computation, distributed and parallel computing frameworks for han‐\ndling large-scale data processing tasks efficiently, and specialized data storage systems\nsuch as vector databases, which store data as vector embeddings for fast retrieval and\nsimilarity search.\n426 | Chapter 11: Financial Data Workflows\nFinally, ensuring the stability, automation, and quality of an ML workflow’s deploy‐\nment and performance requires incorporating software engineering and MLOps best\npractices. MLOps, short for machine learning operations, encompasses methodolo‐\ngies and tools aimed at automating and optimizing the deployment and management\nof ML workflows.\nFigure 11-14  illustrates the several stages involved in a typical machine learning pipe‐\nline as well as the undercurrents that underpin a reliable ML workflow management\nsystem.\nFigure 11-14. ML workflow  lifecycle\nDesigners of ML workflows may need to integrate various domain-specific require‐\nments. For instance, financial institutions must adhere to regulations and guidelines\nconcerning fairness, transparency, and accountability in algorithmic decision-\nTypes of Financial Data Workflows  | 427\nmaking. An example is the US Equal Credit Opportunity Act of 1974, which man‐\ndates creditors to provide applicants with specific reasons for credit denials or altered\nterms upon request. To comply, an ML workflow must include features to track,\nretrieve, and explain data, model mechanics, assumptions, and predictions.\nPrivacy-Preserving Machine Learning Workflows  in Finance\nIn financial markets, the sensitive nature of financial data, coupled with growing pub‐\nlic demand for privacy protection and the introduction of stringent privacy regulation\nworldwide, has imposed restrictions on how and what data can be analyzed, pro‐\ncessed, and shared.\nTo address these issues, financial ML workflows must integrate privacy-preserving\nfeatures to ensure that sensitive data used in machine learning tasks remains secure\nand confidential throughout the various phases of the workflow. Key techniques\ninclude the following:\nHomomorphic encryption\nEnables computation on encrypted data without decrypting it, preserving data\nconfidentiality during processing. This technique typically requires complex\nmathematical operations that can significantly slow down processing speed and\nincrease computational overhead.\nDifferential  privacy\nIntroduces noise to query results to protect individual data privacy while main‐\ntaining statistical accuracy.\nSecure multiparty computation\nEnables computations across multiple parties without revealing each party’s pri‐\nvate data to the others. This protocol ensures that each party can contribute their\ndata securely to the computation process while maintaining confidentiality.\nFederated learning\nTrains machine learning models on decentralized data sources without exchang‐\ning raw data, thereby preserving privacy.\nSynthetic data generation\nCreates artificial data that retains statistical properties of the original dataset\nwhile protecting sensitive information.\nImplementing these techniques ensures that financial machine learning workflows\nuphold privacy standards, fostering responsible data usage and enhancing security\nmeasures in the finance sector.\n428 | Chapter 11: Financial Data Workflows",10374
151-Summary.pdf,151-Summary,"8To learn more about the data-related challenges encountered in ML workflow design, I recommend Neoklis\nPolyzotis, Sudip Roy, Steven Euijong Whang, and Martin Zinkevich’s “Data Lifecycle Challenges in Produc‐\ntion Machine Learning: A Survey” , ACM SIGMOD Record  47, no. 2 (December 2018): 17–28.The challenges involved in building reliable and high-performance ML models\nlargely revolve around effective data management and ensuring data quality. As a\nfinancial data engineer, your responsibility is crucial in establishing robust ML work‐\nflows. These workflows are indispensable in today’s financial markets, where the inte‐\ngration of ML and AI technologies stands as the primary driver of transformation\nand innovation.8\nSummary\nThis chapter provided a comprehensive examination of financial data workflows and\ntheir fundamental concepts. It started by emphasizing the value and need for work‐\nflows through the concept of workflow-oriented software architectures. The chapter\nthen defined both data workflows and workflow management systems, highlighting\ntheir key features. Finally, it provided an in-depth examination of the main types of\ndata workflows used within the financial sector, offering comprehensive insights into\ntheir fundamental concepts and applications. These are ETL, microservices, stream\nprocessing, and machine learning workflows.\nCongratulations on reaching this point of the book. Y our thorough understanding of\nthe foundational concepts of financial data engineering has prepared you for the\npractical hands-on experience that awaits in the next and final chapter, where you\nwill work on four engaging hands-on projects.\nSummary | 429",1687
152-Chapter 12. Hands-On Projects.pdf,152-Chapter 12. Hands-On Projects,,0
153-Project 1 Designing a Bank Account Management System Database with PostgreSQL.pdf,153-Project 1 Designing a Bank Account Management System Database with PostgreSQL,"CHAPTER 12\nHands-On Projects\nNow that you’ve gained a fundamental grasp of financial data engineering, it’s time to\nput it into practice. In this chapter, you will go through a series of practical projects\ndesigned to give you firsthand experience working with financial data.\nFour projects will be discussed, each focusing on a different problem and employing a\nunique technological stack:\n1.Constructing a bank account management system with PostgreSQL\n2.Building a financial data ETL workflow with Mage\n3.Developing a financial microservice workflow with Netflix Conductor\n4.Implementing a reference data store with OpenFIGI, PermID, and GLEIF APIs\nA few points should be mentioned about these projects. First, they are meant to pro‐\nvide hands-on experience with financial data engineering and may not necessarily\nrepresent complete solutions for real business problems. Additionally, they are\nintended to be executed locally on your machine and not deployed in a production\nenvironment. Finally, the employed technologies are not indicative of the author’s\npersonal preferences; instead, they reflect the author’s prudent consideration of what\nwould best clarify the problems and solutions for the reader.\nPrerequisites\nAll projects in this chapter will be packaged and isolated with all their dependencies\ninto Docker containers. Docker is the most popular open source software for operat‐\ning system (OS) virtualization (aka containerization). It is available for Windows,\nmacOS, and Linux through Docker Desktop . Therefore, to run the projects on your\nmachine, you must install the Docker Desktop version that is compatible with your\n431",1670
154-Conceptual Model Business Requirements.pdf,154-Conceptual Model Business Requirements,"operating system. Please follow the instructions in the official Docker documentation\nto complete the installation. Furthermore, as you will be running more than one con‐\ntainer in each project, you will be using a specific multicontainer orchestration tool\ncalled Docker Compose , which is ideal for local testing and development.\nThe other prerequisite is to clone this book’s GitHub repository onto your local\nmachine. First, make sure you have Git installed. Then, open a terminal in your com‐\nputer, navigate to the location where you want to pull the project files, and finally,\nclone the repository .\nProject 1: Designing a Bank Account Management System\nDatabase with PostgreSQL\nAccount management is a core functionality for banks, allowing them to handle and\noversee customer accounts, balances, and transactions. In this project, you will design\nand implement a relational database system for managing bank accounts. Y ou will be\nusing the conceptual/logical/physical data modeling approach discussed in Chapter 8 .\nConceptual Model: Business Requirements\nIn the conceptual phase, the focus is on understanding business requirements and\ndefining the high-level structure of the database system. Operational and business\nmodels vary from one banking institution to another, and not all banks offer the\nsame products and services. As such, you will design a simple and generic bank\naccount management system for this project.\nWe’ll assume that stakeholders have defined their requirements and that these have\nbeen formalized them into entities, relationships, and constraints. Let’s explore each\nin greater depth.\nEntities\nOur bank account management system needs to store data for seven types of entities:\naccounts, customers, loans, transactions, branches, employees, and cards.\nAccounts are the most essential product that banks offer to their customers. For this\nreason, the account entity is needed to maintain detailed records of the various\naccount types offered by the bank (e.g., savings, checking), along with account IDs,\nassociated customers, balances, and more.\n432 | Chapter 12: Hands-On Projects\nLoans are also a key banking product. Thus, the loan entity is needed to store infor‐\nmation such as loan ID, terms (including amount, duration, interest rate, payment\nschedule, and start and end dates), type (such as mortgage or personal loan), and\nother relevant details.\nIn addition to accounts and loans, most banks provide a range of payment cards to\ntheir clients. As a result, the card entity is crucial for storing card-related information,\nincluding cardholder ID, associated accounts, card numbers, issuance and expiration\ndates, and other relevant details.\nCustomers represent the bank’s client base, including both individuals and organiza‐\ntions. Therefore, the customer entity is essential for storing vital client information,\nsuch as IDs, names, addresses, statuses (e.g., active, inactive), and contact details (e.g.,\nemail addresses and phone numbers).\nEmployees are the individuals hired by the bank to deliver various services to cus‐\ntomers. To manage employee information, the employee entity is required to store\nattributes such as employee IDs, names, job titles, and the branch where they are\nassigned.\nBranches are the bank’s physical locations where customers engage with employees to\nperform various transactions. The branch entity is essential for managing branch\ninformation, including branch IDs, names, addresses, and phone numbers.\nLastly, banks must track all transactions associated with customer accounts. There‐\nfore, the transaction entity is vital for recording transaction details such as transac‐\ntion IDs, associated account IDs, employee IDs (for transactions initiated by\nemployees), timestamps, and amounts.\nRelationships\nThe business team has requested that the following relationships be established\nbetween entities:\n•Each account should be linked to a customer.\n•Each loan should be linked to a customer.\n•Each loan payment should be linked to a transaction and an account.\n•Each employee should be affiliated with a branch.\n•Each card should be associated with both a customer and an account.\nThese relationships ensure data integrity and facilitate efficient data retrieval and\nmanagement.\nProject 1: Designing a Bank Account Management System Database with PostgreSQL | 433",4409
155-Logical Model Entity Relationship Diagram.pdf,155-Logical Model Entity Relationship Diagram,"Constraints\nThe business team has outlined the following constraints to be enforced within the\ndatabase implementation:\n•Account balances must never go below a customer-specific minimum amount.\n•All entity records (e.g., accounts, loans, transactions) must be identified with a\nunique ID.\n•Data redundancy should be minimized.\n•Null values are not permitted for certain fields.\n•Specific fields, such as email addresses, must be unique across records.\nConstraints play a pivotal role in ensuring high data quality, consistency, integrity,\nand compliance, which in turn contributes to the reliability of the account manage‐\nment system.\nLogical Model: Entity Relationship Diagram\nNow that we have stakeholder agreement on the account management conceptual\nmodel, the next step is to build the logical model. This stage focuses on selecting the\ndata storage model most suitable for our system (a concept discussed in Chapter 8 ).\nAfter a thorough evaluation, the financial data engineering team has concluded that\nthe relational model is the best fit. This model effectively organizes various entities\ninto distinct tables and ensures data integrity through database constraints. Moreover,\nthe relational model supports the implementation of a normalized data structure,\nwhich is essential for our system.\nTo implement this, we need to create the Entity Relationship Diagram  (ERD) for our\nlogical model. An ERD is a visual representation used to model the structure of a\ndatabase. It illustrates the entities (such as tables or objects) within a system, their\nattributes (such as fields or properties), and the relationships among these entities.\nAn ERD is constructed using information collected and summarized from the previ‐\nous conceptual phase. Various tools are available for drawing ERDs, including Lucid‐\nchart, Creately, DBDiagram, ERDPlus, DrawSQL, QuickDBD, and EdrawMax.\nFigure 12-1  illustrates the ERD of our account management system.\n434 | Chapter 12: Hands-On Projects\nFigure 12-1. Entity Relationship Diagram of our example bank account management\nsystem\nProject 1: Designing a Bank Account Management System Database with PostgreSQL | 435",2188
156-Physical Model Data Definition and Manipulation Language.pdf,156-Physical Model Data Definition and Manipulation Language,,0
157-Project 1 Local Testing.pdf,157-Project 1 Local Testing,"With the ERD design complete, let’s move on to the next step: selecting the database\ntechnology and translating our ERD into database queries.\nPhysical Model: Data Definition  and Manipulation Language\nWith our logical data model ready, it’s time to choose a relational database technology\nand write the queries to create and populate our tables. Let’s assume the financial data\nengineering team has selected PostgreSQL as the database management system due to\nits high reliability and strong adherence to SQL standards.\nNow, let’s shift gears and test things on your machine.\nProject 1: Local Testing\nTo interact with PostgreSQL, you will need to run two Docker containers: one for the\nPostgreSQL database instance and another for pgAdmin, a user-friendly UI for inter‐\nacting with PostgreSQL.\nTo launch the two containers, open a terminal and navigate to the following path:\n# Bash\ncd {path/to/book/repo}/FinancialDataEngineering/book/chapter_12/project_1\nThen execute the following docker compose  command  to run our project containers:\n# Bash\ndocker compose up -d\nAfter that, open a browser and paste the URL http://localhost:8080/  into a new tab.\nWait until you see the pgAdmin UI and log in using the dummy credentials (PGAD‐\nMIN_DEFAULT_EMAIL and PGADMIN_DEFAULT_PASSWORD) in the .env file.\nRemember that this setting is for local testing purposes, and you should never share\nor store passwords publicly or explicitly in your project files.\nOnce logged in, click on Add New Server and insert the following values:\n•In the General tab, give a name to the server (e.g., Bank Account).\n•In the Connections tab, set:\n—Host name/address = postgres\n—Port = 5432\n—Maintenance database = bank_account\n—Username = admin\n—Password = password\n436 | Chapter 12: Hands-On Projects\nThese are the values that you set in the project’s environmental variables file .env.  In\nthe upcoming projects, we’ll utilize pgAdmin, and you’ll need to configure it by fol‐\nlowing similar steps. However, you’ll need to use the credentials provided in the .env\nfile specific to each project. Once completed, click on Save. Figure 12-2  illustrates\nthese steps.\nFigure 12-2. Creating a new server in pgAdmin\nAfter that, from the left sidebar, follow these steps (as shown in Figure 12-3 ):\n1.Navigate to Servers → Bank Account → Databases → bank_account → Schemas\n→ public → Tables.\n2.Right-click on Tables and select Query Tool.\nNow, we are ready to create our tables. In SQL, using the term Data Definition  Lan‐\nguage  (DDL) to denote statements that create or alter database objects is common.\nDDL statements include CREATE , ALTER , and DROP . The full list of table creation quer‐\nies is available on this book’s GitHub repo . Y ou will need to copy and paste all the\nqueries into the Query Tool and click the Run arrow on the top, as illustrated in\nFigure 12-3 . This will create all the tables for our bank account management system.\nTo see the tables, right-click Tables from the left sidebar and hit Refresh. Then you\nwill be able to see the 16 tables, as illustrated in Figure 12-4 .\nProject 1: Designing a Bank Account Management System Database with PostgreSQL | 437\nFigure 12-3. Opening the Query Tool editor\nFigure 12-4. Creation and visualization of the tables\n438 | Chapter 12: Hands-On Projects\nIn this project, you’ll manually copy and execute SQL commands\ndirectly within pgAdmin’s query console. This approach was inten‐\ntionally chosen to familiarize you with the SQL syntax. It’s worth\nnoting that databases can also be interacted with programmatically.\nFor example, in the upcoming projects, we’ll utilize Python’s data‐\nbase driver, psycopg2, to interact with PostgreSQL.\nAs you will see, 16 tables were created. To have an idea about our DDL queries, let’s\ntake a closer look at the CREATE TABLE  statement for the account table:\n-- PostgreSQL\nCREATE TABLE Account (\n    id SERIAL PRIMARY KEY,\n    customer_id  INT NOT NULL REFERENCES  Customer (id),\n    branch_id  INT NOT NULL REFERENCES  Branch(id),\n    type INT NOT NULL REFERENCES  AccountType (id),\n    currency  VARCHAR NOT NULL,\n    number VARCHAR NOT NULL UNIQUE,\n    balance DECIMAL(10,2) NOT NULL,\n    minimum_balance  DECIMAL(10,2) NOT NULL DEFAULT 0,\n    date_opened  DATE NOT NULL,\n    date_closed  DATE,\n    status INT NOT NULL REFERENCES  AccountStatusType (id)\n    CHECK (balance >= minimum_balance )\n);\nAs you can see, the table has a serial ID as a primary key. This is used to automatically\ngenerate an increasing sequence of integers. Moreover, the table has four foreign keys\nreferencing the IDs of four distinct tables: Customer, Branch, AccountType, and\nAccountStatusType. These keys are crucial for ensuring data integrity. Should, for\nexample, a customer with ID 202 not exist in the customer table, the Customer ID\nforeign key will prevent the creation of an account associated with this customer.\nLastly, a key feature of this table is the account balance, which specifies a minimum\nbalance requirement for each customer. A check constraint is added to ensure the\nbalance never falls below the customer-specific minimum. Any attempt to update the\nbalance to a value lower than the specified minimum will result in an error.\nNow, let’s populate the tables with some data. In SQL, it is common to use the term\nDDL to indicate statements that add, update, and delete records in a database. For\nmost tables in our system, the insert operations are quite simple. For example, to add\na new customer, you would need to run the following:\n-- PostgreSQL\nINSERT INTO Customer  (\n  customer_type_id , name, country, city, street, \n  phone_number , email, status\nProject 1: Designing a Bank Account Management System Database with PostgreSQL | 439\n) VALUES (\n  1, 'John Smith' , 'US', 'New York' ,\n  '123 Main St' , '123-456-7890' , 'john@example.com' , 1\n);\nThe full script of the insert queries is available in this GitHub file. Copy all the insert\nqueries, paste them into the Query Tool, and run the operation again. To see the data,\nright-click on a table name → View/Edit Data → All Rows. See Figure 12-5  for an\nillustration.\nFigure 12-5. Visualization of table rows\nInterestingly, some insert queries consist of multiple statements. For example, a pay‐\nment transaction needs to be recorded in the transactions table and requires a con‐\ncurrent update of the account balance in the account table. To this end, both\nstatements need to be wrapped with an explicit SQL transaction. It is called explicit\nbecause PostgreSQL executes any statement by default within an implicit transaction.\nHere is an example where a transaction of 40 USD is recorded for the account with\nID = 1:\n-- PostgreSQL\nBEGIN;\nINSERT INTO transaction  (account_id , type, currency , date, amount)\nVALUES (1, 4, 'USD', '2022-01-01 08:00:00' , -40.00);\nUPDATE account\nSET balance = balance - 40.00\nWHERE id = 1;\nCOMMIT;\n440 | Chapter 12: Hands-On Projects",7021
158-Project 1 Summary.pdf,158-Project 1 Summary,"Last but not least, an even more complex operation is the loan payment. Here, we\nneed to record the payment in the loan payment table, then store it as a transaction in\nthe transaction table, and finally update the customer’s balance in the account table.\nTo illustrate, here is an example of a loan payment of 1,000 USD recorded for the\naccount with ID = 1:\n-- PostgreSQL\nBEGIN;\nDO $$\nDECLARE\n    payment_amount  DECIMAL := 1000.00;\nBEGIN\n    WITH inserted_transaction  AS (\n        INSERT INTO Transaction  (account_id , type, currency , amount)\n        VALUES (1, 1, 'USD', -payment_amount )\n        RETURNING  id\n    )\n    INSERT INTO LoanPayment  (\n      loan_id, transaction_id , payment_amount , scheduled_payment_date , \n      payment_date , principal_amount , interest_amount , paid_amount\n    )\n    SELECT \n      1, id, payment_amount , '2022-04-01' , '2022-04-01' , \n      900.00, 100.00, payment_amount\n    FROM inserted_transaction ;\n \n    UPDATE account\n    SET balance = balance - payment_amount\n    WHERE id = 1;\nEND$$;\nCOMMIT;\nTo create more transactions, run the queries in this GitHub file . The query for creat‐\ning a loan payment is available in this book’s GitHub repo .\nProject 1: Clean Up\nOnce you are done with the project and you want to move to the next one, make sure\nyou run the following command  in the same root directory of Project 1:\n# Bash\ndocker compose down\nThis command will stop and remove containers, networks, volumes, and images cre‐\nated by docker compose up .\nProject 1: Designing a Bank Account Management System Database with PostgreSQL | 441",1627
159-Project 2 Designing a Financial Data ETL Workflow with Mage and Python.pdf,159-Project 2 Designing a Financial Data ETL Workflow with Mage and Python,,0
160-Project 2 Local Testing.pdf,160-Project 2 Local Testing,"Project 1: Summary\nThe goal for this project was to familiarize you with practical data modeling, database\ndesign, DDL and DML (Data Manipulation Language), and multistatement transac‐\ntions. While this project provided a foundational understanding, it’s important to\nnote that a real-world bank account management system demands more extensive\ndevelopment. For example, implementing database security features like user roles\nand permissions, authentication, and data encryption will be necessary. Moreover,\nyou will need a database optimization strategy that involves indexing, clustering, and\ntyping. Finally, you will need to establish a reliable integration between the database\nand your application. This involves using database connectors, clients, connection\npooling, object-relational mappers (e.g., SQLAlchemy), and more. Tackling all of\nthese features is beyond the scope of this book and would require a lot more detailed\nexplanation.\nProject 2: Designing a Financial Data ETL Workflow  with\nMage and Python\nIn this project, you will design and implement a financial data ETL workflow. This\ninvolves retrieving historical stock price data from a financial API, applying various\ntransformations, and ultimately storing the processed data in a database.\nMore specifically, we will be fetching the intraday open, close, high, and low prices,\nalongside trading volume, for four prominent stocks: Google, Amazon, IBM, and\nApple. The source of our data will be the free version of the Alpha Vantage Stock\nMarket API . Subsequently, we’ll aggregate the intraday values to derive daily averages.\nBoth the raw data and the transformed daily data will be stored in a PostgreSQL data‐\nbase. Let’s assume that the workflow must be run once at the start of each month to\nretrieve data for the previous month.\nProject 2: Workflow  Definition\nThis project’s workflow will have a linear structure consisting of the following steps:\n1.Data retrieval\n•Fetch adjusted intraday time series history for the past month using the\nTIME_SERIES_INTRADAY  endpoint . As the free API version allows for 25\nrequests per day, we’ll query data for 4 stocks: Amazon, IBM, Apple, and\nGoogle. The API request specifies the following parameters:\n—A one-minute time interval between consecutive data points.\n—adjusted=true  to retrieve a time series adjusted by historical split and divi‐\ndend distributions.\n442 | Chapter 12: Hands-On Projects\n—The month parameter is set to the past month for monthly data retrieval.\n—outputsize=full  to obtain the full intraday history of the specified month.\n—datatype=csv  to receive the time series as a CSV file.\n2.Raw data storage\n•Store the raw data in the database, recording the ingestion timestamp.\n3.Data aggregation\n•Select the required columns for the primary transformation task, which will\ninvolve aggregating intraday values to compute daily averages.\n•Compute daily aggregates by averaging the open, close, high, low, and volume\ncolumns, grouped by date and ticker symbol.\n4.Deduplication\n•Select columns for deduplication to address the aggregation step’s behavior of\ncomputing aggregates without row grouping, where each row receives the cor‐\nresponding aggregate of its group.\n•Deduplicate the data, retaining the aggregated columns\n5.Data export\n•Export the daily averages to the database for further analysis.\nWe will execute each one of these steps in a linear sequence.\nProject 2: Database Design\nIn our database, we will need two tables: one to store the raw intraday data retrieved\nfrom the API and another to store the transformed daily data produced at the end of\nour workflow. Figure 12-6  illustrates the ERD of these tables.\nFigure 12-6. ERD of the API stock data\nProject 2: Designing a Financial Data ETL Workflow  with Mage and Python | 443\nLet’s move ahead to test our workflow in action.\nProject 2: Local Testing\nTo begin testing, the initial step is to claim your Alpha Vantage API key on the Alpha\nVantage website . Follow the instructions you find on the page, and you will receive an\nAPI key that you need to keep as a secret and never share it with anyone else.\nOnce you have the API key, you will need to navigate to the project’s main directory.\nAs before, this can be done by typing in the following command in your terminal:\n# Bash\ncd {path/to/book/repo}/FinancialDataEngineering/book/chapter_12/project_2\nOnce you are in the main project directory, you will find an .env file with a variable\ncalled ALPHAVANTAGE_API_KEY . Without using quotes, you must assign this variable\nthe API key you get from Alpha Vantage (i.e., ALPHAVANTAGE _API\n_KEY=YOUR_API_KEY ).\nAfter that, run the project containers with the following command:\ndocker compose up -d\nOnce completed, you will have three running containers:\n•Mage  running on http://localhost:6789/ . Mage is an open source data pipeline\ntool renowned for its user-friendly UI, ease of use, and extensive feature set,\nmaking it an ideal option for ETL workflows.\n•A PostgreSQL instance where we will store the data.\n•The pgAdmin client running on http://localhost:8080/ . As before, you will use\npgAdmin to create the project tables and explore the outcome of your workflow\ndata outputs.\nTo begin, let’s create our tables. Open a tab in your browser and navigate to http://\nlocalhost:8080/ . Log in with the PGADMIN_DEFAULT_EMAIL  and PGADMIN_DEFAULT_PASS\nWORD  that you have in your project’s .env file. Follow the same steps as you did in the\nfirst project to create a server, and from the left sidebar, open the database called\nstock_data , then navigate to the schema called public . From there, you should be able\nto open a query tool and execute the queries you find in the create_tables.sql  file to\ncreate the tables.\nOnce the tables are created, it’s time to move on to build our ETL workflow. Open\nanother browser tab and navigate to http://localhost:6789/ . Y ou should be able to see\nMage’s overview page, as illustrated in Figure 12-7 . From the top left, click on “+ New\npipeline” and select “Standard (batch). ” Y ou will be asked to give your pipeline a\nname; let’s call it Adjusted Stock Data.  Once done, click on Create.\n444 | Chapter 12: Hands-On Projects\nFigure 12-7. Mage overview page\nAfter creating your pipeline, you will be redirected to the pipeline edit page, as shown\nin Figure 12-8 .\nFigure 12-8. Pipeline edit page\nFrom the left sidebar, open the file called io_config.yaml , delete all its content, and\nadd the content of the io_config.yaml  file to it. Once done, from the top menu, click\nFile → “Save file” and then close the file editor. The io_config  file is used by Mage to\nstore and access credentials required to connect to databases and various storage\nsystems.\nProject 2: Designing a Financial Data ETL Workflow  with Mage and Python | 445\nAfter this, you will need to create the different components of our ETL workflow.\nFrom the same pipeline edit page ( Figure 12-8 ), you can see different buttons that can\nbe used to create an operator, such as data loader, exporter, and transformer. Let’s cre‐\nate what we need as follows:\n1.Create the API data loader:\na.Click on +Data loader → Python → API.\nb.Name it Load Data from Alpha Vantage  and then click “Save and add. ”\nc.In the code editor that appears, delete all code and replace it with the code in\nfetch_intraday_data.py .\nd.From the top menu, click File → Save pipeline.\ne.Use the small up arrow (^) on the top right of the data loader block to close it\nso you can better see the structure of your pipeline.\nf.In the subsequent phases, repeat steps d and e.\n2.Create the raw data exporter:\na.Click on +Data exporter → Python → PostgreSQL and call it Export Raw\nData .\nb.Delete the content of the editor that appears and replace it with the code in\nexport_intraday_data.py .\n3.Create the aggregation column selection transformer:\na.Click on +Transformer → Python → Column removal → Keep columns and\nname it Select Aggregation Columns.\nb.Paste the code from select_columns_for_aggregation.py  into the editor.\n4.Perform the aggregation:\na.Click on +Transformer → Python → Aggregate → Aggregate by average value\nand name it Compute Averages.\nb.Paste the code from compute_daily_aggregates.py  into the editor.\n5.Create the deduplication column selection transformer:\na.Click on +Transformer → Python → Column removal → Keep columns and\nname it Select Deduplication Columns .\nb.Paste the code from select_columns_for_deduplication.py  into the editor.\n446 | Chapter 12: Hands-On Projects\n6.Drop the columns that contain duplicates:\na.Click on +Transformer → Python → Rows actions → Drop duplicates and\nname it Drop Duplicates.\nb.Paste the code from drop_duplicates.py  into the editor.\n7.Export the daily average data to the database:\na.Click on +Data exporter → Python → PostgreSQL and call it Export Daily\nData.\nb.Paste the code from export_daily_data.py  into the editor.\nOnce these steps are done, your workflow is complete and ready to be executed. It\nshould look like Figure 12-9 .\nFigure 12-9. Overview of the full financial  data pipeline\nTo perform a workflow execution test, navigate to the Pipelines section using Mage’s\nleft sidebar. Locate your pipeline named adjusted_stock_data. Click on the pipeline\nname, then select Run@once from the top menu, and confirm by clicking “Run now. ”\nA new execution line with a random name will appear in the table. Click on the name\ndisplayed, then patiently observe the execution progress as it advances through the\nseven steps.\nOnce completed with success, navigate to the pgAdmin tab in your browser and\ncheck the data in the database. Y ou should see that both tables are populated with the\ndata produced by your pipeline.\nProject 2: Designing a Financial Data ETL Workflow  with Mage and Python | 447",9909
161-Project 2 Clean Up.pdf,161-Project 2 Clean Up,,0
162-Project 2 Summary.pdf,162-Project 2 Summary,,0
163-Project 3 Designing a Microservice Workflow with Netflix Conductor PostgreSQL and Python.pdf,163-Project 3 Designing a Microservice Workflow with Netflix Conductor PostgreSQL and Python,,0
164-Project 3 Workflow Definition.pdf,164-Project 3 Workflow Definition,"Project 2: Clean Up\nRun the following command in the same root directory of Project 2:\n# Bash\ndocker compose down\nProject 2: Summary\nThroughout this project, you’ve gained hands-on experience building a financial ETL\nworkflow with Alpha Vantage API, Mage, Python, and PostgreSQL. In real-world sce‐\nnarios, you’ll probably design and build similar pipelines. Y et, you will very likely\nneed to incorporate more complex transformers and data quality validations. Addi‐\ntionally, you’ll need to deal with challenges in pipeline management, such as schedul‐\ning, variable and secret management, triggers, scaling, concurrency, and data\nintegration, among others. I strongly advise consulting the official Mage documenta‐\ntion to learn about these subjects and beyond.\nProject 3: Designing a Microservice Workflow  with\nNetflix  Conductor, PostgreSQL, and Python\nIn this project, you will implement a microservice-based order management system\n(OMS) to process orders for an online store. Any online company that wants to\nstreamline the entire order fulfillment process must have an OMS.\nIt’s important to note that microservice workflows often entail more complexity than\nother types of workflows. Additionally, because microservices tend to be distributed\nin nature and deployed across diverse environments, accurately replicating microser‐\nvice architectures locally can be quite challenging. Nonetheless, this project will offer\na minimal example to provide insight into the structure and characteristics of\nmicroservices.\nProject 3: Workflow  Definition\nThe first step in designing a microservice workflow is to define its structure. To this\nend, let’s start by outlining the microservices that will constitute our OMS system.\nHere are the five microservices we’ll need:\nOrder acknowledgement service\nHandles the acknowledgment of customer orders. It receives a client order, per‐\nsists it in the database, and returns an acknowledgment message to the customer.\nPayment processing service\nProcesses customer payment transactions and returns a message informing the\ncustomer about the status of their payment operation.\n448 | Chapter 12: Hands-On Projects\nStock and inventory service\nManages the inventory and stock levels of products available for sale. It tracks\nand checks the quantity of each product in stock, books and updates inventory\nbased on incoming orders, and returns a message to the customer about the\nstock booking.\nShipping service\nManages  the shipment  of orders.  This  service  coordinates  with  shipping  carri‐\ners to book a delivery, generate a tracking number, and update the delivery sta‐\ntus of orders.  It returns  a message  informing  the client  about  their  upcoming\ndelivery.\nNotification  service\nSends  notifications  to customers  at various  stages  of the order  fulfillment\nprocess.\nAs a next step, we need to define the dependency structure among our services. To\nkeep things simple, we will design a linear workflow that executes one service after\nanother. Once an order is submitted, it first gets acknowledged, then the payment\ngets processed, then the stock gets booked, and finally, a delivery is scheduled.\nFigure 12-10  illustrates the workflow.\nFigure 12-10. OMS workflow  structure\nProject 3: Designing a Microservice Workflow  with Netflix  Conductor, PostgreSQL, and Python | 449",3386
165-Project 3 Database Design.pdf,165-Project 3 Database Design,"Project 3: Database Design\nA reliable database system is required to store and manage order-related data. In line\nwith the other projects in this chapter, PostgreSQL stands out as our database solu‐\ntion of choice. Different database design patterns exist for microservices. While some\nadvocate for a dedicated database per service, others favor a unified database shared\nacross all microservices. To simplify our microservice structure, let’s proceed with the\nlatter approach—the single shared database pattern.\nIn our database, we will have five tables:\n•The orders table stores orders along with their unique identifiers.\n•The payments table stores transactional data related to order payments.\n•The inventory table stores product inventory details.\n•The stock bookings table stores the allocation of order items within the inven‐\ntory.\n•The delivery schedule table stores tracking delivery and shipment details.\n•The notifications table stores customer notifications.\nFigure 12-11  illustrates the ERD of our OMS database.\n450 | Chapter 12: Hands-On Projects\nFigure 12-11. OMS database ERD\nNext, let’s move on to see our microservice system in action.\nProject 3: Designing a Microservice Workflow  with Netflix  Conductor, PostgreSQL, and Python | 451",1274
166-Project 3 Local Testing.pdf,166-Project 3 Local Testing,"Project 3: Local Testing\nAs usual, you will need to navigate to the project directory in your terminal:\n# Bash\ncd {path/to/book/repo}/FinancialDataEngineering/book/chapter_12/project_3\nThen, run our containers with the following command:\n# Bash\ndocker compose up -d\nOnce completed, you will have three services running that we will be interacting with:\n•JupyterLab to program and execute our workflow in Python. It will be running\non http://localhost:8888/ .\n•Conductor orchestrator to explore and monitor workflows and executions via a\nuser-friendly UI. It will be running on http://localhost:5000/ .\n•PgAdmin client UI to explore the data generated by executions. It will be running\non http://localhost:8081/ .\nOpen three browser tabs and paste the three localhost URLs into each. We will be\ninteracting mostly with JupyterLab, so navigate first to the tab for http://localhost:\n8888/ . Once it’s ready, open a terminal from the Launcher tab. Consult Figure 12-12\nfor an illustration.\n452 | Chapter 12: Hands-On Projects\nFigure 12-12. JupyterLab main overview page\nIn the terminal, type the following command and hit Enter:\n# Bash\npython3 database/create_tables.py\nY ou should see a few logs that say “SQL executed successfully. ” This means that the\ntables we want for our OMS have been created in Postgres.\nNext, we want to insert some data into the customers and inventory tables. This is\nneeded because, later on, we will be executing our workflow with orders that already\ncontain the customer and product information. Again, in the terminal, type the fol‐\nlowing command, then press Enter to execute it:\n# Bash\npython3 database/populate_tables.py\nProject 3: Designing a Microservice Workflow  with Netflix  Conductor, PostgreSQL, and Python | 453\nThe customer and product tables are now populated with data. If you wish to view\nthe content of these files that you have just executed, you can navigate to the database\nfolder using the right sidebar, as shown in Figure 12-12 . If, in addition, you want to\nsee the populated tables in the database, switch to the tab with http://localhost:8081/\nand log in with the PGADMIN_DEFAULT_EMAIL  and PGADMIN_DEFAULT_PASSWORD  values,\nwhich you can find in the .env file of the project. Once logged in, proceed with the\nsame steps you learned in the first project to create a server and explore your tables.\nThe next step is to create and register our microservice workflow. First, we need to\nimplement each mircoservice individually. This has been done for you, with each\nmicroservice residing within its respective folder within the workflow  directory. Y ou\ncan use the left sidebar in JupyterLab to navigate to the workflow  directory. Y ou’ll find\nfive folders in there, each corresponding to one of the previously defined microser‐\nvices. Within each microservice folder, you’ll discover two scripts: one named ser‐\nvice.py , which contains the microservice’s code, and the other named worker.py ,\ncontaining the Conductor worker program  for the respective microservice. Each\nworker is tasked with executing a specific task, where the task corresponds to a\nmicroservice in our context.\nTo keep things simple, the logic defined in each service.py  contains\na database insert/update operation. In real business problems, you\nwill obviously need to include more functionality and business\nlogic in your microservices. This may involve computations, trans‐\nformations, API calls, database calls, and various other operations.\nMoving to our workflow, to see how it is defined using Conductor, open the\norder_workflow.py  file in the workflow  directory from the sidebar. Y ou will see how\neach task is defined by passing a reference name and the values for its parameters.\nThe first task (acknowledge order) receives its input from the workflow input, while\nthe other tasks take their input from the output of their previous tasks. More on pass‐\ning input to tasks in Conductor is available in the Conductor documentation . Using\nConductor, it is possible to create a linear flow using the >> operator. Y ou can see\nhow this is done at the bottom of the order_workflow.py  file.\nNow that our workflow is ready to be created, the next thing you can do is register the\nworkflow in the Conductor database. To do so, go back to the Terminal screen in\nJupyterLab and paste the following command:\n# Bash\npython3 workflow/register_order_workflow.py\nY ou should see a log message saying that the workflow has been successfully regis‐\ntered. To review your newly created workflow, go to the tab running http://localhost:\n5000/ . There, you’ll find the Conductor UI, which lets you check and monitor your\nworkflow and task definitions and executions. Once there, use the top toolbar and\n454 | Chapter 12: Hands-On Projects\nnavigate to the Definitions tab. Y ou will find a table containing your workflows in\nyour local instance. Click on the Order Workflow button to see the full details of the\nOMS workflow you created. Figure 12-13  illustrates what this page looks like.\nFigure 12-13. Order Workflow  definition  details page in Conductor\nOn the left side of Figure 12-13 , you will see a lot of workflow definition parameters,\nmany of which can be configured. To avoid extra complexity, we won’t be setting any\ncustom variables in this project. On the right side, you will see a visual representation\nof our OMS workflow. As you can see, it’s a linear flow where tasks run in a sequence.\nNow that we have our workflow registered, let’s execute it. To do so, we need to run\nthe file called execute_order_workflow.py,  which you can find in the workflow  folder.\nIf you open this file, you will see that I already prepared a dummy workflow input\nwith customer_id, products, payment amount, and delivery address. Y ou can change\nthese input values if you wish, but you need to make sure that the customer and\nproducts you add to the order exist in the database.\nTo execute our workflow, navigate to the Terminal tab in your JupyterLab and paste\nthe following command:\n# Bash\npython3 workflow/execute_order_workflow.py\nProject 3: Designing a Microservice Workflow  with Netflix  Conductor, PostgreSQL, and Python | 455",6246
167-Project 3 Summary.pdf,167-Project 3 Summary,"Y ou should see a few log messages being printed that inform you about the successful\noutcome of the different microservices.\nNow that you have executed the workflow, you can check the results in the database\nusing pgAdmin. Y ou’ll find a record of your order in the orders table, payment details\nlinked to your order in the payments table, updates in the inventory and stock book‐\nings tables, and a booked shipment in the delivery table.\nFinally, you can review the details of your workflow execution via the Conductor UI.\nSimply visit http://localhost:5000/  and click on Executions from the top toolbar. In the\ntable displayed on that page, you’ll find a record corresponding to your workflow\nexecution. Click on the ID listed under the workflowid column to access the details of\nthis particular workflow execution. Y ou should see a page similar to Figure 12-14 .\nFigure 12-14. Order Workflow  execution details page in Conductor\nAs shown in Figure 12-14 , our workflow execution has completed successfully. By\nclicking on a specific task in the diagram on the left side, you can view its execution\ndetails, including the input it received, the output it produced, its logs, and its task\ndefinition details.\nProject 3: Clean Up\nRun the following command in the root directory of Project 3:\n# Bash\ndocker compose down\n456 | Chapter 12: Hands-On Projects",1377
168-Project 4 Prerequisites.pdf,168-Project 4 Prerequisites,"Project 3: Summary\nThis project aimed to familiarize you with the process of building and executing\nmicroservice workflows. In real-world situations, you’ll likely tackle similar projects\nbut with more complex requirements. For example, you are likely to deploy each of\nthe services we defined into a separate environment, such as Cloud Functions or\ncontainer-based deployment services, and configure security and access policies to\nallow them to communicate. Moreover, you will need to have a way to handle concur‐\nrency and dependencies in distributed transactions that span multiple services. Addi‐\ntionally, you might need to configure the workflow and task definitions to handle\nissues such as failure, retries, timeouts, etc.\nProject 4: Designing a Financial Reference Data Store with\nOpenFIGI, PermID, and GLEIF APIs\nIn this project, you will develop a basic reference data store that holds information for\nidentifying financial instruments and various entities, including the following:\n•The full list of ISO 3166 Country Codes.\n•The FIGIs, ISINs, Thomson Reuters Open Perm IDs, and RICs for the S&P 100\nstocks . The S&P 100 is a primary stock index comprising 100 of the largest and\nmost established companies listed on US stock exchanges.\n•ISO 17442 Legal Entity Identifiers.\n•ISO 10383 Market Identifier Codes.\n•LEI-to-ISIN mappings.\n•LEI-to-BIC mappings.\nThroughout this project, you will be using three open APIs:\n•OpenFIGI for retrieving FIGI data\n•Open PermID for retrieving PermIDs and RICs\n•GLEIF API for retrieving LEI and country code identifiers\nIf you are not familiar with financial identifiers, I strongly recommend reading Chap‐\nter 3  before beginning this project.\nProject 4: Designing a Financial Reference Data Store with OpenFIGI, PermID, and GLEIF APIs | 457",1816
169-Project 4 Summary.pdf,169-Project 4 Summary,"Project 4: Prerequisites\nThe only prerequisite for this project is to obtain a PermID API token. To do so, fol‐\nlow these steps:\n1.Go to https://permid.org .\n2.From the top right, click on REGISTER and follow the steps to complete your\nsign-up.\n3.Once completed, go back again to https://permid.org  and sign in with your login\ncredentials.\n4.Once signed in, from the top menu, click on APIs → Display my API Token (see\nFigure 12-15  for an illustration).\nFigure 12-15. Finding your API token on PermID.org\nIn the following section, you’ll use this token to communicate with the PermID API.\nProject 4: Local Testing\nTo start, navigate to the project directory in your terminal:\n# Bash\ncd {path/to/book/repo}/FinancialDataEngineering/book/chapter_12/project_4\nOnce you are in the main project directory, you will find an .env file with a variable\ncalled PERMID_API_KEY . Without using quotes, you must assign this variable the API\naccess token you got from PermID (i.e., PERMID_API_KEY=YOUR_API_TOKEN ).\nRun our containers with the following command:\n# Bash\ndocker compose up -d\n458 | Chapter 12: Hands-On Projects\nThroughout this project, you will be mainly interacting with JupyterLab notebooks.\nTo begin, open your preferred browser and visit http://localhost:8888/  to access\nJupyterLab.\nThe first thing we want to do is to create our tables in the database. These tables will\nstore the various types of reference data that we will be loading. From the main Jupy‐\nterLab overview page, open a Terminal and execute the following command:\n# Bash\npython3 scripts/create_tables.py\nTo view the tables, log in to pgAdmin on http://localhost:8081/  using the PGAD\nMIN_DEFAULT_EMAIL  and PGADMIN_DEFAULT_PASSWORD  credentials you find in the .env\nfile of the project. Once logged in, create a server with the POSTGRES_DB , POST\nGRES_USER , POSTGRES_PASSWORD  credentials following similar steps as in Project 1.\nWhen done, you can check the tables using the left sidebar (Databases → refer‐\nence_data → Schemas → public → Tables).\nNow that you’ve created the tables, it’s time to populate them. To accomplish this,\nyou’ll need to run multiple notebooks that have already been prepared for you. In\nJupyterLab, navigate to the scripts  folder using the left navigation sidebar. Inside the\nscripts folder, you’ll find several directories. Open the first one named country . Inside,\nlocate a file named country.ipynb . Double-click to open it. Then, from the top menu,\nselect Run → Run All Cells. This action will execute all the code cells in your note‐\nbook, fetching and uploading the ISO 3166 Country Codes from the GLEIF API.\nFollow the same steps with these files:\n•scripts/figi/figi.ipynb\n•scripts/lei/lei.ipynb\n•scripts/lei_bic/lei_bic.ipynb\n•scripts/lei_isin/lei_isin.ipynb\n•scripts/mic/mic.ipynb\n•scripts/permid/permid.ipynb\nAfter running all these scripts, our reference data store should be ready. Switch to the\npgAdmin tab and examine the contents of the tables to observe the data in action.\nProject 4: Clean Up\nRun the following command in the same root directory as Project 3:\n# Bash\ndocker compose down\nProject 4: Designing a Financial Reference Data Store with OpenFIGI, PermID, and GLEIF APIs | 459",3265
170-Conclusion.pdf,170-Conclusion,,0
171-Follow Updates on These Projects.pdf,171-Follow Updates on These Projects,,0
172-The Path Forward Trends Shaping Financial Markets.pdf,172-The Path Forward Trends Shaping Financial Markets,"Project 4: Summary\nThis project was designed to offer you an introduction to the world of financial refer‐\nence data. Constructing and managing reference data stores are essential and com‐\nmon tasks in the financial industry. Although the reference data used in this project\nwas simple and limited, handling reference data often presents much more complex\nchallenges. For more information on this topic, refer to the section “Reference Data”\non page 77 .\nConclusion\nCongratulations on finishing the final chapter of the book! This is a significant mile‐\nstone, and I want to thank you for your dedication and commitment to completing\nthis journey.\nThroughout the first 11 chapters of this book, you’ve gained knowledge about a range\nof foundational and finance domain-specific subjects essential for your journey as a\nfinancial data engineer. In this final chapter, you worked on hands-on projects crafted\nto solidify your understanding from earlier sections and refine your abilities through\npractical implementation.\nAlthough this chapter concludes our journey through this book, your exploration of\nfinancial data engineering is just beginning. This field is continuously evolving with\nnew trends that will shape the future of financial markets. In “The Path Forward:\nTrends Shaping Financial Markets” , I’ll briefly highlight these emerging trends, pro‐\nviding you with a glimpse of what lies ahead.\nFollow Updates on These Projects\nIf the projects featured in this chapter undergo any updates or changes, I’ll document\nthem on the GitHub page . Feel free to refer to it if you encounter any issues.\nReport Issues or Ask Questions\nShould you encounter any challenges while setting up or executing any step in the\nprojects outlined in this chapter, please don’t hesitate to create an issue on the proj‐\nect’s GitHub repository . I will make sure to reply to you in a very short time.\n460 | Chapter 12: Hands-On Projects",1950
173-Financial Integration.pdf,173-Financial Integration,,0
174-Digitalization of Financial Markets and Cloud Adoption.pdf,174-Digitalization of Financial Markets and Cloud Adoption,,0
175-Financial Data Sharing and Marketplaces.pdf,175-Financial Data Sharing and Marketplaces,"The Path Forward: Trends Shaping\nFinancial Markets\nAs we look toward the future, the financial industry is expected to undergo signifi‐\ncant transformations, driven by technological advances, regulatory changes, evolving\nconsumer demands, and shifting market dynamics. Financial data engineering will be\ncentral to these changes, playing a key role in guiding and shaping the financial land‐\nscape. The skills and knowledge you have gained through this book will equip you to\ntackle these future challenges head-on. Let’s take a quick look at these major trends\non the horizon.\nFinancial Integration\nThe global financial system is becoming more interconnected, breaking down barri‐\ners among different markets, regions, and institutions. This trend toward financial\nintegration is driven by the need for seamless cross-border transactions and\ninvestments.\nDigitalization of Financial Markets and Cloud Adoption\nThe shift to digital financial markets is accelerating, with cloud technologies emerg‐\ning as the key driver of innovation. This digital transformation enhances scalability,\nreduces costs, and improves access to financial services.\nFinancial Regulation\nRegulations in the financial industry are becoming more stringent and complex, with\nincreased demands for privacy protection, transparency, accountability, and risk\nmanagement. Ensuring compliance through accurate data collection, storage, secu‐\nrity, protection, and reporting will be essential.\n461",1487
176-Financial Standardization.pdf,176-Financial Standardization,,0
177-Artificial Intelligence and Language Models.pdf,177-Artificial Intelligence and Language Models,,0
178-Data Collection.pdf,178-Data Collection,"Financial Data Sharing and Marketplaces\nData sharing is reshaping financial markets by boosting collaboration, innovation,\ntransparency, and efficiency. Financial data marketplaces are emerging as key plat‐\nforms for sharing and exchanging data, providing easy access to financial data for\nthose who need it.\nFinancial Standardization\nStandardization is key to achieving interoperability and reducing complexity in\nfinancial data management. The financial services industry has fallen behind other\nsectors in collaborating to create and adopt comprehensive interoperable standards\nfor storing and exchanging information. However, this is beginning to change, with\ninitiatives such as the adoption of the Legal Entity Identifier (LEI) and ISO 20022\nmarking significant progress in this direction.\nArtificial  Intelligence and Language Models\nArtificial intelligence is revolutionizing financial markets by addressing a wide range\nof applications such as trading, risk management, fraud detection, customer service,\nand financial advising. Generative AI and LLMs are set to redefine the future of\nfinancial services, revolutionizing everything from front-office customer interactions\nand trading strategies to back-office operations such as risk management and\ncompliance.\nRegulations such as the EU AI Act are being established to ensure that AI systems are\nsafe, transparent, and ethically sound. Additionally, standards for AI are being devel‐\noped, with ISO creating the ISO/IEC JTC 1/SC 42 committee to oversee AI standardi‐\nzation. The committee introduced ISO/IEC 42001 , the first global standard for AI\nmanagement systems, providing guidelines for managing AI technologies effectively\nand ethically.\nArchitectures for Specific  Business Domains\nIn financial institutions, distinct business units—such as trading, risk management,\ncompliance, and retail banking—have specific data needs and regulatory require‐\nments. Adopting a domain-oriented approach allows for scalable and efficient data\nmanagement by customizing the architecture to meet the particular needs of each\nunit.\n462 | The Path Forward: Trends Shaping Financial Markets",2172
179-Speed and Efficiency.pdf,179-Speed and Efficiency,,0
180-Tokenization Blockchain and Digital Currencies.pdf,180-Tokenization Blockchain and Digital Currencies,,0
181-What Can You Do Next.pdf,181-What Can You Do Next,"Data Collection\nThe scope of data collection in financial markets is expanding rapidly, encompassing\neverything from market transactions and customer interactions to alternative data\nsources like social media and IoT devices. This broader data collection offers financial\ninstitutions and regulators deeper insights, enabling more detailed analysis of market\ntrends and risks.\nSpeed and Efficiency\nSpeed and efficiency are in high demand within financial markets, reflected in the\nongoing push for rapid, real-time transactions, fast access to market data, and the\nability to quickly respond to market changes and opportunities. Efficient financial\nprocesses enhance customer satisfaction, enable the development of optimal trading\nstrategies, allow for prompt and accurate fraud detection, and ensure the stability and\nefficiency of the financial system.\nTokenization, Blockchain, and Digital Currencies\nThe future of financial markets will be profoundly influenced by tokenization, block‐\nchain technology, and digital currencies, which promise to revolutionize transaction\nsecurity, enable decentralized finance (DeFi) innovations, and enhance transparency\nacross the financial ecosystem. These advancements will facilitate more efficient asset\nmanagement, streamline cross-border transactions, and create new avenues for\ninvestment and financial inclusion.\nRegulations are being introduced to provide legal certainty and consumer protection\nfor crypto-assets and their service providers. The best example is the EU’s Markets in\nCrypto-Assets  (MiCA) regulation, which aims to establish a uniform set of rules for\nthe issuance, trading, and custody of crypto-assets across EU member states.\nWhat Can You Do Next?\nNow that you have acquired the skills and knowledge outlined in this book, it’s your\nturn to explore the diverse and dynamic world of financial data engineering. The\ntrends and challenges discussed here present numerous opportunities to make a\nmeaningful impact. Dive into the various data problems faced by financial markets,\nidentify the areas that excite you the most, and think about how you can contribute to\nsolving these issues.\nThe Path Forward: Trends Shaping Financial Markets | 463\nWhether it’s developing innovative solutions for financial integration, leveraging\ncloud technologies, ensuring regulatory compliance, facilitating data sharing, or har‐\nnessing the power of AI, the possibilities are endless. As a financial data engineer, you\nhave the power to shape the future of finance. Embrace the challenge, push the\nboundaries of what’s possible, and become a key player in the next chapter of finan‐\ncial innovation.\nThe future is in your hands—go out there and make a difference!\n464 | The Path Forward: Trends Shaping Financial Markets",2809
182-Afterword.pdf,182-Afterword,"Afterword\nAs you reach the conclusion of Financial Data Engineering , I trust your answer to the\nquestion, “ Are you ready to navigate today’s complex financial data landscape?” is a\nresounding “yes. ” Like Tamer Khraisha and myself, you’ve likely realized by now that\nthe journey of a financial data engineer is one without a clear endpoint. There is no\nfinal “pot of gold” waiting at the end of this path. Instead, the true reward lies in the\ncontinuous development of creativity, skills, and insights. Each step you take in mas‐\ntering the intricacies of financial data is a reward in itself, contributing not only to\nyour own growth but also to the ongoing evolution of the financial industry.\nAt the intersection of finance, digital data diversity, and data engineering lies an excit‐\ning and dynamic frontier. The opportunities for innovation are limitless, and each\nnew challenge brings with it fresh insights. In an industry driven by data, transforma‐\ntion is constant, and we are all part of this exhilarating change.\nTamer Khraisha’s contributions to this field are a testament to the importance of\nembracing this journey. From his mastery of network science, analysis of lending\npractices, and numerous financial and technical analyses, to his pivotal work, “ A\nHolistic Approach to Financial Data Science: Data, Technology, and Analytics, ” pub‐\nlished in The Journal of Financial Data Science,  Spring 2020, Tamer has provided a\ncritical foundation for understanding the relationship between data, technology, and\nfinance. His motivation to develop this book reflects a desire to share knowledge,\noffer guidance, and inspire future financial data engineers.\nAs we continue to wrestle with the transition from traditional representations of\nvalue—such as coins and bills—to a purely data-driven financial ecosystem, it\nbecomes clear that finance is now an information industry at its core. Data is no\nlonger just a reflection of value; it is value. In this book, Tamer provides a compre‐\nhensive guide that combines finance domain expertise with software engineering,\ndata science, and analytical techniques. These tools are crucial for extracting insights\nfrom financial systems and evaluating data-driven hypotheses.\n465\nThe industry itself is a marvel. Despite countless complexities, idiosyncrasies, and\nconvoluted relationships between financial entities, it operates smoothly on the back‐\nbone of data. This is where the power of financial data engineering shines, and Tam‐\ner’s book draws from personal experience, as well as the shared experiences of many\nprofessionals in this field, to help us navigate these complexities.\nPart one of the book lays the essential foundations: an understanding of finance, the\nkey concepts of financial data engineering, and the interrelationships between finan‐\ncial entities. Tamer brings order to the chaotic, diverse nature of financial data, iden‐\ntifying patterns and structures within the financial ecosystem. It is within this\n“primordial soup” of data that infinite creativity, innovation, and financial ingenuity\nthrive.\nIn part two, the book shifts to the technical frameworks and technologies needed to\nmanage financial data. Tamer outlines critical decisions regarding the shape, size, and\nvelocity of data and provides practical examples for applying data engineering tech‐\nniques. These hands-on exercises are invaluable for deepening your understanding of\nreal-world financial data solutions.\nFinally, Tamer guides us into the future of the financial industry, where ongoing digi‐\ntal transformation demands ever greater mastery of data engineering. Financial sys‐\ntem integration, product innovation, and technological advancements will continue\nto drive the need for financial data engineers who can navigate this new world with\nconfidence and expertise.\nY ou now possess a deeper understanding of the financial data landscape and the ana‐\nlytical frameworks needed to unlock its potential. The tools and skills you’ve gained\nfrom this book have empowered you to analyze complex data and derive actionable\ninsights that will contribute to the financial industry’s future.\nY our journey, like Tamer’s and mine, will be one of continuous learning, adaptation,\nand exploration. The world of financial data is vast, and the challenges it presents are\ncomplex—but the rewards for those who embrace these challenges are profound.\nThis book is just the beginning of what promises to be an exciting and fruitful career\nas a financial data engineer. Take with you the inspiration, creativity, and innovation\nthat define this field, and know that every discovery you make brings you closer to\ndesigning the art and mastering the science of financial data engineering.\n— Brian Buzzelli\nDirector, Head of Data Practice at Meradia\nAuthor of Data Quality Engineering in Financial\nServices  (O’Reilly, 2022)\n466 | Afterword",4946
183-Index.pdf,183-Index,"Index\nA\nAbel Noser, cost monitoring, 400\naccess control, 210-212\naccessibility, financial identification systems\nand, 98\nAccredited Standards Committee X9 (ASC X9),\n20\nAccurate Online Disambiguation (AIDA), 149\nACID (atomicity, consistency, isolation, dura‐\nbility), 284-286\nrelational databases and, 303\nadjacency lists, 61\nAdvanced Encryption Standard (AES), 209\nAdvanced Message Queuing Protocol (AMQP),\n246\nAES (Advanced Encryption Standard), 209\naggregation, 192-193\nAI (artificial intelligence), 21, 26\n(see also generative AI)\nAIDA (Accurate Online Disambiguation), 149\nAladdin, BlackRock, 342\nalgorithmic trading, 395\nAlpha Vantage API, 444\nalternative data, 76-77\nalternative data sources, 54\nAmazon Comprehend, 150\nAMQP (Advanced Message Queuing Protocol),\n246\nanalytics data, 76\nANNA (Association of National Numbering\nAgencies), 20\nannotated datasets, supervised learning and, 22\nannual reports, financial highlights, 69\nanonymization, 203-209Apache Airflow, 416\nApache Avro, 258\nApache Kafka, 325\nfraud detection at ING Group, 328\nLambda architecture and, 418\nmessage brokers and, 326-329, 372, 417\nsequential disk access and, 371\nApache Parquet, 257\nApache Spark, 371\nAPIs (application programming interfaces),\nfinancial, 269-274\napplication traces, 389\narbitrary IDs, 89\narchives, 191-192\nartificial intelligence (see AI)\nASC X9 (Accredited Standards Committee X9),\n20\nasset management firms, 34\nasset pricing theory, 8\nAssociation of National Numbering Agencies\n(ANNA), 20\nasymmetric encryption, 209\natomicity, consistency, isolation, durability\n(ACID), 284-286\naudit trails, 194\nauthenticity, financial identification systems\nand, 99\nAutoML (Automated Machine Learning), 150\nAWS Redshift, 276\nB\nbackfilling, 68\nbackups, 191\nbank identifiers, 122-125\n467\nBASE model, 286\nBasel Committee on Banking Supervision, 31\nbatch transformations, 368-369\nBC (Banking Circle) anti–money laundering,\n334\nbenchmark datasets, 80-84\nbid/ask spread, 71\nbids, 71\nbig data\nattributes, 14\ncolumn-oriented format, 257\ndata ingestion, 257-258\nrow-oriented format, 257\nstart of, 4\nvariety, 17-18\nvelocity, 16\nvolume, 14-15\nbig tech firms, 36\nBitSight Security, 83\nblockchain model, DSMs and, 343-346\nblocking key, 158\nBloomberg, 50\nBloomberg Terminal, 50\nBloomberg tickers, 113\nBloombergGPT, 26\nbrute force attacks, 200\nbusiness and analytical monitoring, 400-403\nbusiness operation traces, 389\nC\ncalendar filters, 363\ncanonicalization, 135\nCAP theorem, 287-288\nCapital One data breach, 236\nCDC (change data capture), 373\nCEI (CUSIP Entity Identifier), 108\nCenter for Research in Security Prices (CRSP),\n81\ncentral banks, monetary policies and, 6\nCFI (Classification of Financial Instruments),\n106\nCGS (CUSIP Global Services), 108\nchange data capture (CDC), 373\nChatGPT, 26\nCI/CD (continuous integration/continuous\ndelivery), 222\nClassification of Financial Instruments (CFI),\n106\nclassification, supervised learning and, 23cloud bursting, 237\ncloud computing, 227, 229\ndata ingestion and, 275-276\nhybrid cloud, 237\nIaaS (infrastructure-as-a-service), 230\nmigrating to cloud, 231-234\nmulti-cloud, 238-239\nmultitenancy, 235\nPaaS (platforms-as-a-service), 230\nprivate cloud, 236-237\npublic cloud, 235-236\nSaaS (software-as-a-service), 230\nsecurity, 232\nshared responsibility principle, 235\nsingle cloud, 238-239\ncloud-based monitoring, 397\nclustering, supervised learning and, 24\nCockroachDB, 312\ncolocation, 227\ncolumn-oriented format, big data, 257\ncommercial banks, 33\ncommercial data\nadvantages, 48\nsources overview, 47-50\ncommercial software advantages, 218-219\nCommittee on Uniform Security Identification\nProcedures (CUSIP), 108\ncompany filters, 363\ncompliance (see regulations)\nCompustat Financials, 81\ncomputational efficiency, transformations, 377\nconferences, events, and summits, 9\nconfidential data sources, 54\ncontinuous integration/continuous deployment\n(CI/CD), 222\ncontractions removal, 136\nConway’s law, 195\ncorporate account takeover, 200\ncoverage filters, 364\ncritical logs, 389\ncross-section queries, 348\ncross-sectional data, 56\nCRSP (Center for Research in Security Prices),\n81\ncryptocurrency, 343\ncryptography, 209\ncurse of dimensionality, 18\nCUSIP (Committee on Uniform Security Iden‐\ntification Procedures), 108\n468 | Index\nCUSIP Entity Identifier (E), 108\nCUSIP Global Services (CGS), 108\ncyberattacks, 200\nD\nDAG (Directed Acyclic Graph), 408\nDAP (data arrival processes), 249\nbulk, 254-256\nevent-driven, 250-252\nheterogeneous, 253\nhomogenous, 253\nscheduled, 249\nsingle-item, 254\ndata analysis, financial identifiers and, 90\ndata archival policy, 192\ndata arrival processes (DAP) (see DAP)\ndata augmentation, 18\ndata availability, 185-187\ndata biases, 179-180\ndata catalogs, 194\ndata cleaning, 358-360\ndata completeness, 185-187\ndata constraints, 188\ndata consumers, 382\ndata contracts, 195-197\ndata controllers, 201\ndata cycle time, 392\nData Definition Language (DDL), 283, 437\ndata duplicates, 182-185\ndata engineering, 5\n(see also financial data engineering)\ndefinition, 11\nevolution, 4\noverview, 4, 10-11\nData Engineering Lifecycle (DEL), 215\ndata errors, 175-177\ndata feeds, 274\ndata governance\ndata lakes and, 295, 298-299\ndefinitions, 171-172\nperformance and, 172\nrisk management and, 173\ndata granularity, 180-182\ndata infrastructure, 3\ndata ingestion, 243\nbest practices, 277-279\nbig data formats, 257-258\ncloud access, 275-276\nDAP (data arrival processes), 249-256data feeds, 274\ndata lakes and, 295\nfile transfer, 275\nfinancial APIs, 269-274\nfinancial formats, 258-269\nfinancial software, 277\ngeneral-purpose formats, 256-257\nin-memory formats, 258\ntransmission protocols, 244, 245-248\nweb access, 277\ndata integrity, 190\ndata lakes\nDSMs and, 294-300\nNASDAQ case study, 300\ndata lineage, 193\ndata maturity, 36-37\ndata modeling\nconceptual phase, 283\ndata lakes and, 296-297\ndocument databases, 315-317\nfinancial use case, 283\ngraph databases, 330-331\nlogical phase, 283\nmessage brokers and, 324\nphysical phase, 283\nrelational databases and, 305-309\ntime series model and, 320\nwarehouse model, 337-339\ndata ownership, 195\ndata permanence, 100\ndata processors, 201\ndata profiling, 392\ndata quality dimensions (DQD), 174\nData Quality Framework (DQF), 174\nData Quality Metrics (DQMs), 390-392\ndata reconciliation, 197-198\ndata relevance, 189-190\ndata sources\nalternative, 54\ncommercial, 47-53\nconfidential and proprietary, 54\npublic, 44-47\nsecurity exchanges, 47\nsurveys, 53\ndata storage system (see DSS)\ndata structures, 17\ndata transformation (see transformations)\ndata vendors, financial identification systems\nand, 92\nIndex | 469\ndata-related technologies, 4\ndata.europa.eu, 46\ndatabase benchmarks, 289\ndatabases\ndocument databases, 314-315\ndata modeling and, 315-317\nin-memory, 321\nindexing and, 351\nOracle Database, 220\nqueries\noptimizing, 351-354\nrelational (see RDMS)\ndatagrams, 247\nDDD (Domain-Driven Design), 13, 421\nDDL (Data Definition Language), 283, 437\nDDoS (Distributed Denial of Service), 200\ndebug logs, 388\ndeep learning, NER and, 144\nDeferred Net Settlement (DNS), 75\ndeficit, 6\nDEL (Data Engineering Lifecycle), 215\nDelivery versus Free (DVF), 74\nDelivery versus Payment (DVP), 74\ndensity estimation, supervised learning and, 24\ndifferential privacy, 428\ndigital asset identifiers, 120-121\ndimensionality reduction, 18\nDirected Acyclic Graph (DAG), 408\ndirected graphs, 62\ndisclosure requirements, regulations and, 44-45\ndisk-based transformations, 370-373\nDistributed Denial of Service (DDoS), 200\ndistributed ledger technology (DLT), 344\nDLT (distributed ledger technology), 344\nDNS (Deferred Net Settlement), 75\nDNS (Domain Name System), 246\nDocker, 431\nDocker Desktop, 431\ndocument databases, 314-315\ndata modeling and, 315-317\ndocument model, DSMs and, 314-319\nDomain Name System (DNS), 246\nDomain-Driven Design (DDD), 13, 421\nDQD (data quality dimensions), 174\nDQF (Data Quality Framework), 174\nDQMs (Data Quality Metrics), 390-392\nDSMs (data storage models), 281, 290-294\nblockchain model, 343-346\ndata lakes and, 294-300document model, 314-319\ngraph model, 329-333\nmessage broker model, 323-328\nrelational model (see RDMS)\ntime series model, 319-323\ntopic modeling, 324\nwarehouse model, 335-341\nDSS (data storage system), 281\nconsistency, 287-288\ndata modeling and, 283-284\nDDL (Data Definition Language) com‐\nmands, 283\ndesign, 282-290\nscalability, 288-289\nsecurity, 290\ntransactional guarantee, 284-286\nduplicate data, 182-185\nDVF (Delivery versus Free), 74\nDVP (Delivery versus Payment), 74\nE\nECB Statistical Data Warehouse, 46\nEDGAR (Electronic Data Gathering, Analysis,\nand Retrieval), 45\nedge lists, 61\nElectronic Data Gathering, Analysis, and\nRetrieval (EDGAR), 45\nELK stack, 389\nencoding systems, 89\nencryption, 209-210\nhomomorphic, 428\nentities\nER (entity resolution), 151-155\nclassification, 161\ncomparison, 160-161\ndeterministic linkage approach, 164-167\nevaluation, 162-164\nindexing, 158-159\npreprocessing, 156-157\nprobabilistic linkage approach, 167-169\nsoftware libraries, 170\nsupervised machine learning approach,\n169\nLOC (places), 128\nMISC (miscellaneous), 128\nNED (named entity disambiguation), 133\nNER (named entity recognition), 129-134\napproaches, 141-149\nentity categorization, 137\n470 | Index\nentity disambiguation, 138\nentity extraction, 136\nevaluation, 138\nlabeled databases, 139\npreprocessing, 135-136\nsoftware libraries, 149-150\nORG (corporations), 128\nPER (individuals), 128\nentity data, 80\nEntity Relationship Diagram (ERD), 434\nEntity Relationship Model (ERM), 132\nER (entity resolution), 151-155\nclassification, 161\ncomparison, 160-161\ndeterministic linkage approach, 164-167\nevaluation, 162-164\nindexing, 158-159\npreprocessing, 156-157\nprobabilistic linkage approach, 167-169\nsoftware libraries, 170\nsupervised machine learning approach, 169\nERD (Entity Relationship Diagram), 434\nERM (Entity Relationship Model), 132\nerror logs, 388\nerrors, trade execution error rate, 394\nETL (extract, transform, and load), workflows,\n414-416, 442\ndatabase design, 443\nlocal testing, 444-447\nworkflow definition, 442\nevents, 388\nexchanges, financial identifiers and, 90\neXtensible Business Reporting Language\n(XBRL), 20, 261\nF\nFactSet, 26, 51, 119\nFDEL (financial data engineering lifecycle), 215\ndefinition, 215-217\ningestion layer, 216\nmonitoring layer, 217\nstorage layer, 216\ntransformation and delivery layer, 217\nfeature engineering, 364-367\nfeature-engineering machine learning approach\nto NER, 142-143\nfeatures stores, ML learning, 426\nfeatures, supervised learning and, 22\nFederal Reserve Economic Data (FRED), 46federated learning, 428\nFESs (financial entity systems), 127\nFIGI (Financial Instrument Global Identifier),\n20, 116-118\nFile Transfer Protocol (FTP), 246\nfile transfer security, 275\nfiltering data, 363-364\nfinance\neconomic function, 6\nmain areas of, 5\nas market, 6-8\nas research field, 8-9\nas technology, 9-10\nfinancial analysis, 90\nfinancial APIs, 20, 269-274\nfinancial assets, 7\nfinancial data\ncurse of dimensionality, 18\nopportunities, 15, 16, 17\npublic, 44-47\nsources overview, 44\ntechnical challenges, 15, 18\nvariety, 17-18\nvelocity, 16\nvendors, selecting, 52-53\nvolume, 3, 14-15\nfinancial data engineering, 5\n(see also data engineering)\ncompared to financial engineering, 13\ndefinition, 12\nneed for, 13-32\noverview, 12-13\nfinancial data engineering lifecycle (see FDEL)\nfinancial data engineers\nemployment, 33-36\noverview, 32\nresponsibilities, 36-37\nroles, 32-33\nskills needed, 38-41\nfinancial data governance (see data governance)\nfinancial data infrastructure, 11\nfinancial data vendors\nfinancial data engineers, 35\nspecialization, 48-50\nfinancial datasets, 88\nfinancial engineering compared to financial\ndata engineering, 13\nfinancial entities defined, 128-129\nfinancial entity systems (FESs), 127\nIndex | 471\nfinancial feeds case study, 395\nfinancial formats, 258\nFIX (Financial Information eXchange), 259\nFpML (Financial products Markup Lan‐\nguage), 262-263\nISO 20022, 264-269\nOFX (Open Financial Exchange), 263\nXBRL (eXtensible Business Reporting Lan‐\nguage), 261\nfinancial identification system, 88-89\naccessibility, 98\nauthenticity, 99\ncompleteness, 97\ncreators, 90-92\ndata permanence and, 100\ngranularity and, 99\nimmutability and, 102\nscalability and, 96-97\nsecurity, 102\nfinancial identifiers, 88-89\nderivative identifiers, 114-116\nglobality, 94-96\nneed for, 89\nproperties, 93-103\nuniqueness, 93\nfinancial industry\nchallenges and issues, 19-20\nfinancial data engineers, 33-36\nregulations, 20\ntechnological innovation, 27\nFinancial Industry Regulatory Authority\n(FINRA), 82\ntransition to AWS Lambda, 381\nFinancial Information eXchange (FIX), 259\nfinancial institutions, 7\ntraditional compared to FinTech, 27\nFinancial Instrument Global Identifier (FIGI),\n20, 116-118\nFinancial Instrument Short Name (FISN), 107\nFinancial Instruments Reference Data System\n(FIRDS), 79\nFinancial Market Infrastructures (FMIs), 31\nfinancial markets, 6\ncategories, 7-8\nfinancial assets, 7\nmarket players, 6-7\nFinancial products Markup Language (FpML),\n262-263\nfinancial technologies, 9FinGPT, 26\nFinOps framework, 399\nFINRA (see Financial Industry Regulatory\nAuthority)\nFinTech\nfinancial data engineers, 33\noverview, 27-31\npayment ecosystem, 29-30\nFIRDS (Financial Instruments Reference Data\nSystem), 79\nFISN (Financial Instrument Short Name), 107\nFIX (Financial Information eXchange), 259\nFMIs (Financial Market Infrastructures), 31\nForex (foreign exchange) markets, 73\nFpML (Financial products Markup Language),\n262-263\nframing bias in surveys, 53\nFRED (Federal Reserve Economic Data), 46\nfree APIs, 47\nFTP (File Transfer Protocol), 246\nfull transformations, 372\nfundamental data, 68-70\nG\nG-SIBs (Global Systemically Important Banks),\n31\nGDPR (General Data Protection Regulation),\n201\ngeneral-purpose technology, 229\ngenerative AI, 26\nGitHub, projects, 432\nGLEIF (Global Legal Entity Identifier Founda‐\ntion), 109\nGlobal New Vehicle Registrations dataset, 84\nGlobal Systemically Important Banks (G-SIBs),\n31\ngovernmental data, 46\ngranularity, financial identification systems\nand, 99\ngraph data, 60\nDAG (Directed Acyclic Graph), 408\ndirected graphs, 62\nmultilayer graphs, 65\nmultipartite graphs, 63\nsimple graphs, 62\nsources, 66\ntemporal graphs, 65\ngraph model, DSMs and, 329-333\n472 | Index\nH\nHDFS (Hadoop Distributed File System), 295\nhedge funds, 35\nhigh-frequency trading, 14, 251\nhomomorphic encryption, 428\nHTTP (HyperText Transfer Protocol), 245\nhybrid cloud computing, 237\nI\nI/B/E/S (Institutional Brokers’ Estimate Sys‐\ntem), 81\nIaaS (infrastructure-as-a-service), 230\nIB (Instant Bloomberg), 51\nidentifiability spectrum, 204\nimmutability, financial identification systems\nand, 102\nin-memory databases, 321\nincremental transformations, 372\nindexing databases, 351\nindustry identifiers, 121-122\nInfluxDB, 387\ninfo logs, 388\ninformation lifecycle, data lineage and, 193\nInformation Security Management Systems\n(ISMS), 199\ninfrastructure, regulatory compliance and, 32\ninfrastructure-as-a-service (IaaS), 230\nING Group fraud detection, 328\ninnovation velocity, 392\nInstant Bloomberg (IB), 51\nInstitutional Brokers’ Estimate System\n(I/B/E/S), 81\ninstitutional data, 46\ninterest rates, supply and demand, 6\ninterlocking directorates, 66\nInternational Organization for Standardization\n(ISO), 20\nfinancial identification systems and, 90\nInternational Securities Identification Number\n(ISIN), 90, 104-105\ninvestment banks, 34\nInvestopedia, 8\nIPsec, 248\nISIN (International Securities Identification\nNumber), 90, 104-105\nISMS (Information Security Management Sys‐\ntems), 199\nISO (see International Organization for Stand‐\nardization)ISO 20022, 264-269\nIvyDB OptionMetrics, 82\nJ\njoint controllers, 202\nJournal of Banking and Finance, 9\nJournal of Finance, 9\nJournal of Financial Data Science, 9\nJournal of Portfolio Management, 9\nJournal of Securities Operations & Custody, 9\nK\nKappa architecture, 418\nknowledge graphs, NER and, 148-149\nL\nlabels, supervised learning and, 22\nlanguage modeling, 26\nlarge language models (see LLMs)\nlatency\nlow latency, 226\nmarket data latency, 395\ntrade execution latency, 394\nLEI (Legal Entity Identifier), 20, 109-110\nlexicon/dictionary approach to NER, 141\nlimit orders, 72\nliquidity filters, 363\nLLMs (large language models), 26, 67\nhallucinations, 146\nNER and, 145-146\nlogs, 388\ncritical logs, 389\ndebug logs, 388\nELK stack, 389\nerror logs, 388\ninfo logs, 388\nwarning logs, 388\nLondon Stock Exchange Group (see LSEG)\nlook-ahead bias, 69, 179\nlow frequency, 68\nlow latency, 226\nLSEG (London Stock Exchange Group), 83\nPermID, 119\nLSEG Eikon, 51\nLuhn algorithm, 105\nM\nmachine learning (see ML)\nIndex | 473\nMage, ETL workflow, 442-448\nmainframe architecture, 229\nmalware, 200\nMarket by Order (MBO), 71\nMarket by Price (MBP), 71\nmarket data, 70-73\nmarket data latency, 395\nmarket depth, 71\nmarket equilibrium, 6\nmarket exchanges, 47\nmarket orders, 72\nmarket price, 6\nmarkets (see financial markets)\nMarkets in Financial Instruments Directive\n(MiFID), 32\nmatrix data, 59-60\nadjacency matrix, 60\nMBO (Market by Order), 71\nMBP (Market by Price), 71\nmean time before failure (MTBF), 396\nmean time to acknowledge (MTTA), 397\nmean time to detect (MTD), 396\nmean time to failure (MTTF), 397\nmean time to recovery (MTTR), 397\nmean-variance optimization (MVO), 59\nmemory-based transformations, 370-373\nmessage broker model, DSMs and, 323-328\nMessage Queuing Telemetry Transport\n(MQTT), 246\nmessage schema registry, 325\nmessaging services, Instant Bloomberg, 51\nmicroservices, 241\nworkflows, 420-424\nproject, 448-457\nMiFID (Markets in Financial Instruments\nDirective), 32\nmigrating to cloud, 231-234\nML (machine learning)\noverview, 21-22\nreinforcement learning, 25-27\nsupervised learning, 22-23\nunsupervised learning, 24\nworkflows, 424-429\nMLOps (ML operations), 427\nmodel registry, ML workflows, 426\nmodular architecture, 241\nmicroservices, 241\nmodulus 10 algorithm, 105\nmonetary policies at central banks, 6monitoring, 385, 390-392\n(see also DQMs)\nbusiness and analytical, 400-403\ncloud-based, 397\ncost monitoring, 397-400\nevents, 388\nInfluxDB, 387\nlogs and, 388-389\nmetrics, 386-387\nperformance monitoring, 392-397\ntraces, 389-390\nmonolith architecture, 240-241\nMorningstar, Inc., data acquisition process, 47\nMQTT (Message Queuing Telemetry Trans‐\nport), 246\nMTBF (mean time before failure), 396\nMTTA (mean time to acknowledge), 397\nMTTD (mean time to detect), 396\nMTTF (mean time to failure), 397\nMTTR (mean time to recovery), 397\nmulti-cloud computing, 238-239\nmultilayer graphs, 65\nmultipartite graphs, 63\nmultitenancy, 235\nMVO (mean-variance optimization), 59\nN\nnamed entity disambiguation (NED), 133,\n149-150\nnamed entity recognition (see NER)\nNamed Entity Recognition and Disambiguation\n(NERD), 149-150\nNational Best Bid and Offer (NBBO), 72\nNational Market System (NMS), 72\nNational Numbering Agency (NNA), 91\nnatural language processing (NLP), 135\nNBBO (National Best Bid and Offer), 72\nNED (named entity disambiguation), 133,\n149-150\nNER (named entity recognition), 129-134\ndeep learning approach, 144\nentity categorization, 137\nentity disambiguation, 138\nentity extraction, 136\nevaluation, 138\nfeature-engineering machine learning\napproach, 142-143\nknowledge graphs, 148-149\nlabeled databases, 139\n474 | Index\nlexicon/dictionary approach, 141\nLLM (large language model) approach,\n145-146\npreprocessing, 135-136\nrule-based approach, 142\nWikification, 146-148\nNERD (Named Entity Recognition and Disam‐\nbiguation), 149-150\nNew Y ork Stock Exchange, data generation, 14\nNLP (natural language processing), 135\nNMS (National Market System), 72\nNNA (National Numbering Agency), 91\nNoSQL, 291\nO\nOATS data validation, 381\nobservability, 404-406\nOffice of Financial Research (OFR), 79\nOFX (Open Financial Exchange), 263\nOLAP (online analytical processing), 293, 337\nOLTP (online transaction processing), 220\nOMS (Order Matching System), 71\non-premises solutions, 227-229\nopen finance initiatives, 272\nOpen Financial Exchange (OFX), 263\nopen source software advantages, 221-223\nOpen Systems Interconnection model (see OSI\nmodel)\nOpenFIGI, 457-460\noperational inefficiency, 172\nOPRA (Options Price Reporting Authority)\ndata collection, 14\noptimization process, supervised learning, 23\nOptimized Row Columnar (ORC), 258\nOracle Database, 220\nOrbis, 82\nORC (Optimized Row Columnar), 258\norder book, 71\norder book depth, 71\nOrder Matching System (OMS), 71\nOSI (Open Systems Interconnection) model,\n244\napplication layer, 245-246\nnetwork access layer, 248\nnetwork layer, 247-248\ntransport layer, 246-247\noutliers, 177-179\nownership, 195P\nPaaS (platforms-as-a-service), 230\npanel data, 57-58\npanel queries, 349-351\npatent data, 84\npayment tokenization, 208\nPayment versus Payment (PvP), 75\nPCI DSS (Payment Card Industry Data Security\nStandard), 211\npeer-reviewed journals, 8-9\nperformance\ndata governance and, 172\nmonitoring, 392-397\nsoftware and, 224-226\nperformance metrics, supervised learning, 23\nPermID (Permanent Identifier), 20, 457-460\npersonally identifiable information (PII), 201\nphishing, 200\nPII (personally identifiable information), 201\nPIMS (Privacy Information Management Sys‐\ntem), 199\nPIT (point-in-time) data, 69\nPMI (Purchasing Managers’ Index), 53\npolyglot persistence, DSSs and, 293-294\nPortfolio Analytics, 26\nPortfolio Commentary (AI tool), 26\nportfolio reconciliation, 198\nPostgreSQL RDMS project\nbusiness requirements, 432\ndata definition, 436\nentity relationships, 434\nlocal testing, 436-441\nprice discreteness, 73\nprivacy, 198, 201-203\nanonymization, 203-209\ndifferential, 428\nencryption, 209-210\nPrivacy Information Management System\n(PIMS), 199\nprivate cloud computing, 236-237\nprojects\nETL workflow, 442-448\nmicroservice workflow, 448-457\nprerequisites, 431\nRDMS, 432-442\nreference data store, 457-460\nproprietary data sources, 54\npublic cloud computing, 235-236\npublic financial data, 44-47\nIndex | 475\nPurchasing Managers’ Index (PMI), 53\nPvP (Payment versus Payment), 75\nPython, ETL workflow, 442-448\nQ\nQuantitative Finance, 9\nquerying\ndatabase side, 351-354\npatterns, 347\ncross-section queries, 348\npanel queries, 349-351\ntime series, 348\nuser side, 354-357\nR\nransomware, 200\nRavenPack News Analytics (RNA), 130\nRDMS (relational database management sys‐\ntem), 301\nACID transactions, 303\ndata modeling and, 305-309\nPostgreSQL project\nbusiness requirements, 432\ndata definition, 436\nentity relationships, 434\nlocal testing, 436-441\nquerying, 305\nSQL and, 302\nReal-Time Gross Settlement (RTGS), 75\nreal-time systems, 251\nreference data, 77-80\nreference data store project, 457-460\nRefinitiv Instrument Code (RIC) ticker, 113\nregression, supervised learning and, 23\nregularization, supervised learning and, 23\nregulations\ndisclosure requirements, 44-45\nfinancial industry, 20, 31-32\nFinTech, 27\ninfrastructure requirements, 32\nmonitoring and, 385\nrisk management and, 173\nregulatory institutions, 35\nReinforcement Learning from Human Feed‐\nback (RLHF), 145\nreinforcement learning, machine learning and,\n25-27\nreinstatements, 68relational database management system (see\nRDMS)\nreporting, financial identifiers and, 89\nreproducibility, 69\nresearch data, public, 46\nReturn on Equity (ROE), 68\nReview of Financial Studies, 9\nRIC (Refinitiv Instrument Code) ticker, 113\nRipple Labs case study, 345\nrisk exposure calculation time, 395\nrisk management, 8\ndata governance and, 173\nmonitoring and, 385\nregulations, 173\nrisk examples, 173\nrisk monitoring, data engineering and, 401\nRivest-Shamir-Adleman (RSA) algorithms, 210\nRLHF (Reinforcement Learning from Human\nFeedback), 145\nRNA (RavenPack News Analytics), 130\nROE (Return on Equity), 68\nrounding, 362\nrow-oriented format, big data, 257\nRSA (Rivest-Shamir-Adleman) algorithms, 210\nRTGS (Real-Time Gross Settlement), 75\nS\nS&P Global Market Intelligence, 51\nSaaS (software-as-a-service), 230\nsaga patterns, 422\nscalability\nDSS (data storage system) and, 288-289\nfinancial identification systems and, 96-97\ntransformations and, 379\nscaling, outliers and, 178\nSDC Platinum, 83\nSDRs (swap data repositories), 189\nSEC (Securities and Exchange Commission), 45\nsector filters, 364\nsector identifiers, 121-122\nSecure File Transfer Protocol (SFTP), 246, 275\nSecure Shell (SSH), 246\nSecurities and Exchange Commission (SEC), 45\nSecurities Information Processor (SIP), 72\nsecurity, 198\naccess control, 210-212\nanonymization, 203-209\ncloud computing, 232\nDSS (data storage system) and, 290\n476 | Index\nencryption, 209-210\nfile transfer, 275\nfinancial identification systems and, 102\nsecurity exchanges, 47\nfinancial data engineers, 36\nsecurity incident traces, 389\nSEDOL (Stock Exchange Daily Official List),\n112-113\nsemi-structured data, 17\nservice level agreements (SLAs), commercial\nsoftware, 218\nsettlement method, 74\nSFTP (Secure File Transfer Protocol), 246, 275\nshared responsibility principle, 235\nSimple Mail Transfer Protocol (SMTP), 245\nsingle cloud computing, 238-239\nSIP (Securities Information Processor), 72\nSIX Group, 106\nsize filters, 363\nSLAs (service level agreements), commercial\nsoftware, 218\nSMTP (Simple Mail Transfer Protocol), 245\nSnowflake, 276\nSociety for Worldwide Interbank Financial\nTelecommunication (SWIFT), 268\nsoftware\ncommercial, 218-219\nease of use, 224-226\nopen source, 221-223\nproprietary, in-house, 224\nspecialized, 277\nsoftware-as-a-service (SaaS), 230\nspam, 200\nSPDJI (Standard & Poor’s Dow Jones Indices),\n83\nspeed, cost, 375\nspoofing, 200\nSQL (Structured Query Language), 291\nrelational databases and, 302\nSQL injection, 200\nSSH (Secure Shell), 246\nStandard & Poor’s Dow Jones Indices (SPDJI),\n83\nstandardization, 361\nchallenges and issues, 19\nstandards, 190-191\nStock Exchange Daily Official List (SEDOL),\n112-113\nstock market APIs, 46stock market exchanges, 47\nstock similarities, 66\nstop words, 135\nstop-loss orders, 72\nSTP (straight-through processing), 74\nstream processing workflows, 417-420\nstreaming transformations, 368-369\nstructured data, 17\nStructured Query Language (see SQL)\nstructures\ncross-sectional data, 56\ngraph data, 60\nmatrix data, 59-60\npanel data, 57-58\ntime series data, 55-56\nsupervised learning, machine learning, 22-23\nsupply and demand, market price, 6\nsurplus, 6\nsurvey data, 53\nswap data repositories (SDRs), 189\nSWIFT (Society for Worldwide Interbank\nFinancial Telecommunication), 268\nsymmetric encryption, 209\nT\nTAQ (Trade and Quote) database, 81\nTASS (Trading Advisor Selection System), 180\ntaxonomies, XBRL, 261\nTCP (Transmission Control Protocol), 246\ntemporal graphs, 65\ntext data, 67\nthroughput, transformations and, 377\nticker symbols, 113-114\ntime series data, 55-56\ntime series model, DSMs and, 319-323\ntime series queries, 348\nTime to Insight (TTI), 392\nTime to Market (TTM), 392\ntimeliness of data, 187\nTLS (Transport Layer Security), 246\ntokenization, 135\ntop of the book, 71\ntopic modeling, 324\nTRACE (Trade Reporting and Compliance\nEngine), 82\ntraces\napplication traces, 389\nbusiness operations, 389\nsecurity incidents, 389\nIndex | 477\nTrade and Quote (TAQ) database, 81\ntrade execution error rate, 394\ntrade execution latency, 394\nTrade Reporting and Compliance Engine\n(TRACE), 82\ntrade settlement time, 395\nTrading Advisor Selection System (TASS), 180\ntrailing stop orders, 72\ntransaction identifiers, 110-111\ntransactional guarantee\nACID model, 284-286\nBASE model, 286\ntransactions\ndata, 73-76\nfinancial identifiers and, 89\ninitiation date, 74\nparties, 75\nprocessing time, 395\nsettlement date, 74\nsettlement method, 74\nspecifications, 73\nthird parties, 75\ntransformation and delivery\ncomputational requirements, 374-380\nconsumers, 382\ndelivery mechanisms, 383\nquery optimization\ndatabase-side queries, 351-354\nuser-side queries, 354-357\nquerying patterns, 347\nanalytical queries, 349-351\ncross-section queries, 348\npanel queries, 349\ntime series queries, 348\ntransformation patterns\nbatch, 368-369\ndisk-based, 370-373\nfull, 372\nincremental, 372\nmemory-based, 370-373\nstreaming, 368-369\ntransformations\ndata adjustments, 360\ndata cleaning, 358-360\ndata filtering, 363-364\ndata standardization, 361\nfeature engineering, 364-367\nformat conversion, 358\ntransformationsdata adjustments, 360\ndata cleaning, 358-360\ndata filtering, 363-364\ndata standardization, 361\nfeature engineering, 364-367\nformat conversion, 358\npatterns\nbatch, 368-369\ndisk-based, 370-373\nfull, 372\nincremental, 372\nmemory-based, 370-373\nstreaming, 368-369\nTransmission Control Protocol (TCP), 246\ntransmission protocols, 244\nOSI (Open Systems Interconnection)\nmodel, 244\napplication layer, 245-246\nnetwork access layer, 248\nnetwork layer, 247-248\ntransport layer, 246-247\ntransparency, 69\nTransport Layer Security (TLS), 246\ntrimmers, outliers and, 178\nTTI (Time to Insight), 392\nTTM (Time to Market), 392\nU\nUDP (User Datagram Protocol), 246\nUnique Swap Identifier (USI), 111\nUnique Transaction Identifier (UTI), 111, 390\nUnited States Patent and Trademark Office\n(USPTO), 84\nunstructured data, 17\nunsupervised learning, machine learning, 24\nUser Datagram Protocol (UDP), 246\nUSI (Unique Swap Identifier), 111\nUSPTO (United States Patent and Trademark\nOffice), 84\nUTI (Unique Transaction Identifier), 111, 390\nV\nvalidation datasets, supervised learning, 23\nValue at Risk (V AR), 20\nvendor selection, 52-53\nVirtual Private Cloud (VPC), 237, 248\nVirtual Private Networks (VPNs), 248\nvisualization, 60\nVPC (Virtual Private Cloud), 237, 248\n478 | Index\nVPNs (Virtual Private Networks), 248\nW\nwarehouse model, DSMs and, 335-341\nwarning logs, 388\nWeather Source, 84\nweb access mode, 277\nWells Fargo\nCards 2.0, 318\nmulti-cloud strategy, 239\nWharton Research Data Services (WRDS), 51\nWikification, NER and, 146-148\nwinsorization, outliers and, 178\nWMS (workflow management system), 410-414\nworkflow-oriented software architecture\n(WOSA), 407-408\nworkflowsabstraction, 408-409\nApache Airflow, 416\nETL (extract, transform, load), 414-416\nmicroservices, 420-424\nML (machine learning), 424-429\nstream processing, 417-420\nWOSA (workflow-oriented software archi‐\ntecture), 407-408\nWorld Bank Open Data, 46\nWOSA (workflow-oriented software architec‐\nture), 407-408\nWRDS (Wharton Research Data Services), 51\nX\nXBRL (eXtensible Business Reporting Lan‐\nguage), 20, 261\nIndex | 479",31145
184-About the Author.pdf,184-About the Author,,0
185-Colophon.pdf,185-Colophon,"About the Author\nTamer Khraisha  is a senior software and data developer, as well as a scientific author,\nwith over a decade of experience in both industry and research. Tamer’s experience\ncombines a solid background in financial markets with substantial expertise in soft‐\nware engineering. He did his undergraduate studies in finance and economics and\nearned a PhD in network science. Tamer’s research interests are focused on financial\nmarkets, data, and technology. During his professional career, Tamer has worked\nwith various FinTech startups, where he designed and built data-driven cloud plat‐\nforms for financial research, artificial intelligence, and asset management, as well as\ninternational payment systems.\nColophon\nThe animal on the cover of Financial Data Engineering  is a pearl oyster.\nAll species in the Pinctada  genus of saltwater oysters produce pearls, as do various\nother oysters and some non-oyster mollusks. Pearls are created when the oyster coats\na foreign particle with nacre, or calcium carbonate (of which its shell is also com‐\nposed), as a protection against the irritant.\nIn the wild, only one in around 10,000 oysters will produce a pearl, and a small per‐\ncentage of those pearls form in a desirable shape and color. As such, most of the\npearls obtained for the commercial jewelry market are farm raised.\nMany of the animals on O’Reilly covers are endangered; all of them are important to\nthe world.\nThe cover illustration is by Jose Marzan, based on an antique line engraving from\nNatural History of Ceylon . The series design is by Edie Freedman, Ellie Volckhausen,\nand Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans.\nThe text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed;\nand the code font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2023 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.  175  7x9.1975",2109
