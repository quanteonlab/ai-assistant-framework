filename,title,text,len
01-Cover.pdf,01-Cover,Computational Physics,21
02-Contents.pdf,02-Contents,"Computational Physics\nProblem Solving with Python\nFourth Edition\nRubin H. Landau\nManuel J. P√°ez\nCristian C. Bordeianu (D)\nWith contributions by Guangliang He\nAuthors\nProf. Rubin H. Landau\nOregonStateUniversity\nCorvallis\nOR\n97331\nUnitedStatesofAmerica\nProf. Manuel J. P√°ez\nDepartamentoFisica\nUniversaddeAntioquia\nMedellin\nColombia\nProf. Cristian C. Bordeianu‚Ä†\nUniversityofBucharest\nStr.Micanr.7\njud.Suceav\n725100\nRomania\nCover Image: ¬©PASIEKA/GettyImagesAllbookspublishedby WILEY-VCH arecarefully\nproduced.Nevertheless,authors,editors,andpublisher\ndonotwarranttheinformationcontainedinthese\nbooks,includingthisbook,tobefreeoferrors.Readers\nareadvisedtokeepinmindthatstatements,data,\nillustrations,proceduraldetailsorotheritemsmay\ninadvertentlybeinaccurate.\nLibrary of Congress Card No.: appliedfor\nBritish Library Cataloguing-in-Publication Data\nAcataloguerecordforthisbookisavailablefrom\ntheBritishLibrary.\nBibliographic information published by\nthe Deutsche Nationalbibliothek TheDeutsche\nNationalbibliothekliststhispublicationinthe\nDeutscheNationalbibliografie;detailedbibliographic\ndataareavailableontheInternetat\n<http://dnb.d-nb.de >.\n¬©2024WILEY-VCHGmbH,Boschstra√üe12,69469\nWeinheim,Germany\nAllrightsreserved(includingthoseoftranslationinto\notherlanguages).Nopartofthisbookmaybe\nreproducedinanyform‚Äìbyphotoprinting,microfilm,\noranyothermeans‚Äìnortransmittedortranslatedinto\namachinelanguagewithoutwrittenpermissionfrom\nthepublishers.Registerednames,trademarks,etc.\nusedinthisbook,evenwhennotspecificallymarked\nassuch,arenottobeconsideredunprotectedbylaw.\nPrint ISBN: 978-3-527-41425-3\nePDF ISBN: 978-3-527-84332-9\nePub ISBN: 978-3-527-84331-2\nTypesetting Straive,Chennai,India\nv\nContents\nPreface xvii\nAcknowledgments xix\nPart I Basics 1\n1 Introduction 3\n1.1 ComputationalPhysicsandScience 3\n1.2 ThisBook‚ÄôsSubjects 4\n1.3 VideoLectureSupplements 4\n1.4 ThisBook‚ÄôsCodesandProblems 5\n1.5 OurLanguage:ThePythonEcosystem 6\n1.6 TheEasyWay:PythonDistributions 6\n2 Software Basics 9\n2.1 MakingComputersObey 9\n2.2 ComputerNumberRepresentations 11\n2.2.1 IEEEFloating-PointNumbers 12\n2.2.1.1 ExamplesofIEEERepresentations 15\n2.2.2 PythonandtheIEEE754Standard 17\n2.3 PythonMiniTutorial 18\n2.3.1 StructureandFunctions 18\n2.3.2 VariableTypesandOperators 19\n2.3.3 BooleanandControlStructures 20\n2.3.4 PythonListsasArrays 21\n2.3.5 PythonI/O 23\n2.3.6 Python‚ÄôsAlgebraicTools 24\n2.4 ProgrammingWarmup 25\n2.4.1 ProgramDesign 26\n2.4.2 FirstProgrammingSteps 27\n2.4.3 OverandUnderflowExercises 28\n2.4.4 MachinePrecision 29\n2.4.5 Experiment:YourMachine‚ÄôsPrecision 30\n2.5 Python‚ÄôsVisualizationTools 30\nviContents\n2.5.1 Visual(VPython)‚Äôs2DPlots 31\n2.5.2 Matplotlib‚Äôs2DPlots 32\n2.5.3 Matplotlib‚Äôs3DSurfacePlots 35\n2.5.4 Matplotlib‚ÄôsAnimations 36\n2.6 PlottingExercises 36\n2.7 CodeListings 38\n3 Errors and Uncertainties 44\n3.1 TypesofErrors 44\n3.1.1 CourtingDisaster:SubtractiveCancelation 46\n3.1.2 SubtractiveCancelationExercises 46\n3.1.3 Round-OffErrors 48\n3.1.4 Round-OffErrorAccumulation 48\n3.2 ExperimentalErrorInvestigation 49\n3.3 ErrorswithPowerSeries 52\n3.3.1 ImplementationandAssessment 53\n3.3.2 ErrorinSpecularReflection 54\n3.4 ErrorsinBesselFunctions 55\n3.4.1 NumericalRecursion(Method) 55\n3.4.2 ImplementationandAssessment:RecursionRelations 57\n3.5 CodeListing 58\n4 Monte Carlo Simulations 59\n4.1 RandomNumbers 59\n4.1.1 RandomNumberGeneration 60\n4.1.2 ComputingaRandomSequence 62\n4.2 SimulatingaRandomWalk 63\n4.2.1 RandomWalkImplementation 64\n4.2.2 RandomWalksinaBrain 65\n4.2.3 RandomProteinFolding 67\n4.3 SpontaneousDecay 68\n4.3.1 DiscreteDecayModel 69\n4.3.2 TheExponentialDecayApproximation 70\n4.3.3 DiscreteDecaySimulation 70\n4.3.4 DecayImplementationandVisualization 71\n4.4 TestingandGeneratingRandomDistributions 71\n4.5 CodeListings 73\n5 Differentiation and Integration 78\n5.1 DifferentiationAlgorithms 78\n5.1.1 ForwardDifference 78\n5.1.2 CentralDifference 79\n5.2 ExtrapolatedDifference 80\n5.2.1 SecondDerivatives 81\n5.2.1.1 Assessment 81\n5.3 IntegrationAlgorithms 83\nContents vii\n5.3.1 BoxCounting 83\n5.3.2 TrapezoidRule 84\n5.3.3 Simpson‚ÄôsRule 85\n5.3.4 SimpleIntegrationErrorEstimates 86\n5.3.5 Higher-OrderAlgorithms 88\n5.4 GaussianQuadrature 89\n5.4.1 MappingGaussianPoints 90\n5.4.2 GaussianQuadratureDerivation ‚äô90\n5.5 MonteCarloIntegrations 91\n5.5.1 StoneThrowingImplementation 92\n5.5.2 IntegrationErrorInvestigation 93\n5.6 MeanValueandN‚ÄìDIntegration 94\n5.6.1 10-DMCErrorInvestigation 95\n5.6.2 Implementation:10-DMonteCarloIntegration 95\n5.7 MCVarianceReduction 96\n5.8 ImportanceSamplingandvonNeumannRejection 96\n5.9 CodeListings 97\n6 Trial-and-Error Searching and Data Fitting 100\n6.1 QuantumBoundStatesI 100\n6.2 BisectionSearch 101\n6.2.1 BisectionExercises 102\n6.3 Newton‚ÄìRaphsonSearch 102\n6.3.1 Search +Backtracking 104\n6.4 MagnetizationSearch 105\n6.5 DataFitting 107\n6.5.1 LagrangeFitting 109\n6.5.2 CubicSplineInterpolation 109\n6.5.3 CubicSplineQuadrature 111\n6.6 FittingExponentialDecay 112\n6.7 Least-SquaresFitting 113\n6.7.1 Least-SquaresImplementation 114\n6.7.2 LinearQuadraticFit 116\n6.7.2.1 LinearQuadraticFitAssessment 118\n6.8 NonlinearFittoaResonance 118\n6.9 CodeListings 120\n7 Matrix Computing and N‚ÄìD Searching 123\n7.1 MassesonaStringandN‚ÄìDSearching 123\n7.2 MatrixGeneralities 126\n7.3 MatricesinPython 129\n7.3.1 ListsasArrays 129\n7.3.2 NumPyMatrices 130\n7.3.3 NumPyLinearAlgebraLibrary 134\n7.4 Exercise:TestsBeforeUse 136\n7.5 SolutiontoStringProblem 139\nviiiContents\n7.6 SpinStatesandHyperfineStructure 139\n7.7 SpeedingUpMatrixComputing ‚äô141\n7.7.1 Vectorization 141\n7.7.2 SpeedupExercises 143\n7.8 CodeListing 144\n8 Differential Equations and Nonlinear Oscillations 147\n8.1 NonlinearOscillators 147\n8.2 ODEReview 149\n8.2.1 Order 149\n8.2.2 OrdinaryandPartial 149\n8.2.3 LinearandNonlinear 150\n8.2.4 InitialandBoundaryConditions 150\n8.3 DynamicFormofODEs 150\n8.4 ODEAlgorithms 152\n8.4.1 Euler‚ÄôsRule 152\n8.4.2 Runge‚ÄìKuttaRule 153\n8.4.3 Adams-Bashful-MoultonPredictor-CorrectorRule 155\n8.4.4 Assessment:rk2 versusrk4versusrk45 156\n8.5 SolutionforNonlinearOscillations 157\n8.5.1 PrecisionAssessmentviaEConservation 158\n8.6 Extensions:NonlinearResonances,Beats,Friction 159\n8.6.1 Friction 159\n8.6.2 ResonancesandBeats 159\n8.6.3 Time-DependentForces 160\n8.7 CodeListings 161\nPart II Data Science 165\n9 Fourier Analyses 167\n9.1 FourierSeries 167\n9.1.1 SawtoothandHalf-WaveFunctions 169\n9.1.2 Exercises:FourierSeriesSummations 170\n9.2 FourierTransforms 170\n9.3 DiscreteFourierTransforms 172\n9.3.1 Aliasing 174\n9.3.2 Assessments 176\n9.3.3 TransformingNonperiodicFunctions 178\n9.4 NoiseFiltering 178\n9.4.1 NoiseReductionviaAutocorrelation 178\n9.4.2 AutocorrelationFunctionExercises 181\n9.4.3 FilteringwithTransforms 181\n9.4.4 DigitalFilters:WindowedSincFilters ‚äô183\n9.5 FastFourierTransform ‚äô185\n9.5.1 BitReversal 187\nContents ix\n9.6 FFTImplementation 189\n9.7 FFTAssessment 190\n9.8 CodeListings 190\n10 Wavelet and Principal Components Analysis 193\n10.1 PartI:WaveletAnalysis 193\n10.2 WavePacketsandUncertaintyPrinciple 195\n10.2.1 WavePacketExercise 196\n10.3 Short-TimeFourierTransforms 197\n10.4 WaveletTransforms 198\n10.4.1 GeneratingWaveletBasisFunctions 198\n10.4.2 ContinuousWaveletTransforms 201\n10.5 DiscreteWaveletTransforms ‚äô203\n10.5.1 PyramidScheme ‚äô205\n10.5.2 DaubechiesWaveletsFilters ‚äô209\n10.5.3 DWTExercise ‚äô212\n10.6 PartII:PrincipalComponentsAnalysis 213\n10.6.1 Multi-dimensionalDataSpace 214\n10.6.2 WondersoftheCovarianceMatrix 215\n10.6.3 DemonstrationofPrincipalComponentAnalysis 218\n10.6.4 PCAExercises 219\n10.7 CodeListings 220\n11 Neural Networks and Machine Learning 224\n11.1 PartI:BiologicalandArtificialNeuralNetworks 225\n11.1.1 ArtificialNeuralNetworks 226\n11.2 ASimpleNeuralNetwork 226\n11.2.1 CodingANeuron 227\n11.2.2 BuildingASimpleNetwork 227\n11.2.3 TrainingASimpleNetwork 228\n11.2.4 DecreasingtheError 230\n11.2.5 CodingandRunningASimpleNetwork 232\n11.3 AGraphicalDeepNet 232\n11.4 PartII:MachineLearningSoftware 234\n11.4.1 TensorFlowInstallationandExecution 235\n11.5 TensorFlowandSkLearnExamples 235\n11.5.1 PreprocessingwithScikit-learn 238\n11.5.1.1 GradientTape 239\n11.5.2 LinearFittoHubble‚ÄôsData 239\n11.6 MLClustering 240\n11.6.1 ReadingFileswithPanda 242\n11.6.2 ClusteringwithPerceptrons 242\n11.6.3 ClusteringwithStochasticGradientDescent 243\n11.7 Keras:Python‚ÄôsDeepLearningAPI 244\n11.8 ImageProcessingwithOpenCV 244\nxContents\n11.8.1 BackgroundSubtraction 246\n11.9 ExploreMLDataRepositories 247\n11.10 CodeListings 247\n12 Quantum Computing (G. He, Coauthor) 254\n12.1 DiracNotationinQuantumMechanics 254\n12.2 FromBitstoQubits 255\n12.2.1 MultipleQubitStates 256\n12.3 EntangledandSeparableStates 257\n12.3.1 PhysicsExercise:TwoEntangledDipoles 258\n12.4 LogicGates 260\n12.4.1 1-QubitGates 261\n12.4.2 2-QubitGates 262\n12.4.3 EntanglementviaGates 263\n12.4.4 3-QubitGates 264\n12.5 AnIntrotoQCProgramming 264\n12.5.1 HalfandFullAdders 269\n12.6 Accessingthe IBM Quantum Computer 270\n12.6.1 IBMQuantumComposer 270\n12.7 QiskitPlusIBMQuantum 272\n12.7.1 AFullAdder 274\n12.7.2 IBMQuantumExercises 275\n12.8 TheQuantumFourierTransform 275\n12.8.1 1-QubitQFT 276\n12.8.2 2-QubitQFT 276\n12.8.3 n-QubitQFT ‚äô277\n12.9 Oracle +Diffuser =Grover‚ÄôsSearchAlgorithm 278\n12.9.1 Grover‚ÄôsImplementation 280\n12.10 Shor‚ÄôsFactoring ‚äô281\n12.11 CodeListings 284\nPart III Applications 289\n13 ODE Applications; Eigenvalues, Scattering, Trajectories 291\n13.1 QuantumEigenvaluesforArbitraryPotentials 291\n13.1.1 Model:NucleoninaBox 292\n13.2 Algorithm:ODESolver +Search 293\n13.2.1 NotRecommended:MatchlessSearching 294\n13.2.2 NumerovAlgorithmforSchr√∂dingerODE‚®Ä294\n13.2.3 Implementation:EigenvaluesviaODESolver +BisectionAlgorithm 295\n13.2.4 Explorations 296\n13.3 ClassicalChaoticScattering 296\n13.3.1 ModelandTheory 297\n13.3.2 Implementation 298\n13.3.3 Assessment 299\nContents xi\n13.4 ProjectileMotionwithDrag 299\n13.4.1 Assessment 300\n13.5 2-and3-BodyPlanetaryOrbits 301\n13.5.1 PlanetsviaTwoofNewton‚ÄôsLaws 301\n13.5.2 TheDiscoveryofNeptune 302\n13.6 CodeListings 303\n14 Fractals and Statistical Growth Models 307\n14.1 TheSierpi¬¥ nskiGasket 308\n14.1.1 MeasuringFractalDimension 309\n14.2 GrowingPlants 310\n14.2.1 Self-AffineConnection 310\n14.2.2 Barnsley‚ÄôsFern 311\n14.2.3 Self-AffineTrees 312\n14.3 BallisticDeposition 312\n14.4 LengthofBritishCoastline 313\n14.4.1 BoxCountingAlgorithm 314\n14.4.2 CoastlineExercise 315\n14.5 CorrelatedGrowth 317\n14.6 Diffusion-LimitedAggregation 318\n14.6.1 FractalofDLAorPollock 319\n14.7 FractalsinBifurcations 320\n14.8 CellularAutomataFractals 320\n14.9 PerlinNoiseAddsRealism ‚äô321\n14.9.1 RayTracingAlgorithms 323\n14.10 CodeListings 324\n15 Nonlinear Population Dynamics 329\n15.1 TheLogisticMap,ABugPopulationModel 329\n15.1.1 ExploringMapProperties 331\n15.1.1.1 StablePopulations 331\n15.1.2 FixedPoints 332\n15.1.3 PeriodDoubling,Bifurcations 332\n15.1.4 MappingImplementation 333\n15.2 Chaos 333\n15.3 BifurcationDiagrams 333\n15.3.1 BifurcationDiagramImplementation 335\n15.3.2 FeigenbaumConstants 335\n15.3.3 OtherMaps 336\n15.4 MeasuresofChaos 336\n15.4.1 LyapunovCoefficients‚®Ä336\n15.4.2 ShannonEntropy 338\n15.5 CoupledPredator‚ÄìPreyModels‚®Ä338\n15.5.1 Lotka‚ÄìVolterraModel 339\n15.5.2 Predator‚ÄìPreyChaos 340\n15.5.3 LVMwithPreyLimit 342\nxiiContents\n15.5.4 LVMwithPredationEfficiency 342\n15.5.5 LVMImplementationandAssessment 343\n15.5.6 TwoPredators,OnePrey 344\n15.6 CodeListings 344\n16 Nonlinear Dynamics of Continuous Systems 348\n16.1 TheChaoticPendulum 348\n16.1.1 FreePendulumOscillations 349\n16.1.2 AnalyticSolutionasEllipticIntegrals 349\n16.1.3 FreePendulumImplementationandTest 350\n16.2 PhaseSpace 351\n16.3 ChaoticExplorations 354\n16.3.1 PhaseSpaceWithoutVelocities 356\n16.3.2 ChaoticBifurcations 357\n16.3.3 FourierorWaveletAnalysis 357\n16.4 OtherChaoticSystems 358\n16.4.1 TheDoublePendulum 358\n16.4.2 Billiards 359\n16.4.3 MultipleScatteringCenters 360\n16.4.3.1 HardDiskScattering 361\n16.4.4 LorenzAttractors 361\n16.4.5 vanderPoolOscillator 363\n16.4.6 TheDuffingOscillator 363\n16.5 CodeListings 364\n17 Thermodynamics Simulations and Feynman Path Integrals 365\n17.1 AnIsingMagneticChain 365\n17.1.1 StatisticalMechanics 367\n17.1.1.1 AnalyticSolution 367\n17.2 MetropolisAlgorithm 368\n17.2.1 MetropolisExercise 369\n17.2.2 EquilibrationandThermodynamicProperties 370\n17.2.3 Explorations 371\n17.3 FastEquilibrationviaWang‚ÄìLandauSampling ‚äô372\n17.3.1 WLSImplementation 373\n17.4 PathIntegralQuantumMechanics ‚äô374\n17.4.1 Bound-StateWaveFunction 376\n17.5 LatticePathIntegration 377\n17.5.1 ATime-SavingTrick 380\n17.6 Implementation 381\n17.6.1 PathIntegrationExercise 382\n17.6.2 QuantumBouncer ‚äô383\n17.6.3 PathIntegralBouncerExercises 384\n17.7 CodeListings 385\nContents xiii\n18 Molecular Dynamics Simulations 391\n18.1 MD VersusThermodynamics 394\n18.2 Initial,Boundary,andLarge rConditions 394\n18.3 VerletAlgorithms 396\n18.3.1 ImplementationandExercise 397\n18.3.2 Analysis 398\n18.4 MDfor16Particles 400\n18.5 CodeListing 402\n19 General Relativity 408\n19.1 Einstein‚ÄôsFieldEquations 408\n19.1.1 CalculatingtheRiemannandRicciTensors 410\n19.1.2 RiemannandRicciTensorProblems 410\n19.1.3 EventHorizons 411\n19.2 GravitationalDeflectionofLight 412\n19.2.1 GravitationalLensing 413\n19.3 PlanetaryOrbitsinGRGravity 414\n19.3.1 Newton‚ÄôsPotentialCorrected 414\n19.3.2 OrbitComputationviaEnergyConservation 414\n19.3.3 PrecessionofthePerihelionofMercury 416\n19.4 VisualizingWormholes 418\n19.5 Problems 420\n19.6 CodeListings 420\n20 Integral Equations 425\n20.1 NonlocalPotentialBinding 425\n20.2 Momentum-SpaceSchr√∂dingerEquation 425\n20.2.1 IntegraltoMatrixEquations 426\n20.2.2 Delta-ShellPotential 428\n20.2.3 WaveFunction(Exploration) 429\n20.3 ScatteringinMomentumSpace ‚äô429\n20.3.1 Schr√∂dingertoLippmann‚ÄìSchwingerEquation 429\n20.3.2 SingularIntegralEvaluations 430\n20.3.3 SingularIntegralEquationstoMatrixEquations 431\n20.3.4 Solution 432\n20.3.5 Exercises 433\n20.3.6 ScatteringWaveFunction(Exploration) 434\n20.4 CodeListings 434\nPart IV PDE Applications 437\n21 PDE Review, Electrostatics and Relaxation 439\n21.1 Review 439\nxivContents\n21.2 Laplace‚ÄôsEquation 441\n21.2.1 FourierSeriesSolution 442\n21.2.2 FourierSeriesasanAlgorithm 443\n21.3 Finite-DifferenceAlgorithm 444\n21.3.1 RelaxationandOverrelaxation 446\n21.4 AlternateCapacitorProblems 447\n21.4.1 Implementation 449\n21.5 ElectricFieldVisualization 449\n21.6 CodeListings 450\n22 Heat Flow and Leapfrogging 452\n22.1 TheParabolicHeatEquation 452\n22.1.1 SolutionasAnalyticExpansion 453\n22.2 TimeStepping(Leapfrog)Algorithm 454\n22.2.1 VonNeumannStabilityCondition 455\n22.2.2 Implementation 456\n22.2.3 AssessmentandVisualization 456\n22.3 Newton‚ÄôsRadiativeCooling 457\n22.4 TheCrank‚ÄìNicolsonAlgorithm 458\n22.4.1 SolutionviaTridiagonalMatrix ‚äô460\n22.4.2 Crank‚ÄìNicolsonImplementation 461\n22.5 CodeListings 462\n23 String and Membrane Waves 464\n23.1 AVibratingString‚ÄôsHyperbolicWaveEquation 464\n23.1.1 SolutionasNormal-ModeExpansion 465\n23.2 Time-SteppingAlgorithm 466\n23.3 vonNeumannStabilityAnalysis 468\n23.3.1 ImplementationandAssessment 469\n23.4 BeyondTheSimpleWaveEquation 469\n23.4.1 IncludingFriction 469\n23.4.2 IncludingVariableTensionandDensity 470\n23.4.3 WavesonCatenary 471\n23.4.4 CatenaryAssessment 472\n23.4.5 IncludingNonlinearTerms 473\n23.5 VibratingMembrane(2DWaves) 474\n23.6 AnalyticalSolution 475\n23.7 NumericalSolution 476\n23.8 CodeListings 478\n24 Quantum Wave Packets and EM Waves 480\n24.1 Time-DependentSchr√∂dingerEquation 480\n24.2 Split-TimeAlgorithm 482\n24.2.1 Implementation 482\n24.2.1.1 Animation 483\n24.2.2 WavePacketsinOtherWells 484\nContents xv\n24.3 SpecialSchr√∂dingerAlgorithm 484\n24.4 QuantumChaos 485\n24.4.1 QuantumBilliards 486\n24.4.2 ThreeDisksScattering 487\n24.5 E&MWaves:FiniteDifferenceTimeDomain 488\n24.6 Maxwell‚ÄôsEquations 488\n24.7 Split-TimeFDTD 489\n24.7.1 ImplementationandAssessment 491\n24.8 MoreE&MProblems 492\n24.8.1 CircularlyPolarizedWaves 492\n24.8.2 WavePlates 493\n24.8.3 AlgorithmandExercise 494\n24.8.4 TwinLeadTransmissionLine 495\n24.9 CodeListings 496\n25 Shock and Soliton Waves 501\n25.1 TheContinuityandAdvectionEquations 502\n25.2 ShockWavesviaBurgers‚ÄôEquation 503\n25.2.1 Lax‚ÄìWendroffAlgorithm 504\n25.2.2 ImplementationandAssessment 505\n25.3 IncludingDispersion 505\n25.4 KdeVSolitons 506\n25.4.1 AnalyticSolution 507\n25.4.2 Algorithm 508\n25.4.3 Implementation 509\n25.4.4 Exploration:PhaseSpaceSolitonsandSolitonCrossings 509\n25.5 PendulumChainSolitons 510\n25.5.1 IncludingDispersion 511\n25.6 ContinuumLimit,theSine-GordonEquation 512\n25.6.1 AnalyticSolution 513\n25.6.2 Numeric2DSolitons(Pulsons) 513\n25.6.3 Implementation 514\n25.7 CodeListings 516\n26 Fluid Hydrodynamics 518\n26.1 Navier‚ÄìStokesEquation 518\n26.2 FlowThroughParallelPlates 520\n26.3 Navier‚ÄìStokesDifferenceEquation 522\n26.3.1 SuccessiveOverrelaxationAlgorithm 523\n26.4 VorticityFormofNavier‚ÄìStokesEquation 523\n26.4.1 VorticityDifferenceEquation 525\n26.4.2 BeamBoundaryConditions 526\n26.5 AssessmentandExploration 527\n26.5.1 Explorations 529\n26.6 CodeLisitings 529\nxviContents\n27 Finite Element Electrostatics ‚äô531\n27.1 ThePotentialofTwoMetalPlates 531\n27.1.1 AnalyticSolution 531\n27.2 FiniteElementMethod 532\n27.2.1 WeakFormofPDE 532\n27.2.2 GalerkinSpectralDecomposition 533\n27.2.3 SolutionviaLinearEquations 534\n27.2.4 ImposingtheBoundaryConditions 536\n27.3 1DFEMProblems 536\n27.4 2DFEMExercises 537\n27.5 CodeListings 539\nAppendix Codes and Animations 543\nReferences 546\nIndex 555",17629
03-Preface.pdf,03-Preface,"xvii\nPreface\nWhenthefirsteditionof Computational Physics waspublishedin1997,whowouldhave\nthought that we would be doing it again in 2024? Back then, we hoped that our writing\nmight encourage the inclusion of more computation into the physics curriculum. Now,\ncomputationalphysics(CP)coursesaretaughtwidely(ifonlymorewithourbook!).Yet,\nwhenMartinPreussofWileyaskedifwemightbeinterestedinafourthedition,thethought\nofgettingtoworkwithsomerecentdevelopmentsincomputationwasjusttooappealing\nforustoresist.Andso,hereweare!\nAnd who would have thought that our youngest coauthor, Christian Bordeianu, who\njoinedusforthesecondandthirdeditions,wouldnotbearoundforthisone!Itissosad\nthatwemustwritewithouthisenthusiasm,knowledge,goodnature,andfriendship.\nThiseditioncontinueswiththethreemainthemesofpreviousones:\n1) ThatthereisvaluewhenfirstlearningCPtosurveyabroadrangeoftopicsinvarious\nspecialties.\n2) ThatthebestwaytolearnCPisbydoingitonacomputerwithexamples,exercises,and\nproblems.\n3) That CP is part of computational science, which means that we are presenting CP as\namixtureofphysics,appliedmathematics,andcomputerscience,tryingnottoshort-\nchangethelattertwo.\nThiseditionhasentirelynewchaptersonneuralnetworksandmachinelearning,quan-\ntumcomputing(withGuangliangHe),andgeneralrelativity,aswellasanexpandedcov-\nerageofprincipalcomponentanalysesandPythonprogramming.Finally,therehasbeen\neditingthroughoutthetexttoimproveclarityandorganization.Thiseditionalsocontin-\nuesouradvocacyofacompiled-typeprogramminglanguage,andespeciallyPython,asthe\nbestvehicleforlearningCP.ThisisincontrasttocomputingenvironmentssuchasMat-\nlabandMathematica,wherethemathematics,algorithms,anddetailsarekept‚Äúunderthe\nhood.‚ÄùIfcomputationalresultsaretobescientificallysound,webelieveitrequiresaphysi-\ncisttounderstandthealgorithms,theirconnectionstomathematics,physics,thelogicofa\nprogram,and,especially,thelimitsanduncertaintiesofthecomputation.Nevertheless,we\nappreciatehowtime-consumingandfrustratingdebuggingprogramsmaybe,especiallyfor\nbeginners,andsoweprovidethereaderwithalargenumberofcodesinthetextandonline\nat:sites.science.oregonstate.edu/ ‚àºlandaur/Books/Problems/Codes/.Ourhopeisthatthis\nxviii Preface\nleaves time for exploration, extensions, and analysis. It also provides experience in the\nmodern work environment, in which one must incorporate new developments into the\npreexistingdevelopmentsofothers.\nTomakeroomforthenew,wehave(sadly)removedsomeoftheoldonesthatwerein\npreviouseditions.However,thosematerialscanstillbefoundinaJupyterNotebookversion\nof the previous edition online at sites.science.oregonstate.edu/‚àºlandaur/Books/CPbook/\neBook/.\nTherearealsovideolecturesatsites.science.oregonstate.edu/‚àºlandaur/Books/CPbook/\neBook/Lectures/andonYouTube‚Äôs Landau Computational Physics Course atwww.youtube.\ncom/playlist?list =PLnWQ_pnPVzmJnp794rQXIcwJIjwy7Nb2U.\nThesevideosandtheconcordantslidescovermostofthetopicsinthetextandmaybe\nhelpfulinblendedorhybridcourses.\nWehopeyouenjoyourworkandwelookforwardtoyourcomments.\nTucson,May2024 Rubin H. Landau, Tucson\nMedellin,May2024 Manuel J. P√°ez, Medellin\nPlanning the first edition.",3153
04-Acknowledgments.pdf,04-Acknowledgments,"xix\nAcknowledgments\nImmature poets imitate;\nmature poets steal .\n‚ÄîT.S.Elliot\nPreviouseditionsofthisbookandourcomputationalphysicscourseswouldnothavebeen\npossiblewithoutfinancialsupportfromtheNationalScienceFoundation‚ÄôsCCLI,EPIC,and\nNPACIprograms,andthePhysicsDepartmentofOregonStateUniversity.Thankyou,we\nhopewehavemadeyouproud.\nOur CP developments have followed the pioneering paths paved by Thompson, Gould\nand Tobochnik, Christian, and Press et al. Indubitably, we have borrowed material from\nthemandmadeitourownwithnofurtherthought.\nWewishtoacknowledgevaluablecontributionsbyGuangliangHe,HansKowallik,Sally\nHaerer (video lecture modules), Paul Fink (deceased) Oscar A. Restrepo, Jaime Zuluaga,\nandHenriJansen.Itisourpleasuretoacknowledgetheinvaluablefriendship,encourage-\nment,helpfuldiscussions,andexperienceswehavehadwithmanycolleaguesandstudents\nover the years. We are particularly indebted to Guillermo Avenda√±o-Franco, Saturo S.\nKano, Melanie Johnson, Jon Maestri (deceased), David McIntyre, Shashikant Phatak,\nViktorPodolskiy,C.E.Yaguna,andZlatcoDimcovic.And,finally,it‚Äôsbeenapleasureto\nworkwithMartinPreuss,AswiniMurugadass,andJudyHowarthatWileyagain.\nInspiteofeveryone‚Äôsbestefforts,therearestillerrorsandconfusingstatementsinthe\nbookandcodesforwhichwearetoblame.",1295
05-Part I Basics.pdf,05-Part I Basics,1\nPart I\nBasics,17
06-Chapter 1 Introduction.pdf,06-Chapter 1 Introduction,,0
07-1.2 This Books Subjects.pdf,07-1.2 This Books Subjects,"3\n1\nIntroduction\nBeginnings are hard .\n‚ÄîChaim PotokNothing is more expensive than a start .\n‚ÄîFriedreich Nietzsche\nWe start this book with a description of how computational physics (CP) Ô¨Åts into the broader\nÔ¨Åeld of computational science, and how CP Ô¨Åts into physics. We describe the subjects we\ncover, the coordinated video lectures, and how the book may be used in a CP course. Finally,\nwe get down to business by discussing the Python language and its many packages, some\nof which we‚Äôll use. In Chapter 2we give an introduction to Python programming, and in\nChapter 7we examine Python‚Äôs treatment of matrices .\n1.1 Computational Physics and Science\nAsillustratedinFigure1.1,weviewCPasabridgethatconnectsphysics,computerscience\n(CS),andappliedmathematics.WhereasCSstudiescomputingforitsownintrinsicinter-\nest and develops the hardware and software tools that computational scientists use, and\nwhileappliedmathematicsdevelopsandstudiesthealgorithmsthatcomputationalscien-\ntists use, CP focuses on using all of that to do better and new physics. Furthermore, just\nasanexperimentalistmustunderstandmanyaspectsofanexperimenttoensurethather\nmeasurementsareaccurateandbelievable,soshouldeveryphysicistundertakingacom-\nputation understand the CS and math well enough to ensure that her computations are\naccurateandprecise.\nAsCPhasmatured,weseeitnotonlyasabridgeamongdisciplines,butalsoasaspecialty\ncontaining core elements of its own, such as data-mining tools, computational methods,\nandaproblem-solvingmindset.Tous,CP‚Äôscommonalityoftoolsandviewpointwithother\ncomputationalsciencesmakesitagoodtraininggroundforstudents,andawelcomechange\nfromtheoverspecializationfoundinsomuchofphysics.\nAs part of this book‚Äôs emphasis on problem solving, we strive to present the subjects\nwithinaproblem-solvingparadigm,asillustratedontherightofFigure1.1.Oursisahands-\non, inquiry-based approach in which there are problems to solve, a theory or an appro-\npriatemodel to apply,an appropriatealgorithmto use, andan assessment ofthe results.\nComputational Physics: Problem Solving with Python ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH",2204
08-1.5 Our Language The Python Ecosystem.pdf,08-1.5 Our Language The Python Ecosystem,"41 Introduction\nScientific\ntruth\nScientific problem solvingMath\ntechniquesPhysics\napplication\nC\nP\nCS\nhard/software Simulation TheoryExperiment\nFigure 1.1 On the left a view of computational physics as a discipline encompassing physics,\napplied mathematics, and computer science. On the right is a broader view of computational\nphysics Ô¨Åtting into various components of scientiÔ¨Åc problem solving.\nThisapproachcanbetracedbacktothepost-WorldWarIIresearchtechniquesdevelopedat\nUSnationallaboratories.Theydeservethecreditforextendingthetraditionalexperimental\nandtheoreticalapproachesofphysicstoalsoincludesimulation.Recentdevelopmentshave\nalsointroducedpowerfuldataminingtools,suchasneuralnetworks,artificialintelligence,\nandquantumcomputing.\n1.2 This Book‚Äôs Subjects\nWedonotintendthisbooktobeascholarlyexpositionofthefoundationsofCP.Instead,\nwe employ a learn-by-doing approach with many exercises, problems, and ready-to-run\ncodes.WesurveymanyofthesubjectsthatconstituteCPatalevelappropriateforunder-\ngraduateeducation,exceptmaybeforthelatterpartsofsomechapters.Ourexperienceis\nthatmanygraduatestudentsandprofessionalsmayalsobenefitfromthissurveyapproach\ninwhichabasicunderstandingofabroadrangeoftopicsfacilitatesfurtherin-depthstudy.\nChapters 1‚Äì8 cover basic numerics, ordinary differential equations with (many) appli-\ncations,matrixcomputingusingwell-developedlinearalgebralibraries,andMonte-Carlo\nmethods. Some powerful data mining tools such as discrete Fourier transforms, wavelet\nanalysis,principalcomponentanalysis,andneuralnetworksarecoveredinthemiddleof\nthebook.\nA traditional way to view the materials in this text is in terms of their use in courses.\nForaone-quarterclass,weusedapproximatelythefirst-thirdofthetext,withitsemphasis\noncomputingtoolfamiliaritywithacompiledlanguage[CPUG,2009].Thelattertwo-thirds\nofthetext,withitsgreateremphasisonphysics,hastypicallybeenusedinatwo-quarter\n(20-week)course.Whatwithmanyofthetopicstakenfromresearch,thesematerialscan\neasilybeusedforafullyear‚Äôscourse,andforsupplementaryresearchprojects.\n1.3 Video Lecture Supplements\nAs an extension of the concept of a ‚Äútext,‚Äù we provide some 60 video lecture modules\n(asinFigure1.2)thatcoveralmosteverytopicinthe\n1.4 This Book‚Äôs Codes and Problems 5\nFigure 1.2 A screenshot from a lecture module showing a dynamic table of contents, a talking\nhead, video controls, a slide with live scribbling, and some old man. (Originally in Flash, now as\nmpegs.)\na mix of Flash, Java, HTML, and mpeg, but with Flash no longer supported, we provide\nthemasmp4videosandPDFslides.Theyareavailableonourwebsite:https://sites.science\n.oregonstate.edu/~landaur/Books/CPbook/eBook/Lectures, as well as on our YouTube\nchannelunder Landau Computational Physics Course :https://www.youtube.com/playlist?\nlist=PLnWQ_pnPVzmJnp794rQXIcwJIjwy7Nb2U.\nThevideolecturescanbeusedtoprevieworreviewmaterials,aspartofanonlinecourse,\norinablendedcourseinwhichtheyreplacesomelectures,therebyfreeinguptimeforlab\nworkwiththeinstructor.\n1.4 This Book‚Äôs Codes and Problems\nSeparatefromtheproblemsandexercisesthroughoutthetext,almosteverychapterstarts\noff with a keynote ‚Äú Problem‚Äù that leads into the various steps in computational prob-\nlemsolving(Figure1.1).Theadditionalproblemsandexercisesdistributedthroughoutthe\nchaptersareessentialingredientsforlearning,andaremeanttobeworkedthrough.This\nentailsstudyingthetext,writing,debugging,andrunningprograms,visualizingtheresults,\nandexpressinginwordswhathasbeenperformed,andwhatcanbeconcluded.Weasked\nourstudentstowriteupminilabreportscontaining\nEquationssolved Numericalmethod Codelisting\nVisualization Discussion Critique\nAlthough we recognize that programming is a valuable skill for scientists, we also\nknowthatitisincrediblyexactingandtime-consuming.Inordertolightentheworkload,\nwe provide programs for most of the problems in the text ,bothattheendofeachchapterand\nonlineat:",3939
09-1.6 The Easy Way Python Distributions.pdf,09-1.6 The Easy Way Python Distributions,"61 Introduction\nA complete list is given in the Appendix. We recommend that these codes be used as\nguidesforthereaderwhenwritingtheirownprograms,or,attheleast,testedandextended\ntosolvetheproblemathand.Wehavebeentoldthatlearninghowtousesomeoneelse‚Äôs\ncodeisavaluableworkplaceskilltodevelop;aswithprogramsencounteredinaworkplace,\ntheyshouldbeunderstoodbeforeuse!\n1.5 Our Language: The Python Ecosystem\nThecodesinthiseditionof Computational Physics employthecomputerlanguage Python.\nPreviouseditionshaveemployedJava,Fortran,andC,andusedpost-computationtoolsfor\nvisualization.1Python‚Äôscombinationoflanguagepluspackagesnowmakesitthestandard\nfortheexplorativeandinteractivecomputingthattypifiespresent-dayscientificresearch.\nAlthoughvaluableforresearch,wehavealsofoundPythontobethebestlanguageyet\nforteachingandlearningCP.Itisfree,robust(programsdon‚Äôtcrash),portable(programs\nrun without modifications on various devices), universal (available for most every com-\nputersystem),hasacleansyntaxthatpermitsrapidlearning,hasdynamictyping(changes\ndata types automatically as needed), has high-level, built-in data types (such as complex\nnumbers),andbuilt-invisualization.Furthermore,becausePythonisinterpreted,students\ncanlearnthelanguagebyexecutingandanalyzingindividualstatementswithinaninter-\nactiveshell,orwithinanotebookenvironment,orbyrunninganentireprograminonefell\nswoop.Finally,itiseasytousethemyriadoffreePythonpackagessupportingnumerical\nalgorithms,state-of-the-artvisualizations,aswellasspecializedtoolkitsthatrivalthosein\nMatlabandMathematica/Maple.Anddidwemention,allofthisisfree?\nAlthoughwedonotexpectthereaderstobeprogrammingexperts,itisessentialtobeable\ntorunandmodifythesamplecodesinthisbook.ForlearningPython,werecommendthe\nonlinetutorials[PyTut,2023;Pguide,2023;Plearn,2023],thebook[Langtangen,2016],and\nthemanybooksinthe‚ÄúPythonforScientistsandEngineers‚Äùgenre.Forgeneralnumerical\nmethods,[Press et al.,2007]isthestandard,andfuntoread.TheNITSDigitalLibraryof\nMathematicalFunctions[NIST,2022]isaconvenientreferenceformathematicalfunctions\nandnumericalmethods.\nPythonhasdevelopedrapidlysinceitsfirstimplementationinDecember1989[History,\n2022].TherapiddevelopmentsofPythonhaveledtoasuccessionofnewversionsandthe\ninevitableincompatibilities.Thecodespresentedinthebookareinthepresentstandard,\nPython3.ThemajordifferencefromPython2istheprintstatement:\n1>>> print ‚ÄôHello, World!‚Äô #P y t h o n2\n>>> print(‚ÄôHello, World!‚Äô ) #P y t h o n3\n1.6 The Easy Way: Python Distributions\nThePythonlanguageplusitsfamilyofpackagescompriseaveritableecosystemforcom-\nputing. A package, or library, or module, is a collection of related methods, or classes of\n1 Allofourcodes,eventheoldones,areavailableonline.\n1.6 The Easy Way: Python Distributions 7\nmethods,thatareassembledanddesignedtoworktogether.Inclusionoftheappropriate\npackagesextendsthelanguagetomeetthespecializedneedsofvariousscienceandengi-\nneering disciplines [CiSE, 2015]. The Python Package Index [PyPi, 2023], a repository of\nfreePythonpackages,currentlycontains425,320projectsand7,313,641files.Inthisbook,\nweuse:\nJupyter Notebooks: Aweb-based,interactivePythoncomputingenvironmentcombining\nlive code, type-set equations, narrative text, visualizations, and whatever. Some of our\nprograms( .ipynbsuffix)weredevelopedinJupyter,andourprogramsusingVpython\nwork only within Jupyter. There is a previous edition of this text in notebook form at\nsites.science.oregonstate.edu/~landaur/Books/CPbook/eBook.\nTheinteractivePythonshell, IPythoncanalsobeusedwithinJupyter.\nNumpy (Numerical Python): A comprehensive library of mathematical functions,\nrandom number generators, linear algebra routines, Fourier transforms, and most\neverything else. Permits the use of fast, high-level multidimensional arrays (explained\ninChapter7).Thesuccessortoboth NumericandNumArray,NumPyisusedbyVisual\nandMatplotlib.\nMatplotlib (Mathematics Plotting Library): A 2D and 3D graphics library that uses\nNumPy,producespublication-qualityfiguresinavarietyofhardcopyformats,andthat\npermitsinteractivegraphics.SimilartoMatlab‚Äôsplotting(exceptMatplotlibisfreeand\ndoesn‚Äôtneeditslicenserenewedyearly).\nPandas (Python Data Analysis Library): A collection of high-performance, user-\nfriendlydatastructures,anddataanalysistools(usedinChapter11).\nSymPy (Symbolic Python): A system for symbolic mathematics using pure Python\n(no external libraries) that provides a simple computer algebra system including\ncalculus, differential equations, etc. Similar to Maple or Mathematica, with the Sage\npackagebeingevenmorecomplete.ExamplesinSection2.3.6.\nVisual (Vpython): The Python language plus the no-longer-supported Visualgraph-\nics module (superseded by GlowScript). Particularly easy for creating educational\n3D demonstrations and animations. Still useful as Web Vpython and within Jupyter\nNotebooks.\nAlthough most Python packages are free, there is true value for both users and ven-\ndors to distribute a collection of packages that have been engineered and tuned to work\nwelltogether,andthatcanbeinstalledinonefellswoop.(ThisissimilartowhatRedHat\nandDebiandistributionsdoforLinux.)Thesedistributionscanbethoughtofascomplete,\nPythonecosystemsandarehighlyrecommended.Inparticular,allyoureallyneedtodoto\ngetstartedwithPythoncomputingforthisbookistoload:\nAnaConda: A free Python distribution including more than 8000 packages for science,\nmathematics,engineering,machinelearning,anddataanalysis.Anacondainstallsinits\nowndirectoryandsorunsindependentlyfromotherPythoninstallationsonyourcom-\nputer.Goto https://www.anaconda.com/products/distributiontodownloadAnaconda.\nOnceyouinstall Anaconda,theNavigatorshouldopen,anditwillletyouchooseallthat\nyouwillneed.\nSpyder IDE: TheScientificPYthonDevelopmentEnviRonment.AnIntegratedDevelop-\nmentEnvironment(IDE)withadvancedediting,interactivetestingofcode,debugging,\nandmore.\n81 Introduction\nJupyter Notebook: TheWeb-basedinteractivecomputingnotebookenvironmentusedfor\neditingandrunningtype-set-likedocuments,whilealsorunningPythoncodewithinthe\ndocuments.Aswehavealreadysaid,anotebook( .ipyn)versionofanearliereditionof\nthistextisat sites.science.oregonstate.edu/~landaur/Books/CPbook/eBook.\nPowershell Prompt: Apowerfulterminalthatruns condacommandsundertheWindows\nshellenvironments cmd.exe(CommandPrompt)and powershell.exe .Applehasa Ter-\nminalappwhereyouwillfindacommandprompt.\nConda:ApackagemanagementandenvironmentsystemincludedinAnacondathatfinds,\ninstalls,andupdatespackagesandtheirdependenciesforyou.\nInChapter11wedescribehowtoloadandrunGoogle‚Äôs TensorFlow packageformachine\nlearning, and in Chapter 12 we describe how to load and run the Quantum Computing\npackages, Cirq, IBM Quantum ,and Qiskit.",6745
10-Chapter 2 Software Basics.pdf,10-Chapter 2 Software Basics,,0
11-2.1 Making Computers Obey.pdf,11-2.1 Making Computers Obey,"9\n2\nSoftware Basics\nThis chapter discusses the computing basics of communications, number representations,\nPython programming, and visualizations. Since we want to do science, there is a particular\nemphasis on the limits of Ô¨Çoating point arithmetic .\n2.1 Making Computers Obey\nThebestprogramsarewrittensothatcomputingmachinescanperformthemquickly\nandsothathumanbeingscanunderstandthemclearly.Aprogrammerisideallyan\nessayistwhoworkswithtraditionalaestheticandliteraryformsaswellasmathemati-\ncalconcepts,tocommunicatethewaythatanalgorithmworksandtoconvinceareader\nthattheresultswillbecorrect.\n‚ÄîDonaldE.Knuth\nAsanthropomorphicasyourviewofyourcomputermaybe,keepinmindthatcomputers\nalwaysdoexactlyastheyaretold.Thismeansthatyoumusttellthemexactlyeverything\nyouwantthemtodo.Ofcourse,theprogramsyourunmayhavetobeatsuchahighlevel\nwithsuchconvolutedlogicthatyoumaynothavetheendurancetofigureoutthedetailsof\njustwhattheyaretellingthecomputertodo,butitisalwayspossibleinprinciple(except\nmaybenotwithAI).Soyourfirst problemistoobtainenoughunderstandingsothatyou\nfeelsufficientlyincontrol,nomatterhowillusionary,tofigureoutwhatthecomputeris\ndoing.\nBeforeyoutellthecomputertoobeyyourorders,youneedtounderstandthatlifeisnot\nsimpleforcomputers.Theinstructionstheyunderstandareina basicmachinelanguage1\nthattellsthehardwaretodothingslikemoveanumberstoredinonememorylocationto\nanotherlocation,ortodosomesimplebinaryarithmetic.Veryfewcomputationalscientists\ntalk to computers in a language computers can understand. When writing and running\nprograms, we usually communicate through shells,i nhigh-levellanguages (Python, Java,\nFortran, and C), or through problem-solving environments (Maple, Mathematica, and\nMatlab). Eventually, our commands or programs are translated into the basic machine\nlanguagethatthehardwareunderstands.\n1 TheBeginner‚ÄôsAll-PurposeSymbolicInstructionCode(BASIC)programminglanguageoftheoriginal\nPCsshouldnotbeconfusedwithbasicmachinelanguage.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n102 Software Basics\nAshellisacommand-lineinterpreter ,thatis,asetofsmallprogramsrunbyacomputer\nthatrespondstothecommands(thenamesoftheprograms)thatyoukeyin.Usuallyyou\nopen a special window to access the shell, and this window is called a shell as well. It is\nhelpfultothinkoftheseshellsastheouterlayersofthecomputer‚Äôsoperatingsystem(OS)in\nFigure2.1,withinwhichliesa kernelofelementaryoperations.(Theuserseldominteracts\ndirectlywiththekernel,exceptpossiblywheninstallingprogramsorwhenbuildinganOS\nfromscratch.)Itisthejoboftheshelltorunprograms,compilers,andutilitiesthatdothings\nlikemovingandcopyingfiles.Therecanbedifferenttypesofshellsonasinglecomputer,\normultiplecopiesofthesameshellrunningatthesametime.\nOperatingsystemshavenamessuchas Unix,Linux,DOS,MacOS ,andMSWindows .An\noperatingsystem isnomorethanagroupofprogramsusedbythecomputertocommunicate\nwithusersanddevices,tostoreandreaddata,andtoexecuteprograms.TheOStellsthe\ncomputerwhattodoinanelementaryway.TheOSviewsyou,otherdevices,andprograms\nasinputdataforittoprocess;inmanyways,itistheindispensableofficemanager.Whileall\nthismayseemcomplicated,thepurposeoftheOSistoletthecomputerdothenitty-gritty\nworksothatyoucanthinkhigher-levelthoughtsandcommunicatewiththecomputerin\nsomethingclosertoyournormaleverydaylanguage.\nWhen you submit a program to your computer in a high-level language , the computer\nmayuseacompilertoprocessit.A compilerisanotherprogramthattreatsyourprogram\nas a foreign language and uses a built-in dictionary and set of rules to translate it into\nbasicmachinelanguage.Asyoucanprobablyimagine,thefinalsetofinstructionsisquite\ndetailed and long, and the compiler may make several passes through your program to\ndecipheryourlogicandtranslateitintoafastcode.Thetranslatedstatementsforman object\norcompiledcode,andwhen linkedtogetherwithotherneededsubprograms,forma load\nmodulethatcanbe loadedintothecomputer‚Äôsmemoryandread,understood,andfollowed\nbythecomputer.\nLanguages such as FortranandCuse compilers to read your entire program and then\ntranslateitintobasicmachineinstructions.InterpretedlanguagessuchasPython, BASIC,\nandMapletranslateor interpreteachlineofyourprogramasitisentered,andthuspermit\nGUI\nShell\nUtilities\nKernel\nWindowsHardware\nAppsProgram developmentFigure 2.1 A schematic view of a\ncomputer‚Äôs kernel and shells. The hardware\nis in the center surrounded by increasingly\nhigher-level software.",4549
12-2.2.1 IEEE FloatingPoint Numbers.pdf,12-2.2.1 IEEE FloatingPoint Numbers,"2.2 Computer Number Representations 11\nline-by-lineinteractions.Compiledlanguagesusuallyleadtomoreefficientprogramsand\npermittheuseofvastsubprogramlibraries.Interpretedlanguagesgiveamoreimmediate\nresponsetotheuserandtherebyappear‚Äúfriendlier.‚ÄùThePythonandJavalanguagesare\nactuallyamixofthetwo.Whenyoufirstcompileyourprogram,Pythoninterpretsitinto\nanintermediate,universal bytecode,whichgetsstoredasa .pycor.pyofile.Thisfilecanbe\ntransportedtoandusedonothercomputers,althoughnotwithdifferentversionsofPython.\nThen,whenyourunyourprogram,Pythonrecompilesthebytecodeintoamachine-specific\nandfast-runningcompiledcode.\n2.2 Computer Number Representations\nComputersmaybepowerful,buttheyarefinite.Aproblemincomputerdesignishowto\nrepresentanarbitrarynumberusingafiniteamountofmemoryspace,andthenhowtodeal\nwiththelimitationsarisingfromthisrepresentation.Asaconsequenceofcomputermem-\noriesbeingbasedonthemagneticorelectronicrealizationsofaspinpointingupordown,\nthemostelementaryunitsofcomputermemoryarethetwobinaryintegers( bits)0and1.\nThismeansthatallnumbersarestoredinmemoryin binaryform,thatis,aslongstringsof\nzerosandones.Accordingly, Nbitscanstoreintegersintherange [0,2N],yetbecausethe\nsignoftheintegerisrepresentedbythefirstbit(azerobitforpositivenumbers),theactual\nrangeforN-bitintegersdecreasesto [0,2N‚àí1].\nLongstringsofzerosandonesarefineforcomputers,butareawkwardforhumans.For\nthisreason,binarystringsareconvertedto octal,decimal,orhexadecimal numbersbefore\ntheresultsarecommunicatedtopeople.Octalandhexadecimalnumbersarenicebecause\ntheconversionmaintainsprecision,butnotallthatnicebecauseourdecimalrulesofarith-\nmetic do not work for them. Converting to decimal numbers makes the numbers easier\nforustoworkwith,butunlesstheoriginalnumberisapowerof2,someprecisionislost.\nAdescriptionofaparticularcomputer‚Äôssystemorlanguagenormallystatesthe wordlength ,\nthatis,thenumberofbitsusedtostoreanumber.Thelengthisoftenexpressedin bytes,\n(amouthfulofbits)where\n1byte ‚â°1Bdef=8bits.\nMemoryandstoragesizesaremeasuredinbytes,kilobytes,megabytes,gigabytes,terabytes,\nandmegabytes(1015).Somecareshouldbetakenherebythosewhochosetocomputesizes\nindetailbecauseKdoesnotalwaysmean1000:\n1Kdef=1KB=210bytes=1024bytes . (2.1)\nThis is often (and confusingly) compensated for when memory size is stated in K, for\nexample,\n512K=29bytes=524,288bytes √ó1K\n1024bytes.\nConveniently,1byteisalsotheamountofmemoryneededtostoreasingleletterlike‚Äúa,‚Äù\nwhichaddsuptoatypicalprintedpagerequiring ‚àº3kB.\nThememorychipsinsomeolderpersonalcomputersused8-bitwords,withmodernPCs\nusing64bits.Thismeantthatthemaximumintegerwasarathersmall27=128(7because\n122 Software Basics\n1bitisusedforthesign).Using64bitspermitsintegersintherange1‚Äì263‚âÉ1019.While\nat first this may seem like a large range, it really is not when compared to the range of\nsizesencounteredinthephysicalworld.Asacaseinpoint,thesizeoftheuniversecom-\nparedtothesizeofaprotoncoversascaleof1041.Tryingtostoreanumberlargerthanthe\nhardwareorsoftwarewasdesignedfor( overflow)wascommononoldermachines,butis\nlesssonow.Anoverflowissometimesaccompaniedbyaninformativeerrormessage,and\nsometimesnot.\n2.2.1 IEEE Floating-Point Numbers\nRealnumbersarerepresentedoncomputersineither fixed-point orfloating-point notation.\nFixed-point notation can be used for numbers with a fixed number of places beyond the\ndecimalpoint(radix)orforintegers.Ithastheadvantagesofbeingabletouse two‚Äôscomple-\nmentarithmeticandbeingabletostoreintegersexactly.2Inthefixed-pointrepresentation\nwithNbitsandwithatwo‚Äôscomplementformat,anumberisrepresentedas\nNfix=sign√ó(ùõºn2n+ùõºn‚àí12n‚àí1+¬∑¬∑¬∑+ùõº020+¬∑¬∑¬∑+ùõº‚àím2‚àím), (2.2)\nwheren+m=N‚àí2.Thatis,1bitisusedtostorethesign,withtheremaining( N‚àí1)bits\nusedtostorethe ùõºivalues(thepowersof2areunderstood).Theparticularvaluesfor N,m,\nandnaremachine-dependent.Integersaretypically4bytes(32bits)inlengthandinthe\nrange\n‚àí2147483648 ‚â§4-Binteger ‚â§2147483647 . (2.3)\nAnadvantageoftherepresentation(2.2)isthatyoucancountonallfixed-pointnumbers\nhaving the same absolute error of 2‚àím‚àí1(the term left off the right-hand end of (2.2)).\nThe corresponding disadvantage is that smallnumbers (those for which the first string\nofùõºvaluesarezeros)havelarge relativeerrors.Forthatreason,relativeerrorsinthereal\nworldtendtobemoreimportantthanabsoluteones;integersareusuallyusedforcounting\npurposesandinspecialapplications(likebanking).\nMost scientific computations use double-precision floating-point numbers with\n64b=8B.Thefloating-pointrepresentation ofnumbersoncomputersisabinaryversion\nofwhat is commonlyknownas scientificorengineeringnotation . For example,the speed\nof lightc=+2.99792458 √ó10+8m/s in scientific notation and +0.299792458 √ó10+9or\n0.299795498E09m/sinengineeringnotation.Ineachofthesecases,thenumberinfront\niscalledthe mantissaandcontainsnine significantfigures .Thepowertowhich10israised\niscalledthe exponent,withtheplussigninfrontasareminderthatthesenumbersmaybe\nnegative.\nFloating-point numbers are stored on the computer as a concatenation (juxtaposition)\nofasignbit,anexponent,andamantissa.Becauseonlyafinitenumberofbitsarestored,\nthe set of floating-point numbers that the computer can store exactly, machine numbers\n(thehashmarksinFigure2.2),ismuchsmallerthanthesetofrealnumbers.Inparticular,\nmachine numbers have a maximum and a minimum (the shading in Figure 2.2). If you\n2T h etwo‚Äôscomplement ofabinarynumberisthevalueobtainedbysubtractingthenumberfrom2Nforan\nN-bitrepresentation.Becausethissystemrepresentsnegativenumbersbythetwo‚Äôscomplementofthe\nabsolutevalueofthenumber,additionsandsubtractionscanbemadewithouttheneedtoworkwiththe\nsignofthenumber.\n2.2 Computer Number Representations 13\nFigure 2.2 The limits of single-precision\nÔ¨Çoating-point numbers and the consequences\nof exceeding these limits (not to scale).\nThe hash marks represent the values of\nnumbers that can be stored; storing a number\nin between these values leads to truncation\nerrors. The shaded areas correspond to over-\nand underÔ¨Çow.UnderflowTruncation\nOverflow Overflow\n0\n‚Äì10+38‚Äì10+38‚Äì10‚Äì4510‚Äì45\nexceedthemaximum,anerrorconditionknownas overflowoccurs;ifyoufallbelowthe\nminimum,anerrorconditionknownas underflowoccurs.Inthelattercase,thesoftware\nandhardwaremaybesetupsothatunderflowsaresettozerowithoutyourevenbeingtold.\nIncontrast,overflowsusuallyhaltaprogram‚Äôsexecution.\nTheactualrelationbetweenwhatisstoredinmemoryandthevalueofafloating-point\nnumberissomewhatindirect,withtherebeinganumberofspecialcasesandrelationsused\novertheyears.Infact,inthepast,eachcomputerOSandeachcomputerlanguagecontained\ntheirownstandardsforfloating-pointnumbers.Differentstandardsmeantthatthesame\nprogram running correctly on different computers could give different results. Although\ntheresultsusuallywereonlyslightlydifferent,theusercouldneverbesureifthelackof\nreproducibilityofatestcasewasasaresultoftheparticularcomputerbeingusedortoan\nerrorintheprogram‚Äôsimplementation.\nIn1987,theInstituteofElectricalandElectronicsEngineers(IEEE)andtheAmerican\nNationalStandardsInstitute(ANSI)adoptedtheIEEE754standardforfloating-pointarith-\nmetic.Whenthestandardisfollowed,youcanexpecttheprimitivedatatypestohavethe\nprecisionandrangesgiveninTable2.1.Inaddition,whencomputersandsoftwareadhere\nto this standard, and most do now, you are guaranteed that your program will produce\nidenticalresultsondifferentcomputers.Nevertheless,becausetheIEEEstandardmaynot\nproducethemostefficientcodeorthehighestaccuracyforaparticularcomputer,some-\ntimesyoumayhavetoinvokecompileroptionstodemandthattheIEEEstandardbestrictly\nTable 2.1 The IEEE 754 standard for primitive data types.\nName Type Bits Bytes Range\nboolean Logical 11\n8trueorfalse\nchar String 16 2 ÃÅ‚ßµu0000ÃÅ‚ÜîÃÅ‚ßµuFFFFÃÅ(ISO Unicode)\nbyte Integer 8 1 ‚àí128‚Üî+127\nshort Integer 16 2 -32,768‚Üî+32,767\nint Integer 32 4 -2,147,483,648‚Üî+2,147,483,647\nlong Integer 64 8 -9,223,372,036,854,775,808‚Üî9,223,372,036,\n854,775,807\nfloat Floating 32 4 ¬±1.401298 √ó10-45‚Üî¬±3.402923 √ó10+38\ndouble Floating 64 8 ¬±4.94065645841246544 √ó10-324‚Üî\n¬±1.7976931348623157 √ó10+308\n142 Software Basics\nfollowedforyourtestcases.Afteryouknowthatthecodeisokay,youmaywanttorunwith\nwhatevergivesthegreatestspeedandprecision.\nThereareactuallyanumberofcomponentsintheIEEEstandard,anddifferentcomputer\norchipmanufacturersmayadheretoonlysomeofthem.Furthermore,asPythondevelops,\nit may not follow all standards, but it probably will in time. Normally, a floating-point\nnumberxisstoredas\nxfloat=( ‚àí1)s√ó1.f√ó2e‚àíbias, (2.4)\nthatis,withseparateentitiesforthesign s,thefractionalpartofthemantissa f,andthe\nexponentialfield e.Allpartsarestoredinbinaryformandoccupyadjacentsegmentsofa\nsingle32-bitwordforsingles,ortwoadjacent32-bitwordsfordoubles.Thesign sisstored\nasasinglebit,with s=0or1forapositiveoranegativesign.Eightbitsareusedtostorethe\nexponente,whichmeansthat ecanbeintherange0 ‚â§e‚â§255.Theendpoints, e=0and\ne=255,arespecialcases(Table2.2). Normalnumbers have0<e<255,andwiththem,the\nconventionistoassumethatthemantissa‚Äôsfirstbitisa1,soonlythefractionalpart fafter\nthebinarypoint isstored.Therepresentationsfor subnormalnumbers andforthespecial\ncasesaregiveninTable2.2.\nNote that the values ¬±INFand NaNare not numbers in the mathematical sense, that is,\nobjects that can be manipulated or used in calculations to take limits and such. Rather,\nthey are signals to the computer and to you that something has gone awry and that the\ncalculationshouldprobablystopuntilyoustraightenthingsout.Incontrast,thevalue ‚àí0\ncanbeusedinacalculationwithnoharm.Somelanguagesmaysetunassignedvariables\nto‚àí0asahintthattheyhaveyettobeassigned,althoughitisbestnottocountonthat!\nAstheuncertainty(error)isonlyinthemantissaandnottheexponent,theIEEErepre-\nsentationsensurethatallnormalfloating-pointnumbershavethesamerelativeprecision.\nBecausethefirstbitofafloatingpointnumberisassumedtobe1,itdoesnothavetobe\nstored,andcomputerdesignersneedonlyrecallthatthereisa phantombit theretoobtain\nan extra bit of precision. During the processing of numbers in a calculation, the first bit\nofanintermediateresultmaybecomezero,butthisischangedbeforethefinalnumberis\nstored.Torepeat,fornormalcases,theactualmantissa(1 .finbinarynotation)containsan\nimplied1precedingthebinarypoint.\nFinally,inordertoguaranteethatthestoredbiasedexponent eisalwayspositive,afixed\nnumber calledthe biasis addedto theactual exponent pbeforeitisstoredas thebiased\nTable 2.2 Representation scheme for normal and abnormal IEEE singles.\nNumber name Values of s,e,a n df Value of single\nNormal 0 <e<255 (‚àí1)s√ó2e‚àí127√ó1.f\nSubnormal e=0,f‚â†0 (‚àí1)s√ó2‚àí126√ó0.f\nSignedzero( ¬±0)e=0,f=0 (‚àí1)s√ó0.0\n+‚àû s=0,e=255,f=0 +INF\n‚àí‚àû s=1,e=255,f=0 -INF\nNotanumber s=u,e=255,f‚â†0 NaN",10705
13-2.2.1.1 Examples of IEEE Representations.pdf,13-2.2.1.1 Examples of IEEE Representations,"2.2 Computer Number Representations 15\nexponente.Theactualexponent,whichmaybenegative,is\np=e‚àíbias. (2.5)\n2.2.1.1 Examples of IEEE Representations\nThere are two basic IEEE floating-point formats, singles and doubles. Singlesorfloatsis\nshorthandfor single-precisionfloating-pointnumbers ,anddoublesisshorthandfor double-\nprecisionfloating-pointnumbers .(InPython,however,floatsaredoubleprecision.)Singles\noccupy32bitsoverall,with1bitforthesign,8bitsfortheexponent,and23bitsforthefrac-\ntionalmantissa(whichgives24-bitprecisionwhenthephantombitisincluded).Doubles\noccupy64bitsoverall,with1bitforthesign,10bitsfortheexponent,and53bitsforthe\nfractionalmantissa(for54-bitprecision).Thismeansthattheexponentsandmantissasfor\ndoublesarenotsimplydoublethoseoffloats,asweseeinTable2.1.(Inaddition,theIEEE\nstandardalsopermits extendedprecision thatgoesbeyonddoubles,butthisisallcompli-\ncatedenoughwithoutgoingintothatrightnow.)\nToseetheschemeinpractice,considerthe32-bitrepresentation(2.4):\nse f\nBitposition 31 30 23 22 0\nThesignbit sisinbitposition31,thebiasedexponent eisinbits30‚Äì23,andthefractional\npartofthemantissa fisinbits22‚Äì0.Because8bitsareusedtostoretheexponent eand\nbecause28=256,ehastherange\n0‚â§e‚â§255. (2.6)\nThevalues e=0and255arespecialcases.Withbias =12710,thefullexponent\np=e10‚àí127, (2.7)\nand,asindicatedinTable2.1,singleshavetherange\n‚àí126‚â§p‚â§127. (2.8)\nThemantissa fforsinglesisstoredasthe23bitsinpositions22‚Äì0.For normalnumbers ,\nthatis,numberswith0 <e<255,fisthefractionalpartofthemantissa,andthereforethe\nactualnumberrepresentedbythe32bitsis\nNormalfloating-pointnumber =( ‚àí1)s√ó1.f√ó2e‚àí127. (2.9)\nSubnormalnumbers havee=0,f‚â†0.Forthese, fistheentiremantissa,sotheactualnum-\nberrepresentedbythese32bitis\nSubnormalnumbers =( ‚àí1)s√ó0.f√ó2e‚àí126. (2.10)\nThe23bits m22‚àím0,whichareusedtostorethemantissaofnormalsingles,correspond\ntotherepresentation\nMantissa =1.f=1+m22√ó2‚àí1+m21√ó2‚àí2+¬∑¬∑¬∑+m0√ó2‚àí23, (2.11)\nwith0.fusedforsubnormalnumbers.Thespecial e=0representationsusedtostore ¬±0\nand¬±‚àûaregiveninTable2.2.\n162 Software Basics\nToseehowthisworksinpractice(Figure2.2),thelargestpositivenormalfloating-point\nnumberpossiblefora32-bitmachinehasthemaximumvalue e=254(thevalue255being\nreserved)andthemaximumvaluefor f:\nXmax=01111111011111111111111111111111\n=(0)(11111110 )(11111111111111111111111 ), (2.12)\nwherewehavegroupedthebitsforclarity.Afterputtingallthepiecestogether,weobtain\nthevalueshowninTable2.1:\ns=0,e=11111110 =254,p=e‚àí127=127,\nf=1.11111111111111111111111 =1+0.5+0.25+¬∑¬∑¬∑‚âÉ2,\n‚áí(‚àí1)s√ó1.f√ó2p=e‚àí127‚âÉ2√ó2127‚âÉ3.4√ó1038. (2.13)\nLikewise,thesmallestpositivefloating-pointnumberpossibleissubnormal( e=0)witha\nsinglesignificantbitinthemantissa:\n00000000000000000000000000000001 . (2.14)\nThiscorrespondsto\ns=0,e=0,p=e‚àí126=‚àí126\nf=0.00000000000000000000001 =2‚àí23\n‚áí(‚àí1)s√ó0.f√ó2p=e‚àí126=2‚àí149‚âÉ1.4√ó10‚àí45. (2.15)\nInsummary,single-precision(32-bitor4-byte)numbershavesixorsevendecimalplaces\nofsignificanceandmagnitudesintherange\n1.4√ó10‚àí45‚â§singleprecision ‚â§3.4√ó1038. (2.16)\nDoublesarestoredastwo32-bitwords,foratotalof64bits(8B).Thesignoccupies1bit,\ntheexponent e,11bits,andthefractionalmantissa,52bits:\ns e f f(cont.)\nBitposition 6362 52 51 32 31 0\nAsweseehere,thefieldsarestoredcontiguously,withpartofthemantissa fstoredin\nseparate32-bitwords.Theorderofthesewords,andwhetherthesecondwordwith fisthe\nmostorleastsignificantpartofthemantissa,ismachine-dependent.Fordoubles,thebias\nisquiteabitlargerthanforsingles,\nBias=11111111112=102310, (2.17)\nsotheactualexponent p=e‚àí1023.\nThe bit patterns for doubles are given in Table 2.3, with the range and precision given\ninTable2.1.Torepeat,ifyouwriteaprogramwithdoubles,then64bits(8bytes)willbe\nusedtostoreyourfloating-pointnumbers.Doubleshaveapproximately16decimalplaces\nofprecision(1partin252)andmagnitudesintherange\n4.9√ó10‚àí324‚â§doubleprecision ‚â§1.8√ó10308. (2.18)",3851
14-2.3 Python Mini Tutorial.pdf,14-2.3 Python Mini Tutorial,"2.2 Computer Number Representations 17\nTable 2.3 Representation scheme for IEEE doubles.\nNumber name Values of s,e,a n df Value of double\nNormal 0 <e<2047 (‚àí1)s√ó2e‚àí1023√ó1.f\nSubnormal e=0,f‚â†0 (‚àí1)s√ó2‚àí1022√ó0.f\nSignedzero e=0,f=0 (‚àí1)s√ó0.0\n+‚àû s=0,e=2047,f=0 +INF\n‚àí‚àû s=1,e=2047,f=0 -INF\nNotanumber s=u,e=2047,f‚â†0 NaN\nIfasingle-precisionnumber xislargerthan2128,afaultconditionknownasan overflow\noccurs(Figure2.2).If xissmallerthan2‚àí128,anunderflowoccurs.Foroverflows,theresult-\ningnumber xcmayendupbeingamachine-dependentpattern,notanumber(NAN),or\nunpredictable.Forunderflows,theresultingnumber xcisusuallysettozero,althoughthis\ncanusuallybechangedviaacompileroption.(Havingthecomputerautomaticallyconvert\nunderflowsto zero is usually a good path to follow;convertingoverflowsto zero may be\nthepathtodisaster.)Sincetheonlydifferencebetweentherepresentationsofpositiveand\nnegativenumbersonthecomputeristhesignbitofonefornegativenumbers,thesame\nconsiderationsholdfornegativenumbers.\nIn our experience, serious scientific calculations almost always require at least 64-bit\n(double-precision)floats .Andifyouneeddoubleprecisioninonepartofyourcalculation,\nyouprobablyneeditallover,whichmeansdouble-precisionlibraryroutinesformethods\nandfunctions.\n2.2.2 Python and the IEEE 754 Standard\nPython has been changing in recent years, and while in the past it did not adhere to all\naspectsoftheIEEE754standard,itdoesnowalmostcompletely.Probablythemostrelevant\ndifferencefromthestandardisthat Pythondoesnotsupportsingle(32bit)precisionfloating-\npointnumbers .Sowhenwedealwithadatatypecalleda floatinPython,itistheequiv-\nalentofadoubleintheIEEEstandard.Becausesinglesareinadequateformostscientific\ncomputing,thisisnotalossforus.Howeverbewary,ifyouswitchovertoJavaorCyou\nshoulddeclareyourvariablesas doublesandnotas floats.WhilePythoneliminatessingle-\nprecisionfloats,itaddsanewdatatype complexfordealingwithcomplexnumbers.Com-\nplexnumbersarestoredaspairsofdoublesandarequiteusefulinphysics.\nThe details of how closely Python adheres to the IEEE 754 standard depend upon the\ndetailsofPython‚ÄôsuseoftheCorJavalanguagetopowerthePythoninterpreter.Inpartic-\nular,withtherecent64-bitarchitecturesforCPUs,therangemayevenbegreaterthanthe\nIEEEstandard,andtheabnormalnumbers( ¬±INF, NaN)maydiffer.Likewise,theexactcon-\nditionsforoverflowsandunderflowsmayalsodiffer.Thatbeingthecase,theexploratory\nexercisestofollowbecomeallthatmoreinterestingbecausewecannotsaythatweknow\nwhatresultsyoushouldobtain!",2506
15-2.3.4 Python Lists as Arrays.pdf,15-2.3.4 Python Lists as Arrays,"182 Software Basics\n2.3 Python Mini Tutorial\nThereisanofficialPythontutorialat docs.python.org/3/tutorial/ andthatisagoodplaceto\ngoifyouarestartingwithPython.Inthissection,wejusthighlightsomebasicsthatshould\nhelpyoubetterunderstandourprograms.Infact,studyingtheprogramsisagoodwayto\nlearn,andwerecommendit!Inaddition,Chapter7discussescomputingwithmatricesand\nhowbesttodoitwithPython.\n2.3.1 Structure and Functions\nWefindPythontobetheeasiestprogramminglanguagetolearnandworkwith.Thisfol-\nlows, in part, from its use of whitespace and indentation to construct code structures, in\ncontrasttoJavaandC,whichusebracesandsemicolons.Forexample,herewedefineand\ncallafunction:\ndefDefunct(x,j): # Defines the function\n2i=1\nmax=1 0\nwhile(i<max):\nprint(i)\n6 i=i+1\nreturni‚àóx‚àó‚àój\nDefunct(x,3) # Calls the function\nHere defisareservedkeywordandisfollowedbythefunctionname,andtwoarguments\narebeingpassedtothefunction.Notehowthespacingsandindentationsareusedtodefine\nthestructureswithintheprogram,thatthecolon:isneededtodefinethefunctionandthe\ncontrolstructure,andthatthehash#isusedforcomments.Theblanklineisignoredby\nPythonandisthereforclarity;youshouldusemanyinordertomaketheprogram‚Äôsstruc-\nture evident. So, while there are no special characters here to separate statements (other\nthan the newlinecontrol character), Python does use the backslash ‚ßµas a continuation\ncharacterforlongstatements:\nT[ix, 1] = T[ix, 0] + cons ‚àó(T[ix+1, 0] \\n+T [i x‚àí1, 0]‚àí2.‚àóT[ix,0])\nPython contains many built-in functions. Here are some of the ones from the C math\nlibrary(insomecasesneedinga math.prefix):\nfactorial(x) expo(x) Ô¨Çoor(x) mod(x, y) log(x[, base]) log10(x)\npow(x, y) sqrt(x) MacOS(x) basin(x) atan(x) atan2(y, x)\ncos(x) sin(x) tan(x) degrees(x) radians(x) cosh(x)\nsinh(x) tanh(x) cosh(x) sinh(x) tanh(x) ref(x)\ngamma(x)\nAlsousefularethemathematicalandcomputerconstants:\nmath.pi math.e math.inf math.nan\n2.3 Python Mini Tutorial 19\n2.3.2 Variable Types and Operators\nVariables in Python are symbols to which values are assigned. You can use almost any\nnameforyourvariable,butnotanyofthebuilt-infunctionnames,reservedwords,orthese\nkeywords:\nFalse def if raise None del\nimport return True elf in try\nand else is while as except\nlambda with assert Ô¨Ånally nonlocal yield\nbreak for not class form or\ncontinue global pass\nVariablenamescancontainletters,numbers,andtheunderscore_,buttheycannotstart\nwithanumberorcontainspaces.Thevariables‚Äôvaluesareassignedwithasingleequalsign:\nLabel = ""Voltage"" # A string\n2x=2 5 # An integer\nPython supports integers ( int), floating point numbers ( Ô¨Çoats), complex numbers\n(complex),booleans( bool),andstrings( str).Integersarecreatedwhentheyareassigned\nwithoutadecimalpoint,whilefloatsarecreatedwhenassignedwithdecimalpoints.The\ndivisionoftwointegersreturnsafloat,aswellasmixedarithmeticwithfloatsandintegers:\n>>> 6/3\n22.0\n>>>3/6 # Note round off\n0.\n>>> 3./6 # Mixed types\n60.50000\nNotethat,incontrasttolanguagessuchasJavaandC,youdonotdeclarethevariabletype\nbefore using it. However, Python will change the variable type depending upon how it‚Äôs\nused:\n>>> i = 3\n2>>>print(i)\n3\n>>> i = 12. ‚àói\n>>>print(i)\n636.0000\nAcomplexnumber zusestwofloatstostoretherealandimaginaryparts:\n>>>importmath # import math"" for complex\n2>>> x = 2\n>>> y = 3\n>>> z = complex(x,y) # Assign a complex number\n>>>print(z.real, z.imag)\n62., 3.\n202 Software Basics\nOnecanalsorepresentacomplexnumberinpolarcoordinates\ncmath.phase(z) # The phase phi\n2>>> phase ( complex(‚àí1.0, 0.0))\n3.141592653589793\nabs(z) # Modulus uses usual abs function\n>>> 1.0\nThefunction cmath.polar(z) convertsacomplexnumberintothepolarrepresentation (r,ùúô),\nwhilethefunction cmath.rect(r, phi) convertsthepolarrepresentationintoaCartesianone.\nAstringisaseriesofcharactersthatistobeunderstoodliterally.Forexample:\nS= ""A string using double quotes""\nS= ""A string using single quotes""\n3S= ‚ÄôIt\‚Äôs possible to escape a quote‚Äô\n"""""" From ""Computational Physics""\nProblem Solving with Python """""" # Double within triple quotes\nSoeithersingle,double,ortriplequotesareusedtosetoffthestring.Inthelasttwolines,the\ninternaldoublequotesarekeptaspartofthestring,andtriplequotesareusedtocontinue\nthestringtoanadditionalline.Youcanaccessindividualelementsofastringviaanindex\nbeginningat0:\nS= """"Problem Solving With Python """"\nprint(S[0]) #P\n3print(S[1]) #r\nprint(S[‚àí1])#n\nSlicingisatechniquetoextractasubstringfromwithinastring:\nS= """"Problem Solving With Python """"\nprint(S[0:3]) #P r o\nMathOperators:\n+addition, ‚àísubtraction, * multiplication, / division,\n% modulus/remainder, ** exponentiation\nComparisonOperators:\n==Equals, ! =Notequals, >Greaterthan, <Lessthan, >=\nGreaterthanorequalto, <=Lessthanorequalto.\nCommonAssignmentOperators:\n=+ =addandassign, ‚àí=subtractandassign, * =multiplyandassign,\n/=divideandassign\n2.3.3 Boolean and Control Structures\nBooleanvariablescanhavethevalues TrueorFalse:\n2.3 Python Mini Tutorial 21\n>>> 1<2\n2False\n>>> 2 > 1\nTrue\n>>>bool(2 > 1)\n6True\nTheifstatementexecutesablockiftheconditionismet(notecolon:andindent):\nifcondition:\n2if‚àíblock\nTheif‚Ä¶elsestatement chooses which block to execute (note colon:, semicolon;, and\nindent):\nifcondition:\n2if‚àíblock;\nelse:\nelse‚àíblock;\nFinally,the if‚Ä¶elif‚Ä¶elsestatementpermitsthechoicefromamongseveralblocks:\nif‚àícondition:\nif‚àíblock\nelif elif ‚àícondition1:\n4elif‚àíblock1\nelif elif ‚àícondition2:\nelif‚àíblock2\n...\n8else:\nelse‚àíblock\nPythonalsosupports forloops:\n>>>forindexin range (1, 3):\nprint(index)\n31\n2\n3\nandwhileloops:\nwhilecounter <max:\nprint(counter)\n3counter += 1\n2.3.4 Python Lists as Arrays\nAlistis Python‚Äôs built-in sequence of numbers or arbitrary objects. Although called a\n‚Äúlist‚Äù it is similar to what other computer languages call an ‚Äúarray‚Äù. (In Section 7.3.2,\nwe will describe a higher-level arraydata type available with the NumPypackage.)\nPythoninterpretsasequenceofordereditems, L=l0,l1,‚Ä¶,N‚àí1,asalistandrepresentsit\nwithasinglesymbol L:\n222 Software Basics\n1 >>> L = [1 , 2 , 3] #C r e a t el i s t\n>>> L[0] # Print element 0 (first)\n3 1 # Python output\n>>> L # Print entire list\n5 [1, 2, 3] # Output\n>>> L[0]= 5 # Change element 0\n7 >>> L\n[5, 2, 3]\n9 >>>len(L) # Length of list\n3\n11 >>>foritemsinL:printitems # For loop over items\n5\n13 2\n3\nObservethatsquarebracketswithcommaseparators,[1,2,3],areusedforlists,andthata\nsquarebracketisalsousedtoindicatetheindexforalistitem,asinline2, L[0].Theitems\ninlistsare mutableorchangeable.Immutableobjectsincludeintegers,floats,strings,and\ntuples;afterastringofthemhasbeendefined,itscontentscannotbechanged.Aswesee\ninline7inthe Lcommand,anentirelistcanbereferencedasasingleobject,inthiscase,\ntoprintit.\nPythonalsohasanotherbuilt-inlistdatatypeknownasa tuplewhoseelementsarenot\nmutable (they also process faster than lists). Tuples are indicated by round parenthesis\n(..,..,.),withindividualelementsstillreferencedbysquarebrackets:\n>>> T = (1 , 2 , 3 , 4) # Create a tuple list\n2 >>> T[3] # Print element 3\n4\n4 >>> T # Print entire tuple\n(1, 2, 3, 4)\n6 >>> T[0] = 5 # Attempt to change element 0\nTraceable (most recent call last):\n8 T[0] = 5\nError: ‚Äôtuple‚Äô objectdoesnotsupport item assignment\nNotePython‚Äôserrormessagewhenwetriedtochangeanelementofatuple.\nManylanguagesrequireyoutospecifythesizeofanarraybeforeyoucanstoreobjects\ninit.Incontrast,Pythonlistsare dynamic,whichmeansthattheirsizesadjustasneeded.\nInaddition,whilealistisessentiallyone-dimensionalbecauseitisasequence,acompound\nlistcanbecreatedinPythonwiththeindividualelementsthemselvesaslists:\n1 >>> L = [[1,2], [3,4], [5,6]] # A list of lists\n>>> L\n3 [[1, 2], [3, 4], [5, 6]]\n>>> L[0] # The first element\n5 [1, 2]\nPythoncanperformalargenumberofoperationsonlists,forexample:\nOperation Effect Operation Effect\nL=[1,2,3,4] Formlist L1+L2 Concatenatelists\nL[i] ithelement len(L) LengthoflistL\niinL TrueifiinL L[i:j] Slicefromitoj",7941
16-2.4.5 Experiment Your Machines Precision.pdf,16-2.4.5 Experiment Your Machines Precision,"2.3 Python Mini Tutorial 23\nOperation Effect Operation Effect\nforiinL Iterationindex L.append(x) AppendxtoendofL\nL.count(x) Numberofx‚ÄôsinL L.index(x) Locationof1stxinL\nL.remove(x) Remove1stxinL L.reverse() ReverseelementsinL\nL.sort() OrderelementsinL\n2.3.5 Python I/O\nOutputting variables to the screen is easy, but as mentioned in Chapter 1, it differs in\nPython2and3:\n>>>print ‚ÄôHello, World!‚Äô #P y t h o n2\nHello, World\n3>>>print(‚ÄôHello, World!‚Äô )#P y t h o n3\n‚ÄôHello, World!‚Äô\nHerethe ‚â´>indicatesthatwewereworkinginaninteractiveshell.Inputtingfromthekey-\nboardisaccomplishedwiththe inputcommand:\nname =input(""Hello, What‚Äôs your name?"" )\nprint(""That‚Äôs nice "" +n a m e+ ""thank you"" )\nage =input(""How old are you?"" )\n4print(""So, you are already "" + astr(age) + "" years old, ""\n+n a m e+ ""!"")\nThis is what we have done in the program AreaFormatted.py in Listing 2.11. This\nshows that we can print the value of a variable just by giving its name. We also see in\nAreaFormatted.py thatwecaninputstrings(literalnumbersandletters)byeitherenclosing\nthestringinquotes(singleordouble),orbyusingthe raw_input(Python2)or input(Python\n3)commandwithoutquotes. AreaFormatted.py alsoshowshowtoinputbothastringand\nnumbersfromafile.\nPython uses its default format when you print a float by giving just its name, with the\nformatvaryingdependingontheprecisionofthenumber.Youcancontroltheformatifyou\nlike.Toprintfloatsyouneedtospecifyhowmanydigits(places)afterthedecimalpointare\ndesired,andhowmanyspacesoverallshouldbeusedforthenumber:\nprint(""x=%6.3f, Pi=%9.6f, Age=%d \n"" )%( x ,m a t h . p i ,a g e )\nprint(""x=%6.3f, %(x), "" Pi=%9.6f, "" %(math.pi), "" Age=%d ""%(age),"" \n)\nx = 12.345, Pi = 3.141593, Age=39 # Output from either\nHere the %6.3fformats a float (which is a double in Python) to be printed in fixed-point\nnotation(the f)withthreeplacesafterthedecimalpointandwithsixplacesoverall(one\nplaceforthedecimalpoint,oneforthesign,oneforthedigitbeforethedecimalpoint,and\nthreeforthedecimal).Thedirective %9.6fhassixdigitsafterthedecimalplaceandnine\noverall.Toprintaninteger,youneedtospecifyonlythetotalnumberofdigits(thereisno\ndecimalpart),andwedothatwiththe %d(dfordigits)format.\n242 Software Basics\nThe %symbolintheseoutputformatsindicatesaconversionfromthecomputer‚Äôsinternal\nformattothatusedforoutput.Noteabovethatwehavealsousea ‚ßµndirectivetoindicatea\nnewline.Otherdirectives,someofwhicharedemonstratedin Directives.py inListing2.12,\nare:\n‚ßµ""doublequote ‚ßµ0NNNoctalNNN ‚ßµ‚ßµbackslash\n‚ßµaalert(bell) ‚ßµbbackspace ‚ßµcnomoreoutput\n‚ßµfformfeed ‚ßµnnewline ‚ßµrcarriageret\n‚ßµthorizontaltab ‚ßµvverticaltab %%asingle%\nNoticeinListing2.11howwereadfromthekeyboard,aswellasfromafile,andthenoutput\ntobothscreenandfile.Beware,ifyoudonotcreatethefile Name.dat,theprogramwillissue\n(‚Äúthrow‚Äù)anerrormessageofthesort:\nError: [Error 2] No such file or directory: ‚ÄôName.dat‚Äô .\n2.3.6 Python‚Äôs Algebraic Tools\nWhilethisbook‚ÄôsfocusismainlyontheuseofPythonfornumericalsimulations,thatis\nnottodiscounttheimportanceofcomputationalsymbolicmanipulations.Pythonactually\nhas(atleast)twopackagesthatcanbeusedforsymbolicmanipulations,andtheyarequite\ndifferent.AsindicatedinSection1.6,the Sagepackageisverymuchinthesameclassas\nMaple and Mathematica. Sages‚Äôs notebook interface lets users create publication-quality\ntext,runprograms,ormanipulateequationssymbolically.YetSageisabigandpowerful\npackagethatgoesbeyondpurePythonbyincludingmultiplecomputeralgebrasystems,as\nwellasvisualizationtools,andmore.UsingthemultiplefeaturesofSagecangettobequite\ncomplicated,and,infact,bookshavebeenwrittenandworkshopstaughtontheuseofSage.\nWerefertheinterestedreadertotheonlineSageDocumentationpage www.sagemath.org/\nhelp.html.\nTheSymPypackage for symbolic manipulations runs within a regular Python shell,\nverymuchlikeanyotherPythonpackage.Itcanbedownloadedfrom github.com/sympy/\nsympy/releases ,oryoucanusetheCanopydistributionthatincludesSymPy.Nowwegive\nsomesimpleexamplesofSymPy‚Äôsuse,butreallyyoushouldstartwiththe SymPyTutorial ,\ndocs.sympy.org/latest/tutorial/ . To start, we‚Äôll take some derivatives to show that SymPy\nknowscalculus:\n1>>>fromSymPyimport ‚àó\n>>> x, y = symbols( ‚Äôx y‚Äô)\n>>> y = diff ( tan (x) ,x) ; y # y = derivative tan(x)\n$\tan^2(x) + 1$\n5>>> y = diff (5 ‚àóx‚àó‚àó4+7 ‚àóx‚àó‚àó2, x, 1); y # Deriv, 1 optional\n$20 x^3 + 14 x$\n>>> y = diff (5 ‚àóx‚àó‚àó4+7‚àóx‚àó‚àó2, x, 2); y #$d^2y/dx^2 $\n$2\, (30 x^2 + 7)$\nWeseethatwefirstimportmethodsfromSymPy, andthenuse the symbolscommandto\ndeclare the variables xandyas algebraic. The rest is rather obvious, with diffbeing the\nderivativeoperator,andthe xargumentindicatingthederivativewithrespectto x.Nowlet‚Äôs\ntryexpansions:\n2.4 Programming Warmup 25\n>>>fromSymPyimport ‚àó\n>>> x, y = symbols( ‚Äôx y‚Äô)\n>>> z = (x + y) ‚àó‚àó8; z\n4$(x + y)^8$\n>>> expand(z)\n$x^8 + 8 x^7 y + 28 x^6 y^2 + 56 x^5 y^3 + 70 x^4 y^4 + 56 x^3 y^5 + 28 x^2 y^6\n+ 8 x y^7 + y^8$\nSymPyalsoknowsaboutinfiniteseries,anddifferentexpansionpoints:\n>>> sin (x) . series (x , 0) #$\sin x$series about 0\n2$x‚àíx^3/6 + x^5/120 + \mathcal{O}(x^6)$\n>>> sin (x) . series (x ,10) #$\sin x$about x= 10\n$\sin(10) + x\cos(10) ‚àíx^2 \sin(10)/2 ‚àíx^3 \cos(10)/6 + x^4 \sin(10)/24 + x^5\n\cos(10)/120 +\mathcal{O}(x^6)$\n>>> z = 1/ cos (x) ; z # Division, not inverse\n6$1/\cos(x)$\n>>> z . series (x , 0) #E x p a n d $1/\cos x $about$x=0$\n$1 + x^2/2 + 5 x^4/24 + \mathcal{O}(x^6)$\nOneoftheclassicdifficultieswithcomputeralgebrasystemsisthateveniftheansweris\ncorrect,itmaynotlooksimple,andthusisnottoouseful.SymPyhasthefunctions simplify,\nfactor, collect, cancel ,and aparttohelpmakeitsoutputeasiertounderstand:\n>>> factor (x ‚àó‚àó2‚àí1)\n$(x‚àí1) (x + 1)$ # A nice answer\n>>> factor (x ‚àó‚àó3‚àíx‚àó‚àó2+x‚àí1)\n4$(x‚àí1) (x^2 + 1)$\n>>> simplify((x ‚àó‚àó3+x ‚àó‚àó2‚àíx‚àí1)/(x ‚àó‚àó2+2 ‚àóx+1 ) )\n$x‚àí1$ # Much better!\n>>> simplify(x ‚àó‚àó3+3‚àóx‚àó‚àó2‚àóy+3‚àóx‚àóy‚àó‚àó2+y‚àó‚àó3)\n8$x^3 + 3 x^2 y + 3 x y^2 + y^3$ #N oh e l p !\n>>> factor (x ‚àó‚àó3+3‚àóx‚àó‚àó2‚àóy+3‚àóx‚àóy‚àó‚àó2+y‚àó‚àó3)\n$(x + y)^3$ # Much better!\n>>> simplify(1 + tan(x) ‚àó‚àó2)\n12$\cos(x)^{( ‚àí2)}$\n>>> simplify(2 ‚àótan(x)/(1+tan(x) ‚àó‚àó2))\n$\sin(2 x)$\n2.4 Programming Warmup\nBeforewegoontoseriousCPwork,wewanttoestablishthatyourlocalcomputeriswork-\ningrightforyou.Assumethatcalculatorshavenotyetbeeninvented,andthatyouneeda\nprogramtocalculatetheareaofacircle.Youmighttry\nread radius #I n p u t\ncalculate area of circle # Numerics\nprintarea # Output\nTheinstruction calculate area of circle hasnomeaningtomostcomputers,soweneed\ntospecifyan algorithm,thatis,asetofrulesforthecomputertofollow:\n1read radius #I n p u t\nPI = 3.141593 # Set constant\narea = PI ‚àór‚àór # Algorithm\nprintarea # Output\n262 Software Basics\nThisisbetter.HereisourPythonprogram Area.py,andyoushouldensurethatitrunsfor\nyou.Thisisasimpleprogramthatoutputstothescreen,withitsinputbuiltintothepro-\ngram.\n# Area.py: Area of a circle , simple program\nfrommathimportpi\nN=1\n4r=1 .\nC=2 . ‚àópi‚àór\nA=p i ‚àór‚àó‚àó2\nprint(‚ÄôProgram number =‚Äô ,N , ‚Äô \ nr ,C ,A=‚Äô ,r ,C ,A )\n2.4.1 Program Design\nProgrammingisawrittenartthatblendselementsofscience,mathematics,andcomputer\nscienceintoasetofinstructionsthatpermitacomputertoaccomplishadesiredtask.And\nnow, with published scientific results increasingly relying on computation, it is increas-\ninglyimportantthatthesourceversionofyourprogramitselfbeavailabletootherssothat\ntheycanreproduceyourresults.Reproducibilitymaynotbeasexcitingasanewdiscovery,\nbut it is an essential ingredient in science [Hinsen, 2013]. In addition to the grammar of\nacomputerlanguage,ascientificprogramshouldincludeanumberofessentialelements\ntoensuretheprogram‚Äôsvalidityandusability.Aswithotherarts,wesuggestthatuntilyou\nknowbetter,youfollowsomesimplerules.Agoodprogramshould:\n‚óèGivethecorrectanswers.\n‚óèBeclearandeasytoread,withtheactionofeachparteasytoanalyze.\n‚óèDocumentitselfforthesakeofreadersandtheprogrammer.\n‚óèBeeasytouse.\n‚óèBebuiltupoutofsmallprogramsthatcanbeindependentlyverified.\n‚óèBeeasytomodifyandrobustenoughtokeepgivingcorrectanswersaftermodification\nanddebugging.\n‚óèDocumentthedataformatsused.\n‚óèUsetrustedlibraries.\n‚óèBepublishedorpassedontootherstouseandtodevelopfurther.\nOneattractionof object-orientedprogramming isthatitenforcestheserulesautomatically.\nAnelementarywaytomakeanyprogramcleareristo structureitwithindentation,skipped\nlines,andstrategicallyplacedbraces.Thisisdonetoprovidevisualcluesastothefunctionof\nthedifferentprogramparts(the‚Äústructures‚Äùinstructuredprogramming).Pythonactually\nusesindentationsasstructureelements.Althoughthespacelimitationsofaprintedpage\nkeepusfrominsertingasmanyblanklinesaswewouldprefer,werecommendthatyoudo\naswesayandnotaswedo!\nWe findflowcharts, such as the basic and the detailed ones in Figure 2.3 for projectile\nmotion,usefulinplanningthechronologicalorderfortheessentialstepsinaprogram,and\nalsoprovidingagraphicaloverviewofthecomputation.Aflowchartisnotmeanttobea\ndetaileddescriptionofaprogram,butinsteadisavisualizationofaprogram‚Äôslogicalflow.\nWerecommendthatyoudrawaflowchartor(secondbest)writeapseudocodebeforeyou\n2.4 Programming Warmup 27\nInitialize constants\nBasic calculations\nLoop over time\nEndStore g, V0, Œ∏\nCalculate R, T\nLoop over time\nCalculate x(t), y(t)\nPrint x, y ‚ÄúNot Yet Fired‚Äù\nEnd‚ÄúGrounded‚Äù0 < t < T ?\nt < 0 ?\nN YN Y\nFigure 2.3 A Ô¨Çowchart illustrating a program to compute projectile motion. On the left are the\nbasic components of the program, and on the right are some of its details. When writing a program,\nÔ¨Årst map out the basic components, then decide upon the structures, and Ô¨Ånally Ô¨Åll in the details.\nThis is called top-down programming .\nwriteaprogram. Pseudocode islikeatextversionofaflowchartthatleavesoutdetailsand\ninsteadfocusesonthelogicandstructures:\n1# A flowchart for projectile motion\nStore g, Vol, andtheta\nCalculate R andT\nBegin time loop\n5Print out ""not yet fired"" ift<0\nPrint out ""grounded"" ift>T\nCalculate , printx(t)andy(t)\nPrint out error message ifx>R ,y>H\n9End time loop End program\n2.4.2 First Programming Steps\n1) Togainsomeexperiencewithyourcomputersystem,useaneditortoentertheprogram\nArea.pythatcomputestheareaofacircle(yes,weknowyoucancopyandpasteit,but\ndon‚Äôt). Save your program to a file in your home (personal) directory. Note: For those\nwhoarefamiliarwithPython,youmaywanttoentertheprogram AreaFormatted.py in\nListing2.11thatproducesformattedoutput.\n2) Compileandexecutetheappropriateversionof Area.py.\n3) Experimentwithyourprogram.Forexample,seewhathappensifyouleaveoutdecimal\npointsintheassignmentstatementfor r,ifyouassign requaltoablank,orifyouassigna\nlettertor.Remember,itisunlikelythatyouwill‚Äúbreak‚Äùorhurtthecomputerbymaking\namistake,anditisgoodtoseehowthecomputerrespondswhensomethingiswrong.\n282 Software Basics\n4) Changetheprogramsothatitcomputesthevolume4\n3ùúãr3ofasphereandprintsitout\nwith the proper name. Save the modified program to a file in your personal directory\nandgiveitthename Vol.py.\n5) Openandexecute Vol.pyandcheckthatyourchangesarecorrectbyrunninganumber\noftrialcases.Goodinputdataare r=1andr=10.\n6) Revise Area.pysothatittakesinputfromafilenamethatyouhavemadeup,thenoutputs\ninadifferentformattoanotherfileyouhavecreated,andthenreadsfromthelatterfile.\n7) Seewhathappenswhenthedatatypeusedforoutputdoesnotmatchthetypeofdata\ninthefile(e.g.,floatingpointnumbersarereadinasintegers).\n8) Revise Area.pysothatitusesamainmethod(whichdoestheinputandoutput)anda\nseparatefunctionormethodforthecalculation.Checkthatyouobtainthesameanswers\nasbefore.\n2.4.3 Over and UnderÔ¨Çow Exercises\n1) Considerthe32-bitsingle-precisionfloating-pointnumber A:\ns e f\nBitposition 3130 23 22 0\nValue 000001110 10100000000000000000000\na) Whatarethebinaryvaluesforthesign s,theexponent e,andthefractionalmantissa\nf.(Hint:e10=14.)\nb) Determinedecimalvaluesforthebiasedexponent eandthetrueexponent p.\nc) Showthat A‚Äôsmantissaequals1.625000.\nd) Determinethefullvalueof A.\n2) Writeaprogramthatdeterminesthe underflow andoverflowlimits(withinafactor\nof2)forPythononyourcomputer.Here‚Äôsasamplepseudocode\nunder = 1.\nover = 1.\n3begin do N times\nunder = under/2.\nover = over ‚àó2.\nwrite out: loop number, under, over\n7end do\nYoumayneedtoincrease Nifyourinitialchoicedoesnotleadtounderflowandoverflow.\nIfyouwanttobemorepreciseregardingthelimitsofyourcomputer,trymultiplyingand\ndividingbyanumbersmallerthan2.\n1) Checkwhereunder-andoverflowoccurfordouble-precisionfloating-pointnumbers.\nGiveyouranswerindecimals.\n2) Checkwhereunder-andoverflowoccurforfloats.\n3) Checkwhereunder-andoverflowoccurforintegers. Note:Thereisnoexponentstored\nfor integers, so the smallest integer corresponds to the most negative one. To deter-\nminethelargestandsmallestintegers,youmustobserveyourprogram‚Äôsoutputasyou\n2.4 Programming Warmup 29\nexplicitlypassthroughthelimits.Youaccomplishthisbycontinuallyaddingandsub-\ntracting1.(Inasmuchasintegerarithmeticuses two‚Äôscomplement arithmetic,youshould\nexpectsomesurprises.)\n2.4.4 Machine Precision\nA recurring concern of computational scientists is that the floating-point representation\nusedtostorenumbersisoflimitedprecision.Ingeneralfora32-bit-wordmachine, single-\nprecision numbers are good to 6‚Äì7 decimal places, while doubles are good to 15‚Äì16 places .\nToseehowlimitedprecisionaffectscalculations,considerthesimplecomputeradditionof\ntwosingle-precisionnumbers:\n7+1.0√ó10‚àí7=? (2.19)\nThecomputerfetchesthesenumbersfrommemoryandstoresthebitpatterns\n7=01000001011100000000000000000000 , (2.20)\n10‚àí7=00110000011010110101111111001010 , (2.21)\ninworkingregisters (piecesoffast-respondingmemory).Becausetheexponentsarediffer-\nent,itwouldbeincorrecttoaddthemantissas,andsotheexponentofthesmallernumberis\nmadelargerwhileprogressivelydecreasingthemantissaby shiftingbits totheright(insert-\ningzeros)untilbothnumbershavethesameexponent:\n10‚àí7=00110000101101011010111111100101 (0)\n=00110001000110101101011111110010 (10) (2.22)\n¬∑¬∑¬∑\n=01000001000000000000000000000000 (0001101¬∑¬∑¬∑0\n‚áí 7+1.0√ó10‚àí7=7. (2.23)\nBecause there is no room left to store the last digits, they are lost, and after all this hard\nworktheadditionjustgives7astheanswer;anexampleofthetruncationerrorindicated\ninFigure2.2.Inotherwords,becausea32-bitcomputerstoresonly6or7decimalplaces,\niteffectivelyignoresanychangesbeyondthesixthdecimalplace.\nTheprecedinglossofprecisioniscategorizedbydefiningthe machineprecision ùúñmasthe\nmaximumpositivenumberthat,onthecomputer,canbeaddedtothenumberstoredas1\nwithoutchangingthatstored1:\n1c+ùúñmdef=1c, (2.24)\nwherethesubscript cisareminderthatthisisacomputerrepresentationof1.Consequently,\nanarbitrarynumber xcanbethoughtofasrelatedtoitsfloating-pointrepresentation xcby\nxc=x(1¬±ùúñ), |ùúñ|‚â§ùúñm, (2.25)\nwheretheactualvalueof ¬±ùúñisnotknown(butcanbedetermined).Inotherwords,except\nforpowersof2thatarerepresentedexactly,weshouldassumethatallsingle-precisionnum-\nberscontainanerrorinthesixthdecimalplace,andthatalldoubleshaveanerrorinthe\n15thplace.And,asisalwaysthecasewitherrors,wemustassumethatwereallydonot",14940
17-2.5.2 Matplotlibs 2D Plots.pdf,17-2.5.2 Matplotlibs 2D Plots,"302 Software Basics\nknowwhattheerroris,forifweknew,thenwewouldeliminateit!Consequently,theargu-\nments we are about to put forth regarding errors should be considered approximate, but\nthat‚Äôstypicalforknownunknowns.\n2.4.5 Experiment: Your Machine‚Äôs Precision\nWriteaprogramtodeterminethemachineprecision ùúñmofyourcomputersystemwithina\nfactorof2.Asamplepseudocodeis\n1eps = 1.\nbegin do N times\neps = eps/2. # Make smaller\none = 1. + eps # Write loop number, one, eps\n5end do\nAPythonimplementationisgiveninListing2.13,whileamorepreciseonewouldworkat\nthebytelevel.\n1) Determineexperimentallytheprecisionofdouble-precisionfloats.\n2) Determineexperimentallytheprecisionofcomplexnumbers.\nIt‚Äôs good to remember that to print out a number in decimal format, the computer must\nmakeaconversionfromitsinternalbinaryrepresentationtodecimal.Thisnotonlytakes\ntime,butunlessthenumberisanexactpowerof2,leadstoalossofprecision.Soifyouwant\natrulypreciseindicationofthestorednumbers,youshouldavoidconversiontodecimals\nandinsteadprintthemoutinoctal( ‚ßµ0NNN)orhexadecimal( 0x)format.\n2.5 Python‚Äôs Visualization Tools\nIfIcan‚Äôtpictureit,Ican‚Äôtunderstandit .\n‚ÄîAlbertEinstein\nInthesectionstofollowwediscusstoolstovisualizedataproducedbysimulationsandmea-\nsurements.Whereasotherbooksmaychoosetorelegatethisdiscussiontoanappendix,ornot\ntoincludeitatall,webelievethatvisualizationissuchanintegralpartofCP,andsousefulfor\nyourworkintherestofthisbook,thatwehaveplacedithere,rightupfront.Wedescribethe\nuseofMatplotlib[Matplotlib ,2023]andVpython/Visual .\nGeneralities One of the most rewarding aspects of computing is visualizing the results.\nWhileinthepastthiswasperformedwith2Dplots,inmoderntimesitisregularpractice\nto use 3D (surface) plots, volume rendering (dicing and slicing), animations, and virtual\nreality(gaming)tools.Thesetypesofvisualizationsareoftenbreathtakinglybeautifuland\nmayprovidedeepinsightsintoproblemsbylettingusseeand‚Äúhandle‚Äùthefunctionswith\nwhichweareworking.Visualizationalsoassistsinthedebuggingprocess,thedevelopment\nofphysicalandmathematicalintuition,andtheall-aroundenjoymentofwork.\nInthinkingaboutwaystoviewyourresults,keepinmindthatthepointofvisualization\nistomakethephysicsclearerandtocommunicateyourworktoothers.Itfollowsthenthat\n2.5 Python‚Äôs Visualization Tools 31\nyoushouldmakeallfiguresasclear,informative,andself-explanatoryaspossible,especially\nifyouwillbeusingtheminpresentationswithoutcaptions.Thismeanslabelsforcurvesand\ndatapoints,atitle,andlabelsontheaxes.3Afterthis,youshouldstudyyourvisualization\nandaskwhethertherearebetterchoicesforunits,rangesofaxes,colors,style,andsoon,\nthatmightgetthemessageacrossbetterandprovidemoreinsight.Andtrytorememberthat\nthose colors which look great on your monitor may turn into uninformative grays when\nprinted. Considering the complexity of human perception and cognition, there may not\nbeasinglebestwaytovisualizeaparticulardataset,andsosometrialanderrormaybe\nnecessaryto‚Äúsee‚Äùwhatworksbest.\n2.5.1 Visual (VPython)‚Äôs 2D Plots\nVpython(PythonplustheVisualpackage)isasimplewaytogettocreatePythonvisual-\nizationsanditwasusedtocreatemanyofthevisualizationsinthisbook.Itsdevelopment\nendedin2006andhasbeensupersededby WebVpython .However,youcanstillrunVpython\nprogramsasWebVpythonorwithinaJupyterNotebook.\nInFigure2.4,wepresenttwoplotsproducedbytheprogram EasyVisual.py inListing2.1.\nNoticethattheplottingtechniqueistocreatefirsttheplotobjects Plot1and Plot2,andthen\ntoaddthepointstotheobjects,one-by-one,andthenusethe plotmethodtoplottheobjects.\n(Incontrast,Matplotlibcreatesavectorofpointsandthenplotstheentirevectorinonefell\nswoop.)\nIt is often a good idea to place several plots in the same figure. The program\n3GraphVisual.py inListing2.2doesthatandproducesthegraphontheleftofFigure2.5.On\nyourcomputerscreenyouwillseewhiteverticalbarscreatedwith gears,reddotscreated\nwith grots,andayellowcurvecreatedwith curve.\nAnimations Creating animations with Visual is essentially just making the same 2D\nplotoverand overagain,with each one ata slightlydifferingtime,and then placingthe\nplotsontopofeachother.Whenperformedproperly,thisgivestheimpressionofmotion.\nSeveral of our sample codes produce animations, for example, HarmosAnimate.py and\n3Danimate.py . Three frames produced by HarmosAnimate.py are shown on the right of\nFigure 2.4 Screen dumps of two x-yplots produced by EasyVisual.py using the Visual package.\nTheleftplot uses default parameters while the right plot uses user-supplied options.\n3 Althoughthismaynotneedsaying,placetheindependentvariable xalongtheabscissa(horizontal),and\nthedependentvariable y=f(x)alongtheordinate.\n322 Software Basics\nFigure 2.5 Left: Output from the program 3GraphVisual.py that places three different types of 2D\nplots on one graph using Visual. Right: Three frames from a Visual animation of a quantum\nmechanical wave packet produced with HarmosAnimate.py.\nFigure2.5.Themajorportionsofthesecodesdealwiththesolutionofpartialdifferential\nequations, which need not concern us (yet). The part which makes the animation\nissimple:\nPlotObj= curve(x=xs, color=color.yellow, radius=0.1)\n...\n3whileTrue: # Runs forever\nrate(500)\nps[1:‚àí1] = ...\npsi[1:‚àí1] = ..\n7PlotObj.y = 4 ‚àó(ps‚àó‚àó2+p s i ‚àó‚àó2)\nHere PlotObjisacurvethatgetscontinuallybuiltfromwithinawhileloopandthusappears\ntobemoving.Notethatbeingabletoplotpointsindividuallywithouthavingtostorethem\nallinanarrayforalltimeskeepsthememorydemandoftheprogramquitesmallandis\nfast.\n2.5.2 Matplotlib‚Äôs 2D Plots\nMatplotlib is a powerful plotting package that lets you create 2D and 3D graphs, his-\ntograms, power spectra, bar charts, error charts, scatter plots, and what not, all directly\nfrom within your Python program. Matplotlib is free, uses the sophisticated numerics\nof NumPy and LAPACK, and, believe it or not, is easy to use. Since Matplotlib is not\npart of standard Python, you must import the entire Matplotlib package, or individual\nmethods,intoyourprogram.Weusuallydothatinourcodesbyimporting pylab,whichis\namodulethatprovidesbothMatplotlibandNumPypackages.Here,from EasyMatPlot.py ,\nishowwedoit:\n1frompylabimport ‚àó # Load Matplotlib\nMin = ‚àí5.; Max = +5.; Npoints= 500\nDel = (Max ‚àíMin) / Points\nx = arrange(Min, Max, Del)\n5y= s i n ( x ) ‚àósin(x ‚àóx) # f(x array)\nlabel( ‚Äôx‚Äô); label( ‚Äôf(x)‚Äô); title( ‚Äô f(x) vs x‚Äô )\ntext(‚àí1.75, 0.75, ‚ÄôMatplotlib \n Example‚Äô ) #T e x to np l o t\n2.5 Python‚Äôs Visualization Tools 33\nplot(x, y, ‚Äô-‚Äô,l w = 2 )\n9grid(True) # Form grid\nshow()\nMatplotlib commands are by design similar to the plotting commands of MATLAB, a\ncommercialproblem-solvingenvironmentthatisparticularlypopularinengineering.Asis\ntrue for MATLAB, Matplotlib assumes that you have placed the xandyvalues that you\nwish to plot into 1D arrays (vectors), and then plots the entire vectors in one fell swoop.\nMatplotlibusesthepowerfulNumPy arrayobjecttostorethedata,whichwediscussfurther\nin Chapter 7. As you can see, NumPy‚Äôs arrangemethod constructs an array covering ‚Äúa\nrange‚Äùbetween Maxand Mininstepsof Del.Becausethelimitsarefloating-pointnumbers,\nsotoowillbethe xi‚Äôs.Andbecause xisanarray, y = -sin(x)*cos(x) isautomaticallyonetoo!\nTheactualplottingisperformedwiththe plotcommand,withadash‚Äò-‚Äôindicatingaline,\nand lw=2settingthelinewidth.TheresultisshownontheleftofFigure2.6,withthedesired\nlabelsandtitle.The show()commandproducesthegraphonyourdesktop.Morecommands\naregiveninTable2.4.Wesuggestyoutryoutsomeoftheoptionsandtypesofplotspossible.\nInListing2.5,wegivethecode GradesMatplot.py ,andontherightofFigure2.6weshowits\noutput.Thisisnotasimpleplot.Herewerepeatthe plotcommandseveraltimesinorder\ntoplotseveraldatasetsonthesamegraph,andtoplotboththedatapointsandthelines\nconnectingthem.OnLine3weimportMatplotlib(pylab),andonLine4weimportNumPy,\nwhichweneedforthe arraycommand.Seeingthatwehaveimportedtwopackages,weadd\nthepylabprefixtothe plotcommandssothatPythonknowswhichpackagetouse.\nInordertoplaceahorizontallinealong y=0,onlines10and11wecreateadatasetasan\narrayofxvalues,‚àí1‚â§x‚â§5,andacorrespondingarrayof yvalues,yi‚â°0.Wethenplotthe\nhorizontalonline12.Next,weplacefourmorecurvesonthefigure.Firstonlines14‚Äì15we\ncreatedataset0,thenplotthepointsasbluecircles(grayonthepage) ‚Äôbo‚Äô,andconnectthe\npointswithgreen( ‚Äôg‚Äô)lines.Onlines19‚Äì21wecreateandplotanotherdatasetasared( ‚Äôr‚Äô)\nline(grayonthepage).Finally,onlines23‚Äì25wedefineunequalloweranduppererrorbars\nandplacethemontheplot.Wefinishbyaddinggridlines(Line27)and showingtheplot\nonthescreen.\n‚Äì1‚Äì6‚Äì4‚Äì20GPAf(x)246\n012\nY ears in college xGrade inflation f(x) vs x\n1. 0\n0.5\n0.0\n‚Äì0.5\n‚Äì1 .0\n‚Äì6 ‚Äì4 ‚Äì2 0 2 4MatPlotLib example\n6 345\nFigure 2.6 Matplotlib plots. Left: Output of EasyMatPlot.py (Listing 2.3) showing a simple, x-y\nplot.Right: Output from GradesMatPlot.py that places two sets of data points, two curves, and\nunequal upper and lower error bars, all on one plot.\n342 Software Basics\nTable 2.4 Some common Matplotlib commands.\nCommand Effect Command Effect\nplot(x,y,‚Äò-‚Äô,lw =2) x-ylinewidth2 myPlot.setYRange( ‚àí8.,8.) Set yrange\nshow() Showgraph myPlot.setSize(500,400) Sizeinpixels\nlabel(‚Äòx‚Äô) x-axislabel pyplot.semilogx Epilog xplot\nlabel(‚Äòf(x)‚Äô) y-axislabel pyplot.semilogy Epilog yplot\ntitle(‚Äòfvs.x‚Äô) Addtitle grid(True) Drawgrid\ntext(x,y,‚Äòs‚Äô) Addtext sat(x,y)myPlot.setColor(false) Black&White\nmyPlot.addPoint Add (x,y)to0 myPlot.setButtons(true) Forzoombutton\n(0,x,y,true) connect\nmyPlot.addPoint Add (x,y)to1, myPlot.fillPlot() Fitrangestodata\n(1,x,y,false) noconnect\npyplot.errorbar Point +errorbar myPlot.setImpulses(true,0) Vertlines,set0\npyplot.clf() Clearfigure pyplot.contour Contourlines\npyplot.scatter Scatterplot pyplot.bar Barcharts\npyplot.polar Polarplot pyplot.gca Forcurrentaxis\nmyPlot.setXRange Set xrange pyplot.acorr Autocorrelation\n(‚àí1.,1.)\nOftenthescienceisclearerifthereareseveralcurvesinoneplot,and,severalplotsinone\nfigure.Matplotlibletsyoudothiswiththe plotandthe subplotcommands.Forexample,\ninMatPlot2figs.py inListing2.6andFigure2.7,wehaveplacedtwocurvesinoneplot,and\nthenoutputtwodifferentfigures,eachcontainingtwoplots.Thekeyhereisarepetitionof\nthesubplotcommand:\nfigure(1) # The 1st figure\n2subplot(2,1,1) # 2 rows, 1 column, 1st subplot\nsubplot(2,1,2) # 2 rows, 1 column, 2nd subplot\nf(x)\nf(x)f(x)0.00.20.40.60.81. 0\nexp(‚Äì x/4)*sin( x)sin^2(x)*cos^2(x^2) ‚Äìsin( x)*cos( x^2)\n‚Äì6 ‚Äì4 ‚Äì2 0\nx24 ‚Äì 6\n‚Äì2‚Äì101234‚Äì0.5\n‚Äì1 .00.00.51. 0\n‚Äì4 ‚Äì2 0\nx246\n‚Äì4 ‚Äì2 0\nx2466\n‚Äì6 ‚Äì60246810f(x)12\n‚Äì4 ‚Äì2 0exp(‚Äì x/2)*sin^2(x)\n246\nx\nFigure 2.7 LeftandRight Columns show two separate outputs, each of two Ô¨Ågures, produced by\nMatPlot2Ô¨Ågs.py. (We used the slider button to add some space between the upper and lower plots).",10682
18-2.5.4 Matplotlibs Animations.pdf,18-2.5.4 Matplotlibs Animations,"2.5 Python‚Äôs Visualization Tools 35\nThe listing is self-explanatory, with sections that set the plotting limits, that create each\nfigure,andthencreatethegrid.\nScatterPlots Sometimesweneedascatterplotofdata,andmaybeevenacurvethrownin\naswell.InFigure5.4,weshowascatterplotcreatedwiththecode PondMapPlot.py inListing\n2.7.Thekeystatementshereareoftheform ax.plot(ox, yo, ‚Äôbo‚Äô, markersize=3) ,which\ninthiscaseaddsabluepoint(grayonthepage)ofsize3.\n2.5.3 Matplotlib‚Äôs 3D Surface Plots\nA2Dplotofthepotential V(r)=1‚àïrversusrisfineforvisualizingtheradialdependenceof\nthepotentialfieldsurroundingasinglecharge,butifyouwanttovisualizeadipolepotential\nsuchasV(x,y)=[B+C(x2+y2)‚àí3‚àï2]x,youneeda3D,orsurface,visualization.Yougetthat\nbycreatingaworldinwhichthe zdimension(mountainheight)isthevalueofthepotential,\nandthexandyaxesdefinetheplanebelowthemountain.Asthesurfaceyouarecreating\nisa3Dobject,itisnottrulypossibletodrawitonaflatscreen,andsodifferenttechniques\nareusedtogivetheimpressionofthreedimensionstoourbrains.Thatisaccomplishedby\nrotatingtheobject(grabbingitwithyourmouse),shadingit,employingparallax,andother\ntricks.\nInFigure2.8,weshowawire-frameplot(left)andacoloredsurface-plus-wire-frameplot\n(right).Theseareobtainedfromtheprogram Simple3Dplot.py inListing2.8.Notethatthere\nisanextraimportof Axes3DfromtheMatplotlibtoolkitneededfor3Dplotting.Lines8and\n9 are the usual creation of xandyarrays of floats using arrange. Line 11 uses the meshed\nmethodtosetuptheentirecoordinatematrixgridfromthe xandycoordinatevectorswith\navectoroperation,andline12constructstheentire Zsurfacewithanothervectoroperation.\nTheremainderoftheprogramisself-explanatory,with figbeingtheplotobject, axthe3D\naxesobject,and plot_airframe and plot_surface creatingwireframewireframeandsurface\nplots,respectively.Anothertypeof3Dplotthatisparticularlyusefulwhenexaminingdata\noftheform (xi,yj,zk),isascatterplotintoa3Dvolume.InListing2.9,wegivetheprogram\nScatter3dPlot.py that created the plot in Figure 2.9. This program, which is taken from\nthe Matplotlibdocumentation,uses theNumPy randomnumber generator,withthe 111\nnotationbeingahand-me-downfromMATLABindicatinga1 √ó1√ó1grid.\n1. 0\n0.5\n0.0z\nxy‚Äì0.5\n‚Äì1 .0\n‚Äì3\n‚Äì3‚Äì2‚Äì10123\n‚Äì2‚Äì101231. 0\n0.5\n0.0z\nxy‚Äì0.5\n‚Äì1 .0\n‚Äì3\n‚Äì3‚Äì2‚Äì10123\n‚Äì2‚Äì10123\nFigure 2.8 Left: A 3D wire frame. Right: A colored surface plot with wire frame. Both are produced\nby the program Simple3dplot.py using Matplotlib.",2434
19-2.6 Plotting Exercises.pdf,19-2.6 Plotting Exercises,"362 Software Basics\nX LabelX22\n24\n26\n2830\n32\n34Y LabelY\n‚Äì20020406080100120‚Äì50‚Äì40‚Äì30‚Äì20‚Äì100\n‚Äì60Z LabelZ\nFigure 2.9 A 3D scatter plot produced by the program Scatter3dPlot.py using Matplotlib.\nFinally, the program FourierMatplot.py , written by Oscar Estrepe, performs a Fourier\nreconstruction of a saw tooth wave, with the number of waves included controlled by\nthe viewer via a slider bar, as shown in Figure 2.10. (We discuss Fourier transforms in\nChapter9.)Theslidermethodisincludedviatheextralines:\n1frommatplotlib.widgets importSlider\n...\nshortwaves = Slider(airwaves, ‚Äô# Waves‚Äô , 1, 20, valinit=T)\n...\n5snumwaves.on_changed(update)\n2.5.4 Matplotlib‚Äôs Animations\nMatplotlibcanalsocreateanimations,althoughnotassimplyasVpython.TheMatplotlib\nexamples page gives a number of them. We have included some Matplotlib animation\ncodesintheCodesdirectory,andshowasamplecodefortheheatequationinListing2.10.\nHere too, most of the code deals with solving a partial differential equation, which need\nnotinterestusyet.Theanimationiscarriedoutatthebottomofthecode.\n2.6 Plotting Exercises\n1) We encourage you to make your own plots and personalize them by trying out other\ncommands and by including further options in the commands. The Matplotlib docu-\nmentationisextensiveandavailableontheWeb.Asanexercise,explore:\n2.6 Plotting Exercises 37\n0.0 0.5 1 .0 1 .5 2.0 2.5\nTimeFourier synthesis of sawtooth function\n# Waves 9.00‚Äì4‚Äì3‚Äì2‚Äì10Signal1234\n3.0\nFigure 2.10 A comparison of a saw tooth function to the sum of its Fourier components, with the\nnumber of included waves varied interactively by a Matplotlib slider. FourierMatplot.py produced\nthis output and was written by Oscar Estrepe.\nFL FR\nFL= 0.00FR= 600.00\nddx\nFigure 2.11 Left: A beam and a box supported at two points. Right: A screenshot from the\nanimation showing the forces on the beam as the weight moves.\na) howtozoominandzoomoutonsectionsofaplot,\nb) howtosaveyourplotstofilesinvariousformats,\nc) howtoprintupyourgraphs,\nd) theoptionsavailablefromthepull-downmenus,\ne) howtoincreasethespacebetweensubplots,\nf) andhowtorotateandscalethesurfaces.\n2) AsshowninFigure2.11,abeamoflength L=10mandweight W=400Nrestsontwo\nsupportsatadistance d=2mapart.Aboxofweight Wb=800N,initiallyabovetheleft\nsupport,slidesfrictionlesslytotherightwithavelocity ùë£=7m/s.\na) Writeaprogramthatcalculatestheforcesexertedonthebeambytherightandleft\nsupportsastheboxslidesalongthebeam.\nb) Extendyourprogramsothatitcreatesananimation,orjustaseriesofstills,show-\ning the forces and the position of the block as the box slides along the beam. In\nFigure2.11,leftwepresentascreenshotcapturedfromoneofouranimations.\nc) Extendthetwo-supportproblemtoaboxslidingtotherightonabeamwithathird\nsupportundertherightedgeofthebeam.",2791
20-2.7 Code Listings.pdf,20-2.7 Code Listings,"382 Software Basics\n2.7 Code Listings\nListing2.1 EasyVisual.py Producestwodifferent2DplotsusingtheVisualpackage.\n# EasyVisual.py: Simple graph object using Visual\n2\nfromvisual.graph import ‚àó # Import Visual\n4 Plot1 = gcurve(color = color.white) # gcurve method\nforxinarange(0., 8.1, 0.1): #xr a n g e\n6 Plot1.plot( pos = (x, 5. ‚àócos(2. ‚àóx)‚àóexp(‚àí0.4‚àóx)) )#P l o tp t s\ngraph1 = gdisplay(width=600, height=450,\\n8 title= ‚ÄôVisual 2-D Plot‚Äô , xtitle= ‚Äôx‚Äô, ytitle= ‚Äôf(x)‚Äô,\\nforeground = color.black, background = color.white)\n10 Plot2 = gdots(color = color.black) #D o t s\nforxinarange( ‚àí5., +5, 0.1 ):\n12 Plot2.plot(pos = (x, cos(x)))\nListing 2.2 3GraphVisual.py Produces a 2D x-y plot with the Matplotlib and NumPy\npackages.\n2 # 3GraphVisual.py: 3 plots in the same figure , with bars, dots and curve\n4 fromvisualimport ‚àó\nfromvisual.graph import ‚àó\n6\nstring = ""blue: sinÀÜ2(x), white: cosÀÜ2(x), red: sin(x)*cos(x)""\n8 graph1 = gdisplay(title=string , xtitle= ‚Äôx‚Äô, ytitle= ‚Äôy‚Äô)\ny1 = gcurve(color=color.yellow, delta=3) # Curve\n10 y2 = gvbars(color=color.white) # Vertical bars\ny3 = gdots(color=color.red, delta=3) #D o t s\n12 forxinarange( ‚àí5, 5, 0.1): #a r a n g ef o rf l o a t s\ny1.plot( pos=(x, sin(x) ‚àósin(x)) )\n14 y2.plot( pos=(x, cos(x) ‚àócos(x)/3.) )\ny3.plot( pos=(x, sin(x) ‚àócos(x)) )\nListing2.3 3Dshapes.py ProducesasampleofVPython‚Äôs3Dshapes.\n2 # 3Dshapes.py: Some 3 ‚àíD Shapes of VPython\n4 fromvisualimport ‚àó\n6 graph1 = display(width=500, height=500, title= ‚ÄôVPython 3-D Shapes‚Äô ,range=10)\nsphere(pos=(0,0,0), radius=1, color=color.green)\n8 sphere(pos= (0,1, ‚àí3), radius=1.5, color=color.red)\narrow(pos=(3,2,2), axis=(3,1,1), color=color.cyan)\n10 cylinder(pos=( ‚àí3,‚àí2,3), axis=(6, ‚àí1,5), color=color.yellow)\ncone(pos=( ‚àí6,‚àí6,0), axis=( ‚àí2,1,‚àí0.5), radius=2, color=color.magenta)\n12 helix(pos=( ‚àí5,5,‚àí2), axis=(5,0,0), radius=2, thickness=0.4, color=color.orange)\nring(pos=( ‚àí6,1,0), axis=(1,1,1), radius=2, thickness=0.3, color=(0.3,0.4,0.6))\n14 box(pos=(5, ‚àí2,2), length=5, width=5, height=0.4, color=(0.4,0.8,0.2))\npyramid(pos=(2,5,2), size=(4,3,2), color=(0.7,0.7,0.2))\n16 ellipsoid(pos=( ‚àí1,‚àí7,1), axis=(2,1,3), length=4, height=2, width=5,\ncolor=(0.1,0.9,0.8))\nListing2.4 EasyMatPlot.py Producesa2D x-yplotusingtheMatplotlibpackage(which\nincludestheNumPypackage).\n# EasyMatPlot.py: Simple use of matplotlib ‚Äôs plot command\n2\nfrompylabimport ‚àó # Load Matplotlib\n2.7 Code Listings 39\n4\nXmin = ‚àí5.; Xmax = +5.; Npoints= 500\n6 DelX = (Xmax ‚àíXmin) / Npoints\nx = arange(Xmin, Xmax, DelX)\n8 y= s i n ( x ) ‚àósin(x ‚àóx) # F(x array)\nprint(‚Äôarange => x[0], x[1],x[499]=%8.2f %8.2f %8.2f‚Äô %(x[0],x[1],x[499]))\n10 print(‚Äôarange => y[0], y[1],y[499]=%8.2f %8.2f %8.2f‚Äô %(y[0],y[1],y[499]))\nprint(""\n Now doing the plotting thing, look for Figure 1 on desktop"" )\n12 xlabel( ‚Äôx‚Äô); ylabel( ‚Äôf(x)‚Äô); title( ‚Äô f(x) vs x‚Äô )\ntext(‚àí1.75, 0.75, ‚ÄôMatPlotLib \n Example‚Äô ) #T e x to np l o t\n14 plot(x, y, ‚Äô-‚Äô,l w = 2 )\ngrid(True) # Form grid\n16 show()\nListing2.5 GradesMatPlot.py Producesa2D x-yplotusingtheMatplotlibpackage.\n# Grade.py: Using Matplotlib ‚Äôs plot command with multi data sets & curves\n2\nimportpylab as p # Matplotlib\n4 fromnumpyimport ‚àó\n6 p.title( ‚ÄôGrade Inflation‚Äô ) # Title and labels\np.xlabel( ‚ÄôYears in College‚Äô )\n8 p.ylabel( ‚ÄôGPA‚Äô)\n10 xa = array([ ‚àí1, 5]) # For horizontal line\nya = array([0, 0]) #"" ""\n12 p.plot(xa, ya) # Draw horizontal line\n14 x0 = array([0, 1, 2, 3, 4]) # Data set 0 points\ny0 = array([ ‚àí1.4, +1.1, 2.2, 3.3, 4.0])\n16 p.plot(x0, y0, ‚Äôbo‚Äô) # Data set 0 = blue circles\np.plot(x0, y0, ‚Äôg‚Äô) # Data set 0 = line\n18\nx1 = arange(0, 5, 1) # Data set 1 points\n20 y1 = array([4.0, 2.7, ‚àí1.8,‚àí0.9, 2.6])\np.plot(x1, y1, ‚Äôr‚Äô)\n22\nerrTop = array([1.0, 0.3, 1.2, 0.4, 0.1]) # Asymmetric error bars\n24 errBot = array([2.0, 0.6, 2.3, 1.8, 0.4])\np.errorbar(x1, y1, [errBot, errTop], fmt = ‚Äôo‚Äô) # Plot error bars\n26\np.grid(True) # Grid line\n28 p.show() # Create plot on screen\nListing2.6 MatPlot2figs.py ProducesthetwofiguresshowninFigure2.7.Eachfigure\ncontainstwoplotswithoneMatplotlibfigure.\n# MatPlot2figs.py: plot of 2 subplots on 1 fig & 2 separate figs\n2\nfrompylabimport ‚àó # Load Matplotlib\n4\nXmin = ‚àí5.0; Xmax = 5.0; Npoints= 500\n6 DelX= (Xmax ‚àíXmin)/Npoints # Delta x\nx1 = arange(Xmin, Xmax, DelX) #x 1r a n g e\n8 x2 = arange(Xmin, Xmax, DelX/20) # Different x2 range\ny1 =‚àísin(x1) ‚àócos(x1 ‚àóx1) # Function 1\n10 y2 = exp( ‚àíx2/4.) ‚àósin(x2) # Function 2\nprint(""\n Now plotting, look for Figures 1 &2 on desktop"" )\n12 figure(1) # Figure 1\nsubplot(2,1,1) # 1st subplot in first figure\n14 plot(x1, y1, ‚Äôr‚Äô,l w = 2 )\nxlabel( ‚Äôx‚Äô); ylabel( ‚Äôf(x)‚Äô); title( ‚Äô-sin(x)*cos(xÀÜ2)‚Äô )\n16 grid(True) # Form grid\nsubplot(2,1,2) # 2nd subplot in first figure\n18 plot(x2, y2, ‚Äô-‚Äô,l w = 2 )\nxlabel( ‚Äôx‚Äô) # Axes labels\n402 Software Basics\n20 ylabel( ‚Äôf(x)‚Äô)\ntitle( ‚Äôexp(-x/4)*sin(x)‚Äô )\n22 figure(2) # Figure 2\nsubplot(2,1,1) # 1st subplot in 2nd figure\n24 plot(x1, y1 ‚àóy1, ‚Äôr‚Äô,l w = 2 )\nxlabel( ‚Äôx‚Äô); ylabel( ‚Äôf(x)‚Äô); title( ‚ÄôsinÀÜ2(x)*cosÀÜ2(xÀÜ2)‚Äô )\n# form grid\n26 subplot(2,1,2) # 2nd subplot in 2nd figure\nplot(x2, y2 ‚àóy2, ‚Äô-‚Äô,l w = 2 )\n28 xlabel( ‚Äôx‚Äô); ylabel( ‚Äôf(x)‚Äô); title( ‚Äôexp(-x/2)*sinÀÜ2(x)‚Äô )\ngrid(True)\n30 show() # Show graphs\nListing2.7 PondMatPlot.py ProducesthescatterplotandthecurveshowninFigure5.4\ninChapter5.\n# PondMatPlot.py: Monte ‚àíCarlo integration via vonNeumann rejection\n2\nimportnumpy as np, matplotlib.pyplot as plt\n4\nN = 100; Npts = 3000; analyt = np.pi ‚àó‚àó2\n6 x1 = np.arange(0, 2 ‚àónp.pi+2 ‚àónp.pi/N,2 ‚àónp.pi/N)\nxi = []; yi = []; xo = []; yo = []\n8 fig ,ax = plt.subplots()\ny1 = x1 ‚àónp.sin(x1) ‚àó‚àó2 # Integrand\n10 ax.plot(x1, y1, ‚Äôc‚Äô, linewidth=4)\nax.set_xlim ((0, 2 ‚àónp.pi))\n12 ax.set_ylim((0, 5))\nax.set_xticks([0, np.pi, 2 ‚àónp.pi])\n14 ax.set_xticklabels([ ‚Äô0‚Äô,‚Äô$\pi$‚Äô,‚Äô2$\pi$‚Äô])\nax.set_ylabel( ‚Äô$f(x) = x\,\sinÀÜ2 x $‚Äô, fontsize=20)\n16 ax.set_xlabel( ‚Äôx‚Äô,fontsize=20)\nfig.patch.set_visible(False)\n18\ndeffx(x): returnx‚àónp.sin(x) ‚àó‚àó2 # Integrand\n20 j=0 # Inside curve counter\nxx = 2. ‚àónp.pi ‚àónp.random.rand(Npts) #0=<x<=2 p i\n22 yy = 5 ‚àónp.random.rand(Npts) #0=<y<=5\nforiin range (1,Npts):\n24 if(yy[i] <= fx(xx[i])): # Below curve\nif(i<=100): xi.append(xx[i])\n26 if(i<=100): yi.append(yy[i])\nj +=1\n28 else:\nif(i<=100): yo.append(yy[i])\n30 if(i<=100): xo.append(xx[i])\nboxarea = 2. ‚àónp.pi ‚àó5. #B o xa r e a\n32 area = boxarea ‚àój/(Npts ‚àí1) # Area under curve\nax.plot(xo,yo, ‚Äôbo‚Äô,markersize=3)\n34 ax.plot(xi,yi, ‚Äôro‚Äô,markersize=3)\nax.set_title( ‚ÄôAnswers: Analytic = %5.3f, MC = %5.3f‚Äô %(analyt,area))\n36 plt .show()\nListing2.8 Simple3Dplot.py ProducestheMatplotlib3DsurfaceplotsinFigure2.8.\n# Simple3Dplot.py: matplotlib 3D plot you can rotate and scale via mouse\n2\nimportmatplotlib.pylab as p\n4 frommpl_toolkits.mplot3d importAxes3D\n6 print(""Please be patient, I have packages to import &points to plot"" )\ndelta = 0.1\n8 x = p.arange( ‚àí3., 3., delta )\ny = p.arange( ‚àí3., 3., delta )\n10 X, Y = p.meshgrid(x, y)\n2.7 Code Listings 41\nZ=p .s i n( X ) ‚àóp.cos(Y) # Surface height\n12 fig = p.figure() # Create figure\nax = Axes3D(fig) #P l o t sa x e s\n14 ax.plot_surface(X, Y, Z) # Surface\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô) # Add wireframe\n16 ax.set_xlabel( ‚ÄôX‚Äô)\nax.set_ylabel( ‚ÄôY‚Äô)\n18 ax.set_zlabel( ‚ÄôZ‚Äô)\np.show() # Output figure\nListing2.9 Scatter3dPlot.py Producesa3DscatterplotusingMatplotlib3Dtools.\n"" Scatter3dPlot.py from matplotlib examples""\n2\nimportnumpy as np\n4 frommpl_toolkits.mplot3d importAxes3D\nimportmatplotlib.pyplot as plt\n6\ndefrandrange(n, vmin, vmax):\n8 return(vmax‚àívmin) ‚àónp.random.rand(n) + vmin\nfig = plt.figure()\n10 ax = fig.add_subplot(111, projection= ‚Äô3d‚Äô)\nn = 100\n12 forc, m, zl, zh in[(‚Äôr‚Äô,‚Äôo‚Äô,‚àí50,‚àí25), ( ‚Äôb‚Äô,‚ÄôÀÜ‚Äô,‚àí30,‚àí5)]:\nxs = randrange(n, 23, 32)\n14 ys = randrange(n, 0, 100)\nzs = randrange(n, zl, zh)\n16 ax.scatter(xs, ys, zs, c=c, marker= m)\nax.set_xlabel( ‚ÄôX Label‚Äô )\n18 ax.set_ylabel( ‚ÄôY Label‚Äô )\nax.set_zlabel( ‚ÄôZ Label‚Äô )\n20 plt .show()\nListing 2.10 EqHeatAnimateMat.py Produces an animation of a cooling bar using\nMatplotlib.\n2 # EqHeat.py Animated heat equation soltn via fine differences\n4 fromnumpyimport ‚àó\nimportnumpy as np\n6 importmatplotlib.pyplot as plt\nimportmatplotlib.animation as animation\n8\nNx = 101\n10 Dx = 0.01414\nDt = 0.6\n12 KAPPA = 210. # Thermal conductivity\nSPH = 900. # Specific heat\n14 RHO = 2700. # Density\ncons = KAPPA/(SPH ‚àóRHO) ‚àóDt/(Dx ‚àóDx);\n16 T=n p .z e r o s( ( N x , 2 ), float) # Temp @ first 2 times\n18 definit():\nforixin range (1, Nx ‚àí1): # Initial temperature\n20 T[ix, 0] = 100.0;\nT[0, 0] = 0.0 #B a re n d sT=0\n22 T[0, 1] = 0.\nT[Nx‚àí1, 0] = 0.\n24 T[Nx‚àí1, 1] = 0.0\ninit()\n26 k=range(0,Nx)\nfig = plt.figure() # Figure to plot\n28 # select axis; 111: only one plot, x,y, scales given\n422 Software Basics\nax = fig.add_subplot(111, autoscale_on=False, xlim=( ‚àí5, 105), ylim=( ‚àí5, 110.0))\n30 ax.grid() # Plot grid\nplt.ylabel( ""Temperature"" )\n32 plt.title( ""Cooling of a bar"" )\nline , = ax.plot(k, T[k,0], ""r"",l w = 2 )\n34 plt.plot([1,99],[0,0], ""r"",lw=10)\nplt.text(45,5, ‚Äôbar‚Äô,fontsize=20)\n36\ndefanimate(dum):\n38 forixin range (1, Nx ‚àí1):\nT[ix, 1] = T[ix, 0] + cons ‚àó(T[ix + 1, 0] + T[ix ‚àí1, 0]‚àí2.0‚àóT[ix, 0])\n40 line.set_data(k,T[k,1] )\nforixin range (1, Nx ‚àí1):\n42 T[ix, 0] = T[ix, 1] # 100 position row @ t =m\nreturnline ,\n44 ani = animation.FuncAnimation(fig , animate,1) # Animation\nplt .show()\nListing2.11 AreaFormatted.py DoesI/Otoandfromkeyboard,aswellasfromafile.\nItworkswitheitherPython2or3byswitchingbetween raw_input andinput.Notetoread\nfromafileusingCanopy,youmustrightclickinthePythonrunwindowandchoose Change\ntoEditorDirectory .\n1# AreaFormatted: Python 2 or 3 formated output, keyboard input, file input\nfromnumpyimport ‚àó\nfromsysimportversion\n5if int(version[0])>2: # Python 3 uses input, not raw_input\nraw_input =input\nname =raw_input (‚ÄôKey in your name: ‚Äô ) # raw_input strings\nprint(""Hi "",name)\n9radius = eval(raw_input (‚ÄôEnter a radius: ‚Äô )) # For numerical values\nprint(‚Äôyou entered radius= %8.5f‚Äô %radius) # formatted output\nprint(‚ÄôEnter new name and r in file Name.dat‚Äô ) # raw_input strings\ninpfile = open(‚ÄôName.dat‚Äô ,‚Äôr‚Äô) #R e a df r o mf i l eN a m e . d a t\n13forlineininpfile:\nline = line.split() # Splits components of line\nname = line [0] # First entry in the list\nprint("" Hi %10s"" %(name)) # print Hi + first entry\n17r=float(line[1]) # convert string to float\nprint("" r = %13.5f"" %(r)) # convert to float &print\ninpfile.close()\nA=m a t h.p i ‚àór‚àó‚àó2\n21print(""Done, look in A.dat\n"" )\noutfile = open(‚ÄôA.dat‚Äô,‚Äôw‚Äô)\noutfile.write( ‚Äôr= %13.5f\n‚Äô %(r))\noutfile.write( ‚ÄôA = %13.5f\n‚Äô %(A))\n25outfile.close()\nprint(‚Äôr = %13.5f‚Äô %(r) , ‚Äô, A = %13.5f‚Äô %(A)) # Screen output\nprint(‚Äô\n Now example of integer input ‚Äô )\nage=int(eval(raw_input (‚ÄôNow key in your age as an integer: ‚Äô )))\n29print(""age: %4d years old, you don‚Äôt look it!\n"" %(age))\nprint(""Enter and return a character to finish"" )\ns=raw_input ()\nListing2.12 Directives.py Illustratesformattingviadirectivesandescapecharacters.\n# Directives.py illustrates escape and formatting characters\nimportsys\n3print(""hello \n"" )\nprint(""\t it‚Äôs me"" ) # tabulator\nb=7 3\nprint(""decimal 73 as integer b = %d "" %(b))# for integer\n7print(""as octal b = %o"" %(b)) #o c t a l\n2.7 Code Listings 43\nprint(""as hexadecimal b = %x "" %(b)) # works hexadecimal\nprint(""learn \""Python\"" "" ) # use of double quote symbol\nprint(""shows a backslash \\"" ) # use of \\\n11print(‚Äôuse of single \‚Äô quotes \‚Äô ‚Äô ) # print single quotes\nListing2.13 Limits.py Determinesmachineprecisionwithinafactorof2.Notehowwe\nskip a line at the beginning of each class or method and how we align the closing brace\nverticallywithitsappropriatekeyword(initalics)\n# Limits.py: determines approximate machine precision\n2\nN=1 0\neps = 1.0\nforiin range (N):\n6eps = eps/2\none_Plus_eps = 1.0 + eps\nprint(‚Äôeps = ‚Äô ,e p s , ‚Äô, one + eps = ‚Äô , one_Plus_eps)",11942
21-Chapter 3 Errors and Uncertainties.pdf,21-Chapter 3 Errors and Uncertainties,,0
22-3.1 Types of Errors.pdf,22-3.1 Types of Errors,"44\n3\nErrors and Uncertainties\nTo err is human, to forgive divine .\n‚ÄîAlexander Pope\nWhether you are careful or not, errors and uncertainties are integral parts of a computation.\nIn this chapter we examine some of the errors and uncertainties that may occur in compu-\ntations. Although we do not keep repeating a mantra about watching for error, the lessons\nof this chapter apply to all other chapters as well .\n3.1 Types of Errors\nSome errors are the ones that humans inevitably make, but some are introduced by the\ncomputer. Computer errors arise because of the limited precision with which computers\nstorenumbers,orbecausealgorithmsormodelsarenotperfect.Althoughitstiflescreativity\ntokeepthinking‚Äúerror‚Äùwhenapproachingacomputation,itcertainlyisawasteoftime\nandbadsciencetoworkwithresultsthataremeaningless(‚Äúgarbage‚Äù)becauseoferrors.\nLet‚Äôssaythatyouhaveaprogramofhighcomplexity.Togaugewhyerrorsshouldbeof\nconcern,imaginethatyourprogramhasthelogicalflow\nstart‚ÜíU1‚ÜíU2‚Üí¬∑¬∑¬∑‚ÜíUn‚Üíend, (3.1)\nwhereeachunit Uimightbeastatementorastep.Ifeachunithasprobability pofbeing\ncorrect,thenthejointprobability Pofthewholeprogrambeingcorrectis P=pn.Let‚Äôsalso\nsaywehaveamedium-sizedprogramwith n=1000stepsandthattheprobabilityofeach\nstepbeingcorrectisalmostone, p‚âÉ0.9993.Thismeansthatyouendupwith P‚âÉ1\n2,thatis,\nafinalanswerthatisaslikelywrongasright(notagoodwaytobuildabridge).Theproblem\nisthat,asascientist,youwantaresultthatiscorrect‚Äîoratleastinwhichtheuncertainty\nissmallandofknownsize,evenifthecodeexecutesmillionsofsteps.\nFourgeneraltypesoferrorsexisttoplagueyourcomputations:\n1. Blundersorbadtheory: typographicalerrorsenteredwithyourprogramordata,run-\nningthewrongprogram,orhavingafaultinyourreasoning(theory),usingthewrong\ndatafile,andsoon.(Ifyourblundercountstartsincreasing,itmaybetimetogohome\nortakeabreak.)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n3.1 Types of Errors 45\n2. Random errors: imprecision caused by events such as fluctuations in electronics,\ncosmicrays,orsomeonepullingaplug.Thesemayberare,butyouhavenocontrolover\nthemandtheirlikelihoodincreaseswithrunningtime;whileyoumayhaveconfidence\nina20-secondcalculation,aweek-longcalculationmayhavetoberunmultipletimes\ntocheckreproducibility.\n3. Approximationerrors: imprecisionarisingfromsimplifyingthemathematicssothat\naproblemcanbesolvedonthecomputer.Theyincludethereplacementofinfiniteseries\nbyfinitesums,infinitesimalintervalsbyfiniteones,andvariablefunctionsbyconstants.\nForexample,\nsin(x)=‚àû‚àë\nn=1(‚àí1)n‚àí1x2n‚àí1\n(2n‚àí1)!(mathematicallyexact),\n‚âÉN‚àë\nn=1(‚àí1)n‚àí1x2n‚àí1\n(2n‚àí1)!+Óà±(x,N)(algorithm). (3.2)\nHereÓà±(x,N)is the approximation error, and it is the ignored series from N+1t o‚àû.\nSinceapproximationerrorarisesfromthealgorithmweusetoapproximatethemathe-\nmatics,itisalsocalled algorithmicerror .Foragoodalgorithm,theapproximationerror\nshoulddecreaseas Nincreases,andshouldvanishinthe N‚Üí‚àûlimit.Specificallyfor\n(3.2),becausethescalefor Nissetbythevalueof x,asmallapproximationerrorrequires\nN‚â´x.SoifxandNarecloseinvalue,theapproximationerrorwillbelarge.\n4. Round-off errors: imprecision arising from the finite number of digits used to store\nfloating-pointnumbers.These‚Äúerrors‚Äùareanalogoustotheuncertaintyinthelabora-\ntory measurement of a physical quantity. The overall round-off error accumulates as\nthecomputerhandlesmorenumbers,thatis,asthenumberofstepsinacomputation\nincreases.Thismaycausesomealgorithmstobecome unstablewithaconcordantrapid\nincreaseinerror.Insomecases,round-offerrormaybecomethemajorcomponentin\nyouranswer,leadingtowhatcomputerexpertscall garbage.\nForexample,ifyourcomputerkeptfourdecimalplaces,thenitwillstore1\n3as0.3333and\n2\n3as0.6667,wherethecomputerhas‚Äúroundedoff‚Äùthelastdigitin2\n3.Accordingly,ifwe\naskthecomputertodoassimpleacalculationas2(\n1\n3)\n‚àí2\n3,itwouldyield\n2(\n1\n3)\n‚àí2\n3=0.6666‚àí0.6667=‚àí0.0001 ‚â†0. (3.3)\nSo although the result may be small, it is not 0, and if we repeat this type of calcula-\ntionmillionsoftimes,thefinalanswermightnotbesmall(smallgarbagebegetslarge\ngarbage).\nWhen considering the precision of calculations it is good to recall our discussion in\nChapter 2 of significant figures. For computational purposes, let us consider how the\ncomputermaystorethefloating-pointnumber\na=11223344556677889900 =1.12233445566778899 √ó1019. (3.4)\nBecausetheexponentisstoredseparatelyandisasmallnumber,wemayassumethatit\nwillbestoredinfullprecision.Incontrast,someofthedigitsofthemantissamaybetrun-\ncated.Indoubleprecision,themantissaof awillbestoredintwowords,the mostsignificant\npartrepresenting the decimal 1.12233,and the leastsignificantpart 44556677.The digits",4729
23-3.1.1 Courting Disaster Subtractive Cancelation.pdf,23-3.1.1 Courting Disaster Subtractive Cancelation,,0
24-3.1.2 Subtractive Cancelation Exercises.pdf,24-3.1.2 Subtractive Cancelation Exercises,"46 3 Errors and Uncertainties\nbeyond7arelost.Asweshallseesoon,whenweperformcalculationswithwordsoffixed\nlength,itisinevitablethaterrorswillbeintroduced(atleast)intotheleastsignificantparts\nofthewords.\n3.1.1 Courting Disaster: Subtractive Cancelation\nCalculations employing numbers that are stored approximately can only be expected to\nyieldapproximateanswers.Todemonstratetheeffectofthistypeofuncertainty,wemodel\nthecomputerrepresentation xcoftheexactnumber xas\nxc‚âÉx(1+ùúñx). (3.5)\nHereùúñxis the relative error in xc, which we expect to be of a similar magnitude to the\nmachine precision ùúñm. If we apply this notation to the simple subtraction a=b‚àíc,w e\nobtain\na=b‚àíc‚áíac‚âÉbc‚àícc‚âÉb(1+ùúñb)‚àíc(1+ùúñc)\n‚áíac\na‚âÉ1+ùúñbb\na‚àíc\naùúñc. (3.6)\nWeseefrom(3.6)thattheresultingerrorin aisessentiallyaweightedaverageoftheerrors\ninbandc,withnoassurancethatthelasttwotermswillcancel.Ofspecialimportancehere\nis the observation that the error in the answer acincreases when we subtract two nearly\nequalnumbers( b‚âÉc)becauseaisthensmall,andwearesubtractingoffthemostsignifi-\ncantpartsofbothnumbersandleavingtheerror-proneleast-significantparts:\nac\nadef=1+ùúña‚âÉ1+b\na(ùúñb‚àíùúñc)‚âÉ1+b\namax(|ùúñb|,|ùúñc|). (3.7)\nThisshowsthateveniftherelativeerrorsin bandccancelsomewhat,theyaremultiplied\nby the large number b‚àïa, which can significantly magnify the error. Because we cannot\nassumeanysignfortheerrors,wemustassumetheworst.\nTheorem Ifyousubtracttwolargenumbersandendupwithasmallone,thesmallone\nislesssignificantthanthelargenumbers.\nWehavealreadyseenanexampleofsubtractivecancelationinthepowerseriessumma-\ntionforsin x‚âÉx‚àíx3‚àï3!+¬∑¬∑¬∑forlargex.Asimilareffectoccursfor e‚àíx‚âÉ1‚àíx+x2‚àï2!‚àí\nx3‚àï3!+¬∑¬∑¬∑forlargex.Herethefirstfewtermsarelargebutofalternatingsign,leadingtoan\nalmosttotalcancelationinordertoyieldthefinalsmallresult.Inthiscase,subtractivecan-\ncelationcanbeeliminatedbyusingtheidentity e‚àíx=1‚àïexandthenevaluating ex,although\nround-offerrorwillstillremain.\n3.1.2 Subtractive Cancelation Exercises\n1) Remember backinhighschoolwhenyoulearnedthatthequadraticequation\nax2+bx+c=0 (3.8)\nhasananalyticsolutionthatcanbewrittenaseither\nx1,2=‚àíb¬±‚àö\nb2‚àí4ac\n2aorx‚Ä≤\n1,2=‚àí2c\nb¬±‚àö\nb2‚àí4ac. (3.9)\n3.1 Types of Errors 47\nInspectionof(3.9)indicatesthatsubtractivecancelation(andconsequentlyanincrease\nin error) arises when b2‚â´4ac, as then the square root and its preceding term nearly\ncancelforoneoftheroots.\na) Writeaprogramthatcalculatesallsolutionsforarbitraryvaluesof a,b,andc.\nb) Investigate how errors in your computed answers become large as the subtractive\ncancelationincreases,andrelatethistotheknownmachineprecision. Hint:Agood\ntestcaseutilizes a=1,b=1,c=10‚àín,n=1,2,3,‚Ä¶.\n2) Aswehaveseen,subtractivecancelationoccurswhensummingaserieswithalternating\nsigns.Asanotherexample,considerthefinitesum\nS(1)\nN=2N‚àë\nn=1(‚àí1)nn\nn+1. (3.10)\nIfyousumtheevenandoddvaluesof nseparately,yougettwosums:\nS(2)\nN=‚àíN‚àë\nn=12n‚àí1\n2n+N‚àë\nn=12n\n2n+1. (3.11)\nAll terms are positivein this formwith just a singlesubtraction at the end of the cal-\nculation.Yeteventhisonesubtractionanditsresultingcancelationcanbeavoidedby\ncombiningtheseriesanalyticallytoobtain\nS(3)\nN=N‚àë\nn=11\n2n(2n+1). (3.12)\nAlthoughallthreesummations S(1),S(2),andS(3)aremathematicallyequal,theymay\ngivedifferentnumericalresults.\n1) Writeadouble-precisionprogramthatcalculates S(1),S(2),andS(3).\n2) Assume S(3)tobetheexactanswer.Makealog‚Äìlogplotoftherelativeerror versusthe\nnumberofterms,thatis,oflog10|(S(1)\nN‚àíS(3)\nN)‚àïS(3)\nN|versuslog10(N).Startwith N=1\nandworkupto N=1,000,000.(Recallthatlog10x=lnx‚àïln10.)Thenegativeofthe\nordinateinthisplotgivesanapproximatevalueforthenumberofsignificantfigures.\n3) Seewhetherstraight-linebehaviorfortheerroroccursinsomeregionsofyourplot.\nThisindicatesthattheerrorisproportionaltoapowerof N.\n3) Inspiteofthepowerofyourtrustycomputer,calculatingthesumofevenasimpleseries\nmayrequiresomethoughtandcare.Considerthetwoseries\nS(up)=N‚àë\nn=11\nn,S(down)=1‚àë\nn=N1\nn. (3.13)\nBothseriesarefiniteaslongas Nisfinite,andwhensummedanalyticallybothgivethe\nsameanswer.Nonetheless,becauseofround-offerror,thenumericalvalueof S(up)will\nnotbepreciselythatof S(down).\na) Writeaprogramtocalculate S(up)andS(down)asfunctionsof N.\nb) Makealog‚Äìlogplotof (S(up)‚àíS(down))‚àï(|S(up)|+|S(down)|)versusN.\nc) Observethelinearregimeonyourgraphandexplainwhythedownwardsumisgen-\nerallymoreprecise.",4360
25-3.1.3 RoundOff Errors.pdf,25-3.1.3 RoundOff Errors,,0
26-3.2 Experimental Error Investigation.pdf,26-3.2 Experimental Error Investigation,"48 3 Errors and Uncertainties\n3.1.3 Round-Off Errors\nLet‚Äôs startbyseeinghowerrorarisesfromasingledivisionofthecomputerrepresentations\noftwonumbers:\na=b\nc‚áíac=bc\ncc=b(1+ùúñb)\nc(1+ùúñc),\n‚áíac\na=1+ùúñb\n1+ùúñc‚âÉ(1+ùúñb)(1‚àíùúñc)‚âÉ1+ùúñb‚àíùúñc,\n‚áíac\na‚âÉ1+|ùúñb|+|ùúñc|. (3.14)\nHere we have ignored the very small ùúñ2terms, and have added the absolute value of the\nerrors since we can‚Äôt assume that good fortune will lead to errors canceling each other.\nBecause we add the errors in absolute value, this same rule holds for multiplication.\nEquation(3.14)isjustthebasicruleoferrorpropagationfromelementarylaboratorywork:\nYouaddtheuncertaintiesineachquantityinvolvedinananalysistoarriveattheoverall\nuncertainty.\nWe can even generalize this model to estimate the error in the evaluation of a general\nfunctionf(x),thatis,thedifferenceinthevalueofthefunctionevaluatedat xandatxc:\nÓà±=f(x)‚àíf(xc)\nf(x)‚âÉdf(x)‚àïdx\nf(x)(x‚àíxc). (3.15)\nSo,forexample,\nf(x)=‚àö\n1+x,df\ndx=1\n21‚àö\n1+x=1\n4f(x)(x‚àíxc) (3.16)\n‚áíÓà±‚âÉ1\n2‚àö\n1+x(x‚àíxc)=x‚àíxc\n2(1+x). (3.17)\nIf we evaluate this expression for x=ùúã‚àï4 and assume an error in the fourth place of x,\nweobtainasimilarrelativeerrorof1.5 √ó10‚àí4in‚àö\n1+x.\n3.1.4 Round-Off Error Accumulation\nThereisausefulmodelforapproximatinghowround-offerroraccumulatesinacalculation\ninvolvingalargenumberofsteps.AsillustratedinFigure3.1,weviewtheerrorineachstep\nofacalculationasaliteral‚Äústep‚Äùina randomwalk ,thatis,awalkforwhicheachstepisina\nR\n1\nŒîy1\nŒîy2Œîx1\n2\n34NFigure 3.1 A schematic of the Nsteps in a random walk\nsimulation that ends up a distance R=‚àö\nNfrom the origin.\nNotice how the Œîx‚Äôs for each step add vectorially.\n3.2 Experimental Error Investigation 49\nrandomdirection.AswewillderiveandsimulateinChapter4,thetotaldistance Rcovered\ninNstepsoflength r,is,ontheaverage,\nR‚âÉ‚àö\nNr. (3.18)\nByanalogy,thetotalrelativeerror ùúñroarisingafter Ncalculationalstepseachwithmachine\nprecisionerror ùúñmis,ontheaverage,\nùúñro‚âÉ‚àö\nNùúñm. (3.19)\nIftheround-offerrorsinaparticularalgorithmdonotaccumulateinarandommanner,\nthenadetailedanalysisisneededtopredictthedependenceoftheerroronthenumberof\nstepsN.Insomecases,theremaybenocancellation,andtheerrormayincreaseas Nùúñm.\nEvenworse,insomerecursivealgorithms,wheretheerrorgenerationiscoherent,suchas\ntheupwardrecursionforsphericalBesselfunctions,theremaybean N!increaseinerror.\n3.2 Experimental Error Investigation\nAlgorithms playavitalroleincomputationalphysics.Your problemistotakeanalgorithm\nanddecide\n1) Doesitconverge,andifso,howfast?\n2) Evenifitconverges,aretheanswersprecise?\n3) Howexpensive(time-consuming)isthealgorithm?\nYour first thought might be ‚ÄúWhat a dumb problem! All algorithms converge if you\nlet them run long enough. If you want more precision, then just let them run longer.‚Äù\nWell,somealgorithmsmaybeasymptoticexpansionsthatjustapproximateafunctionin\ncertainregionsofparameterspace,andconvergeonlyuptoapoint.Yetevenifauniformly\nconvergentpowerseriesisusedasthealgorithm,includingmoretermsmaydecreasethe\nalgorithmicerrorbutincreasetheround-offerror.Andbecauseround-offerrorseventually\ndivergetoinfinity,thebestwecanhopeforisa‚Äúbest‚Äùapproximation. Goodalgorithmsare\ngood not only because fewer steps take less time, but also because fewer steps produces less\nround-offerror .\nLet‚Äôsassumethatanalgorithmtakes Nstepstofindagoodanswer.Asaruleofthumb,\ntheapproximation(algorithmic)errordecreasesrapidly,oftenassomeinversepowerofthe\nnumberoftermsused:\nùúñapp‚âÉùõº\nNùõΩ. (3.20)\nHereùõºandùõΩareempiricalconstantsthatchangefordifferentalgorithmsandmaybeonly\napproximatelyconstant,andeventhenonlyas N‚Üí‚àû.Thefactthattheerror mustfalloff\nforlargeNisjustastatementthatthealgorithmworks.\nIncontrasttoalgorithmicerror,round-offerrorgrowsslowlyandsomewhatrandomly\nwithN. If the round-off errors in each step of the algorithm are not correlated, then we\nknowfrompreviousdiscussionthatwecanmodeltheaccumulationoferrorasarandom\nwalkwithstepsizeequaltothemachineprecision ùúñm:\nùúñro‚âÉ‚àö\nNùúñm. (3.21)\n50 3 Errors and Uncertainties\nThis is the slow growth with Nthat we expect from round-off error. The total error in a\ncomputationisthesumofthetwotypesoferrors:\nùúñtot=ùúñapp+ùúñro (3.22)\nùúñtot‚âÉùõº\nNùõΩ+‚àö\nNùúñm. (3.23)\nFor smallNweexpectthefirsttermtobethelargerofthetwo,butas Ngrowsitwillbe\novercomebytheever-increasinground-offerror.\nAsanexample,inFigure3.2,wepresentalog‚Äìlogplotoftherelativeerrorinnumerical\nintegrationusingtheSimpsonintegrationrule(Chapter5).Weusethelog10oftherelative\nerrorbecauseitsnegativetellsusthenumberofdecimalplacesofprecisionobtained.1Letus\nassume Óà≠istheexactanswerand A(N)thecomputedanswer.If\nÓà≠‚àíA(N)\nÓà≠‚âÉ10‚àí9,then log10||||Óà≠‚àíA(N)\nÓà≠||||‚âÉ‚àí9. (3.24)\nWe see in Figure 3.2 that the error does show a rapid decrease for small N, consistent\nwithaninversepowerlaw(3.20).Inthisregion,thealgorithmisconverging.As Nkeeps\nincreasing,theerrorstartstolooksomewhaterratic,withaslowincreaseontheaverage.\nIn accordance with (3.22), in this region, round-off error has grown larger than the\napproximationerrorandwillcontinuetogrowforincreasing N.Clearlythen,thesmallest\ntotal error will be obtained if we can stop the calculation at the minimum near 10‚àí14,\nthatis,when ùúñapprox‚âÉùúñro.\nIn realistic calculations you would not know the exact answer; after all, if you did,\nthen why would you bother with the computation? However, you may know the exact\nanswerforasimilarcalculation,andyoucanusethatsimilarcalculationtoperfectyour\nnumerical technique. Alternatively, now that you understand how the total error in a\ncomputation behaves, you should be able to look at a table or, better yet, a graph like\n10 10010‚Äì1310‚Äì9\nN| Relative error |Approximation error\nRound off error\nFigure 3.2 A log‚Äìlog plot of relative error versus the number of points used for a numerical\nintegration. The ordinate value of ‚àº10‚àí14at the minimum indicates that ‚àº14 decimal places of\nprecision are obtained before round-off error begins to build up. Notice that while the round-off\nerror does Ô¨Çuctuate indicating a statistical aspect of error accumulation, on the average it is\nincreasing but more slowly than did the algorithm‚Äôs error decrease.\n1 Mostcomputerlanguagesuseln x=logex.Yetbecause x=alogax,wehavelog10x=lnx‚àïln10.\n3.2 Experimental Error Investigation 51\nFigure3.2,ofyouransweranddeducethemannerinwhichyouralgorithmisconverging.\nSpecifically, at some point, you should see that the mantissa of the answer changes only\nin the less significant digits, with that place moving further to the right of the decimal\npointasthecalculationexecutesmoresteps.Eventually,however,asthenumberofsteps\nbecomes even larger, round-off error leads to a fluctuation in the less significant digits,\nwithagradualincreaseontheaverage.Itisbesttoquitthecalculationbeforethisoccurs.\nBased upon this understanding, an approach to obtaining the best approximation is to\ndeducewhenyouranswerbehaveslike(3.22).Todothat,wecall Óà≠theexactanswerand\nA(N)thecomputedanswerafter Nsteps.Weassumethatforlargeenoughvaluesof N,the\napproximationconvergesas\nA(N)‚âÉÓà≠+ùõº\nNùõΩ, (3.25)\nthatis,theround-offerrortermin(3.22)isstillsmall.Wethenrunourcomputerprogram\nwith 2Nsteps, which should give a better answer, and use that answer to eliminate the\nunknown Óà≠:\nA(N)‚àíA(2N)‚âÉùõº\nNùõΩ. (3.26)\nToseeiftheseassumptionsarecorrectanddeterminewhatlevelofprecisionispossiblefor\nthebestchoiceof N,plotlog10|[A(N)‚àíA(2N)]‚àïA(2N)|versuslog10N,similartowhatwe\nhaveperformedinFigure3.2.Ifyouobtainarapidstraight-linedrop-off,thenyouknow\nyouareintheregionofconvergenceandcandeduceavaluefor ùõΩfromtheslope.As Ngets\nlarger,youshouldseethegraphchangefromastraight-linedecreasetoaslowincreaseas\nround-offerrorbeginstodominate.Agoodplacetoquitisbeforethis.Inanycase,nowyou\nunderstandtheerrorinyourcomputationandthereforehaveachancetocontrolit.\nAsanexampleofhowdifferentkindsoferrorsenterintoacomputation,weassumewe\nknowtheanalyticformfortheapproximationandround-offerrors:\nùúñapp‚âÉ1\nN2,ùúñro‚âÉ‚àö\nNùúñm, (3.27)\n‚áíùúñtot=ùúñapprox+ùúñro‚âÉ1\nN2+‚àö\nNùúñm. (3.28)\nThetotalerroristhenaminimumwhen\ndùúñtot\ndN=‚àí2\nN3+1\n2ùúñm‚àö\nN=0, (3.29)\n‚áíN5‚àï2=4\nùúñm. (3.30)\nForadouble-precisioncalculation( ùúñm‚âÉ10‚àí15),theminimumtotalerroroccurswhen\nN5‚àï2‚âÉ4\n10‚àí15‚áíN‚âÉ1099,‚áíùúñtot‚âÉ4√ó10‚àí6. (3.31)\nInthiscasemostoftheerrorisasaresultofround-offandisnotapproximationerror.\nSeeingthatthetotalerrorismainlyround-offerror ‚àù‚àö\nN,anobviouswaytodecrease\ntheerroristouseasmallernumberofsteps N.Letusassumewedothisbyfindinganother\nalgorithmthatconvergesmorerapidlywith N,forexample,onewithapproximationerror\nbehavinglike\nùúñapp‚âÉ2\nN4. (3.32)",8512
27-3.4 Errors in Bessel Functions.pdf,27-3.4 Errors in Bessel Functions,"52 3 Errors and Uncertainties\nThetotalerrorisnow\nùúñtot=ùúñro+ùúñapp‚âÉ2\nN4+‚àö\nNùúñm. (3.33)\nThenumberofpointsforminimumerrorisfoundasbefore:\ndùúñtot\ndN=0‚áíN9‚àï2‚áíN‚âÉ67‚áíùúñtot‚âÉ9√ó10‚àí7. (3.34)\nTheerrorisnowsmallerbyafactorof4,withonly1/16asmanystepsneeded.Subtleare\nthewaysofthecomputer.Inthiscase,thebetteralgorithmisquickerand,byusingfewer\nsteps,produceslessround-offerror.\nExercise Estimatetheerrorforadouble-precisioncalculation.\n3.3 Errors with Power Series\nA classic numerical problem is the summation of a series to evaluate a function. As an\nexample,considertheinfiniteseriesforsin x:\nsinx=x‚àíx3\n3!+x5\n5!‚àíx7\n7!+¬∑¬∑¬∑ ( exact). (3.35)\nYourproblemis to use just this series to calculate sin xforx<2ùúãandx>2ùúã, with an\nabsolute error in each case of less than 1 part in 108. While in a mathematical sense an\ninfiniteseriesisexactandalwaysconverges,itisnotanalgorithmbecausecomputerscan‚Äôt\nsumaninfinitenumberofterms.Analgorithmwouldbethefinitesum\nsinx‚âÉN‚àë\nn=1(‚àí1)n‚àí1x2n‚àí1\n(2n‚àí1)!(algorithm ). (3.36)\nButhowdowedecidewhentostopsumming?(Donoteventhinkofsaying,‚ÄúWhenthe\nansweragreeswithatableorwiththebuilt-inlibraryfunction.‚Äù)Oneapproachwouldbe\ntostopsummingwhenthenexttermissmallerthantheprecisiondesired.Clearlythen,\nifxislargethiswouldrequireverylarge N.Infact,forverylarge x,onewouldhavetogo\nfaroutintheseriesbeforethetermsevenstarttodecrease,letalonetheseriesasawhole\nconverges.\nWeshouldalsobewaryofthealgorithm(3.36)becauseitwouldhaveuscalculate x2n‚àí1\nand then divide that by (2n‚àí1)!. This is not good computation. On the one hand, both\n(2n‚àí1)!andx2n‚àí1canindividuallygetverylargeandtherebycauseoverflows,despitethe\nfact that their quotient may be small. On the other hand, powers and factorials are very\nexpensive(time-consuming)toevaluateonthecomputer.Consequently,abetterapproach\nistouseasinglemultiplicationtorelatethenexttermintheseriestothepreviousone:\n(‚àí1)n‚àí1x2n‚àí1\n(2n‚àí1)!=‚àíx2\n(2n‚àí1)(2n‚àí2)(‚àí1)n‚àí2x2n‚àí3\n(2n‚àí3)!\n‚áínthterm=‚àíx2\n(2n‚àí1)(2n‚àí2)√ó(n‚àí1)thterm. (3.37)\nWhile we might want to insure absolute accuracy for sin x, that is not easy to do. What\niseasytodoistoassumethattheerrorinthesummationisapproximatelythelastterm\n3.3 Errors with Power Series 53\nsummed(thisassumesnoround-offerror).Toobtainarelativeerrorof1partin108,we\nthenwouldstopthecalculationwhen\n||||nthterm\nsum||||<10‚àí8, (3.38)\nwhere‚Äúterm‚Äùisthelasttermkeptintheseries(3.36)and‚Äúsum‚Äùistheaccumulatedsumof\nalltheterms.Ingeneral,youarefreetopickanytolerancelevelyoudesire,althoughifitis\ntoocloseto,orsmallerthan,machineprecision,yourcalculationmaynotbeabletoattain\nit.Apseudocodeforperformingthesummationis\nterm = x, sum= x, eps = 10^( ‚àí8) # Initialize do\n2do term = ‚àíterm ‚àóx‚àóx/(2n‚àí1)/(2 ‚àón‚àí2); #N e ww r to l d\nsum=sum+t e r m #A d dt e r m\nwhile abs (term/sum)>e p s # Break iteration\nend do\n3.3.1 Implementation and Assessment\n1) Write a program that implements this pseudocode for the indicated xvalues. Start\nwith a tolerance of 10‚àí8as in (3.38). Present the results as a table with headings\nxNs u m |sum‚Äìsin(x) |/sin(x),where sin(x)isthevalueobtainedfromthebuilt-in\nfunction Math.sin(x) (you may assume that the built-in function is exact). The last\ncolumnhereistherelativeerrorinyourcomputation.\n2) Showthatforsufficientlysmallvaluesof x,youralgorithmconverges(thechangesin\nsumaresmallerthanyourtolerancelevel)andthattheseriesconvergestothecorrect\nanswer.\n3) Comparethenumberofdecimalplacesofprecisionobtainedwiththatexpectedfrom\n(3.38).\n4) Withoutusingtheidentitysin (x+2nùúã)=sin(x),showthatthereisarangeofsome-\nwhatlargevaluesof xforwhichthealgorithmconverges,butthatitconvergestothe\nwronganswer.\n5) Observe how significant subtractive cancelations occur when large terms are added\ntogether to give small answers. In particular, print out the near-perfect cancellation\naroundn‚âÉx‚àï2.\n6) Showthatasyoukeepincreasing x,youwillreacharegimewherethealgorithmstops\nconverging.\n7) Nowmakeuseoftheidentitysin (x+2nùúã)=sin(x)tocomputesin xforlargexvalues\nwheretheseriesotherwisewoulddiverge.\n8) Byprogressivelyincreasing xfrom1to10,andthenfrom10to100,useyourprogramto\ndetermineexperimentallywhentheseriesstartstoloseaccuracyandwhenitnolonger\nconverges.\n9) Makeaseriesofgraphsoftheerror versusNfordifferentvaluesof x.Youshouldget\ncurvessimilartothoseinFigure3.3.\n10) Repeatthecalculationusinga‚Äúbad‚Äùversionofthealgorithm(onethatcalculatesfac-\ntorials)andcomparetheanswers.\n11) Setyourtoleranceleveltoanumbersmallerthanmachineprecisionandseehowthis\naffectsyourconclusions.\n54 3 Errors and Uncertainties\n05 1 0 1 5 2 0\nNumber of terms in series1e-091e-060.001110001e+06Error\nFigure 3.3 The error in the summation of the series for e‚àíxversus N for various xvalues. The\nvalues of xincrease vertically for each curve. Note that a negative initial slope corresponds to\ndecreasing algorithmic error with N, and that the dip indicates a rapid convergence followed by a\nrapid increase in error. (courtesy of J. Wiren.)\nNotethatbecausethisseriessummationissuchasimple,correlatedprocess,theround-off\nerrordoesnotaccumulaterandomlyasitmightforamorecomplicatedcomputation,and\nwedonotobtaintheerrorbehavior(3.25).Wewillseethepredictederrorbehaviorwhen\nweexamineintegrationinChapter5.\n3.3.2 Error in Specular ReÔ¨Çection\nForaperfectlyreflectingsurface,thebasiclawofopticstellsusthattheangleofincidence\nequalstheangleofreflection(Figure3.4left.Ifnolightisabsorbedduringareflection,a\nlightraywouldcontinuetoreflectendlessly(Figure3.4right).Withanoriginplacedatthe\ncenterofthecircularmirror,welocatetheraybytheangle ùúÉ.Foraninitialangle ùúô<ùúã,the\nangleincreasesby2 ùúôaftereachreflection:\nùúÉnew=ùúÉold+2ùúô. (3.39)\nŒ∏\nœï\nœï\nFigure 3.4 Left: Specular reÔ¨Çection within a circular mirror in which the incident angle equals the\nangle of reÔ¨Çection. Right: InÔ¨Ånite internal reÔ¨Çections between two circular mirrors.",5824
28-3.4.1 Numerical Recursion Method.pdf,28-3.4.1 Numerical Recursion Method,"3.4 Errors in Bessel Functions 55\nAlthoughthisappearstoindicatethat ùúÉincreasesendlessly,theadditionorsubtractionof\n2ùúãtoùúÉdoesnotchangethelocationonthecircle,andsoif ùúô‚àïùúãisarationalnumber,\nùúô\nùúã=n\nm, (3.40)\ntheraywillfalluponitselfandformageometricfigure(Figure3.4right).\n1) Determinethepathfollowedbyalightrayforaperfectlyreflectingmirror.\n2) Plotthelighttrajectoriesforarangeofvaluesfortheinitialangle ùúô.\n3) Repeatthepreviouscalculationusingjustfourplacesofprecision.Youcandothisby\nusingthePythoncommand round,forinstance, round(1.234567,4) = 1.234. Youshould\nfindthatasignificantrelativeerroraccumulates.Asinlargeandcomplicatedcalcula-\ntionswithmanystepsandfiniteprecision,thistypeoferrorincreasesasthenumberof\ncalculationalstepsincreases.\n3.4 Errors in Bessel Functions\nAccumulatinground-offerrorsoftenlimitstheabilityofaprogramtocalculateaccurately.\nYourproblemistocomputethesphericalBesselandNeumannfunctions jl(x)andnl(x).\nThesefunctionsare,respectively,theregular/irregular(nonsingular/singularattheorigin)\nsolutionsofthedifferentialequation\nx2f‚Ä≤‚Ä≤(x)+2xf‚Ä≤(x)+[x2‚àíl(l+1)]f(x)=0. (3.41)\nThe spherical Bessel functions are related to the Bessel function of the first kind by\njl(x)=‚àö\nùúã‚àï2xJn+1‚àï2(x).Theyoccurinmanyphysicalproblems,suchastheexpansionofa\nplanewaveintosphericalpartialwaves,\neik‚ãÖr=‚àû‚àë\nl=0il(2l+1)jl(kr)Pl(cosùúÉ). (3.42)\nFigure3.5showswhatthefirstfew jllookslike,andTable3.1givessomeexplicitvalues.\nForthefirsttwo lvalues,theexplicitformsare\nj0(x)=+sinx\nx,j1(x)=+sinx\nx2‚àícosx\nx(3.43)\nn0(x)=‚àícosx\nx.n1(x)=‚àícosx\nx2‚àísinx\nx. (3.44)\n3.4.1 Numerical Recursion (Method)\nTheclassicwaytocalculate jl(x)wouldbebysummingitspowerseriesforsmallvaluesof\nx‚àïlandsummingitsasymptoticexpansionforlarge x‚àïlvalues.Theapproachweadopthere\nisbasedonthe recursionrelations\njl+1(x)=2l+1\nxjl(x)‚àíjl‚àí1(x),(up), (3.45)\njl‚àí1(x)=2l+1\nxjl(x)‚àíjl+1(x),(down). (3.46)\n56 3 Errors and Uncertainties\n0.0 2.0 4.0 6.0 8.0 10.0 12.0\nx0.0\n‚Äì0.20.20.40.60.81.0jl (x)l = 0\nl = 1\nl = 3\nFigure 3.5 The Ô¨Årst four spherical Bessel functions jl(x) as functions of x. Notice that for small x,\nthe values for increasing lbecome progressively smaller.\nTable 3.1 Approximate values for spherical Bessel functions\n(from Maple).\nxj3(x) j5(x) j8(x)\n0.1+9.51851971910‚àí6+9.61631023110‚àí10+2.90120010210‚àí16\n1+9.00658111810‚àí3+9.25611586210‚àí05+2.82649880210‚àí08\n10‚àí3.94958449810‚àí2‚àí5.55345116210‚àí02+1.25578023610‚àí01\nEquations (3.45) and (3.46) are the same mathematical relation, one written for upward\nrecurrence from small to large lvalues, and the other for downward recurrence from\nlargelto smalll. We shall see that with just a few additions and multiplications, recur-\nrence relations permit rapid, simple computation of the entire set of jlvalues for fixed x\nandalll.\nTorecurupwardin lforfixedx,westartwiththeknownformsfor j0andj1(3.43)anduse\n(3.45).Asyouwillproveforyourself,thisupwardrecurrenceusuallyseemstoworkatfirst,\nbutthenfails.Thereasonforthefailurecanbeseenfromtheplotsof jl(x)andnl(x)versus\nx(Figure3.5).Ifwestartat x‚âÉ2andl=0,weseethataswerecur jluptolarger lvalues\nwith (3.45), we are essentially taking the difference of two ‚Äúlarge‚Äù functions to produce\na ‚Äúsmall‚Äù value for jl. This process suffers from the dreaded subtractive cancelation that\nalways reduces precision. As we continue recurring, we take the difference of two small\nfunctions,eachwithlargeerrors,andproduceayetsmallerfunctionwithayetlargererror.\nAfterawhile,weareleftwithonlyround-offerror(garbage).",3502
29-Chapter 4 Monte Carlo Simulations.pdf,29-Chapter 4 Monte Carlo Simulations,"3.4 Errors in Bessel Functions 57\nTobemorespecific,letuscall j(c)\nlthenumericalvaluewecomputeasanapproximation\nforjl(x).Evenifwestartwithpure jl,afterashortwhilethecomputer‚Äôslackofprecision\neffectivelymixesinabitof nl(x):\nj(c)\nl=jl(x)+ùúñnl(x). (3.47)\nThisisinevitablebecauseboth jlandnlsatisfythesamedifferentialequationand,onthat\naccount,thesamerecurrencerelation.Theadmixtureof nlbecomesaproblemwhenthe\nnumericalvalueof nl(x)ismuchlargerthanthatof jl(x)becauseevenaminusculeamount\nofaverylargenumbermaybelarge.\nThesimplesolutiontothisproblem( Miller‚Äôsdevice )istouse(3.46)fordownwardrecur-\nsion ofthe jlvalues starting at a largevalue l=L. This avoidssubtractive cancelation by\ntakingsmallvaluesof jl+1(x)andjl(x)andproducingalarger jl‚àí1(x)byaddition.Whilethe\nerror may still behave like a Neumann function, the actual magnitude of the error will\ndecreasequickly as we move downward to smaller lvalues. In fact, if we start iterating\ndownward with arbitrary values for j(c)\nL+1andj(c)\nL, after a short while we will arrive at the\ncorrectldependenceforthisvalueof x.Althoughtheprecisevalueof j(c)\n0soobtainedwill\nnotbecorrectbecauseitdependsuponthearbitraryvaluesassumedfor j(c)\nL+1andj(c)\nL,the\nrelativevalueswillbeaccurate.Theabsolutevaluesarefixedfromtheknownvalue(3.43),\nj0(x)=sinx‚àïx.Becausetherecurrencerelationisalinearrelationbetweenthe jlvalues,we\nneedonlynormalizeallthecomputedvaluesvia\njN\nl(x)=jc\nl(x)√ójanal\n0(x)\njc\n0(x). (3.48)\nAccordingly,afteryouhavefinishedthedownwardrecurrence,youobtainthefinalanswer\nbynormalizingall j(c)\nlvaluesbasedontheknownvaluefor j0.\n3.4.2 Implementation and Assessment: Recursion Relations\nA program implementing recurrence relations is most easily written using subscripts.\nIfyouneedtopolishuponyourskillswithsubscripts,youmaywanttostudyourprogram\nBessel.pyinListing3.1beforewritingyourown.\n1) Writeaprogramthatuses bothupwardanddownwardrecursion to calculate jl(x)for\nthefirst25 lvaluesforx=0.1,1,and10.\n2) Tuneyourprogramsothatatleastonemethodgives‚Äúgood‚Äùvalues(meaningarelative\nerror‚âÉ10‚àí10).SeeTable3.1forsomesamplevalues.\n3) Showtheconvergenceandstabilityofyourresults.\n4) Comparetheupwardanddownwardrecursionmethods,printingout l,j(up)\nl,j(down)\nl,and\ntherelativedifference |j(up)\nl‚àíj(down)\nl|‚àï(|j(up)\nl|+|j(down)\nl|).\n5) Theerrorsincomputationdependon x,andforcertainvaluesof x,bothupanddown\nrecursionsgivesimilaranswers.Explainthereasonforthis.\n58 3 Errors and Uncertainties\n3.5 Code Listing\nListing 3.1 Bessel.py Determines spherical Bessel functions by downward recursion\n(youshouldmodifythistoalsoworkbyupwardrecursion).\n# Bessel.py\nfromvisualimport ‚àó\n3fromvisual.graph import ‚àó\nXmax = 40.\nXmin = 0.25\n7step = 0.1 # Global class variables\norder = 10; start = 50 # Plot j_order\ngraph1 = gdisplay(width = 500, height = 500, title = ‚ÄôSperical Bessel, \\nL = 1 (red), 10‚Äô ,xtitle = ‚Äôx‚Äô, ytitle = ‚Äôj(x)‚Äô,\\n11 xmin=Xmin,xmax=Xmax,ymin= ‚àí0.2,ymax=0.5)\nfunct1 = gcurve(color=color.red)\nfunct2 = gcurve(color=color.green)\n15defdown (x, n, m): # Method down, recurs downward\nj = zeros( (start + 2), float)\nj[m + 1] = j[m] = 1. # Start with anything\nforkin range (m, 0, ‚àí1):\n19 j[k‚àí1] = ( (2. ‚àók+1 . )/ x ) ‚àój[k]‚àíj[k+ 1]\nscale = (sin(x)/x)/j[0] # Scale solution to known j[0]\nreturnj[n] ‚àóscale\n23forxinarange(Xmin, Xmax, step):\nfunct1.plot(pos = (x, down(x, order, start)))\nforxinarange(Xmin, Xmax, step):\n27funct2.plot(pos = (x, down(x,1,start)))",3468
30-4.1.1 Random Number Generation.pdf,30-4.1.1 Random Number Generation,"59\n4\nMonte Carlo Simulations\nThis chapter starts with a discussion of how computers generate numbers that appear\nrandom, but really aren‚Äôt, and how we can test for that. We then explore how these\npseudorandom numbers are used to incorporate the element of chance into simulations. We\ndo this Ô¨Årst by simulating a random walk, and then by simulating the spontaneous decay of\nan atom or nucleus. In Section 5.5, we show how to use these random numbers to evaluate\nintegrals, and in Chapter 17, we investigate the use of random numbers to simulate thermal\nprocesses and the Ô¨Çuctuations inherent in quantum systems .\nSomepeopleareattractedtocomputingbecauseofitsdeterministicnature;it‚Äôsnicetohave\naplaceinone‚Äôslifewherenothingislefttochance.Barringmachineerrorsorundefined\nvariables,yougetthesameoutputeverytimeyoufeedyourprogramthesameinput.Nev-\nertheless, many computer cycles are used for Monte Carlo calculations that at their very\ncore include elements of chance. These are calculations in which random-like numbers\ngenerated by the computer are used to simulatenatural random processes, such as ther-\nmal motion or radioactive decay, or to solve equations on the average. Indeed, much of\ncomputationalphysics‚Äôgreatachievementshavecomeaboutfromtheabilityofcomputers\ntosolvepreviouslyintractableproblemsusingtheseso-calledMonteCarlotechniques.\n4.1 Random Numbers\nW edefinea sequencer1,r2,‚Ä¶asrandomiftherearenocorrelationsamongthenumbers.\nYetbeingrandomdoesnotmeanthatallthenumbersinthesequenceareequallylikely\ntooccur.Ifallthenumbersinasequenceareequallylikelytooccur,thenthesequenceis\ncalleduniform,whichdoesn‚Äôtnecessarilymeanthatitisrandom.Toillustrate,1,2,3,4, ‚Ä¶\nisuniform,butprobablynotrandom.Further,itispossibletohaveasequenceofnumbers\nthat,insomesense,arerandombuthaveveryshort-rangecorrelationsamongthemselves,\nforexample,\nr1,(1‚àír1),r2,(1‚àír2),r3,(1‚àír3),‚Ä¶ (4.1)\nhaveshort-rangebutnotlong-rangecorrelations.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n604 Monte Carlo Simulations\nMathematically,thelikelihoodofanumberoccurringisdescribedbyadistributionfunc-\ntionP(r),whereP(r)dris the probability of finding rin the interval [r,r+dr].Auni-\nformdistributionmeansthat P(r)=aconstant.Thestandardrandom-numbergeneratoron\ncomputersgeneratesuniformdistributionsbetween0and1.Inotherwords,thestandard\nrandom-numbergeneratoroutputsnumbersinthisinterval,eachwithanequalprobabil-\nity,yeteachindependentofthepreviousnumbers.Asweshallsee,numberscanalsobe\nmorelikelytooccurincertainregionsthanother,yetstillberandom.\nBy their very nature, computers, being deterministic devices, cannot generate random\nnumbers.Becausecomputedrandomnumbermustcontaincorrelations,theyarenottruly\nrandom. Although it may be a bit of work, if we know a computed random number rm\nanditsprecedingnumbers,thenitshouldbepossibletofigureout rm+1.Forthisreason,\ncomputersaresaidtogenerate pseudorandomnumbers (yetwithourincurablelazinesswe\nwon‚Äôtbothersaying‚Äúpseudo‚Äùallthetime).Whilemoresophisticatedgeneratorsdoabet-\nter job at hiding the correlations, experience shows that if you look hard enough, or use\npseudorandomnumberslongenough,youwillbeabletodiscerncorrelations.Aprimitive\nalternative to generating random numbers is to read in a table of truly random numbers\ngeneratedbynaturallyrandomprocessessuchasradioactivedecay,ortoconnectthecom-\nputertoanexperimentaldevicethatmeasuresrandomevents.Thesealternativesarenot\nidealforproductionwork,buthaveactuallybeenusedasacheckintimesofdoubt.\n4.1.1 Random Number Generation\nThelinear congruent orpower residue method is the common way of generating a pseu-\ndorandomsequenceofnumbers0 ‚â§ri‚â§M‚àí1overtheinterval [0,M‚àí1].Toobtainthe\nnext random number ri+1, you multiply the present random number riby the constant\na, add another constant c, take themodulusbyM, and then keep just the fractional part\n(remainder):1\nri+1def=(ari+c)modM=remainder(ari+c\nM)\n. (4.2)\nThevaluefor r1(theseed)isfrequentlysuppliedbytheuser.The modoperatorisusually\nbuiltintothesoftware,forexample,inPythonit‚Äôsthepercentsign%. Remaindering isessen-\ntiallyabit-shiftoperationthatendsupwiththeleastsignificantpartoftheinputnumber,\nandtherebycountsontherandomnessofround-offerrorstogeneratearandomsequence.\nForexample, c=1,a=4,M=9,andr1=3producesthesequence\nr1=3, (4.3)\nr2=(4√ó3+1)mod9=13mod9 =rem13\n9=4, (4.4)\nr3=(4√ó4+1)mod9=17mod9 =rem17\n9=8, (4.5)\nr4=(4√ó8+1)mod9=33mod9 =rem33\n9=6, (4.6)\nr5‚àí10=7,2,0,1,5,3. (4.7)\n1 Youmayobtainthesameresultforthemodulusoperationbysubtracting Muntilanyfurther\nsubtractionswouldleaveanegativenumber;whatremainsisthe remainder.\n4.1 Random Numbers 61\nWethusobtainasequenceoflength M=9,afterwhichtheentiresequencerepeats.Ifwe\nwantnumbersintherange [0,1],wedividethe r‚ÄôsbyM=9:\n0.333,0.444,0.889,0.667,0.778,0.222,0.000,0.111,0.555,0.333.(4.8)\nThisisstillasequenceoflength9,butisnolongerasequenceofintegers.Ifrandomnum-\nbersintherange [A,B]areneeded,youonlyneedto scale:\nxi=A+(B‚àíA)ri,0‚â§ri‚â§1,‚áíA‚â§xi‚â§B. (4.9)\nAsaruleofthumb: Beforeusingarandom-numbergeneratorinyourprograms,youshould\ncheck its range and that it produces numbers that ‚Äúlook‚Äù random (we‚Äôll explain further).\nAlthoughnotamathematicalproof,youshouldalwaysmakeagraphicaldisplayofyour\nrandomnumbers.Yourvisualcortexisquiterefinedatrecognizingpatternsandwilltell\nyou immediately if there is a pattern in your random numbers. For instance, Figure 4.1\nshowsgeneratedsequencesfrom‚Äúgood‚Äùand‚Äúbad‚Äùgenerators.Itisclearwhichisnotran-\ndom(althoughifyoulookhardenoughattherandompoints,yourmindmaywellpickout\npatternstheretoo).\nThelinearcongruentmethod(4.2)producesintegersintherange [0,M‚àí1]andtherefore\nbecomescompletelycorrelatedifaparticularintegercomesupasecondtime(thewhole\ncyclethenrepeats).Inordertoobtainalongersequence, aandMshouldbelargenumbers,\nbutnotsolargethattheproduct ari‚àí1overflows.Onacomputerusing48-bitintegerarith-\nmetic,thebuilt-inrandom-numbergeneratormayuse Mvaluesaslargeas248‚âÉ3√ó1014.A\n32-bitgeneratormayuse M=231‚âÉ2√ó109.Ifyourprogramusesapproximatelythismany\nrandomnumbers,youmayneedtoreseed(startthesequenceoveragainwithadifferent\ninitialvalue)duringintermediatestepstoavoidthecyclerepeating.\nYour computer probably has random-number generators that are better than the one\nyouwillcomputewiththepowerresiduemethod.InPythonweuse random.random() ,the\nMesennaTwistergenerator.Werecommendthatyouusethebestoneyoucanfindrather\nthan write your own. To initialize a random sequence, you need to plant a seed in it. In\n0 50 100 150 200 250\nx050100150200250y\n0 50 100 150 200 250\nx\nFigure 4.1 Left: A plot of successive random numbers (x,y)=(ri,ri+1)generated with a\ndeliberately ‚Äúbad‚Äù generator. Right: A plot generated with the built in random number generator.\nWhile the plot on the right is not proof that the distribution is random, the plot on the left is proof\nenough that the distribution is not random.",6996
31-4.2.2 Random Walks in a Brain.pdf,31-4.2.2 Random Walks in a Brain,"624 Monte Carlo Simulations\nPython the statement random.seed(None) seeds the generator with the system time (see\nWalk.pyinListing4.1).\nM=248,c=B(base16)=13(base8), (4.10)\na=5DEECE66D (base16)=273673163155 (base8). (4.11)\n4.1.2 Computing a Random Sequence\nForscientificworkwerecommendusinganindustrial-strengthrandom-numbergenerator.\nTo see why, here we assess how bada careless application of the power residue method\ncanbe.\n1) Write a simple program to generate random numbers using the linear congruent\nmethod(4.2).\n2) For pedagogical purposes, try the unwise choice: (a,c,M,r1)=(57,1,256,10). Deter-\nminetheperiod,thatis,howmanynumbersaregeneratedbeforethesequencerepeats.\n3) Takeyourpedagogicalsequenceofrandomnumbersandlookforcorrelationsbyobserv-\ningclusteringonaplotofsuccessivepairs (xi,yi)=(r2i‚àí1,r2i),i=1,2,‚Ä¶.(Donotcon-\nnectthepointswithlines.)Youmay‚Äúsee‚Äùcorrelations(Figure4.1),whichmeansthat\nyoushouldnotusethissequenceforseriouswork.\n4) MakeyourownversionofFigure4.2;thatis,plot riversusi.\n5) Testthebuilt-inrandom-numbergeneratoronyourcomputerforcorrelationsbyplot-\ntingthesamepairsasabove.(Thisshouldbegoodforseriouswork.)\n00.20.40.60.81\n0 20 40 60 80 100Random number r\nSequence number\nFigure 4.2 A plot of a uniform pseudorandom sequence riversus i . The points are connected to\nmake it easier to follow the order. While this does not prove that a distribution is random, it at least\nshows the range of values and that there is Ô¨Çuctuation.\n4.2 Simulating a Random Walk 63\n6) Testthelinearcongruentmethodagainwithreasonableconstantslikethosein(4.10)\nand(4.11).Comparethescatterplotyouobtainwiththatofthebuilt-inrandom-number\ngenerator.(Thisshouldbegoodforsemi-seriouswork.)\n4.2 Simulating a Random Walk\nConsideraperfumemoleculereleasedinthefrontofaclassroom.Sooneveryonecansmell\nit.Amoleculecollidesrandomlywithothermoleculesintheairandeventuallyreachesyour\nnosedespitethefactthatyouarehiddeninthelastrowbehindanewspaper.Your problem\nistodeterminehowmanycollisions,ontheaverage,aperfumemoleculemakesintraveling\nadistanceR.Youaregiventhefactthatamoleculetravelsanaverage( root-mean-square )\ndistancerrmsbetweencollisions.\nThereareanumberofwaystosimulatearandomwalkwith(surprise,surprise)different\nassumptionsleadingtodifferentbehaviors.Wewillpresentasimplemodelfora2Dwalk,\nandendupwithamodelfor normaldiffusion .Theresearchliteratureisfullofdiscussionsof\nvariousversionsofarandomwalk.Forexample,Browningmotioncorrespondstothelimit\ninwhichtheindividualsteplengthsapproachzero,andwithnotimedelaybetweensteps.\nAdditionalrefinementsincludecollisionswithinamovingmedium( abnormaldiffusion ),\nincludingthevelocitiesoftheparticles,orevenpausingbetweensteps.Modelssuchasthese\narediscussedinChapter14, Fractals&StatisticalGrowth .\nIn our random-walk simulation (Figure 4.3) an artificial walkertakes sequential steps\nwiththedirectionofeachstep independent ofthedirectionofthepreviousstep.Westartat\ntheoriginandtake Nstepsinthe XYplaneoflengths(notcoordinates)\n(Œîx1,Œîy1),(Œîx2,Œîy2),(Œîx3,Œîy3),‚Ä¶,(ŒîxN,ŒîyN). (4.12)\nAlthougheachstepmaybeinadifferentdirection,thedistancesalongeachCartesianaxis\njust add algebraically. Accordingly, the radial distance Rfrom the starting point after N\nstepsis\nR2=( Œîx1+Œîx2+¬∑¬∑¬∑+ŒîxN)2+(Œîy1+Œîy2+¬∑¬∑¬∑+ŒîyN)2\n=Œîx2\n1+Œîx2\n2+¬∑¬∑¬∑+Œîx2\nN+2Œîx1Œîx2+2Œîx1Œîx3+2Œîx2Œîx1+¬∑¬∑¬∑\n+(x‚Üíy). (4.13)\nIfthewalkisrandom,theparticleisequallylikelytotravelinanydirectionateachstep.\nIfwetaketheaverageofalargenumberofsuchrandomsteps,allthecrosstermsin(4.13)\nFigure 4.3 Left: A schematic of the Nsteps\nin a random walk simulation that end up a\ndistance Rfrom the origin. Notice how the\nŒîx‚Äôs for each step add vectorially. Right:A\nsimulated walk in 3D from Walk3D.py .\nX Y\nZR\n1Œîy1\nŒîy2Œîx1\n2\n34N\n644 Monte Carlo Simulations\nwillvanish,andwewillbeleftwith\nR2\nrms=‚ü®R2‚ü©‚âÉ‚ü®Œîx2\n1+Œîx2\n2+¬∑¬∑¬∑+Œîx2\nN+Œîy2\n1+Œîy2\n2+¬∑¬∑¬∑+Œîy2\nN‚ü©\n=‚ü®Œîx2\n1+Œîy2\n1‚ü©+‚ü®Œîx2\n2+Œîy2\n2‚ü©+¬∑¬∑¬∑\n=N‚ü®r2‚ü©=Nr2\nrms,\n‚áíRrms‚âÉ‚àö\nNrrms, (4.14)\nwhererrms=‚àö\n‚ü®r2‚ü©istheroot-mean-square(RMS) stepsize.\nTosummarize,ifthewalkisrandom,thenweexpectthatafteralargenumberofsteps\ntheaverage vectordistancefromtheoriginwillvanish:\n‚ü®‚ÉóR‚ü©=‚ü®x‚ü©‚Éói+‚ü®y‚ü©‚Éój‚âÉ0. (4.15)\nYetRrms=‚àö\n‚ü®R2\ni‚ü©doesnotvanish.Equation(4.14)indicatesthattheaverage scalardistance\nfrom the origin is‚àö\nNrrms, where each step is of average length rrms. In other words, the\nvector endpoint will be distributed uniformly in all quadrants, and so the displacement\nvectoraveragestozero,buttheaveragelengthofthatvectordoesnot.Forlarge Nvalues,‚àö\nNrrms‚â™Nrrms(thevalueifallstepswereinonedirectiononastraightline),butdoes\nnotvanish.Inourexperience,computationalsimulationsagreewiththistheory,butrarely\nperfectly,withthelevelofagreementdependinguponthedetailsofhowtheaveragesare\ntakenandhowtherandomnessisbuiltintoeachstep.\n4.2.1 Random Walk Implementation\nTheprogram Walk.pyinListing4.1isasamplerandom-walksimulation.Itskeyelementis\ntherandomvaluesforthe xandycomponentsofeachstep,\nx += (random.random() ‚àí0.5) ‚àó2. #‚àí1=<x=<1\ny += (random.random() ‚àí0.5) ‚àó2. #‚àí1=<y=<1\nwhere here we have omitted the scaling factor that normalizes each step to length 1.\nWhenusingyourcomputertosimulatearandomwalk,youshouldexpecttoobtain(4.14)\nonlyastheaveragedisplacement,averagedovermanytrials,notnecessarilyastheanswer\nfor each trial. You final answer will depend on just how you take your random steps\n(Figure4.4right).\nStartattheoriginandtakea2Drandomwalkwithyourcomputer:\n1) To increase the amount of randomness, independently choose random values for Œîx‚Ä≤\nandŒîy‚Ä≤intherange [‚àí1,1].Thennormalizethemsothateachstepisofunitlength\nŒîx=1\nLŒîx‚Ä≤,Œîy=1\nLŒîy‚Ä≤,L=‚àö\nŒîx‚Ä≤2+Œîy‚Ä≤2. (4.16)\n2) Useaplottingprogramtodrawmapsofseveralindependent2Drandomwalks,eachof\n1000steps.Basedonyoursimulations,commentonwhethertheresultslooklikewhat\nyouwouldexpectarandomwalktolooklike.\n3) If you have your walker taking Nsteps in a single trial, then conduct a total number\nK‚âÉ‚àö\nNoftrials.Eachtrialshouldhave Nstepsandstartwithadifferentseed.\n4.2 Simulating a Random Walk 65\n7 Random walks\nDistance vs Steps\n0100200300\n0 100 200 300\nsqrt(N)R\n‚Äì40.0‚Äì40.0‚Äì20.020.040.0\n0.0\n‚Äì20.0 0.0 20.0 40.0\nFigure 4.4 Left: The steps taken in seven 2D random walk simulations. Right: The distance\ncovered in two walks of Nsteps using different schemes for including randomness. The theoretical\nprediction (4.14) is the straight line.\n4) Calculatethemeansquaredistance R2foreachtrialandthentaketheaverageof R2for\nallyourKtrials:\n‚ü®R2(N)‚ü©=1\nKK‚àë\nk=1R2\n(k)(N). (4.17)\n5) Checkthevalidityoftheassumptionsmadeinderivingthetheoreticalresult(4.14)by\ncheckinghowwell\n‚ü®ŒîxiŒîxj‚â†i‚ü©\nR2‚âÉ‚ü®ŒîxiŒîyj‚ü©\nR2‚âÉ0. (4.18)\nDoyourcheckingforbothasingle(long)runandfortheaverageovertrials.\n6) PlottheRMSdistance, Rmrs=‚àö\n‚ü®R2(N)‚ü©asafunctionof‚àö\nN.V aluesof Nshouldstart\nwithasmallnumber,where R‚âÉ‚àö\nNisnotexpectedtobeaccurate,andendataquite\nlargevalue,wheretwoorthreeplacesofaccuracyshouldbeexpectedontheaverage.\n7)‚äôRepeattheprecedingandfollowinganalysisfora3Dwalkaswell.\n4.2.2 Random Walks in a Brain\nIthasrecentlybeenrealizedthatunderstandingthebraingoesbeyondjustunderstanding\nthenetworksofneuronsinit,toalsounderstandingtheeffectsofthefluid-filledextracellu-\nlarspacesbetweentheneurons[Nicholson,2022].2Thisisimportantforunderstandingthe\nmolecular diffusion of radiographers, drugs, metabolites, and molecular signals within\nthebrain.\n2 FurtherdiscussionofthebrainmaybefoundinChapter11, NeuralNetsandArtificialIntelligence .\n664 Monte Carlo Simulations\nFigure 4.5 Fifty 2D random walk simulations exploring the diffusion of chemical probes within\nthe brain from ([Nicholson, 2022] with permission from AIP publishing). The walks take 1500\nequal-size steps with each walk assigned to one of six colors. Left: The walks with no impediments.\nRight: Circular impediments representing extracellular spaces that block out regions inaccessible to\nthe walks.\nRandom-walksimulations,liketheoneswehavealreadyexamined,haveprovidedunder-\nstandingofdiffusionwithinthebrain.Specifically,theleftofFigure4.5presentsresearch\nresultsfor50randomwalksof1500equal-sizestepswithinabrainmodel,witheachwalk\nassignedtooneofsixcolorsNicholson[2022].Notethestrikingsimilarityofthesewalksto\nthosewehaveshownontheleftofFigure4.4.OntherightofFigure4.5,weshowtheresults\nof a model for diffusion in the brain that accounts for the extracellular spaces between\nneuronsbyrandomlyplacingcircularobstructionswithinthesimulationvolume.\n1) TrytoreproducethesimulationshownontheleftofFigure4.5byrecordingandplotting\n50walks,witheachwalkassignedtooneofsixcolors.Startthewalksattheorigin,use\nequal-sizedsteps,andrestrictthesimulationspacetotwodimensions.\n2) As shown of the right of Figure 4.4, determine the average over all your walks of the\nRMSdistancecovered Rrms.\n3) Takethesame2Dspacecoveredinyoursimulations,andnowinsertcircularobstruc-\ntionsofvariedsizes,similartothoseontherightofFigure4.5.\n4) Yet again conduct and record 50 walks, with each walk assigned to one of six colors.\nStartthewalksattheorigin,useequal-sizedsteps,butincludeobstructionsthat stopthe\nwalkswhentheyhitthem.\n5) DetermineagaintheaverageoverallyourobstructedwalksoftheRMSdistancecovered\nRrms.Theobstructionsshouldleadtoadecreased Rrms.\n6) Againconductandrecord50walks,witheachwalkassignedtooneofsixcolors.Start\nthe walks at the origin, use equal-sized steps, but include obstructions that repel,b u t\ndon‚Äôtstop,thewalkswhentheyhit.\n7) AgaindeterminetheaverageoverallyourobstructedwalksoftheRMSdistancecovered\nRrms,andcomparetotheprevioustworesults.",9549
32-4.3.2 The Exponential Decay Approximation.pdf,32-4.3.2 The Exponential Decay Approximation,"4.2 Simulating a Random Walk 67\nFigure 4.6 Two self-avoiding random walks that simulate protein chains with hydrophobic (H)\nmonomers in large dots, and polar (P) monomers in small dots. The dark dots on the right indicate\nregions where two H monomers are not directly connected.\n8) InEinstein‚Äôs1905paper InvestigationsontheTheoryoftheBrowningMovement ,hepro-\nposed that the effective diffusion coefficient within a medium DR2\nrms\n2dt,w h e r edis the\nnumber of spatial dimensions (2 for a 2D simulation), and tis the average time for a\nwalk.Howmuchofaneffecton Ddotheobstructionscause?\n9) Repeattheproblemforathree-dimensionalvolume.\n4.2.3 Random Protein Folding\nAproteinisalargebiologicalmoleculemadeupofmolecularchains(theresiduesofamino\nacids). These chains are formed from monomers, that is, molecules that bind chemically\nwithothermolecules.Morespecifically,thechainsconsistofnon-polarhydrophobic(H)\nmonomersthatarerepelledbywater,andpolar(P)monomersthatareattractedbywater.\nThe actual structure of a protein results from a folding process in which random coils of\nchainsrearrangethemselvesintoaconfigurationofminimumenergy.Wewanttomodel\nthatprocessonthecomputer.\nAlthoughmoleculardynamics(Chapter18)maybeusedtosimulateproteinfolding,it\nis much slower than Monte-Carlo techniques, and even then, it is hard to find the low-\nest energy states. Here we create a simple Monte-Carlo simulation in which you to take\narandomwalkina2Dsquarelattice[Yue etal.,1995].Attheendofeachstep,youran-\ndomlychooseanHoraPmonomeranddropitonthelattice,withyourchoiceweighted\nsuchthatHmonomersaremorelikelythanPones.Thewalkisrestrictedsuchthattheonly\npositionsavailableaftereachsteparethethreeneighboringsites,withthealreadyoccupied\nsitesexcluded(thisiswhythistechniqueisknownasa self-avoidingrandomwalk ).\nThe goal of the simulation is to find the lowest energy state of a sequence of H and P\nmonomerswithlinksofvariouslengths.Itthenmaybecomparedtothoseinnature.Just\nhowbesttofindsuchastateisanactiveresearchtopic[Yue etal.,1995].Theenergyofa\nchainisdefinedas\nE=‚àíùúñf, (4.19)\n684 Monte Carlo Simulations\nwhereùúñisapositiveconstantand fisthenumberofH‚ÄìHneighbor notconnecteddirectly\n(P‚ÄìPandH‚ÄìPbondsdonotcountatloweringtheenergy).SoiftheneighbornexttoanH\nisanotherH,itlowerstheenergy,butifitisaPitdoesnotlowertheenergy.Weshowa\ntypicalsimulationresultinFigure4.6.Accordingly,foragivenlengthofchain,weexpect\nthenaturalstate(s)ofanH‚ÄìPsequencetobethosewiththelargestpossiblenumber fof\nH‚ÄìHcontacts.Thatiswhatwearelookingfor.\n1) Modifytherandomwalkprogramwehavealreadydevelopedsothatitsimulatesaself-\navoidingrandomwalk.Thekeyhereisthatthewalkstopsatacorner,orwhenthereare\nnoemptyneighboringsitesavailable.\n2) MakearandomchoiceastowhetherthemonomerisanHoraP,withaweightingsuch\nthattherearemoreH‚ÄôsthanP‚Äôs.\n3) Produceavisualizationthatshowsthepositionsoccupiedbythemonomers,withtheH\nandPmonomerindicatedbydifferentcolordots.Ourvisualization,showninFigure4.6,\nisproducedbytheprogram ProteinFold.py ,giveninListing4.2.\n4) Afterthewalkends,recordtheenergyandlengthofthechain.\n5) Runmanyfoldingsimulationsandsavetheoutputs,categorizedbylengthandenergy.\n6) Examinethestate(s)oflowestenergyforvariouschainlengthsandcomparetheresults\nto those frommolecular dynamicsimulations and actual protein structures (available\nontheWeb).\n7) Doyouthinkthissimplemodelhassomemerit?\n8)‚äôExtendthefoldingto3D.\n4.3 Spontaneous Decay\nYour problemis to simulate the time dependence of the decay of a small number Nof\nradioactiveparticles.3Inparticular,youaretodeterminetheconnectionbetweenexponen-\ntialdecayand stochasticdecay(containingelementsofchance).Realizingthatexponential\ndecayisagoodmodelonlywhenthereareverylargenumbersofparticles,theexponential\nmodelisnolongeraccurateasthenumberofdecayingparticlesdecreases,asitalwaysdoes.\nAccordingly,oursimulationshouldbeclosertonaturethanistheexponentialdecaymodel\n(Figure4.7).Infact,ifyou‚Äúlisten‚Äùtotheoutputofthedecaysimulationcode,whatyouwill\nhearsoundsverymuchlikeaGeigercounter,anintuitivelyconvincingdemonstrationof\ntherealismofthesimulation.\nSpontaneousdecayisanaturalprocessinwhichaparticle,withnoexternalstimulation,\ndecays into other particles. Although the probability of decay of any one particle in any\nonetimeintervalisconstant,justwhenitdecaysisrandom.Inasmuchasthepresence,or\ndecay,ofanyoneparticledoesnotinfluencethedecayofanyotherparticle,theprobability\nofdecayisnotinfluencedbyhowlongtheparticlehasbeenaround,orhowmanyother\nparticlesarestillaround.Inotherwords,theprobability Óàºofanyoneparticledecayingper\nunittimeintervalisaconstant,yetwhenthatparticledecays,itisgoneforever.Ofcourse,\n3 SpontaneousdecayisalsodiscussedinChapter6,wherewefitittoanexponential.\n4.3 Spontaneous Decay 69\n0 400 800 1200\nt024100,000\n10,000\n1,000\n100\n10log[N(t)]\nFigure 4.7 Circle: A sample containing Nnuclei, each of which has the same probability of\ndecaying per unit time, Graphs : Semilog plots of the number of nuclei versus time for Ô¨Åve\nsimulations with differing initial numbers of nuclei. Exponential decay would be a straight line\nwithout bumps, similar to the initial behavior for N=100,000.\nasthetotalnumber Nofparticlesdecayswithtime,sowillthenumberthatdecayperunit\ntime, but the probability of any one particle decaying in some time interval remains the\nsameforaslongasthatparticleexists.\n4.3.1 Discrete Decay Model\nImaginehavingasamplecontaining N(t)radioactivenucleiattime t(Figure4.7circle).Let\nŒîNbethenumberofparticlesthatdecayinsomesmalltimeinterval Œît.W econ vertthe\nstatement‚Äútheprobability Óàºofanyoneparticledecayingperunittimeisaconstant‚Äùinto\ntheequation\nÓàº=ŒîN(t)‚àïN(t)\nŒît=‚àíùúÜ, (4.20)\n‚áíŒîN(t)\nŒît=‚àíùúÜN(t), (4.21)\nwhere the constant ùúÜis called the decay rate and the minus sign indicates a decreasing\nnumber.Because N(t)decreasesintime,the activityŒîN(t)‚àïŒît(sometimesalsocalledthe\ndecayrate)alsodecreaseswithtime.Inaddition,becausethetotalactivityisproportional\ntothetotalnumberofparticlespresent,ittooisstochasticwithanexponential-likedecay\nintime.[Actually,becausethenumberofdecays ŒîN(t)isproportionaltothedifferencein\nrandomnumbers,ittendstoshowevenlargerstatisticalfluctuationsthandoes N(t).]\nEquation (4.21) is a finite-differenceequation relating the experimental quantities N(t),\nŒîN(t),andŒît.Althoughadifferenceequationcannotbeintegratedthewayadifferential\nequationcan,itcanbesimulatednumerically.Becausetheprocessisrandom,wecannot\npredict a single value for ŒîN(t), although we can predict the average number of decays\nwhenobservationsaremadeonmanyidenticalsystemsof Ndecayingparticles.",6599
33-4.3.4 Decay Implementation and Visualization.pdf,33-4.3.4 Decay Implementation and Visualization,"704 Monte Carlo Simulations\n4.3.2 The Exponential Decay Approximation\nWhenthenumberofparticles N‚Üí‚àûandtheobservationtimeinterval Œît‚Üí0,thediffer-\nenceequation(4.21)becomesadifferentialequation,andweobtainthefamiliarexponen-\ntialdecaylaw:\nŒîN(t)\nŒît‚àí‚àí‚àí‚àí‚ÜídN(t)\ndt=‚àíùúÜN(t). (4.22)\nThis equation can be integrated to obtain the time dependencies of the total number of\nparticlesandofthetotalactivity:\nN(t)=N(0)e‚àíùúÜt=N(0)e‚àít‚àïùúè, (4.23)\ndN\ndt(t)=‚àíùúÜN(0)e‚àíùúÜt=dN\ndt(0)e‚àíùúÜt. (4.24)\nInthislimitwecanidentifythedecayrate ùúÜwiththeinverselifetime:\nùúÜ=1\nùúè. (4.25)\nWeseefromitsderivationthatexponentialdecayisagooddescriptionofnatureforalarge\nnumber of particles, that is, when ŒîN‚àïN‚âÉ0. However, in nature, N(t)can be a small\nnumber,andinthatcasewehaveastatistical,asopposedtoacontinuousprocess.Thebasic\nlawofnature(4.20)isalwaysvalid,butaswewillseeinthesimulation,exponentialdecay\n(4.24)becomeslessandlessaccurateasthenumberofparticlesgetssmallerandsmaller.\n4.3.3 Discrete Decay Simulation\nAprogramforsimulatingradioactivedecayissurprisinglysimple,butnotwithoutitssub-\ntleties. We increase time in discrete steps of Œît, and for each time interval we count the\nnumberofnucleithathavedecayedduringthat Œît.Thesimulationquitswhenthereareno\nnucleilefttodecay.Suchbeingthecase,wehaveanouterloopoverthetimesteps Œît,and\naninnerloopovertheremainingnucleiforeachtimestep.Thepseudocodeissimple(asis\nthecode):\ninputN,lambda\n2t=0\nwhileN>0\nDelta = 0\nfori=1 . . N\n6if(r_i<lambda) Delta = Delta + 1\nendfor\nt=t+ 1\nN=N‚àíDelta\n10Output t, Delta, N\nendwhile\nWhenwepickavalueforthedecayrate ùúÜ=1‚àïùúètouseinoursimulation,wearesettingthe\nscalefortimes.Forexample,iftheactualdecayrateis ùúÜ=0.3√ó106s‚àí1,andifwedecideto\nmeasuretimesinunitsof10‚àí6s,thenwewillchooserandomnumbers0 ‚â§ri‚â§1,which\nleadstoùúÜvalueslyingsomeplacenearthemiddleoftherange(e.g. ùúÜ‚âÉ0.3).Alternatively,",1855
34-4.4 Testing and Generating Random Distributions.pdf,34-4.4 Testing and Generating Random Distributions,"4.4 Testing and Generating Random Distributions 71\nwecanuseavalueof ùúÜ=0.3√ó106s‚àí1inoursimulationandthenscaletherandomnum-\nbers to the range 0 ‚â§ri‚â§106. However, unless you plan to compare your simulation to\nexperimentaldata,youdonothavetoworryaboutthescalefortime,butinsteadshould\nfocusonthephysicsbehindtheslopesandthederivedfunctionaldependencies.\nDecay.pyisoursamplesimulationofspontaneousdecay.Anextensionofthisprogram,\nDecaySound.py , in Listing 4.3, adds a beep each time an atom decays (unfortunately this\nworks only with Windows). When we listen to the simulation it sounds like a Geiger\ncounter,withitsrandomnessandwithadecayratethatdecreasesintime.Thisprovides\nsomeratherconvincingevidenceoftherealismofthesimulation.\n4.3.4 Decay Implementation and Visualization\nWriteaprogramtosimulateradioactivedecayusingthesimpleprograminListing4.3asa\nguide.YoushouldobtainresultssimilartothoseinFigure4.7.\n1) Plot the logarithm of the number left ln N(t)and the logarithm of the decay rate\nlnŒîN(t)‚àïŒîtv e r s u stime. Note that the simulation measures time in steps of Œît\n(generationnumber).\n2) Checkthatyouobtainwhatlookslikeexponentialdecaywhenyoustartwithlargeval-\nuesforN(0),butthatthedecaydisplaysitsstochasticnatureforsmall N(0)(largeN(0)\nvaluesarealsostochastic;theyjustdon‚Äôtlookitatfirst).\n3) Createtwoplots,oneshowingthattheslopesof N(t)versustareindependent ofN(0),\nandanothershowingthattheslopesareproportionaltothevaluefor ùúÜ.\n4) Createaplotshowingthatwithinexpectedstatisticalvariations,ln N(t)andlnŒîN(t)are\nproportional.\n5) Explaininyourownwordshowaprocessthatisspontaneousandrandomatitsvery\nheartcanleadtoexponentialdecay.\n6) Howdoesyoursimulationshowthatthedecayisexponential-likeandnotapowerlaw\nsuchasN=ùõΩt‚àíùõº?\n4.4 Testing and Generating Random Distributions\nSince the computer‚Äôs random numbers are generated according to a definite rule, they\nmust be correlated with each other. This can affect a simulation that assumes truly ran-\ndomevents.Thereforeitiswisetotestarandom-numbergeneratortoobtainanumerical\nmeasureofitsuniformityandrandomnessbeforeyoustakeyourscientificreputationonit.\nInfact,sometestsaresimpleenoughforyoutomakeitahabittorunthemsimultaneously\nwithyoursimulation.Intheexamplestofollow,wetestforrandomnessanduniformity.\n1) Probablythemostobvious,butoftenneglected,testforrandomnessanduniformityis\njusttolookatthenumbersgenerated.Forexample,Table4.1presentssomeoutputfrom\nPython‚Äôs randommethod.Ifyoujustlookatthesenumbersyouwillknowimmediately\nthat they all lie between 0 and 1, that they appear to differ from each other, and that\nthereisnoobviouspattern(like0.3333).\n724 Monte Carlo Simulations\nTable 4.1 A table of a uniform, pseudo-random sequence rigenerated by Python‚Äôs random method.\n0.04689502438508175 0.20458779675039795 0.5571907470797255 0.05634336673593088\n0.9360668645897467 0.7399399139194867 0.6504153029899553 0.8096333704183057\n0.3251217462543319 0.49447037101884717 0.14307712613141128 0.32858127644188206\n0.5351001685588616 0.9880354395691023 0.9518097953073953 0.36810077925659423\n0.6572443815038911 0.7090768515455671 0.5636787474592884 0.3586277378006649\n0.38336910654033807 0.7400223756022649 0.4162083381184535 0.3658031553038087\n0.7484798900468111 0.522694331447043 0.14865628292663913 0.1741881539527136\n0.41872631012020123 0.9410026890120488 0.1167044926271289 0.8759009012786472\n0.5962535409033703 0.4382385414974941 0.166837081276193 0.27572940246034305\n0.832243048236776 0.45757242791790875 0.7520281492540815 0.8861881031774513\n0.04040867417284555 0.14690149294881334 0.2869627609844023 0.27915054491588953\n0.7854419848382436 0.502978394047627 0.688866810791863 0.08510414855949322\n0.48437643825285326 0.19479360033700366 0.3791230234714642 0.9867371389465821\n2) Aswehaveseen,aquickvisualtest(Figure4.2)involvestakingthissamelistandplot-\ntingitwith riasordinateand iasabscissa.Observehowthereappearstobeauniform\ndistribution between 0 and 1 and no particular correlation between points (although\nyoureyeandbrainwilltrytorecognizesomekindofpattern).\n3) Aswehaveseen,aneffectivetestforrandomnessisperformedbymakingascatterplot\nof(xi=r2i,yi=r2i+1)for manyivalues. If your points have noticeable regularity, the\nsequenceisnotrandom.Ifthepointsarerandom,theyshoulduniformlyfillasquare\nwithnodiscerniblepattern(acloud),asinFigure4.1.\n4) Asimpletestofuniformityevaluatesthe kthmomentofadistribution:\n‚ü®xk‚ü©=1\nNN‚àë\ni=1xk\ni. (4.26)\nIf the numbers are distributed uniformly, then (4.26) is approximately the moment of\nthedistributionfunction P(x):\n1\nNN‚àë\ni=1xk\ni‚âÉ‚à´1\n0dx xkP(x)‚âÉ1\nk+1+O(\n1‚àö\nN)\n. (4.27)\nIf(4.27)holdsforyourgenerator,thenyouknowthatthedistributionisuniform.Ifthe\ndeviationfrom(4.27)variesas1 ‚àï‚àö\nN,thenyoualsoknowthatthedistributionisrandom\nbecausethe1 ‚àï‚àö\nNresultderivesfromassumingrandomness.\n5) Anothersimpletestdeterminesthenear-neighborcorrelationinyourrandomsequence\nbytakingsumsofproductsforsmall k:\nC(k)=1\nNN‚àë\ni=1xixi+k,(k=1,2,‚Ä¶). (4.28)",4987
35-4.5 Code Listings.pdf,35-4.5 Code Listings,"4.5 Code Listings 73\nIfyourrandomnumbers xiandxi+karedistributedwiththejointprobabilitydistribution\nP(xi,xi+k)=1andareindependentanduniform,then(4.28)canbeapproximatedasan\nintegral:\n1\nNN‚àë\ni=1xixi+k‚âÉ‚à´1\n0dx‚à´1\n0dyxyP(x,y)=‚à´1\n0dyxy=1\n4. (4.29)\nIf (4.29) holds for your random numbers, then you know that they are uniform and\nindependent.Ifthedeviationfrom(4.29)variesas1 ‚àï‚àö\nN,thenyoualsoknowthatthe\ndistributionisrandom.\n6) Test your random-number generator with (4.27) for k=1,3,7a n dN=100,10000,\n100000.Ineachcaseprintout\n‚àö\nN||||||1\nNN‚àë\ni=1xk\ni‚àí1\nk+1||||||(4.30)\ntocheckthatitisoforder1.\n4.5 Code Listings\nListing 4.1 Walk.py Calls the random-number generator from the random package.\nNotethatadifferentseedisneededtoobtainadifferentsequence.\n1# Walk. py Random walk with graph\nfromvisualimport ‚àó\nfromvisual.graph import ‚àó\nimportrandom\n5\nrandom.seed(None) # Seed generator , None = > system clock\njmax = 20\nx= 0 . ; y = 0 . # Start at origin\n9graph1 = gdisplay(width=500, height=500, title= ‚ÄôRandom Walk‚Äô , xtitle= ‚Äôx‚Äô,\nytitle= ‚Äôy‚Äô)\npts = gcurve(color = color.yellow)\nforiin range (0, jmax + 1):\n13pts.plot(pos = (x, y) ) # Plot points\nx += (random.random() ‚àí0.5) ‚àó2. #‚àí1=<x=<1\ny += (random.random() ‚àí0.5) ‚àó2. #‚àí1=<y=<1\npts.plot(pos = (x, y))\n17rate(100)\nListing 4.2 ProteinFold.py Aself-avoidingrandomwalk.\n1# ProteinFold .py: Self avoiding random walk\n# Stops in corners or occupied neighbors\n# energy = ‚àíf | eps , f=1 if neighbour = H, f=0 if p\n# Yellow dot indicates unconnected neighbor\n5\nfromvisualimport ‚àó;importrandom\nMaxx = 500; Maxy = 500; ran = 20; L = 100; m= 100; n = 100\n9size = 8; size2 = size ‚àó2; nex = 0\nM= []; D D= [] # Arrays for polymer & grid\ngraph1 = display(width=Maxx, height=Maxy,title= ‚ÄôProtein Folding‚Äô ,\n13 range=ran)\npositions = points(color=color.cyan,size = 2)\n744 Monte Carlo Simulations\ndefselectcol(): # Select atom‚Äôs colors\n17hp = random.random() # Select H or P\nifhp<= 0.7:\ncol = (1,0,0) # Hydrophobic color red\nr=2\n21else:\ncol = (1,1,1) # Polar color white\nr=1\nreturncol,r\n25\ndeffindrest(m,length,fin,fjn): # Check links energies\nener = 0\nfortin range (m,length+1): # Next link not considered\n29 ifDD[t][0]==fin andDD[t][1]==fjn andDD[t][2]==2:\nener = 1 # Red unlinked neighbor\nreturnener\n33deffindenergy(length,DD): # Finds energy of each link\nenergy = 0\nfornin range (0,length+1):\ni=D D [ n ] [ 0 ]\n37 j=D D [ n ] [ 1 ]\ncl =DD[n][2]\nifcl==1:pass # if white\nelse: #r e d\n41 ifn<length+1:\nimin =int(i‚àí1) # Check neighbor i ‚àí1,j\njs =int(j)\nifimin >= 0:\n45 e = findrest(n+2,length ,imin,js) # Return energy 1\nenergy = energy + e\nife==1: # Plot yellow dot at neighbour\nxol = 4 ‚àó(i‚àí0.5)‚àísize2\n49 yol =‚àí4‚àój+size2\npoints(pos=(xol,yol),color=color.yellow, size=6)\nima = i+1\njs = j\n53 ifima<=size‚àí1: # Check neighborr i+1,j\ne = findrest(n+2,length,ima,js)\nenergy = energy+e\nife= =1 : # Plot yellow dot at neighbor\n57 xol = 4 ‚àó(i+0.5) ‚àísize2\nyol =‚àí4‚àój+size2\npoints(pos=(xol,yol),color=color.yellow, size=6)\niss = i\n61 jma = j+1\nifjma<=s i z e‚àí1: # Check neighbor i , j+1\ne = findrest(n+2,length,iss ,jma)\nenergy = energy+e\n65 ife= =1 : # Plot yellow dot at neighbor\nxol = 4 ‚àói‚àísize2 # Start at middle\nyol =‚àí4‚àó(j+0.5)+size2\npoints(pos=(xol,yol),color=color.yellow, size=6)\n69 iss = i\njmi = j ‚àí1\nifjmi >= 0: # Check neighbor i , j ‚àí1\ne = findrest(n+2,length,iss ,jmi)\n73 energy = energy +e\nife==1: # Plot yellow dot at neighbour\nxol = 4 ‚àói‚àísize2 # Start at middle\nyol =‚àí4‚àó(j‚àí0.5)+size2\n77 points(pos=(xol,yol),color=color.yellow, size=6)\nreturnenergy\ndefgrid(): # Plot grid\n81forjin range (0,size):\nyp =‚àí4‚àój+size2 # World to screen coord\nforiin range (0,size): # Horizontal row\nxp = 4 ‚àói‚àísize2\n85 positions.append(pos = (xp,yp))\n4.5 Code Listings 75\ngrid()\nlength = 0\nwhile1: # Adjust for desired number of walks\n89pts2 = label(pos=( ‚àí5,‚àí18), box=0)\nlength = 0\ngrid = zeros((size,size))\nD=z e r o s( (L, m ,n ))\n93DD = []\ni=s i z e / 2 # Center of grid\nj=s i z e / 2\nxol = 4 ‚àói‚àísize2\n97yol =‚àí4‚àój+size2\ncol,c = selectcol()\ngrid[i,j] = c # Particle in center\nM=M+[points(pos=(xol,yol),color=col, size=6)] # Red point at center\n101print("" start "" )\nDD = DD+[[i , j , c ]]\nwhile(i>0andi<size‚àí1andj>0andj<size‚àí1and(grid[i+1,j] = =0\norgrid[i‚àí1,j] == 0 orgrid[i,j+1] == 0 orgrid[i,j ‚àí1] == 0)):\n105 r = random.random()\nifr<0.25 : # Probability 25%\nifgrid[i+1,j]==0: i += 1 # Step right if empty\nelif0.25<randr<0.5: #S t e p l e f t\n109 ifgrid[i‚àí1,j] == 0: i ‚àí=1\nelif0.50<randr<0.75: #U p\nifgrid[i,j ‚àí1]==0: j ‚àí=1\nelse: #D o w n\n113 ifgrid[i ,j+1]==0: j+=1\nifgrid[i,j] == 0:\ncol,c = selectcol()\ngrid[i,j] = 2 # Occupy grid point\n117 length += 1 # Increase length as occupied\nDD = DD+[[i , j , c ]]\nxp = 4 ‚àói‚àísize2\nyp =‚àí4‚àój+size2\n121 curve(pos=[(xol,yol) ,(xp,yp)]) # Connect last to new position\nM=M+ [points(pos=(xp,yp), color=col,size=6)]\nxol = xp # Start n e w line\nyol = yp\n125 while(j = = (size ‚àí1)andi! =0andi! =( s i z e ‚àí1)): # Bottom row\nr1 = random.random()\nifr1<0.2: # Probability 20% move left\nifgrid[i‚àí1,j] == 0: i ‚àí=1\n129 elifr1 > 0.2 andr1<0.4: # Probability 20% move right\nifgrid[i+1,j] == 0: i += 1\nelse: # Probability 60% move up\nifgrid[i,j ‚àí1] == 0: j ‚àí=1\n133 ifgrid[i,j] == 0:\ncol,c = selectcol() # Increase length\ngrid[i,j] = 2 # Grid point occupied\nlength += 1\n137 DD = DD + [[ i , j , c ]]\nxp = 4 ‚àói‚àísize2\nyp =‚àí4‚àój + size2\ncurve(pos=[(xol,yol) ,(xp,yp)]) # Line connecting new point\n141 M=M +[points(pos=(xp,yp), color=col,size=6)]\nxol = xp\nyol = yp # Last row; Stop if corner or occupied neighbors\nif( i==0ori==(size ‚àí1))or(grid[i ‚àí1,size‚àí1]!=0and\ngrid[i+1,size ‚àí1]!=0):\n145 break\nwhile(j = =0 andi! =0andi! =( s i z e ‚àí1)): # First row\nr1 = random.random()\nifr1<0.2:\n149 ifgrid[i‚àí1,j] == 0: i ‚àí=1\nelifr1>0.2andr1<0.4:\nifgrid[i+1,j]==0: i += 1\nelse:\n153 ifgrid[i ,j+1]==0: j += 1\n764 Monte Carlo Simulations\nifgrid[i ,j]==0:\ncol,c = selectcol()\ngrid[i,j] = 2\n157 length += 1\nDD = DD + [[ i , j , c ]]\nxp = 4 ‚àói‚àísize2\nyp =‚àí4‚àój + size2\n161 curve(pos=[(xol,yol) ,(xp,yp)])\nM=M+ [points(pos=(xp,yp), color=col,size=6)]\nxol = xp\nyol = yp\n165 ifi==(size ‚àí1)ori==0or(grid[i ‚àí1,0]!=0 andgrid[i+1,0]!=0):\nbreak\nwhile( i==0andj! = 0andj! = ( s i z e ‚àí1)): # First column\nr1 = random.random()\n169 ifr1<0.2:\nifgrid[i,j ‚àí1] == 0: j ‚àí=1\nelifr1 > 0.2 andr1<0.4:\nifgrid[i,j+1] == 0: j += 1\n173 else:\nifgrid[i+1,j] == 0: i += 1\nifgrid[i,j] == 0:\ncol,c = selectcol()\n177 grid[i,j] = c\nlength += 1\nDD = DD+[[i , j , c ]]\nxp = 4 ‚àói‚àísize2\n181 yp =‚àí4‚àój + size2\ncurve(pos=[(xol,yol) ,(xp,yp)])\nM=M +[points(pos=(xp,yp), color=col,size=6)]\nxol = xp\n185 yol = yp\nifj==(size ‚àí1)orj==0or(grid[0,j+1]!=0 andgrid[0,j ‚àí1]!=0):\nbreak\nwhile(i==(size ‚àí1)andj! = 0andj! = ( s i z e ‚àí1)): # Last column\n189 r1 = random.random()\nifr1<0.2:\nifgrid[i,j ‚àí1] == 0: j ‚àí=1\nelifr1 > 0.2 andr1<0.4:\n193 ifgrid[i,j+1] == 0: j += 1\nelse:\nifgrid[i‚àí1,j] == 0: i ‚àí=1\nifgrid[i,j] == 0:\n197 col,c = selectcol()\ngrid[i,j] = c\nlength += 1\ncol,c=selectcol()\n201 DD = DD + [[ i , j , c ]]\nxp = 4 ‚àói‚àísize2\nyp =‚àí4‚àój + size2\ncurve(pos=[(xol,yol) ,(xp,yp)])\n205 M=M +[points(pos=(xp,yp), color=col,size=6)]\nxol = xp\nyol = yp\nifj==(size ‚àí1)or(grid[size ‚àí1,j+1]!=0 andgrid[size ‚àí1,j‚àí1]!=0):\n209 break\nlabel(pos=( ‚àí10,‚àí18), text= ‚ÄôLength=‚Äô ,b o x = 0 )\nlabel(pos=(10,18,0), text= ‚ÄôClick for new walk‚Äô ,color=color.red, display=graph1)\npts2.text = ‚Äô%4s‚Äô%length\n213label(pos=(5, ‚àí18,0), text= ‚ÄôEnergy‚Äô ,box=0)\nevalue=label(pos=(10, ‚àí18), box=0) #E n e r g y\nevalue.text = ‚Äô%4s‚Äô%findenergy(length,DD) # Walk length walk\nprint(""energy is "" ,findenergy(length,DD))\n217print(""dd"")\ngraph1.mouse.getclick() # Detect mouse click\nforobjingraph1.objects: # Start n e w walk\nif(objispositions orobjiscurve): continue\n221 obj.visible = 0 # Clear curve\n4.5 Code Listings 77\nListing 4.3 DecaySound.py Simulatesspontaneousdecayinwhichadecayoccursifa\nrandomnumberissmallerthanthedecayparameter.The winsoundpackageletsusplaya\nbeepeachtimethereisadecay,andthisleadstothesoundofaGeigercounter.\n1# DecaySound.py spontaneous decay simulation\nfromvisualimport ‚àó;fromvisual.graph import ‚àó;importrandom, winsound\n5lambda1 = 0.005 # Decay constant\nmax= 80.; time_max = 500; seed = 68111\nnumber = nloop = max # Initial value\ngraph1 = gdisplay(title = ‚ÄôSpontaneous Decay‚Äô ,xtitle= ‚ÄôTime‚Äô,\\n9 ytitle = ‚ÄôNumber‚Äô )\ndecayfunc = gcurve(color = color.green)\nfortimeinarange(0, time_max + 1): #T i m el o o p\nforatominarange(1, number + 1 ): # Decay loop\n13 decay = random.random()\nif(decay <lambda1):\nnloop = nloop ‚àí1 #Ad e c a y\nwinsound.Beep(600, 100) # Sound beep\n17number = nloop\ndecayfunc.plot( pos = (time, number) )\nrate(30)",8760
36-Chapter 5 Differentiation and Integration.pdf,36-Chapter 5 Differentiation and Integration,,0
37-5.1 Differentiation Algorithms.pdf,37-5.1 Differentiation Algorithms,,0
38-5.2.1 Second Derivatives.pdf,38-5.2.1 Second Derivatives,"78\n5\nDifferentiation and Integration\nWe start this chapter with a short discussion of numerical differentiation, an important, if\nrather straight-forward, topic. We derive the algorithms for differentiation that will be used\nthroughout the book. The majority of the chapter covers several algorithms for numerical\nintegration, a basic tool of scientiÔ¨Åc computation. We end with a discussion of Monte Carlo\nintegration techniques, which are fundamentally different from all the others.\n5.1 Differentiation Algorithms\nProblem Figure 5.1 shows the trajectory of a projectile with air resistance. The dots\nindicatethetimes tatwhichmeasurementsweremadeandtabulated.Your problemisto\ndeterminetheprojectile‚Äôsvelocity dy‚àïdtasafunctionoftime.Notethatbecausethereis\nrealisticairresistancepresent,thereisnoanalyticfunctiontodifferentiate.\nYou probably did rather well in your first calculus course and feel competent at taking\nderivatives. However, you may never have taken derivatives of numerical data using the\nelementarydefinition\ndy(t)\ndtdef=lim\nh‚Üí0y(t+h)‚àíy(t)\nh. (5.1)\nInfact,evenacomputerwillhaveproblemswiththiskindoflimitbecauseitiswrought\nwith subtractive cancellation; as his made smaller, the computer‚Äôs finite word length\ncauses the numerator to fluctuate between 0 and the machine precision ùúñm, whileas the\ndenominatorapproacheszero,overflowswilloccurs.\n5.1.1 Forward Difference\nThemostdirectmethodfornumericaldifferentiationstartsbyexpandingafunctionina\nTaylorseriestoobtainitsvalueatasmallstep haway:\ny(t+h)=y(t)+hdy(t)\ndt+h2\n2!d2y(t)\ndt2+h3\n3!dy3(t)\ndt3+¬∑¬∑¬∑, (5.2)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n5.1 Differentiation Algorithms 79\ny(t+h)‚àíy(t)\nh=dy(t)\ndt+h\n2!d2y(t)\ndt2+h2\n3!dy3(t)\ndt3+¬∑¬∑¬∑. (5.3)\nIf we ignore the h2terms in (5.3), we obtain the forward-difference algorithm for the\nderivative:\ndy(t)\ndt||||fddef=y(t+h)‚àíy(t)\nh. (5.4)\nAnestimateoftheerrorfollowsfromsubstitutingtheTaylorseries(5.2):\ndy(t)\ndt||||fd‚âÉdy(t)\ndt‚àíh\n2dy2(t)\ndt2+¬∑¬∑¬∑. (5.5)\nYou can think of this approximation as using two points to represent the function by a\nstraight line in the interval from xtox+h(Figure 5.1 left). The approximation (5.4) has\nan error proportional to h(unless the heavens look down upon you kindly and makes\ny‚Ä≤‚Ä≤vanish).Wecanmaketheapproximationerror[thetermsleftoffontheRHSof(5.4)]\nsmallerbymaking hsmaller,yetprecisionwillbelostfortoosmallan hwheny(t+h)‚âÉy(t).\nTotryoutthisforward-differencealgorithm,we‚Äôlltake y(t)=a+bt2.Theexactderivative\nisy‚Ä≤=2bt,whilethecomputedderivativeis\ndy(t)\ndt||||fd‚âÉy(t+h)‚àíy(t)\nh=2bt+bh. (5.6)\nThisclearlybecomesagoodapproximationonlyforsmall h‚â™1‚àïb.\n5.1.2 Central Difference\nAn improvedapproximationtothederivativestartswiththebasicdefinition(5.1),or,geo-\nmetrically,asshownontherightofFigure5.1.Now,ratherthanmakingasinglestepof h\nforward,weforma centraldifference bysteppingforwardhalfastepandbackwardhalfa\nstep:\ndy(t)\ndt||||cd‚â°Dcdy(t)def=y(t+h‚àï2)‚àíy(t‚àíh‚àï2)\nh. (5.7)\nCentralForwardy(t)\n0\nty(t)\n0tt + ht ‚Äì h/2t + h/2\nFigure 5.1 A trajectory of a projectile experiencing air resistance. Left: Forward-difference\napproximation (slanted line) and Right: central-difference approximation (horizontal line) for the\nnumerical Ô¨Årst derivative at time t. (A tangent to the curve at twould yield the correct derivative.)\nThe central difference is seen to be more accurate than the forward difference.\n805 Differentiation and Integration\nWeestimatetheerrorinthecentral-differencealgorithmbysubstitutingtheTaylorseries\nfory(t+h‚àï2)andy(t‚àíh‚àï2)into(5.7):\ny(\nt+h\n2)\n‚àíy(\nt‚àíh\n2)\n‚âÉ[\ny(t)+h\n2y‚Ä≤(t)+h2\n8y‚Ä≤‚Ä≤(t)+h3\n48y‚Ä≤‚Ä≤‚Ä≤(t)+ùí™(h4)]\n‚àí[\ny(t)‚àíh\n2y‚Ä≤(t)+h2\n8y‚Ä≤‚Ä≤(t)‚àíh3\n48y‚Ä≤‚Ä≤‚Ä≤(t)+ùí™(h4)]\n=hy‚Ä≤(t)+h3\n24y‚Ä≤‚Ä≤‚Ä≤(t)+ùí™(h5),\n‚áídy(t)\ndt||||cd‚âÉy‚Ä≤(t)+1\n24h2y‚Ä≤‚Ä≤‚Ä≤(t)+ùí™(h4). (5.8)\nTheimportantdifferencebetweenthiscentral-differencealgorithmandtheforwarddiffer-\nenceoneisthatwhen y(t‚àíh‚àï2)issubtractedfrom y(t+h‚àï2),alltermscontaininganeven\npower ofhin the two Taylor series cancel. This makes the central-difference algorithm\naccuratetoorder h2(h3beforedivisionby h),whiletheforwarddifferenceisaccurateonly\ntoorderh.Ifthey(t)issmooth,thatis,if y‚Ä≤‚Ä≤‚Ä≤h2‚àï24‚â™y‚Ä≤‚Ä≤h‚àï2,thenyoucanexpecttheerror\nincentral-differencealgorithmtobesmallerthanwiththeforwarddifferencealgorithm.\nIf we now return to our parabola example (5.6), we will see that the central difference\ngivestheexactderivative,independentof h:\ndy(t)\ndt||||cd‚âÉy(t+h‚àï2)‚àíy(t‚àíh‚àï2)\nh=2bt. (5.9)\nThis is to be expected because the higher derivatives equal zero for a second-order\npolynomial.\n5.2 Extrapolated Difference\nSince adifferentiationrulebasedonkeepingacertainnumberoftermsinaTaylorseries\nalsoprovidesanexpressionfortheerror(thetermsnotincluded),wecanreducethethe-\noreticalerrorfurtherbyformingacombinationofapproximationswhosesummederrors\nextrapolate to zero. One such algorithm is the central-difference algorithm (5.7) using a\nhalf-step back and a half-step forward. A second algorithm is another central-difference\napproximation,butthistimeusingquarter-steps:\ndy(t,h‚àï2)\ndt||||cddef=y(t+h‚àï4)‚àíy(t‚àíh‚àï4)\nh‚àï2(5.10)\n‚âÉy‚Ä≤(t)+h2\n96d3y(t)\ndt3+¬∑¬∑¬∑.\nA combination of the two, called the extended difference algorithm , eliminates both the\nquadraticandlinearterms:\ndy(t)\ndt||||eddef=4Dcdy(t,h‚àï2)‚àíDcdy(t,h)\n3(5.11)\n‚âÉdy(t)\ndt‚àíh4y(5)(t)\n4√ó16√ó120+¬∑¬∑¬∑. (5.12)\nEquation(5.11)istheextended-differencealgorithmand(5.12)isitserror,with Dcdrepre-\nsentingthecentral-differencealgorithm.If h=0.4andy(5)‚âÉ1,thentherewillbeonlyone",5599
39-5.2.1.1 Assessment.pdf,39-5.2.1.1 Assessment,"5.2 Extrapolated Difference 81\nplaceofround-offerrorandthetruncationerrorwillbeapproximatelymachineprecision\nùúñm;thisreallyisthebestyoucanhopefor.\nWhenworkingwiththese,andsimilarhigher-ordermethods,itisimportanttoremem-\nberthatwhiletheymayworkasdesignedforwell-behavedfunctions,theymayfailbadly\nforfunctionscontainingnoise,asdodatafromcomputationsormeasurements.Ifnoiseis\nevident,itmaybebettertofirstsmooththedata,orfitthemwithsomeanalyticfunction\nusingthetechniquesofChapter6,andthendifferentiate.\n5.2.1 Second Derivatives\nLet‚Äôs saythatyouhavemeasuredtheposition y(t)versustimeforaparticle(Figure5.1).\nYour problemnowistodeterminetheforceontheparticle.Newton‚Äôssecondlawtellsus\nthatforceandaccelerationarelinearlyrelated:\nF=ma=md2y\ndt2. (5.13)\nSobydeterminingthederivative d2y‚àïdt2fromthey(t)values,wedeterminetheforce.\nTheconcernswehaveexpressedabouterrorsinfirstderivativesareevenmoreofacon-\ncernforsecondderivatives,whereadditionalsubtractionsmayleadtoadditionalcancella-\ntions.Let‚Äôslookagainatthecentral-differencemethod:\ndy(t)\ndt||||cd‚âÉy(t+h‚àï2)‚àíy(t‚àíh‚àï2)\nh. (5.14)\nThisalgorithmgivesthederivativeat tbymovingforwardandbackwardfrom tbyh‚àï2.As\noursecondderivativealgorithm,we‚Äôlltakethecentraldifferenceofthefirstderivative:\nd2y(t)\ndt2|||||cd‚âÉy‚Ä≤(t+h‚àï2)‚àíy‚Ä≤(t‚àíh‚àï2)\nh\n‚âÉ[y(t+h)‚àíy(t)]‚àí[y(t)‚àíy(t‚àíh)]\nh2(5.15)\n=y(t+h)+y(t‚àíh)‚àí2y(t)\nh2. (5.16)\nAs we did for first derivatives, we determine the second derivative at tby evaluating the\nfunctionintheregionsurrounding t.Althoughtheform(5.16)ismorecompactandrequires\nfewerstepsthan(5.15),itmayincreasesubtractivecancellationbyfirststoringthe‚Äúlarge‚Äù\nnumbery(t+h)+y(t‚àíh),andthensubtractinganotherlargenumber2 y(t)fromit.Weask\nyoutoexplorethisdifferenceasanexercise.\n5.2.1.1 Assessment\nWrite aprogramtocalculatethesecondderivativeofcos tusingthecentral-differencealgo-\nrithms(5.15)and(5.16).Testitoverfourcycles.Startwith h‚âÉùúã‚àï10andkeepreducing h\nuntilyoureachmachineprecision.Isthereanynoticeabledifferencesbetween(5.15)and\n(5.16)?\nTheapproximationerrorsinnumericaldifferentiationdecreasewithdecreasingstepsize\nh. In turn, round-off errors increase with decreasing step size as you have to take more\n825 Differentiation and Integration\nstepsanddomorecalculations.RememberfromourdiscussioninChapter3thatthebest\napproximationoccursforan hthatminimizesthesumofapplicationandround-offerrors\nùúñapp+ùúñro,andthatoccurswhen ùúñro‚âÉùúñapp.\nWehavealreadyestimatedtheapproximationerrorinnumericaldifferentiationrulesby\nusing the Taylor series expansion of y(x+h). The approximation error with the forward-\ndifferencealgorithm(5.4)is ùí™(h),whilethatwiththecentral-differencealgorithm(5.8)is\nùí™(h2):\nùúñfd\napp‚âÉy‚Ä≤‚Ä≤h\n2,ùúñcd\napp‚âÉy‚Ä≤‚Ä≤‚Ä≤h2\n24. (5.17)\nToobtainaroughestimateoftheround-offerror,weobservethatdifferentiationessentially\nsubtractsthevalueofafunctionatargument xfromthatofthesamefunctionatargument\nx+h,andthendividesthedifferenceby h:y‚Ä≤‚âÉ[y(t+h)‚àíy(t)]‚àïh.Ashismadecontinually\nsmaller,weeventuallyreachtheround-offerrorlimitwhere y(t+h)andy(t)differbyjust\nmachineprecision ùúñm:\nùúñro‚âÉùúñm\nh. (5.18)\nConsequently,round-offandapproximationerrorsbecomeequalwhen\nùúñro‚âÉùúñapp, (5.19)\nùúñm\nh‚âÉùúñfd\napp=y(2)h\n2,ùúñm\nh‚âÉùúñcd\napp=y(3)h2\n24, (5.20)\n‚áíh2\nfd=2ùúñm\ny(2), ‚áíh3\ncd=24ùúñm\ny(3). (5.21)\nWetakey‚Ä≤‚âÉy(2)‚âÉy(3)(whichmaybecrudeingeneral,althoughnotbadfor etorcost)and\nassumedoubleprecision, ùúñm‚âÉ10‚àí15:\nhfd‚âÉ4√ó10‚àí8, hcd‚âÉ3√ó10‚àí5, (5.22)\n‚áíùúñfd‚âÉùúñm\nhfd‚âÉ3√ó10‚àí8,‚áíùúñcd‚âÉùúñm\nhcd‚âÉ3√ó10‚àí11. (5.23)\nThismayseemcontradictorybecausethebetteralgorithmleadstoalarger hvalue.Itisnot.\nTheabilitytousealarger hmeansthattheerrorinthecentral-differencemethodisabout\n1000timessmallerthantheerrorintheforward-differencemethod.\nTheprogrammingfornumericaldifferentiationissimple:\nFD = ( y( t+h) ‚àíy(t) ) /h; // Forward diff\nCD = ( y( t+h/2) ‚àíy(t‚àíh/2) ) /h; // Central diff\n3ED = (8 ‚àó(y(t+h/4) ‚àíy(t‚àíh/4))‚àí(y(t+h/2) ‚àíy(t‚àíh/2)))/3/h; //extra\n1) Useforward-,central-,andextrapolated-differencealgorithmstodifferentiatethefunc-\ntionscostandetatt=0.1,1.0,and100.\na) Printoutthederivativeanditsrelativeerror Óà±asfunctionsof h.Reducethestepsize\nhuntiltheerrorequalsmachineprecision h‚âÉùúñm.\nb) Plotlog10|Óà±|versuslog10handcheckwhetherthenumberofdecimalplacesobtained\nagreeswiththeestimatesinthetext.",4226
40-5.3 Integration Algorithms.pdf,40-5.3 Integration Algorithms,,0
41-5.3.4 Simple Integration Error Estimates.pdf,41-5.3.4 Simple Integration Error Estimates,"5.3 Integration Algorithms 83\nc) Seeifyoucanidentifyregionswherealgorithmic(seriestruncation)errordominates\natlargehandround-offerroratsmall hinyourplot.Dotheslopesagreewithour\nmodel‚Äôspredictions?\n5.3 Integration Algorithms\nProblem Integrate a Spectrum An experiment measured dN(t)‚àïdt, the number of\nparticlesenteringacounterperunittime.Your problemistointegratethisspectrumto\nobtainthenumberofparticlesthatenteredthecounterinthefirstsecond:\nN(1)=‚à´1\n0dN(t)\ndtdt. (5.24)\n5.3.1 Box Counting\nTheintegrationofafunctionmayrequiresomeclevernesstodoanalytically,butisrelatively\nstraightforwardonacomputer.Anancientwaytoperformnumericalintegrationistotake\napieceofgraphpaperandcountthenumberofboxesor quadrilaterals lyingbelowacurve\noftheintegrand.Forthisreason,numericalintegrationisalsocalled numericalquadrature ,\nevenwhenitbecomesmoresophisticatedthansimpleboxcounting.\nTheRiemanndefinitionofanintegralisthelimitofthesumoverboxesasthewidth hof\ntheboxapproacheszero(Figure5.2):\n‚à´b\naf(x)dx=lim\nh‚Üí0[\nh(b‚àía)‚àïh‚àë\ni=1f(xi)]\n. (5.25)\nThenumericalintegralofafunction f(x)isapproximatedastheequivalentofafinitesum\noverboxesofheight f(x)andwidth ùë§i:\n‚à´b\naf(x)dx‚âÉN‚àë\ni=1f(xi)ùë§i. (5.26)\nThis is similar to the Riemann definition (5.25), except that there is no limit to an\ninfinitesimalboxsize.Equation(5.26)isthestandardformforallintegrationalgorithms;\nthe function f(x)is evaluated at Npoints in the interval [a,b], and the function values\nfi‚â°f(xi)are summed with each term in the sum weighted by ùë§i. While, in general, the\nsumin(5.26)givestheexactintegralonlywhen N‚Üí‚àû,itmaybeexactforfinite Nifthe\nFigure 5.2 The integral ‚à´b\naf(x)dxis the area under the\ngraph of f(x)f r o m atob.H e r ew eb r e a ku pt h ea r e ai n t o\nfour regions of equal widths hand Ô¨Åve integration points.\nax xi + 1xi + 2b\nxf(x)\n845 Differentiation and Integration\nintegrandisapolynomial.Thedifferentintegrationalgorithms(alsocalledNewton-Coates\nformulas)amounttodifferentwaysofchoosingthepoints xiandweights ùë§i.Generally,the\nprecisionincreasesas Ngetslarger,atleastuntilround-offerrorbecomessignificant.Since\nthe‚Äúbest‚Äùintegrationruledependsonthespecificbehaviorof f(x),thereisnouniversally\nbestrule.Infact,someoftheautomatedintegrationschemesfoundinsubroutinelibraries\nand computational environments switch from one method to another, as well as change\nthemethodsfordifferentintervals,untiltheyfindonesthatworkwellforeachinterval.\nIngeneral,youshouldnotattemptanumericalintegrationofanintegrandthatcontainsa\nsingularitywithoutfirstsomehowremovingthesingularity.Youmaybeabletodothisvery\nsimply by breaking the interval down into several subintervals so the singularity is at an\nendpointwhereanintegrationpointisnotplaced,orbyachangeofvariable;forexample:\n‚à´1\n‚àí1|x|f(x)dx=‚à´0\n‚àí1f(‚àíx)dx+‚à´1\n0f(x)dx, (5.27)\n‚à´1\n0x1‚àï3dx=‚à´1\n03y3dy,(ydef=x1‚àï3), (5.28)\n‚à´1\n0f(x)dx\n‚àö\n1‚àíx2=2‚à´1\n0f(1‚àíy2)dy\n‚àö\n2‚àíy2,(y2def=1‚àíx). (5.29)\nLikewise,ifyourintegrandhasaveryslowvariationinsomeregion,youcanspeedupthe\nintegration by changing to a variable that compresses that region and places few points\nthere,ordividesuptheintervalandperformsseveralintegrations.Conversely,ifyourinte-\ngrandhasaveryrapidvariationinsomeregion,youmaywanttochangetovariablesthat\nexpandthatregiontoensurethatnooscillationsaremissed.\n5.3.2 Trapezoid Rule\nThe trapezoid and Simpson‚Äôs integration rules both use evenly spaced values of x\n(Figure5.3).Theyuse Npointsxi,i=1,N,evenlyspacedadistance hapartthroughoutthe\nintegration region [a,b],a n dinclude the endpoints in the integration region. This means\nthatthereare (N‚àí1)intervals,eachoflength h:\nh=b‚àía\nN‚àí1,xi=a+(i‚àí1)h,i=1,N, (5.30)\nwherewestartourcountingat i=1.Thetrapezoidruletakeseachintegrationinterval i,\nandconstructsatrapezoidofwidth hinit(Figure5.3).Thisapproximates f(x)byastraight\nlineineachinterval i,andusestheaverageheight (fi+fi+1)‚àï2asthevaluefor f.Thearea\nofeachsuchtrapezoidis\n‚à´xi+h\nxif(x)dx‚âÉh(fi+fi+1)\n2=1\n2hfi+1\n2hfi+1. (5.31)\nIntermsofourstandardintegrationformula(5.26),the‚Äúrule‚Äùin(5.31)isfor N=2points\nwithweights ùë§i‚â°1\n2(Table5.1).\nInordertoapplythetrapezoidruletotheentireregion [a,b],weaddthecontributions\nfromeachsubinterval:\n‚à´b\naf(x)dx‚âÉh\n2f1+hf2+hf3+¬∑¬∑¬∑+hfN‚àí1+h\n2fN. (5.32)\n5.3 Integration Algorithms 85\na\nxbf(x)\nTrap 1 Trap 2 Trap 3 Trap 4\na\nxbf(x)\nParabola 1Parabola 2\nFigure 5.3 Different shapes used to approximate the areas under the curve. Left: Straight-line\nsections used for the trapezoid rule. Right: Two parabolas used in Simpson‚Äôs rule.\nTable 5.1 Elementary weights for uniform-step\nintegration rules.\nName Degree Elementary weights\nTrapezoid 1 (1,1)h\n2\nSimpson‚Äôs 2 (1,4,1)h\n3\n3\n83 (1,3,3,1)3\n8h\nMile 4 (14,64,24,64,14)h\n45\nYouwillnoticethatbecausetheinternalpointsarecountedtwice(attheendofoneinterval\nand at the beginning of the next), they have weights of h‚àï2+h‚àï2=h, whereas the end-\npointsarecountedjustonce,andonthataccounthaveweightsofonly h‚àï2.Intermsofour\nstandardintegrationrule(5.61),wehave\nùë§i={\nh\n2,h,‚Ä¶,h,h\n2}\n(Trapezoidrule) . (5.33)\nInListing5.1,weprovideasimpleimplementationofthetrapezoidrule.\n5.3.3 Simpson‚Äôs Rule\nSimpson‚Äôs ruleapproximatestheintegrand f(x)byaparabolawithineachequallyspaced\ninterval(Figure5.3right):\nf(x)‚âÉùõºx2+ùõΩx+ùõæ. (5.34)\nTheareaundertheparabolaforeachintervalis\n‚à´xi+h\nxi(ùõºx2+ùõΩx+ùõæ)dx=ùõºx3\n3+ùõΩx2\n2+ùõæx|||||xi+h\nxi. (5.35)\n865 Differentiation and Integration\nInordertorelatetheparameters ùõº,ùõΩ,andùõætothefunction,weconsideranintervalfrom\n‚àí1to+1,inwhichcase\n‚à´1\n‚àí1(ùõºx2+ùõΩx+ùõæ)dx=2ùõº\n3+2ùõæ. (5.36)\nButwenoticethat\nf(‚àí1)=ùõº‚àíùõΩ+ùõæ,f(0)=ùõæ,f(1)=ùõº+ùõΩ+ùõæ, (5.37)\n‚áíùõº=f(1)+f(‚àí1)\n2‚àíf(0),ùõΩ=f(1)‚àíf(‚àí1)\n2,ùõæ=f(0). (5.38)\nInthisway,wecanexpresstheintegralastheweightedsumoverthevaluesofthefunction\natthreepoints:\n‚à´1\n‚àí1(ùõºx2+ùõΩx+ùõæ)dx=f(‚àí1)\n3+4f(0)\n3+f(1)\n3. (5.39)\nSeeingthatthreevaluesofthefunctionareneeded,weapplythisresulttoourproblemby\nevaluatingtheintegralovertwoadjacentintervals,inwhichcaseweevaluatethefunction\natthetwoendpointsandinthemiddle(Table5.1):\n‚à´xi+h\nxi‚àíhf(x)dx=‚à´xi+h\nxif(x)dx+‚à´xi\nxi‚àíhf(x)dx\n‚âÉh\n3fi‚àí1+4h\n3fi+h\n3fi+1. (5.40)\nTakenote:Simpson‚Äôsrulerequirestheelementaryintegrationtobeover pairsofintervals,\nwhich,inturn,requiresthatthe totalnumberofintervalsbeevenorthatthenumberofpoints\nNbeodd.InordertoapplySimpson‚Äôsruletotheentireinterval,weaddupthecontributions\nfromeachpairofsubintervals,countingallbutthefirstandlastendpointstwice:\n‚à´b\naf(x)dx‚âÉh\n3f1+4h\n3f2+2h\n3f3+4h\n3f4+¬∑¬∑¬∑+4h\n3fN‚àí1+h\n3fN. (5.41)\nIntermsofourstandardintegrationrule(5.26),wehave\nùë§i={\nh\n3,4h\n3,2h\n3,4h\n3,‚Ä¶,4h\n3,h\n3}\n(Simpson‚Äôsrule) . (5.42)\nThesumoftheseweightsprovidesausefulcheckonyourintegration:\nN‚àë\ni=1ùë§i=(N‚àí1)h. (5.43)\nRemember,thenumberofpoints NmustbeoddforSimpson‚Äôsrule.\n5.3.4 Simple Integration Error Estimates\nIn general,youshouldchooseanintegrationrulethatgivesanaccurateanswerusingthe\nleast number of integration points. We obtain a crude estimate of the approximation or\nalgorithmicerror Óà±fortheequal-spacingrulesandtheirrelativeerror ùúñ,byexpanding f(x)\nin a Taylor series around the midpoint of the integration interval. We then multiply that\nerrorbythenumberofintervals Ntoestimatetheerrorfortheentireregion [a,b].Forthe\n5.3 Integration Algorithms 87\ntrapezoidandSimpson‚Äôsrulesthisyields\nÓà±t=O(\n[b‚àía]3\nN2)\nf(2),Óà±s=O(\n[b‚àía]5\nN4)\nf(4),ùúñt,s=Óà±t,s\nf, (5.44)\nwhereùúñisameasureoftherelativeerror.Weseethatthethird-derivativeterminSimpson‚Äôs\nrulecancels(muchlikethecentral-differencemethoddoesindifferentiation).Equations\n(5.44)areilluminatinginshowinghowincreasingthesophisticationofanintegrationrule\nleadstoanerrorthatdecreaseswithahigherinversepowerof N,yetisalsoproportional\nto higherderivativesof f. Consequently,for smallintervalsandfunctions f(x)withwell-\nbehaved derivatives, Simpson‚Äôs rule should converge more rapidly and be more accurate\nthanthetrapezoidrule.\nTo model the round-off error in integration, we assume that after Nsteps therelative\nround-offerrorisrandomandoftheform\nùúñro‚âÉ‚àö\nNùúñm, (5.45)\nwhereùúñmisthemachineprecision, ùúñ‚àº10‚àí7forsingleprecision,and ùúñ‚àº10‚àí15fordouble\nprecision.Inasmuchasmostscientificcomputationsareperformedwithdoubles,wewill\nassumedoubleprecision.Wewanttodeterminean Nthatminimizesthetotalerror,that\nis,thesumoftheapproximationandround-offerrors:\nùúñtot‚âÉùúñro+ùúñapp. (5.46)\nThisoccurs,approximately,whenthetwoerrorsareofequalmagnitude,whichweapprox-\nimateevenfurtherbyassumingthatthetwoerrorsareequal:\nùúñro=ùúñapp=Óà±trap,simp\nf. (5.47)\nTocontinuethesearchforoptimum Nforageneralfunction f,wesetthescaleoffunction\nsizeandthelengthsbyassuming\nf(n)\nf‚âÉ1,b‚àía=1‚áíh=1\nN. (5.48)\nTheestimate(5.47),whenappliedtothe trapezoid rule ,yields\n‚àö\nNùúñm‚âÉf(2)(b‚àía)3\nfN2=1\nN2, (5.49)\n‚áíN‚âÉ1\n(ùúñm)2‚àï5=(\n1\n10‚àí15)2‚àï5\n=106, (5.50)\n‚áíùúñro‚âÉ‚àö\nNùúñm=10‚àí12. (5.51)\nTheestimate(5.47),whenappliedto Simpson‚Äôs rule ,yields\n‚àö\nNùúñm=f(4)(b‚àía)5\nfN4=1\nN4, (5.52)\n‚áíN=1\n(ùúñm)2‚àï9=(\n1\n10‚àí15)2‚àï9\n=2154, (5.53)\n‚áíùúñro‚âÉ‚àö\nNùúñm=5√ó10‚àí14. (5.54)",8944
42-5.4.1 Mapping Gaussian Points.pdf,42-5.4.1 Mapping Gaussian Points,"885 Differentiation and Integration\nTheseresultsareilluminatinginthattheyshowhow:\n‚óèSimpson‚Äôsrulerequiresfewerpointsandhaslesserrorthanthetrapezoidrule.\n‚óèItispossibletoobtainanerrorclosetomachineprecisionwithSimpson‚Äôsrule(andwith\notherhigher-orderintegrationalgorithms).\n‚óèObtaining the bestnumerical approximation to an integral is not achieved by letting\nN‚Üí‚àû, but with a relatively small N‚â§1000. Larger Nonly gives you more round-off\nerrors.\n5.3.5 Higher-Order Algorithms\nAsinnumericaldifferentiation,wecanusetheknownfunctionaldependenceoftheerror\nonintervalsize htoreducetheintegrationerror.Forsimplealgorithmslikethetrapezoid\nandSimpson‚Äôsrules,wehavetheanalyticestimates(5.47),whileforothersyoumayhaveto\nexperimenttodetermineanapproximate hdependence.Toillustrate,if A(h)andA(h‚àï2)are\nthevaluesoftheintegraldeterminedforintervals handh‚àï2,respectively,andifweassume\nthatthenumericalevaluationoftheintegralhasanerrorwhoseexpansionhasaleading\nerrortermproportionalto h2,\nA(h)‚âÉ‚à´b\naf(x)dx+ùõºh2+ùõΩh4+¬∑¬∑¬∑, (5.55)\nthenA(\nh\n2)\n‚âÉ‚à´b\naf(x)dx+ùõºh2\n4+ùõΩh4\n16+¬∑¬∑¬∑. (5.56)\nConsequently,wecanmakethe h2termintheerrorvanishbycomputingtheintegralas\nthecombination\nA‚âÉ4\n3A(\nh\n2)\n‚àí1\n3A(h)‚âÉ‚à´b\naf(x)dx‚àíùõΩh4\n4+¬∑¬∑¬∑. (5.57)\nClearlythisparticulartrick(Romberg‚Äôsextrapolation)worksonlyifthe h2termdominates\ntheerror.\nInTable5.1,wehavegiventheweightsforseveralequal-intervalrules.Weseethatthe\nSimpson‚Äôsruleusestwointervals,thethree-eighthsruleusesthree,andtheMilnerulefour.1\nDoremember,thesearesingle-intervalrules,andwhenstrungtogethertoobtainarulefor\ntheentireintegrationrange,thepointsthatendoneintervalandbeginthenextarecounted\ntwice. You can easily determine the number of elementary intervals integrated over, and\ncheck whether you and we have written the weights right, by summing just the weights\nforanyrule.Thesumgivestheintegralof f(x)=1andmustequal htimesthenumberof\nintervals(whichinturnequals b‚àía):\nN‚àë\ni=1ùë§i=h√óNintervals=b‚àía. (5.58)\n1 Thereis,notcoincidentally,aMileComputerCenteratOregonStateUniversity,althoughthereno\nlongerisacentralcomputerthere\n5.4 Gaussian Quadrature 89\n5.4 Gaussian Quadrature\nIt isoftenusefultorewritethebasicintegrationformula(5.26)withaweightingfunction\nW(x)separatefromtheintegrand:\n‚à´b\naf(x)dx‚â°‚à´b\naW(x)g(x)dx‚âÉN‚àë\ni=1ùë§ig(xi). (5.59)\nIn the Gaussian quadrature approach to integration, the Npoints and weights in (5.59)\nare chosen to make the integration exact if g(x)were a(2N‚àí1)-degree polynomial. To\nobtainthisincredibleoptimization,thepoints xienduphavingaspecificdistributionover\n[a,b]. In general, if g(x)is smooth, or can be made smooth by factoring out some W(x)\n(seeTable5.2),Gaussianquadraturewillproducehigheraccuracythanthetrapezoidand\nSimpson‚Äôsrulesforthesamenumberofpoints.Sometimestheintegrandmaynotbesmooth\nbecause it has different behaviors in different regions, in which case you could integrate\neachregionseparately,andthenaddtheresults.Infact,some‚Äúsmart‚Äùintegrationsubrou-\ntinesdecideforthemselveshowmanyintervalstouseandwhichruletouseineach.\nAlltherulesindicatedinTable5.2areaformofGaussianquadraturefollowingthegen-\neralform(5.59).Wecanseethatinonecasetheweightingfunctionisanexponential,in\nanotheraGaussian,andinseveralcases,thereisanintegrablesingularity.Incontrastto\ntheequallyspacedrules,thereisneveranintegrationpointattheextremesoftheintervals\n(aorb),withdiffering Nvaluesleadingtocompletelydifferingsetsofpointsandweights.\nThederivationoftheGaussianpointswillbeoutlinedbelow,butwepointoutherethat\nforordinaryGaussian(Gauss‚ÄìLegendre)integration,thepoints yiturnouttobethe Nzeros\noftheLegendrepolynomials,withtheweightsrelatedtothederivatives,\nPN(yi)=0,ùë§i=2\n(1‚àíy2\ni)[P‚Ä≤\nN(yi)]2. (5.60)\nPrograms to generate these points and weights are standard in mathematical function\nlibraries,arefoundintablesAbramowitzandStegun[1972],orcanbecomputed,aswedo\ninour gauss.pyprogram,whichalsoscalesthepointstospanaspecifiedregion.Asacheck\nthatyourprogram‚Äôspointsarecorrect,youmaywanttocomparethemtothisfour-point\nset:\n¬±yi ùë§i\n0.339981043584856 0.652145154862546\n0.861136311594053 0.347854845137454\nTable 5.2 Types of Gaussian integration rules.\nIntegral Name Integral Name\n‚à´1\n‚àí1f(y)dyGauss ‚à´1\n‚àí1F(y)‚àö\n1‚àíy2dyGauss‚ÄìChebyshev\n‚à´‚àû\n‚àí‚àûe‚àíy2F(y)dyGauss‚ÄìHermite ‚à´‚àû\n0e‚àíyF(y)dyGauss‚ÄìLaguerre\n‚à´‚àû\n0e‚àíy\n‚àöyF(y)dyAssociatedGauss‚ÄìLaguerre",4331
43-5.6.1 10D MC Error Investigation.pdf,43-5.6.1 10D MC Error Investigation,"905 Differentiation and Integration\n5.4.1 Mapping Gaussian Points\nOur standardintegrationrule(5.26)forthegeneralinterval [a,b]is\n‚à´b\naf(x)dx‚âÉN‚àë\ni=1f(xi)ùë§i. (5.61)\nWithGaussianpointsandweights,the yinterval‚àí1<yi‚â§1mustbemappedontothex\nintervala‚â§x‚â§b.Herearesomemappingswehavefoundusefulinourwork.Inallcases,\n(yi,ùë§‚Ä≤\ni)aretheelementaryGaussianpointsandweightsfortheinterval [‚àí1,1],andwewant\ntoscalethe xwithvariousranges.\n1)[‚àí1,1]‚Üí[a,b]uniformly, (a+b)‚àï2=midpoint:\nxi=b+a\n2+b‚àía\n2yi,ùë§i=b‚àía\n2ùë§‚Ä≤\ni, (5.62)\n‚áí‚à´b\naf(x)dx=b‚àía\n2‚à´1\n‚àí1f[x(y)]dy. (5.63)\n2)[0‚Üí‚àû],a=midpoint:\nxi=a1+yi\n1‚àíyi,ùë§i=2a\n(1‚àíyi)2ùë§‚Ä≤\ni. (5.64)\n3)[‚àí‚àû‚Üí‚àû],scale set by a:\nxi=ayi\n1‚àíy2\ni,ùë§i=a(1+y2\ni)\n(1‚àíy2\ni)2ùë§‚Ä≤\ni. (5.65)\n4)[a‚Üí‚àû],a+2b=midpoint:\nxi=a+2b+ayi\n1‚àíyi,ùë§i=2(b+a)\n(1‚àíyi)2ùë§‚Ä≤\ni. (5.66)\n5)[0‚Üíb],ab‚àï(b+a)=midpoint:\nxi=ba(1+yi)\nb+a‚àí(b‚àía)yi,ùë§i=2ab2\n(b+a‚àí(b‚àía)yi)2ùë§‚Ä≤\ni. (5.67)\nAsyoucansee,evenifyourintegrationrangeextendsouttoinfinity,therewillbepoints\nat large, but not infinite xvalues. As you keep increasing the number of integration\npointsN,thelastxigetslarger,butalwaysremainsfinite.\n5.4.2 Gaussian Quadrature Derivation ‚äô\nWe wanttoperformanumericalintegrationwith Nintegrationpoints:\n‚à´+1\n‚àí1f(x)dx=N‚àë\ni=1ùë§if(xi), (5.68)\nwheref(x)is a polynomial of degree (2N‚àí1)or less. The unique property of Gaussian\nquadrature is that (5.68) will be exact, as long as we ignore the effect of round-off error.\nDetermining the xi‚Äôs andùë§i‚Äôs require some knowledge of special functions and some\n5.5 Monte Carlo Integrations 91\ncleverness [Hildebrand, 1956]. The knowledge needed is the two properties of Legendre\npolynomials PN(x)oforderN:\n1)PN(x)isorthogonaltoeverypolynomialoforderlessthan N.\n2)PN(x)hasNrealrootsintheinterval ‚àí1‚â§x‚â§1.\nWe define a new polynomial of degree equal to or less than Nobtained by dividing the\nintegrandf(x)bytheLegendrepolynomial PN(x):\nq(x)def=f(x)\nPN(x), (5.69)\n‚áíf(x)=q(x)PN(x)+r(x). (5.70)\nHerer(x)isan(unknown)polynomialofdegree Norless,whichwewillnotneedtodeter-\nmine.Ifwenowsubstitute(5.70)into(5.68),andusethefactthat PNisorthogonaltoevery\npolynomialofdegreelessthanorequalto N,onlythesecond, r(x),termremains:\n‚à´+1\n‚àí1f(x)dx=‚à´+1\n‚àí1q(x)PN(x)dx+‚à´+1\n‚àí1r(x)dx=‚à´+1\n‚àí1r(x)dx. (5.71)\nYetbecause r(x)isapolynomialofdegree Norless,wecanuseastandard Npointruleto\nevaluatetheintegralexactly.\nNowthatweknowitispossibletointegratea (2N‚àí1)orlessdegreepolynomialwith\njustNpoints,wedisplaysomeclevernesstodeterminejustwhatthosepointswillbe.We\nsubstitute(5.70)into(5.68)andnotethat\n‚à´+1\n‚àí1f(x)dx=N‚àë\ni=1ùë§iq(xi)PN(xi)+N‚àë\ni=1ùë§ir(xi)=N‚àë\ni=1ùë§ir(xi). (5.72)\nTheclevernessisrealizingthatifwechoosethe Nintegrationpointstobethezeros(roots)of\ntheLegendrepolynomial PN(x),thenthefirsttermontheRHSof(5.72)willvanishbecause\nPN(xi)=0foreachxi:\n‚à´+1\n‚àí1f(x)dx=N‚àë\ni=1ùë§ir(xi). (5.73)\nThisisourproofthatthe Nintegrationpointsovertheinterval( ‚àí1,1)arethe Nzeros\nof the Legendre polynomial PN(x). As indicated in (5.60), the weights are related to the\nderivativeoftheLegendrepolynomialsevaluatedattherootsofthepolynomial.Weleave\nthederivationoftheweightstoHildebrand[1956].\n5.5 Monte Carlo Integrations\nImagine yourselfasafarmerwalkingtoyourfurthermostfieldtoaddsomealgae-eating\nfishtoapondhavinganalgaeexplosion.Yougetthereonlytoreadtheinstructionlabel\nonthefishcontaineranddiscoverthatyouneedtoknowtheareaofthepondinorderto\ndeterminethecorrectnumberoffishtoadd.Your problemistomeasuretheareaofthis\nirregularlyshapedpondwithjustthematerialsathand[Gould etal.,2006].\nIt is hard to believe that Monte Carlo techniques can be used to evaluate integrals.\nAfterall,wedonotwanttogambleonthevalues!Whileitistruethatothermethodsare\n925 Differentiation and Integration\nPond\nFigure 5.4 Left: Throwing stones into a pond as a technique for measuring its area. The ratio of\n‚Äúhits‚Äù to total number of stones thrown equals the ratio of the area of the pond to that of the box.\nRight: The evaluation of an integral via a Monte Carlo (stone-throwing) technique based on the\nratio of areas.\npreferableforsingleanddoubleintegrals,itturnsoutthatMonteCarlotechniquesarebest\nwhen the dimensionality of integrations gets large! For our pond problem, we will use a\nsamplingtechnique(Figure5.4):\n1) Walkoffaboxthatcompletelyenclosesthepond,andremoveanypebbleslyingonthe\ngroundwithinthebox.\n2) Measurethelengthsofthesidesinnaturalunitslikeyour feet.Thisletsyoucalculate\ntheareaoftheenclosingbox Abox.\n3) Grab a bunch of pebbles, count their number, and then throw them up in the air in\nrandomdirections.\n4) Countthenumberofsplashesinthepond Npondandthenumberofpebbleslyingonthe\ngroundwithinyourbox Nbox.\n5) Assumingthatyouthrewthepebblesuniformlyandrandomly,thenumberofpebbles\nfallingintothepondshouldbeproportionaltotheareaofthepond Apond.Youdetermine\nthatareafromthesimpleratio\nNpond\nNpond+Nbox=Apond\nAbox‚áíApond=Npond\nNpond+NboxAbox. (5.74)\n5.5.1 Stone Throwing Implementation\nUsesampling(Figure5.4)toperforma2Dintegrationandtherebydetermine ùúã:\n1) Imagineacircularpondenclosedinasquareofside2 (r=1).\n2) Weknowtheanalyticanswerthattheareaofacircle ‚àÆdA=ùúã.\n3) Generateasequenceofrandomnumbers ‚àí1‚â§ri‚â§+1.\n4) Fori=1toN,pick(xi,yi)=(r2i‚àí1,r2i).\n5) Ifx2\ni+y2\ni<1,letNpond=Npond+1;otherwiselet Nbox=Nbox+1.\n6) Use(5.74)tocalculatethearea,andinthisway ùúã.\n7) Increase Nuntilyouget ùúãtothreesignificantfigures(wedon‚Äôtaskmuch‚Äìthat‚Äôsonly\nslide-ruleaccuracy).\n5.5 Monte Carlo Integrations 93\n10 10010‚Äì1310‚Äì910‚Äì5\n10‚Äì910‚Äì5\n10‚Äì710‚Äì310‚Äì1\nN|Error| |Error|Trapezoid\nSimpson\nGaussian\nNTrapezoid\nSimpson\nGaussian\n10 100\nFigure 5.5 Log‚Äìlog plots of the error in the integration of exponential decay using the trapezoid\nrule, Simpson‚Äôs rule, and Gaussian quadrature versus the number of integration points N.\nApproximately, 15 decimal places of precision are attainable with double precision ( left), and 7\nplaces with single precision ( right). The algorithms are seen to stop converging when round-off\nerror (the Ô¨Çuctuating and increasing part near the bottom) starts to dominate.\nListing5.3providesourcode vonNeuman.py thatperformsaMonteCarlointegrationvia\nstonethrowing.\n5.5.2 Integration Error Investigation\n1) Write a double-precision program to integrate e‚àítfrom 0 to 1 numerically using the\ntrapezoid rule, the Simpson‚Äôs rule, Gaussian quadrature, and Monte Carlo (MC) inte-\ngration.Inthiscase,thereisananalyticanswerwithwhichtocompare:\ndN(t)\ndt=e‚àít‚áíN(1)=‚à´1\n0e‚àítdt=1‚àíe‚àí1. (5.75)\n2) Computetherelativeerror ùúñ=|(numerical-exact) ‚àïexact|ineachcase.Presentyourdata\ninthetabularform\nN ùùêTùùêSùùêG\n2 ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑\n...\n10 ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑\nwithspacesortabsseparatingthefields.Try Nvaluesof2,10,20,40,80,160, ....( Hint:\nEvennumbersmaynotbetheassumptionofeveryrule.)\n3) Make a log10‚àílog10plot of the relative error ùúñversus N, as in Figure 5.5. You should\nobserve\nùúñ‚âÉCNùõº‚áílogùúñ=ùõºlogN+constant. (5.76)\nIf your graph is similar to straight line, this means that error obeys a power law. The\nordinateonyourplotwillbethenegativeofthenumberofdecimalplacesofprecision\ninyourcalculation.\n945 Differentiation and Integration\n4) Useyourplotortabletoestimatethepower-lawdependenceoftheerror ùúñonthenum-\nber of points N, and to determine the number of decimal places of precision in your\ncalculation.DothisforboththetrapezoidandSimpson‚Äôsrules,andinboththealgorith-\nmicandround-offerrorregimes.(Notethatitmaybehardtomake Nlargeenoughto\nreachtheround-offerrorregimeforthetrapezoidrulebecausetheapproximationerror\nissolarge.)\nInListing5.2,wegiveasampleprogramthatperformsanintegrationwithGaussianpoints.\nThemethod gaussgeneratesthepointsandweightsandmaybeusefulinotherapplications\naswell.\n5.6 Mean Value and N‚ÄìD Integration\nThe standard Monte Carlo technique for integration is based on the meanvaluetheorem\n(presumablyfamiliarfromelementarycalculus):\nI=‚à´b\nadxf(x)=(b‚àía)‚ü®f‚ü©. (5.77)\nThetheoremstatestheobviousifyouthinkofintegralsasareas:Thevalueoftheintegralof\nsomefunction f(x)betweenaandbequalsthelengthoftheinterval (b‚àía)timesthemean\nvalueofthefunctionoverthatinterval ‚ü®f‚ü©(Figure5.6).TheMonteCarlointegrationalgo-\nrithmsimplyusesrandompointstoevaluatethemeanin(5.77).Withasequence a‚â§xi‚â§b\nofNuniformrandomnumbers,wedeterminethe samplemean bysamplingthefunction\nf(x)atNpoints:\n‚ü®f‚ü©‚âÉ1\nNN‚àë\ni=1f(xi). (5.78)\nThisgivesustheverysimpleintegrationrule:\n‚à´b\nadxf(x)‚âÉ(b‚àía)1\nNN‚àë\ni=1f(xi)=(b‚àía)‚ü®f‚ü©. (5.79)\nEquation (5.79) can be thought of as our standard algorithm for integration (5.26) with\nthepoints xichosenrandomly,andwithuniformweights ùë§i=(b‚àía)‚àïN.Seeingthatno\nattempthasbeenmadetoobtainanoptimalanswerforagivenvalueof N,thisdoesnot\nseem like it would be an efficient means to evaluate integrals; but you must admit it is\nsimple.Ifweletthenumberofsamplesof f(x)approachinfinity, N‚Üí‚àû,orifwekeepthe\nnumberofsamplesfiniteandtaketheaverageofinfinitelymanyruns,thelawsofstatistics\nf(x)f(x)\nxFigure 5.6 The area under the curve f(x) is the same as that\nunder the horizontal line whose height y=‚ü®f‚ü©.",8954
44-5.7 MC Variance Reduction.pdf,44-5.7 MC Variance Reduction,"5.6 Mean Value and N‚ÄìD Integration 95\nassureusthat(5.79)willapproachthecorrectanswer,atleastiftherewerenoround-off\nerrors.\nForreaderswhoarefamiliarwithstatistics,weremindyouthattheuncertaintyinthe\nvalueobtainedfortheintegral IafterNsamplesof f(x)ismeasuredbythestandarddevi-\nationùúéI.Ifùúéfisthestandarddeviationoftheintegrand finthesampling,thenfornormal\ndistributionswehave\nùúéI‚âÉ1‚àö\nNùúéf. (5.80)\nSoforlarge N,theerrorinthevalueobtainedfortheintegralshoulddecreaseas1 ‚àï‚àö\nN.\nLet‚Äôssaythatwewanttocalculatesomepropertiesofasmallatomsuchasmagnesium\nwith12electrons.Todothatweneedtointegratetheatomicwavefunctionsoverthethree\ncoordinates for each of 12 electrons. This amounts to a 3 √ó12=36-D integral. If we use\n64pointsforeachintegration,thisrequiresabout6436‚âÉ1065evaluationsoftheintegrand.\nIfthecomputerwerefastandcouldevaluatetheintegrandamilliontimespersecond,this\nwouldtakeabout1059seconds,whichissignificantlylongerthantheageoftheuniverse\n(‚àº1017seconds).\n5.6.1 10-D MC Error Investigation\nWhen we perform a multidimensional integration, the relative error in the Monte Carlo\ntechnique,beingstatistical,decreasesas1 ‚àï‚àö\nN.Thisisvalidevenifthe Npointsaredis-\ntributed over Ddimensions. In contrast, when we use these same Npoints to perform a\nD-dimensional integration as Dseparate 1D integrals using a rule such as Simpson‚Äôs, we\nuseN‚àïDpointsforeachintegration.Forfixed N,thismeansthatthenumberofpointsused\nforeachintegrationdecreasesasthenumberofdimensions Dincreases,andsotheerror\nineachintegration increaseswithD.Furthermore,thetotalerrorwillbeapproximately N\ntimestheerrorineachintegral.Ifyouputthesetrendstogetheranddotheanalysisfora\nparticularintegrationrule,youwillfindthatforadimension D‚âÉ3‚Äì4,theerrorinMonte\nCarlointegrationisapproximatelyequaltothatofconventionalschemes.Forlargervalues\nofD,theMonteCarlomethodismoreaccurate!\n5.6.2 Implementation: 10-D Monte Carlo Integration\nYour problemistofindawaytoperformmultidimensionalintegrationssothatyoulive\nlongenoughtosavortheresults.Specifically,evaluatethe10Dintegral\nI=‚à´1\n0dx1‚à´1\n0dx2¬∑¬∑¬∑‚à´1\n0dx10(x1+x2+¬∑¬∑¬∑+x10)2. (5.81)\nCheckyournumericalansweragainsttheanalyticone,155\n6.\nItiseasytogeneralizemeanvalueintegrationtomanydimensionsbypickingrandom\npointsinamultidimensionalspace.Forexample,in2D:\n‚à´b\nadx‚à´d\ncdyf(x,y)‚âÉ(b‚àía)(d‚àíc)1\nNN‚àë\nif(xi)=(b‚àía)(d‚àíc)‚ü®f‚ü©.(5.82)",2351
45-5.9 Code Listings.pdf,45-5.9 Code Listings,"965 Differentiation and Integration\nUse a built-in random-number generator to perform the 10D Monte Carlo integration in\n(5.81).\n1) Conduct16trialsandtaketheaverageasyouranswer.\n2) Trysamplesizesof N=2,4,8,‚Ä¶,8192.\n3) Plottherelativeerror versus1‚àï‚àö\nN,andseeifalinearbehavioroccurs.\n4) Whatisyourestimatefortheaccuracyoftheintegration?\n5) Showthatforadimension D‚âÉ3‚Äì4,theerrorinmultidimensionalMonteCarlointegra-\ntionisapproximatelyequaltothatofconventionalschemes,andthatforlargervalues\nofD,theMonteCarlomethodismoreaccurate.\n5.7 MC Variance Reduction\nItiscommoninmanyphysicalapplicationstointegrateafunctionwithanapproximately\nGaussiandependenceon x.TherapidfalloffoftheintegrandmeansthatourMonteCarlo\nintegrationtechniquewouldrequireanincrediblylargenumberofpointstoobtaineven\nmodest accuracy. Your problemis to make Monte Carlo integration more efficient for\nrapidlyvaryingintegrands.\nIfthefunctionbeingintegratedneverdiffersmuchfromitsaveragevalue,thenthestan-\ndardMonteCarlomeanvaluemethod(5.79)shouldworkwellwithalarge,butmanageable,\nnumberofpoints.Yetforafunctionwithalarge variance(i.e.,onethatisnot‚Äúflat‚Äù),many\noftheevaluationsofthefunctionmayoccurfor xvaluesatwhichthefunctionisverysmall,\nandthusmakesaverysmallcontributiontothefinalvalueoftheintegral;soit‚Äôsbasically\na waste of time to expend much effort in regions where the integrand is very small. The\nefficiency of the integration can be improved by mapping the function finto a different\nfunctiongthathasasmallervarianceovertheinterval.Weindicatetwomethodshereand\nreferyoutoPress etal.[2007]andKoonin[1986]formoredetails.\nThefirstmethodisa variancereduction inwhichwedeviseaflatterfunctionoverwhich\ntointegrate.Supposeweconstructafunction g(x)withthefollowingpropertieson [a,b]:\n|f(x)‚àíg(x)|‚â§ùúñ,‚à´b\nadxg(x)=J. (5.83)\nWenowevaluatetheintegralofthedifference f(x)‚àíg(x)andaddtheresultto J:\n‚à´b\nadx f(x)=‚à´b\nadx[f(x)‚àíg(x)]+J. (5.84)\nIfwearecleverenoughtofindasimple g(x)thatmakesthevarianceof f(x)‚àíg(x)lessthan\nthatoff(x),wecanobtainevenmoreaccurateanswers.\n5.8 Importance Sampling and von Neumann Rejection\nA secondmethodforimprovingMonteCarlointegrationis importancesampling ,socalled\nbecauseitsamplestheintegrandinthemostimportantregions.Itderivesfromtheidentity\nI=‚à´b\nadx f(x)=‚à´b\nadxùë§(x)f(x)\nùë§(x). (5.85)\n5.9 Code Listings 97\nFigure 5.7 The von Neumann rejection technique\nfor generating random points with weight W(x).\nA random point is accepted if it lies below the curve\nofW(x)and rejected if it lies above. This generates a\nrandom distribution weighted by whatever W(x)\nfunction is plotted. Accept\nRejectw0\nx1\nxx2w2\nw1w(x)\nIf we use a probability distribution for our random numbers that incorporates ùë§(x),t h e\nintegralcanbeapproximatedas\nI=‚ü®f\nùë§‚ü©\n‚âÉ1\nNN‚àë\ni=1f(xi)\nùë§(xi). (5.86)\nTheimprovementarisingfrom(5.86)isthatwithajudiciouschoiceofweightingfunction\nùë§(x)‚àùf(x),wecanmake f(x)‚àïùë§(x)moreconstantandthuseasiertointegrateaccurately.\nAsimpleandingeniousmethodforgeneratingrandompointswithaprobabilitydistri-\nbutionùë§(x)was deduced by von Neumann. This method is essentially the same as the\nrejection or sampling method used to guess the area of a pond, only now the pond has\nbeenreplacedbytheweightingfunction ùë§(x),andthearbitraryboxaroundthelakebythe\narbitraryconstant ùë§0.Imagineagraphof ùë§(x)versusx(Figure5.7).Walkoffyourboxby\nplacingthe line ùë§=ùë§0on the graph,withtheonlyconditionbeing ùë§0‚â•ùë§(x).W en e x t\n‚Äúthrow stones‚Äù at this graph and count only those splashes that fall into the ùë§(x)pond.\nThatis,wegenerateuniformdistributionsin xandy‚â°Wwiththemaximum yvalueequal\ntothewidthofthebox ùë§0:\n(xi,Wi)=(r2i‚àí1,ùë§0r2i). (5.87)\nWethenrejectall xithatdoesnotfallintothepond:\nifWi<ùë§(xi),accept, if Wi>ùë§(xi),reject. (5.88)\nThexivaluessoacceptedwillbeweightedby ùë§(x)(Figure5.7),withthelargestnumberof\nacceptancesoccurringwhere ùë§(x)islarge,inthiscaseformidrange x.InChapter17,we\napplyavariationoftherejectiontechniqueknownasthe Metropolisalgorithm .\n5.9 Code Listings\nListing 5.1 TrapMethods.py Integratesafunction f(y)withthetrapezoidrule.Notethat\nthestepsize hdependsuponthesizeofintervalhereandthattheweightsattheendsand\nmiddleoftheintervalsdiffer.\n1# TrapMethods . py : trapezoid integration , a <x<b, N pts, N ‚àí1 intervals\nfromnumpyimport ‚àó\n985 Differentiation and Integration\n5deffunc(x):\nreturn5‚àó(sin(8 ‚àóx))‚àó‚àó2‚àóexp(‚àíx‚àóx)‚àí13‚àócos(3 ‚àóx)\ndeftrapezoid(A,B,N):\n9h=( B ‚àíA)/(N‚àí1) # step size\nsum= (func(A)+func(B))/2 #( 1 s t+l a s t ) / 2\nforiin range (1, N‚àí1):\nsum+= func (A+i ‚àóh)\n13returnh‚àósum\nA=0 . 5\nB=2 . 3\nN = 1200\n17print(trapezoid(A,B,N ‚àí1))\nListing 5.2 IntegGauss.py Integrates the function f(x)via Gaussian quadrature. The\npoints and weights are generated in the method gauss, which will be the same for other\napplications as well. Note that the level of desired precision is set by the parameter eps,\nwhichshouldbesetbytheuser,asshouldthevaluefor job,whichcontrolsthemappingof\nthepointsontoarbitraryintervals[theyaregeneratedin( ‚àí1,1)].\n1# IntegGauss .py: Gaussian quadrature generator of pts &wts\nfromnumpyimport ‚àó\nfromsysimportversion\n5\nmax_in = 11 # N u m b intervals\nvmin = 0.; vmax = 1. #I n tr a n g e s\nME = 2.7182818284590452354E0 # Euler ‚Äôs const\n9w = zeros( (2001), float)\nx = zeros( (2001), float)\ndeff(x): # The integrand\n13return(exp(‚àíx) )\ndefgauss(npts, job, a, b, x, w):\nm =i=j=t=t 1=p p=p 1=p 2=p 3=0 .\neps = 3.E ‚àí14 # Accuracy : ‚àó‚àó‚àó‚àó‚àó‚àó ADJUST THIS ‚àó‚àó‚àó‚àó‚àó‚àó‚àó !\n17m=int((npts + 1)/2 )\nforiin range (1, m+ 1):\nt=c o s ( m a t h . p i ‚àó(float(i)‚àí0.25)/(float(npts) + 0.5) )\nt1 = 1\n21 while((abs(t‚àít1) ) >= eps):\np1 = 1. ; p2 = 0.\nforjin range (1, npts + 1):\np3 = p2; p2 = p1\n25 p1 = ((2. ‚àófloat(j)‚àí1)‚àót‚àóp2‚àí(float(j)‚àí1.)‚àóp3)/(float(j))\npp = npts ‚àó(t‚àóp1‚àíp2)/(t ‚àót‚àí1.)\nt1 = t; t = t1 ‚àíp1/pp\nx[i‚àí1] =‚àít; x[npts ‚àíi] = t\n29 w[i‚àí1] = 2./( (1. ‚àít‚àót)‚àópp‚àópp)\nw[npts ‚àíi] =w[i ‚àí1]\nif(job = = 0):\nforiin range (0, npts):\n33 x[i] = x[i] ‚àó(b‚àía)/2. + (b + a)/2.\nw[i] =w[i] ‚àó(b‚àía)/2.\nif(job = = 1):\nforiin range (0, npts):\n37 xi = x[i]\nx[i] = a ‚àób‚àó(1. + xi) / (b + a ‚àí(b‚àía)‚àóxi)\nw[i] =w[i] ‚àó2.‚àóa‚àób‚àób/( (b + a ‚àí(b‚àía)‚àóxi)‚àó(b + a ‚àí(b‚àía)‚àóxi))\nif(job = = 2):\n41 foriin range (0, npts):\nxi = x[i]\nx[i] = (b ‚àóx i+ b+a+a )/( 1 . ‚àíxi)\nw[i] =w[i] ‚àó2.‚àó(a + b)/( (1. ‚àíxi)‚àó(1.‚àíxi) )\n45defgaussint (no, min,max):\nquadra = 0.\n5.9 Code Listings 99\ngauss (no, 0, min,max,x ,w ) # Returns pts &wts\nfornin range (0, no):\n49 quadra += f(x[n]) ‚àów[n] # Calculate integral\nreturn(quadra)\nforiin range (3,max\_in + 1, 2):\nresult = gaussint(i, vmin, vmax)\n53print(""i"",i , "" err "",abs(result ‚àí1+1 / M E ) )\nprint(""Enter and return any character to quit"" )\nListing 5.3 vonNeuman.py PerformsaMonteCarlointegrationviastonethrowing.\n# vonNeuman : Monte ‚àíCarlo integration via stone throwing\nimportrandom\n4fromvisual.graph import ‚àó\nN = 100 # points to plot the function\ngraph = display(width=500,height=500,title= ‚ÄôvonNeumann Rejection Int‚Äô )\n8xsinx = curve(x= list(range(0,N)), color=color.yellow, radius=0.5)\npts = label(pos=( ‚àí60,‚àí60), text= ‚Äôpoints=‚Äô ,b o x = 0 ) # Labels\npts2 = label(pos=( ‚àí30,‚àí60), box=0)\ninside = label(pos=(30, ‚àí60), text= ‚Äôaccepted=‚Äô ,b o x = 0 )\n12inside2 = label(pos=(60, ‚àí60), box=0)\narealbl = label(pos=( ‚àí65,60), text= ‚Äôarea=‚Äô,b o x = 0 )\narealbl2 = label(pos=( ‚àí35,60), box=0)\nareanal = label(pos=(30,60), text= ‚Äôanalytical=‚Äô ,b o x = 0 )\n16zero = label(pos=( ‚àí85,‚àí48), text= ‚Äô0‚Äô,b o x = 0 )\nfive = label(pos=( ‚àí85,50), text= ‚Äô5‚Äô,b o x = 0 )\ntwopi = label(pos=(90, ‚àí48), text= ‚Äô2pi‚Äô,box=0)\n20deffx (x): returnx‚àósin(x) ‚àósin(x) # Integrand\ndefplotfunc(): # Plot function\nincr = 2.0 ‚àópi/N\n24foriin range (0,N):\nxx = i ‚àóincr\nxsinx.x[i] = ((80.0/pi) ‚àóxx‚àí80)\nxsinx.y[i] = 20 ‚àófx(xx)‚àí50\n28box = curve(pos=[( ‚àí80,‚àí50), (‚àí80,50), (80,50),\n(80,‚àí50), (‚àí80,‚àí50)], color=color.white) #b o x\nplotfunc() #B o xa r e a=hxw= 5 ‚àó2pi\n32j= 0\nNpts = 3001 # Pts inside box\nanalyt = (pi) ‚àó‚àó2 # Analytical integral\nareanal.text = ‚Äôanalytical=%8.5f‚Äô %analyt\n36genpts = points(size=2)\nforiin range (1,Npts): # points inside box\nrate(500) # slow process\nx=2 . 0 ‚àópi‚àórandom.random()\n40y=5 ‚àórandom.random()\nxp = x ‚àó80.0/pi ‚àí80\nyp = 20.0 ‚àóy‚àí50\npts2.text = ‚Äô%4s‚Äô%i\n44ify‚àó<‚àó=f x ( x ) : # Below curve\nj+ =1\ngenpts.append(pos=(xp,yp), color=color.cyan)\ninside2.text= ‚Äô%4s‚Äô%j\n48else: genpts.append(pos=(xp,yp), color=color.green)\nboxarea = 2.0 ‚àópi‚àó5.0\narea = boxarea ‚àój/(Npts ‚àí1)\narealbl2.text = ‚Äô%8.5f‚Äô%area",8430
46-Chapter 6 TrialandError Searching and Data Fitting.pdf,46-Chapter 6 TrialandError Searching and Data Fitting,,0
47-6.2.1 Bisection Exercises.pdf,47-6.2.1 Bisection Exercises,"100\n6\nTrial-and-Error Searching and Data Fitting\nThis chapter adds some more tools to our computational toolbox. First, we examine ways\nto solve equations via a trial-and-error search. In Chapter 8we will combine trial-and-error\nsearching with the solution of ordinary differential equations to solve the general quantum\neigenvalue problem. The second half of this chapter examines the Ô¨Åtting of curves to data.\nThere we examine interpolating within a table of numbers, and least-squares Ô¨Åtting of a\nfunction to data, the latter often requiring a search .\n6.1 Quantum Bound States I\nManycomputationaltechniquesusewell-definedalgorithmsleadingtodefiniteoutcomes.\nIncontrast,sometechniquesusetrial-and-erroralgorithmsinwhichinternaldecisionsare\nmadeastowhatstepstofollow,andinwhichanumberofsolutionsmaybetriedbefore\noneissettledupon,ornot.(Wealreadydidsomeofthiswhenwesummedapowerseries\nuntilthetermsbecamesmall.)Writingthistypeofprogramisusuallychallengingbecause\nwemustforeseeanumberofpossibleoutcomes,withthechanceoffailurealwayspresent.\nProbablythemoststandardprobleminquantummechanics,1istosolvefortheenergies\nofaparticleofmass mboundwithina1Dsquarewellofradius a:\nV(x)={\n‚àíV0,for|x|‚â§a,\n0,for|x|‚â•a.(6.1)\nAsshowninquantummechanicstexts[GottfriedandYan,2004],theenergiesofthebound\nstatesE=‚àíEB<0withinthiswellaresolutionsofthetranscendentalequations\n‚àö\n10‚àíEBtan(‚àö\n10‚àíEB)\n=‚àö\nEB(even), (6.2)\n‚àö\n10‚àíEBcotan(‚àö\n10‚àíEB)\n=‚àö\nEB(odd), (6.3)\nwhereevenandoddrefertothesymmetryofthewavefunction.Herewehavechosenunits\nsuchthat ‚Ñè=1,2m=1,a=1,andV0=10.\n1 WesolvethissameprobleminSection13.1usinganapproachthatisapplicabletoalmostanypotential,\nandwhichalsoprovidesthewavefunctions.Theapproachhereisspecializedtotheeigenenergiesofa\nsquarewell.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n6.2 Bisection Search 101\nYour problemisto\n1) Findseveralbound-stateenergies EBforevenwavefunctions(6.2).\n2) Explorehowmakingthepotentialdeeper,say,bychangingthe10toa20ora30,affects\nthenumberofboundstatesandtheirenergies.\n6.2 Bisection Search\nTrial-and-errorrootfindinglooksforavalueof xforwhich\nf(x)‚âÉ0, (6.4)\nwherewefollowtheconventionofmovingwhatmight,otherwise,beontheright-hand-\nside(RHS)ofanequationtotheleft-handside(LHS)inordertoleavejusta0ontheRHS.\nThesearchprocedurestartswithaguessedvaluefor x,substitutesthatguessinto f(x)(the\n‚Äútrial‚Äù),andthenseeshowdifferenttheLHSisfromzero(the‚Äúerror‚Äù).Thealgorithmthen\nchangesxbasedontheerror,andtriesoutthenewguessin f(x).Theprocedurecontinues\nuntilf(x)‚âÉ0tosomedesiredlevelofprecision,oruntilthechangesin xareinsignificant,\norwhenthesearchseemsendless.\nThemostelementarytrial-and-errortechniqueisthe bisectionalgorithm .Itisreliablebut\nslow.Ifyouknowsomeintervalinwhich f(x)changessign,thenthebisectionalgorithm\nwill always converge to the root by finding progressively smaller and smaller intervals\nwithin which the zero lies. Other techniques, such as the Newton‚ÄìRaphson method we\ndescribenext,mayconvergemorequickly,butiftheinitialguessdoesnotgetyoucloseto\nthezero,itmaybecomeunstableandmoveoffintothewilderness.\nThebasisofthebisectionalgorithmisshowninFigure6.1.Westartwithtwovaluesof\nx,x‚àí,andx+,betweenwhichweknowazerooccurs.(Youcandeterminethesebymaking\na graph or by stepping through different xvalues and looking for a sign change.) To be\nspecific,letussaythat f(x)isnegativeat x‚àíandpositiveat x+:\nf(x‚àí)<0,f(x+)>0. (6.5)\n(Note that it may well be that x‚àí>x+if the function changes from positive to negative\nasxincreases.)Thuswestartwiththeinterval x+‚â§x‚â§x‚àí,withinwhichweknowazero\noccurs.Asyoucanseein Bisection.py inListing6.1,thealgorithmthenpicksanew xvalue\nFigure 6.1 A graphical representation of the steps\ninvolved in solving for a zero of f(x) using the bisection\nalgorithm. The bisection algorithm takes the midpoint of\nthe interval as the new guess for x,w i t he a c hs t e pr e d u c i n g\nthe interval size by one-half. Four steps are shown here.\nxx+1\nx‚Äì1x+4 x‚Äì2\nx‚Äì3\n+0‚Äìf(x)",4088
48-6.3 NewtonRaphson Search.pdf,48-6.3 NewtonRaphson Search,"102 6 Trial-and-Error Searching and Data Fitting\nequaltothemidpointoftheinterval,andthensetsanewintervalasthehalfoftheprevious\nintervalinwhichthesignchanged:\nx=(p l u s+m i n u s)/2\n2if( f(x) f(plus) > 0 ) plus = x\nelseminus = x\nThisprocesscontinuesuntilthevalueof f(x)islessthanapredefinedlevelofprecision,or\nuntilapredefined(large)numberofsubdivisionsoccurs.\nTheexampleinFigure6.1showsthefirstintervalextendingfrom x‚àí=x+1tox+=x‚àí1.\nWethenbisectthatintervalat x,andbecause f(x)<0atthemidpoint,weset x‚àí=x‚àí2=x\nandlabelit x‚àí2toindicatethesecondstep.Wethenuse x+2=x+1andx‚àí2asthenextinter-\nvalandcontinuetheprocess.Weseethatonly x‚àíchangesforthefirstthreestepsinthis\nexample,butthatforthefourthstep x+finallychanges.Thechangesthenbecometoosmall\nforustoshow.\n6.2.1 Bisection Exercises\n1) The first step in implementing any search algorithm is to get an idea of what your\nfunctionlookslike.Forthepresentproblemyoudothisbymakingaplotoratableof\nf(E)=‚àö\n10‚àíEBtan(‚àö\n10‚àíEB)‚àí‚àö\nEBversus EB. Note from your plot some approxi-\nmatevaluesatwhich f(EB)=0.Yourprogramshouldbeabletofindmoreexactvalues\nforthesezeros.\n2) Write a program that implements the bisection algorithm and uses it to find some\nsolutionsof(6.2).\n3)Warning:Seeingthatthetanfunctionhassingularities,somecareissuggested.Infact,\nyourgraphicsprogrammaynotfunctionaccuratelynearthesesingularities.Onecureis\ntouseadifferent,butequivalent,formoftheequation.Showthatanequivalentformof\n(6.2)is\n‚àö\nEcot(‚àö\n10‚àíE)‚àí‚àö\n10‚àíE=0. (6.6)\n4) Makeasecondplotof(6.6),whichalsohassingularitiesbutatdifferentplaces.Usethis\nplottochoosesome xvaluesthatbracketthezeros.\n5) Afteryouhavefoundasolution,evaluate f(EB)andthusdeterminetheprecisionofyour\nsolution.\n6) ComparetherootsyoufindwiththosegivenbyMapleorMathematica.\n6.3 Newton‚ÄìRaphson Search\nThe Newton‚ÄìRaphson algorithm can find roots of the f(x)=0m o r eq u i c k l yt h a nt h e\nbisection method. As we see graphically in Figure 6.2, this algorithm is the equiva-\nlent of drawing a straight line f(x)‚âÉmx+btangent to the curve at an xvalue for\nwhichf(x)‚âÉ0, and then using the intercept of the line with the x-axis atx=‚àíb‚àïm\nas an improved guess for the root. If the ‚Äúcurve‚Äù were actually a straight line, the\nanswer would be exact; otherwise, it is a good approximation if the guess is close\n6.3 Newton‚ÄìRaphson Search 103\nFigure 6.2 A graphical representation of the steps involved\nin solving for a zero of f(x) using the Newton‚ÄìRaphson\nmethod. The Newton‚ÄìRaphson method takes the new guess\nas the zero of the line tangent to f(x) at the old guess. Two\nguesses are shown.\n32\nxf(x)1\nenough to the root for f(x)to be nearly linear. The process continues until some\nset level of precision is reached or until too many guesses fail to find a root. If a\ng u e s si si nar e g i o nw h e r e f(x)is nearly linear (Figure 6.2), then the convergence is\nveryrapid.\nTheanalyticformulationoftheNewton‚ÄìRaphsonalgorithmstartswithanoldguess x0,\nandexpressesanewguess xastheoldguessplusacorrection Œîx:\nx0=oldguess ,Œîx=unknowncorrection (6.7)\n‚áíx=x0+Œîx. (6.8)\nWenext expandthe knownfunction f(x)in aTaylorseries around x0, andkeep onlythe\nlineartermintheexpansion:\nf(x=x0+Œîx)‚âÉf(x0)+df\ndx|||x0Œîx. (6.9)\nWedeterminethecorrection Œîxbycalculatingthepointatwhichthislinearapproximation\ntof(x)crossesthe x-axis:\nf(x0)+df\ndx|||x0Œîx=0, (6.10)\n‚áíŒîx=‚àíf(x0)\ndf‚àïdx|x0. (6.11)\nTheprocedureisrepeated,startingattheimproved x,untilsomesetlevelofprecisionis\nobtained.\nThe Newton‚ÄìRaphson algorithm (6.11) requires evaluation of the derivative df‚àïdxat\neach value of x0. In many cases, you may have an analytic expression for the derivative\nand can build it into the algorithm. However, especially for more complicated problems,\nit is simple enough to just use a numerical forward-difference approximation to the\nderivative:\ndf\ndx‚âÉf(x+ùõøx)‚àíf(x)\nùõøx, (6.12)\nwhereùõøxissomesmallchangein xthatyouchose[differentfromthe Œîusedforsearchingin\n(6.11)].Whileacentral-differenceapproximationforthederivativewouldbemoreaccurate,\nitwouldrequireadditionalevaluationofthe f‚Äôs,andonceyoufindazero,itdoesnotmatter\nhowyougotthere.InListing6.2,wegiveaprogram NewtonCD.py thatimplementsthesearch\nwiththecentraldifferencederivative.",4233
49-6.4 Magnetization Search.pdf,49-6.4 Magnetization Search,"104 6 Trial-and-Error Searching and Data Fitting\n2\n2413\nXf(X) f(X)\n1\nX\nFigure 6.3 Two examples of how the Newton‚ÄìRaphson algorithm may fail if the initial guess is\nnot in the region where f(x) can be approximated by a straight line. Left: A guess lands at a local\nextremum (minimum/maximum), that is, a place where the derivative vanishes, and so the next\nguess ends up at x=‚àû.Right: The search has fallen into an inÔ¨Ånite loop. The technique known as\n‚Äúbacktracking‚Äù could eliminate this problem.\n6.3.1 Search +Backtracking\nTwo examples of possible problems with the Newton‚ÄìRaphson algorithm are shown in\nFigure 6.3. On the left, we see a case where the search takes us to an xvalue where the\nfunctionhasalocalextremum(minimumormaximum),thatis,where df‚àïdx=0.Because\nŒîx=‚àíf‚àï(df‚àïdx), this leads to a horizontal tangent (division by zero), and so the next\nguessisx=‚àû,fromwhereitishardtoreturn.Whenthishappens,youneedtostartyour\nsearch with a different guess, and pray that you do not fall into this trap again. In cases\nwherethecorrectionisverylarge,butmaybenotinfinite,youmaywanttotrybacktrack-\ning(describedbelow),andhopethatbytakingasmallerstepyouwillnotgetintoasmuch\ntrouble.\nInFigure6.3rightweseeacasewhereasearchfallsintoaninfiniteloopsurroundingthe\nzero,withoutevergettingthere.Asolutiontothisproblemis backtracking .Asthename\nimplies, in cases where the new guess x0+Œîxleads to an increase in the magnitude of\nthefunction, |f(x0+Œîx)|2>|f(x0)|2,youcanbacktracksomewhatandtryasmallerguess,\nsay,x0+Œîx‚àï2.Ifthemagnitudeof fstillincreases,thenyoujustneedtobacktracksome\nmore,say,bytrying x0+Œîx‚àï4asyournextguess,andsoforth.Becauseyouknowthatthe\ntangent line leads to a local decrease in |f|, eventually an acceptable small enough step\nshouldbefound.\nThe problem in both these cases is that the initial guesses were not close enough to\nthe regions where f(x)is approximately linear. So again, a good plot or table may help\nproduceagoodfirstguess.Alternatively,youmaywanttostartyoursearchwiththebisec-\ntion algorithm, and then switch to the faster Newton‚ÄìRaphson algorithm when you get\nclosertothezero.\nExercise\n1) UsetheNewton‚ÄìRaphsonalgorithmtofindsomeenergies EBthataresolutionsof(6.2).\nComparethissolutionwiththeonefoundwiththebisectionalgorithm.\n2) Again,noticethatthe10inthisequationisproportionaltothestrengthofthepotential\nthatcausesthebinding.Seeifmakingthepotentialdeeper,say,bychangingthe10to\na20ora30,producesmoreordeeperboundstates.(Notethatincontrasttothebisec-\ntionalgorithm,yourinitialguessmustbeclosertotheanswerfortheNewton‚ÄìRaphson\nalgorithmtowork.)\n6.4 Magnetization Search 105\n3) Modify your algorithm to include backtracking and then try it out on some difficult\ncases.\n4) Evaluate f(EB)andthusdeterminedirectlytheprecisionofyoursolution.\n6.4 Magnetization Search\nProblem Determine M(T)the magnetization as a function of temperature for simple\nmagneticmaterials.\nA collection of Nspin-1/2 particles each with magnetic moment ùúáis at temperature T.\nThecollectionhasanexternalmagneticfield Bappliedtoit,andcomestoequilibriumwith\nNLparticlesinthelowerenergystate(spinsalignedwiththemagneticfield),andwith NU\nparticlesintheupperenergystate(spinsopposedtothemagneticfield).TheBoltzmann\ndistribution law tells us that the relative probability of a state with energy Eis propor-\ntionaltoexp (‚àíE‚àï(kBT)),wher ekBisBoltzmann‚Äôsconstant.Foradipolewithmoment ùúá,\nitsenergyinamagneticfieldisgivenbythedotproduct E=‚àíùúá‚ãÖB.Accordingly,spin-up\nparticlehavelowerenergyinamagneticfieldthanspin-downparticles,andthusaremore\nprobable.\nApplyingtheBoltzmanndistributiontoourspinproblem,wehavethatthenumberof\nparticlesinthelowerenergylevel(spinup)is\nNL=NeùúáB‚àï(kBT)\neùúáB‚àï(kBT)+e‚àíùúáB‚àï(kBT), (6.13)\nwhilethenumberofparticlesintheupperenergylevel(spindown)is\nNU=Ne‚àíùúáB‚àï(kBT)\neùúáB‚àï(kBT)+e‚àíùúáB‚àï(kBT). (6.14)\nAsdiscussedin[Kittel,2018],wenowassumethatthemolecularmagneticfield B=ùúÜMis\nmuchlargerthantheappliedmagneticfield,andsoreplace Bbythemolecularfield.This\npermitsustoeliminate Bfromtheprecedingequations.The magnetizationM (T)isgiven\nby the individual magnetic moment ùúátimes the net number of particles pointing in the\ndirectionofthemagneticfield:\nM(T)=ùúá√ó(NL‚àíNU) (6.15)\n=Nùúátanh(ùúÜùúáM(T)\nkBT)\n. (6.16)\nNotethatthisdefinitionappearstomakesensebecauseasthetemperatureapproacheszero,\nallspinswillbealignedalongthedirectionof B,andsoM(T=0)=Nùúá.\nM(T)via Searching Equation (6.16) relates the magnetization and the temperature.\nHowever,itisnotreallyasolutiontoourproblembecause MappearsontheLHSofthe\nequationaswellaswithinthehyperbolicfunctionontheRHS.Generally,a transcenden-\ntalequation ofthissortdoesnothaveananalyticsolutionthatwouldgive Masafunction\nofthetemperature T.Butbysortofworkingbackwardwecanfindanumericalsolution.\nTo do that, we first express (6.16) in terms of the reduced magnetization m, the reduced\n106 6 Trial-and-Error Searching and Data Fitting\ntemperature t,andtheCurietemperature Tc:\nm(t)=tanh(\nm(t)\nt)\n, (6.17)\nm(T)=M(T)\nNùúá,t=T\nTc,Tc=Nùúá2ùúÜ\nkB. (6.18)\nWhileitisnoeasiertofindananalyticsolutionto(6.17)thanitwasto(6.16),thesimpler\nformof(6.17)makestheprogrammingeasier.\nOneapproachtoatrial-and-errorsolutionistodefineafunction\nf(m,t)=m‚àítanh(\nm(t)\nt)\n, (6.19)\nandthen,foravarietyoffixed t=tivalues,searchforthose mvaluesatwhich f(m,ti)=0.\n(One could just as well fix the value of mtomjand search for the value of tfor which\nf(mj,t)=0;onceyouhaveasolution,youhaveasolution.)Eachzerosofoundprovidesa\nsinglevalueof m(ti).Aplotoratableofthesevaluesforarangeof tivaluesthenprovides\nthebestwecandoforthedesiredsolution m(t).\nFigure6.4showsthreeplotsof f(m,t)asafunctionofthereducedmagnetization m,each\nplotforadifferentvalueofthereducedtemperature.Asyoucansee,otherthantheunin-\nterestingsolutionat m=0,thereisonlyonesolution(azero)andit‚Äôsnear m=1fort=0.5.\nThereisnosolutionattheothertemperatures.\n1) Findtherootof(6.19)tosixsignificantfiguresfor t=0.5usingthebisectionalgorithm.\n2) Findtherootof(6.19)tosixsignificantfiguresfor t=0.5usingtheNewton‚ÄìRaphson\nalgorithm.\n3) ComparethetimeittakestofindthesolutionsforthebisectionandNewton‚ÄìRaphson\nalgorithms.\nm0‚Äì1‚Äì0.8‚Äì0.6‚Äì0.4‚Äì0.200.20.4\n0.5 1 1.5 2 2.5t = 0.5\nt = 1\nt = 2tanh( m/t) ‚Äì m\nFigure 6.4 A function of the reduced magnetism mat three reduced temperatures t.Az e r oo ft h i s\nfunction determines the value of the magnetism at a particular value of t.",6406
50-6.5 Data Fitting.pdf,50-6.5 Data Fitting,"6.5 Data Fitting 107\n4) Construct a plot of the reduced magnetization m(t)as a function of the reduced\ntemperature t.\n6.5 Data Fitting\nData fitting is an art worthy of serious study by all scientists [Bevington and Robinson,\n2003].Inthesectionstofollowwejustscratchthesurfacebyexamininghowtointerpolate\nwithinatableofnumbersandhowtodoaleast-squaresfittodata.Wealsoshowhowtogo\naboutmakingaleast-squaresfittononlinearfunctionsusingsomeofthesearchtechniques\nandsubroutinelibraries.\nProblem The cross sections measured for the resonant scattering of neutrons from a\nnucleusaregiveninTable6.1.Yourproblemistodeterminevaluesforthecrosssectionsat\nenergyvalueslyingbetweenthoseinthetable.\nYoucansolvethisprobleminanumberofways.Thesimplestistonumerically interpolate\nbetween the values of the experimental f(Ei)given in Table 6.1. This is direct and easy,\nbutdoesnotaccountfortherebeingexperimentalnoiseinthedata.Amoreappropriate\nsolution(discussedinSection6.7)istofindthe bestfitofatheoreticalfunctiontothedata.\nWestartwithwhatwebelievetobethe‚Äúcorrect‚Äùtheoreticaldescriptionofthedata,\nf(E)=fr\n(E‚àíEr)2+Œì2‚àï4, (6.20)\nwherefr,Er,andŒìareunknownparameters.Wethenadjusttheparameterstoobtainthe\nbestfit.Thisisabestfitinastatisticalsense,butinfactmaynotpassthroughall(orany)\nof the data points. For an easy, yet effective, introduction to statistical data analysis, we\nrecommendBevingtonandRobinson[2003].\nThese two techniques of interpolation and least-squares fitting are powerful tools that\nlet you treat tables of numbers as if they were analytic functions, and sometimes let you\ndeducestatisticallymeaningfulconstantsorconclusionsfrommeasurements.Ingeneral,\nyoucanviewdatafittingas globalorlocal.Inglobalfits,asinglefunctionof xisusedto\nrepresenttheentiresetofnumbersinatablesuchasTable6.1.Whileitmaybespiritually\nsatisfyingtofindasinglefunctionthatpassesthroughallthedatapoints,ifthatfunction\nisnotthecorrectfunctionfordescribingthedata,thefitmayshownonphysicalbehavior\n(suchaslargeoscillations)betweenthedatapoints.Theruleofthumbisthatifyoumust\ninterpolate,keepitlocalandviewglobalinterpolationswithacriticaleye.\nTable 6.1 Experimental values for a scattering cross section ( f(E)in the theory), each with\nabsolute error ¬±ùúéi, as a function of energy ( xiin the theory).\ni 1 2345 67 8 9\nEi(MeV) 0 25 50 75 100 125 150 175 200\ng(Ei)(MB) 10.6 16.0 45.0 83.5 52.8 19.9 10.8 8.25 4.7\nError(MB) 9.34 17.9 41.5 85.5 51.5 21.5 10.8 6.29 4.14\n108 6 Trial-and-Error Searching and Data Fitting\nConsiderTable6.1asordereddata.Wecalltheindependentvariable xanditstabulated\nvaluesxi(i=1,2,‚Ä¶), and assume that the dependent variable is the function g(x), with\ntabulatedvalues gi=g(xi).Weassumethat g(x)canbeapproximatedasan (n‚àí1)th-degree\npolynomialineachinterval i:\ngi(x)‚âÉa0+a1x+a2x2+¬∑¬∑¬∑+an‚àí1xn‚àí1. (6.21)\nSeeingthatourfitislocal,wedonotassumethatone g(x)canfitallthedatainthetable,\nbutinsteaduseadifferentpolynomial,thatis,adifferentsetof aivalues,foreachinterval.\nEachpolynomialwillbeoflowdegree,andmultiplepolynomialswillbeneededtospan\ntheentiretable.Ifsomecareistaken,thesetofpolynomialssoobtainedwillbehavewell\nenough to be used in further calculations without introducing much unwanted noise or\ndiscontinuitiesin g(x)oritsderivatives.\nTheclassicinterpolationformulawascreatedbyLagrange.Hefiguredoutaclosed-form\nexpressionthatdirectlyfitsthe( n‚àí1)orderpolynomial(6.21)to nvaluesofthefunction\ng(x)evaluatedatthepoints xi.Theformulaforeachintervaliswrittenasthesumofpoly-\nnomials:\ng(x)‚âÉg1ùúÜ1(x)+g2ùúÜ2(x)+¬∑¬∑¬∑+gnùúÜn(x), (6.22)\nùúÜi(x)=n‚àè\nj(‚â†i)=1x‚àíxj\nxi‚àíxj=x‚àíx1\nxi‚àíx1x‚àíx2\nxi‚àíx2¬∑¬∑¬∑x‚àíxn\nxi‚àíxn. (6.23)\nForthreepoints,(6.22)providesasecond-degreepolynomial,whileforeightpointsitgives\naseventh-degreepolynomial.Forexample,assume wearegiventhepointsandfunction\nvalues\nx1‚àí4=(0,1,2,4)g1‚àí4=( ‚àí12,‚àí12,‚àí24,‚àí60). (6.24)\nWith four points, the Lagrange formula determines a third-order polynomial that repro-\nduceseachofthetabulatedvalues:\ng(x)=(x‚àí1)(x‚àí2)(x‚àí4)\n(0‚àí1)(0‚àí2)(0‚àí4)(‚àí12)+x(x‚àí2)(x‚àí4)\n(1‚àí0)(1‚àí2)(1‚àí4)(‚àí12)\n+x(x‚àí1)(x‚àí4)\n(2‚àí0)(2‚àí1)(2‚àí4)(‚àí24)+x(x‚àí1)(x‚àí2)\n(4‚àí0)(4‚àí1)(4‚àí2)(‚àí60),\n‚áíg(x)=x3‚àí9x2+8x‚àí12. (6.25)\nAsacheckweseethat\ng(4)=43‚àí9(42)+32‚àí12=‚àí60,g(0.5)=‚àí10.125. (6.26)\nIfthedatacontainlittlenoise,thispolynomialcanbeusedwithsomeconfidencewithin\ntherangeofthedata,butwithriskbeyondtherangeofthedata.\nNotice that Lagrange interpolation makes no restriction that the points xibe evenly\nspaced.Usually,theLagrangefitismadetoonlyasmallregionofthetablewithasmall\nvalue ofn,despitethefactthattheformulaworksperfectlywellforfittingahigh-degree\npolynomial to the entire table. The difference between the value of the polynomial\nevaluatedatsome xandthatoftheactualfunctioncanbeshowntobethe remainder\nRn‚âÉ(x‚àíx1)(x‚àíx2)¬∑¬∑¬∑(x‚àíxn)\nn!g(n)(ùúÅ), (6.27)",4777
51-6.5.1 Lagrange Fitting.pdf,51-6.5.1 Lagrange Fitting,,0
52-6.5.2 Cubic Spline Interpolation.pdf,52-6.5.2 Cubic Spline Interpolation,"6.5 Data Fitting 109\nwhereùúÅliessomewhereintheinterpolationinterval.Whatissignificanthereisthatwesee\nthatifsignificanthighderivativesexistin g(x),thentheremaindercanbeverylarge.For\nexample,atableofnoisydatawouldhavesignificantlyhighderivatives.\n6.5.1 Lagrange Fitting\nConsider the experimental neutron scattering data in Table 6.1. The expected theoretical\nfunctionalformthatdescribesthesedatais(6.20),andourempiricalfitstothesedataare\nshowninFigure6.5.\n1) Writeasubroutinetoperforman n-pointLagrangeinterpolationusing(6.22).Treat nas\nanarbitraryinputparameter.(Youmayalsodothisexercisewiththesplinefitsdiscussed\ninSection6.5.2.)\n2) Use the Lagrange interpolation formula to fit the entire experimental spectrum with\nonepolynomial.(Thismeansthatyoumustfitallninedatapointswithan8thdegree\npolynomial.)Thenusethisfittoplotthecrosssectioninstepsof5MeV.\n3) Useyourgraphtodeducetheresonanceenergy Er(yourpeakposition)and Œì(thefull\nwidthathalf-maximum).Compareyourresultswiththosepredictedbyatheoristfriend,\n(Er,Œì) = (78,55)MeV.\n4) AmorerealisticuseofLagrangeinterpolationisforlocalinterpolationwithasmallnum-\nberofpoints,suchasthree.Interpolatetheprecedingcross-sectionaldatain5-MeVsteps\nusingthree-pointLagrangeinterpolationforeachinterval.(Notethattheendintervals\nmaybespecialcases.)\n5) Wedeliberatelyhavenotdiscussed extrapolation ofdatabecauseitcanleadtoserious\nsystematicerrors;theansweryougetmaywelldependmoreonthefunctionyouassume\nthanonthedatayouinput.Addsomeadventuretoyourlifeandusetheprogramsyou\nhavewrittentoextrapolatetovaluesoutsideTable6.1.Compareyourresultstothethe-\noreticalBreit‚ÄìWignershape(6.20).\nThisexampleshowshoweasyitistogowrongwithahigh-degree-polynomialfittodata\nwitherrors.Althoughthepolynomialisguaranteedtopassthroughallthedatapoints,the\nrepresentationofthefunctionawayfromthesepointscanbequiteunrealistic.Usingalow-\norder interpolation formula, say, n=2 or 3, in each interval usually eliminates the wild\noscillations,butmaynothaveanytheoreticaljustification.Iftheselocalfitsarematched\ntogethercarefully,aswediscussinthefollowingsectiononcubicsplineinterpolation,then\na rather continuous curve results. Nonetheless, you must recall that if the data contain\nerrors,acurvethatactuallypassesthroughthemmayleadyouastray.Wediscusshowto\ndothisproperlywithleast-squarefittinginSection6.7.\n6.5.2 Cubic Spline Interpolation\nIf you have followed our suggestions and tried to interpolate the resonant cross section\nwithLagrangeinterpolation,thenyousawthatfittingparabolas(three-pointinterpolation)\nwithinatablemayavoidtheerroneousandpossiblycatastrophicdeviationsofahigh-order\nformula.(Atwo-pointinterpolation,whichconnectsthepointswithstraightlines,maynot\nleadyoufarastray,butitisrarelypleasingtotheeyeorprecise.)Asophisticatedvariation\n110 6 Trial-and-Error Searching and Data Fitting\n0 50 100 150 200\nE (MeV)020406080\nCross sectionData\nLagrange\nCubic splines\nParabola (lst sq)\nFigure 6.5 Three Ô¨Åts to data. Dashed : Lagrange interpolation using an 8th degree polynomial;\nShort dashes : cubic splines Ô¨Åt; Long dashed : Least-squares parabola Ô¨Åt.\nof ann=4 interpolation,known as cubicsplines , often leads to surprisingly smooth and\neye-pleasing fits. In this approach (Figure 6.5), cubic polynomials are fit to the function\nineachinterval,withtheadditionalconstraintthatthefirstandsecondderivativesofthe\ncubicsbecontinuousfromoneintervaltothenext.Thiscontinuityofslopeandcurvature\nmakesthesplinefitparticularlyeye-pleasing.Theanalyticapproachisanalogoustousing\naflexiblesplinedraftingtool(aleadwirewithinarubbersheath),fromwhichthemethod\ndrawsitsname.\nTheseriesofcubicpolynomialsobtainedbyspline-fittingatableofdatacanbeintegrated\nand differentiated, and is guaranteed to have well-behaved derivatives. The existence of\nmeaningfulderivativesisanimportantconsideration.Asacaseinpoint,iftheinterpolated\nfunctionisapotential,youcantakethederivativetoobtaintheforce.Thecomplexityof\nsimultaneouslymatchingpolynomialsandtheirderivativesoveralltheinterpolationpoints\nleadstomanysimultaneouslinearequationstobesolved.Thismakessplinesunattractive\nforhandcalculations,yeteasyforcomputersand,notsurprisingly,popularinbothcalcu-\nlationsandcomputerdrawingprograms.Toillustrate,thesmoothsolidcurveinFigure6.5\nisasplinefit.\nThebasicapproximationofsplinesistherepresentationofthefunction g(x)inthesubin-\nterval[xi,xi+1]withacubicpolynomial:\ng(x)‚âÉgi(x),forxi‚â§x‚â§xi+1, (6.28)\ngi(x)=gi+g‚Ä≤\ni(x‚àíxi)+1\n2g‚Ä≤‚Ä≤\ni(x‚àíxi)2+1\n6g‚Ä≤‚Ä≤‚Ä≤\ni(x‚àíxi)3. (6.29)\nThisrepresentationmakesitclearthatthecoefficientsinthepolynomialequalthevaluesof\ng(x)anditsfirst,second,andthirdderivativesatthetabulatedpoints xi.Derivativesbeyond\nthethirdvanishforacubic.Thecomputationalchoreistodeterminethesederivativesin\ntermsofthe Ntabulatedgivalues.Thematchingof giatthenodesthatconnectoneinterval",4831
53-6.7.1 LeastSquares Implementation.pdf,53-6.7.1 LeastSquares Implementation,"6.5 Data Fitting 111\ntothenextprovidestheequations\ngi(xi+1)=gi+1(xi+1),i=1,N‚àí1. (6.30)\nThematchingofthefirst andsecondderivativesateachinterval‚Äôsboundariesprovidesthe\nequations\ng‚Ä≤\ni‚àí1(xi)=g‚Ä≤\ni(xi),g‚Ä≤‚Ä≤\ni‚àí1(xi)=g‚Ä≤‚Ä≤\ni(xi). (6.31)\nTheadditionalequationsneededtodetermineallconstantsareobtainedbymatchingthe\nthirdderivativesatadjacentnodes.Valuesforthethirdderivativesarefoundbyapproxi-\nmatingthemintermsofthesecondderivatives:\ng‚Ä≤‚Ä≤‚Ä≤\ni‚âÉg‚Ä≤‚Ä≤\ni+1‚àíg‚Ä≤‚Ä≤\ni\nxi+1‚àíxi. (6.32)\nAsdiscussedinChapter5,acentral-differenceapproximationwouldbemoreaccuratethan\naforward-differenceapproximation,yet(6.32)keepstheequationssimpler.\nIt is straightforward, although complicated, to solve for all the parameters in (6.29).\nWe leave that to the references Thompson [1992] and Press et al. [2007]. We can\nsee, however, that matching at the boundaries of the intervals results in only (N‚àí2)\nlinear equations for Nunknowns. Further input is required. It usually is taken to\nbe the boundary conditions at the endpoints a=x1andb=xN, specifically, the sec-\nond derivatives there g‚Ä≤‚Ä≤(a)andg‚Ä≤‚Ä≤(b). There are several ways to determine these second\nderivatives:\nNatural spline :Setg‚Ä≤‚Ä≤(a)=g‚Ä≤‚Ä≤(b)=0;thatis,permitthefunctiontohaveaslopeattheend-\npointsbutnocurvature.Thisis‚Äúnatural‚Äùbecausethederivativevanishesfortheflexible\nsplinedraftingtool(itsendsbeingunconstrained).\nInput values for g‚Ä≤at the boundaries :Thecomputeruses g‚Ä≤(a)toapproximate g‚Ä≤‚Ä≤(a).If\nyoudonotknowthefirstderivatives,youcancalculatethemnumericallyfromthetable\nofgivalues.\nInput values for g‚Ä≤‚Ä≤at the boundaries :Knowingvaluesisofcoursebetterthanapprox-\nimatingthem, but it requires the user to input information.If the values of g‚Ä≤‚Ä≤are not\nknown, they can be approximated by applying a forward-difference approximation to\nthetabulatedvalues:\ng‚Ä≤‚Ä≤(x)‚âÉ[g(x3)‚àíg(x2)]‚àï[x3‚àíx2]‚àí[g(x2)‚àíg(x1)]‚àï[x2‚àíx1]\n[x3‚àíx1]‚àï2. (6.33)\n6.5.3 Cubic Spline Quadrature\nA powerfulintegrationscheme is to fit an integrand with splines, and then integratethe\ncubicpolynomialsanalytically.Iftheintegrand g(x)isknownonlyatitstabulatedvalues,\nthen this is about as good an integration scheme as is possible; if you have the ability to\ncalculate the function directly for arbitrary xvalues, then Gaussian quadrature may be\npreferable.Weknowthatthesplinefitto gineachintervalisthecubic(6.29)\ng(x)‚âÉgi+g‚Ä≤\ni(x‚àíxi)+1\n2g‚Ä≤‚Ä≤\ni(x‚àíxi)2+1\n6g‚Ä≤‚Ä≤‚Ä≤\ni(x‚àíxi)3. (6.34)\n112 6 Trial-and-Error Searching and Data Fitting\nItiseasytointegratethistoobtaintheintegralof gforthisintervalandthentosumover\nallintervals:\n‚à´xi+1\nxig(x)dx‚âÉ(\ngix+1\n2g‚Ä≤\nix2+1\n6g‚Ä≤‚Ä≤\nix3+1\n24g‚Ä≤‚Ä≤‚Ä≤\nix4)||||xi+1\nxi, (6.35)\n‚à´xk\nxjg(x)dx=k‚àë\ni=j(\ngix+1\n2g‚Ä≤\nix2\ni+1\n6g‚Ä≤‚Ä≤\nix3+1\n24g‚Ä≤‚Ä≤‚Ä≤\nix4)||||xi+1\nxi. (6.36)\nMakingtheintervalssmallerdoesnotnecessarilyincreaseprecision,assubtractivecancel-\nlationsin(6.35)maygetlarge.\nSpline Fit of Cross Section (Implementation) Fitting a series of cubics to data is a\nlittlecomplicatedtoprogramyourself,sowerecommendusingalibraryroutine.Wehave\nadapted the splint.cand the spline.cfunctions from Press et al. [2007] to produce the\nSplineInteract.py programshowninListing6.3.\nYour problemforthissectionistocarryouttheassessmentinSection6.5.1usingcubic\nsplineinterpolationratherthanLagrangeinterpolation.\n6.6 Fitting Exponential Decay\nFigure6.6presentsactualexperimentaldataonthenumberofdecays ŒîNoftheùúãmeson\nasafunctionoftime[Stetz etal.,1973].Noticethatthetimehasbeen‚Äúbinned‚Äùintointer-\nvalsŒît=10-ns,andthatthesmoothcurveisthetheoreticalexponentialdecayexpectedif\ntherewereaverylargenumbersofpions(whichthere‚Äôsnot).Yourproblemistodeducethe\nlifetimeùúèoftheùúãmesonfromthesedata(thetabulatedlifetimeis2.6 √ó10‚àí8seconds).\nAssume that we start with N0particlesat time t=0 that can decay to other particles.2\nIfwewaitashorttime Œît,thenasmallnumber ŒîNoftheparticleswilldecay spontaneously ,\nthatis,withnoexternalinfluences.Thisdecayisastochasticprocess,whichmeansthatan\n0 40 80 120\nt (ns)02040\nNumberN(t)\nDataFitFigure 6.6 A reproduction of the\nexperimental measurement of\nStetz et al. [1973] giving the\nnumber of decays of ùúãmesons as a\nfunction of time since their\ncreation. Measurements were made\nduring time intervals (box sizes) of\n10-inch width. The dashed curve is\nthe result of a linear least-square\nÔ¨Åt to the log N(t).\n2 SpontaneousdecayisdiscussedfurtherandsimulatedinSection4.3.\n6.7 Least-Squares Fitting 113\nelementofchancehelpsdeterminejustwhenadecaywilloccur,andsonotwoexperiments\nareexpectedtogiveexactlythesameresults.Thebasiclawofnatureforspontaneousdecay\nis that the number of decays ŒîNin a time interval Œîtis proportional to the number of\nparticlesN(t)presentatthattimeandtothetimeinterval\nŒîN(t)=‚àí1\nùúèN(t)Œît‚áíŒîN(t)\nŒît=‚àíùúÜN(t). (6.37)\nHereùúè=1‚àïùúÜisthelifetimeoftheparticle,with ùúÜarateparameter.Theactualdecay rateis\ngivenbythesecondequationin(6.37).Ifthenumberofdecays ŒîNisverysmallcompared\ntothenumberofparticles N,andifwelookatvanishinglysmalltimeintervals,thenthe\ndifferenceequation(6.37)becomesthedifferentialequation\ndN(t)\ndt‚âÉ‚àíùúÜN(t)=1\nùúèN(t). (6.38)\nThis differential equation has an exponential solution for the number as well as for the\ndecayrate:\nN(t)=N0e‚àít‚àïùúè,dN(t)\ndt=‚àíN0\nùúèe‚àít‚àïùúè=dN(0)\ndte‚àít‚àïùúè. (6.39)\nEquation(6.39)isthetheoreticalformulawewishtofittothedatainFigure6.6.Theoutput\nofsuchafitisabest-fitvalueforthelifetime ùúè.\n6.7 Least-Squares Fitting\nBookshavebeenwrittenandcareershavebeenspentdiscussingwhatismeantbya‚Äúgood\nfit‚Äùtoexperimentaldata.Wecannotdojusticetothesubjecthereandreferthereaderto\nBevington and Robinson [2003], Press et al. [2007], and Thompson [1992]. However, we\nwillemphasizethreepoints:\n1) Ifthedatabeingfitcontainerrors,thenthe‚Äúbestfit‚Äùinastatisticalsenseshouldnot\npassthroughallthedatapoints.\n2) Ifthetheoryisnotanappropriateoneforthedata(e.g.,theparabolainFigure6.5),then\nitsbestfittothedatamaynotbeagoodfitatall.Thisisgood,forthisishowweknow\nthatthetheoryisnotappropriate.\n3) Onlyforthesimplestcaseofalinearleast-squaresfitcanwewritedownaclosed-form\nsolutiontoevaluateandobtainthebestfit.Morerealisticproblemsareusuallysolvedby\ntrial-and-error searchprocedures,sometimesusingsophisticatedsubroutinelibraries.\nInSection6.8,weshowhowtoconductsuchanonlinearsearchusingfamiliartools.\nImaginethatyouhavemeasured NDdatavaluesoftheindependentvariable yasafunction\nofthedependentvariable x:\n(xi,yi¬±ùúéi),i=1,ND, (6.40)\nwhere¬±ùúéiistheexperimentaluncertaintyinthe ithvalueof y.(Forsimplicityweassume\nthat all the errors ùúéioccur in the dependent variable, although this is hardly ever true\n[Thompson,1992]).Forourproblem, yisthenumberofdecaysasafunctionoftime,and\nxiisthetimes.Ourgoalistodeterminehowwellamathematicalfunction y=g(x)(also\ncalledatheoryoramodel)candescribethesedata.Additionally,ifthetheorycontainssome\n114 6 Trial-and-Error Searching and Data Fitting\nparametersorconstants,ourgoalisalsotodeterminethebestvaluesfortheseparameters.\nWeassumethatthetheoryfunction g(x)contains,inadditiontothefunctionaldependence\nonx, an additional dependence upon MPparameters {a1,a2,‚Ä¶,aMP}. Notice that the\nparameters {am}arenotvariables,inthesenseofnumbersreadfromameter,butrather\narepartsofthetheoreticalmodel,suchasthesizeofabox,themassofaparticle,orthe\ndepthofapotentialwell.Fortheexponentialdecayfunction(6.39),theparametersarethe\nlifetimeùúèandtheinitialdecayratedN\ndt(0).Weindicatethisas\ng(x)=g(x;{a1,a2,‚Ä¶,aMP}) =g(x;{am}), (6.41)\nwheretheai‚Äôsareparametersand xtheindependentvariable.\nWeusethechi-square, ùúí2,measureasagaugeofhowwellatheoreticalfunction grepro-\nducesdata[BevingtonandRobinson,2003]:\nùúí2def=ND‚àë\ni=1(yi‚àíg(xi;{am})\nùúéi)2\n, (6.42)\nwherethesumisoverthe NDexperimentalpoints (xi,yi¬±ùúéi).Thedefinition(6.42)issuch\nthatsmallervaluesof ùúí2arebetterfits,with ùúí2=0occurringifthetheoreticalcurvewent\nthrough the center of every data point. Notice also that the 1 ‚àïùúé2\niweighting means that\nmeasurementswithlargererrorscontributelessto ùúí2.3Least-squaresfitting referstoadjust-\ningtheparametersinthetheoryuntilaminimumin ùúí2isfound,thatis,findingacurve\nthat produces the least value for the summed squares of the deviations of the data from\nthefunction g(x).Ingeneral,thisisthebestfitpossibleandthebestwaytodeterminethe\nparametersinatheory.The MPparameters {am,m=1,MP}thatmake ùúí2anextremumare\nfoundbysolvingthe MPequations:\nùúïùúí2\nùúïam=0,‚áíND‚àë\ni=1[yi‚àíg(xi)]\nùúé2\niùúïg(xi)\nùúïam=0,(m=1,MP). (6.43)\nOften, the function g(x;{am})has a sufficiently complicated dependence on the amval-\nuesfor(6.43)toproduce MPsimultaneousnonlinearequationsinthe amvalues.Inthese\ncases,solutionsarefoundbyatrial-and-errorsearchthroughthe MP-dimensionalparam-\neterspace,aswedoinSection6.8.Tobesafe,whensuchasearchiscompleted,youshould\ncheckthattheminimum ùúí2youfoundis globalandnotlocal.Onewaytodothatistorepeat\nthesearchforawholegridofstartingvalues,andifdifferentminimaarefound,topickthe\nonewiththelowest ùúí2.\n6.7.1 Least-Squares Implementation\nWhenthedeviationsfromtheoryareasaresultofrandomerrors,andwhentheseerrorsare\ndescribedbyaGaussiandistribution,therearesomeusefulrulesofthumbtoremember.\nYouknowthatyourfitisgoodifthevalueof ùúí2calculatedviathedefinition(6.42)isapprox-\nimatelyequaltothenumberofdegreesoffreedom ùúí2‚âÉND‚àíMP,whereNDisthenumber\nofdatapointsand MPisthenumberofparametersinthetheoreticalfunction.Ifyour ùúí2\nismuchlessthan ND‚àíMP,itdoesn‚Äôtmeanthatyouhavea‚Äúgreat‚Äùtheoryorreallyprecise\n3 Ifyouarenotgiventheerrors,youcanguessthemonthebasisoftheapparentdeviationofthedatafrom\nasmoothcurve,oryoucanweighallpointsequallybysetting ùúéi‚â°1andcontinuewiththefitting.\n6.7 Least-Squares Fitting 115\nmeasurements;instead,youprobablyhavetoomanyparameters,orhaveassignederrors\n(ùúéivalues) that are too large. In fact, too small a ùúí2may indicate that you are fitting the\nrandomscatterinthedataratherthanmissingapproximatelyone-thirdoftheerrorbars,\nasexpectediftheerrorsarerandom.Ifyour ùúí2issignificantlygreaterthan ND‚àíMP,the\ntheorymaynotbegood,youmayhavesignificantlyunderestimatedyourerrors,oryoumay\nhaveerrorsthatarenotrandom.\nTheMPsimultaneous equations (6.43) can be simplified considerably if the functions\ng(x;{am})dependlinearlyontheparametervalues ai,e.g.,\ng(x;{a1,a2})=a1+a2x. (6.44)\nIn this case (also known as linear regression ), as shown in Figure 6.7, there are MP=2\nparameters,theslope a2,andtheyintercepta1.Noticethatwhilethereareonlytwoparame-\nterstodetermine,therestillmaybeanarbitrarynumber NDofdatapointstofit.Remember,\nauniquesolutionisnotpossibleunlessthenumberofdatapointsisequaltoorgreaterthan\nthenumberofparameters.Forthislinearcase,therearejusttwoderivatives,\nùúïg(xi)\nùúïa1=1,ùúïg(xi)\nùúïa2=xi, (6.45)\nandaftersubstitution,the ùúí2minimizationequations(6.43)canbesolved:\na1=SxxSy‚àíSxSxy\nŒî, a2=SSxy‚àíSxSy\nŒî, (6.46)\nS=ND‚àë\ni=11\nùúé2\ni,Sx=ND‚àë\ni=1xi\nùúé2\ni,Sy=ND‚àë\ni=1yi\nùúé2\ni, (6.47)\nSxx=ND‚àë\ni=1x2\ni\nùúé2\ni,Sxy=ND‚àë\ni=1xiyi\nùúé2\ni,Œî=SSxx‚àíS2\nx. (6.48)\n0100200300400\n0 400 800 1200 1600 2000\nxy(x)\nFigure 6.7 A linear least-squares best Ô¨Åt of a straight line to data. The deviation of theory from\nexperiment is greater than would be expected from statistics, which means that a straight line is\nnot a good theory to describe these data.",11129
54-6.7.2 Linear Quadratic Fit.pdf,54-6.7.2 Linear Quadratic Fit,"116 6 Trial-and-Error Searching and Data Fitting\nStatistics also gives you an expression for the varianceor uncertainty in the deduced\nparameters:\nùúé2\na1=Sxx\nŒî,ùúé2\na2=S\nŒî. (6.49)\nThesearemeasuresoftheuncertaintiesinthevaluesofthefittedparametersarisingfrom\ntheuncertainties ùúéiinthemeasured yivalues.Ameasureofthedependenceoftheparam-\netersoneachotherisgivenbythe correlationcoefficient :\nùúå(a1,a2)=cov(a1,a2)\nùúéa1ùúéa2,cov(a1,a2)=‚àíSx\nŒî. (6.50)\nHerecov (a1,a2)isthecovariance ofa1anda2,andvanishesif a1anda2areindependent.\nThecorrelationcoefficient ùúå(a1,a2)liesintherange ‚àí1‚â§ùúå‚â§1,withapositive ùúåindicating\nthat the errors in a1anda2are likely to have the same sign, and a negative ùúåindicating\noppositesigns.\nThe preceding analytic solutions for the parameters are of the form found in statistics\nbooks,butarenotoptimalfornumericalcalculationsbecausesubtractivecancellationcan\ndecreasetheaccuracyoftheanswers.Arearrangementoftheequationscandecreasethis\ntypeoferror[Thompson1992]:\na1=y‚àía2x,a2=Sxy\nSxx,x=1\nNNd‚àë\ni=1xi,y=1\nNNd‚àë\ni=1yi\nSxy=Nd‚àë\ni=1(xi‚àíx)(yi‚àíy)\nùúé2\ni,Sxx=Nd‚àë\ni=1(xi‚àíx)2\nùúé2\ni. (6.51)\nInFit.pyinListing6.4,wegiveaprogramthatfitsaparabolatosomedata.Youcanuseit\nasamodelforfittingalinetodata,althoughyoucanalsouseourclosed-formexpressions\nforastraight-linefit.\n6.7.2 Linear Quadratic Fit\nAsindicatedearlier,aslongasthefunctionbeingfitteddepends linearlyontheunknown\nparameters ai,theconditionofminimum ùúí2leadstoasetofsimultaneouslinearequations\n0.40.81.21.6\n1 1.2 1.4 1.6 1.8 2\nxy(x)Figure 6.8 A linear least-squares best\nÔ¨Åt of a parabola to data. Here we see\nthat the Ô¨Åt misses approximately\none-third of the points, as expected\nfrom the statistics for a good Ô¨Åt.\n6.7 Least-Squares Fitting 117\nfor thea‚Äôs that can be solved by hand, or on the computer using matrix techniques. To\nillustrate,supposewewanttofitthequadraticpolynomial\ng(x)=a1+a2x+a3x2(6.52)\ntotheexperimentalmeasurements( xi,yi,i=1,ND)showninFigure6.8.Becausethis g(x)is\nlinearintheparameters ai,thefitisstilllinear,eventhough xisraisedtothesecondpower.\n[However,ifwetriedtoafitafunctionoftheform g(x)=(a1+a2x)exp(‚àía3x)tothedata,\nthen we would not be able to make a linear fit because there is not a linear dependence\nona3.]\nThebestfitofthisquadratictothedataisobtainedbyapplyingtheminimum ùúí2condition\n(6.43)forMp=3parametersand ND(stillarbitrary)datapoints.Equation(6.43)leadsto\nthethreesimultaneousequationsfor a1,a2,anda3:\nND‚àë\ni=1[yi‚àíg(xi)]\nùúé2\niùúïg(xi)\nùúïa1=0,ùúïg\nùúïa1=1, (6.53)\nND‚àë\ni=1[yi‚àíg(xi)]\nùúé2\niùúïg(xi)\nùúïa2=0,ùúïg\nùúïa2=x, (6.54)\nND‚àë\ni=1[yi‚àíg(xi)]\nùúé2\niùúïg(xi)\nùúïa3=0,ùúïg\nùúïa3=x2. (6.55)\nNote:Becausethederivativesareindependentoftheparameters(the a‚Äôs),theadependence\narisesonlyfromthetermsinsquarebracketsinthesums,andbecausethosetermshavea\nlineardependenceonthe a‚Äôs,theseequationsarelinearinthe a‚Äôs.\nExercise Showthataftersomerearrangement,(6.53)‚Äì(6.55)canbewrittenas\nSa1+Sxa2+Sxxa3=Sy, (6.56)\nSxa1+Sxxa2+Sxxxa3=Sxy,\nSxxa1+Sxxxa2+Sxxxxa3=Sxxy.\nHerethedefinitionsofthe S‚Äôsaresimpleextensionsofthoseusedin(6.46)‚Äì(6.48)andare\nprogrammedin Fit.pyinListing6.4.Afterplacingthethreeunknownparametersintoa\nvectorxandtheknownthreetermsin(6.56)intoavector ‚Éób,theseequationsassumethe\nmatrixform:\nA‚Éóx=‚Éób, (6.57)\nA=‚é°\n‚é¢\n‚é¢‚é£SSxSxx\nSxSxxSxxx\nSxxSxxxSxxxx‚é§\n‚é•\n‚é•‚é¶, ‚Éóx=‚é°\n‚é¢\n‚é¢‚é£a1\na2\na3‚é§\n‚é•\n‚é•‚é¶,‚Éób=‚é°\n‚é¢\n‚é¢‚é£Sy\nSxy\nSxxy‚é§\n‚é•\n‚é•‚é¶.\nThe solution for the parameter vector ‚Éóxis obtained by solving the matrix equations.\nAlthough for 3 √ó3 matrices we can write out the solution in closed form, for larger\nproblemsthenumericalsolutionrequiresamatrixcomputation.",3589
55-6.7.2.1 Linear Quadratic Fit Assessment.pdf,55-6.7.2.1 Linear Quadratic Fit Assessment,,0
56-6.8 Nonlinear Fit to a Resonance.pdf,56-6.8 Nonlinear Fit to a Resonance,"118 6 Trial-and-Error Searching and Data Fitting\n6.7.2.1 Linear Quadratic Fit Assessment\n1) Fitthequadratic(6.52)tothefollowingdatasets[givenas (x1,y1),(x2,y2),‚Ä¶].Ineach\ncase indicate the values found for the as, the number of degrees of freedom, andthe\nvalueofùúí2.\na)(0,1)\nb)(0,1),(1,3)\nc)(0,1),(1,3),(2,7)\nd)(0,1),(1,3),(2,7),(3,15)\n2) Findafittothelastsetofdatatothefunction y=Ae‚àíbx2.\nHint:Ajudiciouschangeofvariableswillpermityoutoconvertthistoalinearfit.Does\naminimum ùúí2stillhavemeaninghere?\n6.8 Nonlinear Fit to a Resonance\nRecall how earlier in this chapter we interpolated the values in Table 6.1 in order to\nobtaintheexperimentalcrosssection ùúéasafunctionofenergy.Althoughwedidnotuse\nit, we also gave the theory describing these data, namely, the Breit‚ÄìWigner resonance\nformula(6.20):\nf(E)=fr\n(E‚àíEr)2+Œì2‚àï4. (6.58)\nYour problemistodeterminewhatvaluesfortheparameters Er,fr,andŒìin(6.58)provide\nthebestfittothedatainTable6.1.\nSince(6.58)isnotalinearfunctionoftheparameters( Er,fr,Œì),thethreeequationsthat\nresultfromminimizing ùúí2arenotlinearequations,andsocannotbesolvedbythetech-\nniquesoflinearalgebra.However,inourstudyofthemassesonastringproblemweshow\nhowtousetheNewton‚ÄìRaphsonalgorithmtosearchforsolutionsofsimultaneousnon-\nlinearequations.Thattechniqueinvolvedexpansionoftheequationsaboutthe previous\nguess to obtain a set of linear equations, and then solving the linear equations with the\nmatrix libraries. We now use this same combination of fitting, trial-and-error searching,\nandmatrixalgebratoconductanonlinearleast-squaresfitof(6.58)tothedatainTable6.1.\nRecall that the condition for a best fit is to find values of the MPparameters amin the\ntheoryg(x,am)thatminimize ùúí2=‚àë\ni[(yi‚àígi)‚àïùúéi]2.Thisleadstothe MPequations(6.43)\ntosolve\nND‚àë\ni=1[yi‚àíg(xi)]\nùúé2\niùúïg(xi)\nùúïam=0,(m=1,MP). (6.59)\nTofindtheformoftheseequationsappropriatetoourproblem,werewriteourtheoryfunc-\ntion(6.58)inthenotationof(6.59):\na1=fr,a2=ER,a3=Œì2‚àï4,x=E, (6.60)\n‚áíg(x)=a1\n(x‚àía2)2+a3. (6.61)\n6.8 Nonlinear Fit to a Resonance 119\nThethreederivativesrequiredin(6.59)arethen\nùúïg\nùúïa1=1\n(x‚àía2)2+a3,ùúïg\nùúïa2=‚àí2a1(x‚àía2)\n[(x‚àía2)2+a3]2,ùúïg\nùúïa3=‚àía1\n[(x‚àía2)2+a3]2.\nSubstitutionofthesederivativesintothebest-fitcondition(6.59)yieldsthreesimultaneous\nequationsin a1,a2,anda3thatweneedtosolveinordertofitthe ND=9datapoints (xi,yi)\ninTable6.1:\n9‚àë\ni=1yi‚àíg(xi,a)\n(xi‚àía2)2+a3=0,9‚àë\ni=1yi‚àíg(xi,a)\n[(xi‚àía2)2+a3]2=0,\n9‚àë\ni=1{yi‚àíg(xi,a)}(xi‚àía2)\n[(xi‚àía2)2+a3]2=0. (6.62)\nEven without the substitution of (6.58) for g(x,a), it is clear that these three equations\ndepend on the a‚Äôs in a nonlinear fashion. That‚Äôs okay because in Section 6.3 we derived\ntheN-dimensionalNewton‚ÄìRaphsonsearchfortherootsof\nfi(a1,a2,‚Ä¶,aN)=0,i=1,N, (6.63)\nwherewehavemadethechangeofvariable yi‚Üíaiforthepresentproblem.Weusethat\nsameformalismhereforthe N=3equations(6.62)bywritingthemas\nf1(a1,a2,a3)=9‚àë\ni=1yi‚àíg(xi,a)\n(xi‚àía2)2+a3=0, (6.64)\nf2(a1,a2,a3)=9‚àë\ni=1{yi‚àíg(xi,a)}(xi‚àía2)\n[(xi‚àía2)2+a3]2=0, (6.65)\nf3(a1,a2,a3)=9‚àë\ni=1yi‚àíg(xi,a)\n[(xi‚àía2)2+a3]2=0. (6.66)\nBecausefr‚â°a1isthepeakvalueofthecross section, ER‚â°a2istheenergyatwhichthe\npeakoccurs,and Œì=2‚àöa3isthefullwidthofthepeakathalf-maximum,goodguessesfor\nthea‚Äôscanbeextractedfromagraphofthedata.Toobtaintheninederivativesofthethree\nf‚Äôswithrespecttothethreeunknown a‚Äôs,weusetwonestedloopsover iandj,alongwith\ntheforward-differenceapproximationforthederivative\nùúïfi\nùúïaj‚âÉfi(aj+Œîaj)‚àífi(aj)\nŒîaj, (6.67)\nwhereŒîajcorrespondstoasmall,say ‚â§1%,changeintheparametervalue.\nNonlinear Fit Exercise UsetheNewton‚ÄìRaphsonalgorithmasoutlinedinSection6.8\ntoconductanonlinearsearchforthebest-fitparametersoftheBreit‚ÄìWignertheory(6.58)to\nthedatainTable6.1.Comparethededucedvaluesof (fr,ER,Œì)tothatobtainedbyinspection\nofthegraph.",3765
57-6.9 Code Listings.pdf,57-6.9 Code Listings,"120 6 Trial-and-Error Searching and Data Fitting\n6.9 Code Listings\nListing 6.1 TheBisection.py codeisasimpleimplementationofthebisectionalgorithm\nforfindingazeroofafunction,inthiscase2cos x‚àíx.\n1# Bisection .py: zero of f(x) via Bisection algorithm within [a,b]\nfromvpython import ‚àó\neps = 1e ‚àí3; Nmax = 100; a = 0.0; b = 7.0 # Precision , [a,b]\n5\ndeff(x):return2‚àócos(x) ‚àíx # Your function here\ndefBisection(Xminus, Xplus, Nmax, eps): # Do not change\n9foritin range (0, Nmax):\nx=( X p l u s+ X m i n u s ) / 2 .\nprint(""i t= "",i t , ""x="",x , "" f(x) ="" ,f ( x ) )\nif(f(Xplus) ‚àóf(x) > 0.): Xplus = x #C h a n g ex +t ox\n13 else:X m i n u s= x #C h a n g ex ‚àíto x\nif(abs(f(x) ) <=eps): # Converged?\nprint(""\n Root found with precision eps = "" ,e p s )\nbreak\n17 ifit == Nmax ‚àí1:print(""\n No root after N iterations\n"" )\nreturnx\nroot = Bisection(a, b, Nmax, eps)\n21print("" Root ="" , root)\nListing 6.2 NewtonCD.py UsestheNewton‚ÄìRaphsonmethodtosearchforazeroofthe\nfunctionf(x).Acentral-differenceapproximationisusedtodetermine fd/dx.\n1# NewtonCD . py Newton Search with c e n t r a l d i f f e r e n c e\nfrommathimportcos\n5x = 1111.; dx = 3.e ‚àí4; eps = 0.002; Nmax = 100; # Parameters\ndeff(x):return2‚àócos(x) ‚àíx# Function\n9foritin range (0 , Nmax + 1) :\nF=f( x )\nif(abs(F) ‚àó<‚àó=e p s ) : # Converged?\nprint(""\n Root found, f(root) ="" ,F , "" ,e p s="" ,e p s )\n13 break\nprint(""Iteration # = "" ,i t , ""x="",x , "" f(x) = "" ,F )\ndf = (f(x+dx/2) ‚àíf(x‚àídx/2))/dx # Central diff\ndx =‚àíF/df\n17x+ = d x # N e w guess\nListing 6.3 SplineInteract.py Performsacubicsplinefittodatawithinteractivecontrol.\n1# SplineInteract .py Spline fit with slide to control number of points\nfromvisualimport ‚àó; fromvisual.graph import ‚àó;\nfromvisual.graph importgdisplay , gcurve\n5fromvisual.controls importslider , controls , toggle\nx = array([0., 0.12, 0.25, 0.37, 0.5, 0.62, 0.75, 0.87, 0.99]) # input\ny = array([10.6, 16.0, 45.0, 83.5, 52.8, 19.9, 10.8, 8.25, 4.7])\n9n=9 ; n p=1 5\n# Initialize\ny2 = zeros( (n), float); u= zeros( (n), float)\n6.9 Code Listings 121\n13graph1 = gdisplay(x=0,y=0,width=500, height=500,\ntitle= ‚ÄôSpline Fit‚Äô , xtitle= ‚Äôx‚Äô, ytitle= ‚Äôy‚Äô)\nfunct1 = gdots(color = color.yellow)\nfunct2 = gdots(color = color.red)\n17graph1.visible = 0\ndefupdate(): # Nfit = 30 = output\nNfit =int(control.value)\n21foriin range (0, n): # Spread out points\nfunct1.plot(pos = (x[i], y[i]) )\nfunct1.plot(pos = (1.01 ‚àóx[i], 1.01 ‚àóy[i]) )\nfunct1.plot(pos = (.99 ‚àóx[i], .99 ‚àóy[i]) )\n25 yp1 = (y[1] ‚àíy[0]) / (x[1] ‚àíx[0])‚àí(y[2]‚àíy[1])/ \\n(x[2]‚àíx[1])+(y[2] ‚àíy[0])/(x[2] ‚àíx[0])\nypn = (y[n ‚àí1]‚àíy[n‚àí2])/(x[n ‚àí1]‚àíx[n‚àí2])‚àí(y[n‚àí2]‚àíy[n‚àí3])/(x[n ‚àí2]‚àíx[n‚àí3]) +\n(y[n‚àí1]‚àíy[n‚àí3])/(x[n ‚àí1]‚àíx[n‚àí3])\nif(yp1 > 0.99e30): y2[0] = 0.; u[0] = 0.\n29else:\ny2[0] = ‚àí0.5\nu[0] = (3./(x[1] ‚àíx[0]) ) ‚àó(( y [ 1 ] ‚àíy[0])/(x[1] ‚àíx[0])‚àíyp1)\nforiin range (1, n‚àí1): # Decomp loop\n33 sig = (x[i] ‚àíx[i‚àí1])/(x[i + 1] ‚àíx[i‚àí1])\np=s i g ‚àóy2[i‚àí1] + 2.\ny2[i] = (sig ‚àí1.)/p\nu[i] = (y[i+1] ‚àíy[i])/(x[i+1] ‚àíx[i])‚àí(y[i]‚àíy[i‚àí1])/(x[i] ‚àíx[i‚àí1])\n37 u[i] = (6. ‚àóu[i]/(x[i + 1] ‚àíx[i‚àí1])‚àísig‚àóu[i‚àí1])/p\nif(ypn > 0.99e30): qn = un = 0. # Test for natural\nelse:\nqn = 0.5;\n41 un = (3/(x[n ‚àí1]‚àíx[n‚àí2])) ‚àó(ypn‚àí(y[n‚àí1]‚àíy[n‚àí2])/(x[n ‚àí1]‚àíx[n‚àí2]))\ny2[n‚àí1] = (un ‚àíqn‚àóu[n‚àí2])/(qn ‚àóy2[n‚àí2] + 1.)\nforkin range (n‚àí2, 1, ‚àí1):\ny2[k] = y2[k] ‚àóy2[k + 1] + u[k]\n45foriin range (1, Nfit + 2): # Begin fit\nxout = x[0] + (x[n ‚àí1]‚àíx[0]) ‚àó(i‚àí1)/(Nfit)\nklo = 0; khi = n ‚àí1 # Bisection algor\nwhile(khi‚àíklo >1):\n49 k=( k h i+k l o )> >1\nif(x[k] > xout): khi = k\nelse:k l o= k\nh=x [ k h i ] ‚àíx[klo]\n53 if(x[k] > xout): khi = k\nelse:k l o= k\nh=x [ k h i ] ‚àíx[klo]\na=( x [ k h i ] ‚àíxout)/h\n57 b=( x o u t ‚àíx[klo])/h\nyout = a ‚àóy[klo] + b ‚àóy[khi] +\n((a‚àóa‚àóa‚àía)‚àóy2[klo]+(b ‚àób‚àób‚àíb)‚àóy2[khi]) ‚àóh‚àóh/6\nfunct2.plot(pos = (xout, yout) )\nc=controls(x=500,y=0,width=200,height=200) # Control via slider\n61control = slider(pos=( ‚àí50,50,0), min=2 ,max= 100, action = update)\ntoggle(pos = (0, 35, ‚àí5), text1 = ""Number of points"" , height = 0)\ncontrol.value = 2\nupdate()\n65\nwhile1:\nc.interact()\nrate(50) # update <10/sec\n69funct2.visible = 0\nListing 6.4 Fit.py Performs a least-squares fit of a parabola to data using the NumPy\nlinagepackagetosolvethesetoflinearequations S‚Éóa=‚Éós.\n1# Fit .py: Linear least square fit via matrix solution\nimportpylab as p\nfromnumpyimport ‚àó;fromnumpy. linalg importinv, solve\n5\n122 6 Trial-and-Error Searching and Data Fitting\nNd = 7\nA=z e r o s( ( 3, 3 ), float); bvec = zeros((3,1), float) # Initialize\nss= sx = sxx = sy = sxxx = sxxxx = sxy = sxy = sxxy = 0.\n9x = array([1., 1.1, 1.24, 1.35, 1.451, 1.5, 1.92]) #xv a l u e s\ny = array([0.52, 0.8, 0.7, 1.8, 2.9, 2.9, 3.6]) #yv a l u e s\nsig = array([0.1, 0.1, 0.2, 0.3, 0.2, 0.1, 0.1]) # Error bars\nxRange = arange(1.0, 2.0, 0.1) #F o rp l o t s\n13p.plot(x, y, ‚Äôbo‚Äô) #B l u ed a t a\np.errorbar(x,y,sig)\np.title( ‚ÄôLeast Square Fit of Parabola to Blue Data‚Äô )\np.xlabel( ‚Äôx‚Äô); p.ylabel( ‚Äôy‚Äô); p.grid(True) # Plot grid\n17\nforiin range (0, Nd):\nsig2 = sig[i] ‚àósig[i]\nss += 1. / sig2; sx += x[i]/sig2; sy += y[i]/sig2\n21 rhl = x[i] ‚àóx[i]; sxx += rhl/sig2; sxxy += rhl ‚àóy[i]/sig2\nsxy += x[i] ‚àóy[i]/sig2; sxxx +=rhl ‚àóx[i]/sig2; sxxxx +=rhl ‚àórhl/sig2\nA = array([ [ss,sx,sxx], [sx,sxx,sxxx], [sxx,sxxx,sxxxx] ])\nbvec = array([sy, sxy, sxxy])\n25xvec = multiply(inv(A), bvec) # Invert matrix\nprint(‚Äô\n x via Inverse A\n‚Äô ,x v e c , ‚Äô\n‚Äô)\nxvec = solve(A, bvec) # Solve via elimination\nprint(‚Äô\n x via Elimination \n‚Äô ,x v e c , ‚Äô\n Fit to Parabola\n‚Äô )\n29print(‚Äôy(x) = a0 + a1 x + a2 xÀÜ2\n a0 =‚Äô ,x [ 0 ] , ‚Äôa1 =‚Äô,x [ 1 ] , ‚Äôa2 =‚Äô,x [ 2 ] )\nprint(‚Äô\n i xi yi yfit ‚Äô )\nforiin range (0, Nd):\ns = xvec[0] + xvec[1] ‚àóx[i] + xvec[2] ‚àóx[i] ‚àóx[i]\n33print("" %d %5.3f %5.3f %8.7f"" %(i, x[i], y[i], s))\n# red line is the fit , red dots the fits at y[ i ]m\ncurve = xvec[0] + xvec[1] ‚àóxRange + xvec[2] ‚àóxRange ‚àó‚àó2\npoints = xvec[0] + xvec[1] ‚àóx+x v e c [ 2 ] ‚àóx‚àó‚àó2\n37p.plot(xRange, curve, ‚Äôr‚Äô, x, points, ‚Äôro‚Äô)\np.show()",5971
58-Chapter 7 Matrix Computing and ND Searching.pdf,58-Chapter 7 Matrix Computing and ND Searching,,0
59-7.1 Masses on a String and ND Searching.pdf,59-7.1 Masses on a String and ND Searching,"123\n7\nMatrix Computing and N‚ÄìD Searching\nThis chapter discusses how to compute with matrices, and, in particular, the use of the Python\nmatrix and linear algebra packages. The chapter ends with a discussion of how to speed up\nlarge matrix computations .\n7.1 Masses on a String and N‚ÄìD Searching\nProblem Two masses with weights (W1,W2)=(10,20)are connected by three pieces\nof string with lengths (L1,L2,L3)=(3,4,4), and hung from a horizontal bar of length\nL=8(Figure7.1).Findtheanglesassumedbythestringsandthetensionsexertedbythe\nstrings.\nInspiteofthefactthatthisisasimpleproblemrequiringnomorethanfirst-yearphysics\nto formulate, the coupled transcendental equations that result are just about impossible\ntosolveanalytically.1Weapproachitasamatrixproblemcombinedwithatrial-and-error\nsearch.\nWestartwiththegeometricconstraintsthatthehorizontallengthofthestructureis L,\nandthatthestringsbeginandendatthesameheight(Figure7.1):\nL1cosùúÉ1+L2cosùúÉ2+L3cosùúÉ3=L, (7.1)\nL1sinùúÉ1+L2sinùúÉ2‚àíL3sinùúÉ3=0, (7.2)\nsin2ùúÉ1+cos2ùúÉ1=1, (7.3)\nsin2ùúÉ2+cos2ùúÉ2=1, (7.4)\nsin2ùúÉ3+cos2ùúÉ3=1. (7.5)\nObservethatsincewetreatsin ùúÉandcosùúÉasindependentvariables,wehaveincludedthree\ntrigonometricidentitiesasindependentequations.Thebasicsphysics(Figure7.2)saysthat\nbecause there are no accelerations, the sum of the forces in the horizontal and vertical\n1 Almostimpossibleanyway,asL.Molarhassuppliedmewithananalyticsolution.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n124 7 Matrix Computing and N‚ÄìD Searching\nT1L1L\nT2\nL2T3\nL3W1\nW2Œ∏1Œ∏3\nŒ∏2\nŒ∏3Figure 7.1 Two masses with weights (W1,W2)\nare connected by three pieces of string of\nlengths(L1,L2,L3), and hung from a horizontal\nbar of length L. The lengths are all known, but\nthe angles and the tensions in the strings are to\nbe determined.\nTi\nTi+1\nWiŒ∏i\nŒ∏i+1iFigure 7.2 A free-body diagram for one weight in equilibrium.\nBalancing the forces in the xandydirections for all weights leads\nto the equations of static equilibrium.\ndirectionsmustequalzero:\nT1sinùúÉ1‚àíT2sinùúÉ2‚àíW1=0, (7.6)\nT1cosùúÉ1‚àíT2cosùúÉ2=0, (7.7)\nT2sinùúÉ2+T3sinùúÉ3‚àíW2=0, (7.8)\nT2cosùúÉ2‚àíT3cosùúÉ3=0. (7.9)\nHereWiistheweightofmass iandTiisthetensioninstring i.Notethatbecausewedonot\nhavearigidstructure,wecannotassumeanequilibriumoftorques.\nEquations(7.1)‚Äì(7.9)areninesimultaneous, nonlinearequations,whichbeingnonlinear\ncannotbesolvedwith linearalgebra.However,youcanextendtheNewton‚ÄìRaphsonalgo-\nrithmtomultipleequations,and searchforasolution.Tothateffect,werenamethenine\nunknownanglesandtensionsasthesubscriptedvariable yi,andplaceallofthevariables\nintoavector:\ny=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£x1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\nx9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£sinùúÉ1\nsinùúÉ2\nsinùúÉ3\ncosùúÉ1\ncosùúÉ2\ncosùúÉ3\nT1\nT2\nT3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (7.10)\n7.1 Masses on a String and N‚ÄìD Searching 125\nThenineequationstobesolvedarewritteninageneralformwithzerosontheright-hand\nsides(RHS),andalsoplacedinavector:\nfi(x1,x2,‚Ä¶,xN)=0,i=1,N, (7.11)\nf(y)=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£f1(y)\nf2(y)\nf3(y)\nf4(y)\nf5(y)\nf6(y)\nf7(y)\nf8(y)\nf9(y)‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£3x4+4x5+4x6‚àí8\n3x1+4x2‚àí4x3\nx7x1‚àíx8x2‚àí10\nx7x4‚àíx8x5\nx8x2+x9x3‚àí20\nx8x5‚àíx9x6\nx2\n1+x2\n4‚àí1\nx2\n2+x2\n5‚àí1\nx2\n3+x2\n6‚àí1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=0. (7.12)\nIn words, we are looking for set of nine xivalues for which all nine fi‚Äôs vanish simul-\ntaneously. Although these equations are not very complicated (the physics after all is\nelementary),thetermsquadraticin xmakethemnonlinear.\nThe search procedure guesses a solution, expands the nonlinear equations and keeps\njustthelinearterms,solvesthelinearequations,andmakesabetterguessbasedonhow\nclose the previous guess was to making f=0. The search starts with the approximate\nsolution at any one stage called the set xi, and assumes that there are (yet unknown)\ncorrections Œîxiforwhich\nfi(x1+Œîx1,x2+Œîx2,‚Ä¶,x9+Œîx9)=0,i=1,9. (7.13)\nWesolvefortheapproximate Œîxi‚Äôsbyassumingthatourprevioussolutioniscloseenough\ntotheactualonefortwotermsintheTaylorseriestobeaccurate:\nfi(x1+Œîx1,..,x9+Œîx9)‚âÉfi(x1,..,x9)+9‚àë\nj=1ùúïfi\nùúïxjŒîxj=0,i=1,9.\nWenowhaveasetofninelinearequationsinthenineunknowns Œîxi,whichweexpressas\nasinglematrixequation\nf1+ùúïf1‚àïùúïx1Œîx1+ùúïf1‚àïùúïx2Œîx2+¬∑¬∑¬∑+ùúïf1‚àïùúïx9Œîx9=0,\nf2+ùúïf2‚àïùúïx1Œîx1+ùúïf2‚àïùúïx2Œîx2+¬∑¬∑¬∑+ùúïf2‚àïùúïx9Œîx9=0,\n...\nf9+ùúïf9‚àïùúïx1Œîx1+ùúïf9‚àïùúïx2Œîx2+¬∑¬∑¬∑+ùúïf9‚àïùúïx9Œîx9=0,\nor‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£f1\nf2\n...\nf9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶+‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùúïf1‚àïùúïx1ùúïf1‚àïùúïx2¬∑¬∑¬∑ùúïf1‚àïùúïx9\nùúïf2‚àïùúïx1ùúïf2‚àïùúïx2¬∑¬∑¬∑ùúïf2‚àïùúïx9\n...\nùúïf9‚àïùúïx1ùúïf9‚àïùúïx2¬∑¬∑¬∑ùúïf9‚àïùúïx9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Œîx1\nŒîx2\n...\nŒîx9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=0. (7.14)",4812
60-7.2 Matrix Generalities.pdf,60-7.2 Matrix Generalities,"126 7 Matrix Computing and N‚ÄìD Searching\nNotenowthatthederivativesandthe f‚Äôsareallevaluatedatknownvaluesofthe xi‚Äôs,sothat\nonlythevectorofthe Œîxivaluesisunknown.Wewritethisequationinmatrixnotationas\nf+F‚Ä≤ùö´x=0,‚áíF‚Ä≤ùö´x=‚àíf, (7.15)\nùö´x=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Œîx1\nŒîx2\n...\nŒîx9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,f=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£f1\nf2\n...\nf9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,F‚Ä≤=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùúïf1‚àïùúïx1¬∑¬∑¬∑ùúïf1‚àïùúïx9\nùúïf2‚àïùúïx1¬∑¬∑¬∑ùúïf2‚àïùúïx9\n...\nùúïf9‚àïùúïx1¬∑¬∑¬∑ùúïf9‚àïùúïx9‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,\nwhereweuseboldcharacterstodenotethevectorandmatrixnatureoftheequations.\nTheequation F‚Ä≤ùö´x=‚àífisinthestandardformforthesolutionofalinearequation(often\nwrittenAx=b), where ùö´xis the vector of unknowns and b=‚àíf. Matrix equations are\nsolved using the techniques of linear algebra, which we will discuss shortly. In a formal\nsense, the solution of (7.15) is obtained by multiplying both sides of the equation by the\ninverseofthe F‚Ä≤matrix:\nùö´x=‚àíF‚Ä≤‚àí1f, (7.16)\nwheretheinversemustexistifthereistobeauniquesolution.Althoughwearedealingwith\nmatricesnow,thissolutionisidenticalinformtothatofthe1Dproblem,equation(6.11),\nŒîx=‚àí (1‚àïf‚Ä≤)f.Theabstractnotationformatricesisseentorevealthesimplicitythatlies\nwithin.\nAswehavenotedforthesingle-equationNewton‚ÄìRaphsonmethodinSection6.3,even\nifwecanderiveanalyticexpressionsforthederivatives ùúïfi‚àïùúïxj,thereare9 √ó9=81such\nderivativesforthis(small)problem,andenteringthemallwouldbebothtime-consuming\nand error-prone. In contrast, it is straightforward to program up a forward-difference\napproximationforthederivatives,\nùúïfi\nùúïxj‚âÉfi(xj+Œîxj)‚àífi(xj)\nŒîxj, (7.17)\nwhere, for partial derivatives, each individual xjis varied independently, and the ùõøxjare\narbitrarysmallchanges.Whileacentral-differenceapproximationforthederivativewould\nbe more accurate, it would also require more evaluations of the f‚Äô s ,a n do n c ew efi n da\nsolutionitdoesnotmatterhowaccurateouralgorithmforthederivativewas.\nAsalsodiscussedforthe1DNewton‚ÄìRaphsonmethod(Section6.3.1),themethodcan\nfail if the initial guess is not close enough to the zeros of all nine f‚Äôs. Thebacktracking\ntechnique(usingfractional Œîxguesses) maybeappliedhereaswell,inthepresentcase,\nprogressivelydecreasingthecorrections Œîxiuntil |f|2=|f1|2+|f2|2+¬∑¬∑¬∑+ |fN|2becomes\nacceptablysmall.\n7.2 Matrix Generalities\nOftenaphysicaltheoryiseasiertounderstandwhenexpressedmoreabstractlywithmatri-\nces, an example being the rotation of solid bodies with the inertia tensor. It should not\nbe surprising then that scientific computing often involve matrices. This is a good thing\nsince computers are good at continued repetition of simple instructions, and that is just\n7.2 Matrix Generalities 127\nwhatmatrixmanipulationsinvolve.Andasphysicalsystemsgetmorerealisticandmore\ncomplex, the equations used to describe them often involves the manipulations of large\nmatrices,whicharenohardertoprogramupthansmallones.\nWestronglyrecommendtheuseofthepowerfulandrobustlinearalgebralibrariesthathave\nbeen perfected over decades . Their programs are usually an order of magnitude, or more,\nfasterthantheelementarymethodsfoundinlinearalgebratexts,2aredesignedtominimize\nround-off error, and are often ‚Äúrobust,‚Äù that is, have a high chance of workingwell for a\nbroadclassofproblems.Anadditionalvalueoflibraryroutinesisthatyoucanoftenrun\nthesameprogrameitheronadesktopmachineoronaparallelsupercomputer,withmatrix\nroutinesautomaticallyadaptingtothelocalarchitecture.\nThemostbasicmatrixproblemisthesystemoflinearequations:\nAx=b, (7.18)\nwhereAisaknown N√óNmatrix,xisanunknownvectoroflength N,andbisaknown\nvectoroflength N.Theobviouswaytosolvethisequationistomultiplybothsidesbythe\ninverseofA:\nx=A‚àí1b. (7.19)\nBoth the direct solution (7.18) as it stands, and the determination of a matrix‚Äôs inverse\nare standards in subroutine libraries. The direct solution via Gaussian elimination or\nlower‚Äìupper (LU) decomposition tends to be faster, but sometime you may want the\ninverseforotherpurposes.\nIfyouhavetosolvethematrixequation\nAx=ùúÜx, (7.20)\nwithxanunknownvectorand ùúÜanunknownparameter,thenthesolution(7.19)willnot\nbeofmuchhelpbecausetheRHScontainstheunknowns ùúÜandx.Equation(7.20)isthe\neigenvalueproblem ,anditssolutionsexistforonlycertain,ifany,valuesof ùúÜ.T ofindthe\nsolution,weusetheidentitymatrix Itorewrite(7.20)as\n[A‚àíùúÜI]x=0. (7.21)\nWeseethatmultiplicationof(7.21)by [A‚àíùúÜI]‚àí1yieldsthetrivialsolution\nx=0(trivialsolution ). (7.22)\nWhilethetrivialsolutionisabonafidesolution,itisnonethelesstrivial.Amoreinteresting\nsolutionresultsfromtheconditionthatforbidsusfrommultiplyingbothsidesof(7.21)by\n[A‚àíùúÜI]‚àí1,namely,thenonexistenceoftheinverse.IfyourecallthatCrammer‚Äôsrulefor\ntheinverserequiresdivisionbydet [A‚àíùúÜI],itisclearthattheinversefailstoexist(andin\nthiswayeigenvalues doexist)when\ndet[A‚àíùúÜI]=0. (7.23)\nTheùúÜvalues that satisfy this secular equation are the eigenvalues of (7.20). If you are\ninterested in only the eigenvalues for (7.20), you should look for a matrix routine that\n2 AlthoughweprizethebookPress etal.[2007]andwhatithasaccomplished,wecannotrecommend\ntakingmatrixsubroutinesfromit.Theyareneitheroptimizednordocumentedforeasy,stand-aloneuse,\nwhereasthesubroutinelibrariesrecommendedinthischapterare.\n128 7 Matrix Computing and N‚ÄìD Searching\nsolves (7.23). First you need a subroutine to calculate the determinant of a matrix, and\nthen a search routine to zero in on the solution of (7.23). Such routines are available in\nlibraries.\nManyprogrammingbugsarisefromtheimproperuseofarrays.3Thismaybeasaresultof\ntheextensiveuseofmatricesinscientificcomputing,ortothecomplexityofkeepingtrack\nofindicesanddimensions.Inanycase,herearesomerulesofthumbtoobserve:\nTests:Alwaystestalibraryroutineonasmallproblemwhoseansweryouknow(suchas\nthetestsinSection7.4).Thenyou‚Äôllknowifyouaresupplyingitwiththerightarguments\nandifyouhaveallthelinksworking.\nPaging:Operatingsystemsstoredataandvariablesinfixed-length,contiguousblocksof\nmemory called pagesthat are treated as single entries in a page table. If there is not\nenough room in RAM for a page, then the entire page gets stored in virtual memory ,\nwhichmeansitgetsplacedonaslowdisk.Thisiscalled paging.Ifyourprogramdeals\nwith matrices that take up lots of memory, it may be near the memory limit at which\npagingoccurs,andthenevenaslightincreaseinthematrix‚Äôssizemayleadtoanorder-\nof-magnitudeincreaseinexecutiontime.(Therearesimilarissueswithcomputercaches,\nwhicharesmallamountsofsuperfastmemory,suchas1‚Äì8MB,thatfeedstheCPU.)\nComputersarefinite :Unlessyouarecareful,yourmatricesmayuseupsomuchmemory\nthatyourcomputationwillslowdownsignificantly,especiallyifitstartstousevirtual\nmemory. As a case in point, let‚Äôs say that you performing some matrix calculations\ninvolving4-Dmatrices,witheachindexhavinga physicaldimension of200,forexample,\nA[200] [200] [200] [200] . A single array of (200)464-byte words occupies ‚âÉ16GB of\nmemory.\nProcessing time : Matrix operations such as inversion require on the order of N3steps\nforasquarematrixofdimension N.Therefore,doublingthedimensionsofa2Dsquare\nmatrix(ashappenswhenthenumberofintegrationstepsisdoubled)leadstoan eightfold\nincreaseinprocessingtime.\nMatrixstorage :Whilewethinkofmatricesasmultidimensionalblocksofstorednumbers,\nthe computer stores them as linear strings. For instance, a matrix a[3,3]in Python, is\nstoredinrow-majororder :\na0,0a0,1a0,2a1,0a1,1a12a2,0a2,1a2,2‚Ä¶.\nThisdiffersfromFortran,wheresubscriptsusuallystartat1,andwherethestorageisin\ncolumn-majororder :\na1,1a2,1a3,1a1,2a2,2a3,2a1,3a2,3a3,3‚Ä¶.\nItisimportanttokeepthisstorageschemeinmindsincesomeroutinesassumelinear\nstorage,andsometimesPythonandFortranprogramsgetintermixed.\nMinimizingstride :Stride,theamountofmemoryskippedinordertogettothenextele-\nmentneededinacalculation,shouldbeminimized.Forinstance,summingthediagonal\nelementsofamatrixtoformthetrace\nTrA=N‚àë\ni=1a(i,i) (7.24)\n3 Evenavector V(N)iscalledan‚Äúarray,‚Äùalbeita1Done.",7865
61-7.3 Matrices in Python.pdf,61-7.3 Matrices in Python,,0
62-7.3.2 NumPy Matrices.pdf,62-7.3.2 NumPy Matrices,"7.3 Matrices in Python 129\ninvolves large stride because the diagonal elements are stored far apart for large N.\nHowever,thesum\nb(i)=a(i)+a(i+1) (7.25)\nhas stride 1 because adjacent elements of aare accessed. The basic rule for accessing\nindexedvariablesis\n‚óèKeepthestridelow,preferablyat1,whichinpracticemeans:\n‚óèVarytherightmostindexfirstonPythonandCarrays.\nAccessing matrices : Sometimes your effort at elegant programming may be very inef-\nficient. For example, it may be elegant to put all your data in one matrix with many\nindices,suchas VN,M,k,k‚Ä≤,Z,A,butitmayrequirethecomputertomakelarge stridesasyou\ngothroughthousandsof kandk‚Ä≤,values.Amoreefficientapproachmightbetobreak\nupthedataintoseveralmatrices,eachwithfewerindices,suchas VN,M,Uk,k‚Ä≤,andWZ,A.\n7.3 Matrices in Python\n7.3.1 Lists as Arrays\nAlistis Python‚Äôs built-in sequence of numbers or objects. Although called a ‚Äúlist,‚Äù it is\nsimilartowhatothercomputerlanguagescallan‚Äúarray.‚ÄùItmaybeeasierforyoutothink\nofaPythonlistasacontainerthatholdsabunchofitemsinadefiniteorder.(Soonwewill\ndescribe the higher-level, and recommended, arraydata type available with the NumPy\npackage.)Inthissection,wereviewsomeofPython‚Äôsnative listfeatures.\nPythoninterpretsasequenceofordereditems, L=l0,l1,‚Ä¶,lN‚àí1,asalistandrepresents\nitwithasinglesymbol L:\n>>> L = [1 , 2 , 3] #C r e a t el i s t\n>>> L[0] # Print element 0 (first)\n4 1 # Python output\n>>> L # Print entire list\n[1, 2, 3] # Output\n>>> L[0] = 5 # Change element 0\n8 >>> L\n[5, 2, 3]\n>>>len(L) # Length of list\n3\n12 >>>foritemsinL:printitems # for loop over items\n5\n2\n3\nObservethatsquarebracketswithcommaseparatorssuchas[1,2,3]areusedforlists,and\nthatasquarebracketisalsousedtoindicatetheindexforalistitem,asinline2(L[0]).Lists\ncontainsequencesofarbitraryobjectsthatare mutableorchangeable.Asweseeinline7\ninthe Lcommand,anentirelistcanbereferencedasasingleobject,inthiscasetoobtain\nitsprintout.\nPythonalsohasabuilt-intypeoflist,knownasa tuple,withelementsthatarenotmuta-\nble.Tuplesareindicatedbyroundparenthesis(..,..,.),withindividualelementsstillrefer-\nencedbysquarebrackets:\n130 7 Matrix Computing and N‚ÄìD Searching\n>>> T = (1 , 2 , 3 , 4) # Create tuple\n2>>> T[3] # Print element 3\n4\n>>> T\n(1, 2, 3, 4) # Print entire tuple\n6>>> T[0] = 5 # Attempt to change element 0\nTraceable (most recent call last):\nT[0] = 5\nError: ‚Äôtuple‚Äô object does notsupport item assignment\nNotetheerrormessagethatariseswhenwetrytochangeanelementofatuple.\nManylanguagesrequireyoutospecifythesizeofanarraybeforeyoucanstartstoring\nobjectsinit.Incontrast,Pythonlistsare dynamic,whichmeansthattheirsizesadjustas\nneeded. In addition, while a list is essentially one dimensional, a compound list can be\ncreatedbyhavingtheindividualelementsthemselvesaslists:\n>>> L = [[1,2], [3,4], [5,6]] #Al i s t o f l i s t s\n>>> L\n3[[1, 2], [3, 4], [5, 6]]\n>>> L[0] # The first element\n[1, 2]\nHerearesomemorelistoperations:\nOperation Effect Operation Effect\nL=[1,2,3,4] Formlist L1+L2 Concatenatelists\nL[i] ithelement len(L) Lengthoflist L\niinL Trueif iinL L[i:j] Slicefrom itoj\nforiinL Iterationindex L.append(x) Append xtoendofL\nL.count(x) Numberofx‚Äôsin LL.index(x) Locationof1st xinL\nL.remove(x) Remove1st xinLL.reverse() Reverseelementsin L\nL.sort() Orderelementsin L\n7.3.2 NumPy Matrices\nAlthough we have just described Python‚Äôs basic arraydata type, it is rather limited and\nwe suggest using NumPy arrays, which converts Python lists into arrays. In order to use\nNumPy,youmustimport NumPyintoyourprograms,asweshowhererunningourprogram\nMatrix.pyfromashell(the >>>):\n1 >>>fromnumpyimport ‚àó # Import NumPy package\n> > > vector1 = array([1, 2, 3, 4, 5]) # Fill 1D array\n3 >>>print(‚Äôvector1 =‚Äô ,vector1) # Print array (parens if Python 3)\nvector1 = [1 2 3 4 5] # Output\n5 >>> vector2 = vector1 + vector1 # Add 2 vectors\n>>>print(‚Äôvector2=‚Äô ,vector2) # Print vector2\n7 vector2= [ 2 4 6 8 10] # Output\n>>> vector2 = 3 ‚àóvector1 # Multi array by scalar\n9 >>>print(‚Äô3 * vector1 = ‚Äô , vector2) # Print vector\n3‚àóvector1 = [ 3 6 9 12 15] # Output\n11 >>> matrix1 = array(([0,1],[1,3])) # An array of arrays\n>>>print(matrix1) #P r i n tm a t r i x 1\n13 [[0 1]\n[1 3]]\n7.3 Matrices in Python 131\n15 >>>print(‚Äôvector1.shape= ‚Äô ,vector1.shape)\nvector1.shape = (5)\n17 >>>print(matrix1 ‚àómatrix1) # Matrix multiply\n[[0 1]\n19 [1 9]]\nWe see here that we have initialized an array object, have added two 1D array objects\ntogether,andhaveprintedouttheresult.Likewise,weseethatmultiplyinganarraybya\nconstantdoes,infact,multiplyeachelementbythatconstant(line8).Wethenconstruct\na‚Äúmatrix‚Äùasa1Darrayoftwo1Darrays,andwhenweprintitout,wenotethatitdoes\nindeedlooklikeamatrix.However,whenwemultiplythismatrixbyitself,theresultisnot\nthe[13\n31 0]\n, that one normally expects from matrix multiplication. So if you need actual\nmathematicalmatrices,thenyouneedtouseNumPy!\nNowwegivesomeexamplesoftheuseofNumPy,butdoreferthereadertotheNumPy\nTutorial[NumPy, 2023]andto thearticlesin ComputinginScience&Engineering [CiSE,\n2015] for more information. To start, we note that a NumPy array can hold up to 32\ndimensions (32 indices), but each element must be of the same type (a uniformarray).\nTheelementsarenotrestrictedtojustfloating-pointnumbersorintegers,butcanbeany\nobject, as long as all elements are of this same type. (Compound objects may be useful,\nfor example, for storing parts of data sets.) There are various ways to create arrays, with\nsquarebrackets[ ‚Ä¶]usedforindexinginallcases.WestartwithaPythonlist(tupleswork\naswell)andcreateanarrayfromit:\n1 >>>fromNumPyimport ‚àó\n>>> a = array ( [1 , 2 , 3 , 4] ) # Array from a list\n3 >>> a # Check with print\narray([1, 2, 3, 4])\nNoticethatitisessentialtohavethesquarebracketswithintheroundparenthesesbecause\nthesquarebracketsproducethelistobjectwhiletheroundparenthesesindicateafunction\nargument.Notetoothatbecausethedatainouroriginallistwereallintegers,thecreated\narrayisoneof32-bitintegerdatatypes,whichwecancheckbyaffixingthe typemethod:\n>>> a . dtype\n2 type(‚Äôint32‚Äô)\nIfwehadstartedwithfloating-pointnumbers,oramixoffloatsandint‚Äôs,wewouldhave\nendedupwithfloating-pointarrays:\n>>> b = array([1.2, 2.3, 3.4])\n2 >>> b\narray([ 1.2, 2.3, 3.4])\n4 >>> b . dtype\ntype(‚Äôfloat64‚Äô )\nWhen describing NumPy arrays, the number of ‚Äúdimensions,‚Äù dim, means the number\nof indices, which as we said can be as high as 32. What might be called the ‚Äúsize‚Äù\nor ‚Äúdimensions‚Äù of a matrix in mathematics is called the shapeof a NumPy array.\n132 7 Matrix Computing and N‚ÄìD Searching\nFurthermore,NumPydoeshavea sizemethodthatreturnsthetotalnumberofelements.\nBecausePython‚Äôslistsandtuplesareallonedimensional,ifwewantanarrayofaparticular\nshape,wecanattainthatbyaffixingthe reshapemethodwhenwecreatethearray.Where\nPython has a rangefunction to generate a sequence of numbers, NumPy has an arrange\nfunctionthatcreatesanarray,ratherthanalist.Hereweuseitandthenreshapethe1D\narrayintoa3 √ó4array:\n1 >>>importnumpy as np\n>>> np. arange (12) # List of 12 int‚Äôs in 1D array\n3 a r r a y ( [ 0 ,1 ,2 ,3 ,4 ,5 ,6 ,7 ,8 ,9 , 1 0 , 1 1 ] )\n>>> np.arange(12).reshape((3,4)) # Create, shape to 3x4 array\n5 array([[ 0, 1, 2, 3],\n[ 4, 5, 6, 7],\n7 [ 8, 9, 10, 11]])\n>>> a = np.arange(12).reshape((3,4)) # Give array a name\n9 >>> a\narray([[ 0, 1, 2, 3],\n11 [ 4, 5, 6, 7],\n[ 8, 9, 10, 11]])\n13 >>> a . shape # Shape = ?\n(3L, 4L)\n15 >>> a .ndim # Dimension?\n2\n17 >>> a . size # Size of a (number of elements)?\n12\nNotethatherewehaveimportedNumPyastheobject np,andthenaffixedthe arrangeand\nreshapemethodstothisobject.Wethencheckedtheshapeof a,andfoundittohavethree\nrowsandfourcolumnsoflongintegers(Python3mayjustsay int).Notetoo,asweseeon\nline9,NumPyusesparentheses()toindicatetheshapeofanarray,andso (3L,4L)indicates\nanarraywiththreerowsandfourcolumnsoflongint‚Äôs.\nNow that we have shapes on our minds, we should note that NumPy offers a number\nofwaystochangeshapes.Forexample,wecantransposeanarraywiththe .Tmethod,or\nreshapeintoavector:\n>>>fromnumpyimport ‚àó\n2 >>> a = arrange(12).reshape((3,4)) # Give array a name\n>>> a\n4 array([[ 0, 1, 2, 3],\n[ 4, 5, 6, 7],\n6 [ 8, 9, 10, 11]])\n>>> a .T # Transpose\n8 array([[ 0, 4, 8],\n[1 , 5 , 9 ] ,\n10 [ 2, 6, 10],\n[ 3, 7, 11]])\n12 >>> b = a.reshape( (1,12) ) # Form vector length 12\n>>> b\n14 array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]])\nAndagain, (1,12)indicatesanarraywithonerowand12columns.Yetanotherhandyway\nto take a matrix and extract just what you want from it is to use Python‚Äôs sliceoperator\nstart:stop:step: totakeasliceoutofanarray:\n>>> a\n2 array([[ 0, 1, 2, 3],\n[ 4, 5, 6, 7],\n4 [ 8, 9, 10, 11]])\n7.3 Matrices in Python 133\n>>> a [:2 , :] # First 2 rows\n6 array([[0, 1, 2, 3],\n[4, 5, 6, 7]])\n8 >>> a[:,1:3] # Columns 1 ‚àí3\narray([[ 1, 2],\n10 [5 , 6 ] ,\n[ 9, 10]])\nNote here how Python indices start counting from 0, and so 1:3 means indices 0, 1, 2\n(without the 3). Slicing can be very useful in speeding up programs by picking out and\nplacing in memory just the specific data elements from a large data set that need to be\nprocessed. This avoids the time-consuming jumping through large segments of memory,\naswellasexcessivereadingfromdisk.\nFinally, we remind you that while all elements in a NumPy array must be of the same\ndatatype,thatdatatypecanbecompound.Forexample,anarrayofarrays:\n1 >>>fromnumpyimport ‚àó\n>>>M= array( [ (10, 20), (30,40), (50, 60) ] ) # Array of 3 arrays\n3 >>> M\narray([[10, 20],\n5 [30, 40],\n[50, 60]])\n7 >>> M. shape\n(3L, 2L)\n9 >>> M. size\n6\n11 >>> M. dtype\ntype(‚Äôint32‚Äô)\nFurthermore,anarraycanbecomposedofcomplexnumbersbyspecifyingthe complexdata\ntypeasanoptiononthe arraycommand.NumPythenusesthe jsymbolfortheimaginary\nnumberi:\n>>> c = array ( [ [1 , complex(2,2)], [ complex(3,2),4] ], dtype= complex )\n2 >>> c\narray([[ 1.+0.j, 2.+2.j],\n4 [ 3.+2.j, 4.+0.j]])\nInSection7.3.3,wediscussusingtruemathematicalmatriceswithNumPy,whichisone\nuseofanarrayobject.Herewenotethatifyouwantedthefamiliarmatrixproductfrom\ntwoarrays,youwouldusethe dotfunction,whereas *isusedforanelement-by-element\n(direct)product:\n>>> matrix1= array ( [[0 ,1] , [1 ,3]])\n2 >>> matrix1\narray([[0, 1],\n4 [1, 3]])\n>>>print( dot(matrix1,matrix1) ) # Matrix or dot product\n6 [[ 1 3]\n[ 3 10]]\n8 >>>print(matrix1 ‚àómatrix1) # Element ‚àíby‚àíelement product\n[[0 1]\n10 [1 9]]",10371
63-7.3.3 NumPy Linear Algebra Library.pdf,63-7.3.3 NumPy Linear Algebra Library,"134 7 Matrix Computing and N‚ÄìD Searching\nNumPyisactuallyoptimizedtoworkwellwitharrays,andinpartthisisbecausearrays\narehandledandprocessedmuchasiftheyweresimple,scalarvariables.4Forexample,here\nisanotherexampleof slicing,atechniquethatisalsousedinordinaryPythonwithlistsand\ntuples,inwhichtwoindicesseparatedbyacolonindicatearange:\nfromvisualimport ‚àó\n2stuff = zeros(10, float)\nt = arrange(4)\nstuff[3:7] = \sqrt(t+1)\nHere we start by creating the NumPy array stuffof floats, all of whose 10 elements are\ninitialized to zero. Then we create the array tcontaining the four elements [0, 1, 2, 3]\nby assigning 4 variables uniformly in the range 0‚Äì4 (the ‚Äúa‚Äù in arangecreates floating-\npointvariables, rangecreatesintegers).Next,weuseaslicetoassign[sqrt(0 +1),sqrt(1 +1),\nsqrt(2+1),sqrt(3 +1)]=[1,1.414,1.732,2]tothemiddleelementsofthe stuffarray.Note\nthat the NumPy version of the sqrtfunction, one of many universal function ( functions)\nsupportedbyNumPy,hastheamazingpropertyofautomaticallyoutputtinganarraywhose\nlength is that of its argument, in this case, the array t. In general, much of the power of\nNumPycomesfromits broadcasting operation,anoperationinwhichvaluesareassigned\ntomultipleelementsviaasingleassignmentstatement.BroadcastingpermitsPythonto vec-\ntorizearrayoperations,whichmeansthatthesameoperationcanbeperformedondifferent\narrayelementsinparallel(ornearlyso).Broadcastingalsospeedsupprocessingbecause\narrayoperationsuseCinsteadofPython,andwithaminimumofarraycopiesbeingmade.\nHereisasimpleaspectofbroadcasting:\nw = zeros(100, float)\nw = 23.7\nThefirstlinecreatestheNumPyarray w,andthesecondline‚Äúbroadcasts‚Äùthevalue23.7to\nallelementsinthearray.TherearemanypossiblearrayoperationsinNumPyandvarious\nrulespertainingtothem;werecommendthattheserioususerexploretheextensiveNumPy\ndocumentationforadditionalinformation.\n7.3.3 NumPy Linear Algebra Library\nThearrayobjectsofNumPyarenotthesameasmathematicalmatrices.Fortunately,there\nisNumPy‚Äôs LinearAlgebra packagethattreats2Darraysasmathematicalmatrices,andalso\nprovidesasimpleinterfacetothepowerful linearalgebrapackage (LAPACK)linearalgebra\nlibrary.Aswekeepsaying,thereismuchtobegainedinspeedandreliabilityfromusing\ntheselibrariesratherthanwritingyourownmatrixroutines.\nOurfirstexamplefromlinearalgebraisthestandardmatrixequation\nAx=b, (7.26)\nwherewehaveusedaboldcharactertorepresenta1Dmatrix(avector).Equation(7.26)\ndescribesasetoflinearequationswith xanunknownvectorand Aaknownmatrix.Now\nwetakeAtobea3 √ó3,btobe3√ó1,andlettheprogramfigureoutthat xmustbe3 √ó1.5\n4 WethankBruceSherwoodforhelpfulcommentsonthesepoints.\n7.3 Matrices in Python 135\nWestartbyimportingallthepackages,byinputtingamatrixandavector,andbyprinting\noutAandx:\n>>>fromnumpyimport ‚àó\n2>>>fromnumpy.linalg import ‚àó\n>>> A = array( [ [1,2,3], [22,32,42], [55,66,100] ] ) # Array of arrays\n>>>print(‚ÄôA =‚Äô,A )\nA=[ [ 1 2 3 ]\n6[ 22 32 42]\n[ 55 66 100]]\n>>> b = array([1,2,3])\n>>>print(‚Äôb =‚Äô,b )\n10b=[ 123 ]\nSeeingthatwehavethematricesAand b,wecangoaheadandsolve Ax=busingNumPy‚Äôs\nsolvecommand,andthentesthowclose Ax‚àíbistoazerovector:\n>>>fromnumpy.linalg importsolve\n2>>> x = solve (A, b) # Finds solution\n>>>print(‚Äôx =‚Äô,x )\nx=[‚àí1.4057971 ‚àí0.1884058 0.92753623] # The solution\n>>>print(‚ÄôResidual =‚Äô ,d o t ( A , x ) ‚àíb) #L H S‚àíRHS\n6\nResidual = [4.44089210e ‚àí16 0.00000000e+00 ‚àí3.55271368e ‚àí15]\nThisisreallyquiteimpressive.Wehavesolvedtheentiresetoflinearequations(byelim-\nination)withjustthesinglecommand solve,performedamatrixmultiplicationwiththe\nsinglecommand dot,didamatrixsubtractionwiththeusualoperator,andareleftwitha\nresidualessentiallyequaltomachineprecision.\nAlthoughtherearemoreefficientnumericalapproaches,anotherwaytosolve\nAx=b (7.27)\nistocalculatetheinverse A‚àí1,andthenmultiplybothsidesoftheequationbytheinverse,\nyielding\nx=A‚àí1b. (7.28)\n1 >>>fromnumpy. linalg import in\n>>> dot ( in(A), A) # Test inverse\n3\narray([[ 1.00000000e+00, ‚àí1.33226763e ‚àí15,‚àí1.77635684e ‚àí15],\n5 [ 8.88178420e ‚àí16, 1.00000000e+00, 0.00000000e+00],\n[‚àí4.44089210e ‚àí16, 4.44089210e ‚àí16, 1.00000000e+00]])\n7 >>>print(‚Äôx =‚Äô, multiply( in(A), b))\nx=[‚àí1.4057971 ‚àí0.1884058 0.92753623] # Solution\n9 >>>print(‚ÄôResidual =‚Äô ,d o t ( A , x ) ‚àíb)\n11 Residual = [ 4.44089210e ‚àí16 0.00000000e+00 ‚àí3.55271368e ‚àí15]\nHerewefirsttestedthat in(A)isinfacttheinverseof Abyseeingif Atimes in(A)equalsthe\nidentitymatrix.Thenweusedtheinversetosolvethematrixequationdirectly,andgotthe\nsameanswerasbeforeandanerroratthelevelofmachineprecisionasbefore.\n5 Don‚Äôtbebotheredbythefactthatalthoughwethinkofthesevectorsas3 √ó1,theysometimesgetprinted\noutas1√ó3;thinkofallthetreesbeingsaved!",4636
64-7.4 Exercise Tests Before Use.pdf,64-7.4 Exercise Tests Before Use,"136 7 Matrix Computing and N‚ÄìD Searching\nOursecondexamplecomesfromfindingtheprincipal-axesofacube,andrequiresusto\nfind a coordinate system in which the inertia tensor is diagonal. This entails solving the\neigenvalueproblem,\nIùùé=ùúÜùùé, (7.29)\nwhereIistheinertiamatrix(tensor), ùùéisanunknowneigenvector,and ùúÜisanunknown\neigenvalue.Theprogram Eigen.pysolvesfortheeigenvaluesandvectors,andshowshow\neasyitistodealwithmatrices.Hereisaninterpretiveversion:\n1 >>>fromnumpyimport ‚àó\n>>>fromnumpy. linalg importbig\n3 >>> I = array( [[2./3, ‚àí1./4], [ ‚àí1./4,2./3]] )\n>>>print(‚ÄôI =\n‚Äô,I )\n5 I=\n[[ 0.66666667 ‚àí0.25 ]\n7 [‚àí0.25 0.66666667]]\n>>> Es, evectors = big(A) # Solves eigenvalue problem\n9 >>>print(‚ÄôEigenvalues =‚Äô ,E s , ‚Äô\n Eigenvector Matrix =\n‚Äô , evectors)\nEigenvalues = [ 0.91666667 0.41666667]\n11 Eigenvector Matrix =\n[[ 0.70710678 0.70710678]\n13 [‚àí0.70710678 0.70710678]]\n>>> vec = array([ evectors[0, 0], evectors[1, 0] ] )\n15 >>> LHS = dot (I , vec ) # Matrix x vector\n>>> RHS = Es[0] ‚àóvec # Scalar multi\n17 >>>print(‚ÄôLHS - RHS =‚Äô ,L H S‚àíRHS) #T e s tf o rz e r o\nLHS‚àíRHS = [ 1.11022302e ‚àí16‚àí1.11022302e ‚àí16]\nWeseehow,aftersettingupthearray Ionline3,wesolvedforitseigenvaluesandeigenvec-\ntorswiththesinglestatement Es, evectors = big(I) online8.Wethenextractedthefirst\neigenvectoronline14,anduseit,alongwiththefirsteigenvalue,tocheckthat(7.29)isin\nfactsatisfiedtomachineprecision.\nWell,wethinkbynowyouhavesomeideaofthepowerofNumPy.InTable7.1wegive\nsomemoreNumPyoperators.\n7.4 Exercise: Tests Before Use\nBeforeyoudirectthecomputertogooffcrunchingnumbersonamillionelementsofsome\nmatrix,it‚Äôsagoodideatotryoutyourproceduresonasmallmatrix,especiallyoneforwhich\nyouknowtherightanswer.Inthisway,itwilltakeyouonlyashorttimetorealizehowhard\nitistogetthecallingprocedureperfectlyright!Herearesomeexercises.\n1) Findthenumericalinverseof A=‚é°\n‚é¢\n‚é¢‚é£+4‚àí2+1\n+3+6‚àí4\n+2+1+8‚é§\n‚é•\n‚é•‚é¶.\na) Asageneralcheck,applicableevenifyoudonotknowtheanalyticanswer,check\nyour inverse in both directions; that is, check that AA‚àí1=A‚àí1A=I, and note the\nnumber of decimal places to which this is true. This also gives you some idea of\ntheprecisionofyourcalculation.\n7.4 Exercise: Tests Before Use 137\nTable 7.1 The operators of NumPy and their effects.\nOperator Effect Operator Effect\ndot(a,b[,out]) Dotproductarrays vdot(a,b) Dotproduct\ninner(a,b) Innerproductarrays outer(a,b) Outerproduct\ntensordot(a,b) Tensordotproduct einsum() Einsteinsum\nlinalg.matrix_power(M,n) Matrixtopower nkron(a,b) Kroneckerproduct\nlinalg.cholesky(a) Choleskydecomp linalg.qr(a) QRfactorization\nlinalg.svd(a) Singularvaldecomp linalg.eig(a) Eigenproblem\nlinalg.eigh(a) Hermitianeigen linalg.eigvals(a) Generaleigen\nlinalg.eigvalsh(a) Hermitianeigenvals linalg.norm(x) Matrixnorm\nlinalg.cond(x) Conditionnumber linalg.det(a) Determinant\nlinalg.slogdet(a) Signandlog(det) trace(a) Diagnolsum\nlinalg.solve(a,b) Solveequation linalg.tensorsolve(a,b) Solve ax=b\nlinalg.lstsq(a,b) Least-squaressolve linalg.inv(a) Inverse\nlinalg.pinv(a) Penroseinverse linalg.tensorinv(a) InverseN‚ÄìDarray\nb) Determinethenumberofdecimalplacesofagreementthereisbetweenyournumer-\nicalinverseandtheanalyticresult: A‚àí1=1\n263‚é°\n‚é¢\n‚é¢‚é£+52+17+2\n‚àí32+30+19\n‚àí9‚àí8+30‚é§\n‚é•\n‚é•‚é¶.Isthissimilartothe\nerrorinAA‚àí1?\n2) Considerthesamematrix Aasbefore,herebeingusedtodescribethreesimultaneous\nlinearequations, Ax=b,orexplicitly,\n‚é°\n‚é¢\n‚é¢‚é£a00a01a02\na10a11a12\na20a21a22‚é§\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢‚é£x0\nx1\nx2‚é§\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢‚é£b0\nb1\nb2‚é§\n‚é•\n‚é•‚é¶. (7.30)\nNow the vector bon the RHS is assumed known, and the problem is to solve for the\nvectorx.Useanappropriatesubroutinetosolvetheseequationsforthethreedifferent\nxvectorsappropriatetothesethreedifferent bvaluesontheRHS:\nb1=‚é°\n‚é¢\n‚é¢‚é£+12\n‚àí25\n+32‚é§\n‚é•\n‚é•‚é¶,b2=‚é°\n‚é¢\n‚é¢‚é£+4\n‚àí10\n+22‚é§\n‚é•\n‚é•‚é¶,b3=‚é°\n‚é¢\n‚é¢‚é£+20\n‚àí30\n+40‚é§\n‚é•\n‚é•‚é¶.\nThesolutionsshouldbe\nx1=‚é°\n‚é¢\n‚é¢‚é£+1\n‚àí2\n+4‚é§\n‚é•\n‚é•‚é¶,x2=‚é°\n‚é¢\n‚é¢‚é£+0.312\n‚àí0.038\n+2.677‚é§\n‚é•\n‚é•‚é¶,x3=‚é°\n‚é¢\n‚é¢‚é£+2.319\n‚àí2.965\n+4.790‚é§\n‚é•\n‚é•‚é¶. (7.31)\n3) Considerthematrix A=[ùõºùõΩ\n‚àíùõΩùõº]\n,whereyouarefreetouseanyvaluesyouwantfor ùõº\nandùõΩ.Useanumericaleigenvaluesolvertoshowthattheeigenvaluesandeigenvectors\n138 7 Matrix Computing and N‚ÄìD Searching\narethecomplexconjugates\nx1,2=[+1\n‚àìi]\n,ùúÜ1,2=ùõº‚àìiùõΩ. (7.32)\n4) Useyoureigenvaluesolvertofindtheeigenvaluesofthematrix\nA=‚é°\n‚é¢\n‚é¢‚é£‚àí2+2‚àí3\n+2+1‚àí6\n‚àí1‚àí2+0‚é§\n‚é•\n‚é•‚é¶. (7.33)\na) Verify that you obtain the eigenvalues ùúÜ1=5,ùúÜ2=ùúÜ3=‚àí3. Beware, double roots\ncancauseproblems.Inparticular,thereisauniquenessissuewiththeireigenvectors\nbecauseanycombinationoftheseeigenvectorsisalsoaneigenvector.\nb) Verifythattheeigenvectorfor ùúÜ1=5isproportionalto\nx1=1‚àö\n6‚é°\n‚é¢\n‚é¢‚é£‚àí1\n‚àí2\n+1‚é§\n‚é•\n‚é•‚é¶. (7.34)\nc) Theeigenvalue ‚àí3correspondstoadoubleroot.Thismeansthatthecorresponding\neigenvectorsaredegenerate,whichinturnmeansthattheyarenotunique.Twolin-\nearlyindependentonesare\nx2=1‚àö\n5‚é°\n‚é¢\n‚é¢‚é£‚àí2\n+1\n+0‚é§\n‚é•\n‚é•‚é¶,x3=1‚àö\n10‚é°\n‚é¢\n‚é¢‚é£3\n0\n1‚é§\n‚é•\n‚é•‚é¶. (7.35)\nInthiscase,it‚Äôsnotclearwhatyoureigenvaluesolverwillgivefortheeigenvectors.\nTrytofindarelationshipbetweenyourcomputedeigenvectorswiththeeigenvalue\n‚àí3andthesetwolinearlyindependentones.\n5) Imagine that your model of some physical system results in N=100 coupled linear\nequationsin Nunknowns:\na00y0+a01y1+¬∑¬∑¬∑+a0(N‚àí1)yN‚àí1=b0,\na10y0+a11y1+¬∑¬∑¬∑+a1(N‚àí1)yN‚àí1=b1,\n¬∑¬∑¬∑\na(N‚àí1)0y0+a(N‚àí1)1y1+¬∑¬∑¬∑+a(N‚àí1)(N‚àí1)yN‚àí1=bN‚àí1.\nIn many cases the aandbvalues are known, so your exercise is to solve for all the x\nvalues,taking aastheHilbertmatrixand basitsfirstcolumn:\n[aij]=a=[\n1\ni+j‚àí1]\n=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£11\n21\n31\n4¬∑¬∑¬∑1\n100\n1\n21\n31\n41\n5¬∑¬∑¬∑1\n101\n...\n1\n1001\n101¬∑¬∑¬∑ ¬∑¬∑¬∑1\n199‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶, (7.36)\n[bi]=b=[1\ni]\n=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\n1‚àï2\n1‚àï3\n...\n1‚àï100‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (7.37)",5684
65-7.5 Solution to String Problem.pdf,65-7.5 Solution to String Problem,,0
66-7.6 Spin States and Hyperfine Structure.pdf,66-7.6 Spin States and Hyperfine Structure,"7.6 Spin States and HyperÔ¨Åne Structure 139\nComparetotheanalyticsolution\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y1\ny2\n...\nyN‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\n0\n...\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (7.38)\n7.5 Solution to String Problem\nIn Section 7.1 we set up the solution to our two masses on a string problem as a matrix\nproblem.Nowwehavethematrixtoolsneededtosolveit.Your problemistocheckout\nthephysicalreasonablenessofthesolutionforavarietyofweightsandlengths.Youshould\ncheck that the deduced tensions are positive and that the deduced angles correspond\nto a physical geometry (e.g., with a sketch). Inasmuch as this is a realistic problem, we\nknow that the sine and cosine functions must be less than 1 in magnitude and that the\ntensions should be similar in magnitude to the weights of the spheres. Our solution\nNewtonNDanimate.py ,whichisgiveninListing7.1,showsgraphicallythestepsinthesearch.\n1) See at what point your initial guess for the angles of the strings gets so bad that the\ncomputerisunabletofindaphysicalsolution.\n2) Apossibleproblemwiththeformalismwehavejustlaidoutisthatbyincorporatingthe\nidentitysin2ùúÉi+cos2ùúÉi=1intotheequations,wemaybediscardingsomeinformation\naboutthesignofsin ùúÉorcosùúÉ.IfyoulookatFigure7.1,youcanobservethatforsome\nvaluesoftheweightsandlengths, ùúÉ2mayturnouttobenegative,yetcos ùúÉshouldremain\npositive. We can build this condition into our equations by replacing f7‚àíf9withf‚Ä≤s\nbasedontheform\nf7=x4‚àí‚àö\n1‚àíx2\n1,f8=x5‚àí‚àö\n1‚àíx2\n2,f9=x6‚àí‚àö\n1‚àíx2\n3. (7.39)\nSeeifthismakesanydifferenceinthesolutionsobtained.\n3)‚äôSolvethesimilarthree-massproblem.Theapproachisthesame,butthenumberof\nequationsislarger.\n7.6 Spin States and HyperÔ¨Åne Structure\nThe energylevelsofhydrogenexhibita finestructure splittingarisingfromthecouplingof\ntheelectron‚Äôsspintoitsorbitalangularmomentum.(Or,youcanthinkofthisasthecou-\nplingsofmagneticmoments.)Inaddition,thesefinelysplitlevelsexhibitasmaller hyperfine\nsplittingarisingfromthecouplingoftheelectron‚Äôsspintotheproton‚Äôsspin.InGaussian\nCGSunits,themagneticmomentofaparticleofcharge qisrelatedtoitsspin Sby\nùùÅ=gq\n2mS, (7.40)\n140 7 Matrix Computing and N‚ÄìD Searching\nwheregistheparticle‚Äôs gfactorandmitsmass.Anelectronhas\nq=‚àíe,S=‚Ñè\n2ùùà,g‚âÉ‚àí2,‚áíùùÅe‚âÉ( ‚àí2)‚àíe\n2meùúé\n2=ùúáBùùà, (7.41)\nùúáB=e‚Ñè\n2me=5.05082√ó10‚àí27J/T, (7.42)\nwhereùúáBistheelectron‚ÄôsBohrmagneton.Becausetheproton‚Äôsmassis ‚àº2000timeslarger\nthantheelectron‚Äôsmass,theproton‚ÄôsBohrmagnetonandmagneticinteractionis ‚àº2000\ntimessmallerthantheelectron‚Äôs:\nùúáB|p=‚àíe‚Ñè\n2mp=‚àíme\nmpùúáB|e=‚àí1\n1836.15ùúáB. (7.43)\nEventhoughtheelectron‚Äôsandtheproton‚Äôsspins(internaldegreesoffreedom)existin\ndifferentspaces,theyarebothspin1/2particles,andsobothcanbe(separately)represented\nbythePaulimatrices:\nùùà=ùúéxÃÇ ùúñx+ùúéyÃÇ ùúñy+ùúézÃÇ ùúñz, (7.44)\nùúéx=[01\n10]\n,ùúéy=[0‚àíi\ni0]\n,ùúéz=[10\n0‚àí1]\n. (7.45)\nIntermsofthePaulimatrices,theelectron‚Äìprotoninteractionis\nV=Wùùàe‚ãÖùùàp=W(ùúée\nxùúép\nx+ùúée\nyùúép\ny+ùúée\nzùúép\nz). (7.46)\nThespin1/2statesfortheelectronandtheproton,each,canbeeitherupordown:\n|ùõº‚ü©=|‚Üë‚ü©=[1\n0]\n, |ùõΩ‚ü©=|‚Üì‚ü©=[0\n1]\n. (7.47)\n1) Verify,thatifboththeelectronandtheprotonstartoffinspin-upstates,\n|ùúì‚ü©=|ùõºeùõºp‚ü©, (7.48)\nthentheinteraction(7.46)producesthemixedstate\nV|ùúì‚ü©=Wùùàe‚ãÖùùàp|ùõºeùõºp‚ü©=W(ùúée\nxùúép\nx+ùúée\nyùúép\ny+ùúée\nzùúép\nz)|ùõºeùõºp‚ü© (7.49)\n=|ùõΩeùõΩp‚ü©+i|ùõΩeùõΩp‚ü©+|ùõºeùõºp‚ü©. (7.50)\n2) Showthattheinteractionmatrixforthe |ùõºeùõºp‚ü©stateis\n‚ü®ùõºeùõºp|V|ùõºeùõºp‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£W00 0\n0‚àíW2W0\n02W‚àíW0\n00 0 W‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (7.51)\n3) Useasymbolicmanipulationprogramtoshowthattheeigenvaluesof Vare:\n‚àí3W(multiplicity3,tripletstate) ,W(multiplicity1,singletstate) ,(7.52)\nwhere the triplet state refers to ||S=1,mS=¬±1,0‚ü©, and the singlet state to\n||S=0,mS=0‚ü©.Ourprogram Hyperfine.py isgiveninListing7.2.",3640
67-7.7 Speeding Up Matrix Computing.pdf,67-7.7 Speeding Up Matrix Computing,,0
68-7.7.1 Vectorization.pdf,68-7.7.1 Vectorization,"7.7 Speeding Up Matrix Computing ‚äô141\n4) Evaluate the numerical value for the hyperfine splitting of the 1S state Bransden and\nJoachain[1991]:\nùúà=‚ÑèŒîE=4W\n‚Ñè. (7.53)\nComparethistothevaluemeasuredbyBaileyandTownsend[1921]:\nùúà=1420.405751800 ¬±0.000000028Hz(measured). (7.54)\nIn addition to being one of the most accurately measured quantities in physics, you\nshouldfindthat(7.54)agreeswiththeory.\n7.7 Speeding Up Matrix Computing ‚äô\nProgramswritteninFortranandCtendtobefasterthanthosewritteninPythonbecausethe\nformerarecompiledlanguages(theentireprogramisprocessedinonefellswoop),while\nPython is interpreted line by line, although sometimes compiled. However, the NumPy\nlinear algebra routines are mainly written in C and C ++, and are fast. In any case, here\nwegivesometechniquestohelpyouspeedupyourlargematrixcomputations.\n7.7.1 Vectorization\nApowerfulfeatureofNumPyisitshigh-level vectorization .Thisisasimpleandautomatic\nprocess in which a single operation acts on an entire array, as opposed to each element\nindividually,andleadstoorder-of-magnitudespeedups.Ourexampleswillusesmallmatri-\nces,andsowhiletherelativespeedupwillbesignificant,theabsolutesavingsoftimewill\nstillbesmall.However,ifyouweredealingwithverylargematrices,andespeciallydoing\nitofteninaprogram,thenspeedingupyourprogrammaybeworththeeffort.\nHereisourcode TuneNumPy.py thatcomparesthespeedofacalculationusinga forloopto\nevaluateafunctionforeachof100,000elementsinanarray,versusthespeedusingNumPy‚Äôs\nvectoredevaluationofthatfunctionforanarrayobject:\n# TuneNumpy.py: Comparison of NumPy op versus for loop\n2fromdateline importdateline\nimportnumpy as np\ndeff(x):returnx‚àó‚àó2‚àí3‚àóx+4\n6x = np.arange(1e5) # An array of 100,000 integers\nforjin range (0, 3): # Repeat comparison three time\nt1 = datetime.now()\ny=[ f ( i ) foriinx] # The for loop\n10t2 = datetime.now()\nprint(‚Äô For for loop, t2-t1 =‚Äô ,t 2‚àít1)\nt1 = datetime.now()\ny=f ( x ) # Vectored evaluation\n14t2 = datetime.now()\nprint(‚Äô For vector function, t2-t1 =‚Äô ,t 2‚àít1)\nOutput:\nForforloop, t2 ‚àít1 = 0:00:00.384000\n18For vector function , t2 ‚àít1 = 0:00:00.009000\nRecall that we defined strideas the amount of memory skipped in order to get to the\nnextelementneededinacalculation.Itisimportanttohaveyourprogramminimizestride\n142 7 Matrix Computing and N‚ÄìD Searching\nin order to avoid jumping through memory to find a needed value. For example, for a\n1000√ó1000array,thecomputermovesonewordtogettothenextcolumn,but1000words\ntogettothenextrow.Clearlybettertodoacolumn-by-columncalculationthanarow-by-\nrowone.Toseethisinaction,weentera3 √ó3arrayofintegersusingNumPy‚Äôs arangeto\ncreatea1Darray.Wethenreshapeitintoa3 √ó3array,anddeterminethestridesforrows\nandcolumnscalls:\n>>>fromnumpyimport ‚àó\n2 >>> A = arange(0,90,10)\n>>> A\n4 array([ 0, 10, 20, 30, 40, 50, 60, 70, 80])\n>>> A = A.reshape((3,3))\n6 >>> A\narray([[ 0, 10, 20],\n8 [30, 40, 50],\n[60, 70, 80]])\n10 >>> A. strides\n(12, 4)\nLine11tellsusthatittakes12bytes(3words)togettothesamepositioninthenextrow,but\nonly4bytes(oneword)togettothesamepositioninthenextcolumn.It‚Äôsclearlycheaper\ntogofromcolumntocolumnthanrowtorow.\nAneasywaytocutdownonmemoryjumpingistousePython‚Äôs sliceoperatorthatextracts\njustthedesiredpartofalist(liketakinga‚Äúslice‚Äùthroughthecenterofajellydoughnut):\nListName[StartIndex:StopBeforeIndex:Step] .\nTheconventionisthatifnoargumentisgiven,thentheslicestartsat0andstopsattheend\nofthelist.Forexample:\n1 >>> A = arange(0,90,10).reshape((3,3))\n>>> A\n3 array([[ 0, 10, 20],\n[30, 40, 50],\n5 [60, 70, 80]])\n>>> A[:2 ,:] # First two rows (start at 2, go to end)\n7 array([[ 0, 10, 20],\n[30, 40, 50]])\n9 >>> A[:,1:3] #C o l u m n s1 ‚àí3( s t a r ta t1 ,e n da t4 )\narray([[10, 20],\n11 [40, 50],\n[70, 80]])\n13 >>> A[ : : 2 , : ] # Every second row\narray([[ 0, 10, 20],\n15 [60, 70, 80]])\nThisiscalled view-basedindexing ,withtheindexednotationreturninganewarrayobject\nthatpointstotheaddressoftheoriginaldata,asopposedtostoringthevaluesofthenew\narray(think‚Äúpointers‚ÄùinC).Forinstance,youcanoptimizeacalculationofforwardand\ncentraldifferencederivativesquiteelegantly:\n1 >>> x = arange(0,20,2)\n>>> x\n3 a r r a y ( [ 0 ,2 ,4 ,6 ,8 , 1 0 , 1 2 , 1 4 , 1 6 , 1 8 ] )\n>>> y = x ‚àó‚àó2\n5 >>> y",4232
69-7.8 Code Listing.pdf,69-7.8 Code Listing,"7.7 Speeding Up Matrix Computing ‚äô143\narray([ 0, 4, 16, 36, 64, 100, 144, 196, 256, 324], dtype=int32)\n7 >>> dy_dx = (( y[1:] ‚àíy[:1])/(x[1:] ‚àíx[:‚àí1])) # Forward difference\n>>> dy_dx\n9 array([ 2., 8., 18., 32., 50., 72., 98., 128., 162.])\n>>> dy_dx_c = (( y[2:] ‚àíy[:‚àí2])/(x[2:] ‚àíx[:‚àí2])) # Central difference\n11 >>> dy_dx_c\narray([ 4., 8., 12., 16., 20., 24., 28., 32.])\nWenotethatthevaluesofthederivativesaredifferentbecauseforwarddifferenceisevalu-\natedatthestartoftheintervalwhilecentraldifferenceatthecenter.\n7.7.2 Speedup Exercises\n1)Timinganoperation\nimporttime\nstart = time.time()\nprint(""hello"")\n4end = time.time()\nprint(end‚àístart)\n2) Runthetwosimplecodeslistedbelow,timinghowlongeachtakes.Notethatalthough\neachhasthesamenumberofarithmeticoperations,onetakessignificantlymoretime\nbecauseitmakeslargejumpsthroughmemory.\nSequentialcolumnreferences\nforj = 1, 999999;\nx(j) =m(1,j) // Sequential column reference\nSequentialrowreferences\nforj = 1, 999999;\n2x(j) =m(j,1) // Sequential row reference\n3) Testtheeffectofstrideonyourmachinebycomparingthetimeittakestorunthesetwo\nprograms.Runforincreasingcolumnsize idiomandcomparethetimesforloop Aversus\nthoseforloop B.LoopAstepsthroughthematrix vecincolumnorder,whileloop Bsteps\nthroughinroworder.Bothloopstakeusthroughalltheelementsofthematrix,butthe\nstrideisdifferent.\nLoopAbad(large)stride\nDimension vec(N, M) // Stride 1 fetch (f90)\n2 forj=1 ,M ;\nfori=1, N; Ansi = Ansi + vec(i,j) ‚àóvec(i,j)\nLoopBgood(small)stride\n1Dimension vec(N, M) // Stride dim fetch (f90)\nfori=1 ,N ;\nforj=1, M; Ansi = Ansi + vec(i,j) ‚àóvec(i,j)\n144 7 Matrix Computing and N‚ÄìD Searching\n4) Thepenultimateexampleofmemoryusageislarge-matrixmultiplication:\n[C]=[A]√ó[B]‚áícij=N‚àë\nk=1aik√óbkj. (7.55)\nTesttheeffectofstrideonyourmachinebycomparingthetimeittakestorunthesetwo\nprograms.Runforincreasingcolumnsize N\nGOODPython(minstride)\n1fori=1 ,N ;{ / /R o w\nforj = 1, N; { // Column\nc(i,j) = 0.0 // Initialize\nfork=1 ,N ; {\n5 c(i,j)=c(i,j)+a(i,k) ‚àób(k,j) }}} // Accumulate\nBADPython(maxstride)\nforj = 1, N; { // Initialization\nfori=1 ,N ;{\n3 c(i,j) = 0.0 }\nfork=1 ,N ; {\nfori = 1, N; {c(i,j) = c(i,j) + a(i,k) ‚àób(k,j) }}}\n5) Use NumPy‚Äôs vectorized function evaluation to determine the speedup in the matrix\nmultiplication [A][B],wherethematricescontainatleast105floating-pointnumbers.\nCompare the direct multiplication to application of the elementary rule for each\nelement:\n[BA]ij=‚àë\nkaikbkj. (7.56)\n6) DeterminethespeedupobtainedbyusingPythonstrippingtoreducestrideinevaluating\nthe forward-difference and central-difference derivatives over an array of at least 105\nfloating-pointnumbers.\n7.8 Code Listing\nListing7.1 ThecodeNewtonNDanimate.py showsthestep-by-stepsearchforsolution\nofthetwo-mass-on-a-stringproblemviaaNewton‚ÄìRaphsonsearch.\n# NewtonNDanimate.py: MultiDimension Newton Search\n3fromvisualimport ‚àó\nfromnumpy. linalg importsolve\nfromvisual.graph import ‚àó\n7scene = display(x=0,y=0,width=500,height=500,\ntitle= ‚ÄôString and masses configuration‚Äô )\ntempe = curve(x= range(0,500),color=color.black)\n11n=9\neps = 1e ‚àí3\nderiv = zeros( (n, n), float)\nf=z e r o s (( n ) , float)\n15x = array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1., 1., 1.])\n7.8 Code Listing 145\ndefplotconfig():\nforobjinscene.objects:\n19 obj.visible=0 # Erase previous configuration\nL1 = 3.0\nL2 = 4.0\nL3 = 4.0\n23xa = L1 ‚àóx[3] #L 1 ‚àócos(th1)\nya = L1 ‚àóx[0] # L1 sin(th1)\nxb = xa+L2 ‚àóx[4] #L 1 ‚àócos(th1)+L2 ‚àócos(th2)\nyb = ya+L2 ‚àóx[1] #L 1 ‚àósin(th1)+L2 ‚àósen(th2)\n27xc = xb+L3 ‚àóx[5] #L 1 ‚àócos(th1)+L2 ‚àócos(th2)+L3 ‚àócos(th3)\nyc = yb ‚àíL3‚àóx[2] #L 1 ‚àósin(th1)+L2 ‚àósen(th2) ‚àíL3‚àósin(th3)\nmx = 100.0 # for linear coordinate transformation\nbx =‚àí500.0 #f r o m0 = ‚àó<‚àóx=‚àó<‚àó10\n31my =‚àí100.0 #t o ‚àí500 = ‚àó<‚àóx_window=>500\nby = 400.0 # same transformation for y\nxap = mx ‚àóxa+bx # to keep aspect ratio\nyap = my ‚àóya+by\n35ball1 = sphere(pos=(xap,yap), color=color.cyan,radius=15)\nxbp = mx ‚àóxb+bx\nybp = my ‚àóyb+by\nball2 = sphere(pos=(xbp,ybp), color=color.cyan,radius=25)\n39xcp = mx ‚àóxc+bx\nycp = my ‚àóyc+by\nx0 = mx ‚àó0+bx\ny0 = my ‚àó0+by\n43line1 = curve(pos=[(x0,y0),(xap,yap)], color=color.yellow,radius=4)\nline2 = curve(pos=[(xap,yap),(xbp,ybp)], color=color.yellow,radius=4)\nline3 = curve(pos=[(xbp,ybp),(xcp,ycp)], color=color.yellow,radius=4)\ntopline = curve(pos=[(x0,y0),(xcp,ycp)], color=color.red,radius=4)\n47\ndefF(x, f): # F function\nf[0] =3 ‚àóx[3] + 4 ‚àóx[4] + 4 ‚àóx[5]‚àí8.0\nf[1] =3 ‚àóx[0] + 4 ‚àóx[1]‚àí4‚àóx[2]\n51f[2] =x[6] ‚àóx[0]‚àíx[7] ‚àóx[1]‚àí10.0\nf[3] =x[6] ‚àóx[3]‚àíx[7] ‚àóx[4]\nf[4] =x[7] ‚àóx[1] + x[8] ‚àóx[2]‚àí20.0\nf[5] =x[7] ‚àóx[4]‚àíx[8] ‚àóx[5]\n55f[6] =pow(x[0], 2) + pow(x[3], 2) ‚àí1.0\nf[7] =pow(x[1], 2) + pow(x[4], 2) ‚àí1.0\nf[8] =pow(x[2], 2) + pow(x[5], 2) ‚àí1.0\n59defdFi_dXj(x, deriv , n): # Derivatives\nh=1 e‚àí4\nforjin range (0, n):\ntemp = x[j]\n63 x[j] = x[j] + h/2.\nF(x, f)\nforiin range (0, n): deriv[i, j] = f[i]\nx[j] = temp\n67forjin range (0, n):\ntemp = x[j]\nx[j] = x[j] ‚àíh/2.\nF(x, f)\n71 foriin range (0, n): deriv[i, j] = (deriv[i, j] ‚àíf[i])/h\nx[j] = temp\nforitin range (1, 100):\n75 rate(1) # 1 second between graphs\nF(x, f)\ndFi_dXj(x, deriv , n)\nB = array([[ ‚àíf[0]], [ ‚àíf[1]], [ ‚àíf[2]], [ ‚àíf[3]], [ ‚àíf[4]], [ ‚àíf[5]],\\n79 [‚àíf[6]], [ ‚àíf[7]], [ ‚àíf[8]]])\nsol = solve(deriv, B)\ndx = take(sol, (0, ), 1) # First column of sol\nforiin range (0, n):\n83 x[i] = x[i] + dx[i]\nplotconfig()\nerrX = errF = errXi = 0.0\nforiin range (0, n):\n146 7 Matrix Computing and N‚ÄìD Searching\n87 if( x[i] != 0.): errXi = abs(dx[i]/x[i])\nelse: errXi = abs(dx[i])\nif( errXi > errX): errX = errXi\nif(abs(f[i]) > errF ): errF = abs(f[i])\n91 if( (errX <=eps)and(errF<=eps) ): break\nprint(‚ÄôNumber of iterations = ‚Äô ,i t , ""\n Final Solution:"" )\nforiin range (0, n):\n95 print(‚Äôx[‚Äô,i , ‚Äô] = ‚Äô,x [ i ] )\nListing7.2 Hyperfine.py HyperfinesplittinginHusingsymbolicpackageSymPy.\n# Hyperfine.py: Hydrogen hyperfine structure using Sympy\n3fromsympyimport ‚àó\nimportnumpy as np, matplotlib.pyplot as plt\nW, mue, mup, B = symbols( ‚ÄôW mu_e mu_p B‚Äô ) # Symbols & Hamiltonian\n7H = Matrix([[W,0,0,0],[0, ‚àíW,2‚àóW,0],[0,2 ‚àóW,‚àíW,0],[0,0,0,W]])\nHmag = Matrix([[ ‚àí(mue+mup) ‚àóB,0,0,0],[0, ‚àí(mue‚àímup) ‚àóB,0,0],[0,0, ‚àí(‚àímue+mup) ‚àóB,0],\n[0,0,0,(mue+mup) ‚àóB]]) # H with external B\nprint(""\n Hyperfine Hamiltonian H ="" ,H)\n11print(""\n Eigenvalues and multiplicities of H ="" ,H.eigenvals() )\nprint(""\n Hmag ="" ,H m a g )\nHtot = H + Hmag # Hamiltonian + pertubation\nprint(""\n Htot = H + Hmag ="" ,H t o t )\n15print(""\n Eigenvalues of matrix HB"" )\ne1, e2, e3, e4 = Htot.eigenvals() # 4 eigenvalues\nprint(""e 1="" ,e 1 , ""\n e2 = "" ,e 2 , ""\n e3 = "" ,e 3 , ""\n e4 = "" ,e 4 )\nprint(""\n After substitute mu_e = 1, and mu_p = 0 in eigenvalues"" )\n19print(""e 1="" ,e1.subs([(mue,1) ,(mup,0)]), ""\n e2 =\n"",e2.subs([(mue,1) ,(mup,0)]))\nprint(""e 3="" ,e3.subs([(mue,1) ,(mup,0)]), ""\n e4 = "" ,e4.subs([(mue,1) ,(mup,0)]))\nb = np.arange(0,4,0.1)\nE=1\n23E4 =‚àíE+n p .s q r t( b ‚àó‚àó2+ 4 ‚àóE‚àó‚àó2)\nE3 = E ‚àíb\nE2 = E + b\nE1 =‚àíE‚àínp. sqrt(b ‚àó‚àó2+ 4 ‚àóE‚àó‚àó2)\n27plt.figure()\nplt.plot(b,E1, label= ‚ÄôE1‚Äô); plt.plot(b,E2, label= ‚ÄôE2‚Äô)\nplt.plot(b,E3, label= ‚ÄôE3‚Äô); plt.plot(b,E4, label= ‚ÄôE4‚Äô)\nplt.legend(); plt.text( ‚àí0.4,1, ‚ÄôE‚Äô)\n31plt.xlabel( ‚Äô Magnetic Field B‚Äô )\nplt.title( ‚ÄôHyperfine Splitting of H Atom 1S Level‚Äô )\nplt .show()",7270
70-Chapter 8 Differential Equations and Nonlinear Oscillations.pdf,70-Chapter 8 Differential Equations and Nonlinear Oscillations,,0
71-8.1 Nonlinear Oscillators.pdf,71-8.1 Nonlinear Oscillators,"147\n8\nDifferential Equations and Nonlinear Oscillations\nIn this chapter we develop numerical methods for solving ordinary differential equations, and\nfocus on applying those tools to nonlinear systems. We start with simple systems that have\nanalytic solutions, and use them to test various differential-equation solvers. We then let the\noscillations become large so that nonlinear effects are important, and investigate nonlinear\nresonances and beating. In Chapter 16, Continuous Nonlinear Dynamics, we make a related\nstudy of the realistic pendulum and its chaotic behavior .\n8.1 Nonlinear Oscillators\nFigure8.1showsamass mattachedtoaspringthatexertsarestoringforcetowardtheorigin,\nas well as a hand that exerts a time-dependent external force on the mass. The restoring\nforceexertedbythespringisnonlinear.\nProblem Solveforthemotionofthemassasafunctionoftimeforanarbitraryrestoring\nforce.Youmayassumethemotionisconstrainedtoonedimension.\nThis is a classical mechanics problem and so Newton‚Äôs second law provides us with the\nequationofmotion\nFk(x)+Fext(x,t)=md2x\ndt2, (8.1)\nwhereFk(x)isanarbitraryrestoringforceexertedbythespringand Fext(x,t)istheexternal\nforce.Becausewearenottoldjusthowthespringdepartsfrombeinglinear,we‚Äôlljusttry\noutsomedifferentspringmodels.Asourfirstmodel,we‚Äôlllookatapotentialthatislinear\nforsmalldisplacements x,butbecomesnonlinearforlarge xvalues:\nV(x)‚âÉ1\n2kx2(\n1‚àí2\n3ùõºx)\n, (8.2)\n‚áíFk(x)=‚àídV(x)\ndx=‚àíkx(1‚àíùõºx) (8.3)\n‚áímd2x\ndt2=‚àíkx(1‚àíùõºx), (8.4)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n148 8 Differential Equations and Nonlinear Oscillations\nFext(x,t)Fk(x)\nxFigure 8.1 Am a s s m(the block) attached to a spring with\nrestoring force Fk(x)as well as driven by an external\ntime-dependent driving force (the hand).\nwhere we have omitted the time-dependent external force. Equation (8.4) is the second-\norder ordinary differential equation (ODE) we need to solve. If ùõºx‚â™1, we should have\nessentiallyharmonicmotion,butas x‚Üí1‚àïùõºtheanharmoniceffectsshouldincrease.\nWecanunderstandthebasicphysicsofthismodelbylookingatthecurvesontheleftin\nFigure8.2.Aslongas x<1‚àïùõº,therewillbea restoringforce andthemotionwillbeperiodic\n(repeatedexactlyandindefinitelyintime),thoughitmaynotbeharmonic.Iftheamplitude\nofoscillationislarge,therewillbeanasymmetryinthemotiontotherightandleftofthe\nequilibriumposition.Andif x>1‚àïùõº,theforcewillbecomerepulsiveandthemasswillbe\npushedawayfromtheorigin.\nAsasecondmodelofanonlinearoscillator,weassumethatthespring‚Äôspotentialfunction\nisproportionaltosomearbitrary evenpowerpofx:\nV(x)=1\npkxp,(peven). (8.5)\nWerequireaneven ptoensurethattheforce,\nFk(x)=‚àídV(x)\ndx=‚àíkxp‚àí1, (8.6)\ncontains an odd power of p, which guarantees that it is a restoringforce for positive\nand negative xvalues. We display some characteristics of this potential on the right in\nFigure8.2.Weseethat p=2 istheharmonicoscillatorandthat p=6 isnearlyasquare\nwellwiththemassmovingalmostfreelyuntilithitsthewallat x‚âÉ¬±1.Regardlessofthe\npvalue,themotionwillbeperiodic,butitwillbeharmoniconlyfor p=2.Newton‚Äôslaw\nHarmonic\nAnharmonicV(x)\nx\n1/Œ± Linear\nNonlinearUnboundVp = 2\nx xVp = 6\nLinear NonlinearHarmonic Anharmonic\nFigure 8.2 Left: The potentials of an harmonic oscillator (solid curve) and of an anharmonic\noscillator (dashed curve). If the amplitude becomes too large for the anharmonic oscillator, the\nmotion becomes unbound. Right: The shapes of the potential energy function V(x)‚àù|x|pforp=2\nandp=6. The ‚Äúlinear‚Äù and ‚Äúnonlinear‚Äù labels refer to the restoring force derived from these\npotentials.",3672
72-8.2 ODE Review.pdf,72-8.2 ODE Review,,0
73-8.2.1 Order.pdf,73-8.2.1 Order,,0
74-8.2.3 Linear and Nonlinear.pdf,74-8.2.3 Linear and Nonlinear,"8.2 ODE Review 149\n(8.1)givesthesecond-orderODEweneedtosolve:\nmd2x\ndt2=Fext(x,t)‚àíkxp‚àí1. (8.7)\n8.2 ODE Review\nThe background material in this section is presented to avoid confusion over semantics. The\nwell-versed reader may want to skim or skip it.\n8.2.1 Order\nAgeneralformfora first-orderdifferentialequationis\ndy\ndt=f(t,y), (8.8)\nwherethe‚Äúorder‚ÄùreferstothedegreeofthederivativeontheLHS.Thederivativeorforce\nfunctionf(t,y)ontheRHS,isarbitrary.Forinstance,evenif f(t,y)isanastyfunctionof y\nandtsuchas\ndy\ndt=‚àí3t2y+t9+y7, (8.9)\nthisisstillafirst-orderdifferentialequation.Ageneralformfora second-order differential\nequationis\nd2y\ndt2+ùúÜdy\ndt=f(\nt,dy\ndt,y)\n. (8.10)\nThe derivative function fon the RHS is arbitraryand may involveany power ofthe first\nderivativeaswell.Toillustrate,\nd2y\ndt2+ùúÜdy\ndt=‚àí3t2(dy\ndt)4\n+t9y(t) (8.11)\nisasecond-orderdifferentialequation,asinNewton‚Äôslaw(8.1).\nInthedifferentialequations(8.8)and(8.10),thetime tistheindependent variableandthe\npositionyisthedependent variable.Thismeansthatwearefreetovarythetimeatwhich\nwe want a solution, but not the value of the position yat that time. Note that we often\nusethesymbol yorYforthedependentvariable,butthatthisisjustasymbolwhichmay\nrefertoothervariables.Forexample,insomeapplications,weuse ytodescribeaposition\ninsteadoft.\n8.2.2 Ordinary and Partial\nEquations such as (8.1) and (8.8) are ODEs because they contain only oneindependent\nvariable,inthesecases t.Incontrast,anequationsuchastheSchr√∂dingerequation,\ni‚Ñèùúïùúì(x,t)\nùúït=‚àí‚Ñè2\n2m[ùúï2ùúì\nùúïx2+ùúï2ùúì\nùúïy2+ùúï2ùúì\nùúïz2]\n+V(x)ùúì(x,t), (8.12)\ncontainsfourindependentvariables,andthismakesita partialdifferentialequation (PDE).\nThepartialderivativesymbol ùúïisusedtoindicatethatthedependentvariable ùúìdepends",1751
75-8.2.4 Initial and Boundary Conditions.pdf,75-8.2.4 Initial and Boundary Conditions,,0
76-8.3 Dynamic Form of ODEs.pdf,76-8.3 Dynamic Form of ODEs,"150 8 Differential Equations and Nonlinear Oscillations\nsimultaneouslyonseveralindependentvariables.Intheearlypartsofthisbook,welimit\nourselvestoordinarydifferentialequations,yetinChapters20‚Äì27,we‚Äôllexamineavariety\nofPDEs.\n8.2.3 Linear and Nonlinear\nPart of the strength of computational science is that we are no longer limited to solving\nlinear equations. A linear equation is one in which only the first power of yordny‚àïdnt\nappears;anonlinearequationmaycontainhigherpowers.Forexample,\ndy\ndt=g3(t)y(t)(linear),dy\ndt=ùúÜy(t)‚àíùúÜ2y2(t)(nonlinear) . (8.13)\nAnimportantpropertyoflinearequationsisthe lawoflinearsuperposition thatletsusadd\ndifferentsolutionstogethertoformnewones.Asacaseinpoint,if A(t)andB(t)aresolutions\nofthelinearequationin(8.13),then\ny(t)=ùõºA(t)+ùõΩB(t) (8.14)\nisalsoasolutionforarbitraryvaluesoftheconstants ùõºandùõΩ.Incontrast,evenifwewere\ncleverenoughtoguessthatthesolutionofthenonlinearequationin(8.13)is\ny(t)=a\n1+be‚àíùúÜt, (8.15)\n(which we invite you to verify), this wouldn‚Äôt work if we tried to obtain a more general\nsolutionbyaddingtogethertwosuchsolutions:\ny1(t)=a\n1+be‚àíùúÜt+a‚Ä≤\n1+b‚Ä≤e‚àíùúÜt(8.16)\n(whichyouweinviteyoutoverify).\n8.2.4 Initial and Boundary Conditions\nThegeneralsolutionofafirst-orderdifferentialequationcontainsonearbitraryconstant.\nThegeneralsolutionofasecond-orderdifferentialequationcontainstwosuchconstants,\nandsoforth.Foranyspecificproblem,theseconstantsareusuallydeterminedbythe initial\nconditions.Forafirst-orderequationthesoleinitialconditionmaybetheposition y(t)at\nsometime.Forasecond-orderequation,thetwoinitialconditionsmaybethepositionand\nvelocityatsometime.Regardlessofhowpowerfulthehardwareandsoftwarethatyouuti-\nlize,mathematicsremainsvalid,andsoyoumustknowtheinitialconditionsinorderto\nobtainauniquesolutiontoadifferentialequation.\nInadditiontotheinitialconditions,itispossibletofurtherrestrictthesolutionsofdif-\nferentialequations.Onesuchwayisby boundaryconditions thatconstrainthesolutionto\nhavefixedvaluesattheboundariesofthesolutionspace.InChapter13,wediscusshowto\nextendthetechniquesofthischaptertoboundary-valueproblems.\n8.3 Dynamic Form of ODEs\nAstandardformforODEs,whichhasproventobeusefulinbothnumericalanalysis[Press\net al., 2007] and classical dynamics [Scheck, 2010; Tabor, 1989; Jos√© and Salatan, 1998],\n8.3 Dynamic Form of ODEs 151\nistoexpressODEsof anyorderasNsimultaneousfirst-orderODEsinthe Nunknowns,\nyi,i=0,N‚àí1:\ndy(0)\ndt=f(0)(t,{y(i)}), (8.17)\ndy(1)\ndt=f(1)(t,{y(i)}) (8.18)\n......\ndy(N‚àí1)\ndt=f(N‚àí1)(t,{y(i)}). (8.19)\nNote,fcancontainanexplicitdependenceonanyorallofthe y(i)s,butnotexplicitlyon\na derivative dy(i)‚àïdt. These equations can be expressed more succinctly by use of the N-\ndimensionalvectors(indicatedherein boldface)yandf:\ndy(t)‚àïdt=f(t,y), (8.20)\ny=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y(0)(t)\ny(1)(t)\n...\ny(N‚àí1)(t)‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶, f=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£f(0)(t,y)\nf(1)(t,y)\n...\nf(N‚àí1)(t,y)‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (8.21)\nThe utility of such compact notation is that we can study the properties of the ODEs, as\nwellasdevelopalgorithmstosolvethem,bydealingwiththesingleequation(8.20),with-\nouthavingtoworryaboutindividual y(i)‚Äôs.Toseehowthisworksinpractice,let‚Äôsconvert\nNewton‚Äôslaw\nd2x\ndt2=1\nmF(\nt,x,dx\ndt)\n, (8.22)\nto this standard form. The rule is that the RHS may notcontain any explicit derivatives,\nalthoughindividualcomponentsof y(i)mayrepresentderivatives.Topullthisoff,wedefine\ntheposition xasthefirstdependentvariable y(0),andthevelocity dx‚àïdtastheseconddepen-\ndentvariable y(1):\ny(0)(t)def=x(t),y(1)(t)def=dx\ndt=dy(0)(t)\ndt. (8.23)\nThesecond-orderODE(8.22)nowbecomestwosimultaneousfirst-orderODEs:\ndy(0)\ndt=y(1)(t),dy(1)\ndt=1\nmF(t,y(0),y(1)). (8.24)\nThisexpressestheacceleration[thesecondderivativein(8.22)]asthefirstderivativeofthe\nvelocityy(1). These equations are now in the standard form (8.20), with the derivative or\nforcefunction fhavingthetwocomponents\nf(0)=y(1)(t),f(1)=1\nmF(t,y(0),y(1)), (8.25)\nwhereFmaybeanexplicitfunctionoftimeaswellasofpositionandvelocity.Tobeeven\nmorespecific,applyingthesedefinitionstoourspringproblem(8.7),weobtainthecoupled\nfirst-orderequations\ndy(0)\ndt=y(1)(t),dy(1)\ndt=1\nm[Fext(x,t)‚àíky(0)(t)p‚àí1], (8.26)",4175
77-8.4 ODE Algorithms.pdf,77-8.4 ODE Algorithms,,0
78-8.4.2 RungeKutta Rule.pdf,78-8.4.2 RungeKutta Rule,"152 8 Differential Equations and Nonlinear Oscillations\nwherey(0)(t)isthepositionofthemassattime tandy(1)(t)isitsvelocity.Inthestandard\nform,thecomponentsoftheforcefunctionandtheinitialconditionsare\nf(0)(t,y)=y(1)(t), f(1)(t,y)=1\nm[Fext(x,t)‚àík(y(0))p‚àí1],\ny(0)(0)=x0, y(1)(0)=ùë£0. (8.27)\n8.4 ODE Algorithms\nTheclassicwaytosolveanODEisshowninFigure8.3.Onestartswiththeknowninitial\nvalueofthedependentvariable, y0‚â°y(t=0),andthenusesthederivativefunction f(t,y)to\nadvancetheinitialvalueonesmallstep hforwardintimetoproduce y(t=h)‚â°y1.Onceyou\ncandothat,youcansolvetheODEforall tvaluesbyjustcontinuingtosteptolargertimes,\nonesmallhatatime.1Errorisalwaysaconcernwhenintegratingdifferentialequations\nbecausederivativesrequiresmalldifferences,andsmalldifferencesarepronetosubtractive\ncancellationsandround-offerroraccumulation.Inaddition,becausethissteppingproce-\ndure is a continuous extrapolation of the initial conditions, with each step building on a\npreviousextrapolation,thisissomewhatlikeacastlebuiltonsand;incontrasttointerpo-\nlation,therearenotabulatedvaluesonwhichtoanchoryoursolution.Itissimplestifthe\ntimestepsusedthroughouttheintegrationremainconstantinsize,andthatismostlywhat\nwe shall do. Industrial-strength algorithms, such as the one we discuss in Section 8.4.2,\nadaptthestepsizebymaking hlargerinregionswhere yvariesslowly(thisspeedsupthe\nintegrationandcutsdownonround-offerror),andmaking hsmallerinregionswhere y\nvariesrapidly.\n8.4.1 Euler‚Äôs Rule\nEuler‚Äôsrule(Figure8.4)isthesimplestalgorithmforintegratingthedifferentialequation\n(8.8) by one step. It is just an application of the forward-difference algorithm for the\nderivative:\ndy(t)\ndt‚âÉy(tn+1)‚àíy(tn)\nh=f(t,y), (8.28)\n‚áí yn+1‚âÉyn+hf(tn,yn), (8.29)\nwhereyndef=y(tn)isthevalueof yattimetn.Weknowfromourdiscussionofdifferentiation\nthattheerrorintheforward-differencealgorithmis ùí™(h2),andsothenthistooistheerror\ninEuler‚Äôsrule.\nt = 0 t = Ty0y1y2y3 yN h\nFigure 8.3 A sequence of uniform steps of length htaken in solving a differential equation.\nThe solution starts at time t=0, and is integrated in steps of huntil t=T.\n1 Toavoidconfusion,noticethat y(n)isthenthcomponentofthe yvector,while ynisthevalueof yaftern\ntimesteps.Yes,thereisapricetopayforeleganceinnotation.\n8.4 ODE Algorithms 153\nFigure 8.4 Euler‚Äôs algorithm for integration of a\ndifferential equation one step forward in time. This\nlinear extrapolation with the slope evaluated at the\ninitial point is seen to lead to an error Œî.Euler‚Äôs\nruley(t)\ntntn+1Œî\nh\nToindicatethesimplicityofthisalgorithm,weapplyittoouroscillatorproblem(8.4)for\nthefirsttimestep:\ny(0)\n1=x0+ùë£0h,y(1)\n1=ùë£0+h1\nm[Fext(t=0)+Fk(t=0)]. (8.30)\nComparethesetotheprojectileequationsfamiliarfromfirst-yearphysics,\nx=x0+ùë£0h+1\n2ah2,ùë£=ùë£0+ah. (8.31)\nWeseethatwithEuler‚Äôsrule,theaccelerationdoesnotcontributetothechangeindistance\n(noh2term),yetitdoescontributetothechangeinvelocity(andsowillcontributebelatedly\ntothedistanceinthenexttimestep).Thisisclearlyasimplealgorithmthatrequiresvery\nsmallhvaluestoobtainprecision.Yetusingsmallvaluesfor hincreasesthenumberofsteps\nandtheaccumulationofround-offerror,whichmayleadtoinstability.2Whereaswedonot\nrecommendEuler‚Äôsalgorithmforgeneraluse,itiscommonlyusedtostartoffmoreprecise\nalgorithms.\n8.4.2 Runge‚ÄìKutta Rule\nAlthough no one algorithm is good for solving all ODEs, the fourth-order Runge‚ÄìKutta\nalgorithm, rk4,oritsextensionwithadaptivestepsize, rk45,comesclose.Inspiteof rk4\nbeingourrecommendedstandard,wederivethesimpler rk2here,andjuststatetheresult\nforrk4.\nTheRunge‚ÄìKuttaalgorithmforintegratingadifferentialequationisbaseduponthefor-\nmal(exact)integralofourdifferentialequation:\ndy\ndt=f(t,y)‚áíy(t)=‚à´f(t,y)dt (8.32)\n‚áíyn+1=yn+‚à´tn+1\ntnf(t,y)dt. (8.33)\nTo derive the second-order Runge‚ÄìKutta algorithm rk2(Figure 8.5 and rk2.py), we\nexpandf(t,y)in a Taylor series about the midpointof the integration interval and retain\n2 Instabilityisoftenaproblemwhenyouintegratea y(t)thatdecreasesastheintegrationproceeds,\nanalogoustoupwardrecursionofsphericalBesselfunctions.Inthiscase,andifyouhavealinearODE,you\narebestoffintegrating inwardfromlargetimestosmalltimesandthenscalingtheanswertoagreewith\ntheinitialconditions.\n154 8 Differential Equations and Nonlinear Oscillations\nrk2y(t)\ntntn+1slope\ntn+1/2ŒîFigure 8.5 The rk2 algorithm for integration of a\ndifferential equation uses a slope (bold line segment)\nevaluated at the interval‚Äôs midpoint, and is seen to lead to\na smaller error than Euler‚Äôs algorithm in Figure 8.4.\ntwotermsintheexpansion:\nf(t,y)‚âÉf(tn+1‚àï2,yn+1‚àï2)+(t‚àítn+1‚àï2)df\ndt(tn+1‚àï2)+ùí™(h2). (8.34)\nSince(t‚àítn+1‚àï2)raisedtoanyoddpowerisequallypositiveandnegativeovertheinterval\ntn‚â§t‚â§tn+1,theintegralofthe (t‚àítn+1‚àï2)termin(8.34)vanishesandweobtainthe rk2\nalgorithm :\n‚à´tn+1\ntnf(t,y)dt‚âÉf(tn+1‚àï2,yn+1‚àï2)h+ùí™(h3), (8.35)\n‚áíyn+1‚âÉyn+hf(tn+1‚àï2,yn+1‚àï2)+ùí™(h3). (8.36)\nWesee that while rk2containsthe same number of terms as Euler‚Äôs rule, it obtainsa\nhigher level of precision by taking advantage of the cancellation of the ùí™(h)terms. The\npriceforimprovedprecisionishavingtoevaluatethederivativefunctionandthesolution y\natthemiddleofthetimeinterval, t=tn+h‚àï2.Andthere‚Äôstherub,forwedonotknowthe\nvalueofyn+1‚àï2andcannotusethisalgorithmtodetermineit.Thewayoutofthisquandary\nistouseEuler‚Äôsalgorithmtodetermine yn+1‚àï2:\nyn+1‚àï2‚âÉyn+1\n2hdy\ndt=yn+1\n2hf(tn,yn). (8.37)\nPuttingthepiecesalltogethergivesthecomplete rk2algorithm:\nyn+1‚âÉyn+k2, (rk2) (8.38)\nk2=hf(\ntn+h\n2,yn+k1\n2)\n,k1=hf(tn,yn), (8.39)\nwhere we use boldface to indicate the vector nature of yandf. We see that the known\nderivativefunction fisevaluatedattheendsandthemidpointoftheinterval,butonlythe\n(known) initial value of the dependent variable yis required. This makes the algorithm\nself-starting.\nAsanexampleoftheuseof rk2,weapplyittoourspringproblem:\ny(0)\n1=y(0)\n0+hf(0)(\nh\n2,y(0)\n0+k1)\n(8.40)\n‚âÉx0+h[\nùë£0+h\n2Fk(0)]\n, (8.41)",5912
79-8.6 Extensions Nonlinear Resonances Beats Friction.pdf,79-8.6 Extensions Nonlinear Resonances Beats Friction,"8.4 ODE Algorithms 155\ny(1)\n1=y(1)\n0+hf(1)[(\nh\n2,y0+h\n2f(0),y0)]\n(8.42)\n‚âÉùë£0+h\nm[\nFext(\nh\n2)\n+Fk(\ny(1)\n0+k1\n2)]\n. (8.43)\nTheseequationssaythattheposition y(0)changesbecauseoftheinitialvelocityandforce,\nwhile the velocity y(1)changes because of the external force at t=h‚àï2 and the internal\nforceattwointermediatepositions.Weseethattheposition y(0)nowhasan h2timedepen-\ndence,whichatlastbringsusuptotheleveloffirst-yearphysics.\nThefourth-orderRunge‚ÄìKuttamethod rk4.py(Listing8.1)obtains ùí™(h4)precisionby\napproximating yasaTaylorseriesuptoorder h2(aparabola)atthemidpointoftheinterval,\nwhichagainleadstocancellationoflower-ordererror.Allinall, rk4providesanexcellent\nbalanceofpower,precision,andprogrammingsimplicity.Withrk4therearefourinterme-\ndiateslopes,andtheseareapproximatedwiththeEuleralgorithm:\nyn+1=yn+1\n6(k1+2k2+2k3+k4), (8.44)\nk1=hf(tn,yn), k2=hf(\ntn+h\n2,yn+k1\n2)\n,\nk3=hf(\ntn+h\n2,yn+k2\n2)\n, k4=hf(tn+h,yn+k3).\nThis provides an improved approximation to f(t,y)near the midpoint. Although rk4is\ncomputationallymoreexpensivethantheEulermethod,itsprecisionismuchbetter,and\nsometimesismadeupbytheabilitytouselargerstepsizes h.\nA variation of rk4, known as the Runge‚ÄìKutta‚ÄìFehling method [Mathews, 2002], or\nrk45, varies the step size while doing the integration with the hope of obtaining better\nprecisionandmaybebetterspeed.Ourimplementation, rk45.py,isgiveninListing8.2.\nItautomaticallydoublesthestepsizeandteststoseehowanestimateoftheerrorchanges.\nIftheerrorisstillwithinacceptablebounds,thealgorithmwillcontinuetousethelarger\nstep size and thus speed up the computation; if the error is too large, the algorithm will\ndecrease the step size until an acceptable error is found. As a consequence of the extra\ninformationobtainedinthetesting,thealgorithmdoesobtain ùí™(h5)precision,butsome-\ntimesattheexpenseofextracomputingtime.Whetherthatextratimeisrecoveredbybeing\nabletousealargerstepsizedependsupontheapplication.\n8.4.3 Adams-Bashful-Moulton Predictor-Corrector Rule\nAnotherapproachforobtaininghighprecisioninanODEalgorithmusesthesolutionfrom\ntwoprevioussteps, yn‚àí2andyn‚àí1,inadditionto yn,topredict yn+1.(TheEulerand rkmeth-\nodsusejustonepreviousstep.)ManyofthesemethodstendtobelikeaNewton‚Äôssearch\nmethod;westartwithaguessor predictionforthenextstep,andthenuseanalgorithm,\nsuch as rk4, to check on the prediction and thereby obtain a correction. As with rk45,\nonecanusethecorrectionasameasureoftheerrorandthenadjustthestepsizetoobtain\nimproved precision [Press et al., 2007]. For those readers who may want to explore such\nmethods, ABM.pyinListing8.3givesourimplementationofthe Adams-Bashful-Moulton\npredictor-correctorscheme.\n156 8 Differential Equations and Nonlinear Oscillations\n8.4.4 Assessment: rk2 versus rk4versus rk45\nWhileyouarefreetodoasyouplease,unlessyouareverycareful,werecommendthatyou\ndonotwriteyourown rk4orrk45methods.Youwillbeusingthisalgorithmforsome\nhigh-precision work, and unless you get every fraction and method call just right, your\ncodemayappeartoworkwell,butstillnotgivealltheprecisionthatyoucouldobtain.And\nsowegiveyou rk4.py,and rk45.pycodestouse.However,wedorecommendthatyou\nwriteyourown rk2,asdoingsowillmakeitclearerastohowtheRunge‚ÄìKuttamethods\nwork,butwithoutallthepainanddangerof rk4.\n1) Writeyourown rk2method,withthederivativefunction f(t,x)aseparatemethod.\n2) Useyour rk2tosolvetheequationofmotion(8.7)or(8.26).Plotboththeposition x(t)\nandvelocity dx‚àïdtasfunctionsoftime.\n3) OnceyourODEsolverisrunning,doanumberofthingstocheckthatitisworkingwell\nandthatyouknowwhat hvaluestouse:\na) Adjust the parameters in your potential so that it corresponds to a pure harmonic\noscillator(set p=2orùõº=0).Foranoscillatorinitiallyatrest,wehaveananalytic\nresultwithwhichtocompare:\nx(t)=Asin(ùúî0t),ùë£=ùúî0Acos(ùúî0t),ùúî0=‚àö\nk‚àïm. (8.45)\nb) Pickvaluesof kandmsuchthattheperiod T=2ùúã‚àïùúîisanicenumberwithwhich\ntowork(somethinglike T=1).\nc) Startwithastepsize h‚âÉT‚àï5andmake hsmalleruntilthesolutionlookssmooth,\nhasaperiodthatremainsconstantforalargenumberofcycles,andagreeswiththe\nanalyticresult.Alwaystrytostartwithalarge hsothatyoucanseeabadsolution\nturngood.\nd) Make sure that you have exactly the same initial conditions for the analytic and\nnumerical solutions (zero displacement, nonzero velocity), and then plot the two\ntogether.Itisgoodifyoucannottellthemapart,yetthatisnotmuchofatestsince\nitonlyensuresapproximatelytwoplacesofagreement.\ne) Trydifferentinitialvelocitiesandverifythata harmonicoscillatoris isochronous ,that\nis,thatitsperioddoes notchangeastheamplitudevaries.\n4) Now that you know you can get a good solution of an ODE with rk2, compare the\nsolutionsobtainedwiththe rk2,rk4,and rk45solvers.\n5) MakeatableofcomparisonssimilartoTable8.1,wherewecompare rk4andrk45for\nthetwoequations\n2yy‚Ä≤‚Ä≤+y2‚àíy‚Ä≤2=0, (8.46)\ny‚Ä≤‚Ä≤+6y5=0, (8.47)\nwithinitialconditions [y(0),y‚Ä≤(0)] = [1,1].Althoughnonlinear,(8.46)doeshavetheana-\nlytic solution,3y(t)=1+sint. Equation (8.47) corresponds to our standard potential\n(8.5),with p=6.Althoughwehavenottuned rk45,Listing8.2showsthatbysetting\n3 Bewarned,the rkproceduresmaybeinaccurateforthisequationifintegratedthroughthepoint\ny(t)=0,asthentheequationbecomes y‚Ä≤2=0,whichisproblematic.\n8.5 Solution for Nonlinear Oscillations 157\nTable 8.1 Comparison of ODE solvers for different equations.\nEqn. no. Method Initial hNo. of Ô¨Çops Time (ms) Relative error\n(8.46) rk40.01 1000 5.2 2.2 √ó10‚àí8\nrk451.00 72 1.5 1.8 √ó10‚àí8\n(8.47) rk40.01 227 8.9 1.8 √ó10‚àí8\nrk450.1 3143 36.7 5.7 √ó10‚àí11\nFigure 8.6 The logarithm of the\nrelative error in the solution of an ODE\nobtained with rk4 using a differing\nnumber Nof time steps over a Ô¨Åxed\ntime interval. The logarithm\napproximately equals the negative of\nthe number of places of precision.\nIncreasing the number of steps used for\na Ô¨Åxed interval is seen to lead to\nsmaller errors.‚Äì7\n‚Äì9\n‚Äì13\nlog |Rel Error|\nTimeError in rk4\nN = 5000N = 1000N = 500\ni t st o l e r a n c ep a r a m e t e rt oas m a l le n o u g hn u m b e r , rk45will obtain better precision\nthanrk4(Figure8.6),butthatitrequires ‚àº10timesmorefloating-pointoperationsand\ntakes‚àº5timeslonger.For(8.46),weobtainedincreasedprecisioninlesstime.\n8.5 Solution for Nonlinear Oscillations\nUse your rk4program to study anharmonic oscillations by trying powers in the range\np=2‚Äì12 for potential (8.5), or anharmonic strengths in the range 0 ‚â§ùõºx‚â§2 for poten-\ntial(8.2).Do notincludeanyexplicittime-dependentforcesyet.Notethatforlargevalues\nofp,theforcesandaccelerationsgetlargeneartheturningpoints,andsoyoumayneeda\nsmallerstepsize hthanthatusedfortheharmonicoscillator.\n1) Check that the solution remains periodic with constant amplitude and period for all\ninitial conditions regardless of how nonlinear you make the force. In addition, check\nthat the maximum speed occurs at x=0 and zero velocity at the maximum |x|‚Äôs, the\nlatterbeingaconsequenceofenergyconservation.\n2) Verifythatnonharmonicoscillatorsare nonisochronous ,thatis,thatvibrationswithdif-\nferentamplitudeshavedifferentperiods(Figure8.7).\n3) Explainwhytheshapesoftheoscillationschangefordifferent p‚Äôsorùõº‚Äôs.\n4) Deviseanalgorithmtodeterminetheperiod Toftheoscillationbyrecordingtimesat\nwhichthemasspassesthroughtheorigin.Notethatbecausethemotionmaybeasym-\nmetric,youmustrecordatleast threetimestodeducetheperiod.\n5) Constructagraphofthededucedperiodasafunctionofinitialamplitude.\n158 8 Differential Equations and Nonlinear Oscillations\n0‚Äì404Amplitude dependence, p = 7\nTimex(t)\nFigure 8.7 The position versus time for oscillations within the potential V‚àùx7for four different\ninitial amplitudes. Each is seen to have a different period.\n6) Verifythatthemotionisoscillatory,butnotharmonic,astheenergyapproaches k‚àï6ùõº2,\norforp>6.\n7) Verify that for the anharmonic oscillator with E=k‚àï6ùõº2, the motion separates from\noscillatorytotranslational.Seehowcloseyoucangettothis separatrixwhereasingle\noscillationtakesaninfinitetime.(Thereisnoseparatrixforthepower-lawpotential.)\n8.5.1 Precision Assessment via E Conservation\nWehavenotexplicitlybuiltenergyconservationintoourODEsolvers.Nonetheless,unless\nyouhaveexplicitlyincludedafrictionalforce,itfollowsmathematicallyfromtheequations\nofmotionthatenergymustbeaconstantforallvaluesof porùõº.Thatbeingthecase,the\nconstancyofenergyisademandingtestofthenumerics.\n1) PlotthepotentialenergyPE (t)=V[x(t)],thekineticenergyKE (t)=mùë£2(t)‚àï2,andthe\ntotalenergy E(t)=KE(t)+PE(t),for50periods.Commentonthecorrelationbetween\nPE(t)andKE(t)andhowitdependsonthepotentialparameters.\n2) Checkthelong-term stabilityofyoursolutionbyplotting\n‚àílog10||||E(t)‚àíE(t=0)\nE(t=0)||||‚âÉnumberofplacesofprecision (8.48)\nforalargenumberofperiods(Figure8.6).Because E(t)shouldbeindependentoftime,\nthenumeratoristheabsoluteerrorinyoursolution,andwhendividedby E(0),becomes\ntherelativeerror(say10‚àí11).Ifyoucannotachieve11ormoreplaces,thenyouneedto\ndecreasethevalueof hordebug.\n3) Because a particle bound by a large- poscillator is essentially ‚Äúfree‚Äù most of the time,\nyoushouldobservethattheaverageofitskineticenergyovertimeexceedsitsaverage\npotentialenergy.ThisisactuallythephysicsbehindtheVirialtheoremforapower-law\npotential[MarionandThornton,2019]:\n‚ü®KE‚ü©=p\n2‚ü®PE‚ü©. (8.49)",9238
80-8.6.1 Friction.pdf,80-8.6.1 Friction,,0
81-8.7 Code Listings.pdf,81-8.7 Code Listings,"8.6 Extensions: Nonlinear Resonances, Beats, Friction 159\nVerifythatyoursolutionsatisfiestheVirialtheorem.(Thosereaderswhohaveworked\nontheperturbedoscillatorproblemcanusethisrelationtodeduceaneffective pvalue,\nwhichshouldbebetween2and3.)\n8.6 Extensions: Nonlinear Resonances, Beats, Friction\nProblem Sofarouroscillationshavebeenrathersimple.Wehaveignoredfrictionand\nhave assumed that there are no external forces (hands) influencing the system‚Äôs natural\noscillations.Determinethefollowing:\n1) Howtheoscillationschangewhenfrictionisincluded.\n2) Howtheresonancesandbeatsofnonlinearoscillatorsdifferfromthoseoflinearoscil-\nlators.\n3) Howintroducingfrictionaffectsresonances.\n8.6.1 Friction\nTheworldisfulloffriction,andnotallofitisbad.Whilefrictionmakesithardertopedal\nabikethroughthewind,italsoletsyouwalkonice,andgenerallyaddsstabilitytodynam-\nicalsystems.Thesimplestmodelsforfrictionalforcearecalled static,kinetic,andviscous\nfriction:\nF(static)\nf‚â§‚àíùúásN,F(kinetic)\nf=‚àíùúákNùë£\n|ùë£|,F(viscous)\nf=‚àíbùë£. (8.50)\nHereNisthenormalforce ontheobjectunderconsideration, ùúáandbareparameters,and\nùë£is the velocity. This model for static friction is appropriatefor objects at rest, while the\nmodelforkineticfrictionisappropriateforanobjectslidingonadrysurface.Ifthesurface\nislubricated,oriftheobjectismovingthroughaviscousmedium,thenafrictionalforce\nproportionaltosomepowerofthevelocityisabettermodel.4\n1) Extendyourharmonicoscillatorcodetoincludethethreetypesoffrictionin(8.50),and\nobservehowthemotiondiffersforeach.\n2)Hint: For the simulation with static plus kinetic friction, each time the oscillator has\nùë£=0,youneedtocheckthattherestoringforceexceedsthestaticforceoffriction.If\nnot,theoscillationmustendatthatinstant.Checkthatyoursimulationterminatesat\nnonzeroxvalues.\n3) Foryoursimulationswithviscousfriction,investigatethequalitativechangesthatoccur\nforincreasing bvalues:\nUnder damped: b<2mùúî0Oscillatewithindecayingenvelope\nCritically damped: b=2mùúî0Nonconciliatory,finitedecaytime\nOver damped: b>2mùúî0Nonconciliatory,infinitedecaytime\n8.6.2 Resonances and Beats\nStable physical systems will oscillate if displaced slightly from their rest positions. The\nfrequency ùúî0withwhichastablesystemexecutessmalloscillationsaboutitsrestpositions\n4 TheeffectofairresistanceonprojectilemotionisstudiedinSection13.4.\n160 8 Differential Equations and Nonlinear Oscillations\niscalledits naturalfrequency .Ifanexternalsinusoidalforceisappliedtothissystem,and\nifthefrequencyoftheexternalforceisequaltothenaturalfrequency ùúî0,thenaresonance\nmayoccurinwhichthesystemabsorbsenergyfromtheexternalforceandtheamplitudeof\noscillationincreaseswithtime.Iftheoscillationandthedrivingforceremaininphaseover\ntime,theamplitudeofoscillationwillincreasecontinuously,unlessthereissomemecha-\nnism,suchasfrictionornonlinearities,tolimitthegrowth.Ifthefrequencyofthedriving\nforceiscloseto,butnotexactlyequalto,thenaturalfrequencyofthesystem,thenarelated\nphenomena,knownas beating,mayoccur.Inbeatingthereisinterferencebetweenthenat-\nuraloscillationandtheexternalforce.Ifthefrequencyoftheexternaldrivingforceisvery\nclosetothenaturalfrequency,thentheresultingmotion,\nx‚âÉx0sinùúît+x0sinùúî0t=(\n2x0cosùúî‚àíùúî0\n2t)\nsinùúî+ùúî0\n2t, (8.51)\nresemblesthenaturaloscillationofthesystemattheaveragefrequencyùúî+ùúî0\n2,yetwithan\namplitude2 x0cosùúî‚àíùúî0\n2tthatvariesslowlywitha beatfrequencyùúî‚àíùúî0\n2.\n8.6.3 Time-Dependent Forces\nToextendoursimulationtoincludeanexternalforce,\nFext(t)=F0sinùúît, (8.52)\nweneedtoincludeatimedependenceintheforcefunction f(t,y)ofourODEsolver.\n1) Addthesinusoidaltime-dependentexternalforce(8.52)tothespace-dependentrestor-\ningforceinyourprogram(donotincludefrictionyet).\n2) Startwithaverylargevalueforthemagnitudeofthedrivingforce F0.Thisshouldlead\ntomode locking (the 500-pound-gorilla effect), where the system is overwhelmed by\nthe driving force and, after the transients die out, the system oscillates in phase with\nthedriverregardlessofthedriver‚Äôsfrequency.\n3) Now lower F0until it is close to the magnitude of the natural restoring force of the\nsystem.Youneedtohavethisnearequalityforbeatingtooccur.\n4) Verify that the beat frequency for the harmonic oscillator (the number of variations\nin intensity per unit time) equals the frequency difference (ùúî‚àíùúî0)‚àï2ùúãin cycles per\nsecond,where ùúî‚âÉùúî0.\n5) Onceyouhaveavaluefor F0matchedwellwithyoursystem,makeaseriesofrunsin\nwhich you progressively increase the frequency of the driving force for the frequency\nrangeùúî0‚àï10‚â§ùúî‚â§10ùúî0.\n6) Makeofplotofthemaximumamplitudeofoscillation versusthedriver‚Äôs ùúî.\n7) Explore what happens when you make a nonlinear system resonate. If the nonlinear\nsystemisclosetobeingharmonic,youshouldgetbeatinginplaceoftheblowupthat\noccursforthelinearsystem.Beatingoccursbecausethenaturalfrequencychangesas\ntheamplitudeincreases,andthusthenaturalandforcedoscillationsfalloutofphase.\nYetonceoutofphase,theexternalforcestopsfeedingenergyintothesystem,andsothe\namplitudedecreases,andwiththedecreaseinamplitude,thefrequencyoftheoscillator\nreturnstoitsnaturalfrequency,thedriverandoscillatorgetbackinphase,andtheentire\ncyclerepeats.\n8.7 Code Listings 161\n8) Investigatenowhowtheinclusionofviscousfrictionmodifiesthecurveofamplitude\nversusdriverfrequency.Youshouldfindthatfrictionbroadensthecurve.\n9) Explainhowthecharacteroftheresonancechangesastheexponent pinthepotential\nV(x)=k|x|p‚àïpismadelargerandlarger.Atlarge p,themasseffectively‚Äúhits‚Äùthewall\nandfallsoutofphasewiththedriver,andsothedriverislesseffectiveatpumpingenergy\nintothesystem.\n8.7 Code Listings\nListing 8.1 rk4.py solvesanODEwiththeRHSgivenbythemethodf()usingrk4.The\nmethodf()isseparatefromthealgorithm.\n1# rk4.py 4th order Runge Kutta application wi built in rk4\nfromvisual.graph import ‚àó\n5# Initialization\na=0 .\nb = 10.\nn = 100\n9ydumb = zeros((2) , float); y= zeros((2), float)\nfReturn = zeros((2), float); k1= zeros((2), float)\nk2 = zeros((2) , float); k3= zeros((2), float)\nk4 = zeros((2) , float)\n13y[0] = 3.; y[1] = ‚àí5.\nt=a ; h=( b ‚àía)/n;\ndeff( t, y): # Force function\n17fReturn[0] = y[1]\nfReturn[1] = ‚àí100.‚àóy[0]‚àí2.‚àóy[1] + 10. ‚àósin(3. ‚àót)\nreturnfReturn\n21graph1 = gdisplay(x=0,y=0, width = 400, height = 400, title = ‚ÄôRK4‚Äô,\nxtitle = ‚Äôt‚Äô, ytitle = ‚ÄôY[0]‚Äô,xmin=0,xmax=10,ymin= ‚àí2,ymax=3)\nfunct1 = gcurve(color = color.yellow)\ngraph2 = gdisplay(x=400,y=0, width = 400, height = 400, title = ‚ÄôRK4‚Äô,\n25 xtitle = ‚Äôt‚Äô, ytitle = ‚ÄôY[1]‚Äô,xmin=0,xmax=10,ymin= ‚àí25,ymax=18)\nfunct2 = gcurve(color = color.red)\ndefrk4(t,h,n):\n29k1 = [0] ‚àó(n)\nk2 = [0] ‚àó(n)\nk3 = [0] ‚àó(n)\nk4 = [0] ‚àó(n)\n33fR = [0] ‚àó(n)\nydumb = [0] ‚àó(n)\nfR = f(t, y) # Returns RHS‚Äô s\nforiin range (0, n):\n37 k1[i] = h ‚àófR[i]\nforiin range (0, n):\nydumb[i] = y[i] + k1[i]/2.\nk2 = h ‚àóf(t+ h/2., ydumb)\n41foriin range (0, n):\nydumb[i] = y[i] + k2[i]/2.\nk3 = h ‚àóf(t+ h/2., ydumb)\nforiin range (0, n):\n45 ydumb[i] = y[i] + k3[i]\nk4 = h ‚àóf(t+ h, ydumb)\nforiin range (0, 2):\ny[i] = y[i] + (k1[i] + 2. ‚àó(k2[i] + k3[i]) + k4[i])/6.\n49returny\nwhile(t<b): # Time loop\nif((t + h) > b):\n162 8 Differential Equations and Nonlinear Oscillations\n53 h=b‚àít # Last step\ny=r k 4 ( t, h , 2 )\nt=t+h\nrate(30)\n57funct1.plot(pos = (t, y[0]) )\nfunct2.plot(pos = (t, y[1]) )\nListing 8.2 rk45.py solvesanODEwiththeRHSgivenbythemethodf()usingrk4with\nadaptivestepsize.\n# rk45 .py Adaptive step size Runge Kutta\nfromvisual.graph import ‚àó\n4\na = 0.; b = 10. # Error tolerance , endpoints\nTol = 1.0E ‚àí8\nydumb = zeros( (2) , float) # Initialize\n8y=z e r o s (( 2 ), float)\nfReturn = zeros( (2), float)\nerr = zeros( (2), float)\nk1 = zeros( (2) , float)\n12k2 = zeros( (2) , float)\nk3 = zeros( (2) , float)\nk4 = zeros( (2) , float)\nk5 = zeros( (2) , float)\n16k6 = zeros( (2) , float)\nn=2 0\ny [ 0 ]=1 .; y [ 1 ]=0 .\n20h=( b ‚àía)/n; t = a; j = 0\nh m i n=h / 6 4 ; h m a x=h ‚àó64 # Min and m a x step sizes\nflops = 0; Eexact = 0. ; error = 0.\nsum=0 .\n24\ndeff( t, y, fReturn ): # Force function\nfReturn[0] = y[1]\nfReturn[1] = ‚àí6.‚àópow(y[0], 5.)\n28\ngraph1 = gdisplay( width = 600, height = 600, title = ‚ÄôRK 45‚Äô,\nxtitle = ‚Äôt‚Äô, ytitle = ‚ÄôY[0]‚Äô)\nfunct1 = gcurve(color = color.blue)\n32graph2 = gdisplay( width = 500, height = 500, title = ‚ÄôRK45‚Äô,\nxtitle = ‚Äôt‚Äô, ytitle = ‚ÄôY[1]‚Äô)\nfunct2 = gcurve(color = color.red)\nfunct1.plot(pos = (t, y[0]) )\n36funct2.plot(pos = (t, y[1]) )\nwhile(t<b): # Loop over time\nfunct1.plot(pos = (t, y[0]) )\n40funct2.plot(pos = (t, y[1]) )\nif(( t +h )>b) :\nh=b ‚àít # Last step\nf(t, y, fReturn) # Evaluate f , return in fReturn\n44k1[0] = h ‚àófReturn[0]; k1[1] = h ‚àófReturn[1]\nforiin range (0, 2):\nydumb[i] = y[i] + k1[i]/4\nf(t + h/4, ydumb, fReturn)\n48k2[0] = h ‚àófReturn[0]; k2[1] = h ‚àófReturn[1]\nforiin range (0, 2):\nydumb[i] = y[i] + 3 ‚àók1[i]/32 + 9 ‚àók2[i]/32\nf(t + 3 ‚àóh/8, ydumb, fReturn)\n52k3[0] = h ‚àófReturn[0]; k3[1] = h ‚àófReturn[1]\nforiin range (0, 2):\nydumb[i] = y[i] + 1932 ‚àók1[i]/2197 ‚àí7200 ‚àók2[i]/2197. + 7296 ‚àók3[i]/2197\nf(t + 12 ‚àóh/13, ydumb, fReturn)\n56k4[0] = h ‚àófReturn[0]; k4[1] = h ‚àófReturn[1]\nforiin range (0, 2):\n8.7 Code Listings 163\nydumb[i] = y[i] + 439 ‚àók1[i]/216 ‚àí8‚àók2[i] + 3680 ‚àók3[i]/513 ‚àí\n845‚àók4[i]/4104\nf(t + h, ydumb, fReturn)\n60k5[0] = h ‚àófReturn[0]; k5[1] = h ‚àófReturn[1]\nforiin range (0, 2):\nydumb[i] = y[i] ‚àí8‚àók1[i]/27 + 2 ‚àók2[i]‚àí3544 ‚àók3[i]/2565 +\n1859 ‚àók4[i]/4104 ‚àí11‚àók5[i]/40\nf(t + h/2, ydumb, fReturn)\n64k6[0] = h ‚àófReturn[0]; k6[1] = h ‚àófReturn[1];\nforiin range (0, 2):\nerr[i] = abs( k1[i]/360 ‚àí128‚àók3[i]/4275 ‚àí2197 ‚àók4[i]/75240 +\nk5[i]/50. + 2 ‚àók6[i]/55)\nif(e r r [ 0 ] <Tolorerr[1] <Tolorh<=2 ‚àóhmin ): # Accept step\n68 foriin range (0, 2):\ny[i] = y[i] + 25 ‚àók1[i]/216. + 1408 ‚àók3[i]/2565. + 2197 ‚àók4[i]/4104.\n‚àík5[i]/5.\nt=t + h\nj=j + 1\n72if(e r r [ 0 ]= =0 orerr[1] == 0 ):\ns=0 # Trap division by 0\nelse:\ns = 0.84 ‚àópow(Tol ‚àóh/err[0], 0.25) # Reduce step\n76if(s<0.75andh>2 ‚àóhmin ):\nh/ = 2 . # Increase step\nelse:\nif(s>1 . 5 and2‚àóh<hmax ):\n80 h‚àó=2 .\nflops = flops + 1\nE=pow(y[0], 6.) + 0.5 ‚àóy[1] ‚àóy[1]\nEexact = 1.\n84error = abs(( E‚àíEexact)/Eexact)\nsum+= error\nprint(""<error>= "" ,sum/flops, "", flops = "" , flops)\nListing 8.3 ABM.py solvesanODEwiththeRHSgivenbythemethodf()usingtheABC\npredictor-correctoralgorithm.\n# A B M.py: A d a m s B M method to integrate O D E\n#S o l v e sy ‚Äô=( t ‚àíy)/2, with y[0] = 1 over [0, 3]\n4fromvpython import ‚àó\nnumgr = graph(x=0, y=0, width=600, height=300, xmin=0.0, xmax = 3.0,\ntitle= ""Numerical Solution"" , xtitle= ‚Äôt‚Äô, ytitle= ‚Äôy‚Äô,y m a x = 2 . ,y m i n = 0 )\n8numsol = gcurve(color=color.red)\nexactgr = graph(x=0, y=300, width=600, height=300, title= ""Exact solution"" ,\nxtitle= ‚Äôt‚Äô, ytitle= ‚Äôy‚Äô, xmax=3.0, xmin=0.0, ymax=2.0, ymin=0)\n12exsol =gcurve (color = color.cyan)\nn=2 4 # N steps > 3\nA=0 ;B=3 .\nt =[0] ‚àó500; y =[0] ‚àó500; yy=[0] ‚àó4\n16\ndeff(t, y): # R H S F function\nreturn (t‚àíy)/2.0\n20defrk4(t, yy, h1):\nforiin range (0, 3):\nt= h 1 ‚àói\nk0 = h1 ‚àóf(t, y[i])\n24 k1 = h1 ‚àóf(t + h1/2., yy[i] + k0/2.)\nk2 = h1 ‚àóf(t + h1/2., yy[i] + k1/2.)\nk3 = h1 ‚àóf(t +h1, yy[i] + k2 )\nyy[i + 1] = yy[i] + (1./6.) ‚àó(k0 + 2. ‚àók1 + 2. ‚àók2 + k3)\n28 print(i,yy[ i])\nreturnyy[3]\ndefABM(a , b ,N) :\n164 8 Differential Equations and Nonlinear Oscillations\n32# Compute 3 additional starting values using rk\nh=( b‚àía) / N # step\nt[0] = a; y[0] = 1.00; F0 = f(t[0], y[0])\nforkin range (1, 4):\n36 t[k] = a + k ‚àóh\ny[1] = rk4(t[1], y, h) # 1st step\ny[2] = rk4(t[2], y, h) # 2nd step\ny[3] = rk4(t[3], y, h) # 3rd step\n40F1 = f(t[1], y[1])\nF2 = f(t[2], y[2])\nF3 = f(t[3], y[3])\nh2 = h/24.\n44forkin range (3, N): # Predictor\np=y [ k ] + h 2 ‚àó(‚àí9.‚àóF0 + 37. ‚àóF1‚àí59.‚àóF2 + 55. ‚àóF3)\nt[k+ 1] = a +h ‚àó(k+1) # Next abscissa\nF4 = f(t[k+1], p)\n48 y [ k + 1 ]=y [ k ]+h 2 ‚àó(F1‚àí5.‚àóF2 + 19. ‚àóF3 + 9. ‚àóF4) # Corrector\nF0 = F1 # Update values\nF1 = F2\nF2 = F3\n52 F 3=f(t[ k+1 ],y [ k+1 ] )\nreturnt,y\nt, y=A B M (A,B,n)\n56forkin range (0, n+1):\nnumsol.plot( t[k], y[k] )\nexsol.plot( t[k], 3. ‚àóexp(‚àít[k]/2.) ‚àí2. + t[k])",11925
82-Part II Data Science.pdf,82-Part II Data Science,165\nPart II\nData Science,26
83-Chapter 9 Fourier Analyses.pdf,83-Chapter 9 Fourier Analyses,,0
84-9.1 Fourier Series.pdf,84-9.1 Fourier Series,"167\n9\nFourier Analyses\nThis chapter discusses Fourier series and Fourier transforms. When implemented as\nalgorithms, both become the Discrete Fourier Transform (DFT), or its fast cousin, the Fast\nFourier Transform (FFT). In Chapter 14, we discuss the Short-Time Fourier Transform, and\nin Chapter 12, we derive the Quantum Fourier Transform, the quantum computing version\nof the DFT .\n9.1 Fourier Series\nConsider againaparticleoscillatingeitherinthenonharmonicpotentialof(8.5):\nV(x)=1\npk|x|p,p‚â†2, (9.1)\norintheperturbedharmonicoscillatorpotential(8.2),\nV(x)=1\n2kx2(\n1‚àí2\n3ùõºx)\n. (9.2)\nWhilefreeoscillationsinthesepotentialsarealwaysperiodic,theyarenottrulysinusoidal.\nYour problemistotakethesolutionofoneofthesenonlinearoscillatorsandexpanditin\naFourierseries:\ny(t)=b0sinùúî0t+b1sin2ùúî0t+¬∑¬∑¬∑. (9.3)\nForexample,ifyouroscillatorissufficientlynonlineartobehavelikethesawtoothfunction\n(Figure9.1left),thentheFourierspectrumyouobtainshouldbesimilartothatshownon\ntherightinFigure9.1.\nIngeneral,whenweundertakesuchaspectralanalysiswewanttoanalyzethesteady-\nstatebehaviorofasystem.Thismeansthatwehavetowaitfortheinitialtransientstodie\nout.Itiseasytoidentifyjustwhattheinitialtransientisforlinearsystems,butmaybeless\napparentfornonlinearsystemsinwhichthe‚Äústeadystate‚Äùjumpsamonganumberofcon-\nfigurations.Inthelattercase,wecouldconstructdifferentFourierspectraatdifferenttimes,\nasisdonewiththe Short-TimeFourierTransform tobediscussedinChapter10.\nPartofourinterestinnonlinearoscillationsarisesfromtheirlackofstudyintraditional\nphysicscourses,wherejust(approximate)linearoscillationsareoftenstudied.Iftheforce\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n168 9 Fourier Analyses\nonaparticleisalwaystowarditsequilibriumposition(arestoringforce),thentheresulting\nmotionwillbe periodic,butnotnecessarily harmonic.Agoodexampleisthemotioninthe\nhighlyanharmonicpotential,suchas(9.1)with p‚âÉ10,thatproducesan x(t)lookinglikea\nseriesofpyramids;thismotionisperiodicbutnotharmonic.\nOur approach is in contrast to the traditional one in which the fundamental oscilla-\ntion is determined analytically, and the higher-frequency overtonesare determined by\nperturbation theory [Landau and Lifshitz, 1976]. We start with the full solution, and\ndecompose it into harmonics andovertones. When we speak of fundamentals, overtones,\nandharmonics,wespeakofsolutionstothelinear boundary-valueproblem ,forexample,of\nwavesonapluckedviolinstring.Inthislattercase,andwhengiventhecorrectconditions\n(andenoughmusicalskill),itispossibletoexciteindividualharmonics,orsumsofthem,\nfromtheseries(9.3).\nYoumayrecallfromclassicalmechanicsthatthegeneralsolutionforavibratingsystem\ncan be expressed as the sum of the normal modes of that system. These expansions are\npossibleonlyifwehave linearoperators and,consequently,the principleofsuperposition :\nIfy1(t)andy2(t)aresolutionsofsomelinearequation,then ùõº1y1(t)+ùõº2y2(t)isalsoasolu-\ntion.Theprincipleoflinearsuperpositiondoesnotholdwhenwesolvenonlinearproblems.\nNevertheless,itisalwayspossibletoexpanda periodicsolutionofa nonlinearproblemin\ntermsoftrigonometricfunctions.This is aconsequence of Fourier‚Äôstheorem beingappli-\ncabletoanysingle-valuedperiodicfunctionwithonlyafinitenumberofdiscontinuities.\nWeassumeweknowtheperiod T,thatis,that\ny(t+T)=y(t). (9.4)\nThistellsusthe truefrequency ùúî:\nùúî‚â°ùúî1=2ùúã\nT. (9.5)\nAnyperiodicfunction(oftendesignatedasthe signal)canbeexpandedasaseriesofhar-\nmonicfunctionswithfrequenciesthataremultiplesofthetruefrequency:\ny(t)=a0\n2+‚àû‚àë\nn=1(ancosnùúît+bnsinnùúît). (9.6)\nThisequationrepresentsthesignal y(t)asthesimultaneoussumofpuretonesoffrequency\nnùúî.Thecoefficients anandbnmeasureoftheamountofcos nùúîtandsinnùúîtpresentiny(t),\nrespectively.Theintensityor powerateachfrequencyisproportionalto a2\nn+b2\nn.\nTheFourierseries(9.6)isa‚Äúbestfit,‚Äùintheleast-squaressenseofChapter6,toanumber\nofmeasurementsofthesignal.Thismeansthattheseriesconvergestothe averagebehav-\nior of the signal, but misses the signal at discontinuities (at which points it converges to\nthemean),oratsharpcorners(whereitovershoots).Ageneralfunction y(t)maycontain\naninfinitenumberofFouriercomponents,althoughlow-accuracyreproductionisusually\npossiblewithasmallnumberofcomponents.\nThecoefficients anandbnin(9.6)aredeterminedbythestandardtechniquesoforthog-\nonalfunctionexpansion.Tofindthem,multiplybothsidesof(9.6)bycos nùúîtorsinnùúît,\nintegrateoveroneperiod,andprojectasingle anorbn:\n(an\nbn)\n=2\nT‚à´T\n0dt(cosnùúît\nsinnùúît)\ny(t),ùúîdef=2ùúã\nT. (9.7)",4608
85-9.1.2 Exercises Fourier Series Summations.pdf,85-9.1.2 Exercises Fourier Series Summations,"9.1 Fourier Series 169\nt‚Äì101\ny(t)\n02 0‚Äì101\nY(œâ)\nœâ\nFigure 9.1 Left: A periodic sawtooth function. Right: The Fourier spectrum of frequencies\ncontained in this function.\nAsseenin(Figure9.1right),the bn‚Äôsdecreaseinmagnitudeasthefrequencyincreases,and\ncanenterwithapositiveornegativesign,thenegativesignindicatingrelativephase.\nAwarenessofthe symmetryofthefunction y(t)mayeliminatetheneedtoevaluateallthe\nexpansioncoefficients.Forexample,\n‚óèa0istwicetheaveragevalueof y:a0=2‚ü®y(t)‚ü©.\n‚óèFor anoddfunction ,thatis,oneforwhich y(‚àít)=‚àíy(t),allancoefficientsequal0,and\nonlyhalfoftheintegrationrangeisneededtodetermine bn:\nbn=4\nT‚à´T‚àï2\n0dty(t)sinnùúît. (9.8)\nHowever,ifthereisnoinputsignalfor t<0,wedonothaveatrulyoddfunction,andso\nsmallvaluesof anmayoccur.\n‚óèForanevenfunction ,thatis,oneforwhich y(‚àít)=y(t),allbncoefficientequal0,andonly\nhalftheintegrationrangeisneededtodetermine an:\nan=4\nT‚à´T‚àï2\n0dty(t)cosnùúît. (9.9)\n9.1.1 Sawtooth and Half-Wave Functions\nThe sawtooth function (Figure9.1left)isdescribedmathematicallyas\ny(t)=‚éß\n‚é™\n‚é®\n‚é™‚é©t\nT‚àï2,for0‚â§t‚â§T\n2,\nt‚àíT\nT‚àï2,forT\n2‚â§t‚â§T.(9.10)\nIt is clearly periodic, nonharmonic, and discontinuous. Yet it is also odd and so can be\nrepresentedmoresimplybyshiftingthesignaltotheleft:\ny(t)=t\nT‚àï2,‚àíT\n2‚â§t‚â§T\n2. (9.11)\nAlthoughthegeneralshapeofthisfunctioncanbereproducedwithonlyafewtermsofthe\nFouriercomponents,manycomponentsareneededtoreproducethesharpcorners.Asthe",1422
86-9.2 Fourier Transforms.pdf,86-9.2 Fourier Transforms,"170 9 Fourier Analyses\nfunctionisodd,theFourierseriesisasineseries,and(9.7)determinesthe bnvalues:\nbn=2\nT‚à´+T‚àï2\n‚àíT‚àï2dtsinnùúîtt\nT‚àï2=2\nnùúã(‚àí1)n+1, (9.12)\n‚áíy(t)=2\nùúã[\nsinùúît‚àí1\n2sin2ùúît+1\n3sin3ùúît‚àí¬∑¬∑¬∑]\n. (9.13)\nThe half-wave function\ny(t)={sinùúît,for0<t<T‚àï2,\n0,forT‚àï2<t<T,(9.14)\nisperiodic,nonharmonic(theupperhalfofasinewave),andcontinuous,butwithdiscon-\ntinuousderivatives.Becauseitlacksthesharpcornersofthesawtoothfunction,itiseasier\ntoreproducewithafiniteFourierseries.Equation(9.7)determines\nan=‚éß\n‚é™\n‚é®\n‚é™‚é©‚àí2\nùúã(n2‚àí1),nevenor0 ,\n0,nodd,bn={1\n2,n=1,\n0,n‚â†1,\n‚áíy(t)=1\n2sinùúît+1\nùúã‚àí2\n3ùúãcos 2ùúît‚àí2\n15ùúãcos4ùúît+. (9.15)\n9.1.2 Exercises: Fourier Series Summations\nHint:Theprogram FourierMatplot.py writtenbyOscarEstrepeperformsaFourieranal-\nysisofasawtoothfunctionandproducesthevisualizationshownontherightofFigure9.1.\nYoumaywanttousethisprogramtohelpwiththisexercise.\n1)Sawtooth function : Sum the Fourier series for the sawtooth function up to order\nN=2,4,10,20,andplottheresultsovertwoperiods.\n(a) Checkthatineachcasetheseriesgivesthemeanvalueofthefunction atthepoints\nofdiscontinuity.\n(b) Checkthatineachcasetheseries overshootsbyabout9 %thevalueofthefunction\noneithersideofthediscontinuity(the Gibbsphenomenon ).\n2)Half-wave function : Sum the Fourier series for the half-wave function up to order\nN=2,4,10,20,andplottheresultsovertwoperiods.(Theseriesconvergesquitewell,\ndoesn‚Äôtit?)\n9.2 Fourier Transforms\nAlthough a Fourier seriesis the right tool for approximating or analyzing periodicfunc-\ntions,theFourier transformorintegralistherighttoolforanalyzingnonperiodicfunctions.\nWeproceedfromtheseriestothetransformbyimaginingasystemdescribedbyacontin-\nuumof‚Äúfundamental‚Äùfrequencies,namely, wavepackets .1Whilethedifferencebetween\n1 Wehavechosentimeandfrequencyastheconjugatevariableshere,butitcouldbeotherwise,suchas\npositionxandwavevector k.\n9.2 Fourier Transforms 171\nseriesandtransformsmayappearclearmathematically,whenweapproximatetheFourier\nintegralasafinitesum,thetwobecomeequivalent.\nByanalogywith(9.6),wenowimagineourfunctionorsignal y(t)expressedintermsofa\ncontinuousseriesofharmonics( inverseFouriertransform ):\ny(t)=‚à´+‚àû\n‚àí‚àûdùúîY(ùúî)eiùúît\n‚àö\n2ùúã, (9.16)\nwhereforcompactnessweuseacomplexexponentialfunction.2Theexpansionamplitude\nY(ùúî)isanalogoustotheFouriercoefficients (an,bn),andiscalledthe Fouriertransform of\ny(t). The integral (9.16) is the inverse transform because it converts the transform to the\nsignal.The Fouriertransform convertsthesignal y(t)toitstransform Y(ùúî):\nY(ùúî)=‚à´+‚àû\n‚àí‚àûdte‚àíiùúît\n‚àö\n2ùúãy(t). (9.17)\nThe1‚àï‚àö\n2ùúãfactorinboththeseintegralsisacommonnormalizationinquantummechan-\nics, but may not be in engineering, where only a single 1 ‚àï2ùúãfactor is sometimes used.\nLikewise,thesignsintheexponentsarealsoconventionsthatdonotmatteraslongasyou\nmaintainconsistency.\nIfy(t)isthemeasuredresponseofasystem(signal)asafunctionoftime,then Y(ùúî)isthe\nspectralfunction thatmeasurestheamountoffrequency ùúîpresentinthesignal.Inmany\ncases,itturnsoutthat Y(ùúî)isacomplexfunctionwithbothpositiveandnegativevalues,\nandwithpowers-of-tenvariationinmagnitude.Accordingly,itiscustomarytoeliminate\nsomeoftheextremevariationsof Y(ùúî)bymakingasemilogplotofthesquaredmodulus\n|Y(ùúî)|2versusùúî. This is called a powerspectrum and provides an immediate view of the\namountofpowerorstrengthineachcomponent.\nIftheFouriertransformanditsinverseareconsistentwitheachother,weshouldbeable\ntosubstitute(9.16)into(9.17)andobtainanidentity:\nY(ùúî)=‚à´+‚àû\n‚àí‚àûdte‚àíiùúît\n‚àö\n2ùúã‚à´+‚àû\n‚àí‚àûdùúî‚Ä≤eiùúî‚Ä≤t\n‚àö\n2ùúãY(ùúî‚Ä≤) (9.18)\n=‚à´+‚àû\n‚àí‚àûdùúî‚Ä≤{\n‚à´+‚àû\n‚àí‚àûdtei(ùúî‚Ä≤‚àíùúî)t\n2ùúã}\nY(ùúî‚Ä≤). (9.19)\nForthistobeanidentity ,theterminbracesmustbethe Diracdeltafunction :\n‚à´+‚àû\n‚àí‚àûdtei(ùúî‚Ä≤‚àíùúî)t=2ùúãùõø(ùúî‚Ä≤‚àíùúî). (9.20)\nWhile the delta function is one of the most common and useful functions in theoretical\nphysics,itisnotwellbehavedinamathematicalsense,andmisbehavesterriblyinacom-\nputationalsense.Whileitispossibletocreatenumericalapproximationsto ùõø(ùúî‚Ä≤‚àíùúî),they\nmaywellbeborderlinepathological.Itiscertainlybetterforyoutodothedeltafunction\npartofanintegrationanalytically.\n2 Recallthatexp (iùúît)=cosùúît+isinùúît,andwiththelawoflinearsuperpositionthismeansthatthereal\npartofygivesthecosineseries,andtheimaginarypartthesineseries.",4205
87-9.3 Discrete Fourier Transforms.pdf,87-9.3 Discrete Fourier Transforms,"172 9 Fourier Analyses\n9.3 Discrete Fourier Transforms\nIfy(t)orY(ùúî)is known analytically or numerically, the integral (9.16) and (9.17) can be\nevaluated using the integration techniques studied earlier. In practice, the signal y(t)is\nmeasured at just a finite number Nof timest. The resultant DFTis an approximation,\nbothbecausethesignalisnotknownforalltimes,andbecauseweintegratenumerically\n[BriggsandHenson,1995].Oncewehaveadiscretesetof(approximate)transformvalues,\ntheycanbeusedtoreconstructthesignalforanyvalueofthetime.Inthisway,theDFT\ncanbethoughtofasatechniqueforinterpolating,compressing,andextrapolatingasignal.\nWeassumethatthesignal y(t)issampledat (N+1)discretetimes( Ntimeintervals),with\naconstantspacing Œît=hbetweentimes:\nykdef=y(tk),k=0,1,2,‚Ä¶,N, (9.21)\ntkdef=kh,h=Œît. (9.22)\nInotherwords,wemeasure y(t)onceevery hthofasecondforatotaltimeof T.Thiscorre-\nspondinglydefinesthesignal‚Äôsperiod Tandthesamplingrates :\nTdef=Nh,s=N\nT=1\nh. (9.23)\nRegardless of the true periodicity of the signal, when we choose a period Tover which\nto sample the signal, the mathematics will inevitablyproduce a y(t)that is periodic with\nperiodT,\ny(t+T)=y(t). (9.24)\nWerecognizethisperiodicity,andensurethatthereareonly Nindependentmeasurements\nusedinthetransform,bydefiningthefirstandlast y‚Äôstobeequal:\ny0=yN. (9.25)\nIfweareanalyzingatrulyperiodicfunction,thenthe Npointsshouldspanonecomplete\nperiod,butnotmore.Thisguaranteestheirindependence.Unlesswemakefurtherassump-\ntions,theNindependentdata y(tk)candeterminenomorethan Nindependenttransform\nvaluesY(ùúîk).\nThe time interval T(which should be the period for periodic functions) is the largest\ntimeoverwhichwemeasurethevariationof y(t).Consequently,itdeterminesthelowest\nfrequencycontainedinourFourierrepresentationof y(t),\nùúî1=2ùúã\nT. (9.26)\nThefullrangeoffrequenciesinthespectrum ùúînisdeterminedbythenumberofsamples\ntaken,andbythetotalsamplingtime T=Nhas\nùúîn=nùúî1=n2ùúã\nNh,n=0,1,‚Ä¶,N. (9.27)\nHereùúî0=0correspondstothezero-frequencyor DCcomponent ofthetransform,thatis,\nthepartofthesignalthatdoesnotoscillate.\nThe DFT algorithm follows from two approximations. First, we evaluate the integral\n(9.17)fromtime0totime T,overwhichthesignalismeasured,andnotfrom ‚àí‚àûto+‚àû.\n9.3 Discrete Fourier Transforms 173\nSecond,thetrapezoidruleisusedfortheintegration3:\nY(ùúîn)def=‚à´+‚àû\n‚àí‚àûdte‚àíiùúînt\n‚àö\n2ùúãy(t)‚âÉ‚à´T\n0dte‚àíiùúînt\n‚àö\n2ùúãy(t), (9.28)\n‚âÉN‚àë\nk=1hy(tk)e‚àíiùúîntk\n‚àö\n2ùúã=hN‚àë\nk=1yke‚àí2ùúãikn‚àïN\n‚àö\n2ùúã. (9.29)\nTokeepthefinalnotationmoresymmetric,thestepsize hisfactoredfromthetransform Y\nandadiscretefunction Ynisdefined:\nYndef=1\nhY(ùúîn)=N‚àë\nk=1yke‚àí2ùúãikn‚àïN\n‚àö\n2ùúã,n=0,1‚Ä¶,N. (9.30)\nWiththissamecareinaccounting,andwith dùúî‚Üí2ùúã‚àïNh,weinvertthe Yn‚Äôs:\ny(t)def=‚à´+‚àû\n‚àí‚àûdùúîeiùúît\n‚àö\n2ùúãY(ùúî), (9.31)\n‚áíy(t)‚âÉN‚àë\nn=12ùúã\nNheiùúînt\n‚àö\n2ùúãY(ùúîn). (9.32)\nOnceweknowthe Nvaluesofthetransform,wecanuse(9.32)toevaluate y(t)foranytime\nt.Thereisnothingillegalaboutevaluating Ynandykforarbitrarilylargevaluesof nandk,\nyetthereisalsonothingtobegained;becausethetrigonometricfunctionsareperiodic,we\njustgettheoldanswers:\ny(tk+N)=y([k+N]h)=y(tk), (9.33)\nY(ùúîn+N)=Y([n+N]ùúî1)=Y(ùúîn). (9.34)\nAnotherwayofstatingthisistoobservethatnoneoftheequationschangeifwereplace ùúînt\nbyùúînt+2ùúãn.Therearestilljust N-independentoutputnumbersfor Nindependentinputs,\nwiththetransformandthereconstitutedsignalperiodic.\nWeseefrom(9.27)thatthelargerwemakethetime T=Nhoverwhichwesamplethe\nfunction,thesmallerwillbethefrequencystepsorresolution.4Accordingly,ifyouwanta\nsmoothfrequencyspectrum,youwillneedtohaveasmallerfrequencystep2 ùúã‚àïT,which\nmeanslongerobservationtime T.Whilethebestapproachwouldbetomeasuretheinput\nsignalforalltimes,inpracticeameasuredsignal y(t)isoftenextendedintime(‚Äúpadded‚Äù)\nby adding zeros for times beyond the last measured signal; this increases the value of T\nartificiallyandmayleadtospuriousconclusions.Althoughonemaynotthinkofpadding\nasaddingnewinformationtotheanalysis,itdoesbuildintheassumptionthatthesignal\nhasnoexistenceattimesafterthelastmeasurement.\nWhileperiodicityisexpectedforaFourier series,itissomewhatsurprisingforaFourier\nintegral,whichhasbeentoutedastherighttoolfornonperiodicfunctions.Clearly,ifwe\ninput values of the signal for longer lengths of time, then the inherent period becomes\nlonger, and if the repeat period Tis very long, it may be of little consequence for times\n3 Thealertreadermaybewonderingwhathashappenedtothe h‚àï2withwhichthetrapezoidruleweights\ntheinitialandfinalpoints.Actually,theyarethere,butbecausewehaveset y0‚â°yN,twoh‚àï2termshave\nbeenaddedtoproduceone hterm.\n4 SeealsoSection9.3.1wherewediscusstherelatedphenomenonofaliasing.",4610
88-9.3.1 Aliasing.pdf,88-9.3.1 Aliasing,"174 9 Fourier Analyses\nshortcomparedtotheperiod.If y(t)isactuallyperiodicwithperiod Nh,thentheDFTisan\nexcellentwayofobtainingtheFourierseries.Iftheinputfunctionisnotperiodic,thenthe\nDFTcanbeabadapproximationneartheendpointsofthetimeinterval,asthefunction\nwillrepeatthere;likewiseforthelowestfrequencies.\nTheDFTanditsinversecanbewritteninaconciseandinsightfulway,andbeevaluated\nefficiently,byintroducingacomplexvariable Zfortheexponentialandthenraising Zto\nvariouspowers:\nyk=‚àö\n2ùúã\nNN‚àë\nn=1Z‚àínkYn,Z=e‚àí2ùúãi‚àïN, (9.35)\nYn=1‚àö\n2ùúãN‚àë\nk=1Znkyk,Znk‚â°[Zn]k. (9.36)\nWiththisformulationthecomputerneedstocomputeonlypowersof Z.WegiveourDFT\ncodeinListing9.1.Ifyourpreferenceistoavoidcomplexnumbers,wecanrewrite(9.35)\nintermsofseparaterealandimaginarypartsbyapplyingEuler‚Äôstheoremwith ùúÉdef=2ùúã‚àïN:\nZ=e‚àíiùúÉ,‚áíZ¬±nk=e‚àìinkùúÉ=cosnkùúÉ‚àìisinnkùúÉ, (9.37)\n‚áíYn=1‚àö\n2ùúãN‚àë\nk=1[cos(nkùúÉ)Reyk+sin(nkùúÉ)Imyk\n+i(cos(nkùúÉ)Imyk‚àísin(nkùúÉ)Reyk)], (9.38)\nyk=‚àö\n2ùúã\nNN‚àë\nn=1[cos(nkùúÉ)ReYn‚àísin(nkùúÉ)ImYn\n+i(cos(nkùúÉ)ImYn+sin(nkùúÉ)ReYn)]. (9.39)\nReaders new to DFTs are often surprised when they apply these equations to practical\nsituationsandendupwithtransforms Yhavingimaginaryparts,despitethefactthatthe\nsignalyisreal.Equation(9.38)shouldmakeitclearthatarealsignal(Im yk‚â°0)willyield\nanimaginarytransformunless‚àëN\nk=1sin(nkùúÉ)Reyk=0.Thisoccursonlyif y(t)isaneven\nfunctionover ‚àí‚àû‚â§t‚â§+‚àûandweintegrateexactly.Becauseneitherconditionholds,the\nDFTsofreal,evenfunctionsmayhavesmallimaginaryparts.Thisisnotasaresultofan\nerrorinprogramming,andinfactyieldsameasureoftheapproximationerrorintheentire\nprocedure.\nThecomputationtimeforaDFTcanbereducedevenfurtherbyuseofthe FFTalgorithm,\nasdiscussedinSection9.5.Anexaminationof(9.35)showsthattheDFTisevaluatedasa\nmatrixmultiplicationofavectoroflength Ncontainingthe Zvalues,byavectoroflength\nNofyvalue. The time for this DFT scales like N2, while the time for the FFT algorithm\nscalesasNlog2N.Althoughthismaynotseemlikemuchofadifference,for N=102‚àí3,the\ndifferenceof103‚àí5isthedifferencebetweenaminuteandaweek.Forthisreason,itisthe\nFFTthatisoftenusedforon-linespectrumanalysis.\n9.3.1 Aliasing\nThe sampling of a signal by DFT for only a finite number of times and large Œît, limits\ntheaccuracyofthededucedhigh-frequencycomponentspresentinthesignal.Obviously,\n9.3 Discrete Fourier Transforms 175\nsin(2 œÄt) sin( œÄt/2)\n‚Äì101\n246\n7\nFigure 9.2 A plot of the functions sin( ùúãt/2) and sin(2 ùúãt). If the sampling rate is not high enough,\nthese signals may appear indistinguishable in a Fourier decomposition. If the sample rate is too\nlow, and if both signals are present in a sample, the deduced low-frequency components may be\ncontaminated by the higher-frequency ones.\ngoodinformationaboutveryhighfrequenciesrequiressamplingthesignalwithsmalltime\nstepssothatallthewigglescanbeincluded.Whileapoordeductionofthehigh-frequency\ncomponentsmaybetolerable,ifallwecareaboutarethelow-frequencyones,theinaccurate\nhigh-frequencycomponentsmaycontaminatethededucedlow-frequencyones.Thiseffect\niscalledaliasingandisthecauseoftheMoir√©patterndistortionindigitalimages.\nAs an example, consider Figure 9.2 showing the two functions sin (ùúãt‚àï2)and sin(2ùúãt)\nfor0‚â§t‚â§8,withtheirpointsofoverlapinbold.Ifwewereunfortunateenoughtosam-\npleasignalcontainingthesefunctionsatthetimes t=0,2,4,6,8,thenwewouldmeasure\ny‚â°0andassumethattherewasnosignalatall!However,ifwewereunfortunateenough\nto measure the signal at the filled dots in Figure 9.2, where sin (ùúãt‚àï2)=sin(2ùúãt), specifi-\ncally,t=0,12\n10,4\n3,‚Ä¶,thenourFourieranalysiswouldcompletelymissthehigh-frequency\ncomponents. In DFT jargon, we would say that the high-frequency component has been\naliasedbythelow-frequencycomponent.Inothercases,somehigh-frequencyvaluesmay\nbeincludedinoursamplingofthesignal,butoursamplingratemaynotbehighenoughto\nincludeenoughofthemtoseparatethehigh-frequencycomponentproperly.Inthiscase,\nsome high-frequency signals would be included spuriously as part of the low-frequency\nspectrum, and this would lead to spurious low-frequency oscillations when the signal is\nsynthesizedfromitsFouriercomponents.\nMoreprecisely,aliasingoccurswhenasignalcontainingfrequency fissampledatarateof\ns=N‚àïTmeasurementsperunittime,with s‚â§f‚àï2.Inthiscase,thefrequencies fandf‚àí2s\nyieldthesameDFT,andwewouldnotbeabletodeterminethattherearetwofrequencies\npresent.Thatbeingthecase,toavoidaliasingwewantnofrequencies f>s‚àï2tobepresent\ninourinputsignal.Thisisknownasthe Nyquistcriterion .Inpractice,someapplications\navoidtheeffectsofaliasingbyfilteringoutthehighfrequenciesfromthesignal,andthen\nanalyzingonlytheremaininglow-frequencypart.(Thelow-frequency sincfilterdiscussed\nin Section 9.4.4 is often used for this purpose.) Although filtering eliminates some high-\nfrequencyinformation,itlessensthedistortionofthelow-frequencycomponents,andso\nmayleadtoimprovedreproductionofthesignal.",4849
89-9.3.2 Assessments.pdf,89-9.3.2 Assessments,"176 9 Fourier Analyses\nIfaccuratevaluesforthehighfrequenciesarerequired,thenyouwillneedtoincrease\nthesamplingrate sbyincreasingthenumber Nofsamplestakenwithinthefixedsampling\ntimeT=Nh.Bykeepingthesamplingtimeconstantandincreasingthenumberofsamples\ntaken,wemakethetimestep hsmallerandpickupthehigherfrequencies.Byincreasing\nthenumber Noffrequenciesthatyoucompute,youmovetheprevioushigher-frequency\ncomponentsclosertothemiddleofthespectrum,andthusawayfromtheerror-proneends.\nIfweincreasethetotaltimesamplingtime T=Nhandkeephthesame,thenthesam-\nplingrates=N‚àïT=1‚àïhremainsthesame.Since ùúî1=2ùúã‚àïT,thismakes ùúî1smaller,which\nmeanswehavemorelowfrequenciesrecordedandasmootherfrequencyspectrum.And\naswesaid,thisisoftencarriedout,afterthefact,bypaddingtheendofthedatasetwith\nzeros.\nExercise\n1) The sampling of a signal by DFT for only a finite number of times not only limits\nthe accuracy of the deduced high-frequency components, but also contaminates the\ndeduced low-frequency components ( aliasing). Consider the two functions sin (ùúãt‚àï2)\nandsin(2ùúãt)for0‚â§t‚â§8.\n(a) Makegraphsofbothfunctionsonthesameplot.\n(b) PerformaDFTonbothfunctions.\n(c) Sampleattimes t=0,2,4,6,8,‚Ä¶anddrawconclusions.\n(d) Sampleattimes t=0,12‚àï10,4‚àï3,‚Ä¶anddrawconclusionsaboutthehigh-frequency\ncomponents( Hint:Theymaybe aliasedbythelow-frequencycomponents).\n(e) TheNyquistcriterion statesthatwhenasignalcontainingfrequency fissampledat\narateofs=N‚àïT,measurementsperunittime,with s‚â§f‚àï2,thenaliasingoccurs.\nVerifyspecificallythatthefrequencies fandf‚àí2syieldthesameDFT.\n2) PerformaFourieranalysisofthechirpsignal y(t)=sin(60t2).AsseeninFigure10.5,this\nsignalisnottrulyperiodic,andisbetteranalyzedwithmethodssoontobediscussed.\n9.3.2 Assessments\nSimple analytic input :Itisalwaysgoodtodosimplechecksbeforeexaminingmorecom-\nplexproblems,evenifyouareusingapackage‚ÄôsFouriertool.\n1) Sampletheevensignal\ny(t)=3cos(ùúît)+2cos(3ùúît)+cos(5ùúît). (9.40)\n(a) Decomposethisintoitscomponents.\n(b) Checkthatthecomponentsareessentiallyrealandintheratio3:2:1(or9:4:1\nforthepowerspectrum).\n(c) Verifythatthefrequencieshavetheexpectedvalues(notjustratios).\n(d) Verifythecomponentssumuptogivetheinputsignal.\n(e) Experimenton theseparateeffects ofpickingdifferent valuesofthestep size h\nandofenlargingthemeasurementperiod T=Nh.\n2) Sampletheoddsignal\ny(t)=sin(ùúît)+2sin(3ùúît)+3sin(5ùúît). (9.41)\n9.3 Discrete Fourier Transforms 177\nDecompose this into its components, and then check that they are essentially\nimaginaryand in the ratio 1:2:3 (or 1:4:9 if a power spectrum is plotted).Check\nthattheysumuptogivetheinputsignal.\n3) Samplethemixed-symmetrysignal\ny(t)=5sin(ùúît)+2cos(3ùúît)+sin(5ùúît). (9.42)\nDecomposethisintoitscomponents,andthencheckthattheyareintheratio5:2:1\n(or25:4:1ifapowerspectrumisplotted).Checkthattheysumuptogivetheinput\nsignal.\n4) Samplethesignal\ny(t)=5+10sin(t+2).\nCompareandexplaintheresultsobtainedbysampling(a)withoutthe5,(b)asgiven\nbutwithoutthe2,and(c)withoutthe5andthe2.\n5) Inourdiscussionofaliasing,weexaminedFigure9.2showingthefunctionssin (ùúãt‚àï2)\nandsin(2ùúãt).Samplethefunction\ny(t)=sin(ùúã\n2t)+sin(2ùúãt) (9.43)\nandexplorehowaliasingoccurs.Explicitly,weknowthatthetruetransformcontains\npeaksatùúî=ùúã‚àï2andùúî=2ùúã.Samplethesignalataratethatleadstoaliasing,aswell\nasatahighersamplingrateatwhichthereisnoaliasing.ComparetheresultingDFTs\nineachcaseandcheckifyourconclusionsagreewiththeNyquistcriterion.\nHighly nonlinear oscillator : Recall the numerical solution for oscillations of a spring\nwithpower p=12[see(9.1)].DecomposethesolutionintoaFourierseriesanddeter-\nminethenumberofhigherharmonicsthatcontributeatleast10 %;forexample,deter-\nminethenforwhich |bn‚àïb1|<0.1.Checkthatresumingthecomponentsreproducesthe\nsignal.\nNonlinearly perturbed oscillator :Remembertheharmonicoscillatorwithanonlinear\nperturbation(8.2):\nV(x)=1\n2kx2(\n1‚àí2\n3ùõºx)\n,F(x)=‚àíkx(1‚àíùõºx). (9.44)\nForverysmall amplitudesofoscillation( x‚â™1‚àïùõº),thesolution x(t)essentiallyshould\nbeonlythefirsttermofaFourierseries.( Warning:Theùúîyouuseinyourseriesmust\ncorrespondtothe truefrequencyofthesystem,notthe ùúî0ofsmalloscillations.\n1) Wewantthesignaltocontain‚Äúapproximately10%nonlinearity.‚ÄùThisbeingthecase,\nfix your value of ùõºso thatùõºxmax‚âÉ10%,w h e r exmaxis the maximum amplitude of\noscillation.Fortherestoftheproblem,keepthevalueof ùõºfixed.\n2) DecomposeyournumericalsolutionintoadiscreteFourierspectrum.\n3)Plotagr aphoftheper centageofimportanceofthefirst two,non-DCFouriercom-\nponentsasafunctionoftheinitialdisplacementfor0 <x0<1‚àï2ùõº.Youshouldfind\nthathigherharmonicsaremoreimportantastheamplitudeincreases.Becauseboth\nevenandoddcomponentsarepresent, Ynshouldbecomplex.Becausea10%effectin\namplitudebecomesa1%effectinpower,makesurethatyoumakeasemilogplotof\nthepowerspectrum.\n4) Asalways,checkthatresummationsofyourtransformsreproducethesignal.",4817
90-9.3.3 Transforming Nonperiodic Functions.pdf,90-9.3.3 Transforming Nonperiodic Functions,,0
91-9.4 Noise Filtering.pdf,91-9.4 Noise Filtering,,0
92-9.4.1 Noise Reduction via Autocorrelation.pdf,92-9.4.1 Noise Reduction via Autocorrelation,"178 9 Fourier Analyses\n9.3.3 Transforming Nonperiodic Functions\nConsideranelectroninitiallylocalizedaround x=5.Amodeltodescribethis‚Äúlocalized‚Äù\nelectronisaGaussianmultiplyingaplanewave:\nùúì(x,t=0)=exp[\n‚àí1\n2(\nx‚àí5\nùúé0)2]\neik0x, (9.45)\nwhere we use natural units in which ‚Ñè=1. This wave packet is not an eigenstate of the\nmomentumoperator p=id‚àïdx,butrathercontainsaspreadofmomenta.Your problem\nistoevaluatetheFouriertransform,\nùúì(p)=‚à´+‚àû\n‚àí‚àûdxeipx\n‚àö\n2ùúãùúì(x,0), (9.46)\nasawayofdeterminingthemomentacomponentsin(9.45).\n9.4 Noise Filtering\nIn the process of solving this problem, we examine two simple approaches: the use of auto-\ncorrelationfunctionsandtheuseoffilters.Bothapproachesfindwideapplicationsinscience,\nwithourdiscussionnotdoingthesubjectsjustice.Wewillseefiltersagaininthediscussionof\nwaveletsinChapter 10.\nYoumeasureasignal y(t)thatobviouslycontainsnoise.Your problemistodetermine\nthefrequenciesthatwouldbepresentinthespectrumofthesignaliftherewerenonoise.\nOfcourse,onceyouhaveaFouriertransformfromwhichthenoisehasbeenremoved,you\ncantransformittoobtainanoise-freesignal s(t).\n9.4.1 Noise Reduction via Autocorrelation\nWe assume that the measured signal is the sum of the true signal s(t), which we wish to\ndetermine,plussomeunwelcome noisen(t):\ny(t)=s(t)+n(t), (9.47)\nOne approach at removing the noise relies on the fact that noise is usually random, and\nthusshouldnotbecorrelatedwiththesignal.Yetwhatdowemeanwhenwesaythattwo\nfunctionsarenot correlated? Well,ifthetwotendtooscillatewiththeirnodesandpeaksin\nmuchthesameplaces,thenthetwofunctionsareclearlycorrelated.Ananalyticmeasure\nofthecorrelationoftwoarbitraryfunctions y(t)andx(t)isthecorrelationfunction\nc(ùúè)=‚à´+‚àû\n‚àí‚àûdty(t)x(t+ùúè)‚â°‚à´+‚àû\n‚àí‚àûdty(t‚àíùúè)x(t), (9.48)\nHereùúè,thelagtime,isavariable,andweassumethattheaveragevaluesofthefunctions\nhavebeensubtractedoff,sothattheyoscillatearoundzero.Evenifthetwosignalshavedif-\nferentmagnitudes,iftheyhavesimilartimedependencies,exceptforonelaggingorleading\ntheother,thenforcertainvaluesof ùúè,theintegrandin(9.48)willbepositiveforallvalues\n9.4 Noise Filtering 179\noft.Forthosevaluesof ùúè,thetwosignalsinterfereconstructivelyandproducealargevalue\nforthecorrelationfunction.Incontrast,ifbothfunctionsoscillateindependently,regardless\nofthevalueof ùúè,thenitisjustaslikelyfortheintegrandtobepositiveastobenegative,in\nwhichcasethetwosignalsinterferedestructivelyandproduceasmallvaluefortheintegral.\nBeforeweapplythecorrelationfunctiontoourproblem,letusstudysomeofitsproper-\nties.Weuse(9.16)toexpress c,y,andxintermsoftheirFouriertransforms:\nc(ùúè)=‚à´+‚àû\n‚àí‚àûdùúî‚Ä≤‚Ä≤C(ùúî‚Ä≤‚Ä≤)eiùúî‚Ä≤‚Ä≤t\n‚àö\n2ùúã,y(t)=‚à´+‚àû\n‚àí‚àûdùúîY(ùúî)e‚àíiùúît\n‚àö\n2ùúã,\nx(t+ùúè)=‚à´+‚àû\n‚àí‚àûdùúî‚Ä≤X(ùúî‚Ä≤)e+iùúît\n‚àö\n2ùúã. (9.49)\nSeeingthat ùúî,ùúî‚Ä≤,andùúî‚Ä≤‚Ä≤aredummyvariables,othernamesmaybeusedforthemwithout\nchangingtheresults.Whenwesubstitutetheserepresentationsintothedefinition(9.48)of\nthecorrelationfunction,andassumethattheresultingintegralsconvergewellenoughto\nberearranged,weobtain\n‚à´+‚àû\n‚àí‚àûdùúîC(ùúî)eiùúît=‚à´+‚àû\n‚àí‚àûdùúî\n2ùúã‚à´+‚àû\n‚àí‚àûdùúî‚Ä≤Y(ùúî)X(ùúî‚Ä≤)eiùúîùúè2ùúãùõø(ùúî‚Ä≤‚àíùúî)\n=‚à´+‚àû\n‚àí‚àûdùúîY(ùúî)X(ùúî)eiùúîùúè,\n‚áíC(ùúî)=‚àö\n2ùúãY(ùúî)X(ùúî), (9.50)\nwhere the last line follows because ùúî‚Ä≤‚Ä≤andùúîare equivalent dummy variables. Equation\n(9.50)saysthattheFouriertransformofthecorrelationfunctionoftwosignalsispropor-\ntionaltotheproductoftheirtransforms.(Weshallseearelatedconvolutiontheoremfor\nfilters.)\nA special case of the correlation function c(ùúè)is theautocorrelation function A (ùúè)that\nmeasuresthecorrelationofatimesignalwithitself:\nA(ùúè)def=‚à´+‚àû\n‚àí‚àûdty(t)y(t+ùúè)‚â°‚à´+‚àû\n‚àí‚àûdty(t)y(t‚àíùúè). (9.51)\nThisfunctioniscomputedbytakingasignal y(t)thathasbeenmeasuredoversometime\nperiod,andthenaveragingitovertimeusing, y(t+ùúè)asaweightingfunction.Thisprocess\niscalledfolding,orconvoluting ,afunctionontoitself(asmightbedonewithdough).Tosee\nhow this folding removes noise from a signal, we go back to the measured signal (9.47),\nwhichwasthesumofpuresignalplusnoise s(t)+n(t).Asanexample,ontheupperleft\ninFigure9.3,weshowasignalthatwasconstructedbyaddingrandomnoisetoasmooth\nsignal.Whenwecomputetheautocorrelationfunctionforthissignal,weobtainafunction\n(upperrightinFigure9.3)thatlookslikeabroadened,smoothedversionofthesignal y(t).\nWecanunderstandhowthenoiseisremovedbytakingtheFouriertransformof s(t)+n(t)\ntoobtainasimplesumoftransforms:\nY(ùúî)=S(ùúî)+N(ùúî), (9.52)\n{S(ùúî)\nN(ùúî)}\n=‚à´+‚àû\n‚àí‚àûdt{s(t)\nn(t)}\ne‚àíiùúît\n‚àö\n2ùúã. (9.53)\n180 9 Fourier Analyses\n0246810\n0 2 4 6 8 10 12Initial function y(t) + noise\nt (s)y\n0.40.60.81.01.21.4√ó102\n√ó1030 2 4 6 8 10 12Autocorrelation function A(T)\nT (s)A\n0.00.51.01.52.02.53.03.5\nP\n0 510 15 20 25 30 35 40 45Power spectrum (with noise)\nFrequency10\ny\n02468\n0 2 4 6 8 10 12Function y(t) + noise after low-pass filter\nt (s)\nFigure 9.3 From bottom left to right : A function that is a signal plus noise s(t)+n(t);t h e\nautocorrelation function versus time deduced by processing this signal; the power spectrum\nobtained from autocorrelation function; the signal plus noise after passage through a lowpass Ô¨Ålter.\nBecausetheautocorrelationfunction(9.51)for y(t)=s(t)+n(t)involvesthesecondpower\nofy,isnotalinearfunction,thatis, Ay‚â†As+An,butinstead,\nAy(ùúè)=‚à´+‚àû\n‚àí‚àûdt[s(t)s(t+ùúè)+s(t)n(t+ùúè)+n(t)n(t+ùúè)]. (9.54)\nIf we assume that the noise n(t)in the measured signal is truly random, then it should\naverage to zero over long times and be uncorrelated at times tandt+ùúè. This being the\ncase,bothintegralsinvolvingthenoisevanish,andso\nAy(ùúè)‚âÉ‚à´+‚àû\n‚àí‚àûdts(t)s(t+ùúè)=As(ùúè). (9.55)\nThus,thepartofthenoisethatisrandomtendstobeaveragedoutoftheautocorrelation\nfunction,andweareleftwithanapproximationoftheautocorrelationfunctionofthepure\nsignal.\nSohowdoesthishelpus?Applicationof(9.50)with Y(ùúî)=X(ùúî)=S(ùúî)tellsusthatthe\nFouriertransform A(ùúî)oftheautocorrelationfunctionisproportionalto |S(ùúî)|2:\nA(ùúî)=‚àö\n2ùúã|S(ùúî)|2. (9.56)\nThefunction |S(ùúî)|2isthepowerspectrum ofthepuresignal.Thus,evaluationoftheauto-\ncorrelationfunctionofthenoisysignalgivesusthepuresignal‚Äôspowerspectrum,whichis\noftenallthatweneedtoknow.Forexample,inFigure9.3weseeanoisysignal(lowerleft,\ntheautocorrelationfunction(lowerright,whichclearlyissmootherthan thesignal,and\nfinally,thededucedpowerspectrum(upperleft).Noticethatthebroadbandhigh-frequency\ncomponentscharacteristicofnoiseareabsentfromthepowerspectrum.\nYoucaneasilymodifythesampleprogram DFTcomplex.py inListing9.1or DFTreal.py in\nListing 9.2 sample program DFTcomplex.py in Listing 9.1 to compute the autocorrelation\nfunctionandthenthepowerspectrum A(ùúè).Theprogram NoiseSincFilter.py doesjustthat.",6477
93-9.4.2 Autocorrelation Function Exercises.pdf,93-9.4.2 Autocorrelation Function Exercises,,0
94-9.4.3 Filtering with Transforms.pdf,94-9.4.3 Filtering with Transforms,"9.4 Noise Filtering 181\n9.4.2 Autocorrelation Function Exercises\n1) Imagine thatyouhavesampledthepuresignal\ns(t)=1\n1‚àí0.9sint. (9.57)\nAlthough there is just a single sine function in the denominator, there is an infinite\nnumberofovertonesasyoucanseefromtheexpansion\ns(t)‚âÉ1+0.9sint+(0.9sint)2+(0.9sint)3+¬∑¬∑¬∑. (9.58)\n(a) ComputetheDFT S(ùúî).Makesurenottosamplejustoneperiod,butalsotocover\ntheentireperiod.Alsomakesuretosampleatenoughtimes(finescale)toobtain\ngoodsensitivitytothehigh-frequencycomponents.\n(b) Makeasemilogplotofthepowerspectrum |S(ùúî)|2.\n(c) Takeyourinputsignal s(t)andcomputeitsautocorrelationfunction A(ùúè)forafull\nrangeofùúèvalues(ananalyticsolutionisokaytoo).\n(d) ComputethepowerspectrumindirectlybyperformingaDFTontheautocorrela-\ntionfunction.Compareyourresultstothespectrumobtainedbycomputing |S(ùúî)|2\ndirectly.\n2) Addsomerandomnoisetothesignalusingarandomnumbergenerator:\ny(ti)=s(ti)+ùõº(2ri‚àí1),0‚â§ri‚â§1, (9.59)\nwhereùõºisanadjustableparameterand riarerandomnumbers.Tryseveralvaluesof ùõº,\nfromsmallonesthatjustaddsomefuzztothesignaltolargeonesthatnearlyhidethe\nsignal.\n(a) Plot your noisy data, their Fourier transform, and their power spectrum obtained\ndirectlyfromthetransformwithnoise.\n(b) Computetheautocorrelationfunction A(ùúè)anditsFouriertransform A(ùúî).\n(c) ComparetheDFTof A(ùúè)tothetruepowerspectrum.Commentontheeffectiveness\nofreducingnoisebyuseoftheautocorrelationfunction.\n(d) Forwhatvalueof ùõºdoyouessentiallylosealltheinformationintheinput?\n9.4.3 Filtering with Transforms\nA filter(Figure9.4)isadevicethatconvertsaninputsignal f(t)toanoutputsignal g(t),with\nsomespecificpropertyfor g(t).Morespecifically,an analogfilter isdefinedasintegration\noveraninputfunction[Hartmann,1998]:\ng(t)=‚à´+‚àû\n‚àí‚àûdùúèf(ùúè)h(t‚àíùúè)def=f(t)‚àóh(t). (9.60)\nHeretheasterisk ‚àóindicatesa convolution ,whichwehavealreadyseeninthediscussion\noftheautocorrelationfunction.Thefunction h(t)istheunitresponse ortransferfunction of\nFigure 9.4 An input signal f(t)passes through a Ô¨Ålter hthat\noutputs the function g(t).\nf(t) g(t) h\n182 9 Fourier Analyses\nthefilter;itistheresponseofthefiltertoaunitimpulse:\nh(t)=‚à´+‚àû\n‚àí‚àûdùúèùõø(ùúè)h(t‚àíùúè). (9.61)\nEquation(9.60)statesthattheoutput g(t)ofafilterequalstheinput f(t)convolutedwith\nthetransferfunction h(t‚àíùúè).Becausetheargumentoftheresponsefunctionisdelayedby\natimeùúèrelativetothatofthesignalintheintegral(9.60), ùúèiscalledthe lagtime.While\ntheintegrationisoveralltimes,theresponseofagooddetectorusuallypeaksaroundzero\ntime.Inanycase,theresponsemustequalzerofor ùúè>tbecauseeventsinthefuturecannot\naffectthepresent(causality).\nTheconvolutiontheorem statesthattheFouriertransformoftheconvolution g(t)ispro-\nportionaltotheproductofthetransformsof f(t)andh(t):\nG(ùúî)=‚àö\n2ùúãF(ùúî)H(ùúî). (9.62)\nThetheoremresultsfromexpressingthefunctionsin(9.60)bytheirtransforms,andusing\nthe resulting Dirac delta function to evaluate an integral (essentially what we did in our\ndiscussionofthecorrelationfunction).\nFiltering,aswehavedefinedit,isalinearprocessinvolvingjustthefirstpowersofthe\nsignalf.Thismeansthattheoutputatonefrequencyisproportionaltotheinputatthat\nfrequency. The constant of proportionality between the two may change with frequency,\nandthussuppressspecificfrequenciesrelativetoothers,butthatconstantremainsfixedin\ntime.Sincethelawoflinearsuperpositionisvalidforlinearfilters,iftheinputtoafilter\nis the sum of various functions, then the transform of the output will be the sum of the\nfunctions‚ÄôFouriertransforms.\nFilters that remove or decrease high-frequency components more than they do low-\nfrequency ones, are called lowpassfilters. Those that filter out the low frequencies are\ncalledhighpassfilters .Asimplelowpassfilteristhe RCcircuitontheleftofFigure9.5that\nproducesthetransferfunction\nH(ùúî)=1\n1+iùúîùúè=1‚àíiùúîùúè\n1+ùúî2ùúè2, (9.63)\nwhereùúè=RCisthetimeconstant.The ùúî2inthedenominatorleadstoadecreaseinthe\nresponseathighfrequenciesandthereforemakesthisalowpassfilter(the iùúîaffectsonly\nthephase).Asimplehighpassfilteristhe RCcircuitontherightinFigure9.5thatproduces\nthetransferfunction\nH(ùúî)=iùúîùúè\n1+iùúîùúè=iùúîùúè+ùúî2ùúè2\n1+ùúî2ùúè2. (9.64)\nWeseethat H=1atlarge ùúî,yetvanishesas ùúî‚Üí0;asexpectedforahighpassfilter.\nIn In RR\nCC\nOutOut\nFigure 9.5 Left:A nCRcircuit arranged as a lowpass Ô¨Ålter. Right:A nCRcircuit arranged as a\nhighpass Ô¨Ålter.",4296
95-9.4.4 Digital Filters Windowed Sinc Filters.pdf,95-9.4.4 Digital Filters Windowed Sinc Filters,"9.4 Noise Filtering 183\nFigure 9.6 A delay-line Ô¨Ålter in\nwhich the signal at different times is\nscaled by different amounts ci.Œ£œÑ œÑ œÑIn\nOutC3\nC2\nC1\nC0\nFilterscomposedofresistorsandcapacitorsarefineforanalogsignalprocessing,butfor\ndigitalprocessingwewanta digitalfilter thathasaspecificresponsefunctionforeachfre-\nquencyrange.Aphysicalmodelforadigitalfiltermaybeconstructedfromadelaylinewith\ntapsatvariousspacingalongtheline(Figure9.6)[Hartmann,1998].Thesignalreadfrom\ntapnisjusttheinputsignaldelayedbytime nùúè,wherethedelaytime ùúèisacharacteristicof\ntheparticularfilter.Theoutputfromeachtapisdescribedbythetransferfunction ùõø(t‚àínùúè),\npossiblywithscalingfactor cn.AsrepresentedbythetriangleontherightinFigure9.6,the\nsignalsfromalltapsareultimatelysummedtogethertoformthetotalresponsefunction:\nh(t)=N‚àë\nn=0cnùõø(t‚àínùúè). (9.65)\nInthefrequencydomain,theFouriertransformofadeltafunctionisanexponential,and\nso(9.65)resultsinthetransferfunction\nH(ùúî)=N‚àë\nn=0cne‚àíinùúîùúè, (9.66)\nwheretheexponentialindicatesthephaseshiftfromeachtap.\nIf a digital filter is given a continuous time signal f(t)as input, its output will be the\ndiscretesum\ng(t)=‚à´+‚àû\n‚àí‚àûdt‚Ä≤f(t‚Ä≤)N‚àë\nn=0cnùõø(t‚àít‚Ä≤‚àínùúè)=N‚àë\nn=0cnf(t‚àínùúè). (9.67)\nAndofcourse,ifthesignal‚Äôsinputisadiscretesum,itsoutputwillremainadiscretesum.\nIneithercase,weseethatknowledgeofthefiltercoefficients ciprovidesuswithallweneed\ntoknowaboutadigitalfilter.IfwelookbackatourworkontheDFTinSection9.3,wecan\nviewadigitalfilter(9.67)asaFouriertransforminwhichweusean N-pointapproximation\ntotheFourierintegral.The cn‚Äôsthencontainboththeintegrationweightsandthevaluesof\ntheresponsefunctionattheintegrationpoints.Accordingly,thetransformcanbeviewed\nasafilterofthesignalintospecificfrequencies.\n9.4.4 Digital Filters: Windowed Sinc Filters ‚äô\nApopularwaytoseparatethebandsoffrequenciesinasignaliswitha windowedsincfilter\n[Smith,1999].Thisfilterisbasedontheobservationthatanideal lowpassfilterpassesallfre-\nquenciesbelowacutofffrequency ùúîc,andblocksallfrequenciesabovethisfrequency.And\nbecausetheretendstobemorenoiseathighfrequenciesthanatlowfrequencies,remov-\ningthehighfrequenciestendstoremovemorenoisethansignal,althoughsomesignalis\ninevitablylost.OneuseforwindowedsincfiltersisinreducingaliasinginDFTsbyremov-\ningthehigh-frequencycomponentofasignalbeforedeterminingitsFouriercomponents.\nThegraphonthelowerrightinFigure9.1wasobtainedbypassingournoisysignalthrough\nasincfilter(usingtheprogram NoiseSincFilter.py ).\n184 9 Fourier Analyses\n001/21\n1/2œâ\n‚Äì1/2Figure 9.7 The rectangle function rect (ùúî)that is\nconstant for a Ô¨Ånite frequency interval. The Fourier\ntransform of this function is sinc( t).\nIfbothpositiveandnegativefrequenciesareincluded,anideallow-frequencyfilterwill\nlookliketherectangularpulseinfrequencyspace:\nH(ùúî,ùúîc)=rect(\nùúî\n2ùúîc)\n,rect(ùúî)={\n1,if|ùúî|‚â§1\n2,\n0,otherwise .(9.68)\nHererect (ùúî)istherectangularfunction(Figure9.7).Althoughmaybenotobvious,arect-\nangularpulseinthefrequencydomainhasaFouriertransformthatisproportionaltothe\nsincfunction inthetimedomain[Smith,1991]:\n‚à´+‚àû\n‚àí‚àûdùúîe‚àíiùúîtrect(ùúî)=sinc(\nt\n2)def=nsin(ùúãt‚àï2)\nùúãt‚àï2, (9.69)\nwherethe ùúã‚Äôsaresometimesomitted.Consequently,wecanfilteroutthehigh-frequency\ncomponentsofasignalbyconvolutingitwithsin (ùúîct)‚àï(ùúîct),atechniquealsoknownasthe\nNyquist‚ÄìShannon interpolationformula.Intermsofdiscretetransforms,thetime-domain\nrepresentationofthesincfilterissimply\nh[i]=sin(ùúîci)\niùúã. (9.70)\nBecauseallfrequenciesbelowthecutofffrequency ùúîcarepassedwithunitamplitude,while\nallhigherfrequenciesareblocked,wecanseetheimportanceofasincfilter.\nInpractice,thereareanumberofproblemsinusingthesincfunctionasthefilter.First,\nasformulated,thefilteris noncausal;thatis,therearecoefficientsatnegativetimes,which\nviolatescausalitybecausewedonotstartmeasuringthesignaluntil t=0.Second,inorder\ntoproduceaperfectrectangularresponse,wewouldhavetosamplethesignalataninfinite\nnumberoftimes.Inpractice,wesampleat (M+1)points(Meven)placedsymmetrically\naroundthemainlobeofsin (ùúãt)‚àïùúãt,andthenshifttimestopurelypositivevalues:\nh[i]=sin[2ùúãùúîc(i‚àíM‚àï2)]\ni‚àíM‚àï2,0‚â§t‚â§M. (9.71)\nAsmightbeexpected,apenaltyisincurredformakingthefilterdiscrete;insteadoftheideal\nrectangularresponse,weobtainsome Gibbsovershoot ,withroundedcornersandoscilla-\ntionsbeyondthecorner.\nTherearetwowaystoreducethedeparturesfromtheidealfilter.Thefirstistoincreasethe\nlengthoftimesoverwhichthesignalissampled,whichinevitablyleadstolongercompute\ntimes.Theotherwayistosmoothoutthetruncationofthesincfunctionbymultiplyingit\nwithasmoothlytaperedcurve,likethe Hammingwindowfunction :\nùë§[i]=0.54‚àí0.46 cos(\n2ùúãi\nM)\n. (9.72)\nInthiswaythefilter‚Äôskernelbecomes\nh[i]=sin[2ùúãùúîc(i‚àíM‚àï2)]\ni‚àíM‚àï2[\n0.54‚àí0.46 cos(\n2ùúãi\nM)]\n. (9.73)",4695
96-9.5 Fast Fourier Transform.pdf,96-9.5 Fast Fourier Transform,"9.5 Fast Fourier Transform ‚äô185\nThecutofffrequency ùúîcshouldbeafractionofthesamplingrate.Thetimelength Mdeter-\nminesthebandwidth overwhichthefilterchangesfrom1to0.\nExercise Repeattheexercisethataddedrandomnoisetoaknownsignal,thistimeusing\nthesincfiltertoreducethenoise.Seehowsmallyoucanmaketherelativestrengthofthe\nsignal,andstillbeabletoseparateitfromthenoise.\n9.5 Fast Fourier Transform ‚äô\nWehaveseenin(9.35)thataDFTcanbewritteninthecompactform\nYn=1‚àö\n2ùúãN‚àë\nk=1Znkyk,Z=e‚àí2ùúãi‚àïN,n=0,1,‚Ä¶,N‚àí1. (9.74)\nEvenifthesignalelements yktobetransformedarereal, Ziscomplex,andthereforewe\nmustprocessbothrealandimaginarypartswhencomputingtransforms.Becauseboth n\nandkrange over Nintegervalues, the (Zn)kykmultiplicationsin (9.74)require some N2\nmultiplicationsandadditionsofcomplexnumbers.As Ngetslarge,ashappensinrealistic\napplications,thisgeometricincreaseinthenumberofstepsslowsdownthecomputation.\nIn1965,CooleyandTurkeydiscoveredanalgorithm5thatreducesthenumberofoper-\nationsnecessarytoperformaDFTfrom N2toroughly Nlog2N[CooleyandTukey,1965;\nDonnellyandRust,2005].Althoughthismaynotseemlikesuchabigdifference,itrepre-\nsentsa100-foldspeedupfor1000datapoints,whichchangesafulldayofprocessinginto\n15min of work. Due to its widespread use (including cell phones), the FFT algorithm is\nconsideredoneofthe10mostimportantalgorithmsofalltime.\nThe idea behind the FFT is to utilize the periodicity inherent in the definition of the\nDFT(9.74)to reducethetotal numberofcomputationalsteps. Essentially,thealgorithm\ndividestheinputdataintotwoequalgroupsandtransformsonlyonegroup,whichrequires\n‚àº(N‚àï2)2multiplications.Itthendividestheremaining(untransformed)groupofdatain\nhalfandtransformsthem,continuingtheprocessuntilallthedatahavebeentransformed.\nThetotalnumberofmultiplicationsrequiredwiththisapproachisapproximately Nlog2N.\nSpecifically,theFFTstimeeconomyarisesfromthecomputationallyexpensivecomplex\nfactorZnk[= [(Z)n]k]havingvaluesthatarerepeatedastheintegers nandkvarysequen-\ntially.Forinstance,for N=8,\nY0=Z0y0+Z0y1+Z0y2+Z0y3+Z0y4+Z0y5+Z0y6+Z0y7,\nY1=Z0y0+Z1y1+Z2y2+Z3y3+Z4y4+Z5y5+Z6y6+Z7y7,\nY2=Z0y0+Z2y1+Z4y2+Z6y3+Z8y4+Z10y5+Z12y6+Z14y7,\nY3=Z0y0+Z3y1+Z6y2+Z9y3+Z12y4+Z15y5+Z18y6+Z21y7,\nY4=Z0y0+Z4y1+Z8y2+Z12y3+Z16y4+Z20y5+Z24y6+Z28y7,\nY5=Z0y0+Z5y1+Z10y2+Z15y3+Z20y4+Z25y5+Z30y6+Z35y7,\nY6=Z0y0+Z6y1+Z12y2+Z18y3+Z24y4+Z30y5+Z36y6+Z42y7,\nY7=Z0y0+Z7y1+Z14y2+Z21y3+Z28y4+Z35y5+Z42y6+Z49y7,\n5 Actually,thisalgorithmhasbeendiscoveredanumberoftimes,forinstance,in1942byDandelionand\nLancersDanielsonandLanczos[1942],aswellasmuchearlier\n186 9 Fourier Analyses\nwhereweinclude Z0(‚â°1)forclarity.Whenweactuallyevaluatethesepowersof Z,wefind\nonlyfourindependentvalues:\nZ0=exp(0)=+1, Z1=exp(\n‚àí2ùúã\n8)\n=+‚àö\n2\n2‚àíi‚àö\n2\n2,\nZ2=exp(\n‚àí2‚ãÖ2iùúã\n8)\n=‚àíi,Z3=exp(\n‚àí2ùúã‚ãÖ3i\n8)\n=‚àí‚àö\n2\n2‚àíi‚àö\n2\n2,\nZ4=exp(\n‚àí2ùúã‚ãÖ4i\n8)\n=‚àíZ0,Z5=exp(\n‚àí2ùúã‚ãÖ5i\n8)\n=‚àíZ1,\nZ6=exp(\n‚àí2‚ãÖ6iùúã\n8)\n=‚àíZ2,Z7=exp(\n‚àí2‚ãÖ7iùúã\n8)\n=‚àíZ3,\nZ8=exp(\n‚àí2ùúã‚ãÖ8i\n8)\n=+Z0,Z9=exp(\n‚àí2ùúã‚ãÖ9i\n8)\n=+Z1,\nZ10=exp(\n‚àí2ùúã‚ãÖ10i\n8)\n=+Z2,Z11=exp(\n‚àí2ùúã‚ãÖ11i\n8)\n=+Z3,\nZ12=exp(\n‚àí2ùúã‚ãÖ11i\n8)\n=‚àíZ0,‚Ä¶. (9.75)\nWhensubstitutedintothedefinitionsofthetransforms,weobtain\nY0=Z0y0+Z0y1+Z0y2+Z0y3+Z0y4+Z0y5+Z0y6+Z0y7,\nY1=Z0y0+Z1y1+Z2y2+Z3y3‚àíZ0y4‚àíZ1y5‚àíZ2y6‚àíZ3y7,\nY2=Z0y0+Z2y1‚àíZ0y2‚àíZ2y3+Z0y4+Z2y5‚àíZ0y6‚àíZ2y7,\nY3=Z0y0+Z3y1‚àíZ2y2+Z1y3‚àíZ0y4‚àíZ3y5+Z2y6‚àíZ1y7,\nY4=Z0y0‚àíZ0y1+Z0y2‚àíZ0y3+Z0y4‚àíZ0y5+Z0y6‚àíZ0y7,\nY5=Z0y0‚àíZ1y1+Z2y2‚àíZ3y3‚àíZ0y4+Z1y5‚àíZ2y6+Z3y7,\nY6=Z0y0‚àíZ2y1‚àíZ0y2+Z2y3+Z0y4‚àíZ2y5‚àíZ0y6+Z2y7,\nY7=Z0y0‚àíZ3y1‚àíZ2y2‚àíZ1y3‚àíZ0y4+Z3y5+Z2y6+Z1y7,\nY8=Y0. (9.76)\nWeseethatthesetransformsnowrequire8 √ó8=64multiplicationsofcomplexnumbers,in\nadditiontosomelesstime-consumingadditions.Weplacetheseequationsinanappropriate\nformforcomputingbyregroupingthetermsintosumsanddifferencesofthe y‚Äôs:\nY0=Z0(y0+y4)+Z0(y1+y5)+Z0(y2+y6)+Z0(y3+y7),\nY1=Z0(y0‚àíy4)+Z1(y1‚àíy5)+Z2(y2‚àíy6)+Z3(y3‚àíy7),\nY2=Z0(y0+y4)+Z2(y1+y5)‚àíZ0(y2+y6)‚àíZ2(y3+y7),\nY3=Z0(y0‚àíy4)+Z3(y1‚àíy5)‚àíZ2(y2‚àíy6)+Z1(y3‚àíy7),\nY4=Z0(y0+y4)‚àíZ0(y1+y5)+Z0(y2+y6)‚àíZ0(y3+y7),\nY5=Z0(y0‚àíy4)‚àíZ1(y1‚àíy5)+Z2(y2‚àíy6)‚àíZ3(y3‚àíy7),\nY6=Z0(y0+y4)‚àíZ2(y1+y5)‚àíZ0(y2+y6)+Z2(y3+y7),\nY7=Z0(y0‚àíy4)‚àíZ3(y1‚àíy5)‚àíZ2(y2‚àíy6)‚àíZ1(y3‚àíy7),\nY8=Y0. (9.77)\nNotetherepeatingfactorsinsidetheparentheses,withcombinationsoftheform yp¬±yq.\nThese symmetries are systematized by introducing the butterfly operation (Figure 9.8).\nThisoperationtakesthe ypandyqdataelementsfromtheleftwingandconvertsthemto",4331
97-9.5.1 Bit Reversal.pdf,97-9.5.1 Bit Reversal,"9.5 Fast Fourier Transform ‚äô187\nFigure 9.8 The basic butterÔ¨Çy operation in which elements ypand\nyqon the left are transformed into yp+Zyqandyp‚àíZyqon the right.yp\nypZyp + Zyq\nyp ‚Äì Zyq\ny7Z0\nZ2Z2Z0Z0Z0\nZ2\nZ1\nZ3Z0Z0Z0\ny6y5y4y3y2y1y0 y0 + y4 (y0 + y4) + Z0(y2 + y6)\n(y1 + y5) + Z0(y3 + y7)\n(y0 + y4) ‚Äì Z0(y2 + y6)\n(y1 + y5) ‚Äì Z0(y3 + y7)\n(y0 + y4) ‚Äì Z2(y2 ‚Äì y6)\n(y1 ‚Äì y5) ‚Äì Z2(y3 ‚Äì y7)y1 + y5\ny2 + y6\ny3 + y7\ny0 ‚Äì y4\ny1 ‚Äì y5\ny2 ‚Äì y6\ny3 ‚Äì y7Y7 = (y0 ‚Äì y4) ‚Äì Z2 (y2 ‚Äì y6) ‚Äì Z3 (y1 ‚Äì y5) + Z1 (y3 ‚Äì y7)Y3 = (y0 ‚Äì y4) ‚Äì Z2 (y2 ‚Äì y6) + Z3 (y1 ‚Äì y5) + Z1 (y3 ‚Äì y7) \nY5 = (y0 ‚Äì y4) + Z2 (y2 ‚Äì y6) ‚Äì Z1 (y1 ‚Äì y5) ‚Äì Z3 (y3 ‚Äì y7)Y1 = (y0 ‚Äì y4) + Z2 (y2 ‚Äì y6) + Z1 (y1 ‚Äì y5) + Z3 (y3 ‚Äì y7)Y6 = (y0 + y4) ‚Äì (y2 + y6) ‚Äì Z2 (y1 + y5) + Z2 (y3 + y7)Y2 = (y0 + y4) ‚Äì (y2 + y6) + Z2 (y1 + y5) ‚Äì Z2 (y3 + y7)Y4 = (y0 + y4) + (y2 + y6) ‚Äì  (y1 + y5) ‚Äì (y3 + y7)Y0 = (y0 + y4) + (y2 + y6) + (y1 + y5) + (y3 + y7)\n(y0 ‚Äì y4) + Z2(y2 ‚Äì y6)\n(y1 ‚Äì y5) + Z2(y3 ‚Äì y7)\nFigure 9.9 The butterÔ¨Çy operations performing an FFT on the eight data on the left leading to\neight transforms on the right. The transforms are different linear combinations of the input data.\ntheyp+Zyqelements in the upper- and lower-right wings. In Figure 9.9 we show what\nhappens when we apply the butterfly operations to an entire FFT process, specifically to\nthe pairs (y0,y4),(y1,y5),(y2,y6),a n d(y3,y7). Notice how the number of multiplications\nof complex numbers has been reduced: For the first butterfly operation there are 8\nmultiplicationsby Z0;forthesecondbutterfly,operationthereare8multiplications,and\nso forth, until a total of 24 multiplications are made in four butterflies. In contrast, 64\nmultiplicationsarerequiredintheoriginalDFT(9.76).\n9.5.1 Bit Reversal\nThe reader may have observed in Figure 9.9 that we started with 8 data elements in the\norder0‚Äì7,andthatafterthreebutterflyoperatorsweobtainedtransformsintheorder0,4,\n2,6,1,5,3,7.Theastutereadermayfurtherhaveobservedthatthesenumberscorrespond\nto the bit-reversed order of 0‚Äì7. Let us look into this further. We need 3 bits to give the\norderofeachofthe8inputdataelements(thenumbers0‚Äì7).Explicitly,ontheleftinTable\n10.1, we give the binary representation for decimal numbers 0‚Äì7, their bit reversals, and\nthecorrespondingdecimalnumbers.Ontherightwegivetheorderingfor16inputdata\nelements,whereweneed4bitstoenumeratetheirorder.Noticethattheorderofthefirst8\nelementsdiffersinthetwocasesbecausethenumberofbitsbeingreverseddiffers.Notice\ntoothatafterthereordering,thefirsthalfofthenumbersareallevenandthesecondhalf\nareallodd.\n188 9 Fourier Analyses\nBinary-reversed 0‚Äì7 Binary-reversed 0‚Äì16\nDec Bin Rev Dec rev Rev Dec rev\n0 000 000 0 0000 0\n1 001 100 4 1000 8\n2 010 010 2 0100 4\n3 011 110 6 1100 12\n4 100 001 1 0010 2\n5 101 101 5 1010 10\n6 110 011 3 0110 6\n7 111 111 7 1110 14\n8 1000 0001 1\n9 1001 1001 9\n10 1010 0101 5\n11 1011 1101 13\n12 1100 0011 3\n13 1101 1011 11\n14 1101 0111 7\n15 1111 1111 15\ny7y3y5y1y6y2y4y0y0 + y4 (y0 + y4) + (y2 + y6) Y0 = (y0 + y4) + (y2 + y6)\n       + ( y1 + y5) + (y3 + y7)\nY1 = (y0 ‚Äì y4) + Z2(y2 ‚Äì y6)\n       + Z1(y1 ‚Äì y5) + Z3(y3 ‚Äì y7)\nY2 = (y0 + y4) ‚Äì (y2 + y6)\n       + Z2(y1 + y5) + Z2(y3 + y7)\nY3 = (y0 ‚Äì y4) ‚Äì Z2(y2 ‚Äì y6)\n       + Z3(y1 ‚Äì y5) + Z1(y3 ‚Äì y7)\nY4 = (y0 + y4) + (y2 + y6)\n       ‚Äì ( y1 + y5) ‚Äì (y3 + y7)\nY5 = (y0 ‚Äì y4) + Z2(y2 ‚Äì y6)\n       ‚Äì Z1(y1 ‚Äì y5) ‚Äì Z3(y3 ‚Äì y7)\nY6 = (y0 + y4) ‚Äì (y2 + y6)\n       ‚Äì Z2(y1 + y5) + Z2(y3 + y7)\nY7 = (y0 ‚Äì y4) ‚Äì Z2(y2 ‚Äì y6)\n       ‚Äì Z3(y1 ‚Äì y5) + Z1(y3 ‚Äì y7)(y0 + y4) ‚Äì (y2 + y6)\n(y1 + y5) + (y3 + y7)\n(y1 + y5) ‚Äì (y3 + y7)(y0 ‚Äì y5) ‚Äì Z2(y2 ‚Äì y6)\n(y1 ‚Äì y5) + Z2(y3 ‚Äì y7)\n(y1 ‚Äì y5) ‚Äì Z2(y3 ‚Äì y7)(y0 ‚Äì y4) + Z2(y2 ‚Äì y6)\ny2 + y6\ny1 + y5y0 ‚Äì y4\ny2 ‚Äì y6\ny3 + y7y1 ‚Äì y5\ny3 ‚Äì y7Z0\nZ0\nZ0\nZ0Z2Z0Z0\nZ0\nZ1\nZ2\nZ3Z2\nFigure 9.10 A modiÔ¨Åed FFT in which the eight input data on the left are transformed into eight\ntransforms on the right. The results are the same as in the previous Ô¨Ågure, but now the output\ntransforms are in numerical order whereas in the previous Ô¨Ågure the input signals were in\nnumerical order.",4087
98-9.7 FFT Assessment.pdf,98-9.7 FFT Assessment,"9.6 FFT Implementation 189\nTable 9.1 Reordering for 16 data complex points.\nOrder Input data New order Order Input data New order\n00 . 0 +0.0i0.0+0.0i 88 . 0 +8.0i1.0+1.0i\n11 . 0 +1.0i8.0+8.0i 99 . 0 +9.0i9.0+9.0i\n22 . 0 +2.0i4.0+4.0i 10 10.0 +10.i5.0+5.0i\n33 . 0 +3.0i12.0+12.0i 11 11.0 +11.0i13.0+13.0i\n44 . 0 +4.0i2.0+2.0i 12 12.0 +12.0i3.0+3.0i\n55 . 0 +5.0i10.0+10.i 13 13.0 +13.0i11.0+11.0i\n66 . 0 +6.0i6.0+6.0i 14 14.0 +14.i7.0+7.0i\n77 . 0 +7.0i14.0+14.0i 15 15.0 +15.0i15.0+15.0i\nThefactthattheFouriertransformsareproducedinanordercorrespondingtothebit-\nreversedorderofthenumbers0‚Äì7suggeststhatifweprocessthedatainthebit-reversed\norder0,4,2,6,1,5,3,7,thentheoutputFouriertransformswillbeordered(seeTable10.1).\nWe demonstrate this conjecture in Figure 9.10, where we see that to obtain the Fourier\ntransform for the eight input data, the butterfly operation had to be applied three times.\nThenumber3occursherebecauseitisthepowerof2thatgivesthenumberofdata;that\nis, 23=8. In general, in order for an FFT algorithm to produce transforms in the proper\norder,itmustreshuffletheinputdataintobit-reversedorder.Asacaseinpoint,oursample\nprogramstartsbyreorderingthe16(24)dataelementsgiveninTable9.1,andthenthefour\nbutterflyoperationsproducesequentiallyorderedoutput.\n9.6 FFT Implementation\nThe first FFT program we are aware of was written in 1967 in Fortran IV by Norman\nBrunneratMIT‚ÄôsLincolnLaboratory[Higgins,1976],andwashardforustofollow.Our\n(easier-to-follow)PythonversionisinListing9.3.Itsinputis N=2ndatatobetransformed\n(FFTs always require that the number of input data are a power of 2). If the number of\nyourinputdataisnotapowerof2,thenyoucanmakeitsobyconcatenatingsomeofthe\ninitialdatatotheendofyourinputuntilapowerof2isobtained;becauseaDFTisalways\nperiodic,thisjuststartstheperiodalittleearlier.Ourprogramassignscomplexnumbers\natthe16datapoints\nym=m+mi,m=0,‚Ä¶,15, (9.78)\nreordersthedataviabitreversal,andthenmakesfourbutterflyoperations.Thedataare\nstoredinthearray dt[max,2],withthesecondsubscriptdenotingrealandimaginaryparts.\nWeincreasespeedfurtherbyusingthe1Darray datatomakememoryaccessmoredirect:\ndata[1]=dt[0,1],data[2]=dt[1,1],data[3]=dt[1,0],‚Ä¶,(9.79)\nwhich also provides storage for the output. The FFT transforms datausing the butterfly\noperationandstorestheresultsbackin dt[,],wheretheinputdatawereoriginal.",2362
99-9.8 Code Listings.pdf,99-9.8 Code Listings,"190 9 Fourier Analyses\n9.7 FFT Assessment\n1) Compileandexecute FFT.py.Makesureyouunderstandtheoutput.\n2) Taketheoutputfrom FFT.py,inverse-transformitbacktosignalspace,andcompareit\ntoyourinput.[Checkingthatthedoubletransformisproportionaltoitselfisadequate,\nalthoughthenormalizationfactorsin(9.35)shouldmakethetwoequal.]\n3) ComparethetransformsobtainedwithanFFTtothoseobtainedwithaDFT(youmay\nchooseanyofthefunctionsstudiedbefore).Makesuretocomparebothprecisionand\nexecutiontimes.\n9.8 Code Listings\nListing 9.1 DFTcomplex.py Usesthebuilt-incomplexnumbersofPythontocompute\nthediscreteFouriertransformforthesignalinmethodf(signal).\n# DFTcomplex.py: Discrete Fourier Transform with built in complex\nfromvisualimport ‚àó;fromvisual.graph import ‚àó\n4importcmath # Complex math\nN = 100; twopi = 2. ‚àópi; h = twopi/N; sq2pi = 1./sqrt(twopi)\ny=z e r o s ( N + 1 , float); Ycomplex = zeros(N, complex) # Declare arrays\n8SignalGraph = gdisplay(x=0, y=0, width=600, height=250, title = ‚ÄôSignal y(t)‚Äô ,\\nxtitle= ‚Äôx‚Äô,ytitle= ‚Äôy(t)‚Äô, xmax=2. ‚àómath.pi , xmin=0, ymax=30, ymin= ‚àí30)\nSignalCurve = gcurve(color=color.yellow, display=SignalGraph)\nTransformGraph = gdisplay(x=0,y=250,width=600,height=250,title = ‚ÄôIm Y(omega)‚Äô ,\n12 xtitle = ‚Äôx‚Äô,ytitle= ‚ÄôIm Y(omega)‚Äô ,xmax=10.,xmin= ‚àí1,ymax=100,ymin= ‚àí250)\nTransformCurve = gvbars(delta = 0.05,color=color.red, display = TransformGraph)\ndefSignal(y): # Signal\n16h = twopi/N; x = 0.\nforiin range (0, N+1):\ny[i] = 30 ‚àócos(x) + 60 ‚àósin(2 ‚àóx) + 120 ‚àósin(3 ‚àóx)\nSignalCurve.plot(pos = (x, y[i])) #P l o t\n20 x+ =h\ndefDFT(Ycomplex): #D F T\nfornin range (0, N):\nzsum =complex(0.0, 0.0)\n24 forkin range (0, N):\nzexpo = complex(0, twopi ‚àók‚àón/N) # Complex exp\nzsum += y[k] ‚àóexp(‚àízexpo)\nYcomplex[n] = zsum ‚àósq2pi\n28 ifYcomplex[n].imag != 0:\nTransformCurve.plot(pos=(n,Ycomplex[n].imag))\nSignal(y) # Generate signal\nDFT(Ycomplex) # Transform signal\nListing 9.2 DFTreal.py Computes the discrete Fourier transform for the signal in\nmethodf(signal)usingrealnumbers.\n# DFTreal.py: Discrete Fourier Transform using real numbers\n3fromvisual.graph import ‚àó\nsigngr = gdisplay(x=0,y=0,width=600,height=250, \\ntitle= ‚ÄôSignal y(t)= 3 cos(wt)+2 cos(3wt)+ cos(5wt) ‚Äô ,\\n7xtitle= ‚Äôx‚Äô, ytitle= ‚Äôsignal‚Äô ,xmax=2. ‚àómath.pi ,xmin=0,ymax=7,ymin= ‚àí7)\n9.8 Code Listings 191\nsigfig = gcurve(color=color.yellow,display=signgr)\nimagr = gdisplay(x=0,y=250,width=600,height=250,\\ntitle= ‚ÄôFourier transform imaginary part‚Äô ,xtitle= ‚Äôx‚Äô,\\n11 ytitle= ‚ÄôTransf.Imag‚Äô ,xmax=10.0,xmin= ‚àí1,ymax=20,ymin= ‚àí25)\nimpart = gvbars(delta=0.05,color=color.red,display=imagr)\nN = 200\nNp = N\n15signal = zeros((N+1), float)\ntwopi = 2. ‚àópi\nsq2pi = 1./sqrt(twopi)\nh = twopi/N\n19dftimag = zeros((Np), float) # Im. transform\ndeff(signal):\nstep = twopi/N\n23t= 0.\nforiin range (0,N+1):\nsignal[i] = 3 ‚àósin(t ‚àót‚àót)\nsigfig.plot(pos=(t,signal[i]))\n27 t += step\ndeffourier(dftimag): #D F T\nfornin range (0,Np):\nimag = 0.\n31 forkin range (0, N):\nimag += signal[k] ‚àósin((twopi ‚àók‚àón)/N)\ndftimag[n] = ‚àíimag ‚àósq2pi # Im transform\nifdftimag[n] !=0:\n35 impart.plot(pos=(n,dftimag[n]))\nf(signal)\nfourier(dftimag)\nListing 9.3 FFT.py ComputestheFFTorinversetransformdependinguponthesignof\nsign.\n1# FFT . py : FFT for complex numbers in Y [ ] [ 2 ] , returned in Y\nfromnumpyimport ‚àó\nmax= 2100; points = 1026; N = 100; Switch = ‚àí1#S w i t c h = ‚àí1:Y, 1:y\n5y=z e r o s ( 2 ‚àó(N+4),float); Y = zeros((N+3,2), float)\ndeffft(N,Switch): # FFT of Y[n , 2 ]\nn=2 ‚àóN\n9foriin range (0,N+1): #yi nYt oy\nj=2 ‚àói+1\ny[j] = Y[i,0] #R e a lY ,o d dy [ j ]\ny[j+1] = Y[i,1] # Imag Y, even y[ j+1]\n13j=1 # y in bit reverse order\nforiin range (1,n+2, 2):\nif(i‚àíj) < 0 : # Reorder to bit reverse\ntempr = y[j]\n17 tempi = y[j+1]\ny[j] = y[i]\ny[j+1] = y[i+1]\ny[i] = tempr\n21 y[i+1] = tempi\nm=n/2;\nwhile(m‚àí2>0 ):\nif(j‚àím) <= 0 : break\n25 j=j‚àím\nm=m / 2\nj=j + m ;\nprint(""\n Bit-reversed y(t)"" )\n29foriin range (1,n+1,2): print(""%2d y[%2d] %9.5f "" % (i,i,y[i]))\nmmax = 2\nwhile(mmax‚àín) < 0 : # Begin transform\nistep = 2 ‚àómmax\n33 theta = 6.2831853/(1.0 ‚àóSwitch ‚àómmax)\n192 9 Fourier Analyses\nsinth = math.sin(theta/2.0)\nwstpr = ‚àí2.0‚àósinth ‚àó‚àó2\nwstpi = math.sin(theta)\n37 wr = 1.0\nwi = 0.0\nformin range (1 ,mmax+1,2) :\nforiin range (m,n+1,istep):\n41 j = i+mmax\ntempr = wr ‚àóy[j] ‚àíwi‚àóy[j+1]\ntempi = wr ‚àóy[j+1] +wi ‚àóy[j]\ny[j] = y[i] ‚àítempr\n45 y[j+1] = y[i+1] ‚àítempi\ny[i] = y[i] +tempr\ny[i+1] = y[i+1] +tempi\ntempr = wr\n49 wr = wr ‚àówstpr‚àíwi‚àówstpi + wr\nwi = wi ‚àówstpr + tempr ‚àówstpi + wi;\nmmax = istep\nforiin range (0,N):\n53 j=2 ‚àói+1\nY[i,0] = y[j]\nY[i,1] = y[j+1]\nprint(‚Äô\n Input \n i Re y(t) Im y(t)‚Äô )\n57h=2 ‚àópi/N; x = 0.\nforiin range (0,N+1): # Generate signal in Y\nY[i,0] = 30 ‚àócos(x) + 60 ‚àósin(2 ‚àóx) + 120 ‚àósin(3 ‚àóx) #R e a lp a r t\nY[i,1] = 0. #I mp a r t\n61x+ =h\nprint("" %2d %9.5f %9.5f"" %(i,Y[i,0],Y[i,1]))\nfft(N, Switch) # Call F F T, use global Y[][]\nprint ‚Äô\n Fourier Transform Y(omega)‚Äô\n65print("" i ReY(omega) ImY(omega) "" )\nforiin range (0,N):\nprint("" %2d %9.5f %9.5f "" %(i,Y[i,0],Y[i,1]))",5047
100-Chapter 10 Wavelet and Principal Components Analysis.pdf,100-Chapter 10 Wavelet and Principal Components Analysis,,0
101-10.1 Part I Wavelet Analysis.pdf,101-10.1 Part I Wavelet Analysis,"193\n10\nWavelet and Principal Components Analysis\nA number of techniques can extend Fourier analysis to signals whose time-dependencies\nchange in time. Part I of this chapter introduces wavelet analysis, a Ô¨Åeld that has seen\nextensive development and application in areas as diverse as brain waves, stock-market\ntrends, gravitational waves, and compression of photographic images. Part II of this chapter\ncovers the basics of principal components analysis. This is a powerful tool for situations\nin which there are very large data sets, and especially those with space-time correlated\nvariables .\n10.1 Part I: Wavelet Analysis\nProblem YouhavesampledthesignalinFigure10.1thatseemstocontainanincreasing\nnumber of frequencies as time increases. Your problemis to undertake a spectral anal-\nysis of this signal that tells you, in the most compact way possible, the amount of each\nfrequencypresentateachinstantoftime. Hint:Althoughwewantthemethodtobegeneral\nenoughtoworkwithnumericaldata,forpedagogicalpurposesitisusefultoknowthatthe\nsignalis\ny(t)=‚éß\n‚é™\n‚é®\n‚é™‚é©sin2ùúãt, for0‚â§t‚â§2,\n5sin2ùúãt+10sin4ùúãt, for2‚â§t‚â§8,\n2.5sin2ùúãt+6sin4ùúãt+10sin6ùúãt,for8‚â§t‚â§12.(10.1)\nTheFourieranalysisweusedinChapter9revealstheamountoftheharmonicfunctions\nsin(nùúît)andcos(nùúît)thatarepresentinasignal.Anexpansioninperiodicfunctionsisfine\nforstationarysignals(thosewhoseformsdonotchangeintime),buthasshortcomingsfor\nthetimedependenceofour problemsignal(10.1).OnesuchproblemisthattheFourier\nreconstruction has all frequencies nùúîoccurring simultaneously, and so does not contain\ntimeresolution informationindicatingwheneachfrequencyoccurs.Anothershortcoming\nisthatalltheFouriercomponentsarecorrelated,andthatresultsinmoreinformationbeing\nstoredthanisneededtoreconstructthesignal.\nThereareanumberoftechniquesthatextendsimpleFourieranalysistononstationary\nsignals. The idea behind wavelet analysis is to expand a signal in a complete set of\nfunctions(wavelets),eachofwhichoscillatesforafiniteperiodoftime,andeachofwhich\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n194 10 Wavelet and Principal Components Analysis\n0123456789\nFigure 10.1 The input time signal (10.1) we wish to analyze. The signal is seen to contain\nadditional frequencies as time increases. The boxes are possible placements of windows for\nshort-time Fourier transforms.\n‚Äì1.0‚Äì0.50.00.51.0 1.0\n‚Äì6 ‚Äì4 ‚Äì2 0 2 4 6\ntœà\nœàœà\n0.0\n‚Äì4 0 4\nt\n‚Äì1.00.01.0\n‚Äì4 0\nt4 0 200 400 600 800 10000.1\n0\n‚Äì0.1Daub4 e6\nFigure 10.2 Four possible mother wavelets that can be used to generate entire sets of daughter\nwavelets. Clockwise from top : Morlet (real part), Mexican hat, Daub4 e6 (explained later), and Haar.\nThe daughter wavelets are generated by scaling and translating these mother wavelets.\nis centered at a different time. To give you a preview before we go into details, we show\nfour sample wavelets in Figure 10.2. Because each wavelet is local in time, it is a wave\npacket,1with its time localization leading to a spectrum with a range of frequencies.\nThesewavepacketsarecalled‚Äúwavelets‚Äùbecausetheyexistforonlyshortperiodsoftime\n[Polikar,2023].\nAlthoughwaveletsarerequiredtooscillateintime,theyarenotrestrictedtoaparticular\nfunctionalform[Addison,2002;GoswaniandChan,1999;Graps,1995].Asacaseinpoint,\ntheymaybeoscillatingGaussian(Morlet:topleftinFigure10.2),\nŒ®(t)=e2ùúãite‚àít2‚àï2ùúé2=(cos2ùúãt+isin2ùúãt)e‚àít2‚àï2ùúé2(Morlet), (10.2)\nthesecondderivativeofaGaussian(Mexicanhat,topright),\nŒ®(t)=‚àíùúé2d2\ndt2e‚àít2‚àï2ùúé2=(\n1‚àít2\nùúé2)\ne‚àít2‚àï2ùúé2, (10.3)\n1 WediscusswavepacketsfurtherinSection10.2.",3643
102-10.4 Wavelet Transforms.pdf,102-10.4 Wavelet Transforms,"10.2 Wave Packets and Uncertainty Principle 195\nan up-and-down step function (lower left), or a fractal shape (bottom right). All of these\nwavelets are localizedin both time and frequency, that is, they are large for just a finite\ntimeandcontainafiniterangeoffrequencies.Asweshallsee,translatingandscalingthese\nmotherwavelet generatesanentiresetof childwavelets basisfunctions,withthechildren\ncoveringdifferentfrequencyrangesatdifferenttimes.\n10.2 Wave Packets and Uncertainty Principle\nAwavepacket orwavetrain isacollectionofwavesofdifferingfrequenciesaddedtogether\ninsuchawayastoproduceapulseofwidth Œît.Asweshallsee,theFouriertransformofa\nwavepacketisapulseinthefrequencydomainofwidth Œîùúî.We‚Äôllfirststudywavepackets\nanalytically,andthenusethemnumerically.Anexampleofasimplewavepacketisasine\nwave that oscillates at frequency ùúî0forNperiods (Figure 10.3 left) [Arfken and Weber,\n2001]:\ny(t)=‚éß\n‚é™\n‚é®\n‚é™‚é©sinùúî0t,for |t|<Nùúã\nùúî0‚â°NT\n2,\n0,for |t|>Nùúã\nùúî0‚â°NT\n2,(10.4)\nwhere we relate the frequency to the period via the usual ùúî0=2ùúã‚àïT.I nt e r m so ft h e s e\nparameters,thewidthofthewavepacketis\nŒît=NT=N2ùúã\nùúî0. (10.5)\nTheFouriertransformofthewavepacket(10.4)isastraightforwardapplicationofthetrans-\nformformula(9.17):\nY(ùúî)=‚à´+‚àû\n‚àí‚àûdte‚àíiùúît\n‚àö\n2ùúãy(t)=‚àíi‚àö\n2ùúã‚à´Nùúã‚àïùúî0\n0dtsinùúî0tsinùúît (10.6)\n=(ùúî0+ùúî)sin[\n(ùúî0‚àíùúî)Nùúã\nùúî0]\n‚àí(ùúî0‚àíùúî)sin[\n(ùúî0+ùúî)Nùúã\nùúî0]\n‚àö\n2ùúã(ùúî2\n0‚àíùúî2),\nwherewehavedroppedafactorof ‚àíithataffectsonlythephase.Whileatfirstglance(10.6)\nappearstobesingularat ùúî=ùúî0,itactuallyjustpeaksthere(Figure10.3right),reflecting\nthepredominanceoffrequency ùúî0.Notethatalthoughthesignal y(t)appearstohaveonly\nonefrequency,itdoesdropoffsharplyintime(Figure10.3left),andthesecornersgive Y(ùúî)\nafinitewidth Œîùúî.\nThereisafundamentalrelationbetweenthewidths ŒîtandŒîùúîofawavepacket.Although\nweuseaspecificexampletodeterminethatrelation,itistrueingeneral.Whiletheremay\nnotbeaprecisedefinitionof‚Äúwidth‚Äùforallfunctions,onecanusuallydeduceagoodmea-\nsure of the width (say, within 25%). To illustrate, if we look at the right of Figure 10.3, it\nmakessensetousethedistancebetweenthefirstzerosofthetransform Y(ùúî)(10.6)asthe\nfrequencywidth Œîùúî.Thezerosoccurat\nùúî‚àíùúî0\nùúî0=¬±1\nN‚áíŒîùúî‚âÉùúî‚àíùúî0=ùúî0\nN, (10.7)\n196 10 Wavelet and Principal Components Analysis\n‚Äì1.00.01.0\n‚Äì4 0 4\nt œây Y\n0.0\n0 10\nFigure 10.3 Left: A wave packet in time corresponding to the functional form (10.4) with ùúî0=5\nandN=6.Right: The Fourier transform in frequency of this same wave packet.\nwhereNisthenumberofcyclesinouroriginalwavepacket.Becausethewavepacketin\ntimemakes Noscillationseachofperiod T,areasonablemeasureofthetimewidth Œîtof\nthesignaly(t)is\nŒît=NT=N2ùúã\nùúî0. (10.8)\nWhentheproductsofthefrequencywidth(10.7)andthetimewidth(10.8)arecombined,\nweobtain\nŒîtŒîùúî‚â•2ùúã. (10.9)\nThegreater-thansignisusedheretoindicatethatthisisaminimum,thatis,that y(t)and\nY(ùúî)extendbeyond ŒîtandŒîùúî,respectively.Nonetheless,mostofthesignalandtransform\nshouldfallwithintheboundsof(10.9).\nA relation of the form (10.9) also occurs in quantum mechanics, where it is known as\ntheHeisenberguncertaintyprinciple ,withŒîtandŒîùúîcalledtheuncertaintiesin tandùúî.It\nistruefortransformsingeneral,andstatesthatasasignalismademorelocalizedintime\n(smaller Œît), its transform becomes less localized (larger Œîùúî). Conversely, the sine wave\ny(t)=sinùúî0tiscompletelylocalizedinfrequency,andconsequentlyhasaninfiniteextent\nintime,Œît‚âÉ‚àû.\n10.2.1 Wave Packet Exercise\nConsiderthefollowingwavepackets:\ny1(t)=e‚àít2‚àï2,y2(t)=sin(8t)e‚àít2‚àï2,y3(t)=(1‚àít2)e‚àít2‚àï2. (10.10)\nForeachwavepacket:\n1) Estimatethewidth Œît.Agoodmeasuremightbethe fullwidthathalf-maxima (FWHM)\nof|y(t)|.\n2) UseyourDFTprogramtoevaluateandplottheFouriertransform Y(ùúî)foreachwave\npacket.Make bothalinearandasemilogplot(smallcomponentsareoftenimportant,\nyet not evident in linear plots). Make sure that your transform has a good number of\ncloselyspacedfrequencyvaluesoverarangethatislargeenoughtoshowtheperiodicity\nofY(ùúî).\n3) Whataretheunitsfor Y(ùúî)andùúîinyourDFT?\n4) Foreachwavepacket,estimatethewidth Œîùúî.Agoodmeasuremightbethe fullwidth\nathalf-maxima of|Y(ùúî)|.\n10.3 Short-Time Fourier Transforms 197\n5) Foreachwavepacketdetermineapproximatevaluefortheconstant Coftheuncertainty\nprinciple\nŒîtŒîùúî‚â•2ùúãC. (10.11)\n10.3 Short-Time Fourier Transforms\nThe constant amplitude of the functions sin nùúîtand cosnùúîtfor all times can limit the\nusefulnessofFourieranalysisforreproducingsignalswhoseformchangesintime.Seeing\nthatthesebasisfunctionsextendoveralltimeswithaconstantamplitude,thereisconsider-\nableoverlapamongthem,andthustheinformationpresentinvariousFouriercomponents\nare correlated. This is undesirable for data storage and compression, where you want to\nstoreaminimumamountofinformation,andalsowanttoadjusttheamountofinforma-\ntionstoreddependentonthedesiredqualityofthereconstructedsignal.2Losslesscompres-\nsionexactlyreproducestheoriginalsignal.Youcansavespacebystoringhowmanytimes\neachdataelementisrepeated,andwhereeachelementislocated.In lossycompression ,in\nadditiontoremovingrepeatedelements,youalsoeliminatesometransformcomponents\nconsistentwiththeuncertaintyrelation(10.9)andwiththelevelofresolutionrequiredin\nthereproduction.Thisleadstoevengreatercompression.\nInSection9.3wedefinedtheFouriertransform Y(ùúî)ofsignaly(t)as\nY(ùúî)=‚à´+‚àû\n‚àí‚àûdte‚àíiùúît\n‚àö\n2ùúãy(t)‚â°‚ü®ùúî|y‚ü©. (10.12)\nAsistrueforsimplevectors,youcanthinkof(10.12)asgivingtheoverlaporscalarproduct\nof the basis function |ùúî‚ü©=exp(iùúît)‚àï‚àö\n2ùúãand the signal y(t)[notice that the complex\nconjugateoftheexponentialbasisfunctionappearsin(10.12)].Anotherviewof(10.12)is\nthemappingorprojectionofthesignalinto ùúîspace.Inthislatterview,theoverlapprojects\nout the amount of the periodicfunction exp (iùúît)‚àï‚àö\n2ùúãin the signal y(t).I no t h e rw o r d s ,\ntheFouriercomponent Y(ùúî)canbethoughtofasthecorrelationbetweenthesignal y(t)\nand the basis function exp (iùúît)‚àï‚àö\n2ùúã. This is the same as what results from filtering the\nsignaly(t)throughafrequencyfilter.Ifthereisnoexp (iùúît)inthesignal,thentheintegral\nvanishesandthereisnooutput.If y(t)=exp(iùúît),thesignalisatonlyonefrequency,and\ntheintegralisaccordinglysingular.\nThe signal in Figure 10.1 for our problem clearly has different frequencies present at\ndifferenttimes,andfordifferentlengthsoftime.Inthepast,thissignalmighthavebeen\nanalyzedwithaprecursorofwaveletanalysisknownasthe short-timeFouriertransform .\nWiththattechnique,thesignal y(t)is‚Äúchoppedup‚Äùintodifferentsegmentsalongthetime\naxis,withsuccessivesegmentscenteredaboutsuccessivetimes ùúè1,ùúè2,‚Ä¶,ùúèN.Forinstance,\nweshowthreesuchsegmentsintheboxesofFigure10.1.Oncewehavethedissectedsignal,\naFourieranalysisismadeforeachsegment.Wearethenleftwithasequenceoftransforms\n[Y(ST)\nùúè1,Y(ST)\nùúè2,‚Ä¶,Y(ST)\nùúèN],oneforeachshort-timeinterval,wherethesuperscript(ST)indicates\nshorttime.\n2 Waveletshaveproventobeahighlyeffectiveapproachtodatacompression,withtheJointPhotographic\nExpertsGroup(JPEG)2000standardbeingbasedonwavelets.",6895
103-10.4.1 Generating Wavelet Basis Functions.pdf,103-10.4.1 Generating Wavelet Basis Functions,"198 10 Wavelet and Principal Components Analysis\nRather than chopping up a signal by hand, we can express short-time Fourier trans-\nformingmathematicallybyimaginingtranslatinga windowfunction ùë§(t‚àíùúè),whichiszero\noutsideofsomechoseninterval,overthesignalinFigure10.1:\nY(ST)(ùúî,ùúè)=‚à´+‚àû\n‚àí‚àûdteiùúît\n‚àö\n2ùúãùë§(t‚àíùúè)y(t). (10.13)\nHere the values of the translation time ùúècorrespond to different locations of window ùë§\noverthesignal,andthewindowfunctionisessentiallyatransparentboxofsmallsizeonan\nopaquebackground.Anysignalwithinthewidthofthewindowistransformed,whilethe\nsignallyingoutsidethewindowisnotseen.Notethatin(10.13),theextravariable ùúèinthe\nFouriertransformindicatesthelocationofthetimearoundwhichthewindowwasplaced.\nClearly,becausetheshort-timetransformisafunctionoftwovariables,asurfaceor3Dplot\nisneededtoviewtheamplitudeasafunctionofboth ùúîandùúè.\n10.4 Wavelet Transforms\nThewavelettransformofatimesignal y(t)isdefinedas\nY(s,ùúè)=‚à´+‚àû\n‚àí‚àûdtùúì‚àó\ns,ùúè(t)y(t)(wavelettransform), (10.14)\nand is similar in concept and notation to a short-time Fourier transform. The difference\nis rather than using exp (iùúît)as the basis functions, here we are using wave packets\nor wavelets ùúìs,ùúè(t)localized in time, such as those shown in Figure 10.2. Because each\nwaveletislocalizedintime,eachactsasitsownwindowfunction.Becauseeachwaveletis\noscillatory,eachcontainsitsownlimitedrangeoffrequencies.\nEquation (10.14) says that the wavelet transform Y(s,ùúè)is a measure of the amount of\nbasisfunction ùúìs,ùúè(t)presentinthesignal y(t).Theùúèvariableindicatesthetimeportionof\nthe signal being decomposed, while the svariable is equivalent to the frequency present\nduringthattime:\nùúî=2ùúã\ns,s=2ùúã\nùúî(scale-frequencyrelation). (10.15)\nSeeing that it is key to much that follows, it is a good idea to think about (10.15) for a\nmoment.Ifweareinterestedinthetime detailsofasignal,thenthisisanotherwayofsaying\nthatweareinterestedinwhatishappeningatsmallvaluesofthe scales.Equation(10.15)\nindicates that small values of scorrespond to high-frequency components of the signal.\nThatbeingthecase,thetimedetailsofthesignalareinthehigh-frequency,orlow-scale,\ncomponents.\n10.4.1 Generating Wavelet Basis Functions\nTheconceptualdiscussionofwaveletsisover,anditistimetogetdowntowork.Wefirst\nneedatechniqueforgeneratingwaveletbasisfunctions,andthenweneedtodiscretizethis\n10.4 Wavelet Transforms 199\ntechnique.Asisoftenthecase,thefinalformulationwillturnouttobesimpleandshort,\nbutitwillbeawhilebeforewegetthere.\nJustastheexpansionofanarbitraryfunctioninacompletesetoforthogonalfunctionsis\nnotrestrictedtoanyparticularbasisset,sotooisthewavelettransformnotrestrictedtoany\nparticularwaveletbasisset,althoughsomemightbebetterthanothersforagivensignal.\nThestandardwaytogenerateafamilyofwaveletbasisfunctionsstartswith Œ®(t),amother\noranalyzingfunctionoftherealvariable t,andthenusesittogenerate daughterwavelets.\nAsacaseinpoint,westartwiththemotherwavelet\nŒ®(t)=sin(8t)e‚àít2‚àï2. (10.16)\nByscaling,translating,andnormalizingthismotherwaveletweobtaintheset\nùúìs,ùúè(t)def=1‚àö\nsŒ®(t‚àíùúè\ns)\n=1‚àö\nssin[8(t‚àíùúè)\ns]\ne‚àí(t‚àíùúè)2‚àï2s2, (10.17)\nandwithitwegeneratethefourwaveletbasisfunctionsdisplayedinFigure10.4.Wesee\nthatlargerorsmallervaluesof s,respectively,expandorcontractthemotherwavelet,while\ndifferent values of ùúèshift the center of the wavelet. Because the wavelets are inherently\noscillatory,thescalingleadstothesamenumberofoscillationsoccurringindifferenttime\nspans, which is equivalent to having basis states with differing frequencies. We see that\ns<1 produces a higher-frequency wavelet, while s>1 produces a lower-frequency one,\nboth of the same shape. As we shall see, we do not need to store much information to\noutline the large-time-scale sbehavior of a signal (its smooth envelope ), but we do need\nmore information to specify its short-time-scale sbehavior (details). And if we want to\nresolveyetfinerfeaturesinthesignal,thenwewillneedtohavemoreinformationonyet\nfiner details. Here the division by‚àö\nsis made to ensure that there is equal ‚Äúpower‚Äù (or\n‚Äì0.60.00.61.0\n0.0\n‚Äì1.0\n1.0\n0.0\n‚Äì1.0‚Äì6 ‚Äì4 ‚Äì2 0 2 4 6 ‚Äì6 ‚Äì4 ‚Äì2 0 2 4 6\ntŒ®Œ®\nŒ® Œ®s = 2, œÑ = 0\nt\n‚Äì4 ‚Äì2 0 2 4 6 8 10\nt‚Äì0.60.00.6\n‚Äì6 ‚Äì4 ‚Äì2 0 2 4 6\nts = ¬Ω, œÑ = 0\ns = 2, œÑ = 0 s = 2, œÑ = 6\nFigure 10.4 Four wavelet basis functions (daughters) generated by scaling ( s) and translating ( ùúè)\nan oscillating Gaussian mother wavelet. Clockwise from top :(s=1,ùúè=0), (s=1/2,ùúè=0), (s=1,\nùúè=6), and ( s=2,ùúè=60). Note how s<1 is a wavelet with higher frequency, while s>1 has a\nlower frequency than the s=1 mother. Likewise, the ùúè=6 wavelet is just a translated version of\ntheùúè=0 one directly above it.\n200 10 Wavelet and Principal Components Analysis\nenergyorintensity)ineachregionof s,althoughothernormalizationscanalsobefound\nin the literature. After substituting in the definition of daughters, the wavelet transform\n(10.14)anditsinverse[vandenBerg,1999]are\nY(s,ùúè)=1‚àö\ns‚à´+‚àû\n‚àí‚àûdtŒ®‚àó(t‚àíùúè\ns)\ny(t)( Wavelet Transform ),(10.18)\ny(t)=1\nC‚à´+‚àû\n‚àí‚àûdùúè‚à´+‚àû\n0dsùúì‚àó\ns,ùúè(t)\ns3‚àï2Y(s,ùúè)( Inverse Transform ),(10.19)\nwherethenormalizationconstant Cdependsonthewaveletused.\nInsummary,waveletbasesarefunctionsofthetimevariable t,aswellasofthetwoparam-\neterssandùúè.Thetvariableisintegratedovertoyieldatransformthatisafunctionofthe\ntimescales(frequency2 ùúã‚àïs)andwindowlocation ùúè.Youcanthinkofscaleasbeinglikethe\nscaleonamap(alsodiscussedinSection14.4.1inrelationtofractalanalysis)orinterms\nofresolution,asmightoccurinphotographicimages.Regardlessofthewords,asweseein\nChapter14,ifwehaveafractal,thenwehaveaself-similarobjectthatlooksthesameatall\nscalesorresolutions.Similarly,eachwaveletinasetofbasisfunctionsisself-similartothe\nothers,butatadifferentscaleorlocation.\nThe general requirements for a mother wavelet Œ®are [Addison, 2002; van den Berg,\n1999]:\n1)Œ®(t)isreal.\n2)Œ®(t)oscillatesaroundzerosuchthatitsaverageiszero:\n‚à´+‚àû\n‚àí‚àûŒ®(t)dt=0. (10.20)\n3)Œ®(t)islocal,thatis,awavepacket,andissquare-integrable:\nŒ®(|t|‚Üí‚àû)‚Üí0 (rapidly) ,‚à´+‚àû\n‚àí‚àû|Œ®(t)|2dt<‚àû. (10.21)\n4) Thetransformsoflowpowersof tvanish,thatis,thefirst pmoments:\n‚à´+‚àû\n‚àí‚àût0Œ®(t)dt=‚à´+‚àû\n‚àí‚àût1Œ®(t)dt=¬∑¬∑¬∑=‚à´+‚àû\n‚àí‚àûtp‚àí1Œ®(t)dt=0. (10.22)\nThismakesthetransformmoresensitivetodetailsthantogeneralshape.\nAsanexampleofhowweusethe sandùúèdegreesoffreedominawavelettransform,con-\nsidertheanalysisofachirpsignal y(t)=sin(60t2)inFigure10.5.Weseethatasliceatthe\nbeginningofthesignaliscomparedtoourfirstbasisfunction.(Thecomparisoniscarried\noutviatheconvolution ofthewaveletwiththesignal.)Thisfirstcomparisoniswithanarrow\nversionofthewavelet,thatis,atlowscale,andyieldsasinglecoefficient.Thecomparison\natthisscalecontinueswiththenextsignalslice,andeventuallyendswhentheentiresig-\nnal has been covered (the top row in the figure). Then the wavelet is expanded to larger\nsvalues,andthecomparisonsarerepeated.Eventually,thedataareprocessedatallscales\nandatalltimeintervals.Thenarrowsignalscorrespondtoahigh-resolutionanalysis,while\nthebroadsignalscorrespondtolowresolution.Asthescalesgetlarger(lowerfrequencies,\nlowerresolution),fewerdetailsofthetimesignalremainvisible,buttheoverallshapeor\ngrossfeaturesofthesignalbecomeclearer.",7120
104-10.4.2 Continuous Wavelet Transforms.pdf,104-10.4.2 Continuous Wavelet Transforms,"10.4 Wavelet Transforms 201\nFigure 10.5 A schematic representation of the steps followed in performing a wavelet\ntransformation over all time displacements and scales. The dark grey signal is Ô¨Årst analyzed by\nevaluating its overlap with a narrow wavelet at the signal‚Äôs beginning. This produces a coefÔ¨Åcient\nthat measures the similarity of the signal to the wavelet. The wavelet is successively shifted over\nthe length of the signal and the overlaps are successively evaluated. After the entire signal is\ncovered, the wavelet is expanded and the entire analysis is repeated.\n10.4.2 Continuous Wavelet Transforms\nWe want to develop some intuition as to what wavelet transforms look like before going\non to apply them. Accordingly, modify the program you have been using for the Fourier\ntransform so that it now computes the wavelet transform. In contrast to the discrete or\ndigitalversion,thisisa continuous wavelettransform.\n1) Examinetheeffectofusingdifferentmotherwavelets.Accordingly,writeamethodthat\ncalculatesthemotherwaveletfor\na) aMorletwavelet(10.2),\nb) aMexicanhatwavelet(10.3),\nc) aHaarwavelet(thesquarewaveinFigure10.2).\n2) Tryoutyourtransformforthefollowinginputsignalsandseeiftheresultsmakesense:\na) Apuresinewave y(t)=sin2ùúãt,\nb) Asumofsinewaves y(t)=2.5sin2ùúãt+6sin4ùúãt+10sin6ùúãt,\nc) Thenonstationarysignalforourproblem(10.1)\ny(t)=‚éß\n‚é™\n‚é®\n‚é™‚é©sin2ùúãt, for0‚â§t‚â§2,\n5sin2ùúãt+10sin4ùúãt, for2‚â§t‚â§8,\n2.5sin2ùúãt+6sin4ùúãt+10sin6ùúãt,for8‚â§t‚â§12.(10.23)\nd) Thehalf-wavefunction\ny(t)={\nsinùúît,for0<t<T‚àï2,\n0,forT‚àï2<t<T.(10.24)\n3)‚äôUse(10.19)toinvertyourwavelettransformandcomparethereconstructedsignalto\ntheinputsignal(youcannormalizethetwotoeachother).InFigure10.6weshowour\nreconstruction.\n202 10 Wavelet and Principal Components Analysis\n0‚Äì20‚Äì100Signal1020\n2468 1 0 1 2\nTime tInput signal\nInverted transform\nFigure 10.6 Comparison of an input and reconstituted signal (10.23) using Morlet wavelets.\nThe curves overlap nearly perfectly, except at the ends.\nInListing10.1wegiveour continuouswavelettransformation CWT.py[LangandForinash,\n1998].Becausewavelets,withtheirfunctionaldependenceontwovariables,maybesome-\nwhathardtograspatfirst,wesuggestthatyouwriteyourowncodeandincludeaportion\nthat does the inverse transform as a check. In Section 10.5, we will describe the discrete\nwavelet transformation that makes optimal discrete choices for the scale and time trans-\nlationparameters sandùúè. Figure10.7showsthespectrumproduced fortheinputsignal\n(10.1) in Figure 10.1. And it works! We see predominantly one frequency at short times,\ntwofrequenciesatintermediatetimes,andthreefrequenciesatlongertimes.\n0\n1\n2sœÑœàs,œÑ\n0\n4\n8\n12‚Äì101\nFigure 10.7 The continuous wavelet spectrum obtained by analyzing the input signal with Morlet\nwavelets. Observe how at small values of time ùúèthere is predominantly one frequency present,\nhow a second, higher-frequency (smaller-scale) component enters at intermediate times, and how\nat larger times a still higher-frequency components enter. (Figure courtesy of Z. Diabolic.)",3043
105-10.5 Discrete Wavelet Transforms.pdf,105-10.5 Discrete Wavelet Transforms,"10.5 Discrete Wavelet Transforms ‚äô203\n10.5 Discrete Wavelet Transforms ‚äô\nAswastrueforDFTs,ifatimesignalismeasuredatonly Ndiscretetimes,\ny(tm)‚â°ym,m=1,‚Ä¶,N, (10.25)\nthenwecandetermineonly N-independentcomponentsofthetransform Y.Thetrickisto\nremainconsistentwiththeuncertaintyprincipleaswecomputeonlythe N-independent\ncomponents required to reproduce the signal. The discrete wavelet transform (DWT)\nevaluates the transforms with discrete values for the scaling parameter sand the time\ntranslationparameter ùúè:\nùúìj,k(t)=Œ®[(t‚àík2j)‚àï2j]\n‚àö\n2j‚â°Œ®(t‚àï2j‚àík)\n‚àö\n2j(DWT), (10.26)\ns=2j,ùúè=k\n2j,k,j=0,1,‚Ä¶. (10.27)\nHerejandkareintegerswhosemaximumvaluesareyettobedetermined,andwemeasure\ntimeinintegervalues.Thischoiceof sandùúè,basedonpowersof2,iscalleda dyadicgrid\narrangement,andwillbeseentoautomaticallyperformthescalingsandtranslationsatthe\ndifferenttimescalesthatareattheheartofwaveletanalysis.3TheDWTnowbecomes\nYj,k=‚à´+‚àû\n‚àí‚àûdtùúìj,k(t)y(t)‚âÉ‚àë\nmùúìj,k(tm)y(tm)h(DWT), (10.28)\nwherethediscretenessherereferstothewaveletbasissetand notthetimevariable.Foran\northonormalwaveletbasis,theinversediscretetransformisthen\ny(t)=+‚àû‚àë\nj,k=‚àí‚àûYj,kùúìj,k(t)(inverseDWT) . (10.29)\nThis inversion will exactly reproduce the input signal at the Ninput points, but only if\nwe sum over an infinite number of terms [Addison, 2002]. Practical calculations will be\nlessexact.\nNoticein(10.26)and(10.28)thatwehavekeptthetimevariable tinthewaveletbasis\nfunctionscontinuous,despitethefactthat sandùúèhavebeenmadediscrete.Thisisuseful\ninestablishingtheorthonormalityofthebasisfunctions,\n‚à´+‚àû\n‚àí‚àûdtùúì‚àó\nj,k(t)ùúìj‚Ä≤,k‚Ä≤(t)=ùõøjj‚Ä≤ùõøkk‚Ä≤, (10.30)\nwhereùõøm,nistheKroneckerdeltafunction.Beingnormalizedto1meansthateachwavelet\nbasishas‚Äúunitenergy‚Äù;beingorthogonalmeansthateachbasisfunctionisindependentof\ntheothers.Andbecausewaveletsarelocalizedintime,thedifferenttransformcomponents\nhavelowlevelsofcorrelationswitheachother.Altogether,thisleadstoefficientandflexible\ndatastorage.\nThe use of a discrete wavelet basis makes it clear that we sample the input signal at\nthediscretevaluesoftimedeterminedbytheintegers jandk.Ingeneral,youwanttime\n3 Notethatsomereferencesscaledownwithincreasing j,incontrasttoourscalingup.\n204 10 Wavelet and Principal Components Analysis\nTimeFrequencyFigure 10.8 A graphical representation of the relation between time and\nfrequency resolutions (the uncertainty relation). Each box represents an\nequal portion of the time-frequency plane but with different proportions of\ntime and frequency.\nstepsthatsamplethesignalatenoughtimesineachintervaltoobtainthedesiredlevelof\nprecision.Aruleofthumbistostartwith100stepstocovereachmajorfeature.Ideally,the\nneededtimescorrespondtothetimesatwhichthesignalwassampled,althoughthismay\nrequiresomeforethought.\nConsider Figure 10.8. We measure a signal at a number of discrete times within the\nintervals(korùúèvalues)correspondingtotheverticalcolumnsoffixedwidthalongthetime\naxis.Foreachtimeinterval,wewanttosamplethesignalatanumberofscales(frequencies\norjvalues).However,asdiscussedinSection10.2,thebasicmathematicsofFouriertrans-\nforms indicates that the width Œîtof a wave packet ùúì(t)and the width Œîùúîof its Fourier\ntransformY(ùúî)arerelatedbyanuncertaintyprinciple\nŒîùúîŒît‚â•2ùúã.\nThisrelationplacesaconstraintonthetimeintervalsandfrequencyintervals.Furthermore,\nwhilewemaywantahigh-resolutionreproductionofoursignal,wedonotwanttostore\nmoredatathanareneededtoobtainthatreproduction.Ifwesamplethesignalfortimes\ncenteredaboutsome ùúèinanintervalofwidth Œîùúè(Figure10.8),andthencomputethetrans-\nformatanumberofscales sorfrequencies ùúî=2ùúã‚àïscoveringarangeofheight Œîùúî,then\ntherelationbetweentheheightandwidthisrestrictedbytheuncertaintyrelation.Allthis\nmeansthateachoftherectanglesinFigure10.8hasthesamearea ŒîùúîŒît=2ùúã.Theincreas-\ningheightsoftherectanglesathigherfrequenciesmeansthatalargerrangeoffrequencies\nshouldbesampledasthefrequencyincreases.Thepremisehereisthatthelow-frequency\ncomponentsprovidethegrossor smoothoutlineofthesignalwhich,beingsmooth,does\nnotrequiremuchdetail,whilethehigh-frequencycomponentsgivethedetailsofthesig-\nnal over a short time interval, and so require many components in order to record these\ndetailswithhighresolution.\nIndustrial-strengthwaveletanalysesdonotcomputeexplicitintegrals,butinsteadapply\natechniqueknownas multiresolutionanalysis (MRA)[Mallat,1989].Wegiveanexampleof\nthistechniqueinFigure10.9andinthecode DWT.pyinListing10.2.Itisbasedona pyramid\nalgorithmthatsamplesthesignalatafinitenumberoftimes,andthenpassesitsuccessively\nthroughanumberof filters,witheachfilterrepresentingadigitalversionofawavelet.\nFilters were discussed in Chapter 9, where in (9.60) we defined the action of a linear\nfilterasaconvolutionofthefilterresponsefunctionwiththesignal.Acomparisonofthe\ndefinition of a filter to the definition of a wavelet transform (10.14), shows that the two",4848
106-10.5.1 Pyramid Scheme.pdf,106-10.5.1 Pyramid Scheme,"10.5 Discrete Wavelet Transforms ‚äô205\nLL\nH HLL\nLH H\n22\n2\nData\ninput22\n22\nFigure 10.9 A eigenfrequency dyadic (power-of-2) Ô¨Ålter tree used for discrete wavelet\ntransformations. The L boxes represent lowpass Ô¨Ålters and the H boxes represent highpass Ô¨Ålters.\nEach Ô¨Ålter performs a convolution (transform). The circles containing ‚Äú ‚Üì2‚Äù Ô¨Ålter out half of the\nsignal that enters them, which is called subsampling orfactor-of-2 decimation . The signal on the left\nyields a transform with a single low and two high components (less information is needed about\nthe low components for a faithful reproduction).\nare essentially the same. Such being the case, the result of the transform operation is a\nweightedsumovertheinputsignalvalues,witheachweighttheproductoftheintegration\nweighttimesthevalue ofthe waveletfunctionattheintegrationpoint.Therefore, rather\nthantabulateexplicitwaveletfunctions,asetoffiltercoefficientsisallthatisneededforDWT .\nIn as much as each filter in Figure 10.9 changes the relative strengths of the different\nfrequencycomponents,passingthesignalthroughaseriesoffiltersisequivalent,inwavelet\nlanguage,toanalyzingthesignalatdifferentscales.Thisistheoriginofthename‚Äúmultires-\nolutionanalysis.‚ÄùFigure10.9showshowthepyramidalgorithmpassesthesignalthrough\na series of highpass filters (H) and then through a series of lowpass filters (L). Each fil-\nter changes the scale to that of the level below. Notice too, the circles containing ‚Üì2i n\nFigure 10.9. This operation filters out half of the signal and so is called subsampling or\nfactor-of-2decimation .ItisthewaywekeeptheareasofeachboxinFigure10.8constantas\nwevarythescaleandtranslationtimes.Weconsidersubsamplingfurtherwhenwediscuss\nthepyramidalgorithm.\nInsummary,theDWTprocessdecomposesthesignalinto smoothinformationstoredin\nthelow-frequencycomponentsand detailedinformationstoredinthehigh-frequencycom-\nponents.Because high-resolution reproductionsofsignalsrequiremoreinformationabout\ndetailsthanaboutgrossshape,thepyramidalgorithmisaneffectivewaytocompressdata\nwhilestillmaintaininghighresolution.Inaddition,becausecomponentsofdifferentreso-\nlutionsareindependentofeachother,itispossibletolowerthenumberofdatastoredby\nsystematicallyeliminatinghigher-resolutioncomponents,iftheyarenotneeded.Theuseof\nwaveletfiltersbuildsinprogressivescaling,whichisparticularlyappropriateforfractal-like\nreproductions.\n10.5.1 Pyramid Scheme ‚äô\nWenowimplementthepyramidschemeoutlinedinFigure10.9.The HandLfilterswill\nbe represented by matrices, which is an approximate way to perform the integrations or\nconvolutions. Then there is a decimation of the output by one-half, and finally an inter-\nleaving of the output for further filtering. This process simultaneously cuts down on the\nnumberofpointsinthedatasetandchangesthescaleandtheresolution.Thedecimation\n206 10 Wavelet and Principal Components Analysis\nN samples\nN/2\nd(1)\nCoefficientsN/2\nc(1)\nCoefficients\nN/4\nd(2)\nCoefficientsN/4\nc(2)\nCoefficients\nN/8\nc(3)\nCoefficientsN/8\nd(3)\nCoefficients\n2\nd(n)\nCoefficients2\nc(n)\nCoefficientsH\nH\nHH\nLLLLInput\nFigure 10.10 An input signal (top) is processed by a tree of high- and low-band Ô¨Ålters.\nThe outputs from each Ô¨Åltering are downshifted with half the data kept. The process continues\nuntil there are only two data of high-band Ô¨Åltering and two data of low-band Ô¨Åltering.\nreducesthenumberofvaluesoftheremainingsignalbyone-half,withthelow-frequency\npartdiscardedbecausethedetailsareinthehigh-frequencyparts.\nAsindicatedinFigure10.10,thepyramidDWTalgorithmfollowsfivesteps:\n1) Successively applies the (soon-to-be-derived) cmatrix (10.41) to the whole N-length\nvector,\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Y0\nY1\nY2\nY3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c1c2c3\nc3‚àíc2c1‚àíc0\nc2c3c0c1\nc1‚àíc0c3‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.31)\n2) Appliesittothe N‚àï2-lengthsmoothvector.\n3) Repeatstheapplicationuntilonlytwosmoothcomponentsremain.\n4) Aftereachfiltering,theelementsareordered,withthenewesttwosmoothelementson\ntop,thenewestdetailedelementsbelow,andtheolderdetailedelementsbelowthat.\n5) Theprocesscontinuesuntiltherearejusttwosmoothelementsleft.\n10.5 Discrete Wavelet Transforms ‚äô207\nToillustrate,herewefilterandreorderaninitialvectoroflength N=8:\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y1\ny2\ny3\ny4\ny5\ny6\ny7\ny8‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶filter‚Üí‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£s(1)\n1\nd(1)\n1\ns(1)\n2\nd(1)\n2\ns(1)\n3\nd(1)\n3\ns(1)\n4\nd(1)\n4‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶order‚Üí‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£s(1)\n1\ns(1)\n2\ns(1)\n3\ns(1)\n4\nd(1)\n1\nd(1)\n2\nd(1)\n3\nd(1)\n4‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶filter‚Üí‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£s(2)\n1\nd(2)\n1\ns(2)\n2\nd(2)\n2\nd(1)\n1\nd(1)\n2\nd(1)\n3\nd(1)\n4‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶order‚Üí‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£s(2)\n1\ns(2)\n2\nd(2)\n1\nd(2)\n2\nd(1)\n1\nd(1)\n2\nd(1)\n3\nd(1)\n4‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.32)\nThediscreteinversionofatransformvectorbacktoasignalvectorismadeusingthetrans-\npose(inverse)ofthetransfermatrixateachstage.Forinstance,\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c3c2c1\nc1‚àíc2c3‚àíc0\nc2c1c0c3\nc3‚àíc0c1‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Y0\nY1\nY2\nY3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.33)\nAs a more realistic example, imagine that we have sampled the chirp signal y(t)=\nsin(60t2)for 1024 times. The filtering process through which we place this signal is\nillustrated as a passage from the top to the bottom in Figure 10.10. First the original\n1024 samples are passed through a single low band and a single high band (which is\nmathematically equivalent to performing a series of convolutions). As indicated by the\ndown arrows, the output of the first stage is then downshifted, that is, the number is\nreducedbyafactorof2.Thisresultsin512pointsfromthehigh-bandfilteraswellas512\npointsfromthelow-bandfilter.Thisproducesthefirst-leveloutput.Theoutputcoefficients\nfromthehigh-bandfiltersarecalled {d(1)\ni}toindicatethattheyshowdetails,and {s(1)\ni}to\nindicatethattheyshowsmoothfeatures.Thesuperscriptindicatesthatthisisthefirstlevel\nofprocessing.Thedetailcoefficients {d(1)}arestoredtobecomepartofthefinaloutput.\nInthenextleveldown,the512smoothdata {s(1)\ni}arepassedthroughnewlow-andhigh-\nbandfiltersusingabroaderwavelet.The512outputsfromeacharedownshiftedtoform\nasmoothsequence {s(2)\ni}ofsize256andadetailedsequence {d(2)\ni}ofsize256.Againthe\ndetailcoefficients {d(2)}arestoredtobecomepartofthefinaloutput.(Notethatthisisonly\nhalf the size of the previously stored details.) The process continues until there are only\ntwo numbers left for the detail coefficients and two numbers left for the smooth coeffi-\ncients.Becausethislastfilteringiscarriedoutwiththebroadestwavelet,itisofthelowest\nresolutionandthereforerequirestheleastinformation.\nIn Figure 10.11, we show the actual effects on the chirp signal of pyramid filtering for\nvariouslevelsintheprocessing.(Theprocessingiscarriedoutwith Daub4wavelets,which\nwe will discuss soon.) At the uppermost level, the wavelet is narrow, and so convoluting\nthiswaveletwithsuccessivesectionsofthesignalresultsinsmoothcomponentsthatstill\n208 10 Wavelet and Principal Components Analysis\n‚Äì4‚Äì2024\n0 10 20 30 40 50 60 70 90 110 130‚Äì 1.01.0\n00 20 40 60 80 100 120 140 180 220\n2\n0\n‚Äì24\n0\n‚Äì4\n0 15 3035 45 55 65\n0 4 8 12 168\n4\n‚Äì40\n‚Äì8\n012\n0\n2\n‚Äì202\n34024\n024‚Äì2024\n56 7 810 12 1402\n‚Äì26\n4\n2 6 8 4\n164\n0\n‚Äì4\n‚Äì88\n4\n0\n‚Äì4\n‚Äì8\n16 20 24 28 320.2\n0\n‚Äì0.23\n1\n0\n‚Äì1\n‚Äì32\n0\n‚Äì2\n0 50 100 150 250 2000.04\n0.02\n‚Äì0.02\n‚Äì0.04\n300 350 400 450 500600 700 800 900 10000.06\n0.04\n0.02\n01\n0\n‚Äì1\n0 100 200 300 400 5000.8\n0.4\n0\n‚Äì0.4\n‚Äì0.8\n0 0.2 0.4 0.6 0.8 1.0\nFigure 10.11 In successive passes, the Ô¨Åltering of the original signal at the top goes through the\npyramid algorithm and produces the outputs shown. The sampling is reduced by a factor of 2 in\neach step. Note that in the upper graphs, we have connected the points to emphasize their\ncontinuous nature while in the lower graphs, we plot the individual output points as histograms.",8192
107-10.5.2 Daubechies Wavelets Filters.pdf,107-10.5.2 Daubechies Wavelets Filters,"10.5 Discrete Wavelet Transforms ‚äô209\ncontain many large high-frequency parts. The detail components, in contrast, are much\nsmallerinmagnitude.Inthenextstage,thewaveletisdilatedtoalowerfrequency,andthe\nanalysisisrepeated onjustthesmooth(low-band)part .Theresultingoutputissimilar,but\nwithcoarserfeaturesforthesmoothcoefficientsandlargervaluesforthedetails.Notethat\nin the upper graphs we have connected the points to make the output look continuous,\nwhileinthelowergraphs,withfewerpoints,wehaveplottedtheoutputashistogramsto\nmake the points more evident. Eventually the downshiftingleads to just two coefficients\noutputfromeachfilter,atwhichpointthefilteringends.\nToreconstructtheoriginalsignal(called synthesisortransformation )areversedprocess\nis followed: Begin with the last sequence of four coefficients, upsample them, pass them\nthroughlow-andhigh-bandfilterstoobtainnewlevelsofcoefficients,andrepeatuntilall\ntheNvalues of the original signal are recovered. The inverse scheme is the same as the\nprocessingscheme(Figure10.10),onlynowthedirectionofallthearrowsisreversed.\n10.5.2 Daubechies Wavelets Filters ‚äô\nWeshouldnowbeabletounderstandthatdigitalwaveletanalysishasbeenstandardized\nto the point where classes of wavelet basis functions are specified not by their analytic\nforms, but rather by their wavelet filter coefficients . In 1988, the Belgian mathematician\nIngrid Daubechies discovered an important class of such filter coefficients [Daubechies,\n1995;RoweandAbbott,1995].WewillstudyjusttheDaub4classcontainingthefourcoef-\nficientsc0,c1,c2,andc3.\nImaginethatourinputcontainsthefourelements {y1,y2,y3,y4}correspondingtomea-\nsurementsofasignalatfourtimes.Werepresentalowpassfilter Landahighpassfilter H\nintermsofthefourfiltercoefficientsas\nL=[\nc0+c1c2+c3], (10.34)\nH=[\nc3‚àíc2c1‚àíc0]. (10.35)\nTo see how this works, we form an input vector by placing the four signal elements in a\ncolumnandthenmultiplytheinputby LandH:\nL‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=[\nc0c1c2c3]‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=c0y0+c1y1+c2y2+c3y3,\nH‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=[\nc3‚àíc2c1‚àíc0]‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=c3y0‚àíc2y1+c1y2‚àíc0y3.\nWeseethatifwechoosethevaluesofthe ci‚Äôscarefully,theresultof Lactingonthesignal\nvectorisasinglenumberthatmaybeviewedasaweightedaverageofthefourinputsignal\nelements.Sinceanaveragingprocesstendstosmoothoutdata,thelowpassfiltermaybe\nthoughtofasa smoothingfilter thatoutputsthegeneralshapeofthesignal.\nInturn,weseethatifwechoosethe civaluescarefully,theresultof Hactingonthesignal\nvectorisasinglenumberthatmaybeviewedastheweighteddifferencesoftheinputsignal.\n210 10 Wavelet and Principal Components Analysis\nBecauseadifferencingprocesstendstoemphasizethevariationinthedata,thehighpass\nfiltermaybethoughtofasa detailfilterthatproducesalargeoutputwhenthesignalvaries\nconsiderably,andasmalloutputwhenthesignalissmooth.\nWehavejustseenhowtheindividual LandHfilters,eachrepresentedbyasinglerow\nofthefiltermatrix,outputsonenumberwhenactinguponaninputsignalcontainingfour\nelementsinacolumn.Ifwewanttheoutputofthefilteringprocess Ytocontainthesame\nnumber of elements as the input (four y‚Äôs in this case), we just stack the LandHfilters\ntogether:\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Y0\nY1\nY2\nY3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£L\nH\nL\nH‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c1c2c3\nc3‚àíc2c1‚àíc0\nc2c3c0c1\nc1‚àíc0c3‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.36)\nOfcoursethefirstandthirdrowsofthe Yvectorwillbeidentical,aswillthesecondand\nfourth,butwewillgettothatsoon.\nNow wego aboutdeterminingthevaluesofthe filtercoefficients cibyplacingspecific\ndemandsupontheoutputofthefilter.Westartbyrecallingthatinourdiscussionofdis-\ncreteFouriertransformsweobservedthatatransformisequivalenttoarotationfromthe\ntimedomaintothefrequencydomain.Yetweknowfromourstudyoflinearalgebrathat\nrotationsaredescribedbyorthogonalmatrices,thatis,matriceswhoseinversesareequal\ntotheirtransposes.Inorderfortheinversetransformtoreturnustotheinputsignal,the\ntransfermatrixmustbeorthogonal.Forourwavelettransformationtobeorthogonal,we\nmusthavethe4 √ó4filtermatrixtimesitstransposeequaltotheidentitymatrix:\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c1c2c3\nc3‚àíc2c1‚àíc0\nc2c3c0c1\nc1‚àíc0c3‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c3c2c1\nc1‚àíc2c3‚àíc0\nc2c1c0c3\nc3‚àíc0c1‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1000\n0100\n0010\n0001‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,\n‚áíc2\n0+c2\n1+c2\n2+c2\n3=1,c2c0+c3c1=0. (10.37)\nTwoequationsinfourunknownsarenotenoughforauniquesolution,sowenowinclude\nthe further requirement that the detail filter H=(c3,‚àíc0,c1,‚àíc2)must output a zero if\nthe input is smooth. We define ‚Äúsmooth‚Äù to mean that the input is constant or linearly\nincreasing:\n[\ny0y1y2y3]=[\n1111]or[\n0123]. (10.38)\nThisisequivalenttodemandingthatthemomentsuptoorder parezero,thatis,thatwe\nhavean‚Äúapproximationoforder p.‚ÄùExplicitly,\nH[\ny0y1y2y3]=H[\n1111]=H[\n0123]=0,\n‚áíc3‚àíc2+c1‚àíc0=0,0√óc3‚àí1√óc2+2√óc1‚àí3√óc0=0,\n‚áíc0=1+‚àö\n3\n4‚àö\n2‚âÉ0.483,c1=3+‚àö\n3\n4‚àö\n2‚âÉ0.836, (10.39)\n10.5 Discrete Wavelet Transforms ‚äô211\nc2=3‚àí‚àö\n3\n4‚àö\n2‚âÉ0.224,c3=1‚àí‚àö\n3\n4‚àö\n2‚âÉ‚àí0.129. (10.40)\nThesearethebasicDaub4filtercoefficients.Theyareusedtocreatelargerfiltermatrices\nbyplacingtherowversionsof LandHalongthediagonal,withsuccessivepairsdisplaced\ntwocolumnstotheright.Forexample,foreightelements,\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£Y0\nY1\nY2\nY3\nY4\nY5\nY6\nY7‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£c0c1c2c30000\nc3‚àíc2c1‚àíc00000\n00c0c1c2c300\n00c3‚àíc2c1‚àíc000\n0000 c0c1c2c3\n0000 c3‚àíc2c1‚àíc0\nc2c30000 c0c1\nc1‚àíc00000 c3‚àíc2‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£y0\ny1\ny2\ny3\ny4\ny5\ny6\ny7‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.41)\nNote that in order not to lose any information,the last pair on the bottom two rows is\nwrappedovertotheleft.Ifyouperformtheactualmultiplicationsindicatedin(10.41),you\nwill note that the output has successive smoothanddetailedinformation. The output is\nprocessedwiththepyramidscheme.\nThetimedependenciesoftwoDaub4waveletsaredisplayedinFigure10.12.Toobtain\nthesefromourfiltercoefficients,firstimaginethatanelementarywavelet y1,1(t)‚â°ùúì1,1(t)\nis input into the filter. This should result in a transform Y1,1=1. Inversely, we obtain\ny1,1(t)byapplyingtheinversetransformtoa Yvectorwitha1inthefirstpositionandzeros\ninalltheotherpositions.Likewise,the ithmemberoftheDaubechiesclassisobtainedby\napplyingtheinversetransformtoa Yvectorwitha1inthe ithpositionandzerosinallthe\notherpositions.\nOntheleftinFigure10.12isthewaveletforcoefficient6(thusthee6notation).Onthe\nrightinFigure10.12isthesumoftwowaveletscorrespondingtothecoefficients10and58.\nWeseethatthetwowaveletshavedifferentlevelsofscaleaswellasdifferenttimepositions.\n‚Äì0.1‚Äì0.06‚Äì0.020.020.060.1\n0 400 800 1200‚Äì0.3‚Äì0.10.10.3\n0 400 800 1200\nFigure 10.12 Left: The Daub4 e6 wavelet constructed by inverse transformation of the wavelet\ncoefÔ¨Åcients. This wavelet has been found to be particularly effective in wavelet analyses.\nRight: The sum of Daub4 e10 and Daub4 1e58 wavelets of different scale and time displacements.",7181
108-10.6.2 Wonders of the Covariance Matrix.pdf,108-10.6.2 Wonders of the Covariance Matrix,"212 10 Wavelet and Principal Components Analysis\nSodespitethefactthatthetimedependenceofthewaveletsisnotevidentwhenwavelet\n(filter)coefficientsareused,itisthere.\n10.5.3 DWT Exercise ‚äô\nListing10.2givesourprogramforperformingaDWTonthechirpsignal y(t)=sin(60t2).\nThemethod pyrancallsthe daube4methodtoperformtheDWTorinverseDWT,depending\nuponthevalueof sign.\n1) Modifytheprogramsothatyououtputtoafilethevaluesfortheinputsignalthatyour\ncodehasreadin.Itisalwaysimportanttocheckyourinput.\n2) TrytoreproducetheleftofFigure10.11byusingvariousvaluesforthevariable nendthat\ncontrolswhenthefilteringends.Avalue nend=1024shouldproducejustthefirststepin\nthedownsampling(toprowinFigure11.10).Selecting nend=512shouldproducethenext\nrow,while nend=4shouldoutputjusttwosmoothanddetailedcoefficients.\n3) Reproduce the scale-time diagram shown on the right in Figure 10.11. This diagram\nshows the output at different scales and serves to interpret the main components of\nthesignalandthetimeinwhichtheyappear.Thetimelineatthebottomofthefigure\ncorresponds to a signal of length 1 over which 256 samples were recorded. The low-\nband(smooth)componentsareshownontheleft,andthehigh-bandcomponentson\ntheright.\na) Thebottommostfigureresultswhen nend=256.\nb) Thefigureinthesecondrowupresultsfrom end=128,andwehavetheoutputfrom\ntwofilterings.Theoutputcontains256coefficientsbutdividestimeintofourintervals\nandshowsthefrequencycomponentsoftheoriginalsignalinmoredetail.\nc) Continuewiththesubdivisionsfor end=64,32,16,8,and4.\n4) Foreachofthesechoicesexceptthetopmost,dividethetimeby2andseparatetheinter-\nvalsbyverticallines.\n5) Thetopmostspectrumisyourfinaloutput.Canyouseeanyrelationbetweenitandthe\nchirpsignal?\n6) Changethesignof signandcheckthattheinverseDWTreproducestheoriginalsignal.\n7) Use the code to visualize the time dependence of the Daubechies mother function at\ndifferentscales.\na) Start by performing an inverse transformation on the eight-component signal\n[0,0,0,0,1,0,0,0] .Thisshouldyieldafunctionwithawidthofabout5units.\nb) Next perform an inverse transformation on a unit vector with N=32 but with all\ncomponentsexceptthefifthequaltozero.Thewidthshouldnowbeabout25units,\nalargerscalebutstillcoveringthesametimeinterval.\nc) Continuethisprocedureuntilyouobtainwaveletsof800units.\nd) Finally,with N=1024,selectaportionofthemotherwaveletwithdatainthehor-\nizontal interval [590,800]. This should show self-similarity similar to that at the\nbottomofFigure10.12.\n10.6 Part II: Principal Components Analysis 213\n10.6 Part II: Principal Components Analysis\nProblem Given a dataset describing several properties of irises (flowers), separate the\ndataintogroupsinorderofimportance.Theproperties(incm)are\n[‚Äôsepal length‚Äô ,‚Äôsepal width‚Äô ,‚Äôpetal length‚Äô ,‚Äôpetal width‚Äô ]\narray( [5.1, 3.5, 1.4, 0.2],\n3 [4.9, 3.0, 1.4, 0.2],\n[4.7, 3.2, 1.3, 0.2],\n[4.6, 3.1, 1.5, 0.2],\n[5.0, 3.6, 1.4, 0.2],\n7 [5.4, 3.9, 1.7, 0.4],\n[4.6, 3.4, 1.4, 0.3],\n[5.0, 3.4, 1.5, 0.2],\n[4.4, 2.9, 1.4, 0.2],\n11 [4.9, 3.1, 1.5, 0.1],\n[5.4, 3.7, 1.5, 0.2],\n[4.8, 3.4, 1.6, 0.2],\n[4.8, 3.0, 1.4, 0.1],\n15 [4.3, 3.0, 1.4, 0.1],\n[5.8, 4.0, 1.2, 0.2],\n[5.7, 4.4, 1.5, 0.4],\n[5.4, 3.9, 1.3, 0.4],\n19 [5.1, 3.5, 1.4, 0.3] )\nWehaveindicatedthatashortcomingofFourieranalysisisthatitusesaninfinitenumber\nofcomponents,thatallofthesecomponentsarecorrelated,andthusarenotindependent.\nConsequently, truncation of some of the components leads to difficulties in compression\nand reconstitution of the input signal. Wavelet analysis, on the other hand, is excellent\nat data compression, but not appropriate for high-dimensionality data sets, or for non-\ntemporalsignals. PrincipalComponentsAnalysis(PCA) isapowerfulanalysistoolthatuses\nstatisticstoprovideinsightintosignalsthatmaybecontainedwithina high-dimensionality ,\nmultivariate dataset. Examples of high dimensionally data include stellar spectra, brain\nwaves,facialpatterns,andoceancurrents.Inthesecasestheremaybehundredsofdetec-\ntorsinspace,eachofwhichrecordsseveraltypesofsignalsforweeksonend.Furthermore,\nthesekindsofdataareoftennoisy,andpossiblyredundant(differentdetectorsrecording\ncorrelatedsignals),andsoastatisticalapproachseemsappropriate.\nVariations of the PCA approach are used in many fields, where it goes by names\nsuch as the Kronen-Lo√®vet transform, the Hostelling transform, the proper orthogonal\ndecomposition, singular value decomposition, factor analysis, empirical orthogonal\nfunctions,empiricalcomponentanalysis,andempiricalmodalanalysis[Wikipedia,2014].\nThe approach combines statistics with transformation theory, the latter familiar from\nlinear algebra, to rotate from the basis vectors used to collect the data into new basis\nvectorsknownas principalcomponents thatlieinthedirectionofmaximalsignalstrength\n(‚Äúpower‚Äù)inthedataspace.Thisisanalogoustotheprincipalaxistheoremofmechanics\nin which the description of solid object rotations is greatly simplified when moments of\ninertiarelativetotheprincipalaxesareused.OurreferencesJackson[1991],Jolliffe[2002],\nSmith[2002],andShlens[2003]tendtoviewPCAas unsuperviseddimensionalityreduction ,\n214 10 Wavelet and Principal Components Analysis\nwhere ‚Äúunsupervised‚Äù refers to the absence of labels on the data, and ‚Äúreduction‚Äù to the\nsmall number of principal components that ultimately result. We prefer to view PCA as\nthewaytoextractthedominantdynamicscontainedincomplexdatasets.\n10.6.1 Multi-dimensional Data Space\nIt‚Äôs often helpful when dealingwith complex data to imagine an abstract vector space in\nwhichthedataelementslie.Itisinthismulti-dimensional dataspacethatthePCAbasis\nvectors lie. As a simple example, consider the four detectors in Figure 10.13 observing a\nbeam of particles passing by. Each detector records its observations over time, with the\nmeasurementsateachtimeconsideredaseparate,individualsample.Furthermore,each\ndetectormayrecordasetof Mobservables,suchasposition,angle,intensity,thicknessof\natrack,lengthofatrack, etc.ThisproducesanM-dimensionaldataspace ÓàæM.Specifically,\nlet‚Äôssayateachinstantoftime,detectorArecordstheposition (x‚Ä≤\nA,y‚Ä≤\nA),andsoonfordetec-\ntorsB‚ÄìD.Thesampleofspatialdataatthatoneinstantoftimeisthenrepresentedasan\n8-Dvector:\nX‚Ä≤=[x‚Ä≤\nay‚Ä≤\nax‚Ä≤\nby‚Ä≤\nbx‚Ä≤\ncy‚Ä≤\ncx‚Ä≤\ndy‚Ä≤\nd]. (10.42)\nTogetanideaofthesizesofthedataspaceswithwhichwemightbedealing,ifthedetectors\nmaketheirrecordingsfor18minutes(1080seconds)at110Hz,then1080 √ó110=118800\nofthesevectorsarecreatedin ÓàæM.Andthisisasmallproblem.\nExperimentaldatausuallycontainnoiseinadditiontosignalsofinterest.The variance\nùúé2(z)inadatasetof Npointsisameasureofthedispersionofthedatumpointsfromtheir\nmeanz:\nz=1\nNN‚àë\nizi, (10.43)\nùúé2(z)‚â°Var(z)def=1\nN‚àí1N‚àë\ni(zi‚àíz)2. (10.44)\nIfthedataareofhighprecision,thesignalwouldbemuchlargerthanthenoise.Inprac-\ntice,measurementscontainrandomandsystematicerrors,andthereforethesignal-to-noise\nratio(SNR),\nSNR=ùúé2\nsignal\nùúé2\nnoise, (10.45)\nmaynotbelarge.PCAisagoodwaytodealwithsmallSNR.OntheleftofFigure10.14,we\npresentsomemade-up2Ddata (xA,yA)fromdetectorAshowingthedirectionofmaximum\nsignalvariance ùúé2\nsignalalongPC1,andthedirectionofmaximumnoise(orsecondarysignal)\nDetector A Detector B\nDetector C Detector DFigure 10.13 A beam of particles\nbeing observed by four detectors.\n10.6 Part II: Principal Components Analysis 215\nPC1\nPC2PC2ùïΩMùïΩk\nPC1yA\nXASignal\nNoiseœÉ2N œÉ2S\nFigure 10.14 Left: Samples of 2D data (xA,yA)from a detector A showing the direction of\nmaximum signal variance ùúé2\nSalong the principal component PC1basis, and the direction of noise\nvariance ùúé2\nNalong the secondary PC2basis. Right: The same data projected onto the principal\ncomponent axes. (Based on [Shlens, 2003].)\nvariance ùúé2\nnoisealongPC2.Althoughatraditionalviewofstatisticsmaybethatalargevari-\nanceindicateshighnoise,inthePCAviewalargevarianceindicatesthatsomeinteresting\ndynamicsmaybeoccurringinthatdirectionindataspace.\nThekeyPCAassumptionisthatthedirectionwiththelargestvariancecontainsmostof\nthedynamicsofinterest,andthatthedeviationofthedatafrommaximumvariancemay\nbeduetonoise,ormaybesomelessimportantdynamics.The PC1andPC2directionsin\nFigure10.14arethetwoPCAbasisvectors,andarechosentomaximizetheSNRmeasured\nalongPC1,relativetothatalong PC2(we‚Äôllgettohowthesearedeterminedshortly).On\ntherightofthefigureweshowthesamedataprojectedalongthe PC1andPC2axes.Here\nÓàækisalower-dimensionalspacecontainingthe korthonormalanduncorrelatedprincipal\ncomponentsvectors.\n10.6.2 Wonders of the Covariance Matrix\nWe have just seen graphically how PCA isolates the signal from the noise for a 2D\ndataset such as (x‚Ä≤\nA,y‚Ä≤\nA). However, our sample problem has four detectors, and thus a\nhigher-dimensional space to analyze. We generalize one step at a time by extending the\napproachtoalsoincludedetectorB.Wedefinetwodatasets AandB,eachcenteredabout\ntheirmeans, x,y,andexpressedastherowvectors:\nA=[a1(xa,1,ya,1)a2(xa,2,ya,2)‚Ä¶aN(xa,N,ya,N)], (10.46)\nB=[b1(xb,1,yb,1)b2(xb,2,yb,2)‚Ä¶bN(xb,N,yb,N)], (10.47)\nxa,i=x‚Ä≤\na,i‚àíx‚Ä≤,ya,i=y‚Ä≤\na,i‚àíy‚Ä≤. (10.48)\nNextwecomputethevariances(10.44)foreachofthecentered( a=b=0)sets:\nùúé2\nA=1\nN‚àí1N‚àë\nia2\ni,ùúé2\nB=1\nN‚àí1N‚àë\nib2\ni. (10.49)\nThevarianceconceptisextendedto covariance,asameasureofthecorrelationbetweenthe\ncentereddatain AandB:\ncov(A,B)def=ùúé2\nABdef=1\nN‚àí1N‚àë\niaibi. (10.50)\n216 10 Wavelet and Principal Components Analysis\nApositivecovarianceindicatesthatsignalswithin AandBtendtochangetogetherandin\nthe same direction. A large covariance indicates a high correlation or redundancy, while\nazerovalueimpliesnocorrelation.Notethatthevariance(10.44)canbeviewedasaspe-\ncial case of the covariance, var (x)=cov(x,x), and that there is the symmetry cov (x,y)=\ncov(y,x).Alloftheseconceptsarecombinedintothesymmetriccovariancematrix:\nCAB=[\ncov(A,A)cov(A,B)\ncov(B,A)cov(B,B)]\n. (10.51)\nTheseideasgeneralizedirectlytohigherdimensions.StartwiththesetsA(10.46)andB\n(10.47),whereweremindyouthattheelementsmaycontainanumberofmeasurements.\nThecovariancematrixcanbewrittenasthevectordirectproduct(akadotproduct,matrix\nmultiplication,ordyadic):\nCABdef=1\nN‚àí1AB‚â°1\nN‚àí1A‚äóBT. (10.52)\nWiththisnotation,wecangeneralizetohigherdimensionsbydefiningnew rowsubvectors\ncontainingthedatafromeachofthe Mdetectors:\nx1=A,x2=B,‚Ä¶,xM=M. (10.53)\nWecombinetheserowvectorsintoanextended M√óNdatamatrix:\nX=‚é°\n‚é¢\n‚é¢\n‚é¢‚é£x1\n...\nxM‚é§\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£‚áìAll ‚áíAllAmeasurements\n‚áìone ‚áíAllBmeasurements\n‚áìtime ‚áíAllCmeasurements\n‚áìmeasurements ‚áíAllDmeasurements‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (10.54)\nEachrowofthismatrixcontainsallofthemeasurementsfromaparticulardetector,while\neachcolumncontainsallofthemeasurementsforaparticulartime.Withthisnotation(and\nx=0),thecovariancematrixcanbewrittenintheconciseform\nC=1\nN‚àí1XXT. (10.55)\nThis can be thought of as a generalization of the familiar dot product of two 2D vectors,\nx‚ãÖx=xTx,asameasureoftheiroverlap.\nInsummary:\n‚óèThecovariantmatrix Cijisthedotproductofthecenteredmeasurementsvectorfromthe\nithdetector( i=A,B,‚Ä¶)withthecenteredmeasurementsvectorfromthe jthdetector.\n‚óèForanytwovariablesinthedata, Cisasquaresymmetricmatrixmeasuringtherelation-\nshipbetweenthosevariables.\n‚óèThe diagonal elements of Care the variances in the measurements from individual\ndetectors.\n‚óèThe off-diagonal elements of Care the covariances between the measurements from\ndifferentdetectors,thatis,thecorrelationsbetweendetectors.\nSteps in a Principal Component Analysis (Easy with NumPy)\n1) As indicated in Figure 10.14, there is the assumption that the direction in which\nthe variance is largest indicates the ‚Äúprincipal‚Äù component in the data, PC1orp1.\n10.6 Part II: Principal Components Analysis 217\nTable 10.1 PCA demonstration data.\nData Adjusted data In PCA basis\nxyx y x1x2\n2.5 2.4 0.69 0.49 ‚àí0.828 ‚àí0.175\n0.5 0.7 ‚àí1.31 ‚àí1.21 1.78 0.143\n2.2 2.9 0.39 0.99 ‚àí0.992 0.484\n1.9 2.2 0.09 0.29 ‚àí0.274 0.130\n3.1 3.0 1.29 1.09 ‚àí1.68 ‚àí0.209\n2.3 2.7 0.49 0.79 0.913 0.175\n2 1.6 0.19 ‚àí0.31 0.0991 ‚àí0.350\n1.0 1.1 ‚àí0.81 ‚àí0.81 1.14 0.464\n1.6 1.6 ‚àí0.31 ‚àí0.31 0.438 0.0178\n1.1 0.9 ‚àí0.71 ‚àí1.01 1.22 ‚àí0.163\nConsequently,PCAsearchesforthedirectionindataspaceforwhichthevarianceof X\nismaximized.\n2) Once p1hasbeenfound,thebasisvector p2ischosenastheorthonormalto p1.\n3) The process is repeated until there are Morthonormalbasis vectors. These are the M\nprincipalcomponentsofthedata.\n4) The eigenvectors and eigenvalues are ordered according to their corresponding\nvariances.\n5) Explicitly,startingwiththe M√óNdatamatrix X,amatrix Pisdeterminedsuchthat\nCy=1\nN‚àí1YYT=diagonal,where Y=PX. (10.56)\n6) The rows of Pare the principalcomponentbasis vectors (same as the eigenvectorsof\nXXT).\n7) Thediagonalelementsof CYarethevariancesof Xalongthecorresponding pi‚Äôs.\nFigure 10.15 Sample data used in our demonstration\nPCA analysis.\nxy\n++\n++\n+\n++\n+\n+++\n001234\n12 3 4",12809
109-10.7 Code Listings.pdf,109-10.7 Code Listings,"218 10 Wavelet and Principal Components Analysis\n‚Äì2‚Äì1012\n‚Äì2 ‚Äì1 0 1 2PC1PC2\nXy\n‚Äì2‚Äì1012\n‚Äì2 ‚Äì1 0 1 2\nx1x2\nFigure 10.16 Left: The PCA basis vectors (eigenvectors of cov (x,y)).Right: The normalized data\nusing the PCA eigenvectors as basis.\n10.6.3 Demonstration of Principal Component Analysis\nWe‚ÄôllleavetheanalysisoftheirisesdataatthebeginningofPartIIas yourproblem,and,\ninstead,we‚ÄôllanalyzethesimplerdatainTable10.1[Smith,2002].Thesedata,areshown\ninFigure10.15as xversusy,buttheydon‚Äôthavetobespatial.Alsoshownistheanalysisof\nthesedataintermsofthefirsttwoprincipalcomponents.Asexpected,thefirsteigenvector\npointsinthedirectionwiththelargestvariance,whilethenextvectorisorthogonaltothe\nfirst.Thereclearlyislessvariancealong PC2and,consequently,lessdynamicalimportance\nofthatcomponent.\nHerearethestepsintheanalysis:\n1)Enter data as an array :ThefirsttwocolumnsinTable10.1.\n2)Subtract the mean :PCAanalysisassumesthatthedataineachdimensionhaszero\nmean.Accordingly,asshownincolumnstwoandthreeinTable10.1,wecalculatedthe\nmeanforeachcolumn,( x,y),andsubtractedthemfromthedata.Theresultingadjusted\ndataaregiveninthethirdandfourthcolumnsofthetable.\n3)Calculate the covariance matrix :\nvar(x)=1\nN‚àí1N‚àë\ni=1(xi‚àíx)2, (10.57)\ncov(x,y)=1\nN‚àí1N‚àë\ni=1(xi‚àíx)(yi‚àíy), (10.58)\nC=[cov(x,x)cov(x,y)\ncov(y,x)cov(y,y)]\n=[0.6166 0.6154\n0.6154 0.7166]\n. (10.59)\n4)Compute unit eigenvector and eigenvalues of C(easy with NumPy) :\nùúÜ1=1.284,ùúÜ2=0.4908, (10.60)\nPC1=[‚àí0.6779\n‚àí0.7352]\n, PC2=[‚àí0.7352\n0.6789]\n, (10.61)\n10.6 Part II: Principal Components Analysis 219\nwhere we have ordered the eigenvalues and eigenvectors. The eigenvector with the\nlargest eigenvalue is the principal component in the data, typically with ‚àº80% of the\npowerinthesignal.\nIn Figure 10.16 we show the translated data and the two PCA eigenvectors of the\ncovariancematrix(scaledtofilltheframe).Noticethat PC1,whichpointsindirection\nofthemajorvariationinthedata,isessentiallyastraight-linefittothedata.The PC2\neigenvector is clearly orthogonal to PC1, and contains less signal strength. This is as\nshouldbe.\n5)Express the data in terms of principal Components :Wenextexpressedthedatain\ntermsoftheirtwoprincipalcomponentsbyformingtwo featurematrices :\nF1=[‚àí0.6779\n‚àí0.7352]\n,F2=[‚àí0.6779‚àí0.7352\n‚àí0.7352 0.6779]\n. (10.62)\nHereF1keeps just the major principal component, while F2keeps the first two. The\nmatrix gets its name because it focuses on which features of the data are being kept.\nNext we formed FT\n2, the transpose of the feature matrix, and XT, the transpose of the\ntranslateddatamatrix X:\nFT\n2=[‚àí0.6779‚àí0.7352\n‚àí0.7352 0.6779]\n, (10.63)\nXT=[0.69‚àí1.31 0.39 0.09 1.29 0.49 0.19 ‚àí0.81‚àí0.31‚àí0.71\n0.49‚àí1.21 0.99 0.29 1.09 0.79 ‚àí0.31‚àí0.81‚àí0.31‚àí1.01]\n.\nWeexpressedthedataintermsoftheseprincipalcomponentsbymultiplying FT\n2andX\ntogether:\nXPCA=FT\n2√óXT(10.64)\n=[‚àí0.6779‚àí0.7352\n‚àí0.7352 0.6779]\n√ó[0.69‚àí1.31 0.39 0.09 1.29 0.49 0.19 ‚àí0.81‚àí0.31‚àí0.71\n0.49‚àí1.21 0.99 0.29 1.09 0.79 ‚àí0.31‚àí0.81‚àí0.31‚àí1.01]\n=[0.828 1.78 ‚àí0.992‚àí0.274‚àí1.68‚àí0.913 0.0991 1.15 0.438 1.22\n‚àí0.175 0.143 0.384 0.130 ‚àí0.209 0.175 ‚àí0.350 0.464 0.178 ‚àí0.162]\n.\nOntherightofTable10.1,thedataareplottedusingthe PC1andPC2bases.Theplot\nshowswhereeachdatumpointsitsrelativetothetrendinthedata.Ifwehadplotted\nonlythefirstprincipalcomponent,allofthedatawouldfallonastraightline.\n10.6.4 PCA Exercises\n1) UsejusttheprincipaleigenvectorstoperformthePCAanalysisjustcompletedwithtwo\neigenvectors.\n2) Store data from 10 cycles of the chaotic pendulum studied in Chapter 8, but do not\nincludetransients.PerformaPCAofthesedataandplottheresultsusingprincipalcom-\nponentaxes.\n220 10 Wavelet and Principal Components Analysis\n10.7 Code Listings\nListing 10.1 CWT.py UsesMorletwaveletstocomputethecontinuouswavelettransform\nofthesumofsinefunctions.(CourtesyofZ.Diabolic.)\n1# C W T.py Continuous Wavelet TF. Based on program by Zlatko Dimcovic\nimportmatplotlib.pylab as p;\nfrommpl_toolkits.mplot3d importAxes3D ;\n5fromvisual.graph import ‚àó;\noriginalsignal=gdisplay(x=0, y=0, width=600, height=200, \\ntitle= ‚ÄôInput Signal‚Äô ,xmin=0,xmax=12,ymin= ‚àí20,ymax=20)\n9orsigraph=gcurve(color=color.yellow)\ninvtrgr = gdisplay(x=0, y=200, width=600, height=200,\ntitle= ‚ÄôInverted Transform‚Äô ,xmin=0,xmax=12,ymin= ‚àí20,ymax=20)\ninvtr = gcurve(x= list(range(0,240)), display = invtrgr , color= color.green)\n13iT = 0.0; fT = 12.0; W= fT ‚àíiT;\nN = 240; h = W/N # Need ‚àóvery ‚àósmall s for high f\nnoPtsSig = N; noS = 20; noTau = 90;\niTau = 0.; iS = 0.1; tau = iTau; s = iS\n17dTau = W/noTau; dS = (W/iS) ‚àó‚àó(1./noS);\nmaxY = 0.001; sig = zeros((noPtsSig), float) # Signal\ndefsignal(noPtsSig, y): # Signal function\n21t = 0.0; hs =W/noPtsSig; t1 =W/6.; t2 = 4. ‚àóW/6.\nforiin range (0, noPtsSig):\nift> =i T andt<=t 1 :y [ i ] =s i n ( 2 ‚àópi‚àót)\nelift> =t 1 andt<=t 2 : y [ i ] = 5 . ‚àósin(2 ‚àópi‚àót) + 10. ‚àósin(4 ‚àópi‚àót);\n25 elift> =t 2 andt<=f T :\ny[i] = 2.5 ‚àósin(2 ‚àópi‚àót) + 6. ‚àósin(4 ‚àópi‚àót) + 10. ‚àósin(6 ‚àópi‚àót)\nelse:\nprint(""In signal(...) : t out of range."" )\n29 sys.exit(1)\nyy=y[i]\norsigraph.plot(pos=(t,yy))\nt+ =h s\n33signal(noPtsSig, sig) # Form signal\nYn = zeros( (noS+1, noTau+1), float) # Transform\ndefmorlet(t, s, tau): #M o t h e r\nT= (t ‚àítau)/s\n37 returnsin(8 ‚àóT)‚àóexp(‚àíT‚àóT/2. )\ndeftransform(s, tau, sig): # Find wavelet TF\nintegral = 0.\nt=i T ;\n41foriin range (0,len(sig) ):\nt+ =h\nintegral += sig[i] ‚àómorlet(t, s, tau) ‚àóh\nreturnintegral / sqrt(s)\n45definvTransform(t, Yn): # Compute inverse\ns=i S # Transform\ntau = iTau\nrecSig_t = 0\n49foriin range (0, noS):\ns‚àó=d S # Scale graph\ntau = iTau\nforjin range (0, noTau):\n53 tau += dTau\nrecSig_t += dTau ‚àódS‚àó(s‚àó‚àó(‚àí1.5)) ‚àóYn[i , j ] ‚àómorlet(t,s,tau)\nreturnrecSig_t\nprint(""working, finding transform, count 20"" )\n57foriin range ( 0, noS):\ns‚àó=d S # Scaling\ntau = iT\nprint(i)\n61forjin range (0, noTau):\ntau += dTau # Translate\nYn[i, j] = transform(s, tau, sig)\n10.7 Code Listings 221\nprint(""transform found"" )\n65foriin range ( 0, noS):\nforjin range ( 0, noTau):\nifYn[i , j ] > maxY orYn[i , j ] <‚àí1‚àómaxY :\nmaxY =abs(Y n [ i ,j ]) #F i n dm a xY\n69tau = iT\ns= i S\nprint(""normalize"" )\nforiin range ( 0, noS):\n73s‚àó=d S\nforjin range ( 0, noTau):\ntau += dTau # Transform\nYn[i , j ] = Yn[i , j ]/maxY\n77tau = iT\nprint(""finding inverse transform"" ) # Inverse TF\nrecSigData = ""recSig.dat""\nrecSig = zeros( len(sig) )\n81t = 0.0;\nprint(""count to 10"" )\nk c o=0 ; j=0 ; Y i n v= Y n\nforrsin range (0,len(recSig) ):\n85recSig[rs] = invTransform(t, Yinv) #I n v e r t\nxx=rs/20\nyy=4.6 ‚àórecSig[rs]\ninvtr.plot(pos=(xx,yy))\n89t+ =h\nifkco %24 == 0:\nj+ =1\nprint(j)\n93kco += 1\nx=list(range(1, noS + 1))\ny=list(range(1, noTau + 1))\nX,Y = p.meshgrid(x, y)\n97\ndeffunctz(Yn): # Transform function\nz=Y n [ X ,Y ]\nreturnz\n101Z=f u n c t z( Y n )\nfig = p.figure()\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô)\n105ax.set_xlabel( ‚Äôs: scale‚Äô )\nax.set_ylabel( ‚ÄôTau‚Äô)\nax.set_zlabel( ‚ÄôTransform‚Äô )\np.show()\n109print(""Done"")\nListing 10.2 DWT.py UsestheDaub4digitalwaveletsandthepyramidalgorithmtocom-\nputethediscretewavelettransformforthechirpsignalvaluesstoredin f[ ].\n1# D W T.py: Discrete Wavelet Transform, Daubechies , global variables\nfromvisualimport ‚àó\nfromvisual.graph import ‚àó\n5\nsq3 = sqrt(3); fsq2 = 4.0 ‚àósqrt(2); N = 1024 #N=2 ^ n\nc0 = (1+sq3)/fsq2; c1 = (3+sq3)/fsq2 # Daubechies 4\nc2 = (3 ‚àísq3)/fsq2; c3 = (1 ‚àísq3)/fsq2\n9transfgr1 = None # Display\ndefchirp( xi): # Chirp signal\ny = sin(60.0 ‚àóxi‚àó‚àó2);\n13returny;\ndefdaube4(f, n, sign): # D W T if sign > = 0, inverse if sign <0\nglobaltransfgr1 , transfgr2\ntr = zeros( (n + 1), float) # Temporary\n17ifn<4:return\n222 10 Wavelet and Principal Components Analysis\nmp = n/2\nmp1 = mp + 1 # midpoint + 1\nifsign >= 0: #D W T\n21 j=1\ni=1\nmaxx = n/2\nifn > 128: #S c a l e\n25 maxy = 3.0\nminy = ‚àí3.0\nMaxy = 0.2\nMiny = ‚àí0.2\n29 speed = 50 #F a s tr a t e\nelse:\nmaxy = 10.0\nminy = ‚àí5.0\n33 Maxy = 7.5\nMiny = ‚àí7.5\nspeed = 8 #L o w e rr a t e\niftransfgr1:\n37 transfgr1.display.visible = False\ntransfgr2.display.visible = False\ndeltransfgr1\ndeltransfgr2\n41 transfgr1 = gdisplay(x=0, y=0, width=600, height=400,\\ntitle= ‚ÄôWavelet TF, down sample + low pass‚Äô , xmax=maxx,\\nxmin=0, ymax=maxy, ymin=miny)\ntransf = gvbars(delta=2. ‚àón/N,color=color.cyan,display=transfgr1)\n45 transfgr2 = gdisplay(x=0, y=400, width=600, height=400,\\ntitle= ‚ÄôWavelet TF, down sample + high pass‚Äô ,\\nxmax=2 ‚àómaxx, xmin=0, ymax=Maxy, ymin=Miny)\ntransf2 = gvbars(delta=2. ‚àón/N,color=color.cyan,display=transfgr2)\n49 whilej<=n‚àí3:\nrate(speed)\ntr[i] = c0 ‚àóf[j] + c1 ‚àóf[j+1] + c2 ‚àóf[j+2] + c3 ‚àóf[j+3] #l o w‚àípass\ntransf.plot(pos = (i, tr[i]) ) # c coefficients\n53 tr[i+mp] = c3 ‚àóf[j]‚àíc2‚àóf[j+1] + c1 ‚àóf[j+2] ‚àíc0‚àóf[j+3] #h i g h\ntransf2.plot(pos = (i + mp, tr[i + mp]) )\ni+ =1 # d coefficents\nj+ =2 # Downsampling\n57 tr[i] = c0 ‚àóf[n‚àí1] + c1 ‚àóf[n] + c2 ‚àóf[1] + c3 ‚àóf[2] #l o w‚àípass\ntransf.plot(pos = (i, tr[i]) ) # c coefficients\ntr[i+mp] = c3 ‚àóf[n‚àí1]‚àíc2‚àóf[n] + c1 ‚àóf[1]‚àíc0‚àóf[2] #H i g h‚àípass\ntransf2.plot(pos = (i+mp, tr[i+mp]) )\n61else: # Inverse D W T\ntr[1] = c2 ‚àóf[m p] + c1 ‚àóf[n] + c0 ‚àóf[1] + c3 ‚àóf[mp1] #L o w‚àípass\ntr[2] = c3 ‚àóf[m p]‚àíc0‚àóf[n] + c1 ‚àóf[1]‚àíc2‚àóf[mp1] #H i g h‚àípass\nj=3\n65 foriin range (1, mp):\ntr[j] = c2 ‚àóf[i] + c1 ‚àóf[i+ m p] + c0 ‚àóf[i+1] + c3 ‚àóf[i+ mp1] #L o w\nj+ =1 # Upsample\ntr[j] = c3 ‚àóf[i]‚àíc0‚àóf[i+ m p] + c1 ‚àóf[i+1] ‚àíc2‚àóf[i+ mp1] # High\n69 j+ =1 ; # Upsampling\nforiin range (1, n+1):\nf[i] = tr[i] # Copy TF to array\ndefpyram(f, n, sign): # D W T , replaces f by TF\n73if(n<4):return # Too few data\nnend = 4 #W h e nt o s t o p\nifsign >= 0 : # Transform\nnd = n\n77 whilend >= nend: # D o w n s a m p l e filtering\ndaube4(f, nd, sign)\nnd //= 2\nelse: # Inverse TF\n81 whilend<=n : # Upsampling\ndaube4(f, nd, sign)\nnd‚àó=2\nf=z e r o s (( N+1 ) , float) # Data vector\n85i n x i=1 . 0 / N # For chirp signal\nxi = 0.0\n10.7 Code Listings 223\nforiin range (1, N + 1):\nf[i] = chirp(xi) # Function to TF\n89xi += inxi;\nn=N # Must be 2^m\npyram(f , n, 1) #T F\n#p y r a m ( f , n , ‚àí1) # Inverse TF",10068
110-11.1.1 Artificial Neural Networks.pdf,110-11.1.1 Artificial Neural Networks,"224\n11\nNeural Networks and Machine Learning\nAutomated systems should provide explanations that are technically valid, meaning-\nful and useful to you and to any operators or others who need to understand the\nsystem, and calibrated to the level of risk based on the context .\n‚ÄîWhite House AI Blueprint, 2022\nThe human brain, which has evolved (maybe) over six million years, is sometimes effective at\nsolving problems that traditional computer programming Ô¨Ånds hard. This chapter deals with\nneural networks, artiÔ¨Åcial intelligence (AI), and machine learning. Part I deals with simple\nmodels for neurons and neural networks, based on those found in the brain. Part II demon-\nstrates several state-of-the-art AI software packages, whose innards are based on neural\nnets. The applications are from physics, but deliberately simple, in order to demonstrate\nwhat‚Äôs inside the AI programs .\nProblem Developacomputermodelforaneuronandforanetworkoftheseneurons,\nandinvestigateifyournetworkhasthecapacitytolearn.\nArtificial intelligence (AI) simulates human cognitive abilities in learning and problem-\nsolvingbycapturingthetypeoftacitknowledgethatisverydifficulttowriteintosoftware.\nInrecentyears,AIhasmadegreatadvancesinpatternrecognition,decisionmaking,infer-\nence, and generating controversially realistic data.1Machine Learning (ML) is a subfield\nof AI in which neural networks are taught by iteratively and inductively learning from\nteachingdata. DeepLearning isanextensionofmachinelearningthatuseslayersofneural\nnetworkstopassstatisticalassociationsfromonelayerofthenetworktothenext.Finally,\ngenerativeAI usestwoneuralnetworks,onetogeneratedata,andasecondtoevaluatethose\ndata.Theoutputfromtheevaluationthengetsfedbackintothefirstnetworkforfurther\ntrainingandimprovement.\n1 HerbertSimonreceivedtheNobelMemorialPrizeineconomicsciencesin1978andtheTuringAward\nincomputersciencein1975,inpartialrecognitionforhisdevelopmentsinAI.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n11.1 Part I: Biological and ArtiÔ¨Åcial Neural Networks 225\n11.1 Part I: Biological and ArtiÔ¨Åcial Neural Networks\nThe study of neural networks began with studies of the brain. The human brain weighs\nabout three pounds and contains approximately 1011nerve cells called neurons.T h e s e\nneurons are interconnected in complicated networks that somehow provide our mental\ncapacities.AdrawingofaneuronisgivenontheleftofFigure11.1.Onthebottomofthe\nneuron‚Äôs cell body are branch-like dendrites that receive electrical and chemical pulses\nfromthesynapsesofotherneurons.Inturn,ifproperlyexcited,thecellbodysendsoffa\nsingleelectricalpulsealongtheaxontothesynapticterminalsontop.Thepulseiscalled\nanaction potential , and is typically in the range 0‚Äì30 negative millivolts, with a width\nbetween1and6milliseconds.\nThe pulses entering the cell body may be excitatory or inhibitory, which, respectively,\nincreaseordecreasethenetvoltagethatreachesthecellbody.Thecellbodyintegratesthe\nincomingpulsesinavarietyofways,andifsomethresholdisreached,firesitsownpulse\nalongtheaxontothesynapticterminalswhereelectrochemicalinteractionswithotherden-\ndritestakeplace.Theprocessisbinaryinthesensethatapulseissent,ornotsent,each\ntime with essentially the same pulse configuration. After firing, the neuron needs some\ntimetorest.\nOn the right of Figure 11.1, we see an actual image of the neurons in a mouse brain\n[Palmer, 2016; Reid et al., 2016]. This is a true biological neural network, and is seen to\nbe a very complicated, net-like structure of neurons connected by axons, dendrites, and\nsynapses. The sense organs pass signals to the outer layer of the brain, where neurons\nprocessthemthere.Theoutputfromtheouterlayerofthebraingetspassedontolower\nand lower layers in a hierarchical manner, with each layer‚Äôs processing believed to have\nitsownpurpose.Theprocesscontinuesuntilafinalresponseisreached,forexample,the\nAxon\nDendritesSynaptic\nterminals\nNucleus\nFigure 11.1 Left: A sketch of a neuron showing dendrites, a cell body, and synaptic terminals. The\ndendrites transmit pulses to the cell body, where an electrical action potential originates and is\nsent along to synaptic terminals. Right: An electron microscope image of the network of cortical\nneurons in a mouse brain (adapted from [Palmer, 2016; Reid et al., 2016]).",4431
111-11.2.1 Coding A Neuron.pdf,111-11.2.1 Coding A Neuron,"226 11 Neural Networks and Machine Learning\nrecognitionofaface.Biologicalnetworksappeartobehighlyparallel,and,possiblyforthis\nreason,robust,withnosinglegroupofneuronsabsolutelyessential.\n11.1.1 ArtiÔ¨Åcial Neural Networks\nIn 1943, Warren McCullock, a psychiatrist with a deep intellectual interests in how the\nhumanbrainworks,andWalterPitts,alogicianwithaninterestinbiologicalsciences,pro-\nposedalandmarkmathematicalformulationforaneuron,andforanetworkcomposedof\nsuchneurons.Basedonthefunctionalityofthebiologicalneuron,theywentontodevelopa\nsymbolic-logicalcalculusforhowtheirmodelneuronsinteractwitheachother[McCulloch\nandPitts,1943].McCullockandPittstherebyprovedmathematicallythataneuralnetcan\nbetrainedandcanlearn.Theirneuronmodel,the McCulloch-Pittsneuron ,isstillastandard\nofreferenceinthefield.\nThemathematicalmodelofaMcCulloch-Pittsneuronisoftencalleda Perceptron.How-\never,thetermalsoreferstotheelectronicversionofaneuralnetworkbasedontheactual\nneurons‚ÄôbiologycreatedbyFrankRosenblattatCornellin1957[Rosenblatt,1958].2Rosen-\nblatt‚ÄôsPerceptrondisplayedtheabilitytolearn,andwasbothsensationalandcontroversial\natthetime.Specifically,theperceptronwasasimulationonanIBM704that,after50trials,\nwasabletodistinguishpunchedcardsmarkedontheleftfromcardsmarkedontheright.\nAftereachtrial,themachine‚Äôsconnectionswouldbetweaked,anditwouldbenotedifthere\nwasanimprovementinitsprediction,andiftherewas,thenthechangeswouldbekeptand\nfurthertrialsmadeinanefforttoimprovethepredictions.However,thecomputingpower\nofthe1950sand1960swasordersofmagnitudetoolowtoprovidetheconvincingdemon-\nstrationofmachinelearningthatpresent-daycomputerscan,andRosenblattdiedin1971\nwithoutseeingthesuccessofhisideas.\nIn analogy to a biological neuron, an artificial neural network (‚Äúnet‚Äù) processes data\nthrough multiple layers of neurons or nodes. Each node may accept several inputs, pro-\ncessestheminacomputingunit,and,ifsetcriteriaorthresholdisreached,outputsdata\ndownitsaxonor edgetoothernodes.Theinternalalgorithmineachneuronusedindeci-\nsionmakinghaschangeableparameters,andthenetwork‚Äúlearns‚Äùbyiterativelychanging\ntheparametricvaluesofeachneuronbasedontheaccuracyofoverallpredictions.Inthis\nway,differentnodesendupwithdifferentparametricvalues.\n11.2 A Simple Neural Network\nInFigure11.2weseeasimpleartificialintelligence(AI)neuron,alsocalleda node,withtwo\ninputsandoneoutput.The x‚Äôsontheleftaretheinputsignalscomingfromothernodes,\nbW1X1\nX2W2yfŒ£Figure 11.2 An AI neuron with two inputs and one output.\nThe neuron body calculates a weighted sum of the inputs,\nand then processes the sum, possibly with bias band\nthrough a sigmoid function f.\n2 Oneofus(RHL)recallsfondlythatWarrenMcCullockhelpedmentorhimaboutgraduateschool,and\nthatFrankRosenblattwasoneofhisundergraduateteachers.",2789
112-11.2.3 Training A Simple Network.pdf,112-11.2.3 Training A Simple Network,"11.2 A Simple Neural Network 227\nandtheŒ£inthecellbodydenotesaweightedsummationoftheinputsignals:\nŒ£=ùë§1x1+ùë§2x2. (11.1)\nAlso within the cell body is the activationorsigmoid(S-shaped) function fthat decides,\nbased on the value of Œ£, whether or not to fire. Consequently, the output ycan be\nexpressedas\ny=f(x1ùë§1+x2ùë§2+b), (11.2)\nwherebisabias.Theweightsandthebiasesaretheparametersthatgetchangedduring\nlearning.Asasimpleexample,let‚Äôssayourperceptronhasweights ùë§1=‚àí1,ùë§2=1,bias\nb=0,andacceptsthetwoinputvalues,\nx1=12,x2=8. (11.3)\nThenŒ£=‚àí1√ó12+1√ó8=‚àí4,andiff(x)=x,thenthiswouldbetheneuron‚Äôsoutput y.\nThebinarynatureoftheoriginalperceptronneuron,withitsoutputofonly0or1,can\nmake building a network out of them challenging to train. A more trainable and robust\nnetwork would contain sigmoid neurons in which the output is no longer restricted to 0\nor1.Forexample,\nf(x)=1\n1+e‚àíx,f(x)=tanh(x),fReLU(x)=max(0,x). (11.4)\nTheexponentialwouldproduceanoutputbetween0and1,thehyperbolictangentwould\nproduceanoutputbetween ‚àí1and+1,andtheRectifiedLinearUnitoutputsthe x,ifxis\npositive,and0ifthe xisnegative.Forthesimplenetworkthatwewillsoondevelop,we\nshallusetheexponentialsigmoidfunction.\n11.2.1 Coding A Neuron\nExercise Below in Listing 11.1 we give a software model of single neuron coded with\nNumPy[Zhou,2022].Verifythatthiscodereproducesthehandcalculationabovethatgave\n‚àí4asoutput.\n11.2.2 Building A Simple Network\nInFigure11.3weshowanAInetworkcontainingthreelayers:aninputlayerontheleft,\nahiddenlayerinthemiddle,andanoutputlayerontheright.Networkswithmorethan\nthreelayersproducewhat‚Äôscalled deeplearning ,andsoournetworkmightbecalleda‚Äúshal-\nlowlearner.‚ÄùTheweightsforthesignalsenteringeachlayerareshown,whereitshouldbe\nFigure 11.3 A simple neural network with two\nneurons in the input layer, two neurons in the\ninternal hidden layer, and one in the output\nlayer.Oh1X1\nX2w1 w5\nw6\nw4w2\nw3\nInput\nlayerHidden\nlayerOutput\nlayerh2\n228 11 Neural Networks and Machine Learning\nnotedthattheactivationfunctionswithineachcellbodymaybealldifferent.Evenaneu-\nralnetworkwithjustafewneuronscanhaveagoodnumberofconnectionsandsigmoid\nfunctions.We‚Äôllsacrificecomputingpowerforsimplicity,andmakeeachneuronidentical.\nExercise Calculate by hand the expected output of the network in Figure 11.3 for\nx1=2,x2=3,andùë§1=0,ùë§2=1.Youshouldget O=0.7216.\nThecode NeuralNet.py inListing11.1isforaneuralnetworkandmustincludetheprevious\nneuronclass.Checkthatitproducesanoutputof0.7216.\n11.2.3 Training A Simple Network\nFigure 11.4 shows a flowchart for training an AI network. The network is taught by\ninputting predetermined training data for which the ‚Äúcorrect‚Äù output is known, and\ncomparingthepredictedandcorrectoutputs.Onethencalculatesthe CostorLoss(whata\nphysicistmightcall error):\nCost‚â°Loss‚â°Óà∏=CorrectOutput ‚àíPredictedOutput . (11.5)\nForaperfectnetworktheCostwouldbezero.ThemathematicalrelationbetweentheLoss\nand the network‚Äôs many parameters is generally unknown, and remains unknown even\nafter training. During training, if the Cost is higher than some set value, the weights are\ntweaked by an amount based on the numerical evaluation of the derivatives of the Loss\nfunction.Thisprocess,called backpropagation ,isrepeateduntilareasonablysmallCostis\nobtained.Thenthemodelistestedondataithasnotseenbefore,anditstrueaccuracyis\ndetermined.Andifneedbe,furthertrainingmightbeinorder.\nRealisticnetworksmaycontainseveralhundrednodesforeachfeatureinthedatathat\nwewanttounderstand,withhundredsormorenodesineachhiddenlayer,butonlyasmall\nnumberofnodesfortheoutput.Theoptimalnumbersfollowfromintuitionandtrialand\nerror,sometimeswiththenumberincreasinguntilgoodresultsareobtained.Asyoucan\nwellimagine,fullytrainednetworksarelikeblackboxeswithmanyparametervaluesthat,\ninthemselves,donotexplainthestrategyusedtoobtainthecorrectanswers.Sowhileneu-\nral nets are often successful and efficient at recognizing complex patterns, much as the\nhumanbrain,theydonotprovideinsightintohowtheymadegooddecisions.\nNowthatwehaveaneuralnet,let‚Äôstrainit.Ofcourseweshouldnotexpecthighperfor-\nmancefromthissimplenetwork,butthenagain,we‚Äôrenotputtingmuchintoit!Although\nInitial valuesRepeat unit\nminimize MSECompute MSE, mean\nsquared error\nCompute gradient\nto change parameters\nLoss function stable\ndone\nFigure 11.4 A Ô¨Çowchart of the steps in teaching a\nneural net, with the gradient of the Loss function used to\nminimize it.\n11.2 A Simple Neural Network 229\ninsomesense,‚Äútraining‚Äùthenetworkisprogramming,it‚Äôsnotthekindofprogramming\nwe‚Äôre accustomed to, where we go about modifying statements until we get things right.\nRather,we,orsomealgorithm,willdoitsownthingandvarytheinternalparameters‚Äôval-\nues,withthechangesbasedonhowclosethenetworkistoproducingtherightanswer.As\nwehavesaid,oncetrained,itisessentiallyimpossibletopredictwhatthenetworkdoes,or\nhowit‚Äôsdoingit,bystudyingtheinternalparameters‚Äôvalues.\nAsasimpleexample,imaginethatyouwanttoidentify ùúãmesonsfrom ùúámesonsbased\nonthelengthandwidthofthetractstheyhaveleftonafilmstrip.Youstartwiththechar-\nacteristicsoffourtracksthathavealreadybeen(painstakingly)measuredbyeye:\nTrack ID Length (mm) Width (mm) Particle\nA1 3 6 ùúã\nB 16 10 ùúá\nC1 5 9 ùúá\nD1 2 7 ùúã\nInordertomakenumericalpredictions,welabela ùúáasa0anda ùúãasa1.Asisstandard\ninMLanddatamining,weconvertto mean-centereddata ,whichmeansthatwescalethe\ninputdatatohavezeromean(averagelength14,averagewidth8):\nTrack ID Length ‚Äì 14 Width ‚Äì 8 Particle ID\nA ‚àí1 ‚àí21\nB2 +20\nC1 +10\nD ‚àí2 ‚àí11\nNextwedefinethe Loss(11.5)asthemean-squareddifferencebetweenthecorrectanswers\ny(c)\niandthepredictedones y(p)\ni:\nÓà∏=1\nNN‚àë\ni(\ny(c)\ni‚àíy(p)\ni)2\n, (11.6)\nwhereNisthenumberofinputdata.Clearly,thesmallertheloss,thebetterthenetwork.\nExercise Run the code below and check that it predicts that all of the input tracks are\nmuons(y(p)\ni‚â°0).\n1importnumpy as np # For N u m P y arrays\ndefLoss(y_c, y_p):\nreturn((y_c‚àíy_p) ‚àó‚àó2).mean() # Auto mean of array\n5y_c = np.array([1, 0, 0, 1])\ny_p = np.array([0, 0, 0, 0])\nprint(Loss(y_c, y_p))",6043
113-11.2.4 Decreasing the Error.pdf,113-11.2.4 Decreasing the Error,"230 11 Neural Networks and Machine Learning\nThecodegives Óà∏=0.5,whichmeanswegottherightanswerhalfthetime(buttherewere\nonlytwochoices).\n11.2.4 Decreasing the Error\nMinimizingtheLossisessentiallyidenticaltominimizinga ùúí2fittodata,whichwehave\nstudiedinChapter6.Nowwewanttoadjusttheweights ùë§i‚Äôsandthebiases bi‚Äôstomini-\nmizeÓà∏.Evenforsomethingassimpleasourtwo-neuronnetworkinFigure11.3,wehave\nsixweightsandthreebiasestoadjust.Accordingly,anextremumin Óà∏occurswhen\nùúïÓà∏\nùúïùë§i=0,i=1,‚Ä¶,6,ùúïÓà∏\nùúïbi=0,i=1,2,3. (11.7)\nFor a complex network, there might be thousands or more of these equations, with only\nnumericaldeterminationsofthederivativesfeasible.Thisisnotaone-timeaffair;onekeeps\ntrainingthenetworkinhopesthatitwillmoveclosertoaminimum.Althoughtheparam-\netersùë§iandbido not appear explicitly in the definition (11.6) of the Loss, the predicted\nvaluesy(p)\nido, and they are functions of the parameters in some unknown way. Accord-\ningly,weusethechainrule(twice!)toobtaintheneededpartialderivatives.Forexample,\nbecause the weight ùë§1affects only the hidden neuron h1, and because there is only the\nsinglepredictedvalue y(p)\ni,\nùúïÓà∏\nùúïùë§1=ùúïÓà∏\nùúïy(p)\niùúïy(p)\ni\nùúïh1ùúïh1\nùúïùë§1. (11.8)\nFocusingonjustoneweightorbiasatatime,inthiscase ùë§1,isoftenusedintraining,with\nthenetworktrainedsequentiallyforeachoftheotherparameters.\nWenowevaluatethesepartialderivatives.Thedefinition(11.6)ofLoss,makesthe ùúïy(p)\ni\nderivativeeasy:\nùúïÓà∏\nùúïy(p)\ni=‚àí2\nN(\ny(c)\ni‚àíy(p)\ni)\n, (11.9)\nwherethecorrectanswers y(c)\ni‚Äôsareknown,butnotthe y(p)\ni‚Äôs.Inthepresentcase,theoutput,\ny(p)\nout=f(ùë§5h1+ùë§6h2+b3), (11.10)\nmakesthederivativeswithrespecttotheweightsstraightforward:\nùúïy(p)\nout\nùúïùë§5=h1df(x)\ndx(x=ùë§5h1+ùë§6h2+b3), (11.11)\nùúïy(p)\nout\nùúïùë§6=h2df(x)\ndx(x=ùë§5h1+ùë§6h2+b3). (11.12)\nThederivativeofthesigmoidfunction(11.4)iseasy:\nf(x)=1\n1+e‚àíx‚áídf(x)\ndx=e‚àíx\n(1+e‚àíx)2. (11.13)\n11.2 A Simple Neural Network 231\nThenextderivativeweneed, ùúïh1‚àïùúïùë§1,followsfromourdefinitionofthe h‚Äôsandtheiruse,\nasshowninFigure11.3:\nh1=f(ùë§1x1+ùë§2x2+b1) (11.14)\n‚áíùúïh1\nùúïùë§1=x1df\ndx(x=ùë§1x1+ùë§2x2+b1). (11.15)\nNowweputallofthepiecestogether:\nùúïÓà∏\nùúïùë§i=ùúïÓà∏\nùúïy(p)\niùúïy(p)\ni\nùúïùë§i=‚àí2\nN(\ny(c)\ni‚àíy(p)\ni)ùúïy(p)\ni\nùúïùë§i. (11.16)\nThederivatives ùúïy(p)\ni‚àïùúïùë§idependsuponthemodelfortheneuron,andinparticular,onits\nsigmoidfunction f.Forourmodel,itsoneoutputis\ny(p)\nout=f(ùë§5h1+ùë§6h2+b3)=1\n1+e‚àí(ùë§5h1+ùë§6h2+b3). (11.17)\nForthetwo-neuronnetwork(Figure11.3),the hfunctionsare:\nh1=f(ùë§1x1+ùë§2x2+b1),h2=f(ùë§3x1+ùë§4x2+b2). (11.18)\nNowthatwehavegonethroughsomeofthegrubbydetails,itmightbeclearthattheeval-\nuationoftheLossforabigrealisticnetworkisbestnotdonebyhand.Inordertocomplete\nourexamplewiththeleastamountofpain,let‚Äôsalsolimitthenetwork‚Äôsinputtojustone\noftheparticletracks,inthiscase,trackAwithlength x1=‚àí2,widthx2=‚àí1,andparticle\ny(c)=1.Furthermore,forsimplicity,let‚Äôssetalloftheweightsto1,andallofthebiasesto\n0.Wethenhave\nh1=f(ùë§1x1+ùë§2x2+b1)=f(‚àí2‚àí1+0)=1\n1+e‚àí3=0.0474,\nh2=f(ùë§3x1+ùë§4x2+b2)=f(‚àí2‚àí1+0)=1\n1+e‚àí3=0.0474,\n‚áíy(p)\nout=f(ùë§5h1+ùë§6h2+b3)=f(0.0474+0.0474)\n=1\n1+e‚àí0.0948=0.524. (11.19)\nThis prediction says that it is more likely that not that Track A is a ùúã(ID=1), but not\nverylikely.Thatbeingthecase,let‚Äôsadjusttheweightsandseeifitimprovesthenetwork‚Äôs\nprediction. To do that, we need to evaluate the derivative of the loss with respect to the\nparameter ùë§1(11.8):\nùúïÓà∏\nùúïùë§1=ùúïÓà∏\nùúïy(p)\noutùúïy(p)\nout\nùúïh1ùúïh1\nùúïùë§1,\nùúïÓà∏\nùúïy(p)\nout=‚àí2(1‚àíy(p)\nout)=‚àí2(1‚àí0.524)=‚àí0.952, (11.20)\nùúïy(p)\nout\nùúïh1=ùë§5df\ndx(ùë§5h1+ùë§6h2+b3)\n=1√ódf\ndx(0.0474+0.0474+0)=exp(‚àí0.0948)\n(1+exp(0.0948))2=0.249,(11.21)",3545
114-11.2.5 Coding and Running A Simple Network.pdf,114-11.2.5 Coding and Running A Simple Network,,0
115-11.3 A Graphical Deep Net.pdf,115-11.3 A Graphical Deep Net,"232 11 Neural Networks and Machine Learning\nùúïh1\nùúïùë§1=x1df\ndx(ùë§1x1+ùë§2x2+b1)=‚àí2df\ndx(‚àí2‚àí1+0)=‚àí0.0904\n‚áíùúïÓà∏\nùúïùë§1=‚àí0.952√ó0.249√ó(‚àí0.0904)=0.0214. (11.22)\nAtlast!Thistellsusthatifwedecrease ùë§1,thentheLoss Óà∏shouldgetsmaller,andthus\nyieldabetterprediction.\nExercise Repeatthepredictionof y(p)\noutwithanincrementallydecreasing ùë§1,andobserve\nhowthepredictionchanges.\nTodeterminejusthowmuchtochangetheweight,weassumethatwearecloseenough\ntothecorrectanswertoneedonlyafirst-ordercorrectiontotheweight:\nùë§(new)\n1‚âÉùë§(old)\n1‚àíùúÇùúïÓà∏\nùúïùë§1. (11.23)\nHereùúÇiscalledthe learningrate ofthenetwork,andthemethodiscalled stochasticgradient\ndescent(discussedfurtherinSection11.6.3).Evenforlarge-scaleproblems,theprocessis\noftenmuchlikewhatwehaveworkedthroughhere,butprobablyautomated:onefocuses\nonasingleparameter,aswehavedonewith ùë§1,andthenrepeatstheprocessuntiltheLoss\nstops decreasing, or becomes too slow in its response. Then one goes on and repeats the\nprocesswitheachoftheotherweightsandbiases.\n11.2.5 Coding and Running A Simple Network\nListing11.3attheendofthischapterpresentsourcodeforthesimplenetwork.Itstarted\nwithLoop n=0 Loss: 0.164 ,andendedwith Loop n=960 Loss: 0.002 .\n1) Run SimpleNet.py andplottheLossversusthenumberoflearningtrials, N.\n2) OnceyouhavetaughtthenetworkenoughforittoproduceasmallLoss,sayjustafew\npercent,determinethepredictionsitmakesonsomenewinputdata.\n3) Extendoursimpletwo-nodehiddenlayernetworktoathree-nodehiddenlayer.\na) Repeatthelearningexerciseusedforthetwo-nodehiddenlayernowforthethree-\nnodecase.\nb) Compare the effectiveness of learning for the two- and three-node hidden layer\nnetworks.\n4) Extendoursimpletwo-nodehiddenlayernetworktoanetworkwithtwohiddenlayers,\neachcontainingjusttwonodes.\na) Repeatthelearningexerciseusedforanetworkwithatwo-nodehiddenlayer,now\nforonewithtwotwo-nodehiddenlayers.\nb) Compare the effectiveness of learning for the single and double two-node hidden\nlayernetworks.\n11.3 A Graphical Deep Net\nWehavejustbuiltasimpleneuralnetworkwithsinglehiddenlayer,andshowedthatit‚Äôs\ncapableoflearning,ifjustsomewhat.Now,basedonZhou[2018]andRohrer[2017],we\n11.3 A Graphical Deep Net 233\n‚Äì1\n‚Äì1\n110\n0\n‚Äì2‚Äì2\nInputHidden\nlayer 1(h1)\n1 √ó\n‚Äì1 √ó + 1 √ó+ 1 √ó+=\n=\n= (h2, 1)\n(h2, 2)\nFigure 11.5 A deep neural net that classiÔ¨Åes a 4 √ó4 square into different classes.\nexaminegraphicallya deepneuralnetwork withthreehiddenlayers.Thisnetwork,which\nisessentiallyanMLdecisiontree,hasbeendesignedtorecognizetheorientationofaline,\nanddoesitwithnoerror.Thepointhereistoseegraphically,withoutworkingthroughthe\nprogramming,howmorecomplicatedtaskscanbeaccomplishedbyneuralnetworks.\nTheleftofFigure11.5showsathree-hiddenlayernetworkthathasbeentaughttorec-\nognizedifferentpatternswithina4 √ó4square.Thisisanexampleofhierarchicallayering\ninwhicheachsuccessivelayerrecognizesamorecomplexpattern.Thefirsthiddenlayer\nrecognizesasinglepixel,thesecondlayerrecognizestwo-pixelcombinations,andsoforth.\nEachcellhastwodendrites(edges)entering,andtwoleaving,withnoconnectionsbetween\ntwoneuronsinthesamelayer,orbetweennonadjacentlayers.Thenumbersnexttoeach\ncellbodyaretheweightsoftheedgesappliedtothesignalstheyhavereceived.\nThelargesquareontheleftofFigure11.5isthefour-pixelinput,inthiscasewithahor-\nizontalline.Theinputlinecanbe:\nhorizontal: [xx\n‚óΩ‚óΩ],[‚óΩ‚óΩ\nxx],vertical:[x‚óΩ\nx‚óΩ],[‚óΩx\n‚óΩx],diagonal: [x‚óΩ\n‚óΩx],[‚óΩx\nx‚óΩ],noline:[xx\nxx],[‚óΩ‚óΩ\n‚óΩ‚óΩ].\nAs indicated by the white squares in Figure 11.5, each of the input layer‚Äôs four nodes is\nassociatedwithadifferentpixellocation.Accordingly,theinputhorizontallineoccupying\nthetoptwopixelsinthefigureisnotidentifiedwiththetoptwonodesintheinputlayer\n(weight‚àí1),butisidentifiedwiththebottomtwonodes(weight +1).\nOntheleftofFigure11.5,weisolateHiddenLayer1anditsactioninidentifyingoneof\nthefourpixels.Basedontheinputtoitfromtheedges,thislayercombinessinglepixelsinto\ntwo-pixelcombinations,anddeterminesappropriateweights.Forexample,inFigure11.5\nleftweseehowedgesfromthetopandthebottomnodesoftheinputlayerarefedintothe\ntopnodeofHiddenLayer1.Onthe( h1)lineinFigure11.5b,weseehowthesetwoedges\narecombinedintotherecognitionofaverticalline.Therebeingnoverticallineintheinput\npicture,aweightof0isrecorded.Thisisexpressedanalyticallyas\nx1ùë§1+x4ùë§4=( ‚àí1)(1)+(1)(1)=0. (11.24)",4264
116-11.4.1 TensorFlow Installation and Execution.pdf,116-11.4.1 TensorFlow Installation and Execution,"234 11 Neural Networks and Machine Learning\nOntherightofFigure11.5wedemonstratehowthetoptwonodesinHiddenLayer2are\nactivated.Thetopnodeonline( h2,1)performsthecombination\n1√ó[‚óΩx\n‚óΩx]+1√ó[x‚óΩ\nx‚óΩ]=[‚óΩ‚óΩ\n‚óΩ‚óΩ]. (11.25)\nTheseconddownnode( h2,2)performsthecombination\n‚àí1√ó[‚óΩx\n‚óΩx]+1√ó[x‚óΩ\nx‚óΩ]=[x‚óΩ\nx‚óΩ], (11.26)\nwherethenegationofwhiteisdefinedasblack.\nThecellbodiesinthehiddenlayerscontainthe activationfunctions thatdeterminethe\nneurons‚Äôactionsbasedonweightedvaluesoftheinputs.Aslongasthesignaltransmitted\ntothenodeisnonzero,itremainsactiveandasignalgetstransmittedonward.Azero-input\nsignalplacesthenodeinaninactivestatewithnotransmission.\nHiddenLayer3usesthe ReLUactivationfunction(rectifiedlinearunit)thattransmits\npositivesignals,butturnstheneuronoffiftheinputisnegative.Weleaveitasanexercise\ntoworkthroughtheactionsinHiddenLayer3.\nProblem Here are four combinations: [X][‚óΩ],[X][X],[‚óΩ][X],[‚óΩ][‚óΩ].Build a neural\nnetworkthatcandistinguishthesecombinations.\n11.4 Part II: Machine Learning Software\nIn Part II of this chapter, we give examples of using Python with several industrial-strength\nML software packages [Campesato, 2020], [Yalcin, 2021]. Preparing data for ML is often\na time-consuming, and, accordingly, we also discuss a number of tools for preprocessing\ndata.\nTensorFlow isafreeandpowerfulpackageofsoftwareformachinelearningviadeepneural\nnetworks.Itwasdevelopedby GoogleBrain fortheirownAIresearchanddevelopment,but\nin2015wasmadeavailableasopen-sourcesoftware.Themoreuser-friendlyTensorFlow2\nwasreleasedin2019.WhenGoogleusesTensorFlowtheyemploytheirownTensorFlow\nCPU(TPU),whichisalsoavailableinthecloud.(Microsofthasalsoinvestedheavilyinto\nthecomputingpowerneededforAI,butit‚Äôsnotfree.)Google‚ÄôsTPUsarecapableofsome\nfourtrillionoperationspersecond,muchmorethananythingwewillneed.However,that\namountofcomputingpowerisvaluablefortaskslikeexoplanetrecognition,atwhichAI\nexcels, and, indeed, the absence of that power was the reason AI did not prosper in the\n1950s. In general, neural networks are probably best suited to big problems, not for the\nsmallpedagogicalproblemswe‚Äôlllookat.\nTensorFlow uses dataflowgraphs as its basic computational element, with each graph\ncomposedofnodesandedges(cellbodiesandaxonslikethoseinFigure11.2).The‚Äútensor‚Äù\naspectofTensorFlowreferstoitsuseofarrayswithmultipleindices,liketensorsinphysics,\ntorepresenttheedges.ThearraysarecompatiblewithPython‚ÄôsNumPy,withwhichyouare\nfamiliarfromChapter7.ThenodesinTensorFlowperformthemathematicaloperations,\nandtheedgestransferthedata.",2537
117-11.5 TensorFlow and SkLearn Examples.pdf,117-11.5 TensorFlow and SkLearn Examples,"11.5 TensorFlow and SkLearn Examples 235\n11.4.1 TensorFlow Installation and Execution\nEventhoughsomeofthesedirectionsarearepeatofthoseinChapter1,somearenew,andso\nforthesakeofcompleteness,werepeatthemhere .\nInordertorunTensorFlowinteractivelyinanotebookenvironment,you‚Äôllneedafew\nthings:\n‚óèSetupanotebookenvironmentsuchasJupyter[2022].Thisgivesyouaweb-based,free,\ninteractivecomputingplatformthatcombineslivecode,equations,text,visualizations,\nandmuchelse.\n‚óèInstallanup-to-dateversionofPython,asavailablefromAnaConda[2022].\n‚óèInstallapackagemanager ,whichhelpsintheinstallationofalltheassociatedbitsand\npiecesofpackages.Werecommend[Conda,2023]withinaJupyterNotebook.Todothis,\nuseashell(the Commandshellor PowerShell onWindows,orthe TerminalonMacs)to\ncreatetheCondaenvironment:\nconda create -name MyEnv\nwhereyoumayuseanameotherthan MyEnvforyourenvironment.\n‚óèFinally,you‚ÄôllneedTensorFlow.Followtheinstructionsin[Tensor,2022].\n‚óèActivateyourenvironmentbyentering:\nconda activate tensorflow\n‚óèNexttellCondatousetheGitHubrepository conda-forge forneededpackages:\nconda install -c conda-forge tensorflow\n‚óèOnce TensorFlow is installed, call the Anaconda Navigator and select MyEnvfrom\nApplications on/base(root) .Atthetopofthenavigator,therearethreeboxes.Select\nthe middle one, which by default is base(root) , and change it to MyEnv, or whatever\nyouhavedefinedasyourtensorflowenvironmentinJupyter.Also,whenwritinganew\n.ipynbnotebookinthebox New,selectyourTensorFlowenvironment( MyEnv).\n‚óèOnceJupyterislaunched,select New/Python3(ipykernel)/MyEnv .\n‚óèInanotebookcellenter:\nimport tensorflow as tf\nandthenrun tf.\n‚óèInthenextcellenter:\nprint(tf. __version__) .\nEnsurethatyouhave TensorFlow 2 orT2.xandnot T1.x.\n11.5 TensorFlow and SkLearn Examples\nBefore we start using TensorFlow for some AI work, we‚Äôll run through several simple\ncalculationswithitasacheckthatit‚Äôsinstalledandworkingproperly.\nProblem UseTensorFlowtocomputethemassnumber A,giventheatomicnumber Z\nandtheneutronnumber N.\n236 11 Neural Networks and Machine Learning\nAsweallknow,theatomicnumber Zisthenumberofprotonsinanucleus,andthemass\nnumberA=Z+Nisthesumofthenumberofprotonsandneutronsinanucleus.This\nprogramcalculates A,givenZandN:\n1# TensorTest .py: Test TensorFlow\n[1]importtensorflow as tf\n[2] Z = tf.constant(1) # Hydrogen\n[3] N = tf.constant(2) # T w o neutrons = > tritium\n5[4] A = tf.add(Z,N)\n[5]print(""A:"",A )\nA: tf.tensor(3, shape=(), dtype=int32)\nTounderstandthisoutput,here‚ÄôssomeTensorFlowspeak:\n‚óèSize:Totalnumberofelementsinatensor.\n‚óèAxis or Dimension: Aparticulardimensionofatensor.\n‚óèRank 0 (scalar): Atensorwithonevalue,noaxis.\n‚óèRank 1 (vector): Atensorwithalistofvaluesononeaxes.\n‚óèRank 2 (matrix): Atensorwithalistofvaluesontwoaxes.\n‚óèRank N:Atensorwith Nindices(akaorder,degree,orndims).\n‚óèShape:Thelength(numberofelements)oneachaxesofatensor.Aconstanthasshape(),\natensorwithdimensions[2,3]hasshape(2,3).\n‚óèData types: Line6(L6)ofthelistingshownas int32.Otherdatatypesare:\ntf.float32 tf.float64 tf.int8 tf.int16\ntf.int64 tf.uint8 tf.string tf.bool\nProblem UseTensorflowtocomputethemassexcessofthehydrogenisotopes.\nProblem Findthebindingenergyforeachoftheseven Hisotopes,andplotthebinding\nenergiesversus A.\nFigure 11.6 shows the TensorFlow calculation of the hydrogen isotope mass excess, ran\nwithinaJupyternotebook.The Zprotonsand Nneutronswithinanucleusareboundby\nFigure 11.6 A screenshot of a TensorFlow calculation of the mass excess of the hydrogen isotopes\nwithin a notebook environment.\n11.5 TensorFlow and SkLearn Examples 237\nthenuclearforce.Asaconsequence,therestenergy( mc2)ofthenucleusislessthanthe\nsumofitsconstituentmassesbythebindingenergy B:\nB=[Zm(1H)+Nmn‚àíMnuc]c2. (11.27)\nAtomic masses are usually stated in Daltons (Da, or u), with u defined as 1/12 the mass\nofa12Catom=1.660538782 √ó10‚àí27kg‚âà931.5MeV ‚àïc2.Themassexcessisthedifference\nbetweentheatomicmassMandutimestheatomicnumber:\nMassexcessdef=M‚àíAu. (11.28)\nHydrogenexistsassevenisotopes,threeofwhichoccurnaturally[Haynes,2017]:\nNAtomic mass (u) NAtomic mass (u)\n0 1.007827032 4 5.035\n1 2.014101778 5 6.045\n2 3.016049278 6 7.05\n3 4.026\nHere‚Äôsourprogram TensorBE.py thatcalculatesthemassexcessandproducestheleftpart\nofFigure11.7:\n1# TensorBE.py: TensorFlow calc H isotope binding E‚Äôs\nimporttensorflow as tf\nimportmatplotlib.pyplot as mpl\n5importnumpy as np\nB=n p .z e r o s( 7 ) #[0 ,0 ,0 ,0 ,0 ,0 ,0]\nmP = tf.constant(938.2592) # Proton mass\nmN= tf.constant(939.5527) # Neutron mass\n9mH= tf.multiply(1.00784, 931.494028) #Hm a s s i nM e V / c 2\nam = tf.constant([1.007825032, 2.01401778,\\n3.016049278, 4.026, 5.035, 6.045, 7.05]) # Masses\nA = tf.constant([1, 2., 3., 4., 5., 6., 7.]) # Atomic numbers\n13foriin range (7):\nC=m H+(i) ‚àómN‚àíam[ i ] ‚àó931.494028\nAN = A[ i ]\nB[i]= C/AN\n17print(""BN :"",B[i] )\nmpl.ylabel( ‚ÄôBinding energy per nucleon (MeV)‚Äô )\nmpl.xlabel( ‚ÄôAtomic mass number‚Äô )\nmpl.plot(A,B)\n21mpl.show()\n10.00.51.01.52.02.5\n0.00.51.01.52.02.5\n234\nMass number3rd degree poly\n4th degree polyBinding energy per nucleonBinding energy per nucleon\n5 6 7 1234\nMass number567\nFigure 11.7 Left: TensorFlow‚Äôs linear regression Ô¨Åt to the binding energies of seven hydrogen\nisotopes. Right: Third- and fourth-degree polynomial Ô¨Åts to",5272
118-11.5.1.1 Gradient Tape.pdf,118-11.5.1.1 Gradient Tape,"238 11 Neural Networks and Machine Learning\n11.5.1 Preprocessing with Scikit-learn\nThePythonpackage scikit-learn ,akasklearn,isalibraryofalgorithmsusedinMLforthe\nclassification,regression(fitting),andclusteringofdata.Itisoftenusedintheprocessing\noflargedatasets.Thepackageisinstalledfromashellwiththecommand:\npip install scikit-learn .\nHere pipisPython‚Äôspackageinstaller,andit‚Äôsalsousefulforensuringthatyoursoftware\nisuptodate:\npip upgrade scikit-learn .\nProblem Makeabestfit(regression)ofapolynomialtothehydrogenisotopes‚Äôbinding\nenergiesasafunctionofatomicmassnumber.\nHere‚Äôsourprogram SkPolyFit.py thatusessklearn‚Äôspolynomialandlinearregressionmeth-\nods,andwhichwe‚Äôlldiscussbelow:\n# SkPolyFit.py: Polynomial regression with sklearn\n3importnumpy as np\nfromsklearn.preprocessing importPolynomialFeatures\nfromsklearn.linear_model importLinearRegression\nimportmatplotlib.pyplot as plt\n7importnumpy as np\npoly = PolynomialFeatures(degree=6, include_bias=False) # Degree 6 poly\nm A =n p . a r r a y ( [ 1 ,2 ,3 ,4 ,5 ,6 ,7 ] ) #A t o m i cm a s sn u m b e r\n11poly_features = poly.fit_transform(mA.reshape( ‚àí1, 1)) #D a t a\nB = [0.0140, 1.1520, 2.8235, 1.8150, 1.3871, 0.9465, 1.2971] #B E / N\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(poly_features , B)\n15b_predicted = poly_reg_model.predict(poly_features)\nintcp = poly_reg_model.intercept_ ,\nprint(intcp)\ncoefs = poly_reg_model.coef_ # Polynomial coefficients\n19print(coefs)\ndefpredict_y_value(x):\ny=‚àí1.91 + 1.72 ‚àóx + 0.288 ‚àó(x‚àó‚àó2)‚àí0.182 ‚àó(x‚àó‚àó3) + 0.016 ‚àó(x‚àó‚àó4)\n23returny\ndefpred_y_val(x):\ny=i n t c p+c o e f s [ 0 ] ‚àóx+c o e f s [ 1 ] ‚àóx‚àóx+c o e f s [ 2 ] ‚àóx‚àó‚àó3\nreturny\n27xx = np.linspace(1,7,50) # Plot polynomial\nyy = predict_y_value(xx)\ny4 = pred_y_val(xx)\nfig , ax = plt.subplots()\n31ax.scatter(mA,B) # Plot points\nplt.xlabel( ‚ÄôMass Number‚Äô )\nplt.ylabel( ‚ÄôBinding Energy per nucleon‚Äô )\nplt.plot(xx, yy, c = ""red"",l a b e l = ""3rd degree poly"" ) # Solid line\n35plt.legend()\nplt.plot(xx, y4, label = ""4th degree poly"" )\nplt.legend()\nplt .show()\nAlthoughlifewouldbesimpleriftherewereastandardwaytostorematricesandarrays,\nitisnotourchoicetomake.Specifically,scikit-learnworkswith2D verticalarrays ,which\naredifferentfromthefamiliarNumPyarrays.Theseverticalarraysaremoreefficientwhen",2295
119-11.6 ML Clustering.pdf,119-11.6 ML Clustering,"11.5 TensorFlow and SkLearn Examples 239\ndealing with sparsematrices containing many zeros. Here‚Äôs a NumPy array and its SciPy\nsparsematrixversionusingcompressedrowstorage(CSR)format:\nNumPy Array: SciPy Sparse CSR Matrix:\n2[ [1. 0. 0. 0.] (0, 0) 1.0\n[0. 1. 0. 0.] (1, 1) 1.0\n[0. 0. 1. 0.] (2, 2) 1.0\n[0. 0. 0. 1.] ] (3, 3) 1.0\nReturning to our code SkPolyFit.py in the listing above, notice the column of B\nvalues, with the mass array mAreshaped into a 2D column (the -1 on L11). On L16,\nintcp = poly_reg_model.intercept_ and coefs = poly_reg_model.coef_ give the\nintercept and the coefficients of the fitted polynomial. A third- and fourth-degree poly-\nnomial fit are shown on the right of Figure 11.7. Neither fit is very good, which is to be\nexpectedsincenuclearbindingisnotasimpleprocess.\n11.5.1.1 Gradient Tape\nWehavealreadyseeninourworkwithasimpleneuralnetworkthatMLinvolvesso-called\nbackwardpass operationsthatrepeatacalculationusingtheoutputfromapriorexecution.\nToavoidhavingtoredefine‚Äúnew‚Äùand‚Äúold‚Äùversionsofvariablesandfunctions,Tensor-\nFlowhasa GradientTape commandthatactslikeataperecorderthatstoresintermediate\nresultsforfutureuse.Here,andintheprogramstofollow,areexamplesofitsuse:\n# GradTape.py: Use of Tensor Flow ‚Äôs GradientTape\nimporttensorflow as tf\n3m= tf.Variable(1.5)\nb = tf.Variable(2.2)\nx = tf.Variable(0.5)\ny = tf.Variable(1.8)\n7with tf.GradientTape() as tape:\nz = tf.add(tf.multiply(m, x), b) #z= m x + b\nloss = tf.reduce_sum(tf.square(y ‚àíz)) #( y‚àí(mx+ b ) ) ‚àó‚àó2\ndloss_dx = tape.gradient(loss , x) #G r a d i e n t\n11tf.print(‚ÄôdL/dx:‚Äô , dloss_dx) # Output dL/dx: 3.45000029\ntf.print(2‚àó(‚àím)‚àó(y‚àí(m‚àóx+b))) # Check: output 3.45000029\nWeseethat GradientTape.py hasrecordedthelossfunction Óà∏=[y‚àí(mx+b)]2,andeval-\nuatedthegradientof Óà∏.\n11.5.2 Linear Fit to Hubble‚Äôs Data\nIn1924,Hubblefittedastraightlineofslope ‚àº500(km/s)/Mpctohismeasurementsofthe\nrecessionalvelocityofnebulaeversustheirdistancefromEarth[Hubble,1929].Werepeat\nthat fitting using TensorFlow‚Äôs minimization of the Loss function. Listing 11.6 gives our\nprogram Hubble.py thatdoesthefitting,andinFigure11.8weshowtheinitialandfinal\nfits.Noteintheprogram:\n‚óèOnL6-11thedataareenteredexplicitlyviathe tf.Variable command.\n‚óèOn L17 x_trainis assigned to r,a n do nL 1 9 y_trainis set equal to the equation of a\nstraightline, y=mx+b.\n‚óèOnL25 tf.reduce_mean(tf.square(y_pred - y_true)) isusedtopredictthemean-square\nerror.\n240 11 Neural Networks and Machine Learning\n0.00‚Äì20002004006008001000Step 0, Loss 307477.031250, m 24.598164 Step 290, Loss 221.644241, m 477.953796\n0.25 0.50 0.75 1.00\nrV\n1.25 1.50 1.75 2.00 0.00 0.25 0.50 0.75 1.00\nr1.25 1.50 1.75 2.00\nFigure 11.8 TensorFlow‚Äôs Ô¨Årst (left) and Ô¨Ånal (right) Ô¨Åts to Hubble‚Äôs data of velocities ùë£of nebulae\nversus their distance r.\n‚óèThe fitting is done by predicting values for y, computing the resulting loss, and then\nusing the computed gradient of the loss function to guess new values for mand b.T h e\nprocessisrepeatedsome300times.\n11.6 ML Clustering\nA key element in ML‚Äôs interpretation of data, and in its training of a neural network, is\ngroupingdatainto clustersbaseduponsomecommonfeatures.Thiscanrevealsimilarities,\nordifferences,inthedataelements,andcanbeusedtohighlightunusualelementswithin\nthedata.Ifthedataaregivenwithlabels,asinTable11.2,thenthisis supervisedlearning ;\notherwise,it‚Äôs unsupervisedlearning .Nottothinkthatthisisjustbookkeeping;the cluster-\ningproblemisclassifiedascomputationallydifficult( NP-hard),whichmeansitcannotbe\nsolvedinpolynomialtime,whichmeansittakesalotofcomputingtimetosolveit.\nTheScikit-learn package,whichwehaveintroducedinSection11.5.1,clustersdatavia\nunsupervisedML.YoutellScikit-learnthenumber kofclustersyouwanttoform,andthe\nprogramsearchesthroughtheelementstofindthemostmeaningfulclusters.Anintegral\nTable 11.1 Data for 18 elementary particles tabulated by Index,\nName, and Mass [PDG, 2023].\nIndex Name Mass (MeV/ c2)Index Name Mass (MeV/ c2)\n1ùúà0.8√ó10‚àí610K¬±493.677\n2 e 0.5110041 11 p 938.2721\n3ùúá‚àí105.65 12 n 939.5654\n4ùúá105.6583 13Œõ1115.683\n5ùúã0134.98 14Œ£+1180.37\n6ùúã+139.57 15Œ£‚àí1197.449\n7ùúã‚àí139.57 16Œû01314.86\n8ùúÇ547.862 17Œû‚àí1321.71\n9K0497.611 18Œ©‚àí1672.45\n11.6 ML Clustering 241\nTable 11.2 Fourteen elementary particles and their masses.\nNumber Name Masse Number Name Mass Number Name Mass\n0 neutrino 0 7 eta 548 14 Sigma ‚àí1197\n1 electron 0.5 8 K0 498 15 Xi0 1315\n2m u ‚àí1069K +‚àí49416 Xi ‚àí1322\n3 mu 106 10 p 938 17 Omega ‚àí1672\n4 pi0 1345 11 n 940 12 Lambda 1115\n5p i +1406p i ‚àí14013 Sigma +1180\npart of that search is the calculation of the centroid of each cluster, that is, the location\nwithin each cluster that minimizes the squared-distance to that cluster‚Äôs elements. The\nlearningprocesshastheprogramrepeatedlytryingoutdifferentclusters,withtheprocess\nendingwhenthenewcentroidsdonotmove,orafterafixednumberofcycles.\nProblem You are given Table 11.1 containing the masses of 18 elementary particles\n[PDG,2023].Formthreeclustersoftheseparticlesbasedontheirmasses.\nOurprogram KmeansCluster.py isgiveninListing11.4,andFigure11.9showsthethree\nclusters,andtheircentroids,thatitfound.Thereclearlyisalow-masscluster,ahigh-mass\ncluster,andasmallmedium-masscluster.Noticeintheprogram:\n‚óèOnL7-12thedataareentereddirectlyintotheprogramasaNumPyarray.\n‚óèL13informs KMeansthatwewantthreecentroids,withinitialrandomelements.\n‚óèL14determinestheinitialclustersby kmeans.fit .\n‚óèTheprogramtriesoutdifferentclusters,andpredictsnewlocationsforthecentroids,as\nitlooksforaminimumintheLossfunction.\n‚óèTheprogramcreatesascatterplotwiththecentroidlocationsshownasdiamonds,and\nthedataelementsasseparatecolorsforeachcluster.\n2.502505007501000Code125015001750\n5.0 7.5 10.0 12.5 15.0 17.5\nFigure 11.9 Clustered elementary particles from KmeansCluster.py showing three clusters and\ntheir centroids as diamonds.",5855
120-11.6.1 Reading Files with Panda.pdf,120-11.6.1 Reading Files with Panda,,0
121-11.7 Keras Pythons Deep Learning API.pdf,121-11.7 Keras Pythons Deep Learning API,"242 11 Neural Networks and Machine Learning\n11.6.1 Reading Files with Panda\nInthepreviousexercise,weentereddatadirectlyintotheprogram KmeansCluster.py .This\nreally wouldn‚Äôt do for large datasets, or for analysing a number of datasets. The Python\npackagePandasiswhatweneed,asitprovidesanumberoftoolsformanipulatingandana-\nlyzingdata.Itisparticularlyusefulforinputtingdataintabular(column)form,incontrast\ntoNumPy,whichworkswithdatainarrayform.\nProblem RepeatthelastproblemusingPandastoreadinthedatafromafile.\nOurprogram PandaRead.py isgiveninListing11.7,whereyoumaynoticethatL7readsin\ntheentirefile C:ElemnPart.dat fromTable11.1using‚Äúwhitespace‚Äùascolumnseparators.\nL8eliminatesthesuperfluous‚ÄúName‚Äùcolumn,whileonL10the Xvariableisassignedto\n‚ÄúNumber,‚Äùandthe yvariableto‚ÄúMass.‚ÄùThen,asbefore, Kmeansusesthesevariablestofind\nthreeclustersbasedonMass.\n11.6.2 Clustering with Perceptrons\nInSection11.1.1weintroducedPerceptronsasthehistorical,artificialneuralnetinwhicha\nneuronfiresornot,dependingonsomethresholdvalue.Althoughperceptronsarenotstate-\nof-the-artAI,theyareusefulforsmallerdataframes(datastructureswiththedataarranged\nina2Dtableofrowsandcolumns) .\nProblem YouaregivenTable11.2containing14elementaryparticlesandseveraloftheir\nproperties. Use a Perceptron to cluster the particles into four groups, labeled by the type\nindexT,basedontheirproperties.\nThe program Perceptron.py in Listing 11.5 uses Python‚Äôs sklearn package to create a\nperception , and then proceeds to find clusters of the particles. What‚Äôs new here is the\nuse of a classifier algorithm that assumes an approximate linear behavior of the Loss\nfunction:\nÓà∏‚âÉùë§Tx+b, (11.29)\nanddetermines ùë§Tandbfromthetrainingdata.Thelearningroutineiteratesoverthedata,\nupdatingtheweights ùë§via:\nùë§‚Üíùë§‚àíùúÇùúïÓà∏(ùë§Txi+b,yi)\nùúïùë§, (11.30)\nwhereùúÇisthelearningrate parameter.Toprovidestabilityandprecision,thelearningrate\nforcycletismadetodecreasegraduallythroughthetrainingdata:\nùúÇ(t)=1\nùõº(t0+t). (11.31)\nSomeexplicitstepsof Perceptron.py programare:\n‚óèL8 uses pandasto read in the columnar data, and L9-10 assigns Xto ‚ÄúMass‚Äù and y\n(‚ÄúName‚Äù)tothetypeindex T.\n11.6 ML Clustering 243\n‚óèL13-16splitsthedataintotrainingandtestgroups,andplacesthemintoadataframe d\nwithcolumnslabeled Typeandmass.L15specifiesaseedfortherandomnumbers,and\nalearningrate etabetween0and1.\n‚óèThedataarescaledintostandardform(zeromeans,variance1)onL21-22,andreassigned\ntonewtrainingandtestingvariablesonL24-25.\n‚óèL26-29importsthe perceptron ,andusesittomakeanMLfittothedata.Asistypicalfor\nAI,wearenotgivendetailsaboutjusthowthatisdone.The linear_model specification\non L26 tells the perceptron to combine the weights of the input signals linearly, and\ncomparetheresulttoathreshold ùúÉinordertodecidewhetheraneuronshouldfireor\nnot:\nifùúô(ùë§1x1+ùë§2x2+¬∑¬∑¬∑+ùë§mxm)>ùúÉ,fire. (11.32)\nThefitismadewiththe ppn.fit(X_train_std,y_train) command,andobtainsanaccuracy\n0.909(‚Äú missclasified ‚Äú)ongroup1ofthetestingdata.\n‚óèTherestoftheprogramdeterminestheaccuracyofthefit,andthenoutputstheresults\nusingMatplotlibandcolorstodesignatetheclusters.\n‚óèFigure11.10leftshowsthefourclassespredictedbythe perceptron .Thedataarecon-\ntainedwellwithintheirclusters,thoughnotperfectlyso.\n11.6.3 Clustering with Stochastic Gradient Descent\nInSection11.2.4weincorporated StochasticGradientDescent (SGD)inoursimplenetwork\nasanoptimizationtechniquetominimizetheLoss.‚ÄúStochastic‚Äùreferstothepresenceof\nrandomnessintheiterativesearchfortheminimum,and‚Äúgradientdescent‚Äùtotheuseof\nthedirectionofthegradientoftheLossfunctionasthedirectioninwhichtomoveinthe\nsearch.\nAsgiveninListing11.8,weagainanalyzethedatasetofthe14elementaryparticlesin\nTable11.2,butnowwith supervisedlearning .ThePerceptron‚Äôsclusteringoftheparticlesis\nnowbasedon MassandType (Number) asthelabel.Thetrainingdataareinputandplaced\nin random order, and then shuffled after each training period to avoid cycles. The final\noutputisshowninFigure11.10right,wherethedashedlines,called hyperplanes ,arethe\ndividinglinesbetweensubspaces.Itisseenthattheclusteringissimilartothatfoundwith\nthepreviousperceptron( Perceptron.py )ontheleft,butnotidentical.\n‚Äì3 ‚Äì2 ‚Äì2‚Äì2‚Äì1012 0\n1Test set\nTest set\nTest set\nTest set2\n3\n‚Äì2‚Äì1012\n‚Äì1 0 1 2 ‚Äì1 0\nMass MassType\nType\n12\nFigure 11.10 Left: Clustering of the data from Table 11.2. The Perceptron‚Äôs clustering of particles\nbased on their Type (Number) and Mass is shown in shades of grey. Right: The clusters found with\nthe SGD algorithm.",4455
122-11.8 Image Processing with OpenCV.pdf,122-11.8 Image Processing with OpenCV,"244 11 Neural Networks and Machine Learning\n050 000100 000150 000200 000Loss250 000\n250 500 750 1000 1250 1500 1750 2000\nEpoch0.00‚Äì2000200400V\nr6008001000\n0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00\nFigure 11.11 Left: The decrease in Loss with increasing epochs. Right: The linear regression Ô¨Åt to\nHubble‚Äôs data.\n11.7 Keras: Python‚Äôs Deep Learning API\n[Keras, 2023] is Python‚Äôs Application Program Interface (API) for building and training\ndeeplearning neuralnets.AsweindicatedinSection11.3,deeplearningreferstoneural\nnetswithmultiplelayersofneuronsthroughwhichdataaretransferredsuccessivelydown\nthroughthelayers.A layeristhebasicelementinadeepneuralnetwork;itreceivesinput\ninformation,processesitwithvariousactivationfunctions,biases,andweights,andthen\npassesitsoutputontoalowerlayer.A denselayerisoneinwhicheachneuroninthelayer\nreceivesinputfrom alloftheneuronsinapreviouslayer.Computationally,theinputtoa\nlayerisfedthenumberofneurons(units)inthepreviouslayer,theweightsfortheneurons‚Äô\ninputs,theiractivationfunctions,constraintsontheweights,andregularizerstooptimize\ntheoutput.Here‚ÄôstheKerascommandtodoallthat:\ntf.keras.layers.Dense(\nunits, activation = None, use_bias = True, kernel_initializer = ""glorot_uniform"" ,\nbias_initializer = ""zeros"", kernel_regularizer = None, bias_regularizer =\nNone, activity_regularizer = None,\nkernel_constraint = None, bias_constraint = None, ‚àó‚àókwargs )\nOurprogram Keras.pyinListing11.9againfitsastraightlinetoHubble‚Äôsdata,nowwith\nonedenselayer[TechBrij,2020].Figure11.11leftshowsitsoutput,whereyouwillnoticea\nrapiddecreaseinLossasthetraininggoesthroughthousandsofepochs.Figure11.11right\nshowsthefinalfittothedata.Here‚Äôswhatthevariablesmean:\n‚óèunits:Thedimensionoftheoutputvector.\n‚óèactivation : The neurons‚Äô activation functions: sigmoid,relu(rectified linear), tanh\n(hyperbolictangent), selu(scaledexponentiallinearunit).\n‚óèbias:thenumberaddedintotheneurons‚Äôresponses.\n‚óèkernel_initializer :initializestheweightsmatrix.\n11.8 Image Processing with OpenCV\nProblem Separateripestrawberriesfromgreenonesusingimagesofthem.\n11.8 Image Processing with OpenCV 245\nFigure 11.12 Left: Ripe strawberries. Right: Not so ripe strawberries.\nRipe\nTone0020 00040 00060 00080 000100 000120 000140 000\n50 100Blue\nGreen\nRed\n150 200\nToneNot ripe\n250 0 50 100 150 200 250020 00040 00060 00080 000100 000120 000140 000\nGreen\nRedBlue\nFigure 11.13 Left: The 256 tones in each of three colors for ripe strawberries. Right: The 256\ntones in each of three colors for not quite ripe strawberries.\nMLispopularforimagerecognitionandprocessing,and OpenCVisalibraryofcomputer\nvision (CV) programs including modules for machine learning and neural networks.\nOpenCVisinstalledfromashellwithwhicheverofthesecommandsworkbest:\npip install opencv-python\npip install -user opencv-contrib-python\nComputerimagesarecompositesofpixelsusingcombinationsofRed,Green,andBlue\n(RGB)colors.Typically,theamountofeachcolorpresentisrepresentedbyasinglebyte,\nwhichpermits28=256levels(leavingoff0).Thatbeingthecase,therecanbe256 √ó256√ó\n256=16,777,216RGBtones.OpenCVanalyzesanimageanddetermineshowmanypixels\narepresentineachoftheRGBtones.\nTo solve our fruit problem, consider Figure 11.12 showing some ripe, and some not so\nripe,strawberries.WewanttousevisualprocessingandMLtoseparatestrawberriesinto\ntheirdifferentstatesofripeness(orcoffeebeansintodifferentlevelsofroastings,ifyouare\nnotafruitperson.)Wedoitbyforminghistogramsshowingtheamountofeachofthe255\ntonespresent,foreachofthethreecolors.WedothisinFigure11.13,andimagineusing\nthedifferencesinthehistogramstoseparatethefruit.Hereisourprogramthatreadsinthe\nimage ripe2.jpgandproducesthehistogramsinFigure11.13:",3702
123-11.9 Explore ML Data Repositories.pdf,123-11.9 Explore ML Data Repositories,"246 11 Neural Networks and Machine Learning\n1importnumpy as np\nimportcv2 as cv\nimportmatplotlib.pyplot as plt\nimage = cv.imread( ""c:/ripe2.jpg"" ) #R e a di m a g e\n5fig , ax =plt.subplots()\nhist = cv.calcHist([image], [0], None, [256], [0,256]) # 3 colors\nax.plot(hist , color = ‚Äôb‚Äô, linestyle= ‚Äô-‚Äô)\nhist = cv.calcHist([image],[1], None,[256], [0,256])\n9ax.plot(hist , color = ‚Äôg‚Äô,linestyle= ‚Äô-.‚Äô)\nhist = cv.calcHist([image], [2], None, [256], [0,256])\nax.plot(hist , color = ‚Äôr‚Äô, linestyle= ‚Äô:‚Äô)\nplt.legend([ ""blue"",""green"",""red""])\n13plt.title( ""ripe2"")\nplt.xlim([0,256])\nplt.ylim([0,150000])\nplt .show()\n11.8.1 Background Subtraction\nAnothertypeofimageprocessingremovesastaticbackgroundfromavideorecording,as\nmightbeneededinthesearchforexoplanetsorsupernovas.Thisisaccomplishedbylook-\ningatthedifferencebetweensuccessiveimageframes,andremovingthepartsthatdonot\nchange.Ourprogram,below,processedanavifile,andproducedFigure11.14,whereyou\nwillseethat,otherthantherisingsmokeandthemovingpiston,thebackgroundhasbeen\nremoved.Theprogramwaswrittenbyoneoftheauthors(MJP)foravirtualphysicscourse\nintheProgram@udeaatLaUniversidaddeAntioquia.\nimportcv2 as cv\nsub_backg = cv.createBackgroundSubtractorMOG2()\ncap = cv.VideoCapture( ‚Äôc:/vapor.avi‚Äô )\n4while(1):\nret , frame = cap.read()\nimgNoBg = sub_backg.apply(frame)\n#fgmask = fgbg . apply(frame)\n8cv.imshow( ‚Äôframe‚Äô, frame)\ncv.imshow( ""no bkgr"" , imgNoBg)\nk = cv.waitKey(30) & 0xff\nifk == 27: break\nFigure 11.14 Left: One frame from an animation in which the piston is moving back and forth in\nfront of a background image. Right: An image in which the stationary part (background) of the video\nhas been removed.",1689
124-11.10 Code Listings.pdf,124-11.10 Code Listings,"11.10 Code Listings 247\n11.9 Explore ML Data Repositories\nTryusingsomeofthetoolspresentedhereonrealdatasets.Findonethatinterestsyou,or\nlookhere(someofwhichareusedincompetitions):\nDeep learning physics open data: www.deeplearnphysics.org/DataChallenge/\nMLPhysics portal: mlphysics.ics.uci.edu/\nParticle tracking challenge: www.kaggle.com/c/trackml-particle-identification\n17 datasets for physics: paperswithcode.com/datasets?mod=physics\nCarbon nanotubes: www.kaggle.com/inancigdem/carbon-nanotubes\nPublic datasets ‚Äì IML ‚Äì CERN: iml.web.cern.ch/public-datasets\nMolecular properties: www.kaggle.com/c/champs-scalar-coupling\nSteel defect detection: www.kaggle.com/c/severstal-steel-defect-detection\n11.10 Code Listings\nListing 11.1 Neuron.py, AnAIneuron.\n# Neuron . py : An AI neuron\n2\nimportnumpy as np\ndeff(x) :return1./ (1. + np.exp( ‚àíx)) # Activation function\n6classNeuron :\ndef__init__(self , weights, bias) :\nself.weights = weights\nself.bias = bias\n10\ndeffeedforward(self , inputs) : # Process input\nSum = np.dot (self.weights, inputs) + self.bias\nreturnf(S u m)\n14\nweights = np.array([ ‚àí1., 1.]) #w 1= ‚àí1, w 2 = 1\nbias = 0\nn = Neuron(weights,bias)\n18x = np.array([12,8]) # x1 = 12 , x2 = 8\nprint(n.feedforward(x))\n# output: 0.01798620996209156\nListing 11.2 NeuralNet.py AsimpleAIneuralnetwork.\n# NeuralNet .py : A simple AI neural network\n2\nimportnumpy as np\ndeff(x) :return1./ (1. + np.exp( ‚àíx)) # Activation function\n6classNeuron :\ndef__init__(self , weights, bias) :\nself.weights = weights\nself.bias = bias\n10deffeedforward(self , inputs) : # Process input\nSum = np.dot (self.weights, inputs) + self.bias\nreturnf(S u m)\n14weights = np.array([ ‚àí1., 1.]) #w 1= ‚àí1, w 2 = 1\nbias = 0\nn = Neuron(weights,bias)\nx = np.array([12,8]) # x1 = 12 , x2=8\n248 11 Neural Networks and Machine Learning\n18print(n.feedforward(x)) # Output: 0.01798620996209156\nclassNeuralNetwork: #2‚àíneuron network , 2 hidden layers , 1 output\ndef__init__(self):\n22weights = np.array([0,1])\nbias = 0\nself.h1 = Neuron(weights, bias) # Neuron class as before\nself.h2 = Neuron(weights, bias)\n26self.O = Neuron(weights, bias)\ndeffeedforward(self , x):\nout_h1 = self.h1.feedforward(x)\n30out_h2 = self.h2.feedforward(x)\nout_out = self.O.feedforward(np.array([out_h1, out_h2]))\nreturnout_out\nnetwork = NeuralNetwork()\n34x = np.array([2, 3])\nprint(network.feedforward(x)) #output: 0.7216325609518421\nListing 11.3 SimpleNet.py Apythoncodeforoursimpleneuralnetwork.\n1# SimpleNet .py : A simple neuron network\nimportnumpy as np\ndeff(x):return1/(1 + np.exp( ‚àíx)) # Sigmoid activation function\n5deffprime(x): returnnp.exp( ‚àíx)/(1 + np.exp( ‚àíx))‚àó‚àó2# Sigmoid derivs\ndefLoss(y_true, y_out):\nlos = ((y_true ‚àíy_out) ‚àó‚àó2).mean()\n#print(los)\n9returnlos\nclassSimpleNet: # x _ 1 ,x _ 2i n ,h i d d e n h 1 ,h 2 ,y _ o u t\ndef__init__(self): # R a n d o m inits\n13 self.w1=np.random.normal() # Weights\nself.w2=np.random.normal()\nself.w3=np.random.normal()\nself.w4=np.random.normal()\n17 self.w5=np.random.normal()\nself.w6=np.random.normal()\nself.b1=np.random.normal() # Biases\nself.b2=np.random.normal()\n21 self.b3=np.random.normal()\ndeffeedfwd(self , x):\nh1 = f(self .w1 ‚àóx[0] + self.w2 ‚àóx[1] + self.b1)\n25 h2 = f(self .w3 ‚àóx[0] + self.w4 ‚àóx[1] + self.b2)\nout = f(self.w5 ‚àóh1 + self .w6 ‚àóh2 + self .b3)\nreturnout\n29deftrain (self , data, all_y_trues):\nlearn_rate = 0.1\nN = 1000 # Number of learning loops\nfornin range (N):\n33 forx, y_true inzip(data, all_y_trues):\nsum_h1 = self .w1 ‚àóx[0] + self.w2 ‚àóx[1] + self.b1\nh1 = f(sum_h1)\nsum_h2 = self .w3 ‚àóx[0] + self.w4 ‚àóx[1] + self.b2\n37 h2 = f(sum_h2)\nsum_out = self .w5 ‚àóh1 + self .w6 ‚àóh2 + self .b3\nout = f(sum_out)\ny_out = out\n41 d_L_d_yout = ‚àí2‚àó(y_true ‚àíy_out) # Partial deriv\nd_yout_d_w5 = h1 ‚àófprime(sum_out) # Output neuron\nd_yout_d_w6 = h2 ‚àófprime(sum_out)\nd_yout_d_b3 = fprime(sum_out)\n45 d_yout_d_h1 = self.w5 ‚àófprime(sum_out)\nd_yout_d_h2 = self.w6 ‚àófprime(sum_out )\nd_h1_d_w1 = x[0] ‚àófprime(sum_h1) # Hidden Neuron h1\nd_h1_d_w2 = x[1] ‚àófprime(sum_h1)\n11.10 Code Listings 249\n49 d_h1_d_b1 = fprime(sum_h1)\nd_h2_d_w3 = x[0] ‚àófprime(sum_h2) # Hidden Neuron h2\nd_h2_d_w4 = x[1] ‚àófprime(sum_h2)\nd_h2_d_b2 = fprime(sum_h2)\n53 # Update weights and biases\nself.w1 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h1 ‚àód_h1_d_w1 #h 1\nself.w2 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h1 ‚àód_h1_d_w2\nself.b1 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h1 ‚àód_h1_d_b1\n57 self.w3 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h2 ‚àód_h2_d_w3 #h 2\nself.w4 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h2 ‚àód_h2_d_w4\nself.b2 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_h2 ‚àód_h2_d_b2\nself.w5 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_w5 #O u tn ‚Äô s\n61 self.w6 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_w6\nself.b3 ‚àí= learn_rate ‚àód_L_d_yout ‚àód_yout_d_b3\nif(n%10) == 0: #L o s sa tl o o pe n d s\ny_outs = np.apply_along_axis(self.feedfwd,1,data)\n65 TotLoss =Loss(all_y_trues ,y_outs)\n#print("" resta "" ,(( all_y_trues ‚àíy_outs) ‚àó‚àó2) .mean() )\n#print(""y_trues"" , all_y_trues)\nprint("" Loop n = %d Loss: %.3f"" % (n, TotLoss))\n69data = np.array ([[ ‚àí2,‚àí1], [25, 6], [17, 4],[ ‚àí15,‚àí6] ]) #I n p u tD a t a\nall_y_trues = np.array ([ 1, 0, 0, 1 ])\nnetwork = SimpleNet() #T r a i nn e t\nnetwork.train(data, all_y_trues)\nListing 11.4 KmeansCluster.py Clusteringofdatawithsklearn‚ÄôsKmeans.\n# KmeansCluster.py: Clustering with sklearn ‚Äôs KMeans\n2\nfromsklearn.cluster importKMeans\nimportmatplotlib.pyplot as plt\nimportnumpy as np\n6%matplotlib inline\nX = np.array([\n[1, 0], [2, 0.511], [3, 105.65], [4, 105.6583], [5, 134.98],\n[6, 139.57],[7,139.57],[8,547.86],[9,497.68],[10,493.677],\n10[11,938.2721],[12,939.5654],[13,1115.68],[14,1180.37],\n[15,1197.5],[16,1314.86], [17,1321.71],[18,1672.45]\n])\nkmeans = KMeans(n_clusters=3, random_state=42) # 3 random centroids\n14kmeans. fit (X) # Compute clustering\nkmeans.predict(X) # Predict closest cluster\nkmeans.labels_\ncc = kmeans.cluster_centers_ # Cluster centers\n18print(""cc:"",cc)\nfig , ax = plt.subplots()\nplt.xlabel( ""N"")\nplt.ylabel( ""Code"")\n22plt.scatter(X[:,0],X[:,1],c=kmeans.labels_, marker= ""ÀÜ"")\nplt.scatter(cc[:,0],cc[:,1],c= ‚Äôred‚Äô,m a r k e r = ""D"")#with diamonds\nplt .show()\nListing 11.5 Perceptron.py Sklearn‚Äôs Perceptron commandcreatesaperceptron.\n# Perceptron .py: Creat perceptron with sklearn\n2\nimportpandas as pd # To read dataset\nimportmatplotlib.pyplot as plt\nimportnumpy as np\n6%matplotlib inline\nparts = pd.read_table( ""C:particle.dat"" ,delim_whitespace=True)\nX = parts[ ""Mass""] # X: masses\n10y = parts[ ‚ÄôT‚Äô] #y :T y p e\nprint(‚ÄôClass labels:‚Äô , np.unique(y)) # The 4 classes\nd={ ‚Äôcol1‚Äô:X,‚Äôcol2‚Äô:y} #d : 2‚àíD array of X &y\n250 11 Neural Networks and Machine Learning\ndfrom sklearn.model_selection importtrain_test_split # Split array\n14X_train, X_test, y_train, y_test = train_test_split(\ndf, y, test_size=0.3, random_state=1, stratify=y) #F o r m2 ‚àíD dataframe\nfromsklearn.model_selection importtrain_test_split # Split array\n18# Shuffle data dataf= pd.DataFrame(d)\nX_train, X_test, y_train, y_test = train_test_split(\ndf, y, test_size=0.3, random_state=1, stratify=y)\nfromsklearn.preprocessing importStandardScaler\n22sc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n26fromsklearn.linear_model importPerceptron\nppn = Perceptron(eta0=0.1, random_state=1)\nppn.fit(X_train_std,y_train) #F i td a t a\ny_pred = ppn.predict(X_test_std)\n30print(‚ÄôMisclassified examples: %d‚Äô % (y_test != y_pred). sum())\nfromsklearn.metrics importaccuracy_score\nprint(‚ÄôAccuracy: %.3f‚Äô % accuracy_score(y_test , y_pred))\nprint(‚ÄôAccuracy: %.3f‚Äô % ppn.score(X_test_std, y_test))\n34frommatplotlib.colors importListedColormap\nfig , ax = plt.subplots()\nplt.xlabel( ""mass"")\nplt.ylabel( ""Type"")\n38\nforiin range (36): # Plot spin (0, 1, 3/2, 1/2) vs mass\nify[i] = = 0: plt.scatter(X[i],y[i], c= ‚Äôred‚Äô,marker= ‚Äôx‚Äô,s=150)\nify[i] = = 1: plt.scatter(X[i],y[i], c= ‚Äôblue‚Äô,marker= ""ÀÜ"",s=150)\n42ify[i] = = 3: plt.scatter(X[i],y[i], c= ‚Äôbrown‚Äô,m a r k e r= "">"",s=150)\nify[i] = = 2: plt.scatter(X[i],y[i], c= ‚Äômagenta‚Äô ,m a r k e r = ""<"",s=150)\nfrommatplotlib.colors importListedColormap\n46defplot_decision_regions(X, y, classifier , test_idx=None, resolution=0.01):\nmarkers = ( ‚Äôs‚Äô,‚Äôx‚Äô,‚Äôo‚Äô,‚ÄôÀÜ‚Äô,‚Äôv‚Äô) #M a r k e r sf o ra n dc o l o rm a p\ncolors = ( ‚Äôbrown‚Äô,‚ÄôpeachPuff‚Äô ,‚Äôlightgreen‚Äô ,‚Äôgold‚Äô,‚Äôcyan‚Äô)\ncmap = ListedColormap(colors[:len(np.unique(y))])\n50x1_min, x1_max = X[: , 0]. min()‚àí1, X[:, 0]. max() + 1\nx2_min, x2_max = X[: , 1]. min()‚àí1, X[:, 1]. max() + 1\nxx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\nnp.arange(x2_min, x2_max, resolution)) # Decision surface\n54Z = classifier.predict(np.array([xx1.ravel() , xx2.ravel()]).T)\nZ = Z.reshape(xx1.shape)\nplt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) # Alpha : Transp\nplt.xlim(xx1. min() , xx1. max())\n58plt.ylim(xx2. min() , xx2. max())\nforidx, cl inenumerate(np.unique(y)):\nplt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],alpha=0.8,\\nc=colors[idx], marker=markers[idx], label=cl,edgecolor= ‚Äôblack‚Äô)\n62 iftest_idx: # Highlight test examples\nX_test, y_test = X[test_idx , :], y[test_idx]\nplt.scatter(X_test[:, 0], X_test[:, 1],edgecolor= ‚Äôblack‚Äô,\\nalpha=1.0,linewidth=1, marker= ‚Äôo‚Äô,s=100, label= ‚Äôtest set‚Äô )\n66plt .show()\nListing 11.6 Hubble.py AlinearfittoHubble‚ÄôsdatausingTensorFlow.\n# Hubble . py : Fit to Hubble dat , adapted from Campesato tensorflow 2 primer\nimporttensorflow as tf\nimportnumpy as np\n4importmatplotlib.pyplot as plt\nr = tf.Variable([0.032,0.034,0.214,0.263, 0.275, 0.275, 0.45, 0.5, 0.5,\\n0.63,0.8,0.9,0.9,0.9,0.9, 1.0,1.1,1.1,1.4,1.7,2.0,2.0,2.0,2.0]) #R\n8v = tf.Variable([170.,290., ‚àí130.,‚àí70.,‚àí185.,‚àí220.,200.,290.,270.,200.,300.,\n‚àí30.,650.,150.,500.,920.,450.,500.,500.,960. ,500.,850.,800.,1090.])\nm= tf.Variable(0.) #I n i tm ,b ; y = m x + b\nb = tf.Variable(0. )\n12slope = 500.\n11.10 Code Listings 251\nbias = 0.0\nstep = 10\nlearning_rate = 0.02\n16steps = 300\nx_train = r\nprint(x_train)\ny_train = slope ‚àóx_train + bias\n20\ndefpredict_y_value(x): #y ( x )\ny=m ‚àóx+b\nreturny\n24defsquared_error(y_pred, y_true): # Sum squared errors\nreturntf.reduce_mean(tf.square(y_pred ‚àíy_true))\nloss = squared_error(predict_y_value(x_train), y_train)\n28foriin range (steps):\nwith tf.GradientTape() as tape:\npredictions = predict_y_value(x_train)\nloss = squared_error(predictions , y_train)\n32gradients = tape.gradient(loss , [m, b])\nm.assign_sub(gradients[0] ‚àólearning_rate)\nb.assign_sub(gradients[1] ‚àólearning_rate)\nif(i % step) == 0:\n36print(""Step %d, Loss %f, m %f "" % (i, loss.numpy(),m))\ny=m ‚àóx_train + b\nplt.xlabel( ""r Mpc"")\nplt.ylabel( ""v km/s"" )\n40plt.scatter(r, v)\nplt.plot(x_train, y)\nplt .show()\nListing 11.7 PandaRead.py Atablereadwithpandas,andclusterIDwithkmeans.\n# PandaRead . py : Read table with pandas and use kmeans to find clusters\nimportpandas as pd\n4fromsklearn.cluster importKMeans\nimportmatplotlib.pyplot as plt\nimportnumpy as np\nparts = pd.read_table( ""C:\ElemnPart.dat"" , delim_whitespace = True)\n8data = parts.drop( ""Name"",a x i s = 1 ) # Drop this column\ndata.head()\nX = np.array(data[ ""Number"" ]) # 1 st column\ny = np.array(data[ ‚ÄôMass‚Äô]) # 2nd column\n12kmeans = KMeans(n_clusters = 3, random_state = 42) # R a n d o m init clusters\nkmeans. fit (data) # Computes clusters\nkmeans.predict(data) # Predict closest cluster\nkmeans.labels_\n16cc = kmeans.cluster_centers_ # Centroids\nprint(cc) # Show centroids\nfig , ax = plt.subplots()\nplt.xlabel( ""N"")\n20plt.ylabel( ""Code"")\nplt.scatter(X[:],y[:],c=kmeans.labels_, marker= ""ÀÜ"") # Arrows\nplt.scatter(cc[:,0],cc[:,1],c= ‚Äôred‚Äô,m a r k e r = ""D"") # Diamonds\nplt .show()\nListing 11.8 SGDclass.py SupervisedMLclassificationviaastochasticgradientdescent\nalgorithmfittotheLossfunction.\n# SGDclass.py: M L via Stochastic Gradient Descent\n3fromsklearn.linear_model importSGDClassifier # StochGradDescent\nfromsklearn.inspection importDecisionBoundaryDisplay # Def region\nimportpandas as pd # To read dataset\n252 11 Neural Networks and Machine Learning\nimportmatplotlib.pyplot as plt\n7importnumpy as np\n%matplotlib inline # Set matplot for notebook\nparts = pd.read_table( ""part.dat"" ,delim_whitespace=True) #R e a dd a t a\nX = parts[ ""Mass""] # X: masses\n11y = parts[ ‚ÄôType‚Äô] # Types (integers)\nprint(‚ÄôClass labels:‚Äô , np.unique(y)) # 4 classes\nd={ ‚Äôcol1‚Äô:x,‚Äôcol2‚Äô:y} # 2 column X, y array\ndf = pd.DataFrame(d) # Form 2d DataFrame\n15X = np.array(df) # DataFrame to numpy array\nidx = np.arange(X.shape[0]) #I n d e x0 ‚àí35\nnp.random.seed(13)\nnp.random.shuffle(idx) # R a n d o m index shuffle\n19X=X [i d x] #R a n d o mXo r d e r\ny=y [ i d x ] #R a n d o mYo r d e r\ncolors = ""bryg"" # 4 class colors\nmean = X.mean(axis=0) #C a l cm e a n\n23std = X.std(axis=0)\nX=( X ‚àímean)/std #N o wm e a n=0\nprint(""mean std"" , mean,std)\nlrgd = SGDClassifier(alpha=0.001, max_iter=100).fit(X,y)\n27print(lrgd) # Alpha: regularization strength\nax = plt.gca()\ndisp = DecisionBoundaryDisplay.from_estimator(lrgd,X, cmap = plt.cm.Paired, ax =\nax, response_method = ""predict"" ,x l a b e l= ""massMeV/c2"" ,ylabel= ""Type"")\nplt.axis( ""tight"")\n31print(""lclasses"" , lrgd.classes_) # 4 classes\nfori, color inzip(lrgd.classes_ , colors): # Plot training points\nidx = np.where(y == i)\nprint(""scatter"" , X[idx,0], X[idx,1])\n35plt . scatter(X[idx ,0] ,X[idx ,1] , c=color , cmap=plt .cm.Paired ,\nedgecolor= ""black"",s=20)\nplt.axis( ""tight"")\nxmin, xmax = plt .xlim()\n39ymin, ymax = plt .ylim()\ncoef = lrgd.coef_ # Average weights for all steps\nintercept = lrgd.intercept_\n43defplot_hyperplane(c, color):\ndefline(x0):\nreturn(‚àí(x0 ‚àócoef[c,0]) ‚àíintercept[c]) / coef[c,1],\nplt . plot([xmin, xmax], [line(xmin) , line(xmax)],\n47 ls=""--"", color=color)\nprint(lrgd.classes_)\nfori, color inzip(lrgd.classes_ , colors): #P l o tl i n e s\nprint(i,color)\n51plot_hyperplane(i, color)\nplt.legend()\nplt .show()\nListing 11.9 Keras.py LinearfirtoHubbledatausingKeras.\n1# Keras.py: Linear regression fit to Hubble data with Keras\nimportmatplotlib.pyplot as plt\nimporttensorflow as tf\n5fromtensorflow importkeras\nfromkerasimportlayers\nfromkerasimportSequential\nfromkeras.layers importDense\n9importnumpy as np\n#D a t a\nr = [0.032,0.034,0.214,0.263,.275,.275,.45,.5,.5,.63,.8,.9,.9,.9,.9,\n1.0,1.1,1.1,1.4,1.7,2.0,2.0,2.0,2.0] # Distance Mparse\n13v = [170.,290., ‚àí130.,‚àí70.,‚àí185.,‚àí220.,200.,290.,270.,200.,300.,\n‚àí30.,650.,150.,500.,920.,450.,500.,500.,960.\n,500.,850.,800.,1090.] # Recession velocity k m/s\n# Create the model: Sequential() only 1 dense layer\n17layer0 = tf.keras.layers.Dense(units=1,input_shape=[1])\n11.10 Code Listings 253\nmodel = tf.keras.Sequential([layer0])\nmodel.compile(loss= ‚Äômean_squared_error‚Äô ,\noptimizer=tf.keras.optimizers.Adam(1))\n21history = model.fit(r,v,epochs=2000,verbose=0)\nplt.plot(history.history[ ‚Äôloss‚Äô])\nplt.xlabel( ""Epochs number"" )\nplt.ylabel( ""Loss"")\n25plt .show()\nweights = layer0.get_weights()\nweight = weights[0][0]\nbias = weights[1]\n29print(‚Äôweight: {} bias: {}‚Äô .format(weight, bias))\ny_learned = r ‚àóweight + bias\nplt.scatter(r, v, c= ‚Äôblue‚Äô)\nplt.plot(r, y_learned,color= ‚Äôr‚Äô)\n33plt .show()\nweights = layer0.get_weights()\nweight = weights[0][0]\nbias = weights[1]\n37print(‚Äôweight: {} bias: {}‚Äô .format(weight, bias))\ny_learned = r ‚àóweight + bias\n# Output: weight: [448.52048] bias : [ ‚àí34.726036]\nplt.scatter(r, v, c= ‚Äôblue‚Äô)\n41plt.plot(r, y_learned,color= ‚Äôr‚Äô)\nplt .show()",15470
125-Chapter 12 Quantum Computing G. He Coauthor.pdf,125-Chapter 12 Quantum Computing G. He Coauthor,,0
126-12.3.1 Physics Exercise Two Entangled Dipoles.pdf,126-12.3.1 Physics Exercise Two Entangled Dipoles,"254\n12\nQuantum Computing (G. He, Coauthor)\nAlthough this is our most-recently added chapter, it is by no means the last word on Quantum\nComputing (QC). Seeing that QC employs its own version of Dirac notation, we start the\nchapter with the quantum-mechanical version of Dirac notation .1We then discuss the foun-\ndation of QC, namely, qubits, entanglement, and quantum gates. We introduce quantum\nprogramming and execution using the Google Cirq framework, and conclude by using the\nphysical IBM Quantum Computer to solve some realistic problems. You may Ô¨Ånd that, much\nlike in the early days of traditional computing, the ‚Äúprograms‚Äù for QC, using gates to process\nbits, are at a (painfully) low level. Further material on QC can be found in our sources [Hidary,\n2021; Stolze and Suter, 2004; Nielsen and Chuang, 2010; IBMqc, 2023; Cirq, 2023] .\nProblem Develop several computer programs that use quantum mechanical states for\nstorageofinformationandcomputation.\n12.1 Dirac Notation in Quantum Mechanics\nQuantum computing (QC) uses a language based on Dirac‚Äôs quantum mechanical nota-\ntion.InDirac‚Äôsformalism,aquantumstateisrepresentedbya ket|ùúì‚ü©,whichisarayinan\nabstract,infinite,orfinite,dimensional,complexHilbertspace.(InQCwith nqubits,the\ndimensionwouldbe2n.)Thefamiliarwavefunction ùúì(x)istheconcrete,coordinate-space\nrepresentationofthatabstractstate,andisobtainedfromitbytheinnerproduct:\nùúì(x)=‚ü®x|ùúì‚ü©. (12.1)\nHere we have formed the product of the ket|ùúì‚ü©with thebra‚ü®x|,t of o r ma‚Äú b r a - k e t ‚Äù\n(bracket). In this view, the wave function ùúì(x), which is the probability amplitude of\nfindingthestate |ùúì‚ü©atx,canbethoughtofastheprojectionof |ùúì‚ü©ontothexbasisvectors.\n1 Formoreofareview,Section7.6usesspinstatesandmatrixoperatorstocalculatethehyperfine\nstructureofhydrogen.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n12.2 From Bits to Qubits 255\nTheDiracformalismalsoincludesa dualadjoint orcovectorspaceinwhichthestate |ùúì‚ü©\nisrepresentedbythebra ‚ü®ùúì|.The1:1correspondencebetweenketsandbrasisexpressed\nwiththeadjointoperation\n‚ü®ùúì|=|ùúì‚ü©‚Ä†. (12.2)\nThescalarorinnerproduct ofthetwostates |ùúô‚ü©and|ùúì‚ü©isgivenbythebracket\n‚ü®ùúô|ùúì‚ü©‚â°(ùúô,ùúì)=‚ü®ùúì|ùúô‚ü©‚àó. (12.3)\nIncontrast,thejuxtaposition O=|ùúô‚ü©‚ü®ùúì|isanoperator,andnotasimplescalarproduct,\nsinceitchangesonestateintoanother:\nO|ùúì‚ü©=|ùúô‚ü©=|Oùúì‚ü©. (12.4)\nHereweuse Otodenoteanoperator,assume ùúì‚ü©isnormalized,andnotethatoperatorsare\noftenrepresentedasmatrices,andthat,ingeneral, O|ùúì‚ü©isnotproportionalto |ùúì‚ü©.\nIfwewanttobeconsistentwithDiracnotation,thenadescriptionofstatesasvectorsin\naspin1/2space Swouldexpressthestatesas ‚ü®S|ùúì‚ü©.Incommonpracticethebra ‚ü®S|isleft\noff,andtheupanddownspin1/2statesarewrittenas\nùúì+=|||+1\n2‚ü©\n=[1\n0]\n‚â°|0‚ü©, (12.5)\nùúì‚àí=|||‚àí1\n2‚ü©\n=[0\n1]\n‚â°|1‚ü©, (12.6)\nwhere |0‚ü©and|1‚ü©arethesymbolsusedinQC(presumablyasananalogytothetraditional\nbits 0 and1 beingrepresented as spin up andspin down). Likewise,operatorslike Oare\nrepresentedby2 √ó2matrices,forexamplebythe directproduct :\n|||1\n2‚ü©‚ü®\n1\n2|||=[1\n0][\n10]=[10\n00]\n. (12.7)\n12.2 From Bits to Qubits\nQuantumcomputing(QC)isbasedonstoringinformationinquantummechanicalstates,\nandthenmanipulatingthesestatestoperformnumericaloperations.Thisisfundamentally\ndifferentfrom,andpotentiallymorepowerfulthan,thetraditionalapproachtocomputing,\nandmaybeespeciallyapplicableinareassuchascryptographyandsimulationofquantum\nsystems.\nInthetraditionalapproachtoacomputer‚Äôsmemory,informationisstoredusinganumber\nsystem based on the binary integers ( bits) 0 and 1. [Originally, a 0 was stored in a mag-\nneticcorepointingup,anda1inacorepointingdown,likethequbitsin(12.5).]Allthe\nrest of what gets stored consists of arrays of these bits. In the (simplest) quantum com-\nputermemorysystem,informationisstoredinstatesthatarecombinationsofelementary\nspin-like states, called quantum bits orqubits. Although it would be a major advance in\nminiaturizationiftheenergylevelsofneutralatomswereusedforqubits,inpracticethe\nstorageuseselectronicdevices,suchassuperconductingACJosephsonjunctions.\nThesmallestunitofinformationinQCisthequantumbitor qubit.Asinglequbitstorage\nunit is expressed in terms ofthe same bases vectorsused for ‚Äúspin-up‚Äù and ‚Äúspin-down‚Äù\n256 12 Quantum Computing (G. He, Coauthor)\nz|0>\n|0> ‚Äì |1>\ny\nx2\n|1>œÜŒ∏œà\n|0> + i|1>\n2|0> ‚Äì i|1>\n2\n|0> + |1>\n2Figure 12.1 The Bloch sphere, a geometric\nrepresentation of a two-level quantum system\n(modiÔ¨Åed www.pngwing.com).\nquantumstates,butwiththenameschangedto0and1:\n|0‚ü©def=|||+1\n2‚ü©\n=[1\n0]\n, |1‚ü©def=|||‚àí1\n2‚ü©\n=[0\n1]\n. (12.8)\nAqubitisdefinedasalinearcombinationofthesetwobasisstates:\n|ùúì‚ü©=u|0‚ü©+ùë£|1‚ü©‚â°[u\nùë£]\n. (12.9)\nHereuandùë£complexnumberssatisfyingthenormalizationcondition:\n|u|2+|ùë£|2=1. (12.10)\nAlthough probability conservation is important in quantum mechanics, the normaliza-\ntionofstatesisoftenjustanarbitraryoverallconstantappliedtothewavefunction,when\nneeded.InQC,however,statesareuniformlyassumedtobenormalized.\nAsillustratedinFigure12.1,becausethestate |ùúì‚ü©isarayinanabstractvectorspace,it\ncanhaveaconcreterepresentationasthedirectionofarayona Blochsphere withthepolar\nanglerepresentation:\n|ùúì‚ü©=cosùúÉ\n2|0‚ü©+eiùúôsinùúÉ\n2|1‚ü©,ùúÉ‚àà[0,ùúã],ùúô‚àà[0,2ùúã). (12.11)\nAccordingly,apure |0‚ü©state(ùúÉ=0)liesonthe +zaxis,andapure |1‚ü©stateliesalongthe\n‚àízaxis(ùúÉ=ùúã),withcomplexcombinationsofthetwolyingsomeplaceonthesurfaceof\nthesphere.\n12.2.1 Multiple Qubit States\nConsider a state ||ùúìA‚ü©within the Hilbert space HA, and a separate state ||ùúìB‚ü©within the\nHilbertspace HB.Ifwewishtocombinethesetwoketsintoasinglestate,thenthecomposite\nwouldexistwithinanexpandedHilbertspacecreatedbythetensorproductof HAandHB,\nwiththestatevectoralsoadirectproduct:\nHAB=HA‚äóHB, (12.12)\n‚áí||ùúìAB‚ü©=||ùúìA‚ü©‚äó||ùúìB‚ü©, (12.13)\nwhere[a\nb]\n‚äó[c\nd]\ndef=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ac\nad\nbc\nbd‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.14)\n12.3 Entangled and Separable States 257\nAsanexplicitexample,ifwestartwiththestates\n||ùúìA‚ü©=u1|0‚ü©+ùë£1|1‚ü©, ||ùúìB‚ü©=u2|0‚ü©+ùë£2|1‚ü©,then (12.15)\n||ùúìA‚ü©‚äó||ùúìB‚ü©‚â°||ùúìA‚ü©||ùúìB‚ü©=(u1|0‚ü©+ùë£1|1‚ü©)(u2|0‚ü©+ùë£2|1‚ü©)(12.16)\n=u1u2|00‚ü©+u1ùë£2|01‚ü©+ùë£1u2|10‚ü©+ùë£1ùë£2|11‚ü©, (12.17)\nwhere |00‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\n0\n0\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,|01‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n1\n0\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,|10‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,|11‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.18)\nThe4-Dvectorsin(12.18)aretheappropriatebasisvectorsforatwo-qubitsystem.\n12.3 Entangled and Separable States\nStatesformedwithadirectproduct,suchasin(12.16),arecalled separable.Forexample,\na qubit in a |0‚ü©state and a different qubit also in a |0‚ü©state form the separable state\n||0A‚ü©‚äó||0B‚ü©, which is usually written as just |00‚ü©. Yet qubits do not live on isolated qubit\nislands, so they can interact with each other. If two interacting systems are otherwise\nisolated,but cannotbe expressed asthe directproductof the two states,these qubits are\nentangled.Ifthetwosystemsarenotentangled,thentheyare separable.\nEntanglementmayleadtosomeprofoundconsequences.Forexample,thespin-upstate\n|0‚ü©andthespin-downstate |1‚ü©canbephysicallyfarfromeachother,yetstillbeentangled.\nThismeansthattheupstatecannotbedescribedasjustasingleparticlestate,butmustbe\ndescribedaspartofthefullstatevectorincludingthedownstate,whereveritmaybe.Soif\nthetotalstatehasspinzero,andoneparticleisspinup,thentheotherparticle,eveniffar\naway,mustbecorrelatedandmusthavespindown.Justhowthetwoparticlescommunicate\nwitheachotherquantummechanicallyatamacroscopicdistancesisasubjectofcurrent\ndiscussionanddebate(andthe2022NobelPrize).\nLet‚Äôs be more explicit about this entanglement concept. Here are two states in the 2D\nHilbertspaceofcomplexnumbers ‚ÑÇ2,\n||ùúìA‚ü©=[a\nb]\n, ||ùúìB‚ü©=[c\nd]\n. (12.19)\nThetensorordirectproduct ofthesestatesis\n|Œ®‚ü©def=||ùúìA‚ü©‚äó||ùúìB‚ü©=[a\nb]\n‚äó[c\nd]\n=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ac\nad\nbc\nbd‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.20)\nThisproductstateisinthe4-DHilbertspaceofcomplexnumbers ‚ÑÇ4:\n|ùúì‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùë§\nx\ny\nz‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.21)\n258 12 Quantum Computing (G. He, Coauthor)\nThestateisseparable,if,andonlyif ,ùë§z=xy.Fortheproductstatein(12.20),separability\nthusrequires acbd=adbc;whichisinfactthecase,andso(12.20)isseparable.Afamous\nexampleofentanglementisthetwo-qubit BellStates:\n||ùõΩ00‚ü©=1‚àö\n2(|00‚ü©+|11‚ü©), ||ùõΩ01‚ü©=1‚àö\n2(|01‚ü©+|10‚ü©), (12.22)\n||ùõΩ10‚ü©=1‚àö\n2(|00‚ü©‚àí|11‚ü©), ||ùõΩ11‚ü©=1‚àö\n2(|01‚ü©‚àí|10‚ü©). (12.23)\nUsethedefinitionofseparabilityandthebasisvectors(12.18)toprovethattheBellstates\nareentangled.\nApowerfulwaytodescribethequantumstateofasystemisintermsofthe densitymatrix\nùúå.Itcanbeusedtocalculateobservableswithoutresortingtowavefunctions,andispartic-\nularlyusefulwhendealingwithanensembleofpurestates.Thedensitymatrixoroperator\nisdefinedas\nùúå=‚àë\nipi||ùúìi‚ü©‚ü®ùúìi||. (12.24)\nHerepiistheprobabilityofthepurestate ||ùúìi‚ü©beingpresentintheensemble,and,justto\nremindyou,theproductofakettimesabraisanoperator.Asystemconsistingofjusta\npurestatewouldhave pi=1.\n12.3.1 Physics Exercise: Two Entangled Dipoles\nTwointeractingmagneticdipoles ùùàAandùùàB,separatedbyadistance r,havetheinteraction\nHamiltonian:\nH=ùúá2\nr3(ùùàA‚ãÖùùàB‚àí3ùùàA‚ãÖÃÇrùùàB‚ãÖÃÇr), (12.25)\nùùàA=XAÃÇi+YAÃÇj+ZAÃÇk,ùùàB=XBÃÇi+YBÃÇj+ZBÃÇk. (12.26)\nHereweemploy QC notation thatlabelsthePaulimatricesas X,Y,andZ:\nXdef=ùúéx=[01\n10]\n,Ydef=ùúéy=[0‚àíi\ni0]\n,Zdef=ùúéz=[10\n0‚àí1]\n. (12.27)\nAndinyetmoreQCnotation,thetwodipolescanbeinthefour,directproductstates:\n||0A0B‚ü©=|0A‚ü©|0B‚ü©, |0A1B‚ü©=|0A‚ü©|1B‚ü©, (12.28)\n|1A0B‚ü©=|1A‚ü©|0B‚ü©, |1A1B‚ü©=|1A‚ü©|1B‚ü©. (12.29)\nAs discussed in Section 12.1, the Pauli 4 √ó4 matrices (12.27) are operators that transform\nstates.Asweshallsee,inQCtheyrepresentBoolean logicgateswiththeproperties:\nX|0‚ü©=|1‚ü©,X|1‚ü©=+|0‚ü©,Y|0‚ü©=i|1‚ü©,Y|1‚ü©=‚àíi|0‚ü©, (12.30)\nZ|0‚ü©=|0‚ü©,Z|1‚ü©=‚àí|1‚ü©. (12.31)\n12.3 Entangled and Separable States 259\nExercises\n1) Showthatthesedirectproductstatesformabasisfor ‚ÑÇ4:\n|00‚ü©=[1\n0]\n‚äó[1\n0]\n,|01‚ü©=[1\n0]\n‚äó[0\n1]\n, (12.32)\n|10‚ü©=[0\n1]\n‚äó[1\n0]\n,|11‚ü©=[0\n1]\n‚äó[0\n1]\n. (12.33)\nHint: |11‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.34)\n2) Consider PandQastheoperatorsinseparateHilbertspaces,\nP=[p11p12\np21p22]\n,Q=[q11q12\nq21q22]\n. (12.35)\nShowthattheirdirectproductis\nP‚äóQ=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£p11q11p11q12p12q11p12q12\np11q21p11q22p12q21p12q22\np21q11p21q12p22q11p22q12\np21q21p21q22p22q21p22q22‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.36)\n3) Showthatfor ÃÇr=ÃÇk,theHamiltonian(12.25)indirectproductspaceis\nH=ùúá2\nr3(XA‚äóXB+YA‚äóYB+ZA‚äóZB‚àí3ZA‚äóZB). (12.37)\n4) Showthatthedirectproduct:\nXA‚äóXB=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0001\n0010\n0100\n1000‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.38)\n5) Evaluatethedirectproducts YA‚äóYBandZA‚äóZBas4√ó4matrices,andtherebyshow\nthattheHamiltonianinthedirectproductspaceis:\nH=ùúá2\nr3‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£‚àí200 0\n022 0\n022 0\n000‚àí2‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.39)\n6) Usealinearalgebrapackagetoshowthattheeigenvaluesof H‚àï(ùúá2‚àïr3)are4,0,‚àí2,and\n‚àí2,andthatthecorrespondingeigenvectorsare:\nùúô1=1‚àö\n2‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n+1\n+1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|01‚ü©+|10‚ü©\n‚àö\n2,ùúô2=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|11‚ü©, (12.40)",10815
127-12.4.4 3Qubit Gates.pdf,127-12.4.4 3Qubit Gates,"260 12 Quantum Computing (G. He, Coauthor)\nùúô4=1‚àö\n2‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n1\n‚àí1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|01‚ü©‚àí|10‚ü©\n‚àö\n2,ùúô3=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\n0\n0\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|00‚ü©. (12.41)\n7) Recallthediscussionofentanglement.Ofthefoureigenstatesjustobtained,determine\nwhichonesareseparableandwhichonesareentangled.\n8) UsetheseeigenvectorsstatesasbasisstatestoevaluatetheHamiltonianmatrix:\nH=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£‚ü®ùúô1|H|ùúô1‚ü©‚ü®ùúô1|H|ùúô2‚ü©‚ü®ùúô1|H|ùúô3‚ü©‚ü®ùúô1|H|ùúô4‚ü©\n‚ü®ùúô2|H|ùúô1‚ü©‚ü®ùúô2|H|ùúô2‚ü©‚ü®ùúô2|H|ùúô3‚ü©‚ü®ùúô2|H|ùúô4‚ü©\n‚ü®ùúô3|H|ùúô1‚ü©‚ü®ùúô3|H|ùúô2‚ü©‚ü®ùúô3|H|ùúô3‚ü©‚ü®ùúô3|H|ùúô4‚ü©\n‚ü®ùúô4|H|ùúô1‚ü©‚ü®ùúô4|H|ùúô2‚ü©‚ü®ùúô4|H|ùúô3‚ü©‚ü®ùúô4|H|ùúô4‚ü©‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.42)\nIfyouhavedonethiscorrectly,theHamiltonianshouldnowbediagonalwiththeeigen-\nvaluesasthediagonalelements.\nInListing12.1,wepresenttheprogram Entangle.py thatperformsthenecessarylinearalge-\nbrausingthe numpypackage.Itproducestheresults:\nHamiltonian without mu^2/r^3 factor\n[[‚àí2000 ]\n[ 0220 ]\n4[ 0220 ]\n[ 000 ‚àí2]]\nEigenvalues\n[ 4.0000000e+00 4.4408921e ‚àí16‚àí2.0000000e+00 ‚àí2.0000000e+00]\n8Eigenvectors(incolumns)\n[[ 0. 0. 1. 0. ]\n[ 0.70710678 0.70710678 0. 0. ]\n[ 0.70710678 ‚àí0.70710678 0. 0. ]\n12[ 0. 0. 0. 1. ]]\nHamiltonian inEigenvector Basis\n[[ 4.00000000e+00 0.00000000e+00 0.00000000e+00 6.66133815e ‚àí16]\n[ 0.00000000e+00 ‚àí2.00000000e+00 0.00000000e+00 0.00000000e+00]\n16[ 0.00000000e+00 0.00000000e+00 ‚àí2.00000000e+00 0.00000000e+00]\n[ 6.28036983e ‚àí16 0.00000000e+00 0.00000000e+00 9.86076132e ‚àí32]]\n12.4 Logic Gates\nRecallthattraditionalcomputersuseelectroniccircuitscalled logicgatestoperformbasic,\nlogical operations on bits. More complex operations are created by combining multiple\ngates.Therearesixbasiclogicgates:\nAND,NAND,NOT,OR,NOR,XOR(exclusive OR).\nForexample,herearethesymbolsthatrepresentthe ANDandXORgates,aswellasthe truth\ntablesthatdefinetheiroutputsaccordingtotheirinputs:\nAxx A\n0\n0\n1\n100\n0\n0\n11\n0\n1AND\nBx A\n0\n0\n1\n100\n1\n1\n01\n0\n1XOR\nB\nBAx\nB\n12.4 Logic Gates 261\nAsaninstanceofhowthesegatescanbecombined,hereweconstructa half-adder from\nXORandANDgates:\nA A + BBXOR SUM\nCARRY ANDA\nCARRY\n0\n0\n1\n10\n1\n1\n00\n0\n0\n10\n1\n1B\nThehalf-adderaddstwobits,andiftheanswerisgreaterthan1,itcarriesoverabittoa\nhighermemoryposition.\n12.4.1 1-Qubit Gates\nInsimilaritywithclassicalcomputers,quantumcomputersemploy quantumlogicgates to\nperformelementaryoperationsonqubits.ThesegatesarerepresentedinHilbertspaceas\nunitaryoperators ,U||ùúìin‚ü©=||ùúìout‚ü©,whichmeanstheypreserveprobability.Thesearethe\ngates:\nState gate U:\n U ‚îÇœàout‚îÇœàin Determinesthestateofaket.\nPauli matrix gates: Ouroldfriendsthe Paulispinmatrices givenin(12.27)areusedasQC\ngates,wheretheyarerenamedas X=ùúéx,Y=ùúéy,andZ=ùúéz.\nNOTgate X:Thequantum NOTgateflips |0‚ü©(formerlyspinup)to |1‚ü©(formerlyspindown),\nandviseversa.Usedasagate,thePaulimatrix Xactsasthe NOToperator\nNOT,\nchangingonestateintoanother:\nXdef=ùúéx=|0‚ü©‚ü®1|+|1‚ü©‚ü®0|=[01\n10]\n, (12.43)\n‚áíX|0‚ü©=|1‚ü©,X|1‚ü©=|0‚ü©. (12.44)\nYg a t e :ThePaulimatrix ùúéyactsastheYgate:\nYdef=ùúéy=i(|1‚ü©‚ü®0|‚àí|0‚ü©‚ü®1|) =[0‚àíi\ni0]\n. (12.45)\nZg a t e :ThePaulimatrix ùúézactsastheZgate.Itflipsthesignofthe |1‚ü©state,butleavesthe\n|0‚ü©stateunchanged:\nZdef=ùúéz=|0‚ü©‚ü®0|‚àí|1‚ü©‚ü®1|=[10\n0‚àí1]\n, (12.46)\nZ|z‚ü©=( ‚àí1)z|z‚ü©. (12.47)\nAswiththequantumspin,thestates |0‚ü©and|1‚ü©aretheeigenstatesof Z.\nHadamard gate H:convertsqubitsthatareeigenstatesof Ztoonesthatareeigenstates\nofX:\nH|0‚ü©=1‚àö\n2(|0‚ü©+|1‚ü©)‚â°|+‚ü©,H|1‚ü©=1‚àö\n2(|0‚ü©‚àí|1‚ü©)‚â°|‚àí‚ü©, (12.48)\nH= (|+‚ü©‚ü®0|+|‚àí‚ü©‚ü®1|) =1‚àö\n2[11\n1‚àí1]\n. (12.49)\n262 12 Quantum Computing (G. He, Coauthor)\nTheHgatealsocreatesequalmixturesofthe |0‚ü©and|1‚ü©basisstates,andthusisuseful\nintransformingclusteredqubitsintostateswithuniformsuperpositions:\nH|0‚ü©=1‚àö\n2(|0‚ü©+|1‚ü©),H|1‚ü©=1‚àö\n2(|0‚ü©‚àí|1‚ü©). (12.50)\nRùùã:\n RœÜ:alsocalledthe Porphasegate,rotates |1‚ü©byanangle ùúëaboutthe\nz-axis,whileleaving |0‚ü©untouched:\nRùúë|0‚ü©=|0‚ü©,Rùúë|1‚ü©=eiùúë|1‚ü©,Rùúë=[10\n0eiùúë]\n. (12.51)\nSa n dTg a t e s : Arespecialcasesofthe Rùúëgate.Srotatesaketby ùúë=ùúã‚àï2(eiùúë=i),andT\nbyùúô=ùúã‚àï4:\nS S0‚ü©=0‚ü©, S1‚ü©=i1‚ü©, (12.52)\nT T0‚ü©=0‚ü©, T1‚ü©=eiœÄ ‚ÅÑ 41‚ü©, (12.53)\nT=[10\n0e x p(iùúã‚àï4)]\n,S=T2=[10\n0i]\n. (12.54)\nRx,Ry,Rz:These gates perform a general rotation of qubits on the Bloch sphere by the\nangleùõºaroundthe x,y,orzaxes,respectively:\nRx(ùõº)=e‚àíiùõºùúéx‚àï2,Ry(ùõº)=e‚àíiùõºùúéy‚àï2,Rz(ùõº)=e‚àíiùõºùúéz‚àï2. (12.55)\nMeasurement operation:\n ‚îÇœà While not a gate because it‚Äôs not unitary,\ntheclassicalmeasurementofaquantumstateisanoft-usedoperation.\nExercise UsetheBlochsphereofFigure12.1andequation(12.11)todetermine:\n‚óèWhatgatetransforms |0‚ü©to(|0‚ü©+|1‚ü©)‚àï‚àö\n2?\n‚óèWhatgaterotates (|0‚ü©+|1‚ü©)‚àï‚àö\n2byùúã‚àï2into(|0‚ü©+i|1‚ü©)‚àï‚àö\n2?\n‚óèWhatgaterotates |0‚ü©byùúãinto|1‚ü©?\n12.4.2 2-Qubit Gates\nCZ (alt)\nZCZ CNOT SWAP\nThereisawholesetoftwo-qubitgateswiththepropertythattheircombinedactionscan\napproximatea4 √ó4unitarymatrixtoarbitraryprecision.InadditiontotheHadamardgate,\nalreadydefinedforone-qubituse,two-qubitgatesinclude SWAP,CNOT,andCZ:\nSWAP:Transforms |01‚ü©to|10‚ü©,thatis,itswapsthetwokets:\nSWAP |01‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1000\n0010\n0100\n0001‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n1\n0\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|10‚ü©. (12.56)\nInturn,SWAPtransforms |10‚ü©to|01‚ü©.\n12.4 Logic Gates 263\nControlled gates: Controlledgatesacton2-qubitstates |c‚ü© |t‚ü©,where |c‚ü©isthecontrolbit\nand|t‚ü©isthetarget.Ifa1qubitgate Uisusedina2qubitcontrolledgatecombination\nCU,ithasthefollowingeffect:\nCU|c‚ü© |t‚ü©=|c‚ü©Uc|t‚ü©, (12.57)\nwhereUcisaversionof Umodifiedbythecontrol.\nControlled NOT,CNOT:\nusesonequbittocontrolitsactiononthetargetqubit.Ifthe\ncontrolqubitis0,thenthetargetisunchanged;ifthecontrolqubitis1,thenthe target\nqubitisflipped.Withtheleftqubitascontrol:\nCNOT(x,y)={\n(x,y)if x=0\n(x,1‚àíy)if x=1,(12.58)\nCNOT |10‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1000\n0100\n0001\n0010‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|11‚ü©. (12.59)\nControlled Z, CZ: ThecontrolledZgatereversesthesignofthe |11‚ü©qubit:\nCZ|00‚ü©=|00‚ü©,CZ|01‚ü©=|01‚ü©,CZ|10‚ü©=|10‚ü©,CZ|11‚ü©=‚àí|11‚ü©,(12.60)\nCZ=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£100 0\n010 0\n001 0\n000‚àí1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.61)\nExercise DeterminetheeffectoftheCNOTgateon: |10‚ü©,|01‚ü©,|00‚ü©,and|11‚ü©.\nExercise Verifytheaboveeffectsofthe CZgate.\n12.4.3 Entanglement via Gates\nThe entangled Bell states (12.22) can be created with the aforementioned gates. For\nexample,Figure12.2showsaquantumcircuitcreatingtheBellstate ||ùõΩ00‚ü©byemploying\nanH(Hadamard)gatefollowedbya CNOTgate.Herewestartwiththetwoqubitstate |00‚ü©,\nand useHon the first qubit, leaving the second qubit unchanged. Then the CNOTgate,\nwhichusesthefirstqubitforcontrol,andthesecondasthetarget:\n(1)H|0‚ü© |0‚ü©=1‚àö\n2(|0‚ü©+|1‚ü©)|0‚ü©, (12.62)\n(2)CX1‚àö\n2(|0‚ü©+|1‚ü©)|0‚ü©=1‚àö\n2(|0‚ü© |0‚ü©+|1‚ü© |1‚ü©). (12.63)\nFigure 12.2 A quantum circuit for creating an entangled state ||ùõΩ00‚ü©.\nH ‚à£0‚å™\n‚à£0‚å™",6593
128-12.5 An Intro to QC Programming.pdf,128-12.5 An Intro to QC Programming,"264 12 Quantum Computing (G. He, Coauthor)\n12.4.4 3-Qubit Gates\nThree-qubitstatesusebasisvectorscreatedbythedirectproductsofthreekets:\n|ijk‚ü©=|i‚ü© |j‚ü© |k‚ü©‚â°|i‚ü©‚äó|j‚ü©‚äó|k‚ü©. (12.64)\nThereare,accordingly,eightsuchstates, |000‚ü©,|001‚ü©,|010‚ü©,|011‚ü©,|100‚ü©,|101‚ü©,|110‚ü©,\n|111‚ü©.Forexample,\n|111‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n0\n0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,|110‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n0\n0\n0\n1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.65)\nThismeansthatthethree-qubitgatesthatoperateonthese8-Dbasisvectorsarerepresented\nby8√ó8matrices.\nTOFFOLI ,CCNOT GATE :\nisanextensionofthe CNOTgatethatflipsthethirdqubit,ifand\nonlyifthetwofirsttwoqubitsareinthe |011‚ü©state:\nT|110‚ü©=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£10000000\n01000000\n00100000\n00010000\n00001000\n00000100\n00000001\n00000010‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n0\n0\n0\n1\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n0\n0\n0\n0\n0\n0\n1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=|111‚ü© (12.66)\n12.5 An Intro to QC Programming\nNow that we have the building blocks for QC, namely, qubits and gates, it‚Äôs time to put\nthemtogetherintoprograms,aka circuits.WestartbyusingGoogle‚Äôs2018releaseof Cirq,a\n‚ÄúPythonsoftwarelibraryforwriting,manipulatingandoptimizingquantumcircuits,then\nrunning them on quantum computers and quantum simulators‚Äù [Cirq, 2023]. After the\nsimpleexamplesinthissectionimplementingthevariousgates,wewillusethepowerful\nIBMQuantumComputer forsomemoreadvancedprogrammingandrunningonaphysical\nquantumcomputer.\nToinstallCirqusingAnaconda,werecommendusingthe packagemanagerconda ,which\nhelps to install all the associated bits and pieces of packages. To do that, use a shell (the\nCommandshellor PowerShell onWindows,orthe TerminalonMacs)andissuethecom-\nmand:\nconda install -c psi4 cirq\n12.5 An Intro to QC Programming 265\nAlternatively,youcanuse piptoinstallcirq:\npython -m pip install -upgrade, pip python -m pip install cirq\n‚óèHadamard gate:\nRecall,anHgateconvertseigenstatesof Ztoeigenstatesof X.Enterandrunyourfirst\nCirqprogramtocreateanHgatethatoperateson |0‚ü©:\n# Hadamard . py : Cirq program to create H gate\n3importcirq # Import Cirq\ncircuit = cirq.Circuit() # Build circuit\nqubit = cirq.GridQubit(0,0) # Create qubit at (0 ,0)\n7circuit.append(cirq.H(qubit)) # Append Hadamard gate\ns = cirq.Simulator() # Initialize simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nprint(circuit) # Output circuit\n11results = s.simulate(circuit) # Run simulator\nprint(results) # Output resulting kets\nCirque ‚Äôs Output ----------------------------------\nSimulate the circuit:\n15(0,0): __________H__________\noutput vector: 0.707 |0> + 0.707|1>\nNotonlywasthatprettyeasy,butitalsogavethecorrectanswer:\nH|0‚ü©=1‚àö\n2[11\n1‚àí1][1\n0]\n=1‚àö\n2[1\n1]\n‚â°0.707[1\n0]\n+0.707[0\n1]\n. (12.67)\n‚óèTwo Hadamard gates:\nThe application of two H gates to a state should act as the identity operator. Extend\nthe previous program to append a second Hadamard gate, now using the command\na = cirq.QubitNamed(""a"") todefineaqubit:\n# TwoHgates.py: Cirq program to create 2 H gates on one line\nimportcirq\n4\ncircuit = cirq.Circuit() # Build circuit\na = cirq.NamedQubit( ‚Äôa‚Äô) # Define named qubit\ncircuit.append(cirq.H(a)) # Append H gate to a\n8circuit.append(cirq.H(a)) # Append another H gate to a\ns = cirq.Simulator() # Initialize simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nprint(circuit) # Output circuit\n12results = s.simulate(circuit) # Run simulator\nprint(results) # Output resulting kets\nCirque ‚Äôs Output---------------------\nSimulate the circuit:\n16 a: _______ H________ H_______\noutput vector: |0>\nExercise Usetheoutputof Hadamard.py tocheckthat TwoHgates.py actsastheidentity\noperator.\n‚óèXa n dHg a t e s :\nWriteaprogramthatusesanXgatetoconvert |0‚ü©to|1‚ü©,andthenanHgatetocreatean\neigenstateofX:\n266 12 Quantum Computing (G. He, Coauthor)\n00100200300Result count400500\n1\nQubit state\nFigure 12.3 Histogram of the state formed by application of X, Z, and H gates. (If there was no\nnoise, the heights would be equal.)\n# XplusH.py: Cirque program , setup X & H gates and |1>\n3importcirq\ncircuit = cirq.Circuit() # Build circuit\na = cirq.NamedQubit( ‚Äôa‚Äô) # Define qubit\n7circuit.append(cirq.X(a)) # Append X (NOT) gate\ncircuit.append(cirq.H(a)) # Append H gate\ns = cirq.Simulator() # Initialize simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\n11print(circuit)\nresults = s.simulate(circuit) # Run simulator\nprint(results)\n‚àí‚àí‚àí‚àí‚àí‚àí‚àíCirque Output ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí\n15Simulate the circuit:use\na:_______X_____H______\noutput vector: 0.707|0> ‚àí0.707|1>\nExercise VerifythattheoutputisaneigenstateofX.\n‚óèX, Z, and H Gates and M Op:\nAmeasurement operatoractingonaquantumstateprovidesaclassicaloutputoftheprob-\nabilitydistributionofa particularmeasurement. (Recall,‚Äúgates‚Äù areunitaryoperators,\nwhile measurement operatorsare not unitary, and thus not gates.) Extend your circuit\ntoincludeH,X,andZgates,aswellasameasurementoperatorontheresult.Runthe\nsimulation1000timesinordertoaccumulatesomestatistics.BecauseCirqrunswithin\nPython,it‚ÄôseasytouseMatplotlibtoproducethevisualizationofprobabilitydistribution,\naswehavedoneinFigure12.3.\n# X Z H M. py : Cirq Simulation with H, X, Z Gates + Measurement Op\n3importcirq\nimportmatplotlib.pyplot as plt\ncircuit = cirq.Circuit() # Build circuit\n7a = cirq.NamedQubit( ‚Äôa‚Äô) # Create qubit\ncircuit.append(cirq.X(a)) # Append X gate\ncircuit.append(cirq.Z(a)) # Append Z gate\ncircuit.append(cirq.H(a)) # Append H gate\n11s = cirq.Simulator() # Initialize simulator\n12.5 An Intro to QC Programming 267\nprint(‚ÄôSimulate the circuit:‚Äô )\nresults = s.simulate(circuit) # Run simulator\ncircuit.append(cirq.measure(a, key = ‚Äôresult‚Äô ))\n15samples = s.run(circuit ,repetitions =1000)\nprint(circuit)\nprint(resultsC\nprint(samples)\n19cirq.plot_state_histogram(samples)\nplt .show()\nCirq Output\nSimulate the circuit:\n23a: ______X_______Z______H_______M____________\noutput vector: ‚àí0.707|0> + 0.707|1>\nresult=1001110001001101001110111000100001....\nExercise DeducewhatshouldbetheeffectofsuccessiveX,Z,andHgates,andcompare\nwiththeaboveoutput.\nExercise MakesenseoftheMoperator‚Äôsoutput:\nresult=1001110001001101001110111000100001 . . . . .\n‚óèA 2-qubit Cirq circuit:\nCirq contains the command q0, q1 = cirq.LineQubit.range(2) that creates a two-\nqubitcircuit.\nExercise\n1) UseCirqtocreatethecircuit:\nq0\nq1 Z\n2) Runthecircuitandverifythatitsoutputis |10‚ü©.\n3) Explainwhythisistheexpectedoutput.\n4) Recall the SWAP gate (a vertical line connecting qubits) that swaps one qubit with\nanother.Extendyourprogramtoincludea SWAPgatebetweenq0andq1:\nq0\nq1 Z\nOurprogram CirqSwap.py createdthiscircuit:\n# CirqSwap.py: Cirq program to create & swap 2 qubits\n3importcirq\ncircuit = cirq.Circuit()\nq0, q1 = cirq.LineQubit. range(2) # Create two qubits\n7circuit.append(cirq.X(q0)) # Append X to q0\ncircuit.append(cirq.Z(q1)) # Append Z to q1\ncircuit.append(cirq.SWAP(q0,q1)) # Swap qubits\nprint(circuit)\n11s = cirq.Simulator() # Initialize simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nresults = s.simulate(circuit) # Run simulator\nprint(results)\n15 Cirque Output\nSimulate the circuit:\noutput vector: |01>\n268 12 Quantum Computing (G. He, Coauthor)\n‚óèCNOT on 2 qubits:\nCNOT(q0,q1)={\n(q0,q1)ifq0=0,\n(q0,1‚àíq1)ifq0=1.(12.68)\n# CirqCNOT . py : Cirq program with CNOT gate\n3importcirq\ncircuit = cirq.Circuit()\nq0, q1 = cirq.LineQubit. range(2) # Create two qubits\n7circuit.append(cirq.X(q0)) # Append X to q0\ncircuit.append(cirq.Z(q1)) # Append Z to q1\ncircuit.append(cirq.CNOT(q0, q1)) # Append CNOT, q0 = control\nprint(circuit)\n11s = cirq.Simulator() # Initialize Simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nresults = s.simulate(circuit) # Run simulator\nprint(results)\n15 Output\nSimulate the circuit:\nmeasurements: (no measurements)\noutput vector: |11 >\nq0\nq1 Z\nExercise Deducewhatshouldbetheeffectof CNOTonthequbits,andcomparewith\ntheoutput.\nExercise A d daS W A Pg a t eb e f o r e CNOT, and see if the output agrees with what you\nwouldexpect.\n‚óè3-Qubit T OFFOLI gate:\nCreate a circuit that implements the T OFFOLI/CCNOT(controlled-controlled-not) gate. It\nshouldtakethreebitsasinput,andinvertthethirdbitiffthefirsttwobitsare1‚Äôs:\nq0\nq1\nq2 Z\n1# CirToffoli .py: Cirq program with 3 qubit C C N O T gate\nimportcirq\n5q0, q1, q2 = cirq.LineQubit. range(3) # Create 3 qubits\ncircuit = cirq.Circuit() # Build circuit\ncircuit.append(cirq.X(q0)) # Append X to q0\ncircuit.append(cirq.Z(q2)) # Append Z to q2\n9circuit.append(cirq.Toffoli (q0, q1,q2)) # Connect all 3 wi Toffoli\nprint(circuit) # Output circuit\ns = cirq.Simulator() # Initialize Simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nresults = s.simulate(circuit) # Run simulator\n13print(results)\nOutput\nSimulate the circuit:\noutput vector: |101>",8832
129-12.6 Accessing the IBM Quantum Computer.pdf,129-12.6 Accessing the IBM Quantum Computer,"12.5 An Intro to QC Programming 269\nExercise Try all possible values for q0 and q2, and compare the output with the\nexpected CCNOTeffect.\n12.5.1 Half and Full Adders\n‚óèHalf adder:\nAhalf-adderaddsthequbits,q0andq1,andoutputsthesum.Italsooutputsacarrybit\nq2=1,ifq0=q1=1,elseq2 =0.Createahalf-addercircuitusingthreequbitsanda\nTOFFOLIgatefollowedbya CNOTgate:\n# CirqHalfAdder.py: Cirq circuit for half adder\nimportcirq\n4\nq0, q1, q2 = cirq.LineQubit. range(3) # Create 3 qubits\ncircuit = cirq.Circuit() # Build circuit\ncircuit.append(cirq.X(q0)) # Append X to q0\n8circuit.append(cirq.X(q1)) # Append X to q1\ncircuit.append(cirq.Toffoli(q0, q1,q2)) # Append T o f f o li to 3 qs\ncircuit.append(cirq.CNOT(q0, q1)) # Append CNOT to q0 & q1\nprint(circuit) # Output circuit\n12s = cirq.Simulator() # Initialize Simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\nresults = s.simulate(circuit) # Run simulator\nprint(results)\n16 Output\nSimulate the circuit\noutput vector: |101>\nq0\nq1\nq2\nExercise Verifytheadditions:1 +1,1+0,0+1.\n‚óèFull adder:\nDesignafulladderthataddsq0 +q1,withq2asthesumandq3asthecarry.Itcanbe\nimplementedwiththeprogramin FullAdder.py forthecircuit:\nq0\nq1\nq2 = Cinsum\nq3 Cout\n# FullAdder .py: Cirq q0+q1 full adder program\n2\nimportcirq\ncircuit = cirq.Circuit() # Build circuit\n6q0, q1, q2,q3 = cirq.LineQubit. range(4) # Create 4 qubits\ncircuit = cirq.Circuit() # Build circuit with qubits\ncircuit.append(cirq.X(q0)) # Append X to q0\ncircuit.append(cirq.X(q1)) # Append X to q1\n10circuit.append(cirq.Toffoli(q0, q1,q2)) # Append T o f f o li\ncircuit.append(cirq.CNOT(q0, q1)) # Append CNOT to q0 , q1\ncircuit.append(cirq.Toffoli(q1, q2,q3)) # Append T o f f o li",1701
130-12.6.1 IBM Quantum Composer.pdf,130-12.6.1 IBM Quantum Composer,"270 12 Quantum Computing (G. He, Coauthor)\ncircuit.append(cirq.CNOT(q1, q2)) # Append CNOT to q1 , q2\n14circuit.append(cirq.CNOT(q0, q1)) # Append CNOT to q0 , q1\nprint(circuit)\ns = cirq.Simulator() # Initialize Simulator\nprint(‚ÄôSimulate the circuit:‚Äô )\n18results = s.simulate(circuit) # Run simulator\nprint(results)\nOutput\nSimulate the circuit:\n22output vector: |1110>\nUsethisprogramtofillthistable:\nq0 q1 Cin Sum Cout\n|0‚ü©|0‚ü©\n|0‚ü©|1‚ü©\n|1‚ü©|0‚ü©\n|1‚ü©|1‚ü©\n12.6 Accessing the IBM Quantum Computer\nWe have just now checked, and since Amazon has yet to start selling personal quantum\ncomputers, we, instead, will go online and use the IBM Quantum [IBMqc, 2023]. Before\nyoucanuseit,however,thereareanumberofstepstofollow:\n1) Goto QUANTUM-COMPUTING.IBM.COM/LOGIN\n2) Youwillneedtocreatean IBMid.Todothat,haveyourcellphoneanditsQRreaderin\nhand,clickon\nCreate an IBMid account .2\nThiswillletyoulogintothe IBMQuantum ,aswellasgiveyouaccesstotutorialsand\nprogrammingtools.Followtheinstructionsonthatpageforcreatingandauthenticating\nyouraccount.Alternatively,youmaybeabletouseyourGoogleorGitHubaccountto\nlogintoIBMQuantum.\n3) YoucanfindinstructionsonhowtorunIBMQuantumcodesat\nQUANTUM-COMPUTING.IBM.COM/LAB/DOCS/IQL/RUNTIME/START.\nYouwillbegivenatokentousetorunaprogram.\n12.6.1 IBM Quantum Composer\nAfterloggingintotheIBMQuantum,youwillbepresentedwitha dashboard containing\nseveralaccessroutestothecomputer.Wewilldemonstrateoneofthem.Pushingthebut-\nton on the dashboard labeled Launch Composer , brings up the IBMQuantumComposer ,\nwhich is shown in Figure 12.4. The Composer is a graphical tool for creating quantum\ncircuits (programs) by dragging and dropping operators, and then running them on the\nIBMQuantum,orasimulator.Asseeninthefigure,theComposerisdividedintoseveral\n2 Thismightnotworkforallcountries.\n12.6 Accessing the IBM Quantum Computer 271\nFigure 12.4 A screenshot of the dashboard for the IBM Quantum Composer, with the gates section\nmagniÔ¨Åed .\nH q [0]\nq [1]\n0.00.20.40.60.81. 0\n01 00\nComputational basis statesOutput state\n[ 0.707+0j,\n0+0j, 0+0j,\n0.707+0j ]œÄ/2\n3œÄ/20 Phase œÄStatevector i Amplitute\n10 11\nFigure 12.5 Left: A Quantum Composer circuit for generating the Bell state ||ùõΩ00‚ü©\n.Right:T h e\ngenerated ||ùõΩ00‚ü©\nstate vector in histogram and numerical forms.\npanels.Thetopleftpanel,whichwehavemagnified,presentsyouwithasetofcolorfullittle\nicons,eachrepresentingadifferentquantumgate,mostofwhichwehavejustdiscussed.\nAsanexampleofhowthisallworks,inFigure12.5,weshowtheComposer‚Äôsoutputfrom\ngraphicalprogramthatweusedtocreatethe ùõΩ00Bellstate(12.22),\n||ùõΩ00‚ü©=1‚àö\n2(|00‚ü©+|11‚ü©). (12.69)\nOn the left is the circuit that generated the state, and on the right are the two generated\ncomputational basis states, shown as both a histogram and as numbers. Here‚Äôs how our\ndragginganddroppingcreatedthecircuit:\n‚óèWestartedbyselecting File/New,whichgaveusthefourqubits, q[0], q[1], q[2], q[3] ,\nandasingleclassical,4-bitregister c4(cfor‚Äúclassical‚Äù).\n‚óèSeeingthatweneedonlytwoqubitstoconstructthe ùõΩ00state,weeliminated q[2],q[3],\nand c4.",3084
131-12.7 Qiskit Plus IBM Quantum.pdf,131-12.7 Qiskit Plus IBM Quantum,"272 12 Quantum Computing (G. He, Coauthor)\n‚óèNext,wedraggedthe Hadamard Hgate(definedinSection12.4)tothe q[0]line.\n‚óèThenwedraggedthecontrolled-NOTgate\n tothe q[0]lineaftertheHgate,withthe\ntargetsymbolplacedonthe q[1]line.\n‚óèLastly,wesavedthecircuitasthefile beta_00.\n‚óèAswastobehopedfor,theoutputinFigure12.5agreeswith(12.69).\nA Small Caveat TheIBMQuantumemploysareversedDiracnotationinwhichqubits\nareorderedfromrighttoleft,thatis,as ||qn‚àí1,‚Ä¶q1q0‚ü©.SoinIBMspeak:\n|01‚ü©=||q1=0‚ü©‚äó||q0=1‚ü©=|01‚ü©=[1\n0]\n‚äó[0\n1]\n=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0\n1\n0\n0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.70)\n12.7 Qiskit Plus IBM Quantum\nAnother avenue to QC is [Qiskit, 2023],an open-source software developmentkit (SDK)\nthatmaybeusedwiththeIBMQuantum,orwithitsbuilt-inquantumsimulator.Running\nonbothasimulatorandaphysicalQCisrevealing,asthesimulator,beingsoftware,may\nwellgivemoreaccurateresultsthananimperfectphysicalmachine.\nTherearedifferentwaystouseQiskit.HereweusedanAnacondawindowina Jupyter\nNotebook, all in MS Windows, to set up the virtual environment qiskit.W ea c t i v a t e d\nthe environment and installed the Qiskit package, including visualization and the\nqiskit_ibm_provider package:\nconda create ‚àí‚àíname qiskit python jupyter notebook\n2conda activate qiskit\npip install qiskit[visualization] qiskit_ibm_provider\nIfyouwanttoaccessIBMQuantumfromyourlocalcomputer,thenyouwillneedan API\ntokenthatauthenticatesyourexternaluse.YoucancopythetokenfromtheIBMQuantum\ndashboard,oryoucanrunthefollowingcodetosavethetoken, api_token ,onyourlocal\ndiskforfutureuse:\n1fromqiskit_ibm_provider importIBMProvider\nIBMProvider.save_account(api_token)\nYoucannowuseyourlocalcomputertobuildaquantumcircuitandtorunitontheIBM\nQuantum.PlacethiscodeinaJupyterNotebooknamed beta_00:\nfromqiskitimportQuantumCircuit # Load needed package\n2\ncircuit = QuantumCircuit(2) #C r e a t ea2 ‚àíq circuit\n# Apply H to q0 , then C N O T, q0 = control , q1 = target\ncircuit.h(0)\n6circuit.cx(0, 1)\ncircuit.draw( ‚Äômpl‚Äô) # D r a w circuit\n12.7 Qiskit Plus IBM Quantum 273\n0001\n10110001\n101100011011\n000110110.000.10.2\nRe[œÅ]\nIm[œÅ]0.30.4\nFigure 12.6 The real and imaginary part of the density matrix for the ||ùõΩ00‚ü©state computed with\nQisket and the IBM Quantum.\nThis should produce the same results as shown in Figure 12.5. Next, we‚Äôll use this same\ncircuittocomputethestatevectoronQiskit‚Äôsquantum simulator Aer,andvisualizeitwith\nQisket‚Äôs plot_state_city visualizationtool:\n1fromqiskitimportAer\nfromqiskit.visualization importplot_state_city\nbackend = Aer.get_backend( ""statevector_simulator"" )\n5job = backend.run(circuit)\nresult = job.result()\nstatevector = result.get_statevector(circuit , decimals=3)\nstatevector.draw(output= ""latex"")\n9plot_state_city(statevector)\nAsseenontheleftofFigure12.6,thisyieldstherealandimaginarypartsofthestatevector‚Äôs\ndensitymatrix [(12.24)inSection12.3].\nWenowwillrunthesamecircuitontheIBMQuantumComputer.Westartbyfinding\ntheleastbusydevice:\nfromqiskit_ibm_provider importIBMProvider\n3 # Select hub/group/project\nprovider = IBMProvider(instance= ""ibm-q/open/main"" )\n# Get the least busy backend\nfromqiskit.providers.ibmq importleast_busy\n7device = least_busy(provider.backends(\nfilters= lambdax:int(x.configuration().n_qubits) >= 3\nand not x.configuration().simulator\nandx.status().operational isTrue))\n11print(""Running on current least busy device: "" ,d e v i c e )\nNextwetranspile(translatefromonecompilertoanother)thequantumcircuitandrunit\nonourdefined device:\n1fromqiskitimporttranspile\nfromqiskit.tools.monitor importjob_monitor\ncircuit.measure_all() # Measure the two qubits\n5transpiled_circuit = transpile(circuit , device)",3650
132-12.7.2 IBM Quantum Exercises.pdf,132-12.7.2 IBM Quantum Exercises,"274 12 Quantum Computing (G. He, Coauthor)\n0\n00 01 10 111000\n227 24737503968\n2000Count30004000\nFigure 12.7 Histogram of the ||ùõΩ00‚ü©Bell state found using Qiskit and the IBM Quantum.\njob = device.run(tranpiled_circuit , shots=8192)\njob_monitor(job, interval=2)\nresult = job.result()\n9counts = result.get_counts(circuit)\nfromqiskit.visualization importplot_histogram\nplot_histogram(counts)\nTheoutputisshowninFigure12.7.YoumaynotethatthephysicalQuantumComputeris\nnotaperfectquantumdevice,aswitnessedbytheexperimentalerror,namely,thesmall,\nbutnonzero,countsforthe |01‚ü©and|10‚ü©states.\n12.7.1 A Full Adder\nInSection12.5.1weusedGoogle‚ÄôsCirqsimulatortobuildacircuitthataddstwobits,using\ntwoTOFFOLIgates,two CNOTgates,andfourqubits.Nowwe‚ÄôllbuildanIBMQuantumcircuit\nthatdoesthesamething,namelyadds xtoy.Thesimplethree-qubitcircuitthatdoesthatis\nshownontheleftofFigure12.8.OntherightofthefigureweshowamoreadvancedIBM\nQuantum implementation for adding 01 +10 using three T OFFOLI(CCX) gates, three CNOT\n(CX)gates,threemeasurementops,threemeasurementops,andfivequbits.Theprogram,\ngivenbelow,startsbyinitializingthe q0,q1,andq2qubitsthatareusedtorepresent |x‚ü©,|y‚ü©,\nand|0‚ü©respectively.ItthenusesthefirstT OFFOLIgate,taking q0andq1asthecontroland\nq2asthetargetforthecarrybit.Thenacontrolled-NOTgateisusedwith q0asthecontrol\nandq1asthetarget.Thereadoutsofthestatesof q2andq1providetheresult x+y:\n1defadder_circuit(x_in: int,y _ i n : int)‚àí> QuantumCircuit:\n# q [ 0 ,1 ,2 ,3 ,4 ] ‚àí‚àí> x[0 ,1] , y[0 ,1] , c\ns=f ""0{y_in:02b}{x_in:02b}""\n5 qc = QuantumCircuit(5, 3)\nqc.initialize(s)\nqc.ccx(0, 2, 4)\nqc.cx(0, 2)\n9 qc.reset(0)\nqc.ccx(1, 3, 0)",1650
133-12.8.1 1Qubit QFT.pdf,133-12.8.1 1Qubit QFT,"12.8 The Quantum Fourier Transform 275\nq0\nq1\nq2\nq3\nq4\nc3 0 2 143[0, 1, 0, 0, 1]210\n‚à£x‚å™\n‚à£x     y‚å™\n‚à£(xy)‚å™‚à£y‚å™\n‚à£0‚å™‚à£0‚å™\n‚à£œà‚å™\nFigure 12.8 Left: A quantum circuit for adding two bits. Right: The IBM Quantum version of an\nadder for 01 +10.\nqc.cx(1, 3)\nqc.ccx(3, 4, 0)\n13 qc.cx(4, 3)\nqc.measure([2, 3, 0], [0, 1, 2])\nreturnqc\n12.7.2 IBM Quantum Exercises\nUsetheIBMQuantumComposertoperformtheseexercises.\n1) Provethat\nCZ|00‚ü©=|00‚ü©,CZ|01‚ü©=|01‚ü©,CZ|10‚ü©=|10‚ü©,CZ|11‚ü©=‚àí|11‚ü©.\n2) DeterminetheeffectoftheCNOTgateon: |10‚ü©,|01‚ü©,|00‚ü©,and |11‚ü©.\n3) CreateaquantumcircuitforcreatingtheentangledBellstate\n||ùõΩ11‚ü©=1‚àö\n2(|01‚ü©‚àí|10‚ü©). (12.71)\n4) CreateacircuitthatdemonstratestheeffectofanHgateon |0‚ü©:\nH|0‚ü©=1‚àö\n2[1\n1]\n. (12.72)\n5) ProvedthatacircuitwithtwoHadamardgatesactsastheidentityoperator.\n6) VerifytheeffectoftheSWAPandCNOTgatesactingontwoqubits.\n7) CreateacircuitthatshowshowtheT OFFOLI/CCNOTgateactingonthreequbitsinvertsthe\nthirdqubit,ifthefirsttwoqubitsare1‚Äôs.\n8) Createacircuitfora halfadderthataddsthequbits,q0andq1,outputsthesum,aswell\nasacarrybitq2 =1ifq0=q1=1,elseq2 =0.Verifytheadditionsfor1 +1,1+0,0+1.\n12.8 The Quantum Fourier Transform\nAs studied in Chapter 9, the discrete Fourier transform (DFT) transforms Nvalues of a\n‚Äúsignal‚Äùyk,k=0,1,‚Ä¶,N,measuredat Nequally-spacedtimes tk=kh,intoNcomplex,\ntransformcomponents Yn.Thetransformanditsinversecanbewritteninaconciseand",1393
134-12.9 Oracle  Diffuser equals Grovers Search Algorithm.pdf,134-12.9 Oracle  Diffuser equals Grovers Search Algorithm,"276 12 Quantum Computing (G. He, Coauthor)\ninsightfulway,andevaluatedefficiently,byintroducingacomplexvariable Zraisedtovar-\niouspowers:\nY=DFT(y),y=DFT‚àí1Y (12.73)\nYn=1‚àö\n2ùúãN‚àë\nk=1Znkyk,yk=‚àö\n2ùúã\nNN‚àë\nn=1Z‚àínkYn, (12.74)\nZdef=e‚àí2ùúãi‚àïNZnk‚â°[Zn]k. (12.75)\nWiththisformulation,onlyoneexplicitcomputationoftheexponentialisrequired.\nWenowgeneralizetheDFTtoa QuantumFourierTransform (QFT)where nqubitsare\nusedtocomputer2ncomponents.TheQFTtransformsasignalspacestate |y‚ü©intoatrans-\nformspacestate |Y‚ü©:\n|Y‚ü©=QFTN|y‚ü©,|y‚ü©=QFT‚àí1\nN|Y‚ü© (12.76)\n|Y‚ü©=1‚àö\nNN‚àí1‚àë\nk=0N‚àí1‚àë\nl=0ylZ‚àíkl\nN|k‚ü©,|y‚ü©=1‚àö\nNN‚àí1‚àë\nk=0N‚àí1‚àë\nl=0YlZkl\nN|k‚ü©. (12.77)\nHerethesubscript NonZindicatesthenumberofbasisvectorsbeingusedand lthecom-\nponents of yandY.Note, we have switched to the computer science convention of starting\nthesumsat0,Qiskit‚Äôsconventionforthesignofthepowerof Z,andtheabsenceofthe‚àö\n2ùúã\nnormalizationfactorsusedinChapter9 .\n12.8.1 1-Qubit QFT\nOur1-qubitQFTcomputes N=21=2componentsvia(12.77):\nQFT2|0‚ü©=1‚àö\n2(|0‚ü©+|1‚ü©),QFT2|1‚ü©=1‚àö\n2(|0‚ü©‚àí|1‚ü©), (12.78)\n‚áíQFT2|q‚ü©=1‚àö\n22‚àë\np=0Z‚àípq\n2|p‚ü©. (12.79)\nNote,since Z2=‚àí1,theQFT2isthesameastheHadamardgate H,(12.48).\n12.8.2 2-Qubit QFT\nOur2-qubitQFTcomputes N=22=4componentsvia(12.77):\n|Y‚ü©=QFT4|y‚ü©=1‚àö\n43‚àë\nk=03‚àë\nl=0ylZ‚àíkl|k‚ü© (12.80)\n=1\n2[(y0Z0+y1Z0+y2Z0+y3Z0)|0‚ü©\n+(y0Z0+y1Z‚àíl+y2Z‚àí2+y3Z‚àí3)|1‚ü©\n+(y0Z0+y1Z‚àí2+y2Z‚àí4+y3Z‚àí6)|2‚ü©\n+(y0Z0+y1Z‚àí3+y2Z‚àí6+y3Z‚àí9)|3‚ü©]\n12.8 The Quantum Fourier Transform 277\n‚áíQFT4=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£‚ü®0|QFT4|0‚ü©‚ü®0|QFT4|1‚ü©‚ü®0|QFT4|2‚ü©‚ü®0|QFT4|3‚ü©\n‚ü®1|QFT4|0‚ü©‚ü®1|QFT4|1‚ü©‚ü®1|QFT4|2‚ü©‚ü®1|QFT4|3‚ü©\n‚ü®2|QFT4|0‚ü©‚ü®2|QFT4|1‚ü©‚ü®2|QFT4|2‚ü©‚ü®2|QFT4|3‚ü©\n‚ü®3|QFT4|0‚ü©‚ü®3|QFT4|1‚ü©‚ü®3|QFT4|2‚ü©‚ü®3|QFT4|3‚ü©‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶\n=1\n2‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£11 1 1\n1Z‚àí1Z‚àí2Z‚àí3\n1Z‚àí21Z‚àí2\n1Z‚àí3Z‚àí2Z‚àí1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (12.81)\nNote,forclaritywehaveleftoffthesubscript4on ZusedtoindicatethenumberofFourier\ncomponents,thatis,here Z‚â°Z4=e‚àíiùúã‚àï2=‚àíi.Wehavealsomultipliedoutsomepowers\nofZin(12.81).IntheQiskitconventionofSection12.7,thestatesare\n|0‚ü©=|00‚ü©, |1‚ü©=|01‚ü©, |2‚ü©=|10‚ü©, |3‚ü©=|11‚ü©. (12.82)\nTherulesfortheQFTarethus:\nQFT4|00‚ü©=H|0‚ü©‚äóH|0‚ü©,QFT4|01‚ü©=H|1‚ü©‚äóP(ùúã‚àï2)H|0‚ü©,(12.83)\nQFT4|10‚ü©=H|0‚ü©‚äóH|1‚ü©,QFT4|11‚ü©=H|1‚ü©‚äóP(ùúã‚àï2)H|1‚ü©,(12.84)\nwhereHistheHadamardgate,and P(ùúÉ)isthephasegate:\nP(ùúÉ)=[10\n0eiùúÉ]\n. (12.85)\nPuttingallofthepiecestogether,wehave QFT4beingaccomplishedwithjusttwoqubits\nplusfouroperations(threegatesandadirectproduct):\nQFT4||q1q0‚ü©=H||q0‚ü©‚äóPq0(ùúã‚àï2)H||q1‚ü©. (12.86)\nFigure12.9showsthecircuitbasedon(12.86),whereyoumaynotethattheswappingof\ntheresultsoftwoqubits, H||q0‚ü©andPq0(ùúã‚àï2)H||q1‚ü©,isaccomplishedbytheSWAPgateat\ntheend.ThesamecircuitcanbeimplementedinQiskit,aswedoinListing12.2.Thecor-\nrectnessofthecircuitisverifiedbyitsoutputmatchingthematrixin(12.81).\nExercise Takefoursamplevalues, yk‚Äôs,ofthefunction\ny(t)=3cos(ùúît)+2cos(3ùúît). (12.87)\n1) UseQFT4todetermine y(t)‚ÄôsFouriercomponents.\n2) Whatarethefrequenciesofthededucedcomponents?\n3) Checkthatthesummationofthecomponentsreproducestheinputsignal.\n12.8.3 n-Qubit QFT ‚äô\nGivennqubits to work with, we can compute N=2ntransform components. The basis\nvectorsfor n-qubitsarethedirectproductsinan N-dimensionalHilbertspace:\nFigure 12.9 Quantum Fourier transform circuit for 2-qubits.\n ‚à£q0‚å™\n‚à£q1‚å™ HH\nP(œÄ/2)\n278 12 Quantum Computing (G. He, Coauthor)\n||yn‚àí1¬∑¬∑¬∑y0‚ü©=||yn‚àí1‚ü©‚äó¬∑¬∑¬∑‚äó||y0‚ü©,yk‚àà{0,1}, (12.88)\nwherewearefollowingtheQiskitconvention.Thisproductcanalsobelabeledbyitsinteger\nrepresentation y:\ny=n‚àí1‚àë\nk=0yk2k,‚àà{0,1,‚Ä¶,N‚àí1}, (12.89)\n|0‚ü©=|0‚Ä¶0‚ü©, |2‚ü©=|0‚Ä¶010‚ü©,‚Ä¶. (12.90)\nTheQFTofthebasisvector |y‚ü©isthen\nQFT2n|y‚ü©=1‚àö\nNN‚àí1‚àë\nk=0e2ùúãiyk‚àïN|k‚ü© (12.91)\n=1‚àö\nN1‚àë\nk0=0¬∑¬∑¬∑1‚àë\nkn‚àí1=0e2ùúãiyk‚àïN||kn‚àí1¬∑¬∑¬∑k0‚ü©(12.92)\n=1‚àö\nN1‚àë\nk0=0¬∑¬∑¬∑1‚àë\nkn‚àí1=0‚äón‚àí1\nl=0e2ùúãiykl2l‚àín||kl‚ü©(12.93)\n=1‚àö\nN‚äón‚àí1\nl=0(||kl=0‚ü©+e2ùúãi0.yn‚àíl‚àí1¬∑¬∑¬∑y0||kl=1‚ü©)\n=1‚àö\nN(\n|0‚ü©+e2ùúãi0.y0|1‚ü©)‚äó(\n|0‚ü©+e2ùúãi0.y1y0|1‚ü©)\n‚äó¬∑¬∑¬∑‚äó(\n|0‚ü©+e2ùúãi0.yn‚àí1¬∑¬∑¬∑y0|1‚ü©)(12.94)\n=H||y0‚ü©‚äóPy0(ùúã‚àï2)H||y1‚ü©‚äóPy1(ùúã‚àï2)Py0(ùúã‚àï22)H||y2‚ü©\n‚äó¬∑¬∑¬∑‚äóPyn‚àí2(ùúã‚àï2)¬∑¬∑¬∑Py0(ùúã‚àï2n‚àí1)H||yn‚àí1‚ü©. (12.95)\nHerewehaveemployedthebinaryfractionnotation:\n0.xm¬∑¬∑¬∑x0def=m‚àë\nl=0xl2l‚àím‚àí1. (12.96)\nThe Python‚ÄìQiskit implementation of this n-qubit QFT is amazingly short and efficient,\nandgiveninListing12.3.Ofcourse,QiskitalreadyhasaQFTmethod,andwecouldhave\njustusedit.Infact,wediduseittoverifythisimplementation.\n12.9 Oracle +Diffuser =Grover‚Äôs Search Algorithm\nProblem Youaregivenadatabasecontaining N=2nelements,eachreferencedbyaspe-\ncificindex j,andthefunction fforwhich\nf(j)=ùõøij, (12.97)\nwherewehaveemployedtheKroneckerdeltafunction.Use nqubitstofinditem iinthe\ndatabase.\n12.9 Oracle +Diffuser=Grover‚Äôs Search Algorithm 279\nOfcourse,youcouldjuststartoutatthefirstelementandkeeptestingif f(j)=iuntilyou\nfindj=i; but we want something faster than that. As our example, let‚Äôs say we have a\ndatabasewith16elements:\njVjjVjjVjjVj\n23 067 010 110 14 150\n01 045 089 0 12 130\n34 078 011 120 15 160\n12 056 09 100 13 140\nIfwewanttosearchthroughthese16 =24elements,thenweneed n=4qubits.Andifwe\nwanttheelementwith j=8,thenwewanttocomeupwiththevalue90.\nNowwemusttranslatethissearchintoaquantumcomputation.First,weinitializethe\n4-qubitsysteminto |0‚ü©‚äón,andthenweuseadirectproductofHadamardoperatorstotrans-\nformthesystemintoauniformsuperpositionofstates:\n|ùúì‚ü©=H‚äón|0‚ü©‚äón=1‚àö\nNN‚àí1‚àë\nk=0|k‚ü©. (12.98)\nNow we have the stage set for a little magic. In literature, an oracle is a divine commu-\nnication or revelation that provides advice or prophecy. In QC, an oracle ùí™is a unitary\noperatororcircuitthatprovidesawaytodistinguishbetweendifferentstates.Ontheleft\nofFigure12.10,weshowaschematiccircuitthatconstructsanoracle ùí™forthisparticular\ndatabase,andforwhich\nùí™|k‚ü©=( ‚àí1)f(k)|k‚ü©, (12.99)\nwheref(k)isthefunctionin(12.97).Inthemiddle( i=15)andright( i=9)ofFigure12.10\nwe show what‚Äôs inside the black (actually white) box shown on the left of Figure 12.10.\nInListing12.4wegiveaQiskitsimulatorcodeforgeneratingan oraclecircuit.InListing\n12.5,wehaveanotherversionofthatcodeincludingthecallsneededtorunitontheIBM\nQuantum.\nApplyingtheoracletotheexpansion(12.98)yields:\nùí™|ùúì‚ü©=1‚àö\nN‚àë\nk‚â†i|k‚ü©‚àí1‚àö\nN|i‚ü©. (12.100)\nWe see that the oracle distinguishes the desired state |i‚ü©in the expansion by flipping its\nsign, while leaving the other states untouched. However, this hardly isolates the desired\nstate from its brethren. Not to worry, we will now use the oracle to construct the Grover\nq0\nq3q2q1\nH H‚à£k‚å™(‚Äì1)f(k) ‚à£k‚å™\nq0\nq3 H Hq2q1\nFigure 12.10 Left: A schematic quantum circuit for an oracle .Right:A noracle circuit for i=15.\n(c) An oracle circuit for i=9.",6408
135-12.10 Shors Factoring.pdf,135-12.10 Shors Factoring,"280 12 Quantum Computing (G. He, Coauthor)\nalgorithm,whoserepeatedapplicationwillamplifytheamplitudeof |i‚ü©sothatitstandsout\ntoadesiredlevelofprecision.Thealgorithmincorporatesthe diffuseroperatorUùúì:\nUùúìdef=2|ùúì‚ü©‚ü®ùúì|‚àíI,where |ùúì‚ü©=1‚àö\nNN‚àí1‚àë\nk=0|k‚ü©. (12.101)\nThediffuserhastheunusualproperty:\nUùúì‚àë\nkùõºk|k‚ü©=‚àë\nk[ùõº+(ùõº‚àíùõºk)]\n|k‚ü©,whereùõºdef=1\nN‚àë\nkùõºk. (12.102)\nForthoseamplitude ùõºkgreater(less)thantheaverage ùõº,thediffuserdecreases(increases)\ntheamplitudebelow(above)theaverage ùõº,toùõº‚àí(ùõºk‚àíùõº).Geometrically,thediffuser‚Äúre-\nflects‚Äùeachamplitude ùõºkwithrespecttotheaverage ùõº.Andsoasingleapplicationofan\noracle+diffusercomboamplifiestheamplitudeof |i‚ü©relativetothatoftheothers:\nUùúìùí™|ùúì‚ü©=3N‚àí4\nN3‚àï2|i‚ü©+N‚àí4\nN3‚àï2‚àë\nk‚â†ùúî|k‚ü©, (12.103)\n(needwepointoutthat3 N>N?).Thecombination Uùúìùí™,ofthediffuserandoracleiscalled\ntheGroveroperator .Repeatedapplicationsoftheoperatorwillkeepincreasing |i‚ü©‚Äôsrelative\namplitude,atleastuntilthenumberofapplicationsreaches ùúã‚àö\nN‚àï4[NielsenandChuang,\n2010].\n12.9.1 Grover‚Äôs Implementation\nWenowuseQiskittoconstructa4-qubitimplementationofGrover‚Äôsalgorithmthatwill\nsingleoutthe i=15elementsinthedataset.First,wecreateanoraclethatwillflipthesign\nforthestate |15‚ü©‚â°|1111‚ü©.Thatisaccomplishedbyplacingatriple-controlled-Zgate, HXH=\nZ,betweentwoHadamardgates,asshownontheleftofFigure12.10.Theoracleforother\nvaluesoficanbebuiltbyaddingapairof Xgatesbeforeandafterthetriple-controlled-Z\ngate on the corresponding qubits. For example, on the right of Figure 12.10, we form an\noraclefor |9‚ü©‚â°|1001‚ü©byaddingapairof Xgatesontoqubit-0.\nIn Figure 12.11 we give a circuit for the Grover‚Äôs algorithm. In Listing 12.4 we give a\nprogram that generates a circuit for the algorithm,and then runs it on a simulator. (The\ndiffusercodewithinisbasedonShor‚ÄôsAlgorithm[2023].)InListing12.5wegiveadriver\nversionofthissamecode,nowincludingthecallsneededtorunitontheIBMQuantum.\nq0H\nH\nUœâUœà Uœà Uœâ\nH\nH3210\n3210\n3210\n3210\nq1\nq2\nq3\nMeas4 0123\nFigure 12.11 A circuit for Grover‚Äôs Algorithm that combines an oracle (O) and a diffuser algorithm\n(U) that singles out the i=15 elements in a dataset.\n12.10 Shor‚Äôs Factoring ‚äô281\n0\n0000000100100011010001010110011110001001101010111100110111101111 000000010010001101000101011001111000100110101011110011011110111194 5 5 6 4 114 6 2 5 10 7 7 52505007501000Count\n080160240252\n235248\n222293\n260282278288282\n231\n202244\n231235\n217320Count934\nFigure 12.12 Left: Output of Grover‚Äôs search for i=9a sr u no nas i m u l a t o r . Right: Output of\nrunning the same program on the IBM Quantum Computer ibmq_lima .\nFigure 12.12 left shows the histogram resulting from running the Grover algorithm\nfori=9 on the simulator. We see that 926 out of 1000 trials resulted in the state |1001‚ü©\n(the desired i=9); this is a clear indication of the power of the algorithm. In contrast,\nFigure12.12-rightshowsthehistogramresultingfromrunningthecodeon ibmq_lima ,a\nphysicalIBMQuantumcomputer.Youwillnoticethatthephysicalcomputerdoes notyield\naclearlyoutstandingpeakforthe |1001‚ü©state.Thisisaconsequenceofnoiseintheelec-\ntronics.Thediscussionofquantumnoiseisanactiveresearchfieldthatisbeyondthescope\nofthisbook.InterestedreadersarereferredtoWangandKrstic[2020]andreferencetherein.\nExercise UseGrover‚Äôsalgorithmandfourqubitstosearchthetablegivenatthebegin-\nningofthissection.Seeifyoucomeupwith90astheanswer.\n12.10 Shor‚Äôs Factoring ‚äô\nWhat with its reliance on the computational difficulty of factoring large integers, prime\nnumbersareanessentialelementincryptology.The primefactors ofanumberaretheprime\nnumbers that when multiplied together result in the original number. For example, the\nprimefactorsof30are2,3,and5,butnot6sinceit‚Äôsnotprime. Shor‚Äôsalgorithm isacom-\nputationaltechniqueforfindingtheprimefactorsofanintegerusingquantumgates.Dueto\ntheefficiencyoftheQCalgorithm,thequantumcomputationisnearlyexponentiallyfaster\nthantheclassicalalgorithm.HerearethestepsinShor‚Äôsalgorithm[Wikipedia,2023]:\n1) Pickarandominteger1 <r<N.\n2) Computethegreatestcommondivisor(thePythoncommand, K=gcd(r,N)).\n3) IfK‚â†1,thenKisafactorof N,andyouhavefoundafactor.\n4) Useaperiod-findingmethod,tofindthesmallestperiod Tofthefunction\nf(x)=rx(modN),f(x+T)=f(x), (12.104)\nwherethe (mod)operatormakes faperiodicfunctionof x.\n282 12 Quantum Computing (G. He, Coauthor)\n5) IfTisodd,orif rT‚àï2=‚àí1(modN),returntostep1.\n6) Else,gcd (rT‚àï2+1,N)orgcd(rT‚àï2‚àí1,N),orboth,arenontrivialfactorsofN,andyou\nhavefoundatleastonefactor.\nToillustratethealgorithm,wehavechosen N=15andwrittenthemethod Amod15.Itis\ngiveninListing12.6,whereitcreatesaunitaryoperator Usuchthat\nU|y‚ü©=|Cy(modN)‚ü©,forallCcoprimewith15 . (12.105)\nPhase Estimation Before we can go on and implement Shor‚Äôs algorithm, we need QC\nmethodsfor phaseestimation andperiodfinding .Westartwithphaseestimation.Let Ube\naunitaryoperatorwitheigenvectors |u‚ü©andeigenvaluese2ùúãiùúô:\nU|u‚ü©=e2ùúãiùúô|u‚ü©. (12.106)\nWithoutactuallysolvingthiseigenvalueproblem,thequantumphasealgorithmprovides\nanestimateofthephase ùúô.Figure12.13showsacircuitthatimplementsthephasealgo-\nrithmusingtwoquantumregisters(computingunits).Thenumberofqubitsneededforthe\ntregister,theunitontop,isdeterminedbytherequiredaccuracy.Thenumberofqubitsin\nthesecondregisterbelowisthesameasthenumberofqubitsthat Uoperateson.Aswe\nsee in the figure, the tregister starts with the state ||0t‚ü©, and then the H-gates transform\neachqubitinthestateinto (|0‚ü©+|1‚ü©)‚àï‚àö\n2.Thecontrolregister U2jleavesthesecondqubit\nunchanged,butchangesthe jthqubitinthefirstregisterintothestate (|0‚ü©+e2ùúãi2jùúô|1‚ü©)‚àï‚àö\n2\n(a‚Äúphasekickback‚Äù).Afterperformingallofthecontroloperations,the tregisterwillbe\nleftinthestate\n|0‚ü©+e2ùúãi2t‚àí1ùúô|1‚ü©\n2‚äó¬∑¬∑¬∑‚äó|0‚ü©+e2ùúãi20ùúô|1‚ü©\n2=1\n2t‚àï22t‚àí1‚àë\nk=0e2ùúãikùúô|k‚ü©. (12.107)\nYetwealsoknowthatforaninteger0 ‚â§s<2t,theQFTactingonthebasisstate |s‚ü©givesus\nQFT2t|s‚ü©=1\n2t‚àï22t‚àí1‚àë\nk=0e2ùúãiks‚àï2t|k‚ü©. (12.108)\nIf2tùúô=sisaninteger,thenweseethattheRHSof(12.107)isexactlythetransformfor2t\ncomponents, QFT2t|s‚ü©.TheinverseQFTon QFT2t|s‚ü©revealsthevalue s:\n|s‚ü©=QFT‚àí1\n2t[\n|0‚ü©+e2ùúãi2t‚àí1ùúô|1‚ü©\n2‚äó¬∑¬∑¬∑‚äó|0‚ü©+e2ùúãi20ùúô|1‚ü©\n2]\n,ùúô=s\n2t.\nEven if 2tùúôis not an integer, the analysis of Nielsen and Chuang [2010] shows that the\ncircuitestimates ùúôtoanaccuracyof t‚àí‚åà\nlog(\n2+1\n2ùúñ)‚åâ\nbits,where ùúñistheprobabilitythat\nthealgorithmfails.\n‚à£0t‚å™H\nH\nU20U2t‚Äì1QFT‚Äì1\n‚à£u‚å™Figure 12.13 A quantum circuit for phase\nestimation.\n12.10 Shor‚Äôs Factoring ‚äô283\nPeriod Finding ThesecondalgorithmweneedtoimplementShor‚Äôsalgorithmisonethat\ndeterminestheperiodofthefunction\nf(x)=rx(modN). (12.109)\nHerer<Nisapositiveinteger, randNarecoprime(have1astheonlycommonfactor),\nandthe(modN)operatormakes faperiodicfunctionof x.W ew anttofindthesmallest\nvalueofTsuchthat\nf(x+T)=f(x),i.e.,rT(modN)=1. (12.110)\nLetUbeaunitaryoperatorsuchthat\nU|y‚ü©=|ry(modN)‚ü©. (12.111)\nWedefineaneigenstateof U:\n||uS‚ü©=1‚àö\nTr‚àí1‚àë\nk=0e‚àí2ùúãiSk‚àïT|||rk(modN)‚ü©,0‚â§S‚â§T‚àí1, (12.112)\nU||uS‚ü©=e2ùúãiS‚àïT||uS‚ü©. (12.113)\nEventhoughwedonotknowthevalueof T,orallofthedetailsofthe ||uS‚ü©state,wedo\nknowaneatidentity,namely:\n1‚àö\nTT‚àí1‚àë\nS=0||uS‚ü©=|1‚ü©. (12.114)\nSoifwesettheinitialstateofthesecondquantumregisterto |1‚ü©,whichisasuperposition\nof||uS‚ü©‚Äôs,thequantumphaseestimationwillmeasurethephase\nùúô=S\nT, (12.115)\nwheresisarandomintegerbetween0and T‚àí1.Wecannowusethe continuedfractions\nalgorithmtodeterminethevalueof T.\nIn Listing 12.6, we finally give our code for a quantum circuit that computes Shor‚Äôs\nalgorithm [Shor‚Äôs Algorithm, 2023]. It combines quantum circuits for phase estimation\nand period-finding qpe, with the unitary operator amod15, and uses them to factor the\nnumber15.Here‚Äôsatypicaloutputfromit:\n1Attempt #0\nRandom a = 7\nRegister reading: 00000000\nCorresponding phase: 0.000000\n5Phase: 0.0\nr=1\nAttempt #1\nRandom a = 8\n9Register reading: 01000000\nCorresponding phase: 0.250000\nPhase: 0.25\nr=4\n13Found factor: 5\nFound factor: 3",7813
136-12.11 Code Listings.pdf,136-12.11 Code Listings,"284 12 Quantum Computing (G. He, Coauthor)\n12.11 Code Listings\nListing 12.1 Entangle.py computes Hamiltonian, eigenvalues, and eigenvectors for\nentangledquantumstatesusingnumpy.\n# Entangle . py : Calculate entangled quantum s t a t e s\n2\nfromnumpyimport ‚àó;fromnumpy. linalg import ‚àó\nnmax = 4\n6H = zeros((nmax,nmax) , float)\nXAXB = array([[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]]) # sigxA . sigxB\nYAYB = array ([[0 ,0 ,0 , ‚àí1],[0,0,1,0],[0,1,0,0],[ ‚àí1,0,0,0]]) # sigyA . sigyA\nZAZB = array([[1,0,0,0],[0, ‚àí1,0,0],[0,0, ‚àí1,0],[0,0,0,1]]) # sigzA . sigzA\n10SASB = XAXB + YAYB + ZAZB ‚àí3‚àóZAZB # Hamiltonian/ factor\nprint(‚ÄôHamiltonian without muÀÜ2/rÀÜ3 factor \n‚Äô ,SASB)\nes ,ev = eig(SASB) # Eigenvalues &vectors\nprint(‚ÄôEigenvalues \n‚Äô ,es)\n14print(‚ÄôEigenvectors(incolumns)\n‚Äô ,ev)\nphi1 = (ev[0,0], ev[1,0], ev[2,0], ev[3,0]) # Extract vectors\nphi4 = (ev[0,1], ev[1,1], ev[2,1], ev[3,1])\nphi3 = (ev[0,2], ev[1,2], ev[2,2], ev[3,2])\n18phi2 = (ev[0,3], ev[1,3], ev[2,3], ev[3,3])\nbasis = [phi1,phi2,phi3,phi4] # List eigenvectors\nforiin range (0,n m a x): # Hamiltonian in new basis\nforjin range (0, n m a x):\n22 term = dot (SASB, basis [i ] )\nH[i , j] = dot (basis [j],term )\nprint(‚ÄôHamiltonian in Eigenvector Basis \n‚Äô ,H )\nListing 12.2 QFT4.py A 2-qubit circuit that computes four Fourier components using\nQiskit.\n# QFT4.py: A Qiskit program to compute a 2 ‚àíqubit QFT for 4 components\n2\nimportmath\nfromqiskitimportQuantumCircuit\nimportqiskit.quantum_info as qi\n6importnumpy as np\ndefqft2(inverse=False) ‚àí> QuantumCircuit:\nangle = math.pi/2\n10 ifinverse isTrue: angle = ‚àíangle\nqc = QuantumCircuit(2) #C r e a t ea2 ‚àíqubit circuit\nqc.h(1) # H gate on qubit ‚àí1\nqc.cp(angle, 0, 1) # Controlled phase gate\n14 qc.h(0)\nqc.swap(0, 1) # Swap the qubits\nreturnqc # Return circuit as gate\n18if__name__ == ""__main__"" :\nprint(np.around(qi.Operator(qft2()).data, 3)) # Circuit matrix\nListing 12.3 QFTn.py An n-qubit circuit that computes 2nFourier components using\nQiskit.\n# Q F T.py: A Qiskit program using n ‚àíqubits for 2^n component QFT\n3importmath\nfromqiskitimportQuantumCircuit\nimportqiskit.quantum_info as qi\nfromqiskit.circuit.library importQFT\n7importnumpy as np\n12.11 Code Listings 285\ndefqft(n:int,i n v e r s e : bool= False , skip_swap: bool=F a l s e ) ‚àí> QuantumCircuit:\n#b u i l dan ‚àíqubit qft circuit\n11angle = np.pi/2\nifinverse isTrue:\nangle = ‚àíangle\nqc = QuantumCircuit(n)\n15foriin reversed (range(n)):\nqc.h(i)\nforjin range (i):\nqc.cp(angle/2 ‚àó‚àó(i‚àíj‚àí1), j , i)\n19ifskip_swap isFalse:\nforiin range (math.floor(n/2)):\nqc.swap(i , n ‚àíi‚àí1)\nreturnqc\n23\nif__name__ == ""__main__"" :\nforkin range (10):\nprint(np.max(np.abs(qi.Operator(qft(k)).data ‚àíqi.Operator(QFT(k)).data)))\nListing 12.4 OracleSim.py A quantum circuit for Grover‚Äôs algorithm for i=0-15 on a\nsimulator.\n# OracleSim.py: Simulator version of Qiskit code for Oracle circuit , i=0 ‚àí15\nfromqiskitimportQuantumCircuit, Aer, transpile , assemble\n4fromqiskit.visualization importplot_histogram\nfromnumpyimportmath\ndeforacle(omega: int): # With removed ‚àí>Gate\n8ifomega<0oromega >= 16: #F l i ps i g ni f | o m e g a >\nraiseValueError( ""Input should be betwn"" +""0&15, got"" ,o m e g a )\nbit_string =f ""{omega: 04b}"" # Convert omega to bit pattern\nquantum_circuit = QuantumCircuit(4) # 4 bit q u a n t u m circuit\n12[quantum_circuit.x(3 ‚àíidx)foridxin range (4)ifbit_string[idx]== ‚Äô0‚Äô]\nquantum_circuit.h( 3 )\nquantum_circuit.mcx([0,1,2],3)\nquantum_circuit.h(3)\n16[quantum_circuit.x( 3 ‚àíidx)foridxin range (4)ifbit_string[idx]== ‚Äô0‚Äô]\ncap_u_omega = quantum_circuit.to_gate()\ncap_u_omega.name= ""omega"" # Differs\nreturncap_u_omega\n20\n# diffuser .py: circuit for a diffuser with n qubits\ndefdiffuser(n_qubits: int): # remove Gate , I ‚àí22|psi> <psi |\nquantum_circuit = QuantumCircuit(n_qubits) # n qubit circuit\n24quantum_circuit.h( range( n_qubits)) # M a p|psi> to |0...0 >\nquantum_circuit.x( range(n_qubits)) # M a p|0...0 > to |1...1 >\nquantum_circuit.h( n_qubits ‚àí1)# Multi cntrl ‚àíz, flips sign |1...1 >\nquantum_circuit.mcx(list( range(n_qubits ‚àí1)),n_qubits ‚àí1)\n28quantum_circuit.h(n_qubits ‚àí1) # M a p back to |0...0 >\nquantum_circuit.x( range( n_qubits))\nquantum_circuit.h( range( n_qubits)) #M a pb a c k t o | p s i >\ncap_u_psi = quantum_circuit.to_gate()\n32cap_u_psi.name= ""$U_{\\psi} $""\nreturncap_u_psi\n# Grover .py : Driver code for QC Grover algorithm on simulator\n36if__name__ == ""__main__"" :\ncap_n=4\nqc = QuantumCircuit(cap_n)\nqc.h(range(cap_n)) #p u ti n t o | \ p s i >\n40 #R u n\ncap_r = math. ceil(math.pi ‚àómath.sqrt(cap_n)/4) # Iterate Grover R times\nforiin range (cap_r):\nqc.append(oracle(9) , range(cap_n))\n44 qc.append(diffuser(cap_n), range(cap_n))\nqc.measure_all()\n286 12 Quantum Computing (G. He, Coauthor)\nqc.draw(output= ""mpl"",filename= ""grover4_circuit.png"" )\nbackend=Aer.get_backend( ""aer_simulator"" ) # Run on simulator\n48transpiled_circuit = transpile(qc,backend=backend)\njob = backend.run(transpiled_circuit)\nresult = job.result()\nhistogram = result.get_counts()\n52plot_histogram(histogram,filename= ""grover4_sim_histogram.png"" )\nListing 12.5 OracleIBM.py AnIBMQuantumcircuitforGrover‚Äôsalgorithmfori=0-15.\n# OracleIBM.py: I B M Q C Qiskit code for Oracle circuit , i=0 ‚àí15.\n2\nfromqiskitimportQuantumCircuit, Aer, transpile\nfromqiskit.visualization importplot_histogram\nfromqiskit.tools importjob_monitor\n6fromqiskit_ibm_provider importIBMProvider, least_busy\nfromnumpyimportmath\ndeforacle(omega: int): # remove ‚àí>Gate\n10 # Flip the sign if state is |omega >\nifomega<0oromega >= 16:\nraiseValueError( ""Need input"" +""0 - 15, got "" ,o m e g a )\n# Convert omega into bit pattern\n14 # bit_string = f ""{omega: 04b }"" ########################\nbit_string = f ""{omega:04b}""\n# print ("" bit ‚àístring "" , bit_string)\n# Start a q u a n t u m circuit of 4 qubits\n18quantum_circuit = QuantumCircuit(4)\n[quantum_circuit.x(3 ‚àíidx)foridxin range (4)ifbit_string[idx] == ‚Äô0‚Äô]\nquantum_circuit.h(3)\nquantum_circuit.mcx([0, 1, 2], 3)\n22quantum_circuit.h(3)\n[quantum_circuit.x(3 ‚àíidx)foridxin range (4)ifbit_string[idx] == ‚Äô0‚Äô]\ncap_u_omega = quantum_circuit.to_gate()\ncap_u_omega.name = ""$U_\\omega $""\n26returncap_u_omega\n# diffuser .py: a quantum circuit for a general diffuser with n qubits\ndefdiffuser(n_qubits: int): # remove Gate\n30 # Where| psi>is the uniform superposition state\n# Create a circuit with n_qubits\nquantum_circuit = QuantumCircuit(n_qubits)\n# M a p|psi> to |0...0 >\n34quantum_circuit.h( range(n_qubits))\n# M a p|0...0 > to |1...1 >\nquantum_circuit.x( range(n_qubits))\n# Multiply controlled ‚àíz\n38 # T o flip sign for |1...1 >\nquantum_circuit.h(n_qubits ‚àí1)\nquantum_circuit.mcx(list( range(n_qubits ‚àí1)), n_qubits ‚àí1)\nquantum_circuit.h(n_qubits ‚àí1)\n42 # M a p back from |1...1 > to |0...0 >\nquantum_circuit.x( range(n_qubits))\n# M a p back |0...0 > to |psi>\nquantum_circuit.h( range(n_qubits))\n46cap_u_psi = quantum_circuit.to_gate()\ncap_u_psi.name = ""$U_{\\psi} $""\nreturncap_u_psi\n50# Grover.py: Driver code for QCGrover algorithm on simulator &IBMQuantum\nif__name__ == ""__main__"" :\n# these 2 commented lin es only need to run once at the very beginning\n# token =‚Äô ‚àó‚àó‚àó‚àó‚àó‚àó‚àó‚àó‚àó‚àó‚àó ‚Äô\n54 # QiskitRuntimeService . save_account(channel=""ibm_quantum"" , token=token ,\noverwrite=True)\ncap_n = 4 # number of qubits\nqc = QuantumCircuit(cap_n)\nqc.h(range(cap_n)) #p u ti n t o | \ p s i >\n58 # R u n Grover iteration for R times\n12.11 Code Listings 287\ncap_r = math. ceil(math.pi ‚àómath.sqrt(cap_n) / 4)\nforiin range (cap_r):\nqc.append(oracle(9) , range(cap_n))\n62 qc.append(diffuser(cap_n), range(cap_n))\nqc.measure_all()\nqc.draw(output= ""mpl"", filename= ""grover4_circuit.png"" )\n# Run on simulator\n66backend = Aer.get_backend( ""aer_simulator"" )\ntranspiled_circuit = transpile(qc, backend=backend)\njob = backend.run(transpiled_circuit)\nresult = job.result()\n70histogram = result.get_counts()\nplot_histogram(histogram, figsize=(7, 7), filename= ""grover4_sim_histogram.png"" )\nprint(max(histogram, key=histogram.get))\n# Load account and get provider\n74provider = IBMProvider(instance= ""ibm-q/open/main"" )\ndevice = least_busy(provider.backends(\nfilters= lambdax:int(x.configuration().n_qubits) >= cap_n\nand not x.configuration().simulator\n78 andx.status().operational isTrue))\nprint(""Running on least busy device:"" ,d e v i c e )\n# Transpile and run\ntranspiled_circuit = transpile(qc, device)\n82job = device.run(transpiled_circuit)\njob_monitor(job, interval=2)\n# Get result\nresult = job.result()\n86histogram = result.get_counts(qc)\nplot_histogram(histogram, figsize=(7, 7), filename= ""grover4_histogram.png"" )\nListing 12.6 Shor.py AQuantumcircuitforShor‚ÄôsAlgorithm.\n# Shor . py : Shor ‚Äô s algorithm\n# https:// qiskit .org/textbook/ch ‚àíalgorithms/shor .html\n3\nimportrandom\nfromfractions importFraction\nfrommathimportgcd\n7fromtypingimportList\nfromqiskitimportQuantumCircuit, Aer, transpile , assemble\nfromqiskit.circuit.library importQFT\n11defamod15(a_in: int,p _ i n : int)‚àí> QuantumCircuit: #M u l txa _ i nm o d1 5\nifa_innot in[ 2 ,4 ,7 ,8 ,1 1 ,1 3 ,1 4 ] :\nraiseValueError( ""‚Äôa_in‚Äô must be 2,4,7,8,11,13 or 14"" )\nquantum_circuit = QuantumCircuit(4)\n15foriteration in range (p_in):\nifa_inin[2, 13]:\nquantum_circuit.swap(2, 3)\nquantum_circuit.swap(1, 2)\n19 quantum_circuit.swap(0, 1)\nifa_inin[7, 8]:\nquantum_circuit.swap(0, 1)\nquantum_circuit.swap(1, 2)\n23 quantum_circuit.swap(2, 3)\nifa_inin[4, 11]:\nquantum_circuit.swap(1, 3)\nquantum_circuit.swap(0, 2)\n27 ifa_inin[7, 11, 13, 14]: # I added 14 here\nforiin range (4):\nquantum_circuit.x(i)\nquantum_circuit.name = ""%iÀÜ%i mod 15"" %( a _ i n ,p _ i n )\n31returnquantum_circuit # return the circuit\ndefqpe(u_list: List[QuantumCircuit]) ‚àí>float:# Build phase circuit\n# u_list : a l i s t of QuantumCircuit\n35 #[ U ÀÜ(2ÀÜ0) , U ÀÜ(2ÀÜ1), ... U ÀÜ(2ÀÜ(t‚àí1)) ]\nt=len(u_list)\nnum_qubits_u = u_list[0].num_qubits # N qubits for cap_u gate\n288 12 Quantum Computing (G. He, Coauthor)\nqc = QuantumCircuit(t + num_qubits_u, t)\n39 # put the first t_count qubits into superposition\nforiin range (t):\nqc.h(i)\n# put the last n_u qubit into |1> state\n43qc.x(t) # qiskit convention\nforiin range (t): # Add contr ‚àíUÀÜ{2ÀÜj} g a t e\nqc.append(u_list[i].to_gate().control() , [i]\n+[ j + tforjin range (num_qubits_u)])\n47qc.append(QFT(t, inverse=True).to_gate() , range(t)) # Inverse Q F T\nqc.measure( range(t),range(t)) # Finally , measure\nsimulator = Aer.get_backend( ""aer_simulator"" ) # Run on simulator\nq_obj = assemble(transpile(qc, simulator), shots=1)\n51result = simulator.run(q_obj, memory=True).result()\nreadings = result.get_memory()\nprint(""Register reading: "" +r e a d i n g s [ 0 ] )\nphase = int(readings[0], 2)/(2 ‚àó‚àót)\n55print(""Corresponding phase: %f"" % phase)\nreturnphase\nif__name__ == ""__main__"" :\n59cap_n = 15\nfactor_found = False\nattempt = 0\nwhile not factor_found:\n63 print(""Attempt #"" , attempt)\nattempt += 1\na = random.randint(2, cap_n ‚àí1)\nprint(""Random a = "" ,a )\n67 k=g c d ( a ,c a p _ n )\nifk! =1 :\nfactor_found = 1\nprint(""Found factor: "" ,k )\n71 else:\np=q p e ( [ a m o d 1 5 ( a ,2 ‚àó‚àój)forjin range (8)])\nprint(""Phase: "" ,p )\nfraction = Fraction(p).limit_denominator(cap_n)\n75 s, r = fraction.numerator, fraction.denominator\nprint("" r="",r )\nifr%2= =0 : # r is even\nguesses = [gcd(a ‚àó‚àó(r//2)+1, cap_n),\n79 gcd(a ‚àó‚àó(r//2)‚àí1, cap_n)]\nforginguesses:\nifgnot in[1, cap_n] and(cap_n % g) == 0:\nprint(""Found factor: %i"" %g )\n83 factor_found = True",11525
137-Part III Applications.pdf,137-Part III Applications,289\nPart III\nApplications,27
138-Chapter 13 ODE Applications Eigenvalues Scattering Trajectories.pdf,138-Chapter 13 ODE Applications Eigenvalues Scattering Trajectories,,0
139-13.2.1 Not Recommended Matchless Searching.pdf,139-13.2.1 Not Recommended Matchless Searching,"291\n13\nODE Applications; Eigenvalues, Scattering, Trajectories\nNow that we have developed reliable methods to solve ODEs, we apply them to some chal-\nlenging problems. First, we combine our ODE solver with a search algorithm to solve the\nquantum eigenvalue problem for an arbitrary potential. Then we study classical scattering\nin a system that becomes chaotic. Finally, we look upward to balls falling out of the sky and\nplanets that do not .\n13.1 Quantum Eigenvalues for Arbitrary Potentials\nProblem Whatistheenergyofaparticleboundbyapotentialthatconfinesittoanatomic\ndistance?\nQuantummechanicsdescribesphenomenathatoccuratatomicandsubatomic(particle)\nscales.Itisastatisticaltheoryinwhichtheprimeobservableistheprobabilitythataparticle\nis located in a region dxaround point x. Yet, usually, we solve for a wave function ùúì(x),\nandthencalculatetheprobabilityas Óàº=|ùúì(x)|2dx.Ifaparticleofenergy Eismovingin\nonedimensionandexperiencesapotential V(x), thatwavefunctionisdeterminedbyan\nordinarydifferentialequation,thetime-independentSchr√∂dingerequation:1\n‚àí‚Ñè2\n2md2ùúì(x)\ndx2+V(x)ùúì(x)=Eùúì(x). (13.1)\nInpractice,wesolveforthe wavevector ùúÖ,whereitisrelatedtoboundstates( E<0)by:\nùúÖ2=‚àí2m\n‚Ñè2E. (13.2)\nTheSchr√∂dingerequationnowtakestheform\nd2ùúì(x)\ndx2‚àí2m\n‚Ñè2V(x)ùúì(x)=ùúÖ2ùúì(x). (13.3)\nTheproblemstatesthattheparticleisbound,whichmeansthatitisconfinedtosomefinite\nregionofspace,which,inturn,impliesthat ùúì(x)isnormalizable.(Unboundparticlesdo\n1 Theequationfor2Dor3Dmotion,orwithtime-dependence,requiresthesolutionofapartial\ndifferentialequation,asdiscussedinChapter24.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n292 13 ODE Applications; Eigenvalues, Scattering, Trajectories\nnothavenormalizablewavefunctions.)Theonlywaytohaveanormalizablewavefunction,\nisifùúì(x)decaysexponentiallyas x‚Üí¬±‚àû,whereV=0:2\nùúì(x)‚Üí{e‚àíùúÖx,forx‚Üí+‚àû,\ne+ùúÖx,forx‚Üí‚àí‚àû.(13.4)\nInsummary,althoughweknowhowtosolvetheODE(13.1)withournumericaltools,we\nmustalsofigureoutatechniquetodosowithintheconstraintsoftheboundaryconditions\n(13.4).ThisextraconditionturnstheODEproblemintoan eigenvalueproblem ,whichhas\nsolutions(eigenvalues )foronlycertainvaluesoftheenergy EorùúÖ.Theground-stateenergy\ncorresponds to the smallest (most negative) eigenvalue. Because the greater the number\nof oscillationsin a wave function,the greater is the kineticenergy of the bound particle,\nthemorenodesinawavefunction,thehighertheenergy.Accordingly,weexpectthatthe\nboundstatewiththeleastenergy(ground-state)willhaveanodelesswavefunction.\n13.1.1 Model: Nucleon in a Box\nThe numerical methods we describe are capable of handling the most realistic potential\nshapes.Yettomakeaconnectionwiththestandardtextbookcase,andtopermitsomeana-\nlyticchecking,wewilluseasimplemodelinwhichthepotential V(x)in(13.1)isafinite\nsquarewell(Figure13.1):\nV(x)={‚àíV0=‚àí83MeV,for|x|‚â§a=2fm,\n0, for|x|>a=2fm,(13.5)\nwherevaluesof83MeVforthedepth,and2fmfortheradius,aretypicalfornuclear-bound\nstates.WiththispotentialtheSchr√∂dingerequation(13.3)becomes\nd2ùúì(x)\ndx2+(2m\n‚Ñè2V0‚àíùúÖ2)\nùúì(x)=0,for|x|‚â§a, (13.6)\nd2ùúì(x)\ndx2‚àíùúÖ2ùúì(x)=0, for|x|>a. (13.7)\nToevaluatetheratioofconstantshere,weinsert c2,thespeedoflightsquared,intoboth\nthenumeratorandthedenominatorandthensomefamiliarvalues:\n2m\n‚Ñè2=2mc2\n(‚Ñèc)2‚âÉ2√ó940MeV\n(197.32MeVfm )2=0.0483MeV‚àí1fm‚àí2. (13.8)\nxmatch\nxV(x) ‚ÄìV0\n‚Äìa 0 a0Figure 13.1 A square well in bold, and the wave\nfunction within it. At the location xmnear the right edge\nof the well, the wave function computed by integration\nin from the left is matched to the one computed by\nintegration in from the right (dashed curve).\n2 IfwewereworkingwithaCoulombpotential,itsveryslowfalloffwouldrequireusingCoulomb\nfunctionsat ¬±‚àû,notexponentials[Landau,1996].\n13.2 Algorithm: ODE Solver +Search 293\n13.2 Algorithm: ODE Solver +Search\nAkeyelementinsolvingforboundstatesistherealizationthatthesecond-orderODEwill\nhavetwosolutions:onethatdecaysas x‚Üí¬±‚àû,andasecondonethatgrowsas x‚Üí¬±‚àû.\nConsequently,anumericalsolution,beinganapproximation,willalwayscontainsomeof\neach.Yetifweintegratestep-by-steponasolutionthatisincreasinginvalue,thenthebitof\nthedecreasingfunctionthatismixedinwillgetsmallerandsmaller,whiletheincreasing\nfunctionkeepsincreasing,andthusthesolutionbecomesmoreaccurate.Likewise,ifwe\nintegratestep-by-steponasolutionthatisdecreasinginvalue,thenthebitoftheincreasing\nfunctionthatismixedinwillgetgreaterandgreater,whilethedecreasingfunctionkeeps\ndecreasing,andthusthesolutionbecomeslessaccurate. AlwaystrytointegrateanODEas\nthefunctionisincreasing!\nThe solution to our eigenvalue problem combines the numerical solution of the ordi-\nnarydifferentialequation(13.3)withatrial-and-errorsearchforawavefunctionthatalso\nsatisfiestheboundaryconditions(13.4).Thisiscarriedoutinseveralsteps:\n1) Startatthefar leftatx=‚àíx‚àû‚âÉ‚àí ‚àû,wher ex‚àû‚â´a.Sincethepotential V=0inthis\nregion, the analytic solution here is e¬±ùúÖx. Accordingly, assume that the wave function\ntheresatisfiestheleft-handboundarycondition:\nùúìL(x=‚àíx‚àû)=e+ùúÖx=e‚àíùúÖx‚àû. (13.9)\n2) Use your rk4ODE solver to integrate ùúìL(x)toward the origin (to the right), from\nx=‚àíx‚àû,untilyoureachthe matchingradiusxm.Inthisway,weareintegrating,step-by-\nstep,overanincreasingfunction.Theexactvalueofthismatchingradiusisnotimpor-\ntant, and our final solution should be independent of it. In Figure 13.1, we show\na sample solution with xm‚âÉa, that is, we match just beyond the right edge of the\npotential.InFigure13.2,weseesomeguessesthatdonotmatch.\n3) Startattheextreme right,thatis,at x=+x‚àû‚âÉ+ ‚àû,withawavefunctionthatsatisfies\ntheright-handboundarycondition:\nùúìR(x=ùúÖx‚àû)=e‚àíùúÖx=e‚àíùúÖx‚àû. (13.10)\n4) Useyour rk4solvertostep ùúìR(x)intowardtheorigin(totheleft),from x=+x‚àû,until\nyou reach the matching radius xm. In this way, we are integrating, step-by-step, over\nan increasing function. This means that we have integrated up to the potential well\n(Figure13.1).\nFigure 13.2 Two guesses for the energy that are either too\nlow or too high to be an eigenvalue. We see that the low-E\nguess does not oscillate fast enough to match a dying\nexponential, while the high-E guess oscillates too fast.\n0 xLow EHigh E",6206
140-13.2.4 Explorations.pdf,140-13.2.4 Explorations,"294 13 ODE Applications; Eigenvalues, Scattering, Trajectories\n5) Inorderforprobabilityandcurrenttobecontinuousat x=xm,ùúì(x),andùúì‚Ä≤(x)mustbe\ncontinuousthere.Requiringtheratio ùúì‚Ä≤(x)‚àïùúì(x),calledthe logarithmicderivative ,tobe\ncontinuousthereencapsulatesbothcontinuityconditionsintoasinglecondition,andis\nindependentof ùúì‚Äôsnormalization.\n6) Although we do not know ahead of time what value for ùúÖwill be an eigenvalue, we\nstillneedastartingvalueforitinordertouseourODEsolver.Suchbeingthecase,we\nstartthesolutionwithaguess.Agoodguessforground-stateenergywouldbeavalue\nsomewhatupfromthatatthebottomofthewell, E>‚àíV0.\n7) Becauseitisunlikelythataguesswillbecorrect,theleft-andright-wavefunctionswill\nnot quite match at x=xm(Figure 13.2). This is fine because we can use the amount\nofmismatchtoimprovethenextguess.Wemeasurehowwelltherightandleftwave\nfunctionsmatchbycalculatingthedifferenceinlogarithmicderivatives:\nŒî(E,x)=ùúì‚Ä≤\nL(x)‚àïùúìL(x)‚àíùúì‚Ä≤\nR(x)‚àïùúìR(x)\nùúì‚Ä≤\nL(x)‚àïùúìL(x)+ùúì‚Ä≤\nR(x)‚àïùúìR(x)|||||x=xm, (13.11)\nwhere the denominator is there to avoid overly large or small numbers. Next, we try\nadifferentenergy,notehowmuch Œî(E)haschanged,andusethechangetodeducea\nbetter guess for the energy. The search continues until the left and right ùúì‚Ä≤‚àïùúìmatch\nwithinsomesettolerancethatdependsontheprecisioninenergydesired.\n13.2.1 Not Recommended: Matchless Searching\nAsimplerapproachtofindingboundstatesistotryoutabunchofenergies,andforeach\njustintegrateoutfrom0toinfinity.Then,ifthewavefunctionhasnotblownup,youhave\nfoundaboundstate.Theproblemwiththisapproachisthatitintegratesstep-by-stepona\ndecreasingfunction.Thismeansthatthesmallamountofincreasingfunctionthatismixed\ninfromthestartkeepsgettinglarger,whilethedesiredsolutionkeepsgettingsmaller,and\nsoyouwillendupwithawavefunctioncontainingalargepercentageoferror. Alwaystry\ntointegrateonincreasingfunctions!\n13.2.2 Numerov Algorithm for Schr√∂dinger ODE‚®Ä\nWegenerallyrecommendthefourth-orderRunge‚ÄìKuttamethodforsolvingODEs.However,if\ntheODEbeingsolveddoesnotcontainanyfirstderivatives(suchasourSchr√∂dingerequation),\nthen it is possible to use the Numerov algorithm, which is specialized for just this situation.\nWhilethisalgorithmisnotasgeneralasrk4,itisof Óàª(h6),andthusspeedsupthecalculation\nbyprovidingadditionalprecision .\nWestartbyrewritingtheSchr√∂dingerequation(13.3)inthegenericform,\nd2ùúì\ndx2+k2(x)ùúì=0,k2(x)=2m\n‚Ñè2{E+V0,for|x|<a,\nE,for|x|>a,(13.12)\nwherek2=‚àíùúÖ2forboundstates.Observethatalthough(13.12)isspecializedtoasquare\nwell,otherpotentialswouldhave V(x)inplaceof ‚àíV0.ThetrickintheNumerovmethod\nis to get extra precision in the second derivative by taking advantage of there being\nno first derivative dùúì‚àïdxin (13.12). We start with the Taylor expansions of the wave\nfunction,\n13.2 Algorithm: ODE Solver +Search 295\nùúì(x+h)‚âÉùúì(x)+hùúì(1)(x)+h2\n2ùúì(2)(x)+h3\n3!ùúì(3)(x)+h4\n4!ùúì(4)(x)+¬∑¬∑¬∑\nùúì(x‚àíh)‚âÉùúì(x)‚àíhùúì(1)(x)+h2\n2ùúì(2)(x)‚àíh3\n3!ùúì(3)(x)+h4\n4!ùúì(4)(x)+¬∑¬∑¬∑,\nwhereùúì(n)signifiesthe nthderivative dnùúì‚àïdxn.Becausetheexpansionof ùúì(x‚àíh)hasodd\npowers ofhappearing with negative signs, all odd powers cancel when we add ùúì(x+h)\nandùúì(x‚àíh)together:\nùúì(x+h)+ùúì(x‚àíh)‚âÉ2ùúì(x)+h2ùúì(2)(x)+h4\n12ùúì(4)(x)+Óàª(h6),\n‚áíùúì(2)(x)‚âÉùúì(x+h)+ùúì(x‚àíh)‚àí2ùúì(x)\nh2‚àíh2\n12ùúì(4)(x)+Óàª(h4).\nToobtainanalgorithmforthesecondderivative,weeliminatethefourth-derivativeterm\nbyapplyingtheoperator1 +h2\n12d2\ndx2totheSchr√∂dingerequation(13.12):\nùúì(2)(x)+h2\n12ùúì(4)(x)+k2(x)ùúì+h2\n12d2\ndx2[k2(x)ùúì(4)(x)] =0. (13.13)\nWeeliminatethe ùúì(4)termsbysubstitutingthederivedexpressionfor ùúì(2):\nùúì(x+h)+ùúì(x‚àíh)‚àí2ùúì(x)\nh2+k2(x)ùúì(x)+h2\n12d2\ndx2[k2(x)ùúì(x)] ‚âÉ0.(13.14)\nNowweuseacentral-differenceapproximationforthesecondderivative:\nh2d2[k2(x)ùúì(x)]\ndx2‚âÉ[ (k2ùúì)x+h‚àí(k2ùúì)x]+[(k2ùúì)x‚àíh‚àí(k2ùúì)x]. (13.15)\nAfterthissubstitution,weobtaintheNumerovalgorithm:\nùúì(x+h)‚âÉ2[\n1‚àí5\n12h2k2(x)]\nùúì(x)‚àí[\n1+h2\n12k2(x‚àíh)]\nùúì(x‚àíh)\n1+h2k2(x+h)‚àï12. (13.16)\nWeseethattheNumerovalgorithmusesthevaluesof ùúìatthetwoprevioussteps xand\nx‚àíhtomoveùúìforwardto x+h.Tostepbackwardin x,weneedonlytoreversethesign\nofh.Ourimplementationofthisalgorithm, Numerov.py,isgiveninListing13.1.\n13.2.3 Implementation: Eigenvalues via ODE Solver +Bisection Algorithm\n1) Combine your bisection algorithm search program with your rk4or Numerov ODE\nsolverprogramtocreateaneigenvaluesolver.Startwithastepsize h=0.04.\n2) Writeamethodthatcalculatesthematchingfunction Œî(E,x)asafunctionofenergy\nandmatchingradius.Thismethodwillbecalledbythebisectionalgorithmprogramto\nsearchfortheenergyatwhich Œî(E,x=2)vanishes.\n3) Asafirstguess,take E‚âÉ‚àí65MeV.\n4) Searchuntil Œî(E,x)changesinonlythefourthdecimalplace.Wedothisinthecode\nQuantumEigen.py giveninListing13.2.\n5) Printoutthevalueoftheenergyforeachiteration.Thiswillgiveyouafeelastohow\nwelltheprocedureconverges,aswellasameasureoftheprecisionobtained.Trydif-\nferentvaluesforthetoleranceofthelogarithmicderivativeuntilyouareconfidentthat\nyouareobtainingthreegooddecimalplacesintheenergy.",4897
141-13.3.3 Assessment.pdf,141-13.3.3 Assessment,"296 13 ODE Applications; Eigenvalues, Scattering, Trajectories\n6) Buildinalimittothenumberofiterationsyoupermit,withawarningiftheiteration\nschemefails.\n7) Plot the wave function and potential on the same graph (you will have to scale one\nordinate).\n8) Deduce,bycountingthenumberofnodesinthewavefunction,whetherthesolution\nfound is a ground state (no nodes) or an excited state (with nodes) and whether the\nsolutionisevenoroddabouttheorigin(thegroundstatemustbeeven).\n9) IncludeinyourversionofFigure13.1ahorizontallinewithinthepotentialindicating\ntheenergyofthegroundstaterelativetothepotential‚Äôsdepth.\n10) Increasethevalueoftheinitialenergyguessandsearchforexcitedstates.Makesureto\nexaminethewavefunctionforeachstatefoundtoestablishthatitiscontinuous,and\ntocountthenumberofnodestoseeifyouhavemissedanystates.\n11) Addeachnewstatefoundasanotherhorizontalbarwithinthepotential.\n13.2.4 Explorations\n1) Check to see how well your search procedure works by using arbitrary values for the\nstartingenergy.Forexample,becausenobound-stateenergiescanliebelowthebottom\nofthewell,try E‚â•‚àíV0,aswellassomearbitraryfractionsof V0.Ineverycaseexam-\nine the resulting ground state wave function and check that it is both symmetric and\ncontinuous.\n2) Increasethedepthofyourpotentialprogressivelyuntilyoufindseveralboundstates.\nLookatthewavefunctionineachcase,andcorrelatethenumberofnodesinthewave\nfunctionwiththepositionoftheboundstateinthewell.\n3) Explorehowabound-stateenergychangesasyouchangethedepth V0ofthewell.In\nparticular,asyoukeepdecreasingthedepth,watchtheeigenenergymovecloserto E=0\nandseeifyoucanfindthepotentialdepthatwhichtheboundstatehas E‚âÉ0.\n4) Forafixedwelldepth V0,explorehowtheenergyofaboundstatechangesasthewell\nradiusaisvaried.Largerradiusshouldgiveincreasedbinding.\n5) Solveforthewavefunctionofalinearpotential:\nV(x)=‚àíV0{|x|,for|x|<a,\n0,for|x|>a.(13.17)\nThereislesspotentialherethanforasquarewell,soyoumayexpectsmallerbinding\nenergies and a less confined wave function. (For this potential, there are no analytic\nresultswithwhichtocompare.)\n6) Comparetheresultsobtained,andthetimethecomputertooktogetthem,usingboth\ntheNumerovand rk4methods.\n7)Newton‚ÄìRaphson extension: Extend the eigenvalue search by using the Newton‚Äì\nRaphsonmethodinplaceofthebisectionalgorithm.Determinethespeedup.\n13.3 Classical Chaotic Scattering\nOnemightexpectthattheclassicalscatteringofaprojectilefromapassivetargetwillvary\nsmoothly.Yetexperiments(Figure13.3left)havefoundthatwhenaprojectileundergoes\nmultipleinternalscatterings,itsfinaltrajectoryappearsunrelatedtoitsinitialone.\n13.3 Classical Chaotic Scattering 297\nvV(x,y )\nxy\nbv'\nŒ∏\nFigure 13.3 Left: A classic pinball machine in which the bumpers lead to multiple scatterings. A\npotential model that may support multiple internal scatterings. The incident velocity ùë£is in the y\ndirection at an impact parameter b. After scattering, the particle moves off at an angle ùúÉ.\nProblem Determineifmultipleinternalscatteringsmayleadtosuchachaoticsituation.\n(ChaosisdiscussedmorefullyinChapter15.)\n13.3.1 Model and Theory\nOurmodelforscatteringfromthebumpersinpinballmachinesisapointparticlescattering\nfromthestationary2Dpotential[Bleher etal.,1990]\nV(x,y)=¬±x2y2e‚àí(x2+y2). (13.18)\nAsseenontherightofFigure13.3,thispotentialhasfourcircularlysymmetricpeaksin\nthexyplane.Theminussignin(13.18)(whichwewilldropfromnowon)wouldproduce\nattractive potential wells. Due to there being four peaks, it seems possible to have multi-\nplescatteringsinwhichtheprojectilebouncesbackandforthbetweenthepeaks,likeina\npinballmachine.\nThetheoryforthisproblemisclassicaldynamics.Visualizeascatteringexperimentin\nwhichaprojectilestartsoutat (x=b,y=‚àí ‚àû )withvelocity v(Figure13.3).Thedistance\nbiscalledthe impactparameter .Afterscatteringandmovingoutto y=+ ‚àû,theprojectile\nisobservedatthescatteringangle ùúÉ.Becauseafixedpotentialdoesnotrecoilandcarryoff\nenergy,thespeedoftheprojectiledoesnotchange,onlyitsdirection.\nAn experiment would measure the number of particles scattered at each scattering\nangleùúÉ. The analysis would convertthe measurements into the differential cross section\nùúé(ùúÉ):\nùúé(ùúÉ)=lim\nŒîŒ©,ŒîA‚Üí0Nscatt(ùúÉ)‚àïŒîŒ©\nNin‚àïŒîAin. (13.19)\nHereNscatt(ùúÉ)isthenumberofparticlesperunitoftimescatteredintothedetectoratangle\nùúÉthatsubtendsasolidangle ŒîŒ©,andNinisthenumberofparticlesperunitoftimeincident\nonthetargetofcross-sectionalarea ŒîAin.\nThedefinition(13.19)forthecrosssectionistheoneusedbyexperimentalists.Weneed\nnot worry about applying it. Instead, we need to solve for the trajectory [x(t),y(t)]of the\nprojectile scattering from the potential (13.18), and from that deduce the dependence of\n298 13 ODE Applications; Eigenvalues, Scattering, Trajectories\nscatteringangle ùúÉ(b)ontheimpactparameter b.Oncewehavethatwecancalculatethe\ndifferentialcrosssection[MarionandThornton,2019]:\nùúé(ùúÉ)=||||dùúÉ\ndb||||b\nsinùúÉ(b). (13.20)\nAsyourcomputationshouldshow,thereareparametervaluesforwhich dùúÉ‚àïdbgetsvery\nlarge,orevendiscontinuous,andthisleadstochaoticcrosssections.\nWeneedtosolveNewton‚Äôslawfor [x(t),y(t)]inthepotential(13.18):\nF=ma\n‚àíùúïV\nùúïxÃÇi‚àíùúïV\nùúïyÃÇj=md2x\ndt2, (13.21)\n‚áí‚àí2y2x(1‚àíx2)e‚àí(x2+y2)=md2x\ndt2, (13.22)\n‚àí2x2y(1‚àíy2)e‚àí(x2+y2)=md2y\ndt2. (13.23)\nThepeaksofthepotentialinFigure13.3rightareat x=¬±1andy=¬±1,whereVmax=e‚àí2.\nThissetstheenergyscalefortheproblem.\n13.3.2 Implementation\nAlthough (13.22) and (13.23) are simultaneous second-order ODEs, we can still use our\nstandard rk4ODEsolverandformalismbyextendingthemfromtwotofourdimensions:\ndy(t)\ndt=f(t,y), (13.24)\ny(0)def=x(t),y(1)def=y(t),y(2)def=dx\ndt,y(3)def=dy\ndt. (13.25)\n(Theorderinwhichthe y(i)sareassignedisarbitrary.)Whenappliedto(13.22)‚Äì(13.23),and\nexpressedintermsofthe y(i)‚Äôs,wehave:\nf(0)=y(2), f(1)=y(3), (13.26)\nf(2)=‚àí1\nm2y2x(1‚àíx2)e‚àí[x2+y2](13.27)\n=‚àí1\nm2y(1)2y(0)(1‚àíy(0)2)e‚àí[y(0)2+y(1)2], (13.28)\nf(3)=‚àí1\nm2x2y(1‚àíy2)e‚àí[x2+y2](13.29)\n=‚àí1\nm2y(0)2y(1)(1‚àíy(1)2)e‚àí[y(0)2+y(1)2]. (13.30)\nTo deduce the scattering angle from our calculation, we examine the trajectory of the\nprojectileat y‚âÉ‚àû,whichwetaketobethe yvalueforwhichthepotentialhasessentially\nvanished, |PE|‚àïE‚â§10‚àí10.Thescatteringangleisdeducedfromthecomponentsofvelocity,\nùúÉ=tan‚àí1(ùë£y\nùë£x)\n=math.atan2(y, x) . (13.31)\nHere atan2isafunctionthatcomputesthearctangentinthecorrectquadrant,withouta\ndivisionby ùë£xwhichcancauseanoverflow.",6344
142-13.5 2 and 3Body Planetary Orbits.pdf,142-13.5 2 and 3Body Planetary Orbits,"13.4 Projectile Motion with Drag 299\n13.3.3 Assessment\n1) Applythe rk4methodtosolvethesimultaneousODEs(13.22)and(13.23).\n2) The initial conditions are(x=b,y=y‚àû),where |PE(y‚àû)|‚àïE‚â§10‚àí10.\n3) Goodstartingparametersare m=0.5,ùë£y(0)=0.5,ùë£x(0)=0.0,Œîb=0.05,‚àí1‚â§b‚â§1.\nYoumaywanttolowertheenergyanduseafinerstepsizeonceyouhavefoundregions\nofrapidvariationinthecrosssection.\n4) Plotanumberoftrajectories [x(t),y(t)]thatshowusualandunusualbehaviors.Inpar-\nticular,plotthoseforwhichbackanglescatteringoccurs,and,consequently,forwhich\ntheremusthavebeensignificantmultiplescatterings.\n5) Plotanumberofphasespacetrajectories [x(t), Ãáx(t)]and[y(t), Ãáy(t)].Howdothesediffer\nfromthoseofboundstates?\n6) Determinethescatteringangle ùúÉ=atan2(Vx,Vy) bydeterminingthevelocitycompo-\nnents of the scattered particle after it has left the interaction region, that is, when\nPE‚àïE‚â§10‚àí10.\n7) Identifywhichcharacteristicsofatrajectoryleadtodiscontinuitiesin dùúÉ‚àïdbandthus\nùúé(ùúÉ).\n8) Runthesimulationsforbothattractiveandrepulsivepotentials,andforarangeofener-\ngieslessthanandgreaterthan Vmax=exp(‚àí2).\n9)Time delay: Anotherwaytofindunusualbehaviorinscatteringistocomputethe time\ndelayT(b)asafunctionoftheimpactparameter b.Thetimedelayistheincreaseinthe\ntimeittakesaparticletotravelthroughtheinteractionregionduetoitsinteractions.\nLookforhighlyoscillatoryregionsinthesemilogplotof T(b),andonceyoufindsome,\nrepeatthesimulationatafinerscalebysetting b‚âÉb‚àï10(thestructuresarefractals,as\ndiscussedinChapter14).\n10) OK,nowgobackanddothisallagain,butwithanattractivepotentialthatmaynot\nwanttolettheprojectilegofree!\n13.4 Projectile Motion with Drag\nGolfandbaseballplayersclaimthatballsappeartofalloutoftheskyattheendoftheirtra-\njectories(sortoflikethesolidcurveinFigure13.4,whichwascomputedwiththeprogram\nProjectileAir.py inListing13.3).\nYour problemis to determine whether there is a physics explanation for this effect, or\nwhetheritis‚Äúallinthemind‚Äôseye.‚Äù\nFigure13.4showstheinitialvelocity V0andinclination ùúÉforaprojectilelaunchedfromthe\norigin.Ifweignoreairresistance,theprojectilehasonlytheforceofgravityactingonit,a\nFigure 13.4 The trajectories of a projectile Ô¨Åred\nwith initial velocity V0in theùúÉdirection. The lower\ncurve includes air resistance.V0Œ∏y\nRH\nxWith drag0\n0\n300 13 ODE Applications; Eigenvalues, Scattering, Trajectories\nconstantacceleration ay=‚àíg=‚àí9.8m/s2,andthefamiliaranalyticsolutions:\nx(t)=V0cosùúÉt,y(t)=V0sinùúÉt‚àí1\n2gt2, (13.32)\nùë£x(t)=V0x,ùë£y(t)=V0y‚àígt, (13.33)\ny(x)=V0y\nV0xx‚àíg\n2V2\n0x. (13.34)\nLikewise,itiseasytoshowthattherange R=2V2\n0sinùúÉcosùúÉ‚àïgandthemaximumheight\nH=1\n2V2\n0sin2ùúÉ‚àïg.\nTheparabola(13.34)forfrictionlessmotionissymmetricaboutitsmidpoint,whichdoes\nnotresembleaballfallingoutofthesky.Toseeifairresistancewillchangethat,weinclude\nafrictionalforce F(f)inNewton‚Äôssecondlaw:\nF(f)‚àímgÃÇey=md2x(t)\ndt2, (13.35)\n‚áíF(f)\nx=md2x\ndt2,F(f)\ny‚àímg=md2y\ndt2. (13.36)\nAsamodelforwhatisreallymorecomplicated,weassumethatthefrictionalforceispro-\nportionaltosomepower noftheprojectile‚Äôsspeed[MarionandThornton,2019]:\nF(f)=‚àíkm|ùë£|nv\n|ùë£|, (13.37)\nwherethe ‚àív‚àï|ùë£|factorensuresthatthefrictionalforceisalwaysinadirectionopposite\nthat of the velocity. Experiments indicate that the power nis noninteger and varies with\nvelocity.Theequationsofmotionarethus\nd2x\ndt2=‚àíkùë£n\nxùë£x\n|ùë£|,d2y\ndt2=‚àíg‚àíkùë£n\nyùë£y\n|ùë£|,|ùë£|=‚àö\nùë£2\nx+ùë£2\ny. (13.38)\nOrindynamicalform:\ndy(0)\ndt=y(1),dy(1)\ndt=1\nmF(f)\nx(y) (13.39)\ndy(2)\ndt=y(3),dy(3)\ndt=1\nmF(f)\ny(y)‚àíg, (13.40)\nf(0)=y(1),f(1)=1\nmF(f)\nx,f(2)=y(3),f(3)=1\nmF(f)\ny‚àíg.\nConsider three values for n, each of which represents a different model for the air resis-\ntance: (i)n=1 for low velocities; (ii) n=3‚àï2, for medium velocities; and (iii) n=2f o r\nhighvelocities.\n13.4.1 Assessment\n1) Modifyyour rk4programsothatitsolvesthesimultaneousODEsforprojectilemotion\n(13.38)withfriction( n=1).\n2) CheckthatyouobtaingraphssimilartothoseinFigure13.4.\n3) Use(13.37)with n=1forlowvelocities, n=3‚àï2,formedium-velocities,and n=2for\nhigh-velocities.Adjustthevalueof kforthelattertwocasessuchthattheinitialforceof\nfrictionkùë£n\n0isthesameforallthreecases.\n4) Whatisyourconclusionaboutballsfallingoutofthesky?",4171
143-13.6 Code Listings.pdf,143-13.6 Code Listings,"13.5 2- and 3-Body Planetary Orbits 301\n13.5 2- and 3-Body Planetary Orbits\n13.5.1 Planets via Two of Newton‚Äôs Laws\nNewton‚Äôsexplanationofthemotionoftheplanetsintermsofauniversallawofgravitation\nisoneofthegreatestachievementsofscience.Hewasabletoprovethatplanetstraveled\ninellipticalorbitswiththesunatonevertex,andthengoontopredicttheperiodsofthe\nmotions.AllNewtonneededtodowasinventcalculusandpostulatethattheforcebetween\naplanetofmass mandthesunofmass Mis\nFg=‚àíGmM\nr2. (13.41)\nHereris the planet-sun CM distance, Gis the universal gravitational constant, and the\nattractive force lies along the line connecting the planet and the sun (Figure 13.5 left).\nThehardpartforNewtonwassolvingtheresultingdifferentialequations.Incontrast,the\nnumericalsolutionisstraightforward.Evenforplanets,theequationofmotionisstill\nF=ma=md2x\ndt2. (13.42)\nInCartesiancomponents(Figure13.5):\nFx=FgcosùúÉ=Fgx\nr=Fgx‚àö\nx2+y2, (13.43)\nFy=FgsinùúÉ=Fgy\nr=Fgy‚àö\nx2+y2. (13.44)\nTheequationofmotion(13.42)isthustwosimultaneoussecond-orderODEs:\nd2x\ndt2=‚àíGMx\n(x2+y2)3‚àï2,d2y\ndt2=‚àíGMy\n(x2+y2)3‚àï2. (13.45)\n1) Assumeunitssuchthat GM=1andtheinitialconditions\nx(0)=0.5,y(0)=0,ùë£x(0)=0.0,ùë£y(0)=1.63. (13.46)\n2) ModifyyourODEsolverprogramtosolve(13.45).\n3) Makesuretousesmallenoughtimestepstoachievehighprecision.Thenyoushould\nfindthattheorbitsareclosedandfalluponthemselves.\n4) Experimentwiththeinitialconditionsuntilyoufindtheonesthatproduceacircular\norbit(aspecialcaseofanellipse).\n5) Note the effect of progressively increasing the initial velocity until the orbits open up\nandtheplanetsbecomeunbound.\ny fy(x,y)fx\nf r\nxŒ∏Planet motion\nFigure 13.5 Left: The components of the gravitational force on a planet at a distance rfrom the\nsun. Right: The precession of a planet‚Äôs orbit for a gravitational force ‚àù1‚àïr4.\n302 13 ODE Applications; Eigenvalues, Scattering, Trajectories\n6) For the same initial conditions that produced the ellipse, investigate the effect of the\npowerin(13.41)being1 ‚àïr2+ùõºwithùõº‚â†0.Evenforsmallvaluesfor ùõº,youshouldfind\nthattheellipsesnowrotateorprecess(Figure13.5).(Asmallvaluefor ùõºispredictedby\ngeneralrelativity(Chapter19).)\n13.5.2 The Discovery of Neptune\nTheplanetUranuswasdiscoveredin1781byWilliamHerschelandfoundtohaveanorbital\nperiodofapproximately84years.Andyet,by1846,whenUranushadnotevencompleted\nafullorbitaroundthesun,somethingseemedwrong.ThiscouldbeexplainedifUranus\nwas being perturbed by a yet-to-be-discovered planet lying about 50% further away from\nthesunthanUranus.TheplanetNeptunewasthusdiscoveredtheoreticallyandconfirmed\nexperimentally.(IfPlutoisdiscardedasjustadwarfplanet,thenNeptuneisthemostdistant\nplanetinthesolarsystem.)\nAssume that the orbitsof Neptune and Uranus are circular and coplanar,and that the\ninitialangularpositionswithrespecttothe x-axisareasgiveninthistable:\nMass Distance Orbitalperiod Angularposition\n(√ó10‚àí5Solarmasses) (AU) (Years) (in1690)\nUranus 4.366244 19.1914 84.0110 ‚àº205.640\nNeptune 5.151389 30.0611 164.7901 ‚àº288.380\nUsethesedataandrk4tofindthevariationinangularpositionofUranuswithrespecttothe\nSunasaresultoftheinfluenceofNeptuneduringonecompleteorbitofNeptune.Consider\nFigure 13.6 A snapshot from the animated output of the code UranusNeptune.py showing: Left:\nThe orbits of Uranus (inner circle) and of Neptune (outer circle) with the sun in the center. The\narrows indicate the Uranus‚ÄìNeptune force that causes a perturbation in the orbits. Right:T h e\nperturbation in the angular position of Uranus as a function of time resulting from the presence of\nNeptune.\n13.6 Code Listings 303\nonlytheforcesoftheSunandNeptuneonUranus.Intheastronomicalunits, Ms=1and\nG=4ùúã2.Figure13.6showstheoutputofourprogramthatusedtheseconstants:\nG=4 ‚àópi‚àópi # AU, Msun=1\nmu = 4.366244e ‚àí5 # Uranus mass\n3M=1.0 #S u nm a s s\nmn = 5.151389e ‚àí5 # Neptune mass\ndu = 19.1914 # Uranus Sun distance\ndn = 30.0611 # Neptune sun distance\n7Tur = 84.0110 # Uranus Period\nTnp = 164.7901 # Neptune Period\nomeur = 2 ‚àópi/Tur # Uranus angular velocity\nomennp = 2 ‚àópi/Tnp # Neptune angular velocity\n11omreal = omeur\nurvel = 2 ‚àópi‚àódu/Tur # Uranus orbital velocity U A /yr\nnpvel = 2 ‚àópi‚àódn/Tnp # Neptune orbital velocity UA/ yr\nradur = (205.64) ‚àópi/180. # in radians\n15urx = du ‚àócos(radur) # init x Uranus in 1690\nury = du ‚àósin(radur) # init y Uranus in 1690\nurvelx = urvel ‚àósin(radur)\nurvely = ‚àíurvel ‚àócos(radur)\n19radnp = (288.38) ‚àópi/180. # Neptune angular pos .\n13.6 Code Listings\nListing 13.1 QuantumNumerov.py Solvesthetime-independentSchr√∂dingerequation\nforbound-stateenergiesusingaNumerovmethod.\n1# QuantumNumerov . py : Solve quantum bound s t a t e via Numerov algorithm\n# hbarc ‚àóomega=hbarc ‚àósqrt(k/ m )=19.733, r m c ‚àó‚àó2=940 MeV, k=9.4\n# E =( N+1/2)hbarc ‚àóo m e g a = (N +1/2)19.733, N=0,2,4, change if N odd\n5fromnumpyimport ‚àó\nimportnumpy as np, matplotlib.pyplot as plt\nn = 1000; m= 2; imax = 100; Xleft0 = ‚àí10; Xright0 = 10; h = 0.02\n9amin= 81.; amax = 92.; e = amin; de = 0.01; eps= 1e ‚àí4; im = 500\nnl = im + 2; nr = n ‚àíim + 2; xmax = 5.0\nprint(""nl, nr"" ,nl, nr)\nprint(h)\n13xLeft = arange( ‚àí10,0.02,0.02); xRight = arange(10,0.02, ‚àí0.02)\nxp = arange( ‚àí10,10,0.02) # Bisection interval\nuL = zeros((503), float); uR = zeros([503], float)\nk2L = zeros([1000], float); k2R = zeros([1000], float)\n17uL[0] = 0; uL[1] =0.00001; uR[0] = 0; uR[1] = 0.00001\ndefV(x): # Potential harmonic oscillator\nv=4 . 7 ‚àóx‚àóx\n21returnv\ndefsetk2(e): # Set k2L=(sqrt(e ‚àíV) ) ^2 , k2R\nforiin range (0,n):\nxLeft = Xleft0 + i ‚àóh\n25 xr = Xright0 ‚àíi‚àóh\nfact=0.04829 #2 m ‚àóc‚àó‚àó2/hbarc ‚àó‚àó2\nk2L[i] = fact ‚àó(e‚àíV(xLeft))\nk2R[i] = fact ‚àó(e‚àíV(xr))\n29defNumerov (n,h,k2,u,e):\nsetk2(e)\nb=(h ‚àó‚àó2)/12.0 #L&R wave functions\nforiin range (1,n):\n33 u[i+1]=(2 ‚àóu[i] ‚àó(1‚àí5.‚àób‚àók2[i])‚àí(1+b ‚àók2[i‚àí1])‚àóu[i‚àí1])/(1+b ‚àók2[i+1])\ndefdiff(e):\nNumerov (nl ,h,k2L,uL,e) #L e f tw f\nNumerov (nr,h,k2R,uR,e) # Right wf\n304 13 ODE Applications; Eigenvalues, Scattering, Trajectories\n37f0 = (uR[nr ‚àí1] + uL[nl ‚àí1]‚àíuR[nr‚àí3]‚àíuL[nl‚àí3])/(h ‚àóuR[nr‚àí2])\nreturnf0\nistep = 0\n41x1 = arange( ‚àí10,.02,0.02); x2 = arange(10, ‚àí0.02,‚àí0.02)\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.grid()\n45while abs (diff(e)) > eps : # Bisection algorithm\ne =(amin + amax)/2\nprint(e,istep)\nifdiff(e) ‚àódiff(amax) > 0: amax = e\n49else:a m i n=e\nax.clear()\nplt.text(3, ‚àí200, ‚ÄôEnergy= %10.4f‚Äô %(e),fontsize=14)\nplt.plot(x1,uL[: ‚àí2])\n53plt.plot(x2,uR[: ‚àí2])\nplt.xlabel( ‚Äôx‚Äô)\nplt.ylabel( ‚Äôùúì(x)‚Äô,fontsize=18)\nplt.title( ‚ÄôR&L Wavefunctions Matched at x = 0‚Äô )\n57istep = istep+1\nplt.pause(0.8) # Pause to delay figures\nplt .show()\nListing 13.2 QuantumEigen.py Solvesthetime-independentSchr√∂dingerequationfor\nbound-stateenergiesusingtherk4algorithm.\n# QuantumEigen.py: Finds E and psi via rk4 + bisection\n3#m / ( h b a r ‚àóc)‚àó‚àó2= 940 M e V/(197.33M e V ‚àífm)‚àó‚àó2 =0.4829, well width=20 fm\n# well depth 10 MeV, Wave function not normalized\nfromvisualimport ‚àó\n7\npsigr = display(x=0,y=0,width=600,height=300, title= ‚ÄôR&L Wavefunc‚Äô )\nLwf = curve(x= list(range(502)),color=color.red)\nRwf = curve(x= list(range(997)),color=color.yellow)\n11eps = 1E ‚àí3 # Precision\nn_steps = 501\nE= ‚àí17.0 # E guess\nh = 0.04\n15count_max = 100\nEmax = 1.1 ‚àóE # E limits\nEmin = E/1.1\n19deff(x, y, F,E):\nF[0] = y[1]\nF[1] = ‚àí(0.4829) ‚àó(E‚àíV(x)) ‚àóy[0]\ndefV(x):\n23if(abs(x)<10.):return(‚àí16.0) # Well depth\nelse: return(0.)\ndefrk4(t, y,h,Neqs,E):\nF= z e r o s ( ( N e q s ) , float)\n27ydumb = zeros((Neqs) , float)\nk1 = zeros((Neqs) , float)\nk2 = zeros((Neqs) , float)\nk3 = zeros((Neqs) , float)\n31k4 = zeros((Neqs) , float)\nf(t, y, F,E)\nforiin range (0,Neqs):\nk1[i] = h ‚àóF[i]\n35 ydumb[i] = y[i] + k1[i]/2.\nf(t +h/2., ydumb, F,E)\nforiin range (0,Neqs):\nk2[i] = h ‚àóF[i]\n39 ydumb[i] = y[i] + k2[i]/2.\nf(t +h/2., ydumb, F,E)\nforiin range (0,Neqs):\n13.6 Code Listings 305\nk3[i]= h ‚àóF[i]\n43 ydumb[i] = y[i] + k3[i]\nf(t +h, ydumb, F,E);\nforiin range (0,Neqs):\nk4[i]=h ‚àóF[i]\n47 y[i]=y[i]+(k1[i]+2 ‚àó(k2[i]+k3[i])+k4[i])/6.0\ndefdiff(E, h):\ny=z e r o s( ( 2 ), float)\ni_match = n_steps//3 # Matching radius\n51nL = i_match + 1\ny [ 0 ]=1 . E ‚àí15; # Initial left w f\ny[1] = y[0] ‚àósqrt(‚àíE‚àó0.4829)\nforixin range (0,nL + 1):\n55 x=h ‚àó(ix‚àín_steps/2)\nrk4(x, y, h, 2, E)\nleft = y[1]/y[0] # Log derivative\ny [ 0 ]=1 . E ‚àí15; # slope for even; reverse for odd\n59y[1] = ‚àíy[0] ‚àósqrt(‚àíE‚àó0.4829) # Initialize R w f\nforixin range (n_steps,nL+1, ‚àí1):\nx=h ‚àó(ix+1‚àín_steps/2)\nrk4(x, y, ‚àíh, 2, E)\n63right = y[1]/y[0] # Log derivative\nreturn(( l e f t ‚àíright)/(left + right) )\ndefplot(E, h): # Repeat integrations for plot\nx=0 .\n67n_steps = 1501 # # integration steps\ny=z e r o s( ( 2 ), float)\nyL = zeros((2,505), float)\ni_match = 500 # Matching point\n71nL = i_match + 1;\ny [ 0 ]=1 . E ‚àí40 # Initial left w f\ny[1] = ‚àísqrt(‚àíE‚àó0.4829) ‚àóy[0]\nforixin range (0,nL+1):\n75 yL[0][ix] = y[0]\nyL[1][ix] = y[1]\nx=h ‚àó(ix‚àín_steps/2)\nrk4(x, y, h, 2, E)\n79y[0] = ‚àí1.E‚àí15 #‚àíslope : even ; reverse for odd\ny[1] = ‚àísqrt(‚àíE‚àó0.4829) ‚àóy[0]\nj=0\nforixin range (n_steps ‚àí1,nL + 2, ‚àí1): # right wave function\n83 x=h ‚àó(ix + 1 ‚àín_steps/2) # Integrate in\nrk4(x, y, ‚àíh, 2, E)\nRwf.x[j] = 2. ‚àó(ix +1 ‚àín_steps/2) ‚àí500.0\nRwf.y[j] = y[0] ‚àó35e‚àí9 +200\n87 j +=1\nx=x‚àíh\nnormL = y[0]/yL[0][nL]\nj=0\n91 # Renormalize L wf &derivative\nforixin range (0,nL+1):\nx=h ‚àó(ix‚àín_steps/2 + 1)\ny[0] = yL[0][ix] ‚àónormL\n95 y[1] = yL[1][ix] ‚àónormL\nLwf.x[j] = 2. ‚àó(ix‚àín_steps/2+1) ‚àí500.0\nLwf.y[j] = y[0] ‚àó35e‚àí9+200 #F a c t o rf o rs c a l e\nj +=1\n99forcountin range (0,count_max+1):\nrate(1) # Slow rate to show changes\n# Iteration loop\nE=( E m a x+E m i n )/ 2 . #D i v i d eEr a n g e\n103Diff = diff(E, h)\nif(diff(Emax, h) ‚àóDiff > 0): Emax = E # Bisection algorithm\nelse:E m i n = E\nif(abs(Diff) <eps ): break\n107ifcount >3: # First iterates too irregular\nrate(4)\nplot(E, h)\nelabel = label(pos=(700, 400), text= ‚ÄôE=‚Äô,b o x = 0 )\n111elabel.text = ‚ÄôE=%13.10f‚Äô %E\nilabel = label(pos=(700, 600), text= ‚Äôistep=‚Äô ,b o x = 0 )\n306 13 ODE Applications; Eigenvalues, Scattering, Trajectories\nilabel.text = ‚Äôistep=%4s‚Äô %count\nelabel = label(pos=(700, 400), text= ‚ÄôE=‚Äô,b o x = 0 ) # Last iteration\n115elabel.text = ‚ÄôE=%13.10f‚Äô %E\nilabel = label(pos=(700, 600), text= ‚Äôistep=‚Äô ,b o x = 0 )\nilabel.text = ‚Äôistep=%4s‚Äô %count\nprint(""Final eigenvalue E = "" ,E)\n119print(""iterations, max = "" ,count)\nListing 13.3 ProjectileAir.py Solvesforprojectilemotionwithairresistanceaswellas\nanalyticallyforthefrictionlesscase.\n# ProjectileAir .py: Order dt^2 projectile trajectory + drag\n3fromvisualimport ‚àó\nfromvisual.graph import ‚àó\nv0 = 22.; angle = 34.; g = 9.8; kf = 0.8; N= 5\n7v0x = v0 ‚àócos(angle ‚àópi/180); v0y = v0 ‚àósin(angle ‚àópi/180)\nT=2 ‚àóv0y/g; H = v0y ‚àóv0y/2/g; R = 2 ‚àóv0x‚àóv0y/g\ngraph1 = gdisplay(title= ‚ÄôProjectile with &without Drag‚Äô ,\nxtitle= ‚Äôx‚Äô, ytitle= ‚Äôy‚Äô, xmax=R, xmin= ‚àíR/20.,ymax=8,ymin= ‚àí6.0)\n11funct = gcurve(color=color.red)\nfunct1 = gcurve(color=color.yellow)\nprint(‚ÄôNo Drag T =‚Äô ,T,‚Äô, H =‚Äô,H,‚Äô, R =‚Äô,R)\n15defplotNumeric(k):\nvx = v0 ‚àócos(angle ‚àópi/180.)\nvy = v0 ‚àósin(angle ‚àópi/180.)\nx=0 . 0\n19y=0 . 0\ndt = vy/g/N/2.\nprint(""\n With Friction "" )\nprint(""x y "" )\n23foriin range (N):\nrate(30)\nvx = vx ‚àík‚àóvx‚àódt\nvy = vy ‚àíg‚àódt‚àík‚àóvy‚àódt\n27x=x+v x ‚àódt\ny=y+v y ‚àódt\nfunct.plot(pos=(x,y))\nprint("" %13.10f %13.10f "" %(x,y))\n31defplotAnalytic():\nv0x = v0 ‚àócos(angle ‚àópi/180.)\nv0y = v0 ‚àósin(angle ‚àópi/180.)\ndt = 2. ‚àóv0y/g/N\n35print(""\n No Friction "" )\nprint(""x y "" )\nforiin range (N):\nrate(30)\n39 t=i ‚àódt\nx=v 0 x ‚àót\ny=v 0 y ‚àót‚àíg‚àót‚àót/2.\nfunct1.plot(pos=(x,y))\n43 print("" %13.10f %13.10f"" %(x ,y))\nplotNumeric(kf)\nplotAnalytic()",11592
144-14.2 Growing Plants.pdf,144-14.2 Growing Plants,"307\n14\nFractals and Statistical Growth Models\nIn this chapter we implement models that create fractals. We emphasize the simple\nunderlying rules, the statistical aspects of the rules, and the meaning of self-similarity.\nTo the extent that these models generate structures that look like those in nature, it is\nreasonable to assume that the natural processes may be following similar rules arising\nfrom some basic physics or biology .\nItiscommontonoticeregularandeye-pleasingnaturalobjects,suchasplantsandsea\nshells,thatdonothavewell-definedgeometricpatterns.Whenanalyzedmathematically,\nsomeofthesepatternshaveadimensionthatisafractionalnumber.BenoitMandelbrot,\nwhofirststudiedfractional-dimensionfigureswithsupercomputersatIBMResearch,gave\nthemthename fractals[Mandelbrot,1982].Somegeometricobjects,suchasKochcurves,\nareexactfractalswiththesamedimensionforalltheirparts.Otherobjects,suchasbifur-\ncation curves of Chapter 15, are statistical fractals in which elements of randomness are\nmixedin,inwhichcasetheremaybedifferentdimensionsforeachpartoftheobject.\nConsideranabstractobjectsuchasthedensityofchargewithinanatom.Therearean\ninfinitenumberofwaystodefinethe‚Äúsize‚Äùofthisobject.Forexample,eachmoment ‚ü®rn‚ü©\nisameasureofthesize,andthereisaninfinitenumberofmoments.Likewise,whenwe\ndealwithcomplicatedobjects,therearedifferentdefinitionsofdimension,andeachmay\ngiveasomewhatdifferentvalue.\nTheHausdorff‚ÄìBesicovitch dimension df, is based on our knowledge that a line has\ndimension 1, a trianglehas dimension 2, anda cube has dimension 3. It seems perfectly\nreasonable, then, to take a mathematical formula that agrees with our experience with\nregularobjects,andapplyittoirregularobjects.Forsimplicity,letusconsiderobjectsthat\nhave the same length Lon each side, as do equilateral triangles and squares, and that\nhaveuniformdensity.Wepostulatethatthedimensionofanobjectisdeterminedbythe\ndependenceofitstotalmassuponitslength:\nM(L)‚àùLdf, (14.1)\nwherethepower dfisthefractaldimension .Asyoumayverify,thisruleworksforthe1D,\n2D,and3Dfigureswearefamiliarwith,soitisareasonabletotryitelsewhere.When(14.1)\nisappliedtoirregularobjects,weendupwithfractionalvaluesfor df.Actually,wewillfind\niteasiertodeterminethefractaldimension,notfromanobject‚Äôsmass,whichis extensive\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n308 14 Fractals and Statistical Growth Models\n(dependsonsize),butratherfromitsdensity,whichis intensive.Thedensityisdefinedas\nmass/lengthforalinearobject,mass/areaforaplanarobject,andmass/volumeforasolid\nobject.Thatbeingthecase,foraplanarobjectwehypothesizethat\nùúå=M(L)\narea‚àùLdf\nL2‚àùLdf‚àí2. (14.2)\n14.1 The Sierpi¬¥ nski Gasket\nTogenerateourfirstfractal,showninFigure14.1,weplayagameofchanceinwhichwe\nplacedotsatpointsplacedrandomlywithinatriangle[BundeandHavlin,1991].Hereare\ntherules(whichyoushouldtryoutinthemarginsrightnow).\n1) Drawanequilateraltrianglewithverticesandcoordinates:\nvertex1: (a1,b1);vertex2: (a2,b2);vertex3: (a3,b3).\n2) Placeadotatarandompoint P=(x0,y0)withinthistriangle.\n3) Findthenextpointbyselectingrandomlytheinteger1,2,or3:\na) If1,placeadothalfwaybetween Pandvertex1.\nb) If2,placeadothalfwaybetween Pandvertex2.\nc) If3,placeadothalfwaybetween Pandvertex3.\n4) Repeattheprocessusingthelastdotasthenew P.\nMathematically,thecoordinatesofsuccessivepointsaregivenbytheformulas\n(xk+1,yk+1)=(xk,yk)+(an,bn)\n2,n=integer(1+3ri), (14.3)\nwhereriisarandomnumberbetween0and1,andwherethe integerfunctionoutputsthe\nclosestintegersmallerthanorequaltotheargument.After15,000points,youshouldobtain\nacollectionofdotslikethoseontheleftinFigure14.1.\n0100200300\n0 100 200 30010,000 points\nA\nB\nC\nFigure 14.1 Left: The Sierpi¬¥ nski gasket, a statistical fractal containing 10,000 points. Note the\nself-similarity at different scales. Right: A geometric Sierpi¬¥ nski gasket constructed by successively\nconnecting the midpoints of the sides of each equilateral triangle. The Ô¨Årst three steps in the\nprocess are labeled as A, B, and C.\n14.1 The Sierpi¬¥ nski Gasket 309\nExercise Write a program to produce a Sierpi¬¥ nski gasket. Determine empirically the\nfractal dimension of your figure. Assume that each dot has mass 1 and that ùúå=CLùõº.\n(Youcanhavethecomputerdothecountingbydefininganarray boxofall0values,and\nthenbychanginga0toa1whenadotisplacedthere.)\n14.1.1 Measuring Fractal Dimension\nThe topology in Figure 14.1 was first analyzed by the Polish mathematician Sierpi¬¥ nski.\nObservethesamestructureoccursinasmallregionasoccursintheentirefigure.Inother\nwords,ifthefigurehadinfiniteresolution,anypartofthefigurecouldbescaledupinsize,\nandwouldbesimilartothewhole.Thispropertyiscalled self-similarity .\nWe construct a non-statistical form of the Sierpi¬¥ nski gasket by removing an inverted\nequilateral triangle from the center of all filled equilateral triangles (Figure 14.1 right).\nThiscreatesthenextfiguretoworkon.Werepeattheprocessadinfinitum,scalingupthe\ntrianglessoeachonehasside r=1aftereachstep.Toseewhatisunusualaboutthistype\nofobject,welookathowitsdensity(mass/area)changeswithsize,andthenapply(14.2)\ntodetermineitsfractaldimension.Assumethateachtrianglehasmass mandassignunit\ndensitytothesingletriangle:\nùúå(L=r)‚àùM\nr2=m\nr2def=ùúå0(Figure14.1A) . (14.4)\nNow,fortheequilateraltrianglewithside L=2,thedensityis\nùúå(L=2r)‚àù(M=3m)\n(2r)2=3\n4mr2=3\n4ùúå0(Figure14.1B) . (14.5)\nWe see that the extra white space in Figure 14.1B leads to a density that is3\n4that of the\npreviousstage.ForthestructureinFigure14.1C,weobtain\nùúå(L=4r)‚àù(M=9m)\n(4r)2=9\n16m\nr2=(3\n4)2\nùúå0.(Figure14.1C) . (14.6)\nWeseethataswecontinuetheconstructionprocess,thedensityofeachnewstructureis3\n4\nthatofthepreviousone.Interesting.Yetin(14.2)wederivedthat\nùúå‚àùCLdf‚àí2. (14.7)\nEquation(14.7)impliesthataplotofthelogarithmofthedensity ùúåversusthelogarithmof\nthelengthLforsuccessivestructuresyieldsastraightlineofslope\ndf‚àí2=Œîlogùúå\nŒîlogL. (14.8)\nAsappliedtoourproblem,\ndf=2+Œîlogùúå(L)\nŒîlogL=2+log1‚àílog3\n4\nlog1‚àílog2‚âÉ1.58496. (14.9)\nAsisevidentinFigure14.1,asthegasketgrowslarger(andconsequentlymoremassive),\nitcontainsmoreopenspace.Sodespitethefactthatitsmassapproachesinfinityas L‚Üí‚àû,\nitsdensityapproacheszero!BecausetheSierpi¬¥ nskigaskethasaslope df‚àí2‚âÉ‚àí0.41504,it\nfillsspacetoalesserextentthana2Dobject,butmorethana1Dobject;itisafractalwith\ndimensionof ‚àº1.6.",6420
145-14.2.3 SelfAffine Trees.pdf,145-14.2.3 SelfAffine Trees,"310 14 Fractals and Statistical Growth Models\n14.2 Growing Plants\nItseemsparadoxicalthatnaturalprocessessubjecttochancecanproduceobjectsofsuch\nhigh regularity, symmetry, and beauty. For example, it is hard to believe that something\nas graceful as a fern (Figure 14.2 left) has random elements in it. Nonetheless, there is\na clue here that much of the fern‚Äôs beauty arises from the similarity of each part to the\nwhole(self-similarity),withdifferentfernssimilar,butnotidenticaltoeachother.These\nareallcharacteristicsoffractals.Your problemistodiscoverifasimplealgorithmincluding\nsomerandomnesscandrawregularferns.Ifthealgorithmproducesobjectsthatresemble\nferns,then,presumably,youhaveuncoveredsomeunderlyingmathematicssimilartothose\nresponsiblefortheshapesofferns.\n14.2.1 Self-AfÔ¨Åne Connection\nIn(14.3),whichdefinesmathematicallyhowaSierpi¬¥ nskigasketisconstructed,a scaling\nfactorof1\n2ispartoftherelationofonepointtothenext.Amoregeneraltransformationof\napointP=(x,y)intoanotherpoint P‚Ä≤=(x‚Ä≤,y‚Ä≤)viascalingis\n(x‚Ä≤,y‚Ä≤)=s(x,y)=(sx,sy)(scaling). (14.10)\nIfthescalefactor s>0,anamplificationoccurs,whereasif s<0,areductionoccurs.Inour\ndefinition(14.3)oftheSierpi¬¥ nskigasket,wealsoaddedinaconstant an.Thisisatranslation\noperationofthegeneralform\n(x‚Ä≤,y‚Ä≤)=(x,y)+(ax,ay)(translation). (14.11)\nAnotheroperation,notusedintheSierpi¬¥ nskigasket,isa rotationbyangle ùúÉ:\nx‚Ä≤=xcosùúÉ‚àíysinùúÉ,y‚Ä≤=xsinùúÉ+ycosùúÉ(rotation). (14.12)\nThis entire set of transformations, scalings, rotations, and translations, defines an\naffine transformation (affine denotes a close relation between successive points). The\nFigure 14.2 Left: A fractal fern generated by 30,000 iterations of the algorithm (14.13). Enlarging\nthis fern shows that each frond has a similar structure. Right: A fractal tree created with the\nalgorithm (14.16).\n14.2 Growing Plants 311\ntransformation is still considered affine even if there are contractions and reflections.\nWhat is important is that the object created with these rules turns out to be self-similar;\neachstepleadstonewpartsoftheobjectthatbearthesamerelationtotheancestorparts\nastheancestorsdidtotheirs.Thisiswhatmakestheobjectlooksimilaratallscales.\n14.2.2 Barnsley‚Äôs Fern\nWeobtainaBarnsley‚Äôsfern[BarnsleyandHurd,1992]byextendingthedotsgametoone\nin which new points are selected using an affine connection, but with some elements of\nchancemixedin:\n(x,y)n+1=‚éß\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™‚é©(0.5,0.27yn), with2%probability ,\n(‚àí0.139xn+0.263yn+0.57\n0.246xn+0.224yn‚àí0.036),with15%probability ,\n(0.17xn‚àí0.215yn+0.408\n0.222xn+0.176yn+0.0893),with13%probability ,\n(0.781xn+0.034yn+0.1075\n‚àí0.032xn+0.739yn+0.27),with70%probability.(14.13)\nTo select a transformation with probability Óàº, we select a uniform random number\n0‚â§r‚â§1,andthenperformthetransformationif risinarangeproportionalto Óàº:\nÓàº=‚éß\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™‚é©2%,r<0.02,\n15%,0.02‚â§r‚â§0.17,\n13%,0.17<r‚â§0.3,\n70%,0.3<r<1.(14.14)\nTherules(14.13)and(14.14)canbecombinedintoone:\n(x,y)n+1=‚éß\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™‚é©(0.5,0.27yn), r<0.02,\n(‚àí0.139xn+0.263yn+0.57\n0.246xn+0.224yn‚àí0.036),0.02‚â§r‚â§0.17,\n(0.17xn‚àí0.215yn+0.408\n0.222xn+0.176yn+0.0893),0.17<r‚â§0.3,\n(0.781xn+0.034yn+0.1075,\n‚àí0.032xn+0.739yn+0.27),0.3<r<1.(14.15)\nAlthough(14.13)makesthebasicideaclearer(14.15),iseasiertoprogram.\nThestartingpointinBarnsley‚Äôsfern(Figure14.2)is (x1,y1)=(0.5,0.0),andthepointsare\ngeneratedbyrepeatediterations.Animportantpropertyofthisfernisthatitisnotcom-\npletely self-similar, as you can see by noting how different are the stems and the fronds.\nNevertheless, the stem can be viewed as a compressed copy of a frond, and the fractal\nobtainedwith(14.13)isstill self-affine,yetwithadimensionthatvariesfromparttopartof\nthefern.Ourcode Fern3D.pyisgiveninListing14.1.",3751
146-14.4.2 Coastline Exercise.pdf,146-14.4.2 Coastline Exercise,"312 14 Fractals and Statistical Growth Models\n14.2.3 Self-AfÔ¨Åne Trees\nNowthatyouknowhowtogrowferns,lookaroundandnoticetheregularityintrees(such\nas in Figure 14.2 right). Can it be that this also arises from a self-affine structure? Write\na program, similar to the one for the fern, starting at (x1,y1)=(0.5,0.0), and iterate the\nfollowingself-affinetransformation:\n(xn+1,yn+1)=‚éß\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™‚é©(0.05xn,0.6yn), 10%probability ,\n(0.05xn,‚àí0.5yn+1.0), 10%probability ,\n(0.46xn‚àí0.15yn,0.39xn+0.38yn+0.6),20%probability ,\n(0.47xn‚àí0.15yn,0.17xn+0.42yn+1.1),20%probability ,\n(0.43xn+0.28yn,‚àí0.25xn+0.45yn+1.0),20%probability ,\n(0.42xn+0.26yn,‚àí0.35xn+0.31yn+0.7),20%probability .(14.16)\n14.3 Ballistic Deposition\nThere are a number of natural and manufacturing processes in which particles are\ndepositedonasurfaceandformafilm.Iftheparticlesareevaporatedfromahotfilament,\nthere would be a randomness in the emission process, even though the produced films\nseem quite regular. Again we suspect fractals. Your problemis to develop a model that\nsimulatesthisgrowthprocess,andcompareyourproducedstructurestothoseobserved.\nTheideaofsimulatingrandomdepositionswasfirstreportedinVold[1959]intheirsimu-\nlationofthesedimentationofmoistspheresinhydrocarbons.WeshallexamineFamilyand\nVicsek[1985]‚ÄôsmethodofsimulationthatresultsinthedepositionshowninFigure14.3.\n200 100 00100200\nLength\nSurface height\nFigure 14.3 A simulation of the ballistic deposition of 20,000 particles onto a substrate of length\n200. The vertical height increases in proportion to the length of deposition time, with the top being\nthe Ô¨Ånal surface.\n14.4 Length of British Coastline 313\nConsiderparticlesfallingonto,andstickingto,ahorizontallineoflength Lcomposedof\n200depositionsites.Allparticlesstartfromthesameheight,buttosimulatetheirdifferent\nemissionvelocities,weassumetheystartatrandomdistancesfromtheleftsideoftheline.\nThesimulationconsistsofgeneratinguniformrandomsitesbetween0and L,andhaving\naparticlesticktothesiteonwhichitlands.Seeingthatthephysicalsituationwouldhave\ncolumnsofaggregatesofdifferentheights,theparticlemaybestoppedbeforeitgetstoa\nline,oritmaybouncearoundandfallintoahole.Wethereforeassumethatifthecolumn\nheightatwhichtheparticlelandsisgreaterthanthatofbothitsneighbors,itwilladdto\nthatheight.Iftheparticlelandsinahole,orifthereisanadjacenthole,itwillfillupthe\nhole.Wespeedupthesimulationbysettingtheheightoftheholeequaltothemaximum\nofitsneighbors.Herearethesteps:\n1) Choosearandomsite r.\n2) Letthearray hrbetheheightofthecolumnatsite r.\n3) Makethedecision:\nhr={\nhr+1, ifhr‚â•hr‚àí1,hr>hr+1,\nmax[hr‚àí1,hr+1],ifhr<hr‚àí1,hr<hr+1.(14.17)\nTheessentialloopinthesimulationis:\nspot =int(random)\nif(spot == 0)\nif( coast[spot] < coast[spot+1] )\n4 coast[spot] = coast[spot+1];\nelsecoast[spot]++;\nelse if ( spot == coast.length ‚àí1)\nif( coast[spot] < coast[spot ‚àí1] ) coast[spot] = coast[spot ‚àí1];\n8 elsecoast[spot]++;\nelse if ( coast[spot]<coast[spot ‚àí1] && coast[spot]<coast[spot+1] )\nif(c o a s t [ s p o t ‚àí1] > coast[spot+1] ) coast[spot] = coast[spot ‚àí1];\nelsecoast[spot] = coast[spot+1];\n12elsecoast[spot]++;\nThe results of this simulation show several empty regions scattered throughout the line\n(Figure14.3),whichisanindicationofthestatisticalnatureofgrowingfilms.Simulations\nbyFereydoonproducedfractalsurfacesthatreproducedtheexperimentalobservationthat\ntheaverageheightincreaseslinearlywithtime.(Youwillbeaskedtodeterminethefractal\ndimensionofasimilarsurfaceasanexercise.)\nExercise Extendthesimulationofrandomdepositiontotwodimensions,soratherthan\nmakingalineofparticlesyounowdeposituponanentiresurface.\n14.4 Length of British Coastline\nIn 1967 Benoit Mandelbrot asked a classic question, ‚ÄúHow long is the coast of Britain?‚Äù\n[Mandelbrot,1967].IfBritainhadtheshapeofColoradoorWyoming,bothofwhichhave\nstraight-lineboundaries,itsperimeterwouldbeacurveofdimension1withfinitelength.\nHowever, coastlines are geographic, and not geometric curves, with each portion of the\ncoastappearingsomewhatself-similartotheentirecoast.Iftheperimeterofthecoastis,\n314 14 Fractals and Statistical Growth Models\ninfact,afractal,thenitslengthiseitherinfiniteormeaningless.Mandelbrotdeducedthe\ndimensionofthewestcoastofBritaintobe df=1.25,whichimpliesinfinitelength.Inyour\nproblem,weaskyoutodeterminethedimensionoftheperimeterofoneofyourfractal\nsimulations.\nThelengthofthecoastlineofanislandistheperimeterofthatisland.Whiletheconcept\nofperimeterisclearforgeometricfigures,somethoughtisrequiredtogiveitmeaningfor\nanobjectthatmaybeinfinitelyself-similar.Letusassumethatamapmakerhasarulerof\nlengthr.Ifshewalksalongthecoastlineandcountsthenumberoftimes Nthatshemust\nplacetherulerdowninorderto coverthecoastline,shewillobtainavalueforthelength L\nofthecoastas Nr.Imaginenowthatthemapmakerkeepsrepeatingherwalkwithsmaller\nandsmallerrulers.Ifthecoastwereageometricfigure,ora rectifiablecurve ,atsomepoint\nthelength Lwouldbecomeessentiallyindependentof randwouldapproachaconstant.\nNonetheless,asdiscoveredempiricallybyRichardson[1961]fornaturalcoastlines,suchas\nthoseofSouthAfricaandBritain,theperimeterappearstobeanunusualfunctionof r:\nL(r)‚âÉMr1‚àídf, (14.18)\nwhereManddfare empirical constants. For a geometric figure, or for Colorado, df=1,\nandthelengthapproachesaconstantas r‚Üí0.Yetforafractalwith df>1,theperimeter\nL‚Üí‚àûasr‚Üí0.Thismeansthatasaconsequenceofself-similarity,fractalsmaybeoffinite\nsize,buthaveinfiniteperimeters.Physically,atsomepoint,theremaybenomoredetails\ntodiscernas r‚Üí0(say,atthequantumorComptonsizelimit),andsothelimitmaynot\nbephysicallymeaningful.\n14.4.1 Box Counting Algorithm\nConsideralineoflength Lbrokenupintosegmentsoflength r(Figure14.4left).Thenum-\nberofsegmentsor‚Äúboxes‚Äùneededtocoverthelineisrelatedtothesize roftheboxby\nN(r)=L\nr=C\nr, (14.19)\nwhereCis a constant. One definition of fractional dimension is the power of rin this\nexpression as r‚Üí0. In our example, it tells us that the line has dimension df=1.If we\nnowaskhowmanylittlecirclesofradius ritwouldtaketo coverorfillacircleofarea A\n(Figure14.4middle),wewillfindthat\nN(r)=lim\nr‚Üí0A\nùúãr2‚áídf=2, (14.20)\nasexpected.Likewise,countingthenumberoflittlespheresorcubesthatcanbepacked\nwithinalargespheretellsusthataspherehasdimension df=3.Ingeneral,ifittakes N\nlittlespheresorcubesofside r‚Üí0tocoversomeobject,thenthefractaldimension dfcan\nbededucedas\nN(r)=C(1\nr)df=C‚Ä≤sdf(asr‚Üí0), (14.21)\nlogN(r)=logC‚àídflog(r)(asr‚Üí0), (14.22)\n‚áídf=‚àílim\nr‚Üí0ŒîlogN(r)\nŒîlogr. (14.23)\n14.4 Length of British Coastline 315\nHeres‚àù1‚àïriscalledthe scaleingeography,so r‚Üí0correspondstoaninfinitescale.To\nillustrate,youmaybefamiliarwiththelowscaleonamapbeing10,000mtoacentimeter,\nwhile the high scale is 100 m to a centimeter. If we want the map to show small details\n(sizes),weneedamapofhighscale.\nForthecoastlineproblem,we‚Äôlluseboxcountingtodeterminethedimensionofaperime-\nter,andnotofanentirefigure.Oncewehaveavalueforthedimension,wewillgoonand\ndeterminethelengthoftheperimetervia(14.18).\n14.4.2 Coastline Exercise\nRatherthanruinyoureyesfocusingonageographicmap,wesuggestusingsomethingat\nhandthatlookslikeanaturalcoastline,namely,thetopportionofFigure14.3.Determine\ndfbycoveringthisfigure,oroneyouhavegenerated,withasemitransparentpieceofgraph\npaper,1andcountingthenumberofboxescontaininganypartofthecoastline(Figures14.4\nand14.5).\n1) Printyourcoastlinegraphwiththesamephysicalscale( aspectratio )fortheverticaland\nhorizontalaxes.Thisisrequiredbecausethegraphpaperyouwilluseforboxcounting\nhassquareboxesandsoyouwantyourgraphtoalsohavethesameverticalandhorizon-\ntalscales.Placeapieceofgraphpaperoveryourprintoutandlookthroughthegraph\npaper at your coastline. If you do not have a piece of graph paper available, or if you\nareunabletoobtainaprintoutwiththesameaspectratioforthehorizontalandvertical\naxes,addaseriesofcloselyspacedhorizontalandverticallinestoyourcoastlineprintout\n2r\n80 40 0040100\nFigure 14.4 Examples of the use of box counting to determine fractal dimension. In the top left\nthe ‚Äúboxes‚Äù are circles and the perimeter is being covered. In the bottom left an entire Ô¨Ågure is\nbeing covered, and on the right a ‚Äúcoastline‚Äù is being covered by boxes of two different sizes\n(scales). The fractal dimension can be deduced by recording the number of boxes of different scales\nneeded to cover the Ô¨Ågures.\n1 Yes,wearesuggestingapainfullyanalogtechniquebasedonthetheorythattraumaleavesalasting\nimpression.Ifyouprefer,youcanstoreyouroutputasamatrixof1and0values,andletthecomputerdo\nthecounting,butthiswilltakemoreofyourtimethanbeinganalog!\n316 14 Fractals and Statistical Growth Models\nSquare ( m = 2.00)\nCoastline ( m = 1.3)\nStraight line ( m = 1.02)\nlog(scale)0510\n‚Äì3.5 ‚Äì4 ‚Äì3 ‚Äì2.5 ‚Äì2 ‚Äì1.5 ‚Äì1log(Number boxes)\nFigure 14.5 Fractal dimensions of a line, box, and coastline determined by box counting.\nThe slope at vanishingly small scale determines the dimension.\nandusetheselinesasyourgraphpaper.(Boxcountingshouldstillbeaccurateifboth\nyourcoastlineandyourgraphpaperhavethesameaspectratios.)\n2) Theverticalheightinourprintoutwas17cm,andthelargestdivisiononourgraphpaper\nwas1cm.Thissetsthescaleofthegraphas1:17,or s=17forthelargestdivisions(lowest\nscale).Measuretheverticalheightofyourfractal,compareittothesizeofthebiggest\nboxesonwhateveryouareusingasyourpieceofgraphpaper,andthusdetermineyour\nlowestscale.\n3) Withourlargestboxesof1 √ó1cm,wefoundthatthecoastlinepassedthrough N=24\nboxes,thatis,24largeboxescoveredthecoastlineat s=17.Determinehowmanyofthe\nlargestboxes(lowestscale)areneededtocoveryourcoastline.\n4) Withournextsmallerboxesof0.5 √ó0.5cm,wefoundthat51boxescoveredthecoastline\nat a scale of s=34. Determine how many of the midsize boxes (midrange scale) are\nneededtocoveryourcoastline.\n5) Withoursmallestboxesof1 √ó1mm,wefoundthat406boxescoveredthecoastlineata\nscaleofs=170.Determinehowmanyofthesmallestboxes(highestscale)areneeded\ntocoveryourcoastline.\n6) Equation(14.23)tellsusthatastheboxsizesgetprogressivelysmaller,wehave\nlogN‚âÉlogA+dflogs, (14.24)\n‚áídf‚âÉŒîlogN\nŒîlogs=logN2‚àílogN1\nlogs2‚àílogs1=log(N2‚àïN1)\nlog(s2‚àïs1). (14.25)\nClearly,onlytherelativescalesmatterbecausetheproportionalityconstantscancelout\nintheratio.Aplotoflog Nversuslogsshouldyieldastraightlinewithaslopeof df(1.23\nforus).Determinethefractaldimensionforyourcoastline.Althoughonlytwopoints\nareneededtodeterminetheslope,useyourlowestscalepointasanimportantcheck.\n(Becausethefractaldimensionisdefinedasalimitforinfinitesimalboxsizes,thehighest\nscalepointsaremostsignificant.)\n7) Using(14.18),wefindthatthelengthofourcoastlineforour svalueis\nL‚àùs1.23‚àí1=s0.23. (14.26)\nIfwekeepmakingtheboxessmallerandsmaller,sothatwearelookingatthecoastline\nathigherandhigherscale, andifthecoastlineisself-similaratalllevels,thenthescale",10776
147-14.7 Fractals in Bifurcations.pdf,147-14.7 Fractals in Bifurcations,"14.5 Correlated Growth 317\nswillkeepgettinglargerandlargerwithnolimits(oratleastuntilwegetdowntosome\nquantumlimitonsmallsizes),andthus\nL‚àùlim\ns‚Üí‚àûs0.23=‚àû. (14.27)\nDoesyourfractalimplyaninfinitecoastline?Doesitmakesensethatasmallislandlike\nBritain,whichyoucanwalkaround,hasaninfiniteperimeter?\n14.5 Correlated Growth\nIt is an empirical fact that there is increased likelihood that a plant will grow if there is\nanother one nearby (Figure 14.6 left). This type of correlation also seems to occur in the\ndepositionofsurfacefilms.Your problemistoincludecorrelationsinyoursurfacesimu-\nlationandobservethechangeitmakes.\nA variation of the ballistic deposition, known as the correlated ballistic deposition ,\nsimulates mineral deposition onto substrates on which dendrites form [Tait et al., 1990;\nSanderetal.,1994].Weextendtheballisticdepositionalgorithmtoincludethelikelihood\nthat a freshly deposited particle will attract another particle. The extension is to assume\nthat the probability of sticking Óàºdepends inversely on the distance dthat the added\nparticleisfromthelastone(Figure14.6right):\nÓàº=cd‚àíùúÇ. (14.28)\nHereùúÇisaparameterand cisaconstantthatsetstheprobabilityscale.2Forourimplemen-\ntationwechoose ùúÇ=2,whichmeansthatthereisaninversesquareattractionbetweenthe\nparticles(decreasedprobabilityastheygetfartherapart).\nAs in our study of uncorrelated deposition, a uniform random number in the interval\n[0,L]determinesthecolumninwhichtheparticlewillbedeposited.Weusethesamerules\ni\ni + 1 d\nFigure 14.6 Left: A view of what might be the undergrowth of a forest or dendrites formed during\nsurface deposition. Right: The probability of particle i+1 sticking in one column depends upon the\ndistance dfrom the previously deposited particle i.\n2 Theabsoluteprobability,ofcourse,mustbelessthanone,butitisnicetochoose csothattherelative\nprobabilitiesproduceagraphwitheasilyseenvariations.\n318 14 Fractals and Statistical Growth Models\nabouttheheightsasbefore,butnowasecondrandomnumberisusedinconjunctionwith\n(14.28)todecideiftheparticlewillstick. Forinstance,ifthecomputedprobabilityis0.6\nandifr<0.6,theparticlewillbeaccepted(sticks),if r>0.6,theparticlewillberejected.\nOurcode Column.pyisgiveninListing14.2.\n14.6 Diffusion-Limited Aggregation\nConsiderabunchofgrapesonanoverheadvine.Your problemistocreateamodelofhow\nitstantalizingshapemightarise.Inaflashofdivineinsight,yourealizethattheseshapes,as\nwellasothers,suchasthoseofcolloidsandthin-filmstructures,mayresultfromanaggrega-\ntionprocesswithparticlesdiffusingaroundeachother.Infact,amodelofdiffusion-limited\naggregation (DLA) has successfully explained the relation between a cluster‚Äôs perimeter\nandmass[WittenandSander,1981].\nExercise Followthesestepstoconstructyourmodel:\n1) Definea2Dlatticeofpointsrepresentedbythearray grid[400,400] ,withallelements\ninitiallyzero.\n2) Placeaseedparticleatthecenterofthelatticebysetting grid[199,199] = 1 .\n3) Imagine a circle of radius 180 lattice spacings centered at grid[199,199] . This is the\ncirclefromwhichyoureleaseparticles.\n4) Determinetheangularlocationonthecircle‚Äôscircumferencefromwhichtoreleasea\nparticlebygeneratingauniformrandomanglebetween0and2 ùúã.\n5) Youareabouttoreleaseanewparticle,andhaveitexecutearandomwalk,muchlike\ntheonewestudiedinChapter4,butrestrictedtoverticalorhorizontaljumpsbetween\nlatticesites:\na) Generateauniformrandomnumber0 <rxy<1.\nb) ifrxy<0.5,themotionwillbevertical.\nc) ifrxy‚â•0.5,themotionwillbehorizontal.\n6) Make the model more realistic by letting the length of each step vary according to a\nrandomGaussiandistribution.GenerateaGaussian-weightedrandomnumberinthe\ninterval[‚àí‚àû,‚àû].Thisisthesizeofthestep,withthesignindicatingdirection.( Hint:\nThesumofauniformrandomdistributionprovidesaGaussiandistribution.)\n7) Wenowknowthetotaldistanceanddirectiontheparticlewilltravel.Haveitjumpone\nlatticespacingatatimeuntilthistotaldistanceiscovered.\n8) Beforeajump,checkwhetheranearest-neighborsiteisoccupied:\na) Ifoccupied,theparticlessticktogetherandstayinthatposition.Thewalkforthat\nparticleisover.\nb) Ifthesiteisunoccupied,theparticlejumpsonelatticespacing.\n9) Continuethecheckingandjumpinguntilthecalculateddistanceiscovered,untilthe\nparticlesticks,oruntilitleavesthecircleandislostfromourgrip.\n10) Onceonerandomwalkisover,releaseanotherparticle,andrepeattheprocessasoften\nas desired. Because many particles are lost, you may need to generate hundreds of\nthousandsofparticlestoformaclusterofseveralhundredparticles.Yourresultsshould\nlooklikeFigure14.7.\n14.6 Diffusion-Limited Aggregation 319\nFigure 14.7 A globular cluster of particles of the type that might occur in a colloid.\nFigure 14.8 Number 8 by the American painter Jackson Pollock. (Used with permission State\nUniversity of New York.) Some researchers claim that Pollock‚Äôs paintings exhibit a characteristic\nfractal structure, while some others question this [Kennedy, 2006].\n14.6.1 Fractal of DLA or Pollock\nAclustergeneratedwiththeDLAtechniqueisshowninFigure14.7.Wewishtoanalyzeit\ntoseeifthestructureisafractal,and,ifso,todetermineitsdimension.(Asanalternative,\nyoumayanalyzethefractalnatureofthePollockpaintinginFigure14.8,atechniqueused\nto determine the authenticity of this sort of art.) As a control, simultaneously analyze a\ngeometricfigure,suchasasquareorcircle,whosedimensionisknown.Theanalysisisa\nvariationoftheoneusedtodeterminethelengthofthecoastlineofBritain.\n1) Ifyouhavenotalreadydoneso,usethebox-countingmethodtodeterminethefractal\ndimensionofasimplesquare.\n2) Drawasquareoflength L,smallrelativetothesizeofthecluster,aroundtheseedpar-\nticle.(Smallmightbesevenlatticespacingstoaside.)\n3) Countthenumberofparticleswithinthesquare.\n4) Compute the particle density ùúåby dividing the number of particles by the number of\nsitesavailableinthebox(49inourexample).\n5) Repeattheprocedureusinglargerandlargersquares.",5879
148-14.9 Perlin Noise Adds Realism.pdf,148-14.9 Perlin Noise Adds Realism,"320 14 Fractals and Statistical Growth Models\n6) Stopwhentheclusteriscovered.\n7) Thefractaldimension dfisestimatedfromalog-logplotofthedensity ùúåversusL.Ifthe\nclusterisafractal,then(14.2)tellsusthat ùúå‚àùLdf‚àí2,andthegraphshouldbeastraight\nlineofslope df‚àí2.\nThegraphwegeneratedhadaslopeof ‚àí0.36,whichcorrespondstoafractaldimensionof\n1.66.Seeingthatrandomnumbersareinvolved,thegraphyougeneratewillbedifferent,\nbutthefractaldimensionshouldbesimilar.(Actually,thestructureismultifractal,andso\nthedimensionalsovarieswithlocationinthecluster.)\n14.7 Fractals in Bifurcations\nInthenextchapterthereisaprojectinvolvingthelogisticsmapwhereweplotthevalues\nofthenumberofbugs versusthegrowthparameter ùúá.Takeoneofthebifurcationgraphs\nproducedthereanddeterminethefractaldimensionofdifferentpartsofthegraphbyusing\nthesametechniquethatwasappliedtothecoastlineofBritain.\n14.8 Cellular Automata Fractals\nThere is a class of statistical models known as cellular automata that produce complex\nbehaviorsfromverysimplerules.CellularautomataweredevelopedbyvonNeumannand\nUlam in the early 1940s (von Neumann was also working on the theory behind modern\ncomputersthen).Thoughverysimple,cellularautomatahavefoundapplicationsinmany\nbranches of science [Peitgen et al., 1994; Sipper, 1997]. Their definition [Barnsley and\nHurd,1992]:\nAcellularautomatonisadiscretedynamicalsysteminwhichspace,time,andthestates\nofthesystemarediscrete.Eachpointinaregularspatiallattice,calledacell,canhave\nanyoneofafinitenumberofstates,andthestatesofthecellsinthelatticeareupdated\naccordingtoalocalrule.Thatis,thestateofacellatagiventimedependsonlyonits\nownstateonetimesteppreviously,andthestatesofitsnearbyneighborsattheprevious\ntimestep.Allcellsonthelatticeareupdatedsynchronously,andsothestateoftheentice\nlatticeadvancesindiscretetimesteps .\nAcellularautomatonintwodimensionsconsistsofanumberofsquarecellsthatgrowupon\neachother.Afamousoneis Conway‚ÄôsGameofLife .Inthis,cellswithvalue1arealive,while\ncellswithvalue0aredead.Cellsgrowaccordingtotherules:\n1) Ifacellisalive,andiftwoorthreeofitseightneighborsarealive,thenthecellremains\nalive.\n2) Ifacellisalive,andifmorethanthreeofitseightneighborsarealive,thenthecelldies\nduetoovercrowding.\n3) Ifacellisalive,andonlyoneofitseightneighborsisalive,thenthecelldiesofloneliness.\n4) Ifacellisdead,andmorethanthreeofitsneighborsarealive,thenthecellrevives.\n14.9 Perlin Noise Adds Realism ‚äô321\nFigure 14.9 The rules for two versions of the Game of Life. The rules, given graphically on the top\nrow, create the gaskets below. Our code Gameoflife.py is given in Listing 14.3.\nEarly studies of the statistical mechanics of cellular automata were made by Wolfram\n[1983],whoindicatedhowonecanbeusedtogenerateaSierpi¬¥ nskigasket.Becausewehave\nalreadyseenthataSierpi¬¥ nskigasketexhibitsfractalgeometry(Section14.1),thisrepresents\namicroscopicmodelofhowfractalsmayoccurinnature.Thismodeluseseightrules,given\ngraphically at the top of Figure 14.9, to generate new cells from old. We see all possible\nconfigurationsforthreecellsinthetoprow,andthebegettednextgenerationintherow\nbelow.AtthebottomofFigure14.9,aSierpi¬¥ nskigasketissogenerated.\n14.9 Perlin Noise Adds Realism ‚äô\nWehaveseenhowstatisticalfractalsareabletogenerateobjectswithastrikingresemblance\ntothoseinnature.Thisappearanceofrealismmaybefurtherenhancedbyincludingatype\nofcoherentrandomnessknownas Perlinnoise .ThistechniquewasdevelopedbyKenPerlin\nof New York University, who won an Academy Award (an Oscar) in 1997 for it and has\ncontinuedtoimproveitPerlin[2023].Thistypeofcoherentnoisehasfounduseinimpor-\ntantphysicssimulationsofstochasticmedia[Tickner,2004],aswellasinvideogames,and\nmotionpictureslike Tron.\nTheinclusionofPerlinnoiseinasimulationaddsbothrandomnessandatypeofcoher-\nence among points in space that tends to make dense regions denser and sparse regions\nsparser.Thisissimilartoourcorrelatedballisticdepositionsimulations(Section14.3),and\nisrelatedtochaosinitslong-rangerandomnesswithshort-rangecorrelations.Westartwith\nsomeknownfunctionsof xandy,andaddnoisetothem.Forthispurpose,Perlinusedthe\nmappingor easefunction(Figure14.12right)\nf(p)=3p2‚àí2p3. (14.29)\nAsaconsequenceofitsSshape,thismappingmakesregionscloseto0,evencloserto0,\nwhilemakingregionscloseto1,evencloserto1(inotherwords,itincreasesthetendency\ntoclump,whichshowsupashighercontrast).Wethenbreakspaceupintoauniformrect-\nangulargridofpoints(Figure14.10),andconsiderapoint (x,y)withinasquarewithvertices\n322 14 Fractals and Statistical Growth Models\n(x0, y1)\n(x0, y0)(x1, y1)\n(x1, y0)(x, y)\nFigure 14.10 The coordinates used in adding Perlin noise. The rectangular grid is used to locate a\nsquare in space and a corresponding point within the square. As shown with the arrows, unit\nvectors giwith random orientation are assigned at each grid point.\n(x, y)(x0, y1)\n(x0, y0) (x1, y0)(x1, y1)\ng1g2g3\ng0p3p2\np0 p1\nFigure 14.11 The coordinates used in adding Perlin noise. A point within each square is located\nby drawing the four pi.T h egiv e c t o r sa r et h es a m ea so nt h el e f t .\n(x0, y1)\n(x0, y0)(x1, y0)(x1, y1)\n(x, y)\nstuv\n0.20.2\n0.40.4\n0.60.6\n0.80.8\n113p2 ‚Äì 2p3\np\nFigure 14.12 The mapping used in adding Perlin noise. Left: The numbers s,t,u, andùë£are\nrepresented by perpendiculars to the four vertices, with lengths proportional to their values.\nRight: The function 3 p2‚àí2p3is used as a map of the noise at a point like ( x,y) to others close by.",5462
149-14.10 Code Listings.pdf,149-14.10 Code Listings,"14.9 Perlin Noise Adds Realism ‚äô323\n(x0,y0),(x1,y0),(x0,y1),and(x1,y1).Wenextassignunitgradientsvectors g0tog3withran-\ndomorientationateachgridpoint.Apointwithineachsquareislocatedbydrawingthe\nfourpivectors(Figure14.11):\np0=(x‚àíx0)i+(y‚àíy0)j,p1=(x‚àíx1)i+(y‚àíy0)j, (14.30)\np2=(x‚àíx1)i+(y‚àíy1)j,p3=(x‚àíx0)i+(y‚àíy1)j. (14.31)\nNext,thescalarproductsofthe p‚Ä≤sandtheg‚Ä≤sareformed:\ns=p0‚ãÖg0,t=p1‚ãÖg1,ùë£=p2‚ãÖg2,u=p3‚ãÖg3. (14.32)\nAsshownontheleftinFigure14.12,thenumbers s,t,u,andùë£areassignedtothefourver-\nticesofthesquareandrepresentedtherebylinesperpendiculartothesquarewithlengths\nproportionaltothevaluesof s,t,u,andùë£(whichcanbepositiveornegative).\nTheactualmappingproceedsviaanumberofsteps(Figure14.13):\n1) Transformthepoint (x,y)to(sx,sy),\nsx=3x2‚àí2x3,sy=3y2‚àí2y3. (14.33)\n2) Assignthelengths s,t,u,andùë£totheverticesinthemappedsquare.\nPerlin noise\n3) Obtaintheheight a(Figure14.13)vialinearinterpolationbetween sandt.\n4) Obtaintheheight bvialinearinterpolationbetween uandùë£.\n5) Obtainsyasalinearinterpolationbetween aandb.\n6) Thevector csoobtainedisnowthetwocomponentsofthenoiseat (x,y).\n14.9.1 Ray Tracing Algorithms\nRaytracingisatechniquethatrendersanimageofascenebysimulatingthewayraysof\nlighttravel[Pov-Ray,2023].Toavoidtracingraysthatdonotcontributetothefinalimage,\nray-tracingprogramsstartattheviewer,traceraysbackwardontothescene,andthenback\nagainontothelightsources.Youcanvarythelocationoftheviewerandlightsourcesand\nthepropertiesoftheobjectsbeingviewed,aswellasatmosphericconditionssuchasfog,\nhaze,andfire.\nAs an example of what this can do, on the right in Figure 14.14, we show the output\nfrom the ray-tracing program Pov-Ray [2023], using as input the coherent random noise\n324 14 Fractals and Statistical Growth Models\n(x, y)\n(x0, y0)(x0, y1)(x1, y1)\n(x1, y0)tu v\ns stu v\n(sx, sy)b\nNoise\ncc\na\nFigure 14.13 Perlin noise mapping. Left:T h ep o i n t( x, y) is mapped to point ( sx,xy).Right:U s i n g\n(14.33). Then three linear interpolations are performed to Ô¨Ånd c, the noise at ( x, y).\nFigure 14.14 After the addition of Perlin noise, the random scatterplot on the left becomes the\nclusters on the right.\nontheleftinFigure14.14.TheprogramoptionsweusedaregiveninListing14.4,andare\nseentoincludecommandstocolortheislands,toincludewaves,andtogivetexturestothe\nskyandthesea.Pov-RayalsoallowsthepossibilityofusingPerlinnoisetogivetexturesto\ntheobjectstobecreated.Forexample,thestonecupontherightoftheinsetabovehasa\nmarble-liketextureproducedbyPerlinnoise.\n14.10 Code Listings\nListing14.1 Fern3D.py Simulatesthegrowthoffernsin3D.\n# Fern3D.py: Fern in 3D, see Barnsley, ""Fractals Everywhere""\nfromvisualimport ‚àó\n4fromvisual.graph import ‚àó\nimportrandom\nimax = 20000\n8x = 0.5; y = 0.0; z = ‚àí0.2; xn = 0.0; yn = 0.0\ngraph1 = display(width=500, height=500, forward=( ‚àí3,0,‚àí1),\\ntitle= ‚Äô3D Fractal Fern (rotate via right mouse button)‚Äô ,range=10)\ngraph1.show_rendertime = True # Pts/sphs: cycle=27/750 ms, render=6/30\n14.10 Code Listings 325\n12pts = points(color=color.green, size=0.01)\nforiin range (1,imax):\nr = random.random() ;\nif( r <= 0.1): # 10% probability\n16 xn = 0.0\nyn = 0.18 ‚àóy\nzn = 0.0\nelif(r>0 . 1 andr< =0 . 7 ) : # 60% probability\n20 xn = 0.85 ‚àóx\nyn = 0.85 ‚àóy+0 . 1 ‚àóz+1 . 6\nzn =‚àí0.1 ‚àóy + 0.85 ‚àóz\nelif(r>0 . 7 andr <= 0.85): # 15 % probability\n24 xn = 0.2 ‚àóx‚àí0.2 ‚àóy\nyn = 0.2 ‚àóx+0 . 2 ‚àóy+0 . 8\nzn= 0.3 ‚àóz\nelse:\n28 xn =‚àí0.2‚àóx+ 0 . 2 ‚àóy # 15% probability\nyn = 0.2 ‚àóx+ 0 . 2 ‚àóy+0 . 8\nzn = 0.3 ‚àóz\nx=x n\n32y=y n\nz=z n\nxc = 4.0 ‚àóx #l i n e a rT Ff o rp l o t\nyc = 2.0 ‚àóy‚àí7\n36zc = z\npts.append(pos=(xc,yc,zc))\nListing14.2 Column.py Simulatescorrelatedballisticdepositionofmineralsontosub-\nstratesonwhichdendritesform.\n1# Column.py: Fractal growth of columns\nfromvisualimport ‚àó;\nimportrandom\n5\nmaxi = 100000; npoints = 200 # Number iterations , spaces\ni=0 ; d i s t=0 ; r=0 ; x=0 ; y=0\noldx = 0; oldy = 0; pp = 0.0; prob = 0.0\n9hit = zeros( (200), int)\ngraph1 = display(width = 500, height = 500, range=250,\ntitle = ‚ÄôCorrelated Ballistic Deposition‚Äô )\npts = points(color=color.green, size=2)\n13foriin range (0, npoints): hit[i] = 0 # Clear array\noldx = 100; oldy = 0\nforiin range (1, maxi + 1):\nr=int(npoints ‚àórandom.random() )\n17x=r‚àíoldx\ny=h i t [ r ] ‚àíoldy\ndist = x ‚àóx+y ‚àóy\nif(dist = = 0): prob = 1.0 # Sticking prob depends on last x\n21else: prob = 9.0/dist\npp = random.random()\nif(pp < prob):\nif(r>0andr<(npoints ‚àí1) ):\n25 if( (hit[r] >= hit[r ‚àí1])and(hit[r] >= hit[r + 1]) ):\nhit[r] = hit[r] + 1\nelse:\nif(hit[r ‚àí1] > hit[r + 1]):\n29 hit[r] = hit[r ‚àí1]\nelse: hit[r] = hit[r + 1]\noldx = r\noldy = hit[r]\n33 olxc = oldx ‚àó2‚àí200 #T Ff o rp l o t\nolyc = oldy ‚àó4‚àí200\npts.append(pos=(olxc,olyc))\n326 14 Fractals and Statistical Growth Models\nListing 14.3 Gameoflife.py Is an extension of Conway‚Äôs Game of Life in which cells\nalwaysreviveifoneoutofeightneighborsisalive.\n# Gameoflife.py: Cellular automata in 2 dimensions\n3‚Äô‚Äô‚Äô* Rules: a cell can be either dead (0) or alive (1)\n* If a cell is alive:\n* on next step will remain alive if\n* 2 or 3 of its closer 8 neighbors are alive.\n7 * If > 3 of 8 neighbors are alive, cell dies of overcrowdedness\n* If less than 2 neighbors are alive the cell dies of loneliness\n* A dead cell will be alive if 3 of its 8 neighbors are alive‚Äô‚Äô‚Äô\n11fromvisualimport ‚àó\nfromvisual.graph import ‚àó;importrandom\nscene = display(width= 500,height= 500, title= ‚ÄôGame of Life‚Äô )\n15cell = zeros((50,50)); cellu = zeros((50,50))\ncurve(pos= [( ‚àí49,‚àí49),(‚àí49,49),(49,49),(49, ‚àí49),(‚àí49,‚àí49)],color=color.white)\nboxes = points(shape= ‚Äôsquare‚Äô , size=8, color=color.cyan)\n19defdrawcells(ce):\nboxes.pos = [] # Erase previous cells\nforjin range (0,50):\nforiin range (0,50):\n23 ifce[i,j] == 1:\nxx = 2 ‚àói‚àí50\nyy = 2 ‚àój‚àí50\nboxes.append(pos=(xx,yy))\n27definitial():\nforjin range (20,28):\nforiin range (20, 28):\nr= int(random.random() ‚àó2)\n31 cell[j,i] = r\nreturncell\ndefgameoflife(cell):\nforiin range (1,49):\n35 forjin range (1,49):\nsum1 = cell[i ‚àí1,j‚àí1] + cell[i ,j ‚àí1] + cell[i+1,j ‚àí1]#n e i g h b\nsum2 = cell[i ‚àí1,j] + cell[i+1,j] + cell[i ‚àí1,j+1] \\n+c e l l [ i , j + 1 ]+c e l l [ i + 1 , j + 1 ]\n39 alive = sum1+sum2\nifcell[i,j] == 1:\nifalive == 2 oralive == 3: # Alive\ncellu[i,j] = 1 # Lives\n43 ifalive > 3 oralive < 2: # Overcrowded or solitude\ncellu[i,j] = 0 #d i e s\nifcell[i,j] == 0:\nifalive == 3:\n47 cellu[i,j] = 1 # Revives\nelse:\ncellu[i,j] = 0 # Remains dead\nalive = 0\n51returncellu\ntemp = initial ()\ndrawcells(temp)\nwhileTrue:\n55rate(6)\ncell = temp\ntemp = gameoflife(cell)\ndrawcells(cell)\n14.10 Code Listings 327\nListing 14.4 Islands.pov The Pov-Ray ray-tracing commands needed to convert the\ncoherentnoiserandomplotofFigure14.14intothemountain-likeimageinFigure14.14.\n// Islands.pov Pov ‚àíRay program to create Islands, by Manuel J Paez\nplane {\n4<0, 1, 0>, 0 // Sky\npigment { color rgb <0, 0, 1> }\nscale 1\nrotate <0, 0, 0>\n8translate y ‚àó0.2\n}\nglobal_settings {\nadc_bailout 0.00392157\n12assumed_gamma 1.5\nnoise_generator 2\n}\n#declare Island_texture = texture {\n16pigment {\ngradient <0, 1, 0> // Vertical direction\ncolor_map { // Color the islands\n[ 0.15 color rgb <1, 0.968627, 0> ]\n20 [ 0.2 color rgb <0.886275, 0.733333, 0.180392> ]\n[ 0.3 color rgb <0.372549, 0.643137, 0.0823529> ]\n[ 0.4 color rgb <0.101961, 0.588235, 0.184314> ]\n[ 0.5 color rgb <0.223529, 0.666667, 0.301961> ]\n24 [ 0.6 color rgb <0.611765, 0.886275, 0.0196078> ]\n[ 0.69 color rgb <0.678431, 0.921569, 0.0117647> ]\n[ 0.74 color rgb <0.886275, 0.886275, 0.317647> ]\n[ 0.86 color rgb <0.823529, 0.796078, 0.0196078> ]\n28 [ 0.93 color rgb <0.905882, 0.545098, 0.00392157> ]\n}\n}\nfinish {\n32ambient rgbft <0.2, 0.2, 0.2, 0.2, 0.2>\ndiffuse 0.8\n}\n}\n36camera { // Camera characteristics andlocation\nperspective\nlocation < ‚àí15, 6, ‚àí20> // Located here\nsky <0, 1, 0>\n40direction <0, 0, 1>\nright <1.3333, 0, 0>\nup <0, 1, 0>\nlook_at < ‚àí0.5, 0, 4> //looking at that point\n44angle 36\n}\nlight_source {< ‚àí10, 20, ‚àí25>, rgb <1, 0.733333, 0.00392157>} // Light\n48#declare Islands = height_field { // Takes gif and finds heights\ngif""d:\pov\montania.gif"" // Windows directory naming\nscale <50, 2, 50>\ntranslate < ‚àí25, 0, ‚àí25>\n52}\nobject { // Islands\nIslands\ntexture {\n56Island_texture\nscale 2\n}\n}\n60box { // Upper face of the box isthe sea\n<‚àí50, 0, ‚àí50>, <50, 0.3, 50> // Location of 2 opposite vertices\ntranslate < ‚àí25, 0, ‚àí25>\ntexture { // Simulate waves\n64normal {\nspotted\n0.4\nscale <0.1, 1, 0.1>\n328 14 Fractals and Statistical Growth Models\n68}\npigment { color rgb <0.164706, 0.556863, 0.901961> }\n}\n}\n72fog { // A constant fog isdefined\nfog_type 1\ndistance 30\nrgb <0.984314, 1, 0.964706>\n76}",8710
150-Chapter 15 Nonlinear Population Dynamics.pdf,150-Chapter 15 Nonlinear Population Dynamics,,0
151-15.1 The Logistic Map A Bug Population Model.pdf,151-15.1 The Logistic Map A Bug Population Model,"329\n15\nNonlinear Population Dynamics\nWe view nonlinear dynamics as one of the success stories of computational physics. It has\nbeen explored by scientists and engineers with computers as an essential tool, often then\nfollowed by mathematicians [Motter and Campbell, 2013]. The computations have led to\nthe discovery of new phenomena such as chaos, solitons, and fractals; the Ô¨Årst of which we\ncover in this chapter, and the last, later on. Here we look at discrete and continuous models\nof population dynamics that are simple, yet which yield surprising complex behavior. In\nChapter 16, we explore nonlinear behavior in classical oscillations .\n15.1 The Logistic Map, A Bug Population Model\nPopulationsofbugsandpatternsofweatherdonotappeartofollowanysimplelaws.1At\ntimes,thepopulationpatternsappearstable,atothertimestheyvaryperiodically,andat\nothertimestheyappearchaotic,withnodiscernableregularity,onlytosettlebackdownto\nsomethingsimpleagain.\nProblem Deduceifasimplelawcanproducesuchcomplicatedbehaviors.\nImagineabunchofbugsreproducinggenerationaftergeneration.Westartwitha N0bugs,\ntheninthenextgenerationwehavetolivewith N1ofthem,andafter ngenerations,there\nareNnofthemaroundtobugus.Wewanttodevelopamodelofhow Nnvarieswiththe\ngenerationnumber n.Clearly,iftheratesofbreedinganddyingarethesame,thenastable\npopulation occurs. Yet bugs cannot live on love alone, they must also eat, and bugs, not\nbeingfarmers,mustcompetefortheavailablefoodsupply.Thistendstorestricttheirnum-\nbertoliebelowsomemaximumpopulation N‚àó.Wewanttobuildalloftheseobservations\nintoourmodel.\nForguidance,welooktotheradioactivedecaysimulationinChapter4wherethediscrete\ndecaylaw,\nŒîN‚àïŒît=‚àíùúÜN, (15.1)\n1 ExceptmaybeinOregon,wherestormcloudscometospendtheirweekends.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n330 15 Nonlinear Population Dynamics\nledtoexponential-likedecay.Beingclever,westartourmodelingbyreversingthesignof ùúÜ,\nwhichshouldgiveus growth:\nŒîNi\nŒît=ùúÜNi. (15.2)\nYet we know that exponential growth eventually tapers off, with the population reach-\ning a maximum N‚àó(thecarrying capacity ). Consequently, we modify the growth model\n(15.2) by changing the growth rate parameter ùúÜto one that decreases as the population\napproaches N‚àó:\nùúÜ=ùúÜ‚Ä≤(N‚àó‚àíNi), (15.3)\n‚áíŒîNi\nŒît=ùúÜ‚Ä≤(N‚àó‚àíNi)Ni. (15.4)\nWeexpectthatwhen Niissmallcomparedto N‚àó,thepopulationwillgrownearlyexponen-\ntially.Yetwealsoexpectthatas Niapproaches N‚àó,thegrowthratewilldecrease,eventually\nbecomingnegativeif Niexceedsthecarryingcapacity N‚àó.Wecanimaginethetwopossibil-\nitiesleadingtooscillations.\nEquation(15.4)isaversionofthe logisticmap .Itisusuallywritteninaformthatrelates\nthenumberofbugsinthefuturetothenumberinthepresentgeneration:\nNi+1=Ni+ùúÜ‚Ä≤Œît(N‚àó‚àíNi)Ni, (15.5)\n=Ni(1+ùúÜ‚Ä≤ŒîtN‚àó)[\n1‚àíùúÜ‚Ä≤Œît\n1+ùúÜ‚Ä≤ŒîtN‚àóNi]\n. (15.6)\nThisrelationlookssimplerwhenexpressedintermsofdimensionlessvariables:\nxi+1=ùúáxi(1‚àíxi), (15.7)\nùúádef=1+ùúÜ‚Ä≤ŒîtN‚àó, (15.8)\nxidef=ùúÜ‚Ä≤Œît\n1+ùúÜ‚Ä≤ŒîtN‚àóNi‚âÉNi\nN‚àó. (15.9)\nHerexiisadimensionlesspopulationvariableand ùúáisa(yetanother)dimensionlessgrowth\nparameter.Observefrom(15.8),thatifthenumberofbugsbornpergeneration ùúÜ‚Ä≤Œîtislarge,\nthenxi‚âÉNi‚àïN‚àó,thatis,xiisessentiallythefractionofthemaximumpopulation N‚àó.Con-\nsequently,realistic xvaluesgenerallylieintherange0 ‚â§xi‚â§1,withx=0corresponding\nto no bugs, and x=1 corresponding to the carrying capacity. Also note that the growth\nrateùúáequals1,onlyifthebreedingrate ùúÜ‚Ä≤equals0,andisotherwiseexpectedtobelarger\nthan1.\nThe map (15.7) is seen to be the sum of linear and quadratic dependencies on xi.I ti s\ncalledamapbecauseitconvertsonenumberinasequencetothenext,\nxi+1=f(xi). (15.10)\nForthelogisticmap, f(x)=ùúáx(1‚àíx),withthequadraticdependenceon xmakingthisa\nnonlinearmap,andthedependenceononlytheonevariable xmakingita one-dimensional\nmap.\nJustbylookingat(15.7),wereallywouldnotexpectthatanythingassimpleasthismight\nberealisticdescriptionofbugpopulationdynamics.However,ifitexhibitssomefeatures\nsimilartothosefoundinnature,thenitmaywellformthefoundationforamorecomplete\ndescription(aswewilldevelopinSection15.5).",4145
152-15.1.1 Exploring Map Properties.pdf,152-15.1.1 Exploring Map Properties,,0
153-15.1.2 Fixed Points.pdf,153-15.1.2 Fixed Points,"15.1 The Logistic Map, A Bug Population Model 331\n01 02 0 01 02 001 02 000.40.8\n01 02 0xn\nxn\nn nnn\nFigure 15.1 The bug population xnversus the generation number nfor the four growth rates:\n(a)ùúá=2.8, a single attractor; (b) ùúá=3.3, a double attractor; (c) ùúá=3.5, a quadruple attractor;\n(d)ùúá=3.8, a chaotic regime.\n15.1.1 Exploring Map Properties\nRatherthanreadingabouthowfancymathematicalanalysesdeducethepropertiesofthe\nlogisticmap[Rasband,1990],wehereaskyoutoexploreityourselfbygeneratingandplot-\ntingsequencesof xivalues.YoushouldgetresultssimilartothoseshowninFigure15.1.\n15.1.1.1 Stable Populations\nAstablepopulationisonethatremainsthesamefromgenerationtogeneration.\n1) Start with an initial population x0=0.75, called the seed. You should find that the\ndynamicaleffectsarenotsensitivetoit.Also,asacheckonthemodel,startwithsome\nnegativeandzerovaluesfor ùúá,whichshouldproducedecayingpopulations.Makeplots\nofxiversusi.\n2) Nowlookforstablepopulationswith ùúá=0,0.5,1,1.5,2.\n3) Takenoteofthe transientbehaviorsthatoccurforearlygenerationsbeforemoreregular\nbehaviorssetin.\n4) Forafixedvalueof ùúá,trydifferentvaluesfortheseed x0,andtherebyverifythatwhile\nthetransientsmaydiffer,theregularbehaviorsdonot.\nYoushouldhavefoundthatthismodelyieldsstablepopulationsforpositivegrowthrates\nùúá,withthemaximumpopulationreachedmorerapidlyas ùúágetslarger.Thisisagoodvali-\ndationofthemodel.SometypicalbehaviorsareshowninFigure15.1.InFigure15.1a,we\nseeequilibrationintoasinglepopulation;inFigure15.1b,weseeoscillationbetweentwo",1529
154-15.1.4 Mapping Implementation.pdf,154-15.1.4 Mapping Implementation,"332 15 Nonlinear Population Dynamics\npopulationlevels;inFigure15.1c,weseeoscillationamongfourlevels;andinFigure15.1d,\nweseeachaoticsystem.\n15.1.2 Fixed Points\nAnimportantpropertyofthemap(15.7)isthepossibilityofthesequence xireachingone,\normore,fixedpointsx‚àó,thatis,populationsatwhichthesystemremains,orreturnstoregu-\nlarly.Ataone-cyclefixed-point,therewouldbenochangeinthepopulationfromgeneration\nitogeneration i+1,thatis,\nxi+1=xi=x‚àó. (15.11)\nSubstituting this relation into the logistic map (15.7) yields a quadratic equation we can\neasilysolve:\nùúáx‚àó(1‚àíx‚àó)=x‚àó, (15.12)\n‚áíx‚àó=0,orx‚àó=ùúá‚àí1\nùúá. (15.13)\nThenonzerofixed-point, x‚àó=(ùúá‚àí1)‚àïùúá,correspondstoastablepopulationinwhichthere\nis a balance between birth and death, as in Figure 15.1a. In contrast, the x‚àó=0 point is\nunstablesincethepopulationremainsstaticonlyaslongasnobugsexist;ifevenafewbugs\nareintroduced,exponentialgrowthoccurs.Furtheranalysis,whichweareabouttoexplore\ncomputationally,tellsusthatthestabilityofapopulationisdeterminedbythemagnitude\nofthederivativeofthemappingfunction f(xi)atthefixed-point[Rasband,1990]:\n||||df\ndx||||x‚àó<1 (stable) . (15.14)\nFortheonecycleofthelogisticmap(15.7),thederivativeis\ndf\ndx||||x‚àó=ùúá‚àí2ùúáx‚àó={\nùúá,stableatx‚àó=0ifùúá<1,\n2‚àíùúá,stableatx‚àó=ùúá‚àí1\nùúáifùúá<3.(15.15)\n15.1.3 Period Doubling, Bifurcations\nEquation(15.15)tellsusthattherewillnotbeanystablepopulationsfor ùúá>3.Inthiscase,\nthesystemundergoes bifurcations intotwopopulations,aso-called two-cycle.Theeffectis\nknownasperioddoubling ,andisevidentinFigure15.1b.Becausethesystemnowmoves\nbetweenthesetwopopulations,thepopulationsarecalled attractorsorcyclepoints .Wecan\neasilypredictthe xvaluesfortwo-cycleattractorsbydemandingthatgeneration i+2has\nthesamepopulationasgeneration i:\nxi=xi+2=ùúáxi+1(1‚àíxi+1), (15.16)\n‚áíx‚àó=1+ùúá¬±‚àö\nùúá2‚àí2ùúá‚àí3\n2ùúá. (15.17)\nWeseethataslongas ùúá>3,thesquarerootproducesarealnumberandthusthatphysical\nsolutions.Weleaveittoyourexplorationstodiscoverhowthesystemcontinuestobifur-\ncate asùúáis increased further. In all cases, the behavior repeats, with a single population\nbifurcatingintotwo.",2069
155-15.2 Chaos.pdf,155-15.2 Chaos,,0
156-15.3 Bifurcation Diagrams.pdf,156-15.3 Bifurcation Diagrams,"15.3 Bifurcation Diagrams 333\n15.1.4 Mapping Implementation\nItisnowtimetocarryoutamorecarefulinvestigationofthelogisticmap,followingthe\noriginalpathofFeigenbaum[1979]andhishandcalculator:\n1) ConfirmthatyouobtainthedifferentpatternsshowninFigure15.1for ùúá=(0.4,2.4,3.2,\n3.6,3.8304)andseed x0=0.75.\n2) Identifythefollowinginyourgraphs:\na)Transients :Irregularbehaviorsbeforereachingaregularbehavior,andthatthetran-\nsientsdifferfordifferentseeds.\nb)Asymptotes : In some cases, the steady state is reached after only 20 generations,\nwhileforlarger ùúávalues,hundredsofgenerationsmaybeneeded.Thesesteady-state\npopulationsareindependentoftheseed.\nc)Extinction :Ifthegrowthrateistoolow, ùúá‚â§1,thepopulationdiesoff.\nd)Stable states : The stable single-population states attained for ùúá<3s h o u l da g r e e\nwiththeprediction(15.13).\ne)Multiple cycles : Examine populations for a growth parameter ùúáincreasing con-\ntinuously through 3. Observe how the system continues to bifurcate. For example,\nFigure15.1cwith ùúá=3.5containsfourattractors(a four-cycle).\nf)Intermittency : Observe simulations for 3.8264 <ùúá<3.8304. Here the system\nappearsstableforafinitenumberofgenerationsandthenjumpsallaround,onlyto\nbecomestableagain.(Oldradiostendedtodothis.)\n15.2 Chaos\n‚ÄúChaos‚Äùhasdifferentmeaningstodifferentpeople.Forpresentpurposes,wedefinechaos\nasthedeterministicbehaviorofasystemdisplayingnodiscernibleregularity .Thismayseem\ncontradictory; if a system is deterministic, it must have step-to-step correlations, which,\nwhenaddedup,meanslong-rangecorrelations.Butwhenthebehaviorischaotic,thecom-\nplexitiesofthebehaviormayhidethedeterminismwithin.Inanoperationalsense, achaotic\nsystemisonewithanextremelyhighsensitivitytoparametervaluesorinitialconditions .This\nsensitivitytoevenminusculechangesissohighthat,inapracticalsense,itisimpossible\nto predict the long-range behavior without knowing the parameters to infinite precision,\naphysicalimpossibility.Yetbecausethesystemismathematicallydeterministic,itisnot\nrandom. As you may recall from Chapter 4, a random sequence has no correlation from\nonesteptothenext,whereasachaoticonedoes.\n1) Explorethelong-termbehaviorsofthelogisticmapinthechaoticregionstartingwith\nthetwo,essentiallyidentical,seeds x0=0.75andx‚Ä≤\n0=0.75(1+ùúñ),whereùúñ‚âÉ2√ó10‚àí14.\n2) Repeatthesimulationwith x0=0.75andtwoessentiallyidenticalsurvivalparameters,\nùúá=4andùúá=4(1‚àíùúñ),whereùúñ‚âÉ2√ó10‚àí14.Bothsimulationsshouldstartoffthesame,\nbuteventuallydiverge.\n15.3 Bifurcation Diagrams\nWatchingthepopulationchangeasafunctionofgenerationnumberprovidesagoodpicture\nofthebasicdynamicsatwork,atleastuntilthings\n334 15 Nonlinear Population Dynamics\n1. 00.00.20.40.60.81. 0\n2.0\nŒºx*\n3.0 4.0\nFigure 15.2 A bifurcation plot of attractor population x‚àóversus growth rate ùúáfor the logistic map.\nThe inset shows some details of a three-cycle window. (Gray scales indicate the regimes over which\nHans Kowallik distributed the work on different CPUs when run in parallel.)\nIn particular, as the number of bifurcations keeps increasing, the output may seem too\ncomplicated for you to discern any pattern. One way to visualize what is going on, is to\nconcentrateontheattractors,thatis,thosepopulationsthatappeartoattractthesolutions,\nandtowhichthesolutionscontinuouslyreturn(long-termiterates).Aplotofthe xvalues\noftheseattractorsasafunctionofthegrowthparameter ùúá,turnsouttobeanilluminating\nwindowintothedynamics.\nOnesuchbifurcationdiagram forthelogisticmapisshowninFigure15.2.Acorrespond-\ning,andsimilar,diagramforaverydifferentmap,theGaussianmap,isgiveninFigure15.3.\n(MoremapsaregiveninSection15.3.3.)Togeneratesuchadiagram,youhaveyourcalcu-\nlationproceedthroughallvaluesof ùúáinsmallsteps.Foreach ùúávalue,youwaitwhilethe\nsystemgoesthroughhundreds(ormore)ofiterations,sothatthetransientsdieout;atthis,\nyouareatafixedpoint x‚àó.Next,youwritethepair (x‚àó,ùúá)toafile,andcontinuetheitera-\ntionforhundredsofcycles,withoutchanging ùúá.Ifthesystemfallsintoan n-cycleforthis ùúá\nvalue,thenthereshouldpredominantlybe ndifferentx‚àóvalueswrittentothefile.Next,the\nvalueoftheinitialpopulation x0ischangedslightly,andtheentireprocedureisrepeated\ntoensurethatnofixed-pointsaremissed.Whenfinished,yourprogramwillhavestepped\nthroughallthevaluesof ùúáandx0.\nb = 1\nb = 4\nb = 5Figure 15.3 A bifurcation plot,\nx‚àóùë£ersusùúÜ, for the Gaussian map. (W.\nHager.)",4348
157-15.3.1 Bifurcation Diagram Implementation.pdf,157-15.3.1 Bifurcation Diagram Implementation,,0
158-15.3.3 Other Maps.pdf,158-15.3.3 Other Maps,"15.3 Bifurcation Diagrams 335\n15.3.1 Bifurcation Diagram Implementation\nOursampleprogram Bugs.pyisgiveninListing15.1.WeaskyoutoreproduceFigure15.2at\nvariouslevelsofdetail.Youcreateavisualizationofthissortbyplottingindividualpoints,\nwith the density in each region of the screen determined by the number of points plot-\ntedthere.Whenthinkingaboutplottingmanypoints,itisimportanttokeepinmindthat\nyourmonitorandprintercandisplayonlyafinitenumberofpixels(pictureelements).At\npresent, an HD monitor has 1920 √ó1080=2,073,600 pixels, but you do not need to use\nevery pixel; in any case, printing at a finer resolution is a waste of time. Here‚Äôs how to\ndoit:\n1) Breakuptherange1 ‚â§ùúá‚â§4into1000steps.Thesearethe‚Äúbins‚Äùintowhichyouwill\nplacethex‚àóvalues.\n2) Inordernottomissanystructuresinyourbifurcationdiagram,loopthrougharangeof\ninitialx0values.\n3) Waitatleast200generationsfortransientstodieout,andthenoutputthenextseveral\nhundred (ùúá,x‚àó)valuestoafile.\n4) Output your x‚àóvalues to no more than three or four decimal places. You will not be\nabletoresolvemoreplacesthanthisonyourplot,andthisrestrictionwillreducethe\nnumber of duplicate entries. You can use formatted output to control the number of\ndecimalplaces,oryoucandoitviaasimpleconversion:multiplythe xivaluesby1000,\nandthenthrowawaytheparttotherightofthedecimalpoint: Ix[i]= int(1000*x[i]).\nThendivideby1000ifyouwantfloating-pointnumbers.\n5) Plotx‚àóversusùúáusing small symbols for the points, with no connections between\npoints.\n6) Enlarge(zoominon)sectionsofyourplot,andnoticehowasimilarbifurcationdiagram\ntendstobecontainedwithineachmagnifiedportion(thisis self-similarity ).\n7) Lookovertheseriesofbifurcationsoccurring\nùúák‚âÉ3,3.449,3.544,3.5644,3.5688,3.569692,3.56989,‚Ä¶. (15.18)\n8) Notehowtheendofthisseriesisinaregionofchaoticbehavior,andthatthesystem\nsometimesentersintothechaoticregionsquickly.Accordingly,youmayhavetomake\nplotsoveraverysmallrangeof ùúávaluestoseeallofthestructuresthere.Acloseexami-\nnationofFigure15.2showsregionswhere,foraslightincreasein ùúá,averylargenumber\nofpopulationssuddenlychangetoveryfewpopulations.Whereasthesemayappearto\nbeartifactsofthevideodisplay,thisisarealeffect,andtheseregionsarecalled windows.\nCheckthataround ùúá=3.828427chaosmovesintoathree-cyclewindow.\n15.3.2 Feigenbaum Constants\nFeigenbaum [1979] discovered that the sequence of ùúákvalues (15.18) at which bifurca-\ntionsoccurfollowsaregularpattern.Specifically,the ùúávaluesconvergegeometricallywhen\nexpressedintermsofthedistancebetweenbifurcations ùõø:\nùúák‚Üíùúá‚àû‚àíc\nùõøk,ùõø=lim\nk‚Üí‚àûùúák‚àíùúák‚àí1\nùúák+1‚àíùúák. (15.19)",2569
159-15.4 Measures of Chaos.pdf,159-15.4 Measures of Chaos,,0
160-15.4.1 Lyapunov Coefficients.pdf,160-15.4.1 Lyapunov Coefficients,"336 15 Nonlinear Population Dynamics\nUseyoursequenceof ùúákvaluestodeterminethethreeconstantsin(15.19),andcompare\nthemtothosefoundbyFeigenbaum:\nùúá‚àû‚âÉ3.56995,c‚âÉ2.637,ùõø‚âÉ4.6692. (15.20)\nAmazingly,thevalueof ùõøisuniversalforallsecond-ordermaps.\n15.3.3 Other Maps\nBifurcationsandchaosaretypicalcharacteristicsofnonlinearsystems.Yetsystemscanbe\nnonlinearinanumberofways.Thetablebelowlistsfourmapsthatgenerate xisequences\ncontainingbifurcations.\nName f(x)Name f(x)\nLogistic ùúáx(1‚àíx)Tent ùúá(1‚àí2|x‚àí1‚àï2|)\nEcology xeùúá(1‚àíx)Quartic ùúá[1‚àí(2x‚àí1)4]\nGaussian e‚àíbx2+ùúá\nThetentmapderivesitsnonlineardependencefromtheabsolutevalueoperator,whilethe\nlogisticmapisseentobeasubclassoftheecologymap.Explorethepropertiesoftheseother\nmapsandnotethesimilaritiesanddifferences.\n15.4 Measures of Chaos\nOurdefinitionofchaosintermsofunpredictabilityseemsrathersubjective,ormaybehard\ntoapplyanalytically.Accordingly,severalanalyticmeasuresofchaoshavebeendeveloped,\nandinthissection,weexaminetwo,theLyapunovcoefficientsandShannonentropy.\n15.4.1 Lyapunov CoefÔ¨Åcients‚®Ä\nTheLyapunovcoefficient ùúÜprovidesananalyticsignalofchaos[Wolf etal.,1985;Ramasub-\nramanianandSriram,2000;Williams,1997;Manneville,1990].Specifically,thecoefficient\nistherateparameterintheexponentdescribingtheexponentialgrowthof x‚àóversusùúá.For\n1Dproblems,thereisonlyonesuchcoefficient,whereas,ingeneral,thereisacoefficient\nforeachdegreeoffreedom.Theessentialassumptionisthatneighboringpaths xnnearan\nattractorx‚àóhaveatimedependence L‚àùexp(ùúÜt).Consequently,if ùúÜ>0,thenumberoffixed\npointsgrowsexponentially,whichischaotic;if ùúÜ=0,wehaveamarginallystablepopula-\ntion;while ùúÜ<0impliesastableandperiodicpopulation.Mathematically,theLyapunov\ncoefficientisdefinedas\nùúÜ=lim\nt‚Üí‚àû1\ntlogL(t)\nL(t0), (15.21)\nwhereL(t)isthedistancebetweenneighboringphasespacetrajectoriesattime t.\nAsanexample,we‚ÄôllcalculatetheLyapunovexponentforageneral1Dmap,\nxn+1=f(xn), (15.22)\n15.4 Measures of Chaos 337\nwherethegenerationnumber nnowreplacesthetime t.Todeterminestability,weexamine\nperturbationsaboutareferencetrajectory x0byaddingasmallperturbation,anditerating\nonce:\nÃÇx0=x0+ùõøx0, ÃÇx1=x1+ùõøx1. (15.23)\nWesubstitutethisinto(15.22)andexpand finaTaylorseriesaround x0:\nx1+ùõøx1=f(x0+ùõøx0)‚âÉf(x0)+ùõøf\nùõøx||||x0ùõøx0=x1+ùõøf\nùõøx||||x0ùõøx0,\n‚áíùõøx1‚âÉ(ùõøf\nùõøx)\nx0ùõøx0. (15.24)\nThisistheproofofourearlierstatementthatanegative df‚àïdxindicatesstability.Todeduce\nthegeneralresult,weexamineoneiteration:\nùõøx2‚âÉ(ùõøf\nùõøx)\nx1ùõøx1=(ùõøf\nùõøx)\nx0(ùõøf\nùõøx)\nx1ùõøx0, (15.25)\n‚áíùõøxn=n‚àí1‚àè\ni=0(ùõøf\nùõøx)\nxiùõøx0. (15.26)\nThislastrelationtellsushowtrajectoriesdifferontheaverageafter nsteps:\n|ùõøxn|=Ln|ùõøx0|,Ln=n‚àí1‚àè\ni=0|||||(ùõøf\nùõøx)\nxi|||||. (15.27)\nWenowsolveforthe LandtakeitslogarithmtoobtaintheLyapunovcoefficient:\nùúÜ=ln(L)=lim\nn‚Üí‚àû1\nnn‚àí1‚àë\ni=0ln|||||(ùõøf\nùõøx)\nxi|||||. (15.28)\nForthelogisticmap, f(x)=ùúáx(1‚àíx),weobtain\nùúÜ=1\nnn‚àí1‚àë\ni=0ln|ùúá‚àí2ùúáxi|, (15.29)\nwherethesumisoveriterations.\nThe code LyapLog.py in Listing 15.2 computes the Lyapunov exponents for the logistic\nmap. In Figures 15.4 and 15.5 we show its output. Note the sign changes in ùúÜwhere the\nFigure 15.4 Fixed point bifurcations\n(top) and Lyapunov coefÔ¨Åcient (bottom)\nfor the logistic map as functions of the\ngrowth rate ùúá. Notice how the Lyapunov\ncoefÔ¨Åcient, a measure of chaos, changes\nabruptly at the bifurcations with\npositive values indicating instabilities.\nx*\nŒª\nŒº1\n0.5\n0\n‚Äì0.5\n34",3362
161-15.4.2 Shannon Entropy.pdf,161-15.4.2 Shannon Entropy,,0
162-15.5.2 PredatorPrey Chaos.pdf,162-15.5.2 PredatorPrey Chaos,"338 15 Nonlinear Population Dynamics\n‚Äì0.400.40.8\n3.5 3.6 3.7\nŒº3.8 3.9 4Lyapunov exponentEntropyFigure 15.5 Shannon entropy (top) and\nLyapunov coefÔ¨Åcient (bottom) for the logistic\nmap. Notice the close relation between the\nthermodynamic measure of disorder (entropy)\nand the nonlinear dynamics measure of chaos\n(Lyapunov).\nsystem becomes chaotic, and the abrupt changes in slope at the bifurcations. (A similar\ncurve is obtained for the fractal dimension of the logistic map, and, indeed the two are\nproportional.)\n15.4.2 Shannon Entropy\nAnothermeasurethatcanindicatechaoticbehavioristheShannonentropy.Entropyisa\nmeasureofuncertainty(garbledsignal)thathasprovenusefulincommunicationtheory\n[Shannon, 1948; Ott, 2002; Gould et al., 2006]. Imagine that an experiment has Npos-\nsible outcomes. If the probability of each is p1,p2,‚Ä¶,pN, with normalization such that‚àëN\ni=1pi=1,thentheShannonentropyisdefinedas\nSSh=‚àíN‚àë\ni=1pilnpi. (15.30)\nIfpi‚â°0,thereisnouncertainty,and SSh=0,asyoumightexpect.Ifall Noutcomeshave\nequalprobability, pi‚â°1‚àïN,weobtaintheexpressionfamiliarfromstatisticalmechanics,\nSSh=lnN.\nThe code Entropy.py in Listing 15.3 computes the Shannon entropy for the logistic\nmap as a function of the growth parameter ùúá. The results (Figure 15.5, top) are seen to\nbe quite similar to the Lyapunov exponent, again with discontinuities occurring at the\nbifurcations.\n15.5 Coupled Predator‚ÄìPrey Models‚®Ä\nWehaveseencomplicatedbehaviorarisingfromapopulationmodelinwhichweimposeda\npopulationlimit.Nowweextendthatmodeltodescribecoexistingpredatorandpreypopula-\ntions[Lotka,1925;Volterra,1926 ].\nProblem Calculatewhatitmaytaketocontrolapopulationofpests(prey)byintroducing\npredators.Includeinyourconsiderationstheinteractionbetweenthepopulations,aswell\nasthecompetitionforfoodandthetimeneededforpredation.\n15.5 Coupled Predator‚ÄìPrey Models‚®Ä339\n15.5.1 Lotka‚ÄìVolterra Model\nWe extend the logistic map to the Lotka‚ÄìVolterra model (LVM) that describes coexisting\npredatorandapreypopulationbyintroducinganadditionalpopulation:\np(t)=prey(bug)density ,P(t)=Predatordensity . (15.31)\nIntheabsenceofinteractionsbetweenthespecies,weassumethatthepreypopulation p\nbreedsataper-capitarateof a:\nŒîp\nŒît=ap(Discrete), (15.32)\ndp\ndt=ap,(Continuous )‚áíp(t)=p(0)eat. (15.33)\nHerewegiveboththediscreteandcontinuousversionsofthemodel,andwillworkwith\nthecontinuousmodel,whereweseetheexponentialgrowthexplicitly.However,ifthere\narepredatorsthat‚Äúinteractwith‚Äù(gobbleup)anabundanceofprey,thenthismayaffect\nthepreygrowthrate.Weassumethattheinteraction(gobble)rateisproportionaltotheir\njointprobability:\nInteractionrate =bpP, (15.34)\nwherebis a constant. This leads to a prey growth rate including both predation and\nbreeding:\ndp\ndt=ap‚àíbpP,(LVM-Iforprey) . (15.35)\nIflefttothemselves,predators Pwillbreedandincreasetheirpopulationexponentially.Yet\nwe all need to eat, and if there is no prey around, predators will eat each other (or their\nyoung)ataper-capitamortalityrate m:\ndP\ndt||||mort=‚àímP,‚áíP(t)=P(0)e‚àímt. (15.36)\nHowever,ifwealsoincludethepossibilitythattherearepreyto‚Äúinteractwith‚Äùatarate\nbpP,thepredatorpopulationwillgrowattherate\ndP\ndt=ùúñbpP‚àímP(LVM-Iforpredators) . (15.37)\nHereùúñisaconstantthatmeasurestheefficiencywithwhichpredatorsconvertpreyinter-\nactionsintofood.\nEquations (15.35) and (15.37) are two simultaneous ODEs that define the first LVM\nmodel.Theycanbesolvedafterplacingtheminthestandarddynamicform:\ndy‚àïdt=f(y,t),\ny0=p, f0=ay0‚àíby0y1,\ny1=P, f1=ùúñby0y1‚àímy1.(15.38)\nOur code to solve these equations is PredatorPrey.py in Listing 15.4, with results shown\nin Figure 15.6. On the left, we see two populations that oscillate out of phase with\neach other in time. When there are many prey, the predator population eats them and\ngrows, yet then the predators face a decreased food supply, and so their population\ndecreases;that,inturn,permitsthepreypopulationtogrow,andsoforth.Ontherightin\n340 15 Nonlinear Population Dynamics\nt024\n0 200 400p(t)\nP(t)\nPp\n024\n0 1 2\nFigure 15.6 Left: The time dependencies of the prey population of p(t)(solid curve) and of the\npredator population P(t)(dashed curve) for the Lotka‚ÄìVolterra model. Right: A ‚Äúphase space‚Äù plot of\np(t)versus P(t). The different orbits correspond to different initial populations.\nFigure 15.6, we plot a ‚Äúphase space‚Äù plot of P(t)versus p(t).2A closed orbit in the phase\nspaceplotindicatesalimitcyclethatrepeatsindefinitely.Althoughincreasingtheinitial\nnumberofpredatorsdoesdecreasethemaximumnumberofpestsandkeepstheirbehavior\nincheck,itisnotasatisfactorycontrolsincetheirnumbershowsalargevariation.\n15.5.2 Predator‚ÄìPrey Chaos\nItseemsthatintroducingpreyhaskeptthepredatorpopulationsfrombecomingchaotic.\nMathematical analyses tell us that, in addition to nonlinearity, a system must contain a\nnumberofdegreesoffreedombeforechaoswilloccur.Forapredator‚Äìpreymodel,intro-\nducinganotherspeciesortwomaydothetrick.Andsoweextendourprevioustoinclude\nfourspecies,eachwithpopulation picompetingforthesamefinitesetofresources[Vano\netal.,2006].Thisextends(15.37)to:\ndpi\ndt=aipi(\n1‚àí4‚àë\nj=1bijpj)\n,i=1,4. (15.39)\nHereaiisameasureofthegrowthrateofspecies i,andbijisameasureoftherateatwhich\nspeciesjconsumestheresourcesneededbyspecies i.Sincefourspeciescoversaverylarge\nparameterspace,wesuggestthatyoustartyourexplorationusingthesameparametersthat\nVanoetal.[2006]foundproducechaos:\nai=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\n0.72\n1.53\n1.27‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,bij=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1 1.09 1.52 0\n0 1 0.44 1.36\n2.33 0 1 0.47\n1.21 0.51 0.35 1‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (15.40)\nWith chaotic systems being hypersensitive to exact parameter values, you may want to\nmodifythesesomewhat.Notethattheself-interactionterms bii=1,whichisaconsequence\nof measuring the population of each species in units of its individual carrying capacity.\n2 WediscussphasespaceplotsatlengthinChapter16.\n15.5 Coupled Predator‚ÄìPrey Models‚®Ä341\npipk\npj0.05\n0.70.60.50.40.30.20.60.50.40.30.20.100.150.200.250.35\n0.30\nFigure 15.7 A chaotic attractor for the 4D Lotka‚ÄìVolterra model projected onto three axes.\nWesolve(15.39)withinitialconditionscorrespondingtoanequilibriumpointatwhichall\nspeciescoexist:\npi(t=0)=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£0.3013\n0.4586\n0.1307\n0.3557‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (15.41)\nAnilluminatingwaytovisualizethebehaviorofthissystemwouldbetocreatea4Dphase-\nspace plot, [p1(ti),p2(ti),p3(ti),p4(ti)]fori=1,N,w h e r eNis the number of time steps in\nthenumericalsolution.Thegeometricstructuressocreatedmayhaveasmoothandwell-\ndefinedshape.Unfortunately,wehavenowaytoshowsuchaplot,andsoinitsstead,as\nseeninFigure15.7,weprojectthe4Dstructureonto2Dand3Daxes.Weseeaclassictype\nofchaoticattractor,withthe3Dstructurefoldedoverintoanearly2Dstructure.\nExercise\n1) Visualizethesolutionto(15.39)byplotting p1(t),p2(t),p3(t),andp4(t)versustime.\n2) Constructthe4Dchaoticattractorformedbythesolutionsof(15.39).Outputthevalues\n[p1(ti),p2(ti),p3(ti),p4(ti)],foreachtimestep i.Inordertoavoidneedlesslylongfiles,you\nmaywanttoskipanumberoftimesteps.\na) Plotallpossible2Dphasespaceplots,thatis,plotsof piversuspj,i‚â†j=1‚àí3.\nb) Plotallpossible3Dphasespaceplots,thatis,plotsof piversuspjversuspk.\nNote:youhavetoadjusttheparametersorinitialconditionsslightlytoobtaintruly\nchaoticbehavior.",7237
163-15.5.3 LVM with Prey Limit.pdf,163-15.5.3 LVM with Prey Limit,,0
164-15.5.6 Two Predators One Prey.pdf,164-15.5.6 Two Predators One Prey,"342 15 Nonlinear Population Dynamics\n15.5.3 LVM with Prey Limit\nTheinitialassumptionintheLVMthatpreygrowwithoutlimitintheabsenceofpredators\nisunrealistic.Aswiththelogisticmap,weincludealimitonpreynumbersthataccounts\nfordepletionofthefoodsupplyasthepreypopulationgrows.Accordingly,wemodifythe\nconstantgrowthratefrom atoa(1‚àíp‚àïK),sothatgrowthvanisheswhenthepopulation\nreachesthe carryingcapacityK :\ndp\ndt=ap(\n1‚àíp\nK)\n‚àíbpP,(LVM-II). (15.42)\ndP\ndt=ùúñbpP‚àímP. (15.43)\nThe behavior of this model with prey limitations is shown in Figure 15.8. We see that\nboth populations exhibit damped oscillations as they approach their equilibrium values,\nand that, as hoped for, the equilibrium populations are independent of the initial condi-\ntions.Notehowthephase-spaceplotspiralsinwardtoasinglecloselimitcycleonwhichit\nremains,withlittlevariationinpreynumber.Atlast,thedesiredbiological‚Äúcontrol.‚Äù\n15.5.4 LVM with Predation EfÔ¨Åciency\nAnotherunrealisticassumptionintheoriginalLVMisthatthepredatorsimmediatelyeatall\nthepreywithwhichtheyinteract.Asanyonewhohaswatchedacathuntamouseknows,\npredators also spend their time finding, chasing, killing, eating, and digesting prey. This\nhandlingtime decreasestherateof bpPatwhichpreyareeliminated.Wedefinethe func-\ntionalresponsepaastheprobabilityofonepredatorfindingoneprey.Ifasinglepredator\nspendstime tsearchsearchingforprey,then\npa=btsearchp‚áítsearch=pa\nbp. (15.44)\nIf we callththe time a predator spends handling a single prey, then the effective time a\npredatorspendshandlingapreyis path.Suchbeingthecase,thetotaltime Tthatapredator\nspendsfindingandhandlingasinglepreyis\nT=tsearch+thandling=pa\nbp+path, (15.45)\n‚áípa\nT=bp\n1+bpth, (15.46)\nt0123\n0 200 400Pp\np\nP123\n1 2.2\nFigure 15.8 The Lotka‚ÄìVolterra model including a limit on prey population. Left:S o l i d c u r v e : o f\nprey population p(t); dashed curve: predator population P(t).\npopulation pas a function of predator population P.\n15.5 Coupled Predator‚ÄìPrey Models‚®Ä343\nt0400\n0 400Pp\nPopulation\nt0200\n0t0 400Pp\nFigure 15.9 Lotka‚ÄìVolterra model with predation efÔ¨Åciency and prey limitations. From left to\nright: overdamping, b=0.01; damped oscillations, b=0.1, and limit cycle, b=0.3.\nwherepa‚àïTistheeffective rateofeatingprey.Weseethatasthenumberofprey p‚Üí‚àû,\nthe efficiency in eating them ‚Üí1. We include this efficiency in (15.42)by modifying the\nratebatwhichapredatoreliminatespreyto b‚àï(1+bpth):\ndp\ndt=ap(\n1‚àíp\nK)\n‚àíbpP\n1+bpth,(LVM-III ). (15.47)\nTobestillmorerealisticaboutthepredatorgrowth,wealsoplacealimitonthepredator\ncarryingcapacity,butmakeitproportionaltothenumberofprey:\ndP\ndt=mP(\n1‚àíP\nkp)\n,(LVM-III ). (15.48)\nSolutionsfortheextendedmodel(15.47)and(15.48)areshowninFigure15.9.Observethe\nexistenceofthreedynamicregimesasafunctionof b:\n‚óèSmallb:Nooscillations,nooverdamping,\n‚óèMediumb:Dampedoscillationsthatconvergetoastableequilibrium,\n‚óèLargeb:Limitcycle.\nThetransitionfromequilibriumtoalimitcycleiscalleda phasetransition .\nWefinallyhaveasatisfactorysolutiontoour problem.Althoughthepreypopulationis\nnoteliminated,itcanbekeptfromgettingtoolargeandfromfluctuatingwidely.Nonethe-\nless, changes in the parameters can lead to large fluctuations or to nearly vanishing\npredators.\n15.5.5 LVM Implementation and Assessment\n1) SolveallthreeLVMmodelsusingthefollowingparametervalues:\nModel a b ùúñmK k\nLVM-I 0.2 0.1 1 0.1 0\nLVM-II 0.2 0.1 1 0.1 20\nLVM-III 0.2 0.1 0.1 500 0.2\n2) Foreachofthethreemodels,construct\na) atimeseriesforpreyandpredatorpopulations,\nb) phasespaceplotsofpredator versusprey",3557
165-15.6 Code Listings.pdf,165-15.6 Code Listings,"344 15 Nonlinear Population Dynamics\n3)LVM-I:Computetheequilibriumvaluesforthepreyandpredatorpopulations.Doyou\nthinkthatamodelinwhichthecycleamplitudedependsontheinitialconditionscan\nberealistic?Explain.\n4)LVM-II:Calculatenumericalvaluesfortheequilibriumvaluesofthepreyandpredator\npopulations.Makeaseriesofrunsfordifferentvaluesofpreycarryingcapacity K.Can\nyoudeducehowtheequilibriumpopulationsvarywithpreycarryingcapacity?\n5) Makeaseriesofrunsfordifferentinitialconditionsforpredatorandpreypopulations.\nDothecycleamplitudesdependontheinitialconditions?\n6)LVM-III:Makeaseriesofrunsfordifferentvaluesof bandreproducethethreeregimes\npresentinFigure15.9.\n7) Calculatethecriticalvaluefor bcorrespondingtoaphasetransitionbetweenthestable\nequilibriumandthelimitcycle.\n15.5.6 Two Predators, One Prey\n1) AnotherversionoftheLVMincludesthepossibilitythattwopopulationsofpredators\nP1andP2may‚Äúshare‚Äùthesamepreypopulation p.Investigatethebehaviorofasystem\ninwhichthepreypopulationgrowslogisticallyintheabsenceofpredators:\ndp\ndt=ap(\n1‚àíp\nK)\n‚àí(b1P1+b2P2)p, (15.49)\ndP\ndt=ùúñ1b1pP1‚àím1P1,dP2\ndt=ùúñ2b2pP2‚àím2P2. (15.50)\na) Use the following values for the model parameters and initial conditions:\na=0.2,K=1.7,b1=0.1,b2=0.2,m1=m2=0.1,ùúñ1=1.0,\nùúñ2=2.0,p(0)=P2(0)=1.7,andP1(0)=1.0.\nb) Determinethetimedependenceforeachpopulation.\nc) Varythecharacteristicsofthesecondpredatorandcalculatetheequilibriumpopu-\nlationforthethreecomponents.\nd) Whatisyouranswertothequestion,‚ÄúCantwopredatorsthatsharethesameprey\ncoexist?‚Äù\n15.6 Code Listings\nListing 15.1 Bugs.py Producesthebifurcationdiagramofthelogisticmap.Afullpro-\ngramrequiresfinergrids,ascanoverinitialvalues,andremovalofduplicates.\n# Bugs.py The Logistic m a p\n2\nfromvisual.graph import ‚àó\nm_min = 1.0; m_max = 4.0; step = 0.01\ngraph1 = gdisplay(width=600, height=400, title= ‚ÄôLogistic Map‚Äô ,\\n6 xtitle= ‚Äôm‚Äô, ytitle= ‚Äôx‚Äô, xmax=4.0, xmin=1., ymax=1., ymin=0.)\npts = gdots(shape = ‚Äôround‚Äô, size = 1.5, color = color.green)\nlasty = int(1000 ‚àó0.5) # Eliminates some points\ncount = 0 # Plot every 2 iterations\n10forminarange(m_min, m_max, step):\ny=0 . 5\nforiin range (1,201,1): # Avoid\ny=m ‚àóy‚àó(1‚àíy)\n14foriin range (201,402,1):\n15.6 Code Listings 345\ny=m ‚àóy‚àó(1‚àíy)\nforiin range (201, 402, 1): # Avoid transients\noldy=int(1000 ‚àóy)\n18 y=m ‚àóy‚àó(1‚àíy)\ninty = int(1000 ‚àóy)\nifinty != lasty andcount%2 == 0:\npts.plot(pos=(m,y)) # Avoid repeats\n22 lasty = inty\ncount += 1\nListing 15.2 LyapLog.py ComputesLyapunovcoefficientforthebifurcationplotofthe\nlogisticmapasafunctionofgrowthrate.Notethefinenessofthe ùúágrid.\n# LyapLog . py : Lyapunov coef for logistic map\n3fromvisual.graph import ‚àó\nm_min = 3.5; m_max = 4.5; step = 0.25\ngraph1 = gdisplay( title = ‚ÄôLyapunov coef (blue) for LogisticMap (red)‚Äô ,\n7 xtitle = ‚Äôm‚Äô, ytitle = ‚Äôx , Lyap‚Äô ,\nxmax=5.0, xmin=0, ymax = 1.0, ymin = ‚àí0.6)\nfunct1 = gdots(color = color.red)\nfunct2 = gcurve(color = color.yellow)\n11forminarange(m_min, m_max, step): # m loop\ny=0 . 5\nsuma = 0.0\nforiin range (1, 401, 1): y = m ‚àóy‚àó(1‚àíy) # Skip transients\n15foriin range (402, 601, 1):\ny=m ‚àóy‚àó(1‚àíy)\nfunct1.plot(pos = (m, y) )\nsuma = suma + log( abs(m‚àó(1.‚àí2.‚àóy) )) # Lyapunov\n19funct2.plot(pos = (m, suma/401) ) # Normalize\nListing 15.3 Entropy.py ComputestheShannonentropyforthelogisticmapasafunc-\ntionofgrowthparameter ùúá.\n# Entropy.py Shannon Entropy with Logistic map using Tkinter\n3try:fromtkinter import ‚àó\nexcept:fromTkinter import ‚àó\nimportmath\nfromnumpyimportzeros, arange\n7\nglobalXwidth, Yheight\nroot = Tk( ); root.title( ‚ÄôEntropy versus mu ‚Äô )\nmumin = 3.5; mumax = 4.0; dmu = 0.25; nbin = 1000; nmax = 100000\n11prob = zeros( (1000), float)\nminx=mumin; maxx=mumax; miny=0; maxy=2.5; Xwidth=500; Yheight=500\nc = Canvas(root, width = Xwidth, height = Yheight) # Init canvas\nc.pack() # Pack canvas\n15Button(root , text = ‚ÄôQuit‚Äô, command = root . quit) .pack() # To quit\ndefworld2sc(xl, yt, xr, yb): #x‚àíleft , y ‚àítop , x ‚àíright , y ‚àíbottom\nmaxx = Xwidth # canvas width _________________________\n19maxy = Yheight # canvas height | | |tm |\nlm = 0.10 ‚àómaxx # left margin | ___ | ____ | _______ ___ |\nrm = 0.90 ‚àómaxx # right margin |lm | | | |\nbm = 0.85 ‚àómaxy # bottom margin | ___ | | | |\n23tm = 0.10 ‚àómaxy # top margin |__ |__| ____________ | |\nmx = (lm ‚àírm)/( xl ‚àíxr) # || b m r m ||\nbx = (xl ‚àórm‚àíxr‚àólm)/(xl ‚àíxr) # | |__| ____________ | |\nmy = (tm ‚àíbm) /( yt ‚àíyb) #| |\n27by = (yb ‚àótm‚àíyt‚àóbm) /(yb ‚àíyt) # | _______________________ |\nlinearTr = [mx, bx, my, by]\nreturnlinearTr # returns\n346 15 Nonlinear Population Dynamics\n31# Plot y , x , axes ; world coord converted to canvas coordinates\ndefxyaxis(mx, bx, my, by): # to be called after call workd2sc\nx1 = (int)(mx ‚àóminx + bx) # minima and maxima converted to\nx2 = (int)(mx ‚àómaxx + bx) # canvas coordinades\n35y1 = (int)(my ‚àómaxy + by)\ny2 = (int)(my ‚àóminy + by)\nyc = (int)(my ‚àó0.0 + by)\nc.create_line(x1, yc, x2, yc, fill = ""red"") #xa x i s\n39c.create_line(x1, y1, x1, y2, fill = ‚Äôred‚Äô) #y‚àíaxis\nforiin range (7): #xt i c s\nx=m i n x+( i ‚àí1)‚àó0.1 # world coordinates\nx1 = (int)(mx ‚àóx+b x ) # canvas coord\n43 x2 = (int)(mx ‚àóminx + bx)\ny=m i n y+i ‚àó0.5 # real coordinates\ny2 = (int)(my ‚àóy+b y ) # canvas coords\nc.create_line(x1, yc ‚àí4, x1, yc + 4, fill = ‚Äôred‚Äô) #t i c sx\n47 c.create_line(x2 ‚àí4, y2, x2 + 4, y2, fill = ‚Äôred‚Äô) #t i c sy\nc.create_text(x1 + 10, yc + 10, text = ‚Äô%5.2f‚Äô%( x ), \\nfill = ‚Äôred‚Äô, anchor = E) #xa x i s\nc.create_text(x2 + 30, y2, text = ‚Äô%5.2f‚Äô%( y ) ,f i l l= ‚Äôred‚Äô,\\n51 anchor = E) #ya x i s\nc.create_text(70, 30, text = ‚ÄôEntropy‚Äô ,f i l l= ‚Äôred‚Äô, anchor = E)\nc.create_text(420, yc ‚àí10, text = ‚Äômu‚Äô,f i l l= ‚Äôred‚Äô, anchor = E)\n55mx, bx, my, by = world2sc(minx, maxy, maxx, miny) # returns list\nxyaxis(mx, bx, my, by) #a x e sv a l u e s\nmu0 = mumin ‚àómx + bx\nentr0 = my ‚àó0.0 + by\n59formuinarange(mumin, mumax, dmu): # m u loop\nprint(mu)\nforjin range (1, nbin):\nprob[j] = 0\n63y= 0 . 5\nfornin range (1, nmax + 1):\ny=m u ‚àóy‚àó(1.0‚àíy) # Logistic m a p, Skip transients\nif(n > 30000):\n67 ibin =int(y‚àónbin) + 1\nprob[ibin] += 1\nentropy = 0.\nforibinin range (1, nbin):\n71 if(prob[ibin]>0):\nentropy = entropy ‚àí(prob[ibin]/nmax) ‚àómath.log10(prob[ibin]/nmax)\nentrpc = my ‚àóentropy + by # entropy to canvas coords\nmuc = mx ‚àómu + bx # m u to canvas coords\n75c.create_line(mu0, entr0, muc, entrpc, width = 1, fill = ‚Äôblue‚Äô)\nmu0 = muc #b e g i nv a l u e sf o rn e x tl i n e\nentr0 = entrpc\nroot.mainloop() # m a k e s effective events\nListing 15.4 PredatorPrey.py Computespopulationdynamicsforagroupofinteracting\npredatorsandprey.\n# PredatorPrey .py: Lotka ‚àíVolterra models\nfromvisualimport ‚àó\n4fromvisual.graph import ‚àó\nTmin = 0.0\nTmax = 500.0\n8y=z e r o s (( 2 ), float)\nNtimes = 1000\ny[0] = 2.0\ny[1] = 1.3\n12h = (Tmax ‚àíTmin)/Ntimes\nt=T m i n\ndeff( t, y, F): # Modify this function for your problem\n15.6 Code Listings 347\n16 F[0] = 0.2 ‚àóy[0] ‚àó(1‚àí(y[0]/(20.0) )) ‚àí0.1‚àóy[0] ‚àóy[1]\nF[1] = ‚àí0.1‚àóy[1] + 0.1 ‚àóy[0] ‚àóy[1];\ndefrk4(t, y, h, Neqs): #r k 4m e t h o d , D O N O Tm o d i f y\n20F=z e r o s( ( N e q s ), float)\nydumb = zeros((Neqs) , float)\nk1 = zeros((Neqs) , float)\nk2 = zeros((Neqs) , float)\n24k3 = zeros((Neqs) , float)\nk4 = zeros((Neqs) , float)\nf(t, y, F)\nforiin range (0, Neqs):\n28 k1[i] = h ‚àóF[i]\nydumb[i] = y[i] + k1[i]/2.\nf(t +h/2., ydumb, F)\nforiin range (0, Neqs):\n32 k2[i] = h ‚àóF[i]\nydumb[i] = y[i] + k2[i]/2.\nf(t +h/2., ydumb, F)\nforiin range (0, Neqs):\n36 k3[i] = h ‚àóF[i]\nydumb[i] = y[i] + k3[i]\nf(t +h, ydumb, F)\nforiin range (0, Neqs):\n40 k4[i] = h ‚àóF[i]\ny[i] = y[i] + (k1[i] + 2. ‚àó(k2[i] + k3[i]) + k4[i])/6.\ngraph1 = gdisplay(x= 0,y= 0, width = 500, height = 400, \\n44 title = ‚ÄôPrey p(green) and predator P(yellow) vs time‚Äô ,xtitle = ‚Äôt‚Äô,\\nytitle = ‚ÄôP, p‚Äô,xmin=0,xmax=500,ymin=0,ymax=3.5)\nfunct1 = gcurve(color = color.yellow)\nfunct2 = gcurve(color = color.green)\n48graph2 = gdisplay(x= 0,y= 400, width = 500, height = 400,\ntitle = ‚ÄôPredator P vs prey p‚Äô ,\nxtitle = ‚ÄôP‚Äô, ytitle = ‚Äôp‚Äô,xmin=0,xmax=2.5,ymin=0,ymax=3.5)\nfunct3 = gcurve(color = color.red)\n52\nfortinarange(Tmin, Tmax + 1, h):\nfunct1.plot(pos = (t, y[0]) )\nfunct2.plot(pos = (t, y[1]) )\n56funct3.plot(pos = (y[0], y[1]) )\nrate(60)\nrk4(t, y, h, 2)",8218
166-Chapter 16 Nonlinear Dynamics of Continuous Systems.pdf,166-Chapter 16 Nonlinear Dynamics of Continuous Systems,,0
167-16.1.1 Free Pendulum Oscillations.pdf,167-16.1.1 Free Pendulum Oscillations,"348\n16\nNonlinear Dynamics of Continuous Systems\nIn Chapter 15 we explored the complex dynamics and chaos that occur in population models.\nIn this chapter we explore physical systems that exhibit chaos, and, in particular, the driven\nrealistic pendulum. The focus is on understanding chaos and using phase space to uncover\nthe simplicity underlying complex behaviors.\n16.1 The Chaotic Pendulum\nProblem Computethemotionofadrivenpendulumwithnorestrictionsonthemagni-\ntudeofthedisplacements.\nAlthoughtheplanependulumisaclassicsubjectforphysics,itisusuallystudiedforsmall\ndisplacements.Thatmaybeokayforlargegrandfatherclocks,butnotforthischapter.We\nwilllookata chaoticpendulum ,i.e.,onewithfrictionandadrivingtorque(Figure16.1left)\nandwithnorestrictiontosmalldisplacements[Rasband,1990].Newton‚Äôslawsofrotational\nmotiontellusthatthesumofthegravitationaltorque ‚àímglsinùúÉ,thefrictionaltorque ‚àíùõΩÃáùúÉ,\nandtheexternaltorque ùúè0cosùúîtequalsthemomentofinertiaofthependulumtimesits\nangularacceleration:\n‚àímglsinùúÉ‚àíùõΩdùúÉ\ndt+ùúè0cosùúît=Id2ùúÉ\ndt2, (16.1)\n‚áí‚àíùúî2\n0sinùúÉ‚àíùõºdùúÉ\ndt+fcosùúît=d2ùúÉ\ndt2, (16.2)\nùúî0=mgl\nI,ùõº=ùõΩ\nI,f=ùúè0\nI. (16.3)\nEquation (16.2) is a second-order, time-dependent, nonlinear differential equation. The\nnonlinearityarisesfromthesin ùúÉdependenceofthegravitationaltorque.Theconstant ùúî0\nis the natural frequency of oscillationsfor small displacements, with onlya gravitational\ntorque.Theparameter ùõºisameasureofthestrengthoffriction,andtheparameter fisa\nmeasureofthestrengthoftheexternaldrivingtorque.InourstandardODEform, dy‚àïdt=f\nofChapter8,wehavetwo,simultaneous,first-orderequations:\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH",1747
168-16.2 Phase Space.pdf,168-16.2 Phase Space,"16.1 The Chaotic Pendulum 349\ndy(0)\ndt=y(1), (16.4)\ndy(1)\ndt=‚àíùúî2\n0siny(0)‚àíùõºy(1)+fcosùúît,\ny(0)=ùúÉ(t),y(1)=dùúÉ(t)\ndt. (16.5)\n16.1.1 Free Pendulum Oscillations\nIfweignorefrictionandtheexternaltorque,Newton‚Äôslaw(16.2)takesthesimple,yetnon-\nlinearform:\nd2ùúÉ\ndt2=‚àíùúî2\n0sinùúÉ. (16.6)\nIfthedisplacementsaresmall,wecanapproximatesin ùúÉbyùúÉandobtainthefamiliarlinear\nequationofsimpleharmonicmotionwithfrequency ùúî0:\nd2ùúÉ\ndt2‚âÉ‚àíùúî2\n0ùúÉ‚áíùúÉ(t)=ùúÉ0sin(ùúî0t+ùúô). (16.7)\nInChapter8,westudiedhownonlinearitiesproduceanharmonicoscillations,and,indeed,\n(16.6)isanothergoodcandidateforsuchstudies.Asinthatchapter,weexpectsolutions\nof(16.6)tobeperiodic,butwithafrequency ùúîthatequals ùúî0onlyforsmalloscillations.\nFurthermore,becausetherestoringtorque, mglsinùúÉ‚âÉmgl(ùúÉ‚àíùúÉ3‚àï3),islessthanthe mglùúÉ\nassumedinaharmonicoscillator,realisticpendulumsswingslower(havelongerperiods)\nastheirangulardisplacementsaremadelarger.\n16.1.2 Analytic Solution as Elliptic Integrals\nTheanalyticsolutiontotherealisticpendulumisastandardtextbookproblem[Landauand\nLifshitz,1976;MarionandThornton,2019;Scheck,2010].However,itisaratherlimited\nsolutionasitisonlyfortheperiod T,anditisintermsofintegralsthatmustbeevaluated\nnumerically.Thissolutionisbasedonenergybeingaconstant(integral)ofthemotion,and\nisforthependulumreleasedfromrestatitsmaximumdisplacement ùúÉm,whereitsenergy\nisallpotential(Figure16.1):\nE=PE(t=0)=mgl‚àímglcosùúÉm=2mglsin2(\nùúÉm\n2)\n. (16.8)\nf\nm m2m1\nŒ∏\nŒ∏2Œ∏1\nŒ±ll1\nl2\nFigure 16.1 Left: A pendulum of length ldriven through resistive air (dotted arcs) by an external\nsinusoidal torque (semicircle). The strength of the external torque is given by fand that of air\nresistance by ùõº.Right: A double pendulum (Section 16.4.1) with neither air resistance nor a driving\nforce. In both cases, there is a gravitational torque and the possibility of chaos. (The study of the\nsingle pendulum to follow can be replaced by the study of the double pendulum.)\n350 16 Nonlinear Dynamics of Continuous Systems\nYet,because E=KE+PEisaconstant,wecanexpresstheenergyforanyvalueof ùúÉ,and\nthengoontosolvefortheperiod:\n2mglsin2ùúÉm\n2=1\n2I(\ndùúÉ\ndt)2\n+2mglsin2ùúÉ\n2,\n‚áídùúÉ\ndt=2ùúî0[\nsin2ùúÉm\n2‚àísin2ùúÉ\n2]1‚àï2\n‚áídt\ndùúÉ=T0‚àïùúã\n[sin2(ùúÉm‚àï2)‚àísin2(ùúÉ‚àï2)]1‚àï2,\n‚áíT\n4=T0\n4ùúã‚à´ùúÉm\n0dùúÉ\n[sin2(ùúÉm‚àï2)‚àísin2(ùúÉ‚àï2)]1‚àï2, (16.9)\n‚áíT‚âÉT0[\n1+(\n1\n2)2\nsin2ùúÉm\n2+(\n1‚ãÖ3\n2‚ãÖ4)2\nsin4ùúÉm\n2+¬∑¬∑¬∑]\n. (16.10)\nBecausethemotionisperiodic,wehaveassumedthatittakes T‚àï4forthependulumtotravel\nfromùúÉ=0toùúÉ=ùúÉm.Theintegralin(16.9)canbeexpressedasan ellipticintegralofthefirst\nkind.Ifyouthinkofanellipticintegralasageneralizationofatrigonometricfunction,then\nthisisaclosed-formsolution;otherwise,it‚Äôsanintegralneedingcomputation.Theseries\nexpansionoftheperiod(16.10)isobtainedbyexpandingthedenominatorandintegrating\nittermbyterm.Ittellsus,forexample,thatanamplitudeof80‚àòleadstoaperiod10 %longer\nthanthesmall ùúÉperiod.Wewilldeterminetheperiodcomputationallywithouttheneed\nforanyexpansions.\n16.1.3 Free Pendulum Implementation and Test\nAsapreliminarytothesolutionofthefullequation(16.2),modifyyour rk4programtosolve\n(16.6)forthefreeoscillationsofarealisticpendulum.\n1) Start your pendulum at ùúÉ=0 withÃáùúÉ(0)‚â†0. Gradually increase ÃáùúÉ(0)to increase the\nimportanceofnonlineareffects.\n2) Testyourprogramforthelinearcase(sin ùúÉ‚ÜíùúÉ)andverifythat:\na) Yoursolutionisharmonicwithfrequency ùúî0=2ùúã‚àïT0,andthat,\nb) Thefrequencyofoscillationisindependentoftheamplitude.\n3) Deviseanalgorithmtodeterminetheperiod Toftheoscillationbycountingthetimeit\ntakesforthreesuccessivepassesoftheamplitudethrough ùúÉ=0.(Youneed threepasses\nto handlethecase wherethe oscillationis notsymmetricaboutthe origin.)Test your\nalgorithmforsimpleharmonicmotionwhereyouknow T0.\n4) For the realistic pendulum, observe the change in period as a function of increasing\ninitialenergy.Plotyourobservationsalongwith(16.10).\n5) Verify that as the initial KEapproaches 2 mgl, the motion remains oscillatory but not\nharmonic.\n6) AtE=2mgl(theseparatrix),themotiontransitionsfromoscillatorytorotational(‚Äúover\nthe top‚Äù or ‚Äúrunning‚Äù). See how close you can get to the separatrix and to its infinite\nperiod.\n7)‚äôConvert your numerical data to sound and listen to the difference between har-\nmonicmotion(boring)andanharmonicmotioncontainingovertones(interesting).In\nFigure16.2,weshowsometypicalresults.\n16.2 Phase Space 351\nFigure 16.2 T h ed a t as c r e e n( left) and the output screen ( right) of the applet HearData that\nconverts data into sounds. Columns of [ ti,x(ti)] data are pasted into the data window, processed\ninto the graph in the output window, and then converted to sound data that are played by Java.\n(Applets have now been outlawed.)\n16.2 Phase Space\nTheconventionalsolutiontoanequationofmotionistheposition x(t)andthevelocity ùë£(t)\nasfunctionsoftime.Oftenbehaviorsthatappearcomplicatedasfunctionsoftimeappear\nas familiar-looking geometric figures when viewed in the more abstract phasespace .F o r\nthependulum,thephasespaceordinateisthevelocity ùë£(t),andtheabscissaistheposition\nx(t)(Figure16.3,top).Furthermore,themotionofacomplexsystem,whenviewedinphase\nspaceovertime,oftendisplaysamovementback-and-forthfromonephasespacestructure\ntoanother,abehaviorknownas strangeattraction .Thisiseasytounderstand.Forexample,\nthe position and velocity of a free harmonic oscillator are given by the trigonometric\nfunctions\nx(t)=Asin(ùúît),ùë£(t)=dx\ndt=ùúîAcos(ùúît). (16.11)\nPendulum\nfalls\nback  Rotating solutions\nPendulum\nstarts\nrotating \n‚Äì4 V(Œ∏)‚Äì202\n‚Äì 2 02468 1 0\nŒ∏Œ∏‚Ä¢\nFigure 16.3 Top: Phase space trajectories for a pendulum including ‚Äúover the top‚Äù motions. At the\nbottom of the Ô¨Ågure is shown the corresponding ùúÉdependence of the potential.\n352 16 Nonlinear Dynamics of Continuous Systems\nWhensubstitutedintothetotalenergy,weobtaintwoimportantresults:\nE=KE+PE=(\n1\n2m)\nùë£2+(\n1\n2ùúî2m2)\nx2(16.12)\n=mùúî2A2\n2cos2(ùúît)+mùúî2A2\n2sin2(ùúît)=1\n2mùúî2A2. (16.13)\nEquation(16.12),beingthatofanellipsein x-ùë£space,indicatesthattheharmonicoscillator\nfollowsclosedellipticalorbitsinphasespace,withthesizeoftheellipseincreasingwiththe\nsystem‚Äôsenergy.Equation(16.13)provesthatthetotalenergyisaconstantofthemotion.\nDifferent initial conditions having the same energy start at different places on the same\nellipse,yettransversethesameorbits.\nHerearesomephasespacestructuresandbehaviorsthatyouwillbeaskedtoobservein\nyoursimulations:\nEllipses:The orbitsofanharmonicoscillationswillstill be ellipse-like,but withangular\ncornersthatbecomemoredistinctwithincreasingnonlinearity(Figure16.4right).\nClosedfigures: LikethoseinFigures16.3and16.4,describeperiodic(notnecessarilyhar-\nmonic)oscillationswiththesame (x,ùë£)occurringagainandagain.Therestoringforce\nleadstoclockwisemotion.\nOpenorbits: LikethoseinFigures16.3and16.4left,theycorrespondtononperiodicor\n‚Äúrunning‚Äùmotion(apendulumrotatinglikeapropeller).Regionswherethepotentialis\nrepulsivealsoleadtoopentrajectoriesinphasespace.\nSeparatrix: As seen on top of Figure 16.3, this is an orbit in phase space that separates\nopenandclosedorbits.Motionontheseparatrixisindeterminate,asthependulummay\nbalance,ormoveeitherwayatthemaximumpotential.\nNon-crossingorbits: Becausesolutionsfordifferentinitialconditionsareunique,differ-\nentorbitsdonotcross.Yetdifferentinitialconditionscancorrespondtodifferentstarting\npositionsalongasingleorbit.\nHyperbolicpoints: Openorbitsintersectatpointsofunstableequilibriumcalled hyper-\nbolicpoints (Figure16.4left),atwhichpointanindeterminacyresults.\nx xx v(t) v(t) v(t)V(x) V(x) V(x)\nx(t) x(t)x(t)E1\nE1E1E1E2\nE2E3\nE3\nFigure 16.4 Three potentials and their behaviors in phase space. The different orbits below the\npotentials correspond to different energies, as indicated by the limits of maximum displacements\nwithin the potentials (dashed lines). Left: A repulsive potential leads to open orbits. The central\ncrossing is an unstable hyperbolic point. Middle : The symmetric harmonic oscillator potential leads\nto symmetric ellipses. Right: A nonharmonic oscillator for small oscillations producing structures\nthat are neither ellipses nor symmetric.\n16.2 Phase Space 353\nFigure 16.5 Position versus time and position\nversus velocity for two initial conditions of a chaotic\npendulum that ends up with the same limit cycle.\n(W. Hager.)\nx\ntv\nx\nFixedpoints: Theinclusionoffrictionmaycausetheenergyinasystemtodecreasewith\ntime,leadingtophase-spaceorbitsthatspiralintoasingle fixed-point.However,ifthere\nisanexternaldrivingforce,thenthesystemwouldmoveawayfromthefixedpoint.\nLimitcycle: Iftheparametersarejustright,aclosedellipse-likefigurecalleda limitcycle\nmayoccur(Figure16.5right).Here,theaverageenergyputintothesystemduringone\nperiodexactlybalancestheaverageenergydissipatedbyfrictionduringthatperiod:\n‚ü®fcosùúît‚ü©=‚ü®\nùõºdùúÉ\ndt‚ü©\n=‚ü®\nùõºdùúÉ(0)\ndtcosùúît‚ü©\n‚áíf=ùõºdùúÉ(0)\ndt. (16.14)\nWhileasystemmaymoveintoalimitcycle,itmayalsomakesporadicjumpsfromone\nlimitcycletoanother.\nPredictableattractors: Theorbits,suchasfixedpointsandlimitcycles,intowhichthe\nsystemsettlesorreturnstooften,andthatarenotparticularlysensitivetoinitialcondi-\ntions.Ifyourlocationinphasespaceisnearapredictableattractor,ensuingtimeswill\nbringyoutoit.\nStrangeattractors: Well-defined,yetcomplicated,semiperiodicbehaviorsthatappearto\nbe uncorrelated with the motion at an earlier time. These are distinguished from pre-\ndictableattractors by being fractal (Chapter 14) and highly sensitive to the initial con-\nditions[Jos√©andSalatan,1998].Evenaftermillionsofoscillations,themotionremains\nattractedtothem.\nModelocking: Whenthemagnitudeofthedrivingforceislargerthanthatforalimitcycle\n(16.14), the driving force can overpower the natural oscillations, resulting in a steady-\nstatemotionatthefrequencyofthedriver.Whilemodelockingcanoccurforlinearor\nnonlinearsystems,fornonlinearsystemsthedrivingtorquemaylockontoanovertone,\nleadingtoarationalrelationbetweenthedrivingfrequencyandthenaturalfrequency.\nùúî\nùúî0=n\nm,n,m=integers. (16.15)\nRandommotion: Appearsinphasespaceasadiffusecloudfillingtheentireenergetically\naccessibleregion.\nChaotic paths: While periodic motion produces closed figures in phase space, and ran-\ndom motion a cloud, chaotic motion falls someplace in between, with dark or diffuse\nbandsratherthansinglelines(Figure16.7).Thecontinuityoftrajectorieswithinbands\nmeans that the system flows continuously among the different trajectories within the\nband,whichmaywelllookverycomplicatedorchaoticinnormalspace.Theexistence\nof these bands helps explain why the solutions are hypersensitive to the initial condi-\ntionsandparametervalues;theslightestchangeinvaluesmaycausethesystemtoflow\ntonearbytrajectorieswithintheband.\nButterflyeffect: Thehypersensitivityofchaoticweathersystemsmay,theoretically,leadto\ntheweatherpatterninNorthAmericabeingsensitivetotheflappingofabutterfly‚Äôswings\ninSouthAmerica.Althoughthisappearstobecounterintuitivebecauseweknowthat",10863
169-16.3 Chaotic Explorations.pdf,169-16.3 Chaotic Explorations,"354 16 Nonlinear Dynamics of Continuous Systems\nsystemswithessentiallyidenticalinitialconditionsshouldbehavethesame,eventually\nthesystemsdiverge.Asseenontheright,inFigure16.8,theinitialconditionsforboth\npendulumsdifferbyonly1 partin917,andso theinitialpathsinphasespacearethe\nsame.Nonetheless,atjustthetimeshownhere,thependulumsbalanceinthevertical\nposition,andthenonefallsbeforetheother,leadingtodifferingoscillationsanddiffering\nphase-spaceplotsfromthistimeonward.\n16.3 Chaotic Explorations\nA challenge in understanding simulations of the chaotic pendulum (16.4) is that the 4D\nparameterspace (ùúî0,ùõº,f,ùúî)issoimmensethatonlysectionsofitcanbestudiedsystem-\natically. We would expect that sweeping through the driving frequency ùúîshould show\nresonancesandbeating;sweepingthroughthefrictionalforce ùõºshouldshowunderdamp-\ning,criticaldamping,andoverdamping;andsweepingthroughthedrivingtorque fmight\nshowresonancesandmodelocking.Youshouldbeabletoobserveallofthesebehaviorsin\nyoursimulations,althoughtheyaresomewhatmixedtogether.\nStartbytryingtoreproducethebehaviorshowninFigures16.6and16.7. Beware:,because\nthisisapotentiallychaoticsystem,yoursolutionsmaybehighlysensitivetotheexactvalues\noftheinitialconditionsandtothedetailsofyourintegrationroutine.Wesuggestthatyou\nMany cycles 1 cycle 3 cyclesŒ∏(0)  = 0.219 Œ∏(0)  = 0.725 Œ∏(0)  = ‚Äì0.8\n02 0‚Äì10‚Äì2\n‚Äì101\n02\n‚Äì202\n0 0 ‚Äì2 2 ‚Äì8 ‚Äì4 4 00 100 200 0004\n‚Äì4\n‚Äì82\n100 200 00\n‚Äì10\n‚Äì2100 200\n40 0002\n128\n4Y(œâ)Œ∏ vs Œ∏‚Ä¢Œ∏ vs t\n0\n20 40 0 20 40\nFigure 16.6 The position versus time, phase space plot, and Fourier spectrum for a chaotic\npendulum with ùúî0=1,ùõº=0.2,f=0.52, and ùúî=0.666. The three differ only in initial conditions.\n16.3 Chaotic Explorations 355\n‚Äì10‚Äì202‚Äì2‚Äì10\n20\n0\n0 200 400 600\nTimef = 0.54f = 0.52\nŒ∏(t)\nŒ∏(t)\nŒ∏(t)Œ∏(Œ∏)\nŒ∏(Œ∏)0 ‚Äì10 ‚Äì15 200 4000\n02\n01 0 2 0\nFigure 16.7 The behavior of a chaotic pendulum with slightly differing driving forces ( f=0.52,\n0.54). On the left are the phase-space plots and on the right are plot position versus time. For\nf=0.54, there occur the characteristic broadbands of chaos.\nexperiment;startwiththeparametervaluesweusedtoproduceourplots,andthenobserve\ntheeffectsofmaking verysmallchangesintheparametersuntilyouobtaindifferentmodes\nofbehavior.Herearetherecommendedsteps:\n1) Takeyoursolutiontotherealisticpendulumandincludefriction.Runitforavariety\nofinitialconditions,includingover-the-topones.Asnoenergyisfedtothesystem,you\nshouldseespiralsinphasespace.Note,ifyouplotthephase-spacepointsatuniformtime\nsteps,withoutconnectingthem,thenthespacingbetweenthepointsgivesanindication\nofthespeedanddirectionoftravelofthependulum.\n2) Try several small values for the driving torque, but without friction. Verify that you\nobtaindistortedellipsesinphasespace.\n3) Turn friction back on and set the driving torque‚Äôs frequency close to the natural fre-\nquencyùúî0.Searchforbeats.Note,youmayneedtoadjustthemagnitudeandphaseof\nthedrivingtorquetoavoidan impedancemismatch betweenthependulumanddriver\n(beingdriventotherightwhilemovingtotheleft).\n4) Finally,scanthroughfrequencies ùúîofthedrivingtorque,andsearchfornonlinearres-\nonance(itlookslikebeating).\n5)Explorechaos :StartoffwiththeinitialconditionsweusedforFigure16.6:\n(x0,ùë£0)=( ‚àí0.0885,0.8),(‚àí0.0883,0.8),(‚àí0.0888,0.8). (16.16)\nTosavetimeandstorage,youmaywanttousealargertimestepforplottingthanthe\noneusedtosolvethedifferentialequations.\n6) Identifywhichpartsofthephasespaceplotscorrespondtotransients.",3470
170-16.3.2 Chaotic Bifurcations.pdf,170-16.3.2 Chaotic Bifurcations,"356 16 Nonlinear Dynamics of Continuous Systems\nFigure 16.8 Left: The phase space plot for two pendulums with almost exactly the same initial\nconditions. Both arrive at the top (the separatrix), where one goes over the top while the other falls\nback down. Right: The long-term phase space plots for these same pendulums showing dark limit\ncycles.\n7) Ensurethatyoufindthefollowing\na) aperiod-3limitcyclewherethependulumjumpsbetweenthreemajororbits,\nb) arunningsolutionwherethependulumkeepsgoingoverthetop,\nc) chaoticmotioninwhichpathsinthephasespacearecloseenoughtogethertoappear\nasbands.\n8) Look for the ‚Äúbutterfly effect‚Äù (Figure 16.8, left) by starting two pendulums off with\nidenticalpositions,butwithvelocitiesthatdifferby1partin1000.Noticethattheinitial\nmotionsareessentiallyidentical,buteventuallydiverge.\n16.3.1 Phase Space Without Velocities\nImaginethatyouhavemeasuredthedisplacementofasystemasafunctionoftime.Your\nmeasurementsappeartoindicatenonlinearbehaviors,andyouwouldliketoviewthesys-\nteminphasespace,butdon‚Äôthavedataontheconjugatemomentaorvelocity.Amazingly\nenough, a plot of x(t+ùúè)versus x(t)as a function of talso produces a phase space plot\n[Abarbanel etal.,1993].Here ùúèisalagtime,andshouldbechosenassomefractionofa\ncharacteristictimeforthesystem.Whilethismaynotseemlikeavalidphasespaceplot,\nthinkoftheforwarddifferenceapproximationforthevelocity,\nùë£(t)=dx(t)\ndt‚âÉx(t+ùúè)‚àíx(t)\nùúè. (16.17)\nThusplotting x(t+ùúè)versusx(t)issomewhatrelatedtoplotting ùë£(t)versusx(t).\nExercise Createaphasespaceplotfromtheoutputofyourchaoticpendulumbyplotting\nùúÉ(t+ùúè)versusùúÉ(t)foralargerangeof tvalues.Explorehowthegraphschangefordifferent\nvaluesoflagtime ùúè.Compareyourresultstotheconventionalphasespaceplotsyouhad\nobtainedpreviously.",1752
171-16.4 Other Chaotic Systems.pdf,171-16.4 Other Chaotic Systems,"16.3 Chaotic Explorations 357\nFigure 16.9 A bifurcation diagram for the\ndamped pendulum with a vibrating pivot (see\nalso the similar diagram for a double pendulum,\nFigure 16.11). The ordinate is |dùúÉ‚àïdt|,t h e\nabsolute value of the instantaneous angular\nvelocity at the beginning of the period of the\ndriver, and the abscissa is the magnitude of the\ndriving force f. Note that the heavy line results\nfrom the overlapping of points, not from\nconnecting the points (see enlargement in the\ninset).\n01202\nf‚îÇŒ∏(t)‚îÇ\n16.3.2 Chaotic Bifurcations\nWehaveobservedthatachaoticsystemcontainsanumberofdominantfrequencies,and\nthatthesystemtendsto‚Äújump‚Äùfromoneoscillatorymodetoanother.Thisimpliesthatthe\ndominantfrequenciesoccursequentially,andnotsimultaneously,astheydointheFourier\nanalysisoflinearsystems.\nBelow,weoutlinethedetailedstepstoexplorethispossibilityasacomputerexperiment.\nOnestartsbyrecordingtheinstantaneousangularvelocity ÃáùúÉ=dùúÉ‚àïdtofthesolutionatvari-\nousinstancesintime.Bythinkingof dùúÉ‚àïdtasafrequency,weobtainaseriesoffrequencies,\nand,presumably,withthemajorFouriercomponentsoccurringmostoften,asifthesystem\nisattractedtothem.Amazingly,whenascatterplotofthesampled ÃáùúÉ‚Äôsisconstructedasa\nfunctionofthedrivingforce,abifurcationdiagramsimilartothatofthelogisticmap(bugs)\nresults!\nSuchascatterplotisshowninFigure16.9forachaoticpendulumwithavibratingpivot\npoint(incontrasttoourusualvibratingexternaltorque):\nd2ùúÉ\ndt2=‚àíùõºdùúÉ\ndt‚àí(ùúî2\n0+fcosùúît)sinùúÉ. (16.18)\nAnalytic and numerical studies of this system are present in the literature [Landau and\nLifshitz, 1976; DeJong, 1992; Gould et al., 2006]. To obtain the bifurcation diagram in\nFigure16.9:\n1) Usetheinitialconditions ùúÉ(0)=1andÃáùúÉ(0)=1.\n2) Setùõº=0.1,ùúî0=1,ùúî=2,andvary0 ‚â§f‚â§2.25.\n3) To permit transients to die off, wait 150 periods of the driver for each fvalue before\nsampling.\n4) Sample ÃáùúÉfor150timesatthoseinstancesatwhichthedrivingforcepassesthroughzero\n(orwhenthependulumpassesthroughitsequilibriumposition).\n5) Plot150valuesof |ÃáùúÉ|versusf.\n16.3.3 Fourier or Wavelet Analysis\nWehaveseenthatarealisticpendulumexperiencesagravitationalrestoringtorque\nùúèg‚àùsinùúÉ‚âÉùúÉ‚àíùúÉ3\n3!++ùúÉ5\n5!+¬∑¬∑¬∑. (16.19)",2171
172-16.4.3.1 Hard Disk Scattering.pdf,172-16.4.3.1 Hard Disk Scattering,"358 16 Nonlinear Dynamics of Continuous Systems\nThenonlineartermsleadtononharmonicbehaviorinafreependulum.Whenthependu-\nlumisdrivenbyanexternalsinusoidaltorque,itmay modelockwiththedriverandoscillate\natafrequencythatisrationallyrelatedtothedriver‚Äôsfrequency.Consequently,thebehav-\nioroftherealisticpendulumisexpectedtobeacombinationofvariousperiodicbehaviors,\nwithdiscretejumpsbetweenthem(asdiscussedinSection16.3.2).\nInthisassessment,youareaskedtodeterminetheFouriercomponentspresentinsome\nofthependulum‚Äôscomplicatedbehaviors.Youshouldfindathree-cyclestructurecontain-\ningthreemajorFouriercomponents,andafive-cyclestructurewithmorefrequencies.You\nshould also notice that when the pendulum goes over the top, its spectrum contains a\nsteady-state(DC)component.\n1) Dustoffyourprogramforanalyzingsignal y(t)intoFouriercomponents.\n2) Applyyouranalyzertothesolutionofthechaoticpendulumforcaseswherethereare\none-, three-, and five-cycle structures in phase space. Deduce the major frequencies\ncontainedinthesestructures.Waitforthetransientstodieoutbeforeconductingyour\nanalysis.\n3) CompareyourresultstothoseinFigure16.6.\n4) SeeifyoucandeducearelationamongtheFouriercomponents,thenaturalfrequency\nùúî0,andthedrivingfrequency ùúî.\n5) A signal of chaos is a broadband Fourier spectrum, though not necessarily a flat one.\nExamineyoursystemforparametersthatgivechaoticbehavior,andverifythisstatement\nbyplottingthepowerspectrumonbothlinearandsemi-logarithmicplots.\nWaveletExploration WesawinChapter10thatawaveletexpansionismoreappropri-\natethanaFourierexpansionforsignalscontainingcomponentsthatoccurforonlyfinite\nperiodsoftime.Chaoticoscillationsarejustsuchsignals.RepeattheFourieranalysisofthis\nsectionusingwaveletsinsteadofsinesandcosines.Canyoudiscernthetemporalsequence\nofvariouscomponents?\n16.4 Other Chaotic Systems\n16.4.1 The Double Pendulum\nThestudyweoutlinedpreviouslyforthechaoticpendulumcanberepeated,orreplaced,\nbyonefortherealisticdoublependulum.Figures16.1and16.10showsuchasystem.As\nshownontherightofFigure16.1,thedoublependulumhasasecondpendulumconnected\ntothefirst,butnoexternaldrivingforce.Inthiscase,eachpendulumactsasadrivingforce\nfortheother,andsothereareenoughdegreesoffreedomforchaoticbehaviortooccur,even\nwithoutanexternaldrivingforce.\nThe equations of motions for the double pendulum are derived most directly from the\nLagrangianformulationofmechanics.TheLagrangianisfairlysimple,butwiththe ùúÉ1and\nùúÉ2motionsinnatelycoupled:\nL=KE‚àíPE=1\n2(m1+m2)l2\n1ÃáùúÉ12+1\n2m2l2\n2ÃáùúÉ22(16.20)\n+m2l1l2ÃáùúÉ1ÃáùúÉ2cos(ùúÉ1‚àíùúÉ2)+(m1+m2)gl1cosùúÉ1+m2gl2cosùúÉ2.\nTextbooksusuallyapproximatetheseequationsforsmalloscillations,whichdiminishthe\nnonlinear effects, and conclude that the system contains ‚Äúslow‚Äù and ‚Äúfast‚Äù modes. More\n16.4 Other Chaotic Systems 359\nFigure 16.10 Photographs of a double pendulum built by a student after running his simulation\n(there is a video DoublePend.mp4 ). The upper pendulum consists of two separated shafts so that\nboth pendulums can go over their tops. The Ô¨Årst two frames show the pendulum released from rest\nand then moving quickly through various modes. The photographs with a faster shutter speed stop\nthe motion in various stages (R. Landau (Author)).\n00\n4 ‚Äì810\n0\nAngular velocity oflower pendulum\n10\nMass of upper pendulumAngular velocity versus mass\nŒ∏2\nŒ∏2\nFigure 16.11 Left: Phase space trajectories for a double pendulum with m1=10m2and with two\ndominant attractors. Right: A bifurcation diagram for the double pendulum displaying the\ninstantaneous velocity of the lower pendulum as a function of the mass of the upper pendulum.\n(J. Danielson.)\ninterestingmodesresultwhennosmall-angleapproximationsaremade,andwhenthepen-\ndulumsaregivenenoughinitialenergytogooverthetop.OntheleftofFigure16.11,we\nseeseveralphase-spaceplotsofthelowerpendulum‚Äôsmotionfor m1=10m2.Whengiven\nenoughinitialkineticenergytogooverthetop,thetrajectoriesareseentoflowbetween\ntwomajorattractors,withenergybeingtransferredbackandforthbetweenthependulums.\nOntherightofFigure16.11,isabifurcationdiagramforthedoublependulum.Thiswas\ncreatedbysamplingandplottingtheinstantaneousangularvelocity ÃáùúÉ2ofthelowerpendu-\nlum70timesatinstanceswhenthependulumpassedthroughitsequilibriumposition.The\nmassoftheupperpendulum(aconvenientparameter)wasthenchanged,andtheprocess\nrepeated. The resulting structure is fractal with bifurcations in the number of dominant\nfrequenciesinthemotion.AplotoftheFourierorwaveletspectrumasafunctionofmass\nisexpectedtoshowsimilarcharacteristicfrequencies.\n16.4.2 Billiards\nDerivingitsnamefromtheonce-popularparlorgame,amathematical billiardisadynami-\ncalsysteminwhichaparticlemovesfreelyinastraightlineuntilithitsaboundarywall,at\n360 16 Nonlinear Dynamics of Continuous Systems\na, b c, d e\nf\nFigure 16.12 Square (a, c), circular (b, d), Sinai (e), and stadium billiards (f). The arrows are\ntrajectories. The stadium billiard has two semicircles on the ends.\nwhichpointitundergoesspecularreflection,andthencontinuesoninastraightlineuntil\nthenextcollision(Figure16.12).Theconfiningbilliardtablecanbesquare,rectangular,cir-\ncular,polygonal,acombinationofthepreceding,oreventhree-dimensional.Billiardsare\nHamiltoniansystems in whichthere is no loss ofenergy,in whichthe motionscontinue\nendlessly,andwhichmaydisplaychaos.\nIn Figure 16.12, we show square (a, c), circular (b, d), Sinai (e), and stadium billiards\n(f),withthearrowsindicatingpossibletrajectories.Notehowright-anglecollisionsleadto\ntwo-pointperiodicorbitsforbothsquareandcircularbilliards,whilein(c)and(d)wesee\nhow45‚àòcollisionsleadstofour-pointperiodicorbits.Figures(e)and(d)shownonperiodic\ntrajectoriesthatareergodic,thatis,orbitsthatwilleventuallypassthroughallpointsinthe\nallowedspace,andwhichcanbecomechaotic.\n1) Computethetrajectoriesforthesefourbilliards,usingarangeofinitialconditions.In\nListing 16.1, we give a sample program for a square billiard that produces a VPython\nanimation.(InChapter24,wedothequantumversionofthisproblem.)\n2) Plotthedistancebetweensuccessivecollisionpointsasafunctionofcollisionnumber.\nTheplotshouldbesimpleforperiodicmotion,butshouldshowirregularbehaviorasthe\nmotionbecomeschaotic.Keeptrackofhowmanycollisionsoccurbeforechaossetsin\n(typically20‚Äì30).Youneedatleastthismanycollisionstotesthypersensitivitytoinitial\nconditions.\n3) Sincenotallinitialconditionsleadtochaos,especiallyforcircles,youmayneedtoscan\nthroughvariousinitialconditions.\n4) For initial conditions that place you in the chaotic regime, explore the difference in\nbehaviorforarelativelyslight( ‚â§10‚àí3)variationininitialconditions.\n5) Tryinitialconditionsthatdifferatthemachineprecisionleveltogaugejusthowsensi-\ntivechaotictrajectoriesreallyaretoinitialconditions(bepatient).\n16.4.3 Multiple Scattering Centers\nOne expects the scattering of a projectile from a force center to be a continuous process.\nNevertheless,whenthepotentialhasinternalstructureandtheprojectileundergoesmul-\ntipleinternalscatterings,complexbehaviorsmayresult.Wehavealreadyseensomeofthis",7025
173-16.4.4 Lorenz Attractors.pdf,173-16.4.4 Lorenz Attractors,"16.4 Other Chaotic Systems 361\nFigure 16.13 One, two, and three stationary\ndisks on a Ô¨Çat billiard table scatter point particles\nelastically, with some of the internal scattering\nleading to trapped, periodic orbits.R\ninSection13.3.1inourmodelforapinballmachine.Ifyouhavenotsolvedthatproblem\nalready,youmaywanttodosonowandfocusonhowchaosshowsitselfthere.\n16.4.3.1 Hard Disk Scattering\nFigure16.13showsone,two,andthreeharddisksattachedtothesurfaceofa2Dbilliard\ntable.Ineachcase,thedisksscatterpointparticleselastically(noenergyloss).Thedisksall\nhaveradius R,center-to-centerseparations a,withthethree-diskconfigurationformingan\nequilateraltriangle.Alsoshowninthefigureareimaginedtrajectoriesofparticlesscattered\nfromthedisks,withsomeoftheinternalscatteringleadingtotrapped,periodicorbitsin\nwhichtheprojectilebouncesbackandforthendlessly.Forthetwo-diskcase,thereisjust\nasingletrappedorbit,butforthethree-diskcase,thereareinfinitelymany,andthatmay\nleadtochaos.InChapter24,weexplorethequantummechanicalversionofthisproblem.\nListing24.2 3QMdisks.py inthatchapterhasapotentialsubroutinethatcanbeusedtomodel\nthepresentdisks.\n1) ModifytheprogramalreadydevelopedinSection13.3.1forthestudyofscatteringfrom\nthefour-peakedGaussian(13.18)sothatitcanbeappliedtoscatteringfromone,two,\northreedisks.Orwriteanewone.\n2) Because infinite potentials cannot be handled numerically, pick instead a very large\nvalueforthepotential.Theexactvalueshouldnotmatteraslongasit‚Äôslargeenough\nsothatincreasingitsvalueshasnosignificanteffectonthescattering.\n3) Plot a number of trajectories [x(t),y(t)]that show both usual and unusual behaviors.\nInparticular,plotthoseforwhichback-anglescatteringoccurs,and,consequently,for\nwhichtheremusthavebeensignificantnumberofmultiplescatterings.\n4) Plotanumberofphasespacetrajectories [x(t), Ãáx(t)]and[y(t), Ãáy(t)].Howdothesediffer\nfromthoseofboundstates?\n5) Startwithaprojectileat x‚âÉ‚àí ‚àû,thatis,atsomeverylargenumber,andatvariousdis-\ntances (impact parameters) y‚â°bfrom the center of the scattering region. Determine\nthescatteringangle ùúÉ=atan2(Vx,Vy) asafunctionof bbydeterminingthevelocitycom-\nponents of the scattered particle after it has left the interaction region, that is, when\nPE‚àïE‚â§10‚àí10.\n6) Plotthediscontinuitiesin dùúÉ‚àïdbandùúé(ùúÉ):\nùúé(ùúÉ)=||||dùúÉ\ndb||||b\nsinùúÉ(b). (16.21)\n7) Explorethedifferentgeometriesinsearchforchaos.Thisshouldbenear a‚àïR‚âÉ6forthe\nthreedisks.\n16.4.4 Lorenz Attractors\nIn1961,EdwardLorenzwasusingasimplifiedatmosphericconvectionmodeltopredict\nweatherpatterns[Lorenz,1963],andtosavetime,heenteredthedecimal0.506insteadof\n362 16 Nonlinear Dynamics of Continuous Systems\nFigure 16.14 A 3D plot of a Lorenz attractor.\nenteringthefullvalue0.506127foraparameter[Peitgen etal.,1994;MotterandCampbell,\n2013]. The results for the two numbers were so different that at first he thought it to be\nsome kind of numerical error, but in time he realized that this was a nonlinear system\nwithsomeunusualbehaviorthatwenowknowaschaos.Here,weaskyoutorepeathis\ndiscovery.\nWithsimplifiedvariables,theequationusedbyLorenzare\nÃáx=ùúé(y‚àíx), (16.22)\nÃáy=x(ùúå‚àíz)‚àíy, (16.23)\nÃáz=‚àíùõΩz+xy. (16.24)\nHerex(t)isameasureoffluidvelocityasafunctionoftime t,y(t)andz(t)aremeasuresof\nthetemperaturedistributionsintwodirections,and ùúé,ùúå,andùõΩareparameters.Notethat\nthexzandxytermsmaketheseequationsnonlinear.\n1) ModifyyourODEsolvertohandlethethree,simultaneousLorenzequations.\n2) Startwiththeparametervalues ùúé=10,ùõΩ=8‚àï3,andùúå=28.\n3) Makesuretousesmallenoughstepsizessothatgoodprecisionisobtained.Youmust\nhaveconfidencethatyouareseeingchaosandnotnumericalerror.\n4) Makeplotsof xversust,yversust,andzversust,andcomparethemtoFigure16.14.\n5) The initial behavior in these plots are transientsand are not considered dynamically\ninteresting.Leaveoffthesetransientsintheplotstofollow.\n6) Makea‚Äúphasespace‚Äùplotof z(t)versusx(t)(theindependentvariable tdoesnotappear\ninsuchaplot).Thedistorted,numbereight-likefiguresyouobtainaretheLorenzattrac-\ntors,towhichevenchaoticsolutionshaveanaffinity.\n7) Makephasespaceplotsof y(t)versusx(t)andversus z(t).\n8) Makea3Dplotof x(t)versusy(t)andversus z(t).\n9) Theparametersgiventoyoushouldleadtochaoticsolutions.Checkthisclaimbyseeing\nhow small a change you can make in a parameter value and still, eventually, obtain\ndifferentanswers.",4312
174-16.4.5 van der Pool Oscillator.pdf,174-16.4.5 van der Pool Oscillator,,0
175-Chapter 17 Thermodynamics Simulations and Feynman Path Integrals.pdf,175-Chapter 17 Thermodynamics Simulations and Feynman Path Integrals,"16.4 Other Chaotic Systems 363\n16.4.5 van der Pool Oscillator\nThevanderPoolequationdescribesthenonlinearbehaviorinonce-commonobjectssuch\nasvacuumtubesandmetronomes:\nd2x\ndt2+ùúá(x2‚àíx2\n0)dx\ndt+ùúî2\n0x=0. (16.25)\n1) Explainwhy(16.25)describesanoscillatorwith x-dependentdamping.\n2) Createphasespaceplots Ãáx(t)versusx(t).\n3) Verifythatthisequationproducesalimitcyclewithorbitsinternaltothecyclespiraling\nouttothelimitcycle,andthoseexternaltoitspiralingintoit.\n16.4.6 The DufÔ¨Ång Oscillator\nTheDuffingOscillatorisanotherexampleofadamped,driven,nonlinearoscillator.Itis\ndescribedbyadifferentialequation:\nd2x\ndt2=‚àí2ùõædx\ndt‚àíùõºx‚àíùõΩx3+Fcosùúît. (16.26)\nOurcode rk4D uffing.py solvesaformofthisequation.\n1) ModifyyourODEsolvertosolve(16.26).\n2) Startwithparametervaluescorrespondingtoasimpleharmonicoscillator,andverify\nthatyouobtainsinusoidalbehaviorandanellipticalphasespaceplot.\n3) Includeadrivingforce,wait100cyclesinordertoeliminatetransients,andthencreate\naphasespaceplot.Weused ùõº=1.0,ùõΩ=0.2,ùõæ=0.2,ùúî=1.,F=4.0,x(0)=0.009,and\nÃáx(0)=0.\n4) Search for period-three solutions like those in Figure 16.15, where we used ùõº=0.0,\nùõΩ=1.,ùõæ=0.04,ùúî=1.,andF=0.2.\n5) Changeyourparametersto ùúî=1andùõº=0inordertomodelan Uedaoscillator.\n100‚Äì0.6‚Äì0.4‚Äì0.20.00.20.40.4\n0.3\n0.2\n0.1\n0.0\n‚Äì0.1\n‚Äì0.2\n‚Äì0.3\n‚Äì0.4\n‚Äì0.6 ‚Äì0.4 ‚Äì0.2 0.0 0.2 0.4 0.60.6Duffing oscillator Phase diagram duffing oscillator\n120 140 160\nt180 200\nx(t)x(t)\nv(t)\nFigure 16.15 A period three solution for a forced DufÔ¨Ång oscillator. Left:x(t)andRight:ùë£(t)\nversus x(t).\n364 16 Nonlinear Dynamics of Continuous Systems\n16.5 Code Listings\nListing16.1 SqBilliardCM.py Trajectoriesformotiononasquarebilliardtable.\n# SqBillardCM.py: Animated classical billiards on square table\n3fromvisualimport ‚àó\ndt = 0.01; Xo = ‚àí90.; Yo = ‚àí5.4; v = vector(13.,13.1)\nr0 = r= vector(Xo,Yo); eps = 0.1; Tmax = 500; tp = 0\n7scene = display(width=500, height=500, range=120,\\nbackground=color.white, foreground=color.black)\ntable = curve(pos=([( ‚àí100,‚àí100,0),(100, ‚àí100,0),(100,100,0 ),\\n(‚àí100,100,0),( ‚àí100,‚àí100,0)]))\n11ball = sphere(pos=(Xo,Yo,0),color=color.red, radius=3,make_trail=True)\nfortinarange(0,Tmax,dt):\nrate(5000)\n15tp = tp + dt\nr=r 0+v ‚àótp\nif(r.x>= 100orr.x<=‚àí100): # Right and left walls\nv=v e c t o r ( ‚àív.x,v.y,0)\n19 r0 = vector(r.x,r.y,0)\ntp = 0\nif(r.y>= 100orr.y<=‚àí100): # Top and bottom walls\nv=v e c t o r ( v . x , ‚àív.y,0)\n23 r0 = vector(r.x,r.y,0)\ntp = 0\nball.pos = r",2453
176-17.1 An Ising Magnetic Chain.pdf,176-17.1 An Ising Magnetic Chain,"365\n17\nThermodynamics Simulations and Feynman Path Integrals\nThe Ô¨Årst part of this chapter extends the Monte-Carlo techniques studied in Chapter 4,n o w\nto the thermal behavior of a magnetic chain. The second part of this chapter applies the\nMetropolis algorithm, just used in the simulation of thermal behavior, now to Feynman‚Äôs path\nintegral formulation of quantum mechanics. The latter theory, while somewhat advanced for\nundergraduates, provides an unusual view of quantum mechanics, and is the basis for some\nof the most fundamental computations in physics .\n17.1 An Ising Magnetic Chain\nFerromagnetscontainfinitesize domainsinwhichthespinsofalltheatomspointinthe\nsamedirection.Whenanexternalmagneticfieldisappliedtothesematerials,thedifferent\ndomainsalign,andthematerialsbecome‚Äúmagnetized.‚ÄùYet,asthetemperatureisraised,\nthedegreeofmagnetismdecreases,untila phasetransition occursattheCurietemperature,\nandallmagnetizationvanishes.\nProblem Developamodelthatexhibitsthethermalbehaviorofferromagnets.\nConsiderNmagnetic dipoles fixed in place on the links of a linear chain (Figure 17.1).\nBecausetheparticlesarefixed,weneednotworryaboutthesymmetryoftheirwavefunc-\ntion,ortheirpositions,ortheirmomenta.Weassumethattheparticleatsite ihasspinsi,\nwhichiseitherupordown:\nsi‚â°sz,i=¬±1\n2. (17.1)\nAconfigurationofthe Nparticlesisdescribedbyaquantumstatevector:\n||ùõºj‚ü©=||s1,s2,‚Ä¶,sN‚ü©={\n¬±1\n2,¬±1\n2,‚Ä¶}\n,j=1,‚Ä¶,2N. (17.2)\nDuetothespinofeachparticleassumingoneof twovalues,thereare2Ndifferentpossible\nstatesforthe Nparticles.\nTheenergyofthesystemarisesfromtheinteractionofthespinswitheachotherandwith\nanexternalmagneticfield B.Weknowfromquantummechanicsthatanelectron‚Äôsspinand\nmagneticmomentareproportionaltoeachother,soa spin‚Äìspininteractionisequivalent\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n366 17 Thermodynamics Simulations and Feynman Path Integrals\nE = ‚Äì JE = + J Figure 17.1 The 1D lattice of Nspins used in the Ising model of\nmagnetism. The interaction energy between nearest-neighbor pairs\nE=¬±Jis shown for aligned and opposing spins.\nto a magnetic dipole‚Äìdipole interaction. In the simplest version of the model, we assume\nthateachdipoleinteractswiththeexternalmagneticfield,andwithitsnearestneighbor,\nthroughthepotential:\nVi=‚àíJsi‚ãÖsi+1‚àígùúábsi‚ãÖB. (17.3)\nHere, the constants are J,t h eexchange energy ,g, the gyromagnetic ratio, and ùúáb=e‚Ñè‚àï\n(2mec),theBohrmagneton.\nEvenforasmallnumbersofparticles,the2Npossiblespinconfigurationscangettobe\nvery large (220>106), and it is computationally expensive to examine them all. Realistic\nchainsof ‚àº1023particlesarebeyondimagination.Consequently,statisticalapproachesare\nusuallyassumed,evenformoderatevaluesof N.Justhowlarge Nmustbeforstatisticsto\nbevalid,issomethingyouwillbeaskedtoexplorewithyoursimulations.\nTheenergyofthissysteminstate ùõºkistheexpectationvalueofthesumofthepotential\nV,overthespinsofalltheparticles:\nEùõºk=‚ü®\nùõºk|||||‚àë\niVi|||||ùõºk‚ü©\n=‚àíJN‚àí1‚àë\ni=1sisi+1‚àíBùúábN‚àë\ni=1si. (17.4)\nAn apparentparadoxintheIsingmodeloccursifweturnofftheexternalmagneticfield B,\nsincethentherewouldbenopreferreddirectioninspace.Thiswouldimplythattheaver-\nagemagnetizationshouldvanish,despiteourexpectationthatthelowestenergystatewould\nhaveallspinsaligned.Theresolutiontotheparadoxisthatthesystemwith B=0isunsta-\nble;evenifallthespinsarealigned,thereisnothingtostopthespontaneousreversalofall\nthespins.Thisinstabilityleadsto Bloch-walltransitions inwhichregionsofdifferentspin\norientationschangesizespontaneously.Indeed,naturalmagneticmaterialshavemultiple\ndomains with all the spins aligned, but with the different domains pointing in different\ndirections.\nToproceed,weassume B=0,whichleavesjustspin‚Äìspininteractions.However,becog-\nnizant of the fact that this means there is no preferred direction in space, and so, some\ncaremaybeneededwhenaveragingobservablesoverdomains.Forexample,youmayneed\ntotakeanabsolutevalueofthetotalspinwhencalculatingthemagnetization,thatis,to\ncalculate ‚ü®|Œ£isi|‚ü©ratherthan ‚ü®Œ£isi‚ü©.\nThe equilibrium alignment of the spins depends on the sign of the exchange energy J.\nIfJ>0, the lowest energy state will tend to have neighboring spins aligned, and if the\ntemperature is low enough, the ground state will be a ferromagnet .Y e ti fJ<0, the low-\nestenergystatewilltendtohaveneighborswithoppositespins,andifthetemperatureis\nlowenough,thegroundstatewillbea antiferromagnet withalternatingspins.\nThesolutiontothe1DIsingmodelhasitslimitations.Althoughthemodelisaccuratein\ndescribingasysteminthermalequilibrium,itisnotaccurateindescribingthe approachto\nthermalequilibrium,orinpredictingaphasetransitionattheCurietemperature.Also,we\nhavepostulatedthatonlyonespinisflippedatatime,whereasrealmagneticmaterialstend",4832
177-17.1.1 Statistical Mechanics.pdf,177-17.1.1 Statistical Mechanics,,0
178-17.4 Path Integral Quantum Mechanics.pdf,178-17.4 Path Integral Quantum Mechanics,"17.1 An Ising Magnetic Chain 367\ntoflipmanyspinsatatime.Otherlimitationsarestraightforwardtocorrect.Forexample,\nthe addition of long-range interactions rather than just nearest neighbors, the motion of\nthe centers, higher-multiplicity spin states, and extensions to two and three dimensions.\nInfact,the2Dand3Dmodelsdosupportphasetransitions(seeFigure17.4)[Yang,1952].\n17.1.1 Statistical Mechanics\nStatistical mechanicsstartswithelementaryinteractionsamongasystem‚Äôsparticles,and\nconstructsthemacroscopicthermodynamicproperties,suchasspecificheatandmagne-\ntization.Theessentialassumptionisthatallconfigurationsofthesystemconsistentwith\ntheconstraintsarepossible.Insomesimulations,suchasthemoleculardynamiconesin\nChapter18,theproblemissetupsuchthatthe energyofthesystemisfixed.Thestatesof\nthat type of system are described by a microcanonicalensemble . In contrast, for the ther-\nmodynamicsimulationswestudyinthischapter,thetemperature,volume,andnumberof\nparticlesremainfixed,andsowehavea canonicalensemble .\nWhen we say that an object is attemperature T, we mean that the object‚Äôs atoms are\nin thermodynamic equilibrium, and have an average kinetic energy proportional to T.\nAlthoughthismaybeanequilibriumstate,itisalsoadynamiconeinwhichtheobject‚Äôs\nenergy fluctuates as it exchanges energy with its environment. Indeed, one of the most\nilluminating aspects of the simulations to follow is its visualization of the continual and\nrandominterchangeofenergythatoccursatequilibrium.\nTheenergy Eùõºjofstateùõºjinacanonicalensembleisnotconstant,butratherisdistributed\nwithprobabilities P(ùõºj)givenbytheBoltzmanndistribution:\nÓàº(Eùõºj,T)=e‚àíEùõºj‚àïkBT\nZ(T),Z(T)=‚àë\nùõºje‚àíEùõºj‚àïkBT. (17.5)\nHerekBisBoltzmann‚Äôsconstant, Tisthetemperature,and Z(T)isthepartitionfunction,\na weighted sum over the individual statesorconfigurations of the system. Another for-\nmulation,Wang‚ÄìLandausampling(WLS)asdiscussedinSection17.3,insteadsumsover\ntheenergiesof the states of the system with a density-of-states factor g(Ei)[Landau and\nWang,2001].\n17.1.1.1 Analytic Solution\nForverylargenumbersofparticles,onecansolvefortheinternalenergy U=‚ü®E‚ü©ofthe1D\nIsingmodel[PlischkeandBergersen,1994]:\nU\nJ=‚àíNtanhJ\nkBT, (17.6)\n=‚àíNeJ‚àïkBT‚àíe‚àíJ‚àïkBT\neJ‚àïkBT+e‚àíJ‚àïkBT={N,kBT‚Üí0,\n0,kBT‚Üí‚àû.(17.7)\nTheanalyticresultsforthespecificheatperparticleandthemagnetizationare:\nC(kBT)=1\nNdU\ndT=(J‚àïkBT)2\ncosh2(J‚àïkBT), (17.8)\nM(kBT)=NeJ‚àïkBTsinh(B‚àïkBT)\n‚àö\ne2J‚àïkBTsinh2(B‚àïkBT)+e‚àí2J‚àïkBT. (17.9)\n368 17 Thermodynamics Simulations and Feynman Path Integrals\nThe2D Ising model alsohasananalyticsolution,whichisnoteasytoderive[Yang,1952;\nHuang,1987].Whereasinternalenergyandheatcapacityareexpressedintermsofelliptic\nintegrals,thespontaneousmagnetizationperparticlehasthesimpleform:\nÓàπ(T)=‚éß\n‚é™\n‚é®\n‚é™‚é©0, T>Tc,\n(1+z2)1‚àï4(1‚àí6z2+z4)1‚àï8\n‚àö\n1‚àíz2,T<Tc,(17.10)\nkTc‚âÉ2.269185J,z=e‚àí2J‚àïkBT, (17.11)\nwherethetemperatureismeasuredinunitsoftheCurietemperature Tc.\n17.2 Metropolis Algorithm\nWhen tryingtounderstandanalgorithmthatsimulatesthermalequilibrium,itisimportant\ntokeepinmindthattheBoltzmanndistribution(17.5)doesnotrequireasystemtoalways\nproceedtoitslowestenergystate;instead,itjustrequiresittobelesslikelytobefoundin\nahigherenergystatethanalowerenergyone.Ofcourse,as T‚Üí0,onlythelowestenergy\nstate will be populated, but at finite temperatures, we expect the energy to fluctuate on\nthe order of kBTabout the equilibrium energy, with the system sometimes moving to a\nhigher-energystate.\nIn their simulation of neutron transmission through matter, Metropolis et al. [1953]\ndevised an algorithm to improve the Monte Carlo calculation of averages. Because the\nsequence of configurationsthat their Metropolisalgorithm produces accurately simulates\nthe fluctuations occurring during thermal equilibrium, the algorithm has become a\ncornerstoneofcomputationalphysics.\nOnecanviewtheMetropolisalgorithmasacombinationofthevariancereductiontech-\nniquediscussedinSection5.7,andtheVonNeumannrejectiontechnique(stonethrowing)\ndiscussed in Section 5.8. There, we showed how to make Monte Carlo integration more\nefficientbysamplingrandompoints,predominantlywheretheintegrandislarge,andhow\ntogeneraterandompointsweightedbyanarbitraryprobabilitydistribution[nowtobethe\nBoltzmannfunction].\nWewantanapproachthatflipsspinsrandomly,thatequilibratesrapidly,andthatpro-\nducesaBoltzmanndistributionofenergiesintheend:\nÓàº(Eùõºj,T)‚àùe‚àíEùõºj‚àïkBT. (17.12)\nTheprocedurestartswiththesystematafixedtemperatureandanarbitraryinitialspincon-\nfiguration.ItthenappliestheMetropolisalgorithmuntilathermalequilibriumisreached.\nThecontinuedapplicationofthealgorithm(typically10 NtimesforNparticles)generates\nstatistical fluctuations about the equilibrium, from which the thermodynamic quantities\narededuced.Then,inordertodeducethetemperaturedependenceofthethermodynamic\nquantities,thetemperatureischangedandtheprocessisrepeated.Explicitly:\n1) Startwithanarbitraryspinconfiguration ùõºk={s1,s2,‚Ä¶,sN}.(Theequilibriumconfig-\nurationshouldbeindependentoftheinitialdistribution.)\n‚óèA‚Äúhot‚Äùstarthasrandomvaluesforthespins.\n‚óèA‚Äúcold‚Äùstarthasallspinsparallel( J>0),orantiparallel( J<0).\n17.2 Metropolis Algorithm 369\n2) Generateatrialconfiguration ùõºtrby:\n(a) pickingaparticle irandomly,and\n(b) flippingitsspin.\n3) Calculatetheenergy Eùõºtrofthetrialconfiguration.\n4) IfEùõºtr‚â§Eùõºk,acceptthetrialbysetting ùõºk+1=ùõºtr.\n5) IfEùõºtr>Eùõºk,acceptwithprobability Óàæ=Óàºtr‚àïÓàºi=exp(‚àíŒîE‚àïkBT).Todothat:\n(a) Chooseanotheruniformrandomnumber0 ‚â§ri‚â§1.\n(b) Setùõºk+1={ùõºtr,ifÓàæ‚â•rj(accept),\nùõºk,ifÓàæ<rj(reject).\n(c) If the trial configuration is rejected, the next configuration is identical to the\nprecedingone.\n17.2.1 Metropolis Exercise\n1) WriteaprogramthatimplementstheMetropolisalgorithm,thatis,thatproducesanew\nconfiguration ùõºk+1from the present configuration ùõºk. (Alternatively, use the program\nIsingViz.py giveninListing17.1.)\n2) Makethekeydatastructureinyourprogramanarray s[N]containingthevaluesofthe\nspinssi.Fordebugging,printout +and‚àí(oroandblank)togivethespinateachlattice\npoint(asweshowinFigure17.2).Examinethepatternfordifferenttriallengths.\n3) The value for the exchange energy Jfixes the energy scale. Keep it fixed at J=1.\n(You may also wish to study antiferromagnets with J=‚àí1, but first examine\nferromagnetswhosedomainsareeasiertounderstand.)\n4) Thethermalenergy kBTisinunitsof Jandisanindependentvariableforthemodel.\nUsekBT=1fordebugging.\n5) Useperiodicboundaryconditionsonyourchaintominimizeendeffects.Thismeans\nthatthechainisacirclewiththefirstandlastspinsadjacenttoeachother.\n6) TryN‚âÉ20fordebugging,andlargervaluesforproductionruns.\n00204060Position80100\n200 400\nTime600 800 1000\nFigure 17.2 An Ising model simulation on a 1D lattice of 100 initially aligned spins (on the left).\nUp spins are indicated by circles, and down spins by blanks. Although the system starts with all up\nspins (a ‚Äúcold‚Äù start), the system is seen to form domains of up and down spins as time progresses.\n370 17 Thermodynamics Simulations and Feynman Path Integrals\n17.2.2 Equilibration and Thermodynamic Properties\n1) Watch achainof Natomsattainthermalequilibrium.Athightemperatures,orforsmall\nnumberofatoms,youshouldseelargefluctuations,whileatlowertemperatures,you\nshouldseesmallerfluctuations.\n2) Look for evidence of instabilities in which there is a spontaneous flipping of a large\nnumberofspins.Thisbecomesmorelikelyforlarger kBTvalues.\n3) Notehowatthermalequilibrium,thesystemisstillquitedynamic,withspinsflipping\nallthetime.Itisthisenergyexchangethatdeterminesthethermodynamicproperties.\n4) Youmaywellfindthatsimulationsatsmall kBT(say,kBT‚âÉ0.1forN=200)areslowto\nequilibrate.Higher kBTvaluesequilibratefaster,yethavelargerfluctuations.\n5) Observetheformationofdomainsandtheeffecttheyhaveonthetotalenergy.Regard-\nlessofthedirectionofspinwithinadomain,theatom‚Äìatominteractionsareattractive,\nand so, they contribute negative amounts to the energy of the system when aligned.\nHowever,the ‚Üë‚Üìor‚Üì‚Üëinteractionsbetweendomainscontributepositiveenergy.There-\nfore,youshouldexpectamorenegativeenergyatlowertemperatureswherethereare\nlargerandfewerdomains.\n6) Makeagraphofaveragedomainsize versustemperature.\nThermodynamic Properties Foragivenspinconfiguration ùõºj,theenergyandmagneti-\nzationaregivenby:\nEùõºj=‚àíJN‚àí1‚àë\ni=1sisi+1,Óàπj=N‚àë\ni=1si. (17.13)\nTheinternalenergy U(T)isjusttheaveragevalueoftheenergy,\nU(T)=‚ü®E‚ü©, (17.14)\nwheretheaverageistakenoverasysteminequilibrium.Athightemperatures,weexpecta\nrandomassortmentofspins,andsoavanishingmagnetization.Atlowtemperatures,when\nmostthespinsarealigned,weexpect Óàπtoapproach N‚àï2.Althoughthespecificheatcan\nbecomputedfromtheelementarydefinition,\nC=1\nNdU\ndT, (17.15)\nthenumericaldifferentiationmaybeinaccurateduetothestatisticalfluctuationsof U.A\nbetter approach is to first calculate the fluctuations in energy, occurring during Mtrials,\nandthendeterminethespecificheatfromthefluctuations:\nU2=1\nMM‚àë\nt=1(Et)2, (17.16)\nC=1\nN2U2‚àí(U)2\nkBT2=1\nN2‚ü®E2‚ü©‚àí‚ü®E‚ü©2\nkBT2. (17.17)\n1) Extendyourprogramtocalculatetheinternalenergy Uandthemagnetization Óàπfor\nthechain.Donotrecalculateentiresumswhenonlyonespinchanges.\n2) Make sure to wait for your system to equilibrate before calculating thermodynamic\nquantities. (If equilibrated, Uwill fluctuate about its average.) Your results should\nresembleFigure17.3.\n17.2 Metropolis Algorithm 371\nkT kT‚Äì0.8‚Äì0.4\n0.0\n024 0240.10.20.3\n01\nE C\nM\n0.5\nFigure 17.3 Simulation results from a 1D Ising model of 100 spins. Left: Energy and speciÔ¨Åc heat\nas functions of temperature; Right: Magnetization as a function of temperature.\n3) Reduce the statistical fluctuations by running the simulation a number of times with\ndifferentseeds,andbytakingtheaverageoftheresults.\n4) Thesimulationsyourunforsmall Nmayberealistic,butmaynotagreewithstatistical\nmechanics,whichassumes N‚âÉ‚àû.(Youmayassumethat N‚âÉ2000isclosetoinfinity.)\nCheckifthatagreementwiththeanalyticresultsforthethermodynamiclimitisbetter\nforlargeNratherthansmall N.\n5) Checkthatthesimulatedthermodynamicquantitiesareindependentofinitialcondi-\ntions(withinstatisticaluncertainties).Thismeansthatyourcoldandhotstartresults\nshouldagree.\n6) Makeaplotoftheinternalenergy Uasafunctionof kBT,andcompareittotheanalytic\nresult(17.6).\n7) Makeaplotofthemagnetization Óàπasafunctionof kBT,andcompareittotheanalytic\nresult.Doesthisagreewithhowyouexpectaheatedmagnettobehave?\n8) Compute the energy fluctuations U2(17.16)and the specific heat C(17.17).Compare\nthesimulatedspecificheattotheanalyticresult(17.8).\n17.2.3 Explorations\n1) Extendthemodelsothatthespin‚Äìspininteraction(17.3)extendstonext-nearestneigh-\nborsaswellasnearestneighbors.Fortheferromagneticcase,thisshouldleadtomore\nbindingandlessfluctuationbecausewehaveincreasedthecouplingsamongspins,and\nthusincreasedthethermalinertia.\n2) Extendthemodelsothattheferromagneticspin‚Äìspininteraction(17.3)extendstonear-\nestneighborsintwodimensions,andforthetrulyambitious,threedimensions.Con-\ntinue using periodic boundary conditions and keep the number of particles small, at\nleasttostartwith[Gould etal.,2006].\n(a) Formasquarelatticeandplace‚àö\nNspinsoneachside.\n(b) Examinemeanenergyandmagnetizationasthesystemequilibrates.\n(c) Is the temperature dependence of the average energy qualitatively different from\nthatofthe1Dmodel?\n(d) Makeaprintoutofthespinconfigurationforsmall N,andidentifydomains.\n372 17 Thermodynamics Simulations and Feynman Path Integrals\n‚Äì80  000‚Äì40  00040 000\n0\n0 2 4 6 8 10ECVM\nkT2-D Ising\nmodelFigure 17.4 The energy, speciÔ¨Åc\nheat, and magnetization as a\nfunction of temperature from a 2D\nIsing model simulation with 40 000\nspins. Evidence of a phase\ntransition at the Curie temperature\nkT=‚âÉ2 . 5i ss e e ni na l lt h r e e\nfunctions. The values of CandE\nhave been scaled to Ô¨Åt on the same\nplot as M. (Courtesy of J. Wetzel.)\n(e) Onceyoursystemappearstobebehavingproperly,calculatetheheatcapacityand\nmagnetizationofthe2DIsingmodelwiththesametechniqueusedforthe1Dmodel.\nUseatotalnumberofparticlesof100 ‚â§N‚â§2000.\n(f) Lookforaphasetransitionfromorderedtounorderedconfigurationsbyexamining\ntheheatcapacityandmagnetizationasfunctionsoftemperature.Theformershould\ndiverge,whilethelattershouldvanishatthephasetransition(Figure17.4).\n17.3 Fast Equilibration via Wang‚ÄìLandau Sampling ‚äô\nAlthough the Metropolis algorithm has been providing excellent service for more than\n70years,WLS[LandauandWang,2004;ClarkUniversity,2011],withitsshortersimula-\ntiontime,hasbeenshowingincreasingutilityinresearchliterature.1Oursimulationwith\ntheMetropolisalgorithm,whichwehavejustdescribed,usedaBoltzmanndistributionand\nfocusedonitstemperaturedependence.TheWLSalgorithmalsousesaBoltzmanndistri-\nbution,butfocusesonitsenergydependence.Itstartswiththeprobabilitythatasystemat\natemperature Twillcontainthedistributionofenergy:\nÓàº(Ei,T)=g(Ei)e‚àíEi‚àïkBT\nZ(T),Z(T)=‚àë\nEig(Ei)e‚àíEi‚àïkBT. (17.18)\nHere,g(Ei)isthenumber,ordensity,ofstatesofenergy Ei,andZ(T)isthepartitionfunc-\ntion.Thesumin Zisoverallstatesofthesystem,butwithstatesofthesameenergyentering\njustonce,owingto g(Ei)accountingfortheirdegeneracy.Becausethedensity-of-states g(E)\nisafunctionofenergy,butnottemperature,onceithasbeencomputed, Z(T)andallther-\nmodynamicquantities,canbecalculatedwithouthavingtorepeatthesimulationforeach\ntemperature.Forexample,theinternalenergyandtheentropyare:\nU(T)def=‚ü®E‚ü©=‚àë\nEiEig(Ei)e‚àíEi‚àïkBT\n‚àë\nEig(Ei)e‚àíEi‚àïkBT,S=kBlng(Ei). (17.19)\nThe density of states g(Ei)is determined by taking the equivalent of a random walk in\nenergyspace.Wefliparandomlychosenspin,recordtheenergyofthenewconfiguration,\n1 WethankOscarA.RestrepooftheUniversidaddeAntioquiaforlettingususesomeofhismaterial.\n17.3 Fast Equilibration via Wang‚ÄìLandau Sampling ‚äô373\n010203040\n‚Äì2 ‚Äì1 0 1 2log g(E)\nE/N04000800012 000\n‚Äì2 ‚Äì1 0 1 2H(E)\nE/N\nFigure 17.5 Wang‚ÄìLandau sampling used in the 2D Ising model on an 8 √ó8 lattice.\nLeft: Logarithm of the density of states log g(E)versus the energy per particle. Right: The histogram\nH(E)showing the number of states visited as a function of the energy per particle. The aim of WLS\nis to make H(E)Ô¨Çat.\nandkeeponflippingspinsandrecordingenergies.Whendone,wehavea histogramH (Ei)of\nthenumberoftimeseachenergy Eiisattained(Figure17.5right).Iftheflippingwerecon-\ntinuedforaverylongtime,thehistogram H(Ei)wouldeventuallyconvergetothedensity\nofstatesg(Ei).Yet,becausethewalkwouldonlyrarelymoveawayfromthemostprobable\nenergies,evenforsmallsystems,some1019to1030stepsmightberequired.\nWLSincreasesthelikelihoodofsamplinglessprobableconfigurationsbyincreasingtheir\nacceptance,whilesimultaneouslydecreasingtheacceptanceofmorelikelyones.Toaccom-\nplishthistrick,WLSacceptsanewenergy Eiwithaprobabilityinverselyproportionalto\nthe(initiallyunknown)densityofstates,\nÓàº(Ei)=1\ng(Ei), (17.20)\nandthenbuildsupahistogramofvisitedstatesasthewalkcontinues.\nAnapparentproblemwithWLSisthat g(Ei)isunknown.Thisisovercomebydetermining\ng(Ei)simultaneouslywiththeexecutionoftherandomwalk.Onestartswithanarbitrary\ng(Ei)function,andthenmultiplies g(Ei)byanempiricalfactor f>1,whichincreasesthe\nlikelihood of reaching states with small g(Ei)values. As the histogram H(Ei)gets flatter,\nthe multiplicative factor fis decreased until it is close to 1. At that point, we have a flat\nhistogramandadeterminationof g(Ei)inwhichallenergieshavebeenvisitedequally.\n17.3.1 WLS Implementation\nOur implementation of WLS, WangLandau.py , is given in Listing 17.2. It assumes an Ising\nmodelwith J=1,andnearestneighborinteractions.Ratherthanrecalculatingtheenergy\neachtimeaspinisflipped,onlythedifferencesinenergiesarecomputed.Forexample,for\neightspinsinaline:\n‚àíEk=ùúé0ùúé1+ùúé1ùúé2+ùúé2ùúé3+ùúé3ùúé4+ùúé4ùúé5+ùúé5ùúé6+ùúé6ùúé7+ùúé7ùúé0, (17.21)\nwherewehaveassumedperiodicboundaryconditions.Ifspin5isflipped,\n‚àíEk+1=ùúé0ùúé1+ùúé1ùúé2+ùúé2ùúé3+ùúé3ùúé4‚àíùúé4ùúé5‚àíùúé5ùúé6+ùúé6ùúé7+ùúé7ùúé0, (17.22)\nandthedifferenceinenergiesis:\nŒîE=Ek+1‚àíEk=2(ùúé4+ùúé6)ùúé5. (17.23)\n374 17 Thermodynamics Simulations and Feynman Path Integrals\nForthe2Dproblemwithspinsonalattice,thechangeinenergywhenspin ùúéi,jonsite\n(i,j)isflippedis:\nŒîE=2ùúéi,j(ùúéi+1,j+ùúéi‚àí1,j+ùúéi,j+1+ùúéi,j‚àí1). (17.24)\nForNspinstherearetwostatesofminimumenergy E=‚àí2N,oneswithallspinspointing\ninthesamedirection,eitheralluporalldown.Themaximumenergy2 Ncorrespondsto\nalternatingspindirectionsonneighboringsites.Eachspinfliponthelatticechangesthe\nenergybyfourunitsbetweentheselimits:\nEi=‚àí2N,‚àí2N+4,‚àí2N+8,‚Ä¶,2N‚àí8,2N‚àí4,2N. (17.25)\nTheproducedhistogram H(Ei)andentropy S(T)aregiveninFigure17.5.\n17.4 Path Integral Quantum Mechanics ‚äô\nProblem In classicalmechanics,aparticle‚Äôsmotionisdescribedbyitspace-timetrajec-\ntoryx(t).Foraparticleinaharmonicoscillatorpotential,relatetheclassicaltrajectoryto\nthequantummechanicalwavefunction ùúì(x,t).\nAsthestorygoes,Feynmanwaslookingforaformulationofquantummechanicsthathad\namoredirectconnectiontoclassicalmechanicsthantheSchr√∂dingertheorydoes,andthat\nalsoincorporatedthestatisticalnatureofquantummechanicsfromthestart.Hefollowed\nasuggestionbyDiracthat Hamilton‚Äôsprincipleofleastaction ,whichcanbeusedtoderive\nclassicaldynamics,maybethe ‚Ñè‚Üí0limitofaquantumleast-actionprinciple.Seeingthat\nHamilton‚Äôsprincipledeals with the paths of particles through space-time, Feynman pos-\ntulated[FeynmanandHibbs,1965;Mannheim,1983]thatthequantum-mechanicalwave\nfunctiondescribingthepropagationofafreeparticlefromthespace-timepoint a=(xa,ta)\ntothepoint b=(xb,tb),arerelatedby:\nùúì(xb,tb)=‚à´dxaG(xb,tb;xa,ta)ùúì(xa,ta), (17.26)\nwhereGistheGreen‚Äôsfunction orpropagator :\nG(xb,tb;xa,ta)‚â°G(b,a)=‚àöm\n2ùúãi(tb‚àíta)exp[\nim(xb‚àíxa)2\n2(tb‚àíta)]\n. (17.27)\nEquation (17.26) can be viewed as a form of Huygens‚Äôs wavelet principle in which each\npointonthewavefront ùúì(xa,ta)emitsasphericalwavelet G(b;a)thatpropagatesforward\ninspaceandtime.Accordingly,thenewwavefront ùúì(xb,tb)iscreatedbysummationover,\nandinterferenceamong,alloftheemittedwavelets.\nFeynmanimaginedthatanotherwayofviewing(17.26)isasaformofHamilton‚Äôsprin-\ncipleinwhichtheprobabilityamplitude ùúìforaparticletobeat Bisequaltothesumover\nallpathsthroughspace-timeoriginatingattime Aandendingat B(Figure17.6).Thisview\nincorporatesthestatisticalnatureofquantummechanicsbyassigningdifferentprobabili-\ntiesfortravelalongdifferentpaths,withallpathspossible,butwithsomemorelikelythan\nothers.Thevaluesfortheprobabilitiesofthepathsderivefrom Hamilton‚Äôsclassicalprinciple\nofleastaction :\n17.4 Path Integral Quantum Mechanics ‚äô375\nThemostgeneralmotionofaphysicalparticlemovingalongtheclassicaltrajectory x(t)\nfromtimetatotbisalongapathsuchthattheactionS [x(t)]isanextremum :\nùõøS[x(t)] =S[x(t)+ùõøx(t)]‚àíS[x(t)] =0, (17.28)\nwiththepathsconstrainedtopassthroughtheendpoints :\nùõø(xa)=ùõø(xb)=0.\nThis formulation of classical mechanics, which is based on the calculus of variations, is\nequivalenttoNewton‚Äôsdifferentialequationsiftheaction Sistakenasthelineintegralof\ntheLagrangianalongtheclassicaltrajectory:\nS[x(t)] =‚à´tb\ntadtL[x(t), Ãáx(t)],L=T[x, Ãáx]‚àíV[x]. (17.29)\nHere,Tisthekineticenergy, Visthepotentialenergy, Ãáx=dx‚àïdt,andthesquarebrackets\nindicateafunctional2ofthefunction x(t)andÃáx(t).\nFeynmanobservedthattheclassicalactionforafree( V=0)particle,\nS[b,a]=m\n2(Ãáx)2(tb‚àíta)=m\n2(xb‚àíxa)2\ntb‚àíta, (17.30)\nisrelatedtothefree-particlepropagator(17.27)by:\nG(b,a)=‚àöm\n2ùúãi(tb‚àíta)eiS[b,a]‚àï‚Ñè. (17.31)\nEquation(17.31)isthemuchsought-afterconnectionbetweenquantummechanics(LHS)\nandHamilton‚Äôsprinciple(RHS).Feynmanwentontopostulateareformulationofquantum\nmechanics that incorporates its statistical aspects by postulating G(b,a)to be a weighted\nsumofexponentials,eachwithanexponentthatistheactionfora pathconnecting atob:\nG(b,a)=‚àë\npathseiS[b,a]‚àï‚Ñè(Apathintegral) . (17.32)\nFigure 17.6 In the Feynman path-integral formulation of\nquantum mechanics, a collection of paths connect the\ninitial space-time point Ato the Ô¨Ånal point B. The solid line\nis the classical trajectory that minimizes the action S.T h e\ndashed lines are paths that are also sampled by a quantum\nparticle.\nTime\nAB\ntb\nxb xata\nPosition\n2Afunctionalisanumberwhosevaluedependsonthecompletebehaviorofsomefunctionandnotjust\nonitsbehavioratonepoint.",20029
179-17.5 Lattice Path Integration.pdf,179-17.5 Lattice Path Integration,"376 17 Thermodynamics Simulations and Feynman Path Integrals\n0‚Äì2‚Äì1012Position\nProbability\n20 40 60 80 1000\n‚Äì40 ‚Äì20 0 20 40\nPositionQuantum\nClassical\n0.050.10.150.2\nTime\nFigure 17.7 Left: A space-time quantum path resulting from applying the Metropolis algorithm.\nRight: The probability distribution for the harmonic oscillator ground state as determined by a\npath-integral calculation (the classical result has maxima at the two turning points).\nThe sum (17.32) is called a pathintegral because it sums the exponential of the classical\nactionS[b,a]overinfinitelymanypaths(Figure17.6),witheachactionitselfbeingaclas-\nsicallineintegral(intime)alongapath.\nThecorrespondenceprincipleconnectingclassicalandquantummechanicsapplieshere\nvia the realization that because ‚Ñè‚âÉ10‚àí34Js is a very small number, S‚àï‚Ñè‚àº1020is a very\nlargenumber.Accordingly,eventhoughaninfinityofpathsmayenterintothesum(17.32),\nthemaincontributionscomefromthosepathsthatareadjacenttotheclassicaltrajectory x.\nInfact,because Sisanextremumfortheclassicaltrajectory,itremainsconstanttofirstorder\ninthevariationofpaths,andsonearbypathshavevalues(phasesintheexponentials)that\nvarysmoothlyandrelativelyslowly.Incontrast,pathsfarfromtheclassicaltrajectoryare\nweightedbyarapidlyoscillatingexp (iS‚àï‚Ñè),andwhenmanyaresummedover,theytend\nto cancel each other out. In the classical limit ‚Ñè‚Üí0, only the single classical trajectory\ncontributes,and(17.32)becomesHamilton‚Äôsprincipleofleastaction!InFigure17.7left,\nweshowanexampleofanactualtrajectoryusedinpath-integralcalculations.\n17.4.1 Bound-State Wave Function\nAlthough youmaybethinkingthatyouhavealreadyseenenoughexpressionsforGreen‚Äôs\nfunction,thereisyetanotheroneweneedforourcomputation.Westartbyassumingthat\ntheHamiltonianoperator ÃÉHsupportsaspectrumofeigenfunctions,\nÃÉHùúìn=Enùúìn, (17.33)\neachlabeledbytheindex n.Because ÃÉHisHermitian,itswavefunctionsformsacomplete\northonormalsetinwhichwemayexpandageneralsolution:\nùúì(x,t)=‚àû‚àë\nn=0cne‚àíiEntùúìn(x), (17.34)\ncn=‚à´+‚àû\n‚àí‚àûdxùúì‚àó\nn(x)ùúì(x,t=0), (17.35)\nwherethevaluefortheexpansioncoefficients cnfollowsfromtheorthonormalityof ùúìn‚Äôs.If\nwesubstitutethis cnbackintothewavefunctionexpansion(17.34),weobtaintheidentity:\nùúì(x,t)=‚à´+‚àû\n‚àí‚àûdx0‚àë\nnùúì‚àó\nn(x0)ùúìn(x)e‚àíiEntùúì(x0,t=0). (17.36)\n17.5 Lattice Path Integration 377\nComparisonwith(17.26)yieldstheeigenfunctionexpansionfor G:\nG(x,t;x0,t0=0)=‚àë\nnùúì‚àó\nn(x0)ùúìn(x)e‚àíiEnt. (17.37)\nWe relate this to the bound-state wave function ( recallthatour problemistocalculate\nthat)byfirstrequiringallpathstostartandendatthespaceposition x0=x,bythentaking\nt0=0, and, finally, by making an analytic continuation of (17.37) to negative imaginary\ntime(permissibleforanalyticfunctions):\nG(x,‚àíiùúè;x,0)=‚àë\nn|ùúìn(x)|2e‚àíEnùúè=|ùúì0|2e‚àíE0ùúè+|ùúì1|2e‚àíE1ùúè+¬∑¬∑¬∑,\n‚áí|ùúì0(x)|2=lim\nùúè‚Üí‚àûeE0ùúèG(x,‚àíiùúè;x,0). (17.38)\nThelimitherecorrespondstolongimaginarytimes ùúè,afterwhichthepartsof ùúìwithhigher\nenergiesdecaymorequickly,leavingonlythegroundstate ùúì0.\nEquation (17.38) provides a closed-form solution for the ground-state wave function\ndirectlyintermsoftheGreen‚Äôsfunction G.Althoughwewillsoondescribehowtocompute\nthisfunction,fornowlookatFigure17.7right,showingsomeresultsofthecomputation.\nAlthough we start with a probability distribution that peaks near the classical turning\npoints at the edges of the well, after a large number of iterations, we end up with a\ndistribution that resembles the expected Gaussian. So, maybe the new formulation does\nwork.OntheleftofFigure17.7,weseeatrajectorythathasbeengeneratedviastatistical\nvariationsabouttheclassicaltrajectory x(t)=Asin(ùúî0t+ùúô).\n17.5 Lattice Path Integration\nBecause both time and space need to be integrated over when evaluating a path inte-\ngral, our simulation starts with a lattice of discrete space-time points [Potvin, 1993].\nWevisualizeaparticle‚Äôstrajectoryasaseriesofstraightlinesconnectingonetimetothe\nnext(Figure17.8).Wedividethetimebetweenthespace-timepoints AandBintoNequal\ntimestepsofsize ùúÄ,andlabelthemwiththeindex j:\nùúÄdef=tb‚àíta\nN‚áítj=ta+jùúÄ,(j=0,N). (17.39)\nAlthoughitismoreprecisetousetheactualpositions x(tj)ofthetrajectoryattimes tjto\ndeterminethe xj‚Äôs(asinFigure17.8),wealsodiscretizespaceuniformlywiththelinksend-\ningatthenearestlatticepoints.Seeingthatwehavealattice,itiseasytoevaluatederivatives\norintegralsonalink3:\ndxj\ndt‚âÉxj‚àíxj‚àí1\ntj‚àítj‚àí1=xj‚àíxj‚àí1\nùúÄ, (17.40)\nSj‚âÉLjŒît‚âÉ1\n2m(xj‚àíxj‚àí1)2\nùúÄ‚àíV(xj)ùúÄ, (17.41)\nwherewehaveassumedthattheLagrangianisconstantovereachlink.\n3 AlthoughEuler‚Äôsrulehasalargeerror,itisoftenusedinlatticecalculationsbecauseofitssimplicity.\nHowever,iftheLagrangiancontainssecondderivatives,thenthemoreprecisecentral-differencemethodis\nneededtoavoidsingularities.\n378 17 Thermodynamics Simulations and Feynman Path Integrals\nLatticepathintegrationisbasedonthe compositiontheorem forpropagators:\nG(b,a)=‚à´dxjG(xb,tb;xj,tj)G(xj,tj;xa,ta)(ta<tj,tj<tb). (17.42)\nForafreeparticlethisyields:\nG(b,a)=‚àöm\n2ùúãi(tb‚àítj)‚àöm\n2ùúãi(tj‚àíta)‚à´dxjei(S[b,j]+S[j,a])\n=‚àö\nm\n2ùúãi(tb‚àíta)‚à´dxjeiS[b,a], (17.43)\nwherewehaveaddedtheactionsbecauselineintegralscombineas S[b,j]+S[j,a]=S[b,a].\nFortheN-linkedpathinFigure17.8,equation(17.42)becomes:\nG(b,a)=‚à´dx1¬∑¬∑¬∑dxN‚àí1eiS[b,a],S[b,a]=N‚àë\nj=1Sj, (17.44)\nwhereSjisthevalueoftheactionforlink j.Atthispoint,theintegraloverthe singlepath\nshowninFigure17.8hasbecomean N-termsumthatbecomesaninfinitesumasthetime\nstepùúÄapproacheszero.\nTo summarize, Feynman‚Äôs path-integral postulate (17.32) means that we sum over all\npaths connecting AtoBto obtain the Green‚Äôs function G(b,a). This, in turn, means that\nwemustsum,notonlyoverthelinksinonepath,but alsooverallthedifferentpaths,in\nordertoproducethevariationinpathsrequiredbyHamilton‚Äôsprinciple.Thesumiscon-\nstrainedsuchthatpathsmustpassthrough AandBandcannotdoublebackonthemselves\n(causality requires that particles move only forward in time). This is the essence of path\nintegration.Becauseweareintegratingoverfunctionsaswellasalongpaths,thetechnique\nisalsoknownas functionalintegration .\nXa Xaxb = xN\nxi' xj'xjxiB\nB\nCD\ntatitb\ntatjtbŒµ\nAA\nFigure 17.8 Left: A path through a space-time lattice that starts and ends at x=xa=xb.\nThe action is an integral over this path, while the path integral is a sum of integrals over all paths.\nThe dotted path BDis a transposed replica of path AC.Right: The dashed path joins the initial and\nÔ¨Ånal times in two equal time steps; the solid curve uses Nsteps each of size ùúÄ. The position of the\ncurve at time tjdeÔ¨Ånes the position xj.\n17.5 Lattice Path Integration 379\nThe propagator (17.32) is the sum over all paths connecting AtoB,w i t he a c hp a t h\nweightedbytheexponentialoftheactionalongthatpath,explicitly:\nG(x,t;x0,t0)=‚àë\n‚à´dx1dx2¬∑¬∑¬∑dxN‚àí1eiS[x,x0], (17.45)\nS[x,x0]=N‚àí1‚àë\nj=1S[xj+1,xj]‚âÉN‚àí1‚àë\nj=1L(xj, Ãáxj)ùúÄ, (17.46)\nwhereL(xj, Ãáxj)istheaveragevalueoftheLagrangianonlink jattimet=jùúÄ.Thecompu-\ntationismadesimplerbyassumingthatthepotential V(x)isindependentofvelocityand\ndoesnotdependonother xvalues(localpotential).Next,weobservethat Gisevaluated\nwithanegativeimaginarytimeintheexpression(17.38)fortheground-statewavefunction.\nAccordingly,weevaluatetheLagrangianwith t=‚àíiùúè:\nL(x, Ãáx)=T‚àíV(x)=+1\n2m(\ndx\ndt)2\n‚àíV(x), (17.47)\n‚áíL(\nx,idx\ndùúè)\n=‚àí1\n2m(\ndx\ndùúè)2\n‚àíV(x). (17.48)\nWe see that the reversal of the sign of kinetic energy in Lmeans that Lnow equals the\nnegativeoftheHamiltonianevaluatedatarealpositivetime t=ùúè:\nH(\nx,dx\ndùúè)\n=1\n2m(\ndx\ndùúè)2\n+V(x)=E, (17.49)\n‚áíL(\nx,idx\ndùúè)\n=‚àíH(\nx,dx\ndùúè)\n. (17.50)\nInthisway,werewritethe t-pathintegralof Lasaùúè-pathintegralof H,andsoexpressthe\nactionandGreen‚ÄôsfunctionintermsoftheHamiltonian:\nS[j+1,j]=‚à´tj+1\ntjL(x,t)dt=‚àíi‚à´ùúèj+1\nùúèjH(x,ùúè)dùúè, (17.51)\n‚áíG(x,‚àíiùúè;x0,0)=‚à´dx1‚Ä¶dxN‚àí1e‚àí‚à´ùúè\n0H(ùúè‚Ä≤)dùúè‚Ä≤, (17.52)\nwherethelineintegralof Hisoveranentiretrajectory.Next,weexpressthepathintegral\nintermsoftheaverageenergyoftheparticleoneachlink, Ej=Tj+Vj,andthensumover\nthelinkstoobtainthesummedenergy:\n‚à´H(ùúè)dùúè‚âÉ‚àë\njùúÄEj=ùúÄÓà±({xj}), (17.53)\nÓà±({xj})def=N‚àë\nj=1[\nm\n2(xj‚àíxj‚àí1\nùúÄ)2\n+V(xj+xj‚àí1\n2)]\n. (17.54)\nIn(17.54),wehaveapproximatedeachlinkinthepathasa straightline ,usedEuler‚Äôsderiva-\ntive rule to obtain the velocity, and evaluated the potential at the midpoint of the link.\nWenowsubstitute this Gintoourexpression(17.38)fortheground-statewavefunction,",8238
180-17.7 Code Listings.pdf,180-17.7 Code Listings,"380 17 Thermodynamics Simulations and Feynman Path Integrals\nwithidenticalinitialandfinalpointsinspace:\nlim\nùúè‚Üí‚àûG(x,‚àíiùúè,x0=x,0)\n‚à´dxG(x,‚àíiùúè,x0=x,0)=‚à´dx1¬∑¬∑¬∑dxN‚àí1exp[‚àí‚à´ùúè\n0Hdùúè‚Ä≤]\n‚à´dxdx1¬∑¬∑¬∑dxN‚àí1exp[‚àí‚à´ùúè\n0Hdùúè‚Ä≤]\n‚áí||ùúì0(x)||2=1\nZlim\nùúè‚Üí‚àû‚à´dx1¬∑¬∑¬∑dxN‚àí1e‚àíùúÄÓà±, (17.55)\nZ=lim\nùúè‚Üí‚àû‚à´dxdx1¬∑¬∑¬∑dxN‚àí1e‚àíùúÄÓà±. (17.56)\nNotetheadditional dxintegrandintheexpressionfor Z.Thesimilarityoftheseexpressions\ntothermodynamics,evenwithapartitionfunction Z,isnoaccident.Bymakingthetime\nparameterimaginary,wehaveconvertedthetime-dependentSchr√∂dingerequationtothe\nheatdiffusionequation:\niùúïùúì\nùúï(‚àíiùúè)=‚àí‚àá2\n2mùúì‚áíùúïùúì\nùúïùúè=‚àá2\n2mùúì. (17.57)\nItisnotsurprisingthenthatthesumoverpathsinGreen‚Äôsfunctionhaseachpathweighted\nbytheBoltzmannfactor, Óàº=e‚àíùúÄÓà±,whichisusuallyassociatedwiththermodynamics.We\nmaketheconnectioncompletebyidentifyingthetemperaturewiththeinversetimestep:\nÓàº=e‚àíùúÄÓà±=e‚àíÓà±‚àïkBT‚áíkBT=1\nùúÄ‚â°‚Ñè\nùúÄ. (17.58)\nConsequently, the ùúÄ‚Üí0 limit, which makes time continuous, is equivalent to a high-\ntemperature limit. The ùúè‚Üí‚àûlimit, which is required to project the ground-state wave\nfunction,meansthatwemustintegrateoverapaththatislonginimaginarytime,thatis,\nlongcomparedto typicaltime ‚Ñè‚àïŒîE. Just asour simulation ofthe Isingmodelrequired\nustowaitforalongtimeforthesystemtoequilibrate,sotoodoesthepresentsimulation\nrequireustowaitalongtime,sothatallbuttheground-statewavefunctionhasdecayed\naway. At last, we have the solution to our problemof finding the ground-state wave\nfunctionviaitsconnectiontoclassicalmechanics.\nTosummarize,wehaveexpressedtheGreen‚Äôsfunctionasapathintegralrequiringinte-\ngrationsoftheHamiltonianalongallpaths(17.55).Weevaluatethispathintegralasthesum\novertrajectoriesonaspace-timelattice,witheachpathweightedbyaprobabilitybasedon\nthepath‚Äôsaction.WeusetheMetropolisalgorithmtoperformthemany,multi-dimensional\nintegrationsrequiredasweexamineallspace-timepaths.Thisissimilartowhatwedidwith\ntheIsingmodel,however,ratherthanrejectingoracceptinga flipinspin,wenowrejector\nacceptachangeinalink ,alsobasedonthechangeinenergy.Themoreiterationswelet\nthealgorithmrunfor,themoretimethededucedwavefunctionhastoequilibratetothe\ngroundstate,andthusthemoreaccuratetheanswer.\nIngeneral,thesetypesofMonteCarloGreen‚Äôsfunctiontechniquesworkbestifwestart\nwith a good guess at the final answer, and then have the algorithm calculate variations\nonourguess.Forthepresentproblem,thismeansthatifwestartwithapathinspace-time\nclosetotheclassicaltrajectory,thealgorithmmaybeexpectedtodoagoodjobatsimulating\nthequantumfluctuationsaboutthattrajectory.However,itdoesnotappeartobegoodat\nfindingtheclassicaltrajectoryfromarbitrarylocationsinspace-time.\n17.5.1 A Time-Saving Trick\nWehaveformulatedthecomputationsothatyoupickavalueof xandperformmanycom-\nputationsoflineintegrals,overallspaceandtime,toobtain ||ùúì0(x)||2.Toobtainthewave\n17.6 Implementation 381\nfunction at another x, the entire simulation must be repeated from scratch. Rather than\ngoingthroughallthatworkagainandagain,wecancomputetheentire xdependenceof\nthewavefunctioninonefellswoop.Thetrickistoinsertadeltafunctionintotheproba-\nbilityintegral(17.55),therebyfixingtheinitialpositiontobe x0,andthentoalsointegrate\noverallx0s:\n||ùúì0(x)||2=‚à´dx1¬∑¬∑¬∑dxNe‚àíùúÄÓà±(x,x1,‚Ä¶)(17.59)\n=‚à´dx0¬∑¬∑¬∑dxNùõø(x‚àíx0)e‚àíùúÄÓà±(x,x1,‚Ä¶). (17.60)\nThisequationexpressesthewavefunctionasanaverageofadeltafunctionoverallpaths,\naprocedurethatmightseemtotallyinappropriatefornumericalcomputationbecauseone\ncannot compute singular functions. Yet, when we simulate the sum over all paths with\n(17.60), there will always be some xvalue for which the integral is nonzero, and so we\naccumulatethesolutionforwhatever xvaluethatis.\nTo understand how this works in practice, consider path ABin Figure 17.8, imagining\nthatwehavejustcalculatedthesummedenergy.Weformanewpathbyhavingonepoint\nonthechainjumptopoint C(whichchangestwolinks).If wereplicatesection AC,and\nuse it as the extension ADto form the top path, we see that the path CBDhas the same\nsummedenergy(action)aspath ACB,andinthiswayitcanbeusedtodetermine |ùúì(x‚Ä≤\nj)|2.\nThatbeingthecase,oncethesystemisequilibrated,wedeterminenewvaluesofthewave\nfunctionatnewlocations x‚Ä≤\njbyflippinglinkstonewvaluesandcalculatingnewactions.\nThemorefrequentlysome xjisaccepted,thegreateristhewavefunctionatthatpoint.\n17.6 Implementation\nThe program QMC.pyin Listing 17.3 evaluates the integral (17.32) by finding the average\nof the integrand ùõø(x0‚àíx)with paths distributed according to the weighting function\nexp[‚àíùúÄÓà±(x0,x1,‚Ä¶,xN)]. The physics enters via (17.62), the calculation of the summed\nenergy Óà±(x0,x1,‚Ä¶,xN). We evaluate the action integral for the harmonic oscillator\npotential:\nV(x)=1\n2x2, (17.61)\nand for a particle of mass m=1. Using a convenient set of natural units, we measure\nlengths in‚àö\n1‚àïmùúî‚â°‚àö\n‚Ñè‚àïmùúî=1, and times in 1 ‚àïùúî=1. Correspondingly, the oscillator\nhas a period T=2ùúã. Figure 17.7 shows results after the application of the Metropolis\nalgorithm. In this computation, we started with an initial path close to the classical\ntrajectory, and then examined half a million variations about this path. All paths were\nconstrainedtobeginandendat x=1.\nWhenthetimedifference tb‚àítaissmall,saylike2 T,thesystemwillnothaveenough\ntime to equilibrate to its ground state, and so the computed wave function will look like\ntheprobabilitydistributionofanexcitedstate(nearlyclassicalwiththeprobabilityhigh-\nest for the particle to be near its turning points, where its velocity vanishes). However,\nwhentb‚àítaequalsalongertime,suchas20 T,thesystemwillhaveenoughtimetodecay\ntoitsgroundstate,andthewavefunctionwilllookliketheexpectedGaussian.Ineither\n382 17 Thermodynamics Simulations and Feynman Path Integrals\ncase (Figure 17.7 right), the trajectory through space-time fluctuates about the classical\ntrajectory.ThisfluctuationisaconsequenceoftheMetropolisalgorithmoccasionallygoing\nuphillinitssearch;ifyoumodifytheprogramsothatsearchesgoonlydownhill,thespace-\ntimetrajectorywillbeaverysmoothtrigonometricfunction(theclassicaltrajectory),but\nthewavefunction,whichisameasureofthefluctuationsabouttheclassicaltrajectory,will\nvanish!\nHerearetheexplicitsteps[MacKeown,1985;MacKeownandNewman,1987]:\n1) Constructagridof Ntimestepseachoflength ùúÄ(Figure17.8).Startat t=0,andextend\nto timeùúè=NùúÄ[Ntime intervals and (N+1)lattice points in time]. Note that time\nalwaysincreasesmonotonicallyalongapath.\n2) Constructagridof Mspacepointsseparatedbystepsofsize ùõø.Startwith M‚âÉN,and\nusearangeof xvaluesseveraltimelargerthanthecharacteristicsizeofthepotential\nbeingused.\n3) Anyxortvaluefallingbetweenlatticepointsshouldbeassignedtotheclosestlattice\npoint.\n4) Associateaposition xjwitheachtime ùúèj,subjecttotheboundaryconditionsthatthe\ninitialandfinalpositionsalwaysremainat xN=x0=x.\n5) Chooseapathconsistingofstraight-linelinksconnectingthelatticepoints.Thisshould\ncorrespondtotheclassicaltrajectory.Observethatthe xvaluesforthelinksofthepath\nmay have values that increase, decrease, or remain unchanged (in contrast to time,\nwhichalwaysincreases).\n6) Startingat j=0,evaluatetheenergy Óà±bysummingthekineticandpotentialenergies\nforeachlinkofthepath:\nÓà±(x0,x1,‚Ä¶,xN)‚âÉN‚àë\nj=1[\nm\n2(xj‚àíxj‚àí1\nùúÄ)2\n+V(xj+xj‚àí1\n2)]\n. (17.62)\n7) Beginasequenceofrepetitivestepsinwhicharandomposition xjassociatedwithtime\ntjischangedtotheposition x‚Ä≤\nj(pointCinFigure17.8).Thischanges twolinksinthe\npath.\n8) UsetheMetropolisalgorithmtoweighthechangedpositionwiththeBoltzmannfactor.\n9) Foreachlatticepoint,establisharunningsumrepresentingthesquaredmodulusof\nthewavefunctionatthatpoint.\n10) After each single-link change (or decision not to change), increase the running sum\nforthenew xvalueby1.Afterasufficientlylongrunningtime,thesumdividedbythe\nnumberofstepsisthesimulatedvaluefor |ùúì(xj)|2ateachlatticepoint xj.\n11) Repeat the entire link-changing simulation starting with a different seed. A wave\nfunctionaveragedovermanyintermediatelengthrunsisbetterthanonefromavery\nlongrun.\n17.6.1 Path Integration Exercise\n1) Plotsomeoftheactualspace-timepathsusedinthesimulationalongwiththeclassical\ntrajectory.\n2) Foramorecontinuouspictureofthewavefunction,makethe xlatticespacingsmaller;\nforamoreprecisevalueofthewavefunctionatanyparticularlatticesite,samplemore\npoints(runlonger)anduseasmallertimestep ùúÄ.\n17.6 Implementation 383\n3) Becausetherearenosignchangesinaground-statewavefunction,youcanignorethe\nphase,assume ùúì(x)=‚àö\nùúì2(x),andthenestimatetheenergyvia:\nE=‚ü®ùúì|H|ùúì‚ü©\n‚ü®ùúì|ùúì‚ü©=ùúî\n2‚ü®ùúì|ùúì‚ü©‚à´+‚àû\n‚àí‚àûùúì‚àó(x)(\n‚àíd2\ndx2+x2)\nùúì(x)dx, (17.63)\nwherethespacederivativeisevaluatednumerically.\n4) Exploretheeffectofmaking ‚Ñèlarger,andthuspermittinggreaterfluctuationsaround\ntheclassicaltrajectory.DothisbydecreasingthevalueoftheexponentintheBoltzmann\nfactor.Determineifthismakesthecalculationmoreorlessrobustinitsabilitytofind\ntheclassicaltrajectory.\n5) Testyour ùúìforthegravitationalpotential(seequantumbouncerbelow):\nV(x)=mg|x|,x(t)=x0+ùë£0t+1\n2gt2. (17.64)\n17.6.2 Quantum Bouncer ‚äô\nAnother problem for which the classical trajectory is well known is that of a quantum\nbouncer.4Herewehaveaparticledroppedinauniformgravitationalfield,hittingahard\nfloor,andthenbouncingup.Whentreatedquantummechanically,quantizedlevelsforthe\nparticle result [Gibbs, 1975; Goodings and Szeredi, 1992; Whineray, 1992; Vall√©e, 2000].\nIn 2002, an experiment to discern this gravitational effect at the quantum level was per-\nformedbyNesvizhevsky etal.[2002],andisdescribedinShaw[1992].Itconsistedofdrop-\npingultracoldneutronsfromaheightof14 Œºmuntoaneutronmirror,andwatchingthem\nbounce.Itfoundaneutrongroundstateat1.4peV.\nWe start by determining the analytic solution to this problem for stationary states,\nandthengeneralizingittoincludetime-dependence.Thetime-independentSchr√∂dinger\nequationforaparticleinauniformgravitationpotentialis:\n‚àí‚Ñè2\n2md2ùúì(x)\ndx2+mxgùúì(x)=Eùúì(x), (17.65)\nùúì(x‚â§0)=0,(boundarycondition) . (17.66)\nThe boundary condition (17.66) is a consequence of the hard floor at x=0. A change of\nvariablesconverts(17.65)toadimensionlessform,\nd2ùúì\ndz2‚àí(z‚àízE)ùúì=0, (17.67)\nz=x(2gm2\n‚Ñè2)1‚àï3\n,zE=E(\n2\n‚Ñè2mg2)1‚àï3\n. (17.68)\nThereisananalyticsolutionintermsofAiryfunctionsAi( z)[Pressetal.,2007]:\nùúì(z)=NnAi(z‚àízE), (17.69)\nwhereNnisanormalizationconstant.Theboundarycondition ùúì(0)=0implies:\nùúì(0)=NEAi(‚àízE)=0, (17.70)\nwhich means that the allowed energies of the system correspond to the zeros znof Airy\nfunctionswithnegativearguments.Tosimplifythecalculation,wetake ‚Ñè=1,g=2,and\nm=1\n2,whichleadsto z=xandzE=E.\n4 OscarA.Restrepoassistedinthepreparationofthissection.\n384 17 Thermodynamics Simulations and Feynman Path Integrals\nzQMC\nAnalytic‚îÇŒ®(z)‚îÇ20.6\n0.4\n0.2\n0 \n0246Figure 17.9 The analytic and quantum\nMonte Carlo solution for the quantum\nbouncer. The dashed line is the Airy\nfunction squared, and the solid line is\n|ùúì0(z)|2after a million trajectories.\nThetime-dependentsolutionforthequantumbouncerisaninfinitesumovertheeigen-\nfunctions,eachwithatime-dependencedeterminedbyitsenergy:\nùúì(z,t)=‚àû‚àë\nn=1CnNnAi(z‚àízn)e‚àíiEnt‚àï‚Ñè, (17.71)\nwheretheCnsareconstants.\nFigure17.9showstheresultsofsolvingforthequantumbouncer‚Äôsground-stateproba-\nbility|ùúì0(z)|2usingFeynman‚Äôspathintegration,thatis,quantumMonteCarlo.Thetime\nincrement dtandthetotaltime twereselectedbytrialanderrorinsuchawayastosatisfy\nthe boundary condition |ùúì(0)|2‚âÉ0. To account for the potential being infinite for nega-\ntivexvalues, we selected trajectories that have positive xvalues over all their links. This\nincorporatesthefactthattheparticlecanneverpenetratethefloor.Ourprogramisgivenin\nListing17.4.Theresultafterusing106trajectories,andatimestep ùúÄ=dùúè=0.05,areshown\ninFigure17.9.Bothwavefunctionswerenormalizedviaatrapezoidintegration.Ascanbe\nseen,theagreementbetweentheanalyticandpathintegrationwavefunctionissatisfactory,\nthoughnotperfect.\n17.6.3 Path Integral Bouncer Exercises\n1) You are given the fact that a particle falls at distance din timet=‚àö\n2D‚àïg. Assume a\nquadraticdependenceondistanceandtime,\nd=ùõºt+ùõΩt2, (17.72)\nandshow,eitheranalyticallyornumerically,thattheaction S=‚à´t0\n0Ldtfortheparticle‚Äôs\ntrajectoryisanextremumonlywhen ùõº=0andùõΩ=g‚àï2.\n2) Consideramass mattachedtoaharmonicoscillatorwithperiod T=1andfrequency\nùúî=2ùúã:\nx(t)=10cos(ùúît). (17.73)\n(a) Proposeamodificationof(17.73)thatagreeswithitat t=0andt=T,thoughdiffers\nforintermediatevaluesof t.Includeanadjustableparameterinyourmodification.\n(b) Computetheactionforanentirerangeofvaluesfortheparameterinyourproposed\ntrajectory,andtherebyverifythatonlytheknownanalyticformyieldsaminimum\naction.\n17.7 Code Listings 385\n3) Considera1Dharmonicoscillatorwithdisplacement qandmomentum p.Theenergy:\nE(p,q)=p2\n2m+mùúî2q2\n2(17.74)\nisanintegralofthemotion,andtheareaoftheperiodicorbitis:\nA(E)=‚àÆpdq=2‚à´qmax\nqminpdq. (17.75)\n(a) Usetheanalytic,ornumeric,solutionforsimpleharmonicmotiontocomputethe\nareaA(E).\n(b) Computethederivative T=dA(E)‚àïdEviaacentral-differenceapproximationand\ncomparetotheanalyticanswer.\n(c) Now repeat this problem using a nonlinear oscillator for which there is only a\nnumericalsolution.(Evenoscillatorsoftheform V=kxpwithpshouldworkjust\nfine.) You can determine the period from the time dependence of your solution,\nandthenuseyoursolutiontocompute A(E)fordifferentinitialconditions.\n17.7 Code Listings\nListing 17.1 IsingViz.py A1DIsingchainsimulationwiththeMetropolisalgorithm.\n1# IsingViz .py: Ising model\nfromvisualimport ‚àó\nimportrandom\n5fromvisual.graph import ‚àó\n# Display for the arrows\nscene = display(x=0,y=0,width=700,height=200, range=40,title= ‚ÄôSpins‚Äô)\n9engraph = gdisplay(y=200,width=700,height=300, title= ‚ÄôE of Spin System‚Äô ,\\nxtitle= ‚Äôiteration‚Äô , ytitle= ‚ÄôE‚Äô,xmax=500, xmin=0, ymax=5, ymin= ‚àí5)\nenplot = gcurve(color=color.yellow)\nN= 3 0\n13B= 1 .\nmu = .33 #gm u\nJ= . 2 0\nk= 1 . # Boltmann\n17T = 100.\nstate = zeros((N)) # spins up(1) , d o w n (0)\nS= z e r o s ( ( N ) , float)\ntest = state\n21random.seed() # Seed generator\ndefenergy ( S) :\nFirstTerm = 0.\n25SecondTerm = 0.\nforiin range (0,N‚àí2): FirstTerm += S[i] ‚àóS[i + 1]\nFirstTerm ‚àó=‚àíJ\nforiin range (0,N‚àí1): SecondTerm += S[i]\n29SecondTerm ‚àó=‚àíB‚àómu;\nreturn(FirstTerm + SecondTerm);\nES = energy(state)\n33\ndefspstate(state): # Plots spins\nforobjinscene.objects: obj.visible=0 # Erase old arrows\nj=0\n37foriin range (‚àíN,N,2):\nifstate[j]== ‚àí1: ypos = 5 # Spin down\n386 17 Thermodynamics Simulations and Feynman Path Integrals\nelse: ypos = 0\nif5‚àóstate[j]<0: arrowcol = (1,1,1) # White arrow if down\n41 else: arrowcol =(0.7,0.8,0)\narrow(pos=(i ,ypos,0) ,axis=(0,5 ‚àóstate[j],0),color=arrowcol)\nj +=1\n45foriin range (0 ,N): state[i] = ‚àí1 # Initial spins all down\nforobjinscene.objects: obj.visible=0\nspstate(state)\n49ES = energy(state)\nforjin range (1,500):\nrate(3)\n53 test = state\nr=int(N‚àórandom.random()); # Flip spin randomly\ntest[r] ‚àó=‚àí1\nET = energy(test)\n57 p=m a t h . e x p ( ( E S ‚àíET)/(k ‚àóT)) # Boltzmann test\nenplot.plot(pos=(j,ES)) # Adds segment to curve\nifp >= random.random() :\nstate = test\n61 spstate(state)\nES = ET\nListing 17.2 WangLandau.py WangLandaualgorithmfor2-Dspinsystem.\n# WangLandau . py : Wang Landau algorithm for 2 ‚àíD spin system\n"""""" Author in Java: Oscar A. Restrepo,\n4Universidad de Antioquia, Medellin, Colombia\nEach time fac changes, a new histogrm is generated.\nOnly the first Histogram plotted to reduce computational time""""""\nfromvisualimport ‚àó\n8importrandom;\nfromvisual.graph import ‚àó\nL=8 ; N =( L ‚àóL)\n12\n# Set up graphics\nentgr = gdisplay(x=0,y=0,width=500,height=250,title= ‚ÄôDensity of States‚Äô ,\\nxtitle= ‚ÄôE/N‚Äô, ytitle= ‚Äôlog g(E)‚Äô , xmax=2.,\nxmin=‚àí2.,ymax=45,ymin=0)\n16entrp = gcurve(color = color.yellow, display = entgr)\nenergygr = gdisplay(x=0, y=250, width=500, height=250, title= ‚ÄôE vs T‚Äô ,\\nxtitle = ‚ÄôT‚Äô, ytitle= ‚ÄôU(T)/N‚Äô , xmax=8.,xmin=0, ymax =0.,ymin= ‚àí2.)\nenerg = gcurve(color = color.cyan, display = energygr)\n20histogr = display(x = 0, y = 500, width = 500, height = 300,\\ntitle = ‚Äô1st histogram: H(E) vs. E/N, corresponds to log(f) = 1‚Äô )\nhisto = curve(x = list(range(0, N+1)), color=color.red, display=histogr)\nxaxis = curve(pos = [( ‚àíN,‚àí10), (N, ‚àí10)])\n24minE = label(text = ‚Äô-2 ‚Äô,p o s=( ‚àíN+3, ‚àí15), box = 0)\nmaxE = label(text = ‚Äô2‚Äô,p o s=( N ‚àí3,‚àí15), box = 0)\nzeroE = label(text = ‚Äô0‚Äô,p o s=( 0 , ‚àí15), box = 0)\nticm = curve(pos = [( ‚àíN,‚àí10), ( ‚àíN,‚àí13)])\n28tic0 = curve(pos = [(0, ‚àí10), (0, ‚àí13)])\nticM = curve(pos = [(N, ‚àí10), (N, ‚àí13)])\nenr = label(text = ‚ÄôE/N‚Äô,p o s=( N / 2 , ‚àí15), box = 0)\n32s p =z e r o s (( L ,L )) # Grid size , spins\nhist = zeros( (N + 1) )\nprhist = zeros( (N + 1) ) # Histograms\nS= z e r o s ( ( N + 1 ) , float) # Entropy = log g(E)\n36\ndefiE(e): return int ((e + 2 ‚àóN)/4)\ndefIntEnergy():\n40exponent = 0.0\nforTinarange (0.2, 8.2, 0.2 ): # Select lambda m a x\n17.7 Code Listings 387\nEner = ‚àí2‚àóN\nmaxL = 0.0 # Initialize\n44 foriin range (0, N + 1):\nifS[i]!= 0 and(S[i]‚àíEner/T)>maxL:\nmaxL = S[i] ‚àíEner/T\nEner = Ener + 4\n48 sumdeno = 0\nsumnume = 0\nEner = ‚àí2‚àóN\nforiin range (0, N):\n52 ifS[i] != 0:\nexponent = S[i] ‚àíEner/T ‚àímaxL\nsumnume += Ener ‚àóexp(exponent)\nsumdeno += exp(exponent)\n56 Ener = Ener + 4.0\nU = sumnume/sumdeno/N # internal energy U(T)/N\nenerg.plot(pos = (T, U) )\n60defWL() : #W a n g ‚àíLandau sampling\nHinf = 1.e10 # initial values for Histogram\nHsup = 0.\ntol = 1.e ‚àí3 # tolerance , stops the algorithm\n64ip = zeros(L)\nim = zeros(L) # BC R or down, L or up\nheight = abs(Hsup‚àíHinf)/2. # Initialize histogram\nave = (Hsup + Hinf)/2. # about average of histogram\n68percent = height / ave\nforiin range (0, L):\nforjin range (0, L): sp[i, j] = 1 # Initial spins\nforiin range (0, L):\n72 i p [ i ]=i+1\nim[i] = i ‚àí1 # Case plus , minus\nip[L‚àí1] = 0\nim[0] = L ‚àí1 # Borders\n76Eold = ‚àí2‚àóN # Initialize energy\nforjin range (0, N + 1): S[j] = 0 # Entropy initialized\niter=0\nfac = 1\n80whilefac > tol :\ni=int(N‚àórandom.random() ) # Select random spin\nxg = i%L\n84 # Must be i //L , not i /L for Python 3 :\nyg = i//L # Localize x, y, grid point\nEnew = Eold + 2 ‚àó(sp[ip[xg],yg] + sp[im[xg],yg] + sp[xg,ip[yg]]\n+s p [ x g ,i m [ y g ] ]) ‚àósp[xg, yg] # Change energy\n88 deltaS = S[iE(Enew)] ‚àíS[iE(Eold)]\nifdeltaS <= 0 orrandom.random() < exp( ‚àídeltaS):\nEold = Enew;\nsp[xg, yg] ‚àó=‚àí1 # Flip spin\n92 S[iE(Eold)] += fac; # Change entropy\nif iter%10000 == 0: # Check flatness every 10000 sweeps\nforjin range (0 ,N +1 ) :\nifj= =0:\n96 Hsup = 0\nHinf = 1e10 # Initialize new histogram\nifhist[j] == 0 : continue # Energies never visited\nifhist[j] > Hsup: Hsup = hist[j]\n100 ifhist[j] < Hinf: Hinf = hist[j]\nheight = Hsup ‚àíHinf\nave = Hsup + Hinf\npercent = 1.0 ‚àóheight/ave # 1 . 0 to make i t f l o a t number\n104 ifpercent < 0.3 : # Histogram flat ?\nprint("" iter "" ,iter,"" log(f) "" , fac)\nforjin range (0, N + 1):\nprhist[j] = hist[j] #t op l o t\n108 hist[j] = 0 # Save hist\nfac ‚àó=0 . 5 # Equivalent to log(sqrt(f))\niter+= 1\nhist[iE(Eold)] += 1 # Change histogram , add 1 , update\n112 iffac >= 0.5: # just show the first histogram\n388 17 Thermodynamics Simulations and Feynman Path Integrals\n# Speed up by using array calculations :\nhisto.x = 2.0 ‚àóarange(0,N+1) ‚àíN\nhisto.y = 0.025 ‚àóhist‚àí10\n116deltaS = 0.0\nprint(""wait because iter > 13 000 000"" ) # not always the same\nWL() # Call Wang Landau algorithm\ndeltaS = 0.0\n120forjin range (0, N + 1):\nrate(150)\norder = j ‚àó4‚àí2‚àóN\ndeltaS = S[j] ‚àíS[0] + log(2)\n124ifS[j] != 0 : entrp.plot(pos = (1. ‚àóorder/N, deltaS)) # plot entropy\nIntEnergy();\nprint(""Done"")\nListing 17.3 QMC.py Feynmanpathintegrationcalculationofground-stateprobability.\n# Q M C.py: Q u a n t u m MonteCarlo (Feynman path integration)\nfromvisualimport ‚àó;fromvisual.graph import ‚àó;importrandom\n4\nN = 100; Nsteps = 101; xscale = 10. # Initialize\npath = zeros([Nsteps], float); prob = zeros([Nsteps], float)\n8trajec = display(width = 300,height=500, title= ‚ÄôSpacetime Paths‚Äô )\ntrplot = curve(y = range(0, 100), color=color.magenta, display = trajec)\ndefPlotAxes(): #A x i s\n12trax = curve(pos=[( ‚àí97,‚àí100),(100, ‚àí100)],colo =color.cyan,display=trajec)\nlabel(pos = (0, ‚àí110), text = ‚Äô0‚Äô, box = 0, display = trajec)\nlabel(pos = (60, ‚àí110), text = ‚Äôx‚Äô, box = 0, display = trajec)\ndefWaveFunctionAxes(): # Axes for probability\n16wvfax=curve(pos =[( ‚àí600,‚àí155),(800, ‚àí155)],display=wvgraph,color=color.cyan)\ncurve(pos = [(0, ‚àí150), (0,400)], display=wvgraph, color=color.cyan)\nlabel(pos = ( ‚àí80,450), text= ‚ÄôProbability‚Äô , box = 0, display = wvgraph)\nlabel(pos = (600, ‚àí220), text= ‚Äôx‚Äô, box=0, display=wvgraph)\n20label(pos = (0, ‚àí220), text= ‚Äô0‚Äô, box=0, display=wvgraph)\ndefEnergy(path): #H OE n e r g y\nsums = 0.\nforiin range (0,N‚àí2):sums += (path[i+1] ‚àípath[i]) ‚àó(path[i+1] ‚àípath[i])\n24sums += path[i+1] ‚àópath[i+1];\nreturnsums\ndefPlotPath(path): # Plot trajectory\nforjin range (0, N):\n28 trplot.x[j] = 20 ‚àópath[j]\ntrplot.y[j] = 2 ‚àój‚àí100\ndefPlotWF(prob): #P l o tp r o b\nforiin range (0, 100):\n32 wvplot.color = color.yellow\nwvplot.x[i] = 8 ‚àói‚àí400 # Center fig\nwvgraph = display(x=340,y=150,width=500,height=300,title= ‚ÄôGround State‚Äô )\n36wvplot = curve(x = range(0, 100), display = wvgraph)\nwvfax = curve(color = color.cyan)\nPlotAxes(); WaveFunctionAxes() #P l o ta x e s\noldE = Energy(path)\n40whileTrue: # Pick random element\nrate(10) # Slow paintings\nelement = int(N‚àórandom.random() ) # Metropolis\nchange = 2.0 ‚àó(random.random() ‚àí0.5)\n44path[element] += change # Change path\nnewE = Energy(path); #F i n dn e wE\nifnewE > oldE andmath.exp( ‚àínewE + oldE)<= random.random() :\npath[element] ‚àí=c h a n g e # Reject\n48 PlotPath(path) # Plot trajectory\nelem =int(path[element] ‚àó16 + 50) # i f path = 0 , elem = 50\n# elem = m ‚àópath[element] + b is the linear transformation\n52# if path= ‚àí3, elem=2 i f path=3. , elem=98 = > b=50, m =16 linear TF.\n17.7 Code Listings 389\n# this way x = 0 correspond to prob[50]\nifelem < 0: elem = 0,\n56ifelem > 100: elem = 100 # If exceed max\nprob[elem] += 1 # increase probability\nPlotWF(prob) #P l o tp r o b\noldE = newE\nListing 17.4 QMCbouncer.py Feynman path integration computation of a quantum\nparticleinagravitationalfield.\n# QMCbouncer.py: g.s. wavefunction via path integration\n3fromvisualimport ‚àó\nimportrandom\nfromvisual.graph import ‚àó\n7N= 100; dt = 0.05; g = 2.0; h = 0.00; maxel = 0\npath = zeros([101], float); arr = path; prob = zeros([201], float)\ntrajec = display(width = 300, height=500,title = ‚ÄôSpacetime Trajectory‚Äô )\ntrplot = curve(y = range(0, 100), color=color.magenta, display = trajec)\n11\ndeftrjaxs(): # plot axis for trajectories\ntrax=curve(pos=[( ‚àí97,‚àí100),(100, ‚àí100)],color=color.cyan,display=trajec)\ncurve(pos = [( ‚àí65,‚àí100),(‚àí65, 100)], color=color.cyan,display=trajec)\n15label(pos = ( ‚àí65,110), text = ‚Äôt‚Äô, box = 0, display = trajec)\nlabel(pos = ( ‚àí85,‚àí110), text = ‚Äô0‚Äô, box = 0, display = trajec)\nlabel(pos = (60, ‚àí110), text = ‚Äôx‚Äô, box = 0, display = trajec)\nwvgraph = display(x=350, y=80, width=500, height=300, title = ‚ÄôGS Prob‚Äô )\n19wvplot = curve(x = range(0, 50), display = wvgraph) # wave function plot\nwvfax = curve(color = color.cyan)\ndefwvfaxs(): # plot axis for wavefunction\n23wvfax = curve(pos =[( ‚àí200,‚àí155),(800, ‚àí155)],display=wvgraph,color=color.cyan)\ncurve(pos = [( ‚àí200,‚àí150),(‚àí200,400)],display=wvgraph,color=color.cyan)\nlabel(pos = ( ‚àí70, 420),text = ‚ÄôProbability‚Äô , box = 0, display=wvgraph)\nlabel(pos = (600, ‚àí220),text = ‚Äôx‚Äô, box = 0, display = wvgraph)\n27label(pos = ( ‚àí200,‚àí220),text = ‚Äô0‚Äô, box = 0, display = wvgraph)\ntrjaxs(); wvfaxs() #p l o ta x e s\ndefenergy (arr): # Energy of path\n31esum = 0.\nforiin range (0,N):\nesum += 0.5 ‚àó((arr[i+1] ‚àíarr[i])/dt) ‚àó‚àó2+g‚àó(arr[i]+arr[i+1])/2\nreturnesum\n35\ndefplotpath(path): # Plot xy trajectory\nforjin range (0, N):\ntrplot.x[j] = 20 ‚àópath[j] ‚àí65\n39 trplot.y[j] = 2 ‚àój‚àí100\ndefplotwvf(prob): # Plot wave function\nforiin range (0, 50):\n43 wvplot.color = color.yellow\nwvplot.x[i] = 20 ‚àói‚àí200\nwvplot.y[i] = 0.5 ‚àóprob[i] ‚àí150\n47oldE = energy(path)\ncounter = 1\nnorm = 0. # Plot psi every 100\nmaxx = 0.0\n51while1: # ""Infinite"" loop\nrate(100)\nelement = int(N‚àórandom.random() )\nifelement != 0 andelement!= N: # Ends not allowed\n55 change = ( (random.random() ‚àí0.5) ‚àó20.)/10.\nifpath[element] + change > 0.: # No negative paths\npath[element] += change\n390 17 Thermodynamics Simulations and Feynman Path Integrals\nnewE = energy(path) # N e w trajectory E\n59 ifnewE > oldE andexp(‚àínewE + oldE) <= random.random() :\npath[element] ‚àí=c h a n g e # Link rejected\nplotpath(path)\nele =int(path[element] ‚àó1250./100.) # Scale changed\n63 ifele >= maxel: maxel = ele # Scale change 0 to N\nifelement != 0: prob[ele] += 1\noldE = newE;\nifcounter%100 == 0: # Plot psi every 100\n67 foriin range (0, N): # Max x of path\nifpath[i] >= maxx: maxx = path[i]\nh = maxx/maxel # space step\nfirstlast = h ‚àó0.5‚àó(prob[0] + prob[maxel]) # for trap . extremes\n71 foriin range (0, maxel + 1): norm = norm + prob[i] #N o r m\nnorm = norm ‚àóh + firstlast # Trap rule\nplotwvf(prob) # Plot probability\ncounter += 1",24944
181-Chapter 18 Molecular Dynamics Simulations.pdf,181-Chapter 18 Molecular Dynamics Simulations,"391\n18\nMolecular Dynamics Simulations\nYou may recall from introductory chemistry that the ideal gas law can be derived from Ô¨Årst\nprinciples by conÔ¨Åning noninteracting molecules to a box. This chapter extends that model\nto molecules that interact with each other. Although the theory of Molecular Dynamics\n(MD) is straightforward, the simulations have proven to be powerful approaches for studying\nthe physical and chemical properties of solids, liquids, amorphous materials, and biological\nmolecules .\nProblem Determinewhetheracollectionofargonmoleculesplacedinaboxwillcoalesce\nintoanorderedstructureasthetemperatureislowered.\nAlthough we know that quantum mechanics is the proper theory for molecular interac-\ntions, MD uses Newton‚Äôs laws as its basis and focuses on bulk properties, which are not\nparticularlysensitivetothesmall rbehaviors,wherequantumeffects maybeimportant.\nNevertheless, Car and Parrinello [1985] showed how MD can be extended to include\nquantum mechanics by using density functional theory to calculate the force between\nmolecules. That technique, known as quantum MD ,i sa na c t i v ea r e ao fr e s e a r c hb u ti s\nbeyond the realm of the present chapter.1For those with further interests, there are full\ntextsonMD[Rapaport,1995;HockneyandEastwood,1988],fullerdiscussionsin[Gould\netal.,2006;Thijssen,1999;Fosdick etal.,1996],aswellasprimers[Ercolessi,1997],and\ncodesavailableon-line[Nelson etal.,1996;Refson,2000;Anderson etal.,2008].\nAlthough MD‚Äôs solution of Newton‚Äôs laws is conceptually simple, when applied to a\nvery large number of particles, it becomes the ‚Äúhigh school physics problem from hell.‚Äù\nSome approximations must be made in order to avoid solving the ‚àº1024equations of\nmotion of a realistic system, and so, present calculations tend to have an upper limit of\n‚àº109particlesconfinedtoafiniteregionofspace.\nInanumberofways,MDsimulationsaresimilartothethermalMonteCarlosimulations\nwestudiedinChapter17.Bothtypicallyinvolvealargenumber Nofinteractingparticles\nthatstartoutinsomesetconfiguration,andthenequilibrateintoadynamicstate.However,\ninMDwehaveastatisticalmechanical microcanonicalensemble inwhichtheenergy Eand\nvolumeVoftheNparticlesarefixed.WethenuseNewton‚Äôslawstogeneratethedynamics\n1 WethankSatoruS.Kanoforpointingthisouttous.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n392 18 Molecular Dynamics Simulations\nofthesystem.Incontrast,MonteCarlosimulationsdonotstartwithfirstprinciples,but,\ninstead,incorporateanelementofchanceandhavethesystemremainingincontactwitha\nheatbathatafixedtemperature,ratherthankeepingtheenergy Efixed.Thisisa canonical\nensemble.\nSince the molecules in an MD simulation are dynamic, their velocities and positions\nchange continuously with time. After a simulation has run long enough to stabilize, we\nwillcomputetimeaveragesofthedynamicquantitiesinordertorelatethemtothether-\nmodynamicproperties.ThesimulationsapplyNewton‚Äôslawswiththeassumptionthatthe\nnetforceoneachmoleculeisthesumofthetwo-bodyforceswithalloftheother (N‚àí1)\nmolecules:\nmd2ri\ndt2=Fi(r0,‚Ä¶,rN‚àí1), (18.1)\nmd2ri\ndt2=N‚àí1‚àë\ni<j=0fij,i=0,‚Ä¶,(N‚àí1). (18.2)\nHerewehaveignoredthefactthatanargonatomitselfisadynamicsystemcomposedof\n18electronsandanucleus(Figure18.1).Althoughitmaybepossibletoignorethisinternal\nstructurewhendeducingthelong-rangepropertiesofinertelements,itmattersforsystems\nsuchaspolyatomicmoleculesthatdisplayrotational,vibrational,andelectronicdegreesof\nfreedomasthetemperatureisraised.2\nTheforceonmolecule iderivesfromthesumofmolecule‚Äìmoleculepotentials:\nFi(r0,r1,‚Ä¶,rN‚àí1)=‚àíùõÅriU(r0,r1,‚Ä¶,rN‚àí1), (18.3)\nU(r0,r1,‚Ä¶,rN‚àí1)=‚àë\ni<ju(rij)=N‚àí2‚àë\ni=0N‚àí1‚àë\nj=i+1u(rij), (18.4)\n‚áífij=‚àídu(rij)\ndrij(\nxi‚àíxj\nrijÃÇex+yi‚àíyj\nrijÃÇey+zi‚àízj\nrijÃÇez)\n. (18.5)\nHererij=|ri‚àírj|=rjiis the distance between the centers of molecules iandj,a n d\nthe limits on the sums assure that no interaction is counted twice. Because we have\nassumeda conservative potential,thetotalenergyofthesystem,thatis,thepotentialplus\nkinetic energies summed over all particles, should be conserved over time. Nonetheless,\npractical computations usually ‚Äúcut the potential off‚Äù when the molecules are far apart\n[u(rij>rcut)=0], which means that the derivative du‚àïdris infinite at rcut, which is not\nwhataconservativepotentialdoes,that,inturn,meansthatoverallenergywillnolonger\nbe precisely conserved. Yet because the cutoff radius is large, the cutoff occurs when\nthe forces are minuscule, and so the violation of energy conservation should be small\ncomparedtootherapproximationsandround-offerrors.\nee\neee\neee\neee\neeee\ne\neeee\neee\ne\ne\neeeeee + +Figure 18.1 The molecule‚Äìmolecule effective interaction arises\nfrom the many-body interaction of the electrons and nucleus in one\nmolecule (circle) with the electrons and nucleus in another molecule\n(another circle). Note, the size of the nucleus at the center of each\nmolecule is highly exaggerated, and real electrons have no size.\n2 WethankSaturoKanoforclarifyingthispoint.\n18 Molecular Dynamics Simulations 393\nInatruefirst-principlescalculation,thepotentialbetweenanytwoargonatomswould\narisefromthesumofapproximately1000electron‚Äìelectronandelectron‚ÄìnucleusCoulomb\ninteractions.Amorepracticalcalculationwouldemployaneffectivepotentialderivedfrom\namany-bodytheory,suchasHartree‚ÄìFockordensityfunctionaltheory.Ourapproachis\nsimpleryet.WeusethephenomenologicalLennard‚ÄìJonespotential,\nu(r)=4ùúñ[(ùúé\nr)12\n‚àí(ùúé\nr)6]\n, (18.6)\nf(r)=‚àídu\ndrr\nr=48ùúñ\nr2[(ùúé\nr)12\n‚àí1\n2(ùúé\nr)6]\nr. (18.7)\nHeretheparameter ùúñgovernsthestrengthoftheinteraction,theparameter ùúédetermines\nthelengthscale,andbotharededucedbyfitstodata.Sometypicalvaluesfortheparameters\nandscalesforthevariablesaregiveninTable18.1.Inordertomakethesimulationsimpler\nandtoavoidunder-andoverflows,itishelpfultomeasureallvariablesinthenaturalunits\noftheseconstants.Theinterparticlepotentialandforcethentaketheforms\nu(r)=4[1\nr12‚àí1\nr6]\n,f(r)=48\nr[1\nr12‚àí1\n2r6]\n. (18.8)\nThe Lennard‚ÄìJones potential is seen in Figure 18.2 to be the sum of a long-range attrac-\ntiveinteraction ‚àù1‚àïr6andashort-rangerepulsiveone ‚àù1‚àïr12.Thechangefromrepulsion\ntoattractionoccursat r=ùúé,withthepotential‚Äôsminimumat r=21‚àï6ùúé=1.1225ùúé,which\nwouldbetheatom‚Äìatomspacinginasolidboundbythispotential.The1 ‚àïr12termaccounts\nfortheCoulombandPauliprinciplerepulsionsthatarisewhentheelectroncloudsfromtwo\natomsoverlap.This1 ‚àïr12termdominatesatshortdistancesandleadstoatomsbehaving\nlikehardspheres.Theprecisevalueof12isnotoftheoreticalsignificance(althoughitbeing\nlargeis)andmayhavebeenchosenbecauseitis2 √ó6.\nThe1‚àïr6termthatdominatesatlargedistancesmodelstheweak vanderWaals induced\ndipole‚Äìdipoleattractionbetweentwomolecules.Thisattractionarisesfromfluctuationsin\nTable 18.1 Parameter and scales for the Lennard‚ÄìJones potential.\nQuantity Mass Length Energy Time Temperature\nUnit m ùùàùùê‚àö\nmùùà2‚àïùùêùùê ‚àïkB\nValue 6.7 √ó10‚àí26kg 3.4 √ó10‚àí10m 1.65 √ó10‚àí21J4 . 5√ó10‚àí12s 119K\nFigure 18.2 The Lennard‚ÄìJones effective\npotential used in many MD simulations.\nNote the sign change at r=1 and the\nminimum at r‚âÉ1.1225 (natural units).\nNote too that because the raxis does not\nextend to r=0, the inÔ¨Ånitely high central\nrepulsion is not shown.Repulsive\nAttractionu(r)\nr0.8 1 1.2 1.4 1.6 1.8 2010\nLennard‚ÄìJones",7364
182-18.1 MD Versus Thermodynamics.pdf,182-18.1 MD Versus Thermodynamics,,0
183-18.2 Initial Boundary and Large r Conditions.pdf,183-18.2 Initial Boundary and Large r Conditions,"394 18 Molecular Dynamics Simulations\nwhich,atsomeinstantintime,amoleculeontherighttendstobemorepositiveon,say,the\nleftside,likeadipole ‚áê.This,inturn,attractsthenegativechargeinamoleculeonitsleft,\ntherebyinducingadipole ‚áê.Aslongasthemoleculesstayclosetoeachother,thepolarities\ncontinuetofluctuateinsynchronization, ‚áê‚áê,‚áí‚áí,sothattheattractionismaintained.\nThe resultant dipole‚Äìdipole attraction behaves like 1 ‚àïr6, and although it‚Äôs much weaker\nthanaCoulombforce,itisresponsibleforthebindingofneutral,inertelements,suchas\nargon,forwhichtheCoulombforcevanishes.\n18.1 MD Versus Thermodynamics\nAlthough anMDsimulationisvalidforanynumberofparticles,ifweassumethatthenum-\nberofparticlesisverylarge,thenitbecomespossibletousestatisticalmechanicstorelate\nthe results of a simulation to thermodynamic quantities. The equipartition theorem tells\nusthat,onaverage,formoleculesinthermalequilibriumattemperature T,eachdegreeof\nfreedomhasanenergy kBT‚àï2associatedwithit,where kB=1.38√ó10‚àí23J/KisBoltzmann‚Äôs\nconstant.Asimulationprovidesthekineticenergyoftranslation3:\nKE=1\n2‚ü®N‚àí1‚àë\ni=0ùë£2\ni‚ü©\n. (18.9)\nThetimeaverageof KE(forthreedegreesoffreedom)isrelatedtotemperatureby\n‚ü®KE‚ü©=N3\n2kBT‚áíT=2‚ü®KE‚ü©\n3kBN. (18.10)\nThesystem‚Äôspressure Pisdeterminedbyaversionofthe Virialtheorem ,\nPV=NkBT+ùë§\n3,ùë§=‚ü®N‚àí1‚àë\ni<jrij‚ãÖfij‚ü©\n, (18.11)\nwhere the Virial ùë§is seen to be an average of force times interparticle distances. Note\nthatbecauseidealgaseshavenointermolecularforces,theirVirialvanishes,andwewould\nobtaintheidealgaslaw.Thepressureforthegeneralcaseis\nP=ùúå\n3N(2‚ü®KE‚ü©+ùë§), (18.12)\nwhereùúå=N‚àïVisthedensityoftheparticles.\n18.2 Initial, Boundary, and Large rConditions\nAlthoughwemaystartoffanMDsimulationwithavelocitydistributioncharacteristicof\na definite temperature, this is not the true temperature of the system because it has not\nyet equilibrated. Eventually, there will be a redistribution of energy between KE and PE\n[Thijssen,1999],andthenthesystemwillhaveatruetemperature.Itisinterestingtonote\nthat this initial random distribution is the only place where chance enters into our MD\nsimulation, and it is put there only to speed up the equilibration. Once started, the time\nevolution of the MD system is determined by Newton‚Äôs laws, in contrast to Monte Carlo\nsimulations,whichareinherentlystochastic.\n3 Unlessthetemperatureisveryhigh,argonatoms,beinginertspheres,havenorotationalenergy.\n18.2 Initial, Boundary, and Large r Conditions 395\nIt is easy to believe that a simulation of 1023molecules might predict bulk properties\nwell, but with MD simulations employing only 106‚Äì109particles, one must be clever to\nmakelessseemlikemore.Furthermore,becausecomputersarefinite,themoleculesinthe\nsimulationareconstrainedtoliewithinafinitebox,whichinevitablyintroducesartificial\nsurfaceeffects arisingfromthewalls.Surfaceeffectsareparticularlysignificantwhenthe\nnumberofparticlesissmallbecausealargefractionofthemoleculesresidenearthewalls.\nForexample,if1000particlesarearrangedina10 √ó10√ó10cube,therearethen103‚Äì83=\n488particlesoneunitawayfromthesurface,thatis,49%ofthemolecules.For106particles,\nthisfractionfallsto6%.\nTheimpositionof periodicboundaryconditions (PBCs)strivestominimizetheshortcom-\ningsofboththesmallnumbersofparticlesandtheartificialboundaries.Althoughwelimit\nour simulationto an Lx√óLy√óLzbox,we imaginethisbox beingreplicatedtoinfinityin\nalldirections(Figure18.3).Accordingly,aftereachtime-integrationstep,weexaminethe\npositionofeachparticleandcheckifithasleftthesimulationregion.Ifithas,thenwebring\nanimageoftheparticlebackthroughtheoppositeboundary(Figure18.3):\nx‚áí{x+Lx,ifx‚â§0,\nx‚àíLx,ifx>Lx.(18.13)\nConsequently, each box looks the same and has continuous properties at the edges.\nAs shown by the one-headed arrows in Figure 18.3, if a particle exits the simulation\nvolume,itsimageentersfromtheotherside,andthusbalanceismaintained.\nIn principle, a molecule interacts with all other molecules and all of their images, so\ndespite the fact that there are a finite number of atoms in the interaction volume, there\n4\n53 2\n1 4\n53 2\n1 4\n53 2\n1\n4\n53\n1 4\n53 2\n1 4\n53 2\n1\n4\n53 2\n1 4\n53 2\n1 4\n53 2\n12\nFigure 18.3 An imagined inÔ¨Ånite space generated by imposing periodic boundary conditions on\nthe particles within the simulation volume (shaded box). The two-headed arrows indicate how a\nparticle interacts with the nearest version of another particle, be that within the simulation volume\nor an image. The vertical arrows indicate how the image of particle 4 enters when the actual\nparticle 4 exits.",4548
184-18.3.2 Analysis.pdf,184-18.3.2 Analysis,"396 18 Molecular Dynamics Simulations\nshould be an infinite number of interactions [Ercolessi, 1997]. Nonetheless, because\nthe Lennard‚ÄìJones potential falls off so rapidly for large r,V(r=3ùúé)‚âÉV(1.13ùúé)‚àï200,\nfar-off molecules do not contribute significantly to the motion of a molecule. An so we\npick a value of the cutoff radius rcut‚âÉ2.5ùúé, beyond which we ignore the effect of the\npotential:\nu(r)={4(r‚àí12‚àír‚àí6),forr<rcut,\n0, forr>rcut.(18.14)\nAccordingly,ifthesimulationregionislargeenoughfor u(r>Li‚àï2)‚âÉ0,anatominteracts\nwithonlythe nearestimage ofanotheratom.\nAsalreadyindicated,ashortcomingwiththecutoffpotential(18.14)isthatbecausethe\nderivativedu‚àïdris singular at r=rcut, the potential is no longer conservative, and thus\nenergy conservation is no longer ensured. However, because the forces are very small at\nrcut,theviolationis,presumably,verysmall.\n18.3 Verlet Algorithms\nArealistic,butsmall,MDsimulationmayrequireintegrationofthe3Dequationsofmotion\nfor1010timestepsforeachof103‚Äì106particles.Althoughwecoulduseourstandard rk4\nODE solver for this, time is saved by using a simpler rule. The Verlet algorithm uses the\ncentral-differenceapproximation(Chapter5)forthesecondderivativetoadvancethesolu-\ntionssimultaneouslybyasingletimestep hforallNparticles:\nFi[r(t),t]=d2ri\ndt2‚âÉri(t+h)+ri(t‚àíh)‚àí2ri(t)\nh2, (18.15)\n‚áíri(t+h)‚âÉ2ri(t)‚àíri(t‚àíh)+h2Fi(t)+O(h4), (18.16)\nwherewehaveset m=1.(Improvedalgorithmsmayvarythetimestepdependinguponthe\nspeedoftheparticle.)Noticethatalthoughtheatom‚Äìatomforcedoesnothaveanexplicit\ntimedependence,weincludeanimplicit tdependenceinitasawayofindicatingitsdepen-\ndenceupontheotheratoms‚Äôpositionsatthatparticulartime.\nPart of the efficiency of the Verlet algorithm (18.16) lies in its solving for the position\nofeachparticlewithoutrequiringaseparatesolutionfortheparticle‚Äôsvelocity.However,\nonce we have deduced the position for various times, we can use the central-difference\napproximationforthefirstderivativeof ritoobtainthevelocity:\nvi(t)=dri\ndt‚âÉri(t+h)‚àíri(t‚àíh)\n2h+O(h2). (18.17)\nFinally, note that because the Verlet algorithm needs rfrom two previous steps, it is not\nself-starting,andsomustbestartedwithaforwarddifferencederivative,\nr(t=‚àíh)‚âÉr(0)‚àíhv(0)+h2\n2F(0). (18.18)\nVelocity-Verlet Algorithm Another version of the Verlet algorithm, which we recom-\nmend because of its increased stability, uses a forward-difference approximation for the\nderivativetoadvance boththepositionandvelocitysimultaneously:\nri(t+h)‚âÉri(t)+hvi(t)+h2\n2Fi(t)+O(h3), (18.19)\n18.3 Verlet Algorithms 397\nvi(t+h)‚âÉvi(t)+ha(t)+O(h2), (18.20)\n‚âÉvi(t)+h[Fi(t+h)+Fi(t)\n2]\n+O(h2). (18.21)\nAlthoughthisalgorithmappearstobeoflowerorderthan(18.16),theuseofupdatedposi-\ntions when calculating velocities, and the subsequent use of these velocities, give both\nalgorithmssimilarprecision.\nOfinterestisthat(18.21)approximatestheaverageforceduringatimestepas [Fi(t+h)+\nFi(t)]‚àï2.Updatingthevelocityisalittletrickybecauseweneedtheforceattime t+h,which\ndepends on the particle positions at t+h. Consequently, we must update all the particle\npositionsandforcesto t+hbeforeupdatingvelocities,whilesavingtheforcesatanearlier\ntimeforusein(18.21).Assoonasthepositionsareupdated,weimposePBCstoestablish\nthatwehavenotlostanyparticlesandthencalculatetheforces.\n18.3.1 Implementation and Exercise\nIn theonlinematerialsforthisbook,youwillfindanumberofanimations(movies)ofsolu-\ntionstotheMDequations.SomeframesfromtheseanimationsareshowninFigure18.4.\nTheprogram MD1D.pyinListing18.1implementsa1Dsimulationusingthevelocity-Verlet\nalgorithm, MD2D.pyinListing18.2implementsa2Dsimulation,and MDpBC.pyinListing18.3\nimplementsa2DsimulationwithPBCs.Usetheseasmodelsforthefollowing:\n1) Establishthatyoucanrunandvisualize MD2D.py.\n2) Place the particles initially at the sites of a simple cubic lattice. The equilibrium con-\nfigurationforaLennard‚ÄìJonessystematlowtemperatureisaface-centeredcubic,and\nif your simulation is running properly, then, as we show in Figure 18.4, the particles\nshouldmigratefromsimplecubic(SC)toface-centeredcubic(FCC).AnFCClatticehas\nfour-quartersofaparticleperunitcell,soan L3boxwithalatticeconstant L‚àïNcontains\n(partsof)4 N3=32,108,256, ‚Ä¶particles.\n3) To save computing time, assign initial particle velocities corresponding to a fixed-\ntemperature Maxwellian distribution. (Recall, the sums of uniform random numbers\nfollowaGaussiandistribution.)\nFigure 18.4 Left: Two frames from an animation of a 1D simulation. The spaces on the left and\nright differ slightly in these two frames due to an image atom moving off the frame to the right and\nthen popping up on the left. Right: Two frames from the animation of a 2D simulation showing the\ninitial and equilibrated states. Note how the atoms start off in a simple cubic arrangement, but\nthen equilibrate to a face-centered cubic lattice. In both the 1D and 2D simulations, the atoms\nremain conÔ¨Åned as a result of the interatomic forces.\n398 18 Molecular Dynamics Simulations\n4) Printthecodeandindicateonitwhichintegrationalgorithmisused,wherethePBCs\nareimposed,wherethenearestimageinteractionisevaluated,andwherethepotential\niscutoff.\n5) Atypicaltimestepis Œît=10‚àí14s,whichinournaturalunitsequals0.004.Youprobably\nwillneedtomake104‚Äì105suchstepstoequilibrate,whichcorrespondstoatotaltime\nofonly10‚àí9s(alotcanhappentoaspeedymoleculein10‚àí9s).Choosethe largesttime\nstepthatprovidesstabilityandgivesresultssimilartoFigure18.5.\n6) ThePEandKEchangewithtimeasthesystemequilibrates.Evenafterthat,therewill\nbefluctuationsinthembecausethisisadynamicsystem.Evaluatethetime-averaged\nenergiesforanequilibratedsystem.\n7) Compare the final temperature of your system to the initial temperature. Change the\ninitialtemperatureandlookforasimplerelationbetweenitandthefinaltemperature\n(Figure18.6).\n18.3.2 Analysis\n1) Modifyyourprogramsothatitoutputsthecoordinatesandvelocitiesofafewparticles\nthroughout the simulation. Note that you do not need as many time steps to follow a\ntrajectoryasyoudotocomputeit,andsoyoumaywanttousethe modoperator %100for\noutput.\n2) Startyourassessmentwitha1Dsimulationatzerotemperature.Theparticlesshould\nremaininplacewithoutvibration.Increasethetemperatureandnotehowtheparticles\nbegintomoveaboutandinteract.\n3) Try starting off all your particles at the minima in the Lennard‚ÄìJones potential. The\nparticlesshouldremainboundwithinthepotentialatlowtemperatures.\n4) Repeatthesimulationsfora2Dsystem.Thetrajectoriesshouldresemblebilliardball-\nlikecollisions.\n5) Createananimationofthetime-dependentlocationsofseveralparticles.\n6) Calculate and plot the root-mean-square displacement of molecules as a function of\ntemperature:\nRrms=‚àö‚ü®\n|r(t+Œît)‚àír(t)|2‚ü©, (18.22)\nwheretheaverageisoveralltheparticlesinthebox.Determinetheapproximatetime\ndependenceof Rrms.\n7) Testyoursystemfortime-reversalinvariance.Stopitatafixedtime,reverseallvelocities,\nandseeifthesystemretracesitstrajectoriesbacktotheinitialconfigurationafterthis\nsamefixedtime.\n8)Diffusion:Itiswellknownthatlightmoleculesdiffusemorequicklythanheavierones.\nSeeifyoucansimulatediffusionwithyourMDsimulationusingaLennard‚ÄìJonespoten-\ntialandPBCs[Satoh,2011].\n(a) Generalize the velocity-Verlet algorithm so that it can be used for molecules of\ndifferentmasses.\n(b) Modifythesimulationcodesothatitcanbeusedforfiveheavymoleculesofmass\nM=10andfivelightmoleculesofmass m=1.\n(c) Startwiththemoleculesplacedrandomlynearthecenterofthesquaresimulation\nregion.\n18.3 Verlet Algorithms 399\n(d) Assignrandominitialvelocitiestothemolecules.\n(e) Runthesimulationseveraltimesandverifyvisuallythatthelightermoleculestend\ntodiffusemorequicklythantheheavierones.\n(f) Foreachensembleofmolecules,calculatethermsvelocityatregularinstancesof\ntime,andthenplotthermsvelocitiesasfunctionsoftime.Dothelighterparticles\nhaveagreaterrmsvelocity?\nEnergy (J)KE\nVE\n2E‚Äì13 0 4E‚Äì13 6E‚Äì13 8E‚Äì13 1E‚Äì12 1.2E‚Äì122E‚Äì12 4E‚Äì12 6E‚Äì12 8E‚Äì12 1E‚Äì11 1.2E‚Äì11 0Energy (J)8.00E‚Äì19\n6.00E‚Äì19\n4.00E‚Äì19\n2.00E‚Äì19\n0.00E+00\n‚Äì2.00E‚Äì19\n‚Äì4.00E‚Äì19\n‚Äì6.00E‚Äì19\n‚Äì8.00E‚Äì19\n‚Äì1.00E‚Äì18\n‚Äì1.20E‚Äì18\n‚Äì1.40E‚Äì180.00E+005.00E‚Äì201.00E‚Äì191.50E‚Äì19\n‚Äì5.00E‚Äì20\n‚Äì1.00E‚Äì19\n‚Äì1.50E‚Äì19\n2.00E‚Äì19\nTime (s) (568 steps)Time (s) (5000 steps)\nKE\nVEEnergy versus time\nfor 300 particles in a 2D box, initially at 150 kEnergy versus time\nfor 36 particles in a 2D box, initially at 150 k\n1.4E‚Äì12\nFigure 18.5 The kinetic, potential, and total energy for a 2D MD simulation with 36 particles ( top),\nand 300 particles ( bottom ), both with an initial temperature of 150 K. The potential energy is\nnegative, the kinetic energy is positive, and the total energy is seen to be conserved (Ô¨Çat).",8646
185-18.4 MD for 16 Particles.pdf,185-18.4 MD for 16 Particles,"400 18 Molecular Dynamics Simulations\n0100200300\nFinal temperature (K)\n200 E‚Äì19 100 E‚Äì19\nInitial KE (j)\nP\n12\n0\n0 0.1 0.2 0.3\nT\nFigure 18.6 Left: The temperature after equilibration as a function of initial kinetic energy for a\n2D MD simulation with 36 particles. Right: The pressure versus temperature for a simulation with\nseveral hundred particles. An ideal gas (noninteracting particles) would yield a straight line.\n(Courtesy of J. Wetzel.)\n18.4 MD for 16 Particles\nA small number of particles are placed in a box. The forces between the particles derive\nfrom the Lennard‚ÄìJones potential. A number of independent snapshots are taken of the\nparticlesinthebox,andthenumber NrhsofparticlesontheRHSoftheboxisrecordedfor\neach.Ifnisthenumberofframesthatshow Nrhsparticlesintheright-handside,thenthe\nprobabilityoffinding NrhsparticlesontheRHSis:\nÓàº(n)=C(n)\n2Nrhs. (18.23)\nHereC(n)isthenumberofwaysofplacing nparticlesinthelefthalfofthebox.\n1) ModifythepreviouslydevelopedMDprogramsothatitrunsfor16particlesinsidea2D\nboxofsideL=1.AssumePBCs,andcomputethepositionsandvelocitiesoftheparticles\nusingthevelocity-Verletalgorithm(18.21).\n(a) Extend the program so that at the end of each time step, it counts the number of\nparticlesNrhsontheRHSofthebox.\n(b) Create,plot,andupdatecontinuallyahistogramcontainingthedistributionofthe\nnumberoftimes nthataNrhsvalueoccurs,asafunctionof Nrhs.\n(c) Make a histogram showing the probability (18.23) of finding Nrhsparticles on the\nRHS,asafunctionof Nrhs.\n(d) CompareyourplotstothoseinFigure18.7,createdby MDpBC.py.\n2) EventhoughanMDsimulationisdeterministic,theparticlesdotendtoequilibrateafter\narathersmallnumberofcollisions,inwhichcasethesystemresemblesathermalone.\nThisisconsistentwiththeresultfromergodictheorythat,afteralongtime,adynamical\nsystemtendstoforgetitsinitialstate.Testthishypothesisbyrandomlyassigningseveral\ndifferentsetsofinitialpositionsandvelocitiestothe16particles,andthendetermining\nthedistributionsforeachinitialcondition.Ifthehypothesisisvalid,thedistributions\nshouldbemuchthesame.\n3) Useyoursimulationtodeterminethevelocitydistributionoftheparticles.\n(a) Createahistogrambyplottingthenumberofparticleswithavelocityintherange\nùë£toùë£+Œîùë£versusùë£.\n(b) Startwithrandomvaluesfortheinitialpositionsoftheparticles.\n18.4 MD for 16 Particles 401\n(c) Start all particles off with the same speed ùë£0, though with random values for\ndirections.\n(d) Update the histogram after each step and continue until it looks like a normal\ndistribution.\n4) Computeandplottheheatcapacityataconstantvolume, CV=ùúïE‚àïùúïT,asafunctionof\ntemperatureforthe16particlesinabox.\n(a) Asbefore,startwithrandomvaluesfortheinitialpositionsoftheparticles.\n(b) Start all particles off with the same speed ùë£0, though with random values for\ndirections.\n(c) Takeanaverageofthetemperaturefor10initialconditions,allwiththesame ùë£0.\nRelatethetemperaturetothetotalenergy.\n(d) Repeat the computations for increasing values of ùë£0. (We suggest 0.5 ‚â§ùë£0‚â§20 in\nstepsof1.)\n(e) Plotthetotalenergyasafunctionoftheaveragetemperature.\n(f) Use numerical differentiation to evaluate the heat capacity at a constant volume,\nCV=ùúïE‚àïùúïT.Unlessyouchangetheparametersofthesystem,youshouldexpect CV\ntobeconstantwithinstatisticalfluctuations.Resultsofourcalculationareshown\ninFigure18.8.\n(g) Exploretheeffectofaprojectilehittingagroupofparticles,aswedoinFigure18.9.\nFigure 18.7 Top Left : Positions of particles at a single time. Top Right : Distribution showing the\nnumber of times Nrhsparticles are present in the RHS of the box. Lower Left : The probability\ndistribution for Ô¨Ånding Nrhsparticles in the RHS of the box.\n16 particles in the box.",3687
186-18.5 Code Listing.pdf,186-18.5 Code Listing,"402 18 Molecular Dynamics Simulations\nFigure 18.8 Left: The total energy versus temperature for 16 particles in a box. Right: The heat\ncapacity at constant volume versus temperature for 16 particles in a box.\nFigure 18.9 A simulation of a projectile shot into a group of particles. The energy introduced by\nthe projectile is seen to lead to evaporation of the particles. (Courtesy of J. Wetzel.)\n18.5 Code Listing\nListing18.1 MD1D.py A1DMDsimulationwithtoosmallanumberoftoolargetime\nsteps.\n# MD1.py Molecular dynamics in 1D\nfromvisualimport ‚àó\n4fromvisual.graph import ‚àó\nimportrandom\nscene = display(x=0,y=0,width=700,height=350, title= ‚ÄôMolecular Dynamics‚Äô ,\n8 range=12) # plot spheres\nsceneK = gdisplay(x=0,y=350,width=600,height=150,title= ‚ÄôAverage KE‚Äô ,\nymin=0.0,ymax=0.3,xmin=0,xmax=100,xtitle= ‚Äôtime‚Äô,ytitle= ‚ÄôKE avg‚Äô )\nKavegraph=gcurve(color= color.red) #p l o tK E\n12scenePE = gdisplay(x=0,y=500,width=600,height=150,title= ‚ÄôPot Energy‚Äô ,\nymin=‚àí0.6,ymax=0.0,xmin=0,xmax=100,xtitle= ‚Äôtime‚Äô,ytitle= ‚ÄôPE‚Äô)\nPEcurve = gcurve(color=color.cyan)\n18.5 Code Listing 403\nNatom = 8\n16Nmax = 8\nTinit = 10.0 # T initial\nt1 = 0\nx = zeros( (Nmax) , float)\n20vx = zeros( (Nmax) , float)\nfx = zeros( (Nmax, 2) , float)\nL = Natom # Length of atom chain\natoms = []\n24\ndeftwelveran(): # Gaussian as average 12 randoms\ns=0 . 0\nforiin range (1,13):\n28 s += random.random()\nreturns/12.‚àí0.5\ndefinitialposvel(): # Initial positions, velocities\ni=‚àí1\n32forixin range (0, L):\ni=i+1\nx[i] = ix\nvx[i] = twelveran()\n36 vx[i] = vx[i] ‚àósqrt(Tinit)\nforjin range (0,Natom):\nxc = 2 ‚àóx[j]‚àí7 # Linear transform to place spheres\natoms.append(sphere(pos=(xc,0), radius=0.5,color=color.red))\n40defsign(a, b):\nif(b >= 0.0):\nreturn abs (a)\nelse:\n44 return ‚àíabs(a)\ndefForces(t, PE): #F o r c e s\nr2cut = 9. #C u t o f f\nPE = 0.\n48foriin range (0, Natom):\nfx[i][t] = 0.0\nforiin range ( 0, Natom ‚àí1) :\nforjin range (i + 1, Natom):\n52 dx = x[i] ‚àíx[j]\nif(abs(dx) > 0.50 ‚àóL):\ndx = dx ‚àísign(L, dx) # Interact with closer image\nr2 = dx ‚àódx\n56 if(r2 < r2cut):\nif(r2 = = 0.): # Avoid 0 denominator\nr2 = 0.0001\ninvr2 = 1./r2\n60 wij = 48. ‚àó(invr2 ‚àó‚àó3‚àí0.5) ‚àóinvr2 ‚àó‚àó3\nfijx = wij ‚àóinvr2 ‚àódx\nfx[i][t] = fx[i][t] + fijx\nfx[j][t] = fx[j][t] ‚àífijx # opposite sense next i\n64 PE = PE + 4. ‚àó(invr2 ‚àó‚àó3)‚àó((invr2 ‚àó‚àó3)‚àí1.)\nreturnPE\ndeftimevolution():\nt1=0\n68t2 = 1\nh = 0.038 # Unstable if larger\nhover2 = h/2.0\nKE = 0.0\n72PE = 0.0\ninitialposvel()\nPE = Forces(t1 ,PE)\nforiin range (0, Natom): # Kinetic energy\n76 KE=KE+(vx[ i] ‚àóvx[i])/2.0\nt=0\nwhilet<100: # Time loop\nrate(1)\n80 foriin range (0, Natom):\nPE = Forces(t1 ,PE)\nx[i] = x[i] + h ‚àó(vx[i] + hover2 ‚àófx[i][t1])\nifx[i] <= 0.:\n84 x[i] = x[i] +L # Periodic boundary conditions\nifx[i] >= L :\n404 18 Molecular Dynamics Simulations\nx[i] = x[i] ‚àíL\nxc = 2 ‚àóx[i]‚àí8 # Linear transform to plot atoms\n88 atoms[i].pos=(xc,0)\nPE = 0.0\nPE = Forces(t2 , PE)\nKE = 0.\n92 foriin range (0 , Natom):\nvx[i] = vx[i] + hover2 ‚àó(fx[i][t1] + fx[i][t2])\nKE = KE + (vx[ i] ‚àóvx[i] )/2\nT=2 ‚àóKE/(3 ‚àóNatom)\n96 Itemp = t1\nt1 = t2\nt2 = Itemp\nKavegraph.plot(pos=(t,KE)) #P l o tK E\n100 PEcurve.plot(pos=(t,PE),display=scenePE) #P l o tP E\nt+ =1\ntimevolution()\nListing18.2 MD2D.py A2DMDsimulationwithtoosmallanumberoftoolargetime\nsteps.\n# MD2D.py: Molecular dynamics in 2D\nfromvisualimport ‚àó\n4fromvisual.graph import ‚àó\nimportrandom\nscene = display(x=0,y=0,width=350,height=350, title= ‚ÄôMolecular Dynamics‚Äô ,\n8 range=10)\nsceneK = gdisplay(x=0,y=350,width=600,height=150,title= ‚ÄôAverage KE‚Äô ,\nymin=0.0,ymax=5.0,xmin=0,xmax=500,xtitle= ‚Äôtime‚Äô,ytitle= ‚ÄôKE avg‚Äô )\nKavegraph=gcurve(color= color.red)\n12sceneT = gdisplay(x=0,y=500,width=600,height=150,title= ‚ÄôAverage PE‚Äô ,\nymin=‚àí60,ymax=0.,xmin=0,xmax=500,xtitle= ‚Äôtime‚Äô,ytitle= ‚ÄôPE avg‚Äô )\nTcurve = gcurve(color=color.cyan)\nNatom = 25; Nmax = 25; Tinit = 2.; dens = 1.;t1 = 0 # Den 1.20 for fcc\n16x = zeros( (Nmax) , float)\ny = zeros( (Nmax) , float)\nvx = zeros( (Nmax) , float)\nvy = zeros( (Nmax) , float)\n20fx = zeros( (Nmax, 2) , float)\nfy = zeros( (Nmax, 2) , float)\nL=int(1.‚àóNatom ‚àó‚àó0.5) # Side of lattice\natoms=[]\n24\ndeftwelveran(): # Average 12 rands for Gaussian\ns=0.0\nforiin range (1,13):\n28 s += random.random()\nreturns/12.0‚àí0.5\ndefinitialposvel(): # Initialize\ni=‚àí1\n32forixin range (0, L): #x‚àí> 01234\nforiyin range (0, L): # y=0 0 5 10 15 20\ni=i+1 # y=1 1 6 11 16 21\nx[i] = ix # y=2 2 7 12 17 22\n36 y[i] = iy # y=3 3 8 13 18 23\nvx[i] = twelveran() # y=4 4 9 14 19 24\nvy[i] = twelveran() # numbering of 25 atoms\nvx[i] = vx[i] ‚àósqrt(Tinit)\n40 vy[i] = vy[i] ‚àósqrt(Tinit)\nforjin range (0,Natom):\nxc = 2 ‚àóx[j]‚àí4\nyc = 2 ‚àóy[j]‚àí4\n44 atoms.append(sphere(pos=(xc,yc), radius=0.5,color=color.red))\ndefsign(a, b):\nif(b >= 0.0): return abs (a)\nelse:return ‚àíabs(a)\n18.5 Code Listing 405\n48defForces(t, w, PE, PEorW): #F o r c e s\n#i n v r 2=0 .\nr2cut = 9. #S w i t c h :P E o r W=1f o rP E\nPE = 0.\n52foriin range (0, Natom):\nfx[i][t] = fy[i][t] = 0.0\nforiin range ( 0, Natom ‚àí1) :\nforjin range (i + 1, Natom):\n56 dx = x[i] ‚àíx[j]\ndy = y[i] ‚àíy[j]\nif(abs(dx) > 0.50 ‚àóL):\ndx = dx ‚àísign(L, dx) # Interact with closer image\n60 if(abs(dy) > 0.50 ‚àóL):\ndy = dy ‚àísign(L, dy)\nr2 = dx ‚àódx + dy ‚àódy\nif(r2 < r2cut):\n64 if(r2 = = 0.): # To avoid 0 denominator\nr2 = 0.0001\ninvr2 = 1./r2\nwij = 48. ‚àó(invr2 ‚àó‚àó3‚àí0.5) ‚àóinvr2 ‚àó‚àó3\n68 fijx = wij ‚àóinvr2 ‚àódx\nfijy = wij ‚àóinvr2 ‚àódy\nfx[i][t] = fx[i][t] + fijx\nfy[i][t] = fy[i][t] + fijy\n72 fx[j][t] = fx[j][t] ‚àífijx\nfy[j][t] = fy[j][t] ‚àífijy\nPE = PE + 4. ‚àó(invr2 ‚àó‚àó3)‚àó((invr2 ‚àó‚àó3)‚àí1.)\nw=w + w i j\n76if(PEorW == 1):\nreturnPE\nelse:\nreturnw\n80deftimevolution():\navT = 0.0\navP = 0.0\nPavg = 0.0\n84avKE = 0.0\navPE = 0.0\nt1 = 0\nPE = 0.0\n88h = 0.031 # step\nhover2 = h/2.0\nKE = 0.0\nw=0 . 0\n92initialposvel()\nforiin range (0, Natom):\nKE = KE+(vx[ i] ‚àóvx[i]+vy[i] ‚àóvy[i])/2.0\n# System.out.println(""""+t+"" PE= ""+PE+"" KE = ""+KE+"" PE+KE = ""+(PE+KE)) ;\n96PE = Forces(t1 ,w,PE,1)\ntime =1\nwhile1:\nrate(100)\n100 foriin range (0, Natom):\nPE = Forces(t1 ,w,PE,1)\nx[i] = x[i] + h ‚àó(vx[i] + hover2 ‚àófx[i][t1])\ny[i] = y[i] +h ‚àó(vy[i] + hover2 ‚àófy[i][t1]);\n104 ifx[i] <= 0.: x[i] = x[i] + L # Periodic BC\nifx[i] >= L : x[i] = x[i] ‚àíL\nify[i] < = 0.: y[i] = y[i] +L\nify[i] > =L: y[i] = y[i] ‚àíL\n108 xc = 2 ‚àóx[i]‚àí4\nyc = 2 ‚àóy[i]‚àí4\natoms[i].pos=(xc,yc)\nPE = 0.\n112 t2=1\nPE = Forces(t2 , w, PE, 1)\nKE = 0.\nw=0 .\n116 foriin range (0 , Natom):\nvx[i] = vx[i] + hover2 ‚àó(fx[i][t1] + fx[i][t2])\nvy[i] = vy[i] + hover2 ‚àó(fy[i][t1] + fy[i][t2])\n406 18 Molecular Dynamics Simulations\nKE = KE + (vx[ i] ‚àóvx[i] + vy[i] ‚àóvy[i])/2\n120 w = Forces(t2, w, PE, 2)\nP=dens ‚àó(KE+w)\nT=KE/(Natom)\n# increment averages\n124 avT = avT + T\navP = avP + P\navKE = avKE + KE\navPE = avPE + PE\n128 time += 1\nt=time\nif(t==0):\nt=1\n132 Pavg = avP /t\neKavg = avKE /t\nePavg = avPE /t\nTavg = avT /t\n136 pre = (int)(Pavg ‚àó1000)\nPavg = pre/1000.0\nkener = ( int)(eKavg ‚àó1000)\neKavg = kener/1000.0\n140 Kavegraph.plot(pos=(t,eKavg))\npener = ( int)(ePavg ‚àó1000)\nePavg = pener/1000.0\ntempe = ( int)(Tavg ‚àó1000000)\n144 Tavg = tempe/1000000.0\nTcurve.plot(pos=(t,ePavg),display=sceneT)\ntimevolution()\nListing18.3 MDpBC.py A2DMDsimulationwithperiodicboundaryconditions.\n#M D p B C . p y:2 ‚àíDMD with Periodic BC\nfromvisual.graph import ‚àó\n4importrandom\nL = 1; Natom = 16; Nrhs = 0; dt = 1e ‚àí6\nscene = display(width = 500,height = 500, range= (1.3) )\n8ndist = gdisplay(x = 500, ymax = 200,\nwidth = 500, height = 500, xtitle = ‚ÄôNrhs‚Äô, ytitle = ‚ÄôN‚Äô)\ninside = label(pos = (0.4,1.1),text = ‚ÄôPRatomticles here = ‚Äô ,box = 0)\ninside2 = label(pos = (0.8,1.1),box = 0)\n12border = curve(pos = [( ‚àíL,‚àíL),(L,‚àíL),(L,L),( ‚àíL,L),(‚àíL,‚àíL)])# Limits fig\nhalf = curve(pos = [(0, ‚àíL),(0,L)],color = color.yellow) # Middle\npositions = []\nvel = []\n16Atom = [] # For Spheres\ndN = [] # Atoms in right half\nfr = [0] ‚àó(Natom) # Atoms (spheres)\nfr2 = [0] ‚àó(Natom) # second force\n20Ratom = 0.03 # Radius of atom\npref = 4 # Reference velocity\nh = 0.01\nfactor = 1e ‚àí9 #L e n nJ o n e s\n24deltaN = 1 # For histogram\ndistribution = ghistogram(bins=Ratomange(0.,Natom,deltaN),\naccumulate=1, average=1, color=color.red)\nforiin range (0,Natom): # Initial r &v\n28col = (1.3 ‚àórandom.random() ,1.3 ‚àórandom.random() ,1.3 ‚àórandom.random())\nx= 2 . ‚àó(L‚àíRatom) ‚àórandom.random() ‚àíL+Ratom # Positons atoms\ny=2 . ‚àó(L‚àíRatom) ‚àórandom.random() ‚àíL+Ratom # Border forbidden\nAtom = Atom+[sphere(pos=(x,y),radius=Ratom,color=col)] #A d da t o m s\n32theta = 2 ‚àópi‚àórandom.random() # Select angle 0 <=theta<=2 p i\nvx = pref ‚àócos(theta) # x component velocity\nvy = pref ‚àósin(theta)\npositions.append((x,y)) # Add positions to list\n36vel.append((vx,vy)) # Add momentum to list\npos = Ratomray(positions) # Ratomray with positions\nddp = pos[i]\n18.5 Code Listing 407\nifddp[0] >=0 andddp[0] <=L: # count atoms right half\n40 Nrhs+=1\nv = Ratomray(vel)\ndefsign(a, b): # Sign function\nif(b >= 0.0): return abs (a)\n44else:return ‚àíabs(a)\ndefforces(fr):\nfr=[0] ‚àó(Natom)\nforiin range ( 0, Natom ‚àí1) :\n48 forjin range (i + 1, Natom):\ndr = pos[i] ‚àípos[j] # relative position\nif(abs(dr[0]) > L): # smallest distance or image\ndr[0] = dr[0] ‚àísign(2 ‚àóL, dr[0]) # interact closer image\n52 if(abs(dr[1]) > L):\ndr[1] = dr[1] ‚àísign(2 ‚àóL, dr[1])\nifi= =0andj= =1 :\ncurve(pos=[(pos[0]) ,(pos[0] ‚àídr)])\n56 r2 = mag2(dr)\nif(abs(r2) < Ratom): # to avoid 0 denominator\nr2 = Ratom\ninvr2 = 1./r2\n60 fij =invr2 ‚àófactor ‚àó48.‚àó(invr2 ‚àó‚àó3‚àí0.5) ‚àóinvr2 ‚àó‚àó3\nfr[i] = ij ‚àódr+ fr[i]\nfr[j]=‚àífij‚àódr +fr[j]\nreturnfr\n64fortin range (0,1000):\nNrhs = 0 # begin 0 each time\nforiin range (0,Natom):\nfr = orces(fr)\n68 dpos=pos[i]\nifdpos[0] <= ‚àíL:\npos[i] = [dpos[0]+2 ‚àóL,dpos[1]] # x periodic BC\nifdpos[0] >= L:\n72 p o s [ i ]=[ d p o s [ 0 ] ‚àí2‚àóL,dpos[1]]\nifdpos[1] <= ‚àíL:\npos[i] = [dpos[0],dpos[1]+2 ‚àóL] # y periodic BC\nifdpos[1] >= L:\n76 pos[i] = [dpos[0],dpos[1] ‚àí2‚àóL]\ndpos=pos[i]\nifdpos[0] > 0 anddpos[0] <L : # count at right\nNrhs+=1\n80 fr2 = orces(fr)\nfr2 = r\nv[i] = v[i]+0.5 ‚àóh‚àóh‚àó(fr[i]+fr2[i]) # velocity Verlet\npos[i] = pos[i]+h ‚àóv[i]+0.5 ‚àóh‚àóh‚àófr[i]\n84 Atom[i].pos = pos[i] # plot new positions\ninside2.text = ‚Äô%4s‚Äô%Nrhs #R H S\ndN.append(Nrhs) # for histogram\ndistribution.plot(data = dN) # plot histogram",10288
187-Chapter 19 General Relativity.pdf,187-Chapter 19 General Relativity,,0
188-19.1 Einsteins Field Equations.pdf,188-19.1 Einsteins Field Equations,"408\n19\nGeneral Relativity\nThis chapter on general relativity (GR) is new for this 4thedition. It‚Äôs here in response\nto requests, and also in response to recent developments in Einstein lensing, exoplanets,\nblack holes, and computational GR. We review some GR theories, compute some GR\ntensors, and then apply the theory to the deÔ¨Çection of starlight by stars, to gravitational\nlensing, to corrections to planetary orbits, and to the visualization of wormholes as seen\nin movies .\nProblem GRtellsusthatthespaceweliveiniscurved,notflat,andthatthiscurvatureis\ntheoriginofthegravitationalforce.Yourproblemistodeterminehowmuchofadifference\ndoGReffectsmakeinobservablephenomena.\n19.1 Einstein‚Äôs Field Equations\nEinstein‚Äôs theory of GR postulates that the presence of matter or energy in a region of\nspace distorts the spacetime there. The resulting local curvature of space is the origin of\nthegravitationalforce,andtherebyprovidesalinkbetweengeometryanddynamics.The\ntheoryisexpressedsuccinctlybythe Einsteinfieldequations ,whichinasimpleformare:\n[Hartle,2003]\nRùúáùúà‚àí1\n2Rgùúáùúà+Œõgùúáùúà=ùúÖTùúáùúà, (19.1)\nwherethestandardistohaveeachtermwithdimensionof1/length2.HeretheR‚Äôs,about\nwhich we‚Äôll talk more about soon, describe the curvature of spacetime, gùúáùúàis the metric\ntensor(‚Äúthemetric‚Äù)thatdescribesthepathlength, Œõisthecosmologicalconstant, Tùúáùúàis\ntheenergy-stresstensor,and ùúÖistheEinstein gravitationalconstant ,\nùúÖ=8ùúãG\nc4‚âÉ2.077√ó10‚àí43N‚àí1, (19.2)\nwhereGisNewton‚Äôsgravitationalconstant.Einsteinaddedthe cosmologicalconstant Œõto\ntheoriginalformofhisequationstoexplainauniversethatneithercontractsnorexpands.\nItisnowbelievedthat Œõisneededtoexplaintheacceleratingexpansionoftheuniversedue\ntodarkenergyinthevacuumofspace.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n19.1 Einstein‚Äôs Field Equations 409\nTounravel(19.1)abit,westartwiththemetric gùúáùúà,which,bydefininghowtocompute\nthedistancebetweentwopoints,providesthebasicdescriptionofthelocalspacetime:\nds2=g11dx2\n1+g12dx1dx2+g22dx2\n2+‚Ä¶‚â°gùúáùúàdxùúádxùúà, (19.3)\nwhere we have used Einstein‚Äôs convention of summing over repeated upper and lower\nGreekindices.Asanexample,intheEllisextensionofasphericalpolarcoordinatesmetric,\nwhichwewilluseinSection19.4,thearclengthis\nds2=‚àídt2+dùìÅ2+r2(dùúÉ2+sin2ùúÉdùúô2). (19.4)\nExercise Determinethemetrictensor gùúáùúàfortheEllismetric.\nOncewehaveametricdescribingthearclengthinspace,wecanconnectthatmetricto\nsurfacemeasurementsandcurvatureviathe Christoffelsymbols:\nŒìùúá\nùõºùõΩ=1\n2gùúáùúÜ(ùúïgùúÜùõº\nùúïxùõΩ+ùúïgùúÜùõΩ\nùúïxùõº‚àíùúïgùõºùõΩ\nùúïxùúÜ)\n. (19.5)\nOncewehavetheChristoffelsymbols,wecancalculatethe RiccicurvaturetensorRùúáùúàand\nthescalarcurvatureR :\nRùúáùúà=ùúïùúàŒìùõº\nùúáùõº‚àíùúïùõºŒìùõº\nùúáùúà+Œìùõº\nùúàùõæŒìùõæ\nùúáùõº‚àíŒìùõº\nùõºùõæŒìùõæ\nùúáùúà, (19.6)\nR=gùúáùúàRùúáùúà. (19.7)\nThestress-energytensor ontheRHSof(19.1)isthesourceofthecurvatureofspacetime,\nanditarisesfromthepresenceofmatterandenergy.Specifically,thetime-timecomponent\nofTùúáùúàistherelativisticenergydensityduetomassandtheEMfield:\nT00=ùúåE\nc2+1\nc2(\n1\n2ùúñ0E2+1\n2ùúá0B2)\n. (19.8)\nTheTkkcomponentsarerelatedtothestressorpressureinthe kdirection,whilethe Tkl\ncomponentsarerelatedtoshearstressduetomomentumfluxacrossasurface.\nWhenallthepiecesareassembled,weseethattheEinsteinfieldequations(19.1),while\nlookingsimple,areactuallytenindependent,nonlinear,partialdifferentialequations,with\n16 functions, two of which appear arbitrary. Except for some simple cases and assumed\nsymmetries,theyaregenerallytoohardtosolveanalytically.\nAgeodesicistheshortestpathbetweentwopointsinspacetime.Inadditiontothefield\nequations,akeyelementofGRisthe geodesicequation thatdescribesthemotionofafreely\nfallingparticleinspacetime:\nd2xùúá\nds2=‚àí Œìùúá\nùõºùõΩdxùõº\ndsdxùõΩ\nds, (19.9)\nwheresisthescalarpropertimeand Œìùúá\nùõºùõΩistheChristoffelsymbol.Massiveparticlestravel\nontime-likesolutionsofthegeodesicequation(19.9),whilelighttravelsonspace-likesolu-\ntions.(We‚ÄôllsolvetheequationinSection19.2.)Ofcourse‚Äúfree‚ÄùinGR,whilenotexplicitly\nincludingtheforceofgravity,doesaccountforgravityviathestress-energytensorcausing\nspacetocurve.Aftertheuseofthechainruleforderivatives,thegeodesicequationcanbe\nwrittenwithanexplicittimecoordinate:\nd2xùúá\ndt2=‚àí Œìùúá\nùõºùõΩdxùõº\ndtdxùõΩ\ndt+Œì0\nùõºùõΩdxùõº\ndtdxùõΩ\ndtdxùúá\ndt. (19.10)",4254
189-19.1.1 Calculating the Riemann and Ricci Tensors.pdf,189-19.1.1 Calculating the Riemann and Ricci Tensors,,0
190-19.3 Planetary Orbits in GR Gravity.pdf,190-19.3 Planetary Orbits in GR Gravity,"410 19 General Relativity\nThis form of the geodesic equation, with its manifest nonlinearity, is the one used for\nnumerical computations. Because d2xùúá‚àïds2is an acceleration, the geodesic equation is\nanalogoustoNewton‚Äôssecondlawofmotion,withtheforcereplacedbythegeometryof\nspacetime on the RHS. For example, if a test particle‚Äôs velocity is small (nonrelativistic),\nthe terms quadratic and cubic in the velocity can be ignored, and we would be left with\nGalileo‚Äôshypothesis(althoughhewouldnothaveusedtheseequations)thatallparticles,\nregardlessofmass,havethesameacceleration:\nd2xi\ndt2‚âÉ‚àí Œìi\n00,i=1,2,3. (19.11)\n19.1.1 Calculating the Riemann and Ricci Tensors\nFigure19.1showstwofreeparticlesmovingalongthetwoinfinitesimallyclosegeodesics\nxa(ùúè)andxb(ùúè).Weconsidertheparticleon xaasareferenceparticlewith uùúá=dxùúá‚àïdùúèits\n4-velocity.The xaandxbtrajectoriesstartoffparallelattime ùúè=0andareconnectedbythe\nvectorn(ùúè):\nxa=xb+nùõº(ùúè). (19.12)\nForzerorelativeaccelerationoftheparticles,thegeodesicsremainparallel,andso:\nd2n\ndùúè2=0. (19.13)\nThisderivativeactsonthebasisvectors,whichinturnrequiresknowledgeoftheChristoffel\nsymbols:\n(\nd2n\ndùúè2)ùõº\n=(ùúïùúéŒìùõº\nùúáùúà‚àíùúïùúàŒìùõº\nùúáùúé+Œìùõº\nùúéùõæŒìùõæ\nùúáùúà‚àíŒìùõº\nùúàùõæŒìùõæ\nùúáùúé)uùúéuùúáuùúà. (19.14)\nWe recognize the quantity in parenthesis as the uncontracted version of the Riemann\ntensor:\nRùõº\nùúáùúàùúé=ùúïùúéŒìùõº\nùúáùúà‚àíùúïùúàŒìùõº\nùúáùúé+Œìùõº\nùúéùõæŒìùõæ\nùúáùúà‚àíŒìùõº\nùúàùõæŒìùõæ\nùúáùúé. (19.15)\n19.1.2 Riemann and Ricci Tensor Problems\nTheSchwarzschildmetric,\nds2=(\n1‚àí2GM\nc2r)\nc2dt2‚àí(\n1‚àí2GM\nc2r)‚àí1\ndr2‚àír2(dùúÉ2+sin2ùúÉdùúô2), (19.16)\nt\nxn(œÑ)\nœÑxb(œÑ) xa(œÑ)\nœÑ = 0Figure 19.1 Two free particles move along the inÔ¨Ånitesimally\nclose geodesics xa(ùúè)andxb(ùúè). The particles start off parallel at\ntimeùúè=0 and are connected by the vector n(ùúè).\n19.1 Einstein‚Äôs Field Equations 411\npermitsasolutionoftheEinsteinequationforasphericallysymmetricgeometrywithzero\ncosmologicalconstant Œõ,andnomatterpresentso Tùúáùúà=0.Thefollowingproblemscanbe\nsolvedwithvariationsoftheonesamecode.Ourcode Ricci.pyinListings19.1mayhelp.\n1) Create four matrices representing the Christoffel symbols, Œì0\nùúáùúà,Œìr\nùúáùúà,ŒìùúÉ\nùúáùúà,Œìùúô\nùúáùúà,a n d\nuse SymPyor some other symbolic manipulation program to evaluate them for the\nSchwarzschildmetric.\n2) Use SymPytoevaluatetheRiemanntensor, Rùõº\nùúáùúàùúé,fortheSchwarzschildmetric.\n3) Use SymPytoextracttheRiccicurvaturetensor,whichisdefinedasthecontraction\nRùúÜùúá‚â°Rùõº\nùúÜùõºùúá. (19.17)\n4) TheRicciscalargivesasinglenumericalmeasureofthecurvatureateachpointinspace-\ntime.Ifaspacetimeisflat,then R=0,andinitiallyparallelgeodesicsremainsointime.\nIfaspacetimeiscurved,then R‚â†0.Use SymPytoextracttheRicciscalarfromtheRicci\ncurvaturetensor:\nR=gùúáùúÖRùúáùúÖ. (19.18)\n19.1.3 Event Horizons\nThe curvatureofspacetimeandthespeedoflightcreateboundariesinspacebeyondwhich\nevents cannot be observed. These are called eventhorizons and are important near black\nholes.LookagainattheSchwarzschildmetric(19.16),andimagineobservingeventssimul-\ntaneouslyatoneinstantoftimesothat dt=0.Theproperdistance,aswouldbemeasured\nbyputtingdownmetersticks,wouldthenbethespace-likedistance:\n‚àíds2=(\n1‚àí2GM\nc2r)‚àí1\ndr2‚àír2dùúÉ2, (19.19)\nwher ew eha v eleftoffthe ùúôdependence.Thisequationleadstotheconclusionthatdis-\ntances,somehow,becomesingularattheSchwarzschildradius rs:\nrh=rsdef=2GM\nc2. (19.20)\nThis is the event horizon. Although this singularity is a peculiarity of the Schwarzschild\nmetric,physicalblackholesarebelievedtohaveeventhorizons.\nProblem Calculate numerical values for the event horizonsaround the earth, the sun,\nandablackhole.\nBlackholestendtorotate,andsothespacetimenearonewithangularmomentum Jismore\nappropriatelydescribedbytheKerrmetric[Kerr,1963].Formotionintheequatorialplane\n(noùúôvariation),theKerrmetricis:\nds2=‚àí(\n1‚àírsr\nŒ£)\nc2dt2+Œ£\nŒîdr2+Œ£dùúÉ2, (19.21)\nŒ£=r2+a2cos2ùúÉ,Œî=r2‚àírsr+a2,a=J\nMc. (19.22)\nAgain,wedeterminethehorizonradius rhbysolvingforthe rvalueatwhichthedistance\nbecomessingular:\nŒî=0‚áírh=rs¬±‚àö\nr2\ns‚àí4a2\n2. (19.23)\n412 19 General Relativity\nForrealrh,thislimitstheangularmomentumoftheblackholeto\nJ‚â§GM2\nc. (19.24)\nThefulldeterminationoftheequationsofmotionforaparticlenearaKerrmetricblack\nholeisa significantendeavor,andwesuggest [HancandTaylor,2004]and[Gould etal.,\n2006]forthederivation.\n19.2 Gravitational DeÔ¨Çection of Light\nAswehavesaidbefore,a geodesicistheshortestpathbetweentwopointsinspacetime.GR\nassumesthatlighttravelsonspace-likegeodesics,whilemassiveparticlestravelontime-\nlike geodesics. The geodesic path is the solution of the geodesic equation (introduced in\nSection19.1):\nd2xùõΩ\ndùúÜ2+ŒìùõΩ\nùúáùúàdxùúá\ndùúÜdxùúà\ndùúÜ=0. (19.25)\nWeleavetheapplicationofthisequationfromfirstprinciplestothereferencesandfocus\ninsteadonwhat‚Äôsbeenderivedfromit.\nOneoftheearlytestsofGRwasthepredictionfortheangleofdeflection ùúôoflightstart-\ning at an impact parameter b=Rrelative to the sun‚Äôs center and just grazing the sun‚Äôs\nsurface (Figure 19.2). Newtonian mechanics solved this problem by calculating the orbit\nof a massive particle around the sun, and then taking the m‚Üí0 limit for the particle.\nThisyielded\nùúô=2GM\nRc2, (19.26)\nwhereGisthegravitationalconstant, Misthemassofthesun,and Ristheradiusofthesun.\nLater,Einsteinianmechanicswasusedtosolvethegeodesicequation(19.9)approximately,\nandobtainedtwiceaslargeavalue,\nùúô‚âÉ4GM\nRc2. (19.27)\nThisagreedwithmeasurementsandhelpedtoestablishthevalidityofGR.\nNowlet‚Äôstrytocalculatesomenumericalvaluesforthedeflection.In1916,Schwarzschild\nfoundanexactsolutionoftheEinsteinianequationsusing(whatelse?)theSchwarzschild\nmetric(19.16).Forthismetricandforlightjustgrazingthesun( b=R),theorbitequation\ntakesthesimpleform[Moore,2013]:\n(\n1\nrdr\ndùúô)2\n=(\n1‚àí2M\nR)1\nR2‚àí(\n1‚àí2M\nr)1\nr2. (19.28)\nRbœïFigure 19.2 A light ray being bent by an angle ùúôdue to the\ngravitational effect of the sun.\n19.2 Gravitational DeÔ¨Çection of Light 413\nAchangeofvariableto u=R‚àïrproducesthenumericallymorerobustequation:\n(\ndu\ndùúô)2\n=1‚àíu2‚àí2M\nR(1‚àíu3). (19.29)\n1) Verifythatanapproximatesolutionto(19.29)is\nùúô‚âÉ4GM\nRc2. (19.30)\n2) Evaluatethisexpressiontodetermineanumericalvaluefortheangleofdeflectionfor\nlightgrazingthesun‚Äôssurface(hint:It‚Äôssmall).Useparameters M=2√ó1023g,R=7√ó\n1010cm,andG‚àïc2=7.4√ó10‚àí29cm/g.\n3) AlthoughtheODE(19.29)isnonlinear,that‚Äôsnotaproblemforanumericalsolution.\nSolve (19.29) numerically and compare your result with the value obtained from the\napproximateanalyticexpression.\n19.2.1 Gravitational Lensing\nIn adifferentapproachtothedeflectionofalightduetoaverymassivestar,Moore[2013]\nassumes a Schwarzschild spacetime to describe the curved space outside of a spherically\nsymmetricgravitationalsource(astar).Intermsoftheinversevariable u=1‚àïr,thegeodesic\nequationisnow\nd2u\ndùúô2=3GMu2‚àíu. (19.31)\n1) Modify your ODE solver to solve this equation. Employ units such that mass is\nmeasuredinmeters, GM=1477.1m,and M=28M‚äô(M‚äôisasolarmass).Ourprogram\nLensGravity.py isgiveninListing19.2.\n2) Equation(19.31)isquitesensitivetotheinitialconditions.Assumethatinitiallythelight\nisverydistant,say r‚âÉ106,andu(ùúô=0)=du(ùúô)‚àïdùúô=10‚àí6.\n3) Convert your solution for r(ùúô)into one for (x,y), and plot the photons‚Äô paths for\n0‚â§ùúô‚â§ùúã.OurplotsareshownontheleftofFigure19.3.\nSunSource\n‚Äì3\n‚Äì1.00 ‚Äì0.75 ‚Äì0.50 ‚Äì0.25 0.00 0.25 0.50 0.75 1.00‚Äì2‚Äì10y\nx123Gravitational lensing (cross section)\nO \nO\nO\nFigure 19.3 Left: Three trajectories showing the bending of light rays caused by the sun‚Äôs mass.\nNote, the three images on the right would be seen as an Einstein ring. Right: A James Webb\nTelescope image showing an Einstein ring.",7465
191-19.3.1 Newtons Potential Corrected.pdf,191-19.3.1 Newtons Potential Corrected,,0
192-19.3.2 Orbit Computation via Energy Conservation.pdf,192-19.3.2 Orbit Computation via Energy Conservation,"414 19 General Relativity\n4) Employthesymmetryofthisproblemtorotateyoursolutionaboutthe x=0axisand\nthuscreateanEinsteinring.Thisiswhatanobserverseeswhenviewingadistantlight\nsourcelyingbehindamassivestarandfocusingonapointsource(Figure19.3right).\n19.3 Planetary Orbits in GR Gravity\n19.3.1 Newton‚Äôs Potential Corrected\nThe classical solution of Newton‚Äôs laws, including his gravitational potential, is just fine\nformosteverythinghereonorneartheearth.However,therearecorrectionsarisingfrom\nGR,andwhilesmall,thesecorrectionsareactuallycriticaltotheaccuracyofmodernGPS\ndevices.TheusualapproachistodetermineanODEwithaGRcorrectiontothefamiliar\n1‚àïrgravitational potential, and then to solve the ODE. We follow [Hartle, 2003; Moore,\n2013;James etal.,2015],whoassumetheSchwarzschildmetric(19.16),andshowthatan\neffectivepotentialforthismetricis:\nVeff(r)=‚àíGM\nr+ùìÅ2\n2r2‚àíGMùìÅ2\nr3. (19.32)\nHereGisthegravitationalconstant, ùìÅistheangularmomentumperunitrestmass, Mis\nthemassofthestar,andthemiddletermistheusualangularmomentumbarrier.Wesee\nthat (19.32) differs from the Newtonian potential by a ‚àíGMùìÅ2‚àïr3term that, in addition\ntotheusual ‚àíGM‚àïrattraction,providesanadditionalstrongattractionatshortdistances.\nWe obtain a dimensionless, and simpler-to-compute, form of the potential by changing\nvariables:\nVeff(r‚Ä≤)=‚àíG\nr‚Ä≤+ùìÅ‚Ä≤2\n2r‚Ä≤2‚àíGùìÅ‚Ä≤2\nr‚Ä≤3r‚Ä≤=r\nM,ùìÅ‚Ä≤=ùìÅ\nM. (19.33)\n1) PlotVeff(r‚Ä≤)versusr‚Ä≤forùìÅ=4.3(likeFigure19.4).\n2) Describeinwordshowtheorbitswithinthispotentialchangewithenergy.\n3) Atwhatvaluesof r‚Ä≤doestheeffectivepotentialhaveamaximumandaminimum?\n4) At what value of r‚Ä≤does a circular orbit exist? (Hint: the small circles in Figure 19.4\ncorrespondtocircularorbits.)\n5) Determinetherangeof r‚Ä≤valuesthatoccurfor ùìÅ=4.3.\n6) Indicatetheaboverangeonyourplotbyahorizontalline,anddescribetheorbits.\n7) Describetheorbitsforenergiescorrespondingtothemaximuminthepotential.\n19.3.2 Orbit Computation via Energy Conservation\nA fairlysimplewaytodeterminetheorbitsofmassiveparticlesintheeffectivepotential\n(19.33)derivesfromenergyconservation.Itstartswiththeenergyperunitmassexpressed\nasthesumofkineticandpotentialterms:\nE=1\n2(\ndr\ndùúô)2ùìÅ2\nr4‚àíGM\nr+ùìÅ2\n2r2‚àíGMùìÅ2\nr3, (19.34)\nwhereùúôisthepolarangle.WeobtainanODEfortheorbitbydifferentiatingbothsidesof\ntheequationwithrespectto ùúô:\nd2r\ndùúô2=‚àíGM\nr2+ùìÅ2\nr3‚àí3GMùìÅ2\nr4, (19.35)\n19.3 Planetary Orbits in GR Gravity 415\n5‚Äì0.04‚Äì0.02Vr (r')\n0.000.02NewtonianRelativistic and Newton potential\n0.04\n10 15 20\nr/M25 30 35 40\nFigure 19.4 Relativistic and Newtonian potentials for ùìÅ‚àïM=4.3. The two dots correspond to radii\nfor circular orbits.\nwhereacommon dr‚àïdùúôfactorhasbeencanceledout.TheODEissimplifiedbyachange\nofvariablesto:\nd2u\ndùúô2=‚àíu+GM\nùìÅ2+3GMu2,u‚Ä≤=M\nr,ùìÅ‚Ä≤=ùìÅ\nM. (19.36)\nAswithNewtonianorbits,theenergyandangularmomentumofthesystemdeterminethe\norbitcharacteristics.Weusedtheenergyintegral(19.34)todeterminetheinitialconditions\nfortheODE,andthensolvedfor du‚Ä≤‚àïdùúô:\ndu‚Ä≤\ndùúô=‚àö\n2E\nùìÅ‚Ä≤2+2Gu‚Ä≤\nùìÅ‚Ä≤2‚àíu‚Ä≤2+2Gu‚Ä≤3. (19.37)\n1) UseyourODEsolvertoexplorenumericallyandgraphicallyvariousorbitscorrespond-\ning to various initial conditions and energies. Our program, RelOrbits.py is in Listing\n19.3.Introducesomesignalsintoyourfiguressothatyoucantellthedirectionoftravel\n(orproduceatimeseriesofgraphs).\n(a) SetupyourODEsolverappropriatelyfor(19.37)using G=1.\n(b) Chooseanenergycorrespondingtothemaximumoftheeffectivepotentialandcom-\nputeyourversionofFigure19.4.Pickaninitial rvalueatwhichthepotentialisa\nmaximum.Asyoumayhavededuced,thisshouldleadtoanunstableorbit,suchas\nontheleftofFigure19.5.\n(c) Seeifyoucanfindinitialconditionsthatleadtoacircularorbit.Isitstable?\n(d) Investigatetheeffectofgraduallydecreasingtheangularmomentum.\n(e) Chooseanenergythatcorrespondstotheminimumintheeffectivepotentialand\nplot nearby orbits. Examine the sensitivity of these orbits to the choice of initial\nconditions.",3863
193-19.3.3 Precession of the Perihelion of Mercury.pdf,193-19.3.3 Precession of the Perihelion of Mercury,"416 19 General Relativity\n‚Äì20 ‚Äì10 0 10 20‚Äì5\n‚Äì5 00510152020\n10\n0\n‚Äì10\n‚Äì2025\n51 0\nx/M x/M\ny/My/M\n15 20 25\nFigure 19.5 Left: An orbit corresponding to an energy at the maximum of the effective potential.\nRight: A rapidly precessing orbit.\n(f) Determinetheenergyandinitialconditionsthatproduceaprecessingperihelion,\nsuchastheoneseenontherightofFigure19.5.Inthiscase,themassiveparticle\nmovesbetweentwoturningpoints,asshownbythehorizontallineinthepotential\nwellinFigure19.4.\n(g) Examinetheorbitsthatoccurifaparticleisboundbytheinnerstrongattraction.\nCansuchaparticlestartatinfinityandbecaptured?\n19.3.3 Precession of the Perihelion of Mercury\nPlanets follownearlyperfectellipsesaroundthesun,withtheirmajoraxesrotatingvery\nslowly,asshowninFigure19.6.Asviewedfromthesun,theprecessionofMercuryis9.55\nminutes of arc per century [min =(1/60)thof a degree]. Mercury is the fastest of all the\nplanets,andsoitsprecessionisthelargest.Allbutabout0.01ofadegreeoftheprecession\ncanbeexplainedwithNewtonianmechanicsasperturbationsduetotheotherplanets.This\nleavesasmallmystery.Thecalculationofthiscorrectiontoasmallcorrectionwasoneof\ntheimportantearlysuccessesofGR.Inthissection,wepresentafirst-principlescalculation\nofthatprecessionduetoG.He.\nThe Schwarzschild metric with zero cosmological constant Œõdescribes a spacetime\nappropriate to a spherically symmetric geometry surrounding a mass Mwith no other\nmatterpresent( Tùúáùúà=0).Aswenowwanttouseactualmassandorbitalvalues,werewrite\nthemetric(19.16)as\nds2=(\n1‚àírs\nr)\ndt2‚àí1\n1‚àírs‚àïrdr2‚àír2dùúÉ2‚àír2sin2ùúÉdùúô2, (19.38)\nrsdef=2GM. (19.39)\nAtime-liketrajectoryisasolutiontothegeodesicequationforamassiveparticle.If ùúèisthe\npropertime,atime-liketrajectoryisdescribedby\ndùúè2=gùúáùúàdxùúádxùúà. (19.40)\n19.3 Planetary Orbits in GR Gravity 417\nSunMercury at\nperihelion\nMercury‚Äôs orbitPrecession\nFigure 19.6 An artist‚Äôs perception of the precession of the perihelion of Hg (www\n.astronomicalreturns.com/2020/05/the-mystery-of-mercurys-missing).\nForaplanarorbitwith ùúÉ=ùúã‚àï2,thisleadsto\n(\ndùúè\ndt)2\n=(\n1‚àírs\nr)\n‚àíÃár2\n1‚àírs‚àïr‚àír2Ãáùúô2. (19.41)\nWewantanequationrelatingdistanceandangle.Thederivativescanberewritteninterms\noftheconstantsofthemotion,\ndùúè\ndt=1\ne(\n1‚àírs\nr)\n,dùúô\ndt=L\ner2(\n1‚àírs\nr)\n, (19.42)\nwhereL=Rùë£‚àïcis the angular momentum per unit mass, ùë£is the linear velocity at the\napoapsis,and eistheenergyperunitmass.Substitutionleadstothedifferentialequation\n(\ndr\ndt)2\n=1\ne2(\n1‚àírs\nr)2[\n(e2‚àí1)+rs\nr‚àíL2\nr2+L2rs\nr3]\n. (19.43)\nUseofthechainrule,\ndr\ndt=dr\ndùúôdùúô\ndt(19.44)\nleadstothedesireddifferentialequationrelatingdistanceandangle:\n(\ndr\ndùúô)2\n=r4\nL2[(\n1‚àírs\nR)(\n1+L2\nR2)\n‚àí(\n1‚àírs\nr)(\n1+L2\nr2)]\n, (19.45)\nNote that the mass of Mercury does not enter into the calculation, although its distance\nfromthesundoes.ThisisthesameaswhathappenswithNewton‚Äôslaws,\nmMG\nr2=ma‚Üía=Mg\nr2, (19.46)\nwherethem‚Äôscancelout.",2889
194-19.4 Visualizing Wormholes.pdf,194-19.4 Visualizing Wormholes,"418 19 General Relativity\nAlthough(19.45)canbesolvedasitstands,thelargedifferencesinparametervalueslead\nto numerical inaccuracies, and it is better to solve for the inverse distance u=R‚àïr.T h i s\nleadstothequadraticequation:\n(\ndu\ndùúô)2\n=rs\nR(u‚àí1)(u‚àíu+)(u‚àíu‚àí), (19.47)\nu¬±=‚àíb¬±‚àö\nb2‚àí4ac\n2a,a=rs\nR,b=a‚àí1,c=b+Rrs\nL2.\n1) Showthattheperihelionprecessionperrevolutioncanbewrittenas\nŒîùúô=2‚àö\nR\nrs‚à´u‚àí\n1du‚àö\n(u‚àíu+)(u‚àíu‚àí)(u‚àí1)‚àí2ùúã. (19.48)\n2) Compute ŒîùúôandcompareittoLandauandLifshitz[1971]‚Äôsvalueof5.02 √ó10‚àí7.Here\naresomeapoapsisnumericalvalues:\nrs=2950m,ra=69.82√ó109m,rp=46.00√ó109m. (19.49)\nThecode PrecessHg.py ,writtenbyG.Heisgivenonline.\n19.4 Visualizing Wormholes\nProblem Create imagesofawormholeofthetypeseeninthemovie Interstellar .(Asan\nalternative,youcanreproducesomeofthevisualizationsfoundonlineinRoman[1994].)\nEven better wouldbe the creation ofvisualizationsoftravelthrough a wormhole[Nolan\nandNolan,2015].\nDuringChristopherNolan‚Äôsdirectionofthesciencefictionmovie Interstellar ,KipThorne\n(a 2017 Noble laureate for GR) helped develop visualizations of rocket flight based on\nEinstein‚Äôs field equations. The key element of the movie was that interstellar travel\nwas possible in a single human lifetime if a spaceship passed through a wormhole(an\nEinstein-Rosen bridge ). This wormhole would be a tunnel-like structure that connects\nonelocationinspacetimetoanother,orpossiblytoanotheruniverse[James etal.,2015].\nFigure19.7isavisualizationofsuchawormhole.\nAlthoughactualwormholeshaveneverbeenobserved,speculationisthattheymayoccur\nasquantumfluctuationsoverdistancesonthePlanckscale,‚àö\nH‚Ñè‚àïc3‚àº10‚àí35m(10‚àí20of\nthesizeofaproton).Furtherspeculationsimaginethatthesizeofthewormholemightbe\nenlargedtomacroscopicsizeifthereweresometypeofexoticmatterwithnegativeenergy\ndensityatthethroatofthewormhole.Thismightpermitarocketshiptopassthroughit\n[Roman,1994].\nHowever,ifour4Duniverseresidedinahigher-dimensionalspace(calleda bulk),such\nasthe5-Doneimaginedin Interstellar ,thentheremightnotbetheneedforexoticmatter\nto hold open the wormhole. In any case, while unlikely, interstellar travel is not strictly\nforbidden(itissciencefictionafterall).Actually,asanexerciseinGR,MorrisandThorne\n[1988]discussthefundamentalsofspacetravelusingwormholes.\nThe equationsthat Thorneused to create the visualizationswere expressed in units in\nwhichG=1,c=1,andtimeismeasuredinlength1s =c√ó1s=2.998√ó108m.Massis\n19.4 Visualizing Wormholes 419\nFigure 19.7 The Ellis wormhole connecting upper and lower\n(Ô¨Çatter) spaces. Note that this visualization has the wormhole‚Äôs 4D\nbulk embedded within a 3D space. The throat diameter is 2 ùúå,and\nthe proper distance traveled in a radial direction is ùìÅ.\n2œÅ\n‚Ñìr\nœÜ\nmeasured in length, 1kg =G‚àïc2√ó1kg,sothat1kg =0.742√ó10‚àí27m, in which case the\nsun‚Äôsmassequals1.476km.Thecreatedwormholeconnectstwoflat3Dspacesplacedat\ntheendsofa4Dcylinder.The4Dcylinderisoflength2 a,withcrosssectionsthatarespheres\nofradius ùúå.Inordertovisualizethe4Dwormhole,itisembeddedina3Dspace,inwhich\ncasethecrosssectionsarecirclesofradius ùúå(Figure19.7).\nThorneusedtheEllisextensionofasphericalpolarcoordinatemetric:\nds2=‚àídt2+dùìÅ2+r2(dùúÉ2+sin2ùúÉdùúô2). (19.50)\nHere,theradiuscoordinate risafunctionof ùìÅ,andthephysicaldistance(properdistance)\ntraveledinaradialdirection:\nr(ùìÅ)=‚àö\nùúå2+ùìÅ2, (19.51)\nwhereùúåistheradiusofthethroatofthecylindricalwormhole.Notethatthetimecoordinate\ntentersthemetric(19.50)withanegativesign.Thismeansthatforfixed ùìÅ,ùúÉ,andùúô,the\ntimetincreasesinthetimelikedirection.Accordingly, tisthepropertimeasmeasuredby\napersonatrestinthespatial (ùìÅ,ùúÉ,ùúô)coordinatesystem.\nBecauser2(dùúÉ2+sin2ùúÉdùúô2)is the familiarmetricdescribingthe surface of a sphereof\nradiusr, the created wormhole is spherically symmetric. This means that, as ùìÅ‚Üí¬±‚àû,\ntheradiusofthespherewithinthewormholeapproachestheproperdistance ùìÅ.Thisalso\nmeansthatas ùìÅ‚Üí¬±‚àû,wewouldhavetwoseparateflatspacesconnectedbytheworm-\nhole.Inthemovie,thetransitionbetweenthetwoflatspacesviathewormhole‚Äôsthroatis\nmade to resemble the transition to an external space in which a nonspinning black hole\nresides.ThisisdescribedbytheSchwarzschildorholemetric[James etal.,2015]:\nds2=‚àí(\n1‚àí2Óàπ\nr)\ndr2+dr2\n1‚àí2Óàπ‚àïr+r2(dùúÉ2+sin2ùúÉdùúô2), (19.52)\nwhere Óàπistheblackhole‚Äôsmass.Withthismetric,theradius rbecomestheoutwardcoor-\ndinateratherthantheproperdistance ùìÅ.Thevisualizationsinthemovierequiredasolution\nforr(ùìÅ), that is, a solution or an expression for the outward coordinate as a function of\nproperdistance.Toreducetheeffortinvolved,thevisualizationsusedananalyticexpression\nforr(ùìÅ)outsidethewormhole‚ÄôscylindricalinteriorthatissimilartotheSchwarzschild r(ùìÅ):\nr=ùúå+2\nùúã‚à´|ùìÅ|‚àía\n0arctan(2ùúâ\nùúãÓàπ)\ndùúâ (19.53)\n=ùúå+Óàπ[\nxarctanx‚àí1\n2ln(1+x2)]\n,for|ùìÅ|>a. (19.54)\nForcylindricalcoordinates,the zcoordinateistheheightabovethewormhole‚Äôsmidplane\nintheembeddingspace,andsotheembeddingspacemetricbecomes\nds2=dz2+dr2+r2dùúô2. (19.55)",4896
195-19.5 Problems.pdf,195-19.5 Problems,,0
196-19.6 Code Listings.pdf,196-19.6 Code Listings,"420 19 General Relativity\nInthiscase,thespatialmetricofthewormhole‚Äôs2Dequatorialsurfaceis:\nds2=dùìÅ2+r2(ùìÅ)dùúô2. (19.56)\nCombiningtheseequationsletsussolvefor z(ùìÅ):\ndùìÅ2=dz2+dr2, (19.57)\nz(ùìÅ)=‚à´ùìÅ\n0‚àö\n1‚àí(dr‚àïdùìÅ‚Ä≤)2dùìÅ‚Ä≤. (19.58)\nOne obtains the equations needed to visualize the wormhole by substituting (19.53) and\n(19.54)into(19.58).\n19.5 Problems\n1) Inordertoapply(19.58),weneedtoevaluatethederivative dr‚àïdùìÅ.UsePython‚Äôssym-\nbolicalgebrapackage SymPytoshowthat\ndr\ndùìÅ=2\nùúãarctan2ùìÅ‚àía\nùúãÓàπ. (19.59)\nOurprogram WormHole.py inListing19.4evaluatesthisderivative.\n2) Insertthis dr‚àïdùìÅinto(19.58)andevaluatethe z(ùìÅ)integralnumericallyfor\nùúå=1,a=1,Óàπ=0.5. (19.60)\n3) ThecontourlinesorringsshowninFigure19.7correspondtodifferentvaluesof ùìÅ.They\nwereobtainedwiththeprogram VisualWorm.ipynb inListing19.5.\n4) Makeyourownplotofthewormholefor ùìÅ=1,¬∑¬∑¬∑,11.\n5) Create a cylindrical wormhole of length 2 Lwith a spherical cross section of radius ùúå.\nVisualizethewormholewitha3Dembeddingdiagraminwhichthemissingdimension\nresultsinthecrosssectionsappearingascirclesratherthanspheres.Followthesame\nstepsasusedfortheElliswormhole,(19.50),butnowwith\nr(ùìÅ)={ùúå |ùìÅ|‚â§L(Wormholeinterior)\n|ùìÅ|‚àíL+ùúå,|ùìÅ|‚â•L(Wormholeexterior) .(19.61)\n19.6 Code Listings\nListing 19.1 Ricci.py uses SymPy to compute the Riemann and Ricci tensors and the\nRicciscalar.\n# Ricci .py: Riemann & Ricci tensors , Ricci scalar , uses Sympy\n2\nfromsympyimport ‚àó#symbolic python\nimportnumpy as np\n6t,r,th, fi , rg = symbols( ‚Äôt r th fi rg‚Äô ) # Schwarzchild metric\nprint(""contravariant"" ) # Upper indices\n# Inverse matrix\n10gT = Matrix([[1/( ‚àí1 + rg/r),0,0,0], [0, 1 ‚àírg/r, 0,0],\n[0, 0, r ‚àó‚àó(‚àí2), 0],[0, 0, 0, 1/(r ‚àó‚àó2‚àósin(th) ‚àó‚àó2)]])\n19.6 Code Listings 421\n#4‚àíD array for alpha , beta , m u, nu\nRi = [[[[[] fornin range (4)]forain range (4)]forbin range (4)]forcin\nrange(4)]\n14RT = [[[] formin range (4)]forpin range (4)] # Ricci tensor\n# Christoffel symbols, upper t , r , theta , and phi\nCht = Matrix([[0, 0.5 ‚àórg/(r ‚àó(r‚àírg)), 0, 0],\n18[0.5‚àórg/(r ‚àó(r‚àír g ) ) ,0 ,0 ,0 ] ,[ 0 ,0 ,0 ,0 ] ,[ 0 ,0 ,0 ,0 ] ] )\nChr = Matrix([[0.5 ‚àórg‚àó(r‚àírg)/r ‚àó‚àó3,0,0,0], [0, ‚àí0.5‚àórg/(r ‚àó(r‚àírg)),0,0],\n[0,0,‚àí1.0‚àór+1 . 0 ‚àórg, 0], [0,0,0, ( ‚àí1.0‚àór+r g ) ‚àósin(th) ‚àó‚àó2]])\nChth = Matrix([[0, 0, 0, 0], [0, 0, 1.0/r, 0], [0, 1.0/r, 0, 0],\n22 [0, 0, 0, ‚àí0.5‚àósin(2 ‚àóth)]])\nChfi = Matrix([[0, 0, 0, 0], [0, 0, 0, 1.0/r], [0, 0, 0, 1.0/tan(th)],\n[0, 1.0/r, 1.0/tan(th), 0]])\nforalphain range (0,4): # Upper index in Christoffel\n26ifalpha == 0: Chalp = Cht\nelifalpha == 1: Chalp = Chr\nelifalpha == 2: Chalp = Chth\nelse:C h a l p = C h f i\n30forbein range (0,4): #B e t a\nformuin range (0,4):\nifmu == 0: der2 = t # Derivative\nelifmu == 1: der2 = r\n34 elifmu == 2: der2 = th\nelifmu == 3: der2 = fi\nfornuin range (0,4):\nifnu == 0: der1 = t # Other derivative\n38 elifnu == 1: der1 = r\nelifnu == 2: der1 = th\nelifnu == 3: der1 = fi\na1 = diff(Chalp[be,nu],der2) # Christoffel symbol\n42 a2 = diff(Chalp[be,mu],der1) # A n d derivative\nsump = 0\nsumn = 0\nforgamin[t,r,th,fi]:\n46 ifgam == t :\nChgam = Cht\ngama = 0\nelifgam == r:\n50 Chgam = Chr\ngama = 1\nelifgam == th:\nChgam = Chth\n54 gama = 2\nelifgam == fi :\nChgam = Chfi\ngama = 3\n58 sump = sump+Chalp[mu,gama] ‚àóChgam[be,nu]\nsumn = sumn+Chalp[nu,gama] ‚àóChgam[be,mu]\nR = simplify(a1 ‚àía2+sump ‚àísumn) # Riemann tensor\nifR= =0 : # Print nonzero components\n62 Ri[alpha][be][mu][nu] = 0\nelse:\nRi[alpha][be][mu][nu] = R\nprint(""Ri["",alpha, ""]["",be,""]["",mu,""]["",nu,"" ]="", Ri[alpha][be][mu][nu])\n66print(""\n"")\nprint(""Ricci Tensor\n"" )\nforroin range (0,4):\nfordein range (0,4):\n70 sum=0\nforalpin range (0,4):\nsum=sum+Ri[alp][ro][alp][de]\nRT[ro][de] = simplify( sum)\n74 print(""RT["",ro,""]["",de,""] = "",RT[ro][de]) # Ricci ‚Äôs tensor\nsumR = 0 # Ricci Scalar\nforbein range (0,4):\nfornuin range (0,4): sumR = sumR+gT[be,nu] ‚àóRT[be][nu]\n78print(sumR)\nRS = (sumR)\nprint(""RS"",RS) # Ricci Scalar R\n422 19 General Relativity\nListing 19.2 LensGravity.py ComputethedeflectionoflightbythesunwithMatplotlib.\n# LensGravity.py: Deflection of light by the sun wi Matplotlib\n2\nimportnumpy as np\nimportmatplotlib.pyplot as plt\n6y=n p .z e r o s( ( 2 ), float)\nph = np.zeros((181), float) #T i m e\nyy = np.zeros((181), float)\nxx = np.zeros((181), float)\n10rx = np.zeros((181), float)\nry = np.zeros((181), float)\nGsun = 4477.1 # Meters , sum massxG\nGM= 28. ‚àóGsun\n14y [ 0 ]=1 . e ‚àí6; y[1] = 1e ‚àí6 # Initial condition u=1/r\ndeff(t,y): #R H S , c a nm o d i f y\nrhs = np.zeros((2), float)\n18rhs[0] = y[1]\nrhs[1] = 3 ‚àóGM‚àó(y[0] ‚àó‚àó2)‚àíy[0]\nreturnrhs\ndefrk4Algor(t, h, N, y, f): #D on o tm o d i f y\n22k1=np.zeros(N); k2=np.zeros(N); k3=np.zeros(N); k4=np.zeros(N)\nk1 = h ‚àóf(t,y)\nk2 = h ‚àóf(t+ h/2.,y+k1/2.)\nk3= h ‚àóf(t+ h/2.,y+k2/2.)\n26k4= h ‚àóf(t+ h,y+k3)\ny=y + ( k 1 + 2 ‚àó(k2+k3)+k4)/6.\nreturny\n30f(0,y) # Initial conditions\ndphi = np.pi/180. # 180 phi values\ni=0 # counter\nforphiinnp.arange(0,np.pi+dphi,dphi):\n34ph[i] = phi\ny = rk4Algor(phi,dphi,2,y,f) #C a l lr k 4\nxx[i] = np.cos(phi)/y[0]/1000000 # Scale for graph\nyy[i] = np.sin(phi)/y[0]/1000000\n38i=i+1\nm= (yy[180] ‚àíyy[165])/(xx[180] ‚àíxx[165]) #S l o p e\nb = yy[180] ‚àím‚àóxx[180] # Intercept\nj=0\n42forphiinnp.arange(0,np.pi+dphi,dphi):\nry[j] =m ‚àóxx[j]+b # Straight line eqtn\nj=j + 1\nplt.figure(figsize=(12,6))\n46plt.plot(xx,yy) # Light trajectory\nplt.plot(xx, ‚àíyy) # Symmetric for negative y\nplt.plot(0,0, ‚Äôro‚Äô) # Mass at origin\nplt.plot(0.98,0, ‚Äôbo‚Äô) # Source\n50plt.plot(0.98,1.91, ‚Äôgo‚Äô) # Position source seen from O\nplt.plot(0.98, ‚àí1.91, ‚Äôgo‚Äô)\nplt.text(1,0, ‚ÄôS‚Äô)\nplt.text( ‚àí1.04,‚àí0.02, ‚ÄôO‚Äô)\n54plt.text(1.02, 1.91, ""S‚Äô "")\nplt.text(1.02, ‚àí2,""S‚Äô‚Äô"")\nplt.plot([0],[3.]) # Invisible poin\nplt.plot([0],[ ‚àí3.]) # Invisible point at ‚àíy\n58plt.plot(xx,ry) # Upper straight\nplt.plot(xx, ‚àíry) # Lower straight line\nplt.xlabel( ‚Äôx‚Äô)\nplt.ylabel( ‚Äôy‚Äô)\n62plt .show()\n19.6 Code Listings 423\nListing 19.3 RelOrbits.py computesrelativisticandNewtonianpotentialsandorbits.\n# RelOrbits.py Reltv orbits in a gravitational potential , needs rk4\nimportmatplotlib.pyplot as plt\n4importnumpy as np\ndh = 0.03\ndt = dh\n8ell = 4.3 #i se l / M\nG=1 . 0\nN=2\nE=‚àí0.028\n12phi = np.zeros((7000), float)\nrr = np.zeros((7000), float)\ny=n p .z e r o s( ( 2 ), float)\ny[0] = 0.0692\n16y[1] = np.sqrt(2 ‚àóE/ell ‚àó‚àó2+2 ‚àóG‚àóy[0]/ell ‚àó‚àó2‚àíG‚àóy[0]‚àó‚àó2+2‚àóG‚àóy[0] ‚àó‚àó3)\ndeff(t,y):\nrhs = np.zeros(2)\n20rhs[0] = y[1]\nrhs[1] = ‚àíy[0]+G/ell ‚àó‚àó2+ 3 ‚àóG‚àóy[0]‚àó‚àó2\nreturnrhs\n24f(0,y)\ni=0\nforfiinnp.arange(0,12.0 ‚àónp.pi ,dt):\ny=r k 4 ( f i, d t, N , y ,f )\n28rr[i] = (1/y[0]) ‚àónp.sin( fi ) # Note u = 1/r\nphi[i] = (1/y[0]) ‚àónp.cos( fi )\ni=i + 1\nf1 = plt.figure()\n32plt.axes().set_aspect( ‚Äôequal‚Äô) # Equal aspect ratio\nplt.plot(phi[:900],rr[:900])\nplt .show()\nListing 19.4 WormHole.py computes the derivatives needed to construct the Ellis\nwormholeconnectinganupperandlowerspace.\n# W o r m H o l e.py: S y m p y evaluation of derivative for Ellis W o r m h o l e\nfromsympyimport ‚àó\n4L, x, M, rho, a, r, I ,lp= symbols( ‚Äô LxMr h oarIl p ‚Äô )\nx=( 2 ‚àóL‚àía)/(pi ‚àóM)\nr=r h o + M ‚àó(x‚àóatan(x) ‚àílog(1+x ‚àóx)/2)\np = diff(r,L)\n8print(p)\nprint(""hola"")\nn = simplify(p)\nprint(n)\n12v = integrate(sqrt(1 ‚àín‚àón),(L,0,lp))\nprint(""integral"" ,v)\n# Result : 2 ‚àóatan((2 ‚àóL‚àía)/(pi ‚àóM) ) / p i\nListing 19.5 VisualWorm.ipynb visualizes the Ellis wormhole using Vpython in a\nJupyternotebook.\n# VisualWorm . ipynb\nfromvpython import ‚àó\n4escene = canvas(width=400,height=400, range= 15)\nimportnumpy as np\nimportmath\n424 19 General Relativity\n8a=1 # 2a = height inner cylinder\nring(pos=vector(0,0,0),radius=1,axis=vector(0,1,0),color=color.yellow)\ndeff(x): # function to be integrated\nM=0.5 #b l a c kh o l em a s s\n12a=1 # 2a: cylinders height\ny=n p .s q r t ( 1 ‚àí(2‚àónp.arctan(2 ‚àó(x‚àía)/(np.pi ‚àóM))/np. pi) ‚àó‚àó2)\nreturny\n16deftrapezoid(Func,A,B,N):\nh=( B ‚àíA)/(N ) # step , A: initial , B:end\nsum= (Func(A)+Func(B))/2 # initialize , (first + last)/2\nforiin range (1, N): # inside\n20 sum+= Func(A+i ‚àóh) #\nreturnh‚àósum # sum times h\ndefradiuss(L): # radius as function of L\nro = 1 # radius of cylinder (a/ro=1)\n24a=1 # 2a: height of inner cylinder\nM=0.5 #b l a c kh o l e( m a s s M / r o ) = 1\nxx = (2 ‚àó(L‚àía))/(np.pi ‚àóM)\np=M ‚àó(xx‚àónp.arctan(xx))\n28q=‚àí0.5‚àóM‚àómath.log(1+xx ‚àó‚àó2)\nr=r o +p + q\nreturnr\nforiin range (1,12): # Plot rings at z , ‚àíz\n32A=0 # limits of integration\nB=i\nN = 300 # trapezoid rule points\nifi>6: N=600 # more points\n36z = trapezoid(f,A,B,N) # returns z\nL=i + 1\nrr = radiuss(L) # radius\nring(pos=vector(0,z,0),radius=rr,axis=vector(0,1,0),color=color.yellow)\n40ring(pos=vector(0, ‚àíz,0),radius=rr,axis=vector(0,1,0),color=color.yellow)",8642
197-Chapter 20 Integral Equations.pdf,197-Chapter 20 Integral Equations,,0
198-20.1 Nonlocal Potential Binding.pdf,198-20.1 Nonlocal Potential Binding,,0
199-20.2.1 Integral to Matrix Equations.pdf,199-20.2.1 Integral to Matrix Equations,"425\n20\nIntegral Equations\nThe power and accessibility of high-speed computers have changed the view about what\nkind of equations are solvable. We have seen how even nonlinear differential equations\ncan be solved easily and can give new insight into the physical world. In this chapter,\nwe examine how integral equations, even those containing singularities, can be solved as\nmatrix equations. We Ô¨Årst convert the quantum bound-state problem to a matrix eigenvalue\nproblem and solve it. We then examine the quantum scattering problem, which leads to a\nsingular integral equation, and solve it too .\n20.1 Nonlocal Potential Binding\nA particle undergoes an interaction with a many-body medium, as shown on the left of\nFigure 20.1. We replace this very difficult problem by an approximate, but easier one, in\nwhich the particle interacts with an effective, one-particle potential. Because the effec-\ntive potential at rdepends on the wave function at r‚Ä≤, where there are interactions with\notherparticles,theeffectivepotentialat ralsodependson r‚Ä≤,thatis,thepotential V(r,r‚Ä≤)is\nnonlocal.Thismeansthatthe V(r)ùúì(r)termintheSchr√∂dingerequationgetsreplacedby:\nV(r)ùúì(r)‚Üí‚à´dr‚Ä≤V(r,r‚Ä≤)ùúì(r‚Ä≤), (20.1)\n‚áí‚àí1\n2md2ùúì(r)\ndr2+‚à´dr‚Ä≤V(r,r‚Ä≤)ùúì(r‚Ä≤)=Eùúì(r). (20.2)\nYour problemistosolvethis integrodifferentialequation forbound-stateenergies Enand\nwavefunctions ùúìn.(Thesearenaturalunits, ‚Ñè=1.)\n20.2 Momentum-Space Schr√∂dinger Equation\nAlthough integrodifferentialequationscanbesolvediteratively,amoredirectapproachis\ntosolvethemomentum-spaceversionof(20.1)[Landau,1996]:\nk2\n2mùúìn(k)+2\nùúã‚à´‚àû\n0dpp2V(k,p)ùúìn(p)=Enùúìn(k), (20.3)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n426 20 Integral Equations\nkk‚Ä≤‚Äìk\n‚Äìk‚Ä≤\nm2m1\nr‚Ä≤r\nFigure 20.1 Left: A projectile (dark\nparticle at r) scatters from a dense\nmedium. Right: The same process viewed\nin the COM system where the projectile\nand target always have equal and opposite\nmomenta.\nwhere subscript nis used to enumerate the different bound-states. Here V(k,p)is the\nmomentum-space representation (double Fourier transform) of the coordinate-space\npotential, and if we restrict our solution to angular momentum l=0 partial waves, it is\nsimplyrelatedto V(r):\nV(k,p)=1\nkp‚à´‚àû\n0drsin(kr)V(r)sin(pr). (20.4)\nInturn,themomentum-spacewavefunction ùúìn(k)istheprobabilityamplitudeforfinding\ntheparticlewithmomentum k.ItistheFouriertransformof ùúìn(r):\nùúìn(k)=‚à´‚àû\n0drkrùúìn(r)sin(kr). (20.5)\nEquation(20.3)isanintegralequationfor ùúìn(k).Itdiffersfromanintegralrepresentation\nofùúìn(k)in that the integral in it cannot be evaluated until the solution ùúìn(p)is known.\nAlthough this may seem like a paradox, we will transform this equation into a matrix\nequationthatcanbesolvedwiththematrixtechniquesdiscussedinChapter7.\n20.2.1 Integral to Matrix Equations\nWe approximatethe pintegraloverthepotentialin(20.4)asaweightedsumover Ninte-\ngration(usuallyGaussquadrature)points p=kj,j=1‚Ä¶N:\n‚à´‚àû\n0dpp2V(k,p)ùúìn(p)‚âÉN‚àë\nj=1ùë§jk2\njV(k,kj)ùúìn(kj). (20.6)\nThisconvertstheintegralequation(20.3)tothealgebraicequation:\nk2\n2mùúìn(k)+2\nùúãN‚àë\nj=1ùë§jk2\njV(k,kj)ùúìn(kj)=En. (20.7)\nEquation(20.7)contains Nunknownfunctionvalues ùúìn(kj),anunknownenergy En,aswell\nastheunknownfunctionaldependenceof ùúìn(k).Weeliminatethefunctionaldependence\nofùúìn(k)bysolvingtheequationsforthesame k=kivaluesasthoseusedtoapproximate\ntheintegral.Inotherwords,wesolvetheequationsonlyfor kvaluesonthegridshownin\nFigure20.2.Thisleadstoasetof Ncoupledlinearequationsin (N+1)unknowns:\nk2\ni\n2mùúìn(ki)+2\nùúãN‚àë\nj=1ùë§jk2\njV(ki,kj)ùúìn(kj)=Enùúìn(ki),i=1,N. (20.8)\nkN k3 k2 k1\nFigure 20.2 The grid of momentum values on which the integral equation is solved.\n20.2 Momentum-Space Schr√∂dinger Equation 427\nForexample,for N=2wewouldhavethetwosimultaneouslinearequations:\nk2\n1\n2mùúìn(k1)+2\nùúãùë§1k2\n1V(k1,k1)ùúìn(k1)+ùë§2k2\n2V(k1,k2)ùúìn(k1)=Enùúìn(k1),\nk2\n2\n2mùúìn(k2)+2\nùúãùë§1k2\n1V(k2,k1)ùúìn(k1)+ùë§2k2\n2V(k2,k2)ùúìn(k2)=Enùúìn(k2).\nOfcourse,aprecisesolutionwouldrequiremorethantwointegrationpoints.\nWewriteourcoupledequations(20.8)inmatrixformas:\n[H][ùúìn]=En[ùúìn], (20.9)\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£k2\n1\n2m+2\nùúãV(k1,k1)k2\n1ùë§12\nùúãV(k1,k2)k2\n2ùë§2¬∑¬∑¬∑2\nùúãV(k1,kN)k2\nNùë§N\n2\nùúãV(k2,k1)k2\n1ùë§12\nùúãV(k2,k2)k2\n2ùë§2+k2\n2\n2m¬∑¬∑¬∑\n...\n¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑k2\nN\n2m+2\nùúãV(kN,kN)k2\nNùë§N‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶\n√ó‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùúìn(k1)\nùúìn(k2)\n...\nùúìn(kN)‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶=En‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùúìn(k1)\nùúìn(k2)\n...\nùúìn(kN)‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (20.10)\nEquation(20.9)isthematrixrepresentationoftheSchr√∂dingerequation(20.3).Thewave\nfunction ùúìn(k)evaluatedonthegridofintegrationpointisthe N√ó1vector\n[ùúìn(ki)] =‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùúìn(k1)\nùúìn(k2)\n...\nùúìn(kN)‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (20.11)\nTheastutereadermaybequestioningthepossibilityofsolving Nequationsforthe N+1\nunknowns, ùúìn(ki)andEn.Onlysometimes,andonlyforcertainvaluesof En(theeigenval-\nues),willasolutionexist.Let‚Äôsstartbytryingtoapplythematrixinversiontechniqueby\nrewriting(20.9)as:\n[H‚àíEnI][ùúìn]=[0]. (20.12)\nIfwetrytoobtainasolutionbymultiplyingbothsidesof(20.13)bytheinverseof [H‚àíEnI],\nwegetsomethingweird:\n[ùúìn]=[H‚àíEnI]‚àí1[0]. (20.13)\nThis equation tells us that if the inverse exists, then we have the trivialsolution ùúìn‚â°0,\nwhich is a solution, but is trivial. So maybe the assumption that the inverse exists is not\nvalid,whichwouldmeanthatthedeterminantof [H‚àíEnI]vanishes:\ndet[H‚àíEnI]=0 (bound-statecondition) . (20.14)\nEquation(20.14)isthe (N+1)thequationweneedforauniquesolutiontothebound-state\nproblem,inwhich Enistheeigenvalues of(20.9).",5545
200-20.2.3 Wave Function Exploration.pdf,200-20.2.3 Wave Function Exploration,"428 20 Integral Equations\n20.2.2 Delta-Shell Potential\nTo keepthingssimple,andtohaveananalyticanswerwithwhichtocompare,weconsider\nthelocal,delta-shellpotential:\nV(r)=ùúÜ\n2mùõø(r‚àíb). (20.15)\nThismightbeagoodmodelforaninteractionthatoccurswhentwoparticlesarepredomi-\nnantlyafixeddistance bapart.Equation(20.4)determinesthemomentum-spacerepresen-\ntationofthepotential:\nV(k‚Ä≤,k)=‚à´‚àû\n0sin(k‚Ä≤r‚Ä≤)\nk‚Ä≤kùúÜ\n2mùõø(r‚àíb)sin(kr)dr=ùúÜ\n2msin(k‚Ä≤b)sin(kb)\nk‚Ä≤k.(20.16)\nBeware:Wehavechosenthispotentialbecauseitiseasytoevaluatethemomentum-space\nmatrixelements.However,itssingularnaturein rspaceleadsto(20.16)havingaveryslow\nfalloffinkspace,andthisleadstopoornumericalprecision.\nIftheenergyisparameterizedintermsofawavevector ùúÖbyEn=‚àíùúÖ2‚àï2m,thenforthis\npotential there is, at most, one bound state, and it satisfies the transcendental equation\n[GottfriedandYan,2004]:\ne‚àí2ùúÖb‚àí1=2ùúÖ\nùúÜ. (20.17)\nDuetob]oundstatesoccurringonlyforattractivepotentials,wemusthave ùúÜ<0.\nExercise Picksomevaluesof bandùúÜ,andsolve(20.17)for ùúÖ.\nThenumericalcomputationmayfollowtwopaths.Oneevaluatesdet [H‚àíEnI]in(20.14),\nandthensearchesforthosevaluesofenergyatwhichthedeterminantvanishes.Thispro-\nvidesEn, but not wave functions. The other path solves the eigenvalue problem for all\neigenvaluesandeigenfunctions.Inbothcases,thesolutionmustbesearchedfor,andyou\nmayberequiredtoguessstartingvaluesfortheenergy.Wepresentoursolution Bound.pyin\nListing20.1.\nProblems Write a program that solves the integral equation (20.9) for the delta-shell\npotential (20.16). Find either the En‚Äôs for which the determinant vanishes or, the eigen-\nvaluesandeigenvectorsforthis H.\n1) Setthescalebysetting2 m=1andb=10.\n2) SetupthepotentialandHamiltonianmatrices, V(i,j)andH(i,j),forGaussianquadra-\ntureintegrationusingatleast N=16gridpoints.\n3) Adjustthevalueandsignof ùúÜforboundstates.Startwithalargenegativevaluefor ùúÜ\nandthenmakeitprogressivelylessnegative.Youshouldfindthattheeigenvaluesmove\nupinenergy.\n4)Note:Youreigenenergysolvermayreturnseveraleigenenergies.Thetrueboundstate\nwillbeatnegativeenergyandchangelittleasthenumberofgridpointschanges.The\nothersarenumericalartifacts.\n5) Tryincreasingthenumberofgridpointsinstepsof8,forexample,16,24,32,64, ‚Ä¶,and\nseehowtheenergychanges.",2240
201-20.3 Scattering in Momentum Space.pdf,201-20.3 Scattering in Momentum Space,,0
202-20.3.6 Scattering Wave Function Exploration.pdf,202-20.3.6 Scattering Wave Function Exploration,"20.3 Scattering in Momentum Space ‚äô429\n6) Extractthebestvalueforthebound-stateenergy,andestimateitsprecisionbyseeing\nhowitchangeswiththenumberofgridpoints.\n7) Ifyouaresolvingtheeigenvalueproblem,checkyoursolutionbycomparingtheRHS\nandLHSinthematrixmultiplication [H][ùúìn]=En[ùúìn].\n8) Verifythat,regardlessofthepotential‚Äôsstrength,thereisonlyasinglebound-stateand\nthatitgetsdeeperasthemagnitudeof ùúÜincreases.Comparewith(20.17).\n20.2.3 Wave Function (Exploration)\n1) Determine the momentum-space wave function ùúìn(k)using an eigenproblem solver.\nDoesùúìn(k)falloffatk‚Üí‚àû?Doesitoscillate?Isitwell-behavedattheorigin?\n2) Using the same points and weights as used to evaluate the integral in the inte-\ngral equation, determine how the coordinate-space wave function via the Bessel\ntransforms.\nùúìn(r)=‚à´‚àû\n0dkùúìn(k)sin(kr)\nkrk2. (20.18)\nDoesùúìn(r)fall off as you would expect for a bound-state? Does it oscillate? Is it well-\nbehavedattheorigin?\n3) Comparethe rdependenceofthis ùúìn(r)totheanalyticwavefunction:\nùúìn(r)‚àù{e‚àíùúÖr‚àíeùúÖr,forr<b,\ne‚àíùúÖr, forr>b.(20.19)\n20.3 Scattering in Momentum Space ‚äô\nAgain wehaveaparticleinteractingwiththenonlocalpotential,Figure20.1left,onlynow\ntheparticlehassufficientlyhighenergyforittoscatterfromthetargetparticlesandnotbe\nboundbythem.\nProblem Determinethescatteringphaseshift ùõøforthisscattering.\n20.3.1 Schr√∂dinger to Lippmann‚ÄìSchwinger Equation\nBecause scatteringexperimentsmeasurescatteringamplitudes,butnotwavefunctions,it\nismoredirecttohaveourtheorycalculateamplitudes.AnintegralformoftheSchr√∂dinger\nequationdealingwiththescatteringamplitude RistheLippmann‚ÄìSchwingerequation :\nR(k‚Ä≤,k)=V(k‚Ä≤,k)+2\nùúãÓàº‚à´‚àû\n0dpp2V(k‚Ä≤,p)R(p,k)\n(k2\n0‚àíp2)‚àï2m. (20.20)\n(Risactuallythe reactionmatrix ,butisrelatedtothescatteringamplitude,andiseasierto\ncalculate.)Asinthebound-stateproblem,thisequationisforpartialwave l=0and‚Ñè=1.\nIn(20.20)themomentum k0isrelatedtotheenergy Eandthereducedmass mby:\nE=k2\n0\n2m,m=m1m2\nm1+m2. (20.21)\n430 20 Integral Equations\nThe initial and final COM momenta kandk‚Ä≤are the momentum-space variables. The\nexperimentalobservablethatresultsfromasolutionof(20.20)isthediagonal( k=k‚Ä≤=k0)\nmatrix element R(k0,k0), which is related to the scattering phase shift ùõø0, and thus the\ncrosssection:\nR(k0,k0)=‚àítanùõøl\nùúå,ùúå=2mk0. (20.22)\nNotethat(20.20)isnotjusttheevaluationofanintegral,itisanintegralequationinwhich\nR(p,k)must be integrated over all pvalues. Yet because R(p,k)is unknown, the integral\ncannot be evaluated until after the equation is solved! The symbol Óàºin (20.20)indicates\ntheCauchyprincipal-value prescriptionforavoidingthesingularityarisingfromthezeroof\nthedenominatorat p=k0.\n20.3.2 Singular Integral Evaluations\nAsingularintegral\nÓà≥=‚à´b\nag(k)dk, (20.23)\nisoneinwhichtheintegrand g(k)issingularatthepoint k0withintheintegrationinterval,\nyettheintegral Óà≥remainsfinite.(Iftheintegralitselfwereinfinite,wecouldnotcompute\nit.) Unfortunately, computers are notoriously incompetent at dealing with infinite num-\nbers,andifanintegrationpointgetstooneartothesingularity,overwhelmingsubtractive\ncancellation,oroverflow,occurs.Butwecandealwithit.\nInFigure20.3weshowthreewaystoavoidthesingularityat k0.ThepathsinFigure20.3a\nandbmovethesingularityslightlyoffthereal kaxisbygiving k0asmallimaginarypart\n¬±iùúñ. The Cauchy principal-value prescription Óàºin Figure 20.3c says to integrate along a\npaththat‚Äúpinches‚Äùbothsidesofthesingularityat k0,withoutintegratingoverit:\nÓàº‚à´+‚àû\n‚àí‚àûf(k)dk=lim\nùúñ‚Üí0[\n‚à´k0‚àíùúñ\n‚àí‚àûf(k)dk+‚à´+‚àû\nk0+ùúñf(k)dk]\n. (20.24)\nTheprecedingthreeprescriptionsarerelatedbytheidentity\n‚à´+‚àû\n‚àí‚àûf(k)dk\nk‚àík0¬±iùúñ=Óàº‚à´+‚àû\n‚àí‚àûf(k)dk‚Ä≤\nk‚àík0‚àìiùúãf(k0), (20.25)\nwhichfollowsfromCauchy‚Äôsresiduetheorem.\n‚Äìko ko ‚Äì Œµko + Œµ‚Äìko\nkokoIm k\nIm k Im k\nRe k\nFigure 20.3 Three different paths in the complex kplane used to evaluate line integrals when\nthere are singularities. Here the singularities are at ¬±k0, and the integration variable is k.I nLeft\nandCenter the singularity is given a small imaginary part k0‚Üík0¬±iùúñthat moves it slightly off the\nreal axis, while in Rightthe integration path ‚Äúpinches‚Äù both sides of the singularity, without passing\nthrough it.\n20.3 Scattering in Momentum Space ‚äô431\nAnumericalevaluationoftheprincipalvaluelimit(20.24)istroublesomebecauselarge\ncancellationswilloccurnearthesingularity.Anaccuratealgorithmforevaluatingtheinte-\ngralfollowsfromthefactthat\nÓàº‚à´+‚àû\n‚àí‚àûdk\nk‚àík0=0. (20.26)\nThisequationsaysthatagraphof1 ‚àï(k‚àík0)versuskhasequalandoppositeareasonboth\nsidesofthesingularpoint k0:\nÓàº‚à´+‚àû\n‚àí‚àûdk\nk‚àík0=‚à´0\n‚àí‚àûdk\nk‚àík0+‚à´+‚àû\n0dk\nk‚àík0(20.27)\n=‚àí‚à´+‚àû\n0‚àídk\n‚àík‚àík0+‚à´+‚àû\n0dk\nk‚àík0, (20.28)\n‚áíÓàº‚à´‚àû\n0dk\nk2‚àík2\n0=0, (20.29)\nwherewehavebrokentheintegralupintooneoverpositive kandoneover ‚àík,andthen\nchangedvariable k‚Üí‚àíkinthefirstintegral. Wethusseethattheprincipal-valueexclusion\nofthesingularpoint‚Äôscontributiontotheintegralisequivalenttoasimplesubtractionof\nthezerointegral(20.29):\nÓàº‚à´‚àû\n0f(k)dk\nk2‚àík2\n0=‚à´‚àû\n0[f(k)‚àíf(k0)]dk\nk2‚àík2\n0. (20.30)\nNoticethatthereisno ÓàºontheRHSof(20.30)becausetheintegrandisnolongersingularat\nk=k0(itisproportionaltothe df‚àïdk).ThereforetheintegralontheRHScanbeevaluated\nnumericallyusingtheusualrules.Theintegral(20.30)iscalledthe Hilberttransform off\nandalsoarisesinsubjectssuchasinverseproblems.\n20.3.3 Singular Integral Equations to Matrix Equations\nNow thatwehaveputthesingularityoutoftheway,wegobacktoreducingtheintegral\nequation(20.20)toasetoflinearequations.Werewritetheprincipal-valueprescriptionas\nadefiniteintegral[HaftelandTabakin,1970]:\nR(k‚Ä≤,k)=V(k‚Ä≤,k)+2\nùúã‚à´‚àû\n0dpp2V(k‚Ä≤,p)R(p,k)‚àík2\n0V(k‚Ä≤,k0)R(k0,k)\n(k2\n0‚àíp2)‚àï2m.(20.31)\nWeconvertthisintegralequationtoasetofsimultaneouslinearequationsbyapproximating\ntheintegralasasumover NGaussianintegrationpoints kjwithweights ùë§j:\nR(k,k0)‚âÉV(k,k0)+2\nùúãN‚àë\nj=1k2\njV(k,kj)R(kj,k0)ùë§j\n(k2\n0‚àík2\nj)‚àï2m\n‚àí2\nùúãk2\n0V(k,k0)R(k0,k0)N‚àë\nm=1ùë§m\n(k2\n0‚àík2\nm)‚àï2m. (20.32)\nWenotethatthelasttermin(20.32)implementstheprincipal-valueprescriptionandcan-\ncelsthesingularbehaviorofthepreviousterm.Thisequationcontainsthe N+1unknowns\n432 20 Integral Equations\nR(kj,k0)forj=0,N.Weturnitinto N+1simultaneousequationsbyevaluatingitforthe\nNkvaluesonthegridinFigure20.2,andattheobservablemomentum k0:\nk=ki={kj,j=1,N(quadraturepoints),\nk0,i=0 (observablepoint).(20.33)\nTherearenow N+1linearequationsforthe N+1unknowns Ri‚â°R(ki,k0):\nRi=Vi+2\nùúãN‚àë\nj=1k2\njVijRjùë§j\n(k2\n0‚àík2\nj)‚àï2m‚àí2\nùúãk2\n0Vi0R0N‚àë\nm=1ùë§m\n(k2\n0‚àík2\nm)‚àï2m. (20.34)\nWeexpress(20.34)inmatrixformbycombiningthedenominatorsandweightsintoasingle\ndenominatorvector D:\nDi=‚éß\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™‚é©+2\nùúãùë§ik2\ni\n(k2\n0‚àík2\ni)‚àï2m,fori=1,N,\n‚àí2\nùúãN‚àë\nj=1ùë§jk2\n0\n(k2\n0‚àík2\nj)‚àï2m,fori=0.(20.35)\nThelinearequations(20.34)nowassumethematrixform\nR‚àíDVR= [1‚àíDV]R=V, (20.36)\nwhereRandVarethelength N+1vectors:\n[R]=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£R0,0\nR1,0\n...\nRN,0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,[V]=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£V0,0\nV1,0\n...\nVN,0‚é§\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (20.37)\nWewriteourreductionoftheintegralequationasthematrixequation:\n[F][R]=[V], Fij=ùõøij‚àíDjVij. (20.38)\nTheFmatrixisknownasthe wavematrix .WithRtheunknownvector,(20.38)isinthestan-\ndardformAX=B,whichcanbesolvedbythemathematicalsubroutinelibrariesdiscussed\ninChapter7.\n20.3.4 Solution\nAn elegant(butalasnotmostefficient)solutionto(20.38)isbymatrixinversion:\n[R]=[F]‚àí1[V]. (20.39)\nBecause the inversion of even complex matrices is a standard routine in linear algebra\nlibraries, (20.39) is a directsolution for theRamplitude. Unless you need the inverse for\nother purposes (like calculating wave functions), a more efficient approach is Gaussian\nelimination ,whichisalsocontainedinthelinearalgebralibraries.\n20.3 Scattering in Momentum Space ‚äô433\nFigure 20.4 The energy dependence of the\ncross section for angular momentum l=0\nscattering from an attractive delta-shell\npotential with ùúÜb=15. The dashed curve is\nthe analytic solution (20.41), and the solid\ncurve results from numerically solving the\nintegral Schr√∂dinger equation.\n0 2 4 601Sin2Œ¥\nAnalytic\nkbœÄ\nForthisscatteringproblemwewillusethesamedelta-shellpotential(20.16)asweused\ninSection20.2.2forboundstates:\nV(k‚Ä≤,k)=‚àí|ùúÜ|\n2mk‚Ä≤ksin(k‚Ä≤b)sin(kb). (20.40)\nThisisoneofthefewpotentialsforwhichtheLippmann‚ÄìSchwingerequation(20.20)has\nananalyticsolution[GottfriedandYan,2004]withwhichtocheck:\ntanùõø0=ùúÜbsin2(kb)\nkb‚àíùúÜbsin(kb)cos(kb). (20.41)\nOurresultswereobtainedwith2 m=1,ùúÜb=15,andb=10,thesameasinGottfriedand\nYan[2004].InFigure20.4,wegiveaplotofsin2ùõø0versuskb,whichisproportionaltothe\nscattering cross section arising from the angular momentum l=0 phase shift. Note that\nsin2ùõøreachesitsmaximumvaluesatenergiescorrespondingtoresonances.Wepresentour\nsolution, Scatt.py,inListing20.2.\n20.3.5 Exercises\n1 )W r i t eap r o g r a mf o rt h em a t r i c e s V[], D[], and F[,]. Use at least N=16 Gaussian\nquadraturepointsforyourgrid.\n2) Calculatethematrix F‚àí1usingalibrarysubroutine.\n3) Calculatethevector Rbymatrixmultiplication R=F‚àí1V.\n4) Deducethephaseshift ùõøfromR(k0,k0):\nR(k0,k0)=R0,0=‚àítanùõø\nùúå,ùúå=2mk0. (20.42)\n5) Estimatetheprecisionofyoursolutionbyincreasingthenumberofgridpointinsteps\noftwo(wefoundthebestanswerfor N=26).Ifyourphaseshiftchangesinthesecond\northirddecimalplace,youprobablyhavethatmuchprecision.\n6) Plotsin2ùõøversusenergyE=k2\n0‚àï2mstartingatzeroenergyandendingatenergieswhere\nthephaseshiftisagainsmall.YourresultsshouldbesimilartothoseinFigure20.4.Note\nthataresonanceoccurswhen ùõølincreasesrapidlythrough ùúã‚àï2,thatis,whensin2ùõø0=1.\n7) Checkyouransweragainsttheanalyticresults(20.41).",9436
203-20.4 Code Listings.pdf,203-20.4 Code Listings,"434 20 Integral Equations\n20.3.6 Scattering Wave Function (Exploration)\nThewavematrixF‚àí1inoursolutiontotheintegralequation\nR=F‚àí1V=(1‚àíVG)‚àí1V (20.43)\ncanbeusedtocalculatethecoordinate-spacewavefunction:\nu(r)=N0N‚àë\ni=1sin(kir)\nkirF(ki,k0)‚àí1. (20.44)\nHereN0isanormalizationconstant,andthe Ramplitudeisappropriateforstanding-wave\nboundaryconditions.\n1) Plotu(r)andcompareittoafreewave.\n20.4 Code Listings\nListing 20.1 Bound.py SolvestheLippmann‚ÄìSchwingerintegralequationforthequan-\ntumboundsstateswithinadelta-shellpotential.\n# Bound . py : Bound state solutn of Lippmann ‚àíSchwinger equation in p space\n2\nfromvisualimport ‚àó\nfromnumpyimport ‚àó\nfromnumpy. linalg import ‚àó\n6\nmin1 =0.; max1 =200.; u =0.5; b =10.\ndefgauss(npts,a,b,x,w):\n10pp = 0.; m= (npts + 1)//2; eps = 3.E ‚àí10 # Accuracy : ADJUST!\nforiin range (1,m+1):\nt=c o s ( m a t h . p i ‚àó(float(i)‚àí0.25)/(float(npts) + 0.5))\n14 t1 = 1\nwhile((abs(t‚àít1)) >= eps):\np1 = 1. ; p2 = 0.;\nforjin range (1,npts+1):\n18 p3 = p2\np2 = p1\np1=((2 ‚àój‚àí1)‚àót‚àóp2‚àí(j‚àí1)‚àóp3)/j\npp = npts ‚àó(t‚àóp1‚àíp2)/(t ‚àót‚àí1.)\n22 t1 = t; t = t1 ‚àíp1/pp\nx[i‚àí1] =‚àít\nx[npts‚àíi] = t\nw[i‚àí1] = 2./((1. ‚àít‚àót)‚àópp‚àópp)\n26 w[npts‚àíi] =w[i ‚àí1]\nforiin range (0,npts):\nx[i] = x[i] ‚àó(b‚àía)/2. + (b + a)/2.\nw[i] = w[i] ‚àó(b‚àía)/2.\n30\nforMin range (16, 32, 8):\nz=[‚àí1024,‚àí512,‚àí256,‚àí128,‚àí64,‚àí32,‚àí16,‚àí8,‚àí4,‚àí2]\nforlmbdainz:\n34 A=z e r o s( ( M , M ), float) # Hamiltonian\nWR = zeros ( (M) , float) # Eigenvalues , potential\nk=z e r o s( ( M ), float); w= zeros((M ), float); #P t s&wts\ngauss(M, min1, max1, k, w) # Call gauss points\n38 foriin range (0,M): # Set Hamiltonian\nforjin range (0,M):\nVR = lmbda/2/u ‚àósin(k[i] ‚àób)/k[i] ‚àósin(k[j] ‚àób)/k[j]\nA[i ,j] = 2./math.pi ‚àóVR‚àók[j] ‚àók[j] ‚àów[j]\n42 if(i = = j):\n20.4 Code Listings 435\nA[i ,j] += k[i] ‚àók[i]/2/u\nEs, evectors = eig(A)\nrealev = Es.real # Real eigenvalues\n46 forjin range (0,M):\nif(realev[j]<0):\nprint("" M (size), lmbda, ReE = "" ,M,"""",lmbda, """",realev[j])\nbreak\nListing 20.2 Scatt.py Solves the Lippmann‚ÄìSchwinger integral equation for quantum\nscatteringfromadelta-shellpotential.\n1# Scatt . py : Soln p space Lippmann Schwinger for scattering\nfromvisualimport ‚àó\nfromvisual.graph import ‚àó\n5importnumpy.linalg as lina # N u m p y‚Äôs LinearAlgebra\ndefgauss(npts, job, a, b, x, w):\nm = i = j = t = t1 = pp = p1 = p2 = p3 = 0.\n9eps = 3.E ‚àí14 # Accuracy : ‚àó‚àó‚àó‚àó‚àó‚àó ADJUST THIS ‚àó‚àó‚àó‚àó‚àó‚àó‚àó !\nm= (npts + 1)/2\nforiinarange(1, m + 1):\nt=c o s ( m a t h . p i ‚àó(float(i)‚àí0.25)/(float(npts) + 0.5) )\n13 t1 = 1\nwhile((abs(t‚àít1) ) >= eps):\np1 = 1. ; p2 = 0.\nforjin range (1, npts + 1):\n17 p3 = p2; p2 = p1\np1 = ((2. ‚àófloat(j)‚àí1)‚àót‚àóp2‚àí(float(j)‚àí1.)‚àóp3)/(float(j))\npp = npts ‚àó(t‚àóp1‚àíp2)/(t ‚àót‚àí1.)\nt1 = t; t = t1 ‚àíp1/pp\n21 x[i‚àí1] =‚àít; x[npts ‚àíi] = t\nw[i‚àí1] = 2./( (1. ‚àít‚àót)‚àópp‚àópp)\nw[npts ‚àíi] =w[i ‚àí1]\nif(job = = 0):\n25 foriin range (0, npts):\nx[i] = x[i] ‚àó(b‚àía)/2. + (b + a)/2.\nw[i] =w[i] ‚àó(b‚àía)/2.\nif(job = = 1):\n29 foriin range (0, npts):\nxi = x[i]\nx[i] = a ‚àób‚àó(1. + xi) / (b + a ‚àí(b‚àía)‚àóxi)\nw[i] =w[i] ‚àó2.‚àóa‚àób‚àób/( (b + a ‚àí(b‚àía)‚àóxi)‚àó(b + a ‚àí(b‚àía)‚àóxi))\n33if(job = = 2):\nforiin range (0, npts):\nxi = x[i]\nx[i] = (b ‚àóx i+ b+a+a )/( 1 . ‚àíxi)\n37 w[i] =w[i] ‚àó2.‚àó(a + b)/( (1. ‚àíxi)‚àó(1.‚àíxi) )\ngraphscatt = gdisplay(x=0, y=0, xmin=0, xmax=6,ymin=0, ymax=1, width=600,\nheight=400,\ntitle= ‚ÄôS Wave Cross Section vs E‚Äô , xtitle= ‚Äôkb‚Äô, ytitle= ‚Äô[sin(delta)]**2‚Äô )\n41sin2plot = gcurve(color=color.yellow)\nM= 27; b = 10.0; n = 26\nk=z e r o s( ( M ), float); x= zeros((M ), float); w= zeros((M ), float)\nFinv = zeros((M,M), float); F= zeros((M ,M ), float); D= zeros((M ), float)\n45V=z e r o s( ( M ), float); Vvec = zeros((n+1,1), float)\nscale = n/2; lambd = 1.5\ngauss(n, 2, 0., scale , k, w) # Set up points & wts\n49ko = 0.02\nformin range (1,901):\nk[n] = ko\nforiin range (0, n): D[i]=2/pi ‚àów[i] ‚àók[i] ‚àók[i]/(k[i] ‚àók[i]‚àíko‚àóko) #D\n53D[n] = 0.\nforjin range (0,n): D[n]=D[n]+w[j] ‚àóko‚àóko/(k[j] ‚àók[j]‚àíko‚àóko)\nD[n] = D[n] ‚àó(‚àí2./pi)\nforiin range (0,n+1): #S e tu pF& V\n436 20 Integral Equations\n57 forjin range (0,n+1):\npot =‚àíb‚àób‚àólambd ‚àósin(b ‚àók[i]) ‚àósin(b ‚àók[j])/(k[i] ‚àób‚àók[j] ‚àób)\nF[i][j] = pot ‚àóD[j]\nifi==j: F[i][j] = F[i][j] + 1.\n61 V[i] = pot\nforiin range (0,n+1): Vvec[i][0]= V[i]\nFinv = lina.inv(F) # LinearAlgebra for inverse\nR=d o t( F i n v, V v e c ) # Matrix multiply\n65RN1 = R[n][0]\nshift = atan( ‚àíRN1‚àóko)\nsin2 = (sin(shift)) ‚àó‚àó2\nsin2plot.plot(pos = (ko ‚àób,sin2)) # Plot sin ‚àó‚àó2(delta)\n69ko = ko + 0.2 ‚àópi/1000.\nprint(""Done"")",4458
204-Part IV PDE Applications.pdf,204-Part IV PDE Applications,437\nPart IV\nPDE Applications,30
205-Chapter 21 PDE Review Electrostatics and Relaxation.pdf,205-Chapter 21 PDE Review Electrostatics and Relaxation,,0
206-21.1 Review.pdf,206-21.1 Review,"439\n21\nPDE Review, Electrostatics and Relaxation\nThis chapter is the Ô¨Årst of several dealing with partial differential equations (PDEs); several\nbecause PDEs are more complex than ODEs, and several because each type of PDE requires\nits own algorithm. We start with a review of the types of PDEs, and requirements for their\nunique solutions. Then we get down to business by examining the simple, but powerful,\nÔ¨Ånite difference method for solving Poisson‚Äôs and Laplace‚Äôs equations. In Chapter 27,w e\nintroduce the more complicated, but computationally faster, Ô¨Ånite element method (FEM) for\nsolving the same equations .\n21.1 Review\nPhysical quantitiessuchastemperatureandpressurevarycontinuouslyinbothspaceand\ntime.Suchbeingourworld,thefunctionor fieldU(x,y,z,t)usedtodescribethesequan-\ntities must contain independent space and time variations. As time flows, the change in\nU(x,y,z,t)atanyonepositionaffectthefieldatneighboringpoints.Thismeansthatthe\ndynamicequationsdescribingthedependenceof Uonfourindependentspace-timevari-\nablesmustbewrittenintermsofpartialderivatives,andtherefore,theequationsmustbe\npartialdifferentialequations (PDEs),incontrasttoordinarydifferentialequations(ODEs).\nThegeneralformforaPDEwithtwoindependentvariablesis\nAùúï2U\nùúïx2+2Bùúï2U\nùúïxùúïy+Cùúï2U\nùúïy2+DùúïU\nùúïx+EùúïU\nùúïy=F, (21.1)\nwhereA,B,C,andFarearbitraryfunctionsofthevariables xandy[ArfkenandWeber,\n2001]. In Table 21.1, we define the classes of PDEs by the value of the discriminant\nd=AC‚àíB2, and give examples there. We usually think of an elliptic equation as one\ncontaining second-order derivatives of all the variables, with all having the same sign\nwhen placed on the same side of the equal sign; a parabolic equation as one containing\na first-order derivative in one variable and a second-order derivative in the other; and a\nhyperbolic equation as one containing second-order derivatives of all the variables, with\noppositesignswhenplacedonthesamesideoftheequalsign.\nAftersolvingenoughproblems,oneoftendevelopssomephysicalintuitionastowhether\none has sufficient boundary conditions for there to exist a unique solution for a given\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n440 21 PDE Review, Electrostatics and Relaxation\nTable 21.1 The types of PDE, their discriminants, and examples of each.\nd=AC‚àíB2>0 d=AC‚àíB2=0 d=AC‚àíB2<0\n‚àá2U(x)=‚àí4ùúãùúå(x)‚àá2U(x,t)=aùúïU‚àïùúït ‚àá2U(x,t)=c‚àí2ùúï2U‚àïùúït2\nPoisson‚Äôsequation Heatequation Waveequation\nTable 21.2 The relation between boundary conditions and uniqueness for PDEs.\nBoundary\nconditionElliptic\n(Poisson equation)Hyperbolic\n(Wave equation)Parabolic\n(Heat equation)\nDirichletopensurface Underspecified Underspecified Unique&stable(1D)\nDirichletclosedsurface Unique&stable Overspecified Overspecified\nNeumannopensurface Underspecified Underspecified Unique&Stable(1D)\nNeumannclosedsurface Unique&stable Overspecified Overspecified\nCauchyopensurface Nonphysical Unique&stable Overspecified\nCauchyclosedsurface Overspecified Overspecified Overspecified\nphysicalsituation(this,ofcourse,isinadditiontorequisite initialconditions ).Table21.2\ngivestherequisiteboundaryconditionsforauniquesolutiontoexistforeachtypeofPDE.\nFor instance, a string tied at both ends, or a heated bar placed in an infinite heat bath,\nare physical situations for which the boundary conditions are adequate. If the boundary\nconditionisthevalueofthesolutiononasurroundingclosedsurface,wehavea Dirichlet\nboundary condition . If the boundary condition is the value of the normal derivative on\nthesurroundingsurface,wehavea Neumannboundarycondition .Ifthevalueofboththe\nsolutionanditsderivativearespecifiedonaclosedboundary,wehavea Cauchyboundary\ncondition. Although having an adequate boundary condition is necessary for a unique\nsolution,havingtoomanyboundaryconditions,forinstance,bothNeumannandDirichlet,\nmaybeanoverspecificationforwhichnosolutionexists.1\nSolvingPDEsnumericallydiffersfromsolvingODEsinanumberofways.First,because\nweareabletowriteallODEsinastandardform,\ndy(t)\ndt=f(y,t), (21.2)\nwithtthe single independent variable, we are able to use a standard algorithm such as\nrk4tosolveallsuchequations.Yet,becausePDEshaveseveralindependentvariables,for\nexample ùúå(x,y,z,t), we would have to apply (21.2) simultaneously and independently to\neachvariable,whichwouldbeverycomplicated.Second,becausetherearemoreequations\nto solve with PDEs than with ODEs, we need more information than just the two initial\nconditions [x(0),Ãáx(0)]. In addition, because each PDE often has its own particular set of\nboundaryconditions,wehavetodevelopaspecialalgorithmforeachparticularproblem.\n1 AlthoughconclusionsconcerninguniquenessdrawnforexactthePDEsmaydifferfromthosedrawnfor\nthefinitedifferenceequationswewilluse,theyareusuallythesame[Jackson,1988;MorseandFeshbach,\n1953].",4915
207-21.3 FiniteDifference Algorithm.pdf,207-21.3 FiniteDifference Algorithm,"21.2 Laplace‚Äôs Equation 441\n21.2 Laplace‚Äôs Equation\nFigure21.1showsawiresquareinwhichthebottomandsidesare‚Äúgrounded‚Äù(keptat0V),\nwhilethetopwireisconnectedtoavoltagesourcethatkeepsitataconstant100V.There\narenochargeswithinthesquare.\nProblem Findtheelectricpotentialforallpoints insidethesquare.\nThevoltagesontheperimeterofthesquareinFigure21.1aretheboundaryconditionsfor\nthisproblem.(Ifyouimaginetherebeinginfinitesimalinsulatorsatthetopcornersofthe\nbox,thenwehaveaclosedboundary).Sincethevaluesofthepotentialaregivenonallsides,\nwehaveNeumannconditionsontheboundaryand,accordingtoTable21.2,auniqueand\nstablesolutionexists.\nItisknownfromclassicalelectrodynamicsthattheelectricpotential U(x)arisingfrom\nstaticchargessatisfiesPoisson‚ÄôsPDE[Jackson,1988]:\n‚àá2U(x)=‚àí4ùúãùúå(x), (21.3)\nwhereùúå(x)isthechargedensityat x.Incharge-freeregionsofspace,thatis,regionswhere\nùúå(x)=0,thepotentialsatisfies Laplace‚Äôsequation :\n‚àá2U(x)=0. (21.4)\nBoth these equations are elliptic PDEs of a form that occurs in various applications. We\nsolvethemin2Drectangularcoordinates:\nùúï2U(x,y)\nùúïx2+ùúï2U(x,y)\nùúïy2=0, Laplace‚Äôsequation, (21.5)\nùúï2U(x,y)\nùúïx2+ùúï2U(x,y)\nùúïy2=‚àí4ùúãùúå(x), Poisson‚Äôsequation . (21.6)\nInbothcasesweseethatthepotentialdependssimultaneouslyon xandy.ForLaplace‚Äôs\nequation,thecharges,whicharethesourceofthefield,enterindirectlybyspecifyingthe\npotentialvaluesinsomeregionofspace;forPoisson‚Äôsequationtheyenterdirectly aswell.\nV(x, y)\nx 300102030\n20100050100\ny0 V100 V\nxy\nFigure 21.1 Left: The shaded region of space within a square in which we determine the electric\npotential by solving Laplace‚Äôs equation. There is a wire at the top kept at a constant 100 V and a\ngrounded wire (dashed) at the sides and bottom. Right: The computed electric potential as a\nfunction of xandy. The projections onto the shaded xyplane are equipotential contour lines.\n442 21 PDE Review, Electrostatics and Relaxation\n21.2.1 Fourier Series Solution\nFor the simple geometry of Figure 21.1, an analytic solution of Laplace‚Äôs equation (21.5)\nexistsintheformofaninfiniteseries.Ifweassumethatthesolutionistheproductofinde-\npendentfunctionsof xandy,andsubstitutetheproductinto(21.5),weobtain:\nU(x,y)=X(x)Y(y)‚áíd2X(x)‚àïdx2\nX(x)+d2Y(y)‚àïdy2\nY(y)=0. (21.7)\nIfX(x)isafunctionofonly x,andY(y)isafunctionofonly y,thederivativesin(21.7)are\nordinaryasopposedto partialderivatives.Because X(x)andY(y)areindependent,theonly\nway(21.7)canbevalidfor allvalues ofxandyisforeachtermin(21.7)tobeequaltoa\nconstant:\nd2Y(y)‚àïdy2\nY(y)=‚àíd2X(x)‚àïdx2\nX(x)=k2, (21.8)\n‚áíd2X(x)\ndx2+k2X(x)=0,d2Y(y)\ndy2‚àík2Y(y)=0. (21.9)\nWeshallseethatthischoiceofsignfortheconstantmatchestheboundaryconditionsand\ngivesusperiodicbehaviorin x.Theotherchoiceofsignwouldgiveperiodicbehaviorin y,\nandthatwouldnotworkwiththeseboundaryconditions.\nThesolutionsfor X(x)areperiodic,andthosefor Y(y)areexponential:\nX(x)=Asinkx+Bcoskx,Y(y)=Ceky+De‚àíky. (21.10)\nThex=0 boundary condition U(x=0,y)=0 can be met only if B=0. Thex=L\nboundarycondition U(x=L,y)=0canbemetonlyfor:\nkL=nùúã,n=1,2,‚Ä¶. (21.11)\nSuchbeingthecase,foreachvalueof nthereisthesolution:\nXn(x)=Ansin(\nnùúã\nLx)\n. (21.12)\nForeachvalueof kn,Y(y)mustsatisfythe yboundarycondition U(x,0)=0,whichrequires\nD=‚àíC:\nYn(y)=C(ekny‚àíe‚àíkny)‚â°2Csinh(\nnùúã\nLy)\n. (21.13)\nBecausewearesolvinglinearequations,theprincipleoflinearsuperpositionholds,which\nmeansthatthemostgeneralsolutionisthesumoftheproducts:\nU(x,y)=‚àû‚àë\nn=1Ensin(\nnùúã\nLx)\nsinh(\nnùúã\nLy)\n. (21.14)\nTheEnvaluesarearbitraryconstantsandarefixedbyrequiringthesolutiontosatisfythe\nremainingboundaryconditionat y=L,U(x,y=L)=100V:\n‚àû‚àë\nn=1Ensin(\nnùúã\nLx)\nsinh(nùúã)=100V. (21.15)\nWe determine the constants Enby projection ‚Äì Multiply both sides of the equation by\nsin(mùúãx‚àïL),withmaninteger,andintegratefrom0to L:\n‚àû‚àë\nnEnsinh(nùúã)‚à´L\n0dxsinnùúã\nLxsinmùúã\nLx=‚à´L\n0dx100 sinmùúã\nLx. (21.16)\n21.2 Laplace‚Äôs Equation 443\nTheintegralontheLHSisnonzeroonlyfor n=m,whichyields\nEn=‚éß\n‚é™\n‚é®\n‚é™‚é©0, forneven,\n4(100)\nnùúãsinh(nùúã),fornodd.(21.17)\nFinally,weobtainaninfiniteseries(analyticsolution?)forthepotentialatanypoint (x,y):\nU(x,y)=‚àû‚àë\nn=1,3,5,‚Ä¶400\nnùúãsin(\nnùúãx\nL)\nsinh(nùúãy‚àïL)\nsinh(nùúã). (21.18)\n21.2.2 Fourier Series as an Algorithm\nIf we try to use (21.18) as an algorithm, we must terminate the sum at some point. Yet,\ninpractice,theconvergenceoftheseriesissopainfullyslowthatmanytermsareneeded\nfor good accuracy, and so round-off errors may become a problem. In addition, the sinh\nfunctionsin(21.18)overflowsforlarge n,whichcanbeavoidedsomewhatbyexpressing\nthe quotient of the two sinh functions in terms of exponentials, and then taking a large\nnlimit:\nsinh(nùúãy‚àïL)\nsinh(nùúã)=enùúã(y‚àïL‚àí1)‚àíe‚àínùúã(y‚àïL+1)\n1‚àíe‚àí2nùúã‚àí ‚àí‚àí‚Üí\nn‚Üí‚àûenùúã(y‚àïL‚àí1). (21.19)\nAthirdproblemwiththe‚Äúanalytic‚ÄùsolutionisthataFourierseriesconvergesonlyinthe\nmeansquare (Figure21.2).Thismeansthatitconvergestothe averageoftheleft-andright-\nhand limits in the regions where the solution is discontinuous, such as in the corners of\nthebox[Kreyszig,1998].Explicitly,whatyouseeinFigure21.2isaphenomenonknown\nastheGibbsovershoot ,whichoccurswhenaFourierserieswithafinitenumberoftermsis\nusedtorepresentadiscontinuousfunction.Ratherthanfalloffabruptly,theseriesdevelops\noscillationsthattendtoovershootthefunctionatthecorner.Toobtainasmoothsolution,\nwehadtosum40,000terms,where,incontrast,thenumericalsolutiontofollowrequired\nonlyseveralhundredevaluations.\nFigure 21.2 The analytic (Fourier series) solution\nof Laplace‚Äôs equation summing 21 terms.\nGibbs-overshoot leads to the oscillations near\nx=0, and persist even if a larger number of terms\nare summed over.\n00100\n02020\n40x yV(x, y)\n444 21 PDE Review, Electrostatics and Relaxation\n21.3 Finite-Difference Algorithm\nTo solve our 2D PDE numerically, we divide space into a lattice (Figure 21.3), and look\nfor the solution Uonly on the lattice sites. Expressing derivatives in terms of the finite\ndifferencesinthevaluesof Uatthelatticesites,iscalleda finite-difference method.Anumer-\nicallymoreefficientmethod,butwithmorecomplicatedsetup,isthe finite-element method\n(FEM),whichsolvesthePDEforsmallgeometricelements,andthenmatchesthesolutions\nfromalloftheelements.WediscussFEMinChapter27.\nToderivethefinite-differencealgorithmforthenumericsolutionof(21.5),wetakethe\nsameapproachthatweusedinSection5.1toderivetheforward-differencealgorithmfor\ndifferentiation.WestartbyaddingthetwoTaylorexpansionsofthepotentialattheright\nandleftof (x,y),andthetwoforaboveandbelow (x,y):\nU(x+Œîx,y)=U(x,y)+ùúïU\nùúïxŒîx+1\n2ùúï2U\nùúïx2(Œîx)2+¬∑¬∑¬∑, (21.20)\nU(x‚àíŒîx,y)=U(x,y)‚àíùúïU\nùúïxŒîx+1\n2ùúï2U\nùúïx2(Œîx)2‚àí¬∑¬∑¬∑. (21.21)\nU(x,y+Œîy)=U(x,y)+ùúïU\nùúïyŒîy+1\n2ùúï2U\nùúïy2(Œîy)2+¬∑¬∑¬∑, (21.22)\nU(x,y‚àíŒîy)=U(x,y)‚àíùúïU\nùúïyŒîy+1\n2ùúï2U\nùúïy2(Œîy)2‚àí¬∑¬∑¬∑. (21.23)\nAll odd terms cancel when we add these equations in pairs, and we obtain a central-\ndifferenceapproximationforthesecondpartialderivativegoodtoorder Œî4:\nùúï2U(x,y)\nùúïx2‚âÉU(x+Œîx,y)+U(x‚àíŒîx,y)‚àí2U(x,y)\n(Œîx)2, (21.24)\nùúï2U(x,y)\nùúïy2‚âÉU(x,y+Œîy)+U(x,y‚àíŒîy)‚àí2U(x,y)\n(Œîy)2. (21.25)\ni, j + 1i ‚Äì 1, ji, j ‚Äì 1\ni, j i + 1, jyx Figure 21.3 The lattice and\nalgorithm for Laplace‚Äôs equation. The\npotential at the point (x,y)=(i,j)Œî\nequals the average of the potential\nvalues at the four nearest neighbor\npoints. The nodes with white centers\ncorrespond to Ô¨Åxed values of the\npotential along the boundaries.\n21.3 Finite-Difference Algorithm 445\nSubstitution of these approximations in Poisson‚Äôs equation (21.6) produces the finite-\ndifferenceformofthePDE:\nU(x+Œîx,y)+U(x‚àíŒîx,y)‚àí2U(x,y)\n(Œîx)2(21.26)\n+U(x,y+Œîy)+U(x,y‚àíŒîy)‚àí2U(x,y)\n(Œîy)2=‚àí4ùúãùúå. (21.27)\nIfwetakethe xandygridstobeofequalspacings, Œîx=Œîy=Œî,weobtainasimpleform\nforthealgorithm:\nU(x+Œî,y)+U(x‚àíŒî,y)+U(x,y+Œî)+U(x,y‚àíŒî)‚àí4U(x,y)=‚àí4ùúãùúå.(21.28)\nThereaderwillnoticethatthisequationshowsarelationamongthesolutionsatfivepoints\ninspace.When U(x,y)isevaluatedforthe Nxxvaluesonthelattice,andforthe Nyyval-\nues,weobtainasetof Nx√óNysimultaneouslinearalgebraicequationstosolvefor U[i,j].\nOneapproachistosolvetheseequationsexplicitlyasa(big)matrixproblem.Thisisattrac-\ntiveasitisadirectsolution,butitrequiresagreatdealofmemoryandaccounting.\nTheapproachweusefollowsfromthealgebraicsolutionof(21.28)for U(x,y):\n4U(x,y)‚âÉU(x+Œî,y)+U(x‚àíŒî,y)+U(x,y+Œî)+U(x,y‚àíŒî)+4ùúãùúå(x,y)Œî2,\n(21.29)\nwherewewouldomitthe ùúå(x)termforLaplace‚Äôsequation.Intermsofdiscretelocationson\nourlattice,the xandyvariablesare:\nx=x0+iŒî,y=y0+jŒî,i,j=0,‚Ä¶,Nmax‚àí1, (21.30)\nwhere we have placed our lattice in the square of side L. The finite-difference algorithm\n(21.29)becomes,\nUi,j=1\n4[Ui+1,j+Ui‚àí1,j+Ui,j+1+Ui,j‚àí1]+ùúãùúå(iŒî,jŒî)Œî2. (21.31)\nThisequationsaysthatwhenwehaveapropersolution,itwillbetheaverageofthepoten-\ntialatthefournearestneighborsinFigure21.3,plusacontributionfromthelocalcharge\ndensity.Asanalgorithm,(21.31)doesnotprovideadirectsolutiontoPoisson‚Äôsequation,\nbutrathermustberepeatedmanytimestoconvergeuponthesolution.Westartwithan\ninitialguessforthepotential,improveitbysweepingthroughallspace,takingtheaverage\novernearestneighborsateachnode.Wekeeprepeatingtheprocessuntilthesolutionno\nlongerchanges,atleasttosomelevelofprecision,oruntilfailuretoconvergeisevident.\nWhenconverged,theinitialguessissaidtohave relaxedintothesolution,anditdoesnot\nmatterwhatthatguessmayhavebeen.\nA reasonable question with this simple an approach is, ‚ÄúDoes it always converge, and\nif so, does it converge fast enough to be useful?‚Äù In some sense the answer to the first\nquestion is not an issue; if the method does not converge, then we will know it; other-\nwisewehaveendedupwithasolution,andthepathwefollowedtogetthereisnobody‚Äôs\nbusiness! The answer to the question of speed is that relaxation methods may converge\nslowly(althoughstillfasterthanaFourierseries),yetwewillshowyoutwoclevertricksto\nacceleratetheconvergence.",9723
208-21.4 Alternate Capacitor Problems.pdf,208-21.4 Alternate Capacitor Problems,"446 21 PDE Review, Electrostatics and Relaxation\nAtthispoint,itisimportanttorememberthatouralgorithmarosefromexpressingthe\nLaplacian ‚àá2inrectangularcoordinates.Whilethisdoesnotrestrictusfromsolvingprob-\nlems with circular symmetry, there may be geometries where it is better to develop an\nalgorithmbasedonexpressingtheLaplacianincylindricalorsphericalcoordinatesinorder\ntohavegridsthatfitthegeometrybetter.\n21.3.1 Relaxation and Overrelaxation\nThereareanumberofwaysinwhichthealgorithm(21.29)canbeusedtoturnthebound-\nary conditions into a solution. The most basic approach is the Jacobi method , in which\nthe potential values are not changed until (21.29) is applied at each point on the lattice.\nThismaintainsthesymmetryoftheinitialguessandboundaryconditions.Aratherobvi-\nousimprovementontheJacobimethodisthe Gauss-Seidelmethod ,inwhichtheupdated\nguessesforthepotentialin(21.29)areusedassoonastheyhavebeencomputed.Asacase\ninpoint,ifthesweepstartsintheupper-left-handcornerofFigure21.3,thentheleftmost\nU([-1,j]and topmost U[i,j-1]values of the potential used will be from the present gen-\neration of guesses, while the other two values of the potential will be from the previous\ngeneration:\nU(new)\ni,j=1\n4[\nU(old)\ni+1,j+U(new)\ni‚àí1,j+U(old)\ni,j+1+U(new)\ni,j‚àí1]\n. (21.32)\nTheGauss-Seidelmethod usuallyleadstoacceleratedconvergence,which,inturn,leadsto\nlessround-offerrors.Italsouseslessmemoryasthereisnoneedtostoretwogenerations\nofguesses.However,itdoesdistortthesymmetryoftheboundaryconditions,whichone\nhopesisinsignificantwhenconvergenceisreached.\nAlessobviousimprovementintherelaxationtechnique,knownas successiveoverrelax-\nation(SOR),startsbywritingthealgorithm(21.29)inaformthatdeterminesthenewvalues\nofthepotential U(new)astheoldvalues U(old)plusacorrection,orresidual r:\nU(new)\ni,j=U(old)\ni,j+ri,j. (21.33)\nWerewritetheGauss-Seideltechniquehereinthegeneralform:\nri,jdef=U(new)\ni,j‚àíU(old)\ni,j\n=1\n4[\nU(old)\ni+1,j+U(new)\ni‚àí1,j+U(old)\ni,j+1+U(new)\ni,j‚àí1]\n‚àíU(old)\ni,j. (21.34)\nThesuccessiveoverrelaxationtechniquesupposesthatifconvergenceisobtainedbyadding\nrtoU,thenevenmorerapidconvergencemightbeobtainedbyaddingmoreorlessof r\n[Pressetal.,2007;Garcia,2000]:\nU(new)\ni,j=U(old)\ni,j+ùúîri,j,(SOR), (21.35)\nwhereùúîisaparameterthatamplifiesorreducestheresidual.Thenonacceleratedrelaxation\nalgorithm(21.32)correspondsto ùúî=1,acceleratedconvergence(overrelaxation)to ùúî‚â•1,\nandunderrelaxationto ùúî<1.Valuesof1 ‚â§ùúî‚â§2oftenworkswell,with ùúî>2sometimes\nleadingtonumericalinstabilities.Althoughadetailedanalysisofthealgorithmisneeded\nto predict the optimal value for ùúîfor a particular problem, we suggest a trial-and-error\napproachtoseewhatworksbest.\n21.4 Alternate Capacitor Problems 447\n21.4 Alternate Capacitor Problems\nWegiveyouachoicenow.Youcancarryouttheassessmentusingourwire-plus-grounded-box\nproblem, or you can replace that problem with a more interesting one, involving a realistic\ncapacitor,ornonplanarcapacitors .\nElementarytextbookssolvethecapacitorproblemfortheuniformfieldconfinedbetween\ntwoinfiniteparallelplates.However,thefieldinarealistic(finite)capacitorvariesnearthe\nedges (edge effects) and extends beyond the edges (fringe fields). We model the realistic\ncapacitorinagroundedbox(Figure21.4)astwoconductingplates(orwires)offinitelength\nandwidth.Writeyoursimulationsuchthatitisconvenienttovarythegridspacing Œîand\nthegeometryoftheboxandplate.Weposethreeversionsofthisproblem,eachdisplaying\nsomewhatdifferentphysics.Ineachcase,theboundarycondition U=0onthesurrounding\nboxmustbeimposedinordertoobtainauniquesolution.\n1) For the simplest version,assume that the platesare very thin conductivesheets, with\nthe top sheet maintained at 100V, and the bottom at ‚àí100V. Because the sheets are\nconductors,theymustbeequipotentialsurfaces,andsoabatterycouldmaintainthem\nattheseconstantvoltages.WriteormodifythegivenprogramtosolveLaplace‚Äôsequation\nwithfixedvoltageplates.\n2) Forthenextversionofthisproblem,assumethattheplatesarecomposedofalineof\ndielectricmaterialwithuniformchargedensities ùúåonthetop,and ‚àíùúåonthebottom.\nSolvePoisson‚Äôsequation(21.3)intheregionincludingtheplates,andLaplace‚Äôsequation\nelsewhere.Experimentuntilyoufindanumericalvaluefor ùúåthatgivesapotentialsim-\nilartothatshowninFigure21.5forplateswithfixedvoltages.\n3) For the final version of this problem, investigatehow the charges on a capacitorwith\nfinite-thickness conducting plates (Figure 21.6) distribute themselves. Because the\nplatesareconductors,theyarestillequipotentialsurfacesat100and ‚àí100V,onlynow\nyou should make them have a thickness of at least 2 Œî(so we can see the difference\nbetweenthepotentialnearthetopandthebottomsurfacesoftheplates).Suchbeing\nthe case, solve Laplace‚Äôs equation (21.4) to determine U(x,y).O n c ew eh a v e U(x,y),\nsubstitute it into Poisson‚Äôs equation (21.3), and determine how the charge density\n100 V\n‚Äì100 V (X, Y)dw\nL0\n20406080100 020406080100‚Äì1000100 V(x, y)\nx y\nFigure 21.4 Left: A simple model of a parallel-plate capacitor within a box. A realistic model\nwould have the plates close together, in order to condense the Ô¨Åeld, and the enclosing grounded\nbox would be so large that it has no effect on the Ô¨Åeld near the capacitor. Right: A numerical\nsolution for the electric potential for this geometry. The projection on the xyplane gives the\nequipotential lines.\n448 21 PDE Review, Electrostatics and Relaxation\n01020 3040010203040‚Äì100100\n0V(x, y)\nx\ny\nFigure 21.5 Left: A visualization of the computed electric potential for a capacitor with Ô¨Ånite\nwidth plates. Right: A visualization of the charge distribution along one plate, determined by\nevaluating ‚àá2U(x,y)(courtesy of J. Wetzel). Note the ‚Äúlightening rod‚Äù effect of charge accumulating\nat corners and points.\n100  V\n‚Äì100  V+ ++ + + + + + ++ + + + +++ + + ++ +\n+ \n‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\n‚Äì ‚ÄìFigure 21.6 A guess as to how charge may rearrange itself on\nÔ¨Ånite conducting plates.\ndistributes itself along the top and bottom surfaces of the plates. Hint: As the electric\nfield is no longer uniform, we know that the charge distribution also will no longer\nbeuniform.Inaddition,becausetheelectricfieldnowextendsbeyondtheendsofthe\ncapacitor,andbecausefieldlinesbeginandendoncharge,somechargemayendupon\ntheedgesandoutersurfacesoftheplates(Figure21.6).\n4) ThenumericalsolutiontoourPDEcanbeappliedtoarbitraryboundaryconditions.The\ntwoboundaryconditionstoexplorearetriangularandsinusoidal:\nU(x)={200x‚àïùë§,x‚â§ùë§‚àï2,\n100(1‚àíx‚àïùë§),x‚â•ùë§‚àï2,orU(x)=100sin(\n2ùúãx\nùë§)\n.\n5)Square conductors: Youhavedesignedapieceofequipmentconsistingofasmallmetal\nboxat100Vwithinalargergroundedone(Figure21.7).Youfindthatsparkingoccurs\nbetweentheboxes,whichmeansthattheelectricfieldistoolarge.Youneedtodeter-\nminewherethefieldisgreatestsothatyoucanchangethegeometryandeliminatethe\nsparking.Modifytheprogramtosatisfytheseboundaryconditions,anddeterminethe\nfield between the boxes. Gauss‚Äôs law tells us that the field vanishes within the inner\nbox because it contains no charge. Plot the potential and equipotential surfaces, and\nsketch in the electric field lines. Deduce where the electric field is most intense, and\nthenredesigntheequipmenttoreducethefield.\n6)Cracked cylindrical capacitor: Youhavedesignedthecylindricalcapacitorcontain-\ning a long, outer cylinder surrounding a thin, inner cylinder (Figure 21.7 right). The\ncylindershaveasmallcrackintheminordertoconnectthemtothebatterythatmain-\ntainstheinnercylinderat ‚àí100V,andtheoutercylinderat100V.Determinehowthis\nsmallcrackaffectsthefieldconfiguration.Inorderforauniquesolutiontoexist,place\nbothcylinderswithinalarge,groundedbox.",7655
209-21.4.1 Implementation.pdf,209-21.4.1 Implementation,,0
210-21.6 Code Listings.pdf,210-21.6 Code Listings,"21.5 Electric Field Visualization 449\nFigure 21.7 Left: The geometry of a\ncapacitor formed by placing two long, square\ncylinders within each other. Right:T h e\ngeometry of a capacitor formed by placing\ntwo long, circular cylinders within each other.\nThe cylinders are cracked on the side so that\nwires can enter the region.100  V‚Äì100  V100  V\n21.4.1 Implementation\nIn Listing 21.1, we present the code LaplaceLine.py that solves the square-wire\nproblem (Figure 21.1) and produces the visualization there. Here, we have kept the\ncodesimplebysettingthelengthofthebox L=NmaxŒî=100,andbytaking Œî=1:\nU(i,Nmax)=99(top),U(1,j)=0(left),\nU(Nmax,j)=0(right),U(i,1)=0(bottom).(21.36)\n1) Writeormodify LaplaceLine.py tofindtheelectricpotentialforyourchoiceofcapacitor.\n2) Start by having your program undertake 1000 iterations. Examine how the potential\nchangesinsomekeylocationsasyouiteratetowardasolution.\n3) Repeattheprocessfordifferentstepsizes Œî,anddrawconclusionsregardingthestability\nandaccuracyofthesolution.Keepinmindthatthisisasimplealgorithm,andsomay\nrequiremanyiterationsforhighprecision.\n4) Onceyourprogramproducesreliablesolutions,modifyitsothatitstopsiteratingonce\nconvergenceisreached,orifthenumberofiterationsbecomestoolarge.Ratherthan\ntrying to discern small changes in highly compressed surface plots, use a numerical\nmeasureofprecision,forexample, trace=‚àë\ni|U[i,i] |,thatsamplesthesolutionalong\nthediagonal.Youshouldbeabletoobtainchangesinthetracethatarelessthan1part\nin104.The breakcommandora whileloopisusefulforthistypeoftest.\n5) Equation (21.35) expresses the successive overrelaxation technique in which conver-\ngenceisacceleratedbyusingajudiciouschoiceof ùúî.Determinebytrialanderrorthe\nbestvalueof ùúî.Thisshouldletyoudoublethespeedofthealgorithm.\n6) Nowthatyourcodeisaccurate,modifyittosimulateamorerealisticcapacitorinwhich\nthe plate separation is approximately1\n10of the plate length. You should find the field\nmorecondensedandmoreuniformbetweentheplates.\n7) Ifyouareworkingwiththewire-in-the-boxproblem,compareyournumericalsolution\ntotheanalyticone(21.18).Donotbesurprisedifyouneedtosumthousandsofterms\nbeforetheanalyticsolutionconverges!\n21.5 Electric Field Visualization\nCreatea2Dplotoftheequipotentialsurfaces.Youmaywanttostartwithacrude,hand-(or\nmouse-)drawnsketchoftheelectricfieldascurvesorthogonaltotheequipotentiallines.\nWecandobetterthanthat!Because\nE=‚àí ‚àáU(x,y)=‚àíùúïU(x,y)\nùúïxÃÇ ùúñx‚àíùúïU(x,y)\nùúïyÃÇ ùúñy, (21.37)\n450 21 PDE Review, Electrostatics and Relaxation\n05 1 0 1 5\nEquipotential lines Electric field linesContours of constant potential and electric field lines\nfor the parallel plate capacitor\n20 25 30 35 40 45 500510152025\nY\nX3035404550\nFigure 21.8 Computed equipotential surfaces and electric Ô¨Åeld lines for a realistic capacitor.\nitissimpletocalculatethefield.Therefore,justusethecentral-differenceapproximation\nforthederivativetodeterminethefield,forexample:\nEx‚âÉU(x+Œî,y)‚àíU(x‚àíŒî,y)\n2Œî=Ui+1,j‚àíUi‚àí1,j\n2Œî. (21.38)\nOnceyouhaveadatafilecontainingthevectorfield,itcanbevisualizedbyplottingarrows\nofvaryinglengthsanddirections,orwithjustlinesasinFigure21.8.\n21.6 Code Listings\nListing 21.1 LaplaceLine.py solvesLaplace‚Äôsequationviarelaxation.Variousparame-\ntersshouldbeadjustedforanaccuratesolution.\n1# LaplaceLine .py: Matplotlib , Solve Laplace ‚Äôs eqtn in square\nimportmatplotlib.pylab as p, numpy\nfrommpl_toolkits.mplot3d importAxes3D; fromnumpyimport ‚àó;\n5\nNmax = 100; Niter = 50\nV = zeros ((Nmax, Nmax) , float)\nprint(""Working hard, wait for the figure while I count to 60"" )\n9\nforkin range (0 , Nmax ‚àí1): V[0,k] = 100.0 # Line at 100V\nfor iter in range (Niter):\nif iter%10 == 0: print(iter)\n13foriin range (1 , Nmax ‚àí2):\nforjin range (1 ,Nmax ‚àí2):\nV[i,j] = 0.25 ‚àó(V[i+1,j]+V[i ‚àí1,j]+V[i , j+1]+V[i ,j ‚àí1])\nprint(""iter, V[Nmax/5,Nmax/5]"" ,iter, V[Nmax/5,Nmax/5])\n21.6 Code Listings 451\n17x=range(0, 50, 2); y = range(0, 50, 2)\nX, Y = p.meshgrid(x,y)\ndeffunctz(V): #V ( x , y )\n21z=V [ X , Y ]\nreturnz\nZ=f u n c t z( V )\n25fig = p.figure() # Create figure\nax = Axes3D(fig) #P l o ta x e s\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô) # Red wireframe\nax.set_xlabel( ‚ÄôX‚Äô); ax.set_ylabel( ‚ÄôY‚Äô); ax.set_zlabel( ‚ÄôV(x,y)‚Äô )\n29ax.set_title( ‚ÄôPotential within Square V(x=0)=100V (Rotatable)‚Äô )\np.show() # Show f i g",4310
211-Chapter 22 Heat Flow and Leapfrogging.pdf,211-Chapter 22 Heat Flow and Leapfrogging,,0
212-22.2.2 Implementation.pdf,212-22.2.2 Implementation,"452\n22\nHeat Flow and Leapfrogging\nThis chapter introduces the time-stepping (leapfrog) method for solving a PDE on a\nspace-time lattice. We use it, and the more precise Crank‚ÄìNicolson algorithm, to solve the\nheat equation. Time-stepping is simple, yet powerful, and we will use it again and again\nwhen solving wave equations .\nProblem Youaregivenanaluminumbaroflength L=1mandwidth ùë§thatisinitially\nat100‚àòC(Figure22.1).Determinehowthetemperaturevariesalongthelengthofthebar,\naftertheendsareplacedinicewaterat0‚àòC.Assumethatthelengthofthebarisinsulated,\nbutnotitsends.\n22.1 The Parabolic Heat Equation\nIt‚Äôs a basic fact of nature that heat flows from hot to cold, that is, from regions of high\ntemperaturetoregionsoflowtemperature.Weexpressthismathematicallybystatingthat\ntherateofthevectorheatflow H,throughamaterial,isproportionaltothegradientofthe\ntemperature Tacrossthematerial:\nH=‚àíK‚àáT(x,t), (22.1)\nwhereKisthethermalconductivityofthematerial.Thetotalamountofheat Q(t)inthe\nmaterial,atanyonetime,isproportionaltotheintegralofthetemperatureoverthemate-\nrial‚Äôsvolume:\nQ(t)=‚à´dxCùúå(x)T(x,t), (22.2)\nwhereCis the specific heat of the material, and ùúåis its density. Because energy is con-\nserved,therateofdecreaseof Q,withtime,mustequaltheamountofheatflowingoutof\nthematerial.Applyingenergybalanceleadstothe heatequation :\nùúïT(x,t)\nùúït=K\nCùúå‚àá2T(x,t). (22.3)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n22.1 The Parabolic Heat Equation 453\nFigure 22.1 A metallic bar insulated along its length\nwith its ends in contact with ice. The bar is dark and\nthe insulation is of lighter color.\n100 ¬∞C\nLw0¬∞ 0¬∞\nTheheatequation(22.3)isa parabolicPDEwithspaceandtimeasindependentvariables.\nThespecificationofthisproblemimpliesthatthereisnotemperaturevariationindirections\nperpendicular to the bar, and so for our problem, we need to only consider one spatial\ncoordinate, x,alongthelengthofthebar:\nùúïT(x,t)\nùúït=K\nCùúåùúï2T(x,t)\nùúïx2. (22.4)\nAsgiven,theinitialtemperatureofthebarandtheboundaryconditionsare:\nT(x,t=0)=100‚àòC,T(x=0,t)=T(x=L,t)‚â°0‚àòC. (22.5)\n22.1.1 Solution as Analytic Expansion\nAnalogoustoLaplace‚Äôsequation,theanalyticsolutionstartswiththeassumptionthatthe\nsolutionseparatesintotheproductoffunctionsofspaceandtime:\nT(x,t)=X(x)ÓâÄ(t). (22.6)\nWhen (22.6) is substituted into the heat equation (22.4), and the resulting equation is\ndividedby X(x)ÓâÄ(t),twononcoupledODE‚Äôsresult:\nd2X(x)\ndx2+k2X(x)=0,dÓâÄ(t)\ndt+k2C\nCùúåÓâÄ(t)=0, (22.7)\nwherekisaconstantstilltobedetermined.Theboundaryconditionthatthetemperature\nequalszeroat x=0requiresasinefunctionfor X:\nX(x)=Asinkx. (22.8)\nTheboundaryconditionthatthetemperatureequalszeroat x=Lrequiresthesinefunction\ntovanishthere,andso:\nsinkL=0‚áík=kn=nùúã‚àïL,n=1,2,‚Ä¶. (22.9)\nThefunctionoftimecanbeagrowingordecayingexponential.Tobephysicallyreasonable\n(notblowup),itmustbeadecayingexponentialwith kintheexponent:\nÓâÄ(t)=e‚àík2\nnt‚àï(Cùúå), (22.10)\n‚áíT(x,t)=Ansinknxe‚àík2\nnt‚àï(Cùúå), (22.11)\nwherenmaybeanyinteger,and Anisanarbitraryconstant.Since(22.4)isalinearequation,\nthe most general solution is a linear superposition of Xn(x)Tn(t)products for all values\nofn:\nT(x,t)=‚àû‚àë\nn=1Ansinknxe‚àík2\nnt‚àï(Cùúå). (22.12)\n454 22 Heat Flow and Leapfrogging\nThecoefficients Anaredeterminedbytheinitialconditionthatattime t=0,theentirebar\nhastemperature T=100‚àòC:\nT(x,t=0)=100‚áí‚àû‚àë\nn=1Ansinknx=100. (22.13)\nProjectingthesinefunctionsdetermines An=4T0‚àïnùúãfornodd,andso:\nT(x,t)=‚àû‚àë\nn=1,3,‚Ä¶4T0\nnùúãsinknxe‚àík2\nnKt‚àï(Cùúå). (22.14)\n22.2 Time Stepping (Leapfrog) Algorithm\nAs we did with Laplace‚Äôs equation, the numerical solution is based on converting the\ndifferential equation to a finite difference (or just ‚Äúdifference‚Äù) equation. We discretize\nspaceandtimeonthelatticeinFigure22.2,andseeksolutionsonthelatticesites.Thehor-\nizontal nodes with white centers correspond to the known values of the temperature for\ntheinitialtime,whiletheverticalwhitenodescorrespondtothefixedtemperaturealong\ntheboundaries.Ifwe alsoknewthetemperaturefortimesalongthebottomrow,thenwe\ncouldusearelaxationalgorithm,aswedidforLaplace‚Äôsequation.However,withonlythe\ntoprowknown,weshallendupwithanalgorithmthatstepsforwardintimeonerowata\ntime,asinthechildren‚Äôsgame leapfrog.\nAsisoftenthecasewithPDEs,thealgorithmiscustomizedfortheequationbeingsolved,\nand for the constraints imposed by the particular set of initial and boundary conditions.\nWithonlyonerowoftimestostartwith,weuseaforward-differenceapproximationforthe\ntimederivativeofthetemperature:\nùúïT(x,t)\nùúït‚âÉT(x,t+Œît)‚àíT(x,t)\nŒît. (22.15)\nBecauseweknowthespatialvariationofthetemperaturealongtheentiretoprow,andthe\nleft and right sides, we are less constrained with the space derivative than with the time\nderivative.Consequently,aswedidwiththeLaplaceequation,weusethemoreaccurate\ncentral-differenceapproximationforthespacederivative:\nùúï2T(x,t)\nùúïx2‚âÉT(x+Œîx,t)+T(x‚àíŒîx,t)‚àí2T(x,t)\n(Œîx)2. (22.16)\nx\nti ‚Äì 1,ji  + 1,j\ni,j + 1i,jFigure 22.2 The algorithm for the heat equation in\nwhich the temperature, at the location x=iŒîxand\ntimet=(j+1)Œît, is computed from the\ntemperature values at three points from an earlier\ntime. The nodes with white centers correspond to\nknown initial and boundary conditions. (The\nboundaries are placed artiÔ¨Åcially close for\nillustrative purposes.)\n22.2 Time Stepping (Leapfrog) Algorithm 455\nFigure 22.3 A visualization of a numerical calculation\nof the temperature versus position and versus time.\n8080\n400 1000\ntT\nx\nSubstitutionoftheseapproximationsinto(22.4)yieldstheheatdifferenceequation:\nT(x,t+Œît)‚àíT(x,t)\nŒît=K\nCùúåT(x+Œîx,t)+T(x‚àíŒîx,t)‚àí2T(x,t)\nŒîx2. (22.17)\nWereorder(22.17)intoaforminwhich Tcanbesteppedforwardin t:\nTi,j+1=Ti,j+ùúÇ[Ti+1,j+Ti‚àí1,j‚àí2Ti,j],ùúÇ =KŒît\nCùúåŒîx2. (22.18)\nHere,x=iŒîxandt=jŒît.Thisalgorithmis explicitbecauseitprovidesasolutioninterms\nofknownvaluesofthetemperature.Ifwewantedtosolveforthetemperatureatalllattice\nsites in Figure 22.2 simultaneously, then we would be using an implicitalgorithm. With\natimesteppingalgorithm,weneedtokeeptrackofonlyfourtemperatures.Asindicated\nin Figure 22.2, the temperature at space-time point (i,j+1)is computed from the three\ntemperature values at an earlier time j, and at adjacent space values i¬±1,i. We start the\nsolutionatthetoprow,movingitforwardintimeforaslongaswewant,alwayskeeping\nthetemperatureattheendfixedat0K.Figure22.3showsofthetimeandspacedependence\nofthesolution.\n22.2.1 Von Neumann Stability Condition\nWhenthedifference-equationversionofaPDEissolved,thehopeisthatitssolutionisa\ngoodapproximationtothesolutionofthePDE.Ifthesolutiontothedifference-equation\ndiverges, then we don‚Äôt have any solution; but if it does converge, then we should feel\nconfident that we have a good solution to the PDE.1ThevonNeumannstabilityanalysis\ntelluswhatweneeddotogetagoodsolution[Press etal.,2007;Courant etal.,1928].The\nanalysisisbasedontheassumptionthatafterthe jthtime-step,theapproximatesolution\nhastheform:\nTi,j=ùúâ(k)jeIkiŒîx, (22.19)\nwherex=iŒîx,t=jŒît,andI=‚àö\n‚àí1 istheimaginarynumber.Theconstant kin(23.24)\nis an unknown wave vector (2 ùúã‚àïùúÜ), andùúâ(k)is an unknown complex function. We view\n(22.19)asafunctionthatoscillatesinspace(theexponential),withtheamplitudeor ampli-\nficationfactor ùúâ(k)jthatgetsmultipliedbythepowerof ùúâforeachtimestep.Stabilityofthe\nsolutionthenrequires |ùúâ(k)|<1,elsethesolutionwouldgrowintime[Press etal.,2007;\n1 Well,inthecaseofshockwaves,asinChapter25,divergencesmaynotbesuchabadthing.",7525
213-22.4 The CrankNicolson Algorithm.pdf,213-22.4 The CrankNicolson Algorithm,"456 22 Heat Flow and Leapfrogging\nAncona,2002].Tosolvefortheamplitude,wesubstitute(22.19)intothedifferenceequation\n(22.18):\nùúâj+1eikmŒîx=ùúâjeikmŒîx+ùúÇ[ùúâjeik(m+1)Œîx+ùúâjeik(m‚àí1)Œîx‚àí2ùúâjeikmŒîx].\nAftercancelingacommonfactor,itiseasytosolvefor ùúâ(k):\nùúâ(k)=1+2ùúÇ[cos(kŒîx)‚àí1]. (22.20)\nInorderfor |ùúâ(k)|<1forallpossible kvalues,wemusthave:\nùúÇ=KŒît\nCùúåŒîx2<1\n2. (22.21)\nThisequationtellsusthatifwemakethetimestep Œîtsmaller,wewillalwaysimprovethe\nstability,asonewouldexpect.Butifwemakethespacestep Œîxsmaller,withoutaconcor-\ndantquadratic increaseinthetimestep,wewillworsenthestability.\n22.2.2 Implementation\nRecallthatwewanttosolveforthetemperaturedistributionwithinanaluminumbarof\nlengthL=1m,subjecttotheboundaryandinitialconditions:\nT(x=0,t)=T(x=L,t)=0‚àòC,T(x,t=0)=100‚àòC. (22.22)\nThethermalconductivity,specificheat,anddensityforAlare:\nK=237W/(mK) ,C=900J/(kgK) ,ùúå=2700kg/m3. (22.23)\n1) Writeaprogram,ormodify EqHeat.pyinListing22.1,tosolvetheheatequation.\n2) Definea2Darray T[101,2]forthetemperatureasafunctionofspaceandtime.Thefirst\nindexisforthe100spacedivisionsofthebar,andthesecondindexisforpresentand\npasttimes(becauseyoumayhavetomakethousandsoftimesteps,yousavememoryby\nsavingonlytwotimes).\n3) Fortime t=0(j=1),initialize Tsothatallpointsonthebar,excepttheends,areat100.\nSetthetemperatureoftheendsto0.\n4) Apply(22.15)toobtainthetemperatureatthenexttimestep.\n5) Assignthepresenttimevaluesofthetemperaturetothepastvalues: T[i,1] = T[i,2],\ni = 1, . . . , 101.\n6) Startwith50timesteps.Onceyouareconfidenttheprogramisrunningproperly,use\nthousandsofstepstoseethebarcoolsmoothlywithtime.Forapproximatelyevery500\ntimesteps,printthetimeandtemperaturealongthebar.\n22.2.3 Assessment and Visualization\n1) Check that your program gives a temperature distribution that varies smoothly with\ntime,thatsatisfiestheboundaryconditions,andthatreachesequilibrium.Youmayhave\ntovarythetimeandspacestepstoobtainstablesolutions.\n2) Compare the analytic and numeric solutions (and the wall times needed to com-\npute them). If the solutions differ, suspect the one that does not appear smooth and\ncontinuous.\n3) Makeasurfaceplotoftemperature versuspositionand versustime.\n22.3 Newton‚Äôs Radiative Cooling 457\nFigure 22.4 Temperature versus\nposition and time when two bars at\ndiffering temperatures are placed in\ncontact at t=0. The projected contours\nshow the isotherms.\n0\n40\n80 02040050100\nxtT(x, t)\n4) Plotthe isotherms(contoursofconstanttemperature).\n5) Createananimationthatshowsthetemperatureoftheentirebarasafunctionoftime.\nOurcode EqHeatAnimate.py intheonlinecodesdirectorydoesthatwiththeVisualpack-\nage.\n6)Stability test: Verifythestabilitycondition(22.21)byobservinghowthetemperature\ndistributiondivergesif ùúÇ>1\n4.\n7)Material dependence: Repeatthecalculationforiron.Notethatthestabilitycondition\nrequiresyoutochangethesizeofthetimestep.\n8)Two bars in contact: Two identical bars, 0.25m long, are placed in end-to-end con-\ntact,withtheirotherendskeptat0‚àòC.Oneisinitiallyat100‚àòC,andtheotherat50‚àòC.\nDeterminehowthetemperaturevarieswithtimeandlocation(Figure22.4).\n22.3 Newton‚Äôs Radiative Cooling\nImaginenow,abarincontactwithanenvironmentatatemperature Te.Newton‚Äô sla wof\ncoolinggivestherateoftemperaturechange,asaresultofradiation,as:\nùúïT\nùúït=‚àíh(T‚àíTe), (22.24)\nwithhapositiveconstant.Includingradiativecoolingandleavingofftheconstantmodifies\ntheheatequationto:\nùúïT(x,t)\nùúït=K\nCùúåùúï2T\nùúï2x‚àíhT(x,t). (22.25)\n1) ModifytheheatequationalgorithmtoincludeNewton‚Äôscooling.\n2) Comparethecoolingofaradiatingbarwiththatoftheinsulatedbar.\n3) Solveforconductiveandradiativeheatflowwithina2Drectangularironplatewhichis\ninitiallyatatemperatureof100‚àòC.Threesidesaremaintainedat100‚àòC,whilethetop\nisplacedincontactwithicewaterat0‚àòC.\na) Make separate surface plots of T(x,y=fixed,t)andT(x=fixed,y,t),o ras e r i e so f\nplotsofT(x,y,t=fixed)forvarious tvalues.\nb) Afteranequilibriumisreached,makeaplotoftheisotherms T(x,y,t=‚àû ).\n458 22 Heat Flow and Leapfrogging\n4) Sometimes,anequilibriumisreachedinwhichthetemperaturenolongerchangesasa\nfunctionoftime.Inthiscase,theheatequation(22.3)takestheform:\n‚àá2T(x,t)=0. (22.26)\nThis is the same as Laplace‚Äôsequation,whichis studied in Section 21.4.1,and can be\nsolvedusingthesame relaxationalgorithmdevelopedthere.Andso,solveforisotherms\nwithina2Drectangularironplateinwhichthreesidesaremaintainedat100‚àòC,while\nthefourthside,remainingincontactwithicewater,at0‚àòC.\n5) Compute the rate of heat flow from the center to the surface of a sphere of constant\nthermalconductivity.Thecenterofthesphereiskeptat0‚àòCandthesurfaceiskeptat\n100‚àòC.\n6) Aspherewithconstantthermalconductivityisinitiallyat0‚àòC,andisthenplacedina\nheatbathof100‚àòC.Computethetemperatureprofileofthesphereasafunctionoftime.\n7) A composite sphere is composed of a material of high thermal conductivity up to its\nmiddle,and low conductivityfrom its middleto its outside. Compute the rate of heat\nflowfromthecentertothesurfaceofasphere,ifthecenteriskeptat0‚àòCandthesurface\niskeptat100‚àòC.\n22.4 The Crank‚ÄìNicolson Algorithm\nTheCrank‚ÄìNicolsonalgorithmprovidesahigherdegreeofprecisionfortheheatequation\n(22.3) than the simple leapfrog method [Crank and Nicolson, 1946]. The algorithm\ncalculates the time derivative with a central-difference approximation, in contrast to the\nforward-differenceapproximationusedpreviously.Inordertoavoidintroducingerrorfor\ntheinitialtimestep,whereonlyasingletimevalueisknown,themethodusesa splittime\nstep,2sothattimeisadvancedfromtime ttot+Œît‚àï2:\nùúïT\nùúït(\nx,t+Œît\n2)\n‚âÉT(x,t+Œît)‚àíT(x,t)\nŒît+O(Œît2). (22.27)\nYes,weknowthatthislooksjustliketheforward-differenceapproximationforthederiva-\ntiveattime t+Œît,forwhichitwouldbeabadapproximation;regardless,itisabetter,but\nmorecomplicated,approximationforthederivativeattime t+Œît‚àï2.Likewise,in(22.15),\nwegavethecentral-differenceapproximationforthesecondspacederivativefortime t.For\nt=t+Œît‚àï2,thatbecomes:\n2(Œîx)2ùúï2T\nùúïx2(\nx,t+Œît\n2)\n‚âÉ [T(x‚àíŒîx,t)‚àí2T(x,t)+T(x+Œîx,t)]\n+[T(x‚àíŒîx,t+Œît)‚àí2T(x,t+Œît)+T(x+Œîx,t+Œît)]+O(Œîx2).\nIntermsoftheseexpressions,theheatdifferenceequationis:\nTi,j+1‚àíTi,j=ùúÇ\n2[Ti‚àí1,j+1‚àí2Ti,j+1+Ti+1,j+1+Ti‚àí1,j‚àí2Ti,j+Ti+1,j],\nx=iŒîx,t=jŒît,ùúÇ=KŒît\nCùúåŒîx2. (22.28)\n2 InChapter24,wedevelopsplit-timealgorithmsforsolutiontotheSchr√∂dingerequationandMaxwell‚Äôs\nequations.\n22.4 The Crank‚ÄìNicolson Algorithm 459\nWegrouptogethertermsinvolvingthesametemperature,toobtainanequationwithfuture\ntimesontheLHSandpresenttimesontheRHS:\n‚àíTi‚àí1,j+1+(\n2\nùúÇ+2)\nTi,j+1‚àíTi+1,j+1=Ti‚àí1,j+(\n2\nùúÇ‚àí2)\nTi,j+Ti+1,j.(22.29)\nThis equation represents an implicitscheme for the temperature Ti,j, where ‚Äúimplicit‚Äù\nmeans that we must solve simultaneous equations to obtain the solution at current ( j)\nand future ( j+1) times. In contrast, an explicitscheme uses the solution at current and\npast times to obtain it at future times. We start with the initial temperature distribution\nthroughoutallofspace,theboundaryconditionsattheendsofthebarforalltimes,and\ntheapproximatevaluesfromthefirstderivative:\nTi,0,(known),T0,j,(known),TN,j,(known),\nT0,j+1=T0,j=0,TN,j+1=0,TN,j=0.(22.30)\nWerearrange(22.29)sothatwecanusetheseknownvaluesof Ttostepthe j=0solution\nforwardintime,andexpressitasasetofsimultaneouslinearequations:\n‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£(\n2\nùúÇ+2)\n‚àí1\n‚àí1(\n2\nùúÇ+2)\n‚àí1\n‚àí1(\n2\nùúÇ+2)\n‚àí1\n.........\n‚àí1(\n2\nùúÇ+2)\n‚àí1\n‚àí1(\n2\nùúÇ+2)‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£T1,j+1\nT2,j+1\nT3,j+1\n...\nTn‚àí2,j+1\nTn‚àí1,j+1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶\n=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£T0,j+1+T0,j+(\n2\nùúÇ‚àí2)\nT1,j+T2,j\nT1,j+(\n2\nùúÇ‚àí2)\nT2,j+T3,j\nT2,j+(\n2\nùúÇ‚àí2)\nT3,j+T4,j\n...\nTn‚àí3,j+(\n2\nùúÇ‚àí2)\nTn‚àí2,j+Tn‚àí1,j\nTn‚àí2,j+(\n2\nùúÇ‚àí2)\nTn‚àí1,j+Tn,j+Tn,j+1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (22.31)\nObserve that the T‚Äôs on the RHS are all at the present time jfor various positions, and\natfuturetime j+1forthetwoends(whose Tsareknownforalltimesviatheboundary\nconditions).Westartthealgorithmwiththe Ti,j=0valuesoftheinitialconditions,thensolve\namatrixequationtoobtain Ti,j=1.Oncewehavethatsolution,weknowallthetermsonthe\nRHSoftheequations( j=1throughoutthebar,and j=2attheends),andsocanrepeat\nthesolutionofthematrixequationstoobtainthetemperaturethroughoutthebarfor j=2.\nSoagain,wetime-stepforward,onlynowwesolvematrixequationsateachstep.Thatgives\nusthespatialsolutionatalllocationssimultaneously.",8369
214-22.5 Code Listings.pdf,214-22.5 Code Listings,"460 22 Heat Flow and Leapfrogging\nNotonlyistheCrank-Nicolsonmethodmoreprecisethanthelow-ordertime-stepping\nmethod, but it also is stable for all values of ŒîtandŒîx. To prove that, we apply the von\nNeumann stability analysis, discussed in Section 22.2.1 to the Crank-Nicolson algorithm\nbysubstituting(22.18)into(22.29).Thisdeterminestheamplitude:\nùúâ(k)=1‚àí2ùúÇsin2(kŒîx‚àï2)\n1+2ùúÇsin2(kŒîx‚àï2). (22.32)\nBecausethenumeratorisalwayssmallerthanthedenominator, |ùúâ|‚â§1forallŒît,Œîx,and\nk,andsowealwayshavestability.\n22.4.1 Solution via Tridiagonal Matrix ‚äô\nThe Crank-Nicolson equations (22.31) are in the standard form, [A]x=b, for linear\nequations,andsowecanuseourmatrixmethodstosolvethem.However,thecoefficient\nmatrix[A]istridiagonal(zeroelementsexceptforthemaindiagonalandtwodiagonalson\neithersideofit):\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùd1c100¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\na2d2c20¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n0a3d3c3¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑\n0000 ¬∑¬∑¬∑aN‚àí1dN‚àí1cN‚àí1\n0000 ¬∑¬∑¬∑0aNdN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùx1\nx2\nx3\n...\nxN‚àí1\nxN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†=‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùb1\nb2\nb3\n...\nbN‚àí1\nbN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†,\nConsequently,amorerobustandfastersolutionexists,anditmakesthisimplicitmethodas\nfastastheexplicitones.Seeingthattridiagonalsystemsoccurfrequently,wenowoutline\nthespecializedtechniqueforsolvingthem[Press etal.,2007].If westorethematrixele-\nmentsai,jusingbothsubscripts,thenwewillneed N2locationsforelements,and N2oper-\nations to access them. However, if the matrix is tridiagonal, we only need to store those\nelementsalong,above,andbelowthediagonals,{di}\ni=1,N,{ci}\ni=1,N,and{ai}\ni=1,N.Thesin-\nglesubscriptson ai,di,andcireducetheprocessingfrom N2to(3N‚àí2)elements.\nThesolutiontothematrixequationmanipulatestheindividualequationsuntilthecoef-\nficientmatrixisin uppertriangular form,withalltheelementsofthemaindiagonalequal\nto1.Westartbydividethefirstequationby d1,thensubtract a2timesthefirstequation,\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù1c1\nd100¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n0d2‚àía2c1\nd1c20¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n0a3d3c3¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑\n000 0 ¬∑¬∑¬∑aN‚àí1dN‚àí1cN‚àí1\n000 0 ¬∑¬∑¬∑0aNdN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùx1\nx2\nx3\n...\n‚ãÖ\nxN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†=‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùb1\nd1\nb2‚àía2b1\nd1\nb3\n...\n‚ãÖ\nbN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†,\n22.4 The Crank‚ÄìNicolson Algorithm 461\nandthendividingthesecondequationbytheseconddiagonalelement,\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù1c1\nd100 ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n01c2\nd2‚àía2c1\na10¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n0a3d3c3¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ 0\n¬∑¬∑¬∑¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑ ¬∑¬∑¬∑\n00 0 0 aN‚àí1dN‚àí1cN‚àí1\n00 0 0 ¬∑¬∑¬∑0aNdN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùx1\nx2\nx3\n...\n‚ãÖ\nxN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†=‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùb1\nd1\nb2‚àía2b1\nd1\nd2‚àía2c1\nd1\nb3\n...\n‚ãÖ\nbN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†.\nAssuming that we can repeat these steps without ever dividing by zero, the system of\nequationswillbereducedtouppertriangularform,\n‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éù1h100¬∑¬∑¬∑0\n01h20¬∑¬∑¬∑0\n00 1h3¬∑¬∑¬∑0\n0¬∑¬∑¬∑¬∑¬∑¬∑......¬∑¬∑¬∑\n00 0 0 ¬∑¬∑¬∑¬∑¬∑¬∑\n00 0 ¬∑¬∑¬∑01‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùx1\nx2\nx3\n...\n‚ãÖ\nxN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†=‚éõ\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú\n‚éú‚éùp1\np2\np3\n...\n‚ãÖ\npN‚éû\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü\n‚éü‚é†, (22.33)\nwhereh1=c1‚àïd1andp1=b1‚àïd1.Wethenrecurfortheotherelements:\nhi=ci\ndi‚àíaihi‚àí1,pi=bi‚àíaipi‚àí1\ndi‚àíaihi‚àí1. (22.34)\nFinally,backsubstitutionleadstotheexplicitsolutionfortheunknowns:\nxi=pi‚àíhixi‚àí1;i=n‚àí1,n‚àí2,‚Ä¶,1,xN=pN. (22.35)\nInListing22.2,wegivetheprogram HeatCNTridiag.py thatsolvestheheatequationusing\ntheCrank‚ÄìNicolsonalgorithmviaatriadiagonalreduction.\n22.4.2 Crank‚ÄìNicolson Implementation\n1) WriteaprogramusingtheCrank‚ÄìNicolsonmethodtosolveforheatflowinthemetal\nbarofSection22.1.\n2) Solvethelinearsystemofequations(22.31)usingeitherNumPyoraspecialtridiagonal\nalgorithm.\n3) Checkthestabilityofyoursolutionbychoosingdifferentvaluesforthetimeandspace\nsteps.\n4) Constructacontouredsurfaceplotoftemperature versuspositionandversustime.\n5) Comparetheimplicitandexplicitalgorithmsusedinthischapterforrelativeprecision\nandspeed.Youmayassumethatastableanswerthatusesverysmalltimestepsisaccu-\nrate.\n462 22 Heat Flow and Leapfrogging\n22.5 Code Listings\nListing 22.1 EqHeat.py solvesthesinglespacedimensionheatequationonalatticeby\nleapfroggingtheinitialconditionsforwardintime.Theparametersshouldbeadjusted.\n# EqHeat.py: solves heat equation via finite differences , 3 ‚àíDp l o t\nfromnumpyimport ‚àó;importmatplotlib.pylab as p\n4frommpl_toolkits.mplot3d importAxes3D\nNx = 101; Nt = 3000; Dx = 0.03; Dt = 0.9\nkappa = 210.; C = 900.; rho = 2700. # Conductivity , specf heat , density\n8T=z e r o s( ( N x , 2 ), float); Tpl = zeros((Nx, 31), float)\nprint(""Working, wait for figure after count to 10"" )\nforixin range (1, Nx ‚àí1): T[ix, 0] = 100.0; # Initial T\n12T [ 0 , 0 ]=0 . 0; T [ 0 , 1 ]=0 . #1 s t&last T = 0\nT[Nx‚àí1,0] = 0. ; T[Nx ‚àí1,1] = 0.0\ncons = kappa/(C ‚àórho) ‚àóDt/(Dx ‚àóDx);\nm=1 # Counter\n16fortin range (1, Nt):\nforixin range (1, Nx ‚àí1):\nT[ix, 1] = T[ix, 0] + cons ‚àó(T[ix+1, 0] + T[ix ‚àí1, 0]‚àí2.‚àóT[ix,0])\nift%300 == 0 ort= =1 : # Every 300 steps\n20 forixin range (1, Nx ‚àí1, 2): Tpl[ix, m] = T[ix, 1]\nprint(m)\nm=m+1\nforixin range (1, Nx ‚àí1): T[ix, 0] = T[ix, 1]\n24x=list(range(1, Nx ‚àí1, 2)) # Plot alternate pts\ny=list(range(1, 30))\nX, Y = p.meshgrid(x, y)\n28deffunctz(Tpl):\nz=T p l [ X ,Y ]\nreturnz\n32Z=f u n c t z( T p l )\nfig = p.figure() # Create figure\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô)\n36ax.set_xlabel( ‚ÄôPosition‚Äô )\nax.set_ylabel( ‚Äôtime‚Äô)\nax.set_zlabel( ‚ÄôTemperature‚Äô )\np.show()\n40print(""finished"" )\nListing 22.2 HeatCNTridiag.py solves the heat equation via the Crank-Nicolson\nmethodandatridiagonalmatrixalgorithm.\n# HeatCNTridiag .py : solution of heat eqtn via CN method\n2\n"""""" Dirichlet boundary conditions surrounding four walls\nDomain dimensions: WxH, with 2 triangles per square\nBased on FEM2DL_Box Matlab program in Polycarpou, Intro to the Finite\n6Element Method in Electromagnetics, Morgan &Claypool (2006) """"""\nimportmatplotlib.pylab as p;\nfrommpl_toolkits.mplot3d importAxes3D ;\n10fromnumpyimport ‚àó;\nimportnumpy;\n14Max = 51; n = 50; m= 50\nTa = zeros((Max) , float); Tb=zeros((M a x), float); Tc= zeros((M a x), float)\nTd = zeros((Max) , float); a= zeros((M a x), float); b= zeros((M a x), float)\nc= z e r o s ( ( M a x ) , float); d= zeros((M a x), float); x= zeros((M a x), float)\n22.5 Code Listings 463\n18t= z e r o s ( ( M a x , M a x ) , float)\ndefTridiag(a, d, c, b, Ta, Td, Tc, Tb, x, n):\nMax = 51\n22h=z e r o s( ( M a x ), float)\np=z e r o s (( M a x ), float)\nforiin range (1,n+1):\na[i] =Ta[i]\n26 b[i] =Tb[i]\nc[i] = Tc[i]\nd[i] =Td[i]\nh[1] = c[1]/d[1]\n30p[1] = b[1]/d[1]\nforiin range (2,n+1):\nh[i] = c[i] / (d[i] ‚àía[i] ‚àóh[i‚àí1])\np[i] = (b[i] ‚àía[i] ‚àóp[i‚àí1]) / (d[i] ‚àía[i] ‚àóh[i‚àí1])\n34x[n] = p[n]\nforiin range (n‚àí1, 1,‚àí1 ): x[i] =p[i] ‚àíh[i] ‚àóx[i+1]\nwidth = 1.0; height = 0.1; ct = 1.0\n38foriin range (0, n): t[i,0] = 0.0\nforiin range ( 1, m): t[0][i] = 0.0\nh= w i d t h/ ( n ‚àí1)\nk = height / ( m ‚àí1)\n42r= c t ‚àóct‚àók/(h ‚àóh)\nforjin range (1,m+1):\nt[1,j] = 0.0\n46 t[n,j] = 0.0 #B C s\nforiin range ( 2, n): t[i][1] = sin( pi ‚àóh‚àói) # ICs\nforiin range (1, n+1): Td[i] = 2. + 2./r\nTd[1] = 1.; Td[n] = 1.\n50foriin range (1,n ): Ta[i] = ‚àí1.0; Tc[i] = ‚àí1.0; # Off diagonal\nTa[n‚àí1] = 0.0; Tc[1] = 0.0; Tb[1] = 0.0; Tb[n] = 0.0\nprint(""I‚Äôm working hard, wait for fig while I count to 50"" )\n54forjin range (2,m+1):\nprint(j)\nforiin range (2,n): Tb[i] = t[i ‚àí1][j‚àí1] + t[i+1][j ‚àí1] \\n+( 2 / r‚àí2)‚àót[i][j‚àí1]\n58 Tridiag(a, d, c, b, Ta, Td, Tc, Tb, x, n) # Solve system\nforiin range (1, n+1): t[i][j] = x[i]\nprint(""Finished"" )\nx=list(range(1, m+1)) # Plot every other x\n62y=list(range(1, n+1)) # every other y\nX, Y = p.meshgrid(x,y)\ndeffunctz(t): # Potential\n66z=t [ X ,Y ]\nreturnz\nZ=f u n c t z(t)\n70fig = p.figure()\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Z, color= ‚Äôr‚Äô)\nax.set_xlabel( ‚Äôt‚Äô)\n74ax.set_ylabel( ‚Äôx‚Äô)\nax.set_zlabel( ‚ÄôT‚Äô)\np.show() # Display figure",8166
215-Chapter 23 String and Membrane Waves.pdf,215-Chapter 23 String and Membrane Waves,,0
216-23.2 TimeStepping Algorithm.pdf,216-23.2 TimeStepping Algorithm,"464\n23\nString and Membrane Waves\nIn this chapter, and in Chapters 24 ‚Äì26, we explore PDE‚Äôs with wave-like solutions. Here we\ndeal with 1D waves on strings, and 2D waves on membranes. In Chapter 24 we examine\nquantum wave packets and E&M waves, and in Chapter 25 we look at shock waves and\nsolitary waves. The basic technique is the leapfrog algorithm that propagates the initial\nconditions forward in time, step by step. The numerical solutions let us include more physics\nthan is possible with the familiar analytic treatments .\n23.1 A Vibrating String‚Äôs Hyperbolic Wave Equation\nRecalltheelementaryphysicsdemonstrationinwhichastring,tieddownatbothends,is\npluckedgently,andreleased,resultinginapulsethattravelsalongthestring.\nProblem Developanaccuratemodelforwavepropagationonastring,andseeifitcan\nproducebothtravelingandstandingwaves.\nConsider a string of length Ltied down at both ends (Figure 23.1 left). The string has a\nconstantdensity ùúåperunitlength,nofrictionalforcesactingonit,andatension Tthatis\nhighenoughtoletusignoreanysaggingasaresultofgravity.Weassumethatthedisplace-\nmentofthestringfromitsrestposition, y(x,t),isonlyintheverticaldirection,andthatthe\ndisplacementisafunctionofthehorizontallocationalongthestring x,andthetime t.\nToderivealinearequationofmotion,weassumethatthestring‚Äôsrelativedisplacement\ny(x,t)‚àïLand slope ùúïy‚àïùúïxare both small. In Figure 23.1 right, we isolate an infinitesimal\nsectionŒîxofthestring.Weseetherethatthedifferenceintheverticalcomponentsofthe\ntension,ateitherendofthestring,producestherestoringforcethatacceleratesthissection\nofthestringupordown.ByapplyingNewton‚Äôslawstothissection,weobtainthefamiliar\nwaveequation:\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n23.1 A Vibrating String‚Äôs Hyperbolic Wave Equation 465\nLTT\ny(x,t)\nŒîxŒîy\nŒ∏ x\nFigure 23.1 Left: A stretched string of length Ltied down at both ends. The vertical disturbance of\nthe string from its equilibrium position is y(x,t).Right: A differential element of the string showing\nhow the string‚Äôs displacement leads to the restoring force.\n‚àë\nFy=ùúåŒîxùúï2y\nùúït2, (23.1)\n=TsinùúÉ(x+Œîx)‚àíTsinùúÉ(x) (23.2)\n=Tùúïy\nùúïx||||x+Œîx‚àíTùúïy\nùúïx||||x‚âÉTùúï2y\nùúïx2, (23.3)\n‚áíùúï2y(x,t)\nùúïx2=1\nc2ùúï2y(x,t)\nùúït2,c=‚àö\nT\nùúå. (23.4)\nHere,wehaveassumedthat ùúÉissmallenoughforsin ùúÉ‚âÉtanùúÉ=ùúïy‚àïùúïx.Theexistenceofthe\ntwoindependentvariables, xandt,makes(23.4)aPDE.Theconstant chereisthevelocity\nwith which a disturbance travels along the wave, and is seen to decrease for increasing\ndensity,andincreaseforincreasingtension.Notethatthis signalvelocityc isnotthesame\nasthevelocityofastringelement ùúïy‚àïùúït.\nTheinitialconditionforourproblemisthatthestringispluckedgentlyandreleased.We\nassumethatthepluckplacesthestringinatriangularshape,withthecenteroftriangle8\n10\nofthewaydownthestring,andwithaheightof1:\ny(x,t=0)={\n1.25x‚àïL,x‚â§0.8L,\n(5‚àí5x‚àïL),x>0.8L,(initialcondition1) . (23.5)\nBecause (23.4)is second-order in time, a second initial condition is needed to determine\nthesolution.Weinterpretthe‚Äúgentleness‚Äùoftheplucktomeanthatthestringisreleased\nfromrest:\nùúïy\nùúït(x,t=0)=0,(initialcondition2) . (23.6)\nTheboundaryconditionshavebothendsofthestringtieddownatalltimes:\ny(0,t)‚â°0,y(L,t)‚â°0,(boundaryconditions). (23.7)\n23.1.1 Solution as Normal-Mode Expansion\nTheanalyticsolutionto(23.4)isobtainedviathefamiliarseparation-of-variablestechnique.\nWeassumethatthesolutionistheproductofafunctionofspaceandafunctionoftime:\ny(x,t)=X(x)T(t). (23.8)\n466 23 String and Membrane Waves\nWesubstitute(23.8)into(23.4),divideitby y(x,t),andareleftwithanequationthathasa\nsolutiononlyiftherearesolutionstothetwoODEs:\nd2T(t)\ndt2+ùúî2T(t)=0,d2X(x)\ndx2+k2X(x)=0,kdef=ùúî\nc. (23.9)\nThe angular frequency ùúîand the wave vector kare determined by demanding that the\nsolutionssatisfytheboundaryconditions:\nX(x=0,t)=X(x=l,t)=0 (23.10)\n‚áíXn(x)=Ansinknx,kn=ùúã(n+1)\nL,n=0,1,‚Ä¶. (23.11)\nThetimesolutionis:\nTn(t)=Cnsinùúînt+Dncosùúînt,ùúîn=nck0=n2ùúãc\nL, (23.12)\nwhereùúînisthefrequencyofthe nthnormalmode .Theinitialcondition (23.5)ofzeroveloc-\nity,ùúïy‚àïùúït(t=0)=0,requiresthe Cnvaluesin(23.12)tobezero.Puttingthepiecestogether,\nthenormal-modeare:\nyn(x,t)=sinknxcosùúînt,n=0,1,‚Ä¶. (23.13)\nBecausethewaveequation(23.4)islinearin y,theprincipleoflinearsuperpositionholds,\nandthemostgeneralsolutionforwavesonastringwithfixedendscanbewrittenasthe\nsumofnormalmodes:\ny(x,t)=‚àû‚àë\nn=0Bnsinknxcosùúînt. (23.14)\n(Wewillloselinearsuperpositiononceweincludenonlineartermsinthewaveequation.)\nThe Fourier coefficient Bnis determined by the second initial condition (23.5), which\ndescribeshowthewaveisplucked:\ny(x,t=0)=‚àû‚àë\nnBnsinnk0x. (23.15)\nWemultiplybothsidesbysin mk0x,substitutethevalueof y(x,0)from(23.5),andintegrate\nfrom0toLtoobtain:\nBm=6.25sin(0.8mùúã)\nm2ùúã2. (23.16)\nYouwillbeaskedtocomparetheFourierseries(23.14)toyournumericalsolution.Whileit\nisinthenatureoftheapproximationthattheprecisionofthenumericalsolutiondepends\nonthechoiceofstepsize,itisalsorevealingtorealizethattheprecisionoftheso-called\nanalyticsolutiondependsonsumminganinfinitenumberofterms,whichcanbesummed\nonlyapproximately.\n23.2 Time-Stepping Algorithm\nAswiththeheatequation,welookforasolution y(x,t)onlyfordiscretevaluesoftheinde-\npendentvariables,inthiscase xandt(Figure23.2):\nx=iŒîx,i=1,‚Ä¶,Nx,t=jŒît,j=1,‚Ä¶,Nt, (23.17)\ny(x,t)=y(iŒîx,iŒît)def=yi,j. (23.18)\n23.2 Time-Stepping Algorithm 467\nFigure 23.2 The solutions of the wave\nequation for four earlier space-time points are\nused to obtain the solution at the present\ntime. The boundary and initial conditions are\nindicated by the white-centered dots.X\ni, jti ‚Äì 1, j i + 1, ji, j ‚Äì 1\ni, j + 1\nWeseeksolutionsatthelatticesitesofthespace-timegridinFigure23.2.Thatbeingthe\ncase, moving across a row corresponds to increasing xvalues along the string for a fixed\ntime,whilemovingdownacolumncorrespondstoincreasingtimestepsforafixedposition.\nAlthoughthegridinFigure23.2maybesquare,wecannotusearelaxationtechniquelike\nwedidforthesolutionofLaplace‚Äôsequation,becausewedonotknowthesolutiononall\nfoursides.Theboundaryconditionsdeterminethesolutionalongtherightandleftsides,\nwhiletheinitialtimeconditiondeterminesthesolutionalongthetop,butnotthebottom.\nAswiththeheatequation,weusethecentral-differenceapproximationto discretizethe\nwaveequationintoadifferenceequation.First,weexpressthesecondderivativesinterms\noffinitedifferences:\nùúï2y\nùúït2‚âÉyi,j+1+yi,j‚àí1‚àí2yi,j\n(Œît)2,ùúï2y\nùúïx2‚âÉyi+1,j+yi‚àí1,j‚àí2yi,j\n(Œîx)2. (23.19)\nSubstituting(23.19)inthewaveequation(23.4)yieldsadifferenceequation:\nyi,j+1+yi,j‚àí1‚àí2yi,j\nc2(Œît)2=yi+1,j+yi‚àí1,j‚àí2yi,j\n(Œîx)2. (23.20)\nNoticethatthisequationcontainsthreetimevalues: j+1=thefuture, j=thepresent,and\nj‚àí1=the past. Consequently, we rearrange it into a form that permits us to predict the\nfuturesolutionfromthepresentandpastsolutions:\nyi,j+1=2yi,j‚àíyi,j‚àí1+c2\nc‚Ä≤2[yi+1,j+yi‚àí1,j‚àí2yi,j],c‚Ä≤def=Œîx\nŒît. (23.21)\nHerec‚Ä≤isacombinationofnumericalparameters,withthedimensionofvelocity,whose\nsize relative to cdetermines the stability of the algorithm. As shown in Figure 23.2, the\nalgorithm (23.21) propagates the wave from the two earlier times, jandj‚àí1, and from\nthreenearbypositions, i‚àí1,i,andi+1,toalatertime, j+1,andasinglespaceposition, i.\nWestartthealgorithmwiththesolutionalongthetopmostrow,andthenmoveitdown\none step at a time. If we save the solution for present times, then we only need to store\nthreetimevaluesonthecomputer.Infact,becausethetimestepsmustbequitesmallto\nobtainhighprecision,youmaywanttostorethesolutiononlyforeveryfifthortenthtime\nforviewing.\nInitializingtherecurrencerelationisabittrickybecauseitrequiresdisplacementsfrom\ntwoearliertimes,whereastheinitialconditionsareforonlyonetime.Nonetheless,therest",7766
217-23.3.1 Implementation and Assessment.pdf,217-23.3.1 Implementation and Assessment,"468 23 String and Membrane Waves\nofthecondition(23.5),whencombinedwiththe central-difference approximation,letsus\nextrapolatetonegativetime:\nùúïy\nùúït(x,0)‚âÉy(x,Œît)‚àíy(x,‚àíŒît)\n2Œît=0,‚áíyi,0=yi,2. (23.22)\nHerewetaketheinitialtimeas j=1,andsoj=0correspondsto t=‚àí Œît.Substitutingthis\nrelationinto(23.21)yieldsfortheinitialstep:\nyi,2=yi,1+c2\n2c‚Ä≤2[yi+1,1+yi‚àí1,1‚àí2yi,1](forj=2only). (23.23)\nEquation(23.23)usesthesolutionthroughoutallspaceattheinitialtime t=0topropagate\n(leapfrog)itforwardtoatime Œît.Subsequenttimestepsuse(23.21),andarecontinuedfor\naslongasyoulike.\nExercise Devise a procedure for solving for the wave equation for all times in just one\nstep.Estimatehowmuchmemorywouldberequired.\n23.3 von Neumann Stability Analysis\nAs discussed in Section 22.2.1, our approximation of converting a PDE into a difference\nequationwillnotbeagoodapproachifthesolutiontothedifferenceequationisunstable.\nThevonNeumannstabilityanalysis telluswhattodotogetagoodsolution.Theanalysisis\nbasedontheassumptionthateigenmodesofthedifferenceequationhavetheform:\nyi,j=ùúâ(k)jeIkiŒîx, (23.24)\nwherex=iŒîx,t=jŒît,andI=‚àö\n‚àí1istheimaginarynumber.Theconstant kin(23.24)\nis an unknown wave vector (2 ùúã‚àïùúÜ), andùúâ(k)is an unknown complex function. We view\n(23.24) as a function that oscillates in space (the exponential), with the amplitude or\namplificationfactor ùúâ(k)jthatgetsmultipliedbyapowerof ùúâforeachtimestep.Stability\nofthesolutionthenrequires |ùúâ(k)|<1,elsethesolutionwouldgrowintime[Press etal.,\n2007;Ancona,2002].\nExercise Substitute the presumed form for the eigenmode (23.24) into difference\nequation (23.21) being used as the algorithm, and solve for the amplitude ùúâ(k).Hint:i n\nSection22.2.1,wedothisexplicitlyfortheheatequation.\nEven though the application of the stability condition may get complicated, [Press etal.,\n2007;Courant etal.,1928]showthat |ùúâ(k)|<1,andthedifferenceequationsolutionwill\nthereforebestable,forthegeneralclassoftransportequationsif:\nc‚â§c‚Ä≤=Œîx‚àïŒît,(Courantcondition) . (23.25)\nEquation(23.25)meansthatthesolutiongetsbetterwithsmaller timesteps,butgetsworse\nforsmallerspacesteps(unlessyousimultaneouslymakethetimestepsmallertoo).Having\ndifferentsensitivitiestothetimeandspacestepsmayappearsurprisingbecausethewave\nequation(23.4)issymmetricin xandt,yetthesymmetryisbrokenbythenonsymmetric\ninitialandboundaryconditions.",2345
218-23.4 Beyond The Simple Wave Equation.pdf,218-23.4 Beyond The Simple Wave Equation,,0
219-23.7 Numerical Solution.pdf,219-23.7 Numerical Solution,"23.4 Beyond The Simple Wave Equation 469\nIn general, you should perform a stability analysis for every PDE you have to solve,\nalthough it can get complicated. Yet, even if you do not, the lesson here is that you may\nwant to try different combinations ofŒîxandŒîtuntil a stable and reliable solution is\nobtained.Youmayexpect,nonetheless,thattherearechoicesfor ŒîxandŒîtforwhichthe\nnumerical solution fails, and that simply decreasing an individual ŒîxorŒît, in the hope\nthatthiswillincreaseprecision,maynotimprovethesolution.\n23.3.1 Implementation and Assessment\nThe program EqStringMat.py in Listing 23.1 solves the wave equation for a string of\nlengthL=1m,withitsendsfixed,andwiththegentlypluckedinitialcondition.Notethat\nalthoughouruseof L=1violatestheassumptionofsmalldisplacement, y‚àïL‚â™1;youmay\nwanttouse L=1000toberealistic.Thevaluesofdensityandtensionare ùúå=0.01kg/m,\nandT=40N,withthespacegridsetat101points,correspondingto Œî=0.01cm.\n1) Solve the wave equation, and make a surface plot of displacement versustime and\nposition.\n2) Exploreanumberofspacestepandtimestepcombinations.Inparticular,trystepsthat\nsatisfy, and that do not satisfy, the Courant condition (23.25). Does your exploration\nconformwiththestabilitycondition?\n3) Comparetheanalyticandnumericsolutions,summingatleast200termsintheanalytic\nsolution.\n4) Usetheplottedtimedependencetoestimatethepeak‚Äôspropagationvelocity c.Compare\nthededuced cto(23.4).\n5) Oursolutionofthewaveequationforapluckedstringleadstotheformationofawave\npacket that corresponds to the sum of multiple normal modes of the string. Solve the\nwaveequationforastringinitiallyplacedinthesinglenormalmode(standingwave):\ny(x,t=0)=0.001 sin2 ùúãx,ùúïy\nùúït(x,t=0)=0. (23.26)\nSeeifanormalmoderesultsandremains.\n6) Observe the motion of a wave for initial conditions corresponding to the sum of two\nadjacentnormalmodes.Doesbeatingoccur?\n23.4 Beyond The Simple Wave Equation\nThe string problem we have investigated so far can be handled by both numerical and\nanalytic techniques. We now wish to extend the theory to include some more realistic\nphysics. These extensions have only numerical solutions .\n23.4.1 Including Friction\nPlucked strings do not vibrate forever because the real world contains friction. Consider\nagaintheelementsofastringbetween xandx+dxinFigure23.1right,butnowimagine\nthat this element is moving in a viscous fluid such as air. An approximate model for the\nfrictionalforcehasitpointinginadirectionoppositetotheverticalvelocityofthestring,\n470 23 String and Membrane Waves\nt120\n80\n40\n080400\nxFigure 23.3 The vertical displacement as a function of\nposition xand time tfor waves on a string when friction\nis included.\nproportionaltothatvelocity,andalsoproportionaltothelengthofthestringelement:\nFf‚âÉ‚àí2ùúÖŒîxùúïy\nùúït. (23.27)\nHere,ùúÖisaconstantthatisproportionaltotheviscosityofthemedium.Includingthisforce\nintheequationofmotionchangesthewaveequationto:\nùúï2y\nùúït2=c2ùúï2y\nùúïx2‚àí2ùúÖ\nùúåùúïy\nùúït. (23.28)\nInFigure23.3,weshowtheresultingmotionofastringpluckedinthemiddlewhenfriction\nisincluded.Observehowtheinitialpluckbreaksupintowavestravelingtotherightandto\ntheleft,thatthengetreflectedandinvertedbythefixedends.Sincethosepartsofthewave\nwiththehighervelocityexperiencegreaterfriction,thepeaktendstogetsmoothedoutthe\nmostastimeprogresses.\nExercise Generalizethealgorithmusedtosolvethewaveequationtonowincludefric-\ntion.Pickavalueof ùúÖlargeenoughtocauseanoticeabledampening,butnotsolargeasto\nstoptheoscillations.Asacheck,reversethesignof ùúÖandseeifthewavegrowsintime.\n23.4.2 Including Variable Tension and Density\nWehavederivedthepropagationvelocityforwavesonastringas c=‚àö\nT‚àïùúå.Thissaysthat\nwavesmoveslowerinregionsofhighdensity,andfasterinregionsofhightension.Ifthe\ndensityofastringvaries,forinstance,byhavingtheendsthickerinordertosupportthe\nweightofthemiddle,then cwillnolongerbeaconstant,andourwaveequationwillneed\nto be extended. In addition, if the density increases, then so will the tension, because it\ntakesgreatertensiontoaccelerateagreatermass.Ifgravityacts,thenwewillalsoexpect\nthetensionattheendsofthestringtobehigherthaninthemiddle,becausetheendsmust\nsupporttheentireweightofthestring.\nToderivetheequationforwavemotionwithvariabledensityandtension,weagaincon-\nsidertheelementofastringshowninFigure23.1right.Ifwedonotassumethetension T\nisconstant,thenNewton‚Äôssecondlawgives:\nF=ma, (23.29)\nùúï\nùúïx[\nT(x)ùúïy(x,t)\nùúïx]\nŒîx=ùúå(x)Œîxùúï2u(x,t)\nùúït2, (23.30)\n23.4 Beyond The Simple Wave Equation 471\nùúïT(x)\nùúïxùúïy(x,t)\nùúïx+T(x)ùúï2y(x,t)\nùúïx2=ùúå(x)ùúï2y(x,t)\nùúït2. (23.31)\nIfùúå(x)andT(x)areknownfunctions,thentheseequationscanbesolvedwithaslightmod-\nificationofouralgorithm.\nInSection23.4.3,wewillsolveforthetensioninastationaryhangingstringwhengravity\nispresent.Thosereadersinterestedinan alternate easier problem ,thatstillshowsthe\nnewphysics,mayassumethatdensityandtensionareproportional:\nùúå(x)=ùúå0eùõºx,T(x)=T0eùõºx. (23.32)\nWhilewewouldexpectthetensiontobegreaterinregionsofhigherdensity(moremass\ntomoveandsupport),beingproportionalisclearlyjustanapproximation.Substitutionof\n(23.32)into(23.31)yieldsthewaveequation:\nùúï2y(x,t)\nùúïx2+ùõºùúïy(x,t)\nùúïx=1\nc2ùúï2y(x,t)\nùúït2,c2=T0\nùúå0. (23.33)\nHere,cisaconstantthatwouldbethewavevelocityif ùõº=0.Thisequationissimilarto\nthewaveequationwithfriction,onlynowthefirstderivativeiswithrespectto xandnott.\nThe corresponding difference equation follows from using central-difference approxima-\ntionsforthederivatives:\nyi,j+1=2yi,j‚àíyi,j‚àí1+ùõºc2(Œît)2\n2Œîx[yi+1,j‚àíyi,j]+c2\nc‚Ä≤2[yi+1,j+yi‚àí1,j‚àí2yi,j],\nyi,2=yi,1+c2\nc‚Ä≤2[yi+1,1+yi‚àí1,1‚àí2yi,1]+ùõºc2(Œît)2\n2Œîx[yi+1,1‚àíyi,1]. (23.34)\n23.4.3 Waves on Catenary\nUpuntilthispoint,wehavebeenignoringtheeffectofgravityuponthestring‚Äôsshapeand\ntension.Thisisagoodapproximationifthereislittlesaginthestring,asmighthappenifthe\ntensionisveryhighandthestringislight.Evenifthereissomesag,oursolutionfor y(x,t)\ncouldstillbeusedasthedisturbanceabouttheequilibriumshape.However,ifthestring\nismassive,say,likeachainorheavycable,thenthesaginthemiddlecouldbequitelarge\n(Figure23.4),andtheresultingvariationsinshapeandtensionneedtobeincorporatedinto\nxu\nDT\nT0 ds\nFigure 23.4 Left: A uniform string suspended from its ends in a gravitational Ô¨Åeld assumes\na catenary shape. Right: A force diagram of a section of the catenary at its lowest point.\nBecause the ends of the string must support the weight of the string, the tension now varies\nalong the string.\n472 23 String and Membrane Waves\nthewaveequation.Becausethetensionisnolongeruniform,wavestravelfasternearthe\nendsofthestring,whichareundergreatertension.\nThe derivation of the catenary shape is straight forward. Consider a string of uniform\ndensityùúåacteduponbygravity.Toavoidconfusionwithouruseof y(x)todescribeadistur-\nbanceonastring,wecall u(x)theequilibriumshapeofthestring(Figure23.4).Thestatics\nproblemweneedtosolveistodeterminetheshape u(x)andthetension T(x).Theinsetin\nFigure23.4isafree-bodydiagramofthemidpointofthestringandshowsthattheweight\nWofthissectionofarclength sisbalancedbytheverticalcomponentofthetension T.The\nhorizontaltension T0isbalancedbythehorizontalcomponentof T:\nT(x)sinùúÉ=W=ùúågs,T(x)cosùúÉ=T0, (23.35)\n‚áítanùúÉ=ùúågs‚àïT0. (23.36)\nThe trick is to convert(23.36)to a differential equation that we can solve. We do that by\nreplacingtheslopetan ùúÉbythederivative du‚àïdx,andbytakingthederivativewithrespect\ntox:\ndu\ndx=ùúåg\nT0s,‚áíd2u\ndx2=ùúåg\nT0ds\ndx. (23.37)\nYet,because ds=‚àö\ndx2+du2,wehaveourdifferentialequation:\nd2u\ndx2=1\nD‚àö\ndx2+du2\ndx(23.38)\n=1\nD‚àö\n1+(du‚àïdx)2,D=T0‚àïùúåg. (23.39)\nEquation(23.39)istheequationforthe catenary,andhasthesolution[Becker,1954]\nu(x)=Dcoshx\nD. (23.40)\nH e r e ,w eh a v ec h o s e nt h e xaxis to lie at distance Dbelow the bottom of the catenary\n(Figure23.4),sothat x=0isatthecenterofthestringwhere y=DandT=T0.Equation\n(23.37)tellsusthearclength s=Ddu‚àïdx,andsowecansolvefor s(x)andforthetension\nT(x)via(23.35):\ns(x)=Dsinhx\nD,‚áíT(x)=T0ds\ndx=ùúågu(x)=T0coshx\nD. (23.41)\nItisthisvariationintensionthatleadstoan xdependenceofthewavevelocity.\n23.4.4 Catenary Assessment\nInListing23.1,wegivetheprogram EqStringMat.py thatsolvesthewaveequation.Modifyit\ntosolveforwavesonacatenaryincludingfriction,orfortheassumeddensityandtension\ngiven by (23.32) with ùõº=0.5,T0=40 N, and ùúå0=0.01 kg/m. Our code CatFriction.py ,\ngiveninListing23.3,yieldsthesolutionshowninFigure23.5.\n1) Lookforsomeinterestingcasesandcreatesurfaceplotsoftheresults.\n2) Describeinwordshowthewavesdampen,andhowawave‚Äôsvelocityappearstochange.\n23.4 Beyond The Simple Wave Equation 473\nFigure 23.5 The waves of a plucked catenary with friction at six\ndifferent times.\nu(x,t)t = 1\n2\n3\n4\n5\n6\nx\n3)Normal modes: Search for normal mode solutions of the variable tension wave\nequation,thatis,solutionsthatvaryas:\nu(x,t)=Acos(ùúît)sin(ùõæx). (23.42)\nTry usingthisformtostartyourprogramandsee ifyouendupwithstandingwaves.\nUselargevaluesfor ùúî.\n4) When conducting physics demonstrations, we have set up standing wave patterns by\ncontinuouslyshakingoneendofastringupanddown.Trydoingthesamewithyour\nprogram;thatis,buildintoyourcodetheconditionthatforalltimes:\ny(x=0,t)=Asinùúît. (23.43)\nYoumayneedtovary Aandùúîuntilanormalmode(standingwave)isobtained.\n5) (Fortheexponentialdensitycase.)Ifyouwereabletofindstandingwaves,thenverify\nthatthisstringactslikeahigh-frequencyfilter,thatis,thatthereisafrequencybelow\nwhichnowavesoccur.\n6) Forthecatenaryproblem,plotyourresultsshowing boththedisturbance u(x,t)about\nthecatenary,andtheactualheight y(x,t)abovethehorizontalforapluckedstringinitial\ncondition.\n7) Trythefirsttwonormalmodesforauniformstringastheinitialconditionsforthecate-\nnary.Theseshouldbecloseto,butnotexactly,normalmodes.\n8) Wederivedthenormalmodesforauniformstringafterassumingthat k(x)=ùúî‚àïc(x)isa\nconstant.Foracatenarywithouttoomuch xvariationinthetension,weshouldbeable\ntomaketheapproximation:\nc(x)2‚âÉT(x)\nùúå=T0cosh(x‚àïd)\nùúå. (23.44)\nSeeifyougetabetterrepresentationofthefirsttwonormalmodesifyouincludesome\nxdependencein k.\n23.4.5 Including Nonlinear Terms\nShowthatanextensionofthewaveequation,includingthenextorderindisplacements,is\n[Taghipour etal.,2014;Keller,1959]:\nc2ùúï2y(x,t)\nùúïx2=[\n1+ùúï2y(x,t)\nùúïx2]2ùúï2y(x,t)\nùúït2. (23.45)\n474 23 String and Membrane Waves\n1) Extend the leapfrog algorithm to solve this nonlinear equation, making whatever\nassumptionsaboutinitialconditionsareneeded.\n2) Repeatsomeofthesolutionstothewaveequationstudiedpreviously,onlynowforlarge\nvaluesofy‚àïL,inwhichcasenonlineareffectsareimportant.\n3) Examinewhatwerenormalmodesforthelinearproblem,butnowseeiftheypersists\nforlargeamplitudeoscillations.\n23.5 Vibrating Membrane (2D Waves)\nAnelasticmembraneisstretchedsecurelyacrossthetopofasquareboxwithsidesoflength\nùúã,andwiththemembraneintheasymmetricalshape[Kreyszig,1998]:\nu(x,y,t=0)=sin2xsiny,0‚â§x‚â§ùúã,0‚â§y‚â§ùúã, (23.46)\nwhereuistheverticaldisplacementfromequilibrium.\nProblem Describethemotionofthemembranewhenitisreleasedfromrest.\nThe description of wave motion on a membrane is much the same as that of 1D waves\nonthestringdiscussedinSection23.1,onlynowwithwavepropagationintwodirections.\nConsiderFigure23.6showingasquaresectionofthemembraneundertension T.Themem-\nbranemovesonlyverticallyinthe zdirection,yetbecausetherestoringforcearisingfrom\nthe tension in the membrane varies in both the xandydirections, there is wave motion\nalongtheentiresurfaceofthemembrane.\nAlthoughthetensionisconstantoverthesmallareainFigure23.6,therewillbeanet\nverticalforceonthedisplayedsegmentiftheangleofinclineofthemembranevariesaswe\nmovethroughspace.Accordingly,thenetforceonthemembraneinthe zdirection,asa\nresultofthechangein y,is:\n‚àë\nFz(x)=TŒîxsinùúÉ‚àíTŒîxsinùúô, (23.47)\nwhereùúÉistheangleofinclineat y+Œîy,andùúôtheangleat y.Ifweassumethatthedis-\nplacementsandtheanglesaresmall,thenwecanmaketheapproximations:\nsinùúÉ‚âàtanùúÉ=ùúïu\nùúïy|||y+Œîy,sinùúô‚âàtanùúô=ùúïu\nùúïy|||y, (23.48)\n‚áí‚àë\nFz(xfixed)=TŒîx(\nùúïu\nùúïy|y+Œîy‚àíùúïu\nùúïy|y)\n‚âàTŒîxùúï2u\nùúïy2Œîy. (23.49)\nT‚àÜxT‚àÜy\nT‚àÜyT‚àÜx\nx\nxx + ‚àÜxyy y + ‚àÜyŒ∏\nœïzFigure 23.6 A small section of an oscillating\nmembrane and the forces that act on it.\n23.6 Analytical Solution 475\nSimilarly,thenetforceinthe zdirection,asaresultofthevariationin y,is:\n‚àë\nFz(yfixed)=TŒîy(\nùúïu\nùúïx|x+Œîx‚àíùúïu\nùúïx|x)\n‚âàTŒîyùúï2u\nùúïx2Œîx. (23.50)\nThemembranesectionhasmass ùúåŒîxŒîy,whereùúåisthemembrane‚Äôsmassperunitarea.We\nnowapplyNewton‚Äôssecondlawtodeterminetheaccelerationofthemembranesectionin\nzdirection,asaresultofthesumofthenetforcesarisingfromboththe xandyvariations:\nùúåŒîxŒîyùúï2u\nùúït2=TŒîxùúï2u\nùúïy2Œîy+TŒîyùúï2u\nùúïx2Œîx, (23.51)\n‚áí1\nc2ùúï2u\nùúït2=ùúï2u\nùúïx2+ùúï2u\nùúïy2,c=‚àö\nT‚àïùúå. (23.52)\nAsonemighthaveguessed,thisisthe2Dversionofthewaveequation(23.4)thatwestudied\npreviously in one dimension. Here, c, the propagation velocity, is still the square root of\ntensionoverdensity,onlynowitistensionperunitlengthandmassperunitarea.\n23.6 Analytical Solution\nTheanalyticornumericalsolutionofthepartialdifferentialequation(23.52)requiresus\ntoknowboththeboundaryconditionsandtheinitialconditions.Theboundaryconditions\nholdforalltimes,andweregivenwhenweweretoldthatthemembraneisattachedsecurely\ntoasquareboxofside ùúã:\nu(x=0,y,t)=u(x=ùúã,y,t)=0, (23.53)\nu(x,y=0,t)=u(x,y=ùúã,t)=0. (23.54)\nAsrequiredforasecond-orderequation,theinitialconditionshavetwoparts,theshapeof\nthemembraneattime t=0,andthevelocityofeachpointofthemembrane.Thefirstis\ninitialconfiguration:\nu(x,y,t=0)=sin2xsiny,0‚â§x‚â§ùúã,0‚â§y‚â§ùúã. (23.55)\nThesecondisthemembranebeingreleasedfromrest:\nùúïu\nùúït||||t=0=0, (23.56)\nwherewewritepartialderivativebecausetherearealsospatialvariations.\nTheanalyticsolutionisbasedontheguessthatbecausethewaveequation(23.52)hassep-\naratederivativeswithrespecttoeachcoordinateandtime,thefullsolution u(x,y,t)might\nbetheproductofseparatefunctionsof x,y,andt:\nu(x,y,t)=X(x)Y(y)T(t). (23.57)\nAftersubstitutinginto(23.52)anddividingby X(x)Y(y)T(t),weobtain:\n1\nc21\nT(t)d2T(t)\ndt2=1\nX(x)d2X(x)\ndx2+1\nY(y)d2Y(y)\ndy2. (23.58)\n476 23 String and Membrane Waves\nTheonlywaythattheLHSof(23.58)canbetrueforalltimes,whiletheRHSisalsotruefor\nallcoordinates,isifbothsidesareconstant:\n1\nc21\nT(t)d2T(t)\ndt2=‚àíùúâ2=1\nX(x)d2X(x)\ndx2+1\nY(y)d2Y(y)\ndy2, (23.59)\n‚áí1\nX(x)d2X(x)\ndx2=‚àík2,1\nY(y)d2Y(y)\ndy2=‚àíq2, (23.60)\nwhereq2=ùúâ2‚àík2.In(23.60),wehaveincludedthefurtherrealizationthatbecauseeach\nterm on the RHS of (23.59) depends on either xory, the only way for their sum to be\nconstant, is if each term separately is a constant, in this case ‚àík2. The solutions of these\nequationsaresinusoidalstandingwavesinthe xandydirections:\nX(x)=Asinkx+Bcoskx, (23.61)\nY(y)=Csinqy+Dcosqy, (23.62)\nT(t)=Esincùúât+Fcoscùúât. (23.63)\nWenowapplytheboundaryconditions:\nu(x=0,y,t)=u(x=ùúã,y,z)=0‚áíB=0,k=1,2,‚Ä¶,\nu(x,y=0,t)=u(x,y=ùúã,t)=0‚áíD=0,q=1,2,‚Ä¶,\n‚áíX(x)=Asinkx,Y(y)=Csinqy. (23.64)\nThefixedvaluesfortheeigenvalues mandn,describingthemodesforthe xandystanding\nwaves,areequivalenttofixedvaluesfortheconstants q2andk2.Yetbecause q2+k2=ùúâ2,\nwemustalsohaveafixedvaluefor ùúâ2:\nùúâ2=q2+k2‚áí ùúâkq=ùúã‚àö\nk2+q2. (23.65)\nThefullspace-timesolutionnowtakestheform:\nukq=[Gkqcoscùúât+Hkqsincùúât]sinkxsinqy, (23.66)\nwherekandqareintegers.Sincethewaveequationislinearin u,itsmostgeneralsolution\nisalinearcombinationoftheeigenmodes(23.66):\nu(x,y,t)=‚àû‚àë\nk=1‚àû‚àë\nq=1[Gkqcoscùúât+Hkqsincùúât]sinkxsinqy. (23.67)\nWhileaninfiniteseriesisnotagoodalgorithm,theinitialandboundaryconditionsmean\nthatonlythe k=2,q=1termcontributes,andwehaveaclosedformsolution:\nu(x,y,t)=cosc‚àö\n5s i n2xsiny, (23.68)\nwherecisthewavevelocity.\n23.7 Numerical Solution\nThedevelopmentofanalgorithmforthesolutionofthe2Dwaveequation(23.52)follows\nthat of the 1D equation in Section 23.2. We start by expressing the second derivatives in\n23.7 Numerical Solution 477\ntermsofcentraldifferences:\nùúï2u(x,y,t)\nùúït2=u(x,y,t+Œît)+u(x,y,t‚àíŒît)‚àí2u(x,y,t)\n(Œît)2, (23.69)\nùúï2u(x,y,t)\nùúïx2=u(x+Œîx,y,t)+u(x‚àíŒîx,y,t)‚àí2u(x,y,t)\n(Œîx)2, (23.70)\nùúï2u(x,y,t)\nùúïy2=u(x,y+Œîy,t)+u(x,y‚àíŒîy,t)‚àí2u(x,y,t)\n(Œîy)2. (23.71)\nAfter discretizing the variables, u(x=iŒî,y=iŒîy,t=kŒît)‚â°uk\ni,j, we obtain our time-\nstepping algorithm by solving for the future solution in terms of the present and past\nones:\nuk+1\ni,j=2uk\ni,j‚àíuk‚àí1\ni,jc2\nc‚Ä≤2[uk\ni+1,j+uk\ni‚àí1,j‚àí4uk\ni,j+uk\ni,j+1+uk\ni,j‚àí1], (23.72)\nwhere, as before, c‚Ä≤def=Œîx‚àïŒît. Whereas the present and past solutions, ukanduk‚àí1,a r e\nknownafterthefirststep,toinitiatethealgorithmweneedtoknowthesolutionat t=‚àí Œît,\nthatis,beforetheinitialtime.Tofindthat,weusethefactthatthemembraneisreleased\nfromrest,andso:\n0=ùúïu(t=0)\nùúït‚âàu1\ni,j‚àíu‚àí1\ni,j\n2Œît,‚áíu‚àí1\ni,j=u1\ni,j. (23.73)\nAftersubstitutioninto(23.72),weobtainthealgorithmforthefirststep:\nu1\ni,j=u0\ni,j+c2\n2c‚Ä≤2[u0\ni+1,j+u0\ni‚àí1,j‚àí4u0\ni,j+u0\ni,j+1+u0\ni,j‚àí1]. (23.74)\nBecausethe t=0displacement u0\ni,jisknown,wecomputethesolutionforthefirsttimestep\nwith(23.74),andforsubsequentstepswith(23.72).\nThe program Wave2D.pyin Listing 23.2 solves the 2D wave equation using the leapfrog\nalgorithm.Theprogram Waves2Danal.py computestheanalyticsolution.Theshapeofthe\nmembraneatthreedifferenttimesisshowninFigure23.7.\n02040‚Äì101\nt = 45\n‚Äì101\n02040\ny‚Äì101 t = 3 t = 20\n02040\n2040 20402040\nx\nFigure 23.7 The standing wave pattern on a square box top at three different times.",17134
220-23.8 Code Listings.pdf,220-23.8 Code Listings,"478 23 String and Membrane Waves\n23.8 Code Listings\nListing 23.1 EqStringMat.py Solvesthewaveequationforagentlypluckedstring.\n# EqStringMat.py: Animated leapfrog sol Vibrating string + MatPlotLib\n2\nfromnumpyimport ‚àó\nimportnumpy as np, matplotlib.pyplot as plt, matplotlib.animation as animation\n6rho = 0.01; ten = 40.; c = sqrt(ten/rho) # density , tension\nc1 = c; ratio = c ‚àóc/(c1 ‚àóc1) # CFL criterium = 1\nxi = np.zeros((101,3), float) # Declaration\nk=range(0,101)\n10\ndefInitialize(): # Initial conditions\nforiin range (0, 81): xi[i, 0] = 0.00125 ‚àói\nforiin range (81, 101): xi[i, 0] = 0.1 ‚àí0.005 ‚àó(i‚àí80) #2 n dp a r t\n14defanimate(num):\nforiin range (1, 100):\nxi[i,2] = 2. ‚àóxi[i,1] ‚àíxi[i,0]+ratio ‚àó(xi[i+1,1]+xi[i ‚àí1,1]‚àí2‚àóxi[i,1])\nline.set_data(k,xi[k,2]) #D a t at o p l o t , x , y\n18formin range (0,101):\nxi[m, 0] = xi[m, 1] # Recycle array\nxi[m, 1] = xi[m, 2]\nreturnline\n22Initialize() # Plot initial string\nfig = plt.figure()\nax = fig.add_subplot(111, autoscale_on=False, xlim=(0, 101), ylim=( ‚àí0.15, 0.15))\nax.grid() # Plot grid\n26plt.title( ""Vibrating String"" )\nline , = ax.plot(k, xi[k,0], lw=2)\nforiin range (1,100):\nxi[i,1] = xi[i,0] + 0.5 ‚àóratio ‚àó(xi[i+1,0] + xi[i ‚àí1,0]‚àí2‚àóxi[i,0])\n30ani = animation.FuncAnimation(fig , animate,1) # D u m m y argument : 1\nplt .show()\nprint(""finished"" )\nListing 23.2 Waves2D.py Solvesthewaveequationforavibratingmembrane.\nimportmatplotlib.pylab as p; fromnumpyimport ‚àó\n2frommpl_toolkits.mplot3d importAxes3D\ntim = 15; N = 71\nc = sqrt(180./390) # Speed = sqrt (ten []/den[kg/m2; ])\n6u = zeros((N,N,N), float); v= zeros((N,N), float)\nincrx = pi/N; incry = pi/N\ncprime = c;\ncovercp = c/cprime; ratio = 0.5 ‚àócovercp ‚àócovercp # c/c ‚Äô 0.5 for stable\n10\ndefvibration(tim):\ny=0 . 0\nforjin range (0,N): # Initial position\n14 x=0 . 0\nforiin range (0,N):\nu[i][j][0] = 3 ‚àósin(2.0 ‚àóx)‚àósin(y) # Initial shape\nx+ =i n c r x\n18 y+ =i n c r y\nforjin range (1,N‚àí1): # First time step\nforiin range (1,N‚àí1):\nu[i][j][1] = u[i][j][0] + 0.5 ‚àóratio ‚àó(u[i+1][j][0]+u[i ‚àí1][j][0]\n22 + u[i][j+1][0]+u[i][j ‚àí1][0]‚àí4.‚àóu[i][j][0])\nforkin range (1,tim): # Later time steps\nforjin range (1,N‚àí1):\nforiin range (1,N‚àí1):\n26 u[i][j][2] = 2. ‚àóu[i][j][1] ‚àíu[i][j][0] + ratio ‚àó(u[i+1][j][1]\n+u [ i‚àí1][j][1] +u[i][j+1][1]+u[i][j ‚àí1][1]‚àí4.‚àóu[i][j][1])\nu[:][:][0] = u[:][:][1] # Reset past\n23.8 Code Listings 479\nu[:][:][1] = u[:][:][2] # Reset present\n30 forjin range (0,N):\nforiin range (0,N):\nv[i][j] =u[i][j][2] # Convert to 2D for matplotlib\nreturnv\n34v = vibration(tim)\nx1 =range(0, N)\ny1 =range(0, N)\nX, Y = p.meshgrid(x1,y1)\n38\ndeffunctz(v): z = v[X,Y]; returnz\nZ=f u n c t z( v )\n42fig = p.figure()\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô)\nax.set_xlabel( ‚Äôx‚Äô)\n46ax.set_ylabel( ‚Äôy‚Äô)\nax.set_zlabel( ‚Äôu(x,y)‚Äô )\np.show()\nListing 23.3 CatFriction.py Solvesforwavesonacatenarywithfriction.\n# CatFriction .py: Solve for wave on catenary with friction\n3fromnumpyimport ‚àó\ndt = 0.0001; dx = 0.01; T = 1; rho = 0.1; maxtime = 100; kappa = 30\nD=T / (r h o ‚àó9.8)\n7x = zeros((512,3), float)\nq=open(‚ÄôCatFriction.dat‚Äô ,‚Äôw‚Äô); rr = open(‚ÄôCatFunct.dat‚Äô ,‚Äôw+t‚Äô)\nforiin range (0,101): x[i][0] = ‚àí0.08‚àósin(pi ‚àói‚àódx) #I C\n11foriin range (1,100): # First step\nx[i][1] = ( dt ‚àó(T/rho) ‚àó(( x[i+1][0] ‚àíx[i][0] )/dx ‚àó(e x p ( ( i ‚àí50)‚àódx/D)\n‚àíexp(‚àí(i‚àí50)‚àódx/D))/D +(exp((i ‚àí50)‚àódx/D)+exp( ‚àí(i‚àí50)‚àódx/D)) ‚àó\n( x[i+1][0]+x[i ‚àí1][0]‚àí2.0‚àóx[i][0] )/( pow(dx,2)) )\n15 ‚àí2‚àókappa ‚àóx[i][0]+2 ‚àóx[i][0]/dt )/(2 ‚àókappa+(2/dt))\nforkin range (0,300): # Other steps\nforiin range (1,100):\nx[i][2] = (dt ‚àó(T/rho) ‚àó((x[i+1][1] ‚àíx[i][1])/dx ‚àó(exp((i ‚àí50)‚àódx/D) \\n19 ‚àíexp(‚àí(i‚àí50)‚àódx/D))/D +(exp((i ‚àí50)‚àódx/D)+exp( ‚àí(i‚àí50)‚àódx/D)) ‚àó\\n(x[i+1][1]+x[i ‚àí1][1]‚àí2.0‚àóx[i][1])/( pow(dx,2)))\\n‚àí2‚àókappa ‚àóx[i][1] ‚àí(‚àí2‚àóx[i][1]+x[i][0])/dt)/(2 ‚àókappa+(1/dt))\nforiin range (1,101):\n23 x[i][0] = x[i][1]\nx[i][1] = x[i][2]\nif(k%4==0 ork==0):\nforiin range (0,100):\n27 a1=exp((i ‚àí50.)‚àódx/D)\na2=exp( ‚àí(i‚àí50.)‚àódx/D)\nrr.write( ""%7.3f""%(D‚àó(a1+a2)))\nrr.write( ""\n"")\n31 q.write( ‚Äô%7.3f‚Äô%(x[i ,2]))\nq.write( ""\n"")\nq.write( ""\n"");\nrr.write( ""\n"");\n35rr.closed\nq.closed\nprint(""Data stored in CatFrict.dat and CatFunct.dat"" )",4188
221-Chapter 24 Quantum Wave Packets and EM Waves.pdf,221-Chapter 24 Quantum Wave Packets and EM Waves,,0
222-24.1 TimeDependent Schrodinger Equation.pdf,222-24.1 TimeDependent Schrodinger Equation,"480\n24\nQuantum Wave Packets and EM Waves\nThis chapter extends the solution of wave equations that began in Chapter 23,t ow a v e s\npossessing multiple components. This requires algorithms with a bit more sophistication.\nFirst, we explore quantum wave packets, which have their real and imaginary parts solved\nfor at slightly differing (split) times. Then, we explore electromagnetic waves, which have\ntheir interlaced EandHvector components also solved for at split times .\nProblem Anelectronwithadefinitemomentumisplacedwithinanattractivepotential\nthatconfinesittoa1Dregionthesizeofanatom.Determinetheresultingelectron‚Äôsposition\nintimeandspace.\n24.1 Time-Dependent Schr√∂dinger Equation\nBecausetheregionofconfinementinthisproblemisthesizeofanatom,weneedtosolve\nitquantummechanically.Becausetheparticlestartedwithbothadefinitemomentumand\nposition,weneedtosolveforboththespatialandtimedependenciesoftheelectron‚Äôswave\nfunction.Accordingly,wemustsolvethetime- dependentSchr√∂dingerequation.Thisisdif-\nferentfromthetime- independent eigenvalueproblemofaparticleboundinabox,which\nweconsideredinChapters6and13.\nWemodeltheelectronbeinglocalizedinspacebyaGaussianwithfinitewidthcentered\nat5.Becausewearetoldthattheelectronstartswithamomentum,wemultiplytheGaus-\nsianbyaplanewave(astateofdefinitemomentum ko).Andbecausethisistheinitialstate\natjusttheonetime t=0,wedonotincludeanytimedependence:\nùúì(x,t=0)=exp[\n‚àí1\n2(\nx‚àí5\nùúé0)2]\neikox, (24.1)\nwherewehaveset ‚Ñè=1.Inspiteofussettingupthewavefunction(24.1),asifitgivesthe\nelectronadefinitemomentumandlocations,(24.1)isaneigenstateofneithermomentum\nnorposition.Sucharewavepackets!\nTo solve the problem,wemustpropagatetheinitialwavefunctionforwardintimeand\nthroughallofspace(asweshowinFigures24.1and24.2).If(24.1)wereaneigenstateof\nthe Hamiltonian, it would have an exp (‚àíiùúît)time dependence that can be factored out\nof the wave function (the usual in textbooks), and we would have a time-independent\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n24.1 Time-Dependent Schr√∂dinger Equation 481\n020\nt\nx\nFigure 24.1 The probability density as a function of time and space for an electron conÔ¨Åned to a\nsquare well. The electron is seen to start off on the left, spread out with time, and collide with the\nwalls.\n010\nxtx\nt10\n801020607080\n10\n0246810\nTime\nPosition020304050607080\n1010\n5\n0005\nFigure 24.2 The probability density as a function of time for an electron conÔ¨Åned to a 1D\nharmonic oscillator potential well. On the left is a conventional surface plot, while on the right is a\ncolor visualization.\nSchr√∂dingerequation.However, ÃÉHùúì‚â†Eùúìforthiswavepacket,andsowemustsolvethe\ntime-dependentSchr√∂dingerequation:\niùúïùúì(x,t)\nùúït=ÃÉHùúì(x,t) (24.2)\niùúïùúì(x,t)\nùúït=‚àí1\n2mùúï2ùúì(x,t)\nùúïx2+V(x)ùúì(x,t), (24.3)\nw h e r ew eh a v es e t2 m=1a n d‚Ñè=1. Because the initial wave function is complex (the\nplanewave),thewavefunctionwillbecomplexforalltimes.Accordingly,wehaveseparate\nequationsfortherealandimaginarypartsofthewavefunction:\nùúì(x,t)=R(x,t)+iI(x,t), (24.4)\n‚áíùúïR(x,t)\nùúït=‚àíùúï2I(x,t)\nùúïx2+V(x)I(x,t), (24.5)\nùúïI(x,t)\nùúït=+ùúï2R(x,t)\nùúïx2‚àíV(x)R(x,t), (24.6)\nwhereV(x)istheconfiningpotential.",3267
223-24.2 SplitTime Algorithm.pdf,223-24.2 SplitTime Algorithm,,0
224-24.2.2 Wave Packets in Other Wells.pdf,224-24.2.2 Wave Packets in Other Wells,"482 24 Quantum Wave Packets and EM Waves\n24.2 Split-Time Algorithm\nThetime-dependentSchr√∂dingerequationcanbesolvedwithbothimplicit(large-matrix)\nandexplicit(leapfrog)methods.AnextrachallengewhensolvingtheSchr√∂dingerequation\nistheneedtoconserveprobability, ‚à´+‚àû\n‚àí‚àûdxùúå(x,t),toahighlevelofprecisionatalltimes.\nHere,we‚Äôllusean explicitmethodthat,bydesign,providesahighlevelofprobabilitycon-\nservation. It does so by solving for the real and imaginary parts of the wave function at\nslightlydifferent,or‚Äústaggered‚Äù,times[AskarandCakmak,1977;Visscher,1991;Maestri\netal.,2000].Explicitly,therealpart Risdeterminedattimes0, Œît,‚Ä¶,andtheimaginary\npartIattimes1\n2Œît,3\n2Œît,‚Ä¶Thealgorithmisbasedon(whatelse?)theTaylorexpansionsof\nRandI,forexample,\nR(\nx,t+1\n2Œît)\n=R(\nx,t‚àí1\n2Œît)\n+[4ùõº+V(x)Œît]I(x,t)\n‚àí2ùõº[I(x+Œîx,t)+I(x‚àíŒîx,t)], (24.7)\nwhereùõº=Œît‚àï[2(Œîx)2].Indiscreteformwehave:\nRn+1\ni=Rn\ni‚àí2(ùõº[In\ni+1+In\ni‚àí1]‚àí2[ùõº+ViŒît]In\ni), (24.8)\nIn+1\ni=In\ni+2(ùõº[Rn\ni+1+Rn\ni‚àí1]‚àí2[ùõº+ViŒît]Rn\ni), (24.9)\nwherethesuperscript nindicatesthetime,andthesubscript itheposition,e.g., Rt=nŒît\nx=iŒîx.\nTheprobabilitydensity ùúåisdefinedintermsofthewavefunctionevaluatedatthreedif-\nferenttimes:\nùúå(t)=‚éß\n‚é™\n‚é®\n‚é™‚é©R2(t)+I(\nt+Œît\n2)\nI(\nt‚àíŒît\n2)\n,forinteger t,\nI2(t)+R(\nt+Œît\n2)\nR(\nt‚àíŒît\n2)\n,forhalf-integer t.(24.10)\nAlthoughprobabilityisnottobeexactlyconservedbythealgorithm,theerroristwoorders\nhigherthanthatinthewavefunction,andthisisusuallyquitesatisfactory.Ifitisnot,then\nonemustadjustthestepsizes.Whilethisdefinitionof ùúåmaynotseemintuitive,itreduces\ntotheusualonefor Œît‚Üí0,andsocanbeviewedaspartoftheartofnumericalanalysis.\nWewillaskyoutoinvestigatejusthowwellprobabilityisconserved.Wereferthereaderto\nKoonin[1986]andVisscher[1991]fordetailsonthestabilityofthealgorithm.\n24.2.1 Implementation\nInListing24.1,youwillfindtheprogram HarmosAnimate.py thatsolvesforthemotionofthe\nwavepacket(24.1)insideaharmonicoscillatorpotential.Theprogram Slit.pysolvesfor\nthemotionofaGaussianwavepacketasitpassesthroughaslit(Figure24.4).Youshould\nsolveforawavepacketconfinedtoaninfinitesquarewell:\nV(x)={‚àû,x<0,orx>15,\n0,0‚â§x‚â§15.\n1) Define arrays psr[751,2] and psi[751,2] for the real and imaginary parts of ùúì,a n d\nRho[751]fortheprobability.Thefirstsubscriptreferstothe xpositiononthegrid,and\nthesecondtothepresentandfuturetimes.\n24.2 Split-Time Algorithm 483\ny\nx300 500 100\nFigure 24.3 The probability density as a function of xandyat three different times for an\nelectron conÔ¨Åned to a 2D parabolic tube (inÔ¨Ånitely long in ydirection). The electron is initially a\nGaussian wave packet in both the xandydirections.\nFigure 24.4 The probability density as a function of position and time for an electron incident\nupon and passing through a slit. A signiÔ¨Åcant reÔ¨Çection is seen to be occurring.\n2) Usethevalues ùúéo=0.5,Œîx=0.02,ko=17ùúã,andŒît=1\n2Œîx2.\n3) Useequation(24.1)fortheinitialwavepackettodefine psr[ j,1]foralljatt=0,andto\ndefine psi[ j,1]att=1\n2Œît.\n4) Seeing that the wave function must vanish at the infinitely high well walls, set\nRho[1] = Rho[751] = 0.0 .\n5) Incrementtimeby1\n2Œît.Use(24.8)tocompute psr[ j,2]intermsof psr[ j,1],and(24.9)\ntocompute psi[ j,2]intermsof psi[ j,1].\n6) Repeatthestepsthroughallofspace,thatis,for i=2‚Äì750.\n7) Throughoutallofspace,replacethepresentwavepacket(secondindex1)bythefuture\nwavepacket(secondindex2).\n8) After you are sure that the program is running properly, repeat the time-stepping for\n‚àº5000steps.\n24.2.1.1 Animation\n1) Outputtheprobabilitydensityafterevery200steps.\n2) Make a surface plot of probability versuspositionversustime. This should look like\nFigures24.1or24.2.\n3) Makeananimationshowingthewavefunctionasafunctionoftime.\n4) Checkhowwellprobabilityisconservedforearlyandlatetimesbydeterminingtheinte-\ngraloftheprobabilityoverallofspace, ‚à´+‚àû\n‚àí‚àûdxùúå(x),andseeingbyhowmuchitchanges\nintime(itsspecificvaluedoesn‚Äôtmatterasit‚Äôsjustnormalization).\n5) Explainwhycollisionswiththewallscausethewavepackettobroadenandbreakup.\n(Hint:ThecollisionsdonotappearsodisruptivewhenaGaussianwavepacketiscon-\nfinedwithinaharmonicoscillatorpotential.)",4105
225-24.5 EM Waves Finite Difference Time Domain.pdf,225-24.5 EM Waves Finite Difference Time Domain,"484 24 Quantum Wave Packets and EM Waves\n24.2.2 Wave Packets in Other Wells\n1D Well:Nowconfinetheelectrontoaharmonicoscillatorwell:\nV(x)=1\n2x2(‚àí‚àû‚â§x‚â§‚àû). (24.11)\nTake the momentum as k0=3ùúã, the space step Œîx=0.02, and the time step\nŒît=1\n4Œîx2.Notethatthewavepacketbroadensintime,butthenreturnstoits\ninitialshape!\n2D Well:Nowconfinetheelectrontoa2Dparabolictube(Figure24.3):\nV(x,y)=0.9x2,‚àí9.0‚â§x‚â§9.0,0‚â§y‚â§18.0. (24.12)\nTheextradegreeoffreedommeansthatwemustsolvethe2DPDE:\niùúïùúì(x,y,t)\nùúït=‚àí(ùúï2ùúì\nùúïx2+ùúï2ùúì\nùúïy2)\n+V(x,y)ùúì. (24.13)\nAssume that the electron‚Äôs initial localization is described by the 2D Gaussian\nwavepacket:\nùúì(x,y,t=0)=eik0xxeik0yyexp[\n‚àí(x‚àíx0)2\n2ùúé2\n0]\nexp[\n‚àí(y‚àíy0)2\n2ùúé2\n0]\n. (24.14)\nNotethatyoucansolvethe2Dequationbyextendingthemethodwejustused\nin1D,oryoucanlookatSection24.3wherewedevelopaspecialalgorithmfor\ntheSchr√∂dingerequation.\n1) Determinethemotionofa2DGaussianwavepacketwithina2Dharmonicoscillator\npotential:\nV(x,y)=0.3(x2+y2),‚àí9.0‚â§x‚â§9.0,‚àí9.0‚â§y‚â§9.0. (24.15)\na) Centertheinitialwavepacketat (x,y)=(3.0,‚àí3),andgiveitmomentum (k0x,k0y)=\n(3.0,1.5).\nb) Young‚Äôssingle-slitexperimenthasawavepassingthroughasmallslitwiththetrans-\nmittedwaveshowinginterferenceeffects.Inquantummechanics,wherewerepre-\nsentaparticlebyawavepacket,thismeansthataninterferencepatternshouldbe\nformedwhenaparticlepassesthroughasmallslit.PassaGaussianwavepacketof\nwidth3throughaslitofwidth5(Figure24.4),andobservetheresultingquantum\ninterference.\n24.3 Special Schr√∂dinger Algorithm\nWe have just developed an algorithm to solve the time-dependent Schr√∂dinger equation\nin2Dbyextendingthe1Dalgorithmtoanotherdimension.Anotherapproachusesquan-\ntumtheorytoobtainamorepowerfulalgorithm[Maestri etal.,2000].First,wenotethat\nequation (24.13) can be integrated formally to obtain the solution [Landau and Lifshitz,\n1976]:\nùúì(x,y,t)=U(t)ùúì(x,y,t=0) (24.16)\n24.4 Quantum Chaos 485\nU(t)=e‚àíiÃÉHt,ÃÉH=‚àí(\nùúï2\nùúïx2+ùúï2\nùúïy2)\n+V(x,y). (24.17)\nHere,U(t)isanoperatorthattranslatesawavefunctionforwardintime,and ÃÉHistheHamil-\ntonianoperator.Fromthisformalsolution,wededucethatawavepacketcanbetranslated\naheadoftime Œîtvia:\nùúìn+1\ni,j=U(Œît)ùúìn\ni,j, (24.18)\nwhere the superscript denotes time, t=nŒît, and the subscripts denote the two spatial\nvariables,x=iŒîx,y=jŒîy.Likewise,theinverseofthetimeevolutionoperatormovesthe\nsolutionbackonetimestep:\nùúìn‚àí1=U‚àí1(Œît)ùúìn=e+iÃÉHŒîtùúìn. (24.19)\nWhileitwouldbenicetohaveanalgorithmbasedonadirectapplicationof(24.19),the\nreferences show that the resulting algorithm would not be stable [Askar and Cakmak,\n1977].Thatbeingso,webaseouralgorithmonanindirectapplication,namely,therelation\nbetweenthedifferencein ùúìn+1andùúìn‚àí1:\nùúìn+1=ùúìn‚àí1+(e‚àíiÃÉHŒît‚àíe+iÃÉHŒît)ùúìn, (24.20)\nwhere the difference in sign of the exponents is to be noted. The algorithm derives from\ncombining the O(Œîx2)expression for the second derivative obtained from the Taylor\nexpansion,\nùúï2ùúì\nùúïx2‚âÉ‚àí1\n2(ùúìn\ni+1,j+ùúìn\ni‚àí1,j‚àí2ùúìn\ni,j), (24.21)\nwith the corresponding-order expansion of the evolution equation (24.20). Substituting\ntheresultingexpressionforthesecondderivativeintothe2Dtime-dependentSchr√∂dinger\nequationresultsin:1\nùúìn+1\ni,j=ùúìn‚àí1\ni,j‚àí2i[(\n4ùõº+1\n2ŒîtVi,jùúìn\ni,j‚àíùõº(\nùúìn\ni+1,j+ùúìn\ni‚àí1,j+ùúìn\ni,j+1+ùúìn\ni,j‚àí1)]\n,\nwhereùõº=Œît‚àï[2(Œîx)2].Weconvertthisequationwithitscomplexwavefunctiontocoupled\nrealequationsbysubstituting ùúì=R+iI:\nRn+1\ni,j=Rn‚àí1\ni,j+2[(\n4ùõº+1\n2ŒîtVi,j)\nIn\ni,j‚àíùõº(\nIn\ni+1,j+In\ni‚àí1,j+In\ni,j+1+In\ni,j‚àí1)]\n,\nIn+1\ni,j=In‚àí1\ni,j‚àí2[(\n4ùõº+1\n2ŒîtVi,j)\nRn\ni,j+ùõº(\nRn\ni+1,j+Rn\ni‚àí1,j+Rn\ni,j+1+Rn\ni,j‚àí1)]\n.\nThisisthealgorithmforintegratingthe2DSchr√∂dingerequation.Probabilityisdetermined\nusingthesameexpression(24.10)asusedin1D.\n24.4 Quantum Chaos\nFinding signals of chaos in quantum systems can be challenging. To avoid computation,\nmuchoftheresearchhasassumedtheapproximate,semiclassicalformulationofquantum\n1 Theconstantsintheequationchangeasthedimensionoftheequationchanges;thatis,therewillbe\ndifferentconstantsforthe3Dequation,andtherefore,ourconstantsaredifferentfromthoseinthe\nreferences!\n486 24 Quantum Wave Packets and EM Waves\nmechanics.Wewon‚Äôtdothat.Thereareanumberofwaysinwhichchaoticbehaviormay\noccurinquantum systems, someratherobvious,andothersmoresubtle. Inthissection,\nwelookattheobvious;obviousbecausetheclassicalversionsofthesystemsdisplaychaos,\nandsoweknowwhattolookforinthequantumsystems.\n24.4.1 Quantum Billiards\nHere,weexamineaquantumsystemwhoseclassicalanaloghasdefinitechaoticbehavior,\nasseeninFigures24.5and24.6.Intheclassicalchaoticsystem,thebilliardballbounces\naroundendlessly,fillingalloftheallowedspacewithouteverrepeating(Figure24.6left).\nThe program for this is the same as for scattering from three disks, 3QMdisks.py in List-\ning24.2,butwiththedisksremoved.Youmustimpose ùúì=0attheboundariestodescribe\nreflectionfromahardwall.Someofourresultsafter200timestepsareshowninFigure24.6\nright.Weseethatthequantumsystemdisplaysasignatureoftheendlessclassicalreflec-\ntionsfromthetable‚Äôsedges.\nR\nrho(\nx,y\n)\n0\n020\n2040x\ny4060\n6080\n80100\n1000.00.20.4\nFigure 24.5 Left:Classical trajectories for scatterings from one, two, and three stationary disks.\nRight: A wave packet incident on three disks of radius 10 located at the vertices of an equilateral\ntriangle.\nPsi0.2\n0.1\n0.0\n100\n80\n60\n40\n20\n0\n0 20 40\nxy\n60 80 100\nFigure 24.6 Left: The path followed by a classical ball bouncing elastically within a square\nbilliard. Right: A quantum wave packet, initially Gaussian in shape, after 200 time steps conÔ¨Åned\nwithin a square billiard table.\n24.4 Quantum Chaos 487\n1) ComputethemotionofaninitialGaussianwavepacketconfinedtothetopofasquare\nbilliards table. Match the initial conditions to those that lead to periodic orbits of the\nclassicalbilliard.\n(a) Computetheclassicalmotionofasquarebilliardforaseriesoftimes,rangingfrom\nthoseinwhichonlyasmallnumberofreflectionshasoccurred,toverylargetimes.\n(b) Examinethewavefunctionforthesamerangeoftimesasusedintheclassicalcom-\nputation,andcomparetotheclassicalresults.\n(c) Howmanyreflectionsdoesittakeforthewavepackettolosealltracesoftheclassical\ntrajectories?\n2) Examine the motion of a quantum wave packet for the various billiards studied clas-\nsically: by a) a circle, b) a stadium, c) a circle with a disk in the middle. In all cases,\nexamineinitialconditionsthatleadtoclassicallyperiodicorbits.\n24.4.2 Three Disks Scattering\nAnothersysteminwhichwemightbeabletoseethequantumchaoshappeningisquantum\nscatteringfromthreefixedharddisks(Figure24.5left).Here,weexaminethescatteringofa\nwavepacketfromseveralharddiskconfigurations(Figure24.5right),withquantumchaos\npossibleforthethree-diskscattering.Tomakeyourworkeasier,inListing24.2,wegiveour\nprogram 3QMdisks.py for quantum scattering from three fixed hard disks. The disks have\nradiusR,acenter-to-centerseparationsof a,andareplacedattheverticesofanequilateral\ntriangle.AsseeninFigure24.5,aGaussianwavepacket,\nùúì(x,y,t=0)=ei(kxx+kyy)e‚àíA(x‚àíx0)2‚àíA(y‚àíy0)2, (24.22)\nis incident upon the disks. Note: The program, as given, has the disks confined within a\nvery small box. This may be appropriate for a bound state billiard problem, but not for\nscattering. You will needto enlargethe confiningbox inorder to eliminatethe effects of\nreflectionsfromtheboundary.AndwhileyouarefreetomakesurfaceplotsofRe ùúì(x,y)\nandImùúì(x,y),wefindtheinterpretationeasierwithasurfaceplotoftheprobabilitydensity\nz(x,y)=ùúå(x,y).\n1) Startwithscatteringfromonedisk‚Äì\n(a) Producesurfaceplotsof ùúå(x,y)fortimesfrom0untilthepacketleavesthescattering\nregion. Note, the edges of the billiardtable will reflect the wave packetand, ergo,\nshouldbefarfromthedisksinordertonotinterferewiththescatteringfromthe\ndisks.\n(b) Examinethequalitativeeffectofvaryingthesizeofthedisk.\n(c) Examinethequalitativeeffectofvaryingthemomentumofthewavepacket.\n(d) Varytheinitialpositionofthewavepacketsothattherearenearlyhead-oncollisions\naswellasonesatglancingangles.\n2) Next,repeatyourinvestigationforthetwodisksystem.Trytovarytheparameterssothat\nyouobtainmany multiplescatteringsfromthedisks.Inparticular,seeifyoucanfindthe\nanalogoftheclassicalcasewherethereisatrappedorbitwithunendingback-and-forth\nscatterings.( Hint:Trystartingthewavepacketbetweenthetwodisks.)\n3) Next,extendandrepeattheinvestigationtothethree-disksystem.Varytheparameters\nso that you obtain many multiple scatterings from the disks. In particular, see if you",8402
226-24.7 SplitTime FDTD.pdf,226-24.7 SplitTime FDTD,"488 24 Quantum Wave Packets and EM Waves\ncanfindtheanalogoftheclassicalcasewheretherearetrappedorbitswithunending\nback-and-forthscatterings.\n(a) Developanalgorithmthatdeterminesthetimedelayofthewavepacket,thatis,the\ntimeittakesformostoftheinitialpackettoleavethescatteringregion.\n(b) Plot the time delay versus the wave packet momentum and look for indications\nof chaos, such as sharp peaks or rapid changes. The literature indicates that high\ndegreesofmultiplescatteringoccurwhen a‚àïR‚âÉ6.\n24.5 E&M Waves: Finite Difference Time Domain\nSimulationsofelectromagneticwavesareoftremendouspracticalimportance.Aswithother\nwave equations, the basic technique is again calculating the solution on a spacetime lattice\nusingfinitedifferenceandtimestepping.WhenusedforE&Msimulations,thetechniqueis\nknownasthefinitedifferencetimedomain(FDTD)method.Whatisnewhereisthecoupling\nofthe E and H fields,withthevariationsofonevectorgenerating theother. Morecomplete\ntreatmentscanbefoundinSullivan[ 2000]andWardandNelson[ 2004].\nProblem Youaregivenaregioninspace,0 ‚â§z‚â§200,inwhichthe EandHfieldsare\nknowntohaveasinusoidalspatialvariation:\nEx(z,t=0)=0.1sin(\n2ùúãz\n100)\n,Hy(z,t=0)=0.1sin(\n2ùúãz\n100)\n, (24.23)\nwithallothercomponentsvanishing.Determinethefieldsforallsubsequenttimes.\n24.6 Maxwell‚Äôs Equations\nThedescriptionofelectromagnetic(EM)wavesviaMaxwell‚Äôsequationsiscoveredinmany\ntextbooks. For propagation in just the zdimension, and for free space with no sinks or\nsources,therearefourcoupledPDE‚Äôs:\n‚Éó‚àá‚ãÖE=0‚áíùúïEx(z,t)\nùúïx=0 (24.24)\n‚Éó‚àá‚ãÖH=0‚áíùúïHy(z,t)\nùúïy=0, (24.25)\nùúïE\nùúït=+1\nùúñ0‚Éó‚àá√óH‚áíùúïEx\nùúït=‚àí1\nùúñ0ùúïHy(z,t)\nùúïz, (24.26)\nùúïH\nùúït=‚àí1\nùúá0‚Éó‚àá√óE‚áíùúïHy\nùúït=‚àí1\nùúá0ùúïEx(z,t)\nùúïz. (24.27)\nAsindicatedinFigure24.7,wehavechosentheelectricfield E(z,t)tobepolarized(oscillate)\ninthexdirection,andthemagneticfield H(z,t)tobepolarizedinthe ydirection.Thisisa\ntransverseelectromagnetic(TEM)wave.AsindicatedbytheboldarrowinFigure24.7,the\ndirectionofpowerflowisgivenbytheright-handruleappliedto E√óH.Notethatalthough\nwehavesettheinitialconditionssuchthattheEMwaveistravelinginonlythe zdimension,\n24.7 Split-Time FDTD 489\nFigure 24.7 A single electromagnetic\npulse traveling along the zaxis. The\ncoupled EandHpulses are indicated by\nsolid and dashed curves, respectively, and\nthe pulses at different zvalues correspond\nto different times.x\nyzEx\nExHyHy t\ntheelectricfieldoscillatesintheperpendicular xdirection,andthemagneticfieldoscillates\nintheperpendicular ydirection.So,whiletheremaybepropagationinjustonedirection,\nthevectornatureofthefieldsmeansthatthewaveoccursinallthreedimensions.\n24.7 Split-Time FDTD\nWeneedtosolvethetwocoupledPDEs(24.26)and(24.27).AsisusualforPDEs,weapprox-\nimatethederivativesviathecentral-differenceapproximation,hereinbothtimeandspace.\nForexample,\nùúïE(z,t)\nùúït‚âÉE(\nz,t+Œît\n2)\n‚àíE(\nz,t‚àíŒît\n2)\nŒît, (24.28)\nùúïE(z,t)\nùúïz‚âÉE(\nz+Œîz\n2,t)\n‚àíE(\nz‚àíŒîz\n2,t)\nŒîz. (24.29)\nWe next substitute these approximations into Maxwell‚Äôs equations, and rearrange the\nequationsintheformofanalgorithmthatadvancesthesolutionintime.Becauseonlyfirst\nderivatives occur in Maxwell‚Äôs equations, the equations are simple, although the electric\nandmagneticfieldsareintermixed.\nAswehavedonewiththetime-dependentSchrodingerequation,wesetupaspace-time\nlattice(Figure24.8),inwhichtherearehalf-integertimesteps,butnowextendedtohalf-\ninteger space steps as well [Yee, 1966]. The magnetic field will be determined at integer\ntimesitesandhalf-integerspacesites(opencircles),whiletheelectricfieldwillbedeter-\nmined at half-integer time sites and integer space sites (filled circles). This extra level of\ncomplicationofinterlacedlatticesleadstoanaccurateandrobustalgorithm.Becausethe\nfieldsalreadyhavesubscriptsindicatingtheirvectornature,weindicatethelatticeposition\nassuperscripts,forexample,\nEx(z,t)‚ÜíEx(kŒîz,nŒît)‚ÜíEk,n\nx. (24.30)\nMaxwell‚Äôsequations,(24.26)and(24.27),nowbecomethediscreteequations:\nEk,n+1‚àï2\nx‚àíEk,n‚àí1‚àï2\nx\nŒît=‚àíHk+1‚àï2,n\ny‚àíHk‚àí1‚àï2,n\ny\nùúñ0Œîz,\nHk+1‚àï2,n+1\ny‚àíHk+1‚àï2,n\ny\nŒît=‚àíEk+1,n+1‚àï2\nx‚àíEk,n+1‚àï2\nx\nùúá0Œîz.\n490 24 Quantum Wave Packets and EM Waves\ntz\nn\nn + 1n + 1/2n ‚Äì 1/2k k+1k + 1/2\nk ‚Äì 1/2\nHy\nExFigure 24.8 The algorithm for using the known\nvalues of ExandHyat two earlier times, and three\ndifferent space positions, to advance the solution to\nthe present time. The values of Exare determined\non the lattice of Ô¨Ålled circles, corresponding to\ninteger space indices and half-integer time indices.\nIn contrast, the values of Hyare determined on the\nlattice of open circles, corresponding to half-integer\nspace indices and integer time indices.\nInsummary,thisformulationsolvesfortheelectricfieldatintegerspacesteps, k,buthalf-\nintegertimesteps, n,whilethemagneticfieldissolvedforathalf-integerspacesteps,but\nintegertimesteps.\nWeconverttheseequationsintotwosimultaneousalgorithmsbysolvingfor Exattime\nn+1\n2,andHyattimen:\nEk,n+1‚àï2\nx=Ek,n‚àí1‚àï2\nx‚àíŒît\nùúñ0Œîz(\nHk+1‚àï2,n\ny‚àíHk‚àí1‚àï2,n\ny)\n, (24.31)\nHk+1‚àï2,n+1\ny=Hk+1‚àï2,n\ny‚àíŒît\nùúá0Œîz(\nEk+1,n+1‚àï2\nx‚àíEk,n+1‚àï2\nx)\n. (24.32)\nTheverynatureofMaxwell‚Äôsequationsrequiresthesealgorithmstobeappliedsimultane-\nously:thespacevariationof Hydeterminesthetimederivativeof Ex,andthespacevariation\nofExdeterminesthetimederivativeof Hy(Figure24.8).Thesealgorithmsaremoreinvolved\nthan our usual time-stepping ones in that the electric fields (filled circles in Figure 24.8)\natfuturetimes, t=n+1\n2,aredeterminedfromtheelectricfieldsatonetimestepearlier,\nt=n‚àí1\n2,andthemagneticfieldsathalfatimestepearlier, t=n.Likewise,themagnetic\nfields(opencirclesinFigure24.8)atfuturetimes, t=n+1,aredeterminedfromthemag-\nneticfieldsatonetimestepearlier, t=n,andtheelectricfieldathalfatimestepearlier,\nt=n+1\n2.Inotherwords,itisasifwehavetwointerleavedlattices,withtheelectricfields\ndeterminedforhalf-integertimesonlattice1andthemagneticfieldsatintegertimeson\nlattice2.\nAlthoughthesehalf-integertimesarethenormforFDTDmethods[TafloveandHagness,\n2000;Sullivan,2000],itmaybeeasierforsomereaderstounderstandthembydoublingthe\nindexvalues,andreferringtoevenandoddtimesinstead:\nEk,n\nx=Ek,n‚àí2\nx‚àíŒît\nùúñ0Œîz(\nHk+1,n‚àí1\ny‚àíHk‚àí1,n‚àí1\ny)\n, keven,nodd, (24.33)\nHk,n\ny=Hk,n‚àí2\ny‚àíŒît\nùúá0Œîz(\nEk+1,n‚àí1\nx‚àíEk‚àí1,n‚àí1\nx)\n, kodd,neven. (24.34)\nThismakesitclearthat Eisdeterminedforevenspaceindicesandoddtimes,while His\ndeterminedforoddspaceindicesandeventimes.\nWesimplifythealgorithm,andmakeitsstabilityanalysissimpler,byrenormalizingthe\nelectricfieldstohavethesamedimensionsasthemagneticfields:\nÃÉE=‚àöùúñ0\nùúá0E. (24.35)",6476
227-24.8 More EM Problems.pdf,227-24.8 More EM Problems,"24.7 Split-Time FDTD 491\nThealgorithms(24.31)and(24.32)nowbecome:\nÃÉEk,n+1‚àï2\nx=ÃÉEk,n‚àí1‚àï2\nx+ùõΩ(\nHk‚àí1‚àï2,n\ny‚àíHk+1‚àï2,n\ny)\n, (24.36)\nHk+1‚àï2,n+1\ny=Hk+1‚àï2,n\ny+ùõΩ(\nÃÉEk,n+1‚àï2\nx‚àíÃÉEk+1,n+1‚àï2\nx)\n, (24.37)\nùõΩ=c\nŒîz‚àïŒît,c=1‚àöùúñ0ùúá0. (24.38)\nHere,cisthespeedoflightinavacuum,and ùõΩistheratioofthespeedoflighttothegrid\nvelocityŒîz‚àïŒît.\nThespacestep Œîzandthetimestep Œîtmustbechosensothatthealgorithmsaresta-\nble.Thescalesofthespaceandtimedimensionsaresetbythewavelengthandfrequency,\nrespectively,ofthepropagatingwave.Asaminimum,wewantatleast10gridpointstofall\nwithinawavelength:\nŒîz‚â§ùúÜ\n10. (24.39)\nThetimestepisthendeterminedbytheCourantstabilitycondition[TafloveandHagness,\n2000;Sullivan,2000]tobe:\nùõΩ=c\nŒîz‚àïŒît‚â§1\n2. (24.40)\nAswehaveseenbefore,(24.40)impliesthatmakingthetimestepsmallerimprovespre-\ncision and maintains stability, but making the space step smaller must be accompanied\nby a simultaneous decrease in the time step in order to maintain stability (you should\ncheckthis).\n24.7.1 Implementation and Assessment\nInListing24.3,weprovideasimpleimplementationoftheFDTDalgorithmfora zlattice\nof200sites.ItproducestheoutputshowninFigure24.9.Theinitialconditionscorrespond\ntoasinusoidalvariationofthe EandHfieldsfor0 ‚â§z‚â§200:\nEx(z,t=0)=0.1sin(\n2ùúãz\n100)\n,Hy(z,t=0)=0.1sin(\n2ùúãz\n100)\n, (24.41)\nThealgorithmthenstepsoutintimeforaslongastheuserdesires.Thediscreteformof\nMaxwellequationsusedare:\nEx\nEx Ex\nz zHyEx\nHy\nFigure 24.9 TheEÔ¨Åeld (light colored) and the HÔ¨Åeld (dark) at the initial time (left), and at a later\ntime (right). Periodic boundary conditions are used at the ends of the spatial region, which means\nthat the large zwave continues into the z=0w a v e .",1685
228-24.9 Code Listings.pdf,228-24.9 Code Listings,"492 24 Quantum Wave Packets and EM Waves\n1Ex[k, 1] = Ex[k, 0] + beta ‚àó(Hy[k‚àí1, 0]‚àíHy[k+1, 0])\nHy[k, 1] = Hy[k, 0] + beta ‚àó(Ex[k‚àí1, 0]‚àíEx[k+1, 0])\nThesecondindextakesthevalues0and1,with0beingtheoldtimeand1thenew.At\ntheendofeachiteration,thenewfieldthroughoutallofspacebecomestheoldone,and\naneweroneiscomputed.Asthespatialendpoints, k=0and k=xmax-1,arenotdefined,we\nassumeperiodicboundaryconditions.\n1) Imposeboundaryconditionsthatmakeallfieldsvanishontheboundaries.\n2) TesttheCourantcondition(24.40)byexaminingthestabilityofthesolutionfordiffer-\nentvaluesof ŒîzandŒît.\n3) Thedirectionofpropagationofthepulseisinthe E√óHdirection,whichdependson\ntherelativephasebetweenthe EandHfields.Verifythatwithnoinitial Hfield,we\nobtainpulsesbothtotherightandtheleft.\n4) Modifytheprogramsothatthereisaninitial Hpulse,aswellasaninitial Epulse,both\nwithGaussiantimesinasinusoidalshape.\n5) Verify that the direction of propagation changes if the EandHfields have relative\nphasesof0or ùúã.\n6) Investigatetheresonatormodesofawaveguidebypickingtheinitialconditionscor-\nrespondingtoplanewaveswithnodesattheboundaries.\n7) Investigate standing waves with wavelengths longer than the size of the integration\nregion.\n8) Simulateunboundedpropagationbybuildinginperiodicboundaryconditionsintothe\nalgorithm.\n9) Placeamediumwithperiodicpermittivityintheintegrationvolume.Thisshouldact\nasafrequency-dependentfilter,whichblockscertainfrequencies.\n10) Extendthealgorithmtoincludetheeffectofentering,propagatingthrough,andexiting\nadielectricmaterialplacedwithinthe zintegrationregion.\n(a) Ensurethatyouseebothtransmissionandreflectionatthedielectricboundaries.\n(b) Investigatetheeffectofvaryingthedielectric‚Äôsindexofrefraction.\n24.8 More E&M Problems\n24.8.1 Circularly Polarized Waves\nWenowextendourtreatmenttoEMwavesthatarenotrestrictedtolinearpolarizations.\nAccordingly,weadd HxandEyvariationsto(24.26)and(24.27):\nùúïHx\nùúït=1\nùúá0ùúïEy\nùúïz, (24.42)\nùúïEy\nùúït=1\nùúñ0ùúïHx\nùúïz. (24.43)\nWhendiscretizedinthesamewayas(24.31)and(24.32),weobtain:\nHk+1‚àï2,n+1\nx=Hk+1‚àï2,n\nx+Œît\nùúá0Œîz(\nEk+1,n+1‚àï2\ny‚àíEk,n+1‚àï2\ny)\n, (24.44)\n24.8 More E&M Problems 493\nEk,n+1‚àï2\ny=Ek,n‚àí1‚àï2\ny+Œît\nùúñ0Œîz(\nHk+1‚àï2,n\ny‚àíHk‚àí1‚àï2,n\ny)\n. (24.45)\nToproduceacircularlypolarizedtravelingwave,wesetasinitialconditions:\nEx=cos(\nt‚àíz\nc+ùúôy)\n,Hx=‚àöùúñ0\nùúá0cos(\nt‚àíz\nc+ùúôy)\n, (24.46)\nEy=cos(\nt‚àíz\nc+ùúôx)\n,Hy=‚àöùúñ0\nùúá0cos(\nt‚àíz\nc+ùúôx+ùúã)\n. (24.47)\nWetakethephasestobe ùúôx=ùúã‚àï2andùúôy=0,sothattheirdifference ùúôx‚àíùúôy=ùúã‚àï2,which\nleadstocircularpolarization.Weincludetheinitialconditionsinthesamemanneraswe\ndidtheGaussianpulse,onlynowwiththesecosinefunctions.\nListing 24.5 gives our implementation EMcirc.py for waves with transverse two-\ncomponent EandHfields. Some results of the simulation are shown in Figure 24.11,\nwhereyouwillnotethedifferenceinphasebetween EandH.\n24.8.2 Wave Plates\nProblem Developanumericalmodelforawaveplatethatconvertalinearlypolarized\nelectromagneticwaveintoacircularlypolarizedone.\nAsseeninFigure24.10,awaveplateisanopticaldevicethataltersthepolarizationoflight\ntraveling through it by shifting the relative phase of the components of the polarization\nvector.Aquarterwaveplateintroducesarelativephaseof ùúÜ‚àï4,where ùúÜisthewavelength\nof the light. Physically, a wave plate is often a birefringent crystal in which the different\npropagationvelocitiesofwavesintwoorthogonaldirectionsleadtoaphasechange.The\nthicknessoftheplatedeterminestheamountofphasechange.\nTo simulate a wave plate, we start with a linearly polarized wave, with both ExandEy\ncomponentspropagatingalongthe zdirection.Thewaveenterstheplateandemergesfrom\nit while still traveling in the zdirection, but now with the relative phase of these fields\nEy\nz\nFigure 24.10 One frame from the program quarterwave.py in Listing 24.4, showing a linearly\npolarized electromagnetic wave entering a quarter wave plate from the left, and leaving as a\ncircularly polarized wave on the right (the arrow on the left oscillates back and forth at 45‚àòwhile\nthe one on the right rotates).\nFigure 24.11 EandHÔ¨Åelds at t=100 for a\ncircularly polarized wave.\n\n494 24 Quantum Wave Packets and EM Waves\nLŒîxRŒîx\nGŒîx CŒîx\nŒîx20\n10\n0\n‚Äì10\n‚Äì20v20\n10\n0\n‚Äì10\n‚Äì20v\nFigure 24.12 Left: A transmission line that repeats every Œîx.Right: Two frames of an animation\nproduced by TeleMat.py showing a wave transmitted along a telegraph line and being reÔ¨Çected\nfrom an end.\nshifted. Of course, because this is an EM wave, there will also be coupled magnetic field\ncomponentspresent,inthiscase HxandHy,andtheytooneedbecomputed.\nMaxwellequationsforwavepropagatingalongthe zaxisare:\nùúïHx\nùúït=+1\nùúá0ùúïEy\nùúïz,ùúïHy\nùúït=‚àí1\nùúá0ùúïEx\nùúïz, (24.48)\nùúïEx\nùúït=‚àí1\nùúñ0ùúïHy\nùúïz,ùúïEy\nùúït=+1\nùúñ0ùúïHx\nùúïz. (24.49)\nWetakeasinitialconditionsawaveincidentfromtheleftalongthe zaxis,linearlypolarized\n(electricfielddirectionof45‚àò),withcorresponding,andperpendicular, Hcomponents:\nEx(t=0)=0.1cos(\n2ùúãx\nùúÜ)\n,Ey(t=0)=0.1cos(\n2ùúãy\nùúÜ)\n, (24.50)\nHx(t=0)=0.1cos(2ùúãx\nùúÜ),Hy(t=0)=0.1cos(2ùúãy\nùúÜ). (24.51)\nBecauseonlytherelativephasesmatter,wesimplifythecalculationbyassumingthatthe Ey\nandHxcomponentsdonothavetheirphaseschanged,butthatthe ExandHycomponents\ndo (in this case by ùúÜ‚àï4 when they leave the plate). Of course, after leaving the plate and\ntravelinginfreespace,therearenofurtherchangesinrelativephase.\n24.8.3 Algorithm and Exercise\nAsinSection24.7andFigure24.8,wefollowtheFDTDapproachofusingknownvaluesof\nExandHyatthreeearliertimes,andthreedifferentspacepositions,toobtainthesolution\nat the present time. With the renormalized electric fields as in (24.35), this leads to the\nbeautifullysymmetricequations:\nEk,n+1\nx=Ek,n\nx+ùõΩ(\nHk+1,n\ny‚àíHk,n\ny)\n, (24.52)\nEk,n+1\ny=Ek,n\ny+ùõΩ(\nHk+1,n\nx‚àíHk,n\nx)\n, (24.53)\nHk,n+1\nx=Hk,n\nx+ùõΩ(\nEk+1,n\ny‚àíEk,n\ny)\n, (24.54)\nHk,n+1\ny=Hk,n\ny+ùõΩ(\nEk+1,n\nx‚àíEk,n\nx)\n. (24.55)\n24.8 More E&M Problems 495\n1) Modifytheprogram FDTD.pyinListing24.3sothatitsolvesthealgorithm(24.52)‚Äì(24.55).\nUseùõΩ=0.01.\n(a) Aftereachtimestep,imposeagradualincrementofthephasesothatthetotalphase\nchangewillbeonequarterofawavelength.Ourprogramforthisis quarterplat.py .\n(b) Verify that the plate converts an initially linearly polarized wave into a circularly\npolarizedone.\n(c) Verify that the plate converts an initially circularly polarized wave into a linearly\npolarizedone.\n(d) Whathappensifyouputtwoplatestogether?Three?Four?(Verify!)\n24.8.4 Twin Lead Transmission Line\nThemodelofatwin-leadtransmissionlineconsistsoftwoparallelwiresonwhichalter-\nnating current or pulses propagate [Sullivan, 2000]. The equivalent circuit for a segment\noflength ŒîxisshownontheleftofFigure24.12.Thereisaninductance LŒîx,aresistance\nRŒîx, a capacitance CŒîx, and a conductance (inverse resistance of the dielectricmaterial\ninsulatingthewires) GŒîx.Thetelegrapher‚Äôsequationsdescribethevoltageandcurrent:\nùúïV(x,t)\nùúïx=‚àíRI‚àíLùúïI(x,t)\nùúït, (24.56)\nùúïI(x,t)\nùúïx=‚àíGV‚àíCùúïV(x,t)\nùúït. (24.57)\nForlosslesstransmissionlines,thatisthosewith R=G=0,theequationsbecome:\nùúïV(x,t)\nùúïx=‚àíLùúïI(x,t)\nùúït,ùúïI(x,t)\nùúïx=‚àíCùúïV(x,t)\nùúït. (24.58)\nDifferentiationoftheseequations,andsubstitutionintooneanother,leadstothefamiliar\n1Dwaveequation:\nùúï2V(x,t)\nc2ùúït2‚àíùúï2V(x,t)\nùúïx2=0,c=1‚àö\nLC. (24.59)\na) Apply the leapfrog algorithm to the telegrapher‚Äôs equations (24.56), making sure to\naccountforthestabilitycondition:\ncŒît\nŒîx‚â§1. (24.60)\nExperimentwithdifferentvaluesfor ŒîxandŒîtinordertoobtainbetterprecision,orto\nspeedupthecomputation.\nb) Imposetheboundaryconditions V(0,t)=V(L,t)=0,whereListhelengthofthetrans-\nmissionline.\nc) Useasinitialconditionsapulsewithaconstantvoltage:\nV(x,t=0)=10e‚àíx2‚àï0.1,ùúïV(x,t)\nùúït=0. (24.61)\nd) Goodvaluestotryare L=0.1,C=2.5,Œît=0.025,and Œîx=0.05.\ne) Investigatetheeffectofazerovaluefortheconductance Gandtheresistance R.Doyour\nresultsagreewithwhatyoumightexpect?\nf) Fornonzero RandG,investigatethedistortionthatoccurswhenarectangularpulseis\nsentdownthetransmissionline.Atwhatpointwouldyousaythepulseshapebecomes\nunrecognizable?\n496 24 Quantum Wave Packets and EM Waves\n24.9 Code Listings\nListing 24.1 HarmosAnimate.py solvesthetime-dependentSchr√∂dingerequationfor\naGaussianwavepacketmovingwithinaharmonicoscillatorpotential.\n# HarmonsAnimate : Solve t ‚àídependent Sch Eqt for H O wi animation\n2\nfromvisualimport ‚àó\n# Initialize wave function , probability , potential\n6dx = 0.04; dx2 = dx ‚àódx; k0 = 5.5 ‚àópi; dt = dx2/20.0;\nxmax = 6.0; beta = dt/dx2\nxs = arange( ‚àíxmax,xmax+dx/2,dx) # Array x values\n10g = display(width=500, height=250, title= ‚ÄôWave Packet in HO Well‚Äô )\nPlotObj = curve(x=xs, color=color.yellow, radius=0.1)\ng.center = (0,2,0) # Center of scene\n# Initial wave packet\n14R=e x p ( ‚àí0.5‚àó(xs/0.5) ‚àó‚àó2)‚àócos(k0 ‚àóxs) # Array Re I\nI=e x p ( ‚àí0.5‚àó(xs/0.5) ‚àó‚àó2)‚àósin(k0 ‚àóxs) # Array Im I\nV = 15.0 ‚àóxs‚àó‚àó2 # The potential\n18whileTrue:\nrate(500)\nR[1:‚àí1] = R[1: ‚àí1]‚àíbeta ‚àó(I[2:]+I[: ‚àí2]‚àí2‚àóI[1:‚àí1])+dt ‚àóV[1:‚àí1]‚àóI[1:‚àí1]\nI[1:‚àí1] = I[1: ‚àí1] + beta ‚àó(R[2:]+R[: ‚àí2]‚àí2‚àóR[1:‚àí1])‚àídt‚àóV[1:‚àí1]‚àóR[1:‚àí1]\n22PlotObj.y = 4 ‚àó(R‚àó‚àó2+I ‚àó‚àó2)\nListing 24.2 3QMdisks.py solvesforawavepacketscatteringfrom3disks.\n# 3QMdisks. py : Wavepacket scattering from 3 disks wi MatPlot\nimportmatplotlib.pylab as p, numpy as np\n4frommpl_toolkits.mplot3d importAxes3D\nr = 10; N = 101; x1 = 51; # 51 = 90. ‚àósqrt 3/2 ‚àí30\ndx = 0.1; dx2 = dx ‚àódx; k0 = 20.; k1 = 0.\n8dt = 0.002; fc = dt/dx2; Xo = 40; Yo = 25\n# Declare arrays\nV= n p . z e r o s ( ( N , N ) , float); Rho =np.zeros((N,N), float)\nRePsi = np.zeros((N,N), float); ImPsi =np.zeros((N,N), float)\n12ix = np.arange(0, 101); iy = np.arange(0,101)\nX, Y = np.meshgrid(ix, iy)\nfig = p.figure(); ax = Axes3D(fig) # Create figure\n16defPot1Disk(xa,ya): # Potential single disk\nforiin range (ya‚àír,ya+r+1):\nforjin range (xa‚àír,xa+r+1):\nifnp. sqrt((i ‚àíya)‚àó‚àó2+(j‚àíxa)‚àó‚àó2)<=r: V[i ,j] = 5.\n20\ndefPot3Disks(): # Potential three disk\nPot1Disk(30,45); Pot1Disk(70,45); Pot1Disk(50,80)\n24defPsi_0(Xo,Yo): # Initial Psi\nforiinnp.arange(0,N):\nforjinnp.arange(0, N):\nGaussian = np.exp( ‚àí0.03‚àó(i‚àíYo)‚àó‚àó2‚àí0.03‚àó(j‚àíXo)‚àó‚àó2)\n28 RePsi[i,j] = Gaussian ‚àónp.cos(k0 ‚àói+k1 ‚àój)\nImPsi[i,j] = Gaussian ‚àónp.sin(k0 ‚àói+k1 ‚àój)\nRho[i,j] = RePsi[i,j] ‚àó‚àó2+I m P s i [ i,j ] ‚àó‚àó2 + 0.01\nPsi_0(Xo,Yo) # Psi a n d R h o initial\n32Pot3Disks() # Initial Psi\nfortin range (0, 120): # 120‚àí>30 # Compute Psi t <120\nift%5 == 0: print(‚Äôt =‚Äô,t) # Print ea 5th t\nImPsi[1: ‚àí1,1:‚àí1] = ImPsi[1: ‚àí1,1:‚àí1] + fc ‚àó(RePsi[2: ,1: ‚àí1] \\n36 +R e P s i [ : ‚àí2, 1 :‚àí1]‚àí4‚àóRePsi[1: ‚àí1,1:‚àí1] + RePsi[1: ‚àí1,2: ]\\n24.9 Code Listings 497\n+R e P s i [ 1 : ‚àí1, :‚àí2]) + V[1: ‚àí1,1:‚àí1]‚àódt‚àóRePsi[1: ‚àí1,1:‚àí1]\nRePsi[1: ‚àí1,1:‚àí1] = RePsi[1: ‚àí1,1:‚àí1]‚àífc‚àó(ImPsi[2: ,1: ‚àí1]\\n+ImPsi[ : ‚àí2,1:‚àí1]‚àí4‚àóImPsi[1: ‚àí1,1:‚àí1] + ImPsi[1: ‚àí1,2: ]\\n40 +ImPsi[1: ‚àí1, :‚àí2]) + V[1: ‚àí1,1:‚àí1]‚àódt‚àóImPsi[1: ‚àí1,1:‚àí1]\nforiin range (1, N‚àí1): # Compute Rho\nforjin range (1,N‚àí1):\nifV[i,j] !=0: RePsi[i,j] = 0; ImPsi[i,j] = 0 # Hard Disk\n44 Rho[i , j] = 0.1 ‚àó(RePsi[i,j] ‚àó‚àó2+I m P s i [ i,j ] ‚àó‚àó2) + 0.0002 ‚àóV[i,j]\nX, Y = np.meshgrid(ix, iy)\nZ=R h o [ X , Y ]\nax.set_xlabel( ‚Äôy‚Äô)\n48ax.set_ylabel( ‚Äôx‚Äô)\nax.set_zlabel( ‚ÄôRho(x,y)‚Äô )\nax.plot_wireframe(X, Y, Z, color = ‚Äôg‚Äô)\nprint(""finito"" )\n52p.show()\nListing 24.3 FDTD.py solvesMaxwell‚ÄôsequationsviaFDTDalgorithmforlinearlypolar-\nizedwavepropagationinthe zdirection.\n# FDTD . py FDTD Maxwell ‚Äô s e q u a t i o n s i n 1 ‚àíD wi Visual\n2\nfromvisualimport ‚àó\nXm = 201; Ym = 100; Zm = 100; ts = 2; beta = 0.01\n6Ex = zeros((Xm, ts) , float); H y= zeros((X m,ts), float) # Declare arrays\n#S e t u p 3 ‚àíDP l o t s\nscene = display(x=0,y=0,width= 800, height= 500, \\ntitle= ‚ÄôE: cyan, H: red. Periodic BC‚Äô ,forward=( ‚àí0.6,‚àí0.5,‚àí1))\n10Eplot = curve(x= list(range(0,Xm)),color=color.cyan,radius=1.5,display=scene)\nHplot = curve(x= list(range(0,Xm)),color=color.red, radius=1.5,display=scene)\nvplane = curve(pos=[( ‚àíXm,Ym) ,(Xm,Ym) ,(Xm, ‚àíYm) ,(‚àíXm,‚àíYm) ,\n(‚àíXm,Ym)],color=color.cyan)\n14zaxis = curve(pos=[( ‚àíXm,0) ,(Xm,0)],color=color.magenta)\nhplane = curve(pos=[( ‚àíXm,0 ,Zm) ,(Xm,0 ,Zm) ,(Xm,0, ‚àíZm) ,(‚àíXm,0,‚àíZm) ,\n(‚àíXm,0,Zm)],color=color.magenta)\nball1 = sphere(pos = (Xm+30, 0,0), color = color.black, radius = 2)\n18ExLabel1 = label( text = ‚ÄôEx‚Äô,p o s=( ‚àíXm‚àí10, 50), box=0)\nExLabel2 = label( text = ‚ÄôEx‚Äô,p o s=( X m + 1 0 ,5 0 ) ,b o x = 0 )\nHyLabel = label( text = ‚ÄôHy‚Äô,p o s=( ‚àíXm‚àí10, 0,50), box=0)\nzLabel = label( text = ‚ÄôZ‚Äô,p o s = ( X m + 1 0 , 0 ) , b o x = 0 )\n22\ndefPlotFields():\nz = arange(Xm)\nEplot.x = 2 ‚àóz‚àíXm # World to screen coords\n26Eplot.y = 800 ‚àóEx[z,0]\nHplot.x = 2 ‚àóz‚àíXm\nHplot.z = 800 ‚àóHy[z,0]\n30z = arange(Xm)\nEx[:Xm,0] = 0.1 ‚àósin(2 ‚àópi‚àóz/100.0) # Initial field\nHy[:Xm,0] = 0.1 ‚àósin(2 ‚àópi‚àóz/100.0)\nPlotFields()\n34\nwhileTrue:\nrate(600)\nEx[1:Xm ‚àí1,1] = Ex[1:Xm ‚àí1,0] + beta ‚àó(Hy[0:Xm ‚àí2,0]‚àíHy[2:Xm,0])\n38Hy[1:Xm ‚àí1,1] = Hy[1:Xm ‚àí1,0] + beta ‚àó(Ex[0:Xm ‚àí2,0]‚àíEx[2:Xm,0])\nEx[0,1] = Ex[0,0] + beta ‚àó(Hy[Xm‚àí2,0]‚àíHy[1,0]) #B C\nEx[Xm‚àí1,1] = Ex[Xm ‚àí1,0] + beta ‚àó(Hy[Xm‚àí2,0]‚àíHy[1,0])\nHy[0 ,1] = Hy[0 ,0] + beta ‚àó(Ex[Xm‚àí2,0]‚àíEx[1,0]) #B C\n42Hy[Xm‚àí1,1] = Hy[Xm ‚àí1,0] + beta ‚àó(Ex[Xm‚àí2,0]‚àíEx[1,0])\nPlotFields()\nEx[:Xm,0] = Ex[:Xm,1] #N e w‚àí>o l d\nHy[:Xm,0] = Hy[:Xm,1]\n498 24 Quantum Wave Packets and EM Waves\nListing 24.4 QuarterPlate.py Maxwell‚Äôs equations solution via FDTD algorithm for\nquarterwaveplate.\n1# QuarterPlate.py F D T D solution of Maxwell ‚Äôs equations in 1 ‚àíD\nfromvisualimport ‚àó\n5xmax = 401; ymax = 100; zmax = 100; ts = 2; beta = 0.01\nEx = zeros((xmax, ts) , float); Ey = zeros((xmax, ts) , float); H x=\nzeros((xmax, ts) , float)\nHy = zeros((xmax, ts) , float); Hyy = zeros((xmax, ts) , float)\n9Exx = zeros((xmax, ts) , float); Eyy = zeros((xmax, ts) , float)\nHxx = zeros((xmax, ts) , float)\nscene = display(x=0,y=0,width= 800, height= 500,\n13 title= ‚ÄôEy : in cyan, Ex : in yellow. Propagation with periodic\nboundary conditions‚Äô ,\nforward=( ‚àí0.8,‚àí0.3,‚àí0.7))\nExfield = curve(x= list(range(0,xmax)),color= color.yellow,radius=1.5,display=scene)\nEyfield = curve(x= list(range(0,xmax)),color=color.cyan,radius=1.5,display=scene)\n17vplane= curve(pos=[( ‚àíxmax,ymax) ,(xmax,ymax) ,(xmax, ‚àíymax),( ‚àíxmax,‚àíymax) ,\n(‚àíxmax,ymax)],color=color.cyan)\nzaxis = curve(pos=[( ‚àíxmax,0) ,(xmax,0)],color=color.magenta)\nhplane = curve(pos=[( ‚àíxmax,0 ,zmax) ,(xmax,0 ,zmax) ,(xmax,0, ‚àízmax),( ‚àíxmax,0, ‚àízmax) ,\n21 (‚àíxmax,0,zmax)],color=color.magenta)\nball1 = sphere(pos = (xmax+30, 0,0), color = color.black, radius = 2)\nba2 = sphere(pos=(xmax ‚àí200,0),color=color.cyan,radius=3)\nplate = box(pos=( ‚àí100,0,0),height=2 ‚àózmax,width=2 ‚àóymax,length=0.5 ‚àóxmax,\n25 color=(1.0,0.6,0.0),opacity=0.4)\nExlabel1 = label( text = ‚ÄôEy‚Äô,p o s=( ‚àíxmax‚àí10, 50), box = 0 )\nExlabel2 = label( text = ‚ÄôEy‚Äô, pos = (xmax+10, 50) , box = 0 )\nEylabel = label( text = ‚ÄôEx‚Äô,p o s=( ‚àíxmax‚àí10, 0,50), box = 0 )\n29zlabel = label( text = ‚ÄôZ‚Äô, pos = (xmax+10, 0) , box = 0 )\npolfield = arrow(display = scene)\npolfield2 = arrow(display = scene)\nti = 0\n33\ndefInitField():\nkar = arange(xmax)\nphx = 0.5 ‚àópi\n37Hyy[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\nExx[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\nEyy[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\nHxx[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\n41Ey[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\nHx[:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókar/100)\ndefInitExHy():\n45k = arange(101)\nEx[:101,0] = 0.1 ‚àócos(‚àí2‚àópi‚àók/100)\nHy[:101,0] = 0.1 ‚àócos(‚àí2‚àópi‚àók/100)\nkk = arange(101,202) # Inside plate , delay lambda/4\n49Ex[101:202,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókk/100.0 ‚àí0.005 ‚àópi‚àó(kk‚àí101)) # pi/2 phase\nHy[101:202,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókk/100.0 ‚àí0.005 ‚àópi‚àó(kk‚àí101))\nkkk = arange(202,xmax) # After plate , phase diff pi/2\nEx[202:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókkk/100 ‚àí0.5‚àópi)\n53Hy[202:xmax,0] = 0.1 ‚àócos(‚àí2‚àópi‚àókkk/100 ‚àí0.5‚àópi)\ndefPlotFields(ti): # screen coordinates\nk = arange(xmax)\n57Exfield.x = 2 ‚àók‚àíxmax # world to screen coords .\nExfield.y = 800 ‚àóEy[k, ti]\nEyfield.x = 2 ‚àók‚àíxmax # world to screen coords .\nEyfield.z = 800 ‚àóEx[k, ti]\n61\nInitField()\nInitExHy()\nPlotFields(ti)\n65j=0\n24.9 Code Listings 499\nend = 0\nwhileend<5:\nrate(150)\n69Exx[1:xmax ‚àí1,1] = Exx[1:xmax ‚àí1,0] + beta ‚àó(Hyy[0:xmax ‚àí2,0]‚àíHyy[2:xmax,0])\nEyy[1:xmax ‚àí1,1] = Eyy[1:xmax ‚àí1,0] + beta ‚àó(Hxx[0:xmax ‚àí2,0]‚àíHxx[2:xmax,0])\nHyy[1:xmax ‚àí1,1] = Hyy[1:xmax ‚àí1,0] + beta ‚àó(Exx[0:xmax ‚àí2,0]‚àíExx[2:xmax,0])\nHxx[1:xmax ‚àí1,1] = Hxx[1:xmax ‚àí1,0] + beta ‚àó(Eyy[0:xmax ‚àí2,0]‚àíEyy[2:xmax,0])\n73Ex[1:xmax ‚àí1,1] = Ex[1:xmax ‚àí1,0] + beta ‚àó(Hy[0:xmax ‚àí2,0]‚àíHy[2:xmax,0])\nEy[1:xmax ‚àí1,1] = Ey[1:xmax ‚àí1,0] + beta ‚àó(Hxx[0:xmax ‚àí2,0]‚àíHxx[2:xmax,0])\nHy[1:xmax ‚àí1,1] = Hy[1:xmax ‚àí1,0] + beta ‚àó(Ex[0:xmax ‚àí2,0]‚àíEx[2:xmax,0])\nHx[1:xmax ‚àí1,1] = Hx[1:xmax ‚àí1,0] + beta ‚àó(Eyy[0:xmax ‚àí2,0]‚àíEyy[2:xmax,0])\n77polfield.pos = ( ‚àí280,0,0)\npolfield.axis = (0,700 ‚àóExx[60,1],700 ‚àóEyy[60,1])\npolfield2.pos = (380,0,0)\npolfield2.axis = (0,700 ‚àóEx[360,1], ‚àí700‚àóEy[360,1])\n81Exx[0,1] = Exx[0,0] + beta ‚àó(Hyy[xmax ‚àí2,0]‚àíHyy[1 ,0]) # Periodic B C\nEyy[0,1]= Eyy[0,0] + beta ‚àó(Hxx[xmax ‚àí2,0]‚àíHxx[1 ,0])\nHyy[0,1] = Hyy[0,0] + beta ‚àó(Exx[xmax ‚àí2,0]‚àíExx[1,0]) # Periodic BC, 0=100\nHxx[0,1] = Hxx[0,0] + beta ‚àó(Eyy[xmax ‚àí2,0]‚àíEyy[1,0])\n85Hyy[xmax ‚àí1,1] = Hyy[xmax ‚àí1,0] + beta ‚àó(Exx[xmax ‚àí2,0]‚àíExx[1,0])\nHxx[xmax ‚àí1,1] = Hxx[xmax ‚àí1,0] + beta ‚àó(Eyy[xmax ‚àí2,0]‚àíEyy[1,0])\nExx[xmax ‚àí1,1] = Exx[xmax ‚àí1,0] + beta ‚àó(Hyy[xmax ‚àí2,0]‚àíHyy[1 ,0]) #conditions for\nfirst\nEyy[xmax ‚àí1,1] = Eyy[xmax ‚àí1,0] + beta ‚àó(Hxx[xmax ‚àí2,0]‚àíHxx[1 ,0])\n89Ex[0,1] = Exx[0,0] + beta ‚àó(Hyy[xmax ‚àí2,0]‚àíHyy[1 ,0]) # Periodic B C\nEy[0,1] = Eyy[0,0] + beta ‚àó(Hxx[xmax ‚àí2,0]‚àíHxx[1 ,0])\nHy[0,1] = Hyy[0,0] + beta ‚àó(Exx[xmax ‚àí2,0]‚àíExx[1,0]) # Periodic B C 0=100\nHx[0,1] = Hxx[0,0] + beta ‚àó(Eyy[xmax ‚àí2,0]‚àíEyy[1,0])\n93Hy[xmax ‚àí1,1] = Hy[xmax ‚àí1,0] + beta ‚àó(Ex[xmax ‚àí2,0]‚àíEx[xmax ‚àí100,0])\nHx[xmax ‚àí1,1] = Hx[xmax ‚àí1,0] + beta ‚àó(Ey[xmax ‚àí2,0]‚àíEy[1,0])\nEx[xmax ‚àí1,1] = Ex[xmax ‚àí1,0] + beta ‚àó(Hy[xmax ‚àí2,0]‚àíHy[xmax ‚àí100,0])\nEy[xmax ‚àí1,1] = Ey[xmax ‚àí1,0] + beta ‚àó(Hxx[xmax ‚àí2,0]‚àíHxx[1 ,0])\n97PlotFields(ti)\nk = arange(101,202)\nEx[101:202,1] = 0.1 ‚àócos(‚àí2‚àópi‚àók/100‚àí0.005 ‚àópi‚àó(k‚àí101)+2 ‚àópi‚àój/4996.004)\nHy[101:202,1] = 0.1 ‚àócos(‚àí2‚àópi‚àók/100‚àí0.005 ‚àópi‚àó(k‚àí101)+2 ‚àópi‚àój/4996.004)\n101Exx[:xmax,0] = Exx[:xmax,1]\nEyy[:xmax,0] = Eyy[:xmax,1]\nHyy[:xmax,0] = Hyy[:xmax,1]\nHxx[:xmax,0] = Hxx[:xmax,1]\n105Ex[:xmax,0] = Ex[:xmax,1]\nEy[:xmax,0] = Ey[:xmax,1]\nHx[:xmax,0] = Hx[:xmax,1]\nHy[:xmax,0]= Hy[:xmax,1]\n109ifj%4996 == 0:\nj=0\nend += 1\nj=j + 1\nListing 24.5 EMCirc.py solvesMaxwell‚ÄôsequationsviaFDTDalgorithmforcircularly\npolarizedwavepropagationinthe zdirection.\n# EMcirc.py: Maxwell eqs . for circular polarization using F D T D\n2\nfromvisualimport ‚àó\nscene = display(x=0,y=0,width=600,height=400, range=200,\ntitle= ‚ÄôCircular Polarized E (white) &H (yellow) Fields‚Äô )\n6globalphy, pyx\nmax= 201; c = 0.01 #S t a b l e i fc <0.1\nEx = zeros(( max+2,2),float); Ey= zeros(( max+2,2),float)\nHy = zeros (( max+2,2),float); H x= zeros(( max+2,2),float)\n10arrowcol= color.white\nEarrows = []; Harrows = []\nforiin range (0,max,10):\nEarrows.append(arrow(pos=(0,i ‚àí100,0), axis=(0,0,0), color=arrowcol))\n14Harrows.append(arrow(pos=(0,i ‚àí100,0), axis=(0,0,0), color=color.yellow))\ndefplotfields(Ex,Ey,Hx,Hy):\n500 24 Quantum Wave Packets and EM Waves\nforn, arrin enumerate (Earrows):\n18 arr.axis = (35 ‚àóEy[10 ‚àón,1],0,35 ‚àóEx[10 ‚àón,1])\nforn, arrin enumerate (Harrows):\narr.axis = (35 ‚àóHy[10 ‚àón,1],0,35 ‚àóHx[10 ‚àón,1])\ndefinifields(): # Initial E &H\n22phx = 0.5 ‚àópi; phy = 0.0\nz = arange(0, max)\nEx[:‚àí2,0] = cos( ‚àí2‚àópi‚àóz/200+phx); Ey[: ‚àí2,0] = cos( ‚àí2‚àópi‚àóz/200+phy)\nHx[:‚àí2,0] = cos( ‚àí2‚àópi‚àóz/200+phy+pi); Hy[: ‚àí2,0] = cos( ‚àí2‚àópi‚àóz/200+phx)\n26defnewfields():\nwhileTrue: # Time stepping\nrate(1000)\nEx[1:max‚àí1,1] = Ex[1: max‚àí1,0]+c ‚àó(Hy[:max‚àí2,0]‚àíHy[2:max,0])\n30 Ey[1:max‚àí1,1] = Ey[1: max‚àí1,0] + c ‚àó(Hx[2:max,0]‚àíHx[:max‚àí2,0])\nHx[1:max‚àí1,1] = Hx[1: max‚àí1,0] + c ‚àó(Ey[2:max,0]‚àíEy[:max‚àí2,0])\nHy[1:max‚àí1,1] = Hy[1: max‚àí1,0] + c ‚àó(Ex[:max‚àí2,0]‚àíEx[2:max,0])\nEx[0,1] = Ex[0,0] + c ‚àó(Hy[200 ‚àí1,0]‚àíHy[1,0]) # Periodic B C\n34 Ex[200,1] = Ex[200,0] + c ‚àó(Hy[200 ‚àí1,0]‚àíHy[1,0])\nEy[0,1] = Ey[0,0] + c ‚àó(Hx[1,0] ‚àíHx[200 ‚àí1,0])\nEy[200,1] = Ey[200,0] + c ‚àó(Hx[1,0] ‚àíHx[200 ‚àí1,0])\nHx[0 ,1] = Hx[0 ,0] + c ‚àó(Ey[1,0] ‚àíEy[200‚àí1,0])\n38 Hx[200,1] = Hx[200,0] + c ‚àó(Ey[1,0] ‚àíEy[200‚àí1,0])\nHy[0 ,1] = Hy[0 ,0] + c ‚àó(Ex[200 ‚àí1,0]‚àíEx[1,0])\nHy[200,1] = Hy[200,0] + c ‚àó(Ex[200 ‚àí1,0]‚àíEx[1,0])\nplotfields(Ex,Ey,Hx,Hy)\n42 Ex[:max,0] = Ex[: max,1]; Ey[: max,0] = Ey[: max,1] # Update fields\nHx[:max,0] = Hx[: max,1]; Hy[: max,0] = Hy[: max,1]\ninifields() # Initial fields\n46newfields() #N e wf i e l d s",20388
229-25.2.2 Implementation and Assessment.pdf,229-25.2.2 Implementation and Assessment,"501\n25\nShock and Soliton Waves\nThe Ô¨Årst half of this chapter extends the discussion of waves to include nonlinearities,\ndispersion, and hydrodynamic effects. We end up with the Korteweg-de Vries (KDV) equation\nand shallow-water solitons. The second half of this chapter explores a model for solids as a\nchain of coupled, nonlinear oscillators. We end up with the Sine-Gordon equation (SGE) and\nsolitons in solids .\nIn1844,J.ScottRussellreportedanunusualoccurrenceontheEdinburgh-Glasgowcanal\n[Russell,1844](Figure25.1left):\nIwasobservingthemotionofaboatwhichwasrapidlydrawnalonganarrowchan-\nnelbyapairofhorses,whentheboatsuddenlystopped‚Äînotsothemassofwaterin\nthe channel which it had put in motion; it accumulated round the prow of the ves-\nselinastateofviolentagitation,thensuddenlyleavingitbehind,rolledforwardwith\ngreatvelocity,assumingtheformofalargesolitaryelevation,arounded,smoothand\nwell-definedheapofwater,whichcontinueditscoursealongthechannel,apparently\nwithoutchangeofformordiminutionofspeed.Ifolloweditonhorseback,andovertook\nitstillrollingonattherateofsomeeightorninemilesanhour,preservingitsoriginal\nfiguresomethirtyfeetlongandafoottoafootandahalfinheight.Itsheightgradu-\nallydiminished,andafterachaseofoneortwomiles,Ilostitinthewindingsofthe\nchannel.Such,inthemonthofAugust1834,wasmyfirstchanceinterviewwiththat\nsingularandbeautifulphenomenon ‚Ä¶.\nRussellalsonoticedthataninitial,arbitrarywaveformsetinmotioninthechannelevolved\nintotwoormorewavesthatmovedatdifferentvelocities,andthattheyprogressivelymoved\napart until they formed individual, solitary waves. In Figure 25.3 right, we see a single,\nstep-like wave breaking up into approximately eight of these solitary waves (now called\nsolitons).Theseeightsolitonsoccursofrequentlythatsomeofthereferencesconsiderthem\ntheequivalentofthenormalmodesfornonlinearsystems.Russellwentontoproducethese\nsolitarywavesinalaboratory,andempiricallydeducedthattheirspeed cisrelatedtothe\ndepthhofthewaterinthecanal,andtotheamplitude Aofthewaveby,\nc2=g(h+A), (25.1)\nwheregistheaccelerationduetogravity.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n502 25 Shock and Soliton Waves\n6\n80\nxt\n40002468\n120A hydrodynamic soliton\nFigure 25.1 Left:Two computed shallow-water solitary waves crossing each other. The taller\nsoliton on the left catches up with and overtakes the shorter one at t‚âÉ5. The waves resume their\noriginal shapes after the collision. Right: A re-creation of Russel‚Äôs soliton on the Union Canal near\nEdinburgh in July 1995. (Used with permission, Scott [2007].)\nEquation (25.1) implies an effect not found in linear systems, namely, that waves with\ngreateramplitudes Atravelfasterthanthosewithsmalleramplitudes.Observethatthisis\nsimilar to the formation of shock waves, but different from dispersionin which waves of\ndifferentwavelengthshavedifferentvelocities.Thedependenceof contheamplitude Ais\nillustratedinFigure25.1,whereweseeatallersolitoncatchingupwithandpassingthrough\nashorterone.Indeed,thesesolitonsappeartoberelatedtotheformationof tsunamis,ocean\nwavesthatformfromsuddenchangesintheleveloftheoceanfloor,andthentravelover\nlongdistanceswithoutdispersionorattenuation.\nProblem ExplainRussell‚Äôsobservations.\n25.1 The Continuity and Advection Equations\nThemotionoffluidsisdescribedbythecontinuityequationandtheNavier‚ÄìStokesequation\n[LandauandLifshitz,1976].WediscusstheformerhereandthelatterinChapter26.The\ncontinuityequationdescribesconservationofmass:\nùúïùúå(x,t)\nùúït+‚Éó‚àá‚ãÖj=0, jdef=ùúåv(x,t). (25.2)\nHere,ùúå(x,t)is the mass density of the fluid, v(x,t)is its velocity, and j=ùúåvis the mass\ncurrent. As its name implies, the divergence ‚Éó‚àá‚ãÖjdescribes the spreading of the current\nin a region of space, as might occur if there were a current source there. Physically, the\ncontinuityequation(25.2)statesthatchangesinthedensityofthefluidwithinsomeregion\nofspacearisesfromtheflowofcurrentinandoutofthatregion.\nFor1Dflowinthe xdirection,andforafluidthatismovingwithaconstantvelocity ùë£=c,\nthecontinuityequation(25.2)takesasimpleform:\nùúïùúå\nùúït+cùúïùúå\nùúïx=0. (25.3)\n25.2 Shock Waves via Burgers‚Äô Equation 503\nThis is the advection equation , where the term ‚Äúadvection‚Äù is used to describe the hori-\nzontal transport of a quantity from one region of space to another as a result of a flow‚Äôs\nvelocityfield.Forinstance,advectiondescribesthetransportationofdissolvedsaltinwater.\nTheadvectionequationlookslikeafirst-derivativeformofthewaveequation,andindeed,\nthetwoarerelated.Asimplesubstitutionprovesthatanyfunctionwiththeformofatrav-\nelingwave,\nu(x,t)=f(x‚àíct), (25.4)\nwillbeasolutionoftheadvectionequation.Ifweconsiderasurferridingalongthecrest\nofatravelingwave,thatis,remainingatthesamepositionrelativetothewave‚Äôsshapeas\ntimechanges,thenthesurferdoesnotseetheshapeofthewavechangeintime,andthat\nimpliesthat:\nx‚àíct=constant ‚áíx=ct+constant. (25.5)\nThespeedofthesurfer dx‚àïdt=c,whichisaconstant.Anyfunction f(x‚àíct)isclearlya\ntravelingwavesolutioninwhichanarbitrarypulseiscarriedalongwiththefluidatvelocity\ncwithoutchangingshape.\nAlthough the advection equation is simple, trying to solve it by a simple finite differ-\nence scheme (the leapfrog method) may lead to unstable numerical solutions. As we\nshall see when we look at the nonlinear version of this equation, there are better ways.\nListing 25.1 presents our code AdvecLax.py for solving the advection equation using the\nLax-Wendroffmethod(abetterway).\n25.2 Shock Waves via Burgers‚Äô Equation\nInSection25.4,wewillexaminetheKorteweg-deVriesequation‚Äôsdescriptionofsolitary\nwaves. In order to understand the physics contained in that equation, we study, one at\na time, some of the terms in it. We start with two equivalent forms of Burgers‚Äô equation\n[Burgers,1974]:\nùúïu\nùúït+ùúñuùúïu\nùúïx=0, (25.6)\nùúïu\nùúït+ùúñùúï(u2‚àï2)\nùúïx=0, (25.7)\nwherethesecondequationisthe conservativeform .Thisequationcanbeviewedasavari-\nation on the advection equation (25.3), now with the wave speed c=ùúñuproportional to\nthe amplitude of the wave, as Russell found for his waves. The second, nonlinear term\ninBurgers‚Äôequationleadstosomeunusualbehaviors.Indeed,vonNeumannstudiedthis\nequationasasimplemodelforturbulence[FalkovichandSreenivasan,2006].\nIntheadvectionequation(25.3),allpointsonthewavemoveatthesamespeed c,andso\ntheshapeofthewaveremainsunchangedintime.InBurgers‚Äôequation(25.6),thepoints\non the wave move (‚Äúadvect‚Äù) themselves such that the local speed depends on the local\nwave‚Äôsamplitude,withthehighpartsofthewavemovingprogressivelyfasterthanthelow\nparts.Thischangestheshapeofthewaveintime;ifwestartwithawavepacketthathas\nasmoothvariationinheight,thehighpartswillspeedupandpushtheirwaytothefront\n504 25 Shock and Soliton Waves\n0\n2001204\nxtu(x, t)\nFigure 25.2 A visualization showing the formation of a shock wave (sharp edge) in a solution to\nBurgers‚Äô equation that started with a sine wave.\nofthepacket,therebyformingasharpleadingedgeknownasa shockwave [Tabor,1989].\nAshockwavesolutiontoBurgers‚Äôequationwith ùúñ=1isshowninFigure25.2.\n25.2.1 Lax‚ÄìWendroff Algorithm\nWe first solve Burgers‚Äô equation (25.3) via the usual approach in which we express the\nderivatives as central differences. This leads to a leapfrog scheme for the future solution\nintermsofpresentandpastones:\nu(x,t+Œît)=u(x,t‚àíŒît)‚àíùõΩ\n2[u2(x+Œîx,t)‚àíu2(x‚àíŒîx,t)], (25.8)\nui,j+1=ui,j‚àí1‚àíùõΩ\n2[u2\ni+1,j‚àíu2\ni‚àí1,j],ùõΩ=ùúñŒît\nŒîx(CFLnumber) .\nHereu2is the square of u, not its second derivative, and ùõΩis a ratio of constants known\nastheCourant‚ÄìFriedrichs‚ÄìLewy (CFL)number.Asyoushouldproveforyourself, ùõΩ<1is\nrequiredforstability.\nWhilewehaveusedaleapfrogmethodwithsuccessinourprevioussolutionofPDEs,its\nlow-orderapproximationforthederivativebecomesinaccuratewhenthegradientscanget\nlarge,ashappenswithshockwaves,andthismaycausetheleapfrogalgorithmtobecome\nunstable[Press etal.,2007].The Lax‚ÄìWendroffmethod attainsbetterstabilityandaccuracy\nbyretainingsecond-orderdifferencesforthetimederivative:\nu(x,t+Œît)‚âÉu(x,t)+ùúïu\nùúïtŒît+1\n2ùúï2u\nùúït2Œît2. (25.9)\nTocovert(25.9)toanalgorithm,weuseBurgers‚Äôequation ùúïu‚àïùúït=‚àíùúñùúï(u2‚àï2)‚àïùúïxforthe\nfirst-ordertimederivative.Likewise,weuseBurger‚Äôsequationtoexpressthesecond-order\ntimederivativeintermsofspacederivatives:\nùúï2u\nùúït2=ùúï\nùúït[\n‚àíùúñùúï\nùúïx(\nu2\n2)]\n=‚àíùúñùúï\nùúïxùúï\nùúït(\nu2\n2)\n(25.10)\n=‚àíùúñùúï\nùúïx(\nuùúïu\nùúït)\n=ùúñ2ùúï\nùúïx[\nuùúï\nùúïx(\nu2\n2)]\n. (25.11)\nWenextsubstitutethesederivativesintotheTaylorexpansion(25.9)toobtain:\nu(x,t+Œît)=u(x,t)‚àíŒîtùúñùúï\nùúïx(u2\n2)+(Œît)2\n2ùúñ2ùúï\nùúïx[\nuùúï\nùúïx(u2\n2)]\n. (25.12)",8563
230-25.4.3 Implementation.pdf,230-25.4.3 Implementation,"25.3 Including Dispersion 505\nWenowreplacetheouter xderivativesbycentraldifferencesofspacing Œîx‚àï2:\nu(x,t+Œît)=u(x,t)‚àíŒîtùúñ\n2u2(x+Œîx,t)‚àíu2(x‚àíŒîx,t)\n2Œîx+(Œît)2ùúñ2\n2(25.13)\n√ó1\n2Œîx[\nu(x+Œîx\n2,t)ùúï\nùúïxu2(x+Œîx\n2,t)‚àíu(x‚àíŒîx\n2,t)ùúï\nùúïxu2(x‚àíŒîx\n2,t)]\n.\nNextweapproximate u(x¬±Œîx‚àï2,t)bytheaverageofadjacentgridpoints,\nu(x¬±Œîx\n2,t)‚âÉu(x,t)+u(x¬±Œîx,t)\n2, (25.14)\nandapplyacentral-differenceapproximationtothesecondderivatives:\nùúïu2(x¬±Œîx‚àï2,t)\nùúïx=u2(x¬±Œîx,t)‚àíu2(x,t)\n¬±Œîx. (25.15)\nFinally,puttingallthesederivativestogetheryieldsthealgorithm:\nui,j+1=ui,j‚àíùõΩ\n4(u2\ni+1,j‚àíu2\ni‚àí1,j) (25.16)\n+ùõΩ2\n8[\n(ui+1,j+ui,j)(u2\ni+1,j‚àíu2\ni,j)‚àí(ui,j+ui‚àí1,j)(u2\ni,j‚àíu2\ni‚àí1,j)]\n,\nwherewehavesubstitutedtheCFLnumber ùõΩ.ThisLax‚ÄìWendroffschemeisexplicit,cen-\ntereduponthegridpoints,andstablefor ùõΩ<1(smallnonlinearities).\n25.2.2 Implementation and Assessment\nSolveBurgers‚Äôequation(25.7)viatheleapfrogmethod.\n1) Definearrays u0[100]and u[100]fortheoldandnewwaves.\n2) Taketheinitialwavetobesinusoidal, u0[i]=3sin(3.2x),withspeed c=1.\n3) Incorporatetheboundaryconditions u[0]=0and u[100]=0.\n4) KeeptheCFLnumber ùõΩ<1forstability.\n5) Savetheinitialwaveandthesolutionsforanumberoftimesinseparatefiles,andplot\nthemonthesamegraphinordertoseetheformationofashockwave(likeFigure25.2).\n6) Modify your program to solve Burgers‚Äô equation using the Lax‚ÄìWendroff method\n(25.16),andcompareitwiththeleapfrogmethod.Theleapfrogmethodshouldproduce\nshock waves, but with ripples as the square edge develops. The ripples are numerical\nartifacts. The Lax‚ÄìWendroff method should give a better square edge, although some\nripplesmaystilloccur.\n7) RunthecodeforseveralincreasinglylargeCFLvaluesandcheckifthestabilitycondi-\ntionùõΩ<1iscorrect.\nOurLax‚ÄìWendroffcode AdvecLax.py isgiveninListing25.1.\n25.3 Including Dispersion\nWehavejustseenthatBurgers‚Äôequationhasasolutioninwhichaninitiallysmoothwave\ntransformsintoasquare-edgedshockwave.Asortofinverseofthisis dispersion,inwhich\na waveform disperses, or broadens, as it travel through a medium. Dispersion does not\n506 25 Shock and Soliton Waves\ncausewavestoloseenergyorattenuate,butitdoesleadtoalossofinformationwithtime.\nPhysically, dispersion is important when the propagating medium has structures with a\nspatialregularityequaltosomefractionofawavelength.Mathematically,dispersionmay\narise from terms in the wave equation that contain higher-order space derivatives. For\nexample,considerthewaveform:\nu(x,t)=e¬±i(kx‚àíùúît), (25.17)\ncorrespondingtoaplanewavetravelingtotheright(‚Äútraveling‚Äùbecausethephase kx‚àíùúît\nremainsunchangedifyouincrease xwithtime). Whenthis u(x,t)is substituted intothe\nadvectionequation(25.3),weobtain:\nùúî=ck. (25.18)\nThisequationisa dispersionrelation ,thatis,arelationbetweenfrequency ùúîandwavevector\nk.Becausegroupvelocity ofawaveis:\nùë£g=ùúïùúî\nùúïk, (25.19)\nthelineardispersionrelation(25.18)impliesthatallfrequencieshavethesamegroupveloc-\nityc.Thisisdispersionless propagation.\nLetusnowimaginethatawaveispropagatingwithasmallamountof dispersion,thatis,\nwithafrequencythathassmallcorrectiontothelinearincreasewiththewavenumber:\nùúî‚âÉck‚àíùõΩk3. (25.20)\nNotethatbecausetherearenoevenpowersin(25.20),thegroupvelocity,\nùë£g=dùúî\ndk‚âÉc‚àí3ùõΩk2, (25.21)\nisthesameforwavestravelingtotheleft,ortheright.Nowweworkbackwardsanddeduce\nwhatmightbethewaveequationthatproducedthisdispersion.Ifwehaveaplane-wave\nsolutionlike(25.17),the ùúîterminthedispersionrelation(25.20)wouldarisefromafirst-\nordertimederivative.Likewise,the cktermwouldarisefromafirst-orderspacederivative,\nandthek3termfromathird-orderspacederivative.Andthus,thewaveequation:\nùúïu(x,t)\nùúït+cùúïu(x,t)\nùúïx+ùõΩùúï3u(x,t)\nùúïx3=0. (25.22)\nWeleaveitasanexercisetoshowthatsolutionstothisequationdoindeedhavewaveforms\nthatdisperseintime.\n25.4 KdeV Solitons\nNowwecanputtogetherallthepiecesthatareneededtounderstandtheunusualwater\nwavesthatoccurinshallow,narrowchannelssuchascanals[Abar93,Tab89].Theanalytic\ndescriptionofthis‚Äúheapofwater‚ÄùwasgivenbyKorteweganddeVries[1895]withthe KdeV\nequation:\nùúïu(x,t)\nùúït+ùúÄu(x,t)ùúïu(x,t)\nùúïx+ùúáùúï3u(x,t)\nùúïx3=0. (25.23)\n25.4 KdeV Solitons 507\n0204060x048\nt01212 3 4 5 6 7 8\nFigure 25.3 The formation of a tsunami. A single two-level waveform at time zero progressively\nbreaks up into eight solitons (labeled) as time increases. The tallest soliton (1) is narrower and\nfaster than the others in its motion to the right. You can generate an animation of this with the\nprogram SolitonAnimate.py .\nAswediscussedinSection25.2inourstudyofBurgers‚Äôequation,thenonlinear ùúÄuùúïu‚àïùúït\ntermleadstoasharpeningofthewave,andultimatelya shockwave.Incontrast,asweknow\nfromourdiscussionofdispersion,the ùúï3u‚àïùúïx3termtendstobroadenawaveform,whilethe\nùúïu‚àïùúïttermproducesatravelingwave. Fortheproperparametersandinitialconditions,the\ndispersivebroadeningexactlybalancesthenonlinearnarrowing,andastabletravelingwave,\nasoliton,isformed .\nKorteweganddeVries[1895]solved(25.23)analytically,andprovedthatthespeed(25.1)\ngivenbyRussellis,infact,correct.Seventyyearsafteritsdiscovery,theKdeVequationwas\nrediscoveredbyZabuskyandKruskal[1965],whosolveditnumericallyandfoundthata\nstep-likeinitialconditionbrokeupintoeightsolitarywaves(Figure25.3).Theyalsofound\nthat the parts of the wave with larger amplitudes moved faster than those with smaller\namplitudes.Thisiswhy,atlatertimes,thehigherpeakstendtobeontherightinFigure25.3.\nAs if wonders never cease, Zabusky and Kruskal, who coined the name solitonfor these\nsolitary waves, also observed that a faster peak passed unscathed through a slower one\n(Figure25.1).\n25.4.1 Analytic Solution\nThe trick in analytic approaches to these types of nonlinear equations is to substitute a\nguessedsolutionthathastheformofatravelingwave,\nu(x,t)=u(ùúâ=x‚àíct). (25.24)\nThisformmeansthatifwemovewithaconstantspeed c,wewillseeaconstantwaveform.\nThere is no guarantee that this form of a solution will exist, but it may lead you to one.\nSubstituting(25.24)intotheKdeVequationproducesasolvableODE:\n‚àícùúïu\nùúïùúâ+ùúñuùúïu\nùúïùúâ+ùúád3u\ndùúâ3=0, (25.25)\n‚áíu(x,t)=‚àíc\n2sech2[1\n2‚àö\nc(x‚àíct‚àíùúâ0)], (25.26)\nwhereùúâ0istheinitialphase.Weseeinthesolitonwaveform,(25.26),anamplitudethatis\nproportionaltothewavespeed c,andasech2functionthatgivesasinglelump-likewave.\n508 25 Shock and Soliton Waves\n25.4.2 Algorithm\nTheKdeVequationissolvednumericallyusingafinite-differencescheme,withthetime\nandspacederivativesgivenbycentral-differenceapproximations:\nùúïu\nùúït‚âÉui,j+1‚àíui,j‚àí1\n2Œît,ùúïu\nùúïx‚âÉui+1,j‚àíui‚àí1,j\n2Œîx. (25.27)\nToapproximate ùúï3u(x,t)‚àïùúïx3,weexpand u(x,t)toÓàª(Œît)3aboutthefourpoints u(x¬±2Œîx,t)\nandu(x¬±Œîx,t):\nu(x¬±Œîx,t)‚âÉu(x,t)¬±(Œîx)ùúïu\nùúïx+(Œîx)2\n2!ùúï2u\nùúï2x¬±(Œîx)3\n3!ùúï3u\nùúïx3. (25.28)\nWesolvethisfor ùúï3u(x,t)‚àïùúïx3.Finally,thefactor u(x,t)inthesecondtermof(25.23)istaken\nastheaverageofthree xvalues,allwiththesame t:\nu(x,t)‚âÉui+1,j+ui,j+ui‚àí1,j\n3. (25.29)\nWesubstitutetheseapproximationstoobtainthealgorithmfortheKdeVequation:\nui,j+1‚âÉui,j‚àí1‚àíùúñ\n3Œît\nŒîx[ui+1,j+ui,j+ui‚àí1,j][ui+1,j‚àíui‚àí1,j]\n‚àíùúáŒît\n(Œîx)3[ui+2,j+2ui‚àí1,j‚àí2ui+1,j‚àíui‚àí2,j]. (25.30)\nThe algorithm predicts u(x,t)at future times, given the solutions at the present and\npast times. The initial condition provides ui,1for all positions i.T ofi n dui,2,w eu s et h e\nforward-differenceschemeinwhichweexpand u(x,t),keepingonlytwotermsforthetime\nderivative:\nui,2‚âÉui,1‚àíùúñŒît\n6Œîx[ui+1,1+ui,1+ui‚àí1,1][ui+1,1‚àíui‚àí1,1]\n‚àíùúá\n2Œît\n(Œîx)3[ui+2,1+2ui‚àí1,1‚àí2ui+1,1‚àíui‚àí2,1]. (25.31)\nThekeenobserverwillnotethattherearestillsomeundefinedcolumnsofpoints,namely,\nu1,j,u2,j,uNmax‚àí1,j,anduNmax,j,whereNmaxisthetotalnumberofgridpoints.Asimpletech-\nniquefordeterminingtheirvalueistoassumethat u1,2=1anduNmax,2=0.Toobtain u2,2\nanduNmax‚àí1,2,weassumethat ui+2,2=ui+1,2,andui‚àí2,2=ui‚àí1,2.(However,weavoid ui+2,2for\ni=Nmax‚àí1,andui‚àí2,2fori=2).Wecarryoutthesesteps,approximate(25.31),andthus\nsimplifytherelationto:\nui+2,2+2ui‚àí1,2‚àí2ui+1,2‚àíui‚àí2,2‚âÉui‚àí1,2‚àíui+1,2. (25.32)\nThetruncationerrorandstabilityconditionforthisalgorithmarerelated:\nÓà±(u)=Óàª[(Œît)3]+Óàª[Œît(Œîx)2](Error), (25.33)\nŒît\nŒîx[ùúñ|u|+4ùúá\n(Œîx)2]‚â§1 (Stability) . (25.34)\nThefirstequationimpliesthatsmallertimeandspacestepsleadtoasmallerapproximation\nerror.Yet,asdiscussedinChapter3,ifthestepsmadeareverysmall,thenyouwillneedto\ntakealargenumberofsteps,andthentheround-offerrormaygettoolarge.Somebalance\nisalsoindicatedbythestabilitycondition(25.34),whereweseethatmaking Œîxtoosmall\nleadstoinstability.Someexperimentationisadvised.",8338
231-25.6.1 Analytic Solution.pdf,231-25.6.1 Analytic Solution,"25.4 KdeV Solitons 509\n25.4.3 Implementation\nModifyorruntheprogram Soliton.py inListing19.2thatsolvestheKdeVequation(25.23)\nfortheinitialcondition:\nu(x,t=0)=1\n2[\n1‚àítanh(\nx‚àí25\n5)]\n, (25.35)\nwithparameters ùúñ=0.2andùúá=0.1.Startwith Œîx=0.4andŒît=0.1.Theseconstantssat-\nisfy(25.33)with |u|=1.Thecode SolitonAnimate.py producesananimation.\n1) Define a 2D array u[131,3], with the first index corresponding to the position x,a n d\nthe second to the time t. With our choice of parameters, the maximum value for xis\n130√ó0.4=52.\n2) Initializethetimeto t=0,andassignvaluesto u[i,1].\n3) Assign values to u[i,2],i=3,4,‚Ä¶,129, corresponding to the next time interval. Use\n(25.31) to advance the time, but note that you cannot start at i=1, or end at i=131,\nbecause(25.31)wouldinclude u[132,2]and u[-1,1],whicharebeyondthelimitsofthe\narray.\n4) Increment time, and assume that u[1,2]=1a n d u[131,2] =0. To obtain u[2,2]and\nu[130,2], assume that u[i+2,2] =u[i+1,2]and u[i-2,2] =u[i-1,2].A v o i d u[i+2,2]\nfori=130,a n d u[i-2,2]fori=2. To do this, approximate (25.31) so that (25.33) is\nsatisfied.\n5) Incrementtime,andcompute u[i, j]forj=3,andfor i=3, 4,‚Ñ©, 129,usingequation\n(25.30).Again,followthesameprocedurestoobtainthemissingarrayelements u[2, j]\nand u[130, j](set u[1, j]=1.and u[131, j] =0).Printoutthenumbersduringtheiter-\nations,andcheckthatthesearegoodchoices.\n6) Set u[i,1]=u[i,2],and u[i,2]=u[i,3]forall i.Inthisway,youarereadytofindthe\nnext u[i,j]intermsoftheprevioustworows.\n7) Repeat the previous two steps at least 2000 times. Write your solution in a file after\napproximatelyevery250iterations.\n8) Plotyourresultsasa3Dgraphofdisturbance uversuspositionandtime.\n9) Observethewaveprofileasafunctionoftime,andtrytoconfirmRussell‚Äôsobservation\nthatatallersolitontravelsfasterthanasmallerone.\n25.4.4 Exploration: Phase Space Solitons and Soliton Crossings\n1) Explorewhathappenswhenatallsolitoncollideswithashortone.\n(a) Startbyplacingatallsolitonofheight0.8at x=12andasmallersolitoninfrontof\nitatx=26:\nu(x,t=0)=0.8[\n1‚àítanh2(\n3x\n12‚àí3)]\n+0.3[\n1‚àítanh2(\n4.5x\n26‚àí4.5)]\n.\n(25.36)\n(b) Dothetwosolitonsreflectoffeachother?Dotheygothrougheachother?Dothey\ninterfere?Doesthetallsolitonstillmovefasterthantheshortoneafterthecollision\n(Figure25.1)?\n2) Constructphasespaceplotsof[ Ãáu(t)vsu(t)]forallt‚Äôs.Tryoutvariousparametervalues.\nNotethatonlyspecificsetsofparametersproducesolitons.Inparticular,bycorrelating\n510 25 Shock and Soliton Waves\nthebehaviorofthesolutionswithyourphase-spaceplots,showthatthesolitonsolutions\ncorrespond to the separatrixsolutions to the KdeV equation. In other words, the sta-\nbility in time for solitons is analogous to the infinite period for a pendulum balanced\nstraightup.\n25.5 Pendulum Chain Solitons\nIn 1955, Fermi et al. [1955] published their investigation of how a 1D chain of coupled\noscillators disperses waves. Since waves of differing frequencies traveled through the\nchain with differing speeds, they found that a pulse, which inherently includes a range\noffrequencies,broadensastimeprogresses.Surprisingly,whentheoscillatorsweremade\nmorerealisticbyintroducinganonlineartermintoHooke‚Äôslaw,\nF(x)‚âÉ‚àík(x+ùõºx2), (25.37)\nthe authors found that a sharp pulse could survive indefinitely, even in the presence of\ndispersion.\nProblem Developamodelthatexplainshowacombinationofdispersionandnonlinear-\nitycanleadtoastablepulseinachainofcoupledoscillators.\nWe take a chain of realistic pendulums as our model for coupled oscillators, and in this\nway, extend our study of a nonlinear, single pendulum in Chapter 16. Figure 25.4 shows\nourchainofidentical,equally-spacedpendulums,withthecouplingsbetweenpendulums\nprovidedbytorsionbarsthattwistasthependulumsswing.Theangle ùúÉimeasuresthedis-\nplacement of pendulum ifrom its equilibrium position, and the parameter ais the fixed\ndistance betweenpivotpoints. If allthe pendulums are setoff swingingtogether,thatis,\nwithùúÉi‚â°ùúÉj, the coupling torques would vanish and we would have our old friend, the\nequation for a realistic pendulum. We assume that three torques act on each pendulum,\nagravitationaltorquetryingtoreturnthependulumtoitsequilibriumposition,andtwo\ntorquesfromthetwistingofthebartotherightandleftofthependulum.Theequationof\nmotionforpendulum jfollowsfromNewton‚Äôslawforrotationalmotion:\nId2ùúÉj(t)\ndt2=‚àë\nj‚â†iùúèji, (25.38)\n=‚àíùúÖ(ùúÉj‚àíùúÉj‚àí1)‚àíùúÖ(ùúÉj‚àíùúÉj+1)‚àímgLsinùúÉj, (25.39)\n‚áíId2ùúÉj(t)\ndt2=ùúÖ(ùúÉj+1‚àí2ùúÉj+ùúÉj‚àí1)‚àímgLsinùúÉj. (25.40)\nHere,Iisthemomentofinertiaofeachpendulum, Listhelengthofeachpendulum,and\nùúÖisthetorqueconstantofthebar.Aswithourpreviousstudyoftherealisticpendulum,\naa a a\nŒ∏1 Œ∏2Œ∏3Œ∏4 Œ∏5Figure 25.4 A 1D chain of pendulums, coupled via\ntorsion bars between the pendulums. The pendulums\nswing in planes perpendicular to the length of the bar.\n25.5 Pendulum Chain Solitons 511\nthe nonlinearity in (25.40) arises from the sin ùúÉ‚âÉùúÉ‚àíùúÉ3‚àï6+‚Ä¶dependence of the\ngravitational torque. Equation (25.40) is a set of coupled nonlinear equations, with the\nnumberofequationsequaltothenumberofoscillators,whichwouldbelargeformodelof\narealisticsolid.Nowwewanttoincludesomedispersioninthemotionofthependulums.\n25.5.1 Including Dispersion\nConsiderasurferonthecrestofawave.Sinceshedoesnotseethewaveformchangewith\ntime,herpositionisgivenbyafunctionoftheform f(kx‚àíùúît).Consequently,toher,the\nwavehasaconstantphase:\nkx‚àíùúît=constant. (25.41)\nThesurfer‚Äôsphasevelocityisthusconstant:\nùë£p=dx\ndt=ùúî\nk. (25.42)\nIngeneral,thefrequency ùúîmayhaveanonlineardependenceon k,andthisleadstothe\nphasevelocityvaryingwithfrequency,and,consequently, dispersion.Ifthewavewasapulse\ncontainingarangeofFouriercomponents,thenitwouldbroadenandchangeshapeintime,\naseachfrequencymoveswithadifferentvelocity.So,althoughdispersiondoesnotleadto\nenergyloss,itleadstoalossofinformationaspulsesbroadenandoverlap.\nThefunctionalrelationbetweenfrequency ùúîandthewavevector kis,ofcourse,a dis-\npersionrelation .IftheFouriercomponentsinawavepacketarecenteredaroundamean\nfrequency ùúî0,thenthepulse‚Äôsinformationtravels,notwiththephasevelocity ùë£p,butwith\nthegroupvelocity :\nùë£g=ùúïùúî\nùúïk||||ùúî=ùúî0. (25.43)\nAcomparisonof(25.42)and(25.43)makesitclearthatwhenthereisdispersion,groupand\nphasevelocitiesmaywelldiffer.\nToisolatethepuredispersiveaspectof(25.40),weexamineitslinearversion:\nd2ùúÉj(t)\ndt2+ùúî2\n0ùúÉj(t)=ùúÖ\nI(ùúÉj+1‚àí2ùúÉj+ùúÉj‚àí1), (25.44)\nwhereùúî0=‚àö\nmgL‚àïIisthenaturalfrequencyofanyonependulum.Wewanttodetermine\nif a wave with a single frequency can propagate on this chain. To do that, we assume a\ntravelingwavewithfrequency ùúîandwavelength ùúÜ,\nùúÉj(t)=Aei(ùúît‚àíkxj),k=2ùúã\nùúÜ. (25.45)\nSubstitution of (25.45) into the wave equation (25.44) produces the dispersion relation\n(Figure25.5):\nùúî2=ùúî2\n0‚àí2ùúÖ\nI(1‚àícoska),(dispersionrelation) . (25.46)\nIndispersionlesspropagation,allfrequenciespropagatewiththesamevelocity c.T ohave\nthat,weneedalinearrelationbetween ùúîandk:\nùúÜ=c2ùúã\nùúî‚áíùúî=ck,(dispersionlesspropagation) . (25.47)\n512 25 Shock and Soliton Waves\nkœÄ/aœâ0œâ*œâ\n‚ÄìœÄ/aFigure 25.5 The dispersion relation for a linearized chain of\npendulums.\nThiswillbetrueforthechainonlyif kaissmallenoughforcos ka‚âÉ1,inwhichcase ùúî‚âÉùúî0.\nNotonlydoesthedispersionrelation(25.46)changethespeedofwaves,itactuallylimits\nwhichfrequenciescanpropagateonthechain.Inordertohavereal ksolutions, ùúîmustlie\nintherange:\nùúî0‚â§ùúî‚â§ùúî‚àó(wavespropagation). (25.48)\nTheminimumfrequency ùúî0andthemaximumfrequency ùúî‚àóarerelatedthroughthelimits\nofcoskain(25.46),\n(ùúî‚àó)2=ùúî2\n0+4ùúÖ\nI. (25.49)\nWaveswith ùúî<ùúî0donotpropagate,whilethosewith ùúî>ùúî‚àóarenonphysicalbecausethey\ncorrespondtowavelengths ùúÜ<2a,thatis,oscillationswheretherearenoparticles.These\nhighandlow ùúîcutoffschangetheshapeofapropagatingpulse,thatis,causedispersion.\n25.6 Continuum Limit, the Sine-Gordon Equation\nIf the wavelengths in a pulse are longer than the distance abetween pendulums, then\nka‚â™1, and the chain can be approximated as a continuous medium. In this limit,\nabecomes the continuous variable x,a n dt h es y s t e mo fc o u p l e d ordinarydifferential\nequationsbecomesasingle, partialdifferentialequation:\nùúÉj+1‚âÉùúÉj+ùúïùúÉ\nùúïxŒîx, (25.50)\n‚áí(ùúÉj+1‚àí2ùúÉj+ùúÉj‚àí1)‚âÉùúï2ùúÉ\nùúïx2Œîx2‚â°ùúï2ùúÉ\nùúïx2a2, (25.51)\n‚áíùúï2ùúÉ\nùúït2‚àíùúÖa2\nIùúï2ùúÉ\nùúïx2=mgL\nIsinùúÉ. (25.52)\nIfwemeasuretimeinunitsof‚àö\nI‚àïmgLanddistancesinunitsof‚àö\nùúÖa‚àï(mgLb),weobtain\nthestandardformofthesine-Gordonequation(SGE):1\n1\nc2ùúï2ùúÉ\nùúït2‚àíùúï2ùúÉ\nùúïx2=sinùúÉ (SGE). (25.53)\nThesinùúÉontheRHSintroducesnonlineareffects.\n1 Thename‚Äúsine-Gordon‚ÄùiseitherareminderthattheSGEisliketheKlein-Gordonequationof\nrelativisticquantummechanicswithasin uaddedtotheRHS,orareminderofhowcleveronecanbein\nthinkingupnames.",8554
232-25.6.3 Implementation.pdf,232-25.6.3 Implementation,"25.6 Continuum Limit, the Sine-Gordon Equation 513\n25.6.1 Analytic Solution\nAlthoughsimplelooking,thenonlinearityofthesine-GordonPDE(25.53)makesithardto\nsolveanalytically.Thereis,however,atrick:aswedidforsolitons,guessafunctionalform\nofatravelingwave,substituteitinto(25.53),andtherebyconvertthePDEintoasolvable\nODE:\nùúÉ(x,t)?=ùúÉ(ùúâ=t¬±x‚àïùë£), ‚áíd2ùúÉ\ndùúâ2=ùë£2\nùë£2‚àí1sinùúÉ. (25.54)\nYoushouldrecognize(25.54)asouroldfriend,theequationofmotionforarealisticpen-\ndulumwithnodrivingforceandnofriction.Theconstant ùë£isavelocityinnaturalunits,\nandseparatesdifferentregimesofthemotion:\nùë£<1‚à∂pendulainitiallydown ‚Üì‚Üì‚Üì‚Üì‚Üì(stable),\nùë£>1‚à∂pendulainitiallyup ‚Üë‚Üë‚Üë‚Üë‚Üë(unstable).(25.55)\nAlthoughtheequationisfamiliar,weknowthatananalyticsolutiondoesnotexists.How-\never,foranenergy E=¬±1,wehaveseparatrixmotion,andasolutionwithcharacteristic\nsolitonform,\nùúÉ(x‚àíùë£t)=‚éß\n‚é™\n‚é®\n‚é™‚é©4tan‚àí1(\nexp[\n+x‚àíùë£t‚àö\n1‚àíùë£2])\n,forE=1,\n4tan‚àí1(\nexp[\n‚àíx‚àíùë£t‚àö\n1‚àíùë£2])\n+ùúã,forE=‚àí1.(25.56)\nThis soliton corresponds to a solitary kink, traveling with velocity ùë£=‚àí1, that flips the\npendulumsaroundby2 ùúãasitmovesdownthechain.Thereisalsoan antikinkinwhich\ntheinitial ùúÉ=ùúãvaluesareflippedtofinal ùúÉ=‚àíùúã.\n25.6.2 Numeric 2D Solitons (Pulsons)\nIttookabitofmanipulation,butwehavealreadyfoundhowtosolvetheKdeVequation\nfor1Dsolitons.Theelastic-wavesolitonsthatarisefromtheSGEcanbeeasilygeneralized\ntotwodimensions,aswedoherewiththe2DgeneralizationoftheSGEequation(25.53):\n1\nc2ùúï2u\nùúït2‚àíùúï2u\nùúïx2‚àíùúï2u\nùúïy2=sinu(2DSGE) . (25.57)\nWhereasthe1DSGEdescribeswavepropagationalongachainofconnectedpendulums,\nthe 2D form might describe wave propagation in nonlinear elastic media. Interestingly\nenough,thesame2DSGEalsooccursinquantumfieldtheory,wherethesolitonsolutions\nhavebeensuggestedasmodelsforelementaryparticles[ChristiansenandLomdahl,1981;\nChristiansenandOlsen,1979].Theideaisthat,likeelementaryparticles,thesolutionsare\nconfinedtoaregionofspaceforalongperiodoftimebynonlinearforces,anddonotradiate\nawaytheirenergy.\nWesolve(25.57)inthefiniteregionof2Dspaceandforpositivetimes:\n‚àíx0<x<x0,‚àíy0<y<y0,t‚â•0. (25.58)\nWetakex0=y0=7,andimposethe boundaryconditions thatthederivativeofthedisplace-\nmentvanishesattheendsoftheregion:\nùúïu\nùúïx(‚àíx0,y,t)=ùúïu\nùúïx(x0,y,t)=ùúïu\nùúïy(x,‚àíy0,t)=ùúïu\nùúïy(x,y0,t)=0. (25.59)\n514 25 Shock and Soliton Waves\nWe also impose the initial condition that at time t=0 the waveform is that of a pulse\n(Figure25.6)withitssurfaceatrest:\nu(x,y,t=0)=4tan‚àí1(exp3‚àí‚àö\nx2+y2),ùúïu\nùúït(x,y,t=0)=0.(25.60)\nWediscretizetheequationbylookingforsolutionsonaspace-timelattice:\nx=mŒîx,y=lŒîx,t=nŒît, (25.61)\nu(n)\nm,ldef=u(mŒîx,lŒîx,nŒît). (25.62)\nNext,wereplacethederivativesin(25.57)bytheirfinite-differenceapproximations:\nu(n+1)\nm,l‚âÉ‚àíu(n‚àí1)\nm,l+2[\n1‚àí2(\nŒît\nŒîx)2]\nu(n)\nm,l(25.63)\n+(\nŒît\nŒîx)2\n(u(n)\nm+1,l+u(n)\nm‚àí1,l+u(n)\nm,l+1+u(n)\nm,l‚àí1)\n‚àíŒît2sin[\n1\n4(u(n)\nm+1,l+u(n)\nm‚àí1,l+u(n)\nm,l+1+u(n)\nm,l‚àí1)]\n.\nTomakethealgorithmsimplerandestablishstability,weassumethattimeandspacesteps\nareproportional, Œît=Œîx‚àï‚àö\n2.Thisleadstoallofthe u(n)\nm,ltermsdroppingout:\nu(2)\nm,l‚âÉ1\n2(u(1)\nm+1,l+u1\nm‚àí1,l+u(1)\nm,l+1+u(1)\nm,l‚àí1)\n‚àíŒît2\n2sin[\n1\n4(u(1)\nm+1,l+u(1)\nm‚àí1,l+u(1)\nm,l+1+u(1)\nm,l‚àí1)]\n. (25.64)\nLikewise,thediscreteformofvanishinginitialvelocity(25.60)becomes:\nùúïu(x,y,0)‚àïùúït=0 ‚áíu(2)\nm,l=u(0)\nm,l. (25.65)\nThevaluesonlatticepointsontheedgesandcornerscannotbeobtainedfromtheserela-\ntions,but,instead,areobtainedbyapplyingtheboundaryconditions(25.59):\nùúïu\nùúïz(x0,y,t)=u(x+Œîx,y,t)‚àíu(x,y,t)\nŒîx=0, (25.66)\n‚áíu(n)\n1,l=u(n)\n2,l. (25.67)\nSimilarly,theotherderivativesin(25.59)give:\nu(n)\nNmax,l=u(n)\nNmax‚àí1,l,u(n)\nm,2=u(n)\nm,1,u(n)\nm,Nmax=u(n)\nm,Nmax‚àí1, (25.68)\nwhereNmaxisthenumberofgridpointsusedforonespacedimension.\n25.6.3 Implementation\n1) Define an array u[Nmax,Nmax,3]withNmax=201 for the space points, and 3 for the\ntimepoints.\n2) Placethesolution(25.60)fortheinitialtimein u[m,l,1].\n3) Placethesolutionforthesecondtime Œîtinu[m,l,2],andthesolutionfortime2 Œîtin\nu[m,l,3].\n25.6 Continuum Limit, the Sine-Gordon Equation 515\n4) Assignvaluesfortheconstants, Œîx=Œîy=7‚àï100,Œît=Œîx‚àï‚àö\n2,andy0=x0=7.\n5) Start the solution t=0 with the initial conditions, which, along with the boundary\nconditionsdefinesitovertheentirelattice.\n6) Forthesecondtimestep,use(25.64)forallpointsonthelatticeincreasetimeby Œît,\nbutdonotincludetheedges.\n7) Attheedges,for i=1,2,‚Ä¶,200,set:\nu[i,1,2]=u[i,2,2],u[i,Nmax,2]=u[i,Nmax‚àí1,2]\nu[1,i,2]=u[2,i,2],u[Nmax,i,2]=u[Nmax‚àí1,i,2].\n8) To find values for the four points in the corners for the second time step, again use\ninitialcondition(25.64):\nu[1,1,2]=u[2,1,2],u[Nmax,Nmax,2]=u[Nmax‚àí1,Nmax‚àí1,2],\nu[1,1,Nmax]=u[2,Nmax,2],u[Nmax,1,2]=u[Nmax‚àí1,1,2].\n9) Forthethirdtimestep(thefuture),usethefullalgorithm(25.66).\n10) Continuethepropagationforwardintime,reassigningthefuturetothepresent,and\ndetermininganewfuture.Inthisway,youneedtostorethesolutionsforonlythree\ntimesteps.\nWe see in Figure 25.6, the time evolution of a circular ring soliton resulting from the\nstated initial conditions. We note that the ring at first shrinks in size, then expands, and\nthenshrinksbackintoanother(butnotidentical)ringsoliton.Asmallamountofthepar-\nticledoesradiateaway,andinthelastframewecannoticesomeinterferencebetweenthe\nradiationandtheboundaryconditions.Ananimationofthissequencecanbefoundonline.\nFigure 25.6 A circular ring soliton at times 8, 20, 40, 60, 80, and 120. This type of entity has been\nproposed as a model for an elementary particle. (Created with TwoDsol.java .)",5488
233-25.7 Code Listings.pdf,233-25.7 Code Listings,"516 25 Shock and Soliton Waves\n25.7 Code Listings\nListing 25.1 AdvecLax.py solvestheadvectionequationviatheLax‚ÄìWendroffscheme.\n# AdvecLax . py : Solve advection eqnt via Lax ‚àíWendroff scheme\n#d u / d t +c ‚àód(u‚àó‚àó2/2)/dx=0; u(x, t=0)=exp( ‚àí300(x‚àí0.12) ‚àó‚àó2)\n4fromvpython import ‚àó\nm = 100 # N o steps in x\nc=1 . ; d x=1 . / m ; b e t a=0 . 8 # beta = c ‚àódt/dx\nu=[ 0 ] ‚àó(m+1); # I n i t i a l Numeric\n8u0 = [0] ‚àó(m+1);\nuf = [0] ‚àó(m+1)\ndt = beta ‚àódx/c;\nT_final = 0.5;\n12n=int(T_final/dt) # N time steps\ngraph1 = graph(width=600, height=500, xtitle = ‚Äôx‚Äô, xmin=0,xmax=1,\nymin=0, ymax=1, ytitle = ‚Äôu(x), Cyan=exact, Yellow=Numerical‚Äô ,\n16 title= ‚ÄôAdvect Eqn: Initial (red), Exact (cyan),Numerical (yellow)‚Äô )\ninitfn = gcurve(color = color.red);\nexactfn = gcurve(color = color.cyan)\nnumfn = gcurve(color = color.yellow) # Numerical solution\n20\ndefplotIniExac(): # Plot initial &exact solution\nforiin range (0, m):\nx=i ‚àódx\n24 u0[i] = exp( ‚àí300.‚àó(x‚àí0.12) ‚àó‚àó2) # Gaussian initial\ninitfn.plot(pos = (0.01 ‚àói, u 0[i]) ) # Initial function\nuf[i] = exp( ‚àí300.‚àó(x‚àí0.12‚àíc‚àóT_final) ‚àó‚àó2)# Exact = cyan\nexactfn.plot(pos = (0.01 ‚àói, uf[i]) )\n28 rate(50)\nplotIniExac()\ndefnumerical(): # Finds Lax Wendroff solution\n32forjin range (0, n+1): # Time loop\nforiin range (0, m‚àí1): #x l o o p\nu[i + 1] = (1. ‚àíbeta ‚àóbeta) ‚àóu0[i+1] ‚àí(0.5‚àóbeta) ‚àó(1.‚àíbeta) ‚àóu0[i+2]\n+(0.5 ‚àóbeta) ‚àó(1. + beta) ‚àóu0[i] # Algorithm\n36 u[0] = 0.; u[m ‚àí1] = 0.; u0[i] =u[i]\nnumerical()\nforjin range (0, m‚àí1) :\nrate(50)\n40numfn.plot(pos = (0.01 ‚àój, u[j]) ) # Plot numeric soltn\nListing 25.2 Soliton.py solves the KdeV equation for 1D solitons corresponding to a\n‚Äúbore‚Äùinitialconditions.\n# Soliton . py : Korteweg de Vries equation for a soliton\n2\nfromvisualimport ‚àó\nimportmatplotlib.pylab as p;\nfrommpl_toolkits.mplot3d importAxes3D ;\n6importnumpy\nds = 0.4; dt = 0.1; max= 2000; mu = 0.1; eps = 0.2; mx = 131\nu= z e r o s ( ( m x , 3 ) , float); spl = zeros( (mx, 21), float); m=1\n10\nforiin range (0, 131): # Initial w a v e\nu[i, 0] = 0.5 ‚àó(1‚àí((math.exp(2 ‚àó(0.2‚àóds‚àói‚àí5.))‚àí1)/(math.exp(2 ‚àó(0.2‚àóds‚àói‚àí5.))+1)))\nu[0,1] = 1. ; u[0,2] = 1.; u[130,1] = 0. ; u[130,2] = 0. # End points\n14\nforiin range (0, 131, 2): spl[i, 0] = u[i, 0]\nfac = mu ‚àódt/(ds ‚àó‚àó3)\nprint(""Working. Please hold breath and wait while I count to 20"" )\n18foriin range (1, mx‚àí1): # First time step\n25.7 Code Listings 517\na1 = eps ‚àódt‚àó(u[i + 1, 0] + u[i, 0] + u[i ‚àí1, 0])/(ds ‚àó6.)\nifi>1andi<129: a2 = u[i+2,0]+2. ‚àóu[i‚àí1,0]‚àí2.‚àóu[i+1,0] ‚àíu[i‚àí2,0]\nelse:a 2 = u [ i ‚àí1, 0]‚àíu[i+1, 0]\n22a3 = u[i+1, 0] ‚àíu[i‚àí1, 0]\nu[i, 1] = u[i, 0] ‚àía1‚àóa3‚àífac‚àóa2/3.\nforjin range (1,max+1): # Next time steps\nforiin range (1, mx‚àí2):\n26 a1 = eps ‚àódt‚àó(u[i + 1, 1] + u[i, 1] + u[i ‚àí1, 1])/(3. ‚àóds)\nifi>1andi<mx‚àí2:\na2 = u[i+2,1] + 2. ‚àóu[i‚àí1,1]‚àí2.‚àóu[i+1,1] ‚àíu[i‚àí2,1]\nelse:a 2 = u [ i ‚àí1, 1]‚àíu[i+1, 1]\n30 a3 = u[i+1, 1] ‚àíu[i‚àí1, 1]\nu[i, 2] = u[i,0] ‚àía1‚àóa3‚àí2.‚àófac‚àóa2/3.\nifj%100 == 0: # Plot every 100 time steps\nforiin range (1, mx ‚àí2): spl[i, m] = u[i, 2]\n34 print(m)\nm=m+1\nforkin range (0, mx): # Recycle array saves memory\nu[k, 0] = u[k, 1]\n38 u[k, 1] = u[k, 2]\nx=list(range(0, mx, 2)) # Plot every other point\ny=list(range(0, 21)) # Plot 21 lines every 100 t steps\n42X, Y = p.meshgrid(x, y)\ndeffunctz(spl):\nz=s p l [ X ,Y ]\n46returnz\nfig = p.figure() # create figure\nax = Axes3D(fig) #p l o ta x e s\n50ax.plot_wireframe(X, Y, spl[X, Y], color = ‚Äôr‚Äô) # red wireframe\nax.set_xlabel( ‚ÄôPositon‚Äô ) # label axes\nax.set_ylabel( ‚ÄôTime‚Äô)\nax.set_zlabel( ‚ÄôDisturbance‚Äô )\n54p.show() # Show figure , close Python shell\nprint(""That‚Äôs all folks!"" )",3600
234-Chapter 26 Fluid Hydrodynamics.pdf,234-Chapter 26 Fluid Hydrodynamics,,0
235-26.1 NavierStokes Equation.pdf,235-26.1 NavierStokes Equation,"518\n26\nFluid Hydrodynamics\nWe have already covered some Ô¨Çuid dynamics in our discussion of shallow-water solitons in\nChapter 25. This chapter examines the more general equations of Ô¨Çuid dynamics and their\nnumerical solutions.1The equations are nonlinear, yet have striking similarities to those of\nE&M, and support elegant mathematical and computational treatments. Analytic solutions,\nhowever, are rare, which helps explain why computation Ô¨Çuid dynamics (CFD) is an important\nspecialty (think airplanes). We recommend [Fetter and Walecka, 1980; Landau and Lifshitz,\n1987] for those interested in the derivations, and Shaw [1992] for more details about the\ncomputations .\nInorderformigratingsalmontohaveaplacetorestduringtheirarduousupstreamjourney,\ntheOregonDepartmentofEnvironmentwantstoplaceobjectsinseveraldeep,wide,fast-\nflowingstreams.Onesuchobjectisalongbeamofrectangularcrosssection(Figure26.1\nleft),andanotherisasetofplates(Figure26.1right).Theobjectsaretobeplacedfarenough\nbelowthewater‚Äôssurfacesoasnottodisturbthesurfaceflow,andalsofarenoughfromthe\nbottomofthestreamsoasnottodisturbtheflowthere.\nProblem Determinehowthesizeandlocationofthebeamandplatesaffectthestream‚Äôs\nvelocityprofile.\n26.1 Navier‚ÄìStokes Equation\nAswithourstudyofshallow-watersolitons,weassumethatwateris incompressible with\nconstantdensity ùúå.Theproblemdescriptionimpliesthatwecanassumeasteadystate,but\nnotthatwecanignorefriction( viscosity).Asbefore,thefirstequationofhydrodynamicsis\nthecontinuityequation(25.2):\nùúïùúå(x,t)\nùúït+‚àá ‚ãÖj=0, jdef=ùúåv(x,t). (26.1)\n1 WeacknowledgesomehelpfulreadingandcommentsbySatoruS.Kano.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n26.1 Navier‚ÄìStokes Equation 519\nRiver River\nxxyy\nLH\nLSurface\nBottom BottomSurface\nFigure 26.1 Side view of the Ô¨Çow of a stream around a submerged beam ( left) and around two\nparallel plates ( right). Both beam and plates have length Lalong the direction of Ô¨Çow. The Ô¨Çow is\nseen to be symmetric about the centerline and to be unaffected at the bottom and at the surface by\nthe submerged object.\nThe second equation of hydrodynamics employs a special time derivative, the hydrody-\nnamicderivativeD v‚àïDt[FetterandWalecka,1980]:\nDv\nDtdef=(v‚ãÖ‚àá)v+ùúïv\nùúït. (26.2)\nThisderivativegivestherateofchange,asviewedfromastationaryframe,ofthevelocity\nof material within anelementofflowingfluid . It thus incorporates changes as a result of\nthe motion of the fluid (first term), as well as any explicit time dependence of the veloc-\nity(secondterm).Itisparticularlynoteworthythat Dv‚àïDtissecondorderinvelocity,and\nassuchitintroducesnonlinearitiesintothetheory.Youmaythinkofthesenonlinearitiesas\narisingfromfictitious(inertial)forcesthatoccurwhendescribingthemotionofanelement\ninthefluid‚Äôsrestframe,which,ingeneral,isanacceleratingframe.\nThematerialderivativeistheleadingterminthe Navier‚ÄìStokesequation :\nDv\nDt=ùúà‚àá2v‚àí1\nùúå‚àáP(ùúå,T,x), (26.3)\nwhereùúàisthekinematicviscosityand Pisthepressure.Thoughlesseleganttolookat,the\ncomputationalsolutionisbasedon(26.3)‚ÄôsCartesianform:\nùúïùë£x\nùúït+z‚àë\nj=xùë£jùúïùë£x\nùúïxj=ùúàz‚àë\nj=xùúï2ùë£x\nùúïx2\nj‚àí1\nùúåùúïP\nùúïx,\nùúïùë£y\nùúït+z‚àë\nj=xùë£jùúïùë£y\nùúïxj=ùúàz‚àë\nj=xùúï2ùë£y\nùúïx2\nj‚àí1\nùúåùúïP\nùúïy, (26.4)\nùúïùë£z\nùúït+z‚àë\nj=xùë£jùúïùë£z\nùúïxj=ùúàz‚àë\nj=xùúï2ùë£z\nùúïx2\nj‚àí1\nùúåùúïP\nùúïz.\nThe Navier‚ÄìStokes equation describes the transfer of the momentum of the fluid within\nsomeregionofspaceasaresultof(i)theforcesonthefluid(think dp‚àïdt=F),and(ii)the\nfluidflow.The v‚ãÖ‚àávterminDv‚àïDtdescribesthetransportofmomentuminsomeregion\nof space resulting from the fluid‚Äôs flow, and it is often called the convection oradvection",3664
236-26.2 Flow Through Parallel Plates.pdf,236-26.2 Flow Through Parallel Plates,"520 26 Fluid Hydrodynamics\nterm.2The‚àáPtermdescribesthevelocitychangeresultingfrompressurechanges,andthe\nùúà‚àá2vtermdescribesthevelocitychangeresultingfromviscousforcesthattendtoimpede\ntheflow.\nTheexplicitfunctionaldependenceofthepressureonthefluid‚Äôsdensityandtemperature,\nP(ùúå,T,x),isknownasthe equationofstateofthefluid ,anditisassumedtobeknownbefore\ntryingtosolvetheNavier‚ÄìStokesequation.Tokeepourproblemsimple,weassumethatthe\npressureisindependentofdensityandtemperature.Thisleavesuswithfoursimultaneous\npartialdifferentialequationstosolve,thecontinuityequation(26.1),andtheNavier‚ÄìStokes\nequation (26.3). Because our problem is one with steady state flow, we will set all time\nderivativesofthevelocitytozero.Becausewaterisincompressible,thetimederivativeof\nthedensityalsovanishes.Equations(26.1)and(26.3)thenbecome:\n‚àá‚ãÖv‚â°‚àë\niùúïùë£i\nùúïxi=0, (26.5)\n(v‚ãÖ‚àá)v=ùúà‚àá2v‚àí1\nùúå‚àáP. (26.6)\nThefirstequationexpressestheequalityofinflowandoutflow,andisknownasthe condi-\ntionofincompressibility .Inasmuchasthestreaminourproblemismuchwiderthanthe\nwidthofthebeam,andbecausewewantasolutioninthemiddleofthestream,andnot\nnearthebanks,wewillignorethe zdependenceofthevelocity.TheexplicitPDE‚Äôsweneed\ntosolvethenreduceto:\nùúïùë£x\nùúïx+ùúïùë£y\nùúïy=0, (26.7)\nùúà(ùúï2ùë£x\nùúïx2+ùúï2ùë£x\nùúïy2)\n=ùë£xùúïùë£x\nùúïx+ùë£yùúïùë£x\nùúïy+1\nùúåùúïP\nùúïx, (26.8)\nùúà(\nùúï2ùë£y\nùúïx2+ùúï2ùë£y\nùúïy2)\n=ùë£xùúïùë£y\nùúïx+ùë£yùúïùë£y\nùúïy+1\nùúåùúïP\nùúïy. (26.9)\n26.2 Flow Through Parallel Plates\nTheparallelplateproblemisoneofthefewthathaveanalyticsolutions.Yetsincewestill\nhaveplentyofworktodoinordertosetupthenumericalsolution,wewillskipthedetails\nandjustgivetheresultweneedforcomparison:thereisaparabolicvelocityprofile:\nùúåùúàùë£x(y)=1\n2ùúïP\nùúïx(y2‚àíyH). (26.10)\nAuniquesolutiontothePDEs(26.7)‚Äì(26.9)requiresknowledgeofappropriateboundary\nconditions.Asfaraswecantell,settingboundaryconditionsissomewhatofanacquired\nskill.Weassumethatthesubmergedparallelplatesareplacedinastreamthatisflowing\nwithaconstantvelocity V0inthehorizontaldirection(Figure26.1right).Ifthevelocity V0\nisnottoohigh,orifthekinematicviscosity ùúàissufficientlylarge,thentheflowshouldbe\n2 WediscussadvectioninSection25.1.Inoceanologyormeteorology,convectionimpliesthetransferof\nmassintheverticaldirectionwhereitovercomesgravity,whereasadvectionreferstotransferinthe\nhorizontaldirection.\n26.2 Flow Through Parallel Plates 521\nL\nSolid wall\nSymmetry\nplane\nOutletInlet\nvx = vy = 0vy = 0\nvy = 0vx = V0\ndvi/dy = 0\ndvi/dx = 0P = 0\nxy\nH\nFigure 26.2 The boundary conditions for two thin submerged plates. The surrounding box is the\nintegration volume within which we solve the PDE‚Äôs, and upon whose surface we impose the\nboundary conditions. In practice the box would be much larger than LandH.\nsmoothandwithoutturbulence.Suchflowiscalled laminar.Typically,afluidundergoing\nlaminarflowmovesinsmoothpathsthatdonotcloseonthemselves,liketheflowofwater\nfromafaucet.Ifweimagineattachingavectortoeachelementofthefluid,thenthepath\nsweptoutbythatvectoriscalleda streamline,orlineofmotion ,ofthefluid.Thesestream-\nlinescanbevisualizedexperimentallybyaddingcoloreddyetothestream.Weassumethat\ntheplatesaresothinthattheflowthroughandaroundthemremainslaminar.\nIf the plates are thin, then the flow far upstream of them will not be affected, and we\ncanlimitoursolutionspacetotherectangularregioninFigure26.2.Weassumethatthe\nlengthLandseparation Hoftheplatesaresmallcomparedtothesizeofthestream,sothe\nflowreturnstouniformaswegetfardownstreamfromtheplates.AsseeninFigure26.2,\nthereareboundaryconditionsatthe inlet,wherethefluidentersthesolutionspace,atthe\noutlet,whereitleaves,andatthestationaryplates,whereitjustpassesthrough.Inaddition,\nbecausetheplatesarefarfromthestream‚Äôsbottomandsurface,weassumethatthedotted-\ndashedcenterlineisaplaneofsymmetry,withidenticalflowaboveandbelowtheplane.\nWethushavefourdifferenttypesofboundaryconditionstoimposeonoursolution:\nSolid plates: In as much as there is friction (viscosity) between the fluid and the plate\nsurface, the only way to have laminar flow is to have the fluid‚Äôs velocity equal to the\nplate‚Äôsvelocity,whichmeansbotharezero:\nùë£x=ùë£y=0. (26.11)\nSuchbeingthecase,wehavesmoothflowinwhichthenegligiblythinplatesliealong\nstreamlinesofthefluid(likea‚Äústreamlined‚Äùvehicle).\nInlet:The fluid enters the integration domain at the inlet with a horizontal velocity V0.\nBecausetheinletisfarupstreamfromtheplates,weassumethatthefluidvelocityatthe\ninletisunchangedbythepresenceoftheplates:\nùë£x=V0,ùë£y=0. (26.12)\nOutlet:The fluid leaves the integration domain at the outlet. While it is totally reason-\nabletoassumethatthefluidreturnstoitsunperturbedstatethere,wearenotsurewhat\nthatmightbe.So,instead,weassumethatthereisaphysicaloutletattheendwiththe\nwaterjustshootingoutofit.Thismeansthatthewaterpressureequalszeroattheoutlet",4759
237-26.3.1 Successive Overrelaxation Algorithm.pdf,237-26.3.1 Successive Overrelaxation Algorithm,"522 26 Fluid Hydrodynamics\n(as at the end of a garden hose), and that the velocity does not change in a direction\nnormaltotheoutlet:\nP=0,ùúïùë£x\nùúïx=ùúïùë£y\nùúïx=0. (26.13)\nSymmetry plane: Iftheflowissymmetricaboutthe y=0plane,thentherecannotbeflow\nthroughtheplane,whichmeansthatthespatialderivativesofthevelocitycomponents\nnormaltotheplanemustvanish:\nùë£y=0,ùúïùë£y\nùúïy=0. (26.14)\nThisconditionfollowsfromtheassumptionthattheplatesarealongstreamlinesandthat\ntheyarenegligiblythin.Itmeansthatallthestreamlinesareparalleltotheplates,aswell\nastothewatersurface,andsoitmustbethat ùë£y=0everywhere.Thefluidentersinthe\nhorizontaldirection,theplatesdonotchangethevertical ycomponentofthevelocity,\nandtheflowremainssymmetricaboutthecenterline.Thereisaretardationoftheflow\naroundtheplatesasaresultoftheviscousnatureoftheflow,andasaresultofthe v=0\nboundarylayersformedontheplates,buttherearenoactual ùë£ycomponents.\n26.3 Navier‚ÄìStokes Difference Equation\nNowwedevelopthedifferenceequationformsoftheNavier‚ÄìStokesandcontinuityPDEs.\nThey will be solved with successive overrelaxation , a variation of the method used in\nChapter21tosolvePoisson‚Äôsequation.Westartbydividingspaceintoarectangulargrid\nwithspacing hinboththe xandydirections:\nx=ih,i=0,‚Ä¶,Nx;y=jh,j=0,‚Ä¶,Ny.\nNext,weusethecentral-differenceapproximationtoexpressthederivativesin(26.7)‚Äì(26.9)\nas finite differences of the values of the velocities at the grid points. For ùúà=1m2‚àïsa n d\nùúå=1kg/m3,thisyields:\nùë£(x)\ni+1,j‚àíùë£(x)\ni‚àí1,j+ùë£(y)\ni,j+1‚àíùë£(y)\ni,j‚àí1=0, (26.15)\nùë£(x)\ni+1,j+ùë£(x)\ni‚àí1,j+ùë£(x)\ni,j+1+ùë£(x)\ni,j‚àí1‚àí4ùë£(x)\ni,j(26.16)\n=h\n2ùë£(x)\ni,j[ùë£(x)\ni+1,j‚àíùë£(x)\ni‚àí1,j]+h\n2ùë£(y)\ni,j[ùë£(x)\ni,j+1‚àíùë£(x)\ni,j‚àí1]+h\n2[Pi+1,j‚àíPi‚àí1,j],\nùë£(y)\ni+1,j+ùë£(y)\ni‚àí1,j+ùë£(y)\ni,j+1+ùë£(y)\ni,j‚àí1‚àí4ùë£(y)\ni,j(26.17)\n=h\n2ùë£(x)\ni,j[ùë£(y)\ni+1,j‚àíùë£(y)\ni‚àí1,j]+h\n2ùë£(y)\ni,j[ùë£(y)\ni,j+1‚àíùë£(y)\ni,j‚àí1]+h\n2[Pi,j+1‚àíPi,j‚àí1].\nBecauseùë£(y)‚â°0,wecansolvefor ùë£(x):\n4ùë£(x)\ni,j=ùë£(x)\ni+1,j+ùë£(x)\ni‚àí1,j+ùë£(x)\ni,j+1+ùë£(x)\ni,j‚àí1‚àíh\n2ùë£(x)\ni,j[ùë£(x)\ni+1,j‚àíùë£(x)\ni‚àí1,j]\n‚àíh\n2ùë£(y)\ni,j[ùë£(x)\ni,j+1‚àíùë£(x)\ni,j‚àí1]‚àíh\n2[Pi+1,j‚àíPi‚àí1,j]. (26.18)",2052
238-26.4 Vorticity Form of NavierStokes Equation.pdf,238-26.4 Vorticity Form of NavierStokes Equation,"26.4 Vorticity Form of Navier‚ÄìStokes Equation 523\nWe recognize (26.18) as an algorithm similar to the one we used in solving Laplace‚Äôs\nequation by relaxation. Indeed, as we did there, we can accelerate the convergence by\nwriting the algorithm with the new value of ùë£(x)given by the old value plus a correction\n(residual):\nùë£(x)\ni,j=ùë£(x)\ni,j+ri,j,rdef=ùë£x(new)\ni,j‚àíùë£xold\ni,j(26.19)\n‚áír=1\n4{\nùë£(x)\ni+1,j+ùë£(x)\ni‚àí1,j+ùë£(x)\ni,j+1+ùë£(x)\ni,j‚àí1‚àíh\n2ùë£(x)\ni,j[ùë£(x)\ni+1,j‚àíùë£(x)\ni‚àí1,j]\n‚àíh\n2ùë£(y)\ni,j[ùë£(x)\ni,j+1‚àíùë£(x)\ni,j‚àí1]‚àíh\n2[Pi+1,j‚àíPi‚àí1,j]}\n‚àíùë£(x)\ni,j. (26.20)\nAsbefore,successiveiterationssweeptheinteriorofthegrid,continuouslyaddinginthe\nresidual(26.19)untilthechangebecomessmallerthansomesetleveloftolerance,|||ri,j|||<ùúÄ.\nAvariationofthismethod, successiveoverrelaxation (SOR),increasesthespeedatwhich\ntheresidualsapproachzerobyincludinganamplifyingfactor ùúî:\nùë£(x)\ni,j=ùë£(x)\ni,j+ùúîri,j(SOR). (26.21)\nThestandardrelaxationalgorithm(26.19)isobtainedwith ùúî=1,acceleratedconvergence\n(overrelaxation )isobtainedwith ùúî‚â•1,andunderrelaxation occursfor ùúî<1.Values ùúî>2\narefoundtoleadtonumericalinstabilities.Althoughadetailedanalysisofthealgorithm\nisnecessarytopredicttheoptimalvaluefor ùúî,wesuggestthatyoutestdifferentvaluesfor\nùúîtoseewhichoneprovidesthefastest,yetstable,convergencefortheproblemathand.\n26.3.1 Successive Overrelaxation Algorithm\n1) Modifytheprogram Beam.py,orwriteyourown,tosolvetheNavier-Stokesequationfor\nthevelocityofafluidin2Dflow.Representthe xandycomponentsofthevelocityby\nthearrays vx[Nx,Ny] andvy[Nx,Ny] .\n2) Specializeyoursolutiontotherectangulardomainandboundaryconditionsindicated\ninFigure26.2.\n3) Useoftheparametervalues,\nùúà=1m2‚àïs,ùúå=103kg/m3,(flowparameters),\nNx=400,Ny=40,h=1,(gridparameters) ,\nleadstotheequations\nùúïP\nùúïx=‚àí12,ùúïP\nùúïy=0,ùë£(x)=3j\n20(\n1‚àíj\n40)\n,ùë£(y)=0. (26.22)\n4) Fortherelaxationmethod,outputtheiterationnumberandthecomputed ùë£(x).\n5) Verify that your numerical solution agrees with the analytic result (26.10) for flow\nthroughparallelplates.\n6) RepeatthecalculationandseeifSORspeedsuptheconvergence.\n26.4 Vorticity Form of Navier‚ÄìStokes Equation\nNow that the comparison with an analytic solution has shown that our CFD simulation\nworks,wereturntodeterminingifthebeaminFigure26.1mightproduceagoodresting\n524 26 Fluid Hydrodynamics\nplaceforsalmon.Whilewehavenoanalyticsolutionwithwhichtocompare,ourcanoeing\nand fishing adventures have taught us that standing waves with fish in them are often\nformedbehindrocksinstreams,andsowewilllookforevidenceofastandingwaveforming\nbehindthebeam.\nWehaveseenhowtonumericallysolvethehydrodynamicsequations:\n‚àá‚ãÖv=0, (26.23)\n(v‚ãÖ‚àá)v=‚àí1\nùúå‚àáP+ùúà‚àá2v. (26.24)\nThese equations determine the components of a fluid‚Äôs velocity, pressure, and density as\nfunctionsofposition.Recallhowinelectrostaticsitisusuallysimplertosolveforascalar\npotential,ratherthanavectorfield,andthentakethepotential‚Äôsgradienttodeterminethe\nvectorfield.Inanalogy,werecastthehydrodynamicequationintoformsthatpermitusto\nsolvetwosimplerequations,fromwhichthevelocityisobtainedviaagradientoperation.3\nW ed e fi n ea stream function u(x),f r o mw h i c ht h ev e l o c i t yi sd e t e r m i n e db yt h ec u r l\noperator:\nvdef=‚àá√ó u(x)=ÃÇ ùúñx(ùúïuz\nùúïy‚àíùúïuy\nùúïz)\n+ÃÇ ùúñy(ùúïux\nùúïz‚àíùúïuz\nùúïx)\n. (26.25)\nNotetheabsenceofa zcomponentofvelocityforourproblem.Since ‚àá‚ãÖ(‚àá√óu)‚â°0,we\nseethatany vthatcanbeawrittenasthecurlof uautomaticallysatisfiesthecontinuity\nequation ‚àá‚ãÖv=0.Furthermore,becausethe vforourproblemhasonly xandycompo-\nnents, u(x)needshaveonlya zcomponent:\nuz‚â°u‚áíùë£x=ùúïu\nùúïy,ùë£y=‚àíùúïu\nùúïx. (26.26)\nNotethatin2Dflows,thecontourlines u=constantarethe streamlines .\nThesecondsimplerfunctionisthe vorticityfield w(x),whichisrelatedphysically,and\nalphabetically,totheangularvelocity ùùéofthefluid.Vorticityisdefinedasthecurlofthe\nvelocity(sometimeswitha ‚àísign):\nwdef=‚àá√ó v(x). (26.27)\nBecauseourproblem‚Äôsvelocitydoesnotchangeinthe zdirection,the zderivativevanishes:\nùë§z=(ùúïùë£y\nùúïx‚àíùúïùë£x\nùúïy)\n. (26.28)\nPhysically,weseethatvorticityisameasureofhowmuchthefluid‚Äôsvelocitycurlsorrotates,\nwiththedirectionofthevorticitydeterminedbytheright-handruleforrotations.Infact,\nifwecouldremoveasmallelementofthefluidintospace(soitwouldnotfeeltheinternal\nstrainofthefluid),wewouldfindthatitisrotatinglikeasolidwithangularvelocity ùùé‚àùw\n[Lamb,1993].Thatbeingthecase,itisusefultothinkofvorticityasgivingthelocalvalue\nofthefluid‚Äôsangularvelocityvector,with w=0describing irrotational flow.\nIn general, the field lines of ùë§are continuous and move as if attached to elements of\nthefluid.Auniformlyflowingfluidwouldhavevanishingcurls,whileanonzerovorticity\n3 Ifwehadtosolveonlythesimplerproblemof irrotationalflow (noturbulence),thenwewouldbeableto\nuseascalarvelocitypotential,incloseanalogytoelectrostatics[Lamb,1993].Forthemoregeneral\nrotationalflow ,twovectorpotentialsarerequired.",4839
239-26.5 Assessment and Exploration.pdf,239-26.5 Assessment and Exploration,"26.4 Vorticity Form of Navier‚ÄìStokes Equation 525\nindicatesthatthecurrentrotates,orcurlsbackonitself.Fromthedefinitionofthestream\nfunction(26.25),weseethatthevorticity wisrelatedtoitby:\nw=‚àá√ó v=‚àá√ó(‚àá√ó u)=‚àá ( ‚àá ‚ãÖu)‚àí‚àá2u, (26.29)\nwherewehaveusedavectoridentityfor ‚àá√ó(‚àá√ó u).Butbecause uhasonlyazcomponent\nthatdoesnotvarywith z(orbecausethereisnosourcefor u),thedivergence ‚àá‚ãÖu=0.We\nnowhavethebasicrelationbetweenthestreamfunction uandthevorticity w:\n‚àá2u=‚àíw. (26.30)\nEquation(26.30)isanalogoustoPoisson‚Äôsequationofelectrostatics, ‚àá2ùúô=‚àí4ùúãùúå,onlynow\neach component of vorticity wacting as the source for the corresponding component of\nthestreamfunction u.Iftheflowisirrotational,thatis,if w=0,thenweneedonlysolve\nLaplace‚Äôsequationforeachcomponentof u.Rotationalflow,withitscouplednonlinearities\nequations,leadstomoreinterestingbehavior.\nAs is to be expected from the definition of w, the vorticity form of the Navier‚ÄìStokes\nequationisobtainedbytakingthecurlofthevelocityform,thatis,byoperatingonboth\nsideswith ‚àá√ó.Aftersignificantmanipulationsoneobtains\nùúà‚àá2w= [(‚àá√ó u)‚ãÖ‚àá]w. (26.31)\nThisand(26.30)arethetwosimultaneousPDE‚Äôsthatweneedtosolve.In2D,with uand\nwhavingonly zcomponents,theyare\nùúï2u\nùúïx2+ùúï2u\nùúïy2=‚àíùë§, (26.32)\nùúà(\nùúï2ùë§\nùúïx2+ùúï2ùë§\nùúïy2)\n=ùúïu\nùúïyùúïùë§\nùúïx‚àíùúïu\nùúïxùúïùë§\nùúïy. (26.33)\nSoafterallthatwork,weendupwithtwosimultaneous,nonlinear,ellipticPDE‚Äôsthatlook\nlike a mixture of Poisson‚Äôs equation with the wave equation. The equation for uis Pois-\nson‚Äôsequationwithsource ùë§,andmustbesolvedsimultaneouslywiththesecond.Itisthis\nsecondequationthatcontainsmixedproductsofthederivativesof uandùë§,andthusthe\nnonlinearity.\n26.4.1 Vorticity Difference Equation\nWesolve(26.32)and(26.33)onan Nx√óNygridofuniformspacing hwith:\nx=iŒîx=ih,i=0,‚Ä¶,Nx,y=jŒîy=jh,j=0,‚Ä¶,Ny. (26.34)\nBecausethebeamissymmetricaboutitscenterline(Figure26.1left),weneedthesolution\nonlyintheupperhalf-plane.Weapplythenowfamiliarcentral-differenceapproximation\ntotheLaplaciansof uandùë§toobtainthedifferenceLaplacian:\nùúï2u\nùúïx2+ùúï2u\nùúïy2‚âÉui+1,j+ui‚àí1,j+ui,j+1+ui,j‚àí1‚àí4ui,j\nh2. (26.35)\nLikewise,fortheproductoffirstderivatives,\nùúïu\nùúïyùúïùë§\nùúïx‚âÉui,j+1‚àíui,j‚àí1\n2hùë§i+1,j‚àíùë§i‚àí1,j\n2h. (26.36)\n526 26 Fluid Hydrodynamics\nThedifferencevorticityNavier‚ÄìStokesequation(26.32)isnow:\nui,j=1\n4(ui+1,j+ui‚àí1,j+ui,j+1+ui,j‚àí1+h2ùë§i,j), (26.37)\nùë§i,j=1\n4(ùë§i+1,j+ùë§i‚àí1,j+ùë§i,j+1+ùë§i,j‚àí1)‚àíR\n16{[ui,j+1‚àíui,j‚àí1]\n√ó[ùë§i+1,j‚àíùë§i‚àí1,j]‚àí[ui+1,j‚àíui‚àí1,j][ùë§i,j+1‚àíùë§i,j‚àí1]}, (26.38)\nR=1\nùúà(\nV0h\nùúàinnormalunits)\n. (26.39)\nNote,inordertoobtainanalgorithmappropriateforsolutionbyrelaxation,wehaveplaced\nui,jandùë§i,jontheLHSoftheequations.\nTheparameter Rin(26.39)isrelatedtothe Reynoldsnumber .Whenwesolvetheproblem\ninnaturalunits,wemeasuredistancesinunitsofgridspacing h,velocitiesinunitsofinitial\nvelocityV0,streamfunctionsinunitsof V0h,andvorticityinunitsof V0‚àïh.Thesecondform\nisinregularunitsandisdimensionless.This Risknownasthe gridReynoldsnumber ,and\ndiffersfromthephysical R,whichhasapipediameterinplaceofthegridspacing h.\nThegridReynoldsnumberisameasureofthestrengthofthecouplingofthenonlinear\ntermsintheequation.Whenthephysical Rissmall,theviscosityactsasafrictionalforce\nthatdampsoutfluctuationsandkeepstheflowsmooth.When Rislarge(R‚âÉ2000),phys-\nicalfluidsundergophasetransitionsfromlaminartoturbulentflowinwhichturbulence\noccurs at a cascading set of smaller and smaller space scales. However, simulations that\nproducetheonsetofturbulencehavebeenaresearchproblemsinceReynolds‚Äôfirstexperi-\nmentsin1883[Reynolds,1883;FalkovichandSreenivasan,2006].Possiblybecauselaminar\nflowisstableagainstsmallperturbations,itmaybethatsomelarge-scale‚Äúkick‚Äùisneeded\ntochangeitfromlaminartoturbulent.\nAs discussed in Section 26.3, the finite difference algorithm can have its convergence\nacceleratedbytheuseofsuccessiveoverrelaxation(26.37):\nui,j=ui,j+ùúîr(1)\ni,j,ùë§i,j=ùë§i,j+ùúîr(2)\ni,j(SOR). (26.40)\nHereùúîistheoverrelaxationparameter,andshouldlieintherange0 <ùúî<2forstability.\nTheresidualsarejustthechangesinasinglestep, r(1)=unew‚àíuoldandr(2)=ùë§new‚àíùë§old:\nr(1)\ni,j=1\n4(ui+1,j+ui‚àí1,j+ui,j+1+ui,j‚àí1+ùë§i,j)‚àíui,j, (26.41)\nr(2)\ni,j=1\n4(\nùë§i+1,j+ùë§i‚àí1,j+ùë§i,j+1+ùë§i,j‚àí1‚àíR\n4{[ui,j+1‚àíui,j‚àí1]\n√ó[ùë§i+1,j‚àíùë§i‚àí1,j]‚àí[ui+1,j‚àíui‚àí1,j][ùë§i,j+1‚àíùë§i,j‚àí1]})‚àíùë§i,j.\n26.4.2 Beam Boundary Conditions\nAwell-definedsolutiontotheseellipticPDEsrequiresacombinationof(lessthanobvious)\nboundary conditions on uandùë§. Consider Figure 26.3, based on the analysis of Koonin\n[1986].Theassumptionisthattheinlet,outlet,andsurfacearefarfromthebeam(which\n26.5 Assessment and Exploration 527\ndw/dx  = 0du/dx  = 0 vx = du/dy  = V0\nw = 0 Inlet FOutlet H\nHalf\nbeamSurface G\nvx = du/dy  = V0 w = 0\ny\nxvy = -du/dx  = 0\nCenter linew = u = 0 w = u = 0u = 0\nu = 0vy = -du/dx  = 0\nABC\nED\nFigure 26.3 Boundary conditions for Ô¨Çow around the beam in Figure 26.1. The Ô¨Çow is symmetric\nabout the centerline, and the beam has length Lin the xdirection (along Ô¨Çow).\nmaynotbeevidentfromthenot-to-scalefigure).Werefertheinterestedreadertotheref-\nerences,andjustgivethe Boundary Conditions :\nu=0;ùë§=0 CenterlineEA\nu=0,ùë§i,j=‚àí2\nh2(ui+1,j‚àíui,j)BeambackB\nu=0,ùë§i,j=‚àí2\nh2(ui,j+1‚àíui,j)BeamtopC\nu=0,ùë§i,j=‚àí2\nh2(ui‚àí1,j‚àíui,j)BeamfrontD\nùúïu‚àïùúïx=0,ùë§ =0I n l e t F\nùúïu‚àïùúïy=V0,ùë§ =0 SurfaceG\nùúïu‚àïùúïx=0,ùúï ùë§‚àïùúïx=0O u t l e t H\nBeam.pyin Listing 26.1 is our program for solution of the vorticity form of the\nNavier‚ÄìStokes equation. You will notice that while the relaxation algorithm is rather\nsimple,somecareisneededinimplementingthemanyboundaryconditions.Relaxation\nofthestreamfunctionandofthevorticityisperformedbyseparatefunctions.\n26.5 Assessment and Exploration\n1) Use Beam.pyasabasisforyoursolutionforthestreamfunction uandthevorticity ùë§\nusingthefinite-differencesalgorithm(26.37).Figures26.4and26.5showsometypical\nresults.\n2) A good place to start your simulation is with a beam of size L=8h,H=h, Reynolds\nnumberR=0.1, and intake velocity V0=1. Keep your grid small during debugging,\nsay,Nx=24andNy=70.\n3) Exploretheconvergenceofthealgorithm.\na) Printouttheiterationnumberand uvaluesupstreamfrom,above,anddownstream\nfromthebeam.\nb) Determinethenumberofiterationsnecessarytoobtainthree-placeconvergencefor\nsuccessiverelaxation( ùúî=1).\n528 26 Fluid Hydrodynamics\n07060504030200\n1020\n15\n10\n5\n0\n‚Äì5\n5101520\n060\n4050\n30\n20\n10\n010\n50‚Äì5\n5\n10\n15\n200\nStream flow10203040506070\n5\n10\n15\n200\nyx\nYT\nFigure 26.4 Two visualizations of the stream function ufor Reynold‚Äôs number R=5.\nw(x,y)\n0\n‚Äì1\n0\n50 020\nyx\n4006y\nxv\n12\n80\nFigure 26.5 Left: The vorticity as a function of xandy. Rotation is seen to be largest behind the\nbeam. Right: The velocity Ô¨Åeld around the beam as represented by vectors.\nc) Determinethenumberofiterationsnecessarytoobtainthree-placeconvergencefor\nsuccessiveoverrelaxation( ùúî‚âÉ1.3).Usethisnumberforfuturecalculations.\n4) Changethebeam‚Äôshorizontalplacement,sothatyoucanseetheundisturbedcurrent\nentering from the left, and then developing into a standing wave. Note that you may\nneedtoincreasethesizeofyoursimulationvolumetoseetheeffectofalltheboundary\nconditions.\n5) Make surface plots including contours of the stream function uand the vorticity ùë§.\nExplainthebehaviorseen.\n6) Istherearegionwhereabigfishcanrestbehindthebeam?\n7) Theresultsofthesimulation(Figure26.4)arefortheone-componentstreamfunction u.\nMakeseveralvisualizationsshowingthefluidvelocitythroughoutthesimulationregion.",7341
240-26.5.1 Explorations.pdf,240-26.5.1 Explorations,,0
241-26.6 Code Lisitings.pdf,241-26.6 Code Lisitings,"26.6 Code Lisitings 529\nNotethatvelocityisavectorwithtwocomponents,andtheindividualcomponentsare\ninterestingtovisualize.Avectorplotworkswellhere.\n8) ExplorehowincreasingtheReynoldsnumber Rchangestheflowpattern.Startat R=0\nandgraduallyincrease Rwhilewatchingfornumericinstabilities.Toovercomenumer-\nicalinstabilities,reducethesizeoftherelaxationparameter ùúîandcontinuetolarger R\nvalues.\n9) Verifythattheflowaroundthebeamissmoothforsmall Rvalues,butthatitseparates\nfromthebackedgeforlarge R,atwhichpointasmallvortexdevelops.\n26.5.1 Explorations\n1) Determinetheflowbehindacircularrockinthestream.\n2) Theboundaryconditionatanoutletfardownstreamshouldnothavemucheffectonthe\nsimulation.Exploretheuseofotherboundaryconditionsthere.\n3) Determinethepressurevariationaroundthebeam.\n26.6 Code Lisitings\nListing 26.1 Beam.py SolvestheNavier‚ÄìStokesequationfortheflowoveraplate.\n# Beam.py: solves Navier ‚àíStokes equation for flow around beam\n2\nimportmatplotlib.pylab as p;\nfrommpl_toolkits.mplot3d importAxes3D;\nfromnumpyimport ‚àó;\n6\nprint(""Working, wait for the figure after 100 iterations"" )\nNxmax = 70; Nymax = 20; IL = 10; H = 8; T = 8; h = 1.\nu = zeros ((Nxmax+1, Nymax+1), float) # Stream\n10w = zeros ((Nxmax+1, Nymax+1), float) # Vorticity\nV0 = 1.0; omega = 0.1; nu = 1.; iter = 0; R = V0 ‚àóh/nu\ndefborders():\n14foriin range (0, Nxmax+1): # Init stream\nforjin range (0, Nymax+1): # Init vorticity\nw[i , j] = 0.\nu[i, j] = j ‚àóV0\n18foriin range (0, Nxmax+1 ): # Fluid surface\nu[i, Nymax] = u[i, Nymax ‚àí1] + V0 ‚àóh\nw[i , Nymax ‚àí1] = 0.\nforjin range (0, Nymax+1):\n22 u[1, j] = u[0, j]\nw[0, j] = 0. #I n l e t\nforiin range (0, Nxmax+1): # Centerline\nifi<=I Landi>= IL+T:\n26 u[i, 0] = 0.\nw[i , 0] = 0.\nforjin range (1, Nymax ) : #O u t l e t\nw[Nxmax, j] = w[Nxmax ‚àí1, j]\n30 u[Nxmax, j] = u[Nxmax ‚àí1, j]\ndefbeam() : #B Cf o rb e a m\nforjin range (0, H+1): # Sides\nw[IL, j] = ‚àí2‚àóu[IL‚àí1, j]/(h ‚àóh) #F r o n t\n34 w[IL+T, j] = ‚àí2‚àóu[IL + T + 1, j]/(h ‚àóh) #B a c k\nforiin range (IL, IL+T + 1): w[i, H ‚àí1] =‚àí2‚àóu[i, H]/(h ‚àóh);\nforiin range (IL, IL+T+1):\nforjin range (0, H+1):\n38 u[IL, j] = 0. #F r o n t\nu[IL+T, j] = 0. #B a c k\n530 26 Fluid Hydrodynamics\nu[i, H] = 0; # Top\ndefrelax(): # Relax stream\n42beam() # Reset conditions\nforiin range (1, Nxmax) : # Relax stream\nforjin range (1, Nymax) :\nr1 = omega ‚àó((u[i+1,j]+u[i ‚àí1,j]+u[i , j+1]+u[i ,j ‚àí1] + h ‚àóh‚àów[i ,j])/4 ‚àíu[i,j])\n46 u[i, j] += r1\nforiin range (1, Nxmax) : # Relax vorticity\nforjin range (1, Nymax) :\na1 = w[i+1, j] + w[i ‚àí1,j] + w[i ,j+1] + w[i ,j ‚àí1]\n50 a2 = (u[i ,j+1] ‚àíu[i,j‚àí1])‚àó(w[i+1,j] ‚àíw[i‚àí1, j])\na3 = (u[i+1,j] ‚àíu[i‚àí1,j]) ‚àó(w[i , j+1] ‚àíw[i , j ‚àí1])\nr2 = omega ‚àó(( a 1‚àí(R/4.) ‚àó(a2‚àía3) )/4. ‚àíw[i ,j])\nw[i , j] += r2\n54borders()\nwhile(iter<= 100):\niter += 1\nifiter%10 == 0: print(iter)\n58relax()\nforiin range (0, Nxmax+1):\nforjin range (0, Nymax+ 1): u[i,j] = u[i,j]/V0/h # V0h units\nx=range(0, Nxmax ‚àí1); y = range(0, Nymax ‚àí1)\n62X, Y = p.meshgrid(x, y)\ndeffunctz(u): # Stream flow\nz=u [ X ,Y ]\nreturnz\n66Z=f u n c t z( u )\nfig = p.figure()\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Z, color = ‚Äôr‚Äô)\n70ax.set_xlabel( ‚ÄôX‚Äô)\nax.set_ylabel( ‚ÄôY‚Äô)\nax.set_zlabel( ‚ÄôStream Function‚Äô )\np.show()",3217
242-Chapter 27 Finite Element Electrostatics.pdf,242-Chapter 27 Finite Element Electrostatics,,0
243-27.1 The Potential of Two Metal Plates.pdf,243-27.1 The Potential of Two Metal Plates,,0
244-27.2 Finite Element Method.pdf,244-27.2 Finite Element Method,"531\n27\nFinite Element Electrostatics ‚äô\nWe have already discussed the simple, but powerful, solution of PDEs using Ô¨Ånite differences\nto approximate derivatives. In this (optional) chapter, we outline the Ô¨Ånite element method\n(FEM) for solving PDEs that patches together approximate solutions on small Ô¨Ånite ele-\nments to obtain the full solution. FEM is faster to execute than the Ô¨Ånite differences method;\nhowever, it takes much more effort to set up, and so is often implemented via highly devel-\noped FEM packages, such as Python‚Äôs FiPy.\n27.1 The Potential of Two Metal Plates\nProblem Determinetheelectricpotentialbetweenthetwoconductingplatesshownin\nFigure27.1.Theplatesaredistance b‚àíaapart,theloweroneatpotential Ua,theupper\noneatpotential Ub,withauniformchargedensity ùúå(x)betweenthem.\n27.1.1 Analytic Solution\nTherelationbetweenchargedensity ùúå(x)andpotential U(x)isgivenbyPoisson‚Äôsequation\n(21.6).Forourproblem,thepotential Uchangesonlyinthe xdirection,andsothePDE\nbecomestheODE:\nd2U(x)\ndx2=‚àí4ùúãùúå(x)=‚àí1,0<x<1, (27.1)\nwherewehaveset ùúå(x)=1‚àï4ùúãtosimplifytheprogramming.Thesolutionissubjecttothe\nDirichletboundaryconditions:\nU(x=a=0)=0,U(x=b=1)=1, (27.2)\n‚áíU(x)=‚àíx\n2(x‚àí3). (27.3)\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH",1353
245-27.2.3 Solution via Linear Equations.pdf,245-27.2.3 Solution via Linear Equations,"532 27 Finite Element Electrostatics ‚äô\nNodeElementx = b Ub\nUa x = a\nX0XN\nœÅ(x)Figure 27.1 A Ô¨Ånite element solution to Laplace‚Äôs\nequation for two metal plates with a charge density\nbetween them. The large dots are the nodes xi, and the\nlines connecting the nodes are the Ô¨Ånite elements.\n27.2 Finite Element Method\nThetheoryandpracticeofFEMhavebeendevelopedoverthelast60yearsandisstillan\nactivefieldofresearch[Shaw,1992;Li,2014;Otto,2019].AstrengthofFEMisthatitoffers\ngreatflexibilityforproblemsinirregulardomains,orforproblemswithhighlyvaryingcon-\nditionsorevensingularities.FurtheradvantagesofFEMarethatthesamebasictechnique\ncan be applied to many problems with only minor modifications, and that the solutions\nmaybeevaluatedthroughoutallspace,notjustonagrid.Infact,theFEM,withvarious\npreprogrammed multigrid packages, has very much become the standard for large-scale\nengineeringapplications.\nIn FEM, the domain in which the PDE is to be solved is split into subdomains, called\nelements,a n datrial solution to the PDE in each subdomain is hypothesized. Then the\nparametersofthetrialsolutionareadjustedtoobtainthe bestfit,inthesenseofChapter6,to\ntheexactsolution.Sowhilefinite-differencemethodyieldsanapproximatesolutionforan\napproximatePDE,FEMyieldsthebestpossibleglobalagreementbetweenanapproximate\nsolutionandtheexactsolution.\n27.2.1 Weak Form of PDE\n1) StarttheFEMwiththedifferentialequationwewanttosolve,\nd2U(x)\ndx2=‚àí4ùúãùúå(x). (27.4)\n2) Multiply the unknown exact solution U(x)by an approximate trialsolution Œ¶(x),a n d\nintegratetheproductovertheentiresolutiondomain,\n‚à´b\nadxU(x)ùúô(x). (27.5)\nThisintegralisusedasameasureoftheoverallagreementbetweentheexactandtrial\nsolutions.\n3) Assume that the trial solution vanishes at the endpoints, Œ¶(a)=Œ¶ (b)=0, keeping in\nmindthatwe‚Äôllhavetoimposetheboundaryconditionslater.\n4) Multiplybothsidesofthedifferentialequation(27.1)by Œ¶,\nd2U(x)\ndx2Œ¶(x)=‚àí4ùúãùúå(x)Œ¶(x). (27.6)\n5) Integratebypartsfrom atob:\n‚à´b\nadxd2U(x)\ndx2Œ¶(x)=‚àí‚à´b\nadx4ùúãùúå(x)Œ¶ (x),\ndU(x)\ndxŒ¶(x)|b\na‚àí‚à´b\nadxdU(x)\ndxŒ¶‚Ä≤(x)=‚àí‚à´b\nadx4ùúãùúå(x)Œ¶ (x)\n‚áí‚à´b\nadxdU(x)\ndxŒ¶‚Ä≤(x)=‚à´b\nadx4\n27.2 Finite Element Method 533\nEquation(27.7)isa weakformofthePDE,‚Äúweak‚Äùinthesensethatitdoesnotrequire\ntheexistenceofthesecondderivativeof U(x),orthecontinuityof ùúå(x).\n27.2.2 Galerkin Spectral Decomposition\nTheapproximatesolutiontotheweakPDEproceedsviathefollowingthreesteps:\n1) SplitthefulldomainofthePDEintosubdomainscalled elements.AsseeninFigure27.1,\nforour1Dproblemwetakethesubdomainelementstobestraightlinesofequallength.\nAswillbeseeninFigure27.4,fora2Dproblem,theelementsmightbetriangles.\n2) Expandthesolutionwithineachelementintermsofthebasisfunctions ùúôi:\nU(x)‚âÉN‚àí1‚àë\nj=0ùõºjùúôj(x). (27.8)\nEvenwhenthebasisfunctionsarenotsinesorcosines,thisexpansionisstillcalleda\nspectraldecomposition.Choose ùúôi‚Äôsthatareconvenientforcomputation.Thesolution\nreducestodeterminingtheunknownexpansioncoefficients ùõºj.\n3) Matchtheelementalsolutionsontoeachother.\nConsiderablestudyhasgoneintodeterminingtheeffectivenessofdifferentbasisfunc-\ntions,ùúôi‚Äôs,usedtorepresentthesolutiononeachfiniteelement.Iftheelementsaremade\nsufficientlysmall,thengoodaccuracyisobtainedwithsimplepiecewise-continuous ùúôi‚Äôs.\nForour1Dproblem,weuse elementsthatarelinesegmentsbetween xiandxi+1,andweuse\nbasisfunctions thathavetheformoftrianglesor‚Äúhats‚Äùbetween xi‚àí1andxi+1(Figure27.2).\nWealsorequirethateachbasisfunctionequals1atitsparticular xi‚Äôsvertex, ùúôi(xi)=1:\nùúôi(x)=‚éß\n‚é™\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™\n‚é™‚é©0,forx<xi‚àí1,orx>xi+1,\nx‚àíxi‚àí1\nhi‚àí1,forxi‚àí1‚â§x‚â§xi,\nxi+1‚àíx\nhi,forxi‚â§x‚â§xi+1.(hi=xi+1‚àíxi), (27.9)\nDuetothischoiceofhavingeachbasisfunctionequals0or1atthenodes,\nùúôi(xj)=ùõøij, (27.10)\nx0 x1 xN‚Äì1 xi xi‚Äì1 xi+1 xi‚Äì2 xi+2. . .Œ¶N œïiœïiŒ¶1 Œ¶0\n . . .\nFigure 27.2 Basis functions used in Ô¨Ånite-element solution of the 1D Laplace‚Äôs equation.\nLeft: A set of overlapping basis functions ùúôi. Each function is a triangle from xtox.\nMiddle : A piecewise-linear function. Right: A\n534 27 Finite Element Electrostatics ‚äô\nthe values of the expansion coefficients ùõºimust equal the values of the (still unknown)\nsolutionatthenodes:\nU(xi)‚âÉN‚àí1‚àë\ni=0ùõºiùúôi(xi)=ùõºiùúôi(xi)=ùõºi, (27.11)\n‚áíU(x)‚âÉN‚àí1‚àë\nj=0U(xj)ùúôj(x). (27.12)\nEquation(27.12)makesitclearthattheexpansionintermsofbasisfunctionsisessentially\naninterpolationbetweenthesolutionatthenodes.\n27.2.3 Solution via Linear Equations\nBecausethebasisfunctions ùúôiin(27.8)areknown,solvingfor U(x)involvesdetermining\nthe expansion coefficients ùõºj, which, as we just said, are the unknown values of the true\nsolutionU(x)onthenodes.Wedeterminethosevaluesbysubstitutingtheexpansionsfor\nU(x)andŒ¶(x)into the weak form of the PDE (27.7). This converts the integral equation\nintoasetofsimultaneouslinearequations,whichweknowhowtosolve.Asdiscussedin\nChapter7,thisleadstothestandardmatrixform\nAy=b. (27.13)\nInthepresentcase, yisavectorofunknowns,and A(thestiffnessmatrix )and b(theload)\nare known. To that end, we substitute the expansion U(x)‚âÉ‚àëN‚àí1\nj=0ùõºjùúôj(x)into the weak\nform(27.7)toobtain:\n‚à´b\nadxd\ndx(N‚àí1‚àë\nj=0ùõºjùúôj(x))\ndŒ¶\ndx=‚à´b\nadx4ùúãùúå(x)Œ¶(x).\nBy successively selecting Œ¶(x)=ùúô0,ùúô1,‚Ä¶,ùúôN‚àí1,w eo b t a i n Nsimultaneous linear\nequationsfortheunknown ùõºj‚Äôs:\n‚à´b\nadxd\ndx(N‚àí1‚àë\nj=0ùõºjùúôj(x))\ndùúôi\ndx=‚à´b\nadx4ùúãùúå(x)ùúôi(x),i=0,N‚àí1. (27.14)\nHerewefactorouttheunknown ùõºj‚Äôsandwriteouttheexplicitequations:\nùõº0‚à´b\naùúô‚Ä≤\n0ùúô‚Ä≤\n0dx+ùõº1‚à´b\naùúô‚Ä≤\n0ùúô‚Ä≤\n1dx+¬∑¬∑¬∑+ùõºN‚àí1‚à´b\naùúô‚Ä≤\n0ùúô‚Ä≤\nN‚àí1dx=‚à´b\na4ùúãùúåùúô0dx,\nùõº0‚à´b\naùúô‚Ä≤\n1ùúô‚Ä≤\n0dx+ùõº1‚à´b\naùúô‚Ä≤\n1ùúô‚Ä≤\n1dx+¬∑¬∑¬∑+ùõºN‚àí1‚à´b\naùúô‚Ä≤\n1ùúô‚Ä≤\nN‚àí1dx=‚à´b\na4ùúãùúåùúô1dx,\n...\nùõº0‚à´b\naùúô‚Ä≤\nN‚àí1ùúô‚Ä≤\n0dx+ùõº1‚à´¬∑¬∑¬∑+ùõºN‚àí1‚à´b\naùúô‚Ä≤\nN‚àí1ùúô‚Ä≤\nN‚àí1dx=‚à´b\na4ùúãùúåùúôN‚àí1dx.\n27.2 Finite Element Method 535\nBecausewehavechosenthe ùúôi‚Äôstobesimplehatfunctions,thederivativesareeasytoeval-\nuateanalytically(forotherbasestheycanbecarriedoutnumerically):\ndùúôi,i+1\ndx=‚éß\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™\n‚é™\n‚é™\n‚é™‚é©0,x<xi‚àí1,orxi+1<x,\n1\nhi‚àí1,xi‚àí1‚â§x‚â§xi,\n‚àí1\nhi,xi‚â§x‚â§xi+1,\n0,x<xi,orxi+2<x\n1\nhi,xi‚â§x‚â§xi+1,\n‚àí1\nhi+1,xi+1‚â§x‚â§xi+2.(27.15)\nTheintegrationsarenowfairlysimple:\n‚à´xi+1\nxi‚àí1dx(ùúô‚Ä≤\ni)2=‚à´xi\nxi‚àí1dx1\n(hi‚àí1)2+‚à´xi+1\nxidx1\nh2\ni=1\nhi‚àí1+1\nhi,\n‚à´xi+1\nxi‚àí1dxùúô‚Ä≤\niùúô‚Ä≤\ni+1=‚à´xi+1\nxi‚àí1dxùúô‚Ä≤\ni+1ùúô‚Ä≤\ni=‚à´xi+1\nxidx‚àí1\nh2\ni=‚àí1\nhi, (27.16)\n‚à´xi+1\nxi‚àí1dx(ùúô‚Ä≤\ni+1)2=‚à´xi+1\nxidx(ùúô‚Ä≤\ni+1)2=‚à´xi+1\nxidx+1\nh2\ni=+1\nhi.\nWe rewrite these equations in the standard matrix form (27.13) with yconstructed from\nthe unknown ùõºj‚Äôs, and the tridiagonal matrix Aconstructed from the integrals over the\nderivatives:\ny=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£ùõº0\nùõº1\n...\nùõºN‚àí1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,b=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£‚à´x1\nx0dx4ùúãùúå(x)ùúô0(x)\n‚à´x2\nx1dx4ùúãùúå(x)ùúô1(x)\n...\n‚à´xN\nxN‚àí1dx4ùúãùúå(x)ùúôN‚àí1(x)‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶, (27.17)\nA=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£1\nh0+1\nh1‚àí1\nh1‚àí1\nh00‚Ä¶\n‚àí1\nh11\nh1+1\nh2‚àí1\nh20‚Ä¶\n0‚àí1\nh21\nh2+1\nh3‚àí1\nh3‚Ä¶\n......‚àí1\nhN‚àí1‚àí1\nhN‚àí21\nhN‚àí2+1\nhN‚àí1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (27.18)\nTheelementsin Aarejustcombinationsofinversestepsizes,andso,theydonotchange\nfor different charge densities ùúå(x). This is part of what makes FEM so efficient, once it‚Äôs\nallsetup.Theelementsin bdochangefordifferent ùúå‚Äôs,buttherequiredintegralscanbe",7009
246-27.2.4 Imposing the Boundary Conditions.pdf,246-27.2.4 Imposing the Boundary Conditions,,0
247-27.4 2D FEM Exercises.pdf,247-27.4 2D FEM Exercises,"536 27 Finite Element Electrostatics ‚äô\nperformedanalyticallyorwithGaussianquadrature(Chapter5).Once Aandbarecom-\nputed,efficientmethodsfromalinearalgebralibraryareusedtosolvefor y,andthusthe\nexpansioncoefficients ùõºj.\n27.2.4 Imposing the Boundary Conditions\nSincethebasisfunctionsvanishattheendpoints,asolutionexpandedinthemmustalso\nvanishthere.Thiswillnotdoingeneral,andsowemustaddtoourgeneralsolution, U(x),\naparticularone, Uaùúô0(x),thatsatisfiestheboundaryconditions[Li,2014]:\nU(x)=N‚àí1‚àë\nj=0ùõºjùúôj(x)+UaùúôN(x)(satisfiesboundaryconditions), (27.19)\nwhereUa=U(xa).Wesubstitute U(x)‚àíUaùúô0(x)intotheweakformofthePDEtoobtain\n(N+1)simultaneousequations,stilloftheform Ay=b‚Ä≤,butnowwith\nA=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£A0,0¬∑¬∑¬∑A0,N‚àí10\n...\nAN‚àí1,0¬∑¬∑¬∑AN‚àí1,N‚àí10\n00 ¬∑¬∑¬∑1‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶,b‚Ä≤=‚é°\n‚é¢\n‚é¢\n‚é¢\n‚é¢\n‚é¢‚é£b0‚àíA0,0Ua\n...\nbN‚àí1‚àíAN‚àí1,0Ua\nUa‚é§\n‚é•\n‚é•\n‚é•\n‚é•\n‚é•‚é¶. (27.20)\nWeseethatwehavenowaddeda1onthelastrowof A,andaddedatermontoeachelement\nofb:\nb‚Ä≤\ni=bi‚àíAi,0Ua,i=1,‚Ä¶,N‚àí1,b‚Ä≤\nN=Ua. (27.21)\nToimposetheboundaryconditionat x=b,weagainaddaparticularsolution UbùúôN‚àí1(x),\nandsubstituteitintotheweakformtoobtain\nb‚Ä≤\ni=bi‚àíAi,N‚àí1Ub,i=1,‚Ä¶,N‚àí1b‚Ä≤\nN=Ub. (27.22)\nSonowweneedtosolvethematrixequation Ay=b‚Ä≤.For1Dproblems,100‚Äì1000equations\narecommon,whilefor3Dproblemstheremaybemillions.Becausethenumberofcalcu-\nlationsvariesapproximatelyas N2,itisimportanttouseefficientandaccuratealgorithms,\norelseround-offerrorcaneasilydominate.\n27.3 1D FEM Problems\nInListing27.1,wegiveourprogram LaplaceFEM_1D.py whichdeterminesthe1DFEMsolu-\ntion,andinFigure27.3,weshowthatsolution.Weseeontheleftofthefigurethatthree\nelements do not provide even visual agreement with the analytic result, whereas N=11\nelementsdo.\n1) ExaminetheFEMsolutionforthechoiceofparameters\na=0,b=1,Ua=0,Ub=1. (27.23)\n2) Generateyourowntriangulationbyassigningexplicit xvaluesatthenodesoverthe\ninterval[0,1].\n3) Startwith N=3andsolvetheequationsfor Nvaluesupto1000.\n27.4 2D FEM Exercises 537\nFigure 27.3 Exact (line) versus FEM solution (points)\nfor the two-plate problem for N=3a n d N=11 Ô¨Ånite\nelements ( N=3 displaced upward for clarity). On this\nscale, the N=11 solution is identical to the exact one.\n01\n0 1U\nxN = 3 (scaled)\nN = 11\n4) Examinethestiffnessmatrix Aandensurethatitistriangular.\n5) Verifythattheintegrationsusedtocomputetheloadvector bareaccurate.\n6) Verifythatthesolutionofthelinearequation Ay=biscorrect.\n7) Plotthenumericalsolutionfor U(x)forN=10,100,and1000,andcomparewiththe\nanalyticsolution.\n8) Thelogoftherelativeglobalerror(numberofsignificantfigures)is\nÓà±=log10|||||1\nb‚àía‚à´b\nadxUFEM(x)‚àíUexact(x)\nUexact(x)|||||. (27.24)\nPlottheglobalerror versusxforN=10,100,and1000.\n9) Modifyyourprogramtousepiecewise-quadraticfunctionsforinterpolation,andcom-\nparetheresultsobtainedtothoseobtainedwiththelinearfunctions.\n10) Exploretheresultingelectricpotentialandcheckthatthechargedistributionbetween\ntheplateshastheexplicit xdependence\nùúå(x)=1\n4ùúã‚éß\n‚é™\n‚é™\n‚é®\n‚é™\n‚é™‚é©1\n2‚àíx,\nsinx,\n1atx=0,‚àí1atx=1(acapacitor) .(27.25)\n27.4 2D FEM Exercises\nThe steps followed to derive 2D FEM are similar to those for the 1D method, with the\nbigdifferencebeingthatthefiniteelementsarenow2Dtriangles,asopposedto1Dlines.\nFigure27.4showshowanarbitrarilyshapeddomainmightbedecomposedintotriangles.\nAlthoughlifeissimplerifallthefiniteelementsareofthesamesizeandshape,thisisnot\nnecessary,and,indeed,asweseeinthefigure,higherprecisionmaybeobtainedbypicking\nsmallerdomainsinregionswherethesolutionvariesrapidly,andlargerdomainsinregions\nwherethesolutionvariesslowly.\n538 27 Finite Element Electrostatics ‚äô\nDiscretization\nerror1\n23\n45\n67\n89\n1011\n1213\n1415\n1617\n1819\n2021\n2223\n2425\n2627\n2829\n3031\n32\n1 2325\nFigure 27.4 Left: Decomposition of a 2D domain into triangular elements. Smaller triangles are\nused in regions of rapid variation and larger triangles in regions of slow variation. Discretization\nerrors occur at boundaries. Right: A decomposition of a rectangular domain into 32 right triangles\no nam e s hw i t h2 5n o d e s( i ng r a yn u m b e r s ) .\nAsinthe1Dmethod,theapproximatesolution U(x,y)isexpandedinasetofbasisfunc-\ntions,ùúôi(x,y),inthiscase2Dfunctions:\nU(x,y)=N‚àí1‚àë\nj=0ùõºjùúôj(x,y). (27.26)\nAndasyoucanimagine,2Dand3DFEMgettoberathercomplicated.Butnottoworry,we\njustrefertheinterestedreadertoPolycarpou[2006]andReddy[1993]forthedetails.Here\nweprovide,andhaveyouworkwith,thecode LaplaceFEM_2D.py inwhichwehaveapplied\nallthosedetails.\nAsshownontherightofFigure27.4,ourapplicationof2DFEMhasthesolutiondomain\ncoveredbyameshoftriangularelements.Eachtriangleinthemeshisnumbered,inthis\ncasefrom1to32.Inaddition,thethreeverticesofeachtrianglearenumberedinacounter-\nclockwisedirectionfrom1to3.Furthermore,eachnodeinthemesh(thedarkcirclesin\nFigure 27.4 where lines intersect) are numbered, in this case from 1 to 25. Listing 27.2\npresents LaplaceFEM_2D.py ,ourimplementationofthe2DFEMsolutiontothe2DLaplace‚Äôs\nequation,basedontheMatlabcodeofPolycarpou[2006].Itutilizes800elementsand441\nnodes.Theoutputofthiscodeisessentiallythesameasoursimplesolutiontothesame\nproblemusingthefinitedifferencesmethod.\n1) Examinetheeffectofvaryingthedomainheightandwidthin LaplaceFEM_2D.py ,aswell\nasthenumberofelements.\n2) Comparethisnumericalsolutiontotheanalyticone(theFourierseriesinSection21.2.1)\nanddeterminehowtheprecisionchangesasthenumberofelementsvaries.\n3) Modifytheprogramsothatitsolvestheparallelplatecapacitorproblem,andcompare\nittothefinitedifferencesolution.",5468
248-27.5 Code Listings.pdf,248-27.5 Code Listings,"27.5 Code Listings 539\n27.5 Code Listings\nListing 27.1 LaplaceFEM_1D.py Uses finite-elements to solve the 1D Laplace‚Äôs\nequationviaaGalerkinspectraldecomposition.Theresultingmatrixequationsaresolved\nwithMatplotlib.\n1# LaplaceFEM_1D . py : Solutn 1 ‚àíD Laplace E q via finite elements; utf8 coding\n"""""" Dirichlet boundary conditions surrounding four walls\nDomain dimensions : WxH, with 2 t r i a n g l e s per square\n5Based on F E M 2 D L _ B o x Matlab program in Polycarpou , Intro to the Finite\nElement Method in Electromagnetics , Morgan &Claypool (2006) """"""\nfromvisualimport ‚àó\n9fromvisual.graph import ‚àó\nfromnumpyimport ‚àó\nfromnumpy.linalg importsolve\n13N=1 1\nh=1 . /( N ‚àí1)\nu=z e r o s( N , float)\nA=z e r o s( ( N ,N ), float)\n17b=z e r o s( ( N ,N ), float)\nx2 = zeros(21, float)\nu_fem = zeros(21, float)\nu_exact = zeros(21, float)\n21error = zeros(21, float)\nx=z e r o s ( N , float)\ngraph1 = gdisplay(width=500,height=500,title= ‚ÄôAnalytic (Blue) vs FEM‚Äô ,\\n25 xtitle= ‚Äôx‚Äô,ytitle= ‚ÄôU‚Äô,xmax=1, ymax=1, xmin=0, ymin=0)\nfunct1 = gcurve(color=color.blue)\nfunct2 = gdots(color=color.red)\nfunct3 = gcurve(color=color.cyan)\n29\nforiin range (0, N):\nx[i] = i ‚àóh\nforiin range (0, N): # Initialize\n33b[i, 0] = 0.\nforjin range (0, N):\nA[i][j] = 0.\n37deflin1(x, x1, x2): #H a tf u n c\nreturn(x‚àíx1)/(x2 ‚àíx1)\ndeflin2(x, x1, x2):\n41return(x2‚àíx)/(x2‚àíx1)\ndeff(x):\nreturn1.\n45\ndefint1(min,max): # Simpson\nno = 1000\nsum=0 .\n49interval = ( max‚àímin)/( n o ‚àí1)\nfornin range (2, no, 2): # Loop odd points\nx=i n t e r v a l ‚àó(n‚àí1)\nsum+= 4 ‚àóf(x) ‚àólin1(x, min,max)\n53fornin range (3, no, 2): # Loop even points\nx=i n t e r v a l ‚àó(n‚àí1)\nsum+= 2 ‚àóf(x) ‚àólin1(x, min,max)\nsum+= f (min)‚àólin1(min,min,max)+f (max)‚àólin1(max,min,max)\n57sum ‚àó=i n t e r v a l / 6 .\nreturn sum\ndefint2(min,max): # Simpson\n61no = 1000\nsum=0 .\ninterval = ( max‚àímin)/( n o ‚àí1)\n540 27 Finite Element Electrostatics ‚äô\nfornin range (2, no, 2): # Loop odd points\n65 x=i n t e r v a l ‚àó(n‚àí1)\nsum+= 4 ‚àóf(x) ‚àólin2(x, min,max)\nfornin range (3, no, 2): # Loop even points\nx=i n t e r v a l ‚àó(n‚àí1)\n69 sum+= 2 ‚àóf(x) ‚àólin2(x, min,max)\nsum+= f (min)‚àólin2(min,min,max)+f (max)‚àólin2(max,min,max)\nsum ‚àó=i n t e r v a l/6 .\nreturn sum\n73\ndefnumerical(x, u, xp):\nN=1 1 # Interpolate solution\ny=0 .\n77foriin range (0, N‚àí1):\nifxp>=x [ i ]andxp<=x [ i+1 ] :\ny = lin2(xp,x[i],x[i+1]) ‚àóu[i] + lin1(xp,x[i],x[i+1]) ‚àóu[i+1]\nreturny\n81\ndefexact(x): # Analytic solution\nu=‚àíx‚àó(x‚àí3.) / 2.\nreturnu\n85\nforiin range (1, N):\nA[i‚àí1, i‚àí1] = A[i ‚àí1, i‚àí1] + 1. / h\nA[i‚àí1, i] =A[i ‚àí1, i]‚àí1. / h\n89A[i , i ‚àí1] = A[i ‚àí1, i]\nA[i, i] =A[i, i] + 1. / h\nb[i‚àí1, 0] = b[i ‚àí1, 0] + int2(x[i ‚àí1], x[i])\nb[i, 0] = b[i, 0] + int1(x[i ‚àí1], x[i])\n93\nforiin range (1, N): # Dirichlet B C left end\nb[i, 0] = b[i, 0] ‚àí0.‚àóA[i , 0]\nA[i , 0] = 0.\n97A[0, i] = 0.\nA[0, 0] = 1.\nb[0, 0] = 0.\n101foriin range (1, N): # Dirichlet B C right end\nb[i, 0] = b[i, 0] ‚àí1.‚àóA[i , N ‚àí1]\nA[i , N ‚àí1] = 0.\nA[N‚àí1, i] = 0.\n105A[N‚àí1, N‚àí1] = 1.\nb[N‚àí1, 0] = 1.\nsol = solve(A, b)\n109foriin range (0, N):\nu[i] = sol[i, 0]\nforiin range (0, 21):\n113x2[i] = 0.05 ‚àói\nforiin range (0, 21):\nu_fem[i] = numerical(x, u, x2[i])\n117u_exact[i] = exact(x2[i])\nfunct1.plot(pos=(0.05 ‚àói, u_exact[i]))\nfunct2.plot(pos=(0.05 ‚àói, u _ f e m[i]))\nerror[i] = u_fem[i] ‚àíu_exact[i] # Global error\nListing 27.2 LaplaceFEM_2D.py Uses finite-elements to solve the 2D Laplace‚Äôs\nequation.\n# LaplaceFEM_2D.py solve 2D Laplace Eq via Finite elements method; utf ‚àí8coding\n2\n"""""" Dirichlet boundary conditions surrounding four walls\nDomain dimensions: WxH, with 2 triangles per square\nBased on FEM2DL_Box Matlab program in Polycarpou, Intro to the Finite\n6Element Method in Electromagnetics, Morgan &Claypool (2006) """"""\n27.5 Code Listings 541\nfromnumpyimport ‚àó\nfromnumpy.linalg importsolve\n10importpylab as p\nfrommpl_toolkits.mplot3d importAxes3D\n# N u m squares , nodes, triangles , m e s h coords , Initialization\n14\nWidth = 1.; Height = 1.; Nx = 20; Ny = 20; U0 = 100\nXurc = Width; Yurc = Height; Yllc = 0; Xllc = 0\nNs = Nx ‚àóNy; Nn = (Nx + 1) ‚àó(Ny + 1)\n18Dx = (Xurc ‚àíXllc)/Nx; Dy = (Yurc ‚àíYllc)/Ny; Ne = 2 ‚àóNs\nge = zeros(Ne, float)\nx=z e r o s ( N e , float); y= zeros(Ne, float)\nEbcnod = zeros(Ne, int); Ebcval = zeros(Ne, int)\n22node = zeros((Ne + 1, Ne + 1), int)\nforiin range (1, Nn + 1):\nx[i] = (i ‚àí1) % (Nx + 1) ‚àóDx\n26y[i] = floor((i ‚àí1) / (Nx + 1)) ‚àóDy\n# Connectivity Information\nforiin range (1, Ns + 1):\n30node[2 ‚àói‚àí1, 1] = i + floor((i ‚àí1) / Nx)\nnode[2 ‚àói‚àí1, 2] = node[2 ‚àói‚àí1, 1] + 1 + Nx + 1\nnode[2 ‚àói‚àí1, 3] = node[2 ‚àói‚àí1, 1] + 1 + Nx + 1 ‚àí1\nnode[2 ‚àói, 1] = i + floor((i ‚àí1) / Nx)\n34node[2 ‚àói , 2] = node[2 ‚àói, 1]+1\nnode[2 ‚àói , 3] = node[2 ‚àói,1 ]+1+N x+1\n# Dirichlet Boundary Conditions\n38Tnebc = 0\nforiin range (0, Nn):\nifx[i] == Xllc orx[i] == Xurc ory[i] == Yllc:\nTnebc = Tnebc + 1\n42 Ebcnod[Tnebc] = i\nEbcval[Tnebc] = 0\nelify[i] == Yurc:\nTnebc = Tnebc + 1\n46 Ebcnod[Tnebc] = i\nEbcval[Tnebc] = U0\n# Initialize A matrix , b vector , form matrix\n50A = zeros ((Nn + 1, Nn + 1) , float)\nb=z e r o s( ( N n+1 ,1 ), float)\nforein range (1, Ne):\nx21 = x[node[e, 2]] ‚àíx[node[e, 1]]\n54x31 = x[node[e, 3]] ‚àíx[node[e, 1]]\nx32 = x[node[e, 3]] ‚àíx[node[e, 2]]\nx13 = x[node[e, 1]] ‚àíx[node[e, 3]]\ny12 = y[node[e, 1]] ‚àíy[node[e, 2]]\n58y21 = y[node[e, 2]] ‚àíy[node[e, 1]]\ny31 = y[node[e, 3]] ‚àíy[node[e, 1]]\ny23 = y[node[e, 2]] ‚àíy[node[e, 3]]\nJ=x 2 1 ‚àóy31‚àíx31 ‚àóy21\n62\n# Evaluate A matrix , element vector ge\nA[1, 1] = ‚àí(y23 ‚àóy23 + x32 ‚àóx32) / (2 ‚àóJ)\nA[1, 2] = ‚àí(y23 ‚àóy31 + x32 ‚àóx13) / (2 ‚àóJ)\n66A[2, 1] = A[1, 2]\nA[1, 3] = ‚àí(y23 ‚àóy12 + x32 ‚àóx21) / (2 ‚àóJ)\nA[3, 1] = A[1, 3]\nA[2, 2] = ‚àí(y31 ‚àóy31 + x13 ‚àóx13) / (2 ‚àóJ)\n70A[2, 3] = ‚àí(y31 ‚àóy12 + x13 ‚àóx21) / (2 ‚àóJ)\nA[3, 2] = A[2, 3]\nA[3, 3] = ‚àí(y12 ‚àóy12 + x21 ‚àóx21) / (2 ‚àóJ)\nge[1] = 0\n74ge[2] = 0\nge[3] = 0\n542 27 Finite Element Electrostatics ‚äô\n# Evaluate element pe &update A matrix\n78foriin range (1, 4):\nforjin range (1, 4):\nA[node[e, i], node[e, j]] = A[node[e, i], node[e, j]] \\n+A [i, j]\n82 b[node[e, i]] = b[node[e, i]] + ge[i]\n# Imposition of Dirichlet boundary conditions\nforiin range (1, Tnebc):\n86forjin range (1, Nn + 1):\nifj! =E b c n o d [ i ] :\nb[j] = b[j] ‚àíA[j , Ebcnod[i]] ‚àóEbcval[i]\nA[Ebcnod[i], :] = 0\n90A[:, Ebcnod[i]] = 0\nA[Ebcnod[i], Ebcnod[i]] = 1\nb[Ebcnod[i]] = Ebcval[i]\n94# Solution , place on grid , plot\nV = linalg.solve(A, b)\n(X, Y) = p.meshgrid(arange(Xllc , Xurc + 0.1, 0.1 ‚àó(Xurc‚àíXllc)),\narange(Yllc, Yurc + 0.1, 0.1 ‚àó(Yurc‚àíYllc)))\n98Vgrid = zeros((11, 11), float)\nforiinarange(1, 11):\nforjinarange(1, 11):\nforein range (0, Ne):\n102 x2p = x[node[e, 2]] ‚àíX[i, j]\nx3p = x[node[e, 3]] ‚àíX[i, j]\ny2p = y[node[e, 2]] ‚àíY[i, j]\ny3p = y[node[e, 3]] ‚àíY[i, j]\n106 A1 = 0.5 ‚àóabs(x2p ‚àóy3p‚àíx3p ‚àóy2p)\nx2p = x[node[e, 2]] ‚àíX[i, j]\nx1p = x[node[e, 1]] ‚àíX[i, j]\ny2p = y[node[e, 2]] ‚àíY[i, j]\n110 y1p = y[node[e, 1]] ‚àíY[i, j]\nA2 = 0.5 ‚àóabs(x2p ‚àóy1p‚àíx1p ‚àóy2p)\nx1p = x[node[e, 1]] ‚àíX[i, j]\ny21 = y[node[e, 2]] ‚àíy[node[e, 1]]\n114 y1p = y[node[e, 1]] ‚àíY[i, j]\nx21 = x[node[e, 2]] ‚àíx[node[e, 1]]\nA3 = 0.5 ‚àóabs(x1p ‚àóy3p‚àíx3p ‚àóy1p)\ny3p = y[node[e, 3]] ‚àíY[i, j]\n118 x31 = x[node[e, 3]] ‚àíx[node[e, 1]]\nx3p = x[node[e, 3]] ‚àíX[i, j]\ny31 = y[node[e, 3]] ‚àíy[node[e, 1]]\nJ=x 2 1 ‚àóy31‚àíx31 ‚àóy21\n122 if abs(J / 2 ‚àí(A1 + A2 + A3)) <0.00001 ‚àóJ/2 :\nksi = (y31 ‚àó(X[i, j] ‚àíx[node[e, 1]]) ‚àíx31 ‚àó(Y[i, j]\n‚àíy[node[e, 1]])) / J\nita = ( ‚àíy21 ‚àó(X[i, j] ‚àíx[node[e, 1]]) + x21 ‚àó(Y[i,\n126 j]‚àíy[node[e, 1]])) / J\nN1 = 1 ‚àíksi‚àíita\nN2 = ksi\nN3 = ita\n130 Vgrid[i, j] = N1 ‚àóV[node[e, 1]] + N2 ‚àóV[node[e, 2]] \\n+N 3 ‚àóV[node[e, 3]]\n# Plot the finite element solution of V using a contour plot\n134fig = p.figure()\nax = Axes3D(fig)\nax.plot_wireframe(X, Y, Vgrid, color= ‚Äôr‚Äô)\nax.set_xlabel( ‚ÄôX‚Äô)\n138ax.set_ylabel( ‚ÄôY‚Äô)\nax.set_zlabel( ‚ÄôPotential‚Äô )\np.show()",7845
249-Appendix Codes and Animations.pdf,249-Appendix Codes and Animations,"543\nAppendix\nCodes and Animations\nPython Codes (.py sufÔ¨Åx removed)\nSITES.SCIENCE .OREGONSTATE .EDU/~LANDAUR /BOOKS/CODES/\nName Page Description Name Page Description\n3GraphVisual 38 MultiplotsVisual 3Dshapes 38 Visual‚Äôs3Dshapes\n3QMdisks 496 3diskQM,Matplot 3QMdisksVis 496 3diskQM,Vis\nABM 163 ABMODEsolver AdvecLax 516 Advectioneq\nArea 25 SimplescreenI/O AreaFormatted 42 FormattedI/O\nBeam 529 Navier-Stokeseq BeamContour 530 Flowcontours\nBessel 58 Downwardrecur Bisection 120 Bisectionalgorithm\nBound 434 Intregraleqtneigen Bugs 344 Logisticbifurcations\nCirqCNOT 268 QCCNOTgate CirqHalfAdder 269 QC1/2adder\nCirqSwap 255 QCSwap2qubits CirToffoli 264 3qubitCCNOT\nCoastline 313 FractalDboxcount Column 325 Columngrowth\nCWT 220 Continuous\nwaveletsDecaySound 77 Spontaneousdecay\nDFTcomplex 190 ComplexDFT DFTreal 190 RealDFT\nDirectives 42 I/Odirectives,\nescapeDLA 318 Diffusionaggregate\nDWT 221 DiscretewaveletTF EasyMatPlot 38 Matplot2-D\nEasyVisual 38 Visualeasyplot Eigen 136 Matrixeigenvalues\nEMcirc 499 FDTDcircularpol Entropy 345 Shannonentropy\nEntangle 284 Entangledstates EqHeat 462 Heateqsolution\nEqHeatAnimate 457 HeatEqmov EqStringVis 478 WaveeqtnVismov\nEqStringMat 478 WaveeqnMatplot FDTD 497 Finitedifftimedom\nFern 324 1-Dfernfractal Fern3D 324 3Dfernfractal\nFFT 191 FastFourier\ntransformFFTappl 191 FFT +graphs\nFilm 313 Filmdeposition Fit 121 Least-squaresfit\nComputational Physics: Problem Solving with Python ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n544 Appendix Codes and Animations\nPython Codes (.py sufÔ¨Åx removed)\nSITES.SCIENCE .OREGONSTATE .EDU/~LANDAUR /BOOKS/CODES/\nName Page Description Name Page Description\nFourierMatplot 190 InteractiveDFT FullAdder 269 QCFulladder\nGameoflife 312 Gameoflife GradesMatPlot 36 Matplotmultiplots\nGradTape.py 239 TensorFlowGrad\nTapeHadamard 264 QCHadamardGate\nHarmosAnimate 496 Quantumpacket HeatCNTridiag 462 Betterheatalgr\nHubble 250 AIHubbledatafit Hyperfine 146 MatrixHhyperfine\nIntegGauss 98 Gaussian\nquadratureIsingViz 385 Isingmodel\nIslands.pov 327 Raytracing Keras 252 KerasAIfit\nKmeansCluster 249 AIKmeans\nclusteringLagrange 107 Lagrangeinterpoltn\nLaplaceFEM_1D 539 1Dfiniteelement LaplaceFEM_2D 540 2Dfiniteelement\nLaplaceLine 450 Laplaceequation LensGravity 422 GRlightdeflection\nLimits 43 Machineprecision LyapLog 345 Lyapunovcoef\nMatPlot2figs 39 Matplotmultiplots Matrix 130 Matrixarraymult\nMD1D 402 1-DMD MD2D 404 2-DMD\nMDpBC 406 MD,PeriodicBC NeuralNet 247 NeuralNetwork\nNeuron 247 AnAIneuron NewtonCD 120 Newton-Raphsearch\nNewtonNDanimate 144 N-D\nNewton-RaphsonNoiseSincFilter 181 Fourierfiltering\nODEsympy 366 SymbolicHOODE ODEsympy0 366 Paramsfor\nODEsympy\nOracleSim 285 SimulatorGrover\nalgrOracleIBM 286 IBMGroveralgr\nPandaRead 251 Pandastableread Perceptron 249 PerceptronML\nPondMatPlot 40 Matplotscatterplot PrecessHg 418 PrecessionHg\nProteinFold 73 MCProteinfolding PredatorPrey 346 Populationdynamics\nProjectileAir 306 Projectilewithdrag QFT4 284 2qubitQFT\nQFTn 284 NqubitQFT QMC 388 QuantumMC\nQMCbouncer 389 QMCbouncer QuantumEigen 304 Quantumeigenrk4\nQuantumNumerov 303 Schr√∂dingereqtn QuarterPlate 497 FDTD1/2waveplate\nRelOrbits 423 Relativisticorbits Ricci 420 TensorswiSymPy\nrk4 161 rk4ODEsolver rk45 162 Adaptivesteprk4\nScatt 435 Quantumscatt,LS\neqtnScatter3dPlot 41 Matplot3Dscatter\nSGDclass 251 Stochasticgrad\ndescentShor 287 QCShor‚Äôsalgor\nAppendix Codes and Animations 545\nPython Codes (.py sufÔ¨Åx removed)\nsites.science.oregonstate.edu/~landaur/Books/Codes/\nName Page Description Name Page Description\nSierpin 308 Sierpinskygasket Simple3Dplot 40 Matplotsurface\nSimpleNet 248 SimpleAInet SkPolyFit 238 FitwiSkLearn\nSoliton 516 KdeVsolitons SolitonAnimate 516 Solitonmovie\nSpline 120 Splinefitting SplineInteract 120 Interactivesplines\nSqBilliardCM 364 SquareBilliards TelehMat 495 Transmissionline\nTensorBE 238 TensorFlowH\nisotopesTuneNumpy 141 Matrixspeedup\nTensorTest 235 TestTensorFlow TrapMethods 97 Trapezoidrule\nTwoDsol.java 516 Java2DSoliton TwoHgates 265 2Hadamardgates\nUranusNeptune 302 Uranusorbitpertb VisualWorm.ipyn 423 WormholeVizltn\nvonNeuman 99 vonNeumanreject Walk 73 Randomwalk\nWalk3D 73 3Drandomwalk WangLandau 372 Wang-LandauMC\nWaves2D 478 2-Dwaveeq Waves2Danal 478 Analyticmembrane\nWormHole 423 Wormholederivs XplusH 266 X,Hgates- >|1>\nXZHM 266 H,X,Z,Mgates CodesData Variousinputdata\nOnline Animations Directories (multiple Ô¨Åles and multiple formats within)\nmpeg, mp4, avi: require media player like VLC, gif: requires browser\nDirectory Chapter Directory Chapter\nDoublePendulums 16 Fractals 14\nGravityWaves.mp4 19 Laplace&HeatEquations 21,22\nMDSimulations 18 Waves,Shocks,Solitons 23,25\nWavePacketInteractions 24 WavePacket-WavePacketScatt 24\n2DSolitons 25 WavePacketSlits 24",4842
250-References.pdf,250-References,"546\nReferences\nAbarbanel,H.D.I.,Rabinovich,M.I.,andSushchik,M.M.(1993) IntroductiontoNonlinear\nDynamicsforPhysicists, WorldScientific,Singapore.\nAbramowitz,M.andStegun,I.A.(1972) HandbookofMathematicalFunctions, 10thedn,\nU.S.GovernmentPrintingOffice,Washington,DC.\nAddison,P.S.(2002) TheIllustratedWaveletTransformHandbook, InstituteofPhysics\nPublishing,BristolandPhiladelphia,PA.\nAnaConda(2022) AnaConda,open-sourcePythondistributionplatform ,www.anaconda.com/\nproducts/distribution(accessedApril2023).\nAncona,M.G.(2002) ComputationalMethodsforAppliedScience&Engineering, RintonPress,\nPrinceton,NJ.\nAnderson,J.A.,Lorenz,C.D.,andTravesset,A.(2008)HOOMD-blue,generalpurpose\nmoleculardynamicssimulations. J.Compt.Phys. ,227(10),5342;glotzerlab.engin.umich\n.edu/hoomd-blue/(accessedApril2023).\nArfken,G.B.andWeber,H.J.(2001) MathematicalMethodsforPhysicists, Harcourt/Academic\nPress,SanDiego,CA.\nAskar,A.andCakmak,A.S.(1977)Explicitintegrationmethodforthetime-dependent\nSchrodingerequationforcollisionproblems. J.Chem.Phys. ,68,2794.\nBailey,V.A.andTownsend,J.S.(1921)Themotionofelectronsingases. Philos.Mag. ,42,873.\nBarnsley,M.F.,andHurd,L.P.(1992) FractalImageCompression ,A.K.Peters,Wellesley,MA.\nBecker,R.A.(1954) IntroductiontoTheoreticalMechanics ,McGraw-Hill,NewYork.\nvandenBerg,J.C.(ed.)(1999) WaveletsinPhysics ,CambridgeUniversityPress,Cambridge.\nBevington,P.R.andRobinson,D.K.(2003) DataReductionandErrorAnalysisforthePhysical\nSciences,3rdedn,McGraw-Hill,NewYork.\nBleher,S.,Grebogi,C.,andOtt,E.(1990)Bifurcationsinchaoticscattering. PhysicaD,46,87.\nBransden,B.H.andJoachain,C.J.(1991) QuantumMechanics ,2ndedn,CambridgeUniversity\nPress,Cambridge.\nBriggs,W.L.andHenson,V.E.(1995) TheDFT,AnOwner‚ÄôsManual ,SIAM,Philadelphia,PA.\nBunde,A.andHavlin,S.(eds)(1991) FractalsandDisorderedSystems ,Springer-Verlag,Berlin.\nBurgers,J.M.(1974) TheNon-LinearDiffusionEquation;AsymptoticSolutionsandStatistical\nProblems,Reidel,Boston,MA.\nCampesato,O.(2020) TensorFlow2.0PocketPrimer ,MercuryLearningandInformation,\nDulles,VA,Boston,MA,andNewDelhi.\nComputationalPhysics:ProblemSolvingwithPython ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\nReferences 547\nCar,R.andParrinello,M.(1985)Unifiedapproachformoleculardynamicsand\ndensity-functionaltheory. Phys.Rev.Lett. ,55,2471.\nChristiansen,P.L.andLomdahl,P.S.(1981)Numericalstudyof2 +1dimensionalsine-Gordon\nsolitons.PhysicaD,2,482.\nChristiansen,P.L.andOlsen,O.H.(1978)Returneffectforrotationallysymmetricsolitary\nwavesolutionstothesine-Gordonequation. Phys.Lett.,68A,185;(1979) PhysicaScripta ,\n20,531.\nCirq(2023)Anopensourceframeworkforprogrammingquantumcomputers,github.com/\nquantumlib/Cirq(accessedApril2023).\nClarkUniversity(2011) Statistical&ThermalPhysicsCurriculumDevelopmentProject ,\nstp.clarku.edu/(accessedApril2023); DensityofStatesofthe2DIsingModel .\nComputinginScience&Engineering(2015).www.computer.org/csdl/magazine/cs(accessed\nApril2013).\nConda(2023)Package,dependencyandenvironmentmanagementforanylanguage,\ndocs.conda.io/en/latest/(accessedApril2023).\nCooley,J.W.andTukey,J.W.(1965)Analgorithmforthemachinecalculationofcomplex\nFourierseries. Math.Comput. ,19,297.\nCourant,R.,Friedrichs,K.,andLewy,H.(1928)√úberdiepartiellenDifferenzengleichungen\ndermathematischenphysik. Math.Ann. ,100,32.\nCPUG(2009)ComputationalPhysicsdegreeprogramforUndergraduates,\nsites.science.oregonstate.edu/landaur/CPUG/(accessedApril2023).\nCrank,J.andNicolson,P.(1946)Apracticalmethodfornumericalevaluationofsolutionsof\npartialdifferentialequationsoftheheat-conductiontype. Proc.CambridgePhilos.Soc. ,43,50.\nCreateproduction-grademachinelearningmodelswithTensorFlow(2022)(accessedApril\n2023).\nDanielson,G.C.andLanczos,C.(1942)SomeimprovementsinpracticalFourieranalysisand\ntheirapplicationtoX-rayscatteringfromliquids. J.FranklinInst. ,233,365.\nDaubechies,I.(1995) Waveletsandotherphasedomainlocalizationmethods ,Proc.Int.Congr.\nMath.,1, 2,Basel,56,Birkh√§user,Basel.\nDeJong,M.L.(1992)Chaosandthesimplependulum. Phys.Teach. ,30,115.\nDonnelly,D.andRust,B.(2005)ThefastFouriertransformforexperimentalists. Comput.Sci.\nEng.,7,71.\nErcolessi,F.(1997) AMolecularDynamicsPrimer ,www.cse-lab.ethz.ch/wp-content/uploads/\n2013/01/MD-Primer.pdf (accessedApril2023).\nFalkovich,G.andSreenivasan,K.R.(2006)Lessonfromhydrodynamicturbulence,\nPhys.Today ,59,43.\nFamily,F.andVicsek,T.(1985)ScalingoftheactivezoneintheEdenprocessonpercolation\nnetworksandtheballisticdepositionmodel. J.Phys.A,18,L75.\nFeigenbaum,M.J.(1979)Theuniversalmetricpropertiesofnonlineartransformations. J.Stat.\nPhys.,21,669.\nFermi,E.,Pasta,J.,andUlam,S.(1955) StudiesofNonlinearProblems ,DocumentLA-1940,\nLosAlamosNationalLaboratory.\nFetter,A.I.andWalecka,J.D.(1980) TheoreticalMechanicsofParticlesandContinua ,\nMcGraw-Hill,NewYork.\nFeynman,R.P.andHibbs,A.R.(1965) QuantumMechanicsandPathIntegrals ,McGraw-Hill,\nNewYork.\n548 References\nFosdick,L.D,Jessup,E.R.,Schauble,C.J.C.,andDomik,G.(1996) AnIntroductiontoHigh\nPerformanceScientificComputing ,MITPress,Cambridge,MA.\nGarcia,A.L.(2000) NumericalMethodsforPhysics ,2ndedn,PrenticeHall,UpperSaddleRiver,\nNJ.\nGibbs,R.L.(1975)Thequantumbouncer. Am.J.Phys. ,43,25.\nGoodings,D.A.andSzeredi,T.(1992)Thequantumbouncerbythepathintegralmethod.\nAm.J.Phys. ,59,924.\nGoswani,J.C.andChan,A.K.(1999) FundamentalsofWavelets ,Wiley,NewYork.\nGottfried,K.andYan,T.-M.(2004) QuantumMechanics:Fundamentals ,2ndedn,Springer,\nNewYork.\nGould,H.,Tobochnik,J.,andChristian,W.(2006) AnIntroductiontoComputerSimulations\nMethods,3rdedn,Addison-Wesley,Reading,MA.\nGraps,A.(1995)Anintroductiontowavelets. Comput.Sci.Eng. ,2,50.\nHaftel,M.I.andTabakin,F.(1970)Nuclearsaturationandthesmoothnessofnucleon-nucleon\npotentials. Nucl.Phys. ,158,1.\nHanc,J.andTaylor,E.(2004)Fromconservationofenergytotheprincipleofleastaction:\nAstoryline. Am.J.Phys. ,73(7),663. 158,1.\nHartle,J.B.(2003) Gravity,AnIntroductiontoEinstein‚ÄôsGeneralRelativity ,AddisonWesley,\nSanFrancisco,CA.\nHartmann,W.M.(1998) Signals,Sound,andSensation ,AIPPress,Springer,NewYork.\nHaynes,W.M.(ed.)(2017) CRCHandbookofChemistryandPhysics ,TaylorandFrancis,Boca\nRaton,FL.\nHidary,J.D.(2021) QuantumComputing:AnAppliedApproach ,2ndedn,SpringerNatureAG,\nSwitzerland.\nHiggins,R.J.(1976)FastFouriertransform:Anintroductionwithsomeminicomputer\nexperiments. Am.J.Phys. ,44,766.\nHildebrand,F.B.(1956) IntroductiontoNumericalAnalysis ,McGraw-Hill,NewYork.\nHinsen,K.(2013)Softwaredevelopmentforreproducibleresearch. Comput.Sci.Eng. ,2013,60.\nHistoryofPython(2022)https://learnpython.com/blog/history-of-python/(accessedApril\n2023).\nHockney,R.W.andEastwood,J.W.(1988) ComputerSimulationUsingParticles ,AdamHilger,\nBristol.\nHuang,K.(1987) StatisticalMechanics ,Wiley,NewYork.\nHubble,E.(1929)Arelationbetweendistanceandradialvelocityamongextra-galactic\nnebulae.Proc.Natl.Acad.Sci.U.S.A. ,15(3),168.\nIBM(2023) IBMQuantum ,quantum-computing.ibm.com(accessedApril2023).\nInteractivePythonTutorial(2023)www.learnpython.org(accessedApril2023).\nJackson,J.D.(1988) ClassicalElectrodynamics ,3rdedn,Wiley,NewYork.\nJackson,J.E.(1991) AUser‚ÄôsGuidetoPrincipalComponents ,Wiley,NewYork.\nJames,O.,vonTunnzelman,E.,Franklin,P.,andThorne,K.S.(2015)VisualizingInterstellar‚Äôs\nwormhole. Am.J.Phys. ,83,486.\nJolliffe,I.Y.(2002) PrincipalComponentAnalysis ,2ndedn,Springer,NewYork.\nJos√©,J.VandSalatan,E.J.(1998) ClassicalDynamics ,CambridgeUniversityPress,Cambridge.\nJupyter(2022) JupyterNotebook:TheClassicNotebookInterface ,docs.jupyter.org/en/latest/\ninstall.html(accessedApril2023).\nKeller,J.B.(1959)Largeamplitudemotionofastring. Am.J.Phys. ,27,584.\nReferences 549\nKennedy,R.(2006) ThecaseofPollock‚ÄôsFractalsFocusesonPhysics ,NewY orkTimes,2,5\nDecember2006.\nKeras(2023) Keras:ThePythondeeplearningAPI ,keras.io(accessedApril2023).\nKerr,R.P.(1963)Gravitationalfieldofaspinningmassasanexampleofalgebraicallyspecial\nmetrics.Phys.Rev.Lett. ,11,237.\nKittel,C.(2018) IntroductiontoSolidStatePhysics ,8thedn,Wiley,NewYork.\nKoonin,S.E.(1986) ComputationalPhysics ,Benjamin,MenloPark,CA.\nKorteweg,D.J.anddeVries,G.(1895)Onthechangeofformoflongwavesadvancingina\nrectangularcanal,andonanewtypeoflongstationarywaves. Philos.Mag. ,39,4.\nKreyszig,E.(1998) AdvancedEngineeringMathematics ,8thedn,Wiley,NewYork.\nLamb,H.(1993) Hydrodynamics ,6thedn,Cambridge,Cambridge.\nLandau,R.H.(1996) QuantumMechanicsII,ASecondCourseinQuantumTheory ,2ndedn,\nWiley,NewYork.\nLandau,L.D.andLifshitz,E.M.(1971) TheClassicalTheoryofFields ,Pergamon,Oxford.\nLandau,L.D.andLifshitz,E.M.(1976) QuantumMechanics ,Pergamon,Oxford.\nLandau,L.D.andLifshitz,E.M.(1987) FluidMechanics ,Pergamon,Oxford.\nLandau,D.P.andWang,F.(2001)Determiningthedensityofstatesforclassicalstatistical\nmodels:Arandomwalkalgorithmtoproduceaflathistogram. Phys.Rev.E ,64,056101;\nLandau,D.P.,Tsai,S.-H.,andExler,M.(2004)AnewapproachtoMonteCarlosimulations\ninstatisticalphysics:Wang‚ÄìLandausampling. Am.J.Phys. ,72,1294.\nLang,W.C.andForinash,K.(1998)Time-frequencyanalysiswiththecontinuouswavelet\ntransform. Am.J.Phys. ,66,794.\nLangtangen,H.P.(2016) APrimeronScientificProgrammingwithPython ,Springer-Verlag,\nHeidelberg.\nLi,Z.(2014) NumericalMethodsforPartialDifferentialEquations‚ÄìFiniteElementMethod ,\nwww4.ncsu.edu/~zhilin/(accessedApril2023).\nLorenz,E.N.(1963)Deterministicnon-periodicflow. J.Atmos.Sci. ,20,130.\nLotka,A.J.(1925) ElementsofPhysicalBiology ,Williams&Wilkins,Baltimore,MD.\nMacKeown,P.K.(1985)EvaluationofFeynmanpathintegralsbyMonteCarlomethods. Am.J.\nPhys.,53,880.\nMacKeown,P.K.andNewman,D.J.(1987) ComputationalTechniquesinPhysics ,AdamHilger,\nBristol.\nMaestri,J.J.V.,Landau,R.H.andP√°ez,M.J.(2000)Two-particleSchr√∂dingerequation\nanimationsofwavepacket‚Äìwavepacketscattering. Am.J.Phys. ,68,1113.\nMallat,P.G.(1989)Atheoryformultiresolutionsignaldecomposition:Thewavelet\nrepresentation. IEEETrans.PatternAnal.Mach.Intell. ,11(7),674.\nMandelbrot,B.(1967) HowlongisthecoastofBritain?Science ,156,638.\nMandelbrot,B.(1982) TheFractalGeometryofNature ,Freeman,SanFrancisco,CA.\nManneville,P.(1990) DissipativeStructuresandWeakTurbulence ,AcademicPress,SanDiego,\nCA.\nMannheim,P.D.(1983)Thephysicsbehindpathintegralsinquantummechanics. Am.J.Phys. ,\n51,328.\nMarion,J.B.andThornton,S.T.(2019) ClassicalDynamicsofParticlesandSystems ,5thedn,\nHarcourtBraceJovanovich,Orlando,FL.\nMathews,J.(2002) NumericalMethodsforMathematics,ScienceandEngineering ,PrenticeHall,\nUpperSaddleRiver,NJ.\n550 References\nMatplotlib(2023) Matplotlib‚ÄîVisualizationwithPython ,matplotlib.org(accessedApril2023).\nMcCulloch,W.S.andPitts,W.(1943)Alogicalcalculusoftheideasimmanentinnervous\nactivity.Bull.Math.Biophys. ,5,115.\nMetropolis,M.,Rosenbluth,A.W.,Rosenbluth,M.N.,Teller,A.H.,andTeller,E.(1953)\nEquationofstatecalculationsbyfastcomputingmachines. J.Chem.Phys. ,21,1087.\nMoore,T.A.(2013) AGeneralRelativityWorkbook ,UniversityScienceBooks,MillValley,CA.\nMorris,M.S.andThorn,K.S.(1988)Wormholesinspacetimeandtheiruseforinterstellar\ntravel:AtoolforteachingGeneralRelativity. Am.J.Phys. ,56,395.\nMorse,P.M.andFeshbach,H.(1953) MethodsofTheoreticalPhysics ,McGraw-Hill,NewYork.\nMotter,AandCampbell,D.(2013)Chaosatfifty. Phys.Today ,2013,27.\nNelson,M.,Humphrey,W.,Gursoy,A.,Dalke,A.,Kale,L.,Skeel,R.D.,andSchulten,K.(1996)\nNAMD-scalablemoleculardynamics. J.Supercomput.ApplHighPerform.Comput. ,www.ks\n.uiuc.edu/Research/namd(accessedApril2023).\nNesvizhevsky,V.V.,Borner,H.G.,Petukhov,A.K.,Abele,H.,Baessler,S.,Ruess,F.J.,Stoferle,\nT.,Westphal,A.,Gagarski,A.M.,Petrov,G.A.,andStrelkov,A.V.(2002)Quantumstatesof\nneutronsintheEarth‚Äôsgravitationalfield. Nature,415,297.\nNicholson,C.(2022)Thesecretworldinthegapsbetweenbraincells. Phys.Today ,75,26.\nNielsen,M.A.andChuang,I.L.(2010) QuantumComputationandQuantumInformation ,\nCambridgeUniversityPress,CambridgeUK.\nNISTDigitalLibraryofMathematicalFunctions(2022)dlmf.nist.gov(accessedApril2023).\nNolan,J.andNolan,C.(2015) Interstellar-TheWormholeScene ,www.youtube.com/watch?\nv=f3ptQ0CPMmU(accessedApril2023).\nNumericalPython(2023)numpy.org(accessedApril2023).\nOtt,E.(2002) ChaosinDynamicalSystems ,CambridgeUniversityPress,Cambridge.\nOtto,A.(2019) NumericalSimulationsofFluidsandPlasmas ,www.uaf.edu/physics/files/\nSpring_2019/physics-629-syllabus.pdf (accessedApril2023).\nPalmer,K.M.(2016) TheNamelessMouseBehindtheLargest-EverNeuralNetwork ,Wired,\nMarch28,www.wired.com/2016/03/took-neuroscientists-ten-years-map-tiny-slice-brain\n(accessedApril2023).\nParticleDataGroup(2023) TheReviewofParticleProperties ,pdg.lbl.gov(accessedApril2023).\nPeitgen,H.-O.,J√ºrgens,H.,andSaupe,D.(1994) ChaosandFractals ,Springer,NewYork.\nPerlin,K.(2023)NYUMediaResearchLaboratory,mrl.nyu.edu/~perlin(accessedApril2023).\nPlischke,M.andBergersen,B.(1994) EquilibriumStatisticalPhysics ,2ndedn,WorldScientific\nPub.Co.,Singapore.\nPolikar,R.(2023) TheWaveletTutorial ,users.rowan.edu/~polikar/WTtutorial.html(accessed\nApril2023).\nPolycarpou,A.C.(2006) IntroductiontotheFiniteElementMethodinElectromagnetics ,Morgan\n&Claypool,SanRafael,CA.\nPotvin,J.(1993)Computationalquantum-fieldtheory. Comput.Phys. ,7,149.\nPov-Ray,PersistenceofVisionRaytracer(2023)www.povray.org(accessedApril2023).\nPress,W.H.,Flannery,B.P.,Teukolsky,S.A.,andVetterling,W.T.(2007) NumericalRecipes ,\nCambridgeUniversityPress,Cambridge.\nPythonIndexofPackages(2023)pypi.python.org/pypi(accessedApril2023).\nQiskit(2023)Anopen-sourceSDKforworkingwithquantumcomputers,qiskit.org(accessed\nApril2023).\nReferences 551\nRamasubramanian,K.andSriram,M.S.(2000)Acomparativestudyofcomputationof\nLyapunovspectrawithdifferentalgorithms. PhysicaD,139,72.\nRapaport,D.C.(1995) TheArtofMolecularDynamicsSimulation ,CambridgeUniversityPress,\nCambridge.\nRasband,S.N.(1990) ChaoticDynamicsofNonlinearSystems ,Wiley,NewYork.\nReddy,J.N.(1993) AnIntroductiontotheFiniteElementMethod ,2ndedn,McGrawHill,\nNewYork.\nRefson,K.(2000)Moldy,ageneral-purposemoleculardynamicssimulationprogram. Comput.\nPhys.Commun. ,126,310.\nReid,C.C.,Lee,W.-C.,andIngersoll,S.(2016) ResearchonLargestNetworkofCorticalNeurons\nProfiled,neurosciencenews.com/cortical-neural-network-3926(accessedApril2023).\nReynolds,O.(1883)Anexperimentalinvestigationofthecircumstanceswhichdetermine\nwhetherthemotionofwatershallbedirectorsinuous,andofthelawofresistancein\nparallelchannels. Proc.R.Soc.Lond. ,35,84.\nRichardson.L.F.(1961)Problemofcontiguity:Anappendixofstatisticsofdeadlyquarrels.\nGen.Syst.Yearbook ,6,139.\nRohrer,B.(2017) HowNeuralNetworksWork ,e2eml.school/how_neural_networks_work.html\n(accessedApril2023).\nRoman,T.A.(1994)Theinflatedwormhole:AMATHEMATICAanimation. Comput.Phys. ,\n8,480.\nRosenblatt,F.(1958) NewNavyDeviceLearnsByDoing ,NewYorkTimes,8July1958;Preprint\nasamilitaryReport#1196-0-8.\nRowe,A.C.H.andAbbott,P.C.(1995)Daubechieswaveletsandmathematica. Comput.Phys. ,\n9,635.\nRussell,J.S.(1844) Reportofthe14thMeetingoftheBritishAssociationfortheAdvancementof\nScience,JohnMurray,London.\nSander,E.,Sander,L.M.,andZiff,R.M.(1994)Fractalsandfractalcorrelations. Comput.Phys. ,\n8,420.\nSatoh,A.(2011) IntroductiontoPracticeofMolecularSimulation ,Elsevier,Amsterdam.\nScheck,F.(2010) Mechanics,fromNewton‚ÄôsLawstoDeterministicChaos ,5thedn,Springer,\nBerlin.\nScott,A.C.(2007) TheNonlinearUniverse ,Springer-Verlag,Berlin,Heidelberg.\nShannon,C.E.(1948)Amathematicaltheoryofcommunication. BellSyst.Tech.J. ,27,379.\nShaw,C.T.(1992) UsingComputationalFluidDynamics ,PrenticeHall,EnglewoodCliffs,NJ.\nShlens,J.(2003) ATutorialonPrincipalComponentsAnalysis ,www.cs.otago.ac.nz/cosc453/\nstudent_tutorials/principal_components.pdf (accessedApril2023).\nShor‚ÄôsAlgorithm(2023)qiskit.org/textbook/ch-algorithms/shor.html(accessedApril2023).\nSipper,M.(1997) EvolutionofParallelCellularMachines ,Springer-Verlag,Heidelberg.\nSmith,D.N.(1991) ConceptsofObject-OrientedProgramming ,McGraw-Hill,NewYork.\nSmith,S.W.(1999) TheScientistandEngineer‚ÄôsGuidetoDigitalSignalProcessing ,California\nTechnicalPublishing,SanDiego,CA.\nSmith,L.I.(2002) ATutorialonPrincipalComponentsAnalysis ,ourarchive.otago.ac.nz/handle/\n10523/7534(accessedApril2023)\nStetz,A.,Carroll,J.,Chirapatpimol,N.,Dixit,M.,Igo,G.,Nasser,M.,Ortendahl,D.,and\nPerez-Mendez,V.(1973) DeterminationoftheAxialVectorFormFactorintheRadiativeDecay\nofthePion,LBL1707.\n552 References\nStolze,J.andSuter,D.(2004) QuantumComputing,AshortCoursefromTheorytoExperiment,\nWiley-VCHVerlagGmbH&Co,KGaA,Weinheim.\nSullivan,D.(2000) ElectromagneticSimulationsUsingtheFDTDMethods ,IEEEPress,\nNewYork.\nTabor,M.(1989) ChaosandIntegrabilityinNonlinearDynamics ,Wiley,NewYork.\nTaflove,A.andHagness,S.(2000) ComputationalElectrodynamics:TheFiniteDifferenceTime\nDomainMethod ,2ndedn,ArtechHouse,Boston,MA.\nTaghipour,R.,Akhlaghi,T.,andNikkar,A.(2014)Explicitsolutionofthelargeamplitude\ntransversevibrationsofaflexiblestringunderconstanttension. LatinAmericanJournalof\nSolidsandStructures ,11,545‚Äì555.\nTait,R.N.,T.SmyandM.J.Brett(1990) ThinSolidFilms ,187,375.\nTensorFlow2:LinearRegression(2020)techbrij.com/tensorflow-linear-regression-model\n(accessedApril2023).\nThePythonTutorial(2023)docs.python.org/3/tutorial(accessedApril2023).\nThePythonWiki(2023)wiki.python.org(accessedApril2023).\nThijssenJ.M.(1999) ComputationalPhysics ,CambridgeUniversityPress,Cambridge.\nThompson,W.J.(1992) ComputingforScientistsandEngineers ,Wiley,NewYork.\nTickner,J.(2004)SimulatingnuclearparticletransportinstochasticmediausingPerlinnoise\nfunctions.Nucl.Instrum.MethodsPhys.Res.,Sect.B ,203,124.\nVall√©e,O.(2000)Commentonaquantumbouncingball. Am.J.Phys. ,68,672.\nVano,J.A.,Wildenberg,J.C.,Anderson,M.B.,Noel,J.K.,andSprott,J.C.(2006)Chaosin\nlow-dimensionalLotka-Volterramodelsofcompetition. Nonlinearity ,19,2391‚Äì2404.\nVisscher,P.B.(1991)Afastexplicitalgorithmforthetime-dependentSchr√∂dingerequation.\nComput.Phys. ,5,596.\nVold,M.J.(1959)Anumericalapproachtotheproblemofsedimentvolume. J.Colloid.Sci. ,\n14,168.\nVolterra,V.(1926)Fluctuationsintheabundanceofaspeciesconsideredmathematically.\nMem.R.Accad.Naz.deiLincei.Ser.VI ,2,558‚Äì560.\nWang,Y.andKrstic,P.S.(2020)ProspectofusingGrover‚Äôssearchinthe\nnoisy-intermediate-scalequantum-computerera, Phys.Rev.A ,102(4),042609.\nWard,D.W.andNelson,K.A.(2004) FiniteDifferenceTimeDomain,FDTD,Simulationsof\nElectromagneticWavePropagationusingaSpreadsheet ,arxiv.org/abs/physics/0402096\n(accessedApril2023).\nWhineray,J.(1992)Anenergyrepresentationapproachtothequantumbouncer. Am.J.Phys. ,\n60,948.\nWikipedia(2014)en.wikipedia.org/wiki/Principal_component_analysis(accessedApril2023).\nWikipedia(2023) Shor‚Äôs_algorithm ,en.wikipedia.org/wiki/Shor%27s_algorithm(accessed\nApril2023).\nWilliams,G.P.(1997) ChaosTheoryTamed ,JosephHenryPress,Washington,DC.\nWitten,T.A.andSander,L.M.(1981)Diffusion-limitedaggregation,akineticcritical\nphenomenon. Phys.Rev.Lett. ,47,1400;(1983) Phys.Rev.,B2 7,5686.\nWolf,A.,Swift,J.B.,Swinney,H.L.,andVastano,J.A.(1985)DeterminingLyapunovexponents\nfromatimeseries. PhysicaD 16,285.\nWolfram,S.(1983)Statisticalmechanicsofcellularautomata. Rev.Mod.Phys. ,55,601.\nYalcin,O.G.(2021) AppliedNeuralNetworkswithTensorFlow2 ,Apress,Berkeley,CA.\nReferences 553\nYang,C.N.(1952)Thespontaneousmagnetizationofatwo-dimensionalIsingmodel. Phys.\nRev.,85,809.\nYee,K.(1966)NumericalsolutionofinitialboundaryvalueproblemsinvolvingMaxwell‚Äôs\nequationsinisotropicmedia. IEEETrans.AntennasPropag. ,AP-14,302.\nYue,K.,Fiebig,K.M.,Thomas,P.D.,Chan,H.S.,Shakhnovich,E.I.,andDill,A.(1995)Atestof\nlatticeproteinfoldingalgorithms. Proc.Natl.Acad.Sci.U.S.A. ,92,325.\nZabusky,N.J.andKruskal,M.D.(1965)Interactionof""solitons""inacollisionlessplasmaand\ntherecurrenceofinitialstates. Phys.Rev.Lett. ,15,240.\nZhou,M.(2018) ToyNeuralNetworkClassifiesOrientationofLine ,medium.com/colaberry-\nlabs/toy-neural-network-classifies-orientation-of-line-acf143b89c22(accessedApril2023).\nZhou,V.(2022) MachineLearningforBeginners ,towardsdatascience.com/machine-learning-\nfor-beginners-an-introduction-to-neural-networks-d49f22d238f9(accessedApril2023).",20020
251-Index.pdf,251-Index,"555\nIndex\na\nAccuracy 13,53,54,66,90,93,97,117,170,\n176,178,228,230,245,284,416,445,\n451,506,535\nActionpotential 227\nActivationfunction 230,236,246\nAdams-Bashful-Moulton 156\nAdder\nfull 271‚Äì272,276‚Äì277\nhalf 263,271‚Äì272,277\nAdvection 504‚Äì505,508,518,521,522\nAIseeArtificialintelligence(AI)\nAiryfunctions 385,386\nAlgorithm 3,4,9,25,45,50,51,53,55,\n79‚Äì81,84‚Äì89,153‚Äì158,280‚Äì283,\n289‚Äì290,295‚Äì298,316‚Äì317,325‚Äì326,\n370‚Äì374,398‚Äì402,445‚Äì448,456‚Äì463,\n468‚Äì470,484‚Äì487,496‚Äì497,506‚Äì507,\n510,525\nAlias 175‚Äì179,185\nAnaConda 7,8,237,266,274\nAnalogfilters 183\nAnimations 7,30,31‚Äì32,38,42‚Äì43,248,\n362,396,399,400,459,484,485,496,\n509,511,517,545‚Äì547\nAntiferromagnet 368\nArchitecture 17,128\nArrays see alsoMatrices;Python\nvertical 240\nArtificialintelligence(AI) 226,249\ngenerative 226\nneuron seeNeurons\nAsymptotes 335\nAttractors 334,335,359predictable 355\nstrange 355\nAutocorrelationfunction 180‚Äì183\nAxon seeNeurons\nb\nBacktracking 105‚Äì106,127\nBackwardpass 241\nBallisticdeposition 314‚Äì315,319,320,323,\n327\ncorrelated 320\nBasicmachinelanguage 9,10\nBeating 148,161,356,357,471\nBellstates 260,265,273,277\nBesselfunctions 50,56‚Äì59,154\nBias 14‚Äì16,228,229,232,246\nBifurcation 334,335,338,340,359,361\ndiagram 336\ndimensionof 322\nBilliards 361‚Äì363,489\nquantum 488‚Äì489\nBinarynumbers seeNumbers,binary\nBinarypoint 14\nBinning 337\nBisectionalgorithm 102,103,105,107,121,\n297‚Äì298\nBits 11,257\nquantum seeQubits\nreversal 189‚Äì191\nBlackholes 410,413,414,421\nBlochsphere 258,264\nBoltzmanndistribution 106,369,370,374\nBoolean 19‚Äì21,260\nComputational Physics: Problem Solving with Python ,Fourth\nRubinH.Landau,ManuelJ.P√°ez,andCristianC.Bordeianu.\n¬©2024WILEY-VCHGmbH.Published2024byWILEY-VCH\n556 Index\nBoundstates 101‚Äì103,293‚Äì298,301,\n305‚Äì308,363,378‚Äì379,427‚Äì431,435,\n489\nBoundaryconditions 112,151,294,295,371,\n373,375,384‚Äì386,397,408‚Äì409,436,\n441‚Äì444,448‚Äì450,455‚Äì456,458,461,\n467‚Äì470,477‚Äì478,494,497,507,515,\n517,522‚Äì523,525,528‚Äì531,533,538\nBoxcounting 84‚Äì85,316‚Äì319,321\nBra 256‚Äì257,258\nBra-ket 256\nBreakcommand 451\nBurgers‚Äôequation 505,507\nButterflyoperation 188,189,191\nByte 11,12,30,129,143,247\nBytecode 11\nc\nCacheprogramming 129\nCanonicalensemble 369,393,394\nCapacitors 449,451\nCatenary 473‚Äì475,481\nCauchyprincipalvalue 433\nCellularautomata 322,323\nCentraldifferencealgorithm 80‚Äì81\nChaos 338,340,350,353,357,359,360,487,\n489\nFourieranalysisof 359\nofpendulum 350‚Äì353\nphasespace 353,354,358\nChi-squaredmeasure 115,116\nChristoffelsymbols 411‚Äì413\nCirq 266,267,276\ndefinition 266\ninstall 266‚Äì267\nXandHgates 267‚Äì268\nClanguage 6,9,10,17‚Äì19,130,135,142\nClustering 63,240,242‚Äì246\nCodes,tablesof 4‚Äì6,31,32,38,144,157,\n272,393,459,545‚Äì547\nColumn-majororder 129\nCommand-lineinterpreter 10\nCommandshell 237,266\nCompilers 10,18\nComplexnumbers 6,17,19,20,30,134,176,\n187‚Äì189,191,192,258,259Compression 195\nlossless 199\nPCA 216\nwavelets 195\nComputational\nphysics 3‚Äì4\nscience 3‚Äì4\nthinking 4\nComputerlanguages 6,10,13,21,26,130\nControlstructures 18,20‚Äì21\nConvolution 181,183,184,202,206,207,\n209\nConway‚ÄôsGameofLife 322\nCorrelations 180,181,319,323\nauto 180\ncoefficient 117\ngrowth 319‚Äì320\nPCA 215‚Äì221\nCosmologicalconstant 410,413,418\nCost seeLoss\nCourantstabilitycondition 493,494,506\nsee alsovonNeumann\nCovariance 117,217‚Äì221\nCovectorspace 257\nCrank‚ÄìNicolsonmethod 460‚Äì463\nCubicsplines 110‚Äì113,121 see alsoSplines\nCurietemperature 107,367,368,370,374\nCurvaturetensor 411,413\nCurvefitting seeData,fitting\nd\nData\ncompression 195\nfitting 101‚Äì123\ntypes 13\nDataflowgraphs 236\nDecay\nexponential 71,113‚Äì114\nsimulation 71‚Äì72\nspontaneous 113\nDeeplearning 226,229,246,249\nsee alsoMachine,learning\nDeepnet seeMachine,learning\nDensitymatrix 260,275\nDensityofstates 369,374,375\nIndex 557\nDeposition 314\nballistic 314‚Äì315\nDerivatives 79‚Äì83,150\ncentraldifference 379\nforwarddifference 153\nsecond 112,152,296,297,398\nDFT seeDiscreteFouriertransform(DFT)\nDifferentialequations 148‚Äì165,293,303\nalgorithms 153‚Äì158\nboundaryconditions 151\ndynamicalform 151‚Äì153\nEuler‚Äôsrule 153\ninitialconditions 151,153\norder 150,151\npartial 150‚Äì151,441 see alsoPDE‚Äôs\nRunge-Kuttaalgorithm 154\ntypes 150,441\nDifferentiation 79‚Äì100,153,372,403,446,\n497\nDiffuseralgorithm 282\nDiffusion-limitedaggregation 320\nDimension\narray 129\nfractional 309,311,316\nHausdorf-Besicovitch 309\nphysical 129\nschemes 131\nDiracnotation 256‚Äì257,274\nQCversion 274\nDirectproduct 134,218,257‚Äì261,266,\n279‚Äì281\nDiscreteFouriertransform(DFT) 4,169,\n174‚Äì180,183,192,212,277,278\nDispersion 216,503,504,507‚Äì509,512‚Äì514\nrelation 503,504,508\nDouble(s) 14‚Äì17,29,88,156\npendulum 360,361\nprecision 14,29\nDrag 301‚Äì302 see alsoFriction\nDrivingforce 149,161,351,355,357,359,\n360,365,515\nDualadjointspace 257\nDuffingoscillator 365e\nEdges 235,236,379,397,449,450,488,489,\n516,517\nEigenvalues 128,137‚Äì139,141,219‚Äì221,\n262,284,286,293‚Äì308,429‚Äì431,478,\n482\nEinstein 30,68,410‚Äì416,420\nfieldequations 410‚Äì414\nElectrostaticpotential seeLaplace‚Äôsequation\nEllipticintegrals 351‚Äì352,370\nEllismetric 411\nEntangledstates 259‚Äì262,265,277,286\nEntropy 338,340,347,374,376\nEquations 509\nBurgers‚Äô 505\ndifferential 148‚Äì165,293\ndiscrete 70,332\nheat 454‚Äì455\nintegral 427‚Äì438 see alsoIntegral\nequations\nKorteweg-deVries 508\nLaplace‚Äôs 441,452,534,540\nLippmann-Schwinger 431‚Äì432\nmotion 302,303\nNavier-Stokes 504,520‚Äì522,527\nPoisson‚Äôs 441,443,447,533\nSchr√∂dinger 482‚Äì483\nSine-Gordon 514‚Äì517\ntelegraph 497\nVanderPool 365\nwave 466\nErrors 45,56‚Äì58,82,83\nalgorithmic 46,50,87\napproximation50 see alsoErrors,\nalgorithmic\nempirical 50\nintegration 88,94\nminimum 52\nmultiplicative 49\nN-Dintegration 95‚Äì97\nrandom 46\nroundoff 46,48‚Äì57,61,85,88,95,96,153,\n154,510\ntotal 50,51\ntypes 45‚Äì50\nEuler‚Äôsrule 153‚Äì155,379\n558 Index\nEventhorizons 413‚Äì414\nExchangeenergy 368,371\nExecutivesystem 10\nExponentialdecay 69‚Äì72,94,113‚Äì115\nExtrapolateddifference 81‚Äì84\nf\nFactoringalgorithm 283‚Äì285\nFastFouriertransform 169,176,187‚Äì194\nFeigenbaumconstants 337‚Äì338\nFerromagnet 367‚Äì368,371,373\nFeynman\npathintegrals 376‚Äì386\npostulates 376\npropagator 376\nFFTseeFastFouriertransform\nFilters 181,184\nanalog 183\ndigital 185,211\nsinc 186\nwindowed 185\nFinite\ndifferenceequation 70\ndifferencetimedomain 491‚Äì494\ndifferences 70,446,484,524,528\nelements 533‚Äì540\n2Delements 539‚Äì540\nFitting\nbest 108\nglobal 115\ngoodness 116\nleast-squares 114‚Äì119\nlinearleastsquare 116,119\nlocal 115\nNewton-Raphson 120\nnonlinear 119‚Äì120\nFixed-pointnumbers 12\nFixedpointsinmaps 334,355\nFloating-pointnumbers 12‚Äì17,28,33,46,\n132,145,337\nFloats seeFloating-pointnumbers\nFLOPS 158\nFluiddynamics 504‚Äì505,520‚Äì526,528,\n530\nFortran 6,9‚Äì10,129,142\nForwarddifference seeDerivativesFourier\nanalysis 169\nautocorrelationrelation 183\ncomponentsinchaos 360\ndecompositon 170\ndiscretetransform 174‚Äì176 see also\nDiscreteFouriertransform(DFT)\nfasttransform 187‚Äì191 see alsoFast\nFouriertransform\nintegral 172‚Äì173\nquantumtransform 277‚Äì280\nsawtooth 171‚Äì172\nseries 169‚Äì172\nseriesasalgorithm 445\nshort-timetransform 200\ntheorem 170\ntransform 169,172‚Äì173\nFractals 309‚Äì330\ncoastline 315‚Äì319\ndimension 309 see alsoDimension\nplants 312‚Äì313\nPollockpainting 321‚Äì322\ntrees 314\nFriction 159,160,355,357,520,528\ninoscillations 160\ninpendulum 350‚Äì353,355\ninprojectilemotion 301‚Äì302\ninwaves 471‚Äì472\nFunctionalintegration 380 see alsoPath\nintegration\ng\nGalerkindecomposition 535‚Äì536\nGameofLife 322,323,328\nGarbage 45,46,57\nGates\nAND,NAND,NOT,NOR,XOR,OR,state,\nU,Pauli,NOTX,Y ,Z,R œï,S,T\n262‚Äì264\ncontrolledNOT,CNOT 265\ncontrolledZ,CZ 265\nHadamardH 263,267\nlogic 260,262\nmeasurement 264\n2qubit 264\n3qubit 266\nIndex 559\nSWAP 264\nToffoli,CCNOT 266,270\nGaussian\ndistribution 72‚Äì74\nelimination 434\nquadrature 90‚Äì92\nquadraturederivation 91‚Äì92\nGauss-Seidelmethod 448\nGeneralrelativity 410‚Äì426\nGeodesic 411‚Äì415,418\nGeodesicequation 411,412,414,415,418\nGibbsovershoot 172,186,445\nGradienttape 241\nGravitational\nconstant 410\ncurvature 410\nlensing 415‚Äì416\nGreen‚Äôsfunction 376,378,379‚Äì382\nGridpoints 324,325,430,431,435,493,507,\n510,516,524\nGrover‚Äôssearchalgorithm 280‚Äì283\nGrowthmodels 309‚Äì330,332\nh\nHalf-wavefunction 171‚Äì172,203\nHamilton‚Äôsprinciple 376‚Äì378,380\nHarmonics 170,173,179\nHeatbath 394,442,460\nHeatequation 38,454‚Äì456,458‚Äì460,463,\n464,468‚Äì470\nHiddenlayer 230,234‚Äì236\nHilbertspace 256,258,259,261,263,280\nHilberttransform 433\nHuygens‚Äôsprinciple 376\nHydrogenhyperfinestructure 140\ni\nIBMQuantumComputer 256,266,272‚Äì275,\n283\nIEEEfloating-point 12‚Äì15\nImageprocessing 195,246‚Äì248,323\nImportancesampling 97‚Äì98 see alsovon\nNeumann\nInitialconditions 151,153‚Äì154,157‚Äì158,\n301,303‚Äì304,335,343‚Äì344,346,\n354‚Äì359,362,373,387,402‚Äì403,415,417‚Äì418,442,456,458,461,464,\n466‚Äì471,475‚Äì477,489‚Äì490,493‚Äì497,\n509‚Äì511,517‚Äì519\nIntegralequations 427‚Äì438,536\nIntegration 79‚Äì100\nerror 87‚Äì89,94‚Äì95\nGaussianquadrature 90‚Äì92\nmappingpoints 91\nmeanvalue 95‚Äì97\nMonteCarlo 92‚Äì96\nmulti-dimensional 96\nrejectiontechniquesfor 92\nscaling 91\nSimpson‚Äôsrule 86‚Äì87,89\nsplines 112\ntrapezoidrule 85‚Äì86,89\nvariancereduction 97\nvonNeumannrejection 98\nIntegro-differentialequation 427\nIntermittency 335\nInterpolation\nLagrange 109,110\nsplines 111\nInterpreter 17\nInversematrix 128,136,137,212\nIsingmodel 367‚Äì371,373‚Äì375,382\n2D 370,374\nj\nJacobimethod 448\nJupyternotebook 7,31,237,238,274,\n425‚Äì426\nk\nKerasdeeplearning 246,254‚Äì255\nKernel 10,186\nKerrmetric 413‚Äì414 see alsoMetric\nKet 256,260,263‚Äì264\nKmeans 243,244,251,253\nKorteweg-deVriesequation 503,505 see\nalsoEquations\nl\nLagrangeinterpolation 109‚Äì111,113\nLagtime 180,184,358\n560 Index\nLanguages\nBASIC 10\ncompiled 10‚Äì11,17\ncomputer 9\nhigh-level 9\ninterpreted 11\nPython 6‚Äì8\nLaplace‚Äôsequation 441,443‚Äì447,449,452,\n455,456,460,525,527,534,535,\n540‚Äì542\nLatticecomputations 368,379‚Äì383,384\nLatticepoints seeGridpoints\nLax-Wendroffalgorithm 506‚Äì507\nLeapfrogalgorithm seeTimestepping\nLearningrate 234,244,245\nLeast-squaresfitting 101,108,114‚Äì119\nLengthofcoastline 315‚Äì319\nLifetime 71,113‚Äì115,420\nLightdeflection 414‚Äì416\nLimitcycles 342,344‚Äì346,355,358,365\nLinear\nalgebra 117‚Äì119,128,135\ncongruentmethod 62\nleast-squarefitting 116\nregression 116\nsuperposition 151\nLinux 7,10\nLippmann-Schwingerequation 431‚Äì432,\n435\nLoadmodule 10\nLogicgates 262‚Äì266, see alsoGates\nLogisticmap 331‚Äì336,338‚Äì341,344,346,\n347,359\nLoss 230‚Äì232,241\nLotka-Volterramodel 341‚Äì345\nLyapunovcoefficients 338‚Äì340,347\nm\nMachine\nlearning 226\nlearningdata 249\nnumbers 13,46\nprecision 29‚Äì30\nMagneticmaterials 106,368\nMantissa 12,14‚Äì16,28,29,46,52Matplotlib 7,30‚Äì42,245,268,424,\n541\nMatrices 124,127,133,140,145\ncolumn-majororder 129\ncomputing 124‚Äì147\nequations 431\ninversion 127,128,434\nPauli 141,260\nsubroutinelibraries 138\ntri-diagonal 462\nMaxwell‚ÄôsEquations 460,490‚Äì492,499‚Äì501\nMcCulloch-Pittsneuron 228\nMeanvaluetheorem 95\nMemory\narchitecture 128\npages 129\nMetric\nEllis 411,421\nKerr 413\nSchwarzschild 412‚Äì414,418\ntensor 410,411\nMetropolisalgorithm 98,367,370‚Äì374,378,\n382‚Äì384,387\nMicrocanonicalensemble 369,393\nMiller‚Äôsdevice 58\nMLseeMachine,learning\nModelocking 161,355,356\nMoleculardynamics(MD) 393‚Äì409\nMomentumspace 427‚Äì436\nMonteCarlo\nerrorin 96\nintegration 92‚Äì95\nsimulations 60‚Äì78,319,367,370,382,\n394,396\ntechniques 60,68\nMultiresolutionanalysis 206,207\nn\nNAN 17\nNavier-Stokesequation 504,520‚Äì522,\n525‚Äì529\nNeuralnet 228,230,234\nNeuralnetwork 226‚Äì255 see alsoNeuralnet\nNeurons 66,67,226‚Äì230,235,236,246\ncodeforAI 229\nNewton-Cotesmethod 85\nIndex 561\nNewton-Raphsonsearch 103‚Äì106,120\nalgorithm 103,298\nwithbacktracking 105\nNodes 111,112,180,228,230,234‚Äì236,294,\n298,446,456,494,534‚Äì536,538,\n540\nNoise 323\nPerlin 323\nreduction 180‚Äì182\nNonlinear\ndynamics 331,334,350\nlimitcycles 355\nmaps 333,338\nODE 150\noscillations 148‚Äì165 see alsoOscillations\nNonlocalpotentials 427,431\nNonstationarysignals 195,203\nNormal\nmodeexpansions 467‚Äì468\nnumbers 14\nNotebook seeJupiter\nNumbers\nbase 12\nbinary 11\ncomplex 134\nfixed-point 12\nfloating-point 12‚Äì15\nhexadecimal 11\nIEEE 14\nmachine 13\nnormal 14\noctal 11\nrangesof 11\nrepresentationof 11‚Äì17\nsubnormal 14\nuniform 72\nNumericalprecision 430\nNumerovmethod 296,305\nNumPy 131\noptimization 142\nNyquistcriterion 177‚Äì179\nNyquist-Shannoninterpolation 186\no\nObjectscode 10,31,160\nOctalnumbers 11ODE‚Äôs 150‚Äì151,153‚Äì158,293‚Äì308\nsecondorder 300\nOnecyclepopulation 334\nOpenCV 246‚Äì248\nOperatingsystem 10,129\nOptimization 90,245\nOraclealgorithm 280‚Äì283\nOrbits seePlanetary\nOscillations\nanharmonic 149,158,170\ndamped 160\ndoublependulum 361\ndriven 161\nduetoerrors 108,445\nelectromagnetic 491\nfromerrors 110\nFourieranalysisof 169‚Äì172\nharmonic 157,158,170\nisochronous 157,158\nnonlinear 148‚Äì150,157‚Äì160,161,169\ninphasespace 354\npopulations 334,344\nquantum 378,382\nOutputlayer 229\nOverflows 12,13,17,28,53,62,79,300,395,\n445\nOverrelaxation seeRelaxation\np\nPaddingofsignal 178\nPaging seeMemory\nPanda 7,244,253\nPartialdifferentialequations seePDE‚Äôs\nPathintegration 379‚Äì386,391\nPaulimatrices 141,260\nPDE‚Äôs 150,441‚Äì454,466,533\nelliptic 443\nexplicitsolution 484\nhyperbolic 466‚Äì467\nimplicitsolution 484\nparabolic 441,454‚Äì456\ntypes 441\nweakformof 534‚Äì535\nPendulum 359,361\nanalyticsolution 351‚Äì352\nbifurcationdiagram 359\n562 Index\nPendulum ( contd.)\nchaotic 350‚Äì353,356,359\ncoupled 512\ndouble 360‚Äì361\nPerceptrons 228,229,244,245,251\nPerioddoubling 334 see alsoBifurcation\nPeriodicboundaryconditions 371,373,375,\n397,408,493,494\nPerlinnoise 323‚Äì326\nPhantombit 14,15\nPhaseestimationalgorithm 284,285\nPhasespace 301,338,342‚Äì345,350,\n353‚Äì361,363‚Äì365,511‚Äì512\nPhasetransition 345,346,367‚Äì369,374,528\npip 240,267,528\nPixels 235,247,337\nPlanetaryorbits 303‚Äì305,416‚Äì420\nPoisson‚Äôsequation 443,446‚Äì449,527,533\nPopulationdynamics 331‚Äì349\nPopulationextinction 335\nPotentials\ndeltashell 430‚Äì431\nLennard-Jones 395\nmomentumspace 430\nnonlocal 427,431\nPov-Ray 325,326,329\nPower\nPCAcomponent 216\nresiduemethod 61\nspectrum 173,182,360\nPowerShell 8,237,266\nPrecession 303,418‚Äì420\nPrecision 45\nempirical 50\nmachine 29‚Äì30\ntestsof 159\nPredator-preymodels 340‚Äì346\nPredictor-correctormethods 156,164\nPrincipal\ncomponentsanalysis 215‚Äì221\nvalue 433\nvalueintegrals 433\nProblemsolvingparadigm 3\nProgramming 10,18,27\ndesign 26‚Äì27\nquantum 256‚Äì257reproducible 26\nstructured 26\nProjectilemotion 26,27,160,301‚Äì303,308\nPropagator 376,377,380,381\nProteinfolding 68‚Äì69\nPseudocode 26‚Äì28,30,54,71\nPseudorandom seeRandom,numbers\nPulsons 515\nPyramidscheme 207‚Äì211,213\nPython 129\nalgebraictools 24‚Äì25\narrays 21‚Äì23,137\nCanopydistribution 6\ndistributions 6\nI/O 23‚Äì24\nlanguage 6\nlibraries 6\nlinearalgebra 135‚Äì137\nlists 21‚Äì23\npackages 6\nreferences 6\nVisualpackage 31\nq\nQCsee alsoQuantum,computing\nsimulator 275\nQFT 278‚Äì280,284,286 see alsoQuantum,\nFourierTransform(QFT)\nQiskit 274‚Äì282\nQuadrature seeIntegration\nQuantum 101\nbitsseeQubits\nbouncer 385‚Äì386\ncomputing 256‚Äì290\ncomputingoperators 257\nFouriertransform(QFT) 277‚Äì280\nmechanics 293\nscattering 431‚Äì436\nsimulator 274\nwavepackets 482‚Äì502\nQuantumComposer 272‚Äì274,277\nQubits 256‚Äì260,263‚Äì271,273‚Äì284,286‚Äì290\nr\nRadioactivedecay 60,61,69,71,72,321\nRadix 12\nIndex 563\nRAM 129\nRandom 60‚Äì64\nbrainwalks 66‚Äì68\ngenerators 61‚Äì63\nlinearcongruent 61\nnonuniform 96\nnumbers 49,60,61,74,313\npseudo 61\nself-avoidingwalk 68\nsequences 60,61,63‚Äì64\ntestsof 72‚Äì74\nwalks 64‚Äì69,320\nRaytracing 325‚Äì326,329\nRecursion 50,56‚Äì59,154\nRegisters 29,273,284,285\nRegression 116,239,240,246\nRejectiontechniques 97‚Äì98,370\nRelaxation 441‚Äì453,456,460,469,524,525,\n528,529,531\nResonances 110,119‚Äì120,148,160‚Äì162,\n356,357,435\nnonlinear 160‚Äì162\nReynoldsnumber 528,529,531\nRiccicurvaturetensor 411,413\nRiemanntensor 412,413\nrkalgorithm 156,165\nrk4 154,156‚Äì158,162,295‚Äì298,300‚Äì302,\n304,306,352,398,442\nRombergextrapolation 89\nRootmeansquare 64,65,400\nRoundofferrors see alsoErrors\nroundoff 46‚Äì47\nRow-majororder 129\nRunge-Kutta 154‚Äì157,296\ns\nSampling 93,95‚Äì98,174,176‚Äì179,187,207,\n210,214,361,369,370,374‚Äì376\nimportance 97‚Äì98\nSawtoothfunction 169,171,172\nScalarcurvature 411\nScattering 108,110,293‚Äì308,362‚Äì363,427,\n431‚Äì438,488‚Äì490,498‚Äì499\nquantumchaos 487‚Äì490Schr√∂dingerequation 150,293,294,296,\n297,305,306,382,385,427‚Äì431,435,\n460,482‚Äì484,486,487,491,498\ntimedependent 482‚Äì483\nSchwarzschildmetric 412‚Äì414,416,418\nScikit-learn 240‚Äì242\nSearching 101‚Äì127,296,344 see alsoTrial\nanderror\nSecularequation 128\nSeeds 61‚Äì63,65,74,245,320,321,333,335,\n373,384\nSelf\naffineconnection 312‚Äì313\naffinity 312‚Äì314\nlimiting 365\nsimilar 311,312,337\nSeparablestates 259‚Äì262\nSeparatrix 159,352,354,358,512,515\nSeriessummation 47,55,172\nShannonentropy 338,340,347\nShells 9,10,309\nShockwaves 457,466,504‚Äì507\nShor‚Äôsalgorithm 282‚Äì285,289\nSierpi¬¥ nskigasket 310‚Äì312,323\nSigmoidfunction 228‚Äì230,232,233\nSignalprocessing 185\nSignbit 12,15,17\nSignificantfigures/parts 12,46,48,93,107,\n539\nSimpson‚Äôsrule 86‚Äì90,94,95\nSimulation 4,24,30,49,60‚Äì78,160,161,\n228,257,268,301,314‚Äì316,319,323,\n331,335,354,356,361,367‚Äì392,\n393‚Äì532\nSincfilter 177,185‚Äì187\nSine-Gordonequation 503,514‚Äì517\nSingleprecision 13,15‚Äì17,28,29,88,94\nSingularintegrals 427,432‚Äì433\nSkLearn 237‚Äì242,244,251\nSolitons 331,503,504,508‚Äì515,518,520\ncrossing 511‚Äì512\nKdeV 509\nring 517\nSine-Gordon 515\nwaterwave 508\nSpecularreflection 55‚Äì56,362\n564 Index\nSpinstates 140‚Äì142,256,369\nSplines 111\ncubic 110‚Äì113\nnatural 112\nSpontaneousdecay 60,69‚Äì72,78,113,114\nStablestates 335\nStatisticalmechanics 323,340,369‚Äì370,\n373,393,396\nStochasticgradientdescent 234,245‚Äì246,\n253\nStochasticprocesses 69,113\nStrangeattractors 355\nStress-energytensor 411\nStride 129,130,142‚Äì145\nSubnormalnumbers 14,15\nSubroutines 85,90,108,110,114,128,129,\n138,363,434,435\nlibraries/packages 128,135\nSubscripts see alsoDimension\nschemes 129\nSubtractivecancellation 47‚Äì48,54,57,58,\n79,80,82,113,117,153,432\nSuccessiveover-relaxation seeRelaxation\nSupervisedlearning 242,245\nt\nTensorFlow 236‚Äì239,242,246,252\nTensorproduct 258,412‚Äì413\nTexturingimages 326,329\nThermodynamics 340,367‚Äì392,394,396\nThree-bodyproblem 303,363,487,489\nTimedelay 64,301,490\nTimestepping 454,456‚Äì459,462,468‚Äì470,\n485,490,492\nTop-downprogramming 27\nTrainingAI 226,230,231\nTransients 161,169,333,335‚Äì337,357,359,\n360,364,365\nTranspile 275\nTrapezoidrule 85‚Äì86,88,89,94,95,98,175\nTrialanderror 31,101‚Äì124,230,295,386,\n448,451\nTrivialsolutions 128,429\nTwocycle 334\nTwo‚Äôscomplement 12,29u\nUedaoscillator 365\nUncertaintyprinciple 197‚Äì199,205,206\nUnderflows 13,17,28‚Äì29\nUniform\ndistribution 61‚Äì63,72‚Äì74,90\nsequences 62,72\ntestsof 72‚Äì74\nUnix 10\nv\nVanderPoolequation 365\nVariance 97,117,216‚Äì220,245,370\nreduction 97,370\nVectors 33,36,137‚Äì138,152,199,215‚Äì220,\n256‚Äì257,259‚Äì260,266,278,280,\n324‚Äì325,412,434,530\nVelocity-Verletalgorithm 398‚Äì400,402\nVerletalgorithm 398‚Äì402\nVerticalarrays 240\nVirtualmemory 129\nViscosity 472,520‚Äì523,528\nVisualization 6,7,9,24,26,30‚Äì38,69,72,\n172,237,268,274,275,337,369,410,\n420,421,450‚Äì452,457‚Äì459,483,506,\n530\nofvectors 452\nVolumerendering 30\nvonNeumann\nrejection 97‚Äì98,370\nstabilityassessment 400,451,457‚Äì459,\n462,463,469‚Äì471,492,493\nVorticity 525‚Äì529,530\nVPython 7,30‚Äì32,38,39,362,425\nw\nWang-LandauSampling(WLS) 369,\n374‚Äì376\nWave\noncatenary 473‚Äì475\nelectromagnetic 495\nequation 466‚Äì468,472,475\nfunctions 378,383,431,434\npackets 172,466,482,486\nshallowwater 509\nIndex 565\nonstring 466‚Äì481\ntelegraph 497\nWavelets 195,196,222\nbasis 200‚Äì203\ncontinuous 203‚Äì204\nDaubechies 211‚Äì214\ndiscretetransform 205‚Äì214\nmultiresolutionanalysis 206pyramidscheme 207‚Äì211\ntransform 200‚Äì204\nWeakformofPDE 534‚Äì535\nWindows 8,10,72,196,237,266,274,\n337\nWordlength 11,79\nWorkingregisters 29\nWormholes 410,420‚Äì422,425",19823
252-EULA.pdf,252-EULA,WILEY END USER LICENSE AGREEMENT\nGo to www.wiley.com/go/eula to access Wiley‚Äôs ebook EULA.,91
